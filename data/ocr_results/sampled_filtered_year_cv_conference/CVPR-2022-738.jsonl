{"id": "CVPR-2022-738", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Related Work\\n\\nAs enumerating all related literature in this section can be infeasible, we briefly review some of the recent works that are highly related with ours. The issue of amortization error in VAE was raised in [2], after which several semi-amortized methods were attempted [12, 15, 24] that essentially follow a few SVI gradient steps at test time. An alternative line of research approaches the problem by enlarging the representational capacity of the encoder network, including the flow-based models that apply nonlinear invertible transformations to VAE's variational posterior [13, 37].\\n\\nRecently [11] proposed a greedy recursive mixture estimation method for the encoder in VAE, where the idea is to iteratively augment the current mixture with new components to maximally reduce the divergence between the variational and the true posteriors. There were also previous attempts to incorporate additional random variables into inference networks [23]. Although they introduced latent random variables in both encoder and decoder models, their motivation is quite different from our random noise view in the posterior approximation, and they mainly aim at endowing a richer density family and functional capacity.\\n\\nIn parallel, there have been previous attempts to apply the Bayesian approach to the VAE modeling. However, they are in nature different from our random function modeling of the encoder uncertainty. The Bayesian Variational VAE [3] rather focused on modeling uncertainty in the decoder model, and their main focus is how to deal with out-of-distribution samples in the test set, hence more aligned with transfer learning. The Compound VAE [33] also tackled the similar problem of reducing the amortization gap of the VAE, however, their variational density modeling is less intuitive, inferring the latent vector \\\\( z \\\\) and the encoder weights \\\\( W \\\\) from each data instance. Note that we have more intuitive Bayesian inference for the encoder parameters, \\\\( q(W|D) \\\\) given the entire training data \\\\( D \\\\). Their treatment is deemed to augment the latent \\\\( z \\\\) with the weights \\\\( W \\\\) in the conventional VAE. The Variational GP [38], although looking similar to ours, is not specifically aimed for VAE or amortized inference, but for general Bayesian inference. In turn, they built the posterior model using a GP function defined on the Gaussian distributed latent input space, instead of defining GP on the input data as we did.\\n\\n5. Evaluations\\n\\nWe evaluate our Gaussian process VAE model on several benchmarks to show its improved performance over the existing state-of-the-arts. Our focus is two-fold: i) improved test likelihood scores, and ii) faster test time inference than semi-amortized methods. We also contrast with the flow-based models that employ high capacity encoder networks. The competing approaches are as follows. 1) VAE: The standard VAE model with amortized inference [14, 31]. 2) SA: The semi-amortized VAE [12]. We fix the SVI gradient step size as \\\\( 10^{-3} \\\\), but vary the number of SVI steps from \\\\{1, 2, 4, 8\\\\}. 3) IAF: The autoregressive-based flow model for the encoder \\\\( q(z|x) \\\\) [13], which has richer expressive capability than the VAE's post-Gaussian encoder. The number of flows is chosen from \\\\{1, 2, 4, 8\\\\}. 4) HF: The Householder flow encoder model that represents the full covariance using the Householder transformation [37]. The number of flows is chosen from \\\\{1, 2, 4, 8\\\\}. 5) ME: To enlarge the representational capacity of the encoder network, another possible baseline is a mixture model. More specifically, the inference model is defined as:\\n\\n\\\\[\\nq(z|x) = \\\\prod_{m=1}^{M} \\\\alpha(m|x) q_m(z|x),\\n\\\\]\\n\\nwhere \\\\( q_m(z|x) \\\\) are amortized inference models (e.g., having the same network architectures as the VAE's encoder network), and \\\\( \\\\alpha(m|x) \\\\) are mixing proportions, dependent on the input \\\\( x \\\\), which can be modeled by a single neural network. The mixture encoder (ME) model is trained by gradient ascent to maximize the lower bound of \\\\( \\\\log p(x) \\\\) similarly as the VAE. The number of mixture components \\\\( M \\\\) is chosen from \\\\{1, 2, 4, 8\\\\}. 6) RME: The recursive mixture estimation method for the encoder in VAE [11], which showed superiority to ME's blind mixture estimation. 7) GPVAE: Our proposed GP encoder model. The GP means and feature functions have the same network architectures as the VAE's encoder.\\n\\nDatasets.\\n\\nWe use the following five benchmark datasets: MNIST [19], OMNIGLOT [18], CIFAR10 [16], SVHN [26], and CelebA [21]. For CelebA, we use tightly cropped face images of size \\\\((64 \\\\times 64 \\\\times 3)\\\\), and randomly split the data into 80%/10%/10% train/validation/test sets. For the others, we follow the splits provided in the data, with 10% of the training sets held out for validation.\\n\\nNetwork architectures.\\n\\nWe adopt the convolutional neural networks for both encoder and decoder models for all competing approaches. The main reason is that the convolutional networks are believed to outperform fully connected networks for many tasks on the image domain [17, 29, 34]. For the encoder architecture, we first apply \\\\( L \\\\) convolutional layers with \\\\((4 \\\\times 4)\\\\)-pixels kernels, followed by two fully-connected layers with hidden layers dimension \\\\( h \\\\). For the decoder, the input images first go through two fully connected layers, followed by \\\\( L \\\\) transposed convolutional layers with \\\\((4 \\\\times 4)\\\\)-pixels filters. Here, \\\\( L = 3 \\\\) for all datasets except CelebA (\\\\( L = 4 \\\\)), and \\\\( h = 256 \\\\) for the MNIST/OMNIGLOT and \\\\( h = 512 \\\\) for the others. The deep kernel feature functions \\\\( \\\\psi_{m,s}(x) \\\\) in our GPVAE model have exactly the same architecture as the encoder network except that the last fully connected layer is removed. This ensures that the GP functions \\\\( f(x) \\\\) and \\\\( h(x) \\\\) have equal functional capacity to the base encoder network since they are defined to be...\"}"}
{"id": "CVPR-2022-738", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we use the importance weighted estimation (IW AE), where the covariance of the posterior is approximated by a lower triangle matrix with strictly positive diagonals. This ensures positive definiteness (e.g., $\\\\nabla^2 \\\\log p(x)$ having full rank). The IW AE can be applied to ensure positive definiteness (e.g., $\\\\nabla^2 \\\\log p(x)$ having full rank).\\n\\nExperimental setup\\n\\nThe latent dimension is chosen $\\\\text{DIM}_{\\\\text{z}} = 7$. The number of SVI steps in SA, the number of flows in IAF and HF, and the number of mixture components in ME and RME. The superscripts are as follows: $\\\\text{DIM}_{\\\\text{z}}^{(1)} = 7$, $\\\\text{DIM}_{\\\\text{z}}^{(2)} = 7$, $\\\\text{DIM}_{\\\\text{z}}^{(3)} = 7$, $\\\\text{DIM}_{\\\\text{z}}^{(4)} = 7$, $\\\\text{DIM}_{\\\\text{z}}^{(5)} = 7$, $\\\\text{DIM}_{\\\\text{z}}^{(6)} = 7$, $\\\\text{DIM}_{\\\\text{z}}^{(7)} = 7$.\\n\\nThe test log-likelihood scores are summarized in Table 1. The details can be also found in the Supplement.\\n\\nOur GPV AE overall outperforms the competing approaches with the best validation result is chosen. To report the test log-likelihood scores, $\\\\text{LL}_{\\\\text{test}}$, improvement over V AE, but not consistently. SA\u2019s performance is very sensitive to the number of SVI gradient updates, and $\\\\text{LL}_{\\\\text{test}}$ was 35% higher than the statistical significance of the difference between the best model (red) and each competing method. We depict those with $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$. To report the test log-likelihood scores, $\\\\text{LL}_{\\\\text{test}}$, improvement over V AE, but not consistently. SA\u2019s performance is very sensitive to the number of SVI gradient updates, and $\\\\text{LL}_{\\\\text{test}}$ was 35% higher than the statistical significance of the difference between the best model (red) and each competing method. We depict those with $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$. To report the test log-likelihood scores, $\\\\text{LL}_{\\\\text{test}}$, improvement over V AE, but not consistently. SA\u2019s performance is very sensitive to the number of SVI gradient updates, and $\\\\text{LL}_{\\\\text{test}}$ was 35% higher than the statistical significance of the difference between the best model (red) and each competing method. We depict those with $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$. To report the test log-likelihood scores, $\\\\text{LL}_{\\\\text{test}}$, improvement over V AE, but not consistently. SA\u2019s performance is very sensitive to the number of SVI gradient updates, and $\\\\text{LL}_{\\\\text{test}}$ was 35% higher than the statistical significance of the difference between the best model (red) and each competing method. We depict those with $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$. To report the test log-likelihood scores, $\\\\text{LL}_{\\\\text{test}}$, improvement over V AE, but not consistently. SA\u2019s performance is very sensitive to the number of SVI gradient updates, and $\\\\text{LL}_{\\\\text{test}}$ was 35% higher than the statistical significance of the difference between the best model (red) and each competing method. We depict those with $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$. To report the test log-likelihood scores, $\\\\text{LL}_{\\\\text{test}}$, improvement over V AE, but not consistently. SA\u2019s performance is very sensitive to the number of SVI gradient updates, and $\\\\text{LL}_{\\\\text{test}}$ was 35% higher than the statistical significance of the difference between the best model (red) and each competing method. We depict those with $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$. To report the test log-likelihood scores, $\\\\text{LL}_{\\\\text{test}}$, improvement over V AE, but not consistently. SA\u2019s performance is very sensitive to the number of SVI gradient updates, and $\\\\text{LL}_{\\\\text{test}}$ was 35% higher than the statistical significance of the difference between the best model (red) and each competing method. We depict those with $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$. To report the test log-likelihood scores, $\\\\text{LL}_{\\\\text{test}}$, improvement over V AE, but not consistently. SA\u2019s performance is very sensitive to the number of SVI gradient updates, and $\\\\text{LL}_{\\\\text{test}}$ was 35% higher than the statistical significance of the difference between the best model (red) and each competing method. We depict those with $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$. To report the test log-likelihood scores, $\\\\text{LL}_{\\\\text{test}}$, improvement over V AE, but not consistently. SA\u2019s performance is very sensitive to the number of SVI gradient updates, and $\\\\text{LL}_{\\\\text{test}}$ was 35% higher than the statistical significance of the difference between the best model (red) and each competing method. We depict those with $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$. To report the test log-likelihood scores, $\\\\text{LL}_{\\\\text{test}}$, improvement over V AE, but not consistently. SA\u2019s performance is very sensitive to the number of SVI gradient updates, and $\\\\text{LL}_{\\\\text{test}}$ was 35% higher than the statistical significance of the difference between the best model (red) and each competing method. We depict those with $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$. To report the test log-likelihood scores, $\\\\text{LL}_{\\\\text{test}}$, improvement over V AE, but not consistently. SA\u2019s performance is very sensitive to the number of SVI gradient updates, and $\\\\text{LL}_{\\\\text{test}}$ was 35% higher than the statistical significance of the difference between the best model (red) and each competing method. We depict those with $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$. To report the test log-likelihood scores, $\\\\text{LL}_{\\\\text{test}}$, improvement over V AE, but not consistently. SA\u2019s performance is very sensitive to the number of SVI gradient updates, and $\\\\text{LL}_{\\\\text{test}}$ was 35% higher than the statistical significance of the difference between the best model (red) and each competing method. We depict those with $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}$, $\\\\phi_{\\\\text{test}}$, $\\\\phi_{\\\\text{train}}$, $\\\\phi_{\\\\text{valid}}\",null}"}
{"id": "CVPR-2022-738", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We repeat the procedure five times and report the average. To measure the running time, then take the per-batch average. For the test set, we run inference over the entire test set batches, except for one case on CelebA with dim $z = 10$. Inference time.\\n\\n| Model   | SVHN (z = 10) | SVHN (z = 20) | MNIST (z = 10) | OMNIG (z = 10) | CIFAR10 (z = 10) | SVHN (z = 50) | SVHN (z = 50) | CIFAR10 (z = 50) |\\n|---------|---------------|---------------|----------------|----------------|------------------|---------------|---------------|------------------|\\n| GPVAE   | 9.9           | 10.2          | 9.3            | 8.0            | 9.2              | 8.0           | 9.2           | 8.0              |\\n| HF      | 5.1           | 5.7           | 5.1            | 3.4            | 4.4              | 3.4           | 4.4           | 3.4              |\\n| IAF     | 9.7           | 11.6          | 9.8            | 7.0            | 8.4              | 7.0           | 8.4           | 7.0              |\\n| SA      | 5.7           | 6.4           | 5.7            | 3.4            | 4.4              | 3.4           | 4.4           | 3.4              |\\n| RME     | 7.7           | 8.2           | 7.6            | 5.7            | 7.7              | 5.7           | 7.7           | 5.7              |\\n| ME      | 4.8           | 5.7           | 5.1            | 3.4            | 4.4              | 3.4           | 4.4           | 3.4              |\\n\\nCompared to semi-amortized methods, unlike the semi-amortized approaches where one has to perform the SVI gradient adaptation at test time, in our GPVAE model, being quite sensitive to the initial parameters. Exceptions to this are the cases, but slightly underperforms ours on the others. Note that we only report test times for the latent dimension, dim $z$.\"}"}
{"id": "CVPR-2022-738", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Uncertainty vs. posterior approximation difficulty. After the GPV AE model is trained on MNIST with 2D latent space, we evaluate the uncertainty $\\\\text{Tr} V(f(x)|D) + \\\\text{Tr} V(h(x)|D)$, and depict six different instances $x$ in the order of increasing uncertainty values.\\n\\nTop panel shows the true posterior $p_\\\\theta(z|x)$ (contour plots) and the base encoder $N(b(x), c(x)^2)$ (red dots) superimposed (in log scale).\\n\\nBottom panel contains the original inputs $x$ (left) and reconstructed images (right). For the cases with lower uncertainty, the true posteriors are more Gaussian-like. On the other hand, the higher uncertainty cases have highly non-Gaussian true posteriors with multiple modes.\\n\\nTable 4. Test data log-likelihood scores on Binary MNIST with \\\"CNN\\\" and \\\"FC\\\" networks.\\n\\n|          | VAE (1) | SA (1) | SA (4) | IAF (1) | IAF (4) | HF (1) | HF (4) | ME (2) | ME (4) | VLAE (2) | VLAE (4) | RME (2) | RME (4) |\\n|----------|---------|--------|--------|---------|---------|--------|--------|--------|--------|-----------|-----------|---------|---------|\\n| CNN      | -84.49  | -83.64 | -83.85 | -83.37  | -83.08  | -83.82 | -83.87 | -83.77 | -83.83 | -83.14     | -83.09    | -83.18  |         |\\n| FC       | -85.38  | -85.20 | -85.43 | -84.26  | -84.03  | -85.27 | -85.22 | -83.72 | -83.73 | -83.72     | -83.73    |         |         |\\n\\nAnother important benefit of our Bayesian treatment is that we can quantify the uncertainty in posterior approximation. Recall that our GP posterior $p(f, h|D)$ captures the discrepancy between the base encoder $N(b(x), c(x)^2)$ and the true posterior $p_\\\\theta(x)$ via the GP noise processes $f(x)$ and $h(x)$. In particular, the variance $V(f(x)|D)$ (similarly for $h(x)$) at given input $x$, can serve as an indicator that gauges the goodness of posterior approximation via a single Gaussian. For instance, the large posterior variance (high uncertainty) implies that the posterior approximation is difficult, suggesting the true posterior is distinct from a Gaussian (e.g., having multiple modes). On the other hand, if the variance is small (low uncertainty), one can anticipate that the true posterior might be close to a Gaussian. Table 1 illustrates this intuition on the MNIST with 2D latent space, where the uncertainty measured by $\\\\text{Tr} V(f(x)|D) + \\\\text{Tr} V(h(x)|D)$ accurately aligns with Figure 2. True $p(z|x)$ (contour) and two approximates (blue/red). The non-Gaussianity of the true posterior, closely related to the quality of reconstruction. For verification, in Fig. 2 we visually contrast the marginalized $q(z|x)$ (blue) and the base encoder $N(b(x), c(x)^2)$ (red) on the 2D latent MNIST case. Although they look similar, the marginalized one (blue) has a better corrected covariance toward the true posterior in case (b), which helps reducing the approximation error further.\\n\\nResults on Binary MNIST. Performance on binarized input images is shown in Table 4. Our GPV AE performs well compared to IAF and RME although the differences among the competing approaches are not very pronounced compared to real-valued image cases.\\n\\n6. Conclusions\\n\\nWe proposed a novel Gaussian process encoder model to significantly reduce the posterior approximation error of the amortized inference in VAE, while being computationally efficient. Our Bayesian treatment that regards the discrepancy in posterior approximation as a random noise process, leads to improvements in the accuracy of inference within the fast amortized inference framework. It also offers the ability to quantify the uncertainty (variance of stochastic noise) in variational inference, intuitively interpreted as inherent difficulty in posterior approximation. There could be more extensive experimental setups to be carried out, e.g., deeper network architectures, hierarchical models, or larger/structured latent spaces. In this work we aimed to focus on proving our concept mainly, and this was done with popular network architectures on benchmark datasets that we used. We leave this extensive empirical study as our future work. Another important future study not included in this paper is the rigorous analysis of the impact of the series of approximations that we used in our model.\"}"}
{"id": "CVPR-2022-738", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gaussian Process Modeling of Approximate Inference Errors for Variational Autoencoders\\n\\nMinyoung Kim\\nSamsung AI Center Cambridge, UK\\nmikim21@gmail.com\\n\\nAbstract\\nVariational autoencoder (VAE) is a very successful generative model whose key element is the so-called amortized inference network, which can perform test time inference using a single feed forward pass. Unfortunately, this comes at the cost of degraded accuracy in posterior approximation, often underperforming the instance-wise variational optimization. Although the latest semi-amortized approaches mitigate the issue by performing a few variational optimization updates starting from the VAE's amortized inference output, they inherently suffer from computational overhead for inference at test time. In this paper, we address the problem in a completely different way by considering a random inference model, where we model the mean and variance functions of the variational posterior as random Gaussian processes (GP). The motivation is that the deviation of the VAE's amortized posterior distribution from the true posterior can be regarded as random noise, which allows us to view the approximation error as uncertainty in posterior approximation that can be dealt with in a principled GP manner. In particular, our model can quantify the difficulty in posterior approximation by a Gaussian variational density. Inference in our GP model is done by a single feed forward pass through the network, significantly faster than semi-amortized methods. We show that our approach attains higher test data likelihood than the state-of-the-arts on several benchmark datasets.\\n\\n1. Introduction\\nVariational Autoencoder (VAE) \\\\[14, 31\\\\] is a very successful generative model where a highly complex deep nonlinear generative process can be easily incorporated. A key element of the VAE, the deep inference (a.k.a. encoder) network, can perform the test time inference using a single feed forward pass through the network, bringing significant computational speed-up. This feature, known as amortized inference \\\\[130\\\\], allows the VAE to circumvent otherwise time-consuming steps of solving the variational optimization problem for each individual instance at test time, required in the standard variational inference techniques, such as the stochastic variational inference (SVI) \\\\[9\\\\].\\n\\nAs suggested by the recent study \\\\[2\\\\], however, the amortized inference can also be a drawback of the VAE, specifically the accuracy of posterior approximation by the amortized inference network is often lower than the accuracy of the SVI's full variational optimization. There are two general approaches to reduce this amortization error. The first is to increase the network capacity of the inference model (e.g., flow-based models \\\\[13, 37\\\\]). The other direction is the so-called semi-amortized approach \\\\[12, 15, 24, 27\\\\], where the key idea is to use the VAE's amortized inference network to produce a good initial distribution, from which a few SVI steps are performed at test time to further reduce the amortization error, quite similar in nature to the test time model adaptation of the MAML \\\\[6\\\\] in multi-task (meta) learning. Although these models often lead to improved posterior approximation, they raise several issues: Training the models for the former family of approaches is usually difficult because of the increased model complexity; the latter approaches inadvertently suffer from computational overhead of additional SVI gradient steps at test time.\\n\\nIn this paper, we propose a novel approach to address these drawbacks. We retain the amortized inference framework similar to the standard VAE for its computational benefits, but consider a random inference model. Specifically, the mean and the variance functions of the variational posterior distribution are a priori assumed to be Gaussian process (GP) distributed. There are two main motivations for this idea. The first one stems from the suboptimality of the VAE, where the estimated amortized inference network suffers from deviation from the true posteriors. This inaccuracy can be viewed and modeled as uncertainty in the posterior approximation of the deterministic amortized inference network, suggesting the need for a principled Bayesian uncertainty treatment. The second intuition is that the deviation of the VAE's variational posterior distributions from the true posteriors can be naturally regarded as random noise.\"}"}
{"id": "CVPR-2022-738", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Whereas the semi-amortized approaches perform extra SVI gradient updates at test time to account for this noise, we model the discrepancy using a Bayesian neural network (GP), resulting in a faster and more accurate amortized model via principled uncertainty marginalization. Another benefit of the Bayesian treatment is that we can quantify the discrepancy in approximation, which can serve as indicators for goodness of posterior approximations. The inference in our model is significantly faster than that of semi-amortized methods, accomplished by a single feed forward pass through the GP posterior marginalized inference network. We show that our approach attains higher test data likelihood scores than the state-of-the-art semi-amortized approaches and even the high-capacity flow-based encoder models on several benchmark datasets.\\n\\n2. Background\\n\\nLet \\\\( x \\\\in X \\\\) be an input data point and \\\\( z \\\\in \\\\mathbb{R}^d \\\\) be the latent vector. We consider the VAE model:\\n\\n\\\\[\\np(z) = \\\\mathcal{N}(z; 0, I),\\\\quad p_\\\\theta(x|z) = \\\\mathcal{N}(x; g_\\\\theta(z), \\\\sigma^2_x I),\\n\\\\]\\n\\nwhere \\\\( g_\\\\theta : \\\\mathbb{R}^d \\\\to X \\\\) is a (deep) neural network with the weight parameters denoted by \\\\( \\\\theta \\\\), and \\\\( \\\\sigma^2_x \\\\) is the variance of the white noise. For the given data \\\\( D = \\\\{x_i\\\\}_{i=1}^N \\\\), we maximize the data log-likelihood, \\\\( \\\\prod_{i=1}^N \\\\log p_\\\\theta(x_i) \\\\), with respect to \\\\( \\\\theta \\\\) where \\\\( p_\\\\theta(x) = \\\\mathbb{E}_{q(z|x)}[p_\\\\theta(x, z)] \\\\). Due to the infeasibility of the marginal log-likelihood, the variational inference exploits the following inequality,\\n\\n\\\\[\\n\\\\log p_\\\\theta(x) \\\\geq \\\\mathbb{E}_{q(z|x)} \\\\log p_\\\\theta(x, z) - \\\\log q(z|x),\\n\\\\]\\n\\nwhich holds for any density \\\\( q(z|x) \\\\). The inequality becomes tighter as \\\\( q(z|x) \\\\) becomes closer to the true posterior, as the gap equals \\\\( \\\\text{KL}(q(z|x) || p_\\\\theta(z|x)) \\\\). Then we adopt a tractable density family (e.g., Gaussian) \\\\( q_\\\\lambda(z|x) \\\\) parametrized by \\\\( \\\\lambda \\\\), and maximize the lower bound in (2) with respect to \\\\( \\\\lambda \\\\).\\n\\nSince our goal is maximizing the log-marginal, \\\\( \\\\log p_\\\\theta(x) \\\\), we also need to optimize the lower bound with respect to \\\\( \\\\theta \\\\) together with \\\\( \\\\lambda \\\\), either concurrently or in an alternating fashion. Note that at current \\\\( \\\\theta \\\\), the lower bound optimization with respect to \\\\( \\\\lambda \\\\) needs to be specific to each input \\\\( x \\\\), and hence the optimal solution is dependent on \\\\( x \\\\). Formally, we can denote the optimum by \\\\( \\\\lambda^*_\\\\theta(x) \\\\). The stochastic variational inference (SVI) [9] faithfully implements this idea, and the approximate posterior inference for a new input point \\\\( x \\\\) in SVI amounts to solving the ELBO optimization on the fly by gradient ascent. Although this can yield very accurate posterior approximation, it incurs computational overhead since we have to perform full variational optimization for each and every input \\\\( x \\\\). The VAE [14] addresses this problem by introducing a deep neural network \\\\( \\\\lambda(x; \\\\phi) \\\\) with the weight parameters \\\\( \\\\phi \\\\) as a universal function approximator of the optimum \\\\( \\\\lambda^*_\\\\theta(x) \\\\), and optimize the lower bound with respect to \\\\( \\\\phi \\\\). This approach is called the amortized variational inference (AVI). Thus the main benefit of the VAE is computational speed-up as one can simply do feed forward pass through the inference network \\\\( \\\\lambda(x; \\\\phi) \\\\) to perform posterior inference for each \\\\( x \\\\).\\n\\nThe recent study in [2] raised the issue of the amortized inference in the VAE, where the quality of data fitting is degraded due to the approximation error between \\\\( \\\\lambda^*_\\\\theta(x) \\\\) and \\\\( \\\\lambda(x; \\\\phi) \\\\), dubbed the amortization error [465x542]. To retain the AVI's computational advantage and reduce the amortization error, there were attempts to take the benefits of SVI and AVI, which are referred to as semi-amortized variational inference (SAVI) [12, 15, 24]. The key idea is to learn the amortized inference network to produce a reasonably good initial iterate for the follow-up SVI optimization, perhaps just a few steps. This warm-start SVI gradient ascent would be faster than full SVI optimization, and could reduce the approximation error of the AVI. Although the inference in the SAVI is faster than SVI, it still requires gradient ascent optimization at test time, which might be the main drawback. In the next section we propose a novel approach that is much faster than the SAVI, avoiding gradient updates at test time and requiring only feed forward pass through a single network, and at the same time can yield more accurate posterior approximation.\\n\\n3. Gaussian Process Inference Network\\n\\nWe start from the variational density of the VAE, but with slightly different notation, as follows:\\n\\n\\\\[\\nq(z|x, f, h) = \\\\mathcal{N}(z; f(x), \\\\text{Diag}(h(x))),\\n\\\\]\\n\\nwhere \\\\( f, h : X \\\\to \\\\mathbb{R}^d \\\\) are the mean and standard deviation functions of the variational posterior distribution. Note that if we model \\\\( f \\\\) and \\\\( h \\\\) as deterministic functions (neural networks) and optimize their weight parameters (i.e., point estimation), then it reduces to the standard VAE for which \\\\( f \\\\) and \\\\( h \\\\) constitute \\\\( \\\\lambda(x; \\\\phi) \\\\). However, such point estimates may be inaccurate, and we consider random functions following the Bayesian treatment. Specifically we let \\\\( f = [f_1(\\\\cdot), ..., f_d(\\\\cdot)]^T \\\\) and \\\\( h = [h_1(\\\\cdot), ..., h_d(\\\\cdot)]^T \\\\) be independent random GP distributed functions a priori [30],\\n\\n\\\\[\\nf(\\\\cdot) \\\\sim \\\\prod_{j=1}^d \\\\mathcal{GP}(b_j(\\\\cdot), k_m(\\\\cdot, \\\\cdot)),\\n\\\\]\\n\\n\\\\[\\nh(\\\\cdot) \\\\sim \\\\prod_{j=1}^d \\\\mathcal{GP}(c_j(\\\\cdot), k_s(\\\\cdot, \\\\cdot)).\\n\\\\]\"}"}
{"id": "CVPR-2022-738", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To establish a GP framework, and derive an efficient GP posterior approximation via the amortized inference network. For instance, the GP posterior $p(f \\\\mid Y, \\\\theta)$ can serve as a simplified form of the deep kernel [39] by applying the trick [10, 39], here we adopt the linear deep kernel on the outputs of random (scalar) function $h(x)$, which we briefly summarize below.\\n\\nReturning to our GP posterior inference (7), the two GP-priored functions can be written as:\\n\\n$$p(f \\\\mid Y, \\\\theta) \\\\approx \\\\text{inner product in the feature space (of dimension } d \\\\text{), namely}\\n$$\\n\\n$$p(f \\\\mid Y, \\\\theta) = \\\\sum_{j=1}^{d} w^j \\\\psi^j(x, \\\\theta) \\\\psi^j(x, \\\\theta)^\\\\top,$$\\n\\nwhere $w^j \\\\psi^j(x, \\\\theta)$ can be represented as a linear form with an explicit feature space mapping. Consider a feature mapping $\\\\psi(x)$ which follows the 0-mean GP $\\\\mathcal{GP}(0, K(x, x') \\\\propto N^\\\\top x, \\\\mathbf{I})$, namely\\n\\n$$p(f \\\\mid Y, \\\\theta) \\\\approx \\\\text{inner product in the feature space (of dimension } d \\\\text{), namely}\\n$$\\n\\n$$p(f \\\\mid Y, \\\\theta) = \\\\sum_{j=1}^{d} w^j \\\\psi^j(x, \\\\theta) \\\\psi^j(x, \\\\theta)^\\\\top,$$\\n\\nNow, introducing the feature functions $\\\\psi^j(x, \\\\theta)$, where $\\\\psi^j(x, \\\\theta)$ are the GP mean functions which can be modeled by deep neural networks $U^j \\\\approx \\\\mathcal{GP}(0, \\\\mathbf{I})$. The feature functions $\\\\psi^j(x, \\\\theta)$ are denoted by $\\\\psi^j(x, \\\\theta) = \\\\psi^j(x, \\\\theta)$, respectively, where we share the covariance (kernel) parameters of the GP. This way, we can approximately view GP as a special case of Bayesian neural networks. By letting $U^j \\\\approx \\\\mathcal{GP}(0, \\\\mathbf{I})$, we can represent the GP-priored variational posterior $q_U(f \\\\mid Y, \\\\theta)$ as:\\n\\n$$q_U(f \\\\mid Y, \\\\theta) \\\\approx \\\\text{inner product in the feature space (of dimension } d \\\\text{), namely}\\n$$\\n\\n$$q_U(f \\\\mid Y, \\\\theta) = \\\\sum_{j=1}^{d} w^j \\\\psi^j(x, \\\\theta) \\\\psi^j(x, \\\\theta)^\\\\top,$$\\n\\nfor the log-likelihood function $L$ and one has to deal with the difficult normalizing partition function in principle. For simplicity, we do not consider it and regard $L$ as\\n\\n$$L(\\\\theta) \\\\approx \\\\log p(f \\\\mid Y, \\\\theta) - \\\\log \\\\text{disc}_\\\\theta,\\n$$\\n\\nwhere $\\\\text{disc}_\\\\theta$ is the discrepancy between the V AE's point estimate inference model as:\\n\\n$$L(\\\\theta) \\\\approx \\\\log p(f \\\\mid Y, \\\\theta) - \\\\log \\\\text{disc}_\\\\theta,\\n$$\\n\\nClearly, the likelihood model leads to the GP posterior,\\n\\n$$p(f \\\\mid Y, \\\\theta) = \\\\frac{1}{Z(\\\\theta)} \\\\exp \\\\left[ \\\\sum_{i=1}^{m} \\\\log q_U(f_i \\\\mid Y, \\\\theta) \\\\right],$$\\n\\nand more accurate amortized inference model by taking into account uncertainty (stochastic noise) in a principled manner, and more accurate amortized inference model by taking into account uncertainty (stochastic noise) in a principled manner.\\n\\n3.1. Likelihood Model and GP Posterior Inference\\n\\nTo reduce this discrepancy, the semi-amortized approximation techniques in the GP literature [5, 8, 28, 32, 40] can predict the discrepancy $\\\\text{disc}_\\\\theta$ accurately, while their variational lower bound (2), which we denote as:\\n\\n$$\\\\mathcal{L} = \\\\mathbb{E}_{q_U} \\\\left[ \\\\log p(f \\\\mid Y, \\\\theta) - \\\\log q_U(f \\\\mid Y, \\\\theta) \\\\right] - \\\\mathbb{D}_{KL}(q_U(f \\\\mid Y, \\\\theta) \\\\mid \\\\mid p(f \\\\mid Y, \\\\theta)),$$\\n\\nis likely to be generated at test time on the fly. Instead, we aim to establish a valid Bayesian framework, we define a\\n\\n$$q_U(f \\\\mid Y, \\\\theta) \\\\approx \\\\text{inner product in the feature space (of dimension } d \\\\text{), namely}\\n$$\\n\\n$$q_U(f \\\\mid Y, \\\\theta) = \\\\sum_{j=1}^{d} w^j \\\\psi^j(x, \\\\theta) \\\\psi^j(x, \\\\theta)^\\\\top,$$\\n\\nand the true posterior $p(f \\\\mid Y, \\\\theta)$.\\n\\nIf we view the V AE's point estimate inference model as:\\n\\n$$L(\\\\theta) \\\\approx \\\\log p(f \\\\mid Y, \\\\theta) - \\\\log \\\\text{disc}_\\\\theta,\\n$$\\n\\nwe are the GP mean functions which can be modeled by deep neural networks $U^j \\\\approx \\\\mathcal{GP}(0, \\\\mathbf{I})$.\\n\\nThe feature functions $\\\\psi^j(x, \\\\theta)$ are mutually independent random variables.\\n\\nReturning to our GP posterior inference (7), the two GP-priored functions can be written as:\\n\\n$$p(f \\\\mid Y, \\\\theta) \\\\approx \\\\text{inner product in the feature space (of dimension } d \\\\text{), namely}\\n$$\\n\\n$$p(f \\\\mid Y, \\\\theta) = \\\\sum_{j=1}^{d} w^j \\\\psi^j(x, \\\\theta) \\\\psi^j(x, \\\\theta)^\\\\top,$$\\n\\nand the true posterior $p(f \\\\mid Y, \\\\theta)$.\\n\\nIf we view the V AE's point estimate inference model as:\\n\\n$$L(\\\\theta) \\\\approx \\\\log p(f \\\\mid Y, \\\\theta) - \\\\log \\\\text{disc}_\\\\theta,$$\\n\\nwe are the GP mean functions which can be modeled by deep neural networks $U^j \\\\approx \\\\mathcal{GP}(0, \\\\mathbf{I})$.\\n\\nThe feature functions $\\\\psi^j(x, \\\\theta)$ are mutually independent random variables. Given $U^j \\\\approx \\\\mathcal{GP}(0, \\\\mathbf{I})$, we can approximate (9) by\\n\\n$$p(f \\\\mid Y, \\\\theta) \\\\approx \\\\text{inner product in the feature space (of dimension } d \\\\text{), namely}\\n$$\\n\\n$$p(f \\\\mid Y, \\\\theta) = \\\\sum_{j=1}^{d} w^j \\\\psi^j(x, \\\\theta) \\\\psi^j(x, \\\\theta)^\\\\top,$$\\n\\nfor the log-likelihood function $L$ and one has to deal with the difficult normalizing partition function in principle. For simplicity, we do not consider it and regard $L$ as\\n\\n$$L(\\\\theta) \\\\approx \\\\log p(f \\\\mid Y, \\\\theta) - \\\\log \\\\text{disc}_\\\\theta,$$\\n\\nwhere $\\\\text{disc}_\\\\theta$ is the discrepancy between the V AE's point estimate inference model as:\\n\\n$$L(\\\\theta) \\\\approx \\\\log p(f \\\\mid Y, \\\\theta) - \\\\log \\\\text{disc}_\\\\theta,$$\\n\\nClearly, the likelihood model leads to the GP posterior,\\n\\n$$p(f \\\\mid Y, \\\\theta) = \\\\frac{1}{Z(\\\\theta)} \\\\exp \\\\left[ \\\\sum_{i=1}^{m} \\\\log q_U(f_i \\\\mid Y, \\\\theta) \\\\right],$$\\n\\nand the true posterior $p(f \\\\mid Y, \\\\theta)$.\\n\\nIf we view the V AE's point estimate inference model as:\\n\\n$$L(\\\\theta) \\\\approx \\\\log p(f \\\\mid Y, \\\\theta) - \\\\log \\\\text{disc}_\\\\theta,$$\\n\\nwe are the GP mean functions which can be modeled by deep neural networks $U^j \\\\approx \\\\mathcal{GP}(0, \\\\mathbf{I})$.\\n\\nThe feature functions $\\\\psi^j(x, \\\\theta)$ are mutually independent random variables. Given $U^j \\\\approx \\\\mathcal{GP}(0, \\\\mathbf{I})$, we can approximate (9) by\\n\\n$$p(f \\\\mid Y, \\\\theta) \\\\approx \\\\text{inner product in the feature space (of dimension } d \\\\text{), namely}\\n$$\\n\\n$$p(f \\\\mid Y, \\\\theta) = \\\\sum_{j=1}^{d} w^j \\\\psi^j(x, \\\\theta) \\\\psi^j(x, \\\\theta)^\\\\top,$$\\n\\nfor the log-likelihood function $L$ and one has to deal with the difficult normalizing partition function in principle. For simplicity, we do not consider it and regard $L$ as\\n\\n$$L(\\\\theta) \\\\approx \\\\log p(f \\\\mid Y, \\\\theta) - \\\\log \\\\text{disc}_\\\\theta,$$\\n\\nwhere $\\\\text{disc}_\\\\theta$ is the discrepancy between the V AE's point estimate inference model as:\\n\\n$$L(\\\\theta) \\\\approx \\\\log p(f \\\\mid Y, \\\\theta) - \\\\log \\\\text{disc}_\\\\theta,$$\\n\\nClearly, the likelihood model leads to the GP posterior,\\n\\n$$p(f \\\\mid Y, \\\\theta) = \\\\frac{1}{Z(\\\\theta)} \\\\exp \\\\left[ \\\\sum_{i=1}^{m} \\\\log q_U(f_i \\\\mid Y, \\\\theta) \\\\right].$$\"}"}
{"id": "CVPR-2022-738", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"model parameters consist of the parameters in the GP mean by maximizing the lower bound (empirical Bayes). The hood, and hence we can learn the GP variational learning, the objective function that we will determined and compensated by (12).\\n\\n3.3. GP Inference and Learning\\n\\nSince a Gaussian mixture can be approximated by a single Gaussian by the second-order moment matching, and our learned GP posterior (non-zero variances) appears in the GP learning in the next section, we provide an approximation method [1] as the proposal distribution. As it also denote the mean and covariance of Gaussians, it is infeasible to have close-form for this mapping method.\\n\\nNote that (11) can be seen as the final latent inference model for how the deviation from the true posterior marginal data likelihood using our surrogate. The test log-likelihood score, the test likelihood in the decoder can be used to perform reconstruction, and evaluate uncertainties (variance) of the posterior noise. The test likelihood can be used to perform reconstruction, and evaluate uncertainties.\\n\\nBefore we proceed to GP inference and learning in the next section, we provide the derivation for (8) on (11) as a mixture of Gaussians, namely a reparametrization trick [14]. Finally, the first term in (14) is the KL divergence between Gaussian densities, and admits a close form. The second term in (14) is the Gaussian expected variance (variance) of the posterior noise.\\n\\nThe last term in (15) is essentially a Gaussian expected variance (variance) of the posterior noise evaluated at the variational parameters. And our learned GP posterior (non-zero variances) in (8) on (11) as a mixture of Gaussians, namely a reparametrization trick [14]. Finally, the first term in (14) is the KL divergence between Gaussian densities, and admits a close form. The second term in (14) is the Gaussian expected variance (variance) of the posterior noise.\\n\\nWe now discuss how individual terms in the ELBO (14) are evaluated: (a) estimate the marginalized parameters $\\\\theta$, $\\\\psi$, $b$, $c$ of $q(x|\\\\theta, \\\\psi)$, and $z$; (b) estimate the marginalized posterior $p(x|\\\\theta)$, $p(x|\\\\psi)$, $p(x|b)$, $p(x|c)$, $p(x|z)$.\\n\\nThe uncer-\"}"}
{"id": "CVPR-2022-738", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders, 2016. In Proceedings of the Second International Conference on Learning Representations, ICLR. 4, 6\\n\\n[2] Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoencoders. In International Conference on Machine Learning, 2018. 1, 2, 5\\n\\n[3] Erik Daxberger and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Bayesian variational autoencoders for unsupervised out-of-distribution detection. In arXiv preprint, 2019. 5\\n\\n[4] Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks, 2018. In International Conference on Learning Representations. 3\\n\\n[5] A. Dezfouli and E. V. Bonilla. Scalable inference for Gaussian process models with black-box likelihoods, 2015. In Advances in Neural Information Processing Systems. 3\\n\\n[6] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, 2017. 1\\n\\n[7] Adri\u00e0 Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional networks as shallow Gaussian processes, 2019. In International Conference on Learning Representations. 3\\n\\n[8] James Hensman, Nicolas Durrande, and Arno Solin. Variational Fourier features for Gaussian processes. Journal of Machine Learning Research, 18:5537\u20135588, 2017. 3\\n\\n[9] Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 13:1303\u20131347, 2013. 1, 2\\n\\n[10] Wenbing Huang, Deli Zhao, Fuchun Sun, Huaping Liu, and Edward Chang. Scalable Gaussian process regression using deep neural networks, 2015. Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI). 3\\n\\n[11] Minyoung Kim and Vladimir Pavlovic. Recursive inference for variational autoencoders. Advances in Neural Information Processing Systems, 2020. 5, 6\\n\\n[12] Y. Kim, S. Wiseman, A. C. Millter, D. Sontag, and A. M. Rush. Semi-amortized variational autoencoders. In International Conference on Machine Learning, 2018. 1, 2, 5\\n\\n[13] Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improving variational inference with inverse autoregressive flow, 2016. In Advances in Neural Information Processing Systems. 1, 5\\n\\n[14] Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes, 2014. In Proceedings of the Second International Conference on Learning Representations, ICLR. 1, 2, 4, 5\\n\\n[15] R. G. Krishnan, D. Liang, and M. D. Hoffman. On the challenges of learning with inference networks on sparse high-dimensional data. In Artificial Intelligence and Statistics, 2018. 1, 2, 5\\n\\n[16] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images, 2009. Technical report, Computer Science Department, University of Toronto. 5\\n\\n[17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks, 2012. In Advances in Neural Information Processing Systems. 5\\n\\n[18] B. M. Lake, R. R. Salakhutdinov, and J. Tenenbaum. One-shot learning by inverting a compositional causal process, 2013. In Advances in Neural Information Processing Systems. 5\\n\\n[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998. 5\\n\\n[20] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes, 2018. In International Conference on Learning Representations. 3\\n\\n[21] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015. 5\\n\\n[22] C. Lloyd, T. Gunter, M. A. Osborne, and S. J. Roberts. Variational inference for Gaussian process modulated Poisson processes. In International Conference on Machine Learning, 2015. 4\\n\\n[23] Lars Maal\u00f8e, Casper Kaae S\u00f8nderby, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther. Auxiliary deep generative models. In International Conference on Machine Learning, 2016. 5\\n\\n[24] J. Marino, Y. Yisong, and S. Mandt. Iterative amortized inference. In International Conference on Machine Learning, 2018. 1, 2, 5\\n\\n[25] Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, Berlin, Heidelberg, 1996. 3\\n\\n[26] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bisso, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011. 5\\n\\n[27] Yookoon Park, Chris Kim, and Gunhee Kim. Variational Laplace autoencoders. In International Conference on Machine Learning, 2019. 1, 6\\n\\n[28] Joaquin Qui\u00f1onero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research, 6:1939\u20131959, 2005. 3\\n\\n[29] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In arXiv preprint, 2015. 5\\n\\n[30] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006. 2\\n\\n[31] D.J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models, 2014. International Conference on Machine Learning. 1, 5\\n\\n[32] Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs, 2006. In Advances in Neural Information Processing Systems. 3\\n\\n[33] Shang-Yu Su, Shan-Wei Lin, and Yun-Nung Chen. Compound variational auto-encoder, 2019. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 5\"}"}
{"id": "CVPR-2022-738", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In arXiv preprint, 2013.\\n\\nM. K. Titsias. Variational learning of inducing variables in sparse Gaussian processes, 2009. In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics.\\n\\nMichalis K. Titsias, Jonathan Schwarz, Alexander G. Matthews, Razvan Pascanu, and Yee Whye Teh. Functional regularisation for continual learning with Gaussian processes, 2020. International Conference on Learning Representations.\\n\\nJ. M. Tomczak and M. Welling. Improving variational autoencoders using Householder flow, 2016. In Advances in Neural Information Processing Systems, Workshop on Bayesian Deep Learning.\\n\\nDustin Tran, Rajesh Ranganath, and David M. Blei. The Variational Gaussian Process, 2016. International Conference on Learning Representations.\\n\\nAndrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning, 2016. AI and Statistics (AISTATS).\"}"}
