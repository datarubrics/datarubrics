{"id": "CVPR-2023-342", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE international conference on computer vision, pages 5803\u20135812, 2017.\\n\\n[2] Max Bain, Arsha Nagrani, G \u00a8ul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738, 2021.\\n\\n[3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In International Conference on Learning Representations, 2021.\\n\\n[4] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 190\u2013200, 2011.\\n\\n[5] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll \u00b4ar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\\n\\n[6] Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, and Dong Shen. Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss. arXiv preprint arXiv:2109.04290, 2021.\\n\\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, 2019.\\n\\n[8] Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng Huang. Heterogeneous memory enhanced multimodal attention model for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1999\u20132007, 2019.\\n\\n[9] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681, 2021.\\n\\n[10] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In European Conference on Computer Vision, pages 214\u2013229. Springer, 2020.\\n\\n[11] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridging video-text retrieval with multiple choice questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16167\u201316176, 2022.\\n\\n[12] Yuying Ge, Yixiao Ge, Xihui Liu, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie, and Ping Luo. Miles: Visual bert pre-training with injected language semantics for video-text retrieval. Proceedings of the European Conference on Computer Vision (ECCV), 2022.\\n\\n[13] Simon Ging, Mohammadreza Zolfaghari, Hamed Pirsiavash, and Thomas Brox. Coot: Cooperative hierarchical transformer for video-text representation learning. Advances in neural information processing systems, 33:22605\u201322618, 2020.\\n\\n[14] Satya Krishna Gorti, No \u00a8el V ouitsis, Junwei Ma, Keyvan Golestan, Maksims V olkovs, Animesh Garg, and Guangwei Yu. X-pool: Cross-modal language-video attention for text-video retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5006\u20135015, 2022.\\n\\n[15] Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, and Chuang Gan. Location-aware graph convolutional networks for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11021\u201311028, 2020.\\n\\n[16] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2758\u20132766, 2017.\\n\\n[17] Jie Jiang, Shaobo Min, Weijie Kong, Hongfa Wang, Zhifeng Li, and Wei Liu. Tencent text-video retrieval: Hierarchical cross-modal interactions with multi-level representations. IEEE Access.\\n\\n[18] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.\\n\\n[19] Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, and Chang D Yoo. Progressive attention memory network for movie story question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8337\u20138346, 2019.\\n\\n[20] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373, 2017.\\n\\n[21] Alexander Kunitsyn, Maksim Kalashnikov, Maksim Dzabraev, and Andrei Ivaniuta. Mdmmt-2: Multidomain multi-modal transformer for video retrieval, one more step towards generalization. arXiv preprint arXiv:2203.07086, 2022.\\n\\n[22] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9972\u20139981, 2020.\\n\\n[23] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341, 2021.\"}"}
{"id": "CVPR-2023-342", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. Align and prompt: Video-and-language pre-training with entity prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4953\u20134963, 2022.\\n\\nLinjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+ language omni-representation pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2046\u20132065, 2020.\\n\\nXiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu, Wenbing Huang, Xiangnan He, and Chuang Gan. Beyond rnns: Positional self-attention with co-attention for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8658\u20138665, 2019.\\n\\nChengzhi Lin, Ancong Wu, Junwei Liang, Jun Zhang, Wenhang Ge, Wei-Shi Zheng, and Chunhua Shen. Text-adaptive multiple visual prototype matching for video-text retrieval. Advances in Neural Information Processing Systems, 2022.\\n\\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.\\n\\nFei Liu, Jing Liu, Weining Wang, and Hanqing Lu. Hair: Hierarchical visual-semantic relational reasoning for video question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1698\u20131707, 2021.\\n\\nRuyang Liu, Jingjia Huang, Ge Li, Jiashi Feng, Xinglong Wu, and Thomas H Li. Revisiting temporal modeling for clip-based image-to-video knowledge transferring. arXiv preprint arXiv:2301.11116, 2023.\\n\\nYang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In BMVC, 2019.\\n\\nZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3202\u20133211, 2022.\\n\\nEdward Loper and Steven Bird. Nltk: The natural language toolkit. arXiv preprint cs/0205028, 2002.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.\\n\\nHuaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. Univl: A unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353, 2020.\\n\\nHuaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.\\n\\nYiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-clip: End-to-end multi-grained contrastive learning for video-text retrieval. In Proceedings of the 30th ACM International Conference on Multimedia, pages 638\u2013647, 2022.\\n\\nTegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher Pal. A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6884\u20136893, 2017.\\n\\nAntoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9879\u20139889, 2020.\\n\\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2630\u20132640, 2019.\\n\\nJungin Park, Jiyoung Lee, and Kwanghoon Sohn. Bridge to answer: Structure-aware graph interaction network for video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15526\u201315535, 2021.\\n\\nAndrew Rouditchenko, Angie Boggust, David Harwath, Brian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris, et al. Avlnet: Learning audio-visual language representations from instructional videos. In Annual Conference of the International Speech Communication Association. International Speech Communication Association, 2021.\\n\\nPaul Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid. Look before you speak: Visually contextualized utterances. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16877\u201316887, 2021.\\n\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.\\n\\nChen Sun, Austin Myers, Carl VonDrnik, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7464\u20137473, 2019.\\n\\nHao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5100\u20135111, 2019.\"}"}
{"id": "CVPR-2023-342", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atousa Torabi, Niket Tandon, and Leonid Sigal. Learning language-visual embedding for movie understanding with natural-language. arXiv preprint arXiv:1609.08124, 2016.\\n\\nAaron Van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv e-prints, pages arXiv\u20131807, 2018.\\n\\nAlex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. arXiv preprint arXiv:2203.07303, 2022.\\n\\nJinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Object-aware video-language pre-training for retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3313\u20133322, 2022.\\n\\nYi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022.\\n\\nDejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645\u20131653, 2017.\\n\\nHu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze, and Luke Zettlemoyer. Vlm: Task-agnostic video-language model pre-training for video understanding. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4227\u20134239, 2021.\\n\\nDejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645\u20131653, 2017.\\n\\nHu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787\u20136800, 2021.\\n\\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.\\n\\nHongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5036\u20135045, June 2022.\\n\\nHongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo. Clip-vip: Adapting pre-trained image-text model to video-language representation alignment. arXiv preprint arXiv:2209.06430, 2022.\\n\\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686\u20131697, 2021.\\n\\nJianwei Yang, Yonatan Bisk, and Jianfeng Gao. Taco: Token-aware cascade contrastive learning for video-text alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11562\u201311572, 2021.\\n\\nYoungjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV), pages 471\u2013487, 2018.\\n\\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nZheng-Jun Zha, Jiawei Liu, Tianhao Yang, and Yongdong Zhang. Spatiotemporal-textual co-attention network for video question answering. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 15(2s):1\u201318, 2019.\\n\\nLinchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8746\u20138755, 2020.\"}"}
{"id": "CVPR-2023-342", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To get the masked samples for TMA, we conduct masking over the videos and texts to form masked samples using the following two strategies.\\n\\n**Video-block masking strategy.** Objects in a video often appear at similar spatial locations across consecutive frames, which compose a tube area in the spatial-temporal domain. In order to construct incomplete video samples with less information leakage, we extend the block-wise mask [3] to video via performing a block-wise mask at the same position in all video frames. In this paper, we randomly replace 20% patches with a learnable mask token to obtain $V_m$.\\n\\n**Semantic text masking strategy.** The detailed semantic information in the text basically lies in the verb, noun and adjective words. To facilitate learning their representations, we construct incomplete text samples by randomly masking some verbs, nouns and adjectives in the sentence. Specifically, given a sentence, we use a part-of-speech tagger [33] to tag each word. Then we pick the verb phrases and nouns, and replace 30% of them with a special [MASK] token. Note that to avoid drastic semantic changes, we do not mask the auxiliary verb like have, should, will, would, etc. With the masking strategy, when some key elements are masked from the texts, it would form an expression carrying partial information rather than completely changing the information expressed.\\n\\n### 3.3. Training Objective\\n\\n**Pair-wise ranking.** In TMA, we take the masked pairs $\\\\langle V_e, T_m \\\\rangle$ and $\\\\langle V_m, T_e \\\\rangle$ as positive pairs, considering that masked samples would still carry partial meaningful information though some others are missed. For instance, as shown in Fig. 2 (a), given the text \u201cA black swan swimming in a calm lake\u201d, \u201cswimming\u201d and \u201clake\u201d are masked with [MASK] tokens, generating the masked pair $\\\\langle V_e, T_m \\\\rangle$. Although some information is removed, the masked sentence still contains other concepts that make it partially matched with the video, such as the \u201cblack swan\u201d. Therefore, in contrastive learning, $V_m$ and $T_m$ can be considered pseudo positive candidates for $T$ and $V$. However, compared to the original pair $\\\\langle V_e, T_e \\\\rangle$, we consider that the semantic consistency in the masked pairs should be weaker. Based on this prior, we further propose a pair-wise ranking loss:\\n\\n$$L_{\\\\text{rank}} = \\\\max(0, -\\\\frac{\\\\text{sim}(V_e, T_e)}{\\\\tau} - \\\\frac{\\\\text{sim}(V_e, T_m)}{\\\\tau}) + \\\\lambda + \\\\max(0, -\\\\frac{\\\\text{sim}(V_e, T_e)}{\\\\tau} - \\\\frac{\\\\text{sim}(V_m, T_e)}{\\\\tau}) + \\\\lambda),$$\\n\\n(4)\\n\\nwhere $\\\\lambda > 0$ is a margin hyper-parameter. Eq. (4) urges the model to be aware of the gap of semantic consistency between $\\\\langle V_e, T_m \\\\rangle$/$\\\\langle V_m, T_e \\\\rangle$ and $\\\\langle V_e, T_e \\\\rangle$ brought about by concepts missing in the masked pairs. With the pair-wise ranking objective, our model is able to maintain fine-grained perceptual capability while improving its generalizability.\\n\\n**Semantic enhanced masked language modeling.** Masked language modeling (MLM) is a classical pre-training task in VidL, which promotes the interaction between different modalities in the cross-modal encoder. Combining the classical MLM with our semantic text masking strategy, we present the Semantic Enhanced Masked Language Modeling task, which facilitates the representation learning on the key concepts, i.e., verb, noun and adjective words. Besides, considering the class-imbalance between different words, we use focal loss [28] instead of the traditional cross-entropy loss to improve the MLM loss. We apply the MLM loss to the reconstruction of the masked text tokens $\\\\{t_j \\\\in M_T\\\\}$. The MLM loss is defined as:\\n\\n$$L_{\\\\text{mlm}} = -\\\\frac{1}{B} \\\\sum_{i=1}^{B} \\\\sum_{t_j \\\\in M_i} \\\\frac{1}{T_m} \\\\left[ \\\\gamma (1 - p_i t_j) p_i t_j \\\\right],$$\\n\\n(5)\\n\\nwhere $p_i t_j$ denote the predicted probability distribution of the masked token $t_j$ in $i$th sentence in the batch, and $\\\\gamma$ is a hyper-parameter. Finally, the overall pre-training objective of Clover is:\\n\\n$$L = L_{\\\\text{TMA}} + L_{\\\\text{rank}} + L_{\\\\text{mlm}}.$$  \\n\\n(6)\\n\\n### 4. Experiments\\n\\n**4.1. Experiment Setup**\\n\\n**Pre-training datasets.** Following recent work [2, 11, 24], we jointly pre-train our Clover on a video dataset WeBVid2M [2] with 2.5M video-text pairs and an image dataset Google Conceptual Captions (CC3M) [44] with 3.3M image-text pairs (we only obtain 2.8M image-text pairs in CC3M due to image url broken). During pre-training, we treat image data as one frame video data.\\n\\n**Downstream tasks.** We evaluate our proposed models on the following downstream tasks. (a) **Text-to-Video Retrieval** on MSRVTT [55], LSMDC [38] and DiDeMo [1]. For DiDeMo, we follow [23, 31] and evaluate Clover on the paragraph-to-video retrieval, where text sentences for each video are concatenated together as one text query. We do not use the ground-truth proposal for fair comparison with previous works; (b) **Multiple-choice QA** on TGIF-Action [16], TGIF-Transition [16], MSRVTT-MC [60] and LSMDC-MC [47]; (c) **Open-Ended QA** on TGIF-Frame [16], MSRVTT-QA [52], MSVD-QA [52] and LSMDC-FiB [38]. For retrieval tasks, we only use the two uni-modal encoders of Clover for fine-tuning and inference. We adopt the two encoders to get the video and text embeddings, and calculate their cosine similarity for retrieval. For QA tasks, we use all the modules for fine-tuning and inference. More details on these datasets and their evaluation usage are provided in the supplementary material.\"}"}
{"id": "CVPR-2023-342", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Method          | Pre-training dataset | R@1  | R@5  | R@10 | MedR |\\n|-----------------|----------------------|------|------|------|------|\\n| clipBert [23]   | COCO, VG             | 20.4 | 48.0 | 60.8 | 6    |\\n| Frozen [2]      | W2M+C3M              | 31.0 | 59.8 | 72.4 | 3    |\\n| VIOLET [9]      | W2M+C3M+Y180M        | 32.6 | 62.8 | 74.7 | -    |\\n| HD-VILA [56]    | HDV100M              | 28.8 | 57.4 | 69.1 | 4    |\\n| ALPRO [24]      | W2M+C3M              | 35.9 | 67.5 | 78.8 | 3    |\\n| TMVM [27]       | W2M+C3M              | 36.5 | 64.9 | 75.4 | 3    |\\n| All-in-1 [49]   | W2M+H100M            | 32.7 | 61.4 | 73.5 | -    |\\n| OA-Trans [50]   | W2M+C3M              | 34.8 | 64.4 | 75.1 | 3    |\\n| MILS [12]       | W2M+C3M              | 36.6 | 63.9 | 74.0 | 3    |\\n\\nTable 1. Text-to-video retrieval performance comparison under fine-tune and zero-shot setups. Here higher R@k (Recall K) and lower MedR (Median Recall) indicate better performance. W2M, C3M, H100M, HDV100M, Y180M are short for WebVid2M [2], CC3M [44], HowTo100M [40], HD-VILA-100M [56], YT-Temporal-180M [61], respectively.\\n\\n| Method          | Pre-training dataset | R@1  | R@5  | R@10 | MedR |\\n|-----------------|----------------------|------|------|------|------|\\n| JuskAsk [58]    | HTVQA69M [58]        | -    | -    | -    | 41.5 |\\n| ALPRO [24]      | W2M+C3M              | -    | -    | -    | 42.1 |\\n| All-in-1 [49]   | W2M+H100M            | 92.7 | 94.3 | 64.2 | 47.9 |\\n| VIOLET [9]      | W2M+C3M+Y180M        | 92.5 | 95.7 | 68.9 | 47.9 |\\n| MERLOT [61]     | Y180M                | 94.0 | 96.2 | 69.5 | 47.9 |\\n| Clover (ours)   | W2M+C3M              | 95.0 | 98.2 | 71.6 | 52.4 |\\n\\nTable 2. Performance comparison on transferring to downstream video question answering tasks.\"}"}
{"id": "CVPR-2023-342", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Effects of our pre-training tasks for Clover. We report zero-shot text-video retrieval performance on DiDeMo and MSRVTT, and fine-tune video QA performance on LSMDC-MC and TGIF-Frame. TMA, SM and RankL: tri-modal alignment, semantic masking strategy and pair-wise ranking loss.\\n\\n| Method               | R@1  | R@5  | R@10 | MedR | Acc  |\\n|----------------------|------|------|------|------|------|\\n| Baseline             | 22.6 | 47.9 | 58.2 | 7    | 78.8 |\\n| + TMA                | 24.7 | 49.7 | 60.0 | 6    | 80.0 |\\n| + TMA+SM             | 25.3 | 49.5 | 60.5 | 6    | 80.5 |\\n| + TMA+SM+RankL       | 26.4 | 51.1 | 61.3 | 5    | 80.7 |\\n\\nTable 4. Effect of TMA on the video and text representations in learned embedding space. The experiment is conducted on MSRVTT-1kA test set under zero-shot setting.\\n\\n|                      | Averaged similarity scores of positive pairs | Averaged similarity margin between positive and negative samples |\\n|----------------------|---------------------------------------------|---------------------------------------------------------------|\\n| w/o TMA              | 0.56                                        | 0.30                                                         |\\n| w TMA                | 0.65                                        | 0.35                                                         |\\n\\n4.3. Analysis\\n\\nWe conduct ablation experiments on two retrieval datasets (DiDeMo and MSRVTT) and two Video QA datasets (TGIF-Frame and LSMDC-MC). Due to the computational resource limit, we randomly sample 1 million video-text pairs from WebVid2M [2] to build the WebVid-1M subset for model pre-training under all the ablation studies. We keep the model architecture unchanged but pre-train the model with only MLM and InfoNCE losses as a baseline, which is similar to the baseline adopted in VIOLET [9]. The only difference is that we replace the ITM applied to the cross-modal encoder outputs in VIOLET [9] with InfoNCE applied to the outputs of uni-modal encoders and separate the text encoder from the cross-modal encoder. All the ablation experiments are conducted with batch size 1024 on 32 NVIDIA A100 GPUs.\\n\\nEffect of tri-modal alignment (TMA). Tri-modal alignment (TMA) aims to better correlate cross-modal alignment and fusion. To evaluate the contribution of TMA, we remove the pair-wise ranking objective and semantic masking strategy, and employ the classical MLM task as in the baseline method. Compared with the baseline, TMA explicitly associates the outputs of uni-modal encoders with the outputs of the multi-modal encoder. As shown in Tab. 3, the model trained with TMA outperforms the baseline on both retrieval and video QA, demonstrating its effectiveness. For a better understanding of the effect of TMA, we further report averaged cosine similarity of videos and texts in MSRVTT dataset (in zero-shot setting) calculated by models trained with/without TMA, respectively. For a fair comparison, we select all video queries (197 in total) that were successfully responded to by both models. We recall 100 results for each query, and take the ground-truth pairs as positive and others as negative. As shown in Tab. 4, the model armed with tri-modal alignment assigns higher similarity scores to positive pairs compared to the model without it. Meanwhile, with tri-modal alignment, the margin between the positive and negative samples is also increased. It reveals that with the help of tri-modal alignment, the distance between video and text that contain consistent semantics is reduced, while the margin between the misaligned samples is increased. The results demonstrate that with fused multi-modal representation as anchors, tri-modal alignment helps the video and language modalities to be better aligned.\\n\\nEffect of semantic masking strategy. We incorporate a semantic masking strategy to form the masked pairs in tri-modal alignment task. As shown in Tab. 3, on more diverse evaluation benchmarks, e.g. DiDemo, where each video is paired with 5 different ground-truth texts, using masked samples brings more gains. The result reveals that the\"}"}
{"id": "CVPR-2023-342", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Comparisons with task-independently trained models (IND) and naive combination of the uni- and cross-modal encoder (COMB).\\n\\n| Model  | MSRVTT | DiDeMo | TGIF-Frame | LSMDC-MC |\\n|--------|--------|--------|------------|----------|\\n| IND-A  |        |        |            |          |\\n|        | 20.7   |        |            |          |\\n| IND-F  |        |        |            |          |\\n|        |        | 69.0   |            |          |\\n| COMB   |        |        |            |          |\\n|        | 19.8   |        |            |          |\\n| Clover |        |        |            |          |\\n|        | 23.4   |        |            |          |\\n\\nFigure 3. Qualitative results of zero-shot video to text retrieval results on MSRVTT [55].\\n\\nLoaded semantic masking strategy facilitates the model to capture key semantic information in more complex scenes and makes it achieve better results.\\n\\nEffect of pair-wise ranking. To make the model capture the semantic information difference between the masked pairs and complete pairs, we adopt the pair-wise rank loss. As shown in Tab. 3, with pair-wise ranking loss, the performance of the model is further improved. We also show the visualization results below to further illustrate the effect of pair-wise ranking.\\n\\nClover makes cross-modal alignment and fusion mutually improving. We compare Clover with the ones that use the same architecture but employ different pre-training tasks in Tab. 5. IND-A represents the model only trained with InfoNCE loss for cross-modal alignment. IND-F indicates the model trained with MLM loss for cross-modal fusion. We also report results achieved by the model (i.e., COMB) that simply combines the uni-modal encoders and multi-modal encoder as well as the training objectives. We can see that the simple combination hurts the performance, while our Clover achieves superior performance than the task-independently trained IND-A and IND-F. It reveals that Clover is able to get the model's cross-modal alignment and fusion capability mutually enhanced.\\n\\nQualitative analysis. In Fig. 3 and Fig. 4, we show the qualitative results of Clover and the baseline method on zero-shot video-text retrieval and VQA tasks. Specifically, we present the query videos with the matched texts, and the video-question pair with the answer. Clover empowers the model with stronger video-text understanding capability. For example, for the second video in Fig. 3, Clover returns the text with \u201cpaper airplane\u201d, which is a more accurate description of the video content; for the first video in Fig. 4, Clover generates a more accurate answer \u201cLion\u201d than the baseline answer \u201cAnimal\u201d. The results demonstrate the superiority of Clover.\\n\\n5. Conclusion\\n\\nIn this paper, we present Clover, a new end-to-end Video-Language pre-training method for both high-efficiency video-text retrieval and video question answering. Clover introduces a novel Tri-modal Alignment task to better align the representations from visual, text and fused modalities, which explicitly correlate the uni-modal encoder and multi-modal encoder. It also introduces semantic masking strategy and pair-wise ranking loss to further improve the cross-modality modal training. Extensive experiments conducted on the three retrieval datasets and eight video QA datasets clearly demonstrated, as a general video-text model, its consistent superiority for video-text understanding.\\n\\nAcknowledgements. This work was supported by National Key R&D Program of China (No.2022ZD0118201), the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. 62176222, No. 62176223, No. 62072386, No. 62072387, No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province of China (No.2021J01002, No.2022J06001).\"}"}
{"id": "CVPR-2023-342", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nBuilding a universal Video-Language model for solving various video understanding tasks (e.g., text-video retrieval, video question answering) is an open challenge to the machine learning field. Towards this goal, most recent works build the model by stacking uni-modal and cross-modal feature encoders and train it with pair-wise contrastive pre-text tasks. Though offering attractive generality, the resulted models have to compromise between efficiency and performance. They mostly adopt different architectures to deal with different downstream tasks. We find this is because the pair-wise training cannot well align and fuse features from different modalities. We then introduce Clover\u2014a Correlated Video-Language pre-training method\u2014towards a universal Video-Language model for solving multiple video understanding tasks with neither performance nor efficiency compromise. It improves cross-modal feature alignment and fusion via a novel tri-modal alignment pre-training task. Additionally, we propose to enhance the tri-modal alignment via incorporating learning from semantic masked samples and a new pair-wise ranking loss. Clover establishes new state-of-the-arts on multiple downstream tasks, including three retrieval tasks for both zero-shot and fine-tuning settings, and eight video question answering tasks.\\n\\nCodes and pre-trained models will be released at https://github.com/LeeYN-43/Clover.\\n\\n1. Introduction\\n\\nVideo-Language pre-training (VidL) aims to learn generalizable multi-modal models from large-scale video-text samples so as to better solve various challenging Video-Language understanding tasks, such as text-video retrieval [1, 4, 38, 55] and video question answering [16, 47, 52]. Recent studies [9,11,23,24,49,56,58,61] have shown that VidL leads to significant performance improvement and achieves state-of-the-art results on various downstream text-video retrieval and video question answering (VQA) benchmarks.\\n\\nThough achieving encouraging performance, existing VidL models mostly adopt different architectures to deal with different downstream tasks. For the text-video retrieval tasks, they typically use two individual uni-modal encoders for processing video and text data separately, for the sake of retrieval efficiency. While for video question answering tasks, the models usually adopt the multi-modal joint encoder design to learn the association and interaction of different modalities.\\n\\nBuilding a unified model capable of solving various Video-Language tasks is a long-standing challenge for machine learning research. A few recent works [9, 24] attempt to learn a unified VidL model for both tasks, which uses the multi-modal encoder to conduct text-video retrieval. However, the model requires an exhaustive pair-wise comparison between the query texts and gallery videos. Given $N$ text queries and $M$ category videos, the computation complexity of the multi-modal encoder model would be $O(NM)$, which makes it infeasible for large-scale video-text retrieval applications. Another straightforward solution is to simply combine the uni-modal and multi-modal encoders (Fig. 1 (a)), and perform the retrieval and VQA tasks through the uni-modal and multi-modal encoders respectively. Its computation complexity for retrieval tasks is only $O(N+M)$. However, the experiment results in Fig.1 (c) show that, without a carefully designed correlating mechanism between these two types of encoders, the simple combination i.e., COMB yields a compromised performance.\"}"}
{"id": "CVPR-2023-342", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"formance compared with the models i.e. IND individually designed for retrieval and VQA. In this work, we aim to address the above issues and build a unified pre-training model that attains high efficiency and performance simultaneously. We observe: (i) well aligning the features of the video and text from the same data pair is important for text-video matching; (ii) effectively fusing video and text features into unified representations is critical for video-text understanding. However, existing pre-training strategies that rely on either simple supervised or contrastive pre-text tasks hardly achieve promising feature alignment and fusion capability simultaneously. Motivated by these observations, we develop a new VidL method from these two aspects. Specifically, we propose Correlated Video-Language pre-training (Clover), a VidL method that not only unifies Video-Language alignment and fusion, but also makes them mutually boosted. The fused multi-modal representation contains richer context information than the uni-modal representations [11, 35]. As an intermediate modality between video and text, the multi-modal representations are good anchors for cross-modality alignment. Meanwhile, keeping the fused representation closer to the uni-modal representation containing consistent semantic information and away from the inconsistent one will enhance the learning of semantic information in the fused modality. Therefore, we propose the Tri-Modal Alignment (TMA) to get Video-Language alignment and fusion mutually boosted, which takes the alignment between the multi-modal representation and text/video representations as an auxiliary objective. We note that since the tri-modal alignment is well compatible with the classical pre-training tasks e.g., Masked Language Modeling, its computation overhead is negligible. To help the model maintain fine-grained discriminative capability while improving its generalizability, we further introduce a pair-wise ranking loss that urges the model to be aware of the concept missing in masked samples compared to original samples. Extensive experiments are conducted on multiple downstream tasks, including three retrieval tasks with different experimental setups (i.e. zero-shot and fine-tune) and eight video question answering tasks. The results demonstrate that Clover is able to get the cross-modal fusion and alignment capability mutually improved, and consistently outperforms current SOTAs on various downstream tasks. It achieves an average performance improvement of 4.9% and 8.7% Recall@10 score on the zero-shot and fine-tune settings of the three downstream retrieval datasets, while the average accuracy improvement over current SOTAs is 2.3% on the eight video question answering datasets.\\n\\nIn summary, we make the following contributions: (1) we introduce Clover, a pre-training method achieving unified Video-Language alignment and fusion model that can be easily transferred to various downstream video understanding tasks while attaining both high efficiency and performance; (2) we propose a novel tri-modal alignment pre-training task, which correlates the uni-modal encoder and multi-modal encoder to get them mutually boosted.\\n\\n2. Related Work\\n\\nVideo-Language pre-training for text-video retrieval. Existing pre-training methods for text-video retrieval can be mainly divided into two categories. The first kind of method employs two individual encoders to embed video and text, and project them into a common latent space [2, 10, 11, 13, 31, 39, 42, 50, 54, 59]. Then, they leverage a contrastive objective [48] to align the paired representations. Thanks to their high efficiency, this category of methods is widely used in real-world applications. The other kind of method utilizes a joint multi-modal encoder to learn comprehensive representation for a given video-text pair, and predict whether the video and text are matched or not via a binary classifier [9, 23, 25, 45, 53, 63]. Despite the good performance they achieved, they need to exhaustively pair all the videos and texts, and feed them into the model during inference, making them highly inefficient.\\n\\nVideo-Language pre-training for video question answering. Video question answering aims to automatically answer natural language questions given a context video, which requires a joint understanding of both the video and language. Previous works mainly focus on designing better spatio-temporal attention networks [8, 16, 26, 29, 52, 62] and question-video relation networks [15, 19, 22, 41] to extract more accurate multi-modal representations. Recently, many works leverage transformer-based models to build an additional multi-modal encoder upon the uni-modal encoders [9, 24, 43] or input image patches and word tokens [58, 63] directly to fuse cross-modality features. The multi-modal encoder is typically driven by the pre-training tasks e.g. Masked Language Modeling [7] and Visual-Text Matching [46], to perform token-level cross-modal fusion.\\n\\nVideo-Language pre-training for multiple downstream tasks. There are a few recent works trying to construct a unified pre-trained model for multiple downstream tasks. ActBert [63], clipBert [23], VIOLET [9] and ALPRO [24] utilize a cross-modal encoder for both retrieval and VQA tasks. They take video-text retrieval as a binary classification task and construct the classifier upon a visual-text matching head, which brings about extreme computation overload. While All-in-One [49] and HD-VILA [56] proposes a transformer-based unified backbone architecture for joint-modal representation learning, they require a large amount of data for pre-training. HERO [25] and VLM [53] design a task-agnostic model that can be finetuned on multiple downstream tasks. However, they are not end-to-end trainable and need pre-extract video features, which limits the performance of the model.\"}"}
{"id": "CVPR-2023-342", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 1. Different attempts on achieving a universal VidL model. (a) Naive combination by stacking the cross-modal encoder upon the uni-modal encoders with weak association (COMB). (b) Our proposed Clover pre-trained model that better correlates the cross-modal encoder and the uni-modal encoders via Tri-Modal Alignment. (c) Comparisons of the transfer performance on MSRVTT [60] (zero-shot) and TGIF-FrameQA [16] datasets. Benefiting from the proposed tri-modal alignment, our proposed universal model outperforms both the individually designed models (IND) and COMB.\\n\\nNote that there are some methods focus on transferring the pre-trained CLIP model to downstream tasks and achieve significant performance [6, 14, 17, 21, 30, 37, 51, 57]. The CLIP model is pretrained with 400M image-text data pairs. In contrast, our Clover focuses on presenting a better video-text pretrained model, and only leverages 5M data pairs for pretraining.\\n\\n3. Method\\n3.1. Motivation and Overview\\nRecent years have witnessed a significant progress in Video-Language pre-training. Nevertheless, building a universal Video-Language model for solving various video understanding tasks (e.g., text-video retrieval, video question answering) is still a challenging problem. The key to the problem lies in how to unify the representation learning for cross-modal alignment and fusion efficiently.\\n\\nCross-modal alignment aims to learn projection functions \\\\( f(\\\\cdot) \\\\) and \\\\( g(\\\\cdot) \\\\) that project videos and texts into a common embedding space where the similarity between the video and text with consistent semantic information is maximized and otherwise minimized. Taking a video \\\\( V \\\\) as the anchor, the learning objective of cross-modal alignment is formulated as:\\n\\n\\\\[\\n\\\\arg \\\\max_{f,g} s(f(V), g(T^+)) - s(f(V), g(T^-)),\\n\\\\]\\n\\nwhere \\\\( T^+ \\\\), \\\\( T^- \\\\) represent texts that are semantically consistent and inconsistent with \\\\( V \\\\), respectively. The function \\\\( s(\\\\cdot, \\\\cdot) \\\\) measures the dot-product similarity between two embeddings. A model with strong cross-modal alignment capability is desired in the video-retrieval task, which requires matching the semantically aligned videos and texts.\\n\\nCross-modal fusion aims to integrate the correlation and interaction carried by the video and text modalities into a unified multi-modal embedding. Specifically, it can be formulated as learning a function \\\\( \\\\text{Fusion}(\\\\cdot, \\\\cdot) \\\\) that takes in inputs of different modals and outputs a unified representation \\\\( M = \\\\text{Fusion}(V, T) \\\\), which is then used to solve downstream tasks like VQA.\\n\\nExisting pre-training strategies rely on either simple supervised or contrastive pre-text tasks that hardly achieve feature alignment and fusion simultaneously. To better correlate the cross-modal alignment and fusion, we propose the Correlated Video-Language pre-training method (Clover) with three key pre-training innovations (Sec. 3.2): tri-modal alignment; pair-wise ranking loss; and semantic enhanced masked language modeling. The classical masked language modeling task is also incorporated into our method, which is able to boost the generalizability of the model as well as the interaction between visual and language.\\n\\nArchitecture. We briefly explain the architecture of Clover here, which is shown in Fig. 2 as well. It consists of three components: a video encoder, a text encoder and a multi-modal encoder. Following [9], we use VideoSwin Transformer [32] as our video encoder. Given a video \\\\( V \\\\), it outputs a sequence of video embeddings:\\n\\n\\\\[\\nV_e = \\\\{v_1, ..., v_K\\\\} \\\\subset \\\\mathbb{R}^D,\\n\\\\]\\n\\nwhere \\\\( K \\\\) is the number of flattened patches. The text encoder of Clover is a 12-layer bidirectional transformer encoder model [7]. Given an input text sentence \\\\( T \\\\), the encoder outputs an embedding sequence \\\\( T_e = \\\\{t_{\\\\text{cls}}, t_1, ..., t_{L-1}\\\\} \\\\subset \\\\mathbb{R}^D \\\\), where \\\\( t_{\\\\text{cls}} \\\\) indicates the embedding of the \\\\([\\\\text{CLS}]\\\\) token. We employ a 3-layer bidirectional Transformer encoder to learn the fused cross-modal representation of a video-text pair. The multi-modal encoder takes the concatenation of video and text embeddings as input, and outputs the fused multi-modal embeddings:\\n\\n\\\\[\\nM_e = \\\\{m_v_1, ..., m_v_K, m_{\\\\text{CLS}}, m_t_1, ..., m_t_{L-1}\\\\} \\\\subset \\\\mathbb{R}^D,\\n\\\\]\\n\\nwhere \\\\( m \\\\in \\\\mathbb{R}^D \\\\).\"}"}
{"id": "CVPR-2023-342", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overview of Clover. (a) Model architecture. (b) Pre-training Tasks\\n\\n3.2. Tri-Modal Alignment\\n\\nExisting VidL pre-training methods typically align the embeddings of data from different modalities through pair-wise contrastive learning. Differently, we propose a Tri-Modal Alignment (TMA) learning task that not only enforces the video and text modalities to be aligned but also encourages them to well align with the third modality, i.e., their fusion modality. In TMA, as an intermediate modality between video and text, the fused multi-modal representations act as anchors for cross-modality alignment, which reduces the difficulty of the direct alignment between video and text. Meanwhile, it keeps the fused representation closer to the uni-modal representation containing consistent semantic information and away from the others, so as to enhance the learning of semantic information in the fused representation.\\n\\nAs shown in Fig. 2 (a), given an input video-text pair \\\\( \\\\langle V, T \\\\rangle \\\\) and their embeddings \\\\( \\\\langle V_e, T_e \\\\rangle \\\\) from the uni-modal encoder, we mask some regions in \\\\( V \\\\) to make a masked video with embedding \\\\( V_m \\\\), and similarly mask some tokens in \\\\( T \\\\) to make a masked text embedding \\\\( T_m \\\\). Then, we pair the incomplete samples with the complete samples as \\\\( \\\\langle V_m, T_e \\\\rangle \\\\) and \\\\( \\\\langle V_e, T_m \\\\rangle \\\\). We adopt multi-modal encoder to get fused multi-modal embedding sequence \\\\( M_{Vm} \\\\) and \\\\( M_{Tm} \\\\) of \\\\( \\\\langle V_m, T_e \\\\rangle \\\\) and \\\\( \\\\langle V_e, T_m \\\\rangle \\\\), respectively. We denote the embedding of \\\\([CLS]\\\\) token in \\\\( M_{Vm} \\\\) and \\\\( M_{Tm} \\\\) as \\\\( M_{Vmf} \\\\) and \\\\( M_{Tmf} \\\\).\\n\\nFor the convenience of description, we use superscripts \\\\( i \\\\) and \\\\( j \\\\) to index the data sample, and introduce TMA from the perspective of video and language, respectively. Firstly, for a video representation \\\\( V_i e \\\\), besides its associated text embedding \\\\( T_i e \\\\) and \\\\( T_i m \\\\), we also consider its associated fusion embedding \\\\( M_{iVmf} \\\\) as its positive pair\u2014that should be pushed closer within the embedding space. To align these tri-modal representations, we propose a novel exclusive-NCE loss computed within a batch of \\\\( B \\\\) samples as follows:\\n\\n\\\\[\\nL_v = -\\\\frac{1}{B} \\\\sum_{i=1}^{B} \\\\log \\\\frac{e^{s(V_i e,T_i e)}}{e^{s(V_i e,T_i m)}} + \\\\log \\\\frac{e^{s(V_i e,M_{iVmf})}}{e^{s(V_i e,T_i m)}} + \\\\log \\\\frac{e^{s(V_i e,M_{iVmf})}}{e^{s(V_i e,T_i m)}} + \\\\log \\\\frac{e^{s(V_i e,M_{iVmf})}}{e^{s(V_i e,T_i m)}} + \\\\log \\\\frac{e^{s(V_i e,M_{iVmf})}}{e^{s(V_i e,T_i m)}} + \\\\log \\\\frac{e^{s(V_i e,M_{iVmf})}}{e^{s(V_i e,T_i m)}} + \\\\ldots\\n\\\\]\\n\\n(2)\\n\\nHere \\\\( \\\\tau \\\\) is a temperature scalar, and \\\\( s(\\\\cdot,\\\\cdot) \\\\) is the dot-product similarity function to measure the degree of alignment between different modalities. In each term of Eq. (2), we exclude the other positive pairs when performing contrastive learning on one positive pair. Thus the intra-suppression among positive pairs is effectively avoided, and the video-/text-/fusion-modal representations are better aligned. For example, in the first term of Eq. (2), \\\\( e^{s(V_i e,M_{iVmf})}/\\\\tau \\\\) and \\\\( e^{s(V_i e,T_i m)}/\\\\tau \\\\) are excluded from the denominator, which prevents the model from yielding the sub-optimal solution that gets \\\\( T_i e \\\\) closer to \\\\( V_i e \\\\) while pushes \\\\( T_i m \\\\) and \\\\( M_{iVmf} \\\\) away from \\\\( V_i e \\\\).\\n\\nMeanwhile, to align the video to the text and fusion modalities, we have the following objective:\\n\\n\\\\[\\nL_v' = -\\\\frac{1}{B} \\\\sum_{i=1}^{B} \\\\log \\\\frac{e^{s(T_i e,V_i e)}}{\\\\sum_{j \\\\neq i} e^{s(T_i e,V_j e)}} + \\\\log \\\\frac{e^{s(T_i m,V_i e)}}{\\\\sum_{j \\\\neq i} e^{s(T_i m,V_j e)}} + \\\\log \\\\frac{e^{s(M_{iVmf},V_i e)}}{\\\\sum_{j \\\\neq i} e^{s(M_{iVmf},V_j e)}} + \\\\log \\\\frac{e^{s(M_{iVmf},V_i e)}}{\\\\sum_{j \\\\neq i} e^{s(M_{iVmf},V_j e)}} + \\\\ldots\\n\\\\]\\n\\n(3)\\n\\nThe above two losses (2) and (3) are summed up to give the tri-modal alignment objective w.r.t. the visual modality:\\n\\n\\\\[\\nL_v = L_v + L_v'.\\n\\\\]\\n\\nSimilarly, from the perspective of text modality, we define the tri-modal alignment objective \\\\( L_t = L_t + L_t' \\\\) w.r.t. the text modality.\\n\\nThen, we obtain the final loss of tri-modal alignment \\\\( L_{TmA} = L_V + L_T \\\\). For the detail of \\\\( L_t \\\\), please refer to the supplementary material.\"}"}
