{"id": "CVPR-2024-427", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Dejan Azinovic, Ricardo Martin-Brualla, Dan B. Goldman, Matthias Nie\u00dfner, and Justus Thies. Neural RGB-D surface reconstruction. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 6280\u20136291. IEEE, 2022.\\n\\n[2] Tim D Barfoot, Chi Hay Tong, and Simo S\u00e4rkk\u00e4. Batch continuous-time trajectory estimation as exactly sparse Gaussian process regression. In Robotics: Science and Systems, pages 1\u201310. Citeseer, 2014.\\n\\n[3] Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural radiance field with no pose prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4160\u20134169, 2023.\\n\\n[4] Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas Schneider, Joern Rehder, Sammy Omari, Markus W Achtelik, and Roland Siegwart. The euroc micro aerial vehicle datasets. The International Journal of Robotics Research, 35(10):1157\u20131163, 2016.\\n\\n[5] Carlos Campos, Richard Elvira, Juan J G\u00f3mez Rodr\u00edguez, Jos\u00e9 MM Montiel, and Juan D Tard\u00f3s. Orb-slam3: An accurate open-source library for visual, visual\u2013inertial, and multimap slam. IEEE Transactions on Robotics, 37(6):1874\u20131890, 2021.\\n\\n[6] Yu Chen and Gim Hee Lee. Dbarf: Deep bundle-adjusting generalizable neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24\u201334, 2023.\\n\\n[7] Yue Chen, Xingyu Chen, Xuan Wang, Qi Zhang, Yu Guo, Ying Shan, and Fei Wang. Local-to-global registration for bundle-adjusting neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8264\u20138273, 2023.\\n\\n[8] Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and Simon Lucey. GARF: Gaussian activated radiance fields for high fidelity reconstruction and pose estimation. CoRR, abs/2204.05735, 2022.\\n\\n[9] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017.\\n\\n[10] Andrew J Davison, Ian D Reid, Nicholas D Molton, and Olivier Stasse. Monoslam: Real-time single camera slam. IEEE transactions on pattern analysis and machine intelligence, 29(6):1052\u20131067, 2007.\\n\\n[11] Christian Forster, Zichao Zhang, Michael Gassner, Manuel Werlberger, and Davide Scaramuzza. Svo: Semidirect visual odometry for monocular and multicamera systems. IEEE Transactions on Robotics, 33(2):249\u2013265, 2016.\\n\\n[12] Paul Furgale, Timothy D Barfoot, and Gabe Sibley. Continuous-time batch estimation using temporal basis functions. In 2012 IEEE International Conference on Robotics and Automation, pages 2088\u20132095. IEEE, 2012.\\n\\n[13] Ariel Gordon, Hanhan Li, Rico Jonschkowski, and Anelia Angelova. Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8977\u20138986, 2019.\\n\\n[14] Sachini Herath, David Caruso, Chen Liu, Yufan Chen, and Yasutaka Furukawa. Neural inertial localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6604\u20136613, 2022.\\n\\n[15] Javier Hidalgo-Carri\u00f3, Guillermo Gallego, and Davide Scaramuzza. Event-aided direct sparse odometry. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5781\u20135790, 2022.\\n\\n[16] Jiahui Huang, Shi-Sheng Huang, Haoxuan Song, and Shimin Hu. Di-fusion: Online implicit 3d reconstruction with deep priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8932\u20138941, 2021.\\n\\n[17] Weibo Huang and Hong Liu. Online initialization and automatic camera-imu extrinsic calibration for monocular visual-inertial slam. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 5182\u20135189. IEEE, 2018.\\n\\n[18] Inwoo Hwang, Junho Kim, and Young Min Kim. Ev-nerf: Event based neural radiance field. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 837\u2013847, 2023.\\n\\n[19] Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Ani mashree Anandkumar, Minsu Cho, and Jaesik Park. Self-calibrating neural radiance fields, 2021.\\n\\n[20] Mohammad Mahdi Johari, Camilla Carta, and Fran\u00e7ois Fleuret. Eslam: Efficient dense slam system based on hybrid representation of signed distance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17408\u201317419, 2023.\\n\\n[21] Lukas Kaul, Robert Zlot, and Michael Bosse. Continuous-time three-dimensional mapping for micro aerial vehicles with a passively actuated rotating laser scanner. Journal of Field Robotics, 33(1):103\u2013132, 2016.\\n\\n[22] Hanme Kim, Stefan Leutenegger, and Andrew J Davison. Real-time 3d reconstruction and 6-dof tracking with an event camera. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI, pages 349\u2013364. Springer, 2016.\\n\\n[23] Georg Klein and David Murray. Parallel tracking and mapping on a camera phone. In 2009 8th IEEE International Symposium on Mixed and Augmented Reality, pages 83\u201386. IEEE, 2009.\\n\\n[24] Simon Klenk, Lukas Koestler, Davide Scaramuzza, and Daniel Cremers. E-nerf: Neural radiance fields from a moving event camera. IEEE Robotics and Automation Letters, 2023.\\n\\n[25] Stefan Leutenegger, Paul Furgale, Vincent Rabaud, Margarita Chli, Kurt Konolige, and Roland Siegwart. Keyframe-based visual-inertial slam using nonlinear optimization. Proceedings of Robotics Science and Systems (RSS) 2013, 2013.\"}"}
{"id": "CVPR-2024-427", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2024-427", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems, 34:16558\u201316569, 2021.\\n\\nPrune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance fields from sparse and noisy poses. arXiv preprint arXiv:2211.11738, 2022.\\n\\nVladyslav Usenko, Nikolaus Demmel, David Schubert, J\u00f6rg St\u00fcckler, and Daniel Cremers. Visual-inertial mapping with non-linear factor recovery. IEEE Robotics and Automation Letters, 5(2):422\u2013429, 2019.\\n\\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013612, 2004.\\n\\nZirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. NeRF\u2212\u2212: Neural radiance fields without known camera parameters. arXiv preprint arXiv:2102.07064, 2021.\\n\\nYitong Xia, Hao Tang, Radu Timofte, and Luc Van Gool. Sinerf: Sinusoidal neural radiance fields for joint pose estimation and scene reconstruction. CoRR, abs/2210.04553, 2022.\\n\\nXingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), pages 499\u2013507. IEEE, 2022.\\n\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.\\n\\nYi Zhou, Guillermo Gallego, and Shaojie Shen. Event-based stereo visual odometry. IEEE Transactions on Robotics, 37(5):1433\u20131450, 2021.\\n\\nZihan Zhu, Songyou Peng, Viktor Larsson, Zhaopeng Cui, Martin R Oswald, and Marc Pollefeys. Nice-slam: Neural implicit scalable encoding for slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12786\u201312796, 2022.\\n\\nZihan Zhu, Songyou Peng, Viktor Larsson, Zhaopeng Cui, Martin R Oswald, Andreas Geiger, and Marc Pollefeys. Nicer-slam: Neural implicit scene encoding for rgb slam. arXiv preprint arXiv:2302.03594, 2023.\"}"}
{"id": "CVPR-2024-427", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Patch Reconstruction\\n\\nColor-coded patch correspond to Fig 3. Note that patch 2D rigid motion exhibits continuity over time (left to right).\\n\\nTable 1. Image alignment experiment. Results of average 20 sampled 2D rigid motion, CE refer to Corner Error and SR refer to successful rate.\\n\\n| Method   | CE (pixel) | PSNR (dB) | SR (%) |\\n|----------|------------|-----------|--------|\\n| BARF[2]  | 13.55      | 27.61     | 30%    |\\n| B-Spline | 35.14      | 21.95     | 0%     |\\n| Ours     | 0.01       | 37.00     | 100%   |\\n\\nFigure 3. Qualitative results of 2D planar Alignment. We report the results of planar image alignment. Given input as ground truth (d) shown in Fig. 2, the goal is to find the 2D rigid transformation for each patch and optimize the entire neural image. Our method optimizes for accurate alignment and high-fidelity image reconstruction, while baselines fail due to local minima.\\n\\nThus we can supervise PoseNet by constraining the Jacobian with the measured angular velocity. We use $\\\\ell_1$ loss as $L_{\\\\text{tight}} = |\\\\dot{q} - \\\\frac{1}{2}\\\\Omega(\\\\hat{\\\\omega})|$ and jointly optimize it with the tracking target function in Eq 4.\\n\\nIt is noteworthy that, in the aforementioned equation, our PoseNet outputs pose with respect to the body frame rather than the camera frame. Further details regarding the coordinate change can be found in the supplementary materials, along with an explanation of how acceleration is utilized.\\n\\n4. Experiments\\n\\n4.1. NeRF from Inaccurate Poses\\n\\nWe validate the effectiveness of our proposed method through 2D planar image alignment experiments and 3D scene experiments similar to BARF [27]. During this process, BARF refines a discrete set of inaccurate camera poses while our method leverages the continuous pose information and is therefore less prone to local minima.\\n\\n4.1.1 Planar Image Alignment (2D)\\nWe choose the same images as [7, 27] as shown in Fig 3. To obtain a continuous rigid transformation, we initially randomly sample 10 data points from $T \\\\in \\\\text{SE}(2)$, we then interpolate a cubic spline along each dimension. Finally, we interpolate on 7 uniformly spaced points at previous time instants. As a result, the rigid transformation demonstrates temporal correlation, as illustrated in Fig 2. The initialized pose is identity with respect to center crop.\\n\\nExperimental settings. We compare our method against BARF [27] and BaRF with B-spline. For the latter, we introduce continuity by resetting the learned $T \\\\in \\\\text{SE}(2)$ for every 100 steps using B-spline interpolation. The learning rate is $1e^{-3}$ for translation and $2e^{-4}$ for rotation. For the B-Spline method we report with 5 knots placed time-wise uniformly with degree $= 3$.\\n\\nResults. The results are visualized in Fig 2, 3. The alignment performance of BARF suffers from local minima, resulting in sub-optimal performance. Experiments are deemed successful if the corner error is below 1 pixel. Although some patches correctly learn the transformation,\"}"}
{"id": "CVPR-2024-427", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"We introduce continuous errors on the camera trajectories and perform pose refinement in the NeRF setting. (a) Initial pose error; (b) results obtained using BARF [27] that uses a discrete set of poses; (c) results obtained using our continuous pose representation.\\n\\n### Table 2. Real data with unknown pose\\n\\n| Method       | RE (\u00b0) | TE (\u00b0) | PSNR | SSIM | LPIPS |\\n|--------------|--------|--------|------|------|-------|\\n| Fern         |        |        |      |      |       |\\n| Fern/2       | 0.199  | 0.181  | 21.01| 0.62 | 0.33  |\\n| Fern/4       | 0.344  | 0.331  | 19.72| 0.53 | 0.33  |\\n| Fortress     |        |        |      |      |       |\\n| Fortress/2   | 0.289  | 0.264  | 19.65| 0.54 | 0.33  |\\n| Fortress/4   | 0.607  | 0.630  | 20.71| 0.38 | 0.37  |\\n| Orchids      |        |        |      |      |       |\\n| Orchids/2    | 0.809  | 0.730  | 12.60| 0.15 | 0.37  |\\n| Orchids/4    | 92.176 | 0.865  | 11.07| 0.18 | 0.97  |\\n| Room         |        |        |      |      |       |\\n| Room/2       | 0.329  | 0.274  | 21.20| 0.77 | 0.13  |\\n| Room/4       | 118.58 | 0.403  | 11.00| 0.42 | 0.89  |\\n| Average      | 18.44  | 0.446  | 10.77| 0.32 | 17.50 |\\n|              |        |        |      |      |       |\\n\\n### Table 3. Ablation study\\n\\n| Method       | RE (\u00b0) | TE (\u00b0) | PSNR | SSIM | LPIPS |\\n|--------------|--------|--------|------|------|-------|\\n| LE, C        |        |        |      |      |       |\\n\\nWe investigate the effectiveness of our PoseNet with diverse architecture. LE refers to linear encoder and C, D refer to coupled and decoupled representations. RE, TE refer to rotational and translation error. The best and second-best results are in bold and underlined.\\n\\n4.1.2 Synthetic and Real NeRF (3D)\\n\\nWe explore the more challenging problem of learning 3D Neural Radiance Field with inaccurate poses. For the synthetic data, we render Lego [32] with a circular movement as shown in Figure 4. The simulated camera orbits the Lego model in the xy-plane, moving up and down at a constant speed in the z-direction.\\n\\n### Experimental settings.\\n\\nSimilar to the 2D experiment, we introduce temporal correlation between neighboring SE(3) disturbances with interpolation. We use spherical linear interpolation for rotation. The introduced error corresponds to 55\u00b0 in rotation and 110% in translation. For real data, we use the Fern, Fortress, Orchids, and Room datasets in LLFF [31], since these sequences allow us to perform experiments with varying numbers of images, thus simulating fast-moving cameras. Unlike in the synthetic case, we do not use any pose initialization in the real data experiments.\\n\\nFollowing [27] we report the MSE distance and rotational angle after alignment using Procrustes analysis for registration evaluation and PSNR, SSIM [57] and LPIPS [61] to evaluate view synthesis quality.\\n\\n### Results.\\n\\nWe report our experimental results in Table 3 and Table 2, for synthetic and real data, respectively. In Table 2, the proposed continuous pose representation clearly offers better results than the discrete BARF. Ablation experiments further illustrate that the rotation and translation decoupled representation, i.e., two MLPs instead of one, performs better, offering the best results with the embedding frequency bands $F = 5$. In real data with completely unknown camera poses, PoseNet performs significantly better than BARF. These results are reported in Table 2, where dataset/n refers to $1/n$th fraction of uniformly downsampled cases. It can be seen that PoseNet successfully handles all three failure cases of BARF. This is particularly evident when only sparse images are available. At the same time, even when BARF succeeds, PoseNet performs significantly better than BARF. More results and experiments using B-Spline can be checked in supplementary material.\"}"}
{"id": "CVPR-2024-427", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Interpolation error experiments. We improve the EventNeRF [46] using PoseNet. A small number of sparsely known poses are used to estimate the poses in between. PoseNet improves EventNeRF significantly in all six experimental setups.\\n\\n| Dataset | Num | Without Ours | Without Ours | Without Ours | Without Ours |\\n|---------|-----|--------------|--------------|--------------|--------------|\\n| Chair   | 20  | 3.66         | 1.74         | 26.36        | 26.58        |\\n|         |     |              |              |              |              |\\n|         | 10  | 15.63        | 3.38         | 22.48        | 25.02        |\\n|         |     |              |              |              |              |\\n|         | 6   | 59.31        | 22.68        | 21.45        | 22.06        |\\n| Hotdog  | 20  | 3.66         | 2.42         | 23.59        | 25.64        |\\n|         |     |              |              |              |              |\\n|         | 10  | 15.63        | 4.87         | 21.85        | 23.15        |\\n|         |     |              |              |              |              |\\n|         | 6   | 59.31        | 6.70         | 21.06        | 22.03        |\\n\\nFigure 5. With and without calibration experiments. We investigate the effectiveness of our method under different deviations from the actual rotational axis. Our method can successfully reposition the object back to the center without additional calibration.\\n\\n4.2. Continuous Pose for Asynchronous Events\\n\\nBy virtue of continuous pose representation, handling asynchronous event streams acquired by event cameras becomes natural. Hence, we use our PoseNet to learn the radiance field-based 3D scene representation from only colour event streams. This experimental setup is similar to recent work EventNeRF [46]. Note that EventNeRF accumulates asynchronous events to high-frequency synchronous event frames. The poses of each of those event frames are then assumed to be known. We argue that these assumptions limit the potential of the event cameras which come from their asynchronous nature. Therefore, we query for the pose of every event precisely at their trigger times. We conduct two experiments to address two practical issues of using events in EventNeRF setup using both synthetic and real datasets.\\n\\n4.2.1 Unknown continuous pose for single event\\n\\nEvents are triggered asynchronously, and in practice where there is no precisely measured control available such as with a turntable [46] or a motorized linear slider [42], event pose can only be interpolated from measured discrete poses (from Vicon or Colmap [15]). However, this introduces interpolation errors.\\n\\n**Experimental settings.** For synthetic data, we use chair and hotdog sequences from [46]. The events are simulated using the model in [45]. While EventNeRF performs interpolation, our method jointly learns intermediate poses as a continuous function of time.\\n\\n**Results.** In Table 4, we reveal that integration of our PoseNet significantly enhances the overall performance with a notable reduction in translation errors and better scene reconstruction. More visual results can be found in supplementary material.\\n\\n4.2.2 Unknown calibration in practice\\n\\nEventNeRF [46] uses a turntable to achieve stable and consistent object rotation speed. This setup also requires the actual rotational axis. Therefore, an additional checkerboard-based calibration technique, to estimate the axis offset, is also proposed in [46].\\n\\n**Experimental setting.** For real cases, we use sewing machine datasets, which hold difficulties in reconstructing thin structures, view-dependent effects, and colored texts.\\n\\n**Results.** We show that when PoseNet is used, additional calibration may not be required. The qualitative results of these experiments are shown in Figure 5. We demonstrate that when some offset is introduced, the 3D object deviates from the center for EventNeRF, while our method can reduce artifacts, learn the offset angle, and reposition the object back to the image center.\\n\\n4.3. Visual SLAM with Depth and IMUs\\n\\nWhile the previously discussed tasks are offline, vSLAM is an online method with different considerations. In this application we approach the problem as incremental SLAM. For each incoming frame, our objective is to determine its transformation with respect to the last frame \\\\( T_{\\\\text{relative}} \\\\). Similar to NICE-SLAM [63] we maintain a list of all optimal relative poses. It is trivial to solve the forgetting issue by retraining our PoseNet with such a list.\\n\\n**Experimental settings.** We report the tracking results of our method compared with the standard NICE-SLAM. We report results of intrinsic motion on Replica [51], Scannet [9], and TUM-RGBD [52]. Note that during tracking we assume intrinsic motion reference slowly changes over time and only optimize \\\\( f_0 \\\\) for every keyframe, with a frequency set to 10 for our experiments. In EUROC dataset [4] we follow the same pre-processing step as [13] and use nearest interpolation to get dense depth map. In order to evaluate the trajectory quality we report the ATE-RMSE [cm] of all sequences. More details regarding convergence rate and run-time can be found in the supplementary material.\"}"}
{"id": "CVPR-2024-427", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 5.\\nTracking performance on Replica [51]. By integrating our method into the tracking branch of NICE-SLAM, we observe significant improvements. We investigate the impact of varying reference coordinates on tracking. It is evident that our proposed low DOF motion further improves the tracking performance.\\n\\n| Method          | 0000 | 0059 | 0106 | 0181 | 0207 | Avg  |\\n|-----------------|------|------|------|------|------|------|\\n| DI-Fusion       | 62.99| 128.00| 18.50| 87.88| 100.19| 78.89 |\\n| Vox-Fusion*     | 68.84| 24.18| 8.41 | 23.30| 9.41  | 26.90 |\\n| NICE-SLAM       | 12.00| 14.00| 7.90 | 13.40| 6.20  | 10.70 |\\n| Ours            | 10.98| 11.98| 7.10 | 13.50| 5.76  | 9.86  |\\n| Ours (intrinsic)| 11.21| 8.78 | 7.57 | 12.21| 4.87  | 8.93  |\\n\\n### Table 6.\\nTracking performance on ScanNet [9]. Our approach yields consistently better results than the baseline. Note that the gain of utilizing intrinsic motion is relatively small, possibly attributed to the challenges posed by the noisy ground truth poses.\\n\\n| Method          | v101 | v102 | v103 | v201 | v202 | v203 | Avg  |\\n|-----------------|------|------|------|------|------|------|------|\\n| VINS-MONO       | 7.9  | 11.0 | 18.0 | 8.0  | 16.0 | 27.0 | 14.6 |\\n| ORB-SLAM        | 1.5  | 2.0  | N/A  | 2.1  | 1.8  | N/A  | N/A  |\\n| DROID-SLAM      | 3.7  | 1.2  | 2.0  | 1.7  | 1.3  | 1.4  | 2.2  |\\n| NICE-SLAM       | 2.58 | N/A  | 5.66 | 6.56 | N/A  | N/A  | N/A  |\\n| Ours (loose)    | 2.20 | 6.74 | 5.04 | 4.52 | 3.87 | 19.06| 6.77 |\\n| Ours (tight)    | 1.98 | 6.09 | 5.55 | 4.99 | 3.03 | 15.34| 6.16 |\\n\\n### Table 7.\\nTracking performance on TUM-RGBD [52]. Our method consistently outperforms NICE-SLAM and other dense neural RGBD methods. The effectiveness of intrinsic motion is also demonstrated for reducing the tracking error significantly.\\n\\n| Method          | v101 | v102 | v103 | v201 | v202 | v203 | Avg  |\\n|-----------------|------|------|------|------|------|------|------|\\n| DI-Fusion       | 4.4  | 2.0  | 5.8  | 4.1  |      |      |      |\\n| Vox-Fusion*     | 3.52 | 1.49 | 26.01| 10.34|      |      |      |\\n| NICE-SLAM       | 4.26 | 31.73| 3.87 | 13.28|      |      |      |\\n| Ours            | 2.97 | 7.38 | 3.76 | 4.70 |      |      |      |\\n| Ours (intrinsic)| 2.72 | 1.98 | 2.74 | 2.48 |      |      |      |\\n\\n### Table 8.\\nTracking performance on EUROC [4]. Our IMU-fusion improves tracking with lower error and robustness, outperforming NICE-SLAM. We report results with sparse tracking method for reference. Despite the gap, our method narrows differences with state-of-the-art sparse tracking.\\n\\n5. Conclusion\\nWe proposed a simple yet effective technique for optimizing camera pose as a continuous function of time. The benefits of this approach were illustrated through several experiments of diverse applications, namely NeRF from the inaccurate pose, NeRF using Event Cameras, and visual SLAM with Depth and IMUs. We also studied different designs of the time-to-pose mapping continuous function, leading us to prefer a decoupled architecture. Furthermore, we justified the ease of using the proposed PoseNet in a plug-and-play manner. We first propose IMU-Fusion in NeRF-SLAM and analyze the advantage of adopting intrinsic motion frame for camera tracking tasks. Clear advantages in terms of performance were also observed in all settings, thanks to the continuous motion prior.\\n\\nAcknowledgements:\\nResearch is partially funded by VIVO Collaboration Project and the Ministry of Education and Science of Bulgaria (support for INSAIT, part of the Bulgarian National Roadmap for Research Infrastructure).\"}"}
{"id": "CVPR-2024-427", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Continuous Pose for Monocular Cameras in Neural Implicit Representation\\n\\nQi Ma 1,2\\nDanda Pani Paudel 2\\nAjad Chhatkuli 1\\nLuc Van Gool 1,2\\n\\n1 Computer Vision Lab, ETH Zurich\\n2 INSAIT, Sofia University\\n\\nAbstract\\nIn this paper, we showcase the effectiveness of optimizing monocular camera poses as a continuous function of time. The camera poses are represented using an implicit neural function which maps the given time to the corresponding camera pose. The mapped camera poses are then used for the downstream tasks where joint camera pose optimization is also required. While doing so, the network parameters\u2014 that implicitly represent camera poses\u2014are optimized. We exploit the proposed method in four diverse experimental settings, namely, (1) NeRF from noisy poses; (2) NeRF from asynchronous Events; (3) Visual Simultaneous Localization and Mapping (vSLAM); and (4) vSLAM with IMUs. In all four settings, the proposed method performs significantly better than the compared baselines and the state-of-the-art methods. Additionally, using the assumption of continuous motion, changes in pose may actually live in a manifold that has lower than 6 degrees of freedom (DOF) is realized. We call this low DOF motion representation as the intrinsic motion and use the approach in vSLAM settings, showing impressive camera tracking performance. We release our code at: https://github.com/qimaqi/Continuous-Pose-in-NeRF.\\n\\n1. Introduction\\nThe concept of motion, the change of position and orientation of an object in its surroundings, is fundamentally continuous in nature. This continuity is evident in the ways we achieve, perceive and measure motion, with velocity and acceleration being the most common measures for both linear and angular motion. This idea of continuity is also true for the 3D poses of navigating cameras. Often the camera motion needs to be estimated from its measurements\u2014also known as the camera localization problem. In most common settings, the inputs are RGB-only frames, depth frames, asynchronous event streams, or a combination thereof. In some cases, these measurements are augmented by Inertial Measurement Unit (IMU) outputs, which measure a change in pose directly. In all those settings, the camera motion is estimated via some optimization technique that searches \\\\( SE(3) \\\\) pose parameters. While doing so, existing techniques choose to optimize a discrete set of \\\\( SE(3) \\\\) parameters, ignoring the inter-frame continuity of camera poses. This choice can be primarily attributed to the otherwise difficulty in optimization.\\n\\nWhile handling high-frequency IMUs or asynchronous events in common practice, pose optimization at every measurement time is avoided, for computational reasons. Instead, the measurements between two arbitrarily chosen keyframes are accumulated before utilizing them. Then the poses are optimized only for those keyframes. We argue that this raises three major concerns: (i) inaccurate accumulation of intermediate measurements; (ii) loss of fine-grained motion details; (iii) lack of the continuous motion prior.\\n\\nIn order to address these concerns, we represent and optimize the pose of a moving camera as a continuous function of time. Unlike classical state estimation method \\\\([2, 12, 35]\\\\) which models continuous pose with Gaussian Process or B-spline, our neural pose function can be easily optimized jointly with other task-specific implicit neural representation (INR) \\\\([30, 32, 39]\\\\). More precisely, for translation \\\\( v \\\\in \\\\mathbb{R}^3 \\\\) and rotation \\\\( R(q) \\\\in SO(3) \\\\) parameterized by quaternions \\\\( q \\\\in \\\\mathbb{R}^4 \\\\) with \\\\( ||q|| = 1 \\\\), the continuous pose of the monocular camera is given by,\\n\\n\\\\[\\n[q; v] = f_\\\\theta(t),\\n\\\\]\\n\\nwhere \\\\( f_\\\\theta(\\\\cdot) \\\\) is the continuous neural function parameterized by \\\\( \\\\theta \\\\) that maps the time \\\\( t \\\\in \\\\mathbb{R} \\\\) to the pose in \\\\( SE(3) \\\\). While being simple, this representation has numerous benefits including ease of optimization and its cosmopolitan applicability. Some example applications are illustrated in Figure 1. In the following, we further discuss how our simple approach addresses the previously raised concerns.\\n\\nNo error due to measurement accumulation:\\nHigh frequency or asynchronous measurements can be utilized directly without accumulation, integral, or rounding. We infer the camera pose precisely at the measurement time. For example, in the case of an event camera, each asynchronous event's pose is inferred precisely at the event time. Similarly, in the case of IMUs, no motion integration before supervision is required. These abilities protect our approach against error injection due to any form of accumulation.\"}"}
{"id": "CVPR-2024-427", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. We showcase the benefits of optimizing the poses as a continuous function of time in diverse settings. We conduct exhaustive experiments on (a) rectifying inaccurate poses in RGB-only settings; (b) utilizing the asynchronous stream of events, (c) performing vSLAM in RGB-D camera settings; (d) integrating high-frequency IMUs in vSLAM. All experiments use neural functions for both camera poses and scene representations. Additionally, we exploit low dof motion representation in intrinsic motion frame $T_I$.\\n\\nFine-grained motion details: By virtue of the continuous representation, temporally fine-grained details of the pose can be captured. This is particularly interesting with high-frequency IMUs or asynchronous event cameras. Our approach allows for the recovery of the pose at the very moment of measurement, which otherwise often is an ill-posed problem and could only be interpolated with an assumed smoothness and order.\\n\\nContinuous motion prior: The inductive bias of continuous monocular camera motion is meaningfully injected by the proposed method. This resulted in very encouraging results in our experiments. In particular, while denoising the inaccurate camera poses and during the vSLAM experiments, the benefits were evident under the standard settings of BARF [27] and NICE-SLAM [63], respectively. It is important to note that our representation offers first- and second-order derivatives via auto-differentiation of the neural network. Consequently, quantities such as velocity and acceleration do not require additional care. Thus the fusion of IMU measurements is natural and straightforward.\\n\\nIn addition to the above, we further show the utility of the neural pose in order to optimize the continuous pose by decomposing each change in pose into a slowly changing reference and a low DOF motion. We define this as the intrinsic motion. In our experiments, we observed that our continuous pose representation improves the tracking performance significantly in the vSLAM tasks. This can be primarily attributed to the reasons mentioned above, which serve to facilitate the optimization process. Inspired by the fact that actual motion always possesses a lower degree of freedom, we define the intrinsic motion frame as a coordinate system that can express the camera motion with the lowest dimensional manifold. For example: Rotational motion around a fixed axis can be expressed in the coordinate system aligned with the rotational axis with only one degree of freedom. A natural observation is that the relative motion with respect to intrinsic motion is usually sparse, moreover, the continuous motion tends to share the same intrinsic motion frame which can be well modeled as a continuous function of time. By exploiting it, we decompose the camera relative motion with a low-dimensional intrinsic motion $[R_I, v_I]$ and the rigid transformation from camera frame to the intrinsic motion frame $[R_o, v_o]$ as follows:\\n\\n$$[R, v] = [R_o, v_o] [R_I, v_I],$$\\n\\n(2)\\n\\nOur major contributions can be summarized as follows:\\n\\n\u2022 We propose a simple yet effective way to represent the monocular camera motion via a neural function of time that can be optimized efficiently together with implicit neural representations.\\n\\n\u2022 We demonstrate the utility of the proposed representation in four diverse applications with different camera setups, including IMUs and moving event cameras.\"}"}
{"id": "CVPR-2024-427", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Through exhaustive experiments, we demonstrate clear benefits of the proposed representation over the existing alternatives and classical method. These benefits include ease of optimization, widespread use for different camera and sensor types, and notable performance gain with no additional effort.\\n\\nWe further improve the full 6-DOF pose of monocular camera by exploiting the sparsity of the intrinsic motion, which fits neatly into the proposed framework of continuous neural pose. The final pose thus obtained shows remarkable improvement over the conventional baselines.\\n\\n2. Related work\\n\\n2.1. Camera Poses in NERF\\n\\nNERF [32] consists of joint optimization of the surface density and the rendered color given the images with known camera rays. Consequently NERF models are highly sensitive to camera pose errors [8, 27, 29, 55, 58, 59]. Recently several works have tackled the pose error by jointly optimizing poses with the radiance field. [6, 7, 19, 27] optimizes camera poses in bundle adjustment fashion in order to solve the same issue. While these methods use the smooth pose prior, the poses are still optimized as discrete variables. On the other spectrum [55] optimizes noisy poses for sparse camera views with the radiance fields opting for a different class of applications. [3] enforces the inter-frame consistency by incorporating monocular depth prior.\\n\\n2.2. Camera Poses with IMUs\\n\\nThe inertial measurement unit (IMU) serves as a scene-independent sensor that is the ideal complement to cameras in order to achieve robustness in low texture, high speed, and HDR scenarios. Fusing visual information and IMU tightly [48] to estimate pose as discrete states is proposed first by MSCKF [34] (an extended Kalman Filter (EKF)), [25] further improves it with keyframes and bundle adjustment. [11, 17, 41, 56] improve in robustness compared to feature matching by using the direct photometric error. [5] propose fast and accurate IMU initialization based on MAP estimation. Recent research has also focused on integrating IMU and visual priors with neural network, e.g., the camera pose is implicitly used for image deblurring [37] or video stabilization [49]. [14] proposes neural inertial localization with IMU alone for indoor scenes.\\n\\n2.3. Camera Poses in Dense SLAM\\n\\nVisual SLAM [10, 23, 43] is a key 3D vision application where an agent camera is localized simultaneously while building the map using visual information. We again focus on methods in the context of radiance fields [1, 44, 53, 63, 64]. IMAP [53] is a recent seminal work which works on RGBD images to optimize an implicit scene representation with a single MLP network. It optimizes the camera pose while representing them as discrete sets of parameters for the keyframes. NICE-SLAM [63] improves on it by using 3D voxel features along with corresponding 2D image features thus providing a better scene representation. Indeed most approaches [26, 44, 47, 64] focus on improving the scene representation for better localization and mapping or with RGB-only input.\\n\\n2.4. Camera Poses in Event Cameras\\n\\nUnlike standard frame-based camera imaging, event cameras provide image signals as asynchronous events in microsecond intervals [22]. Thus, it forms the perfect use case for a continuous time representation of camera poses. Similar to NERF-less SLAM [10], this is traditionally done using variations of Kalman Filter with motion models [22, 33]. A recent work [62] represents camera tracking as a function of time but uses a Levenberg-Marquardt optimization directly on the sets of poses without intermediate representation. Recently, there have been efforts to use event-based radiance fields in the neural network [18, 24, 28, 46]. However, camera pose optimization as a function of time is still not fully explored in the radiance field literature with events.\\n\\n2.5. Continuous Pose representation\\n\\nWhile discrete-time representations are commonly employed in Simultaneous Localization and Mapping (SLAM) tasks, they face challenges when integrating data from high-frequency sensors like Inertial Measurement Units (IMUs) and asynchronous events. [12] address this issue by proposing representing the continuous-time state using temporal basis functions such as B-spline basis. [2] model the continuous state using Gaussian processes, defining continuous-time priors through covariance functions. [40] leverage cumulative cubic B-splines to mitigate rolling-shutter artifacts. Notably, spline-based continuous-time trajectory representations have found application in laser-based SLAM methods [21, 38].\\n\\n3. Time-to-Pose Mapping Network\\n\\n3.1. Architecture of the Proposed PoseNet\\n\\nIn order to learn time-to-pose mapping, we use 8-layer MLP parameterized by $f(\\\\theta)$ with ReLU activation functions and 256-dimensional hidden units, which we refer to as pose-network (PoseNet). PoseNet first embeds the time variable into high-dimensional space using sinusoidal harmonic functions [32]. The outputs of this network are $[v, q]$: translation vector $v \\\\in \\\\mathbb{R}^3$ and the rotation represented by a quaternion $q \\\\in \\\\mathbb{R}^4$. Finally, we use the tanh activation in the last layer to map output to the range $[-1, 1]$, and normalize it as a unit quaternion. We study different embeddings\"}"}
{"id": "CVPR-2024-427", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The simplicity of PoseNet allows us to use it in diverse applications in a plug-and-play manner. In all applications that we report in the following sections, we optimize the PoseNet parameters $\\\\theta_p$ as a surrogate of the direct pose optimization. We denote the network parameters for the INR of the scene as $\\\\theta_s$. In NeRF from inaccurate poses, the objective is to minimize the radiance field loss [27, 32] given $N$ images and corresponding timestamp $t_i$ for image $i$:\\n\\n$$\\\\min_{\\\\theta_s, \\\\theta_p} \\\\sum_{i=1}^{N} \\\\| I_i - g(\\\\theta_s, f(\\\\theta_p, t_i)) \\\\|.$$  (3)\\n\\n$g(\\\\theta_s, T_i)$ represents the mapping from the camera pose $T_i$ to the RGB value, including ray composition and radiance field model. In case of NeRF with asynchronous events [46], $N$ refers to the number of the sampled events.\\n\\nNote that in both cases, we output the predicted transformation and compose it with the initial pose: $T_i = T_{\\\\text{init}}i \\\\circ T_{\\\\text{refine}}i$. The refined transformation is obtained as $T_{\\\\text{refine}}i = P(f(\\\\theta_p, t_i))$, $P(\\\\cdot)$ being the vector to rigid transformation conversion operator.\\n\\nIn the task of Dense-SLAM tracking, for each tracking iteration we optimize PoseNet with the following objective:\\n\\n$$\\\\min_{\\\\theta_p} \\\\sum_{i=1}^{M} (L_g(D_i, P(f(\\\\theta_p, t_i))) + \\\\lambda_p L_p(I_i, P(f(\\\\theta_p, t_i)))).$$  (4)\\n\\nWe use the same geometric loss $L_g$ and photometric loss $L_p$ as in NICE-SLAM [63]. $D_i$, $I_i$ represent depth and RGB measurements for $M$ sampled pixels respectively, obtained via volume rendering.\\n\\n3.3. Intrinsic Motion Frame\\n\\nWithin the neural dense SLAM application, we additionally introduce intrinsic motion frame in order to improve tracking within a low-dimensional manifold. This is accomplished through motion decomposition and enforcing minimal DOF. More specifically, we use two PoseNet $f_o(\\\\theta_p o)$, $f_I(\\\\theta_p I)$ in order to model the intrinsic motion $T_o = [R_o, v_o]$, $T_I = [R_I, v_I]$, such that $T_i = T_o \\\\circ T_I$. Here, $T_o$ is the transformation to the intrinsic frame or in short, intrinsic transform. $T_I$ then denotes the intrinsic motion. Therefore we can rewrite Eq (4) as:\\n\\n$$\\\\min_{\\\\theta_p o, \\\\theta_p I} \\\\sum_{i=1}^{M} (L_g(D_j, P(f_o(\\\\theta_p o, t_i)) \\\\circ f_I(\\\\theta_p I, t_i)) + L_p(I_j, P(f_o(\\\\theta_p o, t_i)) \\\\circ f_I(\\\\theta_p I, t_i))) + L_{\\\\text{dof}}(f_I(\\\\theta_p I, t_i)) + L_o(f_o(\\\\theta_p o, t_i)).$$  (5)\\n\\nNote that the operator $P$ should be included for absolute correctness in the function compositions in Eq (5). The DOF loss $L_{\\\\text{dof}}$ is computed as follows:\\n\\n- **Step1**: Obtain $[R_I, v_I]$ from intrinsic motion PoseNet\\n- **Step2**: Convert rotation matrix $R_I$ to Euler angles $\\\\alpha_I \\\\in \\\\mathbb{R}^3$, normalize with angle of view $\\\\gamma$, $\\\\hat{\\\\alpha}_I = \\\\frac{2\\\\alpha_I}{\\\\gamma}$\\n- **Step3**: Normalize translation vector with $\\\\hat{v}_I = v_I / \\\\|v_I\\\\|$\\n- **Step4**: DOF Loss $L_{\\\\text{dof}} = \\\\|\\\\hat{\\\\alpha}_I, \\\\hat{v}_I\\\\|_0$\\n\\nWe relax the $\\\\ell_0$ norm to $\\\\ell_1$ norm for optimization. In steps 2 and 3, normalization also serves to balance translation and rotation components during optimization. We employ view angle normalization with the assumption that the angle between two relative views in vSLAM tasks is always smaller than half of the viewing angle. To handle the cases where unconstrained intrinsic motion tends move to infinity in cases of small rotation, we introduce an additional $L_1$ regularization term for the translation $L_o = |v_o|$.\\n\\n3.4. IMU fusion\\n\\nUp to our knowledge we are the first to integrate IMU data in NeRF + SLAM setting. The IMU fusion is straightforward in PoseNet taking advantage of the auto-differentiation of the neural network. We propose two different IMU fusion methods with details as follows:\\n\\n- **Loose coupling.** Given 3-axis angular velocity measurement from gyroscope $\\\\hat{\\\\omega} = (\\\\hat{\\\\omega}_x, \\\\hat{\\\\omega}_y, \\\\hat{\\\\omega}_z)$ we get the time step $\\\\triangle t = \\\\frac{1}{f}$. We can express the rotation angle to be $\\\\triangle t \\\\| \\\\hat{\\\\omega} \\\\|$ around axis $\\\\hat{\\\\omega} / \\\\| \\\\hat{\\\\omega} \\\\|$. This instantaneous rotation from the local sensor between previous and current timestamp can be represented as follows:\\n\\n$$q_{\\\\triangle t} = q_{\\\\triangle t} \\\\| \\\\hat{\\\\omega} \\\\|,$$  (6)\\n\\nBy continuously integrating the measurements we can get the rotation estimation at time $t_i$ with respect to $t_{j-1}$ from gyroscope:\\n\\n$$q'_{t_i} = q_{t_{j-1}} q_{\\\\triangle t}.$$  \\n\\nWe add $\\\\ell_1$ loss $L_{\\\\text{loose}} = |q_{t_i} - q'_{t_i}|$ into Eq 4, where $q_g_{\\\\text{integrate}}$ integrate all gyroscope measurements from timestamps $t_{j-1}$ to $t_j$.\\n\\n- **Tight coupling.** However, simply integrating IMU information leads to large drift and noise over time. As an immediate consequence of our continuous pose representation over time, we can directly fuse the angular velocity using the quaternion derivative [50]:\\n\\n$$\\\\dot{q} = \\\\frac{1}{2} \\\\Omega(\\\\hat{\\\\omega}).$$  (7)\"}"}
