{"id": "CVPR-2023-2056", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tangentially Elongated Gaussian Belief Propagation for Event-based Incremental Optical Flow Estimation\\n\\nJun Nagata\u2020 and Yusuke Sekikawa\u2020\\nDENSO IT LAB., INC., Japan.\\n\\nAbstract\\nOptical flow estimation is a fundamental functionality in computer vision. An event-based camera, which asynchronously detects sparse intensity changes, is an ideal device for realizing low-latency estimation of the optical flow owing to its low-latency sensing mechanism. An existing method using local plane fitting of events could utilize the sparsity to realize incremental updates for low-latency estimation; however, its output is merely a normal component of the full optical flow. An alternative approach using a frame-based deep neural network could estimate the full flow; however, its intensive non-incremental dense operation prohibits the low-latency estimation. We propose tangentially elongated Gaussian (TEG) belief propagation (BP) that realizes incremental full-flow estimation. We model the probability of full flow as the joint distribution of TEGs from the normal flow measurements, such that the marginal of this distribution with correct prior equals the full flow. We formulate the marginalization using a message-passing based on the BP to realize efficient incremental updates using sparse measurements. In addition to the theoretical justification, we evaluate the effectiveness of the TEGBP in real-world datasets; it outperforms SOTA incremental quasi-full flow method by a large margin.\\n\\n1. Introduction\\nOptical flow estimation, which computes the correspondence of pixels in different time measurements, is a fundamental building block of computer vision. One needs to estimate the flow at low latency in many practical applications, such as autonomous driving cars, unmanned aerial vehicles, and factory automation robots. Most of the existing optical flow algorithm utilizes dense video frames; it computes the flow by searching the similar intensity pattern \\\\[1, 2\\\\]. Recently, methods using deep neural network (DNN) \\\\[29\\\\] demonstrate impressive accuracy at the cost of higher-computational cost. Either model-based or DNN-based, the frame-based algorithm needs to compute the entire pixel for every frame, even when there are subtle changes or no changes at all. This dense operation makes it difficult to realize low-latency estimation, especially on the resource-constrained edge device.\\n\\nThe event-based camera is a bio-inspired vision sensor, which asynchronously detects intensity change on each pixel. Thanks to the novel sensing mechanism, the camera equips favorable characteristics for optical flow estimation, such as high dynamic range (HDR), blur-free measurement, and, most importantly, sparse low-latency data acquisition. Many researchers have explored the way to utilize sparsity to realize efficient low-latency estimation; one extends the well-known Lucas-Kanade algorithm \\\\[7\\\\], and the other exploits the local planer shape of the spatiotemporal event streams \\\\[6\\\\]. These methods could utilize the sparsity for efficient incremental processing; however, the optical flow computed in this way (e.g., by plane fitting) is the normal flow, which is a normal component of full flow and often different from them. The normal flow is the component of the full flow perpendicular to the edge (i.e., parallel to the intensity gradient). Some work tried to recover the full flow from the normal flow \\\\[2\\\\]; however, it does not precisely equal the full flow (refer to Sec. 2). There exist methods that could estimate full flow, such as a variational...\"}"}
{"id": "CVPR-2023-2056", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tional method, a multi-scale extension of contrast maximization, or a frame-based DNN. However, they need to apply non-incremental dense operation for all the pixels of the event frame (dense representation constructed from sparse events) for every frame, which prohibits the low-latency estimation on the edge device.\\n\\nOur research goal is to realize an incremental full flow algorithm from sparse normal flow measurements. To this end, we propose Tangentially Elongated Gaussian (TEG) Belief Propagation (BP). We compute the full flow using the normal flow measurements, which can be observed directly from an event camera or computed cheaply using an existing algorithm. Notice that given a single measurement of normal flow, there are infinite possibilities for the full flow along the tangential direction of the normal flow. We model this uncertainty using the TEG, Gaussian distribution, which has a large variance along the tangential direction of the normal flow. The probability density of full flow on each pixel is given as the marginals of the joint distribution of TEG data factor and some prior factor on a sparse graph (Fig. 1, Sec. 3.3.2). We leverage the sparse graph to formulate the incremental full flow estimation algorithm using message-passing based on BP. We evaluate the effectiveness of the TEGBP on real-world data captured from aerial drones and automobiles. TEGBP outperforms SOTA incremental method by a large margin.\\n\\n2. Related work\\nThis section reviews the related literature on optical flow estimation using sparse event signals.\\n\\n2.1. Incremental Algorithm for Normal Flow\\nLucas-Kanade based method\\nThe traditional frame-based Lucas-Kanade method finds the flow by aligning the small patch of intensity between consecutive frames using its spatial gradient and temporal change. The event-based counterpart uses the integration of events to approximate them for each incoming event. This approach realized highly efficient estimation by utilizing the sparsity of input events. However, it can only estimate the normal flow, which is the component of the full flow perpendicular to the edge (i.e., parallel to the intensity gradient).\\n\\nPlane fitting based method\\nWhen an edge undergoes linear motion, triggered events generate a time surface, which can be locally approximated by a plane in a spatiotemporal space. The local plane fitting method computes the optical flow by fitting the plane to the sparse event points. However, the flow obtained by plane fitting is also the normal component of the full flow.\\n\\nWe consider the normal flow as measurement and discuss algorithms to process the sparse measurement of normal flow to estimate the full flow. Recently, a method for computing the quasi-full flow has been proposed; it estimates the full flow by considering the average normal flow in multiple resolutions. It incrementally updates the average and selects the flow with the largest average norm from the multiple resolutions. It expects the flow to have the largest norm to correspond to the full flow. The average operation induces bias on the estimated flow; therefore, it equals the full flow on the very limited scene where the true motion in the window is orthogonal to the edge direction (refer to Sec. 5 in their paper). We'll also experimentally compare this point in Sec. 4.3.\\n\\nIn summary, current sparse incremental algorithms are limited to normal flow.\\n\\n2.2. Dense Algorithm for Full Flow\\nVariational method\\nA variational optimization-based approach realizes the full flow estimation by incorporating intensity frame obtained either by the event/frame hybrid camera or by simultaneously estimating image intensity. It could estimate full flow; however, it requires additional intensity observation or intensive dense optimization to recover the intensity frame.\\n\\nContrast maximization method\\nContrast maximization (CMax) computes the underlying motion (optical flow) of events stream by finding their alignment. It computes the alignment by maximizing the focus score (e.g., variance) of an image generated by warping the events (IWE) using the optical flow parameter. The original algorithm can not be used to estimate pixel-wise flow because it tends to converge to the degenerated solution (e.g., warps all events to a single point). Recently, a hierarchical CMax approach avoiding the degenerated solution for pixel-wise estimation has been proposed. Although this approach works well even in the practical scenario, it is not incremental; it must recompute the IWE for each sliding time window by using all events in the patch for every hierarchical scale (this involves intensive iteration by itself), making it difficult to operate in real-time.\\n\\nDNN based method\\nFrame-based DNN is often utilized to compute the full flow. Recently, a frame-based high-accuracy model called RAFT has been imported into the event-based vision literature showing the SOTA performance for the dense flow estimation. These methods can not consume sparse events directly; the sparse input must be converted into dense frame representation. The dense convolution is computationally expensive because it needs to operate for all pixels, even when there are subtle changes or no changes at all, making it difficult to operate at a higher rate on the edge device.\\n\\nIn summary, some dense algorithms could estimate full flow; however, it compromises the event's sparse asyn\"}"}
{"id": "CVPR-2023-2056", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"chronous nature, and their non-incremental dense operation prohibits them from realizing low-latency processing.\\n\\n2.3. Sparse Asynchronous Computing Architecture\\n\\nEvents are asynchronously triggered only from limited pixels which detect intensity changes. Several techniques have been developed for efficient processing of the sparse signals on CPUs by taking advantage of the highly optimized cache mechanism \\\\[16\\\\], \\\\[28\\\\]. Besides, sparse data-flow architectures \\\\[8\\\\], \\\\[9\\\\], which differ from prevalent Neuman-type computing architectures, are gaining attention for sparse signal processing. For example, bundle adjustment has been solved on the novel architecture using an algorithm based on Gaussian BP \\\\[21\\\\]. Our goal is to establish the incremental full flow estimation algorithm by taking advantage of the sparsity of events, which is expected to run efficiently on CPUs or emerging data-flow processors.\\n\\n3. Proposed method\\n\\n3.1. Preliminary\\n\\n3.1.1 Normal flow\\n\\nThe normal flow \\\\( \\\\mathbf{v} \\\\ni \\\\mathbf{R} \\\\) on pixel \\\\( i \\\\) is a normal component of full flow \\\\( \\\\mathbf{v} | \\\\mathbf{R} \\\\), which we want to estimate. One cannot decide the full flow uniquely from the normal flow, and there are infinitely many possibilities along the line perpendicular to the normal flow (Fig. 3 left). There are several options for obtaining the normal flow; some event-based cameras, such as CeleX-V \\\\[26\\\\] directly observe it, or it can be cheaply computed from the sparse events (Sec. 2.1). The moving edge in 3D space triggers events; we can compute the normal flow by finding the local plane on the triggered events. In this study, we adopt a naive least-squares fitting of the plane using events in a small spatiotemporal window. An erroneous normal flow may significantly deteriorate the full flow accuracy, especially on highly textured regions such as leaves on trees. One can improve the results by utilizing an advanced noise removal algorithm \\\\[4\\\\] or by adopting more sophisticated normal flow estimation methods, such as small neural networks.\\n\\n3.1.2 Belief propagation\\n\\nFactor graphs geometrically represent the structure of probabilistic problems. A factor graph \\\\( G = (V,F,E) \\\\) is composed of a set of variable nodes \\\\( V = \\\\{v_i\\\\} \\\\) from 1 to \\\\( N \\\\), a set of factor nodes \\\\( F = \\\\{f_s\\\\} \\\\) from 1 to \\\\( N \\\\), and a set of edges \\\\( E \\\\). Each factor node \\\\( f_s \\\\) represents a probabilistic constraint \\\\( f_s(V_s) \\\\) between a subset of variables \\\\( V_s \\\\). The factorization of the variables is explicitly represented in the graph by connecting factor nodes with the variable nodes they depend on. Probabilistically, these factors are the independent terms that make up the joint distribution:\\n\\n\\\\[\\np(\\\\mathbf{V}) = \\\\prod_{s=1}^{N_f} f_s(\\\\mathbf{V}_s).\\n\\\\]\\n\\nBelief propagation (BP) is a generic distributed algorithm for computing the marginal distribution of a set of variables from their joint distribution. The marginal for a single variable \\\\( \\\\mathbf{v}_i \\\\) is computed by the integral over all the other variables. BP computed the marginals asynchronously using message-passing between connected nodes on the graph. It is substantially more efficient by leveraging the topology of the sparse graph than naive integration of the entire graph (which involves giant matrix inversion). Refer to the seminal work of \\\\[9\\\\] for more detail about the derivation, condition for convergence guarantee, and discussion about the stability.\\n\\n3.2. Problem formulation\\n\\nGiven measurements of normal flow \\\\( \\\\mathbf{v} \\\\ni \\\\mathbf{R} \\\\) on the pixel grid where each normal flow is assigned to the corresponding pixels (within time interval \\\\( [t, t+1] \\\\)), we want to estimate the posterior distribution of the corresponding full flow \\\\( p(\\\\mathbf{V} | \\\\mathbf{v} \\\\ni \\\\mathbf{R}) \\\\). Once the posterior distribution of full flow is obtained, the actual full flow output is given by the maximum a posteriori (MAP) estimate as:\\n\\n\\\\[\\n\\\\hat{\\\\mathbf{V}}_{t+1} = \\\\arg \\\\max_{\\\\mathbf{V}_{t+1}} p(\\\\mathbf{V}_{t+1} | \\\\mathbf{v} \\\\ni \\\\mathbf{R}).\\n\\\\]\\n\\nOur goal is to formulate an incremental full flow estimation algorithm using sparse observations of local normal flows (Fig. 2). Solving the \\\\( \\\\arg \\\\max \\\\) of the joint distribution of \\\\( \\\\mathbf{V}_t \\\\) for every incoming normal flow measurement is infeasible both in terms of memory and computational perspective; therefore, we want to formulate \\\\( f \\\\) as an incremental sparse update function, i.e., new observation interacts sparsely with the previous results as follows:\\n\\n\\\\[\\nf: [p(\\\\mathbf{V}_t | \\\\mathbf{v} \\\\ni \\\\mathbf{R}), p(\\\\mathbf{v} \\\\ni \\\\mathbf{R})] \\\\rightarrow p(\\\\mathbf{V}_{t+1} | \\\\mathbf{v} \\\\ni \\\\mathbf{R}).\\n\\\\]\"}"}
{"id": "CVPR-2023-2056", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The relation of normal flow, full flow, and TEG. There are infinitely possible candidates for full flows given a normal flow (left). TEG models this uncertainty using Gaussian having large variance along the tangential direction of the normal flow (right).\\n\\nIn the following section, we'll derive TEGBP, an algorithm based on BP for incremental full flow estimation.\\n\\n3.3. Tangentially Elongated Gaussian BP\\n\\n3.3.1 Tangentially Elongated Gaussian\\n\\nThe true full flow exists somewhere along the line perpendicular to the normal flow ($v_{\\\\perp}$) (Fig. 3 left). In other words, we are uncertain about the full flow from the single normal flow measurement. We model this uncertainty using tangentially elongated Gaussian (TEG), A Gaussian distribution having large variance along the tangential direction of the observed normal flow (Fig. 3 right). We defined the precision matrix of TEG as follows:\\n\\n$$\\\\Lambda_{\\\\text{teg}} = R(\\\\theta) \\\\Lambda_m R(\\\\theta)^\\\\top,$$\\n\\n(4)\\n\\nwhere $R(\\\\theta)$ is 2D rotation matrix parameterized by the rotation angle $\\\\theta$ of the normal flow where $\\\\theta = \\\\arctan(y/x)$, and $\\\\Lambda_m = \\\\text{diag}(1/\\\\sigma_r^2, 1/\\\\sigma_t^2)$.\\n\\nIn principle, $\\\\sigma_t = 1$ and $\\\\sigma_r = 0$ correspond to the constraint of the full flow from a single noiseless normal flow measurement. In this case, one could parameterize the observation with a 1D projection of full flow (normal flow), and the uncertainty of the projected vector length is parameterized as normal variance, $\\\\sigma_r$. In other words, this corresponds to the case when we are equally uncertain along the tangential direction. In a real-world scenario, the probability of full flow along the tangential direction is not uniform but highly concentrated around the normal flow (Supp-D). The knowledge about the probability density of full flow is a vital clue to recovering the accurate full flow from the inherently ill-posed observation of sparse normal flow; however, the 1D parameterization can not incorporate this vital information. Therefore, we propose to utilize 2D Gaussian (TEG) by incorporating additional parameters, i.e., finite $\\\\sigma_t$. The 2D parameterization is crucial for estimating the accurate full flow from sparse and noisy measurements, and our TEG (2D) includes the 1D parameterization as a special case when $\\\\sigma_t = 1$.\\n\\nCorner pixel\\n\\nWhen the naive least-square plane fitting is adopted, the quality for the normal flow estimation might be low; we adjust $\\\\sigma_t$, $\\\\sigma_r$ depending on the quality. We use lower precision $\\\\Lambda_m$ depending on the mean square error between observed events and the estimated plane. Our BP algorithm automatically incorporates this confidence to estimate accurate flow even at the corner point.\\n\\nRemark: If full flow observation is available at the corner (e.g., by corner tracking), TEGBP could utilize the reliable information by TEG with smaller $\\\\sigma_t$ to improve accuracy.\\n\\n3.3.2 Marginalization of TEG with Valid Prior\\n\\nAs we saw earlier, the full flow estimation from the normal flow is inherently an ill-posed without some prior knowledge about the consistency with other pixels (when $\\\\sigma_t = 1$ and $\\\\sigma_r = 0$). Without a valid prior, there are infinitely many possibilities along the line perpendicular to the normal flow; any point on the line has the same likelihood when $\\\\sigma_t = 1$.\\n\\nTo obtain a correct solution, we need the correct prior. Our algorithms could incorporate any prior, e.g., simple uniform smoothness (e.g., total variation), an application-specific knowledge such as flow converges to the vanishing-point in an automotive scenario, or they could be learned using neural networks. In this study, we adopt uniform smoothness for simplicity. The important fact is that the mean of the marginal distribution of TEGs having $\\\\sigma_t = 1$ and the correct prior equals the correct full flow, and this still approximately holds when $\\\\sigma_t$ is moderately large (Sec. B.1).\\n\\nExample. To see this, let's consider the case of Fig. 1. We assume the corner having an edge angle of $\\\\theta_1 + \\\\pi/2$, and $\\\\theta_3 + \\\\pi/2$ moves linearly with uniform velocity (all pixels have the same velocity) toward the direction of $\\\\theta_0$, i.e., $\\\\hat{v} = [\\\\cos(\\\\theta_0), \\\\sin(\\\\theta_0)]^\\\\top$. The correct prior in this situation is uniform (infinitely strong smoothness prior, $\\\\sigma_p = 0$). In this scenario, we'll observed $v_{\\\\perp 1} = v_{\\\\perp 2} = [\\\\cos(\\\\theta_1 - \\\\theta_0) \\\\cos(\\\\theta_1), \\\\cos(\\\\theta_1 - \\\\theta_0) \\\\sin(\\\\theta_1)]^\\\\top$, $v_{\\\\perp 3} = [\\\\cos(\\\\theta_3 - \\\\theta_0) \\\\cos(\\\\theta_3), \\\\cos(\\\\theta_3 - \\\\theta_0) \\\\sin(\\\\theta_3)]^\\\\top$.\\n\\nThe joint distribution of full flow $V := f_{v_1, v_2, v_3}$ from normal flow measurements and the prior is given as:\\n\\n$$p(V) = N(v_1; v_{\\\\perp 1}, \\\\Sigma_{\\\\theta 1}) N(v_2; v_{\\\\perp 2}, \\\\Sigma_{\\\\theta 2}) N(v_3; v_{\\\\perp 3}, \\\\Sigma_{\\\\theta 3}) N([v_1; v_2]; \\\\sigma_p^2 I) N([v_2; v_3]; \\\\sigma_p^2 I).$$\\n\\n(5)\\n\\nMarginalizing for $v_2$,\\n\\n$$p(v_2) = \\\\int p(V) dv_1 dv_3$$\\n\\nand putting $\\\\sigma_t = 1$ and $\\\\sigma_r = 0$, we'll get $p(v_2) = N(v_2; [\\\\cos(\\\\theta_0), \\\\sin(\\\\theta_0)]^\\\\top; \\\\text{diag}(0, 0))$; the MAP of this marginal equals to the true flow $\\\\hat{v}$.\\n\\nOne can also visually verify this result by considering the intersection of the lines of the major axis of the TEG. Notice that the marginal is equal to the full flow even when an average of the normal flow, such as employed in [2], differs from the full flow.\"}"}
{"id": "CVPR-2023-2056", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. 2D factor graph of TEGBP. The topology changes dynamically depending on the active set $A_t$. The measurement node and the active variable node on the same pixel are connected by measurement factor node, and each neighbor active variable node is connected via prior factor node.\\n\\nAbove case, the average normal flow ($v = \\\\left( \\\\frac{2 \\\\cos(\\\\theta_0) \\\\cos \\\\theta_1 + \\\\cos(\\\\theta_2) \\\\cos \\\\theta_3}{3}, \\\\frac{2 \\\\cos(\\\\theta_0) \\\\sin \\\\theta_1 + \\\\cos(\\\\theta_2) \\\\sin \\\\theta_3}{3} \\\\right)$) is different from the true flow ($[\\\\cos \\\\theta_0, \\\\sin \\\\theta_0]^T$). This is a steak difference from the existing incremental sparse method using an average of normal flow and is a key to realizing a significant accuracy boost over the technique.\\n\\n3.3.3 Factor Graph of Full Flow\\n\\nNow we construct the sparse factor graph for full flow using the TEG. The joint of Eq. (2) can be factorized using Bayes theorem as follows:\\n\\n$$\\\\hat{V}_t = \\\\arg \\\\max_{V_t} p(V_t)$$\\n\\n(6)\\n\\nAssuming independence of observations and dependence between neighboring-pixel variables, Eq. (6) is further factorized to the measurement factor $\\\\psi$ and the prior factor $\\\\phi$ defined on a sparse graph on the image grid (Fig. 4) as:\\n\\n$$p(V_t) = \\\\prod_{n \\\\in A_t} \\\\psi(v_n) \\\\prod_{(i,j) \\\\in E_t} \\\\phi(v_i, v_j),$$\\n\\n(7)\\n\\nwhere $A_t$ is the set of active nodes corresponding normal flow measurements $V_{\\\\perp t}$, and $E_t$ is the set of edges between active neighbor nodes. The measurements factor $\\\\psi$ defines as TEG (refer to Sec. 3.3.1):\\n\\n$$\\\\psi(v_n) = \\\\frac{1}{N-1} \\\\left( N \\\\cdot \\\\eta_n^{\\\\Lambda_{teg} n} \\\\right),$$\\n\\n(8)\\n\\nwhere $\\\\eta_n = \\\\Lambda_{teg} n v_{\\\\perp n}$. In this study, we assume the flow is smooth; therefore, use the following prior factor $\\\\phi$:\\n\\n$$\\\\phi([v_i, v_j]) = \\\\frac{1}{N-1} \\\\left( N \\\\cdot 0^{\\\\Lambda_p} \\\\right),$$\\n\\n(9)\\n\\nwhere $\\\\Lambda_p = J_p^\\\\top \\\\text{diag}(1/\\\\sigma_p^2, 1/\\\\sigma_p^2) J_p$ and the jacobian $J_p = \\\\begin{bmatrix} -1 & 0 & 1 & 0 \\\\\\\\ 0 & -1 & 0 & 1 \\\\end{bmatrix}$ for computing the difference between the neighbor nodes.\\n\\nTable 1. Basic experimental setup.\\n\\n| Parameter        | ESIM | DVS | MVSEC | DSEC |\\n|------------------|------|-----|-------|------|\\n| Window size      | 5    | 5   | 3     | 5    |\\n| $\\\\Delta t_{pf}$  | 40   | 40  | 40    | 40   |\\n| Active duration  | 50   | 50  | 100   | 100  |\\n| Prior std        | 0.1  | 1   | 0.5   | 1    |\\n| Tangential std   | 10   | 10  | 10    | 10   |\\n| Radial std       | 3    | 3   | 3     | 3    |\\n| Num. of layers   | 5    | 5   | 5     | 5    |\\n| Batch size       | 100  | 100 | 1000  | 1000 |\\n\\nTo model the outlier that can not be expressed as a Gaussian (quadratic cost), we employed the Huber cost both for measurement and prior factors. We adopted the technique of [1, 9] to maintain the Gaussian form.\\n\\n3.3.4 TEGBP\\n\\nEquipped with the sparse probabilistic graph, we now formalize the incremental message-passing algorithm base on BP (Sec. 3.1.2) for the marginalization of the graph to estimate the full flow (Fig. 5). As the graph of active nodes is loopy, GBP needs to store a belief at each variable node for iterative belief update.\\n\\nMessage-passing is asynchronously triggered upon a space observation of normal flow. The measurement factor sends a message to the connected variable node and updates its belief with messages that have been sent from the active node as follows:\\n\\n$$\\\\eta_{b_i} = \\\\sum_{j \\\\in C_i \\\\cap A_t} \\\\eta_{j \\\\rightarrow i}, \\\\quad \\\\Lambda_{b_i} = \\\\sum_{j \\\\in C_i \\\\cap A_t} \\\\Lambda_{j \\\\rightarrow i},$$\\n\\n(10)\\n\\nwhere $b_i(v_i) = \\\\frac{1}{N-1} \\\\left( N \\\\cdot \\\\eta_{b_i}^{\\\\Lambda_{b_i}} \\\\right)$ is the belief on node $i$, $\\\\mu_{j \\\\rightarrow i} = \\\\frac{1}{N-1} \\\\left( N \\\\cdot \\\\eta_{j \\\\rightarrow i}^{\\\\Lambda_{j \\\\rightarrow i}} \\\\right)$ is the message from node $j$ to node $i$, and $C_i$ is the 4-neighbors connected node $j$. After that, the variable node sends the following message about its updated beliefs to its neighbors:\\n\\n$$\\\\eta_{j \\\\rightarrow i} = \\\\Lambda_{p01} \\\\left( \\\\Lambda_{p11}^{\\\\Lambda_{b_j}} \\\\Lambda_{j \\\\rightarrow i} \\\\right) - 1 \\\\left( \\\\eta_{b_j} \\\\eta_{j \\\\rightarrow i} \\\\right),$$\\n\\n(11)\\n\\n$$\\\\Lambda_{j \\\\rightarrow i} = \\\\Lambda_{p00} \\\\Lambda_{p01} \\\\left( \\\\Lambda_{p11}^{\\\\Lambda_{b_j}} \\\\Lambda_{j \\\\rightarrow i} \\\\right) - 1 \\\\Lambda_{p10},$$\\n\\n(12)\\n\\nwhere $\\\\Lambda_{p\\\\alpha\\\\beta}$ is a $2 \\\\times 2$ sub-matrix of $\\\\Lambda_p$ indexed by $\\\\alpha$ and $\\\\beta$.\\n\\nFor a single normal flow measurement, the belief is propagated for $K$ hops, which are repeated for $N_{itr}$ times (Fig. 5 correspond to $K = 2$).\\n\\nNote that our BP algorithm is guaranteed to converge to the correct MAP solution with sufficient iteration because all the belief is realized as a Gaussian (there is no guarantee for a variance because the graph is loopy) [30]. Coarse-to-fine message-passing scheme [10] is adopted to speed up the convergence. The measurement factor of\"}"}
{"id": "CVPR-2023-2056", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. A schematic showing a single iteration of TEGBP. (a) When a single normal flow has been measured, the message from the measurement factor is sent to the variable node as TEG, (b) the node updates its belief using the message received from the neighbor active nodes. (c) The nodes broadcast the updated belief to the neighbor, and (d) the neighbor nodes update their beliefs. (a)-(d) is repeated until convergence. To prevent the blowup of the message communication, it is stopped after $K$ hops.\\n\\nFigure 6. The result on ESIM-bricks. The estimated full flow from ARMS and TEGBP are compared with ground-truth flow indicated by the red arrow. They use the same normal flow measurement (left). TEGBP succeeds while ARMS fails.\\n\\nFigure 7. The results on DVS-stripes. The histograms of the flow ($v_x$) of ARMS and TEGBP are compared. The two red lines indicate the GT flow of the two stripes (the upper stripes have a smaller norm). The result from ARMS are shifted from the GT flow, especially on the area of upper stripes (smaller norm). On the contrary, our TEGBP successfully estimates individual flows.\\n\\nThe coarser layer $l$ is computed as the sum of active measurement factor messages in a $2^2$ block of finer layer $l_{1}$. The BP is executed coarse-to-fine; the coarse layer (message) estimation is copied to the finer layer as the initial guess at the start of the message passing on the finer layer (Supp-C.1). The short paths in the coarse graph improve efficiency for capturing long-range interactions and speed up convergence.\\n\\n4. Experiment\\n\\nWe conducted experiments to compare the accuracy of TEGBP with the state-of-the-art incremental flow estimation algorithm called aperture-robust multi-scale flow estimation (ARMS) \\\\[2\\\\]. Both algorithms are incremental that could asynchronously estimate flow from sparse normal flow measurement. We used the same normal flow measurement for both algorithms for a fair comparison.\\n\\n4.1. Experimental setting\\n\\n4.1.1 Normal flow measurement\\n\\nWe utilize the simple plane fitting to compute the normal flow measurements that serve as inputs to TEGBP and ARMS. We first apply noise removal for the raw events using a refractory filter \\\\[17\\\\] of 40 ms duration. Then using the filtered events, we performed plane fitting for each event within a small spatiotemporal window of the size $r \\\\Delta t_{pf}$. During the plane fitting, we removed the outlier using the criteria proposed in \\\\[2\\\\]; we applied this process three times. We summarize the normal flow computation parameters in Tab. \\\\[1\\\\] (top).\\n\\n4.1.2 Full flow estimation\\n\\nBoth TEGBP and ARMS use the same sparse normal flow measurement, and could be run asynchronously at the rate of normal flow; however, we process $N_{b}$ observation at once for computational efficiency on MATLAB \\\\[18\\\\] (interactive numerical computing environment) we used to compare the accuracy. The specific parameters for the (quasi) full flow computation are summarized in Tab. \\\\[1\\\\] (bottom).\\n\\nIn TEGBP, the message should be propagated to all connected nodes, and the process needs to be repeated until convergence; however, we found a small number of hops ($K=2$) and single iterations ($N_{itr}=1$) works well in practice.\\n\\n4.2. ESIM-bricks\\n\\nIn this experiment, we compare the accuracy on a scene where the motion and edges are not perpendicular to the image edge. We synthesized the event stream by observing the image of bricks from a translating camera. We used ESIM \\\\[23\\\\] library for this simulation, which could synthesize the realistic noisy event stream from the event camera moving around the predefined 3D environment. We compute the normal flow and use the same measurements for both methods. Fig. 6 shows the result. We observe the drift in ARMS due to the average operation. TEGBP correctly\"}"}
{"id": "CVPR-2023-2056", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. The quantitative results on MVSEC. The flow is evaluated on the ground truth intervals.\\n\\n| Indoor flying1 | Indoor flying2 | Indoor flying3 | Outdoor day1 |\\n|---------------|----------------|----------------|--------------|\\n| AEE \u2193 % Out   | AEE \u2193 % Out    | AEE \u2193 % Out    | AEE \u2193 % Out  |\\n| Norm          | ARMS           | Ours           |              |\\n| 2.30          | 1.71           | 1.14           |              |\\n| 24.5          | 12.7           | 6.25           |              |\\n| 3.61          | 2.67           | 1.87           |              |\\n| 42.9          | 26.6           | 16.4           |              |\\n| 3.13          | 21.6           | 11.8           |              |\\n| 36.3          | 25.7           | 11.1           |              |\\n| 3.44          |                |                |              |\\n| 43.0          |                |                |              |\\n\\nFigure 8. The qualitative results on MVSEC. See the color wheel on the bottom left for the color encoding.\\n\\nestimated the full flow from the normal flow measurement directed at a different angle from the full flow.\\n\\n4.3. DVS-strips\\nIn this experiment, we compare the accuracy on a scene where some region includes different flows. We used a scene from [19] where two stripes translating at different speeds are observed from a dynamic vision sensor (DVS). Fig. 7 shows the result. We observed a significant shift in the ARMS\u2019s estimation when motions of different speeds are adjacent. ARMS determines the flow by selecting the scale having the maximum average norm in the window (it falsely picks the large norm in the slow region). In contrast, the TEGBP correctly estimates individual motion.\\n\\n4.4. MVSEC\\nWe quantitatively compare with ARMS in practical robotic scenarios. We used outdoor scenes captured by automobiles and indoor scenes captured by aerial drones from the Multi-vehicle Stereo Event Camera (MVSEC) dataset [33]. This dataset includes ground truth (GT) optical flow computed as the motion field from the camera motion and scene depth. We evaluate the accuracy using the average endpoint error (AEE) and the percentage of pixels with endpoint error greater than 3 pixels (denoted by % Out). Both are measured on pixels where valid GT exists, and the plane fitting has been successful. We output flows asynchronously in batches with both methods and evaluate them at regular intervals (20Hz of GT rate). Tab. 2 and Fig. 8 show the quantitative and qualitative results. ARMS improves the normal flow, while TEGBP improves more. In contrast to the naive averaging in ARMS, our TEGBP properly models the distribution of flows using TEG and prior.\\n\\n4.5. DSEC\\nWe compare the performance of TEGBP and ARMS on more diverse scenarios using more high-resolution input. To this end, we use the DSEC [13], a recently released dataset in automobile scenarios containing high-resolution event data of 480 x 640 captured from the vehicle that undergoes more diverse motion. It is designed for dense optical flow estimation, and the GT of the test sequence is hidden for leaderboard evaluation. Therefore we use their train sequence to compare sparse AEE error with ARMS. Tab. 3 and Fig. 9 shows the quantitative and qualitative results. Similar to the MVSEC experiments, TEGBP sig...\"}"}
{"id": "CVPR-2023-2056", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. The quantitative results on DSEC. The flow is evaluated on the ground truth intervals.\\n\\n| AEE \u2193 % Out | AEE \u2193 % Out | AEE \u2193 % Out | AEE \u2193 % Out |\\n|------------|------------|------------|------------|\\n| Norm       | 7.09       | 68.7       | 8.26       | 37.9       | 12.8       | 86.8       | 8.42       | 73.3       | 5.90       | 63.4       |\\n| ARMS       | 5.31       | 60.0       | 6.32       | 23.8       | 8.98       | 78.7       | 6.22       | 64.4       | 4.57       | 51.4       |\\n| Ours       | 4.26       | 45.5       | 5.61       | 23.2       | 8.70       | 72.6       | 5.08       | 49.5       | 3.27       | 32.2       |\\n\\nFigure 9. The qualitative results on DSEC.\\n\\n5. Conclusion\\nWe proposed TEGBP, a novel incremental full flow estimation algorithm for asynchronous event data. The proposed algorithm is backed up by the probabilistic model, which guarantees equality to the full flow when a valid prior is provided. It realizes efficient event-wise incremental updates based on the BP, where the MAP solution obtained by the local update is guaranteed to match the correct one. We demonstrate the effectiveness of the proposed method on a practical real-world dataset showing a significant boost in accuracy over the existing incremental algorithm.\\n\\nThere is still much room for improvement in accuracy compared to the recent machine learning-based method (although it is computationally intensive). We suspect the gap mainly comes from the noise in the normal flow and poor prior. Toward the ultimate goal of realizing low-latency and high-accuracy full flow estimation in real-time on an edge device, we expect the gap to be closed by incorporating a more robust normal flow and application-specific prior.\\n\\n5.1. Future work\\nRobust normal flow estimation\\nAs discussed in the experiment section, some scenes include strongly erroneous flow, e.g., normal flow from leaves on the tree. These measurement errors significantly degrade the accuracy. We expect the tiny CNN (e.g., input size of $7 \\\\times 7$) trained to regress the normal flow from noisy input will improve the accuracy without sacrificing the computational efficiency.\\n\\nApplication specific prior\\nEven if the perfect normal flow is obtained, we cannot expect good accuracy without good prior (Sec. 3.3.2). The uniform smoothness prior we adopted may not be optimal in many practical scenarios. The discontinuous depth region does not conform to the prior. Even if the planar object undergoes linear motion, we won't observe uniform flow when it is not perpendicular to the optical axis. We can utilize a better prior for a specific application to boost accuracy. For example, one may utilize a vanishing point for automotive applications (assuming the yaw rate is known from the vehicle sensor) similar to [20]. We left these explorations for future work.\\n\\nOptimized implementation\\nOur algorithm is very efficient in principle, requiring lower FLOPS, and the entire process can be asynchronously parallelized. The normal flow could be cheaply obtained, e.g., directly from a camera or computed by the lightweight plane fitting (computation of normal flow is outside this study's scope). The core of our algorithm, message passing of (4), is also very cheap, requiring the communication of 6-dim vectors with neighbors. Therefore, an optimized CPU implementation or adaptation of processors supporting the sparse computation could realize low-energy, low-latency optical flow estimation in the edge device. We left the wall clock time and energy consumption evaluation as future work.\"}"}
{"id": "CVPR-2023-2056", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Pratik Agarwal, Gian Diego Tipaldi, Luciano Spinello, Cyrill Stachniss, and Wolfram Burgard. Robust Map Optimization using Dynamic Covariance Scaling. IEEE International Conference on Robotics and Automation (ICRA), 2013.\\n\\n[2] Himanshu Akolkar, Sio Hoi Ieng, and Ryad Benosman. Real-time high speed motion prediction using fast aperture-robust event-driven visual flow. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 44(1):1\u201312, 2020.\\n\\n[3] Myo Tun Aung, Rodney Teo, and Garrick Orchard. Event-based Plane-fitting Optical Flow for Dynamic Vision Sensors in FPGA. IEEE International Symposium on Circuits and Systems, 2018.\\n\\n[4] R. Wes Baldwin, Mohammed Almatrafi, Vijayan Asari, and Keigo Hirakawa. Event Probability Mask (EPM) and Event Denoising Convolutional Neural Network (EDnCNN) for Neuromorphic Cameras. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1701\u20131710, 2020.\\n\\n[5] Patrick Bardow, Andrew J. Davison, and Stefan Leutenegger. Simultaneous Optical Flow and Intensity Estimation from an Event Camera. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 884\u2013892, 2016.\\n\\n[6] Ryad Benosman, Charles Clercq, Xavier Lagorce, Sio-Hoi Ieng, and Chiara Bartolozzi. Event-Based Visual Flow. IEEE Transactions on Neural Networks and Learning Systems, 25(2):407\u2013417, feb 2014.\\n\\n[7] Ryad Benosman, Sio-Hoi Ieng, Charles Clercq, Chiara Bartolozzi, and Mandyam Srinivasan. Asynchronous frameless event-based optical flow. Neural Networks, 27:32\u201337, mar 2012.\\n\\n[8] Andrew J. Davison. FutureMapping: The Computational Structure of Spatial AI Systems. arXiv, 2018.\\n\\n[9] Andrew J. Davison and Joseph Ortiz. FutureMapping 2: Gaussian Belief Propagation for Spatial AI. arXiv, 2019.\\n\\n[10] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient Belief Propagation for Early Vision. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2004.\\n\\n[11] Guillermo Gallego, Tobi Delbruck, Garrick Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew Davison, Joerg Conradt, Kostas Daniilidis, and Davide Scaramuzza. Event-based Vision: A Survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 44(1), 2020.\\n\\n[12] Guillermo Gallego, Henri Rebecq, and Davide Scaramuzza. A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3867\u20133876, 2018.\\n\\n[13] Mathias Gehrig, Willem Aarents, Daniel Gehrig, and Davide Scaramuzza. DSEC: A Stereo Event Camera Dataset for Driving Scenarios. IEEE Robotics and Automation Letters, 6(3):4947\u20134954, 2021.\\n\\n[14] Mathias Gehrig, Mario Millh\u00f6ausler, Daniel Gehrig, and Davide Scaramuzza. E-RAFT: Dense Optical Flow from Event Cameras. International Conference on 3D Vision (3DV), 2021.\\n\\n[15] B. K. B. Horn and B. G. Schunck. Determining Optical Flow. Artificial Intelligence, 17(1-3):185\u2013203, 1981.\\n\\n[16] Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin, William Leiserson, Sage Moore, Bill Nell, Nir Shavit, and Dan Alistarh. Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference. Proceedings of Machine Learning Research, 119:5533\u20135543, 2020.\\n\\n[17] Weng Fei Low, Zhi Gao, Cheng Xiang, and Bharath Ramesh. SOFEA: A non-iterative and robust optical flow estimation algorithm for dynamic vision sensors. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2020.\\n\\n[18] MATLAB. version 9.13.0 (R2022b). The MathWorks Inc., Natick, Massachusetts, 2022.\\n\\n[19] Elias Mueggler, Christian Forster, Nathan Baumli, Guillermo Gallego, and Davide Scaramuzza. Lifetime Estimation of Events from Dynamic Vision Sensors. IEEE International Conference on Robotics and Automation (ICRA), 2015.\\n\\n[20] Jun Nagata, Yusuke Sekikawa, Kosuke Hara, and Yoshimitsu Aoki. FOE-based Regularization for Optical Flow Estimation from an In-vehicle Event Camera. Proceedings of SPIE - The International Society for Optical Engineering, 11049, 2019.\\n\\n[21] Joseph Ortiz, Mark Pupilli, Stefan Leutenegger, and Andrew J. Davison. Bundle Adjustment on a Graph Processor. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2413\u20132422, 2020.\\n\\n[22] Liyuan Pan, Miaomiao Liu, and Richard Hartley. Single Image Optical Flow Estimation with an Event Camera. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[23] Henri Rebecq, Daniel Gehrig, and Davide Scaramuzza. ESIM: an Open Event Camera Simulator. Conf. on Robotics Learning (CoRL), 87:969\u2013982, 2018.\\n\\n[24] Shintaro Shiba, Yoshimitsu Aoki, and Guillermo Gallego. Event Collapse in Contrast Maximization Frameworks. MDPI Sensors, 22(14):1\u201320, 2022.\\n\\n[25] Shintaro Shiba, Yoshimitsu Aoki, and Guillermo Gallego. Secrets of Event-Based Optical Flow. European Conference on Computer Vision (ECCV), pages 1\u201323, 2022.\\n\\n[26] Chen Shoushun and Guo Menghan. Live Demonstration : CeleX-V : a 1M Pixel Multi-Mode Event-based Sensor Chen Shoushun. IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 1\u20132, 2019.\\n\\n[27] Timo Stoffregen and Lindsay Kleeman. Event Cameras , Contrast Maximization and Reward Functions : an Analysis. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 12300\u201312308, 2019.\\n\\n[28] Daniel C. Stumpp, Himanshu Akolkar, Alan D. George, and Ryad B. Benosman. hARMS: A Hardware Acceleration Architecture for Real-Time Event-Based Optical Flow. IEEE Access, 10:58181\u201358198, 2022.\"}"}
{"id": "CVPR-2023-2056", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zachary Teed and Jia Deng. RAFT: Recurrent All-Pairs Field Transforms for Optical Flow. European Conference on Computer Vision (ECCV), 2020.\\n\\nYair Weiss and William T Freeman. Correctness of belief propagation in Gaussian graphical models of arbitrary topology. Neural Information Processing Systems (NIPS), pages 673\u2013679, 2000.\\n\\nChengxi Ye, Anton Mitrokhin, Cornelia Ferm\u00fcller, James A Yorke, and Yiannis Aloimonos. Unsupervised Learning of Dense Optical Flow, Depth and Egomotion from Sparse Event Data. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\\n\\nAlex Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis. EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based Cameras. Proceedings of Robotics: Science and Systems, jun 2018.\\n\\nAlex Zihao Zhu, Dinesh Thakur, Tolga Ozaslan, Bernd Pfrommer, Vijay Kumar, and Kostas Daniilidis. The Multi Vehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception. IEEE Robotics and Automation Letters, 3(3):2032\u20132039, 2018.\\n\\nAlex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis. Unsupervised event-based optical flow using motion compensation. European Conference on Computer Vision (ECCV) Workshop, pages 711\u2013714, 2018.\\n\\nAlex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis. Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 989\u2013997, 2019.\"}"}
