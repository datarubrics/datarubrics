{"id": "CVPR-2024-2169", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Selective-Stereo: Adaptive Frequency Information Selection for Stereo Matching\\n\\nXianqi Wang*, Gangwei Xu*, Hao Jia, Xin Yang\u2020\\nHuazhong University of Science and Technology\\n{xianqiw, gwxu, haojia, xinyang2014}@hust.edu.cn\\n\\nAbstract\\nStereo matching methods based on iterative optimization, like RAFT-Stereo and IGEV-Stereo, have evolved into a cornerstone in the field of stereo matching. However, these methods struggle to simultaneously capture high-frequency information in edges and low-frequency information in smooth regions due to the fixed receptive field. As a result, they tend to lose details, blur edges, and produce false matches in textureless areas. In this paper, we propose Selective Recurrent Unit (SRU), a novel iterative update operator for stereo matching. The SRU module can adaptively fuse hidden disparity information at multiple frequencies for edge and smooth regions. To perform adaptive fusion, we introduce a new Contextual Spatial Attention (CSA) module to generate attention maps as fusion weights. The SRU empowers the network to aggregate hidden disparity information across multiple frequencies, mitigating the risk of vital hidden disparity information loss during iterative processes. To verify SRU\u2019s universality, we apply it to representative iterative stereo matching methods, collectively referred to as Selective-Stereo. Our Selective-Stereo ranks 1st on KITTI 2012, KITTI 2015, ETH3D, and Middlebury leaderboards among all published methods. Code is available at https://github.com/Windsrain/Selective-Stereo.\\n\\n1. Introduction\\nStereo matching is a fundamental area of research in computer vision. It explores the calculation of displacement, referred to as disparity, between matching points in a pair of rectified images. This technique plays a significant role in various applications, including 3D reconstruction and autonomous driving.\\n\\nWith the advancement of deep learning, learning-based stereo matching [4, 14, 15, 17, 34, 35] have progressively displaced traditional methods and significantly enhancing the accuracy of disparity estimation. Initially, aggregation-based methods [7, 32, 41] led the development of stereo matching algorithms. These methods begin by defining a maximum range of disparity, constructing a 4D cost volume using feature maps, and subsequently employing 3D CNN to filter the volume and derive the final disparity map. Such methods focus on filtering the initially coarse cost volume, thus effectively aggregating geometry information. However, cost aggregation requires a large number of convolutions, resulting in high computational costs, making it difficult to be applied to high-resolution images.\\n\\nRecently, a novel class of methods based on iterative optimization [11, 16, 17, 44] has been gaining prominence and this CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\\n\\nFigure 1. Row 1: Comparisons with state-of-the-art stereo methods on KITTI 2012 [13] and KITTI 2015 [21], ETH3D [24] and Middlebury [23] leaderboards. Row 2: Visual comparison with RAFT-Stereo on ETH3D. Row 3: Visual comparison with IGEV-Stereo on Middlebury. Our method distinguishes subtle details and sharp edges and performs well in weak texture regions.\"}"}
{"id": "CVPR-2024-2169", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"achieving state-of-the-art performance on several leaderboards. These methods begin by constructing an all-pairs cost volume, indexing a local cost volume from the original cost volume, and subsequently employing recurrent units [10] to calculate disparity residuals and update the disparity prediction. One major advantage of these methods is their ability to capture all candidate matching points without predefining the range of disparities. Additionally, these methods don't need to aggregate the cost volume using a large number of redundant convolutions. Instead, a continuous update of the disparity prediction is achieved through lightweight recurrent units during iterations. Therefore, these methods are capable of processing high-resolution images.\\n\\nHowever, iterative methods encounter several challenges. Firstly, the all-pairs cost volume includes considerable noisy information [34], potentially causing the loss of crucial information when iterating the hidden information. Besides, as the network iterates, the hidden information increasingly incorporates global low-frequency information while losing local high-frequency information like edges and thin objects [44]. Secondly, the existing recurrent units possess a fixed receptive field, leading the network to solely concentrate on information at the current frequency and ignore other frequencies, such as detailed, edge, and textureless information.\\n\\nIn this paper, we propose Selective Recurrent Unit (SRU) to address the limitations of traditional recurrent units. As Chen et al. [5] mentions, features contain information at different frequencies, high-frequency information describes rapidly changing fine details, while low-frequency information describes smoothly changing structures. Unlike traditional recurrent units that treat information at different frequencies equally, our SRU incorporates multiple branches of GRU, each with a distinct kernel size representing different receptive fields. The hidden information obtained from each GRU branch is fused and then fed into the next iteration. This fusion enables the capture of information from different receptive fields at different frequencies, while also performing secondary filtering to reduce noise information from local cost volume. To further enhance the fusion process, we propose a Contextual Spatial Attention (CSA) module to utilize the context information. Instead of simply summarizing information from different branches, CSA introduces attention maps extracted from the context information. After doing so, information captured by small kernels has large weights in regions like edge, while information captured by large kernels has large weights in regions like low-texture. These attention maps determine the weight of fusion, allowing the network to adaptively select suitable information based on different image regions. Besides, we prove the effectiveness and universality of our module by transferring it to different iterative networks. All networks are collectively referred to as Selective-Stereo. By doing so, we consistently improve the performance of these networks without introducing a significant increase in parameters and time.\\n\\nWe demonstrate the effectiveness of our method on several stereo benchmarks. On Scene Flow [20], our Selective-RAFT reaches the state-of-the-art EPE of 0.47, and our Selective-IGEV even achieves a new state-of-the-art EPE of 0.44. And as shown in Fig. 1, our Selective-RAFT surpasses RAFT-Stereo by a large margin and achieves competitive performance compared with the state-of-the-art methods on KITTI [13, 21] leaderboards. Our Selective-IGEV ranks 1st on KITTI, ETH3D [24], and Middlebury [23] leaderboards among all published methods.\\n\\nOur main contributions can be summarized as follows:\\n\\n\u2022 We propose a novel iterative update operator SRU for iterative stereo matching methods.\\n\u2022 We introduce a new Contextual Spatial Attention module that generates attention maps for adaptively fusing hidden disparity information at multiple frequencies.\\n\u2022 We verify the universality of our SRU on several iterative stereo matching methods.\\n\u2022 Our method outperforms existing published methods on public leaderboards such as KITTI, ETH3D, and Middlebury.\"}"}
{"id": "CVPR-2024-2169", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Frequency information application in vision. There are several works\\\\[5, 6, 39\\\\] that focus on using frequency information in computer vision. Chen \\\\et al\\\\[5\\\\] propose the octave convolution to factorize the mixed feature maps by their frequencies. Xu \\\\et al\\\\[39\\\\] propose a method of learning in the frequency domain and suggest that CNN models are more sensitive to low-frequency channels than high-frequency. DSGAN \\\\[12\\\\] introduces the frequency separation into super-resolution. LITv2 \\\\[22\\\\] proposes to disentangle the high/low-frequency patterns in an attention layer.\\n\\n3. Method\\n\\nIn this section, we present the overall architecture of Selective-Stereo. Because our method can be plugged into different networks, we take Selective-RAFT (Fig. 2) as an example and focus on illustrating its key components.\\n\\n3.1. Feature Extraction\\n\\nTo ensure fair comparisons, Selective-RAFT maintains consistency with RAFT-Stereo \\\\[17\\\\] by employing its feature extraction network. Feature extraction comprises two main components: feature network and context network.\\n\\nFeature Network. Given the left and the right images $I_l, I_r \\\\in \\\\mathbb{R}^{3 \\\\times H \\\\times W}$, we first downsample them to $1/2$ resolution using a $7 \\\\times 7$ convolutional layer. Then, a series of residual blocks is employed to extract features and we apply another downsampling layer to get features at $1/4$ resolution in the middle. Lastly, a $1 \\\\times 1$ convolutional layer is applied to get the final left and right features $f, g \\\\in \\\\mathbb{R}^{C \\\\times H/4 \\\\times W/4}$ with suitable dimensions.\\n\\nContext Network. Its architecture remains consistent with the feature network, and it adds a series of residual blocks and two additional downsampling layers, obtaining multi-level context features $f_{ci}(i = 1, 2, 3)$ at $1/4, 1/8, 1/16$ resolutions. Then we can get the initial hidden and the context features.\"}"}
{"id": "CVPR-2024-2169", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\text{hi} = \\\\tanh(fc_i) \\\\]\\n\\\\[ ci = \\\\text{ReLU}(fc_i) \\\\] (1)\\n\\n3.2. Cost Volume Construction\\n\\nGiven the left and the right features \\\\( f, g \\\\), we first construct an all-pairs correlate cost volume:\\n\\\\[ C_{ijk} = X_{hi} f_{hij} \\\\cdot g_{hik}, C \\\\in \\\\mathbb{R}^{H_4 \\\\times W_4 \\\\times W_4} \\\\] (2)\\n\\nThen we construct a 4-level correlation pyramid \\\\( \\\\{C_i\\\\} \\\\) \\\\( i = 1, 2, 3, 4 \\\\) by using 1D average pooling with a kernel size of 2 and a stride of 2 at the last dimension.\\n\\n3.3. Contextual Spatial Attention Module\\n\\nTo help information from different receptive fields and frequencies fuse, the Contextual Spatial Attention (CSA) module extracts multi-level attention maps from context information as guidance. As illustrated in Fig. 3, CSA can be divided into two submodules: Channel Attention Enhancement (CAE) and Spatial Attention Extractor (SAE). These submodules are derived from CBAM [31] and we simplify them to better adapt to stereo matching.\\n\\nChannel Attention Enhancement.\\n\\nGiven a context information map \\\\( c \\\\in \\\\mathbb{R}^{C \\\\times H \\\\times W} \\\\), we first use an average-pooling and a max-pooling operation on the spatial dimension to get two maps \\\\( f_{avg}, f_{max} \\\\in \\\\mathbb{R}^{C \\\\times 1 \\\\times 1} \\\\). Then we use two convolutional layers to perform feature transformation on these maps separably. After that, we add these two maps together and use the sigmoid function to convert them into weights \\\\( M_c \\\\in \\\\mathbb{R}^{C \\\\times 1 \\\\times 1} \\\\) between 0 and 1. Lastly, using an element-wise product, the initial map can capture which channel map has high feature values to be enhanced, and which channel map has low feature values to be suppressed.\\n\\nSpatial Attention Extractor.\\n\\nAfter the CAE module, we continue to use the same pooling operations, but now we pool on the channel dimension. Then we concatenate these pooling maps to form a map in \\\\( \\\\mathbb{R}^{2 \\\\times H \\\\times W} \\\\) and use one convolutional layer with a sigmoid function to generate the final attention map. Reviewing previous operations, this attention map has high weights in regions needing high-frequency information because this information possesses high feature values in the context information. Similarly, it has low weights in regions needing low-frequency information. In general, the attention map can explicitly distinguish regions that need information at different frequencies.\\n\\n3.4. Selective Recurrent Unit\\n\\nTo capture information at different frequencies, Selective Recurrent Unit (SRU) uses attention maps extracted by CSA to fuse hidden information derived from GRUs with different kernel sizes.\\n\\nMulti-level update structure.\\n\\nAs illustrated in Fig. 4, SRUs at \\\\( 1/8, 1/16 \\\\) resolutions take the attention map, context information, hidden information at the same resolution, and the hidden information at adjacent resolutions as inputs. At \\\\( 1/4 \\\\) resolution, SRUs take disparity, and local cost volume as additional inputs, and then their outputs will go through two convolutional layers to generate disparity residuals. The local cost volume is derived from the all-pairs correlation pyramid in the same way as RAFT-Stereo [17]. At last, disparities at \\\\( 1/4 \\\\) resolution will be upsampled into full resolution using the convex combination.\\n\\nSRU's architecture.\\n\\nA single GRU can be defined as follows:\\n\\\\[ z_k = \\\\sigma(\\\\text{Conv}(\\\\lfloor h_{k-1}, x_k \\\\rfloor, W_z)), r_k = \\\\sigma(\\\\text{Conv}(\\\\lfloor h_{k-1}, x_k \\\\rfloor, W_r)) \\\\]\\n\\\\[ \\\\tilde{h}_k = \\\\text{tanh}(\\\\text{Conv}(\\\\lfloor r_k \\\\odot h_{k-1}, x_k \\\\rfloor, W_h)) \\\\]\\n\\\\[ h_k = (1 - z_k) \\\\odot h_{k-1} + z_k \\\\odot \\\\tilde{h}_k \\\\] (3)\\n\\nwhere \\\\( x_k \\\\) is the concatenation of disparity, correlation, hidden information, and context information previously defined. Unlike RAFT-Stereo [17] that divide the context information into \\\\( c_z, c_r, c_h \\\\), we add it into \\\\( x_k \\\\) because using convolutions with different kernel sizes can fully utilize context information.\\n\\nAs illustrated in Fig. 3, a single SRU can be defined as follows:\\n\\\\[ h_k = A \\\\odot h_s + (1 - A) \\\\odot h_l \\\\] (4)\\n\\nwhere \\\\( A \\\\) denotes the attention map derived from CSA at the same resolution, \\\\( h_s \\\\) denotes the GRU with smaller kernel sizes and \\\\( h_l \\\\) denotes the larger one.\\n\\nAs Sec. 3.3 mentioned, the attention map has high weights in regions needing high-frequency information. Therefore, the GRU with smaller kernel sizes that can capture high-frequency information like edge, and thin objects should do element-wise products with the attention map directly, and the GRU with larger kernel sizes should do element-wise products with the contrary attention map.\\n\\nReceptive fields analysis.\\n\\nThe respective fields computing formula [1] can be defined as follows:\\n\\\\[ r_0 = L \\\\times l = 1 (((k_l - 1) \\\\times s_i) + 1) \\\\] (5)\\n\\nwhere \\\\( k_l \\\\) denotes the kernel size, \\\\( s_i \\\\) denotes the stride size, and \\\\( r_0 \\\\) denotes the whole network.\\n\\nGiven a multi-level structure like Fig. 4, if we take the \\\\( 1/4 \\\\) resolution as the basis, and the downsampling operations can be regarded as a convolution with kernel size 3, stride size 2, this structure's receptive fields are \\\\( k_2k + 3, 3k + 6 \\\\). That means it only has 3 fixed receptive fields in total.\"}"}
{"id": "CVPR-2024-2169", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"1. Channel Attention Enhancement\\n\\n2. Spatial Attention Extractor\\n\\n3. MaxPool\\n\\n4. AvgPool\\n\\n5. Conv\\n\\n6. Conv\\n\\n7. Attention Map\\n\\n8. Element-wise Sum\\n\\n9. Element-wise Product\\n\\n10. Sigmoid\\n\\n---\\n\\n1. Context Information\\n\\n2. Hidden Information\\n\\n3. Small GRU\\n\\n4. Large GRU\\n\\n5. Hidden Information\\n\\n6. Attention Map\\n\\n7. Contrary Attention Map\\n\\n8. Max/AvgPool\\n\\n9. Fused Hidden Information\\n\\n---\\n\\n3.5. Loss Function\\n\\nWe supervise our network on the L1 distance between all predicted disparities $d_i$ and the ground truth disparity $d_{gt}$ with increasing weights. The total loss is defined as:\\n\\n$$L = \\\\sum_{i=1}^{N} \\\\gamma N - i ||d_i - d_{gt}||_1$$\\n\\nwhere $\\\\gamma = 0.9$, and $N$ is the number of iterations.\\n\\n---\\n\\n4. Experiments\\n\\nScene Flow [20] is a synthetic dataset including 35,454 training pairs and 4,370 testing pairs with dense disparity maps. For training and testing, we use the finalpass version, because it contains more realistic and difficult effects than the cleanpass version.\\n\\nKITTI 2012 [13] and KITTI 2015 [21] are datasets for real-world driving scenes. KITTI 2012 contains 194 training pairs and 195 testing pairs, and KITTI 2015 contains 200 training pairs and 200 testing pairs.\\n\\nETH3D [24] is a collection of gray-scale stereo pairs containing 27 training pairs and 20 testing pairs for indoor and outdoor scenes.\\n\\nMiddlebury [23] is a high-resolution dataset containing 15 training pairs and 15 testing pairs for indoor scenes.\\n\\n4.1. Implementation Details\\n\\nWe implement our Selective-Stereo with PyTorch and the model is trained on NVIDIA RTX 3090 GPUs. For all experiments, we use the AdamW [19] optimizer and clip gradients to the range $[-1, 1]$. We use the one-cycle learning rate schedule with a learning rate of $2e^{-4}$. We first train our model on Scene Flow with a batch size of 8 for 200k steps as the pretrained model. The crop size is $320 \\\\times 720$, and we use 22 update iterations during training.\\n\\n4.2. Ablation Study\\n\\nIn this section, we evaluate our model in different settings to verify our proposed modules in several aspects. All results use 32 update iterations.\\n\\nEffectiveness of proposed modules. To verify the effectiveness of our proposed modules, we take RAFT-Stereo [17] as the baseline and replace its GRUs with our SRUs. As shown in Tab. 1, the proposed SRU can improve the accuracy even without CSA. It means that if we just sum up the information from different branches, the growth of receptive fields can be beneficial for inference. If we add our CSA but invert the weights of the attention maps, the effect even decreases. That validates that our CSA's attention maps do indeed reflect the weights of information at different frequencies in regions. Therefore, if we add CSA normally, the full model (Selective-RAFT) can achieve the best performance with only a 4% increase in parameters.\\n\\nUniversality of proposed modules. To verify the universality of our proposed modules, we take three typical iterative stereo matching methods as the baseline and replace their GRUs with SRUs. Especially, in DLNR [44], the recurrent units are LSTMs but not GRUs, so we just replace GRUs inside SRUs with LSTMs to make a fair comparison.\"}"}
{"id": "CVPR-2024-2169", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Ablation study of the effectiveness of proposed modules on the Scene Flow test set. SRU denotes Selective Recurrent Unit, and CSA denotes Contextual Spatial Attention. Contrary means we invert the weights of the attention maps. The baseline is RAFT-Stereo.\\n\\n| Model                     | EPE (px) | (%) | Param (M) |\\n|---------------------------|----------|-----|-----------|\\n| Baseline (RAFT-Stereo)    | 0.53     | 6.08| 11.12     |\\n| SRU                       | 0.50     | 5.38| 11.65     |\\n| SRU+CSA (Contrary)        | 0.50     | 5.58| 11.65     |\\n| Full model (Selective-RAFT)| 0.47   | 5.32| 11.65     |\\n\\nTable 2. Ablation study of the universality of proposed modules.\\n\\n| Model                     | Number of Iterations |\\n|---------------------------|----------------------|\\n| RAFT-Stereo [17]          | 2.08 1.13 0.87 0.75 0.58 0.53 | 1 2 3 4 8 32 |\\n| Selective-RAFT            | 1.95 1.06 0.81 0.69 0.53 0.47 |\\n| IGEV-Stereo [34]          | 0.66 0.62 0.58 0.55 0.50 0.47 |\\n| Selective-IGEV            | 0.65 0.60 0.56 0.53 0.48 0.44 |\\n\\nTable 3. Ablation study of the number of iterations.\\n\\n| Kernel Sizes | EPE (px) | (%) |\\n|--------------|----------|-----|\\n| 1\u00d71 + 1\u00d75    | 0.48     | 5.41|\\n| 3\u00d73 + 1\u00d75    | 0.48     | 5.30|\\n| 1\u00d71 + 3\u00d73    | 0.47     | 5.32|\\n\\nTable 4. Ablation study of the size of convolutional kernels.\\n\\nAs shown in Tab. 2, all methods have a significant improvement in the EPE metrics on Scene Flow, and the insertion of modules only results in a slight increase in parameters. Besides, as shown in Fig. 6, the CSA module generates different attention maps in different networks. In Selective-RAFT, because the cost volume contains a large amount of noisy information, the network needs more large kernels to filter the local cost volume. On the contrary, the cost volume has already been aggregated in Selective-IGEV, so the network tends to maintain high-frequency information using small kernels. Moreover, the cost volume in Selective-IGEV faces an over-smooth problem [34], and that's why the attention map tends to increase the weights of large kernels to recover edge regions. In general, the CSA module shows different tendencies in different networks, which is a reflection of its adaptive ability.\\n\\nAs shown in Tab. 3, our Selective-RAFT get the same performance with only 8 iterations compared to RAFT-Stereo [17], and for IGEV-Stereo [34], our Selective-IGEV also get a slight improvement with a few iterations. It shows that our modules can make secondary filtering to reduce noisy information from the initial cost volume.\\n\\nSize of convolution kernels. We verify different kernel sizes on Scene Flow as shown in Tab. 4. At last, we choose the combination of 1\u00d71 and 3\u00d73 as our default configuration.\"}"}
{"id": "CVPR-2024-2169", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Quantitative evaluation on Scene Flow test set.\\n\\n| Method                    | EPE (px) |\\n|---------------------------|----------|\\n| CSPN                      | 0.78     |\\n| LEASTEREO                 | 0.78     |\\n| ACVNet                    | 0.72     |\\n| IGEV-Stereo               | 0.48     |\\n| Selective-RAFT (Ours)     | 0.47     |\\n| Selective-IGEV (Ours)     | 0.44     |\\n\\nTable 6. Quantitative evaluation on KITTI 2012 and KITTI 2015.\\n\\n| Method                    | EPE-noc | EPE-all |\\n|---------------------------|---------|---------|\\n| AcfNet [42]               | 1.83    | 2.35    |\\n| LEASTEREO [9]             | 1.90    | 2.39    |\\n| ACVNet [32]               | 1.83    | 2.35    |\\n| RAFT-Stereo [17]          | 1.92    | 2.42    |\\n| PCWNet [25]               | 1.69    | 2.18    |\\n| LaC + GANet [18]          | 1.72    | 2.26    |\\n| CREStereo [16]            | 1.72    | 2.18    |\\n| IGEV-Stereo [34]          | 1.71    | 2.17    |\\n| Selective-RAFT (Ours)     | 1.64    | 2.09    |\\n| Selective-IGEV (Ours)     | 1.59    | 2.05    |\\n\\nTable 7. Quantitative evaluation on Scene Flow test set in different regions.\\n\\n| Method                    | EPE >1px | EPE >1px |\\n|---------------------------|----------|----------|\\n| RAFT-Stereo [17]          | 3.21     | 29.16    |\\n| Selective-RAFT            | 2.40     | 21.63    |\\n| IGEV-Stereo [34]          | 2.23     | 20.42    |\\n| Selective-IGEV            | 2.18     | 20.01    |\\n\\n4.3. Comparisons with State-of-the-art\\n\\nAll fine-tuned models use the model pretrained on Scene Flow. Different target datasets use different finetune strategies. We validate two models called Selective-RAFT and Selective-IGEV using RAFT-Stereo [17] and IGEV-Stereo [34] as the baseline respectively.\\n\\nScene Flow.\\n\\nAs shown in Tab. 5, we achieve a new state-of-the-art EPE of 0.44 on Scene Flow with Selective-IGEV, which surpasses LaC + GANet [18] by 38.89%. Besides, our Selective-RAFT also achieves a competitive EPE of 0.47 compared to IGEV-Stereo [34] with smaller parameters. To validate the ability to fuse information by regions of our modules, we then split Scene Flow test set into two regions: edge regions and non-edge regions using the Canny operator. As shown in Tab. 7, our Selective-RAFT outperforms RAFT-Stereo [17] by 25.23% and 24.53% in edge regions and non-edge regions. Due to IGEV-Stereo's aggregated cost volume [34], there's only a slight improvement in edge regions, but our Selective-IGEV still outperforms it by 7.32% in non-edge regions.\\n\\nKITTI.\\n\\nWe finetune our model on the mixed dataset of KITTI 2012 and KITTI 2015 with a batch size of 8 for 50k steps. Then we evaluate our Selective-Stereo on the test set of KITTI 2012 and KITTI 2015. As shown in Tab. 6, we achieve the best performance among all published methods for almost all metrics. On KITTI 2012, our Selective-RAFT outperforms RAFT-Stereo [17] by 14.58% and 13.64% on 2-noc and 2-all metrics, and our Selective-IGEV ranks 1st on these metrics. On KITTI 2015, our Selective-RAFT outperforms RAFT-Stereo [17] by 10.44% on the D1-all metric, and our Selective-IGEV ranks 1st on all metrics with only 16 iterations same as IGEV-Stereo [34]. As shown in Fig. 5, our Selective-IGEV outperforms IGEV-Stereo [34] in detailed and textureless regions.\\n\\nETH3D.\\n\\nFollowing CREStereo [16] and GMStereo [38], we use a collection of several public stereo datasets for training. The crop size is $384 \\\\times 512$ and we first finetune the Scene Flow pretrained model on the mixed Tartan Air [29], CREStereo Dataset [16], Scene Flow [20], Sintel Stereo [3], InStereo2k [2] and ETH3D [24] datasets for 300k steps. Then we finetune it on the mixed CREStereo Dataset [16], InStereo2k [2] and ETH3D [24] datasets with for another 90k steps. As shown in Tab. 8, our Selective-RAFT outperforms RAFT-Stereo [17] by 17.90% on Bad 0.5 metric, and our Selective-IGEV achieves the best performance among all published methods for almost all metrics.\\n\\nMiddlebury.\\n\\nFollowing CREStereo [16] and GMStereo [38], we first finetune the Scene Flow pretrained model on the mixed Tartan Air [29], CREStereo Dataset [16], Scene Flow [20], Falling Things [28], InStereo2k [2], CARLA HR-VS [40] and Middlebury [23] datasets using a crop size of $384 \\\\times 512$ for 200k steps. Then we finetune it on the mixed CREStereo Dataset [16], Falling Things [28], InStereo2k [2], CARLA HR-VS [40] and Middlebury [23] datasets using a crop size of $384 \\\\times 768$ with a batch size of 8 for another 100k steps. As shown in Tab. 8, our Selective-IGEV achieves the best performance among all published methods for almost all metrics.\"}"}
{"id": "CVPR-2024-2169", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 8. Quantitative evaluation on ETH3D and Middlebury benchmarks. Note: Middlebury only allows one publication per paper, so we only publish our Selective-IGEV.\\n\\n| Method              | Bad 1.0 | Bad 0.5 | Bad 4.0 | AvgErr |\\n|---------------------|---------|---------|---------|--------|\\n| ETH3D               |         |         |         |        |\\n| Middlebury          |         |         |         |        |\\n| CroCo-Stereo [30]   | 0.99    | 3.27    | 0.13    | 0.14   |\\n| GMStereo [38]       | 1.83    | 5.94    | 0.08    | 0.19   |\\n| HITNet [26]         | 2.79    | 7.83    | 0.19    | 0.20   |\\n| IGEV-Stereo [34]    | 1.12    | 3.52    | 0.11    | 0.14   |\\n| RAFT-Stereo [17]    | 2.44    | 7.04    | 0.15    | 0.18   |\\n| CREStereo [16]      | 0.98    | 3.58    | 0.10    | 0.13   |\\n| EAI-Stereo [43]     | 2.31    | 5.21    | 0.70    | 0.21   |\\n| DLNR                | -       | -       | -       | 3.20   |\\n| Selective-RAFT (Ours)| -     | -       | -       | -      |\\n| Selective-IGEV (Ours)| 1.69  | 5.78    | 0.13    | 0.17   |\\n\\n**Figure 7. Qualitative results on the test set of Middlebury. The third column is the visualization of attention maps generated by the CSA module. Our Selective-IGEV outperforms IGEV in large textureless and thin object regions.**\\n\\n5. **Conclusion**\\n\\nWe propose Selective-Stereo, a novel iterative stereo matching method. The proposed Contextual Spatial Attention module and Selective Recurrent Unit help the network capture information at different frequencies for edge and smooth regions. Our Selective-Stereo ranks first on KITTI, ETH3D, and Middlebury in almost all metrics among all published methods. It shows an ability to fuse information adaptively for edge and smooth regions with the help of attention maps extracted by CSA.\\n\\nHowever, our method still faces some challenges. Firstly, although our method can fuse information adaptively using attention maps, the SRU's receptive field is still limited by predefined values. Secondly, adding branches or increasing the sizes of convolutional kernels leads to high memory and time costs, so we will explore the combination of lightweight convolutions and our method to reduce memory costs. Lastly, it's also a good direction to do research on the combination of convolutions and self-attention due to their different advantages and receptive fields.\\n\\n---\\n\\n**Acknowledgement.** This research is supported by National Natural Science Foundation of China (62122029, 62061160490, U20B200007).\"}"}
{"id": "CVPR-2024-2169", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Andr\u00e9 Araujo, Wade Norris, and Jack Sim. Computing receptive fields of convolutional neural networks. Distill, 4(11):e21, 2019.\\n\\n[2] Wei Bao, Wei Wang, Yuhua Xu, Yulan Guo, Siyu Hong, and Xiaohu Zhang. Instereo2k: a large real dataset for stereo matching in indoor scenes. Science China Information Sciences, 63:1\u201311, 2020.\\n\\n[3] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J Black. A naturalistic open source movie for optical flow evaluation. In Computer Vision\u2013ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI, pages 611\u2013625. Springer, 2012.\\n\\n[4] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5410\u20135418, 2018.\\n\\n[5] Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yan-Ni Kalantidis, Marcus Rohrbach, Shuicheng Yan, and Jiashi Feng. Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3435\u20133444, 2019.\\n\\n[6] Yajie Chen, Xin Yang, and Xiang Bai. Confidence-weighted mutual supervision on dual networks for unsupervised cross-modality image segmentation. Science China Information Sciences, 66(11):210104, 2023.\\n\\n[7] Junda Cheng, Xin Yang, Yuechuan Pu, and Peng Guo. Region separable stereo matching. IEEE Transactions on Multimedia, 2022.\\n\\n[8] Junda Cheng, Gangwei Xu, Peng Guo, and Xin Yang. Coatrsnet: Fully exploiting convolution and attention for stereo matching by region separation. International Journal of Computer Vision, pages 1\u201318, 2023.\\n\\n[9] Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao Dai, Xiaojun Chang, Hongdong Li, Tom Drummond, and Zongyuan Ge. Hierarchical neural architecture search for deep stereo matching. Advances in Neural Information Processing Systems, 33:22158\u201322169, 2020.\\n\\n[10] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\\n\\n[11] Miaojie Feng, Junda Cheng, Hao Jia, Longliang Liu, Gangwei Xu, and Xin Yang. Mc-stereo: Multi-peak lookup and cascade search range for stereo matching. arXiv preprint arXiv:2311.02340, 2023.\\n\\n[12] Manuel Fritsche, Shuhang Gu, and Radu Timofte. Frequency separation for real-world super-resolution. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 3599\u20133608. IEEE, 2019.\\n\\n[13] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pages 3354\u20133361. IEEE, 2012.\\n\\n[14] Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, and Hongsheng Li. Group-wise correlation stereo network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3273\u20133282, 2019.\\n\\n[15] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry. End-to-end learning of geometry and context for deep stereo regression. In Proceedings of the IEEE international conference on computer vision, pages 66\u201375, 2017.\\n\\n[16] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng Liu. Practical stereo matching via cascaded recurrent network with adaptive correlation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16263\u201316272, 2022.\\n\\n[17] Lahav Lipson, Zachary Teed, and Jia Deng. Raft-stereo: Multilevel recurrent field transforms for stereo matching. In 2021 International Conference on 3D Vision (3DV), pages 218\u2013227. IEEE, 2021.\\n\\n[18] Biyang Liu, Huimin Yu, and Yangqi Long. Local similarity pattern and cost self-reassembling for deep stereo matching networks. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1647\u20131655, 2022.\\n\\n[19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\\n\\n[20] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4040\u20134048, 2016.\\n\\n[21] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3061\u20133070, 2015.\\n\\n[22] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision transformers with hilo attention. Advances in Neural Information Processing Systems, 35:14541\u201314554, 2022.\\n\\n[23] Daniel Scharstein, Heiko Hirschm\u00fcller, York Kitajima, Greg Krathwohl, Nera Ne\u0161i\u0107, Xi Wang, and Porter Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In Pattern Recognition: 36th German Conference, GCPR 2014, M\u00fcnster, Germany, September 2-5, 2014, Proceedings, 36, pages 31\u201342. Springer, 2014.\\n\\n[24] Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and multi-camera videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3260\u20133269, 2017.\\n\\n[25] Zhelun Shen, Yuchao Dai, Xibin Song, Zhibo Rao, Dingfu Zhou, and Liangjun Zhang. Pcw-net: Pyramid combination and warping cost volume for stereo matching. In European Conference on Computer Vision, pages 280\u2013297. Springer, 2022.\\n\\n[26] Vladimir Tankovich, Christian Hane, Yinda Zhang, Adarsh Kowdle, Sean Fanello, and Sofien Bouaziz. Hitnet: Hierarchical iterative tile refinement network for real-time stereo.\"}"}
{"id": "CVPR-2024-2169", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[27] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II, pages 402\u2013419. Springer, 2020.\\n\\n[28] Jonathan Tremblay, Thang To, and Stan Birchfield. Falling things: A synthetic dataset for 3d object detection and pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 2038\u20132041, 2018.\\n\\n[29] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: A dataset to push the limits of visual slam. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4909\u20134916. IEEE, 2020.\\n\\n[30] Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Br\u00e9gier, Gabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, and J\u00e9r\u00f4me Revaud. Croco v2: Improved cross-view completion pretraining for stereo matching and optical flow. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17969\u201317980, 2023.\\n\\n[31] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. CBAM: Convolutional block attention module. In Proceedings of the European conference on computer vision (ECCV), pages 3\u201319, 2018.\\n\\n[32] Gangwei Xu, Junda Cheng, Peng Guo, and Xin Yang. Attention concatenation volume for accurate and efficient stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12981\u201312990, 2022.\\n\\n[33] Gangwei Xu, Shujun Chen, Hao Jia, Miaojie Feng, and Xin Yang. Memory-efficient optical flow via radius-distribution orthogonal cost volume. arXiv preprint arXiv:2312.03790, 2023.\\n\\n[34] Gangwei Xu, Xianqi Wang, Xiaohuan Ding, and Xin Yang. Iterative geometry encoding volume for stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21919\u201321928, 2023.\\n\\n[35] Gangwei Xu, Yun Wang, Junda Cheng, Jinhui Tang, and Xin Yang. Accurate and efficient stereo matching via attention concatenation volume. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\\n\\n[36] Gangwei Xu, Huan Zhou, and Xin Yang. CGI-stereo: Accurate and real-time stereo matching via context and geometry interaction. arXiv preprint arXiv:2301.02789, 2023.\\n\\n[37] Haofei Xu and Juyong Zhang. AANet: Adaptive aggregation network for efficient stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1959\u20131968, 2020.\\n\\n[38] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\\n\\n[39] Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang Chen, and Fengbo Ren. Learning in the frequency domain. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1740\u20131749, 2020.\\n\\n[40] Gengshan Yang, Joshua Manela, Michael Happold, and Deva Ramanan. Hierarchical deep stereo matching on high-resolution images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5515\u20135524, 2019.\\n\\n[41] Feihu Zhang, Victor Prisacariu, Ruigang Yang, and Philip HS Torr. GaNet: Guided aggregation net for end-to-end stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 185\u2013194, 2019.\\n\\n[42] Youmin Zhang, Yimin Chen, Xiao Bai, Suihanjin Yu, Kun Yu, Zhiwei Li, and Kuiyuan Yang. Adaptive unimodal cost volume filtering for deep stereo matching. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 12926\u201312934, 2020.\\n\\n[43] Haoliang Zhao, Huizhou Zhou, Yongjun Zhang, Yong Zhao, Yitong Yang, and Ting Ouyang. EAI-stereo: Error aware iterative network for stereo matching. In Proceedings of the Asian Conference on Computer Vision, pages 315\u2013332, 2022.\\n\\n[44] Haoliang Zhao, Huizhou Zhou, Yongjun Zhang, Jie Chen, Yitong Yang, and Yong Zhao. High-frequency stereo matching network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1327\u20131336, 2023.\"}"}
