{"id": "CVPR-2022-1976", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Quality Control\\n\\nWe adopt a set of measurements for quality control during the calibration process. Before submitting the task, the survey will first check the modifying parts by the worker to ensure that the modifying parts meet the base requirement: a dependency in DT is aligned to a RELATIONSHIP node in SG. If we find this kind of misalignment during annotation, we will prompt a message asking the workers to recheck their annotation. We publish datasets to workers one by one and request at least two workers to process the same sample to check whether there are disagreements. To ensure high-quality labeling, we restrict participated workers who have finished 500 human intelligence tasks (HITs) with high accuracy in the labeling history.\\n\\nWe do the post-processing double-check after the human refinement. We collect the flag disagreements for multiple decisions from several workers. All samples that have disagreement are double-checked manually by a third-party worker. We also flag annotations from workers whose work seems inadequate and filter out their results from the final collections.\\n\\n3.4. Dataset Analysis\\n\\nFor the training dataset, we inherit MSCOCO training dataset \\\\[28\\\\]. We annotate VL structures based on the intersection of MSCOCO dev + test datasets and Visual Genome \\\\[26\\\\]. We collect an annotated dataset with 850 images and 4,250 captions (each image is associated with 5 captions). Then we split the 850 images into dev and test datasets by 1:1. The remains in dev + test are merged into the training dataset. Table 1 shows the data summary.\\n\\n|        | Train | Dev | Test |\\n|--------|-------|-----|------|\\n| # Images | 83933 | 425 | 425  |\\n| # Sentences | 419665 | 2125 | 2125 |\\n| # Avg. Instances in DT | 20 | 21 |\\n| # Avg. Instances in SG | 135 | 134 |\\n\\nTable 1. Data analysis of VLParse. # Avg.: The average number. Instances include zero-order instances, first-order relationship, and second-order relationship.\\n\\n3.5. Human Performance\\n\\nFive different workers are asked to label parse trees of 100 sentences from the test set. A different set of five workers on AMT were asked to align the visual terms and the language terms on the same sentences and their corresponding images. Then the averaged human performance is calculated as 96.15%.\\n\\nBased on these observations, our designed dataset presents language representation and cross-modality understanding clearly and keeps vision-language alignment concrete. It demonstrates the reliability of our new dataset and benchmark through manual review.\\n\\n4. Unsupervised Vision-Language Parsing\\n\\nIn this section, we introduce the task of unsupervised vision-language (VL) parsing, short for VLParse. We formalize the task of VL parsing followed by evaluation metrics.\\n\\n4.1. Task Formulation\\n\\nGiven an input image $I$ and the associating sentence with a sequence of $N$ words $w_1, w_2, ..., w_N$, the task is to predict the joint parse tree $pt$ in a unsupervised manner. Specifically, the goal is to induce VL structures from only image-caption pairs without annotations of DT, SG nor phrase-region correspondence annotations for training. Of note, we do use a pre-trained object detector to obtain 50 bounding boxes as candidates, while the labels of the bounding boxes are not given. For a fully unsupervised setting, the process of obtaining the bounding boxes can be replaced by an object proposal method (e.g., [35]). Compared with the weakly-supervised scene graph grounding task as in [32], the scene graphs in VLParse are unknown.\\n\\nEach OBJECT node in language DT will be mapped to a box region $o_i P R$ given $M$ candidate object proposals $O$ of the corresponding image. So are the relationships.\\n\\n4.2. Evaluation Metrics\\n\\nDue to lacking annotations of VL structure, we indirectly assess our model by two derived tasks from each modality's perspective, i.e., language dependency parsing and phrase grounding.\\n\\nDirected / Undirected Dependency Accuracy (DDA/UDA)\\n\\nDDA and UDA are two widely used evaluation metrics of dependency parsing. DDA denotes the proportion of tokens assigned with the correct parent node. UDA denotes the proportion of correctly predicted undirected dependency relation.\\n\\nZero-Order Alignment Accuracy (Zero-AA)\\n\\nZero-AA assesses the alignment results on the zero-order level. A word is considered successfully grounded if two conditions are satisfied. First, the predicted bounding box of a language vertex has at least 0.5 IoU (Intersection over Union) with the box of ground-truth SG vertex if the ground-truth is a OBJECT node or ATTRIBUTE node, or the connected two boxes both have at least 0.5 IoU scores if the ground-truth is a RELATIONSHIP node. Second, although OBJECT node and ATTRIBUTE node share the same region, we ask models to distinguish them.\\n\\nFirst/Second-Order Alignment Accuracy (First/Second-AA)\\n\\nWe are also interested in whether the first- and second-order alignment.\"}"}
{"id": "CVPR-2022-1976", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"second-order relationships remain after alignment to an-\\nother modality. That is, whether two zero-order instances\\n(subject and predicate) in the first-order relationship remain\\nadjacent in the aligned SG. For the second-order relation-\\nship, we consider whether three zero-order instances (a sub-\\nject, a predicate, and an object) remain adjacent. For the\\nsecond-order relationships, there are multiple approach to\\nconnect the three words obj-pred-sub (e.g., obj \u00d1 pred \u00d1 sub).\\nWe consider them all correct because distinguishing their adjacency to an unsupervised parser is\\nmore important to identify the semantics.\\n\\n5. Vision-Language Graph Auto-Encoder\\n\\nIn this section, we introduce a novel CL based archite-\\ncture, VLGAE, to benchmark the VLParse task. The ar-\\nchitecture is composed of feature extraction, structure con-\\nstruction and cross-modality matching modules; Figure 3\\ndepicts the overall computational framework. Below we\\nwill discuss details of each module and then the learning\\nand inference algorithms.\\n\\n5.1. Modeling\\n\\nFeature Extraction\\nFor visual features, we start by\\nusing an off-the-shelf object detector Faster R-CNN [30]\\nto generate a set of object proposals (RoIs) O to iuM i\\non an input image I and extracting corresponding features\\ntvuoMi. For each OBJECT node v,\\nan ATTRIBUTE node is tagged along, with its feature de-\\nnoted as mv q. For two arbitrary OBJECT nodes\\nvoi and voj, we denote the zero-order\\nRELATIONSHIP node as img i \u00d1 j, 0. We also add an dummy node representing the full\\nimage and take the average of all\\nOBJECT node features as its feature. For all nodes except for\\nOBJECT nodes, we use\\nrandomly initialized neural networks to represent the fea-\\ntures.\\n\\nFor textual features, each word wi in sentence w is repre-\\nsented as the concatenation of a pretrained word embedding\\nwi and a randomly initialized POS tag embedding ti. Sim-\\nilar to RELATIONSHIP nodes in SGs, the representation of\\ndependency between two words, wi \u00d1 j is extracted by neu-\\nral networks fed with pw i, w j q. We use Biaffine scorers [8]\\nfor the first-order relationship:\\n\\n\\\\[ w_{1st, parent}^i, w_{1st, child}^i ] \\\\Rightarrow MLP_1 \\\\{ child p w_i q \\\\}\\n\\\\[ w_{1st}^i \u00d1 j ] \\\\Rightarrow Biaffine_1 w_{1st, parent}^i, w_{1st, child}^i q\\n\\\\[ Biaffine_2 w_{1st}, w_{j} q ] \\\\Rightarrow T_i W_1 w_j ` p w_i ` w_j q T_2 ` b,\\n\\nwhere MLP denotes the multi-layer perceptron,\\nW_1, W_2 and b are trainable parameters. The calculation of a second-\\norder relationship's score follows a similar way.\\n\\nStructure Construction\\nInspired by neural DT con-\\nbruction algorithms [15], we use an encoder-decoder\\nframework that employs the dynamic programming algo-\\nrithm (namely, inside algorithm) and calculate the posteri-\\nors of instances pppt|wi, I q recursively retrieved during the\\nstructure construction.\\n\\nEncoder\\nThe encoder is to produce a joint representa-\\ntion of an input image I and its corresponding caption\\nw. Specifically, we obtain contextual encoding\\nc_PC by fusing\\nthe text features with the visual information via attention\\nmechanisms, where C denotes the space for the attended\\nlanguage context. For each token in captions twi and\\nSG representations v_iq, v_i \u00d1 j u, we calculate attention\\nscores between them and then obtain weighted summation\\nover all terms, i.e.,\\nc_i = \\\\sum Attn_{w_i, v_i q}w_i. Finally, we use\\nan average-pooling layer to summarize all information into\\na continuous context vector s, which represents the global\\ninformation of the vision-language context.\\n\\nDecoder\\nThe decoder generates the tag sequence t and\\nparse tree pt conditioned on the joint representation\\ns w.r.t. the joint probability pppt|s q. To consider the exponential\\nscale of possible parse trees, we use dynamic programming\\nto consider all possible dependencies over the sentence. Re-\\nfer to Section 5.2 for the learning process.\\n\\nCross-modality Matching\\nWe employ cross-modality\\nmatching to align vision and language features in different\\nlevels.\\n\\nMatching Score\\nWe define sim_p\u00a8q as the cross-modality\\nmatching function. Following Wang et al. [38], we first\\ncompute the similarity score between each\\nc_PC and each\\nv_PV:\\n\\nsim_p v, c q = x_v, c\\n(1)\\n\\nwhere x\u00a8y is an inter-product function. Heuristically, we\\ncan define the similarity score between instance\\nc and the entire image I as\\nsim_p I, c q = \\\\max v_PV sim_p v, c q,\\n(2)\\n\\nMatching Score Enhanced by Posterior\\nTo leverage the\\ncontextual information, we use a posterior pp_c|s q computed\\nfrom the decoder to reflect how likely\\nc exists given the joint\\nrepresentation s. Then we fuse the matching scores with\\nthe posteriors to provide an enhanced simliarity function,\\n\\nsim`_p I, c q = sim_p I, c q \\\\hat{p}_p c|s q.\\n\\n5.2. Learning\\n\\nMaximum Likelihood Estimation (MLE)\\nWith the\\ncompressed representation \\\\( s_i \\\\) for image-sentence pair\\n\\\\( p_I i, w_i q \\\\), VLGAE generates the tag sequence\\nti and the\\nparse tree pt. The learning objective is to maximize the\"}"}
{"id": "CVPR-2022-1976", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Diagram of VLGAE. It first extracts features from both modalities and builds representations for all instances in DT and SG. Then the encoder of the structure construction module encodes the language feature with visual cues and output a compressed representation $s$. With this compressed global representation $s$, the decoder incorporates the inside algorithm to construct the VL structure recursively as well as compute the posteriors. On top of the resulting posteriors generated from the structure construction module, an enhanced matching score between language context $c$ and image region $v$ is used to promote the cross-modality fine-grained correspondence.\\n\\n\\\\[ \\\\ell_{\\\\text{mle}} = \\\\frac{1}{K} \\\\sum_{i=1}^{K} \\\\log p_{\\\\Theta}^{\\\\text{pt}} | w_{i} \\\\quad \\\\text{q} \\\\]\\n\\n\\\\[ \\\\ell_{\\\\text{cl}} = \\\\frac{1}{P_{\\\\text{PT}}} \\\\sum_{i=1}^{P_{\\\\text{PT}}} \\\\ell \\\\quad \\\\text{p} | \\\\quad \\\\text{I} \\\\quad \\\\text{c} \\\\quad \\\\text{q} \\\\]\\n\\n\\\\[ \\\\text{L}_{\\\\text{tot}} = \\\\lambda \\\\ell_{\\\\text{mle}} + (1-\\\\lambda) \\\\ell_{\\\\text{cl}} \\\\]\\n\\nwhere $\\\\Theta$ parameterizes the encoder-decoder neural network and $P_{\\\\text{PT}}$ denotes the set of all possible parse trees. Given some $\\\\Theta$, Eqn. (3) can be computed using the inside algorithm, an $O(n^3)$ dynamic programming procedure. Therefore, we perform structure construction and parameter learning via an expectation-maximization (EM) process. Specifically, the E-step is to compute possible structures given current $\\\\Theta$ and the M-step is to optimize $\\\\Theta$ by gradient descent w.r.t. Eqn. (3). Of note, the posterior $p_{\\\\text{p}} | s \\\\quad \\\\text{q}$ used for matching score can be computed in the back-propagation process [11].\\n\\nContrastive Loss\\n\\nDue to the lack of fine-grained annotations in an unsupervised setting, the objective referring to the alignment is employed in a contrastive loss. The contrastive learning strategy is based on maximizing the matching score between paired fine-grained instances. For each $c$, the sentence's corresponding image is a positive example and all the other images in the current batch are negative examples. Of note, compared with coarse image-sentence pairs, our design of fine-grained vision-language alignments yield stronger negative pairs for contrastive training. Formally, given a vision-language pair $w_{i}, I_{q}$ within a batch, the contrastive loss can be defined as,\\n\\n\\\\[ \\\\ell_{\\\\text{p} | \\\\text{I} \\\\quad \\\\text{c} \\\\quad \\\\text{q}} \\\\quad \\\\text{P} \\\\quad \\\\text{pt} | \\\\quad \\\\text{w} \\\\quad \\\\text{q} \\\\]\\n\\nFinally, the total loss is defined as \\\\[ \\\\text{L}_{\\\\text{tot}} = \\\\lambda \\\\ell_{\\\\text{mle}} + (1-\\\\lambda) \\\\ell_{\\\\text{cl}} \\\\]\\n\\nwhere $\\\\lambda$ is pre-defined to balance different scalars between two losses.\\n\\n5.3. Inference\\n\\nGiven a trained model with trained parameters $\\\\Theta$, the model can predict the VL structure and further the parse tree of the sentence and its visual grounding on the SG. The parse tree can be parsed by searching for $\\\\text{pt} | \\\\quad \\\\text{s} \\\\quad \\\\text{P} \\\\quad \\\\text{PT} \\\\quad \\\\text{p} \\\\quad \\\\text{s} \\\\quad \\\\text{q}$ using the dynamic programming [25]:\\n\\n\\\\[ \\\\text{pt} | \\\\quad \\\\text{s} \\\\quad \\\\text{P} \\\\quad \\\\text{PT} \\\\quad \\\\text{p} \\\\quad \\\\text{s} \\\\quad \\\\text{q} \\\\]\\n\\nFor each $c \\\\quad \\\\text{P} \\\\quad \\\\text{C}$, we can predict its corresponding image region $\\\\text{o} \\\\quad \\\\text{m} \\\\quad \\\\text{c}$ using enhanced similarity score as in Eqn. (1):\"}"}
{"id": "CVPR-2022-1976", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Dependency structure induction results on the test split.\\n\\n| Method         | D-NDMV | VLGAE |\\n|----------------|--------|-------|\\n| Dependencies   | 58.06  | 71.43 |\\n| Random         | 32.44  | 53.61 |\\n| Vision-Language| 30.75  | 53.19 |\\n| Language Only  | 23.01  | 23.01 |\\n\\nIt is worth noting that, when the ground truth dependency tree is known for sentence, we can directly retrieve corresponding scene graph w.r.t. Eqn. (8).\\n\\n6. Experiments\\n\\n6.1. Setup\\n\\nThe candidate bounding boxes are given in the following setting. For an input image, we use an external object detector, Faster R-CNN as MAF [38], to generate top-50 object proposals. For each proposal, we use RoI-Align [16] and global average pooling to compute the object feature [38]. Since we do not have the ground-truth structure of the captions, we follow [31] and [49] to use predictions as ground truth produced by an external parser. We report the average score of three runs with different random seeds.\\n\\n6.2. Evaluation on Language Structure Induction\\n\\nWe compare VLGAE with prior language-only baselines on language structure induction using UDA and DDA metrics in Table 2. We can observe a performance boosting after incorporating visual cues. In particular, VLGAE outperforms D-NDMV by 1.69% score on DDA and 0.66% score on UDA.\\n\\n6.3. Evaluation on Visual Phrase Grounding\\n\\nIn addition to language structure induction, we evaluate our approach on the weakly-supervised visual phrase grounding task.\\n\\nExperimental results in Table 3 show that VLGAE outperforms the previous multimodal baseline MAF [38] by 1.0%. Moreover, a significant improvement is observed, especially for high-order relations, indicating the effectiveness of our multi-order alignments. We also report performance if ground truth bounding boxes (and relationships) are used as a reference instead of proposals (and dense connections); see VLGAE in Table 3.\\n\\nWe apply the learning strategy of MAF on weakly-supervised visually grounding of a DT instead of given noun phrases in the training stage.\\n\\nTable 3. Visual grounding results on the test split.\\n\\n| Method  | Random | MAF | VLGAE | VLGAE 2 |\\n|---------|--------|-----|-------|---------|\\n| Obj.    | 12.2   | 27.7| 28.7  | 42.3    |\\n| Attr.   | 15.9   | 38.5| 36.1  | 67.2    |\\n| Rel.    | 9.4    | 20.7| 21.0  | 41.8    |\\n| First   | 0.0    | 0.1 | 10.2  | -       |\\n| Second  | 0.0    | 0.0 | 3.4   | -       |\\n\\n6.4. Ablation Analysis on Arc Length\\n\\nWe further investigate the recall rate for different lengths of arcs $l_{pq}$ in Figure 4. The experiments are on the Dev split. VLGAE enhanced by visual cues has been proven to boost DDA/UDA than its non-visual version (D-NDMV) in Table 2. Moreover, this boost is observed not only on short arcs but also longer arcs. This phenomenon is contrary to VC-PCFG [49], showing that dependency structures in VLGAE can be beneficial for all the arcs regardless of the arc length, compared with constituent structures.\\n\\nTable 3. Visual grounding results on the test split.\\n\\n| Method  | Random | MAF | VLGAE | VLGAE 2 |\\n|---------|--------|-----|-------|---------|\\n| Obj.    | 12.2   | 27.7| 28.7  | 42.3    |\\n| Attr.   | 15.9   | 38.5| 36.1  | 67.2    |\\n| Rel.    | 9.4    | 20.7| 21.0  | 41.8    |\\n| First   | 0.0    | 0.1 | 10.2  | -       |\\n| Second  | 0.0    | 0.0 | 3.4   | -       |\"}"}
{"id": "CVPR-2022-1976", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs with Language Structures via Dependency Relationships\\n\\nChao Lou 1, 2*, Wenjuan Han 1\u2020, Yuhuan Lin 3, Zilong Zheng 1*\\n\\n1 Beijing Institute for General Artificial Intelligence (BIGAI), Beijing, China\\n2 ShanghaiTech University, Shanghai, China\\n3 Tsinghua University, Beijing, China\\n\\nlouchao@shanghaitech.edu.cn, hanwenjuan@bigai.ai\\nlin-yh20@mails.tsinghua.edu.cn, zlzheng@bigai.ai\\n\\nhttps://github.com/bigai-research/VLGAE\\n\\nAbstract\\n\\nUnderstanding realistic visual scene images together with language descriptions is a fundamental task towards generic visual understanding. Previous works have shown compelling comprehensive results by building hierarchical structures for visual scenes (e.g., scene graphs) and natural languages (e.g., dependency trees), individually. However, how to construct a joint vision-language (VL) structure has barely been investigated. More challenging but worthwhile, we introduce a new task that targets on inducing such a joint VL structure in an unsupervised manner. Our goal is to bridge the visual scene graphs and linguistic dependency trees seamlessly. Due to the lack of VL structural data, we start by building a new dataset VLParse. Rather than using labor-intensive labeling from scratch, we propose an automatic alignment procedure to produce coarse structures followed by human refinement to produce high-quality ones. Moreover, we benchmark our dataset by proposing a contrastive learning (CL)-based framework VLGAE, short for Vision-Language Graph Autoencoder. Our model obtains superior performance on two derived tasks, i.e., language grammar induction and VL phrase grounding. Ablations show the effectiveness of both visual cues and dependency relationships on fine-grained VL structure construction.\\n\\n1. Introduction\\n\\nVisual scene understanding has long been considered a primal goal for computer vision. Going beyond the success of high-accurate individual object detection in complicated environments, various attempts have been made for higher-order visual understanding, such as predicting an explainable, structured, and semantically-aligned representation from scene images [18, 22, 41]. Such representations not only provide fine-grained visual cues for low-level recognition tasks, but have further demonstrated their applications on numerous high-level visual reasoning tasks, e.g., visual question answering (VQA) [34, 50], image captioning [3, 44], and scene synthesis [18, 21].\\n\\nScene graph (SG), one of the most popular visual structures, serves as an abstraction of objects and their complex relationships within scene images [22, 26]. Conventional scene graph generation models recognize and predict objects, attributes, relationships, and their corresponding semantic labels purely from natural images in a fully-supervised manner [33, 43]. Despite the promising performance achieved on large-scale SG benchmarks, these methods suffer from limitations on existing datasets and task settings [13]. First, a comprehensive scene graph requires different semantic levels of visual understanding [27], whilst most current datasets only capture a small portion of accessible.\"}"}
{"id": "CVPR-2022-1976", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"possible semantics for classification [13], which will cause the prediction model bias towards those most-frequent labels. Second, building such datasets requires exhaustive label- ing of bounding boxes, relations, and corresponding semantics, which are time-consuming and inefficient. Third, it is typically hard to induce a semantically-consistent graphical structure solely from visual inputs, which typically requires an extra visual relation recognition module with heavy manually-labeled supervision.\\n\\nDifferent from dense and noisy visual information, natural language directly provides symbolic and structured information (e.g., grammar) to support the comprehension process. Researches on language structure induction can date back to early computational linguistic theories [4, 5, 6]. Empowered by advances in deep learning techniques, a variety of neural structured prediction algorithms were proposed to analyze more complicated structure information and apply them to natural language tasks [9, 10, 23].\\n\\nDependency tree (DT) parsing, as one essential branch of language structured prediction, aims to generate a parse tree that is composed of vertices representing each word's semantic and syntactic meanings, and directed edges representing the dependency relationships among them. Of note, such tree structure shares a similar idea as in SG. However, the ground truth structure (commonly referred to as \\\"gold structure\\\") requires professional linguists' labeling. To mitigate the data issue, pioneer works have also demonstrated the success of DT learning in an unsupervised schema [19, 23].\\n\\nIn this work, we leverage the best of both modalities and introduce a new task \u2013 unsupervised vision-language (VL) parsing (short for VLParse) \u2013 aiming to devise a joint VL structure that bridges visual scene graphs with linguistic dependency trees seamlessly. By \\\"seamless\\\", we mean that each node in the VL structure shall present the well-aligned information of some node in SG and DT, so are their relationships, as shown in Figure 1. To the best of our knowledge, this is the first work that formally defines the joint representation of VL structure with dependency relationships. Respecting the semantic consistency and independent characteristics, the joint VL structure considers both the shared multimodal instances and the independent instances for each modality. In such a heterogeneous graph, semantically consistent instances across two graphs (DT and SG) are aligned in different levels, which maximizes the retention of the representation from two modalities. Some previous attempts have shown the benefits of exploring multi-modality information for structured understanding. For example, Shi et al. [31] first proposes a visually grounded syntax parser to induce the language structure. [46, 48] further exploit visual semantics to improve the structure for language. These structures, however, are still for language syntactic parsing rather than for joint vision-language understanding. One closest work to us is VL-Grammar [17], which builds separate image structures and language structures via compound PCFG [23]. However, the annotations (i.e., segmentation parts) are provided in advance.\\n\\nVLParse aims to conduct thoughtful cross-modality understanding and bridge the gap between multiple sub-tasks: structure induction for the image and language separately and unsupervised visual grounding. As a complex task, it is comprised of several instances, such as objects, attributes, and different levels of relationships. The interactions among different instances and sub-tasks can provide rich information and play a complementary or restrictive role during identification and understanding.\\n\\nTo address this challenging task, we propose a novel contrastive learning (CL)-based architecture, Vision-Language Graph Autoencoder (VLGAE), aiming at constructing a multimodal structure and aligning VL information simultaneously. The VLGAE is comprised of feature extraction, structure construction, and cross-modality matching modules. The feature extraction module extracts features from both modalities and builds representations for all instances in DT and SG. The structure construction module follows the encoder-decoder paradigm, where the encoder obtains a compressed global VL representation from image-caption pair using attention mechanisms; the decoder incorporates the inside algorithm to construct the VL structure recursively as well as compute the posteriors of spans. The VL structure induction is optimized by Maximum Likelihood Estimation (MLE) with a negative likelihood loss. For cross-modality matching, we compute the vision-language matching score between visual image regions and language contexts. We further enhance the matching score with posterior values achieved from the structure construction module. This score is used to promote the cross-modality fine-grained correspondence with the supervisory signal of the image-caption pairs via a CL strategy; see Figure 3 and Section 5 for details.\\n\\nIn summary, our contributions are five-fold: (i) We design a joint VL structure that bridges visual scene graph and linguistic dependency tree; (ii) We introduce a new task VLParse for better cross-modality visual scene understanding (\u00a74); (iii) We present a two-step VL dataset creation paradigm without labor-intensive labelling and deliver a new dataset (\u00a73); (iv) We benchmark our dataset with a novel CL-based framework VLGAE (\u00a75); (v) Empirical results demonstrate significant improvements on single modality structure induction and cross-modality alignment with the proposed framework.\"}"}
{"id": "CVPR-2022-1976", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"region in an image referred by natural language expressions, such as phrases \\\\[39\\\\], sentences \\\\[1, 34\\\\] or dialogues \\\\[50\\\\]. Weakly-supervised visual phrase grounding, which infers region-phrase correspondences using only image-sentence pairs, has drawn researchers' attention. There are multiple approaches to weakly-supervised visual phrase grounding. Gupta et al. \\\\[14\\\\] leverage contrastive learning to train model based on image-sentence pairs data. Wang et al. \\\\[38\\\\] build visually-aware language representations for phrases that could be better aligned with the visual representations. Wang et al. \\\\[36\\\\] develop a method to distill knowledge from Faster R-CNN for weakly supervised phrase grounding.\\n\\nLanguage sentences contain rich semantics and syntactic information. Thus, some researches focus on how to extract and leverage useful information in a sentence to facilitate visual grounding. For example, Xiao et al. \\\\[42\\\\] use the linguistic structure of natural language descriptions for visual phrase grounding. Yu et al. \\\\[45\\\\] learn to parse captions automatically into three modular components related to subject appearance, location, and relationship to other objects, which get rich different types of information from sentences. In this work, we propose to induce structures from realistic image-caption pairs without any structure annotations, nor phrase-region correspondence annotations. Note that different from Wang et al. \\\\[37\\\\] who predict the corresponding regions for a given set of noun phrases, noun phrases in VL grammar induction are unknown and all spans in the VL structure are corresponding regions in the image.\\n\\nLanguage Dependency Parsing\\n\\nDependency parsing, a fundamental challenge in natural language processing (NLP), aims to find syntactic dependency relations between words in sentences. Due to the challenge of achieving gold structures for all available language corpus, unsupervised dependency parsing, whose goal is to obtain a dependency parser without using annotated sentences, has attracted more attention over recent years. The pioneer work Dependency Model with Valence (DMV) \\\\[25\\\\] proposes to model dependency parsing as a generative process of dependency grammars. Empowered by deep learning techniques, NDMV \\\\[19\\\\] employ neural networks to capture the similarities between part-of-speech (POS) tags, and learn the grammar based on DMV. However, generative models often are limited by independence assumption, so more researchers have paid attention to autoencoder-based approaches \\\\[2\\\\], e.g., Discriminative NDMV (D-NDMV) \\\\[15\\\\].\\n\\nVisual-Aided Grammar Induction\\n\\nVisual-aided word representation learning and sentence representation learning achieve positive results. Shi et al. \\\\[31\\\\] first propose the visually grounded grammar induction task and present a visually-grounded neural syntax learner (VG-NSL). They use an easy-first bottom-up parser \\\\[12\\\\] and use REINFORCE \\\\[40\\\\] as gradient estimator for image-caption matching. Zhao and Titov \\\\[49\\\\] propose an end-to-end training algorithm for Compound PCFG \\\\[24\\\\], a powerful grammar inducer. Jin and Schuler \\\\[20\\\\] formulate a different visual grounding task. They use an autoencoder as a visual model and fuse language and vision features on the hidden states. Different from visually-grounded grammar induction, we not only care about the language structure accuracy but also the fine-grained alignment accuracy.\\n\\n3. The VLParse Dataset\\n\\nIn this section, we start by formalizing the joint VL structure to represent the shared semantics for vision and language. Then we introduce how the dataset, VLParse, is formed in a semi-automatic manner.\\n\\n3.1. Joint Vision-Language Structure\\n\\nThe vision-language (VL) structure is composed of a visual structure SG, a linguistic structure DT and a hierarchical alignment between SG and DT.\\n\\nScene graph (SG)\\n\\nWe define SG on an image \\\\(I\\\\) as a structured representation composed of three types of nodes: \\\\(T_{OBJECT}\\\\), \\\\(T_{ATTRIBUTE}\\\\), \\\\(T_{RELATIONSHIP}\\\\) u, denoting the image's objects features, conceptual attribute features, and relationship features between two objects. Each \\\\(T_{OBJECT}\\\\) node is associated with an \\\\(T_{ATTRIBUTE}\\\\) node; between each pair of \\\\(T_{OBJECT}\\\\) nodes, there exists a \\\\(T_{RELATIONSHIP}\\\\) node. Let \\\\(R\\\\) be the set of all relationship types (including \\\"none\\\" relationship), we can denote the set of all variables in SG as \\\\(v_{cls_i}, v_{bbox_i}, v_{type_i}, v_{i\u00d1j}^u; i\u2260j\\\\), where \\\\(v_{cls_i}\\\\) is the class label of the \\\\(i\\\\)-th bounding box, \\\\(v_{bbox_i}^R\\\\) denotes the bounding box offsets, \\\\(v_{type_i}^T\\\\) is the node type, and \\\\(v_{i\u00d1j}^R\\\\) is the relationship from node \\\\(v_i\\\\) to \\\\(v_j\\\\).\\n\\nDependency tree (DT)\\n\\nConventional DT is a hierarchy with directed dependency relationships. Given the textual description denoted as a sequence of \\\\(N\\\\) words \\\\(w_1, w_2, ..., w_N\\\\), each dependency within DT can be denoted as triplet \\\\(p_{w_i, w_j, w_i\u00d1j^q}\\\\), representing a parent node \\\\(w_i\\\\), a child node \\\\(w_j\\\\) and the direct dependency relationship from \\\\(w_i\\\\) to \\\\(w_j\\\\), respectively. Similar to SG, for each node's representation, we additionally append the node's type label \\\\(w_{type_i}^T\\\\). Thus, all variables within in DT becomes \\\\(t_{w_i, w_{type_i}, w_{i\u00d1j}}; i\u2260j\\\\).\\n\\nAlignment\\n\\nThe alignment between DT and SG can be seen as a realization of visual grounding for instances on different levels of the linguistic structure. We hereby define three levels of alignment (see Figure 1 for illustration):\\n\\n- **Zero-order Alignment**. It defines connections between each node \\\\(w_i\\\\) in DT with a node \\\\(v_i\\\\) in SG.\\n- **First-order Alignment**. A first-order relationship can be defined as a triplet \\\\(p_{w_i, w_j, w_i\u00d1j^q}\\\\), including two nodes and a directed dependency. Then the first-order\"}"}
{"id": "CVPR-2022-1976", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. An illustration of the process of the automatic rule-based alignment. After the process of DT rewriting and DT-SG alignment, every instance in DT can be aligned to a SG instance. Through SG, instances in DT can match to image regions.\\n\\nAlignment aims to align the triplet in DT with a similar triplet \\\\(p_v^i, v_j, v_i^q\\\\) in SG.\\n\\n- **Second-order Alignment.** A second-order relationship builds upon the first-order relationship and is represented as dependencies among three nodes, e.g., \\\\(w_i, w_j, w_k\\\\) in DT. Similar as the first-order alignment, the second-order alignment aligns such relationships between DT and SG with similar semantics.\\n\\n### 3.2. Automatic Rule-based Alignment\\n\\nIn practice, the alignment between SG and DT is labor-intensive and expensive to obtain, whilst unlabeled data is low in cost and large in scale. Thus we design a set of rules to automatically ground the language instances of DT to the vision instances of SG. This automatic alignment provides beneficial information to reduce the labeling burden on workers. Specifically, we introduce a two-step alignment process, i.e., rule-based DT rewriting (DT Rewriting) followed by the alignment between DT and SG (DT-SG Alignment).\\n\\n**DT Rewriting**\\n\\nWe start by introducing the rewriting procedure that extends and deforms DT in order to mitigate the difference between DT and SG. The rewriting considers two modules:\\n\\n- **Type Classification.** We append the type label \\\\(x_{type}\\\\) for conventional DT. More specifically, we label words from DT with three node types as follows:\\n  - **OBJECT**: The OBJECT node in DT is referred as to a word/phrase that can be grounded to a specific image area. A noun phrase including all words except for the attribute is designed as an OBJECT node.\\n  - **ATTRIBUTE**: The ATTRIBUTE node is mostly an adjective used to decorate its linked OBJECT node. In our designed rules, we set words with dependency type acomp (adjectival complement) as ATTRIBUTE nodes.\\n  - **RELATIONSHIP**: Two OBJECT nodes are linked to a RELATIONSHIP node with a directed dependency. OBJECT nodes are connected to each other through a RELATIONSHIP node. For example in Figure 2, \\\"sitting\\\" as a RELATIONSHIP node between two OBJECT nodes \\\"drinks\\\" and \\\"table\\\".\\n\\n- **Parent Identification.** Since it is hard to identify a grounding area in image for an ATTRIBUTE node or a function word (such as the coordinating conjunction and determiner, etc.), we instead define words of these types share the corresponding OBJECT node's, says parent nodes' grounding area. A parent node is a noun representing the core semantics of a noun phrase and ATTRIBUTE nodes, as dependent, modify it. This parent-dependent relationship is encoded in dependency types and dependency directions of DT [7]. Through the rules we design, every word in DT is assigned with a node type and a parent node. We design 7 rules for OBJECT-ATTRIBUTE, 12 rules for RELATIONSHIP-OBJECT, 1 rule for OBJECT-OBJECT, 10 for OBJECT-RELATIONSHIP and 22 rules for function word processing.\\n\\n**DT-SG Alignment**\\n\\nBased on the rewritten DT, we perform DT-SG alignment to map the rewritten DT to SG. In details, we calculate the similarity score between the SG node and the word's parent, and choose top k results as the alignment result. The words labeled attribute leverage parent to retrieve OBJECT node it attributes in SG. Then we retrieve the ATTRIBUTE node in the subtree rooted by the OBJECT node by calculating the similarity score between the word and ATTRIBUTE node name. An illustration of the process of alignment from words to SG nodes is shown in Figure 2.\\n\\n### 3.3. Crowd-Sourcing Human Refinement\\n\\nTo obtain a high-quality dataset, a human-refinement stage is adapted, providing an automatically annotated VL structure and asking the annotators to output the refined one. We utilize Amazon Mechanical Turk (AMT) to hire remotely native speakers to perform a crowd-sourcing survey.\\n\\n**Human Refinement**\\n\\nWe create a survey in AMT that allows workers to assess and refine data generated from the automatic rule-based alignment stage. We provide workers with comprehensive instructions and a set of well-defined examples to judge the quality of alignments, and modify those unsatisfied ones. During the task, we will show workers an interface with paired images and captions grouped by the image. We ask workers to check DT, SG, and the cross-modality alignment. Then the workers correct the inappropriate areas when necessary. The final results are combined using a majority vote.\\n\\nThe dependencies used here is based on Stanford typed dependencies [7], a framework for annotation of grammar (parts of speech and syntactic dependencies http://catalog.ldc.upenn.edu/LDC99T42) [29]. Following [49], a learned parser [47] on this annotated data is used to label the dependency existence and dependency type.\"}"}
{"id": "CVPR-2022-1976", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Arjun Akula, Spandana Gella, Yaser Al-Onaizan, Song-Chun Zhu, and Siva Reddy. Words aren't enough, their order matters: On the robustness of grounding visual referring expressions. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 6555\u20136565, Online, July 2020. Association for Computational Linguistics.\\n\\n[2] Jiong Cai, Yong Jiang, and Kewei Tu. CRF autoencoder for unsupervised dependency parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1638\u20131643, Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics.\\n\\n[3] Shizhe Chen, Qin Jin, Peng Wang, and Qi Wu. Say as you wish: Fine-grained control of image caption generation with abstract scene graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 9962\u20139971, 2020.\\n\\n[4] Noam Chomsky. Three models for the description of language. IRE Transactions on information theory, 2(3):113\u2013124, 1956.\\n\\n[5] Noam Chomsky. On certain formal properties of grammars. Information and control, 2(2):137\u2013167, 1959.\\n\\n[6] Noam Chomsky. Syntactic structures. De Gruyter Mouton, 2009.\\n\\n[7] Marie-Catherine De Marneffe and Christopher D Manning. Stanford typed dependencies manual. Technical report, Technical report, Stanford University, 2008.\\n\\n[8] Timothy Dozat and Christopher D. Manning. Deep biaffine attention for neural dependency parsing. In International Conference on Learning Representations (ICLR). OpenReview.net, 2017.\\n\\n[9] Andrew Drozdov, Subendhu Rongali, Yi-Pei Chen, Tim O'Gorman, Mohit Iyyer, and Andrew McCallum. Unsupervised parsing with s-diora: Single tree encoding for deep inside-outside recursive autoencoders. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.\\n\\n[10] Andrew Drozdov, Pat Verga, Mohit Yadav, Mohit Iyyer, and Andrew McCallum. Unsupervised latent tree induction with deep inside-outside recursive autoencoders. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\\n\\n[11] Jason Eisner. Inside-outside and forward-backward algorithms are just backprop (tutorial paper). In Proceedings of the Workshop on Structured Prediction for NLP, pages 1\u201317, Austin, TX, Nov. 2016. Association for Computational Linguistics.\\n\\n[12] Yoav Goldberg and Michael Elhadad. An efficient algorithm for easy-first non-directional dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 742\u2013750, Los Angeles, California, June 2010. Association for Computational Linguistics.\\n\\n[13] Jiuxiang Gu, Handong Zhao, Zhe Lin, Sheng Li, Jianfei Cai, and Mingyang Ling. Scene graph generation with external knowledge and image reconstruction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1969\u20131978, 2019.\\n\\n[14] Tanmay Gupta, Arash Vahdat, Gal Chechik, Xiaodong Yang, Jan Kautz, and Derek Hoiem. Contrastive learning for weakly supervised phrase grounding. In European Conference on Computer Vision (ECCV), pages 752\u2013768. Springer, 2020.\\n\\n[15] Wenjuan Han, Yong Jiang, and Kewei Tu. Enhancing unsupervised generative dependency parser with contextual information. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 5315\u20135325, 2019.\\n\\n[16] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.\\n\\n[17] Yining Hong, Qing Li, Song-Chun Zhu, and Siyuan Huang. Vlgrammar: Grounded grammar induction of vision and language. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2021.\\n\\n[18] Chenfanfu Jiang, Siyuan Qi, Yixin Zhu, Siyuan Huang, Jenny Lin, Lap-Fai Yu, Demetri Terzopoulos, and Song-Chun Zhu. Configurable 3d scene synthesis and 2d image rendering with per-pixel ground truth using stochastic grammars. International Journal of Computer Vision (IJCV), 126(9):920\u2013941, 2018.\\n\\n[19] Yong Jiang, Wenjuan Han, and Kewei Tu. Unsupervised neural dependency parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 763\u2013771, 2016.\\n\\n[20] Lifeng Jin and William Schuler. Grounded PCFG induction with images. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 396\u2013408, Suzhou, China, Dec. 2020. Association for Computational Linguistics.\\n\\n[21] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1219\u20131228, 2018.\\n\\n[22] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3668\u20133678, 2015.\\n\\n[23] Yoon Kim, Chris Dyer, and Alexander Rush. Compound probabilistic context-free grammars for grammar induction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 2369\u20132385, Florence, Italy, July 2019. Association for Computational Linguistics.\\n\\n[24] Yoon Kim, Chris Dyer, and Alexander Rush. Compound probabilistic context-free grammars for grammar induction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2369\u20132385, Florence, Italy, July 2019. Association for Computational Linguistics.\"}"}
{"id": "CVPR-2022-1976", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[26] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. *Int. J. Comput. Vision*, 123(1):32\u201373, may 2017.\\n\\n[27] Yikang Li, Wanli Ouyang, Bolei Zhou, Kun Wang, and Xiaogang Wang. Scene graph generation from objects, phrases and region captions. In *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*, pages 1261\u20131270, 2017.\\n\\n[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, *European Conference on Computer Vision (ECCV)*, pages 740\u2013755, Cham, 2014. Springer International Publishing.\\n\\n[29] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. *Computational Linguistics*, 19(2):313\u2013330, 1993.\\n\\n[30] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In *Workshop on Conference on Neural Information Processing Systems (NeurIPS)*, 2015.\\n\\n[31] Haoyue Shi, Jiayuan Mao, Kevin Gimpel, and Karen Livescu. Visually grounded neural syntax acquisition. In *Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)*, pages 1842\u20131861, 2019.\\n\\n[32] Jing Shi, Yiwu Zhong, Ning Xu, Yin Li, and Chenliang Xu. A simple baseline for weakly-supervised scene graph generation. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 16393\u201316402, 2021.\\n\\n[33] Mohammed Suhail, Abhay Mittal, Behjat Siddiquie, Chris Broaddus, Jayan Eledath, Gerard Medioni, and Leonid Sigal. Energy-based learning for scene graph generation. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 13936\u201313945, 2021.\\n\\n[34] Kewei Tu, Meng Meng, Mun Wai Lee, Tae Eun Choe, and Song-Chun Zhu. Joint video and text parsing for understanding events and answering queries. *IEEE MultiMedia*, 21(2):42\u201370, 2014.\\n\\n[35] J. R. Uijlings, K. E. Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. *International Journal of Computer Vision (IJCV)*, 2013.\\n\\n[36] Liwei Wang, Jing Huang, Yin Li, Kun Xu, Zhengyuan Yang, and Dong Yu. Improving weakly supervised visual grounding by contrastive knowledge distillation. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 14090\u201314100, 2021.\\n\\n[37] Qinxin Wang, Hao Tan, Sheng Shen, Michael Mahoney, and Zhewei Yao. MAF: Multimodal alignment framework for weakly-supervised phrase grounding. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 2030\u20132038, Online, Nov. 2020. Association for Computational Linguistics.\\n\\n[38] Qinxin Wang, Hao Tan, Sheng Shen, Michael W Mahoney, and Zhewei Yao. MAF: Multimodal alignment framework for weakly-supervised phrase grounding. In *Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)*, 2020.\\n\\n[39] Shuo Wang, Yizhou Wang, and Song-Chun Zhu. Learning hierarchical space tiling for scene modeling, parsing and attribute tagging. *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*, 37(12):2478\u20132491, 2015.\\n\\n[40] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. *Machine Learning*, 8:229\u2013256, 2004.\\n\\n[41] Tian-Fu Wu, Gui-Song Xia, and Song-Chun Zhu. Compositional boosting for computing hierarchical image structures. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 1\u20138. IEEE, 2007.\\n\\n[42] Fanyi Xiao, Leonid Sigal, and Yong Jae Lee. Weakly-supervised visual grounding of phrases with linguistic structures. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 5945\u20135954, 2017.\\n\\n[43] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi Parikh. Graph r-cnn for scene graph generation. In *European Conference on Computer Vision (ECCV)*, pages 670\u2013685, 2018.\\n\\n[44] Benjamin Z Yao, Xiong Yang, Liang Lin, Mun Wai Lee, and Song-Chun Zhu. I2t: Image parsing to text description. *Proceedings of the IEEE*, 98(8):1485\u20131508, 2010.\\n\\n[45] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 1307\u20131315, 2018.\\n\\n[46] Songyang Zhang, Linfeng Song, Lifeng Jin, Kun Xu, Dong Yu, and Jiebo Luo. Video-aided unsupervised grammar induction. In *Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)*, pages 1513\u20131524, 2021.\\n\\n[47] Yu Zhang, Zhenghua Li, and Min Zhang. Efficient second-order TreeCRF for neural dependency parsing. In *Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)*, pages 3295\u20133305, Online, July 2020. Association for Computational Linguistics.\\n\\n[48] Yanpeng Zhao and Ivan Titov. Visually grounded compound PCFGs. In *Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 4369\u20134379, 2020.\\n\\n[49] Yanpeng Zhao and Ivan Titov. Visually grounded compound PCFGs. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 4369\u20134379, Online, Nov. 2020. Association for Computational Linguistics.\\n\\n[50] Zilong Zheng, Wenguan Wang, Siyuan Qi, and Song-Chun Zhu. Reasoning visual dialogs with structural and partial observations. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 6669\u20136678, 2019.\"}"}
