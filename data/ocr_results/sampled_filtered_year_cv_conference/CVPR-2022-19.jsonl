{"id": "CVPR-2022-19", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Our proposed Dual Contrastive Learning (DCL) for few-shot GAN adaptation. To slow down diversity degradation during adaptation, in both Generator CL (Eqn. 5) and Discriminator CL (Eqn. 7), we maximize the similarity between positive pairs generated using the same noise input, and push away those that are negative pairs. See details in Sec. 5.1.\\n\\nLdist in [33], see Eqn. 2) competes with main GAN loss Ladv which is responsible for quality improvement. Note that such observation is consistent with findings in multi-task learning [12, 39]. On the other hand, interestingly, we observe that these methods can still reach high quality on the target domain rather quickly, and they achieve similar quality as other methods after quality converges. We conjecture that, due to the informative prior knowledge on the source domain, it is not difficult to obtain adequate realisticness when transferring to the small target domain.\\n\\n- Image diversity (Observation 2): In Figure 1 and Figure 2b, we demonstrate the qualitative and quantitative results of the diversity change during few-shot adaptation. While different methods can achieve similar quality on the target domain, their diversity-degrading rates vary drastically. For example, in Figure 1, TGAN [47] immediately loses the diversity and the generator replicates the target samples at the early adaptation stage. Since the achieved quality is similar for all these methods, the better methods are those that do not suffer from rapid diversity degradation.\\n\\nAs shown in Figure 2b, for all existing methods, the loss of diversity is inevitable in adaptation. On the other hand, since all methods achieve similar realisticness after the quality converges, the better generators are those that can slow down the diversity degradation, and the worse ones are those that lose diversity rapidly before the convergence of the quality improvement. Recalling the concerns we raise in Sec. 4.1: we show that, there is still plenty of room to further reduce the rate of diversity degradation, besides, the gain of diversity preservation becomes saturated only until the rate of diversity degradation is much reduced. These findings motivate our proposed method in the next section to further slow down diversity degradation.\\n\\n5. Dual Contrastive Learning\\n\\n5.1. Overall Framework\\n\\nRich diversity exists in the source images $G_s(z)$ at different semantic levels: middle levels such as different hair style, high levels such as different facial expression. To preserve source images' diversity information in the target domain generator, we propose to maximize the mutual information (MI) between the source/target image features originated from the same input noise. In particular, for an input noise $z_i$, we seek to maximize:\\n\\n$$MI(\\\\pi_l(G_t(z_i))); \\\\pi_l(G_s(z_i)))$$\\n\\n(4)\\n\\nwhere $G_t(z_i), G_s(z_i)$ are generated images by the target / source generators respectively, and $\\\\pi_l(\\\\cdot)$ is a feature encoder to extract $l$-th level features (to be further discussed). As $G_s$ is fixed during adaptation, the MI maximization enables $G_t$ to learn to generate images that include source images' diversity at various semantic levels (with the use of multiple $l$ during adaptation). Furthermore, in GAN, we have $G_t$ and $D_t$, and we take advantage both of them to use as the feature encoders $\\\\pi_l(\\\\cdot)$ to extract different features. As directly maximizing MI is challenging [36], we apply contrastive learning [34] to solve Eqn. 4.\\n\\nSpecifically, we propose Dual Contrastive Learning (DCL) for few-shot GAN adaptation, as Figure 3. There are several goals for DCL: i) Maximize the MI between generated images on the source/target domain originated from the same noise input; ii) push away the generated images on the source and target domain that use different noise input; iii) push away the generated target images and the real target images, to prevent collapsing to the few-shot target set. To achieve these goals, we let the generating and the discriminating views using the same noise input, on source and target, as the positive pair, and maximize the agreement between them. Concretely, DCL includes two parts.\\n\\nGenerator CL: Given a batch of noise input $\\\\{z_0, z_1, ..., z_{N-1}\\\\}$, we obtain images $\\\\{G_s(z_0), G_s(z_1), ..., G_s(z_{N-1})\\\\}$\\n\\n\"}"}
{"id": "CVPR-2022-19", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"be pushed away from the real data at different feature scales. \\n\\nWhile fitting to the target (with the same noise input). Differently, Discriminator CL is different scales that are similar to that of the source generator. \\n\\nregularization to slow down the inevitable the highest diversity knowledge from the source as the strong\\n\\n\\\\[ \\\\tau = 0 \\\\]\\n\\non source and target, scaled by a hyperparameter temperature\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\nDiscriminator CL: We focus on the view of the discrim-\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\nination, before the quality improvement converges. The Gener-\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B} \\\\]\\n\\n\\\\[ \\\\mathcal{C} \\\\]\\n\\n\\\\[ \\\\mathcal{D} \\\\]\\n\\n\\\\[ \\\\mathcal{E} \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\mathcal{H} \\\\]\\n\\n\\\\[ \\\\mathcal{I} \\\\]\\n\\n\\\\[ \\\\mathcal{J} \\\\]\\n\\n\\\\[ \\\\mathcal{K} \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B} \\\\]\\n\\n\\\\[ \\\\mathcal{C} \\\\]\\n\\n\\\\[ \\\\mathcal{D} \\\\]\\n\\n\\\\[ \\\\mathcal{E} \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\mathcal{H} \\\\]\\n\\n\\\\[ \\\\mathcal{I} \\\\]\\n\\n\\\\[ \\\\mathcal{J} \\\\]\\n\\n\\\\[ \\\\mathcal{K} \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B} \\\\]\\n\\n\\\\[ \\\\mathcal{C} \\\\]\\n\\n\\\\[ \\\\mathcal{D} \\\\]\\n\\n\\\\[ \\\\mathcal{E} \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\mathcal{H} \\\\]\\n\\n\\\\[ \\\\mathcal{I} \\\\]\\n\\n\\\\[ \\\\mathcal{J} \\\\]\\n\\n\\\\[ \\\\mathcal{K} \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B} \\\\]\\n\\n\\\\[ \\\\mathcal{C} \\\\]\\n\\n\\\\[ \\\\mathcal{D} \\\\]\\n\\n\\\\[ \\\\mathcal{E} \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\mathcal{H} \\\\]\\n\\n\\\\[ \\\\mathcal{I} \\\\]\\n\\n\\\\[ \\\\mathcal{J} \\\\]\\n\\n\\\\[ \\\\mathcal{K} \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B} \\\\]\\n\\n\\\\[ \\\\mathcal{C} \\\\]\\n\\n\\\\[ \\\\mathcal{D} \\\\]\\n\\n\\\\[ \\\\mathcal{E} \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\mathcal{H} \\\\]\\n\\n\\\\[ \\\\mathcal{I} \\\\]\\n\\n\\\\[ \\\\mathcal{J} \\\\]\\n\\n\\\\[ \\\\mathcal{K} \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B} \\\\]\\n\\n\\\\[ \\\\mathcal{C} \\\\]\\n\\n\\\\[ \\\\mathcal{D} \\\\]\\n\\n\\\\[ \\\\mathcal{E} \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\mathcal{H} \\\\]\\n\\n\\\\[ \\\\mathcal{I} \\\\]\\n\\n\\\\[ \\\\mathcal{J} \\\\]\\n\\n\\\\[ \\\\mathcal{K} \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B} \\\\]\\n\\n\\\\[ \\\\mathcal{C} \\\\]\\n\\n\\\\[ \\\\mathcal{D} \\\\]\\n\\n\\\\[ \\\\mathcal{E} \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\mathcal{H} \\\\]\\n\\n\\\\[ \\\\mathcal{I} \\\\]\\n\\n\\\\[ \\\\mathcal{J} \\\\]\\n\\n\\\\[ \\\\mathcal{K} \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B} \\\\]\\n\\n\\\\[ \\\\mathcal{C} \\\\]\\n\\n\\\\[ \\\\mathcal{D} \\\\]\\n\\n\\\\[ \\\\mathcal{E} \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\mathcal{H} \\\\]\\n\\n\\\\[ \\\\mathcal{I} \\\\]\\n\\n\\\\[ \\\\mathcal{J} \\\\]\\n\\n\\\\[ \\\\mathcal{K} \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B} \\\\]\\n\\n\\\\[ \\\\mathcal{C} \\\\]\\n\\n\\\\[ \\\\mathcal{D} \\\\]\\n\\n\\\\[ \\\\mathcal{E} \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\mathcal{H} \\\\]\\n\\n\\\\[ \\\\mathcal{I} \\\\]\\n\\n\\\\[ \\\\mathcal{J} \\\\]\\n\\n\\\\[ \\\\mathcal{K} \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B} \\\\]\\n\\n\\\\[ \\\\mathcal{C} \\\\]\\n\\n\\\\[ \\\\mathcal{D} \\\\]\\n\\n\\\\[ \\\\mathcal{E} \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\mathcal{H} \\\\]\\n\\n\\\\[ \\\\mathcal{I} \\\\]\\n\\n\\\\[ \\\\mathcal{J} \\\\]\\n\\n\\\\[ \\\\mathcal{K} \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B} \\\\]\\n\\n\\\\[ \\\\mathcal{C} \\\\]\\n\\n\\\\[ \\\\mathcal{D} \\\\]\\n\\n\\\\[ \\\\mathcal{E} \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\mathcal{H} \\\\]\\n\\n\\\\[ \\\\mathcal{I} \\\\]\\n\\n\\\\[ \\\\mathcal{J} \\\\]\\n\\n\\\\[ \\\\mathcal{K} \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B} \\\\]\\n\\n\\\\[ \\\\mathcal{C} \\\\]\\n\\n\\\\[ \\\\mathcal{D} \\\\]\\n\\n\\\\[ \\\\mathcal{E} \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\mathcal{H} \\\\]\\n\\n\\\\[ \\\\mathcal{I} \\\\]\\n\\n\\\\[ \\\\mathcal{J} \\\\]\\n\\n\\\\[ \\\\mathcal{K} \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\mathcal{M} \\\\]\\n\\n\\\\[ \\\\mathcal{N} \\\\]\\n\\n\\\\[ \\\\mathcal{O} \\\\]\\n\\n\\\\[ \\\\mathcal{P} \\\\]\\n\\n\\\\[ \\\\mathcal{Q} \\\\]\\n\\n\\\\[ \\\\mathcal{R} \\\\]\\n\\n\\\\[ \\\\mathcal{S} \\\\]\\n\\n\\\\[ \\\\mathcal{T} \\\\]\\n\\n\\\\[ \\\\mathcal{U} \\\\]\\n\\n\\\\[ \\\\mathcal{V} \\\\]\\n\\n\\\\[ \\\\mathcal{W} \\\\]\\n\\n\\\\[ \\\\mathcal{X} \\\\]\\n\\n\\\\[ \\\\mathcal{Y} \\\\]\\n\\n\\\\[ \\\\mathcal{Z} \\\\]\\n\\n\\\\[ \\\\mathcal{A} \\\\]\\n\\n\\\\[ \\\\mathcal{B}"}
{"id": "CVPR-2022-19", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Left: Transferring a GAN pretrained on FFHQ to 10-shot target samples. We fix the noise input by each column to observe the relationship of generated images before and after adaptation. Mid: We observe that, most of existing methods lose diversity quickly before the quality improvement converges, and tend to replicate the training data. Our method, in contrast, slows down the loss of diversity and preserves more details. For example: In red frames (Upper), the hair style and hat are better preserved. In pink frames (Bottom), the smile teeth are well inherited from the source domain. We also outperform others in quantitative evaluation (Right). See details in Sec. 6.2.\\n\\n6.2. Comparison with State-of-the-art Methods\\n\\nQualitative results. In Figure 4, we visualize the generated images with different methods after adaptation on few-shot target samples. In Figure 5, we show more results with different source $\\\\rightarrow$ target adaptation setups. Regularized by DCL, the adapted target generator needs to retain the connection to the generated images on source, a slow diversity degradation is thus achieved. We show that, the refined details, e.g., the smile teeth, hairline or structure appearance are well preserved, while the style and texture features are fitted to the target domain. Compared with recent state-of-the-art methods, these semantic meaningful features in the source domain are well inherited by DCL.\\n\\nQuantitative comparison. We mainly focus on two metrics to evaluate our method. i) For datasets which contain a lot of real data, e.g., FFHQ-Babies, FFHQ-Sunglasses, we apply the widely used Fr\u00e9chet Inception Distance (FID) [18] to evaluate the generated fake images. ii) For those datasets...\"}"}
{"id": "CVPR-2022-19", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Generated images with different source \u2192 target adaptation setups. We show that the target generator after adaptation with our proposed method can generate visually pleasant images without much loss of diversity. The facial expressions of the characters, the structure appearance of the building, and the form/color of vehicles are well preserved and corresponded. See quantitative comparison in Table 2.\\n\\nFigure 6. Left: Ablation of the realisticness classifier. We show that 1): Using alternative datasets as non-target to train the classifier give similar results. 2): Moreover, if we input randomly generated noise images, the classifier will give low confidence of being target. Right: Ablation study of DCL, with the same setup as Figure 1. Which contain only few-shot real data, FID becomes tricky and unstable, since it summarizes the quality and diversity to a single score. Therefore, we use Intra-LPIPS (see Sec. 4.3) to measure the diversity of generated images. The better diversity comes with a higher score. As shown in Table 1 and Table 2, our method outperforms the baseline methods. This indicates that the target generator we obtain can cover a wide range of modes, and the loss of diversity is further reduced.\\n\\n6.3. Analysis\\nEffect of our methods. The goal of our proposed binary classifier is to detect if an input image is \u201cnon-target\u201d or \u201ctarget\u201d. In Figure 6 (left), we replace the source with equally sampled images from \\\\{FFHQ, ImageNet [9], CUB [43], Cars [26]\\\\} as \u201cnon-target\u201d and Sketches as \u201ctarget\u201d, and we observe the similar results, compared to Figure 2a. We also show that both Generator CL and Discriminator CL can slow down the diversity degradation, and the better results are achieved when combined together. This further confirms our observations in Sec. 4: the slower diversity degradation leads to the better target generator for few-shot adaptation.\\n\\nEffect of target data size. In this work, we mainly focus on 10-shot adaptation, similar to the prior literature [28, 33]. We further perform ablation study on 5-shot and 1-shot adaptation, use the same setup as Sec. 4 and Sec. 6. As results in Supp., we have the similar qualitative and quantitative observations, which confirms our findings stated in Sec. 4.5.\\n\\n7. Conclusion\\nFocusing on few-shot image generation, our first contribution is to analyze existing few-shot image generation methods in a unified framework to gain insights on quality/diversity progress during adaptation. Of surprise is that the achieved quality is similar for different existing methods despite some of them have disproportionate focus on preserving diversity. Of interest is that the different rates of diversity degradation is the main factor for different performances. Informed by our analysis, as our second contribution we propose a mutual information based method to slow down degradation of diversity during adaptation. We connect the source and the target generators through the latent code, and construct positive-negative pairs to facilitate the contrastive learning. Our proposed method shows successful results both visually and quantitatively. Our study provides concrete information that future work could continue to focus on reducing the rate of diversity degradation in order to further improve few-shot image generation.\\n\\nAcknowledgments: This research is supported by the National Research Foundation, Singapore under its AI Singapore Programmes (AISG Award No.: AISG2-RP-2021-021; AISG Award No.: AISG-100E2018-005). This project is also supported by SUTD project PIE-SGP-AI-2018-01. We thank anonymous reviewers and Chandrasegaran Keshigeyan for their fruitful discussion and kind help.\"}"}
{"id": "CVPR-2022-19", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Milad Abdollahzadeh, Touba Malekzadeh, and Ngai-Man Man Cheung. Revisit multimodal meta-learning through the lens of multi-task learning. Advances in Neural Information Processing Systems, 34, 2021.\\n\\n[2] Abulikemu Abuduweili, Xingjian Li, Humphrey Shi, Cheng-Zhong Xu, and Dejing Dou. Adaptive consistency regularization for semi-supervised transfer learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\n[3] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340, 2017.\\n\\n[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. International Conference on Learning Representations, 2018.\\n\\n[5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912\u20139924, 2020.\\n\\n[6] Keshigeyan Chandrasegaran, Ngoc-Trung Tran, and Ngai-Man Cheung. A closer look at fourier spectrum discrepancies for cnn-generated images detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7200\u20137209, 2021.\\n\\n[7] Keshigeyan Chandrasegaran, Ngoc-Trung Tran, Yunqing ZHAO, and Ngai man Cheung. To smooth or not to smooth? on compatibility between label smoothing and knowledge distillation, 2022.\\n\\n[8] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15750\u201315758, 2021.\\n\\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255, 2009.\\n\\n[10] Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594\u2013611, 2006.\\n\\n[11] Qianli Feng, Chenqi Guo, Fabian Benitez-Quiroz, and Aleix M Martinez. When do gans replicate? on the choice of dataset size. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6701\u20136710, 2021.\\n\\n[12] Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Measuring and harnessing transfer-ence in multi-task learning. arXiv preprint arXiv:2010.15413, 2020.\\n\\n[13] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017.\\n\\n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.\\n\\n[15] Jean-Bastien Grill, Florian Strub, Florent Altch \u00b4e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doerchsch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.\\n\\n[16] Yiluan Guo and Ngai-Man Cheung. Attentive weights generation for few shot learning via information maximization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13499\u201313508, 2020.\\n\\n[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729\u20139738, 2020.\\n\\n[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\\n\\n[19] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NeurIPS Deep Learning and Representation Learning Workshop, 2015.\\n\\n[20] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125\u20131134, 2017.\\n\\n[21] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A survey on contrastive self-supervised learning. Technologies, 9(1):2, 2021.\\n\\n[22] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in Neural Information Processing Systems, 33:12104\u201312114, 2020.\\n\\n[23] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.\\n\\n[24] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8110\u20138119, 2020.\\n\\n[25] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.\\n\\n[26] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for fine-grained categorization. In 2013 IEEE International Conference on Computer Vision Workshops, pages 554\u2013561, 2013.\\n\\n[27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25:1097\u20131105, 2012.\"}"}
{"id": "CVPR-2022-19", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yijun Li, Richard Zhang, Jingwan (Cynthia) Lu, and Eli Shechtman. Few-shot image generation with elastic weight consolidation. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 15885\u201315896. Curran Associates, Inc., 2020.\\n\\nWeide Liu, Chi Zhang, Guosheng Lin, and Fayao Liu. Crnnet: Cross-reference networks for few-shot segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4165\u20134173, 2020.\\n\\nIshan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6707\u20136717, 2020.\\n\\nSangwoo Mo, Minsu Cho, and Jinwoo Shin. Freeze the discriminator: a simple baseline for fine-tuning gans. In CVPR AI for Content Creation Workshop, 2020.\\n\\nAtsuhiro Noguchi and Tatsuya Harada. Image generation from small datasets via batch statistics adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2750\u20132758, 2019.\\n\\nUtkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A Efros, Yong Jae Lee, Eli Shechtman, and Richard Zhang. Few-shot image generation via cross-domain correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10743\u201310752, 2021.\\n\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\\n\\nSinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359, 2009.\\n\\nLiam Paninski. Estimation of entropy and mutual information. Neural computation, 15(6):1191\u20131253, 2003.\\n\\nTaesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-image translation. In European Conference on Computer Vision, pages 319\u2013345. Springer, 2020.\\n\\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in neural information processing systems, pages 4077\u20134087, 2017.\\n\\nTrevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should be learned together in multi-task learning? In International Conference on Machine Learning, pages 9120\u20139132. PMLR, 2020.\\n\\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. In CVPR, pages 1199\u20131208, 2018.\\n\\nNgoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, and Ngai-Man Cheung. On data augmentation for gan training. IEEE Transactions on Image Processing, 30:1882\u20131897, 2021.\\n\\nHung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang. Regularizing generative adversarial networks under limited data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7921\u20137931, 2021.\\n\\nC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.\\n\\nSheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images are surprisingly easy to spot... for now. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8695\u20138704, 2020.\\n\\nXiaogang Wang and Xiaoou Tang. Face photo-sketch synthesis and recognition. IEEE transactions on pattern analysis and machine intelligence, 31(11):1955\u20131967, 2008.\\n\\nYaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, and Joost van de Weijer. Minegan: Effective knowledge transfer from gans to target domains with few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9332\u20139341, 2020.\\n\\nYaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, and Bogdan Raducanu. Transfer-learning gans: generating images from limited data. In Proceedings of the European Conference on Computer Vision (ECCV), pages 218\u2013234, 2018.\\n\\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3733\u20133742, 2018.\\n\\nJiayu Xiao, Liang Li, Chaofei Wang, Zheng-Jun Zha, and Qingming Huang. Few shot generative model adaption via relaxed spatial structural alignment. arXiv preprint arXiv:2203.04121, 2022.\\n\\nRenjun Xu, Pelen Liu, Liyan Wang, Chao Chen, and Jindong Wang. Reliable weighted optimal transport for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4394\u20134403, 2020.\\n\\nCeyuan Yang, Yujun Shen, Yinghao Xu, and Bolei Zhou. Data-efficient instance generation from instance discrimination. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nCeyuan Yang, Yujun Shen, Zhiyi Zhang, Yinghao Xu, Jiapeng Zhu, Zhirong Wu, and Bolei Zhou. One-shot generative domain adaptation. arXiv preprint arXiv:2111.09876, 2021.\\n\\nJordan Yaniv, Yael Newman, and Ariel Shamir. The face of art: landmark detection and geometric style in portraits. ACM Transactions on graphics (TOG), 38(4):1\u201315, 2019.\\n\\nFisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\\n\\nHan Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for generative adversarial networks. Proceedings of 6th International Conference on Learning Representations ICLR, 2020.\\n\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.\"}"}
{"id": "CVPR-2022-19", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efficient gan training. Advances in Neural Information Processing Systems, 33:7559\u20137570, 2020.\\n\\nZhengli Zhao, Zizhao Zhang, Ting Chen, Sameer Singh, and Han Zhang. Image augmentations for gan training. arXiv preprint arXiv:2006.02595, 2020.\\n\\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223\u20132232, 2017.\"}"}
{"id": "CVPR-2022-19", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Closer Look at Few-shot Image Generation\\n\\nYunqing Zhao\\nHenghui Ding\\nHoujing Huang\\nNgai-Man Cheung\\n\\n1 Singapore University of Technology and Design (SUTD)\\n2 ETH Z\u00fcrich\\n3 ByteDance Ltd.\\n\\nyunqing.zhao@mymail.sutd.edu.sg, ngaiman.cheung@sutd.edu.sg\\n\\nFigure 1. Upper Left: Transferring a GAN pretrained on a large-scale source domain (FFHQ [23]) to 10-shot Amedeo Modigliani\u2019s paintings [53]. Right: During adaptation, we randomly select four fixed noise input and visualize the generated images with different existing methods. We observe that all methods including those with disproportionate focus on diversity preserving achieve similar quality after convergence (evaluated by our proposed realisticness classifier in Sec. 4.2). Therefore, the better methods are those that can slow down diversity degradation (Bottom Left, evaluated by intra-LPIPS in Sec. 4.3). We perform a rigorous study and show quantitative results in Sec. 4.5.\\n\\nAbstract\\n\\nModern GANs excel at generating high quality and diverse images. However, when transferring the pretrained GANs on small target data (e.g., 10-shot), the generator tends to replicate the training samples. Several methods have been proposed to address this few-shot image generation task, but there is a lack of effort to analyze them under a unified framework. As our first contribution, we propose a framework to analyze existing methods during the adaptation. Our analysis discovers that while some methods have disproportionate focus on diversity preserving which impede quality improvement, all methods achieve similar quality after convergence. Therefore, the better methods are those that can slow down diversity degradation. Furthermore, our analysis reveals that there is still plenty of room to further slow down diversity degradation.\\n\\nInformed by our analysis and to slow down the diversity degradation of the target generator during adaptation, our second contribution proposes to apply mutual information (MI) maximization to retain the source domain\u2019s rich multi-level diversity information in the target domain generator. We propose to perform MI maximization by contrastive loss (CL), leverage the generator and discriminator as two feature encoders to extract different multi-level features for computing CL. We refer to our method as Dual Contrastive Learning (DCL). Extensive experiments on several public datasets show that, while leading to a slower diversity-degrading generator during adaptation, our proposed DCL brings visually pleasant quality and state-of-the-art quantitative performance.\"}"}
{"id": "CVPR-2022-19", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"these GANs often require large-scale datasets and computationally expensive resources to achieve good performance. For example, StyleGAN [23] is trained on Flickr-Faces-HQ (FFHQ) [23], which contains 70,000 images, with almost 56 GPU days. When the dataset size is decreased, however, the generator often tends to replicate the training data [11].\\n\\nWould it be possible to generate sufficiently diverse images, given only limited training data? For example, with 10-shot sketch style human faces [45], could we generate diverse face sketch paintings? This few-shot image generation task is important in many real-world applications with limited data, e.g., artistic domains. It can also benefit some downstream tasks, e.g., few-shot image classification [3].\\n\\n1.1. A Closer Look at Few-shot Image Generation\\n\\nTo address this few-shot image generation task, instead of training from scratch [42, 51], recent literature focus on transfer learning [2, 35, 50], i.e., leveraging the prior knowledge of a GAN pretrained on a large-scale, diverse dataset of the source domain and adapting it to a small target domain, without access to the source data. The early method is based on fine-tuning [47]. In particular, starting from the pretrained generator $G_s$, the original GAN loss [14]\\n\\n$$\\\\min_{G_t} \\\\max_{D_t} L_{adv} = \\\\mathbb{E}_{x \\\\sim p_{data}(x)}[\\\\log D_t(x)] + \\\\mathbb{E}_{z \\\\sim p_z(z)}[\\\\log (1 - D_t(G_t(z)))]$$ (1)\\n\\nwhere $z$ is sampled from a Gaussian noise distribution $p_z(z)$, $p_{data}(x)$ is the probability distribution of the real target domain data $x$, $G_t$ and $D_t$ are generator and discriminator of the target domain, and $G_t$ is initialized by the weights of $G_s$. This GAN loss in Eqn. 1 forces $G_t$ to capture the statistics of the target domain data, thereby to achieve both good quality (realisticness w.r.t. target domain data) and diversity, the criteria for a good generator.\\n\\nHowever, for few-shot setup (e.g., only 10 target domain images), such approach is inadequate to achieve diverse target image generation as very limited samples are provided to define $p_{data}(x)$. Recognizing that, recent methods [28, 33] have focused disproportionately on improving diversity by preserving diversity of the source generator during the generator adaptation. In [28], Elastic Weight Consolidation (EWC) [25] is proposed to limit changes in some important weights to preserve diversity. In [33], an additional Cross-domain Correspondence (CDC) loss is introduced to preserve the sample-wise distance information of source to maintain diversity, and the whole model is trained via a multi-task loss with the diversity loss $L_{dist}$ as an auxiliary task to regularize the main GAN task with loss $L_{adv}$:\\n\\n$$\\\\min_{G_t} \\\\max_{D_t} L_{adv} + L_{dist}.$$ (2)\\n\\nIn [33], a patch discriminator [20, 59] is also used to further improve the performance in $L_{adv}$. Details of $L_{dist}$ in [33].\\n\\nDiversity preserving methods [28, 33] have demonstrated impressive results based on Fr\u00e9chet Inception Distance (FID) [18] which measures the quality and diversity of the generated samples simultaneously. However, second thoughts about these methods reveal some questions:\\n\\n- With disproportionate focus on diversity preserving in recent works [28,33], will quality of the generated samples be compromised? For example, in Eqn. 2, $L_{adv}$ is responsible for quality improvement during adaptation, but $L_{dist}$ may compete with $L_{adv}$ as it has been observed in multi-task learning [12, 39]. We note that this has not been analyzed thoroughly.\\n\\n- With recent works' strong focus on diversity preserving [28, 33], will there still be room to further improve via diversity preserving? How could we know when the gain of diversity preserving approaches become saturated (without excessive trial and error)?\\n\\n1.2. Our Contributions\\n\\nIn this paper, we take the first step to address these research gaps for few-shot image generation. Specifically, as our first contribution, we propose to independently analyze the quality and diversity during the adaptation. Using this analysis framework, we obtain insightful information on quality/diversity progression. In particular, on one hand, it is true that strong diversity preserving methods such as [33] indeed impede the progress of quality improvement. On the other hand, interestingly, we observe that these methods can still reach high quality rather quickly, and after quality converges they have no worse quality compared to other methods such as [47] which uses simple GAN loss (Eqn. 1). Therefore, methods with disproportionate focus on preserving diversity [33] stand out from the rests as they can produce slow diversity-degrading generators, maintaining good diversity of generated images when their quality reaches the convergence. Furthermore, our analysis reveals that there is still plenty of room to further slow down the diversity degradation across several source $\\\\rightarrow$ target domain setups.\\n\\nInformed by our analysis, our second contribution is to propose a novel strong regularization to take a further step in slowing down the diversity degradation, with the understanding that it is unlikely to compromise quality as observed in our analysis. Our proposed regularization is based on the observation that rich diversity exists in the source images at different semantic levels: diversity in middle levels such as hair style, face shape, and that in high levels such as facial expression (smile, grin, concentration). However, such source diversity can be easily ignored in the images produced by target domain generators. Therefore, to preserve source diversity information, we propose to maximize the mutual information (MI) between the source/target image features originated from the same latent code, via contrastive loss [34] (CL). To compute CL, we leverage the generator and discriminator as two feature encoders to extract image features.\"}"}
{"id": "CVPR-2022-19", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"at multiple feature scales, such that we can preserve diversity at various levels. By combining the two feature encoders (generator and discriminator), we gain additional feature diversity. We show that our proposed Dual Contrastive Learning (DCL) outperforms the previous work in slowing down the diversity degradation without compromising the image quality on the target domain, and hence achieving the state-of-the-art performance.\\n\\n2. Related Work\\n\\nConventional few-shot learning \\\\([10, 13, 38]\\\\) aims at learning a discriminative classifier \\\\([1, 16, 29, 38, 40]\\\\). In contrast, generative few-shot learning \\\\([28, 33, 49, 52]\\\\) often follows a transfer learning \\\\([35]\\\\) pipeline to adapt a pretrained GAN on a small target domain, without access to the source data. Specifically, our analysis and comparison in this paper mainly focus on the following recent baseline models:\\n\\n- **Transferring GAN (TGAN)** \\\\([47]\\\\): directly fine-tune all parameters of both the generator and the discriminator on the target domain;\\n- **Adaptive Data Augmentation (ADA)** \\\\([22]\\\\): apply data augmentation \\\\([41, 57, 58]\\\\) during adaptation which does not leak to the generator;\\n- **BSA** \\\\([32]\\\\): only update the learnable scale and shift parameters of the generator during adaptation;\\n- **FreezeD** \\\\([31]\\\\): freeze a few high-resolution layers of the discriminator, during the adaptation process;\\n- **MineGAN** \\\\([46]\\\\): use additional modules between the noise input and the generator. It aims at matching the target distribution before input to the generator;\\n- **Elastic Weight Consolidation (EWC)** \\\\([28]\\\\): apply EWC loss to regularize GAN, preventing the important weights from drastic changes during adaptation;\\n- **Cross-domain Correspondence (CDC)** \\\\([33]\\\\): preserve the distance between different instances in the source.\\n\\nWe firstly take a rigorous study of the above methods under the same setup. Then, motivated by our findings, we aim to address this few-shot image generation task.\\n\\nOur proposed method is related to contrastive learning \\\\([5, 8, 15, 17, 34, 55]\\\\). Contrastive learning for unsupervised instance discrimination aims at learning invariant embeddings with different transformation functions of the same object \\\\([17, 21, 30]\\\\), which can benefit the downstream tasks. To slow down the diversity degradation during few-shot GAN adaptation, we present a simple and novel method via contrastive loss \\\\([34, 48]\\\\): we hypothesize that, the same noise input could be mapped to fake images in the source and target domains with shared semantic information \\\\([37]\\\\). In experiments, we demonstrate convincing and competitive results of our approach for few-shot GAN adaptation, with sufficiently high quality and diversity.\\n\\n3. Preliminary\\n\\nGiven a GAN pretrained on the source domain, we denote the generator as \\\\(G_s\\\\). In the adaptation stage, the target generator \\\\(G_t\\\\) and the discriminator \\\\(D_t\\\\) can be obtained by fine-tuning the pretrained GAN on the target domain via Eqn. 1. We follow the settings of the prior work \\\\([33, 46, 47]\\\\): during adaptation, there is no access to the abundant source data, and we can only leverage the pretrained GAN and target data for transfer learning.\\n\\nFor the target data size, some recent work \\\\([22, 47]\\\\) applies more than 1,000 images in adaptation, and show satisfied results. In contrast, we focus on the challenging case: fine-tune the pretrained GAN with only few-shot (e.g., 10-shot) data.\\n\\n4. Revisit Few-Shot GAN Adaptation\\n\\n4.1. Motivation\\n\\nWhile a few recent works \\\\([28, 33]\\\\) propose different ideas, including to inherit the diversity during adaptation, to our knowledge, no effort attends to analyze the underlying mechanism of this problem. The specific issues concerning this study are as follows: will quality be compromised due to the disproportionate focus on diversity preserving? will there still be room to further improve via diversity preserving? We study these concerns from a novel perspective: compare different methods by visualizing and quantifying the few-shot adaptation process. In particular, we decouple their performance by evaluating the quality and diversity on the target domain separately, under a unified framework.\\n\\n4.2. Binary Classification for Quality Evaluation\\n\\nThe probability output of a classifier indicates the confidence of how likely an input sample belongs to a specific class \\\\([7, 19]\\\\). Therefore, we employ the probability output of a binary classifier to assess to what extent the generated images belong to the target domain \\\\([458]\\\\). In particular, we train a convolutional network \\\\(C\\\\) on two sets of real images (from source and target, excluded during adaptation). Then, we apply \\\\(C\\\\) to the synthetic images from the adapted generator \\\\(G_t\\\\) during adaptation. The soft output of \\\\(C\\\\) are \\\\(p_t\\\\), predicted probability of input belonging to the target domain, and \\\\((1 - p_t)\\\\), that of not belonging to the target domain. Therefore, we take \\\\(p_t\\\\) as an assessment of the realisticness of the synthetic images on the target domain:\\n\\n\\\\[\\np_t = \\\\mathbb{E}_{z \\\\sim p_z}(C(G_t(z)))\\n\\\\]\\n\\n\\\\(z\\\\) is a batch of random noise input (size=1,000) fixed during adaptation, which is a proxy to indicate the quality and realisticness evolution process of different methods.\\n\\n4.3. Intra-Cluster Diversity Evaluation\\n\\nTo evaluate the diversity, we introduce a Learned Perceptual Image Patch Similarity (LPIPS) \\\\([56]\\\\) based metric, intra-cluster LPIPS \\\\((\\\\text{intra-LPIPS})\\\\) \\\\([33]\\\\), to identify the similarity\"}"}
{"id": "CVPR-2022-19", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Analysis of existing few-shot GAN adaptation methods under a unified setup (see Sec. 4.4).\\n\\nObservation 1: While methods such as CDC [33] have disproportionate focus on diversity preserving which impedes quality improvement, all methods we have evaluated achieve similar quality/realisticness after convergence.\\n\\nObservation 2: Different methods exhibit substantially dissimilar diversity degrading rates.\\n\\nCombined the results in Figure 2a and 2b: since the achieved quality is similar, the better methods are those that can slow down diversity degradation. We summarize our findings in Sec. 4.5, which motivate our proposed method in Sec. 5. Best viewed in color.\\n\\nbetween the generated images and the few-shot target samples during adaptation. The standard LPIPS evaluates the perceptual distance between two images, and it is empirically shown to align with human judgements [56]. Intra-LPIPS is a variation of the standard LPIPS: We firstly generate abundant (size=1,000) images, then assign each of them to one of the $M$ target samples (with lowest standard LPIPS), and form $M$ clusters. The intra-LPIPS is obtained by computing the average standard LPIPS for random paired images within each cluster, then average over $M$ clusters. We provide the pseudo-code in Supp.\\n\\nIdeally, we hypothesize that the highest diversity knowledge is achieved by the generator pretrained on the large source domain, and it will degrade during the few-shot adaptation. In the worst case, the adapted generator simply replicates the target images, and intra-LPIPS will be zero. To justify our conjecture, we sample the target generator at different adaptation iterations, then apply intra-LPIPS to assess the diversity of the generated images.\\n\\n4.4. Experiment settings\\n\\nBasic setup: For all methods: We follow the previous work [28, 33] to use StyleGAN-V2 [24] with default hyperparameters. We use AlexNet [27] as the binary classifier. We fine-tune the pretrained GAN on the target domain with batch size 4 and 3,000 iterations, on a Tesla V100 GPU.\\n\\nSource $\\\\rightarrow$ target adaptation: Training the binary classifier requires rich data on both source and target domains. Therefore, we transfer the model pretrained on FFHQ [23] (70,000 images) to 10-shot samples from target domains that contain abundant data originally: Sketches, Babies, and Sunglasses (roughly 300, 2500, 2700 images, respectively). To obtain unbiased classifiers, we form each dataset by keeping the source and target training data balanced.\\n\\nEvaluation: The baseline methods for evaluation are introduced in Sec. 2. In this section, we do not include BSA [32] and MineGAN [46]: BSA applies a different GAN architecture (BigGAN [4]), and MineGAN employs additional training modules during adaptation, and it is time-consuming for training. We include the results of these two models in Sec. 6. To compute the intra-LPIPS, we follow [33] to use the base implementation from [56].\\n\\n4.5. Empirical Analysis Results\\n\\nThe analysis results are shown in Figure 1 and Figure 2. Generally, our observations can be stated as follows:\\n\\n\u2022 Image quality/realisticness (Observation 1): As shown in Figure 2a, strong diversity preserving methods such as CDC [33] (and ours, to be discussed in Sec. 5) indeed impede the progress of quality improvement, most noticeable in FFHQ $\\\\rightarrow$ Babies. This is because additional regularization for diversity preserving (e.g.}\\n\\nhttps://github.com/richzhang/PerceptualSimilarity\\n\\n1\"}"}
