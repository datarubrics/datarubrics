{"id": "CVPR-2023-811", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generalist: Decoupling Natural and Robust Generalization\\n\\nHongjun Wang\\nYisen Wang\\n\\n1 National Key Lab of General Artificial Intelligence\\nSchool of Intelligence Science and Technology, Peking University\\n2 Institute for Artificial Intelligence, Peking University\\n\\nAbstract\\nDeep neural networks obtained by standard training have been constantly plagued by adversarial examples. Although adversarial training demonstrates its capability to defend against adversarial examples, unfortunately, it leads to an inevitable drop in the natural generalization. To address the issue, we decouple the natural generalization and the robust generalization from joint training and formulate different training strategies for each one. Specifically, instead of minimizing a global loss on the expectation over these two generalization errors, we propose a bi-expert framework called Generalist where we simultaneously train base learners with task-aware strategies so that they can specialize in their own fields. The parameters of base learners are collected and combined to form a global learner at intervals during the training process. The global learner is then distributed to the base learners as initialized parameters for continued training. Theoretically, we prove that the risks of Generalist will get lower once the base learners are well trained. Extensive experiments verify the applicability of Generalist to achieve high accuracy on natural examples while maintaining considerable robustness to adversarial ones. Code is available at https://github.com/PKU-ML/Generalist.\\n\\n1. Introduction\\nModern deep learning techniques have achieved remarkable success in many fields, including computer vision [14, 16], natural language processing [10, 31], and speech recognition [28, 36]. Yet, deep neural networks (DNNs) suffer a catastrophic performance degradation by human imperceptible adversarial perturbations where wrong predictions are made with extremely high confidence [13, 29, 34]. The vulnerability of DNNs has led to the proposal of various defense approaches [3, 24, 25, 33, 40] for protecting DNNs from adversarial attacks. One of those representative techniques is adversarial training (AT) [20, 21, 37, 38], which dynamically injects perturbed examples that deceive the current model but preserve the right label into the training set. Adversarial training has been demonstrated to be the most effective method to improve adversarially robust generalization [2, 39].\\n\\nDespite these successes, such attempts of adversarial training have found a tradeoff between natural and robust accuracy, i.e., there exists an undesirable increase in the error on unperturbed images when the error on the worst-case perturbed images decreases, as illustrated in Figure 1. Prior works [30, 43] even argue that natural and robust accuracy are fundamentally at odds, which indicates that a robust classifier can be achieved only when compromising the natural generalization. However, the following works found that...\"}"}
{"id": "CVPR-2023-811", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the tradeoff may be settled in a roundabout way, such as incorporating additional labeled/unlabeled data [1, 8, 22, 26] or relaxing the magnitude of perturbations to generate suitable adversarial examples for better optimization [18, 44]. These works all focus on the data used for training while we propose to tackle the tradeoff problem from the perspective of the training paradigm in this paper.\\n\\nInspired by the spirit of the divide-and-conquer method, we decouple the objective function of adversarial training into two sub-tasks: one is used for natural example classification while the other one is used for adversarial example classification. Specifically, for each sub-task, we train a base learner on natural/adversarial datasets with the task-specific configuration while sharing the same model architecture. The parameters of base learners are collected and combined to form a global learner at intervals during the training process, which is then distributed to base learners as initialized parameters for continued training. We name the framework as **Generalist** whose proof-of-concept pipeline is shown in Figure 2. Different from the traditional joint training framework for natural and robust generalization, our proposed Generalist fully leverages task-specific information to individually train the base learners, which makes each sub-task to be solved better. Theoretically, we show that if the base learners are well trained, the final global learner is guaranteed to have a lower risk. Our proposed Generalist is the first to effectively address the tradeoff between natural and robust generalization by utilizing task-aware training strategies to achieve high clean accuracy in the natural setting, while also maintaining considerable robustness to the adversarial setting (as shown in Figure 1).\\n\\nIn summary, the main contributions are as follows:\\n\\n\u2022 For the tradeoff between natural and robust generalization, previous methods have struggled to find a sweet point to meet both goals in the joint training framework. Here, we propose a novel Generalist paradigm, which constructs multiple task-aware base learners to respectively achieve the generalization goal on natural and adversarial counterparts separately.\\n\\n\u2022 For each task, rather than being constricted in a stiff manner, every detail of the training strategies (e.g., optimization scheme) can be totally customized, thus each base learner can better explore the optimal trajectory in its field while the global learner can fully leverage the merits of all base learners.\\n\\n\u2022 We conduct extensive experiments in common settings against a wide range of adversarial attacks to demonstrate the effectiveness of our approach. Results show that our Generalist paradigm greatly improves both clean and robust accuracy on benchmark datasets compared to relevant techniques.\\n\\n2. Preliminaries and Related Work\\n\\nIn this section, we briefly introduce some relevant background knowledge and terminology about adversarial training and meta-learning.\\n\\n**Notations.** Consider an image classification task with input space \\\\( X \\\\) and output space \\\\( Y \\\\). Let \\\\( x \\\\in X \\\\subseteq \\\\mathbb{R}^d \\\\) denote a natural image and \\\\( y \\\\in Y = \\\\{1, 2, \\\\ldots, K\\\\} \\\\) denote the corresponding ground-truth label. The natural and adversarial datasets \\\\( X \\\\times Y = \\\\{(x_i, y_i)\\\\}_{i=1}^n \\\\) and \\\\( X' \\\\times Y = \\\\{(x'_i, y_i)\\\\}_{i=1}^n \\\\) are sampled from a distribution \\\\( D_1 \\\\) and \\\\( D_2 \\\\), respectively. We denote a DNN model as \\\\( f_{\\\\theta} : X \\\\rightarrow \\\\mathbb{R}^K \\\\) whose parameters are \\\\( \\\\theta \\\\in \\\\Theta \\\\), which should classify any input image into one of \\\\( K \\\\) classes. The objective functions \\\\( \\\\ell_1 \\\\) and \\\\( \\\\ell_2 \\\\) for the natural and adversarial setting can be defined as:\\n\\n\\\\[\\n\\\\ell_1(\\\\theta) = D_1 \\\\times \\\\Theta \\\\rightarrow [0, \\\\infty),\\n\\\\]\\n\\n\\\\[\\n\\\\ell_2(\\\\theta) = D_2 \\\\times \\\\Theta \\\\rightarrow [0, \\\\infty),\\n\\\\]\\n\\nwhich are usually positive, bounded, and upper-semi continuous [4, 6, 32].\\n\\n2.1. Standard Adversarial Training\\n\\nThe goal of the adversary is to generate a malignant example \\\\( x' \\\\) by adding an imperceptible perturbation \\\\( \\\\epsilon \\\\in \\\\mathbb{R}^d \\\\) to \\\\( x \\\\). And the generated adversarial example \\\\( x' \\\\) should be in the vicinity of \\\\( x \\\\) so that it looks visually similar to the original one. This neighbor region \\\\( B_\\\\epsilon(x) \\\\) anchored at \\\\( x \\\\) with apothem \\\\( \\\\epsilon \\\\) can be defined as \\\\( B_\\\\epsilon(x) = \\\\{(x', y) \\\\in D_2 | \\\\|x - x'\\\\|_\\\\infty \\\\leq \\\\epsilon\\\\} \\\\). For adversarial training, it first generates adversarial examples and then updates the parameters over these samples. The iteration process of adversarial training can be summed up as:\\n\\n\\\\[\\n\\\\begin{align*}\\n(x'(t+1) &= \\\\Pi_{B}(x, \\\\epsilon) x'\\\\left(t\\\\right) + \\\\alpha \\\\text{sign} \\\\nabla_{x'} \\\\ell_2(x', y; \\\\theta_t), \\\\\\\\\\n\\\\theta(t+1) &= \\\\theta(t) - \\\\tau \\\\nabla_{\\\\theta} E[\\\\ell_1(x, y; \\\\theta_t) + \\\\beta R(x', x, y; \\\\theta_t)],\\n\\\\end{align*}\\n\\\\]\"}"}
{"id": "CVPR-2023-811", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $\\\\alpha$ is the step size, $\\\\tau$ is the learning rate, and $\\\\beta$ when\\n\\n$\\\\beta$ when replacing $\\\\Pi$ with corresponding models $\\\\theta$ on dataset $\\\\{D \\\\alpha \\\\}_{\\\\forall \\\\alpha}$. Our approach Generalist\\n\\nsimilar to a physical-world Generalist who has broad\\n\\nknowledge across many topics and expertise in a few, our\\n\\n3. The Proposed New Framework: Generalist\\n\\nbase learners to transfer knowledge between tasks.\\n\\nmeta-learning $[12, 23]$ is often designed to generalize across\\n\\nunseen tasks, whereas the goal of MTL is to tackle a series\\n\\nmeta-learning while each assignment can be optimized by\\n\\nheterogeneous strategies in Generalist.\\n\\nform in MTL while each assignment can be optimized by\\n\\nthem tend to learn a specific predictive model for different\\n\\nis directly related to MTL at first glance because both of\\n\\ntrained using $\\\\theta$ to find optimal parameters\\n\\ntrivial pairwise intersections, and are trained in a joint model\\n\\ncontaining data distribution and loss function defined as\\n\\ndifferent models $[5, 19, 41]$. Consider a set of assignments\\n\\nimprove performance across tasks through joint training of\\n\\nMulti-Task Learning (MTL) is to\\n\\nMulti-Task Learning.\\n\\nThe formulation degenerates to standard natural\\n\\n$\\\\beta$ and when adversarial examples\\n\\nis the learning rate, and $\\\\tau$ is the projection operator,\\n\\n adversar-\"}"}
{"id": "CVPR-2023-811", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"distributed manner:\\n\\\\[\\n\\\\{ \\\\theta^*_{jn}, \\\\theta^*_{jr} \\\\} = \\\\arg\\\\min_{S^2_{WE}} \\\\left( \\\\sum_{W=1}^{n} \\\\theta_{WE} \\\\right)\\n\\\\]\\n\\nSpecifically, during the process, base learners \\\\( f^*_{jn} \\\\) and \\\\( f^*_{jr} \\\\) are assigned different subproblems that only requires accessing their own data distribution, respectively. Note that two base learners work in a complementary manner, meaning the update of parameters is independent among base learners and the global learner always collects parameters of both base learners. So the subproblem for each base learner is defined as:\\n\\n\\\\[\\n\\\\theta^*_{W} = \\\\arg\\\\min_{\\\\theta} Z^T_W \\\\left[ \\\\mathbb{E}_W \\\\left( \\\\nabla_{\\\\theta} \\\\ell_W(D_W; \\\\theta_W) \\\\right), \\\\tau_W \\\\right],\\n\\\\]\\n\\nwhere the task-aware optimizer \\\\( Z^T_W(\\\\cdot, \\\\cdot) \\\\) searches the optimal parameter states \\\\( \\\\theta^*_{W} \\\\) over the subproblem \\\\( W \\\\) in \\\\( T \\\\) rounds.\\n\\nLoss functions can also be task-specific and applied to each base learner separately. It is natural to consider minimizing the 0-1 loss in the natural and robust errors, however, solving the optimization problem is NP-hard thus computationally intractable. In practice, we select cross-entropy as the surrogate loss for both \\\\( \\\\ell_1 \\\\) and \\\\( \\\\ell_2 \\\\) since it is simple but good enough.\\n\\n### 3.3. Initialization from the Global Learner\\n\\nDuring the initial training periods, base learners are less instrumental since they are not adequately learned. Directly initializing parameters of base learners may mislead the training procedure and further accumulate bias when mixing them. Therefore, we set aside \\\\( t' \\\\) epochs from the beginning for fully training base learners and just aggregates states on the searching trajectory of base learners through optimization by exponential moving average (EMA), computed as:\\n\\n\\\\[\\n\\\\theta_g \\\\leftarrow \\\\alpha' \\\\theta_g + (1 - \\\\alpha') \\\\left( \\\\gamma \\\\theta_r + (1 - \\\\gamma) \\\\theta_t \\\\right),\\n\\\\]\\n\\nwhere \\\\( \\\\alpha' \\\\) is the exponential decay rates for EMA and \\\\( \\\\gamma \\\\) is the mixing ratio for base learners. They then learn an initialization from parameters of the global learner every \\\\( c \\\\) epochs when each base learner is well trained in its field. Thus, the optimization of each base learner for every interlude can be expressed in Eq. 6:\\n\\n\\\\[\\n\\\\theta^*_{W} = \\\\arg\\\\min_{\\\\theta} Z^c_W \\\\left[ \\\\mathbb{E}_W \\\\left( \\\\nabla_{\\\\theta} \\\\ell_W(D_W; \\\\theta_g) \\\\right), \\\\tau_W \\\\right].\\n\\\\]\\n\\nNote that \\\\( \\\\theta_g \\\\) contains both \\\\( \\\\theta_{jn} \\\\) and \\\\( \\\\theta_{jr} \\\\), meaning there always exists a term updated by gradient information of distribution different from the current subproblem. This mechanism enables fast learning within a given assignment and improves generalization, and the acceleration is applicable to the given assignment for its corresponding base learner only (proof in Appendix B.1).\\n\\nWith all discussed above, the learning progress of Generalist can be constructed by descending the gradient of \\\\( \\\\theta_r, \\\\theta_n \\\\) and mixing both of them. The calculating steps in Algorithm 1 can be summarized in Eq. 7.\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\theta^*_n &= Z^n_{E(x, y) \\\\sim D_1(\\\\nabla_{\\\\theta} \\\\ell_1(x, y; \\\\theta_{t-1}^n))}, \\\\\\\\\\n\\\\theta^*_r &= Z^r_{E(x', y) \\\\sim D_2(\\\\nabla_{\\\\theta} \\\\ell_2(x', y; \\\\theta_{t-1}^r))}, \\\\\\\\\\n\\\\theta^*_{t+1} &= \\\\alpha' \\\\theta_{t-1}^g + (1 - \\\\alpha') \\\\left( \\\\gamma \\\\theta^*_r + (1 - \\\\gamma) \\\\theta^*_n \\\\right),\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\( B(t, t', c) \\\\) is a Boolean function that returns one only when both \\\\( t \\\\geq t' \\\\) and \\\\( t \\\\mod c = 0 \\\\), otherwise it returns zero. \\\\( Z^n \\\\) and \\\\( Z^r \\\\) are optimizers for natural training and adversarial training assignments.\\n\\n### 3.4. Theoretical Analysis\\n\\nIn this part, we theoretically analyze how base learners help global learner in Generalist. For brevity, we omit the expectation notation over samples from each distribution without losing generalization.\\n\\n**Definition 1.** (Tradeoff Regret with Mixed Strategies) For the natural training assignment \\\\( a_1 \\\\) and adversarial training assignment \\\\( a_2 \\\\), consider an algorithm generates the trajectory of states \\\\( \\\\theta_1 \\\\) and \\\\( \\\\theta_2 \\\\) for two base learners, the regret of both base learners on its corresponding loss function \\\\( \\\\ell_1, \\\\ell_2 \\\\) is\\n\\n\\\\[\\nR_T = \\\\frac{1}{2} \\\\sum_{a=1}^{T} \\\\sum_{t=1}^{X} \\\\ell_a(\\\\theta_t^a) - \\\\inf_{\\\\theta_t^a \\\\in \\\\Theta} \\\\sum_{t=1}^{X} \\\\ell_a(\\\\theta_{t}^a)!\\n\\\\]\\n\\nThe last term obtains the oracle state \\\\( \\\\theta^*_a \\\\), theoretically optimal parameters for each task \\\\( a \\\\). \\\\( R_T \\\\) is the sum of the difference between the parameters of each base learner and the theoretically optimal parameters for each task. Based on the definition, we can give the following upper bound on the expected error of classifier trained by Generalist with respect to \\\\( R_T \\\\) as:\\n\\n**Theorem 1.** (Proof in Appendix B.2) Consider an algorithm with regret bound \\\\( R_T \\\\) that generates the trajectory of states for two base learners, for any parameter state \\\\( \\\\theta \\\\in \\\\Theta \\\\), given a sequence of convex surrogate evaluation functions \\\\( \\\\ell : \\\\Theta \\\\rightarrow [0, 1] \\\\) drawn i.i.d. from some distribution \\\\( L \\\\), the expected error of the global learner \\\\( \\\\theta_g \\\\) on both tasks over the test set can be bounded with probability at least \\\\( 1 - \\\\delta \\\\):\\n\\n\\\\[\\nE_{\\\\ell \\\\sim L} \\\\ell(\\\\theta_g) \\\\leq E_{\\\\ell \\\\sim L} \\\\ell(\\\\theta) + R_T + 2r_T \\\\log \\\\frac{1}{\\\\delta}.\\n\\\\]\"}"}
{"id": "CVPR-2023-811", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"will get lower once the error for each task becomes lower. In practice, we can apply customized learning rate strategies, optimizers, and weight averaging to guarantee the error reduction of each base learner.\\n\\n4. Experiments\\n\\nWe conduct a series of experiments on ResNet-18 [14] and WRN-32-10 [42] on benchmark datasets MNIST, SVHN, CIFAR-10, and CIFAR-100 under the $L_\\\\infty$ norm.\\n\\n**Baselines.** We select six approaches to compare with: AT using PGD ($\\\\beta = 1$ in Eq. 1) [20], AT using the half-half loss ($\\\\beta = 1/2$ in Eq. 1) [13], TRADES with different $\\\\lambda$ [43], Friendly Adversarial Training (FAT) [44], Interpolated Adversarial Training (IAT) [17], and Robust Self Training (RST) [26] used labeled data for fair comparison. For Generalist, we set $t' = 75$ and the optimal mixing strategy will be discussed in Section 4.2.2.\\n\\n**Evaluation.** To evaluate the robustness of the proposed method, we apply several adversarial attacks including PGD [20], MIM [11], CW [7], AutoAttack (AA) [9] and all its components ($\\\\text{APGD}_{ce}$, $\\\\text{APGD}_{dlr}$, $\\\\text{APGD}_t$, $\\\\text{FAB}_t$, and Square attacks).\\n\\n4.1. Tradeoff Performance on Benchmark Datasets\\n\\nTo comprehensively manifest the power of our Generalist method, we present the results of both ResNet-18 and WRN-32-10 on CIFAR-10 in Table 1.\\n\\nIn Table 1(a), Generalist consistently improves standard test error relative to models trained by several robust methods, while maintaining adversarial robustness at the same level. More specifically, Generalist achieves the second highest standard accuracy of 89.09% (only lower than 93.04% obtained by natural training (NT)), while meantime robust accuracy against AA is 46.07%, hanging on to 48.2% from TRADES. If we force TRADES to meet the same level of clean accuracy as Generalist (89%), the robustness of TRADES against APGD will drop to 30% (see TRADES in Appendix A.4), which is significantly worse than Generalist. That means it is hard to obtain acceptable robustness but maintain clean accuracy above 89% in the joint training framework even if it is equipped with an advanced loss function, while the improvement of Generalist is notable since we only use the naive cross-entropy loss. Contrary to FAT managing the tradeoff through adaptively decreasing the step size of PGD, which still hurts robustness a lot, Generalist is the only method with clean accuracy above 89% and robust accuracy against AA above 46%. We should emphasize the final obtained model of Generalist is the same size as other trained models are. For the training time, Generalist does perform both NT and naive AT but the cost of NT is negligible, so the overhead of Generalist is smaller than TRADES, and whatever serial and parallel versions of Generalist are even faster than TRADES (see Appendix A.4).\\n\\nThings become more obvious when it comes to WRN-32-10. In Table 1(b), the gap between test natural accuracy of Generalist and NT is reduced to 2.27%, a relative decrease of 3.65% in standard test error as compared to the second highest natural accuracy (except NT) achieved by FAT. It is also remarkable that the boost of accuracy does not hurt the robustness of Generalist, instead, Generalist even outperforms TRADES across multiple types of adversarial attacks. In particular, we find that Generalist has a standard test error of 6.7% while TRADES with $\\\\lambda = 6$ has a standard test error of 14.89% only. And the improved robustness of Generalist among PGD20/100, MIM, CW, FAT$^t$ and Square is conspicuous. Besides, the best performance on AA, which is an ensemble of different attacks and the most powerful adaptive adversarial attack so far, demonstrates the reliability of Generalist. Likewise, only Generalist attains robust accuracy of AA higher than 52% along with clean accuracy higher than 90%. It should be emphasized that these features confirm the practicability of Generalist. In short, Generalist has consistently improved robustness without loss of natural accuracy. More results on benchmark datasets of MNIST, SVHN, and CIFAR-100 are in Appendix A.2 - A.3.\\n\\n4.2. Comprehensive Understanding of Generalist\\n\\nWe run a number of ablations to analyze the Generalist framework in this part. As illustrated in Algorithm 1, two factors control the tradeoff between accuracy and robustness of the global learner: frequency of communication $c$ and mixing ratio $\\\\gamma$. Here, we investigate how these parameters affect performance. If not specified otherwise, the experiments are conducted on CIFAR-10 using ResNet-18.\\n\\n4.2.1 Mixing Strategies of $\\\\gamma$\\n\\nIn Generalist, $\\\\gamma$ controls the tradeoff via balancing the contribution of individuals to the global learner when base learners are gradually well trained. Note that $\\\\gamma$ is a scalar but we do not explicitly assign a fixed value to it. Instead, we set several breakpoints and dynamically adjust the value along the training process using a piecewise linear function to decrease.\\n\\nResults are shown in Figure 3. The numbers in brackets are the values at the 0/40/80/120-th epoch. If $\\\\gamma$ gets smaller, the base learner in charge of natural classification has a pronounced influence on the global learner. Among all configurations, the best one is to apply $\\\\gamma = (1, 1, 1, 0)$ and $c = 5$ to the global learner after the 75th epoch. When compared to strategies that $\\\\gamma$ decays during late periods, $\\\\gamma = (1, 1, 0, 0.8, 0.2)$ shows lower standard and robust accuracy, confirming that more sophisticated initialization could be useful for both accuracy and robustness. With the increase of the last breakpoint of dynamical strategies, the robust accuracy gradually increases; while the standard accuracy decreases.\"}"}
{"id": "CVPR-2023-811", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison of our algorithm with different training methods using ResNet-18 and WRN-32-10 on CIFAR-10. The maximum perturbation is $\\\\varepsilon = 8/255$. The best checkpoint is selected based on the tradeoff between clean accuracy and robust accuracy against PGD20 on the test set. We highlight the top two results on each task. We omit standard deviations of Generalist as they are very small (<0.5%).\\n\\nAverage accuracy rates (in %) have shown that the proposed Generalist method greatly mitigates the tradeoff of the model.\\n\\n(a) Evaluation results based on ResNet-18.\\n\\n| Method       | NAT | PGD20 | PGD100 | MIM | CW | APGDce | APGDdlr | APGDt | FAT | Square AA |\\n|--------------|-----|-------|--------|-----|----|--------|--------|-------|-----|-----------|\\n| NT           | 93.04 | 0.00  | 0.00   | 0.00 | 0.00 | 0.00   | 0.00   | 0.00  | 0.00 | 0.00      |\\n| AT ($\\\\beta = 1$) | 84.32 | 48.29 | 48.12  | 47.95 | 49.57 | 47.47  | 48.57  | 45.14 | 46.17 | 54.21     |\\n| AT ($\\\\beta = 1/2$) | 87.84 | 44.51 | 44.53  | 47.30 | 44.93 | 40.58  | 42.55  | 40.20 | 44.56 | 50.76     |\\n| TRADES ($\\\\lambda = 6$) | 83.91 | 54.25 | 52.21  | 55.65 | 52.22 | 53.47  | 50.89  | 48.23 | 48.53 | 55.75     |\\n| TRADES ($\\\\lambda = 1$) | 87.88 | 45.58 | 45.60  | 47.91 | 45.05 | 42.95  | 42.49  | 40.38 | 43.89 | 53.49     |\\n| FAT           | 87.72 | 46.69 | 46.81  | 47.03 | 49.66 | 46.20  | 47.51  | 44.88 | 45.76 | 52.98     |\\n| IAT           | 84.60 | 40.83 | 40.87  | 43.07 | 39.57 | 37.56  | 37.95  | 35.13 | 36.06 | 49.30     |\\n| RST           | 84.71 | 44.23 | 44.31  | 45.33 | 42.82 | 41.25  | 42.01  | 40.41 | 46.54 | 50.49     |\\n| Generalist    | 89.09 | 50.01 | 50.00  | 52.19 | 50.04 | 46.53  | 48.70  | 46.37 | 47.32 | 56.68     |\\n\\n(b) Evaluation results based on WRN-32-10.\\n\\n| Method       | NAT | PGD20 | PGD100 | MIM | CW | APGDce | APGDdlr | APGDt | FAT | Square AA |\\n|--------------|-----|-------|--------|-----|----|--------|--------|-------|-----|-----------|\\n| NT           | 93.30 | 0.01  | 0.02   | 0.05 | 0.00 | 0.00   | 0.00   | 0.87  | 0.28 | 0.00      |\\n| AT ($\\\\beta = 1$) | 87.32 | 49.01 | 48.83  | 48.25 | 52.80 | 48.83  | 49.00  | 46.34 | 48.17 | 54.26     |\\n| AT ($\\\\beta = 1/2$) | 89.27 | 48.95 | 48.86  | 51.35 | 49.56 | 45.98  | 47.66  | 44.89 | 46.42 | 56.83     |\\n| TRADES ($\\\\lambda = 6$) | 85.11 | 54.58 | 54.82  | 55.67 | 54.91 | 54.89  | 55.50  | 52.71 | 52.61 | 57.62     |\\n| TRADES ($\\\\lambda = 1$) | 87.20 | 51.33 | 51.65  | 52.47 | 53.19 | 51.60  | 51.88  | 49.97 | 50.01 | 54.83     |\\n| FAT           | 89.65 | 48.74 | 48.69  | 48.24 | 52.11 | 48.50  | 48.81  | 46.70 | 46.17 | 51.51     |\\n| IAT           | 87.93 | 50.55 | 50.72  | 52.37 | 48.71 | 47.71  | 46.55  | 43.84 | 45.78 | 56.52     |\\n| RST           | 87.27 | 46.55 | 46.76  | 47.02 | 45.99 | 45.73  | 46.58  | 45.78 | 43.18 | 52.44     |\\n| Generalist    | 91.03 | 56.88 | 56.92  | 58.87 | 57.23 | 53.94  | 55.80  | 53.00 | 53.65 | 63.10     |\\n\\n4.2.2 Communication Frequency\\n\\nIn Generalist, $c$ controls the communication frequency between the global learner and base learners. Therefore, for $c$, with the fixed mixing ratio strategy, we sweep over the frequency of communication from 1 to 15. Results are shown in Figure 3, and we have the following observations. Intuitively, a larger $c$ means base learners communicate with the global learner less frequently to get the initialization, so they barely have the opportunity to move alternately towards two optimal solution manifolds. But specifically, the natural accuracy falls back down after reaching the peak while the robust accuracy in different adversarial settings roughly shows a trough. Such observation manifests that too much/little communication has a negative influence on standard accuracy but results in relatively higher robustness. It captures a tradeoff between natural and robust errors with respect to $c$.\\n\\n4.2.3 Parameter Selection\\n\\nIn practice, it is natural to select the mixing parameter $\\\\gamma$ and the frequency of communication $c$ under a scenario without knowing the target model or dataset. We can find the best parameters on specific architecture and dataset, which is\"}"}
{"id": "CVPR-2023-811", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. (a) We apply weight averaging to one of the base learners or both of them. Results demonstrate that using weight averaging through training can bring performance boost in its corresponding sub-task, and thus has an effect on predictions of the global learner. (b) Base learners of Generalist optimized by different optimizers. The optimal selection is using Adam for the natural classification task but maintaining SGD for the adversarial one.\\n\\nThen transferred to others, i.e., choosing the best *\u03b3*, *c*, and their strategies on one model/dataset and then used for other models/datasets, which still works well. Specifically, for the experimental results on MNIST/SVHN/CIFAR100 shown in Appendix A.2 and A.3, we just find the optimal parameters and updating strategies on CIFAR10 and apply similar *\u03b3* and *c* on the target datasets and architectures without much fine-tuning. The performance is still very good.\\n\\n4.3. Customized Policies for Individuals\\n\\nAs stressed above, one of the major advantages of Generalist in comparison with the standard joint training framework is that each base learner enables to customize the corresponding strategy for their own tasks freely rather than using the same strategy for all tasks. In this part, we investigate whether Generalist performs better when cooperating with diverse techniques.\\n\\nWeight Averaging. Recent works [15,27,35] have shown that weight averaging (WA) greatly improves both natural and robust generalization. The average parameters of all history model snapshot through the training process to build an ensemble model on the fly. However, such technique cannot benefit both accuracy and robustness in the joint training framework. Therefore, we introduce WA into base learners separately. Results are shown in Figure 4 (a). We employ WA in either NT (NT_only) or AT (AT_only) or both of them (NT+AT). Overall, the results confirm that the performance of the global learner can be further improved after both base learners exploit WA. But unfortunately, an obvious tradeoff happens if only one of the base learner is equipped with WA. For instance, the standard test accuracy of NT_only continues to increase at the expense of the drop in the ability to defend attacks. A likely reason is that WA implicitly controls the learning speed of base learners. Indeed, the base learner with WA becomes an expert much faster than the one without WA in its sub-task, meaning the fast one is not in accordance with the slow one. This result is important because it not only illustrates the potential of Generalist comes from its base learners but also identifies a key challenge of tradeoff for future improvement.\\n\\nDifferent Optimizers. We also investigate the effect of optimizers designed for different tasks. We choose AT (\u03b2 = 1) using SGD with momentum and Adam for piecewise learning rate schedule optimized by joint training as the baseline. The initial learning rate for Adam is 0.0001. We alternately apply these two optimizers in each subproblem. The comparison of the results is shown in Figure 4 (b). We can see that the gap of robust accuracy between models adversarially trained by Adam and the ones trained by SGD is significant. All three schemes equipped with Adam, namely NT (Adam)+AT (Adam), NT (SGD)+AT (Adam), and Baseline (Adam), perform worse than the ones using SGD when evaluated by adversarial attacks. But on the other hand, by comparing the results of Baseline (Adam) and NT (Adam)+AT (SGD), it confirms a proper optimization scheme with respect to data distribution can effectively benefit the corresponding performance without overlooking the other. That not only demonstrates the necessity of Generalist to decouple task-aware assignments from joint training but also indicates using Adam may not be the principal reason for robustness drop. It is just ill-suited for the outer and inner optimization in AT. Besides, though the best results still come from using SGD, the learning rate for different tasks can be customized which is not feasible in the joint framework, as shown in Appendix A.5.\\n\\n4.4. Visualization\\n\\nConsidering the proposed method achieves impressive clean accuracy without a harsh drop in robustness, it is naturally to ask what improvements Generalist has secured in comparison with robust methods in detail. Thus, we further investigate the predictions that robust classifiers are prone to make. As shown in Figure 5, we provide two perspectives to analyze the differences that classifiers trained by different AT methods.\\n\\nTo broadly study the case, we perform experiments on NT, TRADES with different *\u03bb*, FAT and Generalist, then plot the distribution of the correct predictions of all methods.\"}"}
{"id": "CVPR-2023-811", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Analyses of predilections that different robust classifiers have on CIFAR-10 using ResNet-18. (a) Distribution of the correct predictions of different training methods for each class. We separate out results on natural examples from adversarial ones (AA). Note that results of \u2018NT Adv\u2019 does not appear in the figure just because they are literally zero. (b) Visualization of samples that other methods misclassify while Generalist makes right predictions.\\n\\nFor each class in Figure 5(a). As evident at first glance, we note that animals are more frequently misclassified, especially cats/dogs in the natural scenario and cats/deers in the adversarial scenario. In addition, the classifier trained by standard natural training does not always outperform the ones adversarially trained. Actually, they are equally skilled at most categories and the outcome is decided by specific categories (e.g. birds, cats and dogs). Generalist keeps pace with NT in the natural task, and meanwhile promotes the higher improvements in difficult items (e.g. cats and deers) against AA attack.\\n\\nIn Figure 5(b), we display specific samples in the testing dataset that are misclassified by robust classifiers (TRADES and FAT) but recognized by our proposed method, including both natural examples (the first two rows) and adversarial examples (the last two rows). Here, images shown in the first row are easy ones where the foreground objects stand out from the clear backgrounds, while hard samples are referred to those having confused objects with messy backgrounds. It is worth noting that TRADES delivers poor performances not only on hard examples with complex backgrounds or obscured objects but also on simple ones. For example, each image in the first row is typically plain and regular, however, TRADES fails in categorizing them into the right class. A plausible explanation for the issue is that TRADES lacks in a set of support measures specially devised for the natural classification task unlike Generalist does, highlighting design differentiation for sub-tasks is necessary.\\n\\nAnother interesting finding is that though both TRADES and FAT can build a robust classifier, they still rely on spurious background information and thus are easily deceived when encountering images with similar backgrounds but different objects. This phenomenon can be verified from the misclassification of the fourth and fifth images in the first row (taking white/blue backgrounds as evidence), and the fifth image in the fourth row (confused by the green background). But Generalist has the ability to sift the invariant feature of the foreground object while ignoring the background information spuriously correlated with the categories in both natural and adversarial settings. On the whole, Generalist demonstrates its strength to differentiate difficult samples close to the decision boundary and its potential to learn a background-invariant classifier.\\n\\n5. Conclusion\\nIn this paper, we propose a bi-expert framework named Generalist for improving the tradeoff issue between natural and robust generalization, which trains two base learners responsible for complementary fields and collects their parameters to construct a global learner. By decoupling from the joint training paradigm, each base learner can wield customized strategies based on data distribution. We provide theoretical analysis to justify the effectiveness of task-aware strategies and extensive experiments show that Generalist better mitigates the tradeoff of accuracy and robustness.\\n\\nAcknowledgement\\nYisen Wang is partially supported by the National Key R&D Program of China (2022ZD0160304), the National Natural Science Foundation of China (62006153), Open Research Projects of Zhejiang Lab (No. 2022RC0AB05), and Huawei Technologies Inc.\"}"}
{"id": "CVPR-2023-811", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, and Pushmeet Kohli. Are labels required for improving adversarial robustness? In NeurIPS, 2019.\\n\\n[2] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In ICML, 2018.\\n\\n[3] Yang Bai, Yan Feng, Yisen Wang, Tao Dai, Shu-Tao Xia, and Yong Jiang. Hilbert-based generative defense for adversarial examples. In ICCV, 2019.\\n\\n[4] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. In COLT, 2001.\\n\\n[5] Hakan Bilen and Andrea Vedaldi. Integrated perception with recurrent multi-task neural networks. In NeurIPS, 2016.\\n\\n[6] Jose H. Blanchet and Karthyek R. A. Murthy. Quantifying distributional model risk via optimal transport. Math. Oper. Res., 44(2):565\u2013600, 2019.\\n\\n[7] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In SP, 2017.\\n\\n[8] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C. Duchi, and Percy Liang. Unlabeled data improves adversarial robustness. In NeurIPS, 2019.\\n\\n[9] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In ICML, 2020.\\n\\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.\\n\\n[11] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In CVPR, 2018.\\n\\n[12] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017.\\n\\n[13] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2015.\\n\\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\n[15] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In UAI, 2018.\\n\\n[16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NeurIPS, 2012.\\n\\n[17] Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial training: Achieving robust neural networks without sacrificing too much accuracy. In ACM AISec Workshop, 2019.\\n\\n[18] Saehyung Lee, Hyungyu Lee, and Sungroh Yoon. Adversarial vertex mixup: Toward better adversarially robust generalization. In CVPR, 2020.\\n\\n[19] Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, and Rog\u00e9rio Schmidt Feris. Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification. In CVPR, 2017.\\n\\n[20] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.\\n\\n[21] Yichuan Mo, Dongxian Wu, Yifei Wang, Yiwen Guo, and Yisen Wang. When adversarial training meets vision transformers: Recipes from training to architecture. In NeurIPS, 2022.\\n\\n[22] Amir Najafi, Shin-ichi Maeda, Masanori Koyama, and Takeru Miyato. Robustness to adversarial perturbations in learning from incomplete data. In NeurIPS, 2019.\\n\\n[23] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. CoRR, abs/1803.02999, 2018.\\n\\n[24] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In SP, 2016.\\n\\n[25] Yao Qin, Nicholas Frosst, Sara Sabour, Colin Raffel, Garrison W. Cottrell, and Geoffrey E. Hinton. Detecting and diagnosing adversarial images with class-conditional capsule reconstructions. In ICLR, 2020.\\n\\n[26] Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C. Duchi, and Percy Liang. Understanding and mitigating the tradeoff between robustness and accuracy. In ICML, 2020.\\n\\n[27] Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A. Calian, Florentin Stimberg, Olivia Wiles, and Timothy A. Mann. Fixing data augmentation to improve adversarial robustness. CoRR, abs/2103.01946, 2021.\\n\\n[28] Hasim Sak, Andrew W. Senior, Kanishka Rao, and Fran\u00e7oise Beaufays. Fast and accurate recurrent neural network acoustic models for speech recognition. In INTERSPEECH, 2015.\\n\\n[29] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.\\n\\n[30] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In ICLR, 2019.\\n\\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\\n\\n[32] C. Villani. Topics in optimal transportation. 2003.\\n\\n[33] Hongjun Wang, Guanbin Li, Xiaobai Liu, and Liang Lin. A hamiltonian monte carlo method for probabilistic adversarial attack and learning. T-PAMI, 2020.\\n\\n[34] Hongjun Wang, Guangrun Wang, Ya Li, Dongyu Zhang, and Liang Lin. Transferable, controllable, and inconspicuous adversarial attacks on person re-identification with deep mis-ranking. In CVPR, 2020.\\n\\n[35] Hongjun Wang and Yisen Wang. Self-ensemble adversarial training for improved robustness. In ICLR, 2022.\\n\\n[36] Yisen Wang, Xuejiao Deng, Songbai Pu, and Zhiheng Huang. Residual convolutional ctc networks for automatic speech recognition. arXiv preprint arXiv:1702.07793, 2017.\"}"}
{"id": "CVPR-2023-811", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the convergence and robustness of adversarial training. In ICML, 2019.\\n\\nYisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In ICLR, 2020.\\n\\nDongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. In NeurIPS, 2020.\\n\\nWeilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. In NDSS, 2018.\\n\\nYongxin Yang and Timothy M. Hospedales. Deep multi-task representation learning: A tensor factorisation approach. In ICLR, 2017.\\n\\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016.\\n\\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan. Theoretically principled trade-off between robustness and accuracy. In ICML, 2019.\\n\\nJingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan S. Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In ICML, 2020.\"}"}
