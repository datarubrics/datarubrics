{"id": "CVPR-2023-744", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In Proc. CVPR, 2017.\\n\\n[2] Long Chen, Hanwang Zhang, Jun Xiao, Wei Liu, and Shih-Fu Chang. Zero-shot visual recognition using semantics-preserving adversarial embedding networks. In Proc. CVPR, 2018.\\n\\n[3] Zhenfang Chen, Lin Ma, Wenhan Luo, and Kwan-Yee Kynneth Wong. Weakly-supervised spatio-temporally grounding natural sentence in video. In Proc. ACL, 2019.\\n\\n[4] Anoop Cherian, Chiori Hori, Tim K Marks, and Jonathan Le Roux. (2.5+1) d spatio-temporal scene graphs for video question answering. In Proc. AAAI, 2022.\\n\\n[5] Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang, and Xun Wang. Dual encoding for zero-example video retrieval. In Proc. CVPR, 2019.\\n\\n[6] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proc. ICML, 2017.\\n\\n[7] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In Proc. ECCV, 2020.\\n\\n[8] Carolina Galleguillos, Andrew Rabinovich, and Serge Belongie. Object categorization using co-occurrence, location and appearance. In Proc. CVPR, 2008.\\n\\n[9] Kaifeng Gao, Long Chen, Yifeng Huang, and Jun Xiao. Video relation detection via tracklet based visual transformer. In Proc. ACM MM, 2021.\\n\\n[10] Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. Modeling relationships in referential expressions with compositional modular networks. In Proc. CVPR, 2017.\\n\\n[11] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos Niebles. Action genome: Actions as compositions of spatio-temporal scene graphs. In Proc. CVPR, 2020.\\n\\n[12] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In Proc. CVPR, 2015.\\n\\n[13] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, Proc. ICLR, 2015.\\n\\n[14] Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, et al. Siamese neural networks for one-shot image recognition. In Proc. ICML-W, 2015.\\n\\n[15] Ranjay Krishna, Ines Chami, Michael Bernstein, and Li Fei-Fei. Referring relationships. In Proc. CVPR, 2018.\\n\\n[16] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 123:32\u201373, 2017.\\n\\n[17] Yicong Li, Xun Yang, Xindi Shang, and Tat-Seng Chua. Interventional video relation detection. In Proc. ACM MM, 2021.\\n\\n[18] Yiming Li, Xiaoshan Yang, and Changsheng Xu. Dynamic scene graph generation via anticipatory pre-training. In Proc. CVPR, 2022.\\n\\n[19] Kongming Liang, Yuhong Guo, Hong Chang, and Xilin Chen. Visual relationship detection with deep structural ranking. In Proc. AAAI, 2018.\\n\\n[20] Pan Lu, Lei Ji, Wei Zhang, Nan Duan, Ming Zhou, and Jianyong Wang. R-vqa: learning visual relation facts with semantic attention for visual question answering. In Proc. SIGKDD, 2018.\\n\\n[21] Xinyu Lyu, Lianli Gao, Yuyu Guo, Zhou Zhao, Hao Huang, Heng Tao Shen, and Jingkuan Song. Fine-grained predicates learning for scene graph generation. In Proc. CVPR, 2022.\\n\\n[22] Judea Pearl. Fusion, propagation, and structuring in belief networks. Artif. Intell., 29(3):241\u2013288, 1986.\\n\\n[23] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proc. ICLR, 2017.\\n\\n[24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In Proc. NeurIPS, 2015.\\n\\n[25] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 115:211\u2013252, 2014.\\n\\n[26] Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun Yang, and Tat-Seng Chua. Annotating objects and relations in user-generated videos. In ICMR, 2019.\\n\\n[27] Xindi Shang, Tongwei Ren, Jingfan Guo, Hanwang Zhang, and Tat-Seng Chua. Video visual relation detection. In Proc. ACM MM, 2017.\\n\\n[28] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Proc. NeurIPS, 2017.\\n\\n[29] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. In Proc. CVPR, 2018.\\n\\n[30] Revant Teotia, Vaibhav Mishra, Mayank Maheshwari, and Anand Mishra. Few-shot visual relationship co-localization. In Proc. ICCV, 2021.\\n\\n[31] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Commun. ACM, 59:64\u201373, 2015.\\n\\n[32] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Proc. NeurIPS, 2016.\\n\\n[33] Weitao Wang, Meng Wang, Sen Wang, Guodong Long, Lina Yao, Guilin Qi, and Yang Chen. One-shot learning for long-tail visual relation detection. In Proc. AAAI, 2020.\\n\\n[34] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In Proc. ICIP, 2017.\\n\\n[35] Junbin Xiao, Xindi Shang, Xun Yang, Sheng Tang, and Tat-Seng Chua. Visual relation grounding in videos. In Proc. ECCV, 2020.\"}"}
{"id": "CVPR-2023-744", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bangpeng Yao and Li Fei-Fei. Modeling mutual context of object and human pose in human-object interaction activities. In Proc. CVPR, 2010.\\n\\nTing Yao, Yingwei Pan, Yehao Li, and Tao Mei. Exploring visual relationship for image captioning. In Proc. ECCV, 2018.\\n\\nYibing Zhan, Jun Yu, Ting Yu, and Dacheng Tao. On exploring undetermined relationships for visual relationship detection. In Proc. CVPR, 2019.\\n\\nHanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and Tat-Seng Chua. Visual translation embedding network for visual relation detection. In Proc. CVPR, 2017.\\n\\nSipeng Zheng, Shizhe Chen, and Qin Jin. Vrdformer: End-to-end video visual relation detection with transformers. In Proc. CVPR, 2022.\\n\\nDimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Cross-task weakly supervised learning from instructional videos. In Proc. CVPR, 2019.\"}"}
{"id": "CVPR-2023-744", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Few-Shot Referring Relationships in Videos\\n\\nYogesh Kumar and Anand Mishra\\n\\nIndian Institute of Technology Jodhpur\\n\\nkumar.204, mishra@iitj.ac.in\\n\\nhttps://vl2g.github.io/projects/refRelations/\\n\\nFigure 1. The proposed problem setup. (a) Given a query visual relationship as \\\\(<\\\\text{subject}, \\\\text{predicate}, \\\\text{object}>\\\\) and a test video, our goal is to localize the subject and object on the test video using a support set containing a few videos sharing the same predicate. In this example, the goal is to spatiotemporally localize the \\\\(\\\\text{plane}\\\\) (subject), and \\\\(\\\\text{person}\\\\) (object) that are connected via \\\\(\\\\text{fly above}\\\\) (predicate), using a support set containing only four videos sharing predicate \\\\(\\\\text{fly above}\\\\). It should be noted here that \\\\(\\\\text{fly above}\\\\) is unseen during training. We refer to this problem as few-shot referring relationship in videos. This problem setup is inspired by the real-world scenario where obtaining large-scale annotations for every visual relationship is practically infeasible. As shown in (b), a popular visual relationship video dataset, namely ImageNet-VidVRD [27], contains many predicates with very few examples, i.e., it has long-tail distribution. Further, as shown in (c), the success of a recent visual relationship localization technique (vRGV) [35] is clearly proportional to predicate distribution in the train set. This calls for solving referring relationship tasks in a few-shot setup. We propose this task and present a novel principled solution.\\n\\nAbstract\\n\\nInterpreting visual relationships is a core aspect of comprehensive video understanding. Given a query visual relationship as \\\\(<\\\\text{subject}, \\\\text{predicate}, \\\\text{object}>\\\\) and a test video, our objective is to localize the subject and object that are connected via the predicate. Given modern visio-lingual understanding capabilities, solving this problem is achievable, provided that there are large-scale annotated training examples available. However, annotating for every combination of subject, object, and predicate is cumbersome, expensive, and possibly infeasible. Therefore, there is a need for models that can learn to spatially and temporally localize subjects and objects that are connected via an unseen predicate using only a few support set videos sharing the common predicate. We address this challenging problem, referred to as few-shot referring relationships in videos for the first time. To this end, we pose the problem as a minimization of an objective function defined over a \\\\(T\\\\)-partite random field. Here, the vertices of the random field correspond to candidate bounding boxes for the subject and object, and \\\\(T\\\\) represents the number of frames in the test video. This objective function is composed of frame-level and visual relationship similarity potentials. To learn these potentials, we use a relation network that takes query-conditioned translational relationship embedding as inputs and is meta-trained using support set videos in an episodic manner. Further, the objective function is minimized using...\"}"}
{"id": "CVPR-2023-744", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a belief propagation-based message passing on the random field to obtain the spatiotemporal localization or subject and object trajectories. We perform extensive experiments using two public benchmarks, namely ImageNet-VidVRD and VidOR, and compare the proposed approach with competitive baselines to assess its efficacy.\\n\\n1. Introduction\\n\\nConsider the following problem: given a video, a visual relationship query represented as a \\\\(<\\\\text{subject}, \\\\text{predicate}, \\\\text{object}>\\\\) tuple, and a support set of a few videos containing the same predicate but not necessarily the same subjects and objects, our objective is to spatially and temporally localize both subjects and objects that are related via the predicate within the video. We refer to this problem as Few-shot Referring Relationship and illustrate it in Figure 1.\\n\\nSolving this problem has the potential to benefit cross-task video understanding [41] and video retrieval [5, 7], among other applications. Identifying its utility, referring relationship task for images has been first introduced by [15]. However, referring relationships in videos pose additional video-specific challenges, such as understanding dynamic visual relationships. Some of these challenges have been addressed in recent research by Xiao et al. [35], but with a reliance on strong supervision. Referring relationships in videos within a few-shot setup is an under-explored area. We aim to fill this research gap via our work.\\n\\nVisual relationships inherently have long-tail distributions in any video collection. For example, Image-Net VidVRD [27] dataset includes approximately 18.9% predicates with more than 100 instances but 20.5% predicates with less than 10 instances. This phenomenon is also shown in Figure 1, where most predicates belong to the tail side of the distribution. The methods that work best for frequent visual relationships do not necessarily generalize well to unseen visual relationships. Moreover, in a real-world scenario annotating visual relationships for each combination of subject, object, and predicate are cumbersome, expensive, and possibly infeasible. Therefore, there is a need to study visual relationship tasks in a few-shot setup. For instance: only with a few examples of the \\\\(<\\\\text{fly above}>\\\\) predicate, such as \\\\(<\\\\text{bird, fly above, person}>\\\\), \\\\(<\\\\text{helicopter, fly above, train}>\\\\) as shown in Figure 1 (a), a model should be able to generalize to the unseen visual relationship, such as \\\\(<\\\\text{plane, fly above, person}>\\\\). We propose a solution for Few-shot Referring Relationship in videos in this work.\\n\\nWe pose the problem of a few-shot referring relationship in the video as a minimization of an objective function defined over a $T$-partite random field where $T$ is the number of frames in the test video. Furthermore, the vertices of the random field are treated as random variables and represent candidate bounding boxes for the subject and objects. The objective function consists of frame-level potentials and visual relationship similarity potentials, both of which are learned using a relation network that takes query-conditioned translational relationship embeddings as inputs. We meta-train the relation network using support set videos in an episodic manner. Further, the objective function is minimized using a belief propagation-based message passing on the random field to obtain subject and object trajectories. We perform extensive experiments on two public benchmarks, namely ImageNet-VidVRD [27] and VidOR [31], and report the accuracy of localizing subject, object, and relation, denoted by $A_{\\\\text{sub}}$, $A_{\\\\text{obj}}$, and $A_{\\\\text{r}}$, respectively, along with other popular measures used in the literature. Our proposed approach clearly outperforms the related baselines.\\n\\nThe contributions of this work are three folds. (i) We propose a novel problem setup for referring relationship in videos, where the model must learn to localize the subject and object corresponding to a query visual relationship that was unseen during training using only a few support videos. (ii) We propose a new formulation to solve this task based on the minimization of an objective function on $T$-partite random field where $T$ is the number of frames in the test video, and the vertices of the random field representing potential bounding boxes for subject and objects correspond to the random variables. (Section 3.1). (iii) Additionally, to enrich query-conditioned relational embeddings, we present two aggregation techniques, namely global semantic and local localization aggregations. The use of these aggregation techniques results in enhanced relationship representations, which helps to obtain better trajectories for objects and subjects related via the query visual relationship. This is evidenced by extensive experiments and ablations. (Sections 3.2 and 4.5).\\n\\n2. Related Work\\n\\nVisual Relationships:\\n\\nInterpreting visual relationships in images [19, 21, 38] as well as videos [4, 9, 17, 18, 40] have gained huge attention over the last few years. They have also been key components of large-scale popular datasets such as Visual Genome [16] and Action Genome [11]. Visual relationships were studied with respect to object segmentation to leverage spatial relations [8] and to understand human-object interactions [36] via human-centric relationships. Krishna et al. [16] proposed the concept of scene graphs by combining multiple visual relationships in a graph structure. The structured representation of the scene graph is exploited for several tasks, including image retrieval [12]. Shang et al. [27] extended scene graphs from images to videos. Videos have spatiotemporal nature introducing dynamic relations that are not present in images. Scene graphs in videos represent fine-grain information that helps in the downstream task for spatiotemporal reasoning.\"}"}
{"id": "CVPR-2023-744", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Several works have been introduced that utilize visual relationships to improve downstream tasks such as image or video captioning [37], visual grounding [10], visual question answering [20]. To localize subjects and objects connected via a query visual relationship, Krishna et al. [15] proposed referring relationship for images. Their proposed method used an iterative message-passing mechanism between subject and object using language priors to ground the query relations. Further, Xiao et al. [35] extended referring relationship to videos and refer it as visual relationship grounding in videos. In a similar spirit, but, in a few-shot set-up, we propose to spatiotemporally localize subjects and objects in a video for a given relationship.\\n\\nFew-Shot Learning: Few-shot learning in the literature can be grouped into: (i) metric-based [14, 28, 32] and (ii) model-based [6, 23, 29] methods. We limit our discussion to only closely related works. Vinyals et al. [32] proposed matching networks that learn to compare using a small support set. Further, Sung et al. [29] proposed a relation network that learns from a few labeled images of the support set by comparing the query image. In this work, we have used a relation network to learn relationship similarity using the given support set videos, and this enabled us to obtain both frame-level and visual relationship similarity potentials.\\n\\n3. Proposed Method\\n\\nTask Definition: Given an unseen visual relationship consisting of subject (s), predicate (p), and object (o), i.e. \\\\( r = <s, p, o> \\\\) as a query and a test video \\\\( v \\\\) along with a small support set of \\\\( K \\\\) videos \\\\( S_{sup} = \\\\{ <s_i, p, o_i>, v_i \\\\}_{K=1} \\\\) containing the same predicate \\\\( p \\\\), the goal is to obtain the sequence of bounding boxes (also known as trajectories) \\\\( T^*_s \\\\) and \\\\( T^*_o \\\\) corresponding to subject and object, respectively on the test video \\\\( v \\\\). We refer to this task as few-shot referring relationship in videos. As an example, in Figure 1 (a), \\\\( r = <\\\\text{plane}, \\\\text{fly above}, \\\\text{person}> \\\\) is a query that needs to be spatiotemporally localized on a test video using a support set that contains four videos of \\\\( \\\\text{fly above} \\\\) predicate. We pose this task as an optimization problem on a \\\\( T \\\\)-partite random field which we describe next.\\n\\n3.1. Few-shot Referring Relationship as an optimization on a \\\\( T \\\\)-partite random field\\n\\nGiven a test video \\\\( v \\\\), we split it into \\\\( T \\\\) frames. Then, we obtain \\\\( M \\\\) most confident object bounding boxes on each frame using FasterRCNN [24]. Video \\\\( v \\\\) can be represented as a sequence of extracted bounding boxes, \\\\( v = \\\\{ B_{ji} | i \\\\in [1, T], j \\\\in [1, M] \\\\} \\\\). The pair of these bounding boxes in each frame is a candidate solution for referring relationship task. While finding the optimal solution using a brute force technique is combinatorial and practically infeasible, we solve it using optimization on a \\\\( T \\\\)-partite random field. To this end, we construct a \\\\( T \\\\)-partite graph \\\\( G = (V, E) \\\\) representing the test video as follows: for a frame \\\\( i \\\\), we represent each pair of \\\\( M \\\\) bounding boxes as nodes and their all possible next and previous-frame connections as edges. More precisely, the set of vertices \\\\( V \\\\) for a video \\\\( v \\\\) contains ordered pair of bounding boxes as follows: \\\\( \\\\{ u_{jk}^i = (B_j^i, B_k^i) : i \\\\in [1, T]; j, k \\\\in [1, M]; j \\\\neq k \\\\} \\\\), and the set of edges is defined as \\\\( E = \\\\{ (u_{jk}^i, u_{jk}^{i+1}) : i \\\\in [1, T-1]; j, k \\\\in [1, M]; j \\\\neq k \\\\} \\\\). Each vertex \\\\( u_{jk}^i \\\\) of this graph is a binary random variable that takes one of two labels \\\\{ select (1), reject (0) \\\\} for the given query \\\\( r \\\\). Figure 2 illustrates the construction of graph \\\\( G \\\\) for a three-frame test video. It should be noted here that the selected nodes (corresponding subject and object bounding box) from each frame form subject and object trajectories. To obtain optimal subject and object trajectories \\\\( T^*_s \\\\) and \\\\( T^*_o \\\\) for a given visual relationship \\\\( r \\\\), we solve following optimization problem:\\n\\n\\\\[\\nT^*_s, T^*_o = \\\\arg \\\\min_{\\\\theta, i, j, k} \\\\sum_{i=1}^{X} \\\\sum_{j, k}^{X} \\\\left[ \\\\Psi_i(u_{jk}^i, r, \\\\theta) + \\\\sum_{l \\\\in n(i)} \\\\Psi_{il}(u_{jk}^i, u_{jk}^l, r, \\\\theta) \\\\right]\\n\\\\]\\n\\nHere, \\\\( n(i) \\\\) represents the neighboring nodes to frame \\\\( i \\\\), and \\\\( \\\\theta \\\\) is a learnable parameter of a neural network that needs to be trained using support set videos. Specifically, we use support set videos to meta-train relation network \\\\( R_\\\\theta \\\\) in a few-shot way. We describe the relation network used in our framework and its training strategy in detail in Section 3.3. Further, in the aforementioned objective function, the terms \\\\( \\\\Psi_i \\\\) and \\\\( \\\\Psi_{il} \\\\) denote frame-level and visual relationship similarity potentials that are defined next.\\n\\n3.1.1 Frame-level Potentials \\\\( \\\\Psi_i \\\\)\\n\\nWe compute frame-level potentials \\\\( \\\\Psi_i(u_{jk}^i, r, \\\\theta) \\\\) such that the cost of selecting a node to form a trajectory is low if the selected node is semantically similar to query predicate \\\\( r \\\\), otherwise high. Mathematically,\\n\\n\\\\[\\n\\\\Psi_i(u_{jk}^i, r, \\\\theta) = -P \\\\sum_{m=1}^{K} R_\\\\theta(f_r(u_{jk}^i), f_r(u_{jk}^i_m))\\n\\\\]\\n\\nwhere, \\\\( u_{jk}^i_m \\\\) is the relationship pair in the \\\\( m \\\\)th support set video connected via query predicate \\\\( r \\\\) and \\\\( f_r(u_{jk}^i) \\\\) is a visual relationship embedding in video \\\\( v \\\\) for \\\\( j \\\\)th and \\\\( k \\\\)th bounding boxes of \\\\( i \\\\)th frame. Thus, \\\\( \\\\Psi_i(u_{jk}^i, r, \\\\theta) \\\\) gives the negative of average similarity of node \\\\( u_{jk}^i \\\\) with respect to the relationship of \\\\( S_{sup} \\\\) having predicate as \\\\( r \\\\). The \\\\( R_\\\\theta \\\\) returns a value closer to 1 when it represents visually and semantically similar visual relationships, and closer to 0, otherwise.\"}"}
{"id": "CVPR-2023-744", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Frame 1 Frame 2 Frame 3\\n\\nFigure 2. Illustration of T-partite random field. Here, we illustrate graph construction using three frames (T = 3) and three object bounding boxes (M = 3). The ordered pair of bounding boxes are represented as a node. For this example, each frame will result in $2 \\\\times 3^2 = 6$ nodes. Each node corresponds to a binary random variable ($u_{jk}$) that takes one of the two labels, i.e. select (1), reject (0).\\n\\nThe green- and red-filled nodes correspond to those random variables that the optimization framework aims to assign 1 and 0, respectively, and the dotted blue line indicates the ground truth tracklet.\\n\\n3.1.2 Visual Relationship Similarity Potentials ($\\\\Psi_{il}$)\\n\\nTo ensure that coherent (both semantically and visually) visual relationships are being selected across frames, we define visual relationship similarity potential such that the cost of selecting a similar visual relationship in the optimal tracklet is low. To this end, the visual relationship similarity potential $\\\\Psi_{il}(u_{jk}, u_{jk}, \\\\theta)$ is computed as follows:\\n\\n$$\\n\\\\Psi_{il}(u_{jk}, u_{jk}, \\\\theta) = -R_{\\\\theta} f_{r}(u_{jk}), f_{r}(u_{jk}),\\n$$\\n\\nwhere $f_{r}(u_{jk})$ is a visual relationship embedding in the video $v$ for $j$th and $k$th bounding boxes of $i$th frame.\\n\\nThe computation of these relationship embeddings and details of the relation network follows next.\\n\\n3.2. Query-conditioned Relationship Embedding\\n\\nWe describe the representation of a frame-level visual relationship in this section. To this end, we first obtain $M$ extracted objects for each frame of the video using Faster-RCNN [24]. Then, given a query relationship $r = <s, p, o>$, for each frame, an object or subject representation corresponding to $j$th and $k$th bounding box of $i$th frame respectively are obtained by the following equations:\\n\\n$$\\nf_{s}(B_{ij}) = W_{s2} \\\\text{ReLU}(W_{s1}[f_{sapp}(B_{ij}); f_{spra}(B_{ij}); G(s)])$$\\n\\n$$\\nf_{o}(B_{ij}) = W_{o2} \\\\text{ReLU}(W_{o1}[f_{oapp}(B_{ij}); f_{ospa}(B_{ij}); G(o)])$$\\n\\nHere, $W_{s1}, W_{s2}, W_{o1}$ and $W_{o2}$ are learnable parameters. In addition, the variables $f_{sapp} \\\\in \\\\mathbb{R}^{2048}$ and $f_{spra} \\\\in \\\\mathbb{R}^{4}$ represent the ROI appearance and spatial features of the corresponding bounding boxes, while $G(s) \\\\in \\\\mathbb{R}^{300}$ and $G(o) \\\\in \\\\mathbb{R}^{300}$ represent the GloVe word embeddings for the subject and object, respectively. Moreover, $[\\\\cdot; \\\\cdot; \\\\cdot]$ denotes the concatenation operation.\\n\\nFrom here onwards, for the sake of simplicity of notations, we represent $f_{s}(B_{ij})$ and $f_{o}(B_{ij})$ as $f_{s}$ and $f_{o}$ respectively. It should be noted that $f_{s}$ and $f_{o}$ can be used directly for obtaining relationship embeddings that can be used to compute frame-level and visual relationship similarity potentials. However, to further enrich these representations, we present the following two attention-based aggregation techniques:\\n\\n(i) Global Semantic Aggregation (GSA):\\n\\nThe subject and object representations, i.e. $f_{s}$ and $f_{o}$ learned using eq. (4) and (5) are independent of other frames in the video. However, the global semantic context information from other frames may help in enriching subject and object representation. Therefore, we fused the $I_{3D}$-features [1] of every frame by weighting them with a global attention vector $\\\\alpha_{g}$ which is computed as follows:\\n\\n$$\\n\\\\alpha_{g} = G_{\\\\text{Att}}(f_{I_{3D}}, G(s)).\\n$$\\n\\nWhere $G_{\\\\text{Att}}$ is a learnable attention unit, and it is defined as follows:\\n\\n$$\\ns_{g}j = W_{gs1} \\\\text{ReLU}(W_{gs2}[f_{jI_{3D}}; G(s)] + b_{gs})$$\\n\\n$$\\n\\\\alpha_{g}j = \\\\text{softmax}(s_{g}j),\\n$$\\n\\nwhere, $W_{gs1}, W_{gs2}$ and $b_{gs}$ are learnable parameters and $f_{jI_{3D}}$ represent the $I_{3D}$-feature for $j$th frame. Finally, after aggregating the global semantic information the subject representation is obtained as:\\n\\n$$\\nf_{g}s = \\\\sum_{j=1}^{T} (\\\\alpha_{g}j \\\\odot f_{jI_{3D}}) \\\\cdot f_{s}.$$\\n\\nSimilarly, object representation after global semantic aggregation is obtained as:\\n\\n$$\\nf_{g}o = \\\\sum_{j=1}^{T} (\\\\alpha_{g}j \\\\odot f_{jI_{3D}}) \\\\cdot f_{o}.$$\"}"}
{"id": "CVPR-2023-744", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here, \\\\( f_s \\\\) and \\\\( f_o \\\\) are initial subject and object features obtained using eq. (4) and (5), respectively.\\n\\n(ii) Local Localization Aggregation (LLA):\\n\\nFor adding the context from the adjacent frames as a local localization context, we considered a window size of five, i.e., for the \\\\( i^{th} \\\\) frame, we considered \\\\( i-2 \\\\) to \\\\( i+2 \\\\) frames. The local context helps with partially visible or occluded objects. We fuse the local spatial and ROI features from the adjacent frames weighted by the local attention vector for the subject \\\\( (\\\\alpha_l) \\\\) which is computed as follows:\\n\\n\\\\[\\n\\\\alpha_l = \\\\text{LAtt}(t - 2 \\\\leq t \\\\leq 2 l, f_s).\\n\\\\]\\n\\nHere, \\\\( f_s \\\\) is the initial representation of subjects obtained from eq. (4) and \\\\( f_{-2 \\\\leq t \\\\leq 2} \\\\) is obtained as:\\n\\n\\\\[\\nf_{-2 \\\\leq t \\\\leq 2} = \\\\text{Wls}2 \\\\text{ReLU}(\\\\text{Wls1}[(f_{t-2} - f_{t+1}) \\\\cdot (f_{t+2} - f_{t+3})]).\\n\\\\]\\n\\nWhere, \\\\( f_{t-2} \\\\) to \\\\( f_{t+2} \\\\) are stacked ROI and spatial features of all objects from their respective frame numbers. \\\\( \\\\text{Wls1} \\\\), \\\\( \\\\text{Wls2} \\\\), \\\\( \\\\text{Wls3} \\\\) and \\\\( \\\\text{Wls4} \\\\) are learnable parameters. Then, from eqs. (7) to (10), local localization information-aggregated subject and object features, \\\\( f_l \\\\) and \\\\( f_l \\\\), are obtained.\\n\\nObtaining Relationship Embedding:\\n\\nTo obtain relationship embedding, we first enrich subject and object representations as follows:\\n\\n\\\\[\\nf_s = \\\\text{Wsr1ReLU}(\\\\text{Wsr2}[f_g; f_l]),\\n\\\\]\\n\\n\\\\[\\nf_o = \\\\text{Wor1ReLU}(\\\\text{Wor2}[f_g; f_l]).\\n\\\\]\\n\\nWhere, \\\\( \\\\text{Wsr1} \\\\), \\\\( \\\\text{Wsr2} \\\\), \\\\( \\\\text{Wor1} \\\\) and \\\\( \\\\text{Wor2} \\\\) are learnable parameters. Finally, for any pair of subject and object, \\\\((j, k)\\\\) of the \\\\( i^{th} \\\\) frame of a video \\\\( v \\\\), the relationship embedding with respect to query visual relationship \\\\( r \\\\) is computed as a translation vector in lower dimensional relation space similar to TrasE [39] as follows:\\n\\n\\\\[\\n\\\\text{f_r}_{jk}(u_{jk}) = \\\\text{Wr2ReLU}(\\\\text{Wr1ReLU}([\\\\text{Wr3fj}; \\\\text{Wr4fo}] + (\\\\text{G}(p)))).\\n\\\\]\\n\\nWhere, \\\\( \\\\text{Wr1} \\\\), \\\\( \\\\text{Wr2} \\\\), \\\\( \\\\text{Wr3} \\\\) and \\\\( \\\\text{Wr4} \\\\) are learnable parameters and \\\\( f_j \\\\), \\\\( f_k \\\\) are subject and object representation obtained from eq. (13) and (14) respectively.\\n\\n3.3. Learning Relation Network with Few Examples\\n\\nThe proposed problem formulation has to learn the similarity between object pairs as the visual relationships using a few videos from \\\\( S_{sup} \\\\). We have selected the Relation Network (R\\\\( _\\\\theta \\\\)) [29], a metric-based meta-learning approach, as our method of choice. After learning the similarity measure for the unseen predicate, \\\\( R\\\\_\\\\theta \\\\) is used to compute both frame-level as well as visual relationship similarity potentials.\\n\\nFor a pair of object pairs \\\\( p_1 = (x_1, y_1) \\\\) and \\\\( p_2 = (x_2, y_2) \\\\), their representation as a visual relationship, \\\\( f_{p1r} \\\\), \\\\( f_{p2r} \\\\) are obtained using eq. (15). Then pairwise similarity score is computed as:\\n\\n\\\\[\\nR\\\\_\\\\theta(f_{p1r}, f_{p2r}) = \\\\text{WT}(\\\\text{W}(f_{p1r}; f_{p2r}) + b) + b,\\n\\\\]\\n\\nwhere \\\\( \\\\text{W} \\\\), \\\\( b \\\\) are learnable parameters matrix and bias vector respectively. Further, \\\\( \\\\Phi \\\\) is computed using the following equation:\\n\\n\\\\[\\n\\\\Phi(f_{p1r}, f_{p2r}) = \\\\text{tanh}(\\\\text{W1}(f_{p1r}; f_{p2r}) + b_1) \\\\cdot \\\\sigma(\\\\text{W2}(f_{p1r}; f_{p2r}) + b_2) + ((f_{p1r} + f_{p2r})/2).\\n\\\\]\\n\\nWhere, \\\\( \\\\text{W1} \\\\), \\\\( \\\\text{W2} \\\\), and \\\\( b_1 \\\\), \\\\( b_2 \\\\) are learnable parameters matrices, and bias vectors, and \\\\( \\\\sigma \\\\) and \\\\( \\\\text{tanh} \\\\) are sigmoid and hyperbolic tangent activation functions, respectively. For each support set video, we extract ground truth positive and negative object pairs. Positive object pairs are connected via the query relation predicate \\\\( p \\\\), while negative object pairs are picked randomly from a set containing pairs of objects that are not connected via predicate \\\\( p \\\\). We used ground-truth spatial and its corresponding ROI features to get the relation embedding using eq. (15). \\\\( R\\\\_\\\\theta \\\\) learns to return higher similarity between semantically similar pairs of relationships while lower similarity for other pairs. Let us consider a set of positive object pairs \\\\( p^+ = \\\\{\\\\{x_{ij}^+, y_{ij}^+\\} \\\\} \\\\) and negative object pairs \\\\( p^- = \\\\{\\\\{x_{ij}^-, y_{ij}^-\\\\} \\\\} \\\\) extracted from the \\\\( S_{sup} \\\\), where \\\\( l \\\\) is the number of positive or negative object pairs extracted from each support set video. Finally, the \\\\( R\\\\_\\\\theta \\\\) is meta-trained on \\\\( S_{sup} \\\\) using the following episodic loss.\\n\\n\\\\[\\nL = \\\\sum_{a=1}^{K} \\\\sum_{b=1}^{X} \\\\sum_{c=1}^{X} \\\\log \\\\left( 1 + e^{-R\\\\_\\\\theta(f_{a,b}^+, f_{a,b}^-)} \\\\right).\\n\\\\]\\n\\n3.4. Trajectory Generation\\n\\nTo generate the final trajectories of the subject and object, we used three different optimization approaches. In the first technique, we used only frame-level potential \\\\( \\\\Psi \\\\) to select a node with the optimal potential value, which was determined as the minimum value among all the nodes at frame \\\\( i \\\\). We skipped a frame if the optimal potential value was below a certain threshold.\"}"}
{"id": "CVPR-2023-744", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Method                                      | Asubs\u2212t mIoU | Aobjs\u2212t mIoU | Ars\u2212t mIoU | Asubs\u2212t mIoU | Aobjs\u2212t mIoU | Ars\u2212t mIoU |\\n|---------------------------------------------|--------------|--------------|------------|--------------|--------------|------------|\\n| ImageNet-VidVRD [31]                        | 8.4          | 10.7         | 11.9       | 8.6          | 10.3         | 11.2       |\\n| vRGV-fs [35]                                | 8.2          | 11.1         | 12.4       | 8.3          | 11.6         | 12.9       |\\n| Tracklet-based                              | 12.6         | 15.1         | 15.9       | 12.8         | 14.8         | 15.3       |\\n| Ours                                        | 19.6         | 21.4         | 19.1       | 18.2         | 18.7         | 20.8       |\\n| w/o vis.sim. potential, w/o GSA, w/o LLA    | 25.3         | 26.9         | 25.8       | 25.7         | 25.1         | 26.5       |\\n| w/o GSA, w/o LLA                           | 22.6         | 22.8         | 21.9       | 22.3         | 21.4         | 22.9       |\\n| Greedy solver                               | 25.4         | 26.5         | 24.7       | 24.3         | 23.8         | 26.4       |\\n| Full model                                  | 26.8         | 28.4         | 27.1       | 26.0         | 25.1         | 27.9       |\\n\\nTable 1. Performance of few-shot referring relationship in videos. Each method is trained on the same split of train and test datasets with a support size of four videos. Ours (full model) represents the proposed method where global semantic aggregation (GSA) and local localization aggregation (LLA) are performed to enrich the query-conditioned relationship representation, a translational relation embedding is learned, and an objective function containing frame-level and visual similarity potential is optimized using belief propagation on the T-partite random field. For more details, refer Section 4.\\n\\n4.1. Datasets\\nWe have used two video benchmark datasets, namely, VidOR [26, 31] and ImageNet-VidVRD [27] in our experiments. ImageNet-VidVRD contains 1000 videos obtained from the ILVSRC2016-VID dataset [25]. It has 132 predicates and 35 object categories. VidOR is a large-scale dataset containing 10,000 videos obtained from YFCC100M collection. VidOR contains 50 predicates and 80 object categories. For our problem setting, we split both datasets into disjoint sets based on predicates and videos by randomly assigning 35 and 15 predicates to the train and test sets of VidOR, while assigning 88 and 22 predicates to the train and test sets of ImageNet-VidVRD, respectively.\\n\\n4.2. Performance Measure\\nIn this work, we adopt widely-used evaluation metrics in the referring relationship and video understanding literature, as described in [3, 15, 35]. Specifically, we calculate the subject and object accuracy, which represents the percentage of correct trajectories returned by the model for the entire test set. A relation accuracy is defined as the percentage of correct subject and object pair trajectories. Here, a pair is considered correct if both subject and object trajectories are correct. For spatiotemporal accuracy, \\\\( A_{\\\\text{sub}}-t \\\\), \\\\( A_{\\\\text{obj}}-t \\\\), of the subject or object, a trajectory is considered correct if at least 50% of the bounding boxes across frames have \\\\( \\\\geq 0.5 \\\\) intersection over union (IoU) with respect to ground truth bounding boxes. Similarly, for spatiotemporal accuracy of relation, \\\\( A_{\\\\text{rs}}-t \\\\), a pair of subject and object is considered true only if both trajectories are spatiotemporally accurate (i.e. at least 50% of the bounding boxes across frames have \\\\( \\\\geq 0.5 \\\\) IoU). For spatial accuracy, \\\\( A_{\\\\text{sub}}, A_{\\\\text{obj}} \\\\), a trajectory is considered true if the average of IoUs of the bounding box with respect to ground truth is at least 0.5. The mean IoU for a trajectory is an average of the IoU score for all bounding boxes with respect to ground truth. The mean IoU of the subject or object, \\\\( m\\\\text{IoU}_{\\\\text{sub}}, m\\\\text{IoU}_{\\\\text{obj}} \\\\) is the average of the mean IoU of the subject or object for the entire test set.\\n\\n4.3. Baselines\\nTo the best of our knowledge, we are the first to approach referring relationships in videos in a few-shot setup, and there are no existing baselines in the literature that can be directly compared to our approach. Therefore, we adopt closely related methods as our baselines to compare the effectiveness of our method as follows:\\n\\n(i) Few-Shot Visual Relation Co-Localization (VRC): Few-shot VRC has originally been proposed for localizing common subjects and objects in a bag of images in a few-shot setup [30]. We adopt it for videos by treating object trajectory pairs in the test and support set videos as a bag and performing visual relationship co-localization at the frame level.\\n\\n(ii) Visual vRGV [35] tackles the same task as ours but differs significantly in supervision. We adapted vRGV for our few-shot problem setup for a fair comparison. Recall that in our problem setup, the model is trained in episodes, and each episode contains a support set of a few videos and a test video for evaluation. We utilize all available videos from the train set to train the vRGV model. During testing for each episode, we fine-tune the model on the support set and then perform localization for the given query visual relationship on the test video. We refer to these baselines as vRGV in a few-shot setup or vRGV-fs.\"}"}
{"id": "CVPR-2023-744", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Qualitative result on a selection of test videos. Each subject and object of the query relationship is spatiotemporally localized on the test videos. Red and blue color bounding boxes represent the subject and object, respectively. [Best viewed in color].\\n\\nTable 2. Ablation study to demonstrate the importance of global semantic aggregation (GSA) and local localization aggregation (LLA) on the ImageNet-VidVRD dataset. Our full model (the last row) performs better as compared to the settings where one or both of these are removed.\\n\\n| Approach | GSA | LLA | Predicate Frequency (higher to lower) | Relation Accuracy |\\n|----------|-----|-----|---------------------------------------|-------------------|\\n| Full Model | \u2713 \u2713 | \u2713 \u2713 | \u2713 \u2713 | \u2713 \u2713 | 26.8 26.0 25.1 |\\n| No GSA | \u2713 \u2713 | \u2713 \u2713 | \u2713 \u2713 | \u2713 \u2713 | 22.3 21.7 21.1 |\\n| No LLA | \u2713 \u2713 | \u2713 \u2713 | \u2713 \u2713 | \u2713 \u2713 | 21.3 22.1 20.8 |\\n| Baseline | \u2713 \u2713 | \u2713 \u2713 | \u2713 \u2713 | \u2713 \u2713 | 24.7 23.5 22.8 |\\n\\n4.4. Implementation Details\\n\\nFor frame-level detection, we utilized FasterRCNN [24], which was pre-trained on MS-COCO with ResNet-101 as the backbone. We extracted the 30 most confident object bounding boxes for each frame. Our implementation was done using PyTorch. We optimized the model parameters using Adam [13] with an initial learning rate of 1e-5, while also employing a dropout rate of 0.3 to reduce overfitting. To prevent overfitting, we used early stopping. We trained the model on an Nvidia-RTX A6000 GPU.\\n\\n4.5. Results and Discussions\\n\\nWe compare our approach with baselines discussed in Section 4.3 on VidOR and ImageNet-VidVRD datasets in Table 1. We observe that the baselines modified using prior works, namely VRC and vRGV, exhibit weaker performance across all the evaluated metrics. The tracklet-based approach performs marginally better than these baselines. However, its performance is heavily bottle-necked with the quality of tracklets generated. Our approach, without the proposed global semantic and local localization aggregation itself, surpasses these baselines. We also observed a positive impact of frame-level and visual relationship similarity potential; for example, in Table 1, the accuracy of localizing relationship (rs\u2212t) has increased by 2.3% (absolute scale) in the full model with respect to the greedy solver on ImageNet-VidVRD dataset. The superior performance of our method can be attributed to the principled optimization framework, the proposed aggregation techniques, query-conditioned learning of relationship embeddings, and the effective utilization of support set videos to meta-train relation networks. To show the efficacy of our method and to justify the choice of different modules, we conduct the following ablations and analyses:\\n\\nGlobal semantic and local localization aggregation: We remove global semantic (GSA) and local localization aggregation (LLA) from our model to understand their impact on performance. The results show a significant drop in performance when either of these is removed, indicating their importance in the framework.\"}"}
{"id": "CVPR-2023-744", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Predicate-wise performance of the proposed method on ImageNet-VidVRD Dataset grouped into static and dynamic categories.\\n\\n| Relationship Embedding | Static (\u2713) | Dynamic (\u2717) |\\n|------------------------|-----------|-------------|\\n| Concatenation          | 28.3      | 27.6        |\\n| Translation            | 26.8      | 21.4        |\\n\\nTable 4. Performance comparison of translational and Concat relationship embedding on ImageNet-VidVRD dataset. In this study, both embeddings are fused with the global and local context.\\n\\nIn other words, we used equations (4) and (5) to obtain the subject and object representation and analyze its effect on the ImageNet-VidVRD dataset in Table 2. We observe that our full model that uses these two aggregation techniques performs better as compared to the settings where one or both of these are removed.\\n\\nTranslational relation embedding:\\nIn the second ablation, we justify the choice of translation visual relationship embedding. To this end, we replaced the translational embedding in eq. (15) with a simple concatenation of subject and object representation. As shown in Table 4, translational relationship embedding used in our model is significantly more robust than simple concatenation embeddings.\\n\\nEffect of visual relationship similarity potential:\\nTo study the effect of visual relationship similarity potential in our optimization framework, we evaluate our model with and without this potential. Table 1 clearly indicates the utility of this potential where our full model that uses both visual relationship similarity and frame-level potential significantly surpass the variant that does not use visual relationship similarity potential.\\n\\nEffect of support set size/long-tail:\\nTo analyze the effect of support set size, we perform experiments by varying its size. Specifically, we present a bar chart showing achieved using our approach on ImageNet-VidVRD dataset with a support set varying from 1 to 70 videos in Figure 4. We observe an intuitive gain with a larger support set.\\n\\nUnseen subjects or objects:\\nThe predicate in the query visual relationship is always unseen in our setup. We further analyze cases where either subject or object is also unseen during training in Table 5. The model performs reasonably well for localizing subjects and objects in videos, even in these cases with an intuitive drop in performance as compared to cases where only the predicate is unseen, and both subject and object are seen.\\n\\nPerformance on static and dynamic relations:\\nIn order to analyze the performance of our approach for different types of relations, we present the predicate-wise results in Table 3. The results reveal that our proposed approach generally achieves better performance for static relationships. Nonetheless, despite the additional challenges posed by dynamic relationships, such as the presence of moving objects and subjects, occlusion, motion blur, and deformation, our approach demonstrates reasonable results for dynamic predicates highlighting its robustness.\\n\\nQualitative Results:\\nFigure 3 shows a selection of qualitative results of our approach on ImageNet-VidVRD dataset where query visual relationships, <bird, fly with, bird>, and <person, fall off, bicycle> are referred in the respective video by localizing the subject and object connected to the query predicate. Our method is successful in localizing subjects and objects connected via dynamic relationships indicating the effectiveness of our approach. We provide more qualitative results on our project website.\\n\\n5. Conclusions\\nIn this work, we approached the problem from frame-level object detection to video-level trajectory generation by optimizing an objective function on a random field in a few-shot way. We used global semantic and local localization aggregation to enhance query-conditioned translation visual relationship embedding. The objective function is minimized using belief propagation on the random field. We performed experimental comparisons along with ablation studies to show the efficacy of our approach. We firmly believe that our work will open up several future research towards the larger exciting goal of comprehensive cross-task video understanding.\\n\\nAcknowledgment:\\nThis work is partly supported by a gift grant from Accenture Labs (project number: S/ACT/AM/20220078). Y. Kumar is supported by a UGC fellowship.\\n\\n1 https://vl2g.github.io/projects/refRelations/\"}"}
