{"id": "CVPR-2022-1866", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ent at the finer class level, as follows\\n\\n\\\\[ \\\\mathbf{g}_k = \\\\mathbf{X}_i \\\\in D_{lk} \\\\|\\\\| \\\\mathbf{g}_k \\\\|^2 + \\\\mathbf{X}_i \\\\in \\\\hat{D}_{uk} I[\\\\hat{y}_u_i(\\\\alpha(x_u_i)) \\\\geq \\\\tau] \\\\|\\\\| \\\\mathbf{g}_u_i \\\\|^2, \\\\] (5)\\n\\nwhere the instance indexes of the \\\\( k \\\\)-th class in labeled and unlabeled batches are respectively \\\\( D_{lk} = \\\\{ i | (x_l_i, y_l_i) \\\\in X_l \\\\land y_l_i = k \\\\} \\\\) and \\\\( \\\\hat{D}_{uk} = \\\\{ i | x_u_i \\\\in X_u \\\\land \\\\hat{y}_u_i = k \\\\} \\\\), and the feature gradients of labeled and unlabeled samples are respectively \\\\( \\\\mathbf{g}_l_i = \\\\frac{\\\\partial -\\\\log p_{y_l_i}(\\\\alpha(x_l_i))}{\\\\partial z_{l_i}} \\\\), \\\\( \\\\mathbf{g}_u_i = \\\\frac{\\\\partial -\\\\log p_{\\\\hat{y}_u_i}(\\\\alpha(x_u_i))}{\\\\partial z_{u_i}} \\\\).\\n\\nWe note that in each training iteration, samples in the labeled and unlabeled batches are taken at random (since we use stochastic gradient descent (SGD)), which may provide incomplete knowledge on the optimization dynamic, leading to biased sample selection. To avoid it, we apply the exponential moving average \\\\([45]\\\\) to each majority gradient over the past batches in the same training epoch, as follows\\n\\n\\\\[ \\\\mathbf{g}_m_k \\\\leftarrow \\\\eta_m \\\\mathbf{g}_m_k + (1 - \\\\eta_m) \\\\mathbf{g}_m_k[t], \\\\] (6)\\n\\nwhere \\\\( t \\\\in \\\\{1, 2, \\\\ldots, T\\\\} \\\\), \\\\( T \\\\) is the number of iterations in one training epoch, \\\\( \\\\mathbf{g}_m_k[t] \\\\) is the majority gradient at the current iteration \\\\( t \\\\), \\\\( \\\\mathbf{g}_m_k = \\\\mathbf{g}_m_k[t] \\\\) when \\\\( t = 1 \\\\), and \\\\( \\\\eta_m \\\\in [0, 1] \\\\) is the moving average coefficient. Then, we employ the cosine similarity to measure the degree of synchronization between the feature gradient of one moderately confident unlabeled sample \\\\( x_u_i \\\\) and its corresponding majority gradient \\\\( \\\\mathbf{g}_m_{\\\\hat{y}_u_i} \\\\) as\\n\\n\\\\[ s_u_i = 0.5(1 + \\\\mathbf{g}_u_i \\\\cdot \\\\mathbf{g}_m_{\\\\hat{y}_u_i}) \\\\|\\\\| \\\\mathbf{g}_u_i \\\\|^2 \\\\|\\\\| \\\\mathbf{g}_m_{\\\\hat{y}_u_i} \\\\|^2, \\\\] (7)\\n\\nwhich is scaled to be in \\\\([0, 1]\\\\). From the samples whose confidence \\\\( p_{\\\\hat{y}_u_i} \\\\) is in \\\\((0.75, 0.95)\\\\), we select the ones with the gradient synchronization degree \\\\( s_u_i \\\\geq \\\\tau_s \\\\). Here, \\\\( \\\\tau_s \\\\) is a threshold hyperparameter for our proposed gradient synchronization filter (GSF). The moderately confident samples selected by GSF together with the highly confident ones are used to compute the unsupervised loss \\\\( J_{\\\\text{uns}} \\\\) in each training iteration. An illustration is given in Fig. 1.\\n\\n### 3.4. Prototype Proximity Filter\\n\\nThe barely supervised study from FixMatch \\\\([37]\\\\) has shown that the samples in one class differ in their prototypicality, i.e., to what extent they can characterize semantics of the specific class. The most representative samples are most suitable for data-efficient learning in that they yield a significant performance gain under the same low-label protocol. Inspired by it, the feature-based filter derived from the proposed TEIF framework is in fact to select the most prototypical examples that are close to the corresponding prototypes, from the moderately confident unlabeled data.\\n\\nWe follow \\\\([27, 36]\\\\) to define each class-wise prototype, i.e., prototypical representation, by the mean over sample features of one particular semantic class, which is written as\\n\\n\\\\[ z_{pk} = \\\\frac{1}{n_k} \\\\left( \\\\sum_{i \\\\in D_{lk}} z_{l_i} + \\\\sum_{i \\\\in \\\\hat{D}_{uk}} I[\\\\hat{y}_u_i(\\\\alpha(x_u_i)) \\\\geq \\\\tau] z_{u_i} \\\\right), \\\\] (8)\\n\\nwhere \\\\( n_k \\\\) is the total number of instances of the \\\\( k \\\\)-th class in labeled and highly confident unlabeled sets. The categorical information contained in \\\\( z_{pk} \\\\) is usually insufficient to represent the semantic meaning of the \\\\( k \\\\)-th class since it only considers training data in the current iteration. For instance, it is possible that some classes are missing in the current labeled and unlabeled batches since the two batches are randomly sampled. To address the issue, we also follow \\\\([45]\\\\) to conduct the exponential moving average of each class-wise prototype over all previous iterations, as follows\\n\\n\\\\[ z_{pk} \\\\leftarrow \\\\eta_p z_{pk} + (1 - \\\\eta_p) z_{pk}[t], \\\\] (9)\\n\\nwhere \\\\( z_{pk} = z_{pk}[t] \\\\) when \\\\( t = 1 \\\\) and \\\\( \\\\eta_p \\\\in [0, 1] \\\\) is the moving average coefficient. Then, we use a variant of Student t-distribution \\\\([39, 43]\\\\) to measure the extent of vicinity between the feature of one moderately confident unlabeled sample \\\\( x_u_i \\\\) and its corresponding prototype \\\\( z_{pk_{\\\\hat{y}_u_i}} \\\\) as\\n\\n\\\\[ v_u_i = \\\\frac{1}{1 + \\\\|z_u_i - z_{pk_{\\\\hat{y}_u_i}}\\\\|^2}, \\\\] (10)\\n\\nwhich is ranged from 0 to 1. From the moderately confident samples with \\\\( p_{\\\\hat{y}_u_i} \\\\in (0.75, 0.95) \\\\), we select the ones whose prototype vicinity extent \\\\( v_u_i \\\\geq \\\\tau_v \\\\). Here, \\\\( \\\\tau_v \\\\) is the threshold of our proposed prototype proximity filter (PPF). During feature learning, the extent of instance-to-center vicinity could be in an arbitrary scale since the Euclidean distance is in a range of \\\\([0, +\\\\infty)\\\\), which is the base of Eq. (10). To solve it, we propose to use an adaptive threshold, which is...\"}"}
{"id": "CVPR-2022-1866", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Error rates w.r.t. the lower (left) and upper (center) bounds of moderate confidence, and the gradient synchronization threshold (right) on 40- and 250-label settings.\\n\\nThe moving average of minimal vicinity extent over labeled and highly confident unlabeled samples, as follows\\n\\n\\\\[ \\\\tau_v[t] = \\\\min_{i \\\\in D_l} \\\\min_{i \\\\in \\\\hat{D}_u} p(y_i(x_u)) \\\\geq \\\\tau_v, \\\\]  \\\\hspace{1cm} (11)\\n\\n\\\\[ \\\\tau_v \\\\leftarrow \\\\eta \\\\tau_v + (1 - \\\\eta) \\\\tau_v[t], \\\\]  \\\\hspace{1cm} (12)\\n\\nwhere \\\\( D_l \\\\) and \\\\( \\\\hat{D}_u \\\\) collect the indexes of instances in the current labeled and unlabeled batches respectively. The unsupervised loss \\\\( J_{uns} \\\\) in Eq. (3) is then computed using the moderately confident samples selected by PPF and the highly confident ones. An illustration is given in Fig. 2.\\n\\n4. Experiments\\n\\nWe choose the state-of-the-art FixMatch [37] as the baseline and examine our proposed gradient synchronization filter (GSF) and prototype proximity filter (PPF) on three commonly used SSL benchmarks. Specifically, we conduct experiments with various number of labeled samples on the datasets of CIFAR-10 [15], CIFAR-100 [15], and SVHN [25]. CIFAR-10 has 10 classes, 50,000 training images, and 10,000 test ones; CIFAR-100 has 100 classes, 50,000 training images, and 10,000 test ones, which is very challenging due to much more classes; SVHN has 10 classes, 73,257 training images, and 26,032 test ones.\\n\\nWe follow the evaluation protocols of FixMatch and use the training set with a few labels for training and the test set for inference. We use a WRN-28-2 [47] as the backbone except for CIFAR-100 that uses a wider WRN-28-8, where the last FC layer is the instantiation of \\\\( F(\\\\cdot) \\\\). We set the confidence threshold \\\\( \\\\tau \\\\) as 0.95, which is the higher bound of the moderate confidence; we denote its lower bound by \\\\( \\\\tau_l \\\\) and set \\\\( \\\\tau_l \\\\) as 0.75. We set \\\\( \\\\tau_s \\\\) as 0.95. \\\\( \\\\tau_v \\\\) is dynamically updated by Eqs. (11) and (12). We empirically set \\\\( \\\\eta_m \\\\) and \\\\( \\\\eta_p \\\\) as 0.7. The hyperparameters \\\\( \\\\lambda_u, \\\\mu, \\\\) and \\\\( B \\\\) are set as 1, 7, and 64 respectively. The weak augmentation \\\\( \\\\alpha(\\\\cdot) \\\\) is implemented by random flipping and cropping; the strong one \\\\( \\\\beta(\\\\cdot) \\\\) performs additional image processing by RandAugment [8]. Other training details are the same as [37].\\n\\n4.1. Ablation Study\\n\\nWe by convention conduct the ablation study on a single 250-label setting from CIFAR-10. For a more thorough examination, we also do the study on a single 40-label setting.\\n\\nModerate Confidence Bounding. To examine the range of moderate confidence used in our methods, we provide the classification accuracy as (1) changing its lower bound \\\\( \\\\tau_l \\\\) in \\\\([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\\\\), with its upper bound \\\\( \\\\tau \\\\) fixed as 0.95, and (2) altering \\\\( \\\\tau \\\\) in \\\\([0.8, 0.85, 0.9, 0.95, 1]\\\\) with \\\\( \\\\tau_l \\\\) fixed as 0.75. The results of the two cases are shown in Fig. 3. It is observed that the performance is almost always benign and stable in the respective small-value ranges of the lower and upper bounds. A possible reason is that the increase in the number of selected unlabeled samples (i.e., more samples are selected at smaller values) could compensate the negative effects caused by including low-quality samples in training. As the lower and upper bounds increase, the performance stays almost the same on the 250-label setting whereas it does not on the 40-label setting, revealing that the sensitivity of one SSL method to the hyperparameters is increased with fewer labels per class due to less reliable pattern learning.\\n\\nNote that when the upper bound has a value of 1, all unlabeled samples for training are selected by our GSF or PPF, and thus all highly confident samples may be discarded. In this case, the error rates go up, suggesting that the use of highly confident samples is indispensable for learning a good model. Our methods achieve minimal error rates when the lower and upper bounds are 0.75 and 0.95 respectively.\\n\\nGradient Synchronization Thresholding. To inspect the influence of the gradient synchronization threshold \\\\( \\\\tau_s \\\\) (cf. Sec. 3.3), we perform the sensitivity analysis by varying \\\\( \\\\tau_s \\\\) in \\\\([0.2, 0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\\\\) with the moderate confidence ranged in \\\\([0.75, 0.95]\\\\). The results are shown in Fig. 3. We can observe that with 25 labels per class, the error rate fluctuates very slightly with the increase of \\\\( \\\\tau_s \\\\), indicating that more highly confident samples (on account of more labels) can build a better foundation of model optimization for pattern learning. Hence, the selected low-quality samples have less adverse impact on the...\"}"}
{"id": "CVPR-2022-1866", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"learned model's classification behavior. On the 40-label setting, the performance changes considerably as varying the value of $\\\\tau$, confirming that the hyperparameter sensitivity is inversely proportional to the number of labels available. As we can see, at the value of 0.95, the error rates are at their lowest. Particularly, when $\\\\tau$ is 1, our GSF degenerates into the baseline FixMatch, whose error rates are higher. Sample Selection Ratio. Recall that we train the classification model by using the moderately confident samples filtered by GSF or PPF, together with the highly confident ones. To know how the ratios of all selected samples and highly confident ones in the unlabeled batch evolve during the model training of our methods, we plot the change curves of these ratios in Fig. 4, where each point represents the averaged ratio over all previous and the current training iterations. As the training process proceeds, these ratios first increase and then stabilize at the level close to 1, indicating that an increasing number of unlabeled samples participate in training; on the 250-label setting, these ratios are consistently higher than those on the 40-label setting, manifesting that more experience, more confidence in making decisions; the gap between the paired ratios of all selected samples and highly confident ones, i.e., the ratio of selected moderately confident samples, is big in the earlier stage and decreases in the later stage since the instinct of self-training is to produce more and more samples of high confidence. We also show the mislabeled ratio for different methods in Fig. 4c, where we observe that our GSF and PPF enjoy a lower mislabeled ratio, suggesting that our methods can learn better decision boundaries closer to the ground-truth ones. Saliency Map Visualization. To intuitively understand what the network has learned, we utilize the typical Grad-CAM [35] to visualize the saliency maps from FixMatch and our proposed GSF and PPF. The saliency map for an example is obtained by weighting the feature maps with the gradients w.r.t. the features. The results are shown in Fig. 5, where the examples are randomly sampled from each class. We find that all methods attend to the image regions that are semantically related to classification in most examples; with 25 labels per class, they produce more accurate visualizations, implying that more supervisory information is conducive to pattern learning and thus the SSL technique self-training is in urgent need of improvement to better generate and utilize the massive amounts of pseudo-labeled data. Notably, our methods learn better feature representations that capture more complete semantic patterns, e.g., first example. 4.2. Results We compare our methods with existing ones on the standard CIFAR-10, CIFAR-100, and SVHN benchmarks. The compared methods can be divided into two groups.\"}"}
{"id": "CVPR-2022-1866", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Error rates (%) for CIFAR-10, CIFAR-100, and SVHN.\\n\\n| Method             | CIFAR-10  | CIFAR-100 | SVHN  |\\n|--------------------|-----------|-----------|-------|\\n| \u03a0-Model            | 54.26 \u00b1 3.97 | 14.01 \u00b1 0.38 | -     |\\n| Pseudo-Labeling    | 49.78 \u00b1 0.43 | 16.09 \u00b1 0.28 | -     |\\n| Mean Teacher       | 32.32 \u00b1 2.30 | 9.19 \u00b1 0.19  | -     |\\n| MixMatch           | 47.54 \u00b1 11.50 | 11.05 \u00b1 0.86  | -     |\\n| UPS                | -         | 6.42 -      | -     |\\n| Meta-Semi          | -         | 6.10 \u00b1 0.10  | 29.69 \u00b1 0.18 |\\n| UDA                | 29.05 \u00b1 5.93 | 8.82 \u00b1 1.08  | 4.88 \u00b1 0.18 |\\n| ReMixMatch         | 19.10 \u00b1 9.64 | 5.44 \u00b1 0.05  | 4.72 \u00b1 0.13 |\\n| FixMatch           | 13.81 \u00b1 3.37 | 5.07 \u00b1 0.65  | 4.26 \u00b1 0.05 |\\n| CoMatch            | 6.91 \u00b1 1.39  | 4.91 \u00b1 0.33  | -     |\\n| GSF                | 7.73 \u00b1 2.50  | 4.53 \u00b1 0.11  | 3.82 \u00b1 0.10 |\\n| PPF                | 7.71 \u00b1 3.06  | 4.84 \u00b1 0.17  | 3.94 \u00b1 0.14 |\\n\\nThe results are reported in Tab. 1, from which we take several interesting observations below.\\n\\n1. The methods in the second group exhibit a clear performance gain over those in the first group mostly, indicating that the design of technique combination is reasonable and effective.\\n2. The performance gain decreases as the number of labels per class increases. For example, on CIFAR-10, MixMatch improves over Mean Teacher by 21.27% with 25 labels per class but only by 2.77% with 400 labels per class. This is reasonable since the goal of SSL is to learn models that perform better in cases of fewer labels.\\n3. By enforcing prediction consistency between weakly- and strongly-augmented samples, UDA outperforms MixMatch by a significant margin on most settings; in particular, with only 4 labels per class, UDA is 18.49% and 8.33% better than MixMatch on CIFAR-10 and CIFAR-100 respectively.\\n4. With distribution alignment and augmentation anchoring, ReMixMatch performs much better than UDA, e.g., gains of 9.95%, 15%, and 49.29% on the 40-label setting from CIFAR-10, CIFAR-100, and SVHN respectively.\\n5. Without distribution alignment to encourage predictions to follow the class distribution of labeled data, FixMatch is inferior to ReMixMatch, i.e., decreases of 4.57% and 0.62% with 4 labels per class on CIFAR-100 and SVHN respectively, despite the gain of 5.29% on CIFAR-10 with 40 labeled samples.\\n6. Our GSF and PPF are substantially better than FixMatch, e.g., gains of 6.89% and 6.77% with 4 labels per class, 1.97% and 1.87% with 25 labels per class, and 1.15% and 1.26% with 100 labels per class on the challenging CIFAR-100, respectively. It is noteworthy that without distribution alignment, our methods still largely exceed ReMixMatch, e.g., by 2.32% and 2.2% on the 400-label setting from CIFAR-100 respectively, confirming the effectiveness of our methods on discovering helpful samples of moderate confidence.\\n7. With the number of labels decreased, our methods exhibit an increasing performance gain over FixMatch on all benchmarks and are comparable to the state-of-the-art CoMatch, suggesting that the strategy of discovering the effectiveness of moderately confident samples has the potential to handle label-scarce scenarios.\\n\\n5. Conclusion and Future Work\\n\\nIn this work, we found that some moderately confident samples are useful to improve recognition accuracy for semi-supervised learning (SSL). We pick them out by gradient synchronization filter or prototype proximity filter, which are derived from the proposed Taylor expansion inspired filtration framework. The former can strengthen the optimization dynamic of fully-supervised learning by selecting samples whose gradients are similar to class-wise majority gradients. The latter can enhance the semantic prototypicality of learned feature representation by selecting samples close to prototypes. Experiments on SSL benchmarks show that our methods based on FixMatch achieve significant improvements in accuracy, verifying their efficacy in filtering samples of moderate confidence.\\n\\nAlthough our methods as plug-ins incur very little extra cost, FixMatch-like methods themselves are time-consuming. It discourages research groups with limited GPU resources from advancing research. A possible solution is to build an international idle resource dynamic scheduling platform by the conference organizer.\\n\\nAcknowledgments. This work is supported in part by Program for Guangdong Introducing Innovative and Enterprising Teams (No.: 2017ZT07X183), National Natural Science Foundation of China (No.: 61771201), and Guangdong R&D key project of China (No.: 2019B010155001).\"}"}
{"id": "CVPR-2022-1866", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Devansh Arpit, Stanis\u0142aw Jastrz\u0119bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look at memorization in deep networks. In Proc. Int. Conf. Mach. Learn., page 233\u2013242, 2017.\\n\\n[2] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, Nicolas Ballas, and Michael Rabbat. Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples. In ICCV, pages 8443\u20138452, October 2021.\\n\\n[3] Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In NeurIPS, page 3365\u20133373, 2014.\\n\\n[4] David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In ICLR, 2020.\\n\\n[5] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In NeurIPS, volume 32, 2019.\\n\\n[6] W. Chang, T. You, S. Seo, S. Kwak, and B. Han. Domain-specific batch normalization for unsupervised domain adaptation. In CVPR, pages 7346\u20137354, 2019.\\n\\n[7] O. Chapelle and A. Zien. Semi-supervised classification by low density separation. In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, pages 57\u201364, 2005.\\n\\n[8] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In NeurIPS, volume 33, pages 18613\u201318624, 2020.\\n\\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.Imagenet: A large-scale hierarchical image database. In CVPR, pages 248\u2013255, 2009.\\n\\n[10] Geoff French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation. In ICLR, 2018.\\n\\n[11] Manfred Gilli, Dietmar Maringer, and Enrico Schumann. Chapter 11 - basic methods. In Manfred Gilli, Dietmar Maringer, and Enrico Schumann, editors, Numerical Methods and Optimization in Finance (Second Edition), pages 229\u2013271. Academic Press, second edition edition, 2019.\\n\\n[12] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, pages 529\u2013536, 2004.\\n\\n[13] Lan-Zhe Guo, Zhen-Yu Zhang, Yuan Jiang, Yu-Feng Li, and Zhi-Hua Zhou. Safe deep semi-supervised learning for unseen-class unlabeled data. In Hal Daume III and Aarti Singh, editors, Proc. Int. Conf. Mach. Learn., volume 119 of Proceedings of Machine Learning Research, pages 3897\u20133906. PMLR, 13-18 Jul 2020.\\n\\n[14] Lanqing Hu, Meina Kan, Shiguang Shan, and Xilin Chen. Unsupervised domain adaptation with hierarchical gradient synchronization. In CVPR, pages 4042\u20134051, 2020.\\n\\n[15] A. Krizhevsky. Learning multiple layers of features from tiny images. In Technical report, 2009.\\n\\n[16] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2016.\\n\\n[17] Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks. In Proc. Int. Conf. Mach. Learn. Worksh., 07 2013.\\n\\n[18] Junnan Li, Caiming Xiong, and Steven C.H. Hoi. Comatch: Semi-supervised learning with contrastive graph regularization. In ICCV, pages 9475\u20139484, October 2021.\\n\\n[19] Wei-Hong Li, Chuan-Sheng Foo, and Hakan Bilen. Learning to impute: A general framework for semi-supervised learning. CoRR, abs/1912.10364, 2019.\\n\\n[20] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele. Learning to self-train for semi-supervised few-shot classification. In H. Wallich, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00b4e-Buc, E. Fox, and R. Garnett, editors, NeurIPS, volume 32. Curran Associates, Inc., 2019.\\n\\n[21] Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs for semi-supervised learning. In CVPR, pages 8896\u20138905, 2018.\\n\\n[22] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, page 152\u2013159, 2006.\\n\\n[23] G. J. McLachlan. Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis. Journal of the American Statistical Association, 70:365\u2013369, 1975.\\n\\n[24] Takeru Miyato, Shin-Ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. IEEE TPAMI, 41:1979\u20131993, 2019.\\n\\n[25] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bisacca, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In Workshop of Proc. Neur. Info. Proc. Sys., 2011.\\n\\n[26] Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, and Ian J. Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In NeurIPS, page 3239\u20133250, 2018.\\n\\n[27] Y . Pan, T. Yao, Y . Li, Y . Wang, C. Ngo, and T. Mei. Transferrable prototypical networks for unsupervised domain adaptation. In CVPR, pages 2234\u20132242, 2019.\\n\\n[28] Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc V . Le. Meta pseudo labels. In CVPR, pages 11557\u201311568, June 2021.\\n\\n[29] Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko. Semi-supervised learning with ladder networks. In NeurIPS, page 3546\u20133554, 2015.\\n\\n[30] Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle, and Richard S. Zemel. Meta-learning for semi-supervised few-shot classification. In ICLR, 2018.\\n\\n[31] Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-labeling: An\"}"}
{"id": "CVPR-2022-1866", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"uncertainty-aware pseudo-label selection framework for semi-supervised learning. In ICLR, 2021.\\n\\n[32] Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. Semi-supervised self-training of object detection models. In Seventh IEEE Workshops on Applications of Computer Vision, volume 1, pages 29\u201336, 2005.\\n\\n[33] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In NeurIPS, volume 29, 2016.\\n\\n[34] H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transactions on Information Theory, 11:363\u2013371, 1965.\\n\\n[35] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, pages 618\u2013626, 2017.\\n\\n[36] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In NeurIPS, page 4080\u20134090, 2017.\\n\\n[37] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In NeurIPS, volume 33, pages 596\u2013608, 2020.\\n\\n[38] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS, volume 30, 2017.\\n\\n[39] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journ. of Mach. Learn. Res., 9:2579\u20132605, 2008.\\n\\n[40] J.E. van Engelen and H.H Hoos. A survey on semi-supervised learning. Mach. Learn., 109:373\u2013440, 2020.\\n\\n[41] Yulin Wang, Jiayi Guo, Shiji Song, and Gao Huang. Meta-semi: A meta-learning approach for semi-supervised learning. CoRR, abs/2007.02394, 2020.\\n\\n[42] Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical analysis of self-training with deep networks on unlabeled data. In ICLR, 2021.\\n\\n[43] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In Proc. Int. Conf. Mach. Learn., pages 478\u2013487, 2016.\\n\\n[44] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. In NeurIPS, volume 33, pages 6256\u20136268, 2020.\\n\\n[45] Shaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen. Learning semantic representations for unsupervised domain adaptation. In Proc. Int. Conf. Mach. Learn., pages 5419\u20135428, 2018.\\n\\n[46] Xin-She Yang. Chapter 1 - introduction to algorithms. In Xin-She Yang, editor, Nature-Inspired Optimization Algorithms, pages 1\u201321. Elsevier, Oxford, 2014.\\n\\n[47] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.\\n\\n[48] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jin-dong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, NeurIPS, 2021.\\n\\n[49] Liheng Zhang and Guo-Jun Qi. Wcp: Worst-case perturbations for semi-supervised deep learning. In CVPR, pages 3911\u20133920, 2020.\\n\\n[50] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. In ICLR, 2021.\\n\\n[51] Yang Zou, Zhiding Yu, B. V. K. Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In ECCV, pages 297\u2013313, 2018.\"}"}
{"id": "CVPR-2022-1866", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Discovering the Effectiveness of Moderately Confident Samples for Semi-Supervised Learning\\n\\nHui Tang and Kui Jia\\nSouth China University of Technology\\neehuitang@mail.scut.edu.cn, kuijia@scut.edu.cn\\n\\nAbstract\\nSemi-supervised learning (SSL) has been studied for a long time to solve vision tasks in data-efficient application scenarios. SSL aims to learn a good classification model using a few labeled data together with large-scale unlabeled data. Recent advances achieve the goal by combining multiple SSL techniques, e.g., self-training and consistency regularization. From unlabeled samples, they usually adopt a confidence filter (CF) to select reliable ones with high prediction confidence. In this work, we study whether the moderately confident samples are useless and how to select the useful ones to improve model optimization. To answer these problems, we propose a novel Taylor expansion inspired filtration (TEIF) framework, which admits the samples of moderate confidence with similar feature or gradient to the respective one averaged over the labeled and highly confident unlabeled data. It can produce a stable and new information induced network update, leading to better generalization. Two novel filters are derived from this framework and can be naturally explained in two perspectives. One is gradient synchronization filter (GSF), which strengthens the optimization dynamic of fully-supervised learning; it selects the samples whose gradients are similar to class-wise majority gradients. The other is prototype proximity filter (PPF), which involves more prototypical samples in training to learn better semantic representations; it selects the samples near class-wise prototypes. They can be integrated into SSL methods with CF. We use the state-of-the-art FixMatch as the baseline. Experiments on popular SSL benchmarks show that we achieve the new state of the art.\\n\\n1. Introduction\\nDeep learning has achieved great success in computer vision tasks, with image classification [9] as one of the prominent examples. The success can be mainly attributed to large-scale labeled data. However, annotating enormous training data for all tasks of interest is practically infeasible. To reduce the labeling cost, the topic of semi-supervised learning (SSL) has been proposed and there are already a large number of research works in SSL [40]. The goal of SSL is to achieve good model generalization using limited labeled data and many unlabeled data that are assumed to follow the same distribution. In this work, we investigate the classical topic, aiming to push the limit of SSL.\\n\\nRecent SSL methods rely on deep models [47] to learn feature representations that can facilitate the subsequent classification. A common strategy is self-training [12, 17, 31], where the pseudo labels are iteratively generated and then used as supervision to guide the model training on unlabeled samples. Another popular paradigm is consistency regularization [29, 33], which constrains the model to produce consistent predictions for two different duplicates of the same unlabeled sample. The difference between the two duplicates can be made by random data augmentation [33] or perturbation of network parameters [29]. After, a lot of extensions have been proposed [2, 16, 21, 24, 38, 49]. The two techniques are effective but not optimal on their own, as suggested in [26]. The current best practice in SSL is technique combination, e.g., combining self-training and consistency regularization [4, 5, 18, 37, 44, 48, 48]. The cluster and smoothness assumptions are enforced simultaneously. The former [7] assumes that the decision boundaries are located in low-density regions and the latter [40] assumes that the adjacent samples have similar labels. Such a combination can progressively improve the model performance, as verified in the theoretical work [42]. Note that the two techniques would be uninformative if the model predicts a uniform distribution over classes for unlabeled samples. To address it, existing methods adopt confidence filtering [10, 18, 37], which abandons the samples whose prediction confidences (ranged in $[0, 1]$) are lower than a predefined high threshold (e.g., 0.95 [37]). It is reasonable that the least confident samples are extremely unreliable. But are all the moderately confident samples useless, e.g., ranged in $(0.75, 0.95)$? Is there any way to pick out the useful ones to enhance the optimization power applied to the model?\"}"}
{"id": "CVPR-2022-1866", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this work, we solve the questions by introducing a novel framework of Taylor expansion inspired filtration (TEIF). The Taylor formula of the cross-entropy loss function w.r.t. the feature of one sample with true or pseudo label mainly includes terms of the multiplication of gradient and feature of finite orders. To make the change of loss consistent in the neighborhood of the feature, this framework selects the samples of moderate confidence, whose feature or gradient is similar to the respective one averaged over the labeled and highly confident unlabeled data, which are the most reliable. Hence, the final network update is still close to the one determined by the most reliable samples and further incorporates the new information contained in the selected samples of moderate confidence, such that the model optimization could be steady and improved.\\n\\nFrom this framework, two novel filters are derived to select the helpful samples from the moderately confident unlabeled data. The selected samples together with the highly confident ones are then used to train the classification model. The first filter based on gradients assumes that one moderately confident sample is useful if it follows the optimization dynamic of fully-supervised learning [1, 50]. The previous research [1] has verified that deep neuron networks learn simple patterns first that are better fitted by easy examples. The fact implies that pattern learning could be improved if such an optimization dynamic is strengthened. On the other hand, the recent approach [14] relies on the sample feature gradients to characterize the optimization dynamic, i.e., constraining the local and global alignments to be consistent. By nature of the gradient-based filter, we can thus approximate the optimization dynamic by class-wise majority gradients, which are computed on features of the labeled and highly confident unlabeled samples, i.e., easy examples. From those moderately confident samples, we select the ones that have similar feature gradients to the corresponding majority gradient. We thus term this method as gradient synchronization filter (GSF). The second filter based on features assumes that one moderately confident sample is useful if it has a certain level of prototypicality [36]. Specifically, the class-wise prototypical representations, which best characterize specific semantic classes (as suggested in [36]), are computed by taking an average over sample features of each class. The samples near prototypes are selected from those moderately confident unlabeled data. We thus term this method as prototype proximity filter (PPF). Our methods can be naturally integrated into SSL frameworks with confidence filter. To challenge the current state of the art, we choose FixMatch [37] as the baseline. Experiments on commonly used SSL benchmarks show that our methods outperform FixMatch. The empirical study also answers the previously raised questions: some moderately confident samples are useful and there are ways to pick them out. Our main contributions are summarized below.\\n\\n1. We introduce new and significant questions for SSL and provide preliminary answers for them, i.e., whether all the moderately confident samples are useless and how to select the useful ones from them.\\n2. To solve the questions, we propose a novel Taylor expansion inspired filtration (TEIF) framework, which relies on the Taylor expansion of the loss function to inspire the key measurement index of sample filtration, i.e., gradient and feature of finite orders. The principle wherein is to make the network update stable and improved after adding the selected moderately confident samples.\\n3. Two novel filters are derived from this framework and make sense from different perspectives of optimization dynamic and prototype proximity, leading to gradient synchronization filter (GSF) and prototype proximity filter (PPF) respectively. The moderately confident samples selected by GSF or PPF are then involved in model training, which helps learn decision boundaries closer to the ground-truth ones.\\n\\n2. Related Works\\n\\nWe briefly review the recent deep semi-supervised learning (SSL) methods, focusing on the components of FixMatch on which we base our work. A comprehensive survey is provided in [40] and pseudo label generation via learning to learn is also a hot topic [13, 19, 20, 28, 30, 41].\\n\\nSelf-Training. The work [12] aims to enforce the cluster assumption that the decision boundaries should be located in low-density regions; technically, it minimizes the information entropy of label distributions predicted by the model for the unlabeled data. Lee [17] picks up the class of maximum predicted probability as a hard target for each unlabeled sample; then, the pseudo labels are used to fine-tune the model. To improve, UPS [31] selects more accurate pseudo labels by considering both uncertainty and confidence of network predictions. Many other applications also adopt self-training, e.g., natural language processing [22], object detection [32], and domain adaptation [6].\\n\\nConsistency Regularization. This technique aims to implement the smoothness assumption by enforcing consistency between the model's outputs of the original and perturbed versions of the same input [42]. It is first introduced in [3], which regularizes the behavior of a pseudo-ensemble to be robust to the noise process generating it. Rasmus et al. [29] and Sajjadi et al. [33] develop it by perturbing network parameters, applying stochastic transformations on images, or utilizing the randomness involved in some components of the network, e.g., dropout and random max-pooling. In [16], the model prediction for an unlabeled sample is matched to its temporal ensembling counterpart. Differently, the work [38] turns the match object to the prediction of mean teacher, i.e., the moving average of previous models. In [49], the worst-case perturbations are imposed...\"}"}
{"id": "CVPR-2022-1866", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"on network weights and structures, which can be derived by solving respective optimization problems with spectral methods. PAWS [2] enforces consistent non-parametrical predictions between the anchor and positive views.\\n\\nTechnique Combination. Individual SSL techniques on their own are unable to achieve higher levels of performance, as revealed in recent researches [26, 44, 48]. Therefore, a better strategy is to combine these simple but effective techniques. In [4], multiple strongly-augmented versions of an unlabeled sample are involved in mixup training, where they use the sharpened model prediction of the sample\u2019s weakly-augmented version as the pseudo label; a distribution alignment component is introduced to make the predicted label distribution close to the ground truth one. Recently, a simplified version [37] trains the model with the strongly-augmented version of any unlabeled sample and uses as supervision the class of maximum prediction probability of its weakly-augmented version, where only the high-confidence samples are selected. CoMatch [18] enforces the relation consistency between pseudo label and feature embedding graphs. Differently, we take the first step towards discovering the effectiveness of moderately confident samples by proposing a novel framework of Taylor expansion inspired filtration, which derives the sample filters based on optimization dynamic or prototype proximity.\\n\\n3. Method\\n\\nThe task at hand is assumed to distinguish between \\\\( K \\\\) classes, given a small batch of \\\\( B \\\\) labeled examples \\\\( X_l = \\\\{ (x_{li}, y_{li}) \\\\}_{i=1}^{B} \\\\) and a large batch of \\\\( \\\\mu B \\\\) unlabeled examples \\\\( X_u = \\\\{ x_{ui} \\\\}_{i=1}^{\\\\mu B} \\\\), where \\\\( y \\\\in \\\\{ 1, 2, \\\\ldots, K \\\\} \\\\) and \\\\( \\\\mu \\\\in \\\\mathbb{N}^+ \\\\).\\n\\nThe objective of semi-supervised learning (SSL) is to learn a feature extractor \\\\( E(\\\\cdot) \\\\) that lifts any input image \\\\( x \\\\) to the feature space, i.e., \\\\( z = E(x) \\\\), and a classifier \\\\( F(\\\\cdot) \\\\) that maps the learned feature to a probability vector \\\\( p = F(z) \\\\), where each element \\\\( p_k (k \\\\in \\\\{ 1, 2, \\\\ldots, K \\\\}) \\\\) represents the possibility of assigning the sample to a class \\\\( k \\\\). We also write \\\\( p(x) \\\\) (resp. \\\\( p_k(x) \\\\)) as \\\\( p(x) \\\\) (resp. \\\\( p_k(x) \\\\)) when the contexts require. The classification model \\\\( F(E(\\\\cdot)) \\\\) is trained with \\\\( X_l \\\\) and \\\\( X_u \\\\), aiming to generalize well on unseen test samples. Following [37], we apply to unlabeled training samples the two types of strategies, i.e., the weak and strong augmentations, which are denoted by \\\\( \\\\alpha(\\\\cdot) \\\\) and \\\\( A(\\\\cdot) \\\\) respectively.\\n\\n3.1. Preliminaries\\n\\nWe start by introducing the popular SSL techniques, i.e., self-training and consistency regularization. Self-training uses the model prediction itself as the artificial label to guide the model training on unlabeled data [23, 34]. The idea is instantiated by two representative approaches, i.e., pseudo-labeling [17] and entropy minimization [12], which differ in the way of label generation. The former takes the class of highest score as the label (of one-hot form). Let \\\\( \\\\hat{y}_u = \\\\text{arg max}_k p_u(k) \\\\) be the pseudo label for an unlabeled example \\\\( x_u \\\\). The objective of self-training in [17] is \\\\( J_{ST} = -\\\\frac{1}{B} \\\\sum_{i=1}^{B} \\\\log p_{\\\\hat{y}_u} (x_u) \\\\), (1) which is a standard cross-entropy loss. Here, \\\\( p_{\\\\hat{y}_u} (x_u) \\\\) indicates the element of the probability vector at the predicted class \\\\( \\\\hat{y}_u \\\\) for the example \\\\( x_u \\\\). Optimizing Eq. (1) reduces the prediction uncertainty for unlabeled data, similar to [12].\\n\\nConsistency regularization is based on the smoothness assumption [40]. It is typically implemented by matching the model predictions of two different augmented versions of the same unlabeled example. The following objective of consistency regularization is used in [33] \\\\( J_{CR} = \\\\frac{1}{B} \\\\sum_{i=1}^{B} | | p(\\\\alpha(x_u)) - p(A(x_u)) | |_2^2 \\\\), (2) where we note that \\\\( \\\\alpha(\\\\cdot) \\\\) is a stochastic function and thus produces varying data augmentations. Minimizing \\\\( J_{CR} \\\\) encourages the smoothness and stability of model prediction over the entire data manifold [40]. Other variants of consistency regularization include matching predictions of an example and its virtual adversarial counterpart [24], or matching the current and temporal ensemble predictions [16], to name a few. Recent works [4, 24, 37, 44] also use cross-entropy to measure the prediction divergence.\\n\\nThe state-of-the-art method of FixMatch [37] combines self-training and consistency regularization as follows. It uses the pseudo label of a weakly-augmented version \\\\( \\\\alpha(x_u) \\\\), i.e., \\\\( \\\\hat{y}_u = \\\\text{arg max}_k p(k) (\\\\alpha(x_u)) \\\\), as the supervision of a strongly-augmented counterpart \\\\( A(x_u) \\\\). Collectively, the model is trained with the weakly-augmented labeled data \\\\( \\\\{ (\\\\alpha(x_{li}), y_{li}) \\\\}_{i=1}^{B} \\\\) and strongly-augmented unlabeled data \\\\( \\\\{ (A(x_{ui}), \\\\hat{y}_u) \\\\}_{i=1}^{\\\\mu B} \\\\). Considering that the two techniques would be uninformative if model predictions follow the uniform distribution, FixMatch adopts the typical confidence filter to discard the unlabeled examples whose confidence (i.e., maximum class probability) is lower than a pre-defined high threshold \\\\( \\\\tau \\\\). Thus, the selected set \\\\( \\\\{ (A(x_{ui}), \\\\hat{y}_u) | p_{\\\\hat{y}_u} (\\\\alpha(x_u)) \\\\geq \\\\tau \\\\} \\\\) is in fact involved in training. The overall objective of FixMatch is composed of a supervised loss \\\\( J_{sup} \\\\) and an unsupervised loss \\\\( J_{uns} \\\\), as \\\\( \\\\min_{E,F} J_{sup} + \\\\lambda_u J_{uns} \\\\), (3) where \\\\( \\\\lambda_u \\\\) is a trade-off hyperparameter and \\\\( J_{sup} = -\\\\frac{1}{B} \\\\sum_{i=1}^{B} \\\\log p(y_{li}) (\\\\alpha(x_{li})) \\\\), \\\\( J_{uns} = -\\\\frac{1}{\\\\mu B} \\\\sum_{i=1}^{\\\\mu B} I[p_{\\\\hat{y}_u} (\\\\alpha(x_u)) \\\\geq \\\\tau] \\\\log p_{\\\\hat{y}_u} (A(x_u)) \\\\).\"}"}
{"id": "CVPR-2022-1866", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here, \\\\( \\\\cdot \\\\) is an indicator function. In FixMatch, the threshold \\\\( \\\\tau \\\\in [0, 1) \\\\) of confidence filter is set to a high value, e.g., 0.95, since the pseudo labels of the most confident samples are assumed to be mostly correct \\\\[51\\\\]. Put it another way, FixMatch abandons all the samples with \\n\\\\[\\n\\\\hat{p}_y(u(\\\\alpha(x_u))) < \\\\tau\\n\\\\]\\nat each training iteration. The least confident samples are indeed too unreliable to use. However, are all the samples with moderate confidence useless, e.g., 0.75 < \\\\( \\\\hat{p}_y(u(\\\\alpha(x_u))) < 0.95 \\\\)? We in this work struggle to answer the question by introducing a novel Taylor expansion inspired filtration (TEIF) framework, as detailed shortly. The filters derived from this framework can select the helpful ones from the moderately confident samples to improve optimization over the model, as empirically verified in Sec. 4.\\n\\n### 3.2. Taylor Expansion Inspired Filtration\\n\\nWe first define the Taylor formula by borrowing from the established knowledge of function approximation in the advanced-math community \\\\[11, 46\\\\], as follows\\n\\\\[\\nJ(z') = J(z) + [\\\\nabla J(z)]^T (z' - z) + \\\\frac{1}{2}(z' - z)^T [H J(z)] (z' - z) + R_2, \\\\tag{4}\\n\\\\]\\nwhere \\\\( J(z) \\\\) is the cross-entropy loss function at the current feature \\\\( z \\\\) of a sample \\\\( x \\\\) with true or pseudo label, \\\\( z' \\\\) is in the neighborhood of \\\\( z \\\\), \\\\( \\\\nabla J(z) \\\\) is the first-order gradient, \\\\( H J(z) \\\\) is the Hessian matrix, and \\\\( R_2 \\\\) is the infinitesimal remainder of the second order expansion. In Eq. (4), the loss difference between \\\\( z' \\\\) and \\\\( z \\\\), i.e., \\\\( J(z') - J(z) \\\\), mainly comprises two terms that are the multiplication of gradient and feature of finite orders.\\n\\nRecall that existing SSL methods \\\\[37, 44\\\\] compute the loss by using the labeled and highly confident unlabeled samples only, which are commonly believed to be the most reliable ones. On this basis, the selected samples of moderate confidence should satisfy the condition that the change direction of the loss in the neighborhood of \\\\( z \\\\), i.e., rise or fall, is similar to the one averaged over the most reliable samples. Hence, the final network update does not deviate too much from the one determined by the labeled and highly confident samples; meanwhile, the new knowledge is introduced by the selected moderately confident samples, and thus the volumes of information contained in the update are increased. Therefore, the model optimization could be improved, leading to better generalization.\\n\\nTo this end, we do the sample filtration for the moderately confident unlabeled data by selecting the samples whose gradient or feature is similar to the respective one averaged over the labeled and highly confident samples, termed the Taylor expansion inspired filtration (TEIF) framework. Due to limited computation overhead, we only consider the first-order quantities in this work and will study the higher-order information in future work by designing algorithms to reduce the computational complexity.\\n\\nWe note that the gradients are closely related to the optimization dynamic \\\\[1, 14, 50\\\\] and the features characterize a certain level of semantics of the specific class \\\\[27, 36, 37\\\\], i.e., semantic prototypicality; we accordingly depict the two filters derived from our TEIF framework in perspectives of optimization dynamic and prototype proximity later.\\n\\nRemarks. It is obvious that additional correct predictions would exist for unlabeled examples whose confidence of pseudo labels is less than \\\\( \\\\tau \\\\); self-training with these additional examples would improve performance of the learned model. The theoretical result in \\\\[42\\\\] also tells that given enough correctly pseudo-labeled examples, self-training with consistency regularization is guaranteed to achieve high accuracy on the true labels for the unlabeled data. The following introduced filters aim to identify more correctly pseudo-labeled examples from those with moderately confident predictions, as empirically demonstrated in Fig. 4c.\\n\\n### 3.3. Gradient Synchronization Filter\\n\\nThe optimization dynamic of fully-supervised learning is that deep models learn simple patterns first, which are better fitted by easy examples, as suggested in \\\\[1\\\\]. The optimization over deep models is content-aware, i.e., multiple training samples that share patterns are utilized first. Existing methods characterize the optimization dynamic by gradients of the loss function w.r.t. the network parameters \\\\[50\\\\] or sample features \\\\[14\\\\]. The latter scheme naturally corresponds to the gradient-based filter derived from the proposed TEIF framework and has low computation and memory overheads. In other words, from the moderately confident samples, the filter selects the ones that follow the optimization dynamic of fully-supervised learning, which can facilitate pattern learning. Specifically, we first compute the majority gradient by taking the sum over normalized feature gradients of labeled samples and highly confident unlabeled ones that are assumed to be mostly correct. To capture the optimization dynamic exactly, we define the majority gradient by taking the sum over normalized feature gradients of labeled samples and highly confident unlabeled ones that are assumed to be mostly correct.\"}"}
