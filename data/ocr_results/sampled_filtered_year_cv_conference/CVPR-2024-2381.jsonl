{"id": "CVPR-2024-2381", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We report mIoU for Pascal, mIoU for ADE20K, and RMSE for NYUv2 depth (right). (Top) Latent scaling improves performance on Pascal \u21e00.8 mIoU (higher is better), \u21e00.3 mIoU, and \u21e05.5% relative RMSE (lower is better). (Bottom) We see a similar effect for BLIP minimum token length, with longer captions performing better, improving \u21e00.8 mIoU on Pascal, \u21e00.9 mIoU on ADE20K, and \u21e00.6% relative RMSE.\\n\\nSecond, we explore a generic prompt, a string of class names separated by spaces: \\n\\nG\\\\text{ClassNames}(B) = \\\\{'' + b | b \\\\in B \\\\}\\n\\nThese prompts are similar to the ones used for averaged EOS tokens C avg w.r.t. overall text-image alignment but instead use the token corresponding to the word representing the class name. We evaluate these variations on Pascal VOC2012 segmentation. We find that C\\\\text{ClassNames} improves performance by 1.0 mIoU, but C\\\\text{ClassEmbs} reduces performance by 0.3 mIoU (see Tab. 1). We perform more in-depth analyses of the effect of text-image alignment on the diffusion model's cross-attention maps and image generation properties in Appendix A.\\n\\nTADP. To align the diffusion model text input to the image, we use BLIP-2 \\\\cite{zhou2021blip} to generate captions for every image in our single-domain datasets (Pascal, ADE20K, and NYUv2).\\n\\nG\\\\text{TADP}(x) = \\\\text{BLIP-2}(x)!\\n\\nBLIP-2 is trained to produce image-aligned text captions and is designed around the CLIP latent space. However, other vision-language algorithms that produce captions could also be used. We find that these text captions improve performance in all datasets and tasks (Tabs. 1, 2, 3). Performance improves on Pascal segmentation by \u21e04% mIoU, ADE20K by \u21e01.4% mIoU, and NYUv2 Depth by a relative RMSE improvement of 4%. We see stronger effects on the fast schedules for ADE20K with an improvement of \u21e05 mIoU at (4k), \u21e02.4 mIoU (8K). On NYUv2 Depth, we see a smaller gain on the fast schedule \u21e02.4%. All numbers are reported relative to VPD with latent scaling.\\n\\n| Method                          | #Params | FLOPs | Crop mIoU |\\n|---------------------------------|---------|-------|-----------|\\n| self-supervised pre-training    |         |       |           |\\n| EV A \\\\cite{jiang2021ev}         | 1.01B   | 696   | 61.2      |\\n| InternImage-L \\\\cite{lin2022internimage} | 256M   | 2526G | 53.9      |\\n| InternImage-H \\\\cite{lin2022internimage} | 1.31B | 4635G | 62.5      |\\n| multi-modal pre-training        |         |       |           |\\n| CLIP-ViT-B \\\\cite{radford2021clip} | 105M   | 1043G | 50.6      |\\n| ViT-Adapter \\\\cite{dong2022vitadapter} | 571M  |       | 61.2      |\\n| BEiT-3 \\\\cite{gan2022beit}       | 1.01B   | 896   | 62.0      |\\n| ONE-PEACE \\\\cite{lerer2022one}   | 1.52B   |       | 62.0      |\\n| diffusion-based pre-training    |         |       |           |\\n| VPD A32 \\\\cite{chen2021vit}      | 862M    | 891G  | 53.7      |\\n| VPD(R)                          | 862M    | 891G  | 53.1      |\\n| VPD(LS)                         | 862M    | 891G  | 53.7      |\\n| TADP-40 (Ours)                  | 862M    | 2168G | 54.8      |\\n| TADP-Oracle                     | 862M    | 512G  | 72.0      |\\n\\nTable 2. Semantic segmentation with different methods for ADE20k. Our method (green) achieves SOTA within the diffusion-pretrained models category. The results of our oracle indicate the potential of diffusion-based models for future research as it is significantly higher than the overall SOTA (highlighted in yellow). See Tab. 1 for a notation key and Tab. S1 for fast schedule results.\\n\\n| Method | RMSE | #1 | #2 | #3 | REL | #log10 |\\n|--------|------|----|----|----|-----|-------|\\n| default schedule |      |    |    |    |     |       |\\n| SwinV2-L \\\\cite{liu2022swin} | 0.287| 0.949| 0.994| 0.999| 0.083| 0.035 |\\n| AiT \\\\cite{wang2021ai} | 0.275| 0.954| 0.994| 0.999| 0.076| 0.033 |\\n| ZoeDepth \\\\cite{yu2021zoe} | 0.270| 0.955| 0.995| 0.999| 0.075| 0.032 |\\n| VPD \\\\cite{chen2021vit} | 0.254| 0.964| 0.995| 0.999| 0.069| 0.030 |\\n| VPD(R) | 0.248| 0.965| 0.996| 0.999| 0.068| 0.029 |\\n| VPD(LS) | 0.235| 0.971| 0.997| 0.999| 0.064| 0.028 |\\n| TADP-40 | 0.225| 0.976| 0.997| 0.999| 0.062| 0.027 |\\n\\nTable 3. Depth estimation in NYUv2. We find latent scaling accounts for a relative gain of \u21e05.5% on the RMSE metric. Additionally, image-text alignment improves \u21e04% relative on the RMSE metric. A minimum caption length of 40 tokens performs the best. We also explore adding a text-adapter (TA) to TADP, but find no significant gain. See Table 1 for a notation key.\\n\\nWe perform some ablations to analyze what aspects of the captions are important. We explore the minimum token number hyperparameter for BLIP-2 to explore if longer captions can produce more useful feature maps for the downstream task. We try a minimum token number of 0, 20, and 40 tokens (denoted as C\\\\text{TADP-N}) and find small but consistent gains with longer captions, resulting on average 0.75% relative gain for 40 tokens vs. 0 tokens (Fig. 3). Next, we ablate the Pascal C\\\\text{TADP-20} captions to understand what in the\"}"}
{"id": "CVPR-2024-2381", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Cross-attention maps for different types of prompting (before training).\\n\\nWe compare the cross-attention maps for four types of prompting: oracle, BLIP, Average EOS tokens, and class names as space-separated strings. The cross-attention maps for different heads at all different scales are upsampled to 64x64 and averaged. When comparing Average Template EOS and Class Names, we see (qualitatively) averaging degrades the quality of the cross-attention maps. Furthermore, we find that class names that are not present in the image can have highly localized attention maps (e.g., 'bottle'). Further analysis of the cross-attention maps is available in Sec. A, where we explore image-to-image generation, copy-paste image modifications, and more.\\n\\nWe use NLTK [4] to filter for the nouns in the captions. In the C TADP(NO)-20 nouns-only caption setting, we achieve 86.4% mIoU, similar to 86.2% mIoU with C TADP-20 (Tab. 1), suggesting nouns are sufficient.\\n\\nOracle. This insight about nouns leads us to ask if an oracle caption, in which all the object class names in an image are provided as a caption, can improve performance further. We define \\\\( G_{\\\\text{Oracle}}(x) = \\\\{b + b^2 \\\\mid b^2 \\\\in B(x) \\\\} \\\\) where \\\\( B(x) \\\\) is the set of class names present in image \\\\( x \\\\).\\n\\nWhile this is not a realistic setting, it serves as an approximate upper bound on performance for our method on the segmentation task. We find a large improvement in performance in segmentation, achieving 89% mIoU on Pascal and 72.2% mIoU on ADE20K. For depth estimation, multi-class segmentation masks are only provided for a smaller subset of the images, so we cannot generate a comparable oracle. We perform ablations on the oracle captions to evaluate the model's sensitivity to alignment. For ADE20K, on the 4k iteration schedule, we modify the oracle captions by randomly adding and removing classes such that the recall and precision are at 0.5, 0.75, and 1.0 (independently) (Tab. S2). We find that both precision and recall have an effect, but recall is significantly more important. When recall is lower (0.50), improving precision has minimal impact (<1% mIoU). However, precision has progressively larger impacts as recall increases to 0.75 and 1.00 (\u21e03% mIoU and \u21e07% mIoU). In contrast, recall has large impacts at every precision level: 0.5 - (\u21e06% mIoU), 0.75 - (\u21e09% mIoU), and 1.00 - (\u21e013% mIoU). BLIP-2 captioning performs similarly to a precision of 1.00 and a recall of 0.5 (Tab. 2). Additional analyses w.r.t. precision, recall, and object sizes can be found in Appendix B.\\n\\n4.3. Cross-domain alignment\\n\\nNext, we ask if text-image alignment can benefit cross-domain tasks. In cross-domain, we train a model on a source domain and test it on a different target domain. There are two aspects of alignment in the cross-domain setting: the first is also present in single-domain, which is image-text alignment; the second is unique to the cross-domain setting, which is text-target domain alignment. The second\"}"}
{"id": "CVPR-2024-2381", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method                  | Watercolor2k | Comic2k | AP AP | AP AP |\\n|-------------------------|--------------|---------|-------|-------|\\n| Dark Zurich-val ND      |              |         |       |       |\\n| mIoU mIoU               |              |         |       |       |\\n| DAFormer                | 20           | 54.1    |       |       |\\n| Refign-DAFormer         | 7            | 56.8    |       |       |\\n| PTDiffSeg               | 17           | 37.0    | -     |       |\\n| TADP null               | 42.8         | 57.5    |       |       |\\n| TADP simple             | 39.1         | 56.9    |       |       |\\n| TADP TextualInversion   |              | 41.4    |       |       |\\n| TADP DreamBooth         |              | 38.9    |       |       |\\n| TADP NearbyDomain       |              | 41.9    |       |       |\\n| TADP UnrelatedDomain    |              | 42.3    |       |       |\\n\\nTable 4. Cross-domain semantic segmentation. Cityscapes (CD) to Dark Zurich (DZ) val and Nighttime Driving (ND). We report the mIoU. Our method sets a new SOTA for Dark Zurich and Nighttime Driving.\\n\\nOur intuition is that while the model has no information on the target domain from the training images, an appropriate text prompt may carry some general information about the target domain. Our cross-domain experiments focus on the text-target domain alignment and use G TADP for image-text alignment (following our insights from the single-domain setting).\\n\\nTraining. Our experiments in this setting are designed in the following manner: we train a diffusion model on the source domain captions $C_{TADP}(x)$. With these source domain captions, we experiment with four different caption modifications (each increasing in alignment to the target domain), a null $M_{null}(P)$ caption modification where $M_{null}(P) = $, a simple $M_{simple}(P)$ caption modifier where $M_{simple}(P) = $ is a hand-crafted string describing the style of the target domain appended to the end and $M_{simple}(P) = $, a Textual Inversion $M_{TI}(P)$ caption modifier where the output $M_{TI}(P)$ is a learned Textual Inversion token $<*>$ and $M_{TI}(P) = $, and a DreamBooth $M_{DB}(P)$ caption modifier where $M_{DB}(P) = $ is a learned DreamBooth token $<SKS>$ and $M_{DB}(P) = $ is a DreamBoothed diffusion backbone. We also include two additional control experiments. In the first, an unrelated target domain style is appended to the end of the string. In the second, a nearby but a different target domain style is appended to the caption. $M_{TI}(P)$ and $M_{DB}(P)$ require more information than the other methods, such that $P$ represents a subset of unlabelled images from the target domain.\\n\\nTesting. When testing the trained models on the target domain images, we want to use the same captioning modification for the test images as in the training setup. However, $G_{TADP}$ introduces a confound since it naturally incorporates target domain information. For example, $G_{TADP}(x)$ might produce the caption \\\"a watercolor painting of a dog and a bird\\\" for an image from the Watercolor2K dataset. Using the $M_{simple}(P)$ captioning modification on this prompt would introduce redundant information and would not match the caption format used during training. In order to remove target domain information and get a plain caption that can be modified in the same manner as in the training data, we use GPT-3.5 to remove all mentions of the target domain shift. For example, after using GPT-3.5 to remove mentions of the watercolor style in the above sentence, we are left with \\\"an image of a bird and a dog\\\". With these GPT-3.5 cleaned captions, we can match the caption modifications used during training when evaluating test images. This caption-cleaning strategy lets us control how target domain information is included in the test image captions, ensuring that test captions are in the same domain as train captions.\\n\\nEvaluation. We evaluate cross-domain transfer on several datasets. We train our model on Pascal VOC object detection and evaluate on Watercolor2K (W2K) and Comic2K (C2K). We also train our model on the Cityscapes dataset and evaluate on the Nighttime Driving (ND) and Dark Zurich-val (DZ-val) datasets. We show results in Tabs. 4, 5. In the following sections, we also report the average performance of each method on the cross-domain segmentation datasets (average mIoU) and the cross-domain object detection datasets (average AP).\"}"}
{"id": "CVPR-2024-2381", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The null captions have no target domain information. In this setting, the model is trained with captions with no target domain information and tested with GPT-3.5 cleaned target domain captions. We find diffusion pre-training to be extraordinarily powerful on its own, with just plain captions (no target domain information); the model already achieves SOTA on VOC! W2K with 72.1 AP, SOTA on CD! DZ-val with 42.8 mIoU and SOTA on CD! ND with 60.8 mIoU. Our model performs better than the current SOTA \\\\[44\\\\] on VOC! W2K and worse on VOC! C2K (highlighted in yellow in Tab. 5). However, \\\\[44\\\\] uses a large extra training dataset from the target (comic) domain, so we highlight in bold our results in Tab. 5 to show they outperform all other methods that use only images in C2K as examples from the target domain. Furthermore, these results are with a lightweight FPN \\\\[24\\\\] head, in contrast to other competitive methods like Refign \\\\[7\\\\], which uses a heavier decoder head. These captions achieve 50.5 average mIoU and 36.6 average AP.\\n\\nWe then add target domain information to our captions by prepending the target domain's semantic shift to the generic captions. These caption modifiers are hand-crafted. For example, \u201ca dog and a bird\u201d becomes \u201ca X style painting of a dog and a bird\u201d (where X is watercolor for W2K and comic for C2K) and \u201ca dark night photo of a dog and a bird\u201d for DZ. These captions achieve 48.0 average mIoU and 37.7 average AP.\\n\\nTextual Inversion is a method that learns a target concept (an object or style) from a set of images and encodes it into a new token. We learn a novel token from target domain image samples to further increase image-text alignment (for details, see Sec. D.1). In this setting, the sentence template becomes \u201ca \\\\textless token\\\\textgreater style painting of a dog and a bird\u201d. We find that, on average, Textual Inversion captions perform the best, achieving 51.1 average mIoU and 38.2 average AP.\\n\\nDreamBooth-ing \\\\[37\\\\] aims to achieve the same goal as textual inversion. Along with learning a new token, the stable-diffusion backbone itself is fine-tuned with a set of target domain images (for details, see Sec. D.1). We swap the stable diffusion backbone with the DreamBooth-ed backbone before training. We use the same template as in textual inversion. These captions achieve 49.7 average mIoU and 38.1 average AP.\\n\\nAblations.\\n\\nWe ablate our target domain alignment strategy by introducing unrelated and nearby target-domain style modifications. For example, this would be \u201ca dashcam photo of a dog and a bird\u201d (unrelated) and \u201ca constructivism painting of a dog and a bird\u201d (nearby) for the W2K and C2K datasets. \u201cA watercolor painting of a car on the street\u201d (unrelated) and \u201ca foggy photo of a car on the street\u201d for the ND and DZ-val datasets. We find these off-target domains reduce performance on all datasets.\\n\\n5. Discussion\\n\\nWe present a method for image-text alignment that is general, fully automated, and can be applied to any diffusion-based perception model. To achieve this, we systematically explore the impact of text-image alignment on semantic segmentation, depth estimation, and object detection. We investigate whether similar principles apply in the cross-domain setting and find that alignment towards the target domain during training improves downstream cross-domain performance.\\n\\nWe find that EOS token averaging for prompting does not work as effectively as strings for the objects in the image. Our oracle ablation experiments show that our diffusion pre-trained segmentation model is particularly sensitive to missing classes (reduced recall) and less sensitive to off-target classes (reduced precision), and both have a negative impact. Our results show that aligning text prompts to the image is important in identifying/generating good multi-scale feature maps for the downstream segmentation head. This implies that the multi-scale features and latent representations do not naturally identify semantic concepts without the guidance of the text in diffusion models. Moreover, proper latent scaling is crucial for downstream vision tasks. Lastly, we show how using a captioner, which has the benefit of being open vocabulary, high precision, and downstream task agnostic, to prompt the diffusion pre-trained segmentation model automatically improves performance significantly over providing all possible class names.\\n\\nWe also find that diffusion models can be used effectively for cross-domain tasks. Our model, without any captions, already surpasses several SOTA results in cross-domain tasks due to the diffusion backbone\u2019s generalizability. We find that good target domain alignment can help with cross-domain performance for some domains, and misalignment leads to worse performance. Capturing information about target domain styles in words alone can be difficult. For these cases, we show that model personalization through Textual Inversion or Dreambooth can bridge the gap without requiring labeled data. Future work could explore how to expand our framework to generalize to multiple unseen domains. Future work may also explore closed vocabulary captioners that are more task-specific to get closer to oracle-level performance.\\n\\nAcknowledgements.\\n\\nPietro Perona and Markus Marks were supported by the National Institutes of Health (NIH R01 MH123612A) and the Caltech Chen Institute (Neuroscience Research Grant Award). Pietro Perona, Neehar Kondapaneni, Rog\u00e9rio Guimar\u00e3es, and Markus Marks were supported by the Simons Foundation (NC-GB-CULM-00002953-02). Manuel Knott was supported by an ETH Zurich Doc.Mobility Fellowship. We thank Oisin Mac Aodha, Yisong Yue, and Mathieu Salzmann for their valuable inputs that helped improve this work.\"}"}
{"id": "CVPR-2024-2381", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers. arXiv preprint arXiv:2211.01324, 2022.\\n\\n[2] Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalogeiton, and St\u00e9phane Lathuili\u00e8re. One-shot Unsupervised Domain Adaptation with Personalized Diffusion Models. arXiv preprint arXiv:2303.18080, 2023.\\n\\n[3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias M\u00fcller. ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth. arXiv preprint arXiv:2302.12288, 2023.\\n\\n[4] Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text with the natural language toolkit. O'Reilly Media, Inc., 2009.\\n\\n[5] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. SEGA: Instructing Diffusion using Semantic Dimensions. arXiv preprint arXiv:2301.12247, 2023.\\n\\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\n[7] David Br\u00fcgge mann, Christos Sakaridis, Prune Truong, and Luc Van Gool. Refign: Align and Refine for Adaptation of Semantic Segmentation to Adverse Conditions. 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2022.\\n\\n[8] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Y. Qiao. Vision Transformer Adapter for Dense Predictions. arXiv preprint arXiv:2205.08534, 2022.\\n\\n[9] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes Dataset for Semantic Urban Scene Understanding. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3213\u20133223, 2016.\\n\\n[10] Dengxin Dai and Luc Van Gool. Dark Model Adaptation: Semantic Image Segmentation from Daytime to Nighttime. 2018 21st International Conference on Intelligent Transportation Systems (ITSC), pages 3819\u20133824, 2018.\\n\\n[11] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Unbiased mean teacher for cross-domain object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4091\u20134101, 2021.\\n\\n[12] David Eigen, Christian Puhrsch, and Rob Fergus. Depth Map Prediction from a Single Image using a Multi-Scale Deep Network. In Advances in Neural Information Processing Systems 27 (NIPS 2014), 2014.\\n\\n[13] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.\\n\\n[14] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012), 2012.\\n\\n[15] Yuxin Fang, Wen Wang, Binhui Xie, Quan-Sen Sun, Ledell Yu Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. EV A: Exploring the Limits of Masked Visual Representation Learning at Scale. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19358\u201319369, 2022.\\n\\n[16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. arXiv preprint arXiv:2208.01618, 2022.\\n\\n[17] Rui Gong, Martin Danelljan, Han Sun, Julio Delgado Manegas, and Luc Van Gool. Prompting Diffusion Representations for Cross-Domain Semantic Segmentation. arXiv preprint arXiv:2307.02138, 2023.\\n\\n[18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-Prompt Image Editing with Cross Attention Control. arXiv preprint arXiv:2208.01626, 2022.\\n\\n[19] Luwei Hou, Yu Zhang, Kui Fu, and Jia Li. Informative and consistent correspondence mining for cross-domain weakly supervised object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9929\u20139938, 2021.\\n\\n[20] Lukas Hoyer, Dengxin Dai, and Luc Van Gool. DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9914\u20139925, 2022.\\n\\n[21] Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiyoharu Aizawa. Cross-Domain Weakly-Supervised Object Detection Through Progressive Domain Adaptation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5001\u20135009, 2018.\\n\\n[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. arXiv preprint arXiv:2102.05918, 2021.\\n\\n[23] Junguang Jiang, Baixu Chen, Jianmin Wang, and Mingsheng Long. Decoupled adaptation for cross-domain object detection. arXiv preprint arXiv:2110.02578, 2021.\\n\\n[24] Alexander Kirillov, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Panoptic Feature Pyramid Networks. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6392\u20136401, 2019.\\n\\n[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. arXiv preprint arXiv:2301.12597, 2023.\"}"}
{"id": "CVPR-2024-2381", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm. arXiv preprint arXiv:2110.05208, 2022.\\n\\nZe Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, and Baining Guo. Swin Transformer V2: Scaling Up Capacity and Resolution. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11999\u201312009, 2021.\\n\\nGrace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. arXiv preprint arXiv:2305.14334, 2023.\\n\\nJia Ning, Chen Li, Zheng Zhang, Zigang Geng, Qi Dai, Kun He, and Han Hu. All in Tokens: Unifying Output Space of Visual Tasks via Soft Token. arXiv preprint arXiv:2301.02229, 2023.\\n\\nShengxiong Ouyang, Xinglu Wang, Kejie Lyu, and Yongming Li. Pseudo-label generation-evaluation framework for cross domain weakly supervised object detection. In 2021 IEEE International Conference on Image Processing (ICIP), pages 724\u2013728. IEEE, 2021.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. International Conference on Machine Learning, pages 8748\u20138763, 2021.\\n\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv preprint arXiv:2204.06125, 2022.\\n\\nYongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18061\u201318070, 2022.\\n\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(6):1137\u20131149, 2017.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10674\u201310685, 2022.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015, pages 234\u2013241. Springer International Publishing, Cham, 2015.\\n\\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. arXiv preprint arXiv:2208.12242, 2022.\\n\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Madavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. arXiv preprint arXiv:2205.11487, 2022.\\n\\nChristos Sakaridis, Dengxin Dai, and Luc Van Gool. Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 7373\u20137382, 2019.\\n\\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. arXiv preprint arXiv:2111.02114, 2021.\\n\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\\n\\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor Segmentation and Support Inference from RGBD Images. European Conference on Computer Vision (ECCV), 2012.\\n\\nLuming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. arXiv preprint arXiv:2306.03881, 2023.\\n\\nBar\u0131s Batuhan Topal, Deniz Yuret, and Tevfik Metin Sezgin. Domain-adaptive self-supervised pre-training for face & body detection in drawings. arXiv preprint arXiv:2211.10641, 2022.\\n\\nEric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167\u20137176, 2017.\\n\\nVidit Vidit, Martin Engilberge, and Mathieu Salzmann. CLIP-the Gap: A Single Domain Generalization Approach for Object Detection. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3219\u20133229, 2023.\\n\\nPeng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang Zhou. ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities. arXiv preprint arXiv:2305.11172, 2023.\\n\\nWenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiao-hua Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, and Y. Qiao. InternImage: Exploring Large-Scale Vision Foundation Models with 13892\"}"}
{"id": "CVPR-2024-2381", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deformable Convolutions.\\n\\n2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14408\u201314419, 2022.\\n\\nWen Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiangbo Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei.\\n\\nImage as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks.\\n\\n2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19175\u201319186, 2023.\\n\\nWeijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen. DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models.\\n\\narXiv preprint arXiv:2303.11681, 2023.\\n\\nYunqiu Xu, Yifan Sun, Zongxin Yang, Jiaxu Miao, and Yi Yang. H2far-cnn: Holistic and hierarchical feature alignment for cross-domain weakly supervised object detection.\\n\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14329\u201314339, 2022.\\n\\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling Autoregressive Models for Content-Rich Text-to-Image Generation.\\n\\narXiv preprint arXiv:2206.10789, 2022.\\n\\nWenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing Text-to-Image Diffusion Models for Visual Perception.\\n\\narXiv preprint arXiv:2303.02153, 2023.\\n\\nZhen Zhao, Yuhong Guo, Haifeng Shen, and Jieping Ye. Adaptive object detection with dual multi-label prediction.\\n\\nIn Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXVIII 16, pages 54\u201369. Springer, 2020.\\n\\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene Parsing through ADE20K Dataset.\\n\\n2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5122\u20135130, 2017.\"}"}
{"id": "CVPR-2024-2381", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Text-image Alignment for Diffusion-based Perception\\n\\nNeehar Kondapaneni\\n\\nMarkus Marks\\n\\nManuel Knott\\n\\nRogerio Guimaraes\\n\\nPietro Perona\\n\\n1 California Institute of Technology\\n\\n2 ETH Zurich, Swiss Data Science Center, Empa\\n\\nAbstract\\n\\nDiffusion models are generative models with impressive text-to-image synthesis capabilities and have spurred a new wave of creative methods for classical machine learning tasks. However, the best way to harness the perceptual knowledge of these generative models for visual tasks is still an open question. Specifically, it is unclear how to use the prompting interface when applying diffusion backbones to vision tasks. We find that automatically generated captions can improve text-image alignment and significantly enhance a model's cross-attention maps, leading to better perceptual performance. Our approach improves upon the current state-of-the-art (SOTA) in diffusion-based semantic segmentation on ADE20K and the current overall SOTA for depth estimation on NYUv2. Furthermore, our method generalizes to the cross-domain setting. We use model personalization and caption modifications to align our model to the target domain and find improvements over unaligned baselines. Our cross-domain object detection model, trained on Pascal VOC, achieves SOTA results on Watercolor2K. Our cross-domain segmentation method, trained on Cityscapes, achieves SOTA results on Dark Zurich-val and Nighttime Driving.\\n\\nProject page:\\nvision.caltech.edu/TADP/\\n\\nCode page:\\ngithub.com/damaggu/TADP\\n\\n1. Introduction\\n\\nDiffusion models have set the state-of-the-art (SOTA) for image generation [32, 35, 38, 52]. Recently, a few works have shown diffusion pre-trained backbones have a strong prior for scene understanding that allows them to perform well in advanced discriminative vision tasks, such as semantic segmentation [17, 53], monocular depth estimation [53], and keypoint estimation [28, 43]. We refer to these works as diffusion-based perception methods. Unlike contrastive vision language models (e.g., CLIP) [22, 26, 31, 61],\\n\\n*Equal contribution.*\"}"}
{"id": "CVPR-2024-2381", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We find that unaligned text prompts can introduce semantic shifts to the feature maps of the diffusion model and that these shifts can make it more difficult for the task-specific head to solve the target task. Specifically, we ask whether unaligned text prompts, such as averaging class-specific sentence embeddings, hinder performance by interfering with feature maps through the cross-attention mechanism. Through ablation experiments on Pascal VOC2012 segmentation and ADE20K, we find that off-target and missing class names degrade image segmentation quality. We show automated image captioning achieves sufficient text-image alignment for perception. Our approach (along with latent representation scaling, see Sec. 4.1) improves performance for semantic segmentation on Pascal and ADE20k by 4.0 mIoU and 1.7 mIoU, respectively, and depth estimation on NYUv2 by 0.2 RMSE (+8% relative) setting the new SOTA.\\n\\nNext, we focus on cross-domain adaptation: can appropriate image captioning help visual perception when the model is trained in one domain and tested on a different domain? Training models on the source domain with the appropriate prompting strategy leads to excellent unsupervised cross-domain performance on several benchmarks. We evaluate our cross-domain method on Pascal VOC to Watercolor2k (W2K) and Comic2k (C2K) for object detection and Cityscapes (CS) to Dark Zurich (DZ) and Nighttime (ND) Driving for semantic segmentation. We explore varying degrees of text-target domain alignment and find that improved alignment results in better performance. We also demonstrate using two diffusion personalization methods, Textual Inversion and DreamBooth, for better target domain alignment and performance. We find that diffusion pre-training is sufficient to achieve SOTA (+5.8 mIoU on CS to DZ, +4.0 mIoU on CS to ND, +0.7 mIoU on VOC to W2k) or near SOTA results on all cross-domain datasets with no text-target domain alignment, and including our best text-target domain alignment method further improves +1.4 AP on Watercolor2k, +2.1 AP on Comic2k, and +3.3 mIoU on Nighttime Driving.\\n\\nOverall, our contributions are as follows:\\n\\n\u2022 We propose a new method using automated caption generation that significantly improves performance on several diffusion-based vision tasks through increased text-image alignment.\\n\u2022 We systematically study how prompting affects diffusion-based vision performance, elucidating the impact of class presence, grammar in the prompt, and previously used average embeddings.\\n\u2022 We demonstrate that diffusion-based perception effectively generalizes across domains, with text-target domain alignment improving performance, which can be further boosted by model personalization.\\n\\n2. Related Work\\n\\n2.1. Diffusion models for single-domain vision tasks\\n\\nDiffusion models are trained to reverse a step-wise forward noising process. Once trained, they can generate highly realistic images from pure noise. To control image generation, diffusion models are trained with text prompts/captions that guide the diffusion process. These prompts are passed through a text encoder to generate text embeddings that are incorporated into the reverse diffusion process via cross-attention layers.\\n\\nRecently, some works have explored using diffusion models for discriminative vision tasks. This can be done by either utilizing the diffusion model as a backbone for the task or through fine-tuning the diffusion model for a specific task and then using it to generate synthetic data for a downstream model. We use the diffusion model as a backbone for downstream vision tasks. VPD encodes images into latent representations and passes them through one step of the Stable Diffusion model. The cross-attention maps, multi-scale features, and output latent code are concatenated and passed to a task-specific head. Text prompts influence all these maps through the cross-attention mechanism, which guides the reverse diffusion process. The cross-attention maps are incorporated into the multi-scale feature maps and the output latent representation. The text guides the diffusion process and can accordingly shift the latent representation in semantic directions. The details of how VPD uses the prompting interface are described in Sec. 3. In short, VPD uses unaligned text prompts. In our work, we show how aligning the text to the image by using a captioner can significantly improve semantic segmentation and depth estimation performance.\\n\\n2.2. Image captioning\\n\\nCLIP introduced a novel learning paradigm to align images with their captions. Shortly after, the LAION-5B dataset was released with 5B image-text pairs; this dataset was used to train Stable Diffusion. We hypothesize that text-image alignment is important for diffusion-pretrained vision models. However, images used in advanced vision tasks (like segmentation and depth estimation) are not naturally paired with text captions. To obtain image-aligned captions, we use BLIP-2, a model that inverts the CLIP latent space to generate captions for novel images.\\n\\n2.3. Diffusion models for cross-domain vision tasks\\n\\nA few works explore the cross-domain setting with diffusion models. Benigmim et al. use a diffusion model to generate data for a downstream unsupervised domain adaptation (UDA) architecture. In, the diffusion model is fine-tuned for a specific task and then used to generate synthetic data for a downstream model. We use the diffusion model as a backbone for downstream vision tasks. VPD encodes images into latent representations and passes them through one step of the Stable Diffusion model. The cross-attention maps, multi-scale features, and output latent code are concatenated and passed to a task-specific head. Text prompts influence all these maps through the cross-attention mechanism, which guides the reverse diffusion process. The cross-attention maps are incorporated into the multi-scale feature maps and the output latent representation. The text guides the diffusion process and can accordingly shift the latent representation in semantic directions. The details of how VPD uses the prompting interface are described in Sec. 3. In short, VPD uses unaligned text prompts. In our work, we show how aligning the text to the image by using a captioner can significantly improve semantic segmentation and depth estimation performance.\"}"}
{"id": "CVPR-2024-2381", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The backbone is frozen, and the segmentation head is trained with a consistency loss with category and scene prompts guiding the latent code towards target cross-domains. Similar to VPD, the category prompts consist of token embeddings for all classes present in the dataset, irrespective of their presence in any specific image. The consistency loss forces the model to predict the same output mask for all the different scene prompts, helping the segmentation head become invariant to the scene type. Instead of using a consistency loss, we train the diffusion model backbone and task head on the source domain data with and without incorporating the style of the target domain in the caption. We find that better alignment with the target domain (i.e., target domain information included in the prompt) results in better cross-domain performance.\\n\\n2.4. Cross-domain object detection\\n\\nCross-domain object detection can be divided into multiple subcategories, depending on what data/labels are at train/test time available. Unsupervised domain adaptation objection detection (UDAOD) tries to improve detection performance by training on unlabeled target domain data with approaches such as self-training \\\\cite{11,44}, adversarial distribution alignment \\\\cite{54} or generating pseudo labels for self-training \\\\cite{23}. Cross-domain weakly supervised object detection (CDWSOD) assumes the availability of image-level annotations at training time and utilizes pseudo labeling \\\\cite{21,30}, alignment \\\\cite{51} or correspondence mining \\\\cite{19}. Recently, \\\\cite{46} used CLIP \\\\cite{31} for Single Domain Generalization, which aims to generalize from a single domain to multiple unseen target domains. Our text-based method defines a new category of cross-domain object detection that tries to adapt from a single source to an unseen target domain by only having the broad semantic context of the target domain (e.g., foggy/night/comic/watercolor) as text input to our method. When we incorporate model personalization, our method can be considered a UDAOD method since we train a token based on unlabeled images from the target domain.\\n\\n3. Methods\\n\\nStable Diffusion \\\\cite{35}. The text-to-image Stable Diffusion model is composed of four networks: an encoder $E$, a conditional denoising autoencoder (a U-Net in Stable Diffusion) $\\\\mathbf{\u2713}$, a language encoder $\\\\langle\\\\rangle$, and a decoder $D$. $E$ and $D$ are trained before $\\\\mathbf{\u2713}$, such that $D(E(x)) = \\\\tilde{x}$. Training $\\\\mathbf{\u2713}$ is composed of a pre-defined forward process and a learned reverse process. The reverse process is learned using LAION-400M \\\\cite{40}, a dataset of 400 million images ($x$) and captions ($y$). In the forward process, an image $x$ is encoded into a latent $z_0 = E(x)$, and $t$ steps of a forward noise process are executed to generate a noised latent $z_t$. Then, to learn the reverse process, the latent $z_t$ is passed to the denoising autoencoder $\\\\mathbf{\u2713}$, along with the time-step $t$ and the image caption's representation $C = \\\\langle\\\\rangle(y)$. $\\\\langle\\\\rangle$ adds information about $y$ to $\\\\mathbf{\u2713}$ using a cross-attention mechanism, in which the query is derived from the image, and the key and value are transformations of the caption representation. The model $\\\\mathbf{\u2713}$ is trained to predict the noise added to the latent in step $t$ of the forward process:\\n\\n$$L_{LDM} = \\\\mathbb{E}_{E(x),y,\\\\mathbf{\u2713} \\\\sim \\\\mathcal{N}(0,1),t} k_{\\\\mathbf{\u2713}}(z_t, t, \\\\langle\\\\rangle(y))$$\\n\\nwhere $t \\\\in \\\\{0, \\\\ldots, T\\\\}$. During generation, a pure noise latent $z_T$ and a user-specified prompt are passed through the denoising autoencoder $\\\\mathbf{\u2713}$ for $T$ steps and decoded $D(z_0)$ to generate an image guided by the text prompt.\\n\\nDiffusion for Feature Extraction. Diffusion backbones have been used for downstream vision tasks in several recent works \\\\cite{17,28,43,53}. Due to its public availability and performance in perception tasks, we use a modified version (see Sec. 4.1) of the feature extraction method in VPD. An image latent $z_0 = E(x)$ and a conditioning $C$ are passed through the last step of the denoising process $\\\\mathbf{\u2713}(z_0, 0, C)$. The cross-attention maps $A$ and the multi-scale feature maps $F$ of the U-Net are concatenated $V = A \\\\odot F$ and passed to a task-specific head $H$ to generate a prediction $\\\\hat{p} = H(V)$. The backbone $\\\\mathbf{\u2713}$ and head $H$ are trained with a task-specific loss $L_H(\\\\hat{p}, p)$.\\n\\nAverage EOS Tokens. To generate $C$, previous methods \\\\cite{17,53} rely on a method from CLIP \\\\cite{31} to use averaged text embeddings as representations for the classes in a dataset. A list of 80 sentence templates for each class of interest (such as \\\"a \\\\text{<adjective>} photo of a \\\\text{<class name>}\\\") are passed through the CLIP text encoder. We use $B$ to denote the set of class names in a dataset. For a specific class ($b \\\\in B$), the CLIP text encoder returns an $80 \\\\times N \\\\times D$ tensor, where $N$ is the maximum number of tokens over all the templates, and $D$ is 768 (the dimension of each token embedding). Shorter sentences are padded with EOS tokens to fill out the maximum number of tokens. The first EOS token from each sentence template is averaged and used as the representative embedding for the class such that $C \\\\in \\\\mathbb{R}^{|B| \\\\times 768}$. This method is used in \\\\cite{17,53}, we denote it as $C_{avg}$ and use it as a baseline. For semantic segmentation, all of the class embeddings, irrespective of presence in the image, are passed to the cross-attention layers. Only the class embedding of the room type is passed to the cross-attention layers for depth estimation.\\n\\n3.1. Text-Aligned Diffusion Perception (TADP)\\n\\nOur work proposes a novel method for prompting diffusion-pretrained perception models. Specifically, we explore different prompting methods $G$ to generate $C$. In the single-domain setting, we show the effectiveness of a method...\"}"}
{"id": "CVPR-2024-2381", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overview of TADP. We test several prompting strategies and evaluate their impact on downstream vision task performance. Our method concatenates the cross-attention and multi-scale feature maps before passing them to the vision-specific decoder. In the blue box, we show three single-domain captioning strategies with differing levels of text-image alignment. We propose using BLIP [25] captioning to improve image-text alignment. We extend our analysis to the cross-domain setting (yellow box), exploring whether aligning the source domain text captions to the target domain may impact model performance by appending caption modifiers to image captions generated in the source domain and find model personalization modifiers (Textual Inversion/Dreambooth) work best.\\n\\nThat uses BLIP [25], an image captioning algorithm, to generate a caption as the conditioning for the model: \\n\\n$$G(x) = \\\\tilde{y} \\\\! C.$$ \\n\\nWe then extend our method to the cross-domain setting by incorporating target domain information to \\n\\n$$C = C + M(P)s,$$ \\n\\nwhere \\n\\n$$M$$ is a caption modifier that takes target domain information \\n\\n$$P$$ as input and outputs a caption modification \\n\\n$$M(P)s$$ and a model modification \\n\\n$$M(P)\\\\checkmark.$$ \\n\\nIn Sec. 4, we analyze the text-image interface of the diffusion model by varying the captioner \\n\\n$$G$$ and caption modifier \\n\\n$$M$$ in a systematic manner for three different vision tasks: semantic segmentation, object detection, and monocular depth estimation. Our method and experiments are presented in Fig. 2. Following [53], we train our ADE20k segmentation and NYUv2 depth estimation models with fast and regular schedules. On ADE20k, we train using 4k steps (fast), 8k steps (fast), and 80k steps (normal). For NYUv2 depth, we train on a 1-epoch (fast) schedule and a 25-epoch (normal) schedule. For implementation details, refer to Appendix D.\\n\\n4. Results\\n\\n4.1. Latent scaling\\n\\nBefore exploring image-text alignment, we apply latent scaling to encoded images (Appendix G of Rombach et al. [35]). This normalizes the image latents to have a standard normal distribution. The scaling factor is fixed at 0.18215. We find that latent scaling improves performance using \\n\\n$$C_{avg}$$ for segmentation and depth estimation (Fig. 3). Specifically, latent scaling improves \\n\\n$$\\\\succeq 0.8\\\\%$$ mIoU on Pascal, \\n\\n$$\\\\succeq 0.3\\\\%$$ mIoU on ADE20K, and a relative \\n\\n$$\\\\succeq 5.5\\\\%$$ RMSE on NYUv2 Depth (Fig. 3).\\n\\n| Method       | Avg TA LS G OT | mIoU  |\\n|--------------|----------------|-------|\\n| VPD(R)       | XX X           | 82.34 |\\n| VPD(LS)      | XXX X          | 83.06 |\\n| Class Embs   | XX             | 82.72 |\\n| Class Names  | XX             | 84.08 |\\n| TADP-0       | XX             | 86.36 |\\n| TADP-20      | XX             | 86.19 |\\n| TADP-40      | XX             | 87.11 |\\n| TADP(NO)-20  | X              | 86.35 |\\n| TADP-Oracle  | X              | 89.85 |\\n\\nTable 1. Prompting for Pascal VOC2012 Segmentation. We report the single-scale validation mIoU for Pascal experiments. (R): Reproduction of VPD, Avg: EOS token averaging, LS: Latent Scaling, G: Grammar, OT: Off-target information. For our method, we indicate the minimum length of the BLIP caption with TADP- and nouns only with (NO).\\n\\n4.2. Single-domain alignment\\n\\nAverage EOS Tokens. We scrutinize the use of average EOS tokens for \\n\\n$$C$$ (see Sec. 3). While average EOS tokens are sensible when measuring cosine similarities in the CLIP latent space, it is unsuitable in diffusion models, where the text guides the diffusion process through cross-attention. In our qualitative analysis, we find that average EOS tokens degrade the cross-attention maps (Fig. 4). Instead, we explore using CLIP to embed each class name independently and use the tokens corresponding to the actual word (not the EOS token) and pass this as input to the cross-attention layer:\\n\\n$$G_{ClassEmbs}(B) = \\\\text{concat}(\\\\text{CLIP}(b) | b^2 B) ! C_{ClassEmbs}(2).$$\"}"}
