{"id": "CVPR-2024-1159", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Improving Bird's Eye View Semantic Segmentation by Task Decomposition\\n\\nTianhao Zhao 1\\nYongcan Chen 1 *\\nYu Wu 1\\nTianyang Liu 1\\nBo Du 1 \u2020\\nPeilun Xiao 2\\nShi Qiu 2\\nHongda Yang 2\\nGuozhen Li 2\\nYi Yang 2\\nYutian Lin 1 \u2020\\n\\n1 Institute of Artificial Intelligence, School of Computer Science, Hubei Luojia Laboratory, Wuhan University, Wuhan, China\\n2 Didi Chuxing, China\\n\\n{happytianhao, chenyongcan, wuyucs, bodu, yutian.lin}@whu.edu.cn\\n\\nAbstract\\n\\nSemantic segmentation in bird's eye view (BEV) plays a crucial role in autonomous driving. Previous methods usually follow an end-to-end pipeline, directly predicting the BEV segmentation map from monocular RGB inputs. However, the challenge arises when the RGB inputs and BEV targets from distinct perspectives, making the direct point-to-point predicting hard to optimize. In this paper, we decompose the original BEV segmentation task into two stages, namely BEV map reconstruction and RGB-BEV feature alignment. In the first stage, we train a BEV autoencoder to reconstruct the BEV segmentation maps given corrupted noisy latent representation, which urges the decoder to learn fundamental knowledge of typical BEV patterns. The second stage involves mapping RGB input images into the BEV latent space of the first stage, directly optimizing the correlations between the two views at the feature level. Our approach simplifies the complexity of combining perception and generation into distinct steps, equipping the model to handle intricate and challenging scenes effectively. Besides, we propose to transform the BEV segmentation map from the Cartesian to the polar coordinate system to establish the column-wise correspondence between RGB images and BEV maps. Moreover, our method requires neither multi-scale features nor camera intrinsic parameters for depth estimation and saves computational overhead. Extensive experiments on nuScenes and Argoverse show the effectiveness and efficiency of our method. Code is available at https://github.com/happytianhao/TaDe.\\n\\n1. Introduction\\n\\nIn the field of autonomous driving, bird's eye view (BEV) perception tasks can obtain adequate semantic information such as road layout, object categories, positions, and scales around vehicles. This kind of information is called BEV representation and can be directly applied to downstream tasks such as layout mapping, action prediction, route planning, and collision avoidance. In this work, we focus on the monocular BEV segmentation task, which is a fundamental task that aims to predict road segments from camera inputs.\\n\\nPrevious BEV segmentation methods [6,16,18,23,25,28,31,34,40,41] usually follow an end-to-end pipeline, which adopts a transformer-based architecture to directly predict BEV segmentation maps from the input RGB images. However, different from the general segmentation task, the input images and targets in BEV are of distinct views (perspective view versus bird's eye view), thus it is hard to train the model by direct point-to-point optimization. And it is even harder when attempting to achieve both source information perception and target view generation in a single step. As shown in Figure 1 (b), the generated BEV segmentation map of the end-to-end baseline looks distorted and does not conform to the expected regular patterns found in real-world traffic scenes.\\n\\nTo achieve the complex cross-view BEV segmentation, we propose a Task Decomposition (TaDe) method that decompose the traditional end-to-end pipeline into two distinct but goal-oriented stages. In the first stage, we train an BEV autoencoder to reconstruct the BEV segmentation maps given corrupted noisy latent representation, which urges the decoder to learn fundamental knowledge of typical BEV patterns. The second stage involves mapping RGB input images into the BEV latent space of the first stage, directly optimizing the correlations between the two views at the feature level. Our approach simplifies the complexity of combining perception and generation into distinct steps, equipping the model to handle intricate and challenging scenes effectively. Besides, we propose to transform the BEV segmentation map from the Cartesian to the polar coordinate system to establish the column-wise correspondence between RGB images and BEV maps. Moreover, our method requires neither multi-scale features nor camera intrinsic parameters for depth estimation and saves computational overhead. Extensive experiments on nuScenes and Argoverse show the effectiveness and efficiency of our method. Code is available at https://github.com/happytianhao/TaDe.\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2024-1159", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"autoencoder model to learn a basic understanding of typical BEV scene patterns, enabling it to generate coherent and rational BEV segmentation maps independent from RGB images. In the second stage, given input RGB images, we only intend to obtain the latent BEV feature, leaving the workload of BEV segmentation map prediction to the pretrained decoder of the first stage. This two-stage approach, with each stage focusing on only one mission, equips the model with the capacity to handle more intricate and challenging scene perception. As shown in Figure 1 (c), with the help of task decomposition, the predicted BEV segmentation maps share greater structural similarity with the RGB image.\\n\\nTo be specific, in the first stage, to generate BEV maps that are consistent with physical rules, it is crucial to have a specific expert to comprehend the global traffic layout. For instance, walkways are usually to be continuous rather than discontinuous, and cars are supposed to be shaped as filled rectangles instead of hollow circles. To incorporate this prior knowledge, we employ an autoencoder to learn how to reconstruct/generate the target BEV segmentation maps. To learn a more intrinsic and robust generating ability, we disturb the autoencoder by corrupting the latent representation with random noise during training and enforce the model perfectly reconstruct the ground truth with only the noised latent. This reconstruction can locally recognize and predict typical patterns in BEV segmentation maps, thereby ensuring reasonable BEV map generation given imperfect latent representations.\\n\\nIn the second stage, we aim to learn a smooth transition from the perspective RGB input to the BEV semantic latent space. Specifically, the model encodes the input RGB images into features and leverages a column-wise transformer to transit them into the BEV latent space. The training objective of the second stage is to ensure that the transited RGB representations align seamlessly with the target BEV latent space. Thus we input the BEV label maps to the frozen BEV encoder of Stage I to get the BEV latent feature, and then take this BEV feature as the optimization target for the transition model of Stage II. Different from existing works, the optimization is in the autoencoder latent space, rather than the traditional pixel segmentation classification space. During inference, since the representations of both views have been already aligned, the transited RGB feature can directly go through the BEV decoder of Stage I for the final BEV segmentation mask prediction.\\n\\nFurthermore, to achieve better cross-view alignment, we introduce a coordinate transform step that converts the original BEV segmentation map from Cartesian to polar coordinates. In the transformed BEV map, each column shares the same spatial information with the input monocular RGB images, which also benefits the smooth transition of cross-view features in the second stage. Extensive experiments on two large-scale datasets including nuScenes \\\\cite{2} and Argoverse \\\\cite{4} clearly demonstrate our superiority compared to prior end-to-end works.\\n\\nOur contributions can be summarized as follows:\\n\\n\u2022 We break down the traditional end-to-end pipeline into two separate stages, with each stage focusing on only one mission (either generation or perception).\\n\u2022 We propose a BEV autoencoder to automatically learn typical patterns by reconstructing BEV maps from corrupted latent features, ensuring the decoder prediction is rational and adheres to the real world.\\n\u2022 We also transform the BEV label map from Cartesian to polar coordinates to learn better alignment across perspective views and bird's eye views.\\n\\n2. Related Works\\n\\n2.1. Bird's Eye View (BEV) Perception\\n\\nAs BEV representations contain rich semantic information and can be directly applied to numerous downstream tasks, BEV perception has received widespread attention in the field of autonomous driving. However, converting the perspective view captured by cameras to the bird's eye view is an ill-posed problem, which makes it hard to meet the autonomous driving requirements of complex real-world scenarios. Early works \\\\cite{13, 14, 17, 22, 26, 27, 29, 39} addressed this challenge based on a bottom-up strategy. They first predict the position in the 3D space by estimating the depth of each pixel in the perspective view and then project the 3D position to the BEV space based on the internal and external parameters of the cameras. Nevertheless, despite being intuitive, bottom-up methods depend on the accuracy of depth estimation and have high computational complexity. Recent works follow a top-down strategy. Inspired by DETR \\\\cite{3}, DETR3D \\\\cite{36} detects objects from object queries in 3D space with a transformer. BEVFormer \\\\cite{18} and BEVFormer v2 \\\\cite{37} add dense object queries based on DETR3D to achieve map segmentation. PETR \\\\cite{20} and PETRv2 \\\\cite{21} introduce 3D position encoding to DETR3D and significantly simplify the pipeline.\\n\\n2.2. Monocular BEV Segmentation\\n\\nTraditional BEV perception methods focus on calculating the corresponding relationship between the coordinates of the perspective view and the bird's eye view by using the internal and external parameters of the cameras. However, due to the geometric distortion and the presence of overlapping parts and gaps between the perspective views, the perception effect is not ideal. In this regard, some works \\\\cite{1, 5, 10, 11, 24, 33} propose to predict the BEV segmentation map of road layout and objects directly in the case of giving a monocular perspective view.\"}"}
{"id": "CVPR-2024-1159", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"PYVA proposes a GAN-based framework and designs a cross-view conversion module to handle significant differences between views. VED employs a variational encoder-decoder network to directly anticipate a semantic occupancy grid from an image. However, the inclusion of a fully connected bottleneck layer in the network results in the loss of significant spatial context and an output that is relatively coarse and incapable of capturing smaller objects like pedestrians. CVT designs a transformer and utilizes the camera's intrinsic and extrinsic calibrations to efficiently understand the surrounding environment. PON adopts a semantic Bayesian occupancy grid framework to estimate BEV segmentation maps directly from monocular perspective views. HFT provides a balanced approach that combines structured geometric information from model-based methods with the global context-capturing ability of model-free methods to overcome their respective limitations in image processing. DiffBEV and DDP propose an end-to-end framework to generate a more comprehensive BEV representation and denoise noisy samples by applying the diffusion model to BEV perception.\\n\\n2.3. Polar-based Methods\\n\\nSo far, many works have made significant progress in the BEV segmentation task. However, due to the fact that the perspective view is established in a 3D polar coordinate system, while the bird's eye view is established in a 2D Cartesian coordinate system with the camera as the origin, this gap has led to the previous method's unsatisfactory performance. Therefore, in order to reduce this gap, some methods choose to convert the ground truth of the bird's eye view in the Cartesian coordinate system to the polar coordinate system, corresponding to the perspective view, in order to better learn the relationship between them. PolarDETR converts the object representation in DETR3D from the Cartesian coordinate system to the polar coordinate system. PolarFormer incorporates a polar alignment module to aggregate rays from multiple cameras and generate a structured polar feature map. TIM formulates BEV segmentation map generation from an image as a set of sequence-to-sequence translations and proposes a novel form of end-to-end transformer network to translate between a vertical scanline in the image and rays passing through the camera location in the bird's eye view. GitNet first learns visibility-aware features and learnable geometry to translate into BEV space and then deforms the pre-aligned coarse BEV features with visibility knowledge by ray-based transformers.\\n\\n3. Method\\n\\nWe propose TaDe (Task Decomposition) to break down the traditional end-to-end training pipeline into two simple and distinct stages: (1) With the BEV autoencoder, BEV segmentation maps with 50% noise are reconstructed to recognize and predict the typical patterns and obtain the BEV latent representations. (2) With the RGB-BEV alignment, the input monocular RGB images are mapped into the BEV latent space via a column-wise transformer to achieve view alignment. During testing, the BEV decoder is employed to generate BEV semantic segmentation maps. In this section, we first introduce the coordinate system transformation of target BEV maps in Section 3.1. After that, we elaborate on the BEV autoencoder and column-wise feature mapping in Section 3.2. and Section 3.3., respectively.\"}"}
{"id": "CVPR-2024-1159", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This document discusses a two-stage method for generating BEV (Bird's-Eye View) maps. The first stage involves training a BEV autoencoder, and the second stage aligns RGB (Red-Green-Blue) images to the BEV latent representations for decoding.\\n\\n### Stage 1: BEV Autoencoder\\n\\nThe BEV autoencoder consists of a BEV encoder and a BEV decoder. Given a BEV segmentation map in polar coordinates, the autoencoder is represented as follows:\\n\\n\\\\[\\n\\\\begin{align*}\\n    z &= E(y), \\\\\\\\\\n    \\\\hat{y} &= D(\\\\tilde{z}),\\n\\\\end{align*}\\n\\\\]\\n\\nWhere \\\\(z\\\\) and \\\\(\\\\hat{z}\\\\) denote the BEV latent representation, \\\\(\\\\hat{y}\\\\) denotes the reconstructed vector in input space. The latent feature \\\\(z\\\\) is obtained with a low scale of \\\\(8 \\\\times 22\\\\), which can be seen as a latent representation of \\\\(y\\\\).\\n\\nDue to the simplicity of BEV segmentation maps, the criterion is not sufficient to obtain a useful representation or effective decoder. Therefore, a large amount of Gaussian noise (e.g., 50%) is added to the latent feature map \\\\(z\\\\), forcing the BEV decoder to predict the regular traffic scene patterns from noise. The latent feature with noise is calculated by:\\n\\n\\\\[\\n\\\\tilde{z} = p_1 - \\\\eta z + \\\\sqrt{\\\\eta \\\\epsilon},\\n\\\\]\\n\\nwhere \\\\(\\\\eta\\\\) is set to 0.5 in our experiments and \\\\(\\\\epsilon\\\\) denotes the standard Gaussian distribution, which shares the same dimensionality with \\\\(z\\\\).\\n\\nThe BEV autoencoder is trained using a weighted BCE loss to reconstruct the input \\\\(y\\\\) as accurately as possible, ensuring that the latent feature map \\\\(z\\\\) retains as much information from the original input \\\\(y\\\\) as possible:\\n\\n\\\\[\\nL_{BCE} = -w(y \\\\log \\\\hat{y} + (1 - y) \\\\log(1 - \\\\hat{y}))\\n\\\\]\\n\\nWhere \\\\(w\\\\) denotes the training weights of each class. During implementation, the reciprocal of the frequency of occurrence is used as the weight for each class, enabling balanced learning for each type of object.\\n\\n### Stage 2: RGB-BEV Alignment\\n\\nThe goal of RGB-BEV alignment is to learn a transition from the perspective RGB images to the BEV latent representations. This is achieved with the pretrained BEV autoencoder, leveraging the BEV latent representation for alignment while using the BEV decoder for BEV segmentation map generation during testing.\"}"}
{"id": "CVPR-2024-1159", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Florent Bartoccioni, \u00b4Eloi Zablocki, Andrei Bursuc, Patrick P\u00e9rez, Matthieu Cord, and Karteek Alahari. Lara: Latents and rays for multi-camera bird\u2019s-eye-view semantic segmentation. In Conference on Robot Learning, pages 1663\u20131672. PMLR, 2023.\\n\\n[2] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-modal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621\u201311631, 2020.\\n\\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer, 2020.\\n\\n[4] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, Dea Wang, Peter Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d tracking and forecasting with rich maps. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8748\u20138757, 2019.\\n\\n[5] Shaoyu Chen, Tianheng Cheng, Xinggang Wang, Wenming Meng, Qian Zhang, and Wenyu Liu. Efficient and robust 2d-to-bev representation learning via geometry-guided kernel transformer. arXiv preprint arXiv:2206.04584, 2022.\\n\\n[6] Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang, Chang Huang, and Wenyu Liu. Polar parametrization for vision-based surround-view 3d detection. arXiv preprint arXiv:2206.10965, 2022.\\n\\n[7] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[8] Isht Dwivedi, Srikanth Malla, Yi-Ting Chen, and Behzad Dariush. Bird\u2019s eye view segmentation using lifted 2d semantic features. In British Machine Vision Conference (BMVC), pages 6985\u20136994, 2021.\\n\\n[9] Shi Gong, Xiaoqing Ye, Xiao Tan, Jingdong Wang, Errui Ding, Yu Zhou, and Xiang Bai. Gitnet: Geometric prior-based transformation for birds-eye-view segmentation. In European Conference on Computer Vision, pages 396\u2013411. Springer, 2022.\\n\\n[10] Nikhil Gosala, Kursat Petek, Paulo LJ Drews-Jr, Wolfram Burgard, and Abhinav Valada. Skyeye: Self-supervised bird\u2019s-eye-view semantic mapping using monocular frontal view images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14901\u201314910, 2023.\\n\\n[11] Nikhil Gosala and Abhinav Valada. Bird\u2019s-eye-view panoramic segmentation using monocular frontal view images. IEEE Robotics and Automation Letters, 7(2):1968\u20131975, 2022.\\n\\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[13] Anthony Hu, Zak Murez, Nikhil Mohan, Sof\u00eda Dudas, Jeffrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and Alex Kendall. Fiery: Future instance prediction in bird\u2019s-eye view from surround monocular cameras. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15273\u201315282, 2021.\\n\\n[14] Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, and Dalong Du. Bevdet: High-performance multi-camera 3d object detection in bird-eye-view. arXiv preprint arXiv:2112.11790, 2021.\\n\\n[15] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp: Diffusion model for dense visual prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21741\u201321752, 2023.\\n\\n[16] Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin Gao, Weiming Hu, and Yu-Gang Jiang. Polarformer: Multi-camera 3d object detection with polar transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1042\u20131050, 2023.\\n\\n[17] Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran Wang, Yukang Shi, Jianjian Sun, and Zeming Li. Bevdepth: Acquisition of reliable depth for multi-view 3d object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1477\u20131485, 2023.\\n\\n[18] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning bird\u2019s-eye-view representation from multi-camera images via spatiotemporal transformers. In European conference on computer vision, pages 1\u201318. Springer, 2022.\\n\\n[19] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2117\u20132125, 2017.\\n\\n[20] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun. Petr: Position embedding transformation for multi-view 3d object detection. In European Conference on Computer Vision, pages 531\u2013548. Springer, 2022.\\n\\n[21] Yingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Aqi Gao, Tiancai Wang, and Xiangyu Zhang. Petrv2: A unified framework for 3d perception from multi-camera images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3262\u20133272, 2023.\\n\\n[22] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L Rus, and Song Han. Bevfusion: Multi-task multi-sensor fusion with unified bird\u2019s-eye view representation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 2774\u20132781. IEEE, 2023.\\n\\n[23] Chenyang Lu, Marinus Jacobus Gerardus van de Molengraft, and Gijs Dubbelman. Monocular semantic occupancy grid mapping with convolutional variational encoder\u2013decoder networks. IEEE Robotics and Automation Letters, 4(2):445\u2013452, 2019.\\n\\n[24] Jiachen Lu, Zheyuan Zhou, Xiatian Zhu, Hang Xu, and Li Zhang. Learning ego 3d representation as ray tracing.\"}"}
{"id": "CVPR-2024-1159", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[25] Kaustubh Mani, Swapnil Daga, Shubhika Garg, Sai Shankar Narasimhan, Madhava Krishna, and Krishna Murthy Jatavlabhula. Monolayout: Amodal scene layout from a single image. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1689\u20131697, 2020.\\n\\n[26] Bowen Pan, Jiankai Sun, Ho Yin Tiga Leung, Alex Andonian, and Bolei Zhou. Cross-view semantic segmentation for sensing surroundings. IEEE Robotics and Automation Letters, 5(3):4867\u20134873, 2020.\\n\\n[27] Jinhyung Park, Chenfeng Xu, Shijia Yang, Kurt Keutzer, Kris M Kitani, Masayoshi Tomizuka, and Wei Zhan. Time will tell: New outlooks and a baseline for temporal multi-view 3d object detection. In The Eleventh International Conference on Learning Representations, 2022.\\n\\n[28] Lang Peng, Zhirong Chen, Zhangjie Fu, Pengpeng Liang, and Erkang Cheng. Bevsegformer: Bird's eye view semantic segmentation from arbitrary camera rigs. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5935\u20135943, 2023.\\n\\n[29] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIV, pages 194\u2013210. Springer, 2020.\\n\\n[30] Lennart Reiher, Bastian Lampe, and Lutz Eckstein. A sim2real deep learning approach for the transformation of images from multiple vehicle-mounted cameras to a semantically segmented image in bird's eye view. In 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), pages 1\u20137. IEEE, 2020.\\n\\n[31] Thomas Roddick and Roberto Cipolla. Predicting semantic map representations from images using pyramid occupancy networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11138\u201311147, 2020.\\n\\n[32] Thomas Roddick, Alex Kendall, and Roberto Cipolla. Orthographic feature transform for monocular 3d object detection. British Machine Vision Conference, 2019.\\n\\n[33] Avishkar Saha, Oscar Mendez, Chris Russell, and Richard Bowden. Enabling spatio-temporal aggregation in birds-eye-view vehicle estimation. In 2021 ieee international conference on robotics and automation (icra), pages 5133\u20135139. IEEE, 2021.\\n\\n[34] Avishkar Saha, Oscar Mendez, Chris Russell, and Richard Bowden. Translating images into maps. In 2022 International conference on robotics and automation (ICRA), pages 9200\u20139206. IEEE, 2022.\\n\\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\n[36] Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, and Justin Solomon. Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In Conference on Robot Learning, pages 180\u2013191. PMLR, 2022.\\n\\n[37] Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao, Lewei Lu, et al. Bevformer v2: Adapting modern image backbones to bird's-eye-view recognition via perspective supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17830\u201317839, 2023.\\n\\n[38] Weixiang Yang, Qi Li, Wenxi Liu, Yuanlong Yu, Yuexin Ma, Shengfeng He, and Jia Pan. Projecting your view attentively: Monocular road scene layout estimation via cross-view transformation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15536\u201315545, 2021.\\n\\n[39] Yunpeng Zhang, Zheng Zhu, Wenzhao Zheng, Junjie Huang, Guan Huang, Jie Zhou, and Jiwen Lu. Beverse: Unified perception and prediction in birds-eye-view for vision-centric autonomous driving. arXiv preprint arXiv:2205.09743, 2022.\\n\\n[40] Brady Zhou and Philipp Kr\u00e4henb\u00fchl. Cross-view transformers for real-time map-view semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13760\u201313769, 2022.\\n\\n[41] Jiayu Zou, Zheng Zhu, Junjie Huang, Tian Yang, Guan Huang, and Xingang Wang. Hft: Lifting perspective representations via hybrid feature transformation for bev perception. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 7046\u20137053. IEEE, 2023.\\n\\n[42] Jiayu Zou, Zheng Zhu, Yun Ye, and Xingang Wang. Diffbev: Conditional diffusion model for bird's eye view perception. arXiv preprint arXiv:2303.08333, 2023.\"}"}
{"id": "CVPR-2024-1159", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"image is taken as the input into the pipeline, where the output is supposed to predict the latent representation $z$. \\n\\nFollowing previous works [31, 34], we first resize the input RGB images from 900\u00d71600 to 396\u00d7704, and then crop off the upper part that contains inessential information like the sky and buildings, leaving only the bottom 256\u00d7704, in order to better adapt to our model.\\n\\nAt the beginning of the pipeline, we primarily utilize ResNet [12] and FPN [19], denoted as $F(\\\\cdot)$ together, to extract the feature maps $f$ with a lower scale of 64\u00d7176 from the preprocessed input RGB image $x$:\\n\\n$$f = F(x).$$ (5)\\n\\nDespite the fact that we have transformed the BEV segmentation map from Cartesian to polar coordinates to establish spatial correspondence with RGB images from a perspective view of polar coordinates, it is noteworthy that for each column, objects in the RGB image adhere to the characteristic of being larger closer and smaller farther away, whereas objects in the BEV segmentation map maintain the same scale regardless of their distance. To address this, we treat this task as a column-wise sequence-to-sequence translation problem and employ a transformer [35] following [9, 34]. Specifically, we first permute and reshape the feature map from $f_{N \\\\times C \\\\times H \\\\times W}$ to $f_{H \\\\times (N \\\\times W) \\\\times C}$ and treat each column as a sequence input to the transformer [35] to produce a feature map $t_{H \\\\times (N \\\\times W) \\\\times C}$, which is then resized back to the scale of $t_{N \\\\times C \\\\times H \\\\times W}$.\\n\\nFor the last module of the pipeline, we apply multiple convolutional layers, denoted as $\\\\text{conv}(\\\\cdot)$, to reduce the scale (64\u00d7176) of the feature map $t$ obtained from the transformer to match the scale (8\u00d722) of the feature map $\\\\hat{z}$, $\\\\hat{z} = \\\\text{conv}(t)$, (6) where $\\\\hat{z}$ is the output of the whole pipeline.\\n\\nIn the training phase, we utilize the mean squared error (MSE) loss to facilitate the output of the pipeline to approach the latent feature map $z$ generated by the frozen BEV encoder according to Equation 1:\\n\\n$$L_{\\\\text{MSE}} = (\\\\hat{z} - z)^2.$$ (7)\\n\\nDuring testing, we use the frozen BEV decoder to decode the feature map $\\\\hat{z}$ predicted by the pipeline into the BEV segmentation map according to Equation 2. Finally, we transform it from the polar to the Cartesian coordinates system following Section 3.1.\\n\\nAlthough we can decode the aligned latent feature map $\\\\hat{z}$ predicted from the pipeline with the frozen BEV decoder into the BEV segmentation map and achieve good performance, we observe that the decoder has only been exposed to samples of BEV segmentation maps during its training process and thus struggles to adequately adapt to the data distribution of input RGB images. To address this issue, we introduce a fine-tuning strategy that enables the decoder to better adapt to the data distributions of both input RGB images and latent features, thus establishing a connection between the data distribution of BEV segmentation maps and that of input RGB images. To achieve this, we freeze the parameters of the pipeline and only apply fine-tuning to the BEV decoder using the BCE loss in Equation 4. \\n\\n**4. Experiments**\\n\\n**4.1. Experimental Setup**\\n\\n**Datasets.** The nuScenes dataset [2] comprises 1,000 scenes captured by six cameras in Boston and Singapore. Each scene spans approximately 20 seconds. For semantic mapping, nuScenes provides detailed vectorized maps covering 11 semantic classes. In terms of dynamic objects, the dataset includes comprehensive annotations of 3D bounding boxes for 23 classes, complemented by 8 unique attributes. The Argoverse dataset [4] includes 65 training and 24 validation sequences recorded in Miami and Pittsburgh, utilizing seven surround-view cameras. Similar to the nuScenes dataset, Argoverse offers 3D object annotations across 15 categories and detailed semantic map data that encompasses road masks, lane geometry, and ground elevation. For a fair comparison, we use the same ground truth generation process, layout and object classes, and training/validation splits outlined in PON [31].\\n\\n**Evaluation Metric.** We present our results using the Intersection-over-Union (IoU) metric across background classes. Additionally, we compute the class-averaged mean IoU to assess overall performance. We ignore invisible grid cells during the evaluation process, as a LiDAR ray that cannot penetrate them will render them invisible in the image. Considering the possibility of class overlap, such as a car driving in the drivable area, we perform an independent binary segmentation evaluation for each category. We then determine the best performance by selecting the maximum IoU from various thresholds.\\n\\n**Implementation Details.** Referring to PON [31], we perform segmentation in a $[0, 50] \\\\times [-25, 25]$ square meters region around the ego car with a resolution of 0.25 meters per pixel, resulting in a final map size of 200\u00d7200. For each stage and fine-tuning, we conduct training for 50 epochs. We employ the Lion optimizer [7], set the initial learning rates at $5 \\\\times 10^{-4}, 2 \\\\times 10^{-5}, 2 \\\\times 10^{-4}$ for the first, second, and fine-tuning stages, respectively. Additionally, we use a weight decay factor of 0.01, complemented by warm-up and cosine learning rate decay strategies.\\n\\n**4.2. Quantitative Comparison** Although our method predicts based on monocular RGB images without exploring the relationships between multi...\"}"}
{"id": "CVPR-2024-1159", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Quantitative results of IoU (%) on the nuScenes [2] validation set. \u201cMean\u201d indicates IoU across all classes. \u201cCrossing\u201d, \u201cC.V.\u201d, \u201cMotor.\u201d, \u201cPed.\u201d, and \u201cCone\u201d denote pedestrian crossing, construction vehicle, motorcycle, pedestrian, and traffic cone, respectively. IPM and Depth Unpr. are proposed in PON [31].\\n\\nTable 2. Quantitative results of IoU (%) on the Argoverse [4] validation set.\\n\\nTable 3. Comparison of computational overhead on nuScenes [2]. We report the GPU hours of BEV autoencoder, RGB-BEV alignment, and fine-tuning of our method respectively.\\n\\nComputational Overhead. We compare the computational overhead with two open-source methods, considering the following four metrics: parameters, FLOPs, FPS, and GPU hours, as presented in Table 3. GPU hours represent the estimated GPU (NVIDIA GeForce RTX 4090) hours required for training to converge. Notably, in inference, our method achieves the best FLOPs and FPS. In terms of training, despite three stages, our method consumes 80 GPU hours in total, while TIM [34] requires 160 GPU hours. In summary, compared with state-of-the-art methods, our method achieves the best results and maintains low computational overhead thanks to not using multi-scale features.\\n\\nIoU over Distance. We further discuss the experimental results of IoU over distance. We divide the BEV maps into 5-meter intervals along the distance from the camera and report the mIoU for layouts and objects at various distances in Figure 5. The experimental results met our expectations, with IoU showing a decreasing trend as the distance increases.\"}"}
{"id": "CVPR-2024-1159", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative results on nuScenes [2]. We compare with other methods following the color scheme used in PON [31].\\n\\nFigure 5. Trend demonstration and comparison with PON [31] of IoU over distance (0-50m) on nuScenes [2].\\n\\nbetween the target and the camera increases. Compared to PON [31], we achieve significant and consistent improvements across distances. Notably, for layouts such as drivable areas, we obtain even more absolute performance gains at larger distances. As for small objects like pedestrians and bikes, we observe a relative performance improvement of 200% (5.4 vs 1.8) at 45-50 meters, while the relative improvement at 0-5 meters is 173% (38.7 vs 14.2). In summary, our method shows its superiority, especially at large distances for both layouts and objects.\\n\\n4.3. Ablation Study\\n\\nIn this section, we explore the effectiveness of each proposed component through an ablation study on nuScenes [2]. The experimental groups and results are shown in Table 4.\\n\\n| Group | CST | TD | CWT | FT | Layout | Large Obj. | Small Obj. | Mean |\\n|-------|-----|----|-----|----|--------|------------|-----------|------|\\n| I     | \u2713   |    |     |    | 35.0   | 22.0       | 10.4      | 21.6 |\\n| II    | \u2713   | \u2713  |     |    | 41.9   | 22.9       | 11.7      | 24.4 |\\n| III   | \u2713   | \u2713  |     |    | 41.5   | 25.6       | 13.8      | 25.9 |\\n| IV    | \u2713   | \u2713  | \u2713   |    | 44.1   | 27.5       | 13.9      | 27.4 |\\n| V     | \u2713   | \u2713  | \u2713   | \u2713  | 41.2   | 26.4       | 15.5      | 26.7 |\\n| VI    | \u2713   | \u2713  | \u2713   | \u2713  | 44.8   | 28.6       | 15.9      | 28.7 |\\n| VII   | \u2713   | \u2713  | \u2713   | \u2713  | 45.0   | 29.2       | 16.6      | 29.2 |\\n\\nTable 4. Ablations of different key components on nuScenes [2]. CST, TD, CWT, and FT denote coordinate system transformation, task decomposition, column-wise transformer, and fine-tuning, respectively. Large objects include bus, car, construction vehicle, trailer, and truck. Small objects include barrier, bike, motorcycle, pedestrian, and traffic cone.\\n\\nTask Decomposition.\\n\\nWe deploy ablations to demonstrate the effectiveness of task decomposition. For Groups I, III, and V in Table 4, we train the pipeline and the BEV decoder simultaneously with the BCE loss in an end-to-end paradigm. Comparing Group V and Group VI, we observe that task decomposition brings a significant improvement to layouts (41.2 vs 44.8) and large objects (26.4 vs 28.6) which commonly appearing typical patterns. Similar improvements could be observed in the comparison between Group III and Group IV, especially between Group I and Group II (6.9 IoU improvement to layouts). The above ablations demonstrate that the BEV autoencoder successfully learned prior patterns of traffic scenes to decode a more rational BEV segmentation map.\\n\\nColumn-wise Transformer.\\n\\nIn RGB images, objects exhibit the characteristic of appearing larger when closer and smaller when farther away, while in BEV segmentation maps, objects maintain the same scale regardless of their distance. Motivated by TIM [34], we adopt a column-wise transformer to translate each column of the feature map to match the characteristics between the perspective view and the bird's eye view. As shown in Table 4, when compared with Group IV and Group VI, the column-wise transformer improves the IoU of small objects significantly (13.9 vs 15.5).\"}"}
{"id": "CVPR-2024-1159", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Qualitative results of ablations on nuScenes [2]. I, III, V, and VI are corresponded to the relevant ablations in Table 4.\\n\\n15.9) while also improving the IoU of large objects (27.5 vs 28.6). Similar improvements are observed in the comparison between Group III and Group V. These experimental results prove that the column-wise transformer could better capture the information of objects.\\n\\n4.4. Qualitative Comparison\\n\\nFigure 4 shows qualitative results on nuScenes [2], comparing our predicted segmentation maps with other methods and the ground truth. Our segmentation maps appear more precise and rational, benefiting from our task decomposition into two stages. Specifically, our model demonstrates a more coherent road layout, pedestrian crossing, and walkway layout, thanks to prior knowledge learned in advance. For dynamic objects, our model effectively leverages the polar coordinate system to predict nearby and distant objects with high accuracy, particularly excelling in determining the scale of cars and buses.\\n\\nFigure 6 provides further qualitative results of our ablation experiments. Groups III, V, and VI in the first row display more coherent and accurate road layouts due to the introduction of the coordinate system transformation. Group V in the second row successfully predicts distant cars, thanks to the effectiveness of the column-wise transformer in translating each column. In the third row of Figure 6, Group VI demonstrates that the BEV autoencoder has successfully learned prior patterns about layouts and objects, accurately predicting the size of cars and the layout of pedestrian crossings compared to other experiments.\\n\\n5. Conclusion\\n\\nIn this paper, we leverage a clearer and more direct optimization target, by decomposing the end-to-end pipeline into two separate stages, with each stage focusing on only one mission (either generation or perception). Specifically, in the first stage, we propose a BEV autoencoder to reconstruct the BEV segmentation map with corrupted noise, where the decoder is forced to learn typical BEV traffic scene patterns. In the second stage, a mapping from input RGB images and BEV latent representations is learned. Furthermore, we transform the BEV segmentation map from a Cartesian coordinate system to a polar one, thus enhancing its alignment with RGB input. Notably, our method abstains from utilizing multi-scale features or camera intrinsic parameters for depth estimation, resulting in a significantly lower computational overhead. Additionally, our method consistently achieves outstanding prediction accuracy across varying distances from the camera. Extensive experiments on two datasets demonstrate our effectiveness.\\n\\nAcknowledgement\\n\\nThis work was partially supported by the National Natural Science Foundation of China under Grant 62372341 and sponsored by CCF-DiDi GAIA Collaborative Research Funds for Young Scholars.\"}"}
