{"id": "CVPR-2023-1041", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Joint HDR Denoising and Fusion: A Real-World Mobile HDR Image Dataset\\n\\nShuaizheng Liu\u00b9,\u00b2, Xindong Zhang\u00b9,\u00b2, Lingchen Sun\u00b9,\u00b2, Zhetong Liang\u00b2, Hui Zeng\u00b2, Lei Zhang\u00b9,\u00b2*\\n\\n\u00b9The HongKong Polytechnic University, \u00b2OPPO Research\\n\\n{shuaizhengliu21, cshzeng} @gmail.com; ling-chen.sun@connect.polyu.hk; liangzhetong@oppo.com; {csxdzhang, cslzhang} @comp.polyu.edu.hk\\n\\nAbstract\\n\\nMobile phones have become a ubiquitous and indispensable photographing device in our daily life, while the small aperture and sensor size make mobile phones more susceptible to noise and over-saturation, resulting in low dynamic range (LDR) and low image quality. It is thus crucial to develop high dynamic range (HDR) imaging techniques for mobile phones. Unfortunately, the existing HDR image datasets are mostly constructed by DSLR cameras in daytime, limiting their applicability to the study of HDR imaging for mobile phones. In this work, we develop, for the first time to our best knowledge, an HDR image dataset by using mobile phone cameras, namely Mobile-HDR dataset. Specifically, we utilize three mobile phone cameras to collect paired LDR-HDR images in the raw image domain, covering both daytime and nighttime scenes with different noise levels. We then propose a transformer based model with a pyramid cross-attention alignment module to aggregate highly correlated features from different exposure frames to perform joint HDR denoising and fusion. Experiments validate the advantages of our dataset and our method on mobile HDR imaging. Dataset and codes are available at https://github.com/shuaizhengliu/Joint-HDRDN.\\n\\n1. Introduction\\n\\nWith the rapid development of mobile communication techniques and digital imaging sensors, mobile phones have surpassed DSLR cameras and become the most prevalent device for photography in our daily life. Nonetheless, due to the low dynamic range (LDR) of mobile phone sensors [7], the captured images may lose details in dark and bright regions under challenging lighting conditions. Therefore, high dynamic range (HDR) imaging [3] is critical for improving the quality of mobile phone photography.\\n\\nActually, HDR imaging has been a long standing research topic in computational photography, even for DSLR cameras. An effective and commonly used way to construct an HDR image is to fuse a stack of LDR frames with different exposure levels. If the multiple LDR frames can be well aligned (e.g., in static scenes), they can be easily fused to generate the HDR image [3, 23]. Unfortunately, in dynamic scenes where there exist camera shaking and/or object motion, the fused HDR image may introduce ghost artifacts caused by inaccurate alignment [45]. Some deghosting methods have been proposed to reject pixels which can be hardly registered [12, 16]. However, precisely detecting moving pixels is challenging and rejecting too many pixels will sacrifice useful information for HDR fusion.\\n\\nIn the past decade, deep learning [15] has demonstrated its powerful capability to learn image priors from a rich amount of data [4]. Unfortunately, the development of deep models for HDR imaging is relatively slow, mainly due to the lack of suitable training datasets. Kalantari et al. [10] built the first dataset with LDR-HDR image pairs by DSLR cameras in daytime. Benefiting from this dataset, many deep learning algorithms have been proposed for HDR imaging. Some works [10] employ the convolutional neural network (CNN) for fusion after aligning multiple frames with optical flow [18], which is however unreliable under occlusion and large motions. Subsequent works resort to employing various networks to directly reconstruct the HDR image from LDR frames. Liu et al. [19] developed a deformable convolution based module to align the features of input frames. Yan et al. [40] proposed a spatial attention mechanism to suppress undesired features and employed a dilated convolution network [42] for frame fusion. Following this spatial attention mechanism, some fusion networks have been developed with larger receptive fields, such as non-local networks [41] and Transformer networks [21].\\n\\nThough the dataset developed in [10] has largely facilitated the research of deep learning on HDR imaging, it is not well suited for the investigation of HDR imaging techniques for mobile phone cameras. Firstly, due to the small aperture and sensor size, images captured by mobile phones...\"}"}
{"id": "CVPR-2023-1041", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are more susceptible to noise than DSLR cameras, especially in nighttime. However, the images in dataset [10] are generally very clean since they are collected by DSLR cameras in daytime. Compared with DSLR cameras, the normal exposed frame of mobile phone cameras contains stronger noise, which should be reduced by fusing with other frames. Secondly, mobile phone cameras often have fewer recording bits (12 bit) than DSLR (14 bit), resulting in larger overexposure areas in the reference frame. Therefore, mobile HDR imaging is a more challenging problem, and new dataset and new solutions are demanded.\\n\\nTo address the above limitations of existing HDR datasets and facilitate the research on real-world mobile HDR imaging, we establish a new HDR dataset, namely Mobile-HDR, by using mobile phone cameras. Specifically, we utilize three mobile phones to collect LDR-HDR image pairs in raw image domain, covering both daytime and nighttime scenes with different noise levels. In order to obtain high-quality ground truth of HDR images, we first collect noise-free LDR images under each exposure by multi-frame averaging, and then synthesize the ground truth HDR image by fusing the generated clean LDR frames. For dynamic scenes with object motion, we follow [10] to first capture multiple exposed frames from static scenes to synthesize the ground truth HDR images, and then replace the non-reference frames with the images captured in dynamic scenes as input. To our best knowledge, this is the first mobile HDR dataset with paired training data.\\n\\nWith the established dataset, we propose a new transformer based model for joint HDR denoising and fusion. To enhance denoising and achieve alignment, we design a pyramid cross-attention module to implicitly align and fuse input features. The cross-attention operation enables searching and aggregating highly correlated features from different frames, while the pyramid structure facilitates the feature alignment under severe noise, large overexposure and large motion. A transformer module is then applied to fuse the aligned features for HDR image recovery.\\n\\nThe contributions of our work can be summarized as follows. First, we build the first mobile HDR dataset with LDR-HDR image pairs under various scenes. Second, we propose a cross-attention based alignment module to perform effective joint HDR denoising and fusion. Third, we perform extensive experiment to validate the advantages of our dataset and model. Our work provides a new platform for researchers to investigate and evaluate real-world mobile HDR imaging techniques.\\n\\n2. Related work\\n\\nHDR Image Datasets. Datasets are the cornerstone of algorithm development and evaluation. Before the era of deep learning, Sen et al. [33] and Tursun et al. [36] provided 8 and 16 scenes of real-world HDR data without ground-truth HDR images, respectively, for qualitative evaluation and comparison of different algorithms. In [10], Kalantari et al. proposed the first paired LDR-HDR dataset, including 74 training and 15 test pairs, making the learning of deep HDR models possible. Prabhakar et al. [28] later built a dataset with 582 LDR-HDR pairs. These two datasets regard the medium-exposed image as the reference frame. In order to explore the cases when other exposures should be used as the reference, Li et al. [17] collected a dataset where different LDR frames can be taken as the reference frame, but it is not publicly available. All the above datasets are collected by DSLR cameras, and they are unsuitable for investigating mobile HDR imaging methods due to the different characteristics of camera sensors and lens, especially for nighttime scenes with strong noise. In order to facilitate the development of mobile HDR imaging techniques, we construct an HDR dataset using mobile phones, covering different scenes and noise levels. HDR Image Reconstruction. In case the LDR frames can be strictly aligned, the HDR image can be easily obtained by fusing them with different weight functions [3, 23]. In practice, however, ghost artifacts can be generated by camera motion and subject moving. A number of methods have been proposed for HDR deghosting in dynamic scenes. Early works can be divided into two classes. The first class aligns LDR frames and fuses them to an HDR image. The global rigid alignment by translation or homography [35, 38] is simple to use but can fail to handle the foreground motion. Bogoni et al. [2] and Kang et al. [11] utilized optical flow to deal with moving objects, which are not robust to occlusion, large motion and saturated areas. Some works [8, 33, 43, 44] perform patch-based registration, which are more robust to motion but suffer from heavy computation. The other class of methods detect inconsistent pixels and discard them after global alignment, such as local entropy [9], color consistency [5, 6, 31], median threshold bitmaps [26], and rank minimization [16, 25]. However, the accurate detection of such pixels is difficult and the rejection strategy may lose much useful information for fusion. Recently, with the availability of paired LDR-HDR image dataset [10], deep learning based HDR reconstruction methods have been developed. Kalantari et al. [10] and Prabhakar et al. [27, 28] employed optical flow [18] or flow net [34] to align input frames and utilized CNN to merge them. Wu et al. [39] used an encoder-decoder network to synthesize HDR image directly from input frames without alignment. Liu et al. and Pu et al. [30] adopted deformable convolution to align features implicitly. Yan et al. [40] designed a spatial attention module to detect unaligned area to suppress the ghosting artifacts. Following this spatial attention module, various CNNs with large receptive fields [40, 41] have been developed. Since transformers could better model long-range dependencies than CNN, Liu et al.\"}"}
{"id": "CVPR-2023-1041", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Averaging Denoising\\nHNoise-free HDR GT: \\nStatic Sets for Generating GT\\n\\\\(L_m\\\\)\\n\\\\(L_u\\\\)\\n\\\\(L_o\\\\)\\nLDR Inputs\\n... ...\\nMerge\\n\\nFigure 1. The synthesis process of LDR-HDR data pairs for dynamic scenes. We first keep the foreground object still to capture three static sets of successive raw images with under-, middle- and over-exposures, and use them to generate three noise-free LDR images by average denoising, which are merged to synthesize the noise-free HDR image as ground-truth (GT). One middle-exposure frame, denoted by \\\\(L_m\\\\), is extracted from the corresponding static set. We then move the foreground object and tripods to capture another two LDR frames, denoted by \\\\(L_u\\\\) and \\\\(L_o\\\\), with under- and over-exposures, respectively. The final LDR inputs are composed of \\\\(L_u\\\\), \\\\(L_m\\\\) and \\\\(L_o\\\\). The LDR frames and synthesized HDR image are visualized through a simple ISP pipeline.\\n\\nal. [21] proposed a transformer network for HDR fusion and achieved state-of-the-art results. Meanwhile, there were some attempts to utilize GAN networks [24] and few-shot learning [29] to hallucinate HDR details or relieve the dependency on abundant training data.\\n\\nMost of the existing methods are developed on images collected by DSLR cameras, which may not fit well to mobile phone images. Recently, Lecouat et al. [14] proposed to perform joint HDR and super-resolution with mobile raw burst images. However, they utilized synthetic raw images for training, resulting in color halos in the saturated area and failing to deal with large motion. In this work, we construct a real-world mobile HDR image dataset and propose a pyramid cross-attention module to achieve alignment against noise, saturated area and large motion.\\n\\n3. The Established Dataset\\n\\nTo facilitate the research on mobile HDR imaging, we establish a paired LDR-HDR image dataset in raw image domain by using mobile phone cameras. Specifically, we utilized four mobile phones equipped with three types of mobile sensors (IMX586, IMX766 and IMX800) to capture images under different exposures and lighting conditions, including indoor, outdoor, daytime and nighttime scenes. The ISO settings in our dataset range from 100 to 6400, covering a variety of noise levels. Our dataset is composed of three subsets: a subset of static scenes with ground-truth (GT) HDR images, a subset of dynamic scenes with GT HDR images, and a subset of dynamic scenes without GT HDR images (used only for visual comparison).\\n\\nFor the subset of static scenes, we capture LDR sequences with three exposures (i.e., under-, middle- and over-exposures) by a mobile phone fixed on tripod with a customized app, which detects and removes the defective pixels in each shot. Under each exposure, we take 120 to 400 successive images to facilitate denoising. Generally, the number of shots increases with the increase of ISO and/or the decrease of exposure. We then average the shots to obtain the noise-free LDR image for each exposure. After the noise-free LDR frames of the three exposures are acquired, we merge them using the weighting function proposed in [3] to generate the high-quality HDR image as GT, denoted by \\\\(H\\\\). Then three LDR frames, denoted by \\\\(L_u\\\\), \\\\(L_m\\\\), \\\\(L_o\\\\), are extracted from the captured successive LDR images with under-, middle- and over-exposures, respectively, as the LDR inputs, building the LDR-HDR data pairs. We check the quality of each sequence and discard the outliers. Finally, 136 static scenes are collected, including 49 daytime and 87 nighttime scenes.\\n\\nFor the subset of dynamic scenes with foreground object motion, we utilize controllable objects to simulate the motion between LDR frames, following the strategy in [10]. The process is illustrated in Fig. 1. We first keep the object still and capture three static sets of images with three exposures, and synthesize the noise-free HDR GT image \\\\(H\\\\) of this scene by the method applied in static scenes. Meanwhile, we extract one middle-exposure LDR frame \\\\(L_m\\\\) from the static set as one of the LDR inputs. Then we move the object and tripod to capture an under-exposure LDR frame \\\\(L_u\\\\) and an over-exposure LDR frame \\\\(L_o\\\\). Finally, \\\\(L_u\\\\), \\\\(L_m\\\\), \\\\(L_o\\\\) and \\\\(H\\\\) are taken as the LDR inputs and HDR GT, respectively. In total, we collected 115 dynamic scenes, including 15 daytime and 100 nighttime ones.\\n\\nThe above subsets of static and dynamic scenes with HDR GT can be used to train HDR reconstruction models and evaluate them quantitatively. In addition, we capture 30 scenes without HDR GT, which contain uncontrolled moving or static objects captured by hand-held mobile phones, for qualitative evaluation of different models.\\n\\nWe compare the statistics of our Mobile-HDR dataset and the Sig17 dataset [10] in Table 1. One can clearly see that our dataset covers more diverse real-world scenarios than the Sig17 dataset. Unlike Sig17, which only contains daytime dynamic scenes, our dataset covers both daytime and nighttime, dynamic and static scenes, representing the diverse lighting conditions and various noise levels in practical scenarios. In addition, the image resolution (4K in general) of our dataset is much higher than that in Sig17 (1500 \u00d7 1000).\"}"}
{"id": "CVPR-2023-1041", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. The statistics comparison between Sig17 dataset [10] and our Mobile-HDR dataset.\\n\\n| Camera          | Sensor Image Resolution | Dynamic Scenes | Static Scenes | Dynamic Scenes | Total |\\n|-----------------|-------------------------|----------------|---------------|----------------|-------|\\n| Sig17 [10]      | Canon EOS-5D Mark II    | 1500 \u00d7 1000    | None          | None           | 89    |\\n| Mobile-HDR (ours)| OPPO A95                | 4000 \u00d7 3000    | 15            | 100            | 281   |\\n|                 | OPPO FindX3 Pro         | 4608 \u00d7 3456    | 49            | 87             |       |\\n|                 | HONOR 70 Pro            | 4096 \u00d7 3072    | 15            | 20             |       |\\n\\nFigure 2. Sample LDR frames in our Mobile-HDR dataset. Various noise levels can be observed in the zoomed-in regions. The images are visualized through a simple ISP pipeline.\\n\\n4. Method\\n\\nOverview. Different from HDR reconstruction on DSLR data, where the normally exposed area of the reference frame can be directly used for HDR recovery, the normally exposed areas in the reference frame of mobile camera data still have strong noise, which needs to be suppressed by fusing with other frames. In addition, the larger overexposed areas in mobile camera data rely on information from underexposed frames to recover detail. Without an effective alignment module, ghosting artifacts will become severe in mobile HDR images. We therefore propose a transformer-based network with a novel pyramid cross-attention alignment module to aggregate and align the correlated features from LDR frames more effectively, achieving denoising and HDR reconstruction jointly.\\n\\nGiven the set of noisy LDR frames of three exposures \\\\{L_1, L_2, L_3\\\\} (sorted by their exposure times), we aim to reconstruct the noise-free HDR frame H. The middle exposure frame L_2 is regarded as the reference frame, so the estimated noise-free HDR b_H should be consistent with L_2 in structure but contain the dynamic range information from all frames. Since LDR images in our dataset are in the RAW format, they have linear response curve with ambient lighting. So we do not need to linearize the LDR images by using the camera response function (CRF) or gamma correction. In order to facilitate the alignment, we map the input LDR images \\\\{L_i\\\\} to the domain of brightness constancy based on the exposure time to get the corresponding set of \\\\{H_i\\\\}:\\n\\n\\\\[ H_i = L_i t_i, \\\\forall i = 1, 2, 3 \\\\]  \\n\\n(1)\\n\\nwhere \\\\( t_i \\\\) denotes the exposure time of the image \\\\( L_i \\\\). Following [40], images \\\\( I_i \\\\) and \\\\( H_i \\\\) are concatenated along the channel dimension to obtain the tensors \\\\( X_i = [L_i, H_i] \\\\), \\\\( i = 1, 2, 3 \\\\) as the network input. The network outputs the estimated noise-free HDR image \\\\( b_H \\\\).\\n\\nFig. 3(a) illustrates the overall architecture of our proposed model. It mainly consists of three components, i.e., the pyramid cross-attention alignment module for aligning neighborhood frames to the reference frame, the attention fusion module for fusing aligned features and the merging subnet for final HDR reconstruction. For each input tensor \\\\( X_i \\\\), \\\\( i = 1, 2, 3 \\\\), we first extract the shallow features \\\\( F_i \\\\) by convolution layers. Then, a pyramid cross-attention alignment module is used to align non-reference features \\\\( F_i, i = 1, 3 \\\\) to reference features \\\\( F_2 \\\\). A local skip connection is used for better training. The aligned features \\\\( f_{F_1}, f_{F_3} \\\\) and \\\\( F_2 \\\\) are fed into the attention fusion module to get the fused features. Finally, a merging subnet, which consists of context-aware transformer blocks, takes the fused features as input to generate the HDR image. A global residual connection is used to accelerate the training process.\\n\\nPyramid Cross Attention Alignment Module. We propose a pyramid cross attention alignment module to align features from neighborhood frames to the reference frame. Since the computation of cross attention will also aggregate correlated features, the cross attention facilitates the alignment and denoising at the same time.\\n\\nGiven the neighbor-frame features \\\\( F_i, i = 1, 3 \\\\) and the reference-frame features \\\\( F_2 \\\\) of the same size \\\\( H \\\\times W \\\\times C \\\\), we first partition them into non-overlapping \\\\( M \\\\times M \\\\) local windows to get two reshaped inputs of size \\\\( HW M^2 \\\\times M^2 \\\\times C \\\\), where \\\\( HW M^2 \\\\) is the total number of windows. Then we compute the cross-attention separately for each window. For the local window feature from neighbor-frame \\\\( F_i \\\\in \\\\mathbb{R}^{M^2 \\\\times C}, i = 1, 3 \\\\) and the ones from reference-frame \\\\( F_2 \\\\in \\\\mathbb{R}^{M^2 \\\\times C} \\\\), the query, key, and value matrices \\\\( Q, K \\\\) and \\\\( V \\\\) are computed.\"}"}
{"id": "CVPR-2023-1041", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3. (a) The overall architecture of the proposed joint HDR denoising and fusion model. (b) The structure of attention fusion module. (c) The structure of Context-Aware Vision Transformer (CA-ViT) module used in Context-Aware Transformer Block.\\n\\nAttention Calculation:\\n\\n\\\\[\\nQ = F_2 P_Q, \\\\quad K = F_i P_K, \\\\quad V = F_i P_V, \\\\quad \\\\forall i = 1, 3\\n\\\\]\\n\\nwhere \\\\(P_Q, P_K, P_V\\\\) are projection matrices shared across different windows. The attention matrix is thus computed in a local window as follows:\\n\\n\\\\[\\n\\\\text{Attention}(Q, K, V) = \\\\text{SoftMax} \\\\left( \\\\frac{QK^T}{\\\\sqrt{d}} + B \\\\right) V, \\\\quad (3)\\n\\\\]\\n\\nwhere \\\\(B\\\\) is the learnable relative positional encoding. Following [20], the attention function are performed for \\\\(h\\\\) times in parallel and the result are concatenated for multi-head cross attention.\\n\\nIn order to handle complex motion, we adopt the pyramid processing and cascading operation like the PCD alignment module [37] used in video super-resolution. Considering that the saturated regions or severely noisy regions in the reference image are generally difficult to perform reliable feature matching, we propose an attention transfer mechanism. Intuitively, if the size of query patch is enlarged, the patch may include some region with details, which will enable more reliable matching. Since features at coarse scales are extracted from larger receptive fields, we perform feature matching in a coarse scale and transfer the attention coefficients to finer scales.\\n\\nThe proposed pyramid cross-attention module is illustrated in Fig. 4(a). We generate an \\\\(L\\\\)-level pyramid of feature representation for each LDR frame. Given the features \\\\(F_l\\\\) at \\\\(l\\\\)-level, we use strided convolution filters to get the downsampled features with a factor of 2 at the \\\\((l+1)\\\\)-th pyramid level. At the \\\\(l\\\\)-th level, cross attention is performed on reference feature \\\\(F_l^2\\\\) and neighborhood features \\\\(F_{l^-1}\\\\), \\\\(i = 1, 3\\\\) to get features \\\\(F_{l^-1}^c\\\\). The attention coefficient \\\\(A_{l+1}^i\\\\) computed by \\\\(F_{l+1}^2\\\\) and \\\\(F_{l+1}^i\\\\) from the upper \\\\((l+1)\\\\)-th level is multiplied with neighborhood feature \\\\(F_{l^-1}^i\\\\) to get \\\\(F_{l^-1}^t\\\\). The specific computation process is illustrated in Fig. 4(b).\\n\\nFinally, the aligned features at the \\\\(l\\\\)-th level \\\\(e\\\\) \\\\(F_l^i\\\\) are predicted by using \\\\(F_{l^-1}^c\\\\), \\\\(F_{l^-1}^t\\\\), and \\\\(\\\\times 2\\\\) upsampled aligned features from the the upper \\\\((l+1)\\\\)-th level as follows:\\n\\n\\\\[\\ne F_l^i = \\\\text{Conv}(F_{l^-1}^c, F_{l^-1}^t, (e F_{l+1}^i) \\\\uparrow 2).\\n\\\\]\\n\\nAttention Fusion Module.\\n\\nAfter obtaining the aligned features, we adopt the attention module proposed in [40] to suppress harmful features from misaligned, over-exposed and under-exposed areas, as illustrated in Fig. 3(b). For each aligned feature from non-reference LDR image (i.e., \\\\(f\\\\) \\\\(F_1\\\\) and \\\\(f\\\\) \\\\(F_3\\\\)), we concatenate it with the reference feature \\\\(F_2\\\\) as the input of two convolutional layers, generating a spatial attention map \\\\(m_i\\\\), \\\\(i = 1, 3\\\\) ranging between 0 and 1. We then perform the element-wise multiplication of \\\\(m_i\\\\) and \\\\(e F_i\\\\) to get the attentioned features \\\\(F_i'\\\\):\\n\\n\\\\[\\nF_i' = m_i \\\\odot e F_i, \\\\quad i = 1, 3.\\n\\\\]\"}"}
{"id": "CVPR-2023-1041", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The features $F'_{1}, F_{2}, F'_{3}$ are concatenated and passed through a convolution layer to obtain the fused features.\\n\\nThe merging network is composed of several context-aware Transformer blocks (CTB) [21], which consists of a dual-branch context aware vision Transformer (CA-ViT) and a dilated convolution layer, as illustrated in Fig. 3(a). The structure of CA-ViT is shown in Fig. 3(c), which employs a window-based multi-head Transformer encoder [20] to extract globally long-range features, and a convolution block with channel attention as another parallel branch to capture the local information.\\n\\nThe proposed model outputs the estimated HDR image $b_{H}$ within range $[0, 1]$. If the loss is applied to $b_{H}$ and ground truth $H$ directly, the training will be dominated by the brighter areas, hindering the restoration of dark areas. So we apply the $\\\\mu$-law tone-mapping function to the HDR image in HDR domain:\\n\\n$$T(H) = \\\\log(1 + \\\\mu H)$$\\n\\nwhere $T(H)$ is the tone-mapped HDR image and $\\\\mu$ is set to 5000 in our work. Given the estimated HDR image $b_{H}$ and the ground truth HDR image $H$, we utilize the $\\\\ell_1$ loss in tone-mapped domain to optimize the network:\\n\\n$$L = \\\\|T(b_{H}) - T(H)\\\\|_1.$$ \\n\\n5. Experiments\\n\\nTraining Data Preparation. Since there is a lack of paired HDR datasets captured by mobile phones, we constructed such a paired dataset, i.e., Mobile-HDR, in this work for HDR model training and evaluation. We apply black level correction and range normalization to the raw data to obtain each $L_{i}$, and conduct joint HDR denoising and fusion in a raw-in-raw-out manner. For static scenes, we add random global motions to the non-reference frames, i.e., random translation in the range of $[0, 20]$ pixels. We divide our Mobile-HDR dataset with ground-truth into 223 training samples and 28 test samples. For the training samples, 102 samples are taken from dynamic scenes and 121 from static scenes. While for the test samples, 13 samples are taken from dynamic scenes and 15 samples are taken from static scenes. Meanwhile, there are 30 test samples without ground-truth for visual comparison. Each sample is composed of three LDR frames with exposure values $\\\\{-2, 0, 2\\\\}$ or $\\\\{-3, 0, 3\\\\}$ and the corresponding HDR frame. Before training, we crop the images into $512 \\\\times 512$ patches with stride 200. During training, we randomly crop $128 \\\\times 128$ regions from the $512 \\\\times 512$ patch as training samples.\\n\\nImplement Details. We call our method as Joint-HDRDN for that it performs HDR denoising and fusion jointly. The overall model is optimized by the Adam optimizer [13] with default parameters. The batch size is set as 16, and the initial learning rate is $2e^{-4}$ and halved after 500 epochs. Our pyramid cross attention alignment module adopts 3-layer pyramid with partition window size $M$ of 8. The number of channels is set as 60, and there are 3 context-aware transformer blocks in our merging subnet. Different from the HDR-Tranformer that employs 6 CA-ViT in each context-aware transformer block, our Joint-HDRDN has only 4 CA-ViT in each block. Benefiting from our proposed pyramid cross-attention alignment module, our merging network does not need to stack many transformer blocks to enlarge the receptive field. The whole training is conducted on four NVIDIA V100 GPUs and costs about three days to converge.\\n\\nEvaluation Metrics. We use PSNR and SSIM in both raw domain and sRGB domain, as well as HDR-VDP-2 in sRGB domain [22], as evaluation metrics. The HDR results in sRGB domain is obtained by passing the HDR results in raw domain through a simple ISP pipeline as in SIDD [1], which involves white balance, demosaicking, color correction and sRGB space transfer. The parameters are from the metadata of the reference frame. For both raw domain and sRGB domain, PSNR and SSIM are evaluated in both linear domain (i.e., PSNR-$\\\\mu$ and SSIM-$\\\\mu$) and the tone-mapped domain with $\\\\mu$-law (i.e., SSIM-$\\\\mu$ and SSIM-$\\\\mu$). Moreover, since HDR-VDP-2 is developed specifically for qualitative evaluation of HDR images, we compute it in sRGB domain.\\n\\nComparison with State-of-the-Arts. We compare our proposed Joint-HDRDN method with state-of-the-art HDR reconstruction methods, including DeepHDR [39], AHDR-Net [40], NHDRRNet [41] and HDR-Transformer [21]. For fair comparison, we retrain these deep HDR models on our training dataset and then evaluate them on our test dataset. Table 2 compares the quantitative results of competing methods. It can be observed that transformer based algorithms outperform CNN based methods, while our proposed Joint-HDRDN surpasses HDR-Transformer, which is the previous state-of-the-art, by up to 0.38dB and 0.95dB in terms of PSNR-$\\\\mu$ and PSNR-$\\\\mu$ in raw domain. Furthermore, after rendering images from raw domain to sRGB domain, our model still outperforms other competitors by a large margin, which demonstrates the effectiveness of our strategy of performing HDR fusion and denoising jointly by using pyramid cross-attention.\\n\\nFig. 5 compares the visual results of our method and its competitors on some challenging scenarios in our dataset. All HDR results are first passed through a simple ISP and then tone-mapped by the Reinhard [32] operator. It can be seen that our method achieves significantly better visual results. On the reference frames which have high-exposure or severe noise, our method can recover fine details without introducing much artifacts. In comparison, the other methods...\"}"}
{"id": "CVPR-2023-1041", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pyramids reference feature neighbor feature cross attention attention transfer aligned feature conv\\n\\nFigure 4. (a) The structure of pyramid cross-attention alignment module. (b) The attention transfer mechanism between two scales.\\n\\nDeepHDR NHDRRNet A-HDRNet HDRTransformer Ours GT\\nLDRs Our tonemapped HDR images LDR patches\\n\\n(a) Scene 1 from our test dataset.\\n\\nDeepHDR NHDRRNet A-HDRNet HDRTransformer Ours GT\\nLDRs Our tonemapped HDR images LDR patches\\n\\n(b) Scene 2 from our test dataset.\\n\\nFigure 5. Visual comparison between our Joint-HDRDN and other state-of-the-arts HDR reconstruction methods on two scenes from our test dataset. All images go through a simple ISP pipeline for visualization. The HDR results are tone-mapped for better comparison.\\n\\nsuffer from the ghosting artifacts or residual noise. Previous methods don't consider the impact of noise on the final HDR image quality. Meanwhile, they usually just resort to modeling long-range dependency to hallucinate reasonable content for over-exposed areas and attention module to suppress unaligned areas to alleviate ghost artifacts. So they do not make efficient use of other frames to recover details. Additionally, they will produce ghosting artifacts inevitably due to the lack of specific alignment design, especially for the large over-exposed areas. In contrast, our proposed pyramid cross-attention alignment module searches and aggregates beneficial features from other frames more effectively, which can better reproduce the details and alleviate the artifacts. More visual comparison examples can be found in the supplementary file.\\n\\nAblation Study on Training Dataset.\\n\\nTo further demonstrate the necessity of constructing our Mobile-HDR dataset to develop mobile HDR techniques, we train the HDR-Transformer [21] and our Joint-HDRDN models on the DSLR camera captured Sig17 dataset [10] and our Mobile-HDR dataset, respectively, and evaluate them on our test dataset, which consists of data captured by mobile phone cameras. Since Sig17 provides the demosaicked data, we compare the results in the sRGB domain.\\n\\nThe quantitative results are shown in Table 3, which verifies that the DSLR dataset cannot well support the research.\"}"}
{"id": "CVPR-2023-1041", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2. Quantitative comparison of proposed Joint-HDRDN method with state-of-the-art HDR reconstruction methods.\\n\\n| Methods            | Raw PSNR-\u00b5 | Raw PSNR-\u03bb | Raw SSIM-\u00b5 | Raw SSIM-\u03bb |\\n|--------------------|------------|------------|------------|------------|\\n| HDR-VDP-2          | 38.54      | 35.42      | 0.9599     | 0.9873     |\\n| DeepHDR [39]       | 37.94      | 35.29      | 0.9559     | 0.9869     |\\n| AHDRNet [40]       | 38.16      | 35.49      | 0.9571     | 0.9872     |\\n| NHDRRNet [41]      | 37.57      | 34.54      | 0.9517     | 0.9850     |\\n| HDR-Transformer    | 38.54      | 35.42      | 0.9599     | 0.9873     |\\n| Joint-HDRDN (ours) | 38.92      | 36.37      | 0.9616     | 0.9888     |\\n\\n### Table 3. Quantitative comparison between models trained on different training sets.\\n\\n| Training Set       | HDR-Transformer | Ours |\\n|--------------------|-----------------|------|\\n| DSLR training set  | PSNR-\u03bb (sRGB)   | 23.59 | 23.45 |\\n|                    | SSIM-\u03bb (sRGB)   | 0.4276 | 0.4134 |\\n| Our training set   | HDR-Transformer | Ours |\\n|                    | PSNR-\u03bb (sRGB)   | 36.67 | 37.46 |\\n|                    | SSIM-\u03bb (sRGB)   | 0.9809 | 0.9825 |\\n\\nFigure 6. Visual comparison between models trained on DSLR dataset [10] and our Mobile-HDR dataset.\\n\\nThis is mainly because the mobile phone images have stronger noise and larger overexposed areas caused by the smaller aperture and sensor size. As shown in Fig. 6, models trained on the DSLR dataset are hard to remove the heavy noise in mobile phone images, and the over-exposed areas have obvious ghost artifacts. In addition, existing methods such as HDR-Transformer are not designed for mobile HDR imaging. Even retrained on our Mobile-HDR dataset, they still show many ghosting artifacts in over-exposed areas and blurry details in noisy regions. Therefore, it is necessary to construct a mobile HDR image dataset to facilitate the research on mobile HDR imaging, such as more effective denoising, alignment, fusion and the joint tasks of them.\\n\\n### Ablation Study on Network.\\n\\nIn order to validate the effectiveness of different components in our Joint-HDRDN network, we evaluate the following variants of our model:\\n\\n- **Baseline.** We replace our pyramid cross-attention alignment module with the attention feature extractor adopted by HDR-Transformer and AHDRNet, while keeping the merging network unchanged. That is, the baseline shares the same components as HDR-Transformer but has fewer CA-ViT blocks (12 vs. 18).\\n\\n- **w/o Attention Transfer.** This variant removes the attention transfer mechanism adopted in our pyramid cross-attention alignment module.\\n\\n- **w/o Attention Fusion.** This variant removes the attention fusion module and directly stacks the aligned features as the fused features.\\n\\nTable 4 lists the quantitative results of our ablation study. Compared with the baseline, which shares the same merging network as our full model but removes the alignment module, the full model achieves 0.51dB and 1.11dB advantages in PSNR-\u00b5 and PSNR-\u03bb, respectively, demonstrating the effectiveness of our proposed pyramid cross-attention alignment module. The attention transfer mechanism is proposed to alleviate the difficulties in aligning with overexposed reference regions. As shown in the table, if we remove the attention transfer from our alignment module, the performance will drop by 0.37dB in PSNR-\u03bb, validating the roles of attention transfer mechanism. The attention fusion module is also useful since the PSNR-\u00b5 will drop by 0.21dB if we remove it.\\n\\n### 6. Conclusion\\n\\nWe established, for the first time to our best knowledge, a real-world mobile HDR image dataset, namely Mobile-HDR, to facilitate researches on mobile HDR imaging. Different from the existing HDR image datasets, which were mostly collected in daytime with DSLR cameras, our dataset was collected by mobile phone cameras under different lighting conditions and scenes, which contained stronger noises and larger over-exposed areas. We consequently developed a new HDR image reconstruction network, namely Joint-HDRDN, which employed a novel pyramid cross-attention alignment module to perform HDR fusion and denoising jointly. Extensive experiments validated the effectiveness of our proposed dataset and model.\"}"}
{"id": "CVPR-2023-1041", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S Brown. A high-quality denoising dataset for smartphone cameras. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1692\u20131700, 2018.\\n\\n[2] Luca Bogoni. Extending dynamic range of monochrome and color images through fusion. In Proceedings 15th International Conference on Pattern Recognition. ICPR-2000, volume 3, pages 7\u201312. IEEE, 2000.\\n\\n[3] Paul E Debevec and Jitendra Malik. Recovering high dynamic range radiance maps from photographs. In ACM SIGGRAPH 2008 classes, pages 1\u201310. 2008.\\n\\n[4] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In European conference on computer vision, pages 184\u2013199. Springer, 2014.\\n\\n[5] Orazio Gallo, Natasha Gelfandz, Wei-Chao Chen, Marius Tico, and Kari Pulli. Artifact-free high dynamic range imaging. In 2009 IEEE International conference on computational photography (ICCP), pages 1\u20137. IEEE, 2009.\\n\\n[6] Thorsten Grosch et al. Fast and robust high dynamic range image generation with camera and object movement. Vision, Modeling and Visualization, RWTH Aachen, pages 277\u2013284, 2006.\\n\\n[7] Samuel W Hasinoff, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan T Barron, Florian Kainz, Jiawen Chen, and Marc Levoy. Burst photography for high dynamic range and low-light imaging on mobile cameras. ACM Transactions on Graphics (ToG), 35(6):1\u201312, 2016.\\n\\n[8] Jun Hu, Orazio Gallo, Kari Pulli, and Xiaobai Sun.Hdr deghosting: How to deal with saturation? In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1163\u20131170, 2013.\\n\\n[9] Katrien Jacobs, Celine Loscos, and Greg Ward. Automatic high-dynamic range image generation for dynamic scenes. IEEE Computer Graphics and Applications, 28(2):84\u201393, 2008.\\n\\n[10] Nima Khademi Kalantari, Ravi Ramamoorthi, et al. Deep high dynamic range imaging of dynamic scenes. ACM Trans. Graph., 36(4):144\u20131, 2017.\\n\\n[11] Sing Bing Kang, Matthew Uyttendaele, Simon Winder, and Richard Szeliski. High dynamic range video. ACM Transactions on Graphics (TOG), 22(3):319\u2013325, 2003.\\n\\n[12] Erum Arif Khan, Ahmet Oguz Akyuz, and Erik Reinhard. Ghost removal in high dynamic range images. In 2006 International Conference on Image Processing, pages 2005\u20132008. IEEE, 2006.\\n\\n[13] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[14] Bruno Lecouat, Thomas Eboli, Jean Ponce, and Julien Mairal. High dynamic range and super-resolution from raw image bursts. arXiv preprint arXiv:2207.14671, 2022.\\n\\n[15] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436\u2013444, 2015.\\n\\n[16] Chul Lee, Yuelong Li, and Vishal Monga. Ghost-free high dynamic range imaging via rank minimization. IEEE signal processing letters, 21(9):1045\u20131049, 2014.\\n\\n[17] Wei Li, Shuai Xiao, Tianhong Dai, Shanxin Yuan, Tao Wang, Cheng Li, and Fenglong Song. Sj-hd\u00b2r: Selective joint high dynamic range and denoising imaging for dynamic scenes. arXiv preprint arXiv:2206.09611, 2022.\\n\\n[18] Ce Liu et al. Beyond pixels: exploring new representations and applications for motion analysis. PhD thesis, Massachusetts Institute of Technology, 2009.\\n\\n[19] Zhen Liu, Wenjie Lin, Xinpeng Li, Qing Rao, Ting Jiang, Mingyan Han, Haoqiang Fan, Jian Sun, and Shuaicheng Liu. Adnet: Attention-guided deformable convolutional network for high dynamic range imaging. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 463\u2013470, 2021.\\n\\n[20] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021.\\n\\n[21] Zhen Liu, Yinglong Wang, Bing Zeng, and Shuaicheng Liu. Ghost-free high dynamic range imaging with context-aware transformer. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XIX, pages 344\u2013360. Springer, 2022.\\n\\n[22] Rafa\u0142 Mantiuk, Kil Joong Kim, Allan G Rempel, and Wolfgang Heidrich. Hdr-vdp-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions. ACM Transactions on graphics (TOG), 30(4):1\u201314, 2011.\\n\\n[23] Tom Mertens, Jan Kautz, and Frank Van Reeth. Exposure fusion: A simple and practical alternative to high dynamic range photography. In Computer graphics forum, volume 28, pages 161\u2013171. Wiley Online Library, 2009.\\n\\n[24] Yuzhen Niu, Jianbin Wu, Wenxi Liu, Wenzhong Guo, and Rynson WH Lau.Hdr-gan: Hdr image reconstruction from multi-exposed ldr images with large motions. IEEE Transactions on Image Processing, 30:3885\u20133896, 2021.\\n\\n[25] Tae-Hyun Oh, Joon-Young Lee, Yu-Wing Tai, and In So Kweon. Robust high dynamic range imaging by rank minimization. IEEE transactions on pattern analysis and machine intelligence, 37(6):1219\u20131232, 2014.\\n\\n[26] Fabrizio Pece and Jan Kautz. Bitmap movement detection: Hdr for dynamic scenes. In 2010 Conference on Visual Media Production, pages 1\u20138. IEEE, 2010.\\n\\n[27] K Ram Prabhakar, Susmit Agrawal, Durgesh Kumar Singh, Balraj Ashwath, and R Venkatesh Babu. Towards practical and efficient high-resolution hdr deghosting with cnn. In European Conference on Computer Vision, pages 497\u2013513. Springer, 2020.\\n\\n[28] K Ram Prabhakar, Rajat Arora, Adhitya Swaminathan, Kunal Pratap Singh, and R Venkatesh Babu. A fast, scalable, and reliable deghosting method for extreme exposure fusion. In 2019 IEEE International Conference on Computational Photography (ICCP), pages 1\u20138. IEEE, 2019.\\n\\n[29] K Ram Prabhakar, Gowtham Senthil, Susmit Agrawal, R Venkatesh Babu, and Rama Krishna Sai S Gorthi. Labeled...\"}"}
{"id": "CVPR-2023-1041", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"from unlabeled: Exploiting unlabeled data for few-shot deep hdr deghosting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4875\u20134885, 2021.\\n\\n[30] Zhiyuan Pu, Peiyao Guo, M Salman Asif, and Zhan Ma. Robust high dynamic range (hdr) imaging with complex motion and parallax. In Proceedings of the Asian Conference on Computer Vision, 2020.\\n\\n[31] Shanmuganathan Raman and Subhasis Chaudhuri. Reconstruction of high contrast images for dynamic scenes. The Visual Computer, 27(12):1099\u20131114, 2011.\\n\\n[32] Erik Reinhard, Michael Stark, Peter Shirley, and James Ferwerda. Photographic tone reproduction for digital images. In Proceedings of the 29th annual conference on Computer graphics and interactive techniques, pages 267\u2013276, 2002.\\n\\n[33] Pradeep Sen, Nima Khademi Kalantari, Maziar Yaesoubi, Soheil Darabi, Dan B Goldman, and Eli Shechtman. Robust patch-based hdr reconstruction of dynamic scenes. ACM Trans. Graph., 31(6):203\u20131, 2012.\\n\\n[34] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8934\u20138943, 2018.\\n\\n[35] Anna Tomaszewska and Radoslaw Mantiuk. Image registration for multi-exposure high dynamic range image acquisition. 2007.\\n\\n[36] Okan Tarhan Tursun, Ahmet O\u02d8guz Aky\u00a8uz, Aykut Erdem, and Erkut Erdem. An objective deghosting quality metric for hdr images. In Computer Graphics Forum, volume 35, pages 139\u2013152. Wiley Online Library, 2016.\\n\\n[37] Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy. Edvr: Video restoration with enhanced deformable convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 0\u20130, 2019.\\n\\n[38] Greg Ward. Fast, robust image registration for compositing high dynamic range photographs from hand-held exposures. Journal of graphics tools, 8(2):17\u201330, 2003.\\n\\n[39] Shangzhe Wu, Jiarui Xu, Yu-Wing Tai, and Chi-Keung Tang. Deep high dynamic range imaging with large foreground motions. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.\\n\\n[40] Qingsen Yan, Dong Gong, Qinfeng Shi, Anton van den Hengel, Chunhua Shen, Ian Reid, and Yanning Zhang. Attention-guided network for ghost-free high dynamic range imaging. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1751\u20131760, 2019.\\n\\n[41] Qingsen Yan, Lei Zhang, Yu Liu, Yu Zhu, Jinqiu Sun, Qinfeng Shi, and Yanning Zhang. Deep hdr imaging via a non-local network. IEEE Transactions on Image Processing, 29:4308\u20134322, 2020.\\n\\n[42] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122, 2015.\\n\\n[43] Jinghong Zheng and Zhengguo Li. Superpixel based patch match for differently exposed images with moving objects and camera movements. In 2015 IEEE International Conference on Image Processing (ICIP), pages 4516\u20134520. IEEE, 2015.\\n\\n[44] Jinghong Zheng, Zhengguo Li, Zijian Zhu, Shiqian Wu, and Susanto Rahardja. Hybrid patching for a sequence of differently exposed images with moving objects. IEEE Transactions on Image Processing, 22(12):5190\u20135201, 2013.\\n\\n[45] Henning Zimmer, Andr \u00b4es Bruhn, and Joachim Weickert. Freehand hdr imaging of moving scenes with simultaneous resolution enhancement. In Computer Graphics Forum, volume 30, pages 405\u2013414. Wiley Online Library, 2011.\"}"}
