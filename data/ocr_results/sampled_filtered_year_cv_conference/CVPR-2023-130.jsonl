{"id": "CVPR-2023-130", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"sample a half of image-text samples and a half of video-text samples for each batch. Then, we pass the images to the self-attention block directly. For video-text pairs, the input is first fed into the Temporal Token Rolling Layer and then each self-attention block. In contrast to previous work [63], we use a shared pretext head for both image and video. The weighted loss for co-training over both image and videos samples is computed as:\\n\\n$$L_{ct} = \\\\sum w_{image} L(y_{image}) + \\\\sum w_{video} L(y_{video})$$\\n\\nwhere $w_{image}$ means the weight for the image and $w_{video}$ for the video. We also analyze the other variations of co-training strategies in the supplementary material and show the superiority of our balanced sampling co-training.\\n\\n| Model | Embed Dim | #Heads | #Params | Throughput |\\n|-------|-----------|--------|---------|------------|\\n| All-in-one-Ti | 192 | 3 | 1.2M | 745 |\\n| All-in-one-S | 384 | 6 | 3.3M | 285 |\\n| All-in-one-B | 768 | 12 | 11.0M | 89 |\\n\\nTable 1. Variants of our All-in-one architecture. Embed Dim is short for Embedding Dimension. The throughput is measured for videos at a resolution of 224\u00d7224. We use All-in-one-B as default without specific explanation.\\n\\nFigure 6. The parameters & performance over eleven downstream datasets with the varying of model size. The origin point represent All-in-one-Ti.\\n\\n3.5. Setup\\n\\nTo explore model scalability, we use large-scale Webvid2.5M [3], HowTo100M [39] and YT-Temporal 180M [61] for Pre-training. For Image and Video Co-training, we adopt additional image dataset CC3M [42]. We evaluate All-in-one on four popular downstream video-language tasks: text-to-video retrieval, video question answering, multiple-choice, video captioning across 9 different datasets. We also transfer our model to video action recognition. We also provide extensive ablation studies to analyze the key factors that contribute to All-in-one\u2019s success, with insights and qualitative results.\\n\\n3.5.1 Model Variants.\\n\\nWhen considering the generality of All-in-one, we consider using three configurations based on ViT [8] and DeiT [45], as summarized in Tab. 1. To simplify, we use the brief notation to indicate the model size: for instance, All-in-one-B/16 means the \u201cBase\u201d variant with 16\u00d716 input patch size. We varying the model size from tiny to base. Following ViLT [21], we use the bert-base-uncased tokenizer [7] to tokenize text inputs. For input video, we random sample 3 frames and resize each frame to 224\u00d7224.\\n\\n3.5.2 Pre-training & Fine-tuning.\\n\\nConsidering YT-Temporal 180M [61] partially overlaps with HowTo100M [39], we pre-train on WebVid2.5M + Howto100M by default. If the model is trained with additional YT-Temporal 180M, we named it as All-in-one*. For All-in-one+, we pre-train on WebVid2.5M, HowTo100M, and CC3M as default. When we train All-in-one+ on more datasets, we also list the datasets for reference. We refer readers to supplementary for more pre-training details.\\n\\n3.6. Downstream Tasks Settings\\n\\nAll-in-one is evaluated on five video-language tasks: text-to-video retrieval, video question answering, multiple-choice, captioning, and action recognition across 10 datasets. We also provide extensive ablation studies to analyze the key factors that contribute to All-in-one\u2019s success, with insights and qualitative results. We refer readers to supplementary material for the datasets and evaluation setting for downstream tasks.\\n\\n4. Main Results\\n\\nWe extensively evaluate the capabilities of All-in-one on a wide range of downstream tasks as a pretrained foundation model. We mainly consider core tasks of two categories that examine (1) video-text understanding capabilities, (2) video-text alignment, and (3) video captioning and action recognition capabilities. Fig. 6 summarizes the performance on key benchmarks of All-in-one compared to other foundation models. In addition, we also transfer our model to more downstream image-text tasks (refer to supplementary for more details).\\n\\n4.1. Multimodal Understanding Tasks\\n\\n4.1.1 Video-question Answering.\\n\\nIn this experiment, we compare three variations of our All-in-one to state-of-the-art methods from the literature. For multiple-choice VQA, we evaluate our All-in-one on two sub splits of TGIF-QA and report the result in Tab. 2. We find All-in-one especially good at this type of VQA. With only 1 frame input, our All-in-one-B outperforms previous...\"}"}
{"id": "CVPR-2023-130", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1. Comparison with state-of-the-art methods on VQA.\\n\\n| Method               | Frames | Accuracy |\\n|----------------------|--------|----------|\\n| Heterogeneous [9]    | 35     | 73.9     |\\n| ClipBERT [26]        | 35     | 82.9     |\\n| VIOLET [10]          | 16     | 87.1     |\\n| GIT [48]             | 6      | 43.2     |\\n| FrozenBiLM [57]      | 10     | 47.0     |\\n| All-in-one - Ti      | 3      | 39.5     |\\n| All-in-one - B       | 3      | 42.9     |\\n| All-in-one - B+      | 3      | 44.6     |\\n| All-in-one - B *     | 3      | 46.8     |\\n\\nTable 1 shows the performance of different methods on the VQA task. The All-in-one methods achieve the best accuracy across all datasets.\\n\\n### Table 2. Comparison with state-of-the-art methods on VQA.\\n\\nThe columns with gray color are open-ended VQA and the others are multiple-choice VQA.\\n\\n| Method               | Frames | Accuracy |\\n|----------------------|--------|----------|\\n| QueST [17]           | 10     | 36.1     |\\n| HCRN [25]            | 16     | 36.1     |\\n| SSML [2]             | 16     | 35.1     |\\n| CoMVT [41]           | 30     | 42.6     |\\n| Just-Ask \u2020 [56]      | 32     | 46.3     |\\n| All-in-one - S       | 3      | 41.7     |\\n| All-in-one - B       | 3      | 46.5     |\\n| All-in-one - B+      | 3      | 48.2     |\\n| All-in-one - B *     | 3      | 48.3     |\\n\\n### Table 3. Comparison with state-of-the-art methods on multiple-choice task.\\n\\nTable 3 shows the performance of different methods on the multiple-choice task.\\n\\n| Method               | Frames | MSRVTT LSMDC |\\n|----------------------|--------|--------------|\\n| JSFusion [59]        | 40     | 83.4 73.5    |\\n| ActBERT [64]         | 32     | 85.7 -      |\\n| ClipBERT [26]        | 8      | 88.2 -      |\\n| MERLOT [61]          | 8      | - 81.7      |\\n| VIOLET [10]          | 16     | - 82.9      |\\n| All-in-one - B       | 3      | 91.4 83.1    |\\n| All-in-one - B+      | 3      | 91.9 (3.8\u2191) 83.9 (1.0\u2191) |\\n| All-in-one - B+ (zero-shot) | 3 | 82.2 58.1 |\\n\\n### Table 4. The multiple-choice result on first-view ego-4d.\\n\\nTable 4 shows the performance of different methods on the first-view ego-4d task.\\n\\n| Method               | Parameters | #Frames | Zero-shot Fine-tune |\\n|----------------------|------------|--------|---------------------|\\n| Frozen [3]           | 232M       | 8      | 32.47 60.32         |\\n| V ATT \u2020 [1]          | 264M       | 3      | 27.34 59.44         |\\n| All-in-one - B       | 110M       | 3      | 36.52 65.89         |\\n\\n\u2020 means our implementation.\\n\\n### 4.1.2 Multiple-choice.\\nTab. 3 shows that All-in-one improves the ClipBERT model by 3.2% on accuracy, on MSRVTT multiple-choice test task. We also report the zero-shot results for comparison and find that zero-shot accuracy already close to JSFusion [59] in MSRVTT multiple-choice with only three frames as input.\"}"}
{"id": "CVPR-2023-130", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Comparison with state-of-the-art methods on text-to-video retrieval.\\n\\nWe gray out dual-stream networks that only do retrieval tasks. Notice that OA-Trans [46] uses additional offline object features. Apart with static cues only, but our All-in-one still outperforms Frozen [3] clearly with half the parameters and less frames. With same pretrain data, our method also outperforms VATT [1] clearly.\\n\\nFigure 7. Cloze evaluation: Given a video and masked text pair, the model fills masked words and displays their corresponding high-attention patch. These samples are randomly selected from the Webvid [3] dataset\u2019s validation set.\\n\\n4.2. Video-text Alignment Task\\n\\n4.2.1 Text-to-video Retrieval. In this experiment, we fine-tune All-in-one on MSRVTT, ActivityNet Caption, and DiDeMo datasets. Tab. 5 summarizes results on text-to-video retrieval. In Tab. 5(a), All-in-one achieves significant performance gain over existing methods on MSRVTT retrieval in both 9K and 7K training settings. Compare with these related works, we only use one Cross-modality Encoder and the parameter is half of the Frozen [3]. All-in-one even leads to 2.1% relative improvement on R@1 when compare with OA-Trans [46], which use additional offline object feature and only focus on retrieval. When adopt to LSMDC and DiDeMo dataset, our method also show competitive performance.\\n\\n4.3. Video Captioning and Action Recognition\\n\\n4.3.1 Video Captioning. Follow SwinBERT [32], we add a light Language Modeling Head [7] on top of All-in-one. For fair comparison, we compare with related pre-training works on TVC and YouCook2 datasets in Tab. 7. We observe All-in-one outperforms ActBert in terms of CIDEr metric by a large margin on YouCook2. All-in-one even leads to better result on TVC which use additional text script as input. These results showcase the generative capability of All-in-one as an\"}"}
{"id": "CVPR-2023-130", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3.2 Action Recognition via Linear Probe.\\nTo evaluate the transfer ability of our model on the single-modality task. We transfer the learned representation to downstream linear probe results on K400, UCF101 and HMDB51 datasets. Specifically, we frozen the overall unified model and only learn linear layers based on the cls token of the last layer. By pre-training model on these two datasets, we compare the base model with Time Average and the previous pre-training method Frozen.\\n\\nThe linear probe results are given in Tab. 6. We observe the number of frames has a large impact on this task. When adopting the same 8 frames, our All-in-one-B clearly outperforms Frozen especially on the large-scale K400 dataset. We also outperform MIL-NCE clearly on UCF101 and HMDB51 datasets. Interestingly, we find the model have more stable results on this temporal-related task if we train two pre-text heads independently for image-text and video-text inputs. But there have no large difference for both shared or not shared for other tasks.\\n\\n5. Visualization\\nTo better understand the pre-trained All-in-one, we analyze its internal representations. Specifically, given paired ground truth text and raw video, we mask some keywords (both verb and nouns) and ask the model to predict these masked words and further find out which video patch has strong correlations with the masked words. We use optimal transports to calculate the correlation between video and text. We only show the attention weight that is larger than the given threshold and give some examples of cross-modal alignment in Fig. 7. We find the model can predict correct nouns and verbs in most cases. Sometimes, it predicts the wrong word but with a similar meaning to the correct word. e.g. \u201cguy\u201d and \u201cman\u201d. Benefiting from temporal modeling, we also find that the model attends to the motion regions for verbs like \u201cwaving\u201d and \u201cwalking\u201d.\\n\\n6. Conclusions & Future Work\\nIn this paper, we present the first unified end-to-end Video-Language Pre-training (VLP) architecture with raw video and text as input, All-in-one. All-in-one is able to compete with contemporaries who are equipped with additional robust off-the-shelf video visual embedding networks and shows potential for the future by learning just one cross-modality fusion network. Instead of solely concentrating on heavier single-modality embedders or larger fusion models, we expect that the VLP community would place more emphasis on lightweight end-to-end modal interactions within Transformer modules. Although these preliminary findings are promising, this novel approach to unified video-language interaction also poses additional difficulties, particularly with regard to fine-grained word region matching. Additionally, the temporal modeling has yet to be completely investigated, and we hope the usage of All-in-one for other single-modality tasks in future research.\\n\\nAcknowledgement\\nThis project is supported by the National Research Foundation, Singapore under its NRFF Award NRF-NRFF13-2021-0008. The computational work for this article was partially performed on resources of the National Supercomputing Centre, Singapore.\"}"}
{"id": "CVPR-2023-130", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"All in One: Exploring Unified Video-Language Pre-training\\n\\nJinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Kevin Qinghong Lin, Satoshi Tsutsui, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, Mike Zheng Shou\\n\\n1. Introduction\\n\\nScience advances rather steadily for most of the time, but sometimes has a disruptive episode, where \\\"an older paradigm is replaced in whole or in part by an incompatible new one.\\\" [24] In this regard, Video-Language Pre-training (VLP) models have recently experienced steady progress, where joint representations are generally produced with a multimodal fusion network after extracting the visual and language features through unimodal encoders [10, 26, 50, 64]. We are here to break it and replace them with \\\"an incompatible new one\\\" that has NO unimodal encoders.\\n\\nThe pre-train and then fine-tune scheme, has become a standard paradigm to learn transferable video-language representations for a wide range of downstream video-text tasks [6, 15, 52, 52, 53, 61]. Mainstream methods attempt to boost the pre-training in two ways:\\n\\ni. adopting more expensive video/text encoders to obtain more powerful unimodal features [3, 10]\\n\\nii. designing heavier fusion networks to enhance the association between modalities [61, 64].\\n\\nInstead of following these trends, we fundamentally rethink design decisions and develop the simplest and most lightweight architecture that learns video-language representations from their raw inputs in an end-to-end manner. Our model does not need any unimodal encoders (e.g., object detector in [64] or ResNet visual encoder in [26]) or complex fusion layers, but embeds visual and text signals in a unified manner, termed as All-in-one Transformer in our paper. Our design is inspired by recent studies [1, 21, 37].\"}"}
{"id": "CVPR-2023-130", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that perform multimodal pre-training under the presumption that Transformer can process visual data in the same way as it processes text. However, our work is not the straightforward application of them. It is not trivial how to embed videos for our unified Transformer due to the unique challenge of modeling temporal information without adding much computational cost.\\n\\nExisting works model temporal information by designing temporal attention layers [3] or using temporal-aware visual encoders (e.g., 3D convnets in [64] or Video Swin [36] in [10]). We cannot simply use them in our unified All-in-one Transformer because they are modality-dependent and computationally too expensive. To address this issue, we design a novel, effective, and efficient method to model temporal information. Our model only needs three frames per video clip, which is much lower than other models (e.g., 16 [26] or 32 [1]) but can achieve the comparable performance to them. Nevertheless, we are still not satisfied with the computational cost in the self-attention layer. To further reduce the computational cost, we propose the temporal token rolling operation, which is a cyclic attention between small proportions of the visual tokens in each frame (Fig. 3-right). This is much more efficient than a naive self-attention approach on flattened tokens (Fig. 3-bottom). Furthermore, our modality-agnostic design enables us to use our pre-trained model as a powerful uni-modal feature extractor by feeding only video or text inputs. This can significantly reduce the computational cost for retrieval task because we can simply compute the cosine similarity of texts and videos soon after the pre-training, eliminating the need for training additional fusion module of projecting the disjoint text and visual features into a common space (Fig. 1-b). Taken together, our All-in-one architecture achieves much less FLOPs and better text-to-video performance than previous work (Fig. 1-c), despite the fact that we use the same pre-training objectives [10, 26].\\n\\nContributions.\\n\\n(1) We introduce the simplest, most lightweight, and most efficient video-language model, namely All-in-one Transformer, which is the first to capture video-language representations from the raw visual and textual signals end-to-end in a unified backbone architecture.\\n\\n(2) We elucidate and tackle the difficulties of applying a unified and shared backbone for multimodal video and text data, that is, how to properly process the unique temporal information of videos. A novel temporal token rolling operation is proposed to capture the temporal representations of sparsely sampled frames without any extra parameters or increasing time complexity.\\n\\n(3) We propose a success practical to overcome the slow retrieval of one-stream model and explore how to cotrain the image and video data together in better ways.\\n\\n(4) Comprehensive experiments on five downstream video-text tasks of eleven datasets fully demonstrate the superiority of our pre-trained All-in-one Transformer on both effectiveness and efficiency compared to recent mainstream methods [3, 10, 26].\\n\\n2. Related Work\\n\\nVideo-Language Pre-training. Pre-training on large-scale video-text pairs and fine-tuning on specific downstream tasks gradually become the standard paradigm in the video-language domain. Pre-trained models show strong transfer ability in a series of popular downstream video-language tasks including Text-to-Video Retrieval [6, 53], Video Question Answering [15, 52], and Visual Storytelling [61]. Previous approaches [43, 58, 64] leverage offline video and text features extracted from off-the-shelf visual and language backbones. Some recent methods [3, 26, 33, 55] have attempted to train models in an end-to-end fashion but still rely on well-trained visual encoders for feature extraction. In addition, these works mainly pre-train models on the image-text datasets, like Google Conceptual Captions [42] and Visual genome [23], and fine-tune the pre-trained models for downstream video-language tasks. In this work, we try to challenge this paradigm and explore an effective strategies for pre-training on pure large-scale video-text benchmarks with only one network, and adapt our approach to various video-language downstream tasks.\\n\\nTemporal Modeling in Video Understanding. Temporal modeling is a fundamental yet challenging topic in video representation learning. Several classic ideas including sparse sampling [49], 3D-type operations [5, 36] are proposed for temporal modeling in both convolution and Transformer architectures. 3D-type temporal modeling like Timesformer [4] is extremely time-consuming because of the increasing number of sampled frames, which can be disastrous for large-scale pre-training techniques. Sparse sampling along the temporal dimension, a type of data augmentation proposed in TSN [49], has been widely adopted to train video backbones. Based on this, more related works [31, 50] try to shift channels among different frames for temporal modeling in action recognition. Inspired by these works, we try to roll video tokens for better alignment between modalities. This work focuses on parameter-free temporal modeling based on sparsely sampled frames without heavy 3D-type operation.\\n\\nUnified Architecture Design for Multimodal Data. Recently the unified model, which is capable of processing either unimodal or multimodal inputs with a shared encoder, has attracted a lot of attention. V ATT [1] and Merlot Reserve [60] trains a shared transformer with unimodal inputs to process Video, Audio, and Text via multimodal contrastive learning. Omnivore [13] converts the image, video, and single-view 3D modalities into embeddings that are fed into a Transformer model and trains the model with multi-task learning, which focuses on image/video/scene classification. In image-text pre-training, the early work Unimo [29]...\"}"}
{"id": "CVPR-2023-130", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Our model is simple, efficient, and based on the commonly-used ViT [8], where our additional parameters are only in the light text tokenizer and the task heads. Since the ViT cannot model the temporal information, we also introduce a parameter-free temporal token rolling layer before each self-attention block.\\n\\n[29] solves both understanding and generation tasks with cross-modal contrastive learning. UFO [47] also uses contrastive learning and employs a momentum teacher to guide the pre-training of an image-text shared encoder, which incurs large computational costs. Based on cross-modal contrastive learning, our work can also process unimodal inputs and perform retrieval tasks in a dual-stream manner, which is very efficient. To the best of our knowledge, All-in-one Transformer is the first unified network for VLP.\\n\\n3. Method\\n\\nWe propose All-in-one Transformer, a generic framework that enables end-to-end learning on video and language data, by learning joint representations directly from raw video pixels and raw text tokens, instead of the deeper feature from two separate deep embedders. All-in-one has a succinct architecture as a Video-Language Pre-training (VLP) model with a parameter-free temporal modeling layer. In model design, we make the pipeline as simple as possible so that the model can be used almost out of the box.\\n\\n3.1. Unified Video-language Transformer\\n\\nFig. 2 gives an overview of All-in-one framework, it mainly contains three parts: Video and Text Tokenizer, \\\\( N \\\\) Transformer blocks and a pretext head. For each video, All-in-one uses a sparse sampling strategy with \\\\( S \\\\) segments (one frame per segment) at each training step, rather than full-length videos. The sampled video clip and text are inputted into the same Transformer, as described below.\\n\\n**Video & Text Tokenizer.** The video tokenizer slices an input video into patches, maps the patches into tokens with a linear projection, and adds learnable spatio-temporal position embeddings. The text tokenizer similarly maps the words into tokens with a word embedding layer. Following common practices [21, 26], we also add modality type embeddings to distinguish the token of video or texts.\\n\\n**Cross-modality Fusion.** The All-in-one fuses the text and video tokens using the \\\\( N \\\\) transformer blocks as follows. It concatenates the text and vision tokens of each frame as \\\\( z_0 = [\\\\hat{t}; \\\\hat{v}] \\\\). It then feeds the \\\\( z_0 \\\\) into the \\\\( N \\\\) Transformer blocks, where each block consists of a temporal Token Rolling layer, a multi-head self-attention layer, and a multilayer perceptron, whose weights are initialized from pre-trained ViT [8] or DeiT [45]. Formally, for \\\\( d = 1 \\\\ldots N \\\\),\\n\\n\\\\[\\n    z_{d-1}^{r} = \\\\text{TTR}(z_{d-1}^{r}),\\n    z_{d} = \\\\text{MLP}(\\\\text{MSA}(z_{d-1}^{r})),\\n\\\\]\\n\\nwhere \\\\( \\\\text{MSA} \\\\) means multiheaded self-attention, \\\\( \\\\text{MLP} \\\\) is multilayer perceptron and \\\\( \\\\text{TTR} \\\\) is short for Temporal Token Rolling Module, which aims to learn temporal information.\\n\\n**3.2. Temporal Token Rolling**\\n\\n**Motivation.** In VLP, the common way to model the temporal information is to add additional time attention layers [3] in the vision encoder or to use the feature from a deep off-the-shelf video encoder [10, 64] (e.g, VideoSwin [36]). However, these techniques are particularly designed for video and thus cannot be applied to process text signals, as well as bringing a large amount of additional parameters. For example, simply adding one temporal attention layer to each block of the Transformer will increase the model's parameters from 86M to 121.7M (an increase of 42%) [4]. Thus, we cannot use these techniques in our unified framework in an affordable way, so we turn to finding new ways to learn temporal information with modest parameters.\\n\\n**Approach.** A straightforward approach, denoted as \\\"Flatten\\\", is to concatenate video and text tokens together and flatten into one tensor, which will be fed into the self-attention blocks. Given a text token of length \\\\( m \\\\) and a video token of length \\\\( S \\\\times n \\\\), we show the flatten version in Fig. 3.\\n\\nHowever, as the self-attention layer has quadratic complexity, the computation cost will be \\\\( O((m + 3n)^2) \\\\), about \\\\( S^2 \\\\) times more than 1-frame All-in-one. To reduce the computational cost, we exchange information for different time steps.\"}"}
{"id": "CVPR-2023-130", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Text to video attention weight distribution over tokens: The Temporal Token Rolling layer causes text tokens to focus more on rolled tokens, contrasting with prior centric attention.\\n\\n3.3. Training Objectives\\n\\n3.3.1 Pre-training\\n\\nWe pre-train All-in-one with two commonly used objectives to train VLP models: video-text matching (VTM) [26] and masked language modeling (MLM) [7].\\n\\nVideo-text Matching. Given a paired video-text input, we randomly replace the paired video with a different video with the probability of 0.5 and ask the model to distinguish them. For the cls token of the last block, a single linear layer VTM head projects tokens to logits over binary classes. We compute the negative log-likelihood loss as our VTM loss.\\n\\nMasked Language Modeling. MLM [7] aims to predict the ground truth labels of masked text tokens from the remaining text and video tokens. Following common practices [7, 21], we randomly mask text tokens with a probability of 0.15 and model it as a classification task.\\n\\n3.3.2 BVTC for fast downstream retrieval\\n\\nVision-text Contrastive loss has shown great success for efficient retrieval task of dual-stream models, which encode image and text independently [16, 40]. However, this objective cannot transfer to one-stream models easily because the input of these models are the joint of vision and text. For these models, the common way to do retrieval is to measure vision-text pairwise matching scores, which is very slow and cannot be applied to large-scale retrieval [29, 30]. To overcome the disadvantage of the low retrieval efficiency of one-stream models, we introduce a new paradigm to utilize a contrastive loss for retrieval.\\n\\nBackbone-shared Video-text Contrastive Loss. As shown in Fig. 5, we input video and text independently to the shared encoder to obtain high-level features for video-text pairs. We then feed these features into a modality-specific projection head to project them into the shared embedding space. Following common practice [3, 46], we use a symmetric (both text-to-video and video-to-text) contrastive loss based on these features. When doing retrieval tasks, we only need to extract unimodal features once, which significantly reduces the computational cost. More discussion is reported in the supplementary material.\\n\\n3.4. Image Video Co-Training\\n\\nSince image datasets often provide more comprehensive and fine-grained annotations than video datasets, we use both image and video datasets. Inspired by recent studies in action recognition that demonstrates a unified Transformer model can be extended to both image and video classification tasks [37, 63], we propose to leverage both image and video data to train All-in-one jointly.\\n\\nThe naive solution is to change the training pipeline with a minimal modification by considering an image as a single-frame video [13, 37]. However, we experimentally find that this simple solution damages the learning of temporal information and leads to unstable training (refer to supplementary material for more discussion). In this work, we propose a balanced sampling co-training strategy. Specifically, we\"}"}
{"id": "CVPR-2023-130", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in Neural Information Processing Systems, 34, 2021.\\n\\n[2] Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein. Noise estimation using density estimation for self-supervised multimodal learning. In AAAI, 2021.\\n\\n[3] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738, 2021.\\n\\n[4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding. arXiv preprint arXiv:2102.05095, page 4, 2021.\\n\\n[5] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017.\\n\\n[6] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 190\u2013200, 2011.\\n\\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\n[9] Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng Huang. Heterogeneous memory enhanced multimodal attention model for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1999\u20132007, 2019.\\n\\n[10] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681, 2021.\\n\\n[11] Yuying Ge, Yixiao Ge, Xihui Liu, Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie, and Ping Luo. Miles: visual bert pre-training with injected language semantics for video-text retrieval. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXV, pages 691\u2013708. Springer, 2022.\\n\\n[12] Shijie Geng, Ji Zhang, Zuohui Fu, Peng Gao, Hang Zhang, and Gerard de Melo. Character matters: Video story understanding with character-aware relations. arXiv preprint arXiv:2005.08646, 2020.\\n\\n[13] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, and Ishan Misra. Omnivore: A single model for many visual modalities. arXiv preprint arXiv:2201.08377, 2022.\\n\\n[14] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. arXiv preprint arXiv:2110.07058, 2021.\\n\\n[15] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2758\u20132766, 2017.\\n\\n[16] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.\\n\\n[17] Jianwen Jiang, Ziqiang Chen, Haojie Lin, Xibin Zhao, and Yue Gao. Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11101\u201311108, 2020.\\n\\n[18] Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, and Chang D Yoo. Gaining extra supervision via multi-task learning for multi-modal video question answering. In 2019 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2019.\\n\\n[19] Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, and Chang D Yoo. Progressive attention memory network for movie story question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8337\u20138346, 2019.\\n\\n[20] Junyeong Kim, Minuk Ma, Trung Pham, Kyungsu Kim, and Chang D Yoo. Modality shifting attention network for multi-modal video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10106\u201310115, 2020.\\n\\n[21] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR, 2021.\\n\\n[22] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706\u2013715, 2017.\\n\\n[23] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, pages 32\u201373, 2017.\\n\\n[24] Thomas S. Kuhn. The structure of scientific revolutions. University of Chicago Press, 1962.\\n\\n[25] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4163\u20134172, 2022.\"}"}
{"id": "CVPR-2023-130", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[26] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341, 2021.\\n\\n[27] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvqa+: Spatio-temporal grounding for video question answering. arXiv preprint arXiv:1904.11574, 2019.\\n\\n[28] Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, et al. Value: A multi-task benchmark for video-and-language understanding evaluation. arXiv preprint arXiv:2106.04632, 2021.\\n\\n[29] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409, 2020.\\n\\n[30] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020.\\n\\n[31] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7083\u20137093, 2019.\\n\\n[32] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. Swin-bert: End-to-end transformers with sparse attention for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17949\u201317958, 2022.\\n\\n[33] Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric video-language pretraining. 2022.\\n\\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.\\n\\n[35] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019.\\n\\n[36] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. arXiv preprint arXiv:2106.13230, 2021.\\n\\n[37] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019.\\n\\n[38] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9879\u20139889, 2020.\\n\\n[39] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2630\u20132640, 2019.\\n\\n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language super-vision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\\n\\n[41] Paul Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid. Look before you speak: Visually contextualized utterances. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16877\u201316887, 2021.\\n\\n[42] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.\\n\\n[43] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7464\u20137473, 2019.\\n\\n[44] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7464\u20137473, 2019.\\n\\n[45] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357. PMLR, 2021.\\n\\n[46] Alex Jinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Object-aware video-language pre-training for retrieval. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022.\\n\\n[47] Jianfeng Wang, Xiaowei Hu, Zhe Gan, Zhengyuan Yang, Xiyang Dai, Zicheng Liu, Yumao Lu, and Lijuan Wang. Ufo: A unified transformer for vision-language representation learning. arXiv preprint arXiv:2111.10023, 2021.\\n\\n[48] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022.\\n\\n[49] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks for action recognition in videos. IEEE transactions, 6607.\"}"}
{"id": "CVPR-2023-130", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip: A new paradigm for video action recognition. arXiv preprint arXiv:2109.08472, 2021.\\n\\nYujia Xie, Xiangfeng Wang, Ruijia Wang, and Hongyuan Zha. A fast proximal point method for computing exact Wasserstein distance. In Uncertainty in artificial intelligence, pages 433\u2013453. PMLR, 2020.\\n\\nDejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645\u20131653, 2017.\\n\\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.\\n\\nHongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo. Clip-vip: Adapting pre-trained image-text model to video-language representation alignment. arXiv preprint arXiv:2209.06430, 2022.\\n\\nRui Yan, Mike Zheng Shou, Yixiao Ge, Alex Jinpeng Wang, Xudong Lin, Guanyu Cai, and Jinhui Tang. Video-text pre-training with learned regions. arXiv preprint arXiv:2112.01194, 2021.\\n\\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686\u20131697, 2021.\\n\\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. In Advances in Neural Information Processing Systems, 2022.\\n\\nJianwei Yang, Yonatan Bisk, and Jianfeng Gao. Taco: Token-aware cascade contrastive learning for video-text alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11562\u201311572, 2021.\\n\\nYoungjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV), pages 471\u2013487, 2018.\\n\\nRowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16375\u201316387, 2022.\\n\\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nBowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In Proceedings of the European Conference on Computer Vision (ECCV), pages 374\u2013390, 2018.\\n\\nBowen Zhang, Jiahui Yu, Christopher Fifty, Wei Han, Andrew M Dai, Ruoming Pang, and Fei Sha. Co-training transformer with videos and images improves action recognition. arXiv preprint arXiv:2112.07175, 2021.\\n\\nLinchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8746\u20138755, 2020.\"}"}
