{"id": "CVPR-2023-786", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multi-view results. Multiple object views for selected scenes, camouflaged using our proposed model with four input views. The views shown here were held out and not provided to the network as input during training.\\n\\nWe randomly select real image crops \\\\( y \\\\) from each background image \\\\( I \\\\) and select fake crops \\\\( \\\\hat{y} \\\\) containing the camouflaged object from \\\\( \\\\hat{I} \\\\). We use the standard GAN loss as our objective. To train the discriminator, \\\\( D \\\\), we minimize:\\n\\n\\\\[\\nL_D = -E_{y}[\\\\log D(y)] - E_{\\\\hat{y}}[\\\\log(1 - D(\\\\hat{y}))]\\n\\\\]\\n\\nwhere the expectation is taken over patches randomly sampled from a training batch. We implement our discriminator using the fully convolutional architecture of Isola et al. [24].\\n\\nOur texturing function, meanwhile, minimizes:\\n\\n\\\\[\\nL_{adv} = -E_{\\\\hat{y}}[\\\\log D(\\\\hat{y})]\\n\\\\]\\n\\nSelf-supervised multi-view camouflage. We train our texturing function \\\\( G \\\\) (Eq. 2), which is fully defined by the image encoder \\\\( E \\\\) and the MLP \\\\( T \\\\), by minimizing the combined losses:\\n\\n\\\\[\\nL_G = L_{photo} + \\\\lambda_{adv} L_{adv}\\n\\\\]\\n\\nwhere \\\\( \\\\lambda_{adv} \\\\) controls the importance of the two losses.\\n\\nIf we were to train the model with only the input object, the discriminator would easily overfit, and our model would fail to obtain a learning signal. Moreover, the resulting texturing model would only be specialized to a single input shape, and may not generalize to others. To address both of these issues, we provide additional supervision to the model by training it to camouflage randomly augmented shapes at random positions, and from random subsets of views.\\n\\nWe sample object positions on the ground plane \\\\( g \\\\), within a small radius proportional to the size of input object \\\\( S \\\\). We uniformly sample a position within the disk to determine the position for the object. In addition to randomly sampled locations, we also randomly scale the object within a range to add more diversity to training data. During training, we randomly select \\\\( N_i \\\\) input views and \\\\( N_r \\\\) rendering views without replacement from a pool of training images sampled from \\\\( V \\\\). We calculate \\\\( L_{photo} \\\\) on both \\\\( N_i \\\\) input views and \\\\( N_r \\\\) views while \\\\( L_{adv} \\\\) is calculated on \\\\( N_r \\\\) views.\\n\\n4. Results\\n\\nWe compare our model to previous multi-view camouflage methods using cube shapes, as well as on complex animal and furniture shapes.\\n\\n4.1. Dataset\\n\\nWe base our evaluation on the scene dataset of [33], placing objects at their predefined locations. Each scene contains 10-25 photos from different locations. During capturing, only background images are captured, with no actual object is placed in the scene. Camera parameters are estimated using structure from motion [47]. To support learning-based methods that take 4 input views, while still having a diverse evaluation set, we use 36 of the 37 scenes (removing one very small 6-view scene). In [33], their methods are only evaluated on cuboid shape, while our method can be adapted to arbitrary shape without any change to the model. To evaluate our method on complex shapes, we generate camouflage textures for a dataset of 49 animal meshes from [60]. We also provide a qualitative furniture shape from [11] (Fig. 1).\\n\\n4.2. Implementation Details\\n\\nFor each scene, we reserve 1-3 images for testing (based on the total number of views in the scene). Following other work in neural textures [23], we train one network per scene. We train our models using the Adam optimizer [26] with a learning rate of \\\\( 2 \\\\times 10^{-4} \\\\) for the texturing function \\\\( G \\\\) and \\\\( 10^{-4} \\\\) for the discriminator \\\\( D \\\\). We use \\\\( \\\\lambda_{adv} = 0 \\\\) in Eq. 8. We resize all images to be 384 \u00d7 576 and use square crops of 128 \u00d7 128 to calculate losses.\\n\\nTo ensure that our randomly chosen object locations are likely to be clearly visible from the cameras, we randomly sample object positions on the ground plane (the base of the cube in [33]). We allow these objects to be shifted at most \\\\( 3 \\\\times \\\\) the cube's length. During training, for each sample, we randomly select \\\\( N_i = 4 \\\\) views as input views and render the object on another \\\\( N_r = 2 \\\\) novel views. The model is trained with batch size of 8 for approximately 12k iterations. For evaluation, we place the object at the predefined position from [33] and render it in the reserved test views.\"}"}
{"id": "CVPR-2023-786", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Comparison between methods for cuboids and complex shapes. We compare our method with previous approaches for the task of concealing (a) cuboids and (b) animal shapes. Our method produces objects with more coherent texture, with the 4-view model filling in textures that tend to be occluded.\\n\\n4.3. Experimental Settings\\n\\n4.3.1 Cuboid shapes\\n\\nWe first evaluate our method using only cuboid shapes to compare with the state-of-the-art methods proposed in Owens et al. [33]. We compare our proposed 2-view and 4-view models with the following approaches:\\n\\n- **Mean.** The color for each 3D point is obtained by projecting it into all the views that observe it and taking the mean color at each pixel.\\n- **Iterative projection.** These methods exploit the fact that an object can (trivially) be completely hidden from a single given viewpoint by back-projecting the image onto the object. When this is done, the object is also generally difficult to see from nearby viewpoints as well. In the Random method, the input images are selected in a random order, and each one is projected onto the object, coloring any surface point that has not yet been filled. In Greedy, the model samples the photos according to a heuristic that prioritizes viewpoints that observe the object head-on (instead of random sampling). Specifically, the photos are sorted based on the number of object faces that are observed from a direct angle (>70\u00b0 with the viewing angle).\\n- **Example-based texture synthesis.** These methods use Markov Random Fields (MRFs) [1, 16, 36] to perform example-based texture synthesis. These methods simultaneously minimize photoconsistency, as well as smoothness cost that penalizes unusual textures. The Boundary MRF model requires nodes within a face to have same labels, while Interior MRF does not.\\n\\n4.3.2 Complex shapes\\n\\nWe also evaluated our model on a dataset containing 49 animal meshes [60]. Camouflaging these shapes presents unique challenges. In cuboids, the set of object points that each camera observes is often precisely the same, since each viewpoint sees at most 3 adjacent cube faces (out of 6 total). Therefore, it often suffices for a model to camouflage the most commonly-viewed object points with a single, coherent texture taken from one of the images, putting any conspicuous seams elsewhere on the object. In contrast, when the meshes have more complex geometry, each viewpoint sees a very different set of object points. Since our model operates on arbitrary shapes, using these shapes requires no changes to the model. We trained our method with the animal shapes and placed the animal object at the same position as in the cube experiments. We adapt the simpler baseline methods of [33] to these shapes, however we note that the MRF-based synthesis methods assume a grid graph structure on each cube face, and hence cannot be adapted to complex shapes without significant changes.\"}"}
{"id": "CVPR-2023-786", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method          | Confusion rate | Avg. time (s) | Med. time (s) |\\n|-----------------|----------------|---------------|---------------|\\n| Mean            | 16.09 \u00b1 2.29%  | 4.82 \u00b1 0.37 s | 2.95 \u00b1 0.14 s |\\n| Random          | 39.66 \u00b1 3.02%  | 7.63 \u00b1 0.50 s | 4.68 \u00b1 0.35 s |\\n| Greedy          | 40.32 \u00b1 2.96%  | 7.94 \u00b1 0.52 s | 4.72 \u00b1 0.36 s |\\n| Boundary MRF [33] | 41.29 \u00b1 2.95%  | 8.50 \u00b1 0.51 s | 5.39 \u00b1 0.40 s |\\n| Interior MRF [33] | 44.66 \u00b1 3.01%  | 8.19 \u00b1 0.51 s | 5.19 \u00b1 0.42 s |\\n| Ours (2 views)  | 51.58 \u00b1 2.99%  | 9.19 \u00b1 0.51 s | 6.46 \u00b1 0.42 s |\\n| Ours (4 views)  | 53.95 \u00b1 3.05%  | 9.29 \u00b1 0.57 s | 6.11 \u00b1 0.50 s |\\n\\nTable 1. Perceptual study results with cubes. Higher numbers represent a better performance. We report the 95% confidence interval of these metrics.\\n\\n| Method          | Confusion rate | Avg. time (s) | Med. time (s) |\\n|-----------------|----------------|---------------|---------------|\\n| Mean            | 36.46 \u00b1 2.17%  | 6.39 \u00b1 0.30 s | 4.04 \u00b1 0.17 s |\\n| Pixel-wise greedy | 50.43 \u00b1 2.20%  | 7.25 \u00b1 0.32 s | 4.73 \u00b1 0.20 s |\\n| Random          | 51.61 \u00b1 2.29%  | 7.81 \u00b1 0.36 s | 5.25 \u00b1 0.36 s |\\n| Greedy          | 52.50 \u00b1 2.18%  | 7.69 \u00b1 0.34 s | 5.13 \u00b1 0.25 s |\\n| Ours (4 views)  | 61.93 \u00b1 2.14%  | 8.06 \u00b1 0.33 s | 5.66 \u00b1 0.27 s |\\n\\nTable 2. Perceptual study results on animal shapes. Higher numbers represent a better performance. We report the 95% confidence interval of these metrics.\\n\\nAs with cube experiment, we take the mean color from multiple input views as the simplest baseline. Iterative projection. We use the same projection order selection strategy as in cube experiment. We determine whether a pixel is visible in the input views by using a ray-triangle intersection test.\\n\\n4.4. Perceptual Study\\nTo evaluate the effectiveness of our method, we conduct a perceptual study. We generally follow the setup of [33], however we ask users to directly click on the camouflaged object [53], without presenting them with a second step to confirm that the object (or isn\u2019t) present. This simplified the number of camouflaged objects that subjects see by a factor of two. We recruited 267 and 375 participants from Amazon Mechanical Turk for the perceptual study on cuboid and complex shapes, respectively, and ensured no participant attended both of the perceptual studies.\\n\\nEach participant was shown one random image from the reserved images of each scene in a random order. The first 5 images that they were shown were part of a training exercise, and are not included in the final evaluation. We asked participants to search for the camouflaged object in the scene, and to click on it as soon as they found it. The object in the scene was camouflaged by a randomly chosen algorithm, and placed at the predefined position. After clicking on the image, the object outline was shown to the participant. We recorded whether the participant correctly clicked on the camouflaged object, and how long it took them to click. Each participant had one trial for each image and a maximum of 60 s to find the camouflaged object.\\n\\nResults on cuboid shapes.\\nThe perceptual study results on cuboid shapes are shown in Table 1. We report the confusion rate, average time, and median time measured over different methods. We found that our models significantly outperform the previous approaches on all metrics. To test for significance, we followed [33] and used a two-sided t-test for the confusion rate and a two-sided Mann-Whitney U test (with a 0.05 threshold for significance testing). We found that our method outperforms all the baseline methods significantly in the confusion rate metric. Both of our model variations outperform Interior MRF (p < 2 \u00d7 10^-3 and p < 3 \u00d7 10^-5). There was no significant difference between 2 and 4 views (p = 0.28). In terms of time-to-click, our method also beats the two MRF-based methods. Compared with Boundary MRF, our method requires more time for participants to click the camouflaged object (p = 0.0024 for 2 views and p = 0.039 for 4 views).\\n\\nResults on complex shapes.\\nThe perceptual study results on complex shapes are shown in Table 2. We found that our model obtained significantly better results than previous work on confusion rate. Our model also obtained significantly better results on the time-to-find metric. We found that in terms of confusion rate, our method with 4 input views is significantly better than the baseline methods, 9.42% better than Greedy method and 10.32% better than Random method. For time-to-click, our method also performs better than baseline methods compared with Greedy and Random.\\n\\n4.5. Qualitative Results\\nWe visualize our generated textures in several selected scenes for both cube shapes and animal shapes in Figure 3. We compare our method qualitatively with baseline methods from [33] in Figure 4. We found that our model obtained significantly more coherent textures than other approaches. The 2-view model has a failure case when none of the input views cover an occluded face, while the 4-view model is able to generally avoid this situation. We provide additional results in the supplement.\\n\\nEffects of shadows.\\nPlacing an object in a real scene may create shadows. We ask how these shadows effect...\"}"}
{"id": "CVPR-2023-786", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Ablations. We show how the choice of different components changes the quality of the camouflage texture.\\n\\nOur model's solution (Figure 5), exploiting the fact that these shadows are independent of the object's texture and hence function similarly to other image content. In [33], photos with (and without) a real cube are taken from the same pose. We manually composite these paired images to produce an image without the real cube but with its real shadow. We then provide these images as conditioning input to our model, such that it incorporates the presence of the shadow into its camouflage solution. While our solution incorporates some of the shadowed region, the result is similar. Note that other lighting effects can be modeled as well (e.g., by compensating for known shading on the surface).\\n\\n4.6. Automated evaluation metrics\\n\\nTo help understand our proposed model, we perform an automated evaluation and compare with ablations:\\n\\n- **Adversarial loss:** To evaluate the importance of $\\\\mathcal{L}_{adv}$, we set $\\\\lambda_{adv}$ to 0 in Eq. 8. We evaluate the model performance with only $\\\\mathcal{L}_{photo}$ used during training.\\n\\n- **Photoconsistency:** We evaluate the importance of using all $N_i$ input views in Eq. 5. The ablated model has $\\\\mathcal{L}_{photo}$ only calculated on $N_r$ rendering views during training.\\n\\n- **Architecture:** We evaluate the importance of our pixel-aligned feature representation. In lieu of this network, we use the feature encoder from pixelNeRF [55].\\n\\n- **Inpainting:** Since inpainting methods cannot be directly applied to our task without substantial modifications, we combine several inpainting methods with the Greedy model. We selected several recent inpainting methods: DeepFillv2 [56], LaMa [50], LDM [39] to inpaint the object shape in each view, then backproject this texture onto the 3D surface, using the geometry-based ordering from [33].\\n\\nEvaluation metrics.\\n\\nTo evaluate the ablated models, we use LPIPS [59] and SIFID metrics [43]. Since the background portion of the image remains unmodified, we use crops centered at the rendered camouflaged objects.\\n\\nResults.\\n\\nQuantitative results are shown in Table 3 and qualitative results are in Figure 6. We found that our full 4-view model is the overall best-performing method. In particular, it significantly outperforms the 2-view model, which struggles when the viewpoints do not provide strong coverage from all angles (Fig. 6). We also found that the adversarial loss significantly improves performance. As can be seen in Fig. 6, the model without an adversarial loss fails to choose a coherent solution and instead appears to average all of the input views. The model that uses all views to compute photoconsistency tends to generate more realistic textures, perhaps due to the larger availability of samples. Compared with the pixelNeRF encoder, our model generates textures with higher fidelity, since it receives more detailed feature maps from encoder. We obtain better performance on LPIPS but find that this variation of the model achieves slightly better SIFID. This suggests that the architecture of our pixel-aligned features provides a modest improvement. Finally, we found that we significantly outperformed the inpainting and MRF-based methods.\\n\\n5. Discussion\\n\\nWe proposed a method for generating textures to conceal a 3D object within a scene. Our method can handle diverse and complex 3D shapes and significantly outperforms previous work in a perceptual study. We see our work as a step toward developing learning-based camouflage models. Additionally, the animal kingdom has a range of powerful camouflage strategies, such as disruptive coloration and mimicry, that cleverly fool the visual system and may require new learning methods to capture.\\n\\nLimitations.\\n\\nAs in other camouflage work [33], we do not address the problem of physically creating the camouflaged object, and therefore do not systematically address practicalities like lighting and occlusion.\\n\\nEthics.\\n\\nThe research presented in this paper has the potential to contribute to useful applications, particularly to hiding unsightly objects, such as solar panels and utility boxes. However, it also has the potential to be used for negative applications, such as hiding nefarious military equipment and intrusive surveillance cameras.\\n\\nAcknowledgements.\\n\\nWe thank Justin Johnson, Richard Higgins, Karan Desai, Gaurav Kaul, Jitendra Malik, and Derya Akkaynak for the helpful discussions and feedback. This work was supported in part by an NSF GRFP for JC.\"}"}
{"id": "CVPR-2023-786", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"GANmouflage: 3D Object Nondetection with Texture Fields\\n\\nRui Guo 1,3*\\nJasmine Collins 2\\nOscar de Lima 1\\nAndrew Owens 1\\n\\nUniversity of Michigan 1\\nUC Berkeley 2\\nXMotors.ai 3\\n\\nFigure 1. We learn to camouflage 3D objects within scenes. Given an object's shape, position, and a distribution of possible viewpoints it will be seen from, we estimate a texture field that will conceal it. We show example outputs from our model, with two viewpoints for each camouflaged object. Please see the videos on our webpage (https://rrrrrguo.github.io/ganmouflage) for more examples.\\n\\nAbstract\\n\\nWe propose a method that learns to camouflage 3D objects within scenes. Given an object's shape and a distribution of viewpoints from which it will be seen, we estimate a texture that will make it difficult to detect. Successfully solving this task requires a model that can accurately reproduce textures from the scene, while simultaneously dealing with the highly conflicting constraints imposed by each viewpoint. We address these challenges with a model based on texture fields and adversarial learning. Our model learns to camouflage a variety of object shapes from randomly sampled locations and viewpoints within the input scene, and is the first to address the problem of hiding complex object shapes. Using a human visual search study, we find that our estimated textures conceal objects significantly better than previous methods.\\n\\n*Work done while at University of Michigan\\n\\n1. Introduction\\n\\nUsing fur, feathers, spots, and stripes, camouflaged animals show a remarkable ability to stay hidden within their environment. These capabilities developed as part of an evolutionary arms race, with advances in camouflage leading to advances in visual perception, and vice versa.\\n\\nInspired by these challenges, previous work [33] proposed the object nondetection problem: to create an appearance for an object that makes it undetectable. Given an object's shape and a sample of photos from a scene, the goal is to produce a texture that hides the object from every viewpoint that it is likely to be observed from. This problem has applications in hiding unsightly objects, such as utility boxes [7], solar panels [29, 49], and radio towers, and in concealing objects from humans or animals, such as surveillance cameras and hunting platforms. Moreover, since camouflage models must ultimately thwart highly effective visual systems, they may provide a better scientific understanding of the cues that these systems use. Animal camouflage, for instance, has developed strategies for avoiding perceptual grouping and boundary detection cues [30, 52].\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2023-786", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A successful learning-based camouflage system, likewise, must gain an understanding of these cues in order to thwart them. Previous object nondetection methods are based on non-parametric texture synthesis. Although these methods have shown success in hiding cube-shaped objects, they can only directly \\\"copy-and-paste\\\" pixels that are directly occluded by the object, making it challenging to deal with complex backgrounds and non-planar geometry. While learning-based methods have the potential to address these shortcomings, they face a number of challenges. Since even tiny imperfections in synthesized textures can expose a hidden object, the method must also be capable of reproducing real-world textures with high fidelity. There is also no single texture that can perfectly conceal an object from all viewpoints at once. Choosing an effective camouflage requires 3D reasoning, and making trade-offs between different solutions. This is in contrast to the related problem of image inpainting, which can be posed straightforwardly as estimating masked image regions \\\\cite{34}, and which lack the ability to deal with multi-view constraints.\\n\\nWe propose a model based on neural texture fields \\\\cite{23, 32, 35, 42} and adversarial training that addresses these challenges (Figure 2). The proposed architecture and learning procedure allow the model to exploit multi-view geometry, reproduce a scene's textures with high fidelity, and satisfy the highly conflicting constraints provided by the input images. During training, our model learns to conceal a variety of object shapes from randomly chosen 3D positions within a scene. It uses a conditional generative adversarial network (GAN) to learn to produce textures that are difficult to detect using pixel-aligned representations \\\\cite{55} with hypercolumns \\\\cite{20} to provide information from each view.\\n\\nThrough automated evaluation metrics and human perceptual studies, we find that our method significantly outperforms the previous state-of-the-art in hiding cuboid objects. We also demonstrate our method's flexibility by using it to camouflage a diverse set of complex shapes. These shapes introduce unique challenges, as each viewpoint observes a different set of points on the object surface. Finally, we show through ablations that the design of our texture model leads to significantly better results.\\n\\n2. Related Work\\n\\nComputational camouflage\\n\\nWe take inspiration from early work by Reynolds \\\\cite{38} that formulated camouflage as part of an artificial life simulation, following Sims \\\\cite{45} and Dawkins \\\\cite{13}. In that work, a human \\\"predator\\\" interactively detects visual \\\"prey\\\" patterns that are generated using a genetic algorithm. While our model is also trained adversarially, we do so using a GAN, rather than with a human-in-the-loop. Later, Owens et al. \\\\cite{33} proposed the problem of hiding a cuboid object at a specific location from multiple 3D viewpoints, and solved it using nonparametric texture synthesis. In contrast, our model learns through adversarial training to hide both cuboid and more complex objects. Bi et al. \\\\cite{5} proposed a patch-based synthesis method that they applied to the multi-view camouflage problem, and extended the method to spheres. However, this work was very preliminary: they only provide a qualitative result on a single scene (with no quantitative evaluation). Other work inserts difficult-to-see patterns into other images \\\\cite{10, 58}.\\n\\nAnimal camouflage.\\n\\nPerhaps the most well-known camouflage strategy is background matching, whereby animals take on textures that blend into the background. However, animals also use a number of other strategies to conceal themselves, such as by masquerading as other objects \\\\cite{48}, and using disruptive coloration to elude segmentation cues and to hide conspicuous body parts, such as eyes \\\\cite{12}. The object nondetection problem is motivated by animals that can dynamically change their appearance to match their surroundings, such as the octopus \\\\cite{19}. Researchers have also begun using computational models to study animal camouflage. Troscianko et al. \\\\cite{53} used a genetic algorithm to camouflage synthetic bird eggs, and asked human subjects to detect them. Talas et al. \\\\cite{52} used a GAN to camouflage simple triangle-shaped representations of moths that were placed at random locations on synthetic tree bark. In both cases, the animal models are simplified and 2D, whereas our approach can handle complex 3D shapes.\\n\\nCamouflaged object detection.\\n\\nRecent work has sought to detect camouflaged objects using object detectors \\\\cite{15, 28, 54} and motion cues \\\\cite{8, 27}. The focus of our work is generating camouflaged objects, rather than detecting them.\\n\\nAdversarial examples.\\n\\nThe object nondetection problem is related to adversarial examples \\\\cite{6, 18, 51}, in that both problems involve deceiving a visual system (e.g., by concealing an object or making it appear to be from a different class). Other work has generalized these examples to multiple viewpoints \\\\cite{2}. In contrast, the goal of the nondetection problem is to make objects that are concealed from a human visual system, rather than fool a classifier.\\n\\nTexture fields.\\n\\nWe take inspiration from recent work that uses implicit representations of functions to model the surface texture of objects \\\\cite{23, 32, 35, 42}. Oechsle et al. \\\\cite{32} learned to texture a given object using an implicit function, with image and shape encoders, and Saito et al. \\\\cite{42} learned a pixel-aligned implicit function for clothed humans. There are three key differences between our work and these methods. First, these methods aim to reconstruct textures from given images while our model predicts a texture that can conceal an object. Second, our model is conditioned on a 3D input scene with projective structure, rather than a set of images. Third, our model is adversarially trained, whereas the previous methods are not.\\n\\nFor a striking demonstration, see this video from Roger Hanlon: https://www.youtube.com/watch?v=JSq8nghQZqA\"}"}
{"id": "CVPR-2023-786", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Camouflage model. (a) Our model creates a texture for a 3D object that conceals it from multiple viewpoints. (b) We generate a texture field that maps 3D points to colors. The network is conditioned on pixel-aligned features from training images. We train the model to create a texture that is (c) photoconsistent with the input views, as measured using a perceptual loss, and (d) difficult for a discriminator to distinguish from random background patches. For clarity, we show the camouflaged object\u2019s boundaries.\\n\\n3. Learning Multi-View Camouflage\\n\\nOur goal is to create a texture for an object that camouflages it from all of the viewpoints that it is likely to be observed from. Following the formulation of Owens et al. [33], our input is a 3D object mesh $S$ at a fixed location in a scene, a sample of photos $I_1, I_2, ..., I_N$ from distribution $V$, and their camera parameters $K_j, R_j, t_j$. We desire a solution that camouflages the object from $V$, using this sample. We are also provided with a ground plane $g$, which the object has been placed on. Also following [33], we consider the camouflage problem separately from the display problem of creating a real-world object. We assume that the object can be assigned arbitrary textures, and that there is only a single illumination condition. We note that shadows are independent of the object texture, and hence could be incorporated into this problem framework by inserting shadows into images (Sec. 4.5).\\n\\nMoreover, changes in the amount of lighting are likely to affect the object and background in a consistent way, producing a similar camouflage.\\n\\n3.1. Texture Representation\\n\\nWe create a surface texture for the object that, on average, is difficult to detect when observed from viewpoints randomly sampled from $V$. As in prior work [33], we render the object and synthetically insert it into the scene. Similar to recent work on object texture synthesis [23, 32, 35], we represent our texture as a continuous function in 3D space, using a texture field:\\n\\n$$ t_\\\\theta : \\\\mathbb{R}^3 \\\\rightarrow \\\\mathbb{R}^3. $$\\n\\nThis function maps a 3D point to an RGB color, and is parameterized using a multi-layer perceptron (MLP) with weights $\\\\theta$.\\n\\nWe condition our neural texture representation on input images, their projection matrices $P_j = K_j [R_j | t_j]$, and a 3D object shape $S$. Our goal is to learn a texturing function that produces a texture field from an input scene:\\n\\n$$ G_\\\\theta (x; \\\\{I_j\\\\}, \\\\{P_j\\\\}, S). $$\\n\\nInpainting and texture synthesis. The camouflage problem is related to image inpainting [3, 4, 14, 21, 34, 57], in that both tasks involve creating a texture that matches a surrounding region. However, in contrast to the inpainting problem, there is no single solution that can completely satisfy the constraints provided by all of the images, and thus the task cannot be straightforwardly posed as a self-supervised data recovery problem [34]. Our work is also related to image-based texture synthesis [3, 14, 17] and 3D texture synthesis [23, 32, 35]. Since these techniques fill a hole in a single image, and cannot obtain geometrically-consistent constraints from multiple images, they cannot be applied to our method without major modifications. Nevertheless, we include an inpainting-based baseline in our evaluation by combining these methods with previous camouflage approaches.\"}"}
{"id": "CVPR-2023-786", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where \\\\( x \\\\) is a 3D query point on the object surface.\\n\\n### 3.2. Camouflage Texture Model\\n\\nTo learn a camouflaged texture field (Eq. 2), we require a representation for the multi-view scene content, geometry, and texture field. We now describe these components in more detail. Our full model is shown in Figure 2.\\n\\n**Pixel-aligned image representation.** In order to successfully hide an object, we need to reproduce the input image textures with high fidelity. For a given 3D point \\\\( x_i \\\\) on the object surface and an image \\\\( I_j \\\\), we compute an image feature \\\\( z(j)_i \\\\) as follows.\\n\\nWe first compute convolutional features for \\\\( I_j \\\\) using a U-net [40] with a ResNet-18 [22] backbone at multiple resolutions. We extract image features \\\\( F(j) = E(I_j) \\\\) at full, \\\\( \\\\frac{1}{4} \\\\), and \\\\( \\\\frac{1}{16} \\\\) scales. At each pixel, we concatenate features for each scale together, producing a multiscale hypercolumn representation [20].\\n\\nInstead of using a single feature vector to represent an entire input image, as is often done in neural texture models that create a texture from images [23, 32], we exploit the geometric structure of the multi-view camouflage problem. We extract pixel-aligned features \\\\( z(j)_i \\\\) from each feature map \\\\( F(j) \\\\), following work in neural radiance fields [55].\\n\\nWe compute the projection of a 3D point \\\\( x_i \\\\) in viewpoint \\\\( I_j \\\\):\\n\\n\\\\[\\n u(j)_i = \\\\pi(j)(x_i),\\n\\\\]\\n\\nwhere \\\\( \\\\pi \\\\) is the projection function from object space to screen space of image \\\\( I_j \\\\). We then use bilinear interpolation to extract the feature vector \\\\( z(j)_i = F(j)(u(j)_i) \\\\) for each point \\\\( i \\\\) in each input image \\\\( I_j \\\\).\\n\\n**Perspective encoding.** In addition to the image representation, we also condition our texture field on a perspective encoding that conveys the local geometry of the object surface and the multi-view setting. For each point \\\\( x_i \\\\) and image \\\\( I_j \\\\), we provide the network with the viewing direction \\\\( v(j)_i \\\\) and surface normal \\\\( n(j)_i \\\\). These can be computed as:\\n\\n\\\\[\\n v(j)_i = \\\\frac{K - 1}{u(j)_i} \\\\parallel K - 1 u(j)_i \\\\parallel_2 \\\\quad \\\\text{and} \\\\quad n_i = R_j n_i,\\n\\\\]\\n\\nwhere \\\\( u(j)_i \\\\) is the point's projection (Eq. 3) in homogeneous coordinates, and \\\\( n_i \\\\) is the surface normal in object space. To obtain \\\\( n_i \\\\), we extract the normal of the face closest to \\\\( x_i \\\\).\\n\\nWe note that these perspective features come from the images that are used as input images to the texture field, rather than the camera viewing the texture, i.e., in contrast to neural scene representations [9, 31, 55], our textures are not viewpoint-dependent.\\n\\n**Texture field architecture.** We use these features to define a texture field, an MLP that maps a 3D coordinate \\\\( x_i \\\\) to a color \\\\( c_i \\\\) (Eq. 1). It is conditioned on the set of image features for the \\\\( N \\\\) input images \\\\{\\\\( z(j)_i \\\\)\\\\}, as well as the sets of perspective features \\\\{\\\\( v(j)_i \\\\)\\\\} and \\\\{\\\\( n(j)_i \\\\)\\\\):\\n\\n\\\\[\\n c_i = T(\\\\gamma(\\\\cdot); \\\\{z(j)_i\\\\}, \\\\{v(j)_i\\\\}, \\\\{n(j)_i\\\\}) (4)\\n\\\\]\\n\\nwhere \\\\( \\\\gamma(\\\\cdot) \\\\) is a positional encoding [31]. For this MLP, we use a similar architecture as Yu et al. [55]. The network is composed of several fully connected residual blocks and has two stages. In the first stage, which consists of 3 blocks, the vector from each input view is processed separately with shared weights. Mean pooling is then applied to create a unified representation from the views. In the second stage, another 3 residual blocks are used to predict the color for the input query point. Please see the supplementary material for more details.\\n\\n**Rendering.** To render the object from a given viewpoint, following the strategy of Oechsle et al. [32], we determine which surface points are visible using the object's depth map, which we compute using PyTorch3D [37]. Given a pixel \\\\( u_i \\\\), we estimate a 3D surface point \\\\( x_i \\\\) in object space through inverse projection:\\n\\n\\\\[\\n x_i = d_i R_T K_{-1} u_i - R_T t,\\n\\\\]\\n\\nwhere \\\\( d_i \\\\) is the depth of pixel \\\\( i \\\\), \\\\( K, R, t \\\\) are the view's camera parameters, and \\\\( u_i \\\\) is in homogeneous coordinates. We estimate the color for all visible points, and render the object by inserting the estimated pixel colors into a background image, \\\\( I \\\\). This results in a new image that contains the camouflaged object, \\\\( \\\\hat{I} \\\\).\\n\\n### 3.3. Learning to Camouflage\\n\\nWe require our camouflage model to generate textures that are photoconsistent with the input images, and that are not easily detectable by a learned discriminator. These two criteria lead us to define a loss function consisting of a photocoherence term and adversarial loss term, which we optimize through a learning regime that learns to camouflage randomly augmented objects from random positions.\\n\\n**Photocoherence.** The photocoherence loss measures how well the textured object, when projected into the input views, matches the background. We use a perceptual loss, \\\\( L_{photo} \\\\) [17, 25] that is computed as the normalized distance between activations for layers of a VGG-16 network [44] trained on ImageNet [41]:\\n\\n\\\\[\\n L_{photo} = \\\\sum_{j \\\\in J} L_{P}(\\\\hat{I}_j, I_j) = \\\\sum_{j, k \\\\in L} 1_N \\\\parallel \\\\phi_k(\\\\hat{I}_j) - \\\\phi_k(I_j) \\\\parallel_1 (5)\\n\\\\]\\n\\nwhere \\\\( J \\\\) is the set of view indices, \\\\( L \\\\) is the set of layers used in the loss, and \\\\( \\\\phi_k \\\\) are the activations of layer \\\\( k \\\\), which has total dimension \\\\( N_k \\\\). In practice, due to the large image size relative to the object, we use a crop centered around the object, rather than \\\\( I_j \\\\) itself (see Figure 2(c)).\\n\\n**Adversarial loss.** To further improve the quality of generated textures, we also use an adversarial loss. Our model\"}"}
{"id": "CVPR-2023-786", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Aseem Agarwala, Mira Dontcheva, Maneesh Agrawala, Steven Drucker, Alex Colburn, Brian Curless, David Salesin, and Michael Cohen. Interactive digital photomontage. In ACM SIGGRAPH 2004 Papers, pages 294\u2013302. 2004.\\n\\n[2] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. In International conference on machine learning, pages 284\u2013293. PMLR, 2018.\\n\\n[3] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. Patchmatch: A randomized correspondence algorithm for structural image editing. ACM Trans. Graph., 28(3):24, 2009.\\n\\n[4] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, pages 417\u2013424, 2000.\\n\\n[5] Sai Bi, Nima Khademi Kalantari, and Ravi Ramamoorthi. Patch-based optimization for image-based texture mapping. ACM Trans. Graph., 36(4):106\u20131, 2017.\\n\\n[6] Tom B Brown, Dandelion Man \u00b4e, Aurko Roy, Mart\u00b4\u0131n Abadi, and Justin Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.\\n\\n[7] Joshua Callaghan. Public art projects, 2016.\\n\\n[8] Hala Lamdouar Charig Yang, Erika Lu, Andrew Zisserman, and Weidi Xie. Self-supervised video object segmentation by motion grouping. 2021.\\n\\n[9] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. arXiv preprint arXiv:2103.15595, 2021.\\n\\n[10] Hung-Kuo Chu, Wei-Hsin Hsu, Niloy J Mitra, Daniel Cohen-Or, Tien-Tsin Wong, and Tong-Yee Lee. Camouflage images. ACM Trans. Graph., 29(4):51\u20131, 2010.\\n\\n[11] Jasmine Collins, Shubham Goel, Achleshwar Luthra, Leon Xu, Kenan Deng, Xi Zhang, Tomas F Yago Vicente, Himanshu Arora, Thomas Dideriksen, Matthieu Guillaumin, and Jitendra Malik. Abo: Dataset and benchmarks for real-world 3d object understanding. arXiv preprint arXiv:2110.06199, 2021.\\n\\n[12] Hugh Bamford Cott. Adaptive coloration in animals. 1940.\\n\\n[13] Richard Dawkins et al. The blind watchmaker: Why the evidence of evolution reveals a universe without design. WW Norton & Company, 1996.\\n\\n[14] Alexei A Efros and Thomas K Leung. Texture synthesis by non-parametric sampling. In Proceedings of the seventh IEEE international conference on computer vision, volume 2, pages 1033\u20131038. IEEE, 1999.\\n\\n[15] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2777\u20132787, 2020.\\n\\n[16] William T Freeman, Thouis R Jones, and Egon C Pasztor. Example-based super-resolution. IEEE Computer graphics and Applications, 22(2):56\u201365, 2002.\\n\\n[17] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2414\u20132423, 2016.\\n\\n[18] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\\n\\n[19] Roger Hanlon. Cephalopod dynamic camouflage. Current Biology, 17(11):R400\u2013R404, 2007.\\n\\n[20] Bharath Hariharan, Pablo Arbel \u00b4aez, Ross Girshick, and Jitendra Malik. Hypercolumns for object segmentation and fine-grained localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 447\u2013456, 2015.\\n\\n[21] James Hays and Alexei A Efros. Scene completion using millions of photographs. ACM Transactions on Graphics (TOG), 26(3):4\u2013es, 2007.\\n\\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[23] Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Learning a neural 3d texture space from 2d exemplars. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\n[24] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125\u20131134, 2017.\\n\\n[25] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, 2016.\\n\\n[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[27] Hala Lamdouar, Charig Yang, Weidi Xie, and Andrew Zisserman. Betrayed by motion: Camouflaged object discovery via motion segmentation. In Proceedings of the Asian conference on Computer Vision, 2020.\\n\\n[28] Trung-Nghia Le, Yubo Cao, Tan-Cong Nguyen, Minh-Quan Le, Khanh-Duy Nguyen, Thanh-Toan Do, Minh-Triet Tran, and Tam V Nguyen. Camouflaged instance segmentation in-the-wild: Dataset and benchmark suite. arXiv preprint arXiv:2103.17123, 2021.\\n\\n[29] Rob Matheson. Solar panels get a face-lift with custom designs, 2017.\\n\\n[30] Sami Merilaita, Nicholas E Scott-Samuel, and Innes C Cuthill. How camouflage works. Philosophical Transactions of the Royal Society B: Biological Sciences, 372(1724):20160341, 2017.\\n\\n[31] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, pages 405\u2013421. Springer, 2020.\"}"}
{"id": "CVPR-2023-786", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2023-786", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Silvia Zuffi, Angjoo Kanazawa, David Jacobs, and Michael J. Black. 3D menagerie: Modeling the 3D shape and pose of animals. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), July 2017. 4712\"}"}
