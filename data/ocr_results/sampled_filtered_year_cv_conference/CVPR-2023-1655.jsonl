{"id": "CVPR-2023-1655", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Performance profiles for all BAL problems show the percentage of problems solved to a given accuracy tolerance $\\\\tau \\\\in \\\\{0.1, 0.01, 0.003, 0.001\\\\}$ with relative runtime $\\\\alpha$. Our proposed solver PoBA using series expansion of the Schur complement significantly outperforms all the competing solvers up to the high accuracy $\\\\tau = 0.003$.\\n\\nFigure 4. Memory consumption for all BAL problems. The proposed PoBA solver (orange and blue points) is five times less memory-consuming than $\\\\sqrt{BA}$ solvers.\\n\\n5. Implementation\\n\\nWe implement our PoBA solver in C++ in single (PoBA-32) and double (PoBA-64) floating-point precision, directly on the publicly available implementation of [4]. This recent solver presents excellent performance to solve the bundle adjustment by using a QR factorization of the landmark Jacobians. It notably competes the popular Ceres solver. We additionally add a comparison with Ceres' sparse Schur complement solvers, similarly as in [4]. Ceres-explicit and Ceres-implicit iteratively solve (12) with the conjugate gradients algorithm preconditioned by the Schur-Jacobi preconditioner. The first one saves $S$ in memory as a block-sparse matrix, the second one computes $S$ on-the-fly during iterations.\\n\\n$\\\\sqrt{BA}$ and Ceres offer very competitive performance to solve the bundle adjustment problem, that makes them very challenging baselines to compare PoBA to. We run experiments on MacOS 11.2 with an Intel Core i5 and 4 cores at 2GHz.\\n\\n1 https://github.com/NikolausDemmel/rootba\"}"}
{"id": "CVPR-2023-1655", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Illustration of the inequality (3) in Proposition 1 for the first LM iteration of two BAL problems: (a) Ladybug with 49 poses and (b) Trafalgar with 193 poses. The spectral norm of the error matrix $\\\\mathbf{R}$ is plotted in green for $m < 20$. The right-side of the inequality plotted in blue represents the theoretical upper bound of the spectral norm of the error matrix and depends on the considered $m$ and on the spectral norm of $\\\\mathbf{M} = \\\\mathbf{U} - \\\\lambda_1 \\\\mathbf{WV} - \\\\lambda_2 \\\\mathbf{W}^\\\\top$.\\n\\nWith Spectra library \\\\cite{23}, $\\\\rho(\\\\mathbf{M})$ takes the values (a) 0.999858 for L-49 and (b) 0.999879 for T-193. Both values are smaller than 1 and $\\\\rho(\\\\mathbf{R})$ is always smaller than $\\\\rho(\\\\mathbf{M})^{m+1/(1-\\\\rho(\\\\mathbf{M}))}$, as stated in Lemma 1.\\n\\nEfficient storage. We leverage the special structure of BA problem and design a memory-efficient storage. We group the Jacobian matrices and residuals by landmarks and store them in separate dense memory blocks. For a landmark with $k$ observations, all pose Jacobian blocks of size $2 \\\\times d_p$ that correspond to the poses where the landmark was observed, are stacked and stored in a memory block of size $2k \\\\times d_p$. Together with the landmark Jacobian block of size $2k \\\\times 3$ and the residuals of length $2k$ that are also associated to the landmark, all information of a single landmark is efficiently stored in a memory block of size $2k \\\\times (d_p + 4)$. Furthermore, operations involved in (15) and (23) are parallelized using the memory blocks.\\n\\nPerformance Profiles. To compare a set of solvers the user may be interested in two factors, a lower runtime and a better accuracy. Performance profiles \\\\cite{6} evaluate both jointly. Let $S$ and $P$ be respectively a set of solvers and a set of problems. Let $f_0(p)$ be the initial objective and $f(p, s)$ the final objective that is reached by solver $s \\\\in S$ when solving problem $p \\\\in P$. The minimum objective the solvers in $S$ attain for a problem $p$ is $f^*(p) = \\\\min_{s \\\\in S} f(p, s)$. Given a tolerance $\\\\tau \\\\in (0, 1)$ the objective threshold for a problem $p$ is given by $f^\\\\tau(p) = f^*(p) + \\\\tau(f_0(p) - f^*(p))$ (35) and the runtime a solver $s$ needs to reach this threshold is noted $T^\\\\tau(p, s)$. It is clear that the most efficient solver $s^*$ for a given problem $p$ reaches the threshold with a runtime $T^\\\\tau(p, s^*) = \\\\min_{s \\\\in S} T^\\\\tau(p, s)$. Then, the performance profile of a solver for a relative runtime $\\\\alpha$ is defined as $\\\\rho(s, \\\\alpha) = 100 |\\\\{p \\\\in P | T^\\\\tau(p, s) \\\\leq \\\\alpha \\\\min_{s \\\\in S} T^\\\\tau(p, s)\\\\}| / |P|$ (36) Graphically the performance profile of a given solver is the percentage of problems solved faster than the relative runtime $\\\\alpha$ on the x-axis.\\n\\n5.1. Experimental Settings\\n\\nDataset. For our extensive evaluation we use all 97 bundle adjustment problems from the BAL project page. They are divided within five problems families. Ladybug is composed with images captured by a vehicle with regular rate. Images of Venice, Trafalgar and Dubrovnik come from Flickr.com and have been saved as skeletal sets \\\\cite{1}. Recombination of these problems with additional leaf images leads to the Final family. Details about these problems can be found in Appendix.\\n\\nLM loop. PoBA is in line with the implementation \\\\cite{4} and with Ceres. Starting with damping parameter $10^{-4}$ we update $\\\\lambda$ depending on the success or failure of the LM loop. We set the maximal number of LM iterations to 50, terminating earlier if a relative function tolerance of $10^{-6}$ is reached. Concerning (23) and (32) we set the maximal number of inner iterations to 20 and a threshold $\\\\epsilon = 0.01$. Ceres and $\\\\sqrt{BA}$ use same forcing sequence for the inner CG loop, where the maximal number of iterations is set to 500. We add a small Gaussian noise to disturb initial landmark and camera positions.\"}"}
{"id": "CVPR-2023-1655", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Convergence plots of Ladybug-1197 (left) from BAL dataset with 1197 poses and Venice-1102 (right) from BAL dataset with 1102 poses. Fig. 1 shows a visualization of 3D landmarks and camera poses for these problems. The dotted lines correspond to cost thresholds for the tolerances $\\\\tau \\\\in \\\\{0.1, 0.01, 0.003, 0.001\\\\}$.\\n\\nFigure 7. Performance profiles for all BAL problems with stochastic framework. Our proposed solver PoST outperforms the challenging STBA across all accuracy tolerances $\\\\tau \\\\in \\\\{0.1, 0.01, 0.003\\\\}$, both in terms of speed and precision, and rivals STBA for $\\\\tau = 0.001$. \\n\\n287 287\"}"}
{"id": "CVPR-2023-1655", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8. Convergence plots of Ladybug-138 (left) from BAL dataset with 138 poses and Dubrovnik-356 (right) from BAL dataset with 356 poses. The dotted lines correspond to cost thresholds for the tolerances $\\\\tau \\\\in \\\\{0.1, 0.01, 0.003, 0.001\\\\}$. \\n\\n5.2. Analysis\\n\\nFigure 3 shows the performance profiles for all BAL datasets with tolerances $\\\\tau \\\\in \\\\{0.1, 0.01, 0.003, 0.001\\\\}$. For $\\\\tau = 0.1$ and $\\\\tau = 0.01$, PoBA-64 clearly outperforms all challengers both in terms of runtime and accuracy. PoBA-64 remains clearly the best solver for the excellent accuracy $\\\\tau = 0.003$ until a high relative time $\\\\alpha = 4$. For higher relative time it is competitive with $\\\\sqrt{BA-32}$ and still outperforms all other challengers. Same conclusion can be drawn from the convergence plot of two differently sized BAL problems (see Figure 6). Figure 4 highlights the low memory consumption of PoBA with respect to its challengers for all BAL problems. Whatever the size of the problem PoBA is much less memory-consuming than $\\\\sqrt{BA}$ and Ceres. Notably it requires almost five times less memory than $\\\\sqrt{BA}$ and almost twice less memory than Ceres-implicit and Ceres-explicit.\\n\\n5.3. Power Stochastic Bundle Adjustment (PoST)\\n\\nStochastic Bundle Adjustment. STBA decomposes the reduced camera system into clusters inside the Levenberg-Marquardt iterations. The per-cluster linear sub-problems are then solved in parallel with dense $LL^T$ factorization due to the dense connectivity inside camera clusters. As shown in [22] this approach outperforms the baselines in terms of runtime and scales to very large BA problems, where it can even be used for distributed optimization. In the following we show that replacing the subproblem solver with our Power Bundle Adjustment can significantly boost runtime even further.\\n\\nWe extend STBA by incorporating our solver instead of the dense $LL^T$ factorization. Each subproblem is then solved with a power series expansion of the inverse Schur complement with the same parameters as in Section 5.1. In accordance to [22] we set the maximal cluster size to 100 and the implementation is written in double in C++.\\n\\nAnalysis.\\n\\nFigure 7 presents the performance profiles with all BAL problems for different tolerances $\\\\tau$. Both solvers have similar accuracy for $\\\\tau = 0.001$. For $\\\\tau \\\\in \\\\{0.1, 0.01, 0.003\\\\}$, PoST clearly outperforms STBA both in terms of runtime and accuracy, most notably for $\\\\tau = 0.01$. Same observations are done when we plot the convergence for differently sized BAL problems (see Figure 8).\\n\\n6. Conclusion\\n\\nWe introduce a new class of large-scale bundle adjustment solvers that makes use of a power expansion of the inverse Schur complement. We prove the theoretical validity of the proposed approximation and the convergence of this solver. Moreover, we experimentally confirm that the proposed power series representation of the inverse Schur complement outperforms competitive iterative solvers in terms of speed, accuracy, and memory-consumption. Last but not least, we show that the power series representation can complement distributed bundle adjustment methods to significantly boost its performance for large-scale 3D reconstruction.\\n\\nAcknowledgement\\n\\nThis work was supported by the ERC Advanced Grant SIMULACRON, the Munich Center for Machine Learning, the EPSRC Programme Grant VisualAI EP/T028572/1, and the DFG projects WU 959/1-1 and CR 250 20-1 \u201cSplitting Methods for 3D Reconstruction and SLAM\u201d.\\n\\n288\"}"}
{"id": "CVPR-2023-1655", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce Power Bundle Adjustment as an expansion type algorithm for solving large-scale bundle adjustment problems. It is based on the power series expansion of the inverse Schur complement and constitutes a new family of solvers that we call inverse expansion methods. We theoretically justify the use of power series and we prove the convergence of our approach. Using the real-world BAL dataset we show that the proposed solver challenges the state-of-the-art iterative methods and significantly accelerates the solution of the normal equation, even for reaching a very high accuracy. This easy-to-implement solver can also complement a recently presented distributed bundle adjustment framework. We demonstrate that employing the proposed Power Bundle Adjustment as a sub-problem solver significantly improves speed and accuracy of the distributed optimization.\\n\\n1. Introduction\\n\\nBundle adjustment (BA) is a classical computer vision problem that forms the core component of many 3D reconstruction and Structure from Motion (SfM) algorithms. It refers to the joint estimation of camera parameters and 3D landmark positions by minimization of a non-linear reprojection error. The recent emergence of large-scale internet photo collections [1] raises the need for BA methods that are scalable with respect to both runtime and memory. And building accurate city-scale maps for applications such as augmented reality or autonomous driving brings current BA approaches to their limits.\\n\\nAs the solution of the normal equation is the most time consuming step of BA, the Schur complement trick is usually employed to form the reduced camera system (RCS). This linear system involves only the pose parameters and is significantly smaller. Its size can be reduced even more by using a QR factorization, deriving only a matrix square root of the RCS, and then solving an algebraically equivalent problem [4]. Both the RCS and its square root formulation are commonly solved by iterative methods such as the popular preconditioned conjugate gradients algorithm for large-scale problems or by direct methods such as Cholesky factorization for small-scale problems.\\n\\nIn the following, we will challenge these two families of solvers by relying on an iterative approximation of the inverse Schur complement. In particular, our contributions...\"}"}
{"id": "CVPR-2023-1655", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce Power Bundle Adjustment (PoBA) for efficient large-scale BA. This new family of techniques that we call inverse expansion methods challenges the state-of-the-art methods which are built on iterative and direct solvers.\\n\\nWe link the bundle adjustment problem to the theory of power series and we provide theoretical proofs that justify this expansion and establish the convergence of our solver.\\n\\nWe perform extensive evaluation of the proposed approach on the BAL dataset and compare to several state-of-the-art solvers. We highlight the benefits of PoBA in terms of speed, accuracy, and memory-consumption. Figure 1 shows reconstructions for two out of the 97 evaluated BAL problems.\\n\\nWe incorporate our solver into a recently proposed distributed BA framework and show a significant improvement in terms of speed and accuracy.\\n\\nWe release our solver as open source to facilitate further research: https://github.com/simonwebertum/poba\\n\\nRelated Work\\n\\nSince we propose a new way to solve large-scale bundle adjustment problems, we will review works on bundle adjustment and on traditional solving methods, that is, direct and iterative methods. We also provide some background on power series. For a general introduction to series expansion we refer the reader to [14].\\n\\nScalable bundle adjustment.\\n\\nA detailed survey of bundle adjustment can be found in [16]. The Schur complement [20] is the prevalent way to exploit the sparsity of the BA Problem. The choice of resolution method is typically governed by the size of the normal equation: With increasing size, direct methods such as sparse and dense Cholesky factorization [15] are outperformed by iterative methods such as inexact Newton algorithms. Large-scale bundle adjustment problems with tens of thousands of images are typically solved by the conjugate gradient method [1, 2, 8]. Some variants have been designed, for instance the search-space can be enlarged [17] or a visibility-based preconditioner can be used [9]. A recent line of works on square root bundle adjustment proposes to replace the Schur complement for eliminating landmarks with nullspace projection [4, 5]. It leads to significant performance improvements and to one of the most performant solver for the bundle adjustment problem in term of speed and accuracy. Nevertheless these methods still rely on traditional solvers for the reduced camera system, i.e. preconditioned conjugate gradient method (PCG) for large-scale [4] and Cholesky decomposition for small-scale [5] problems, besides an important cost in term of memory-consumption. Even with PCG, solving the normal equation remains the bottleneck and finding thousands of unknown parameters requires a large number of inner iterations. Other authors try to improve the runtime of BA with PCG by focusing on efficient parallelization [13]. Recently, Stochastic BA [22] was introduced to stochastically decompose the reduced camera system into subproblems and solve the smaller normal equation by dense factorization. This leads to a distributed optimization framework with improved speed and scalability. By encapsulating the general power series theory into a linear solver we propose to simultaneously improve the speed, the accuracy and the memory-consumption of these existing methods.\\n\\nPower series solver.\\n\\nWhile power series expansion is common to solve differential equations [3], to the best of our knowledge it has never been employed for solving the bundle adjustment problem. A recent work [21] links the Schur complement to Neumann polynomial expansion to build a new preconditioner. Although this method presents interesting results for some physics problems such as convection-diffusion or atmospheric equations, it remains unsatisfactory for the bundle adjustment problem (see Figure 2). In contrast, we propose to directly apply the power series expansion of the inverse Schur complement for solving the BA problem. Our solver therefore falls in the category of expansion methods that \u2013 to our knowledge \u2013 have never been applied to the BA problem. In addition to being an easy-to-implement solver it leverages the special structure of the BA problem to simultaneously improve the trade-off speed-accuracy and the memory-consumption of the existing methods.\\n\\nPower Series\\n\\nWe briefly introduce power series expansion of a matrix. Let \\\\( \\\\rho(A) \\\\) denote the spectral radius of a square matrix \\\\( A \\\\), i.e. the largest absolute eigenvalue and denote the spectral norm by \\\\( \\\\|A\\\\| = \\\\rho(A) \\\\). The following proposition holds:\\n\\n**Proposition 1.** Let \\\\( M \\\\) be a \\\\( n \\\\times n \\\\) matrix. If the spectral radius of \\\\( M \\\\) satisfies \\\\( \\\\|M\\\\| < 1 \\\\), then\\n\\n\\\\[\\n(I - M)^{-1} = \\\\sum_{i=0}^{m} M^i + R,\\n\\\\]\\n\\nwhere the error matrix \\\\( R = \\\\sum_{i=m+1}^{\\\\infty} M^i \\\\).\"}"}
{"id": "CVPR-2023-1655", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Although [21] explores the use of power series as a preconditioner for some physics problems it suffers from the special structure of the BA formulation. Given a preconditioner $M^{-1}$ and the Schur complement $S$, the condition number $\\\\kappa(M^{-1}S)$ is linked to the convergence of the conjugate gradients algorithm. (a) illustrates the behaviour of $\\\\kappa$ for the ten first iterations of the LM algorithm for the real problem Ladybug-49 with 49 poses from BAL dataset and for different orders $m$ of the power series expansion (22) used as preconditioner for the CG algorithm. The condition number associated to the popular Schur-Jacobi preconditioner is reduced with this power series preconditioner, that is illustrated by a better convergence of the CG algorithm and then a smaller number of CG iterations (b). Nevertheless each supplementary order $m$ is more costly in terms of runtime as the application of the power series preconditioner involves $4^m$ matrix-vector product, whereas the Schur-Jacobi preconditioner can be efficiently stored and applied. (c) It leads to an increase of the overall runtime when solving the normal equation (6).\\n\\nA proof is provided in Appendix and an illustration with real problems is given in Figure 5.\\n\\n4. Power Bundle Adjustment\\n\\nWe consider a general form of bundle adjustment with $n_p$ poses and $n_l$ landmarks. Let $x = (x_p, x_l)$ be the state vector containing all the optimization variables, where the vector $x_p$ of length $d_p n_p$ is associated to the extrinsic and (possibly) intrinsic camera parameters for all poses and the vector $x_l$ of length $3 n_l$ is associated to the 3D coordinates of all landmarks. In case only the extrinsic parameters are unknown then $d_p = 6$ for rotation and translation of each camera. For the evaluated BAL problems we additionally estimate intrinsic parameters and $d_p = 9$. The objective is to minimize the total bundle adjustment energy $F(x) = \\\\frac{1}{2} \\\\| r(x) \\\\|^2_2 = \\\\frac{1}{2} \\\\sum_i \\\\| r_i(x) \\\\|^2_2$, where the vector $r(x) = [r_1(x)^\\\\top, \\\\ldots, r_k(x)^\\\\top]^\\\\top$ comprises all residuals capturing the discrepancy between model and observation.\\n\\n4.1. Least Squares Problem\\n\\nThis nonlinear least squares problem is commonly solved with the Levenberg-Marquardt (LM) algorithm, which is based on the first-order Taylor approximation of $r(x)$ around the current state estimate $x_0 = (x_0_p, x_0_l)$. By adding a regularization term to improve convergence the minimization turns into\\n\\n$$\\\\min \\\\Delta x_p, \\\\Delta x_l \\\\quad 1/2 r_0^2 + J_p J_l \\\\Delta x_p \\\\Delta x_l + \\\\lambda D_p D_l \\\\Delta x_p \\\\Delta x_l^2,$$\\n\\nwith $r_0 = r(x_0)$, $J_p = \\\\partial r_p / \\\\partial x_p | x_0$, $J_l = \\\\partial r_l / \\\\partial x_l | x_0$, $\\\\lambda$ a damping coefficient, and $D_p$ and $D_l$ diagonal damping matrices for pose and landmark variables. This damped problem leads to the corresponding normal equation $H \\\\Delta x_p \\\\Delta x_l = -b_p b_l$, where\\n\\n$$H = U_{\\\\lambda} W V_{\\\\lambda},$$\\n\\n$$U_{\\\\lambda} = J_p^\\\\top J_p + \\\\lambda D_p^\\\\top D_p,$$\\n\\n$$V_{\\\\lambda} = J_l^\\\\top J_l + \\\\lambda D_l^\\\\top D_l,$$\\n\\n$$W = J_p^\\\\top J_l.$$\\n\\n$U_{\\\\lambda}$, $V_{\\\\lambda}$ and $H$ are symmetric positive-definite [16].\\n\\n4.2. Schur Complement\\n\\nAs inverting the system matrix $H$ of size $(d_p n_p + 3 n_l)^2$ directly tends to be excessively costly for large-scale problems it is common to reduce it by using the Schur complement trick. The idea is to form the reduced camera system.\"}"}
{"id": "CVPR-2023-1655", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ S \\\\Delta x_p = -\\\\tilde{b}, \\\\]  \\n(12)\\n\\nwith\\n\\\\[ S = U_\\\\lambda - W V^{-1} \\\\lambda W^\\\\top, \\\\]  \\n(13)\\n\\n\\\\[ \\\\tilde{b} = b_p - W V^{-1} \\\\lambda b_l. \\\\]  \\n(14)\\n\\n(12) is then solved for \\\\( \\\\Delta x_p \\\\). The optimal \\\\( \\\\Delta x_l \\\\) is obtained by back-substitution:\\n\\\\[ \\\\Delta x_l = -V^{-1} \\\\lambda (-b_l + W^\\\\top \\\\Delta x_p). \\\\]  \\n(15)\\n\\n4.3. Power Bundle Adjustment\\n\\nFactorizing (13) with the block-matrix\\n\\\\[ U_\\\\lambda S = U_\\\\lambda (I - U^{-1} \\\\lambda W V^{-1} \\\\lambda W^\\\\top). \\\\]  \\n(16)\\n\\nleads to formulate the inverse Schur complement as\\n\\\\[ S^{-1} = (I - U^{-1} \\\\lambda W V^{-1} \\\\lambda W^\\\\top)^{-1} U^{-1} \\\\lambda. \\\\]  \\n(17)\\n\\nIn order to expand (17) into a power series as detailed in Proposition 1, we require to bound the spectral radius of \\\\( U^{-1} \\\\lambda W V^{-1} \\\\lambda W^\\\\top \\\\) by 1.\\n\\nBy leveraging the special structure of the BA problem we prove an even stronger result:\\n\\nLemma 1.\\nLet \\\\( \\\\mu \\\\) be an eigenvalue of \\\\( U^{-1} \\\\lambda W V^{-1} \\\\lambda W^\\\\top \\\\).\\n\\n\\\\[ \\\\mu \\\\in [0, 1]. \\\\]  \\n(18)\\n\\nProof.\\nOn the one hand \\\\( U^{-1/2} \\\\lambda W V^{-1/2} \\\\lambda W^\\\\top U^{-1/2} \\\\lambda \\\\) is symmetric positive semi-definite, as \\\\( U_\\\\lambda \\\\) and \\\\( V_\\\\lambda \\\\) are symmetric positive definite. Then its eigenvalues are greater than 0. As \\\\( U^{-1/2} \\\\lambda W V^{-1/2} \\\\lambda W^\\\\top U^{-1/2} \\\\lambda \\\\) and \\\\( U^{-1} \\\\lambda W V^{-1} \\\\lambda W^\\\\top \\\\) are similar, \\\\( \\\\mu \\\\geq 0 \\\\).\\n\\nOn the other hand \\\\( U^{-1/2} \\\\lambda S U^{-1/2} \\\\lambda \\\\) is symmetric positive definite as \\\\( S \\\\) and \\\\( U_\\\\lambda \\\\) are. It follows that the eigenvalues of \\\\( U^{-1} \\\\lambda S \\\\) are all strictly positive due to its similarity with \\\\( U^{-1/2} \\\\lambda S U^{-1/2} \\\\lambda \\\\). As \\\\( U^{-1} \\\\lambda W V^{-1} \\\\lambda W^\\\\top = I - U^{-1} \\\\lambda W V^{-1} \\\\lambda W^\\\\top \\\\), it follows that \\\\( \\\\mu < 1 \\\\), that concludes the proof.\\n\\nLet be\\n\\\\[ \\\\tilde{S}^{-1}(m) = \\\\sum_{i=0}^{m} (U^{-1} \\\\lambda W V^{-1} \\\\lambda W^\\\\top)^i U^{-1} \\\\lambda, \\\\]  \\n(22)\\n\\nand\\n\\\\[ x(m) = -\\\\tilde{S}^{-1}(m) \\\\tilde{b}, \\\\]  \\n(23)\\n\\nfor \\\\( m \\\\geq 0 \\\\). The following proposition confirms that the approximation indeed converges with increasing order of \\\\( m \\\\):\\n\\nProposition 2.\\n\\\\[ \\\\|x(m) - \\\\Delta x_p\\\\|^2 \\\\xrightarrow{m \\\\to +\\\\infty} 0. \\\\]  \\n(31)\\n\\nProof.\\nWe denote \\\\( P = U^{-1} \\\\lambda W V^{-1} \\\\lambda W^\\\\top \\\\). Due to Lemma 1\\n\\\\[ \\\\|P\\\\| < 1. \\\\]  \\n(24)\\n\\nThe inverse Schur complement associated to (6) admits a power series expansion:\\n\\\\[ S^{-1} = \\\\tilde{S}^{-1}(m) + R_m, \\\\]  \\n(25)\\n\\nwhere\\n\\\\[ R_m = \\\\sum_{i=m+1}^{\\\\infty} P i U^{-1} \\\\lambda. \\\\]  \\n(26)\\n\\nsatisfies\\n\\\\[ \\\\|R_m\\\\| \\\\leq \\\\|P\\\\|^m \\\\|U^{-1} \\\\lambda\\\\| \\\\|\\\\tilde{b}\\\\|^2. \\\\]  \\n(27)\\n\\nIt follows that:\\n\\\\[ x(m) - \\\\Delta x_p = R_m \\\\tilde{b}. \\\\]  \\n(28)\\n\\nThe consistency of the spectral norm with respect to the vector norm implies:\\n\\\\[ \\\\|R_m \\\\tilde{b}\\\\|^2 \\\\leq \\\\|R_m\\\\| \\\\|\\\\tilde{b}\\\\|^2. \\\\]  \\n(29)\\n\\nFrom (24), (27) and (29) we conclude the proof:\\n\\\\[ \\\\|R_m \\\\tilde{b}\\\\|^2 \\\\xrightarrow{m \\\\to +\\\\infty} 0, \\\\]  \\n(30)\\n\\nand then\\n\\\\[ \\\\|x(m) - \\\\Delta x_p\\\\|^2 \\\\xrightarrow{m \\\\to +\\\\infty} 0. \\\\]  \\n(31)\\n\\nThis convergence result proves that\\n- an approximation of \\\\( \\\\Delta x_p \\\\) can be directly obtained by applying (22) to the right-hand side of (12);\\n- the quality of this approximation depends on the order \\\\( m \\\\) and can be as small as desired.\\n\\nThe power series expansion being iteratively derived, a termination rule is necessary.\\n\\nBy analogy with inexact Newton methods [11, 12, 18] such that the conjugate gradients algorithm we set a stop criterion\\n\\\\[ \\\\left(\\\\|x(i) - x(i-1)\\\\|^2 / \\\\|x(i)\\\\|^2\\\\right) < \\\\epsilon, \\\\]  \\n(32)\\n\\nfor a given \\\\( \\\\epsilon \\\\). This criterion ensures that the power series expansion stops when the refinement of the pose update by expanding the inverse Schur complement into a supplementary order\\n\\\\[ \\\\|x(i) - x(i-1)\\\\|^2 \\\\]  \\n(33)\\n\\nis much smaller than the average refinement when reaching the same order\\n\\\\[ \\\\|x(j) - x(j-1)\\\\|^2 / \\\\sum_{i=1}^{i+1} \\\\|x(j) - x(j-1)\\\\|^2 / \\\\|x(j)\\\\|^2 \\\\]  \\n(34)\\n\\nfor \\\\( i+1 \\\\) and \\\\( j \\\\).\"}"}
{"id": "CVPR-2023-1655", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] S. Agarwal, N. Snavely, S. M. Seitz, and R. Szeliski. Bundle adjustment in the large. In European Conference on Computer Vision (ECCV), pages 29-42. Springer, 2010.\\n\\n[2] M. Byr \u00f6d, K. \u02daAstr\u00a8om. Conjugate gradient bundle adjustment. In European Conference on Computer Vision (ECCV), 2010.\\n\\n[3] E. A. Coddington, N. Levinson. Theory of Ordinary Differential Equations. McGraw\u2013Hill, 1955.\\n\\n[4] N. Demmel, C. Sommer, D. Cremers, V . Usenko. Square Root Bundle Adjustment for Large-Scale Reconstruction. In Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\n[5] N. Demmel, D. Schubert, C. Sommer, D. Cremers, V . Usenko. Square Root Marginalization for Sliding-Window Bundle Adjustment. In International Conference on Computer Vision (ICCV), 2021.\\n\\n[6] E. D. Dolan, and J. J. More. Benchmarking optimization software with performance profiles. In Mathematical Programming 91(2), pages 201\u2013213, 2002.\\n\\n[7] G. Guennebaud, and B. Jacob, et al. Eigen v3, http://eigen.tuxfamily.org, 2010.\\n\\n[8] M. R. Hestenes, and E. Stiefel. Methods of conjugate gradients for solving linear systems. In Journal of research of the National Bureau of Standards 49(6), pages 409-436, 1952.\\n\\n[9] A. Kushal, and S. Agarwal. Visibility based preconditioning for bundle adjustment. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\\n\\n[10] M. Lourakis, A. A. Argyros. Is levenberg-marquardt the most efficient optimization algorithm for implementing bundle adjustment? In International Conference on Computer Vision (ICCV), 2005.\\n\\n[11] S. G. Nash, A Survey of Truncated Newton Methods, Journal of Computational and Applied Mathematics, 124(1-2), 45-59, 2000.\\n\\n[12] S. G. Nash, A. Sofer, Assessing A Search Direction Within A Truncated Newton Method, Operation Research Letters 9(1990) 219-221.\\n\\n[13] J. Ren, W. Liang, R. Yan, L. Mai, X. Liu . MegBA: A High-Performance and Distributed Library for Large-Scale Bundle Adjustment. In European Conference on Computer Vision (ECCV), 2022.\\n\\n[14] Y . Saad. Itervative methods for sparse linear systems, 2nd ed. In SIAM, Philadelpha, PA, 2003.\\n\\n[15] L. Trefethen, D. Bau. Numerical linear algebra. SIAM, 1997.\\n\\n[16] B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon. Bundle adjustment\u2014a modern synthesis. In International workshop on vision algorithms, pages 298-372. Springer, 1999.\\n\\n[17] S. Weber, N. Demmel, and D. Cremers. Multidirectional conjugate gradients for scalable bundle adjustment. In German Conference on Pattern Recognition (GCPR), pages 712-724. Springer, 2021.\\n\\n[18] S. J. Wright, and J. N. Holt. An inexact Levenberg-Marquardt method for large sparse nonlinear least squares. In J. Austral. Math. Soc. Ser. B 26, pages 387-403, 1985.\\n\\n[19] C. Zach. Robust bundle adjustment revisited. In European Conference on Computer Vision (ECCV), 2014.\\n\\n[20] F. Zhang. The Schur complement and its applications. In Numerical Methods and Algorithms. Vol. 4, Springer, 2005.\\n\\n[21] Q. Zheng, Y . Xi, and Y . Saad. A power Schur complement low-rank correction preconditioner for general sparse linear systems. In SIAM Journal on Matrix Analysis and Applications, 2021.\\n\\n[22] L. Zhou, Z. Luo, M. Zhen, T. Shen, S. Li, Z. Huang, T. Fang, and L. Quan. Stochastic bundle adjustment for efficient and scalable 3d reconstruction. In European Conference on Computer Vision (ECCV), 2020.\\n\\n[23] https://spectralib.org\"}"}
