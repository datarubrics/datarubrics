{"id": "CVPR-2022-2054", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"needs to learn and then use it to train networks, networks become more robust to wild-content perturbations. Since there is no class information in the wild set $D_w$, Eq. (4) cannot be directly applied to this wild content extension. We address this issue from the perspective that similar semantic contents will be located close to each other in the embedding space. Inspired by [12], we take the wild content $z_w$ closest to the wild-stylized source content $z_{sw}$ from $Q$ as $z_w = \\\\arg\\\\min_{q \\\\in Q} \\\\|z_{sw} - q\\\\|_2$ (7) and encourage the source content $z_{si}$ to come close to it. Since $z_{sw}$ and $q$ are normalized early on, Eq. (7) can be calculated efficiently using a dot product and rewritten as $z_w = \\\\arg\\\\max_{q \\\\in Q} (z_{sw} \\\\cdot q)$.\\n\\nFig. 3 shows the wild content $z_w$ matched to the source content $z_{si}$ using the stylized source content $z_{sw}$. In this way, we provide various contents of the wild to the networks without category information. Now Eq. (4) can be adapted to the wild content extension as follows:\\n\\n$$L_{iWCE} = -\\\\log \\\\psi(z_{si}, z_w) + \\\\sum_{j=1}^{Nz} \\\\sum_{i=1}^{Ns} \\\\psi(z_{si}, z_{sw})$$\\n\\nIn the wild content extension, we reuse the negative samples of the source content extension. Some negative extension approaches may give better performance and we left this for future work. Then we can apply the pixel-wise wild content extension loss to the entire source image by\\n\\n$$L_{WCE} = \\\\frac{1}{Nz\\\\cdot Ns} \\\\sum_{i=1}^{Nz} \\\\sum_{j=1}^{Ns} L_{iWCE}$$\\n\\nBy combining the source content extension loss and wild content extension loss, the CEL loss is defined as\\n\\n$$L_{CEL} = L_{SCE} + L_{WCE}$$\\n\\nOur model learns to capture generalized semantic information from diverse contents by using the proposed CEL loss.\\n\\n3.4. Style Extension Learning\\n\\nAnother reason for overfitting to the source domain is that networks overlearn a limited amount of the source style [36, 43, 67]. To address this issue, FS has diversified the styles of the source feature with the help of the wild. Interestingly, the style of the source features has changed while preserving spatial information, but networks fail to predict semantic categories from the wild-stylized feature as shown in Fig. 4f. In this subsection, we propose SEL for adapting networks to diversified styles. SEL aims to allow networks to naturally adapt to various styles by learning task-specific information from the wild-stylized feature.\\n\\nWhen the wild-stylized source feature $z_{sw}$ enters the classification head $\\\\phi_{cls}$, it outputs the pixel-wise softmax segmentation map $p_{sw} \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times K}$. Because $z_{sw}$ is the wild-stylized source feature, networks must predict the semantic label $y_s$ of $x_s$ from $z_{sw}$. For this objective, we train networks by minimizing the following SEL loss:\\n\\n$$L_{SEL} = \\\\frac{1}{HW} \\\\sum_{h=1}^{H} \\\\sum_{w=1}^{W} \\\\sum_{k=1}^{K} y_{hwk} \\\\log(p_{sw})$$\\n\\nOur model learns task-specific information from the wild-stylized features by applying the SEL loss. This enables our model to naturally learn domain-generalized semantic information from various styles.\\n\\n3.5. Semantic Consistency Regularization\\n\\nFor high generalization capability on unseen domains, the classifier should capture consistent semantic information from features [5, 25], even if there are perturbations in both the style and content. However, as shown in Figs. 4e and 4f, the predicted result $p_{sw}$ of the wild-stylized source feature $z_{sw}$ differs from the predicted result $p_s$ of the source feature $z_s$. Although SEL allows networks to learn task-specific information from $z_{sw}$, this does not guarantee that $p_{sw}$ and $p_s$ are identical. To address this issue, we propose SCR that regularizes networks to capture consistent semantic information even when both the content and style of the source domain are extended to the wild. SCR aims to train networks so that the predicted probability distributions $p_{sw} = \\\\phi_{cls}(z_{sw})$ from the wild-stylized source features get closer to the $p_s = \\\\phi_{cls}(z_s)$ from the source features. To this end, we adapt the Kullback-Leibler (KL) divergence loss as\\n\\n$$L_{SCR} = \\\\frac{1}{HW} \\\\sum_{h=1}^{H} \\\\sum_{w=1}^{W} \\\\sum_{k=1}^{K} p_{sw} \\\\log(p_{sw})$$\\n\\nWith the SCR loss, our model learns consistent semantic information even with perturbations of style and content by the proposed wild extension methods.\"}"}
{"id": "CVPR-2022-2054", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Experiments\\n\\n4.1. Datasets\\n\\nReal semantic segmentation datasets. Cityscapes [10], BDD100K [61] and Mapillary [37] consist of 2975, 7000, and 18000 images for train set and 500, 1000, and 2000 for validation set. We consider 19 classes that are compatible with other datasets. In all of the tables, $C$, $B$, and $M$ denote Cityscapes, BDD100K, and Mapillary, respectively.\\n\\nSynthetic semantic segmentation datasets. GTA V [46] contains 24966 images rendered from the Grand Theft Auto V game engine. It has 12403, 6382, and 6181 images for train, validation, and test sets, respectively. SYNTHIA [49] contains 9400 images and we split it into 6580 and 2820 images for train and validation sets, following [7]. In all tables, $G$ and $S$ denote GTA V and SYNTHIA, respectively.\\n\\nWild dataset. ImageNet [11] is a large-scale image classification dataset used for network pre-training in most studies. In this paper, we use images without class labels as wild domain data. The generalization performance according to the number of images used for training our WildNet is presented in Table 6a.\\n\\n4.2. Experimental Setup\\n\\nWe conduct extensive experiments over five different semantic segmentation datasets and report the mean intersection over union (mIoU) score on several domain generalization scenarios: GTA V $\\\\rightarrow\\\\{\\\\text{Cityscapes, BDD100K, Mapillary, SYNTHIA, GTA V}\\\\}$ and Cityscapes $\\\\rightarrow\\\\{\\\\text{GTA V, BDD100K, Mapillary, SYNTHIA, Cityscapes}\\\\}$. For fair comparisons with other DG methods, we re-implement IBN-Net [41] and RobustNet [7] on our baseline models and denote our re-implemented models. Our model is trained on one source domain train set (GTA V or Cityscapes) and validated on five domain validation sets (four unseen domains and one seen domain). To show the overall domain generalization performance, we additionally report the average value of mIoU on five domain validation sets ($\\\\text{Avg}$). In all of the tables, the best results for each domain are marked in bold.\\n\\n4.3. Implementation Details\\n\\nWe adapt ResNet-50, ResNet-101 [19], and VGG-16 [52] with DeepLabV3+ [2] as segmentation networks, and all backbones are pre-trained on ImageNet [11]. In the ResNet-based models, we use the SGD optimizer [47] with a momentum of 0.9 and weight decay of 5e-4. The initial learning rate is set to 2.5e-3 and is decreased using the polynomial policy with a power of 0.9. We train the models for 60K iterations with a batch size of 8. In the VGG-based models, we use the Adam optimizer [28] with a momentum of (0.9, 0.99). The initial learning rate is set to 1e-5 and the batch size is set to 8. Following [7], we apply random scaling within a range of $[0.5, 2.0]$ and random cropping with a size of $768 \\\\times 768$. The output size of the projection head is $192 \\\\times 192$ and we use uniformly sampled $64 \\\\times 64$ size feature maps for CEL to prevent memory issues. For the diversity of the wild content dictionary, the wild feature maps are stored after uniform sampling with a size of $16 \\\\times 16$. The FS layer replaces first batch normalization and is added immediately after the addition operation of the first two residual blocks in ResNet, and it added right after the first ReLU after the first three maxpool layers in VGG. After training, all FS layers, projection head, and wild-content dictionary are removed, and our model can be applied to multiple unseen domains without further training on the target domains.\\n\\n4.4. Comparison with DG methods\\n\\nWe compare our results with existing DG methods: IBN-Net [41], DRPC [62], ASG [4], FSDR [22], RobustNet [7], and GLTR [45]. Table 1 shows the generalization performance of the ResNet-50 model trained on GTA V. We evaluate models on five validation sets consisting of four unseen domains, including the Cityscapes, BDD100K, Mapillary, SYNTHIA, GTA V, and Cityscapes. Table 2 shows the generalization performance of the ResNet-101 model trained on GTA V.\"}"}
{"id": "CVPR-2022-2054", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Methods\\n\\n| Baseline | CRP | ASG | FSDR | GLTR |\\n|----------|-----|-----|------|------|\\n| Avg.     |     |     |      |      |\\n| Baseline | 30.04 | 24.59 | 26.63 | -   |\\n| DRPC     | 36.11 | 31.56 | 32.25 | -   |\\n| IBN-Net  | 30.25 | 30.09 | 31.87 | 26.22 |\\n| RobustNet| 30.13 | 29.22 | 33.96 | 26.16 |\\n| WildNet  | 39.18 | 34.49 | 40.75 | 27.25 |\\n\\n### Table 3. Comparison of mIoU(%) using VGG-16 as backbone under the domain generalization setting $G \\\\rightarrow \\\\{C, B, M, S, G\\\\}$.\\n\\n| Baseline | CRP | ASG | FSDR | GLTR |\\n|----------|-----|-----|------|------|\\n| Avg.     |     |     |      |      |\\n| Baseline | 29.40 | -   | -   | -   |\\n| IBN-Net  | 37.90 | -   | -   | -   |\\n| RobustNet| 42.55 | 44.96 | 51.68 | 23.29 |\\n| WildNet  | 40.50 | 42.35 | 20.67 | 8.08 |\\n\\n### Table 4. Comparison of mIoU(%) using ResNet-50 as backbone under the domain generalization setting $C \\\\rightarrow \\\\{G, B, M, S, C\\\\}$.\\n\\n### Table 5. Effect of the proposed losses on the domain generalization setting $G \\\\rightarrow \\\\{C, B, M, S, G\\\\}$ using ResNet-50 as backbone in mIoU(%). Losses $L_{\\\\text{orig}}$, $L_{CEL}$, $L_{SEL}$, and $L_{SCR}$ are defined in Eq. (1), Eq. (11), Eq. (12), and Eq. (13), respectively.\\n\\n### Ablation Studies\\n\\nIn this subsection, extensive experiments with ResNet-50 model on the DG scenario from GTA V to Cityscapes, BDD100K, Mapillary, SYNTHIA, and GTA V are conducted to study the effectiveness of each component in the proposed method. Table 5 shows the effect of the proposed losses on domain generalization. The baseline model trained only with $L_{\\\\text{orig}}$ overfits the source domain and has poor performance on unseen domains. Even with only $L_{CEL}$ applied, our model achieves an Avg of 43.46% with +4.40% improvement. This shows the importance of content diversification that is overlooked in many studies. Further, we make the wild-stylized features learn task-specific information with $L_{SEL}$ to achieve an Avg of 45.48%, and regularize the model to learn consistent semantic information with $L_{SCR}$, finally achieving an Avg of 46.33%. Next, we conduct more ablations for important components.\\n\\n#### Number of wild images.\\n\\nIn Table 6a, the number of wild images used to train our model is considered. Even if only 10 wild images are used, the generalization performance is significantly enhanced by +5.54% compared with the baseline by preventing overfitting to the source domain. Moreover, the generalization performance of the model gradually improves as the number of wild images used increases. This shows that the extension of both content and style to the wild helps networks to learn domain-generalized semantic features.\\n\\n#### Amount of FS.\\n\\nTable 6b shows the influence of the amount of FS on generalization performance. By replacing only the first batch normalization with FS, we can extend contents and styles to the wild based on diversified stylized features and improve generalization performance compared to baseline by +5.23%. Adding FS to some shallow layers boosts performance further. However, applying FS to deeper layers degrades performance slightly, as semantic content should be captured more important than style as the layer deepens. A suitable amount of FS, which does not disturb the semantic information, helps to train the generalized model by augmenting the source features to have various wild styles.\\n\\n#### Size of wild content dictionary.\\n\\nTable 6c shows sensitivity to the size of the wild content dictionary. Extending the source content to the wild improves generalization performance, and even when extended to wild content within a mini-batch of size 2048 without a content dictionary, our model achieves higher generalization performance than without content extension. We take size of 393216.\\n\\n#### FS with wild style.\\n\\nIn Table 6d, we show the effect of FS using statistics of wild features on the generalization performance of the model. To apply FS without the help of the wild, the mean and standard deviation of the source features were multiplied by random values in the range $[0.5, 1.5]$ and then used instead of the statistics of the wild features.\"}"}
{"id": "CVPR-2022-2054", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6. Ablation Study. For each setting, we report mIoU(%) using ResNet-50 as backbone in DG scenario: $G \\\\rightarrow \\\\{C, B, M, S, G\\\\}$.\\n\\nFigure 5. Visualization of extended wild contents. More visualizations are available in the supplementary material.\\n\\nSampling methods. By using sampled feature maps, CEL stores various wild contents in the fixed-size dictionary and reduces memory consumption due to pixel-level contrastive loss calculations. Since two adjacent pixels have almost similar semantic information, uniform sampling makes learning more diverse contents than random sampling, leading to high generalization performance as shown in Table 6e.\\n\\n5. Discussion\\n\\n5.1. Qualitative Analysis\\n\\nTo analyze the wild content extension, we visualize the wild contents closest to the stylized source contents in Fig. 5. As can be seen in the figure, the source content is extended to wild content with semantic information similar to itself, e.g., the road under the car, the wheel of the bicycle, and the head of a man wearing a hat. Our model learns domain-generalized features by inducing source content closer to these wild content in the feature space. With learning various wild contents, WildNet makes reliable predictions on unseen contents. Figs. 1 and 4 show segmentation results and visualization of wild-stylized features, and further analysis is provided in the supplementary material.\\n\\n5.2. Limitations and Future Works\\n\\nWe have shown that the source content extends to wild content with semantic information similar to itself. However, similar semantic information in the two contents does not guarantee that the classes of the two contents are always the same, as observed for the rider of Fig. 5. Extending the rider with the hat to the person with the hat may bridge between the rider-class and person-class. Our future works will involve positive content selection using predicted class probabilities on wild images and negative content extension to further boost the class discrimination ability.\\n\\n6. Conclusion\\n\\nWe presented WildNet which exploits unlabeled wild images for domain-generalized semantic segmentation. Our approach effectively extends style and content from source to wild, resulting in drastic performance improvement even we leverage 10 wild images. In contrast to previous studies that exploit generalization cues only from style, we additionally exploit the potential to generalize domain from content. We thoroughly ablated to demonstrate the efficacy of our WildNet and achieved superior segmentation performance under several domain generalization scenarios. We believe that our approach provides an opportunity to utilize huge amounts of unlabeled data for domain generalization.\\n\\nAcknowledgement. This research was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF-2019R1A2C1007153).\"}"}
{"id": "CVPR-2022-2054", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WildNet: Learning Domain Generalized Semantic Segmentation from the Wild\\n\\nSuhyeon Lee Hongje Seong Seongwon Lee Euntai Kim\\n\\nSchool of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea\\n{hyeon93, hjseong, won4113, etkim}@yonsei.ac.kr\\n\\nAbstract\\nWe present a new domain generalized semantic segmentation network named WildNet, which learns domain-generalized features by leveraging a variety of contents and styles from the wild. In domain generalization, the low generalization ability for unseen target domains is clearly due to overfitting to the source domain. To address this problem, previous works have focused on generalizing the domain by removing or diversifying the styles of the source domain. These alleviated overfitting to the source-style but overlooked overfitting to the source-content. In this paper, we propose to diversify both the content and style of the source domain with the help of the wild. Our main idea is for networks to naturally learn domain-generalized semantic information from the wild. To this end, we diversify styles by augmenting source features to resemble wild styles and enable networks to adapt to a variety of styles. Furthermore, we encourage networks to learn class-discriminant features by providing semantic variations borrowed from the wild to source contents in the feature space. Finally, we regularize networks to capture consistent semantic information even when both the content and style of the source domain are extended to the wild. Extensive experiments on five different datasets validate the effectiveness of our WildNet, and we significantly outperform state-of-the-art methods. The source code and model are available online: https://github.com/suhyeonlee/WildNet.\\n\\n1. Introduction\\nDomain generalized semantic segmentation aims to better predict pixel-level semantic labels on multiple unseen target domains while learning only on the source domain. Unfortunately, the domain shift between the source and target domains makes a segmentation model trained on the given source data behave stupidly on the unseen target data, as shown in Fig. 1b. In domain generalization (DG), the low generalization performance for unseen domains is obviously due to overfitting to the source domain since the model cannot see any information about the target domains in the learning process and even unlabeled target images are not provided unlike domain adaptation (DA), it over-learns the statistical distribution of the given source data. Recently, some studies [7, 29, 41, 42] have proposed learning the domain-generalized content feature by 'removing' domain-specific style information from the data to prevent overfitting to the source domain. Based on the correlation between the feature\u2019s covariance matrix and style [13, 14], they assumed that only content features would remain if elements of features considered the domain-specific style were whitened [23, 30, 50, 53]. However, since the content and style are not orthogonal, whitening the style may cause a loss of semantic content, which is indispensable for semantic category prediction. As a result, they predict semantic categories from incomplete content features and have difficulty making accurate predictions, as shown in Fig. 1c.\\n\\nIn this paper, we propose a new domain generalized semantic segmentation network called WildNet, which learns...\"}"}
{"id": "CVPR-2022-2054", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the domain-generalized semantic feature by 'extending' both content and style to the wild. Although some previous works [22, 45, 62] utilized various styles from the wild, e.g., ImageNet [11] for real styles and Painter by Numbers [38] for unreal styles, they overlooked that the high generalization ability comes from learning not only various styles but also various contents. In contrast to previous studies, our main idea is to naturally learn domain-generalized semantic information by leveraging a variety of contents and styles from the wild, without forcing whitening on domain-specific styles.\\n\\nTo extend both content and style to the wild, we present four effective learning methods. (i) Based on the relevance of style and feature statistics, feature stylization diversifies the style of the source feature by transferring the statistics of the wild feature to the source feature over several layers. (ii) To prevent overfitting to the source contents, we propose content extension learning to increase the intra-class content variability in the latent embedding space. Extending content from source to wild helps networks make generalized predictions on unseen contents. (iii) To prevent overfitting to the source style, we propose style extension learning to encourage networks to adapt to the various styles extended to the wild. (iv) Finally, semantic consistency regularization enables networks to capture consistent semantic information even when both the content and style of the source domain are extended to the wild. With the proposed learning methods, our WildNet learns domain-generalized semantic features by leveraging a variety of contents and styles from the wild. Extensive experiments over multiple domains show that our network achieves superior performance on domain generalization for semantic segmentation.\\n\\nOur main contributions are as follows:\\n\u2022 We present a novel domain generalized semantic segmentation network named WildNet, which learns domain-generalized semantic features by leveraging a variety of contents and styles from the wild.\\n\u2022 We propose four learning techniques to train domain-generalized networks by extending both the content and style of the source domain to the wild. These enable our model to make reliable predictions on various unseen target domains without training on them.\\n\u2022 Our network achieves superior performance in extensive experiments on domain generalization for semantic segmentation constructed over multiple domains.\\n\\n2. Related Work\\n\\n2.1. Domain Adaptation and Generalization\\n\\nDomain adaptation (DA) aims to increase the performance on the target domain by reducing the domain gap between the source and target domains. In semantic segmentation, DA is exploited to tackle the effort of annotating pixel-level categories in an image. Most DA methods train networks using the 'given' target images via image translation [17, 20, 33, 60, 64], feature alignment [21, 44, 55\u201357], and self-training [16, 29, 32, 40, 63] strategies. However, it is hard to acquire target images from various environments during the learning process, and efforts to retrain networks are required whenever applying networks to a new target domain.\\n\\nTo overcome these limitations, domain generalization (DG) has recently attracted considerable attention. However, most DG studies have focused on image classification and there are only a few recent studies on semantic segmentation. In this study, we deal with DG for semantic segmentation. Unlike DA, DG does not have access to the target domains during the learning process. To make reliable predictions on various 'unknown' target domains, most existing studies focus on whitening [7], normalizing [41], and diversifying [22, 45, 62] styles to avoid overfitting to the style of the source domain. This paper focuses on extending both the content and style of the source domain to the wild [11], enabling networks to learn domain-generalized semantic features from diversified contents and styles.\\n\\n2.2. Contrastive Learning\\n\\nContrastive learning [9, 39] is a strategy that minimizes the distance from a positive sample and maximizes the distance from a negative sample in the embedding space. Recently, He et al. [18] used a dynamic dictionary with a queue and Chen et al. [3] used two views of the same image as a positive pair to learn visual representations. To diversify a positive pair, a recent work [12] proposed to use the positive's nearest neighbor in the latent space as a positive. After supervised contrastive learning [27] has been proposed, there are recent efforts to apply contrastive learning to fully- and semi-supervised semantic segmentation [1, 65, 66]. To obtain positive samples, these works perform image augmentation or store features using label information in a memory bank [58]. These enhance class discrimination in the seen source domain but do not guarantee improving class discrimination in various unseen domains. To adapt contrastive learning to DG for semantic segmentation, we propose a learning method using the wild-stylized feature and its closest wild content as positive samples.\\n\\n2.3. Free ImageNet\\n\\nMost studies regard ImageNet [11] as free and use it to pre-train networks. The ImageNet pre-trained model is commonly used in various fields such as object detection [34, 68], semantic segmentation [8, 54], panoptic segmentation [6, 35, 59], and video object segmentation [51], and is considered to be the same basis. The ImageNet pre-trained model is also used in most DA and DG for semantic segmentation methods, and ImageNet is used to borrow\"}"}
{"id": "CVPR-2022-2054", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Proposed Method\\n\\nIn this section, we introduce four learning techniques consisting of Feature Stylization (FS), Content Extension Learning (CEL), Style Extension Learning (SEL), and Semantic Consistency Regularization (SCR) for learning domain-generalized features by extending both the content and style of the source domain to the wild. Our WildNet achieves superior generalization ability with them and the overall learning process is outlined in Fig. 2.\\n\\n3.1. Problem Setup and Overview\\n\\nDomain generalization (DG) aims to enhance the generalization capability on both the seen source domain $S$ and unseen target domains $T = \\\\{T_1, \\\\ldots, T_N\\\\}$. Let $\\\\phi$ be a semantic segmentation model that outputs pixel-wise category predictions $p$ from image $x$. This model consists of a feature extractor $\\\\phi_{\\\\text{feat}}$ and classifier $\\\\phi_{\\\\text{cls}}$. In DG, when we train the model, we have access to the source domain training dataset $D_s = \\\\{(x_s, y_s)\\\\}$ while inaccessible to the target domains, where $x_s \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 3}$ is an image, $y_s \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times K}$ is its pixel-wise label, and $K$ is a number of semantic categories.\\n\\nThe baseline model is trained with the segmentation loss $L_{\\\\text{orig}} = -\\\\frac{1}{HW} \\\\sum_{h=1}^{H} \\\\sum_{w=1}^{W} \\\\sum_{k=1}^{K} y_{shw} \\\\log(\\\\phi(x_s))$. (1)\\n\\nIn this paper, we focus on extending both the content and style of the source domain to obtain high generalization performance on unknown target domains $T$. We utilize the unlabeled wild dataset $D_w = \\\\{x_w\\\\}$, which has various contents and styles. At each training iteration, a random pair of source and wild images is provided as input, and the style and content of the source image are extended to the wild domain $W$ in the feature space. With the help of the wild, our network naturally learns domain-generalized semantic information from a variety of contents and styles. After the training, the model is evaluated on validation sets of both the seen source domain $S$ and unseen target domains $T$.\"}"}
{"id": "CVPR-2022-2054", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\n\\n3.3. Content Extension Learning\\n\\nSo FS applies only to some shallow layers in this work. The wild-stylized feature $z$ w of the source feature $z$ s of the classification layer $\\psi_l$ is output after the feature extractor. FS provides the feature output from the layer. For each training iteration, we transfer the style of the source feature $z_s$ to the wild-style while maintaining the spatial information. The wild-stylized feature $z_w$ contains exactly the same semantic information from various wild-style features, we swap the style of $z_l$, and obtain the source content extension loss for the $l$-th pixel of the source image $x_s$ as follows:\\n\\n\\\\[\\nL_{SCE} = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\log \\\\psi(z_i, z_{sw_i}) + \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\log \\\\left(1 - \\\\psi(z_i, z_{sw_i})\\\\right)\\n\\\\]\\n\\nwhere $\\\\psi(z_i, z_{sw_i}) = \\\\exp(-\\\\|z_i - z_{sw_i}\\\\|_2^2/\\\\tau)$ is the similarity score between the input feature $z_i$ and the wild-stylized feature $z_{sw_i}$.\\n\\nThe subscript proj is sometimes omitted for convenience.\\n\\nIn this subsection, we propose to extend the contents in the source domain to the wild. One of the reasons for overfiting to the source domain is that networks overlearn a limited amount of source content. We address this issue by inverting the source domain is that networks overlearn a limited amount of source content. We address this issue by inverting the source domain to the wild. In this work, we diversify the styles of source contents to the wild in the embedding space.\\n\\nThe wild-stylized feature $z_w$ of the source feature $z_s$ after the feature extractor $\\\\psi_l$ can be diversified by adjusting the statistics of features containing the same semantic information, but also encouraging them from contents containing other semantic information. In order to obtain reliable semantic information from unseen contents, networks should be able to cluster contents mainly in shallow layers of networks [41], the styles of features can be diversified by adjusting the statistics of features containing the same semantic information, but content perturbation exists.\\n\\nAt each training iteration, we swap the style of $z_l$ from shallow layers. As the layer deepens, semantic deviation of feature $z_l$ is re-normalized with channel-wise statistics of $z_l$. The subscript proj is flattened and stored in the wild-content dictionary $Q$. In the learning process, this enables us to augment source features in a few AdaIN [24] layers to the feature extractor in the source domain. We structure our model using the dynamic dictionary structure $Q$ by utilizing the wild-content dictionary $Q$.\\n\\nAt each training iteration, we swap the style of $z_l$ and $z_s$ independently of the classification $\\\\psi_l$ and define the source content extension loss for the $i$-th pixel as:\\n\\n\\\\[\\nL_{SCE} = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\log \\\\psi(z_i, z_{sw_i}) + \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\log \\\\left(1 - \\\\psi(z_i, z_{sw_i})\\\\right)\\n\\\\]\\n\\nwhere $\\\\psi(z_i, z_{sw_i}) = \\\\exp(-\\\\|z_i - z_{sw_i}\\\\|_2^2/\\\\tau)$ is the similarity score between the input feature $z_i$ and the wild-stylized feature $z_{sw_i}$.\\n\\nThe subscript proj is sometimes omitted for convenience.\"}"}
{"id": "CVPR-2022-2054", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Inigo Alonso, Alberto Sabater, David Ferstl, Luis Montesano, and Ana C Murillo. Semi-supervised semantic segmentation with pixel-level contrastive learning from a class-wise memory bank. In ICCV, 2021.\\n\\n[2] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, pages 801\u2013818, 2018.\\n\\n[3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, pages 1597\u20131607. PMLR, 2020.\\n\\n[4] Wuyang Chen, Zhiding Yu, Zhangyang Wang, and Animashree Anandkumar. Automated synthetic-to-real generalization. In ICML, pages 1746\u20131756. PMLR, 2020.\\n\\n[5] Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and Jia-Bin Huang. Crdoco: Pixel-level domain transfer with cross-domain consistency. In CVPR, pages 1791\u20131800, 2019.\\n\\n[6] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation. In CVPR, pages 12475\u201312485, 2020.\\n\\n[7] Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim, Seungryong Kim, and Jaegul Choo. Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. In CVPR, pages 11580\u201311590, 2021.\\n\\n[8] Sungha Choi, Joanne T Kim, and Jaegul Choo. Cars can't fly up in the sky: Improving urban-scene segmentation via height-driven attention networks. In CVPR, pages 9373\u20139383, 2020.\\n\\n[9] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with application to face verification. In CVPR, volume 1, pages 539\u2013546. IEEE, 2005.\\n\\n[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, pages 3213\u20133223, 2016.\\n\\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248\u2013255. IEEE, 2009.\\n\\n[12] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In ICCV, 2021.\\n\\n[13] Leon Gatys, Alexander S Ecker, and Matthias Bethge. Texture synthesis using convolutional neural networks. NeurIPS, 28:262\u2013270, 2015.\\n\\n[14] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In CVPR, pages 2414\u20132423, 2016.\\n\\n[15] Leon A Gatys, Alexander S Ecker, Matthias Bethge, Aaron Hertzmann, and Eli Shechtman. Controlling perceptual factors in neural style transfer. In CVPR, pages 3985\u20133993, 2017.\\n\\n[16] Xiaoqing Guo, Chen Yang, Baopu Li, and Yixuan Yuan. Metacorrection: Domain-aware meta loss correction for unsupervised domain adaptation in semantic segmentation. In CVPR, pages 3927\u20133936, 2021.\\n\\n[17] Jianzhong He, Xu Jia, Shuaijun Chen, and Jianzhuang Liu. Multi-source domain adaptation with collaborative learning for semantic segmentation. In CVPR, pages 11008\u201311017, 2021.\\n\\n[18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, pages 9729\u20139738, 2020.\\n\\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.\\n\\n[20] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018.\\n\\n[21] Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016.\\n\\n[22] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Fsdr: Frequency space domain randomization for domain generalization. In CVPR, pages 6891\u20136902, 2021.\\n\\n[23] Lei Huang, Dawei Yang, Bo Lang, and Jia Deng. Decorrelated batch normalization. In CVPR, pages 791\u2013800, 2018.\\n\\n[24] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, pages 1501\u20131510, 2017.\\n\\n[25] Takashi Isobe, Xu Jia, Shuaijun Chen, Jianzhong He, Yongjie Shi, Jianzhuang Liu, Huchuan Lu, and Shengjin Wang. Multi-target domain adaptation with collaborative consistency learning. In CVPR, pages 8187\u20138196, 2021.\\n\\n[26] Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, and Mingli Song. Neural style transfer: A review. IEEE Transactions on Visualization and Computer Graphics, 26(11):3365\u20133385, 2019.\\n\\n[27] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In NeurIPS, volume 33, 2020.\\n\\n[28] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n\\n[29] Suhyeon Lee, Junhyuk Hyun, Hongje Seong, and Euntai Kim. Unsupervised domain adaptation for semantic segmentation by content transfer. In AAAI, pages 8306\u20138315, 2021.\\n\\n[30] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature transforms. In NeurIPS, pages 385\u2013395, 2017.\\n\\n[31] Yanghao Li, Naiyan Wang, Jiaying Liu, and Xiaodi Hou. Demystifying neural style transfer. In IJCAI, pages 2230\u20132236, 2017.\"}"}
{"id": "CVPR-2022-2054", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yunsheng Li, Lu Yuan, and Nuno Vasconcelos. Bidirectional learning for domain adaptation of semantic segmentation. In CVPR, pages 6936\u20136945, 2019.\\n\\nHaoyu Ma, Xiangru Lin, Zifeng Wu, and Yizhou Yu. Coarse-to-fine domain adaptive semantic segmentation with photometric alignment and category-center regularization. In CVPR, pages 4051\u20134060, 2021.\\n\\nNingning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In ECCV, pages 116\u2013131, 2018.\\n\\nRohit Mohan and Abhinav Valada. Efficientps: Efficient panoptic segmentation. International Journal of Computer Vision, 129(5):1551\u20131579, 2021.\\n\\nHyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, pages 8690\u20138699, 2021.\\n\\nGerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In ICCV, pages 4990\u20134999, 2017.\\n\\nKiri Nichol. Painter by numbers, wikiart. https://www.kaggle.com/c/painter-by-numbers, 2016.\\n\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\\n\\nFei Pan, Inkyu Shin, Francois Rameau, Seokju Lee, and In So Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision. In CVPR, pages 3764\u20133773, 2020.\\n\\nXingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In ECCV, pages 464\u2013479, 2018.\\n\\nXingang Pan, Xiaohang Zhan, Jianping Shi, Xiaoou Tang, and Ping Luo. Switchable whitening for deep representation learning. In ICCV, pages 1863\u20131871, 2019.\\n\\nKwanyong Park, Sanghyun Woo, Inkyu Shin, and In So Kweon. Discover, hallucinate, and adapt: Open compound domain adaptation for semantic segmentation. In NeurIPS, volume 33, pages 10869\u201310880, 2020.\\n\\nSujoy Paul, Yi-Hsuan Tsai, Samuel Schulter, Amit K Roy-Chowdhury, and Manmohan Chandraker. Domain adaptive semantic segmentation using weak labels. In ECCV, pages 571\u2013587. Springer, 2020.\\n\\nDuo Peng, Yinjie Lei, Lingqiao Liu, Pingping Zhang, and Jun Liu. Global and local texture randomization for synthetic-to-real semantic segmentation. IEEE Transactions on Image Processing, 30:6594\u20136608, 2021.\\n\\nStephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In ECCV, pages 102\u2013118. Springer, 2016.\\n\\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400\u2013407, 1951.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234\u2013241. Springer, 2015.\\n\\nGerman Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In CVPR, pages 3234\u20133243, 2016.\\n\\nSubhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Samuel Rota Bulo, Nicu Sebe, and Elisa Ricci. Unsupervised domain adaptation using feature-whitening and consensus loss. In CVPR, pages 9471\u20139480, 2019.\\n\\nHongje Seong, Seoung Wug Oh, Joon-Young Lee, Seongwon Lee, Suhyeon Lee, and Euntai Kim. Hierarchical memory matching network for video object segmentation. In ICCV, pages 12889\u201312898, 2021.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.\\n\\nBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, pages 443\u2013450. Springer, 2016.\\n\\nAndrew Tao, Karan Sapra, and Bryan Catanzaro. Hierarchical multi-scale attention for semantic segmentation. arXiv preprint arXiv:2005.10821, 2020.\\n\\nYi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In CVPR, pages 7472\u20137481, 2018.\\n\\nTuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick P\u00b4erez. Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation. In CVPR, pages 2517\u20132526, 2019.\\n\\nHaoran Wang, Tong Shen, Wei Zhang, Ling-Yu Duan, and Tao Mei. Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation. In ECCV, pages 642\u2013659. Springer, 2020.\\n\\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In CVPR, pages 3733\u20133742, 2018.\\n\\nYuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min Bai, Ersin Yumer, and Raquel Urtasun. Upsnet: A unified panoptic segmentation network. In CVPR, pages 8818\u20138826, 2019.\\n\\nYanchao Yang and Stefano Soatto. FDA: Fourier domain adaptation for semantic segmentation. In CVPR, pages 4085\u20134095, 2020.\\n\\nFisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. BDD100K: A diverse driving dataset for heterogeneous multitask learning. In CVPR, pages 2636\u20132645, 2020.\\n\\nXiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, and Boqing Gong. Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data. In ICCV, pages 2100\u20132110, 2019.\\n\\nPan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, and Fang Wen. Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation. In CVPR, pages 12414\u201312424, 2021.\"}"}
{"id": "CVPR-2022-2054", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yiheng Zhang, Zhaofan Qiu, Ting Yao, Dong Liu, and Tao Mei. Fully convolutional adaptation networks for semantic segmentation. In CVPR, pages 6810\u20136818, 2018.\\n\\nXiangyun Zhao, Raviteja Vemulapalli, Philip Andrew Mansfield, Boqing Gong, Bradley Green, Lior Shapira, and Ying Wu. Contrastive learning for label efficient semantic segmentation. In ICCV, pages 10623\u201310633, 2021.\\n\\nYuanyi Zhong, Bodi Yuan, Hong Wu, Zhiqiang Yuan, Jian Peng, and Yu-Xiong Wang. Pixel contrastive-consistent semi-supervised semantic segmentation. In ICCV, pages 7273\u20137282, 2021.\\n\\nKaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In ICLR, 2021.\\n\\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR, 2020.\"}"}
