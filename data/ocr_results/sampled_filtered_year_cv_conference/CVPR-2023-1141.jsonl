{"id": "CVPR-2023-1141", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Brett Allen, Brian Curless, and Zoran Popovi\u0107. The space of human body shapes: reconstruction and parameterization from range scans. ACM Trans. Graph., 2003.\\n\\n[2] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, and James Davis. SCAPE: shape completion and animation of people. ACM Trans. Graph., 2005.\\n\\n[3] Tristan Aumentado-Armstrong, Stavros Tsogkas, Allan Jepsson, and Sven Dickinson. Geometric disentanglement for generative latent shape models. In Int. Conf. Comput. Vis., 2019.\\n\\n[4] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image. In Eur. Conf. Comput. Vis., 2016.\\n\\n[5] Federica Bogo, Javier Romero, Gerard Pons-Moll, and Michael J Black. Dynamic FAUST: Registering human bodies in motion. In IEEE Conf. Comput. Vis. Pattern Recog., 2017.\\n\\n[6] Davide Boscaini, Jonathan Masci, Simone Melzi, Michael M Bronstein, Umberto Castellani, and Pierre Vandergheynst. Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks. In Comput. Graph. Forum, 2015.\\n\\n[7] Giorgos Bouritsas, Sergiy Bokhnyak, Stylianos Ploumpis, Michael Bronstein, and Stefanos Zafeiriou. Neural 3D morphable models: Spiral convolutional networks for 3D shape representation learning and generation. In Int. Conf. Comput. Vis., 2019.\\n\\n[8] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.\\n\\n[9] Haoyu Chen, Hao Tang, Henglin Shi, Wei Peng, Nicu Sebe, and Guoying Zhao. Intrinsic-extrinsic preserved GANs for unsupervised 3D pose transfer. In Int. Conf. Comput. Vis., 2021.\\n\\n[10] Zhixiang Chen and Tae-Kyun Kim. Learning feature aggregation for deep 3D morphable models. In IEEE Conf. Comput. Vis. Pattern Recog., 2021.\\n\\n[11] Luca Cosmo, Antonio Norelli, Oshri Halimi, Ron Kimmel, and Emanuele Rodola. LIMP: Learning latent shape representations with metric preservation priors. In Eur. Conf. Comput. Vis., 2020.\\n\\n[12] Lin Gao, Yu-Kun Lai, Jie Yang, Ling-Xiao Zhang, Shihong Xia, and Leif Kobbelt. Sparse data driven mesh deformation. IEEE Trans. Vis. Comput. Graph., 2019.\\n\\n[13] Zhongpai Gao, Junchi Yan, Guangtao Zhai, Juyong Zhang, Yiyan Yang, and Xiaokang Yang. Learning local neighboring structure for robust 3D shape representation. In AAAI, 2021.\\n\\n[14] Shunwang Gong, Lei Chen, Michael Bronstein, and Stefanos Zafeiriou. SpiralNet++: A fast and highly efficient mesh convolution operator. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019.\\n\\n[15] Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, and Mathieu Aubry. 3D-CODED: 3D correspondences by deep deformation. In Eur. Conf. Comput. Vis., 2018.\\n\\n[16] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Adv. Neural Inform. Process. Syst., 2017.\\n\\n[17] Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163, 2015.\\n\\n[18] Boyi Jiang, Juyong Zhang, Jianfei Cai, and Jianmin Zheng. Disentangled human body embedding based on deep hierarchical neural network. IEEE Trans. Vis. Comput. Graph., 2020.\\n\\n[19] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In IEEE Conf. Comput. Vis. Pattern Recog., 2018.\\n\\n[20] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015.\\n\\n[21] Linjie Li, Zhe Gan, Yu Cheng, and Jingjing Liu. Relation-aware graph attention network for visual question answering. In Int. Conf. Comput. Vis., 2019.\\n\\n[22] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. SMPL: A skinned multi-person linear model. ACM Trans. Graph., 2015.\\n\\n[23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. 2019.\\n\\n[24] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black. Expressive body capture: 3D hands, face, and body from a single image. In IEEE Conf. Comput. Vis. Pattern Recog., 2019.\\n\\n[25] Leonid Pishchulin, Stefanie Wuhrer, Thomas Helten, Christian Theobalt, and Bernt Schiele. Building statistical shape spaces for 3D human modeling. Pattern Recognition, 2017.\\n\\n[26] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and Michael J Black. Generating 3D faces using convolutional mesh autoencoders. In Eur. Conf. Comput. Vis., 2018.\\n\\n[27] Kathleen M Robinette, Hans Daanen, and Eric Paquet. The caesar project: a 3-d surface anthropometry survey. In International Conference on 3-D Digital Imaging and Modeling, 1999.\\n\\n[28] Javier Romero, Dimitrios Tzionas, and Michael J Black. Embodied hands: Modeling and capturing hands and bodies together. ACM Trans. Graph., 2017.\\n\\n[29] Hyewon Seo and Nadia Magnenat-Thalmann. An automatic modeling of human bodies from sizing parameters. In Proceedings of the Symposium on Interactive 3D Graphics, 2003.\\n\\n[30] Qingyang Tan, Lin Gao, Yu-Kun Lai, and Shihong Xia. Variational autoencoders for deforming 3D mesh models. In IEEE Conf. Comput. Vis. Pattern Recog., 2018.\"}"}
{"id": "CVPR-2023-1141", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Qingyang Tan, Lin Gao, Yu-Kun Lai, Jie Yang, and Shihong Xia. Mesh-based autoencoders for localized deformation component analysis. In AAAI, 2018.\\n\\nYating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang. Recovering 3D human mesh from monocular images: A survey. arXiv preprint arXiv:2203.01923, 2022.\\n\\nNitika Verma, Edmond Boyer, and Jakob Verbeek. Dynamic filters in graph convolutional networks. arXiv preprint arXiv:1706.05206, 2017.\\n\\nJiashun Wang, Chao Wen, Yanwei Fu, Haitao Lin, Tianyun Zou, Xiangyang Xue, and Yinda Zhang. Neural pose transfer by spatially adaptive instance normalization. In IEEE Conf. Comput. Vis. Pattern Recog., 2020.\\n\\nYipin Yang, Yao Yu, Yu Zhou, Sidan Du, James Davis, and Ruigang Yang. Semantic parametric reshaping of human body models. In International Conference on 3D Vision, 2014.\\n\\nYanhong Zeng, Jianlong Fu, and Hongyang Chao. 3D human body reshaping with anthropometric modeling. In International Conference on Internet Multimedia Computing and Service, 2017.\\n\\nHongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang, Yebin Liu, Limin Wang, and Zhenan Sun. PyMAF: 3D human pose and shape regression with pyramidal mesh alignment feedback loop. In Int. Conf. Comput. Vis., 2021.\\n\\nKeyang Zhou, Bharat Lal Bhatnagar, and Gerard Pons-Moll. Unsupervised shape and pose disentanglement for 3D meshes. In Eur. Conf. Comput. Vis., 2020.\\n\\nShizhe Zhou, Hongbo Fu, Ligang Liu, Daniel Cohen-Or, and Xiaoguang Han. Parametric reshaping of human bodies in images. ACM Trans. Graph., 2010.\\n\\nSilvia Zuffi, Angjoo Kanazawa, David W Jacobs, and Michael J Black. 3D menagerie: Modeling the 3D shape and pose of animals. In IEEE Conf. Comput. Vis. Pattern Recog., 2017.\"}"}
{"id": "CVPR-2023-1141", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"significant the line contributes to the shape variation along o(d). As can be seen, the larger the angle, the more significant the line, and working out the angle makes with the bone orientation. We apply the following thresholding and normalization to focus on shape variations along o(t). An intuitive idea to solve this issue is to use the matrix therefore should be scaled accordingly when the part length is coupled with the Euclidean distance matrix, which leads to incomplete decoupling and affects editing precision.\\n\\nFurthermore, preserving geometric features is a challenging issue. An intuitive idea to solve this issue is to use the matrix of pairwise Euclidean distances between all vertices in x(k).\\n\\nTo alleviate this problem, we propose an orientation-adaptive weighting strategy enforcing the bone and shape disentangled representation for body parts, i.e., the edited mesh swp(k) = edit\u2225swp\u2225norm1\u00b7L1(k). As these are bone-independent shape features (including bone orientation and length) of parts by modifying bone and shape disentangled representation for body parts, it serves bone information during the editing process.\\n\\nThe weighted pairwise distance matrix is then defined as\\n\\n\\\\[ \\\\text{dis}_{\\\\text{swp}}(x) = \\\\text{dis}(x) \\\\cdot L \\\\]\\n\\nwhere \\\\( L \\\\) is the matrix therefore should be scaled accordingly when the part length is coupled with the Euclidean distance matrix, which leads to incomplete decoupling and affects editing precision. Nevertheless, the bone orientation-adaptive weighting strategy enforces the bone and shape disentangled representation for body parts, i.e., the edited mesh swp(x) = edit\u2225swp\u2225norm1\u00b7L1(x). Besides, the geometric features largely capture the pairwise distances along o(t). As can be seen, the larger the angle, the more significant the line makes with the bone orientation.\\n\\nWith the constraints in Sec. 3.4.2, our autoencoder learns a function that measures the length of a mesh part between the two joints. We can then rewrite the length information to be separated from the Euclidean distance matrix as much as possible. Nonetheless, this strategy also ignores some useful geometric information along o(t). To address this problem, we further impose part-level volume constraint, which provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\n\\n\\\\[ \\\\text{vol}(x) = v(x) \\\\]\\n\\nwhere \\\\( v(x) \\\\) is a function that calculates the volume of a mesh part according to the tetrahedral volume formula, and \\\\( v(x) \\\\) provides strong geometric supervision to achieve natural and reasonable editing results, which can be calculated as:\\"}
{"id": "CVPR-2023-1141", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"However, the editing flow easily causes training collapses. We solve this problem by imposing a vector norm regularization $L_1$ norm to establish a mapping between the norm of localized shape codes and the circumference of body parts, which not only benefits converge, but also allows users to edit part shape size on a unified scale. This regularization can be defined as:\\n\\n$$\\nL_1 = \\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\|z_{s_1}^k\\\\|_2 - \\\\text{circ}(x_{s_1}^k),\\n$$\\n\\nwhere $\\\\text{circ}(\\\\cdot)$ is a function that measures the circumference of a part using the identified landmarks.\\n\\n$L_{\\\\text{edit}}$ successfully allows shape codes' norm and direction to represent shape size and style respectively, which not only enables flexible part-level editing but also ensures the continuity of the learned latent space to a certain extent.\\n\\n### 3.5. Implementation Details\\n\\nFor the spiral convolution encoder, we use a framework similar to [7]. Specifically, it consists of four spiral convolution layers and downsampling layers, and the structure of the decoder is a mirror of the encoder, except that the downsampling layers are replaced by the upsampling layers. Our algorithm is implemented in PyTorch [23]. All the training and test experiments are carried out on a PC with an RTX 3090 GPU. We train our network for 300 epochs with a learning rate of $1 \\\\times 10^{-3}$, a learning rate decay of $0.99$ after each epoch and the Adam optimizer [20]. The entire training time takes around 24 hours. We use 16-dimensional latent codes (8 for bones and 8 for shape) to embed each part. More implementation details can be found in the supplementary material.\\n\\n### 4. Experiments\\n\\n#### 4.1. Datasets\\n\\n- **DFAUST.** The dynamic human body dataset with the same mesh connectivity as SMPL [22] from Bogo et al. [5], captures 14 different body motion sequences (e.g., hips, running, and jumping) for each of the 10 human subjects. We evenly extract one-twentieth of the original dataset for the convenience of training. Then we randomly split the extracted data into a test set of 182 meshes and a training set of 1936 meshes.\\n\\n- **SPRING.** The large human body dataset with the same mesh connectivity as SCAPE [2] from Yang et al. [35], consists of 3000+ subject meshes with a rough A-pose registered from the CAESAR dataset [27] using a non-rigid deformation algorithm. For the subsequent experiments, the SPRING dataset is randomly split into 2743 training and 305 test meshes.\\n\\n#### Data Preprocessing\\n\\nFor the training data, our method requires a joint regressor, body part semantics of each vertex, and landmarks for calculating circumferences. Since the mesh connectivity of datasets is consistent (which is required by nearly all 3D human representation learning works), the SMPL model only needs to be registered once to meet the requirements. More details about data preprocessing are given in the supplementary material.\\n\\n#### 4.2. Comparison\\n\\nIn the following, we evaluate the representation ability (reconstruction precision) and editing capacity (semantics) of our approach on the DFAUST and SPRING datasets.\\n\\n**Reconstruction Experiments.** We first compare four kinds of methods to validate the reconstruction precision of our representation: the spectral-based approach (COMA [26]), the spiral-based methods (Neural3DMM [7] and Spiralplus [14]), the attention-based approaches (Pai3DMM [13] and Deep3DMM [10]), and the disentanglement representation works (DHNN [18] and Unsup [38]). For a fair comparison, we use the official implementation of the compared methods with the same latent space dimension. Note that DHNN [18] did not release the training code, so we train and test our model on its dataset for a fair comparison. We utilize the average point-wise Euclidean distance $E_{\\\\text{avd}}$ (in millimeters) between corresponding vertices in the input and its reconstruction as metrics.\\n\\nAs shown in Tab. 1, our proposed approach shows excellent representation capability with a small model size. Fig. 5 visualizes some reconstruction results and their error maps. It can be observed that our approach outperforms other methods in reconstruction accuracy, especially for complex geometric details (e.g., faces and hands), which demonstrates the effectiveness of our bone-guided autoencoder.\"}"}
{"id": "CVPR-2023-1141", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Qualitative reconstruction results on DFAUST [5] and SPRING [35]. The per-vertex Euclidean distance error is color-coded on the reconstructed meshes for visual inspection. Since Unsup [38] has a data constraint that the same subject in different poses should be given, it cannot be trained on the SPRING dataset. Note that our representation is trained without the need of data constraint, which is used in the compared methods.\\n\\nFigure 6. Qualitative editing results by Unsup [38] (first row), HBR [36] (second and third rows) and our method. We show the reconstructed bodies and edited bodies on the left and right of the source mesh.\\n\\n| Method | Bone Orientation | Bone Length | Shape Size |\\n|--------|----------------|-------------|------------|\\n| Unsup [38] | 36.79 | 14.16 | - |\\n| HBR [36] | - | 38.37 | 15.27 | 12.87 | 18.78 |\\n| Ours | 3.26 | 9.75 | 1.57 | 5.92 | 0.45 | 17.64 |\\n\\nTable 3. Quantitative editing results. - : not supported for this task.\\n\\nIn addition, Tab. 2 gives the quantitative results on the DHNN dataset [18]. It is important to note that the compared method [18] uses a training data assumption (i.e., each posed mesh has a paired mesh in a neutral pose) for shape and pose disentanglement, but our method does not make use of this constraint. Under this unfair condition, the reconstruction accuracy of our method is only slightly lower than DHNN [18], and our model is more lightweight and semantically finer. Please refer to our supplementary material for more experimental results and details.\\n\\nEditing Experiments. We also demonstrate the flexible editing ability of our model on three editing tasks: editing bone orientation and bone length, and editing part shape size. Since bone orientation is approximately equivalent to pose, we compare with the unsupervised pose-and-shape disentanglement work Unsup [38] on the task of editing bone orientation on the DFAUST dataset. In contrast, bone length and shape size are shape information, so we compare with the Human Body Reshaping work HBR [36] on the task of editing bone length and shape size on the SPRING dataset. As there is no ground truth for these editing tasks, how to measure the editing performance is a problem. We utilize the joint and circumference errors $E_{\\\\text{joint}}$, $E_{\\\\text{circ}}$ (in millimeters) to evaluate the accuracy of editing bone and shape.\\n\\nSpecifically, for each test mesh, we randomly select editing targets and then calculate $E_{\\\\text{joint}}$, $E_{\\\\text{circ}}$ between the target attribute value and the actual attribute value of the edited human body, extracted using $J(\\\\cdot)$ and $\\\\text{circ}(\\\\cdot)$, which can be defined as:\\n\\n$$E_{\\\\text{joint}} = \\\\| T_{\\\\text{joint}} - J(x_{\\\\text{edited}}) \\\\|_2,$$\\n\\n$$E_{\\\\text{circ}} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} T_{\\\\text{circ}} - \\\\text{circ}(x_k_{\\\\text{edited}}),$$\\n\\nwhere $T_{\\\\text{joint}}$ and $T_{\\\\text{circ}}$ are the editing targets of joint positions and circumference.\\n\\nTab. 3 gives the quantitative editing results. Our method not only achieves the best performance on all the editing tasks but also preserves other unedited attributes well. Some visual results are shown in Fig. 6. Compared with Unsup [38] and HBR [36], our method can edit the human body more accurately, reasonably and flexibly. Since the attribute error does not always reflect the editing performance of models, we also design a user study for better evaluation (see the supplementary material).\\n\\n4.3. Ablation Study\\n\\nEffect of Orientation-Adaptive Weighting Strategy. We validate the effectiveness of the orientation-adaptive weighting strategy by ablating it during training. As shown in Tab. 4, introducing this strategy helps our representation to focus on geometric features along $o_\\\\perp$, leading to higher editing accuracy and more complete decoupling.\"}"}
{"id": "CVPR-2023-1141", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Effect of Other Losses. To evaluate the impact of $L_{edge}$, $L_{dis}$ and $L_{edit}$, we remove them from the supervision one by one during training. As compared in Tab. 4, $L_{edge}$ effectively reduces the reconstruction error, and the use of $L_{dis}$ and $L_{edit}$ is required to enable flexible bone and part shape editing. The ablation study proves that all losses are necessary.\\n\\nEffect of Part-Level Volume Regularization. To analyze the impact of volume regularization, we remove it during the training process. Fig. 7 shows the comparison results of edited bodies. It can be seen that our volume constraint provides strong geometric supervision, resulting in more natural and reasonable editing results.\\n\\n4.4. Applications\\n\\nLatent Space Interpolation. Our representation decouples the bone and shape of each part, which allows us to interpolate meshes by linearly interpolating the bone and shape latent codes. Fig. 8 shows reasonable and meaningful results of such interpolation. It is worth noting that, thanks to the latent space with fine-grained semantics, we can perform mesh interpolation at the component level (see Figs. 8 (b) and (c)).\\n\\nShape Style Transfer. As mentioned in Sec. 3.4.3, we successfully make the norm and direction of shape vectors represent shape size and style, respectively. Thus, we can transfer shape style by swapping the direction of the shape vectors. Some results are shown in Fig. 9. It can be easily observed that our approach changes the shape style of the source mesh naturally and preserves its other attributes (e.g., bone and circumference).\\n\\n5. Conclusion and Discussion\\n\\nConclusion. In this paper, we propose a human body representation with fine-grained semantics and high reconstruction-accuracy in an unsupervised setting. The key idea is to exploit a part-aware skeleton-separated decoupling strategy to establish a correspondence between latent vectors and geometric properties of body parts, which benefits personalized editing of human bodies by modifying the corresponding latent codes. Based on this disentanglement strategy, we propose a bone-guided autoencoder and well-designed losses to learn representation in an unsupervised manner. At last, with the geometrically meaningful latent space, the application can be extended from human body editing to latent code interpolation and shape style transfer.\\n\\nLimitations. Since our decoupling strategy is based on the cylinder assumption (i.e., the shape of body parts can be approximated as cylinders with bones as axes), we cannot controllably edit the bone length and circumference of parts (e.g., faces, hands, and feet) whose geometry is too complex to meet this assumption. Additionally, editing bone orientation with our method may fail when the target orientation is uncommon in the training data. In further work, we will dig deeper into the prior knowledge about the human body to improve the generalization capability of our representation.\\n\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of China (62122058 and 62171317).\"}"}
{"id": "CVPR-2023-1141", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning Semantic-Aware Disentangled Representation for Flexible 3D Human Body Editing\\n\\nXiaokun Sun 1\\nQiao Feng 1\\nXiongzheng Li 1\\nJinsong Zhang 1\\nYu-Kun Lai 2\\nJingyu Yang 1\\nKun Li 1\\n\\n1 Tianjin University, China\\n2 Cardiff University, United Kingdom\\n\\n{sxk26,fengqiao,lxz,jinszhang,yjy,lik}@tju.edu.cn, LaiY4@cardiff.ac.uk\\n\\nFigure 1\\n\\nEffect display\\n\\nEdit Bone Orientation Edit Bone Length Edit Shape Style Edit Shape Size Embedding\\n\\nEmbedding\\n\\nRight arm and right leg\\n\\nSemanticHuman. We present a semantic-aware and editable human body representation with high reconstruction precision in an unsupervised setting. The key idea is to employ a part-aware skeleton-separated decoupling strategy to learn a geometrically meaningful latent space with fine-grained semantics, which benefits part-level personalized human body editing. After embedding a human body into the bone code $Z_b$ and shape code $Z_s$ (left), we can flexibly edit human body attributes by modifying the corresponding latent codes (right).\\n\\nAbstract\\n\\n3D human body representation learning has received increasing attention in recent years. However, existing works cannot flexibly, controllably and accurately represent human bodies, limited by coarse semantics and unsatisfactory representation capability, particularly in the absence of supervised data. In this paper, we propose a human body representation with fine-grained semantics and high reconstruction-accuracy in an unsupervised setting. Specifically, we establish a correspondence between latent vectors and geometric measures of body parts by designing a part-aware skeleton-separated decoupling strategy, which facilitates controllable editing of human bodies by modifying the corresponding latent codes. With the help of a bone-guided auto-encoder and an orientation-adaptive weighting strategy, our representation can be trained in an unsupervised manner. With the geometrically meaningful latent space, it can be applied to a wide range of applications, from human body editing to latent code interpolation and shape style transfer. Experimental results on public datasets demonstrate the accurate reconstruction and flexible editing abilities of the proposed method. The code will be available.\\n\\nCorresponding author\\n\\nat http://cic.tju.edu.cn/faculty/likun/.\\n\\n1. Introduction\\n\\nLearning low-dimensional representations of human bodies plays an important role in various applications including human body reconstruction [4, 19, 32, 37], generation [7, 30, 31] and editing [35, 36, 39]. Existing methods [2, 18, 22, 25, 29] are either too semantically coarse to enable personalized human body editing, or suffer from poor reconstruction performance due to limited representation capability. This paper aims to develop a fine-grained semantic-aware human body representation with flexible representation ability.\\n\\nSince human bodies are rich in variations of poses and shapes, traditional linear models [1, 25, 29, 35, 36] cannot handle complex nonlinear structures of human body meshes accurately. Therefore, parametric models have been proposed for better representation. The landmark works SCAPE [2] and SMPL [22] represent human bodies by the shape and pose parameters. However, the semantics of their shape parameters are not sufficiently precise, making it impossible to flexibly edit the body shape. Furthermore, the...\"}"}
{"id": "CVPR-2023-1141", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"representation ability of these methods is limited by the linear shape space of human body shapes, and hence their reconstruction accuracy is often unsatisfactory. With the success of deep learning, the encoder-decoder architecture has demonstrated excellent representation capability \\\\([7,10,13,14,26]\\\\). Such methods improve the reconstruction precision by constructing different convolution-like operators for feature extraction on irregular meshes. However, these works lack disentangled representation and fail to obtain promising results for geometrically complex human body parts. Several works \\\\([3,9,11,18,38]\\\\) pursue the disentanglement of latent representations, i.e., each latent code has clear semantics. But these methods either require paired supervised data or have poor performance on the reconstruction, which significantly affects their generalization and robustness. In addition, the semantics of the above representations are coarse, which only enables person-level attribute transfer and cannot be applied to part-level flexible editing.\\n\\nIn this paper, we aim to build a human body representation with fine-grained semantics and high reconstruction-accuracy in an unsupervised setting, which needs to overcome two main challenges. First, how to disentangle the human body to reconstruct precise semantics is a key but difficult problem. Although it is straightforward to decompose a human body into articulated parts for part-level editing, the hidden space of each part is still coupled. Secondly, providing paired supervised data requires a lot of manual effort, and it is very challenging to make the representation disentangled without sacrificing reconstruction accuracy in an unsupervised manner.\\n\\nTo address these challenges, we propose SemanticHuman, an editable human body representation with fine-grained semantics and high reconstruction-precision, which facilitates controllable human body editing without paired supervised data. To reconstruct fine-grained semantics, we design a part-aware skeleton-separated decoupling strategy with anatomical priors of the human body. Specifically, we disentangle body part variations into bone-related variations (e.g., length and orientation variations) and bone-independent variations (e.g., circumference variations). In contrast to the previous pose and shape disentanglement on the entire person \\\\([2, 18, 22]\\\\), this part-aware skeleton-separated decoupling strategy establishes a correspondence between latent vectors and geometric properties of body parts, which benefits part-level controllable editing.\\n\\nTo ensure high reconstruction accuracy and fine-grained semantics of the representation by unsupervised learning, we propose a bone-guided autoencoder architecture and an orientation-adaptive geometry-preserving loss. The bone-guided auto-encoder fuses the geometric features of body parts with their joint information to achieve accurate and efficient modeling of human bodies. Besides, an orientation-adaptive weighting strategy is introduced to compute the geometry-preserving loss, which can provide effective geometric regularization for unsupervised disentanglement and part-level editing. Experimental results on two public datasets with different mesh connectivities demonstrate the high reconstruction-precision and controllable editing capability of the proposed method. An example is given in Fig. 1. The code will be available at http://cic.tju.edu.cn/faculty/likun/SemanticHuman.\\n\\nOur main contributions are summarized as follows:\\n\\n\u2022 We propose a semantic-aware and editable human body representation with fine-grained representation ability. The latent space of our approach facilitates personalized editing of human bodies by modifying their latent vectors.\\n\\n\u2022 We propose a part-aware skeleton-separated decoupling strategy exploiting structural priors of the human body to learn geometrically meaningful latent codes with fine-grained semantics.\\n\\n\u2022 We propose a bone-guided auto-encoder architecture and an orientation-adaptive geometry-preserving loss to ensure the robust and effective disentanglement of the representation learned without supervision.\\n\\n2. Related Work\\n\\nClassical Human Parametric Models. Since human bodies are geometric structures with strong priors, it is straightforward to use a statistical parametric model to represent human bodies. SCAPE \\\\([2]\\\\) is one of pioneering works, which models variations between different human bodies as shape-related and pose-related deformations. SMPL \\\\([22]\\\\) represents the human body more accurately and robustly based on vertices instead of triangle deformations. The SMPL has been extended to represent animals \\\\([40]\\\\), hands \\\\([28]\\\\), and a combination of hands and faces \\\\([24]\\\\).\\n\\nDeep Learning for Mesh Analysis. Traditional convolutional neural networks cannot be directly applied to meshes with irregular structures. A series of investigations \\\\([6, 8, 16, 17, 21, 33]\\\\) has been devoted to constructing convolution-like operators on irregular meshes. Ranjan et al. \\\\([26]\\\\) learn nonlinear representations of human faces by applying spectral convolutions to meshes. Additionally, Bouritsas et al. \\\\([7]\\\\) and Gong et al. \\\\([14]\\\\) propose to analyze per-vertex spatial features using spiral convolution. Chen et al. \\\\([10]\\\\) and Gao et al. \\\\([13]\\\\) improve the robustness and efficiency by adopting attention mechanisms in spatial aggregation. Recent works \\\\([30, 31]\\\\) analyze meshes based on as-consistent-as-possible (ACAP) features \\\\([12]\\\\) rather than Euclidean coordinates to learn large-scale deformations on meshes. However, these methods cannot provide explicit...\"}"}
{"id": "CVPR-2023-1141", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. Semantic human consists of a reconstruction branch, a disentanglement branch, and an editing branch. \\n\\n(a) The encoder $E$ maps a mesh $x_1$ into the bone code $Z_{b1} = \\\\{z_{1b1}, ..., z_{Kb1}\\\\}$ and shape code $Z_{s1} = \\\\{z_{1s1}, ..., z_{Ks1}\\\\}$, where $K$ is the number of parts, and the decoder $D$ aims to recover the original mesh $D(Z_{b1}, Z_{s1})$. \\n\\n(b) With the help of $L_{dis}$, the swapped codes $(Z_{b2}, Z_{s1})$ will be fed into $D$ to generate the target mesh $D(Z_{b2}, Z_{s1})$ retaining the skeleton of $x_2$ and shape features of $x_1$. \\n\\n(c) By introducing $L_{edit}$, we force the generated mesh from the scaled codes $(Z_{b1}, \\\\alpha Z_{s1})$ to deform as desired, where $\\\\alpha$ is a scale factor.\\n\\n3. Method\\n\\n3.1. Overview\\n\\nGiven a set of human meshes with consistent connectivity, our goal is to learn a latent representation that has both fine-grained semantics and high reconstruction-precision in an unsupervised setting. In previous unsupervised decoupling works [3, 9, 11, 38], it is a common way to design novel loss functions capturing the shape or pose information to make representations disentangled. Nonetheless, limited by this traditional disentanglement idea focusing on the whole body, these methods are semantically coarse and not robust enough. In contrast, we design a part-aware skeleton-separated decoupling strategy (Sec. 3.2), which not only provides fine-grained semantics for flexible and precise editing of human attributes (e.g., circumference, bone orientation and length) but also facilitates the construction of robust information-preserving losses to achieve unsupervised learning.\\n\\nBased on this decoupling strategy, we introduce a bone-guided encoder-decoder framework (Sec. 3.3) and exploit three losses (Sec. 3.4) to achieve three core tasks by unsupervised learning: accurate geometric reconstruction (Sec. 3.4.1), unsupervised disentanglement (Sec. 3.4.2) and part-level shape editing (Sec. 3.4.3). An overview of our method is illustrated in Fig. 2, three flows of our pipeline complete the corresponding tasks with the help of specific losses.\\n\\n3.2. Part-Level Bone and Shape Disentanglement\\n\\nWe leverage an observation that a human body is composed of $K = 17$ anatomical components each containing a bone defined by joints, as shown in Figs. 3 (a) and (b).\"}"}
{"id": "CVPR-2023-1141", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4 network\\n\\nIn particular, for geometrically simple human body parts such as the waist, arms, and legs, their shapes can be approximated as cylinders with bones as axes, i.e., the geometry of these parts can be modeled as variations along their bone orientations $o_b$, and along their orthogonal directions $o_b \\\\perp$, which are indicated by solid arrows and hollow arrows in Fig. 3 (b).\\n\\nInspired by this observation, we propose a part-aware skeleton-separated decoupling strategy. An overview of our decoupling strategy is shown in Fig. 3 (c). Specifically, we separate body part variations into bone-related variations (e.g., length and orientation variations) and bone-independent shape variations (e.g., size and style variations), which are represented by the bone embedding $z_k^b$ and shape embedding $z_k^s$ for the $k$-th part, respectively.\\n\\nThis part-aware skeleton-separated decoupling strategy establishes a correspondence between latent codes and geometric properties of body parts, which provides fine-grained semantics and enables part-level personalized human body editing.\\n\\n3.3. Bone-Guided Autoencoder\\n\\nThe basic architecture of our framework is shown in Fig. 4. All three tasks are based on this architecture. Given a human body mesh $x$, the bone branch and shape branch of encoder $E$ embed the mesh into the bone code $Z_b = \\\\{z_1^b, ..., z_K^b\\\\}$ and shape code $Z_s = \\\\{z_1^s, ..., z_K^s\\\\}$, respectively, where $z_k^b$ and $z_k^s$ are localized latent codes for the $k$-th body part.\\n\\nIn particular, the bone branch infers localized bone codes $\\\\{z_1^b, ..., z_K^b\\\\}$ with global information (e.g., length and orientation) about each body part from joints $B$ predicted by a linear regressor $J(\\\\cdot)$ [22]. Besides, the shape branch takes mesh $x$ as input with a hierarchical spiral convolution encoder for learning geometric features in multiple scales. Geometric features belonging to each part are subsequently fed into the corresponding fully connected layer according to the part labels of vertices to obtain localized bone codes $\\\\{z_1^s, ..., z_K^s\\\\}$ containing regional geometric details.\\n\\nFinally, the decoder with a similar structure to the shape branch accurately and efficiently reconstructs the original mesh $D(Z_b, Z_s)$ by integrating local and global information of each body part.\\n\\n3.4. Losses for Unsupervised Learning\\n\\nWith the above framework, we utilize three losses to achieve the corresponding tasks in an unsupervised setting, and the full objective function is defined as:\\n\\n$$L_{\\\\text{full}} = L_{\\\\text{rec}} + L_{\\\\text{dis}} + L_{\\\\text{edit}},$$\\n\\n(1)\\n\\nwhere $L_{\\\\text{rec}}$ is a geometric reconstruction loss for accurate human body reconstruction, $L_{\\\\text{dis}}$ is a disentanglement loss to ensure the bone and shape disentanglement of body parts, and $L_{\\\\text{edit}}$ is introduced to enable part-level shape editing. The losses and training flows will be described in detail in the following sections.\\n\\n3.4.1 Accurate Geometric Reconstruction\\n\\nAs shown in Fig. 2 (a), in order to reconstruct the original mesh as accurately as possible, we adopt the geometric reconstruction loss as follows:\\n\\n$$L_{\\\\text{rec}} = L_{\\\\text{vert}} + \\\\lambda_{\\\\text{edge}} \\\\cdot L_{\\\\text{edge}},$$\\n\\n(2)\\n\\nwhere $\\\\lambda_{\\\\text{edge}}$ is the weight of edge regularization. The vertex loss forces the reconstructed mesh $D(E(x))$ to be as close as possible to the original mesh $x$ with the supervision of vertex-wise $L_1$ distance, which is calculated as:\\n\\n$$L_{\\\\text{vert}} = \\\\|x - D(E(x))\\\\|_1.$$  \\n\\n(3)\\n\\nHowever, only using this loss to supervise vertex positions cannot avoid producing over-length edges, which seriously affects the smoothness and reasonableness of results. Inspired by [15,34], we address this problem by introducing an edge length regularization $L_{\\\\text{edge}}$, which is formulated as:\\n\\n$$L_{\\\\text{edge}} = \\\\sum_{p \\\\in X} \\\\sum_{v \\\\in N(p)} \\\\|p - v\\\\|_2^2,$$\\n\\n(4)\\n\\nwhere $N(p)$ is the set of 1-ring neighbors of vertex $p$. This loss ensures the smoothness of output meshes by enforcing their surface to be tight.\\n\\n3.4.2 Unsupervised Disentanglement\\n\\nAfter the reconstruction flow in Sec. 3.4.1, our representation is already capable of accurate reconstruction, but its latent space is still entangled. Based on the part-aware skeleton-separated decoupling strategy (Sec. 3.2), we use $L_{\\\\text{dis}}$ to ensure the bone and shape disentanglement of body parts, which can be defined as:\"}"}
