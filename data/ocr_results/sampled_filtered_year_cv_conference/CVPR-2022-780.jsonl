{"id": "CVPR-2022-780", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GraftNet: Towards Domain Generalized Stereo Matching with a Broad-Spectrum and Task-Oriented Feature\\n\\nBiyang Liu, Huimin Yu, Guodong Qi\\n\\n1 College of Information Science and Electronic Engineering, Zhejiang University\\n2 ZJU-League Research & Development Center\\n3 State Key Lab of CAD&CG, Zhejiang University\\n4 Zhejiang Provincial Key Laboratory of Information Processing, Communication and Networking\\n\\nEmail: {biyangliu, yhm2005, guodongqi}@zju.edu.cn\\n\\nAbstract\\n\\nAlthough supervised deep stereo matching networks have made impressive achievements, the poor generalization ability caused by the domain gap prevents them from being applied to real-life scenarios. In this paper, we propose to leverage the feature of a model trained on large-scale datasets to deal with the domain shift since it has seen various styles of images. With the cosine similarity based cost volume as a bridge, the feature will be grafted to an ordinary cost aggregation module. Despite the broad-spectrum representation, such a low-level feature contains much general information which is not aimed at stereo matching. To recover more task-specific information, the grafted feature is further input into a shallow network to be transformed before calculating the cost. Extensive experiments show that the model generalization ability can be improved significantly with this broad-spectrum and task-oriented feature. Specifically, based on two well-known architectures PSMNet and GANet, our methods are superior to other robust algorithms when transferring from SceneFlow to KITTI 2015, KITTI 2012, and Middlebury. Code is available at https://github.com/SpadeLiu/Graft-PSMNet.\\n\\n1. Introduction\\n\\nAs a low-cost means to acquire depth, stereo matching has been studied as a fundamental problem in the vision society for decades. Given a rectified image pair, the objective is to search for the corresponding points and calculate their disparities. Stereo matching algorithms generally involve four steps [27]: matching cost computation, cost aggregation, disparity optimization, and disparity refinement.\\n\\nAlthough Convolutional Neural Network (CNN) based supervised stereo matching methods have achieved admirable performances, huge amounts of annotated data are required to train the models, which is cumbersome for real-life applications. Synthetic data [20] is sufficient while the domain gap between the source and the target images prevents the models from generalizing well. There are three solutions to this issue: the unsupervised image reconstruction loss [34, 35], domain adaptation techniques [18, 31], and domain generalized approaches [2, 45]. In this paper, we focus on the third situation which is more challenging since the target images are not available during training.\\n\\nIn domain generalized stereo matching, feature representation plays a crucial role [45] since the feature extraction module directly confronts images from different domains. Then a question is raised: can the goal be achieved by replacing the feature extraction module of an ordinary stereo matching network (ordinary means it is trained with synthetic data) with a broad-spectrum feature (i.e., the feature of a model trained on large-scale datasets)? Since this feature has seen various styles of images and learns to generalize well. In traditional algorithms, various feature descriptors [13] and cost aggregation methods [12, 42] could be\"}"}
{"id": "CVPR-2022-780", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"combined with each other to use. However in deep frame-\\nworks, the parameterized modules are entangled through\\nend-to-end training, is this grafting operation (i.e.\\ncombin-\\ning two trained modules without finetuning) practical?\\nTo answer this question, we first conduct a toy experi-\\nment. With PSMNet [3] as the basic architecture, we train\\na model on a synthetic dataset SceneFlow [20], then its fea-\\nture extraction module is replaced with the feature of VGG\\n[30] trained on ImageNet [7]. Finally, the cross-domain per-\\nformances are evaluated on KITTI 2015 [21]. As illustrated\\nin Subfigure (c) and (d) of Figure 1, simply grafting a broad-\\nspectrum feature to an ordinary cost aggregation module\\nleads to a collapse of the disparity result. We analyze this\\nis caused by the feature concatenation based cost volume,\\nwhich forces the cost aggregation module to learn to mea-\\nsure the similarity based on the feature. When the feature is\\nreplaced, the learned metric will not be effective.\\nIn order to disentangle the feature extraction module and\\nthe cost aggregation module, it is necessary to construct a\\ngeneralized cost space [2]. On one hand, the cost volume\\nshould contain pure similarity information. In this way,\\nthe prior knowledge about the similarity metric is injected,\\npreventing the cost aggregation module from overfitting to\\nthe used feature. Besides, the semantical information [10]\\nwhich may interfere with cost aggregation due to the varied\\nsemantic classes of different domains is discarded. On the\\nother hand, integrating the normalization of the cost value\\nis beneficial for the generalization ability [31]. To this end,\\nwe utilize the elegant cosine similarity to construct the cost\\nvolume. In addition to satisfying the above demands, co-\\nsine similarity projects features with arbitrary channels to a\\nscalar, making the cost accessible for various features.\\nOwing to the generalized cost space, when the feature\\nextraction module trained with synthetic data is replaced\\nwith a broad-spectrum feature, the cross-domain perfor-\\nance is improved significantly, as shown in Subfigure (e)\\nand (f) of Figure 1. This also experimentally validates that a\\nbroad-spectrum feature can be employed to handle the do-\\nmain shift. However, grafting such a low-level feature of\\nthe classification model is still suboptimal since it contains\\nmuch general information that serves various tasks. It is\\nnecessary to adapt the grafted feature to our stereo matching\\ntask. Inspired by the researches in multi-task learning [15]\\nand transfer learning [24], we build a shallow network and\\nforce it to recover more task-specific information from the\\ngrafted feature. Although this training process is conducted\\non the source domain, the feature adaptor is robust since its\\ninput, the broad-spectrum feature has weakened the influ-\\nence of the image style. Besides, the small amount of the\\nparameters will reduce the risk of overfitting [38].\\nIn summary, there are two fundamental steps in our\\ndomain generalized stereo matching network GraftNet.\\nFirstly, grafting a broad-spectrum feature (i.e. the feature\\nof a model trained on large-scale datasets) to the cost ag-\\ngregation module of an ordinary stereo matching network.\\nSecondly, transforming the feature with a shallow network\\nto recover the task-specific information. In practice, we\\nfind retraining the cost aggregation module with this trans-\\nformed feature can further improve the performance. It\\nis worth noting that our method can be built upon arbi-\\ntrary stereo matching networks, the only modification is to\\nconstruct the cost volume with cosine similarity. Without\\nbells and whistles, our models based on PSMNet [3] and\\nGANet [44] are superior to other robust and domain gener-\\nalized algorithms when transferring from a synthetic dataset\\nSceneFlow [20] to some realistic datasets such as KITTI\\n2015 [21], KITTI 2012 [8], and Middlebury [26].\\n\\n2. Related Work\\n2.1. Deep Stereo Matching Networks\\nMC-CNN [43] firstly introduced CNN to stereo match-\\ning, where a siamese network was built to compute the\\nmatching cost of two patches. The subsequent studies in-\\nvolved multi-scale feature representation [5] and the accel-\\neration of the similarity calculation [19]. Although a deep\\nembedding is powerful, these works are limited by the tra-\\nditional cost aggregation and disparity refinement steps.\\nDispNetC [20] was the first end-to-end stereo match-\\ning network, where the disparity was regressed from the\\ncorrelation maps through 2D convolutions. This pipeline\\nhas been widely adopted since then. SegStereo [41] and\\nEdgeStereo [32] designed multi-task frameworks to exploit\\nthe semantic clues and the edge information. AANet [40]\\nintegrated deformable convolution to adaptively aggregate\\nthe cost. Despite the low complexities, the performances of\\n2D-CNN based stereo matching networks are not superior.\\nAnother common fashion is to build the cost volume by\\nconcatenating the left and the right features and aggregate\\nthe cost with 3D convolutions, which is initially proposed\\nin GCNet [14]. PSMNet [3] further introduced the spa-\\ntial pyramid pooling module to deal with textureless re-\\ngions. For more effective cost aggregation, image content\\nguided layers were designed in GANet [44]. Currently,\\nLEAStereo [6], an architecture searched by a deep network,\\nranks top on the KITTI benchmarks. While 3D-CNN based\\nstereo matching networks perform well on several datasets,\\nthe poor generalization abilities hinder their applications in\\nreal-life scenes. In this work, we show how to alleviate this\\nproblem with a domain-invariant and task-oriented feature.\\n\\n2.2. Domain Generalized Stereo Matching\\nIn domain generalized stereo matching, the model is ag-\\nnostic to the image style of the target domain, thus it is\\nmore challenging than domain adaptation [18,31,34,35]. To\\nachieve the goal, DSMNet [45] proposed domain normal-\\n\"}"}
{"id": "CVPR-2022-780", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The overall architecture of GraftNet, consisting of a broad-spectrum feature, a feature adaptor, and a cost aggregation module. To build GraftNet, we first graft the feature from a classic model trained on the large-scale dataset ImageNet to the cost aggregation module of an ordinary stereo matching network. Then the feature is input to a shallow U-shape network to be transformed to recover more task-related information. Note the cost volume is formed by cosine similarity rather than feature concatenation to obtain a generalized cost space.\\n\\nOur method is most similar to MS-Net [2], where traditional descriptors are utilized to construct the cost. However, in our work, the generalized matching space is realized with a deep feature which is more discriminative.\\n\\nIn addition, there are researchers devoted to tackling the problem from other perspectives. Poggi et al. [23] modulated the cost distribution with the sparse depth measurements obtained from some devices. Watson et al. [39] proposed an approach to generate labeled data from single images and showed that models trained on their MfS transferred better than those trained on SceneFlow [20].\\n\\n2.3. Broad-Spectrum Features\\n\\nThe features of the classic architectures (e.g., ResNet [11], VGG [30]) trained on ImageNet [7] have been widely utilized to initialize the model parameters in several tasks [4, 9]. These pretrained classic models can be easily loaded from the libraries, e.g., PyTorch [22]. In our work, the feature is leveraged to obtain a robust representation since ImageNet covers various domains. To preserve the property of the feature, we keep the parameters fixed and build a network to transform the feature [15, 24].\\n\\n3. Method\\n\\nIn this section, we will describe how to build our domain generalized stereo matching model GraftNet, whose key component is a broad-spectrum and task-oriented feature.\\n\\n3.1. Stereo Matching Network\\n\\nIn a typical deep stereo matching network [3, 6, 14, 44], the left and the right images are first passed through the feature extraction module, then a cost volume is constructed by concatenating the left and the right features at different displacements. After that, the cost is aggregated with several 3D convolutions, followed by softmax and weighted average to calculate the final disparity.\\n\\nAs demonstrated in [45], feature representation plays a crucial role in the generalization ability of the model. To this end, we intend to achieve the domain generalized stereo matching with a broad-spectrum feature. At the same time, the other parameterized part of a stereo matching network, the cost aggregation module, can only be trained with the synthetic data. Therefore, it is necessary to construct a generalized cost space [2] to disentangle the feature extraction module and the cost aggregation module.\\n\\nIn our model, the elegant cosine similarity is utilized to build the cost volume. Compared with feature concatenation, it has three advantages: 1) The semantical information [10] which is susceptible to the domain shift is eliminated, resulting in a cost volume with pure similarity information; 2) the capacity of the network is reduced; 3) the training process is more efficient.\"}"}
{"id": "CVPR-2022-780", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"formation. 2) The normalization ensures the numerical sta-\\n\\nbility of the cost values, which is beneficial for the cross-\\n\\ndomain evaluation performance [31, 45]. 3) Features with\\n\\narbitrary channels could be taken as the input since all of\\n\\nthem will be projected to a scalar. Formally, the cosine sim-\\n\\nilarity cost volume is expressed as:\\n\\n$$CV_{\\\\cos}(i, d, x, y) = \\\\langle F_l(i, x, y), F_r(i, x - d, y) \\\\rangle / \\\\|F_l(i, x, y)\\\\|^2 \\\\cdot \\\\|F_r(i, x - d, y)\\\\|^2 (1)$$\\n\\nwhere $d$ is the disparity index, and $(x, y)$ denotes the pixel\\n\\ncoordinate. $F_l$ and $F_r$ are the left and the right features,\\n\\nboth with $C$ channels. The calculated cost is a 4D tensor\\n\\nwith only one channel, thus the input channel of the first 3D\\n\\nconvolutional layer of the cost aggregation module should\\n\\nbe modified to 1.\\n\\nWithout other adjustments, this basic stereo matching\\n\\nnetwork is trained on the source domain with the cross en-\\ntropy loss [36] and the smooth $L_1$ loss [3] to supervise the\\n\\ndisparity probability distributions and the final disparity val-\\n\\nues respectively:\\n\\n$$L_{\\\\text{ce}}(\\\\hat{P}(d), P(d)) = 1 / N \\\\sum_{i=1}^{N} d_{\\\\text{max}} \\\\sum_{d=0}^{d_{\\\\text{max}}} - \\\\hat{P}_i(d) \\\\cdot \\\\log P_i(d) (2)$$\\n\\n$$L_{\\\\text{sm}}(\\\\hat{D}, D) = 1 / N \\\\sum_{i=1}^{N} \\\\text{smooth} L_1(\\\\hat{D}_i, D_i) (3)$$\\n\\nwhere $\\\\hat{P}(d)$ is the predicted distribution from\\n\\nsoftmax and $P(d)$ is the ground truth distribution, a normalized Lapla-\\ncian distribution centered at the disparity ground truth $D$.\\n\\n$\\\\hat{D}$ is the predicted disparity calculated by\\n\\nweighted average. $N$ denotes the number of the pixels in an image.\\n\\nNormally, there are multiple disparity results output\\n\\nfrom the cost aggregation module in the training phase [3, 10, 44]. In our model, each result is supervised with the above two loss functions, then the total loss is:\\n\\n$$L = \\\\sum_{m=1}^{M} \\\\lambda_m (L_{\\\\text{ce}} + \\\\mu L_{\\\\text{sm}}) (4)$$\\n\\nwhere $M$ is the number of the disparity outputs. As for the\\n\\nbalance weights, $\\\\lambda_m$ is set as same as in the adopted basic\\n\\narchitecture, and $\\\\mu$ is set to 0.1 heuristically.\\n\\nAfter training, the feature extraction module is discarded\\n\\nsince it is susceptible to the domain shift, while the cost\\n\\naggregation module is reserved for grafting other features.\\n\\nOwing to the generalized cost space, the cost aggregation\\n\\nmodule is less affected by the domain gap.\\n\\n3.2. Broad-Spectrum and Task-Oriented Feature\\n\\nIn this work, we employ the feature of a model trained\\n\\non large-scale datasets to resist the domain shift since it has\\n\\nseen various styles of images and has learned to general-\\n\\nize well. In the meanwhile, such a feature is easy to ac-\\n\\nquire, e.g. the classic models [11, 30] trained on ImageNet\\n\\n[7] can be directly loaded from the PyTorch library [22].\\n\\nRather than utilizing the pretrained parameters to initialize\\n\\nthe model backbones [4, 9], we keep the module fixed to\\n\\npreserve the inherent property of the feature.\\n\\nSpecifically, the broad-spectrum feature will be grafted\\n\\nto the trained ordinary cost aggregation module in Section\\n\\n3.1. To keep consistency, we adopt the feature that has the\\n\\nsame resolution as the one used in the original stereo match-\\ning network. For example, if the basic architecture is PSM-\\n\\nNet [3] and the grafted feature is from VGG [30], then the\\n\\nfeature before the third pooling layer that has the quarter\\n\\nresolution of the image will be employed.\\n\\nAlthough the influence of the domain shift has been\\n\\nweakened by a broad-spectrum feature, a simple grafting\\n\\noperation is ill-considered. The reason is that the feature is\\n\\nrelatively low-level, containing much general information\\n\\nthat serves various downstream tasks. It is necessary to ex-\\n\\ntract more information specific to our stereo matching task.\\n\\nTo this end, we build a feature adaptor before calculating\\n\\nthe cost, i.e. the shallow U-shape network [25] illustrated\\n\\nin Figure 2 (c). The feature adaptor is trained as a part of\\n\\nthe stereo matching network, in which process the param-\\n\\neters of the broad-spectrum feature and the cost aggrega-\\n\\ntion module are fixed and only serve as the intermediums\\n\\nto propagate the gradients. Although this training process\\n\\nis conducted on the source domain, the feature adaptor is\\n\\neffective on the target domain for two reasons: 1) Its input\\n\\nis a broad-spectrum representation which will weaken the\\n\\ninfluence of the image style. 2) The small amount of the\\n\\nparameters will reduce the risk of overfitting [38].\\n\\n3.3. GraftNet\\n\\nWith the broad-spectrum and task-oriented feature out-\\n\\nput from the feature adaptor, we find retraining the cost ag-\\n\\ngregation module can further improve the performance. In\\n\\nthis step, our method is similar to [2], i.e. constructing a\\n\\ngeneralized matching space and training a cost aggregation\\n\\nmodule with the synthetic data. However, experimental re-\\n\\nsults in Section 4.5 show that an appropriate deep feature is\\n\\nmore representative than traditional descriptors [2].\\n\\nFrom the perspective of the model architecture, GraftNet\\n\\nconsists of three components: a broad-spectrum feature, a\\n\\nfeature adaptor, and a cost aggregation module. Although\\n\\nwe are inspired by the toy grafting experiment in Figure\\n\\n1, can the feature adaptor and the cost aggregation module\\n\\nbe trained together? In practice, we find jointly training is\\n\\nnot as effective as separately training (Please refer to the\\n\\nsupplementary material). We conjecture that when the two\\n\\nmodules are optimized individually, a trained module can\\n\\nprovide a beneficial initialization for the other one.\"}"}
{"id": "CVPR-2022-780", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Filippo Aleotti, Fabio Tosi, Pierluigi Zama Ramirez, Matteo Poggi, Samuele Salti, Stefano Mattoccia, and Luigi Di Stefano. Neural disparity refinement for arbitrary resolution stereo. arXiv preprint arXiv:2110.15367, 2021.\\n\\n[2] Changjiang Cai, Matteo Poggi, Stefano Mattoccia, and Philippos Mordohai. Matching-space stereo networks for cross-domain generalization. In 2020 International Conference on 3D Vision (3DV), pages 364\u2013373. IEEE, 2020.\\n\\n[3] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5410\u20135418, 2018.\\n\\n[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848, 2017.\\n\\n[5] Zhuoyuan Chen, Xun Sun, Liang Wang, Yinan Yu, and Chang Huang. A deep visual correspondence embedding model for stereo matching costs. In Proceedings of the IEEE International Conference on Computer Vision, pages 972\u2013980, 2015.\\n\\n[6] Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao Dai, Xiaojun Chang, Tom Drummond, Hongdong Li, and Zongyuan Ge. Hierarchical neural architecture search for deep stereo matching. arXiv preprint arXiv:2010.13501, 2020.\\n\\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\\n\\n[8] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3354\u20133361. IEEE, 2012.\\n\\n[9] Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3828\u20133838, 2019.\\n\\n[10] Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, and Hongsheng Li. Group-wise correlation stereo network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3273\u20133282, 2019.\\n\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[12] Heiko Hirschmuller. Accurate and efficient stereo processing by semi-global matching and mutual information. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), volume 2, pages 807\u2013814. IEEE, 2005.\\n\\n[13] Heiko Hirschmuller and Daniel Scharstein. Evaluation of cost functions for stereo matching. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20138. IEEE, 2007.\\n\\n[14] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry. End-to-end learning of geometry and context for deep stereo regression. In Proceedings of the IEEE International Conference on Computer Vision, pages 66\u201375, 2017.\\n\\n[15] Wei-Hong Li and Hakan Bilen. Knowledge distillation for multi-task learning. In European Conference on Computer Vision, pages 163\u2013176. Springer, 2020.\\n\\n[16] Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding, Francis X Creighton, Russell H Taylor, and Mathias Unberath. Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6197\u20136206, 2021.\\n\\n[17] Lahav Lipson, Zachary Teed, and Jia Deng. Raft-stereo: Multilevel recurrent field transforms for stereo matching. arXiv preprint arXiv:2109.07547, 2021.\\n\\n[18] Rui Liu, Chengxi Yang, Wenxiu Sun, Xiaogang Wang, and Hongsheng Li. Stereogan: Bridging synthetic-to-real domain gap by joint optimization of domain translation and stereo matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12757\u201312766, 2020.\\n\\n[19] Wenjie Luo, Alexander G Schwing, and Raquel Urtasun. Efficient deep learning for stereo matching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5695\u20135703, 2016.\\n\\n[20] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4040\u20134048, 2016.\\n\\n[21] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3061\u20133070, 2015.\\n\\n[22] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\\n\\n[23] Matteo Poggi, Davide Pallotti, Fabio Tosi, and Stefano Mattoccia. Guided stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 979\u2013988, 2019.\\n\\n[24] Pierluigi Zama Ramirez, Alessio Tonioni, Samuele Salti, and Luigi Di Stefano. Learning across tasks and domains. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8110\u20138119, 2019.\\n\\n[25] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.\"}"}
{"id": "CVPR-2022-780", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Daniel Scharstein, Heiko Hirschm\u00fcller, York Kitajima, Greg Krathwohl, Nera Ne\u0161i\u0107, Xi Wang, and Porter Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In German conference on pattern recognition, pages 31\u201342. Springer, 2014.\\n\\nDaniel Scharstein and Richard Szeliski. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. International journal of computer vision, 47(1):7\u201342, 2002.\\n\\nThomas Sch\u00f6ps, Johannes L. Sch\u00f6nberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and multi-camera videos. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\nZhelun Shen, Yuchao Dai, and Zhibo Rao. Cfnet: Cascade and fused cost volume for robust stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13906\u201313915, 2021.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\\n\\nXiao Song, Guorun Yang, Xinge Zhu, Hui Zhou, Zhe Wang, and Jianping Shi. Adastereo: a simple and efficient approach for adaptive stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10328\u201310337, 2021.\\n\\nXiao Song, Xu Zhao, Liangji Fang, Hanwen Hu, and Yizhou Yu. Edgestereo: An effective multi-task learning network for stereo matching and edge detection. International Journal of Computer Vision, 128(4):910\u2013930, 2020.\\n\\nZachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pages 402\u2013419. Springer, 2020.\\n\\nAlessio Tonioni, Oscar Rahnama, Thomas Joy, Luigi Di Stefano, Thalaiyasingam Ajanthan, and Philip HS Torr. Learning to adapt for stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9661\u20139670, 2019.\\n\\nAlessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mottoccia, and Luigi Di Stefano. Real-time self-adaptive deep stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 195\u2013204, 2019.\\n\\nStepan Tulyakov, Anton Ivanov, and Fran\u00e7ois Fleuret. Practical deep stereo (pds): Toward applications-friendly deep stereo matching. Advances in Neural Information Processing Systems, 31:5871\u20135881, 2018.\\n\\nXinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3024\u20133033, 2021.\\n\\nYaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning. ACM Computing Surveys (CSUR), 53(3):1\u201334, 2020.\\n\\nJamie Watson, Oisin Mac Aodha, Daniyar Turmukhambetov, Gabriel J Brostow, and Michael Firman. Learning stereo from single images. In European Conference on Computer Vision, pages 722\u2013740. Springer, 2020.\\n\\nHaofei Xu and Juyong Zhang. Aanet: Adaptive aggregation network for efficient stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1959\u20131968, 2020.\\n\\nGuorun Yang, Hengshuang Zhao, Jianping Shi, Zhidong Deng, and Jiaya Jia. Segstereo: Exploiting semantic information for disparity estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 636\u2013651, 2018.\\n\\nKuk-Jin Yoon and In So Kweon. Adaptive support-weight approach for correspondence search. IEEE transactions on pattern analysis and machine intelligence, 28(4):650\u2013656, 2006.\\n\\nJure Zbontar and Yann LeCun. Computing the stereo matching cost with a convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1592\u20131599, 2015.\\n\\nFeihu Zhang, Victor Prisacariu, Ruigang Yang, and Philip HS Torr. Ga-net: Guided aggregation net for end-to-end stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 185\u2013194, 2019.\\n\\nFeihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor Prisacariu, Benjamin Wah, and Philip Torr. Domain-invariant stereo matching networks. In European Conference on Computer Vision, pages 420\u2013439. Springer, 2020.\"}"}
{"id": "CVPR-2022-780", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Quantitative results of the ablation experiment. PSMNet and GANet-11 are the used two basic architectures. Models are trained on SceneFlow and evaluated on four realistic datasets. CV represents cost volume and CA denotes the cost aggregation module.\\n\\nSince grafting is the first and fundamental step in the whole pipeline, our domain generalized stereo matching network is termed GraftNet. Moreover, we wish the grafting operation could provide a novel viewpoint: Can parts of two trained CNNs be integrated without finetuning to obtain a new model? This question is worth exploring, especially for scenarios where training data is not available.\\n\\n4. Experiment\\n\\n4.1. Datasets & Evaluation Metrics\\n\\nSource Domain. In the experiment, all of the stereo matching networks are trained on SceneFlow [20], a synthetic dataset containing 35454 training pairs and 4370 testing pairs, both with dense disparity ground truth. Since only the generalization ability is concerned in the domain generalized problem, the test set will not be used.\\n\\nTarget Domain. The models trained on SceneFlow are evaluated on the following realistic datasets:\\n\\n- KITTI datasets consist of KITTI 2015 [21] and KITTI 2012 [8], whose ground truth disparity maps are sparse. On KITTI 2015, there are 200 training pairs and 200 testing pairs. On KITTI 2012, there are 194 training pairs and 195 testing pairs.\\n- Middlebury 2014 [26] provides 15 training pairs and 15 testing pairs, where some samples are under inconsistent illumination or color conditions. All of the images are available in three different resolutions, we select the half-resolution ones.\\n- ETH3D [28] is a gray-scale dataset with 27 training pairs and 20 testing pairs.\\n\\nFor all the realistic datasets, we use their training sets to evaluate the cross-domain performance. The utilized metrics are EPE (End Point Error, the mean average error) and Cost Normal. P. Simi. EPE (px) > 3px Concat % % 3.24 19.5% Concat ! % 3.14 18.3% L2 Distance % ! 2.86 Cosine Simi. ! ! 2.98 15.4%\\n\\nTable 2. Effects of the manner of building the cost volume. Normal.: whether the features are normalized before building the cost. P. Simi.: whether the cost contains pure similarity information. N Concat means the cost is formed by concatenating the normalized features. Results are evaluated on KITTI 2015.\\n\\n\u03c4-pixel error rate (percentage of the points with absolute error larger than \u03c4 pixel).\\n\\n4.2. Implementation Details\\n\\nThe framework is implemented on PyTorch [22], with Adam (\u03b2\u2081 = 0.9, \u03b2\u2082 = 0.999) as the optimizer. For the basic stereo matching architecture, we train it for 8 epochs with a learning rate of 0.001. Then a broad-spectrum feature is grafted to the cost aggregation module, in which process no training is involved. After grafting, the feature adaptor is trained for 1 epoch with a learning rate of 0.001. Finally, the cost aggregation module is retrained for 10 epochs with the learning rate set as 0.001 for the first 5 epochs and 0.0001 for the remaining epochs.\\n\\nFor all experiments, PSMNet [3] is adopted as the basic architecture. In the ablation study (Section 4.3) and the comparison experiment with other robust algorithms (Section 4.5), GANet-11 [44] is additionally utilized to demonstrate the effectiveness and versatility of our method. The grafted feature is from VGG16 [30] which is trained on ImageNet [7], and in Section 4.4 more features are explored.\"}"}
{"id": "CVPR-2022-780", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Illustration of the adapted features. For each image, the top row shows four channels of the features before adaption (i.e., VGG's feature) and the bottom row shows four channels of the features after adaption. Images are from KITTI 2015.\\n\\n4.3. Ablation Study\\n\\nIn this section, we study the effects of the components in GraftNet, the evaluation results are listed in Table 1. First of all, although feature concatenation is commonly used to build the cost in supervised frameworks, cosine similarity is more suitable for domain generalized stereo matching. There are two reasons: 1) the normalization keeps the values stable, 2) the semantical information which is susceptible to the domain shift is discarded.\\n\\nFor the purpose of deeply investigating the influences of these two aspects, we compare several manners of building the cost in Table 2. Results show that a cost volume containing pure similarity information (e.g., calculated by cosine similarity or $L_2$ distance) is better when generalization ability is considered. We also emphasize that cosine similarity allows us to graft features from other models. If the cost volume is constructed by feature concatenation, the disparity result of the assembled model will be collapsed.\\n\\nFrom the 2nd and the 3rd rows of the two subtables in Table 1, grafting the feature of VGG trained on ImageNet is beneficial for KITTI datasets. This indicates the overfitting problem of current stereo matching networks does exist, and a broad-spectrum feature can improve the generalization ability. However, this feature does not work for Middlebury (when equipped with GANet) and ETH3D. We analyze though a domain-generalized representation is obtained, it does not fit the task, the low-level feature from the classification model contains seldom detailed information. Coincidentally, on KITTI, the ground truth is sparse especially at the disparity discontinuities, thus a feature with more global context information is effective as well.\\n\\nTo restore the task-specific information, we build a shallow network to transform the broad-spectrum feature. From the 4th and the 9th rows of Table 1, the evaluation performances on Middlebury and ETH3D are improved significantly with the feature adaptor. This result suggests that both the broad-spectrum property and the task-oriented property of the feature is important. In Figure 3, we further exhibit the features before and after adaption. As it can be seen, rich texture information which is essential for stereo matching is recovered with the adaptor.\\n\\nIn Table 3, we compare different architectures of the feature adaptor. Although a linear layer is enough in [15], in our work a more complex network is needed since not only the task gap but also the feature level should be considered. In the meanwhile, the parameter number of the adaptor can not be too large to prevent overfitting to the source data. Therefore, a shallow U-shape network [25] is adopted.\\n\\nFinally, in Table 1, retraining the cost aggregation module with the adapted feature can further improve the evaluation performance. The reason might be that compared with the original feature trained on the source domain, the broad-spectrum and task-oriented feature provides a more robust cost volume, guiding the optimization of the cost aggregation module towards the goal of domain generalized stereo matching. Some qualitative results of our final model on the four realistic datasets are displayed in Figure 4.\\n\\n4.4. Grafting Various Features\\n\\nIn this section, we attempt to graft various features to further investigate the impact of the feature. Six features are adopted: VGG16 [30], ResNet18 [11], and ResNet50 [11] trained for Classification (C) on ImageNet, ResNet18 trained for Monocular Depth Estimation (MDE) [9] on...\"}"}
{"id": "CVPR-2022-780", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Experimental results of grafting features from various models.\\n\\n| Model | Recognition & Tracking | KITTI 2015 | KITTI 2012 | Middlebury | ETH3D |\\n|-------|------------------------|------------|------------|------------|-------|\\n| VGG16 | Classification          | 1.86       | 6.39%      |            |       |\\n| ResNet18 | Classification        | 1.90       | 6.62%      |            |       |\\n| ResNet18 | Monocular Depth Estimation | 1.73      | 6.54%      |            |       |\\n| ResNet50 | Classification        | 2.06       | 6.19%      |            |       |\\n| ResNet50 | Dense Contrastive Learning | 2.20      | 9.17%      |            |       |\\n| ResBlocks | Optical Flow Estimation | 1.59       | 6.22%      |            |       |\\n\\nResults are evaluated on KITTI 2015, the best is shown in bold and the second is underlined.\\n\\nKITTI, ResNet50 trained by Dense Contrastive Learning (DCL) [37] on ImageNet, stacked ResBlocks trained for Optical Flow Estimation (OFE) [33] on KITTI.\\n\\nThe qualitative results evaluated on KITTI 2015 are presented in Table 4. Comparing the 2nd row and the 3rd row, a broad-spectrum feature performs close to a domain-specific feature, meaning that the domain shift can be handled with the feature of a model trained on large-scale datasets. From the 3rd and the 5th rows, although MDE and DCL are dense prediction tasks, their features cannot satisfy the needs of stereo matching. The feature trained for a closer task OFE might be more helpful, while there is still a performance gap between it and the feature used in our model (6.22% vs 5.60%). These conclusions once again stress the importance of the task-oriented property of the feature.\\n\\nIn addition, considering image classification models and stereo matching models are usually trained with different input resolutions, and the input resolution is vital for the pixelwise task stereo matching, we study the effect of the input resolution when training the broad-spectrum feature. Please refer to the supplementary material for more results.\\n\\n4.5. Comparison with Robust Algorithms\\n\\nIn this section, we compare our models with other robust and domain generalized methods. As reported in [16, 39], augmenting images with a random color and brightness transform can improve the model generalization ability. Therefore, for a fair comparison, the algorithms are separated into two categories according to whether data augmentation strategies including color jitter are involved.\\n\\nAs shown in Table 5, among the methods that do not utilize the random color transform strategy, our Graft-PSMNet and Graft-GANet are superior, especially on KITTI and Middlebury. When integrating more data augmentation approaches, the model performance can be further boosted. On ETH3D, our models are not the best, we analyze the reason is that ImageNet contains few grayscale images, making the grafted feature difficult to express well on ETH3D. This inspires us that more styles of images should be collected for an absolutely domain-invariant representation.\\n\\n5. Limitation & Future Work\\n\\nThere are two main limitations of our work: 1) As discussed in Section 4.5, the grafted feature is not perfectly domain-invariant. 2) The annotations for image classification are implicitly used through loading the parameters of the model trained on ImageNet, meaning that additional labeled data (but not limited to stereo matching) are needed. Aiming at these limitations, we intend to deeply combine self-supervised representation learning with our GraftNet in the future. By this means, only images are required and huge amounts of data from the Internet can be leveraged to improve the robustness of the learned feature.\\n\\n6. Conclusion\\n\\nThis paper attempts to achieve domain generalized stereo matching from the perspective of data, where the key is a broad-spectrum and task-oriented feature. The former property comes from the various styles of images seen during training, and the latter property is realized by recovering task-related information from the broad-spectrum feature. By constructing a generalized cost space with cosine similarity, the feature is combined with an ordinary cost aggregation module. Experimental results on several datasets show that our Graft-PSMNet and Graft-GANet are superior to other robust and domain generalized algorithms. We hope our method can inspire subsequent studies, including multi-task learning and domain generalized approaches.\"}"}
{"id": "CVPR-2022-780", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative results of Graft-PSMNet and Graft-GANet when transferring from SceneFlow to KITTI 2015, KITTI 2012, Middlebury, and ETH3D (from top to bottom). Best viewed in color.\\n\\nSocietal Impact\\n\\nThe fundamental purpose of our work is to improve the quality of the depth obtained by stereo systems, which plays an essential role in many industries such as robot navigation and autonomous driving. On the other hand, the developments of these industries bring huge convenience to human activities. On the other hand, there are still lots of questions to be answered about security, emotion, etc. To handle these potential problems, we must not only carefully evaluate the security of the AI (Artificial Intelligence) systems, but also establish and complete some related laws.\"}"}
