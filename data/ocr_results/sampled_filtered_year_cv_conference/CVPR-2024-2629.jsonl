{"id": "CVPR-2024-2629", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"object from any possible viewpoint directly with our network. To this end, we propose an autoregressive image generation scheme, i.e., we condition the generation of next viewpoints on previously generated images. We provide the timesteps \\\\( t_0: N \\\\) of each image as input to the U-Net. By varying \\\\( t_0: N \\\\), we can achieve different types of conditioning.\\n\\nUnconditional Generation. All samples are initialized to Gaussian noise and are denoised jointly. The timesteps \\\\( t_0: N \\\\) are kept identical for all samples throughout the reverse process. We provide different cameras per image and a single text prompt. The generated images are 3D-consistent, showing the object from the desired viewpoints (Figs. 4 and 5).\\n\\nImage-Conditional Generation. We divide the total number of samples \\\\( N = n_c + n_g \\\\) into a conditional part \\\\( n_c \\\\) and generative part \\\\( n_g \\\\). The first \\\\( n_c \\\\) samples correspond to images and cameras that are provided as input. The other \\\\( n_g \\\\) samples should generate novel views that are similar to the conditioning images. We start the generation from Gaussian noise for the \\\\( n_g \\\\) samples and provide the un-noised images for the other samples. Similarly, we set \\\\( t_0: n_c = 0 \\\\) for all denoising steps, while gradually decreasing \\\\( t_{n_c: N} \\\\).\\n\\nWhen \\\\( n_c = 1 \\\\), our method performs single-image reconstruction (Fig. 6). Setting \\\\( n_c > 1 \\\\) allows to autoregressively generate novel views from previous images (Fig. 1 bottom). In practice, we first generate one batch of images unconditionally and then condition the next batches on a subset of previous images. This allows us to render smooth trajectories around 3D objects (see the supplemental material).\\n\\n3.4. Implementation Details\\n\\nDataset. We train our method on the large-scale CO3Dv2 \\\\([31]\\\\) dataset, which consists of posed multi-view images of real-world objects. Concretely, we choose the categories Teddybear, Hydrant, Apple, and Donut. Per category, we train on 500\u20131000 objects with each 200 images at resolution 256 \\\\( \\\\times \\\\) 256. We generate text captions with the BLIP-2 model \\\\([20]\\\\) and sample one of 5 proposals per object.\\n\\nTraining. We base our model on a pretrained latent-diffusion text-to-image model. We only fine-tune the U-Net and keep the VAE encoder and decoder frozen. In each iteration, we select \\\\( N = 5 \\\\) images and their poses. We sample one denoising timestep \\\\( t \\\\sim [0, 1000] \\\\), add noise to the images according to Eq. 2, and compute the loss according to Eq. 3. In the projection layers, we skip the last image when building the voxel grid, which enforces to learn a 3D representation that can be rendered from novel views. We train our method by varying between unconditional and image-conditional generation (Sec. 3.3). Concretely, with probabilities \\\\( p_1 = 0.25 \\\\) and \\\\( p_2 = 0.25 \\\\) we provide the first and/or second image as input and set the respective timestep to zero. Similar to Ruiz et al. \\\\([32]\\\\), we create a prior dataset and use it during training to maintain the 2D prior (see supplement for details).\\n\\nTable 1. Quantitative comparison of unconditional image generation. We report average FID \\\\([12]\\\\) and KID \\\\([2]\\\\) per category and improve by a significant margin. This signals that our images are more similar to the distribution of real images in the dataset. We mask away the background for our method and the real images to ensure comparability of numbers with the baselines.\\n\\n| Category | HF  | VD  | Ours  |\\n|----------|-----|-----|-------|\\n| FID      | \u2193   | \u2193   | \u2193     |\\n| KID      | \u2193   | \u2193   | \u2193     |\\n| Teddybear| 81.93 | 0.072 | 49.39 | 0.036 |\\n| Hydrant  | 61.19 | 0.042 | 46.45 | 0.033 |\\n| Donut    | 105.97 | 0.091 | 68.86 | 0.054 |\\n| Apple    | 62.19 | 0.056 | 56.85 | 0.043 |\\n\\nWe fine-tune the model on 2 \\\\( \\\\times \\\\) A100 GPUs for 60K iterations (7 days) with a total batch size of 64. We set the learning rate for the volume renderer to 0.005 and for all other layers to \\\\( 5 \\\\times 10^{-5} \\\\), and use the AdamW optimizer \\\\([33]\\\\).\\n\\nDuring inference, we can increase \\\\( N \\\\) and generate up to 30 images/batch on an RTX 3090 GPU. We use the UniPC \\\\([55]\\\\) sampler with 10 denoising steps, which takes 15 seconds.\\n\\n4. Results\\n\\nBaselines. We compare against recent state-of-the-art works for 3D generative modeling. Our goal is to create multi-view consistent images from real-world, realistic objects with authentic surroundings. Therefore, we consider methods that are trained on real-world datasets and select HoloFusion (HF) \\\\([ 18]\\\\), ViewsetDiffusion (VD) \\\\([ 38]\\\\), and DFM \\\\([41]\\\\). We show results on two tasks: unconditional generation (Sec. 4.1) and single-image reconstruction (Sec. 4.2).\\n\\nMetrics. We report FID \\\\([12]\\\\) and KID \\\\([2]\\\\) as common metrics for 2D/3D generation and measure the multi-view consistency of generated images with peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and LPIPS \\\\([54]\\\\). To ensure comparability, we evaluate all metrics on images without backgrounds, as not every baseline models them.\\n\\n4.1. Unconditional Generation\\n\\nOur method can be used to generate 3D-consistent views of an object from any pose with only text as input by using our autoregressive generation (Sec. 3.3). Concretely, we sample an (unobserved) image caption from the test set for the first batch and generate \\\\( N = 10 \\\\) images with a guidance scale \\\\( \\\\lambda_{cfg} = 7 \\\\).5 We then set \\\\( \\\\lambda_{cfg} = 0 \\\\) for subsequent batches, and create a total of 100 images per object.\\n\\nWe evaluate against HoloFusion (HF) \\\\([18]\\\\) and ViewsetDiffusion (VD) \\\\([38]\\\\). We report quantitative results in Tab. 1 and qualitative results in Figs. 4 and 5. HF \\\\([18]\\\\) creates diverse images that sometimes show view-dependent floating artifacts (see Fig. 5). VD \\\\([38]\\\\) creates consistent but blurry\"}"}
{"id": "CVPR-2024-2629", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HoloFusion (HF) [18] ViewsetDiffusion (VD) [38] Ours\\n\\nTeddybear\\nHydrant\\nApple\\nDonut\\n\\nFigure 4. Unconditional image generation of our method and baselines. We show renderings from different viewpoints for multiple objects and categories. Our method produces consistent objects and backgrounds. Our textures are sharper in comparison to baselines. Please see the supplemental material for more examples and animations.\\n\\n4.2. Single-Image Reconstruction\\n\\nOur method can be conditioned on multiple images in order to render any novel view in an autoregressive fashion (Sec. 3.3). To measure the 3D-consistency of our generated images, we compare single-image reconstruction against ViewsetDiffusion (VD) [38] and DFM [41]. Concretely, we sample one image from the dataset and generate 20 images at novel views also sampled from the dataset. We follow Szymanowicz et al. [38] and report the per-view maximum PSNR/SSIM and average LPIPS across multiple objects and viewpoints for all methods. We report quantitative results in Tab. 2 and show qualitative results in Fig. 6. VD [38] creates plausible results without backgrounds. DFM [41] creates...\"}"}
{"id": "CVPR-2024-2629", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Multi-view consistency of unconditional image generation. HoloFusion (HF) [18] has view-dependent floating artifacts (the base in first row). ViewsetDiffusion (VD) [38] has blurrier renderings (second row). Without the projection layer, our method has no precise control over viewpoints (third row). Without cross-frame-attention, our method suffers from identity changes of the object (fourth row). Our full method produces detailed images that are 3D-consistent (fifth row).\\n\\nTable 2. Quantitative comparison of single-image reconstruction.\\n\\n| Method    | Teddybear | Hydrant | Donut | Apple |\\n|-----------|-----------|---------|-------|-------|\\n| PSNR      | \u2191         | \u2191       | \u2191     | \u2191     |\\n| SSIM      | \u2191         | \u2191       | \u2191     | \u2191     |\\n| LPIPS     | \u2193         | \u2193       | \u2193     | \u2193     |\\n\\n| VD [38]   | 19.68     | 0.70    | 0.30  | 22.36 |\\n| DFM [41]  |           |         |       |       |\\n| Ours      | 21.98     | 0.84    | 0.13  | 22.49 |\\n\\nconsistent results with backgrounds at a lower image resolution (128\u00d7128). Our method produces higher resolution images with similar reconstruction results and backgrounds.\\n\\n4.3. Ablations\\n\\nThe key ingredients of our method are the cross-frame-attention and projection layers that we add to the U-Net (Sec. 3.2). We highlight their importance in Tab. 3 and Fig. 5.\\n\\nHow important are the projection layers? They are necessary to allow precise control over the image viewpoints (e.g., Fig. 5 row 3 does not follow the specified rotation). Our goal is to generate a consistent set of images from any viewpoint directly with our model (Sec. 3.3). Being able to control the pose of the object is therefore an essential part of our contribution. The projection layers build up a 3D representation of the object that is explicitly rendered into 3D-consistent features through volume rendering. This allows us to achieve viewpoint consistency, as also demonstrated through single-image reconstruction (Tab. 3).\\n\\nHow important are cross-frame-attention layers? They are necessary to create images of the same object. Without them, the teddybear in Fig. 5 (row 4) has the same general color scheme and follows the specified poses. However, differences in shape and texture lead to an inconsistent set of images.\"}"}
{"id": "CVPR-2024-2629", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Single-image reconstruction of our method and base-lines. Given one image/pose as input, our method produces plausible novel views that are consistent with the real shape and texture. We can also produce detailed backgrounds that match the input.\\n\\nsample 1\\nsample 2\\n\\nFigure 7. Diversity of generated results. We condition our method on text input, which allows to create objects in a desired style. We show samples for hand-crafted text descriptions that combine attributes (e.g., color, shape, background) in a novel way. Each row shows a different generation proposal from our method and we denote the object category (Teddybear, Hydrant) as [C]. This showcases the diversity of generated results, i.e., multiple different objects are generated for the same description.\\n\\nTable 3. Quantitative comparison of our method and ablations. We report average PSNR, SSIM, LPIPS [54], FID [12], KID [2] over Teddybear and Hydrant categories. We compare against dropping the projection layer (\u201cno proj\u201d) and cross-frame-attention (\u201cno cfa\u201d) from the U-Net (see Sec. 3.2). While still producing high-quality images with similar FID/KID, this shows that our proposed layers are necessary to obtain 3D-consistent images.\\n\\n| Method       | PSNR  | SSIM  | LPIPS  | FID     | KID    |\\n|--------------|-------|-------|--------|---------|--------|\\n| Ours (no proj)  | 16.55 | 0.71  | 0.29   | 47.95   | 0.034  |\\n| Ours (no cfa)  | 18.15 | 0.76  | 0.25   | 47.93   | 0.034  |\\n| Ours           | 22.24 | 0.84  | 0.11   | 47.92   | 0.034  |\\n\\nDoes the 2D prior help? We utilize a 2D prior in form of the pretrained text-to-image model that we fine-tune in a 3D-consistent fashion (Sec. 3.1). This enables our method to produce sharp and detailed images of objects from different viewpoints. Also, we train our method on captioned images.\\n\\n4.4. Limitations\\nOur method generates 3D-consistent, high-quality images of diverse objects according to text descriptions or input images. Nevertheless, there are several limitations. First, our method sometimes produces images with slight inconsistency, as shown in the supplement. Since the model is fine-tuned on a real-world dataset consisting of view-dependent effects (e.g., exposure changes), our framework learns to generate such variations across different viewpoints. A potential solution is to add lighting condition through a ControlNet [53]. Second, our work focuses on objects, but similarly scene-scale generation on large datasets [6, 52] can be explored.\\n\\n5. Conclusion\\nWe presented ViewDiff, a method that, given text or image input, generates 3D-consistent images of real-world objects placed in authentic surroundings. Our method leverages the expressivity of large 2D text-to-image models and fine-tunes this 2D prior on real-world 3D datasets to produce diverse multi-view images in a joint denoising process. The core insight of our work are two novel layers, namely cross-frame-attention and the projection layer (Sec. 3.2). Our autoregressive generation scheme (Sec. 3.3) allows to directly render high-quality novel views of a generated 3D object.\\n\\n6. Acknowledgements\\nThis work was done during Lukas\u2019 internship at Meta Reality Labs Zurich as well as at TU Munich, funded by a Meta sponsored research agreement. Matthias Nie\u00dfner was also supported by the ERC Starting Grant Scan2CAD (804724). We also thank Angela Dai for the video voice-over.\"}"}
{"id": "CVPR-2024-2629", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\n3D asset generation is getting massive amounts of attention, inspired by the recent success of text-guided 2D content creation. Existing text-to-3D methods use pretrained text-to-image diffusion models in an optimization problem or fine-tune them on synthetic data, which often results in non-photorealistic 3D objects without backgrounds. In this paper, we present a method that leverages pretrained text-to-image models as a prior, and learn to generate multi-view images in a single denoising process from real-world data. Concretely, we propose to integrate 3D volume-rendering and cross-frame-attention layers into each block of the existing U-Net network of the text-to-image model. Moreover, we design an autoregressive generation that renders more 3D-consistent images at any viewpoint. We train our model on real-world datasets of objects and showcase its capabilities to generate instances with a variety of high-quality shapes and textures in authentic surroundings. Compared to the existing methods, the results generated by our method are consistent, and have favorable visual quality (\u221230% FID, \u221237% KID).\\n\\n1. Introduction\\n\\nIn recent years, text-to-image (T2I) diffusion models [29, 33] have emerged as cutting-edge technologies, revolutionizing high-quality and imaginative 2D content creation guided by text descriptions. These frameworks have found widespread applications, including extensions such as ControlNet [53] and DreamBooth [32], showcasing their versatility and potential. An intriguing direction in this domain is to use T2I models as powerful 2D priors for generating three-dimensional (3D) assets. How can we effectively use these models to create photo-realistic and diverse 3D assets?\\n\\nExisting methods like DreamFusion [27], Fantasia3D [5], and ProlificDreamer [47] have demonstrated exciting results by optimizing a 3D representation through score distillation sampling [27] from pretrained T2I diffusion models. The...\"}"}
{"id": "CVPR-2024-2629", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3D assets generated by these methods exhibit compelling diversity. However, their visual quality is not consistently as high as that of the images generated by T2I models. A key step to obtaining 3D assets is the ability to generate consistent multi-view images of the desired objects and their surroundings. These images can then be fitted to 3D representations like NeRF [25] or NeuS [45]. HoloDiffusion [19] and ViewsetDiffusion [38] train a diffusion model from scratch using multi-view images and output 3D-consistent images. GeNVS [4] and DFM [41] additionally produce object surroundings, thereby increasing the realism of the generation. These methods ensure (photo)-realistic results by training on real-world 3D datasets [31, 56]. However, these datasets are orders of magnitude smaller than the 2D dataset used to train T2I diffusion models. As a result, these approaches produce realistic but less-diverse 3D assets. Alternatively, recent works like Zero-1-to-3 [23] and One-2-3-45 [22] leverage a pretrained T2I model and fine-tune it for 3D consistency. These methods successfully preserve the diversity of generated results by training on a large synthetic 3D dataset [7]. Nonetheless, the produced objects can be less photo-realistic and are without surroundings.\\n\\nIn this paper, we propose a method that leverages the 2D priors of the pretrained T2I diffusion models to produce photo-realistic and 3D-consistent 3D asset renderings. As shown in the first two rows of Fig. 1, the input is a text description or an image of an object, along with the camera poses of the desired rendered images. The proposed approach produces multiple images of the same object in a single forward pass. Moreover, we design an autoregressive generation scheme that allows to render more images at any novel viewpoint (Fig. 1, third row). Concretely, we introduce projection and cross-frame-attention layers, that are strategically placed into the existing U-Net architecture, to encode explicit 3D knowledge about the generated object (see Fig. 2). By doing so, our approach paves the way to fine-tune T2I models on real-world 3D datasets, such as CO3D [31], while benefiting from the large 2D prior encoded in the pretrained weights. Our generated images are consistent, diverse, and realistic renderings of objects.\\n\\nTo summarize, our contributions are:\\n\u2022 a method that utilizes the pretrained 2D prior of text-to-image models and turns them into 3D-consistent image generators. We train our approach on real-world multi-view datasets, allowing us to produce realistic and high-quality images of objects and their surroundings (Sec. 3.1).\\n\u2022 a novel U-Net architecture that combines commonly used 2D layers with 3D-aware layers. Our projection and cross-frame-attention layers encode explicit 3D knowledge into each block of the U-Net architecture (Sec. 3.2).\\n\u2022 an autoregressive generation scheme that renders images of a 3D object from any desired viewpoint directly with our model in a 3D-consistent way (Sec. 3.3).\\n\\n2. Related Work\\n\\nText-To-2D.\\n\\nDenoising diffusion probabilistic models (DDPM) [14] model a data distribution by learning to invert a Gaussian noising process with a deep network. Recently, DDPMs were shown to be superior to generative adversarial networks [8], becoming the state-of-the-art framework for image generation. Soon after, large text-conditioned models trained on billion-scale data were proposed in Imagen [33] or Dall-E 2 [29]. While [8] achieved conditional generation via guidance with a classifier, [13] proposed classifier-free guidance. ControlNet [53] proposed a way to tune the diffusion outputs by conditioning on various modalities, such as image segmentation or normal maps. Similar to ControlNet, our method builds on the strong 2D prior of a pretrained text-to-image (T2I) model. We further demonstrate how to adjust this prior to generate 3D-consistent images of objects.\\n\\nText-To-3D.\\n\\n2D DDPMs were applied to the generation of 3D shapes [28, 34, 39, 42, 44, 50, 58] or scenes [10, 15, 40] from text descriptions. DreamFusion [27] proposed score distillation sampling (SDS) which optimizes a 3D shape whose renders match the belief of the DDPM. Improved sample quality was achieved by a second-stage mesh optimization [5, 21], and smoother SDS convergence [35, 47]. Several methods use 3D data to train a novel-view synthesis model whose multi-view samples can be later converted to 3D, e.g. conditioning a 2D DDPM on an image and a relative camera motion to generate novel views [23, 48]. However, due to no explicit modelling of geometry, the outputs are view-inconsistent. Consistency can be improved with epipolar attention [43, 57], or optimizing a 3D shape from multi-view proposals [22]. Our work fine-tunes a 2D T2I model to generate renders of a 3D object; however, we propose explicit 3D unprojection and rendering operators to improve view-consistency. Concurrently, SyncDreamer [24] also add 3D layers in their 2D DDPM. We differ by training on real data with backgrounds and by showing that autoregressive generation is sufficient to generate consistent images, making the second 3D reconstruction stage expendable.\\n\\nDiffusion on 3D Representations.\\n\\nSeveral works model the distribution of 3D representations. While DiffRF [26] leverages ground-truth 3D shapes, HoloDiffusion [19] is supervised only with 2D images. HoloFusion [18] extends this work with a 2D diffusion render post-processor. Images can also be denoised by rendering a reconstructing 3D shape [1, 38]. Unfortunately, the limited scale of existing 3D datasets prevents these 3D diffusion models from extrapolating beyond the training distribution. Instead, we exploit a large 2D pretrained DDPM and add 3D components that are tuned on smaller-scale multi-view data. This leads to improved multi-view consistency while maintaining the expressivity brought by pretraining on billion-scale image data.\"}"}
{"id": "CVPR-2024-2629", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Method Overview.\\n\\nWe augment the U-Net architecture of pretrained text-to-image models with new layers in every U-Net block. These layers facilitate communication between multi-view images in a batch, resulting in a denoising process that jointly produces 3D-consistent images. First, we replace self-attention with cross-frame-attention (yellow) which compares the spatial features of all views. We condition all attention layers on pose ($RT$), intrinsics ($K$), and intensity ($I$) of each image. Second, we add a projection layer (green) into the inner blocks of the U-Net. It creates a 3D representation from multi-view features and renders them into 3D-consistent features. We fine-tune the U-Net using the diffusion denoising objective (Eq. 3) at timestep $t$, supervised from captioned multi-view images.\\n\\n3. Method\\n\\nWe propose a method that produces 3D-consistent images from a given text or posed image input (see Fig. 1 top/mid). Concretely, given desired output poses, we jointly generate all images corresponding to the condition. We leverage pretrained text-to-image (T2I) models [29, 33] and fine-tune them on multi-view data [31]. We propose to augment the existing U-Net architecture by adding new layers into each block (see Fig. 2). At test time, we can condition our method on multiple images (see Fig. 1 bottom), which allows us to autoregressively render the same object from any viewpoint directly with the diffusion model (see Sec. 3.3).\\n\\n3.1. 3D-Consistent Diffusion\\n\\nDiffusion models [14, 36] are a class of generative models that learn the probability distribution $p_\\\\theta(x_0)$ over data $x_0 \\\\sim q(x_0)$ and latent variables $x_1:T= x_1, \\\\ldots, x_T$. Our method is based on pretrained text-to-image models, which are diffusion models $p_\\\\theta(x_0|c)$ with an additional text condition $c$. For clarity, we drop the condition $c$ for the remainder of this section.\\n\\nTo produce multiple images $x_0:N= x_0, \\\\ldots, x_T$ at once, which are 3D-consistent with each other, we seek to model their joint probability distribution $p_\\\\theta(x_0:N) = \\\\int p_\\\\theta(x_0:N:T) dx_0:N:T$. Similarly to concurrent work by Liu et al. [24], we generate one set of images $p_\\\\theta(x_0:N)$ by adapting the reverse process of DDPMs [14] as a Markov chain over all images jointly:\\n\\n$$p_\\\\theta(x_0:N:T) := p(x_0:T) \\\\prod_{t=1}^{N} p_\\\\theta(x_n|\\\\ldots,x_0,t), \\\\quad (1)$$\\n\\nwhere we start the generation from Gaussian noise sampled separately per image $p(x_n:T) = N(x_n:T; 0, I)$, $\\\\forall n \\\\in [0, N]$. We gradually denoise samples $p_\\\\theta(x_n|\\\\ldots,x_0,t) = N(x_{t-1}; \\\\mu_n(\\\\ldots,x_0,t), \\\\sigma^2_t I)$ by predicting the per-image mean $\\\\mu_n(\\\\ldots,x_0,t)$ through a neural network $\\\\mu$ that is shared between all images. Importantly, at each step, the model uses the previous states $x_0:N$ of all images, i.e., there is communication between images during the model prediction. We refer to Sec. 3.2 for details on how this is implemented. To train $\\\\mu$, we define the forward process as a Markov chain:\\n\\n$$q(x_0:N:T|x_0:N) = \\\\prod_{t=1}^{N} q(x_n|\\\\ldots,x_n-1), \\\\quad (2)$$\\n\\nwhere $q(x_n|\\\\ldots,x_n-1) = N(x_n; \\\\sqrt{1-\\\\beta_t} x_{n-1}, \\\\beta_t I)$ and $\\\\beta_1, \\\\ldots, \\\\beta_T$ define a constant variance schedule, i.e., we apply separate noise per image to produce training samples. We follow Ho et al. [14] by learning a noise predictor $\\\\epsilon_\\\\theta$ instead of $\\\\mu_\\\\theta$. This allows to train $\\\\epsilon_\\\\theta$ with an $L^2$ loss:\\n\\n$$E_{x_0:N,\\\\epsilon_0:N \\\\sim N(0,I),\\\\epsilon} \\\\epsilon_n - \\\\epsilon_n|\\\\ldots,x_0,t|^2. \\\\quad (3)$$\\n\\n3.2. Augmentation of the U-Net architecture\\n\\nTo model a 3D-consistent denoising process over all images, we predict per-image noise $\\\\epsilon_n(\\\\ldots,x_0,t)$ through a neural network $\\\\epsilon$. This neural network is initialized from the pretrained weights of existing text-to-image models, and is usually defined as a U-Net architecture [29, 33]. We seek to leverage the previous states $x_0:N$ of all images to arrive at a 3D-consistent denoising step. To this end, we propose to add two layers into the U-Net architecture, namely a cross-frame-attention layer and a projection layer. We note that the predicted per-image noise needs to be image specific, since all images are generated starting from separate Gaussian\"}"}
{"id": "CVPR-2024-2629", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"noise. It is therefore important to keep around 2D layers that act separately on each image, which we achieve by finetuning the existing ResNet [11] and ViT [9] blocks. We summarize our architecture in Fig. 2. In the following, we discuss our two proposed layers in more detail.\\n\\nCross-Frame Attention. Inspired by video diffusion [49, 51], we add cross-frame-attention layers into the U-Net architecture. Concretely, we modify the existing self-attention layers to calculate $\\\\text{CFAttn}(Q, K, V) = \\\\text{softmax}(QK^T/\\\\sqrt{d})V$ with $Q = W_Q h_i$, $K = W_K [h_j]_{j \\\\neq i}$, $V = W_V [h_j]_{j \\\\neq i}$, (4) where $W_Q, W_K, W_V$ are the pretrained weights for feature projection, and $h_i \\\\in \\\\mathbb{R}^{C \\\\times H \\\\times W}$ is the input spatial feature of each image $i \\\\in [1, N]$. Intuitively, this matches features across all frames, which allows generating the same global style. Additionally, we add a conditioning vector to all cross-frame and cross-attention layers to inform the network about the viewpoint of each image. First, we add pose information by encoding each image's camera matrix $p \\\\in \\\\mathbb{R}^{4 \\\\times 4}$ into an embedding $z_1 \\\\in \\\\mathbb{R}^4$ similar to Zero-1-to-3 [23]. Additionally, we concatenate the focal length and principal point of each camera into an embedding $z_2 \\\\in \\\\mathbb{R}^4$. Finally, we provide an intensity encoding $z_3 \\\\in \\\\mathbb{R}^2$, which stores the mean and variance of the image RGB values. At training time, we set $z_3$ to the true values of each input image, and at test time, we set $z_3 = [0.5, 0]$ for all images. This helps to reduce the view-dependent lighting differences contained in the dataset (e.g., due to different camera exposure). We construct the conditioning vector as $z = [z_1, z_2, z_3]$, and add it through a LoRA-linear-layer $W_Q'$ to the feature projection matrix $Q$. Concretely, we compute the projected features as: $Q = W_Q h_i + s \\\\cdot W_Q'[h_i; z]$, (5) where we set $s = 1$. Similarly, we add the condition via $W_K'$ to $K$, and $W_V'$ to $V$.\\n\\nProjection Layer. Cross-frame attention layers are helpful to produce globally 3D-consistent images. However, the objects do not precisely follow the specified poses, which leads to view-inconsistencies (see Fig. 5 and Tab. 3). To this end, we add a projection layer into the U-Net architecture (Fig. 3). The idea of this layer is to create 3D-consistent features that are then further processed by the next U-Net layers (e.g. ResNet blocks). By repeating this layer across all stages of the U-Net, we ensure that the per-image features are in a 3D-consistent space. We do not add the projection layer to the first and last U-Net blocks, as we saw no benefit from them at these locations. We reason that the network processes image-specific information at those stages and thus does not need a 3D-consistent feature space.\\n\\nInspired by multi-view stereo literature [3, 17, 37], we create a 3D feature voxel grid from all input spatial features. First, we unproject the compressed image features into 3D and aggregate them into a joint voxel grid with an MLP. Then we refine the voxel grid with a 3D CNN. A volume renderer similar to NeRF [25] renders 3D-consistent features from the grid. Finally, we apply a learned scale function and expand the feature dimension. We produce 3D-consistent output features from posed input features. First, we unproject the compressed image features into 3D and aggregate them into a joint voxel grid with an MLP. Then we refine the voxel grid with a 3D CNN. A volume renderer similar to NeRF [25] renders 3D-consistent features from the grid. Finally, we apply a learned scale function and expand the feature dimension.\\n\\n$h_0: \\\\mathbb{R}^{C \\\\times H \\\\times W}$ by projecting each voxel into each image plane. First, we compress $h_0: \\\\mathbb{R}^{C \\\\times H \\\\times W}$ with a $1 \\\\times 1$ convolution to a reduced feature dimension $C' = 16$. We then take the bilinear interpolated feature at the image plane location and place it into the voxel. This way, we create a separate voxel grid per view, and merge them into a single grid through an aggregator MLP. Inspired by IBRNet [46], the MLP predicts per-view weights followed by a weighted feature average. We then run a small 3D CNN on the voxel grid to refine the 3D feature space. Afterwards, we render the voxel grid into output features $h_0: \\\\mathbb{R}^{C' \\\\times H \\\\times W}$ with volumetric rendering similar to NeRF [25]. We dedicate half of the voxel grid to foreground and half to background and apply the background model from MERF [30] during ray-marching. We found it is necessary to add a scale function after the volume rendering output. The volume renderer typically uses a sigmoid activation function as the final layer during ray-marching [25]. However, the input features are defined in an arbitrary floating-point range. To convert $h_0: \\\\mathbb{R}^{C' \\\\times H \\\\times W}$ back into the same range, we non-linearly scale the features with $1 \\\\times 1$ convolutions and ReLU activations. Finally, we expand $h_0: \\\\mathbb{R}^{C'} \\\\times H \\\\times W$ to the input feature dimension $C$. We refer to the supplemental material for details about each component's architecture.\"}"}
{"id": "CVPR-2024-2629", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Titas Anciukevi\u010dius, Zexiang Xu, Matthew Henderson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Renderdiffusion: Image diffusion for 3D reconstruction, inpainting and generation. In CVPR, 2023.\\n\\n[2] Miko\u0142aj Binkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In ICLR, 2018.\\n\\n[3] Alja\u017e Bo\u017ei\u010d, Pablo Palafox, Justus Thies, Angela Dai, and Matthias Nie\u00dfner. TransformerFusion: Monocular RGB scene reconstruction using transformers. In NeurIPS, 2021.\\n\\n[4] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view synthesis with 3D-aware diffusion models. arXiv:2304.02602, 2023.\\n\\n[5] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3D: Disentangling geometry and appearance for high-quality text-to-3D content creation. In ICCV, 2023.\\n\\n[6] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In CVPR, 2017.\\n\\n[7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3D objects. In CVPR, 2023.\\n\\n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021.\\n\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth $16 \\\\times 16$ words: Transformers for image recognition at scale. In ICLR, 2021.\\n\\n[10] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-NeRF2NeRF: Editing 3D scenes with instructions. In ICCV, 2023.\\n\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\n[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two-time-scale update rule converge to a local Nash equilibrium. In NeurIPS, 2017.\\n\\n[13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshops, 2021.\\n\\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.\\n\\n[15] Lukas H\u00f6lllein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nie\u00dfner. Text2Room: Extracting textured 3D meshes from 2D text-to-image models. In ICCV, 2023.\\n\\n[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022.\\n\\n[17] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. JMLR, 24(251):1\u201343, 2023.\\n\\n[18] Animesh Karnewar, Niloy J. Mitra, Andrea Vedaldi, and David Novotny. HoloFusion: Towards photo-realistic 3D generative modeling. In ICCV, 2023.\\n\\n[19] Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy J. Mitra. HoloDiffusion: Training a 3D diffusion model using 2D images. In CVPR, 2023.\\n\\n[20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv:2301.12597, 2023.\\n\\n[21] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-3D content creation. In CVPR, 2023.\\n\\n[22] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. One-2-3-45: Any single image to 3D mesh in 45 seconds without per-shape optimization. arXiv:2306.16928, 2023.\\n\\n[23] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Von Drnck. Zero-1-to-3: Zero-shot one image to 3D object. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 9298\u20139309, 2023.\\n\\n[24] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from a single-view image. In The Twelfth International Conference on Learning Representations, 2024.\\n\\n[25] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\\n\\n[26] Norman M\u00fcller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bul\u00f2, Peter Kontschieder, and Matthias Nie\u00dfner. DiffRF: Rendering-guided 3D radiance field diffusion. In CVPR, 2023.\\n\\n[27] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. In ICLR, 2023.\\n\\n[28] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Alaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3D object generation using both 2D and 3D diffusion priors. arXiv:2306.17843, 2023.\\n\\n[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. arXiv:2204.06125, 2022.\\n\\n[30] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hedman. MERF: Memory-efficient radiance fields for real-time rendering. Communications of the ACM, 65(1):99\u2013106, 2021.\"}"}
{"id": "CVPR-2024-2629", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"view synthesis in unbounded scenes.\\n\\nACM Transactions on Graphics (TOG), 42(4):1\u201312, 2023.\\n\\n[31] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3D: Large-scale learning and evaluation of real-life 3D category reconstruction. In ICCV, 2021.\\n\\n[32] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine-tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023.\\n\\n[33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022.\\n\\n[34] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, and Seungryong Kim. Let 2D diffusion model know 3D-consistency for robust text-to-3D generation. arXiv:2303.07937, 2023.\\n\\n[35] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. MVDream: Multi-view diffusion for 3D generation. arXiv:2308.16512, 2023.\\n\\n[36] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015.\\n\\n[37] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In CVPR, 2021.\\n\\n[38] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. ViewSet diffusion: (0-)image-conditioned 3D generative models from 2D data. In ICCV, 2023.\\n\\n[39] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3D: High-fidelity 3D creation from a single image with diffusion prior. In ICCV, 2023.\\n\\n[40] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa. MVDiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion. In NeurIPS, 2023.\\n\\n[41] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B. Tenenbaum, Fr\u00e9d\u00e9 Durand, William T. Freeman, and Vincent Sitzmann. Diffusion with forward models: Solving stochastic inverse problems without direct supervision. In NeurIPS, 2023.\\n\\n[42] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari. TextMesh: Generation of realistic 3D meshes from text prompts. In 3DV, 2024.\\n\\n[43] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-Bin Huang, and Johannes Kopf. Consistent view synthesis with pose-guided diffusion models. In CVPR, 2023.\\n\\n[44] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score Jacobian chaining: Lifting pretrained 2D diffusion models for 3D generation. In CVPR, 2023.\\n\\n[45] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In NeurIPS, 2021.\\n\\n[46] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. IBRNet: Learning multi-view image-based rendering. In CVPR, 2021.\\n\\n[47] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity and diverse text-to-3D generation with variational score distillation. arXiv:2305.16213, 2023.\\n\\n[48] Daniel Watson, William Chan, Ricardo Martin Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. In ICLR, 2023.\\n\\n[49] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, 2023.\\n\\n[50] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong. 3D-aware image generation using 2D diffusion models. In ICCV, 2023.\\n\\n[51] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-to-video translation. In SIGGRAPH Asia, 2023.\\n\\n[52] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie\u00dfner, and Angela Dai. ScanNet++: A high-fidelity dataset of 3D indoor scenes. In ICCV, 2023.\\n\\n[53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023.\\n\\n[54] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.\\n\\n[55] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. UniPC: A unified predictor-corrector framework for fast sampling of diffusion models. In NeurIPS, 2023.\\n\\n[56] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. ACM Transactions on Graphics, 37(4):65:1\u201312, 2018.\\n\\n[57] Zhizhuo Zhou and Shubham Tulsiani. SparseFusion: Distilling view-conditioned diffusion for 3D reconstruction. In CVPR, 2023.\\n\\n[58] Joseph Zhu and Peiye Zhuang. HiFA: High-fidelity text-to-3D with advanced diffusion guidance. arXiv:2305.18766, 2023.\"}"}
