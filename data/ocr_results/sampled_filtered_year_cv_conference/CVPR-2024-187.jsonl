{"id": "CVPR-2024-187", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Madhav Agarwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri, and CV Jawahar. Audio-visual face reenactment. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5178\u20135187, 2023.\\n\\n[2] Zohar Barzelay and Yoav Y. Schechner. Harmony in motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1\u20138, 2007.\\n\\n[3] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Localizing visual sounds the hard way. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16867\u201316876, 2021.\\n\\n[4] Zida Cheng, Shuai Xiao, Zhonghua Zhai, Xiaoyi Zeng, and Weilin Huang. Mixer: Image to multi-modal retrieval learning for industrial application. arXiv preprint arXiv:2305.03972, 2023.\\n\\n[5] Joon Son Chung, Andrew W. Senior, Oriol Vinyals, and Andrew Zisserman. Lip reading sentences in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3444\u20133453. IEEE Computer Society, 2017.\\n\\n[6] Marco Cristani, Manuele Bicego, and Vittorio Murino. Audio-visual event recognition in surveillance video sequences. IEEE Transactions on Multimedia, 9(2):257\u2013267, 2007.\\n\\n[7] Marco Crocco, Marco Cristani, Andrea Trucco, and Vittorio Murino. Audio surveillance: A systematic review. ACM Computing Surveys (CSUR), 48(4):1\u201346, 2016.\\n\\n[8] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation. ACM Trans. Graph., 37(4):112, 2018.\\n\\n[9] Jules Franc\u00b8oise, Norbert Schnell, Riccardo Borghesi, and Fr\u00b4ed\u00b4eric Bevilacqua. Probabilistic models for designing motion and sound relationships. In Proceedings of the 2014 international conference on new interfaces for musical expression, pages 287\u2013292, 2014.\\n\\n[10] Gunnar Farneb \u00a8ack. Two-frame motion estimation based on polynomial expansion. In Image Analysis: 13th Scandinavian Conference, SCIA 2003 Halmstad, Sweden, June 29\u2013July 2, 2003 Proceedings, pages 363\u2013370. Springer, 2003.\\n\\n[11] Dennis Fedorishin, Deen Dayal Mohan, Bhavin Jawade, Srinagaraj Setlur, and Venu Govindaraju. Hear the flow: Optimal flow-based self-supervised visual sound source localization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2278\u20132287, 2023.\\n\\n[12] John W Fisher III, Trevor Darrell, William Freeman, and Paul Viola. Learning joint statistical models for audio-visual fusion and segregation. Advances in Neural Information Processing Systems, 13, 2000.\\n\\n[13] Denis Fortun, Patrick Bouthemy, and Charles Kervrann. Optical flow modeling and computation: A survey. Computer Vision and Image Understanding, 134:1\u201321, 2015.\\n\\n[14] Chuang Gan, Yiwei Zhang, Jiajun Wu, Boqing Gong, and Joshua B Tenenbaum. Look, listen, and act: Towards audio-visual embodied navigation. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 9701\u20139707. IEEE, 2020.\\n\\n[15] Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, and Tong Lu. Avsegformer: Audio-visual segmentation with transformer. CoRR, abs/2307.01146, 2023.\\n\\n[16] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776\u2013780. IEEE, 2017.\\n\\n[17] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. International Journal of Computer Vision, 129:1789\u20131819, 2021.\\n\\n[18] Longyin Guo, Qijun Zhao, and Hongmei Gao. Look longer to see better: Audio-visual event localization by exploiting long-term correlation. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 01\u201307. IEEE, 2022.\\n\\n[19] Sharon E Guttman, Lee A Gilroy, and Randolph Blake. Hearing what the eyes see: Auditory encoding of visual temporal sequences. Psychological science, 16(3):228\u2013235, 2005.\\n\\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.\\n\\n[21] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn architectures for large-scale audio classification. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 131\u2013135. IEEE, 2017.\"}"}
{"id": "CVPR-2024-187", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Distilling the knowledge in a neural network.\\n\\nDi Hu, Feiping Nie, and Xuelong Li. Deep multimodal clustering for unsupervised audiovisual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9248\u20139257, 2019.\\n\\nDi Hu, Rui Qian, Minyue Jiang, Xiao Tan, Shilei Wen, Errui Ding, Weiyao Lin, and Dejing Dou. Discriminative sounding objects localization via self-supervised audiovisual matching. Advances in Neural Information Processing Systems, 33:10077\u201310087, 2020.\\n\\nDi Hu, Yake Wei, Rui Qian, Weiyao Lin, Ruihua Song, and Ji-Rong Wen. Class-aware sounding objects localization via audiovisual correspondence. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\n\\nHanzhe Hu, Fangyun Wei, Han Hu, Qiwei Ye, Jinshi Cui, and Liwei Wang. Semi-supervised semantic segmentation via adaptive equalization learning. Advances in Neural Information Processing Systems, 34:22106\u201322118, 2021.\\n\\nChen Ju, Peisen Zhao, Ya Zhang, Yanfeng Wang, and Qi Tian. Point-level temporal action localization: Bridging fully-supervised proposals to weakly-supervised losses. arXiv preprint arXiv:2012.08236, 2020.\\n\\nChen Ju, Peisen Zhao, Siheng Chen, Ya Zhang, Yanfeng Wang, and Qi Tian. Divide and conquer for single-frame temporal action localization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13455\u201313464, 2021.\\n\\nChen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting visual-language models for efficient video understanding. In European Conference on Computer Vision, pages 105\u2013124. Springer, 2022.\\n\\nChen Ju, Peisen Zhao, Siheng Chen, Ya Zhang, Xiaoyun Zhang, Yanfeng Wang, and Qi Tian. Adaptive mutual supervision for weakly-supervised temporal action localization. IEEE Transactions on Multimedia, 2022.\\n\\nChen Ju, Zeqian Li, Peisen Zhao, Ya Zhang, Xiaopeng Zhang, Qi Tian, Yanfeng Wang, and Weidi Xie. Multi-modal prompting for low-shot temporal action localization. arXiv preprint arXiv:2303.11732, 2023.\\n\\nChen Ju, Haicheng Wang, Zeqian Li, Xu Chen, Zhonghua Zhai, Weilin Huang, and Shuai Xiao. Turbo: Informativity-driven acceleration plug-in for vision-language models. arXiv preprint arXiv:2312.07408, 2023.\\n\\nChen Ju, Haicheng Wang, Jinxiang Liu, Chaofan Ma, Ya Zhang, Peisen Zhao, Jianlong Chang, and Qi Tian. Constraint and union for partially-supervised temporal sentence grounding. arXiv preprint arXiv:2302.09850, 2023.\\n\\nChen Ju, Kunhao Zheng, Jinxiang Liu, Peisen Zhao, Ya Zhang, Jianlong Chang, Qi Tian, and Yanfeng Wang. Distilling vision-language pre-training to collaborate with weakly-supervised temporal action localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14751\u201314762, 2023.\\n\\nEvangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen. Epic-fusion: Audio-visual temporal binding for egocentric action recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5492\u20135501, 2019.\\n\\nE. Kidron, Y. Y. Schechner, and M. Elad. Pixels that sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88\u201395 vol. 1, 2005.\\n\\nJun-Tae Lee, Mihir Jain, Hyoungwoo Park, and Sungrack Yun. Cross-attentional audio-visual fusion for weakly-supervised action localization. In International Conference on Learning Representations, 2020.\\n\\nSeung Hyun Lee, Gyeongrok Oh, Wonmin Byeon, Chanyoung Kim, Won Jeong Ryoo, Sang Ho Yoon, Hyunjun Cho, Jihyun Bae, Jinkyu Kim, and Sangpil Kim. Sound-guided semantic video generation. In European Conference on Computer Vision, pages 34\u201350. Springer, 2022.\\n\\nSeung Hyun Lee, Wonseok Roh, Wonmin Byeon, Sang Ho Yoon, Chanyoung Kim, Jinkyu Kim, and Sangpil Kim. Sound-guided semantic image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3377\u20133386, 2022.\\n\\nKexin Li, Zongxin Yang, Lei Chen, Yi Yang, and Jun Xiao. Catr: Combinatorial-dependence audio-queried transformer for audio-visual video segmentation. In Proceedings of the 31st ACM International Conference on Multimedia, pages 1485\u20131494, 2023.\\n\\nYan-Bo Lin, Hung-Yu Tseng, Hsin-Ying Lee, Yen-Yu Lin, and Ming-Hsuan Yang. Unsupervised sound localization via iterative contrastive learning. CoRR, abs/2104.00315, 2021.\\n\\nYan-Bo Lin, Yu-Jhe Li, and Yu-Chiang Frank Wang. Dual-modality seq2seq network for audio-visual event localization. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2002\u20132006. IEEE, 2019.\\n\\nChen Liu, Peike Patrick Li, Xingqun Qi, Hu Zhang, Lincheng Li, Dadong Wang, and Xin Yu. Audio-visual segmentation by exploring cross-modal mutual semantics. In Proceedings of the 31st ACM International Conference on Multimedia, pages 7590\u20137598, 2023.\\n\\nJinxiang Liu, Chen Ju, Weidi Xie, and Ya Zhang. Exploiting transformation invariance and equivariance for self-supervised sound localisation. In Proceedings of the 30th ACM International Conference on Multimedia, pages 3742\u20133753, 2022.\\n\\nJinxiang Liu, Chen Ju, Chaofan Ma, Yanfeng Wang, Yu Wang, and Ya Zhang. Audio-aware query-enhanced transformer for audio-visual segmentation. CoRR, abs/2307.13236, 2023.\\n\\nJinxiang Liu, Yu Wang, Chen Ju, Chaofan Ma, Ya Zhang, and Weidi Xie. Annotation-free audio-visual segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5604\u20135614, 2024.\\n\\nYen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt Kira, and Peter Vajda. Unbiased teacher for semi-supervised object detection. In Proceedings of the International Conference on Learning Representations. OpenReview.net, 2021.\"}"}
{"id": "CVPR-2024-187", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2024-187", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lihe Yang, Lei Qi, Litong Feng, Wayne Zhang, and Yinghuan Shi. Revisiting weak-to-strong consistency in semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7236\u20137246, 2023.\\n\\nHang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Von-odrick, Josh McDermott, and Antonio Torralba. The sound of pixels. In Proceedings of the European Conference on Computer Vision, pages 570\u2013586, 2018.\\n\\nHang Zhao, Chuang Gan, Wei-Chiu Ma, and Antonio Torralba. The sound of motions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1735\u20131744. IEEE, 2019.\\n\\nPeisen Zhao, Lingxi Xie, Chen Ju, Ya Zhang, Yanfeng Wang, and Qi Tian. Bottom-up temporal action localization with mutual regularization. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VIII, pages 539\u2013555. Springer, 2020.\\n\\nJinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. Audio-visual segmentation. In Proceedings of the European Conference on Computer Vision, 2022.\\n\\nYuliang Zou, Zizhao Zhang, Han Zhang, Chun-Liang Li, Xiao Bian, Jia-Bin Huang, and Tomas Pfister. Pseudoseg: Designing pseudo labels for semantic segmentation. In Proceedings of the International Conference on Learning Representations. OpenReview.net, 2021.\"}"}
{"id": "CVPR-2024-187", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Audio-Visual Segmentation via Unlabeled Frame Exploitation\\n\\nJinxiang Liu, Yikun Liu, Fei Zhang, Chen Ju, Ya Zhang, Yanfeng Wang\\n\\n1 Cooperative Medianet Innovation Center, Shanghai Jiao Tong University\\n2 Shanghai AI Laboratory\\n\\n{jinxiangliu, yikunliu, feirenas, ju, yanzhang, wangyanfeng622}@sjtu.edu.cn\\n\\nAbstract\\n\\nAudio-visual segmentation (AVS) aims to segment the sounding objects in video frames. Although great progress has been witnessed, we experimentally reveal that current methods reach marginal performance gain within the use of the unlabeled frames, leading to the underutilization issue. To fully explore the potential of the unlabeled frames for AVS, we explicitly divide them into two categories based on their temporal characteristics, i.e., neighboring frame (NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame, often contain rich motion information that assists in the accurate localization of sounding objects. Contrary to NFs, DFs have long temporal distances from the labeled frame, which share semantic-similar objects with appearance variations. Considering their unique characteristics, we propose a versatile framework that effectively leverages them to tackle AVS. Specifically, for NFs, we exploit the motion cues as the dynamic guidance to improve the objectness localization. Besides, we exploit the semantic cues in DFs by treating them as valid augmentations to the labeled frames, which are then used to enrich data diversity in a self-training manner. Extensive experimental results demonstrate the versatility and superiority of our method, unleashing the power of the abundant unlabeled frames.\\n\\n1. Introduction\\n\\nHumans perceive the surroundings not only by seeing but also by hearing to accurately and efficiently obtain the target information [23]. In the audio-visual understanding field, the demand to visually attend to the auditory objects has driven the exploration of the audio-visual segmentation (AVS) task [81]. The goal of AVS is to localize and segment the sounding objects in the video frames with the guidance of audio signals. And the successful grounding of auditory objects with the AVS task will benefit a wide range of downstream tasks such as multi-modal content editing [1, 42, 43], video surveillance [6, 7], and robot industry [17, 74].\\n\\nTo address the task, current methods [19, 44, 47, 49, 50, 54, 81] are based on the dataset which is sparsely annotated. Concretely, due to the high labeling costs, only few frames in a video frame sequence are annotated with groundtruth masks, leaving the rest abundance of frames unlabeled. For example, in AVSBench-S4 dataset [81], only one sampled frame is annotated for a 5-second video. Despite the predominance of unlabeled frames within the datasets, current approaches [19, 44, 47, 49, 54, 81] adopt global temporal modeling module (GTM) that overemphasizes on exploiting the labeled frames to help address AVS, which may lead to the underutilization of the abundant unlabeled frames. To further verify this, we perform experiments...\"}"}
{"id": "CVPR-2024-187", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ments based on the typical TPA VI [81] method. The results in Fig. 1 (c) demonstrate that compared with the baseline model (w/o GTM) trained with only labeled frames, current approach (w/ GTM) without tailored handling for the unlabeled frames only provides marginal improvement. Therefore, we are motivated to explore a more effective way to utilize the unlabeled frames for the A VS task.\\n\\nBefore delving into the exploitation of the unlabeled frames, let us rethink the characteristics of the abundant unlabeled frames. Taking Fig. 1 (b) as an example, given a target labeled frame describing \u201cdog jumping\u201d, its neighboring unlabeled frames usually have very tiny visual appearance changes. For the distant frames, they usually contain the same object but with large appearance variations to the object in the labeled frame, e.g., the dog has transformed from the pose \u201cjumping\u201d in the labeled frame to \u201cwalking\u201d or \u201cstanding still\u201d in the distant frames. Based on the observation, we start by first dividing the unlabeled frames into two categories: neighboring frame (NF) and distant frame (DF), based on the temporal distance with the target labeled frame. Though the visual changes are very limited, NFs often contain rich dynamic motion information that is important to the audio-visual understanding [2, 8, 12, 64, 78]. If properly used, the motion can not only assist in the accurate localization of the sounding objects but also provide the shape details of objects. For the DFs, both they and the labeled frame reflect the different stages of an audio-visual event [22, 23, 53, 62]. Contrary to the NFs, this long-term temporal relationship means that the DFs generally share the same or semantic-similar objects but with large appearance variations. Therefore, DFs could serve as the natural semantic augmentations for the labeled frames, which can be utilized to diversify the training data, thereby enhancing the model generalization capabilities.\\n\\nConsidering the characteristics of NFs and DFs, we propose a universal unlabeled frame exploitation (UFE) framework to leverage the two types of unlabeled frames with different strategies. For NFs, we extract the motion by calculating the optical flow between the target labeled frame and its NFs. And we explicitly feed the flow as model input to incorporate the motion guidance, which is complementary to the still RGB frame. In terms of DFs, since they are the natural semantic augmentations to labeled frames, the training data could be significantly enriched beyond the labeled frames. To this end, we propose a teacher-student network training framework to provide valid supervision for the unlabeled frames with the weak-to-strong consistency, where the predictions for the strong-augmented frames from the student are supervised by the predictions for the weak-augmented ones from the teacher. We perform the experiments by applying our proposed framework to two representative methods TPA VI [81] and AVSegFormer [19]. Extensive experimental results demonstrate the effectiveness of our proposed method to attack the A VS task by exploiting the unlabeled frames. The main contributions are:\\n\\n- We propose a simple but effective partition strategy for the unlabeled frames based on the temporal characteristics, i.e., neighboring frames and distant frames, relieving the underutilization issue in A VS.\\n- We propose UFE, a versatile framework that leverages the NFs and DFs, where NFs provide motion guidance and DFs enhance the data diversity beyond the labeled frames, explicitly improving the objectness segmentation.\\n- Extensive experiments show our method can effectively exploit the abundant unlabeled frames and achieves new state-of-the-art performance on the A VS task, e.g., 78.96 mIoU with ResNet backbone and 83.15 mIoU with PVT backbone on A VS Bench-S4 dataset.\\n\\n2. Related Work\\n\\nAudio-Visual Segmentation. With the advancement of multi-modal learning [4, 33, 35\u201337], many audio-visual understanding problems have been studied, such as audio-visual sound separation [9, 18, 70, 77, 79], audio-visual segmentation [3, 27\u201329, 45, 48, 50, 60, 61, 66] and audio-visual video understanding [39, 41, 46, 69, 69]. In this paper, we focus on the audio-visual segmentation (A VS) task, whose purpose is to segment the sounding objects in video frames. Previous methods [3, 27\u201329, 45, 48, 59\u201361, 66] usually tackled the task in self-supervised or weakly-supervised learning and termed the task as visual sound source localization. Recently researchers [19, 44, 47, 49, 50, 54, 81] have been tackling A VS under the umbrella of supervised learning on the sparsely-annotated A VSBench dataset [81]. And based on architecture, we divide these methods into two categories: FCN-based [54, 81] and transformer-based methods [19, 44, 47, 49, 50]. For the FCN-based methods, the typical model is TPA VI [81]. The key design of [81] is the temporal pixel-wise audio-visual interaction (TPA VI) module which performs audio-visual feature fusion similar to the non-local block [73]. In terms of the transformer-based models [19, 44, 47, 49, 50], the general idea is to achieve audio-visual fusion and mask decoding with transformer [71]. For example, AVSegFormer [19] employs cross-attention to merge the spatial-temporal mask visual features and the audio embeddings. Although the architecture designs of FCN-based and transformer-based methods differ, they share a similar architecture pipeline, which consists of feature extraction, audio-visual fusion, and mask decoding. Besides, we observe current methods usually process the labeled frames and unlabeled frames from a video equally and predict the segmentations for all frames. However, due to the lack of annotations for the unlabeled data, the predictions for the unlabeled frames have no supervision. Inevitably, the methods without special handling for unlabeled frames lead to a suboptimal utilization problem.\"}"}
{"id": "CVPR-2024-187", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Motion and Sound.\\nObject motions and air vibration cause sound. We humans usually perceive sound together with the motion of visual objects. This strong relation between sound and motion has been studied by previous methods, e.g., [14, 40] employed probabilistic models to investigate the relationship between motion and sound. Moreover, many previous works [2, 5, 8, 11, 12, 16, 58, 64, 78] have shown the important role that motion plays in audio-visual learning. For example, lip motion is a vital clue for speech processing tasks such as speech denoising [16] and speech separation [8, 58]. Other studies [2, 78] also modelled the temporal motion information [31, 32, 34, 80] in visual frames to solve the cocktail-party problem. However, few works explore motion for the A VS task.\\n\\nTeacher-Student Network.\\nTeacher-student network has become the dominant architecture for many problems [15, 21, 26, 30, 38, 51, 65, 68, 75, 76, 82]. In a teacher-student network, the prediction of the teacher model is used to regularize the prediction of the student model, thereby, transferring the teacher's knowledge. When employed to handle scenarios encompassing both labeled and unlabeled data, the teacher trained with labeled data can generate pseudo-labels for the unlabeled data, which the student then tries to match. In our model design, we first separate the labeled frames and unlabeled frames from a sequence for independent processing, and frame-wisely consider the labeled and the unlabeled frames across the dataset in our framework. To exploit the unlabeled frames, we adopt the weak-to-strong consistency [51, 55, 65, 76] by regularizing the predictions for the strong-augmented unlabeled frames from the student with the predictions for the weak-augmented unlabeled frames from the teacher. Thereby, the unlabeled frames can obtain supervision in a self-supervised manner.\\n\\n3. Motivation\\nFor the task of audio-visual segmentation (A VS), the input data consists of a sequence of sampled video frames $V = \\\\{I_i\\\\}_{T_i=1}^T$, where $I_i \\\\in \\\\mathbb{R}^{3 \\\\times H_0 \\\\times W_0}$, and its corresponding audios $A = \\\\{a_i\\\\}_{T_i=1}^T$, where $a_i \\\\in \\\\mathbb{R}^{d}$ is the audio clip with each video frame. The objective of the A VS task is to segment the sounding objects corresponding to its audio in each frame in $V$. The target segmentation is the binary masks for each frame $Y = \\\\{y_i\\\\}_{T_i=1}^T$, where $y_i \\\\in \\\\{0, 1\\\\}^{H_0 \\\\times W_0}$.\\n\\nAs mentioned in Sec. 2, current mainstream approaches to A VS falls into two categories: the FCN-based methods [54, 81] and transformer-based methods [19, 44, 47, 49, 50]. Though the methods vary, the pipelines of methods can be abstracted into three subsequent steps: feature extraction with image encoder $\\\\Phi_{image}$ and audio encoder $\\\\Phi_{audio}$, multi-modal feature fusion with fusion module $\\\\Phi_{fusion}$, and mask prediction with mask decoder $\\\\Phi_{dec}$. Formally, the predicted segmentation $P$ is obtained with:\\n\\n$$P = \\\\Phi_{dec}(\\\\Phi_{fuse}(\\\\Phi_{image}(V), \\\\Phi_{audio}(A))).$$\\n\\nNotably, mainstream methods treat the labeled frames and unlabeled frames sampled from a video sequence equally and predict the masks for all frames. However, only the labeled frames have groundtruth supervision while the remaining abundant unlabeled frames have no supervision. And the only possible benefit which the unlabeled frames might provide for labeled frames is the contextual information with the global temporal modeling (GTM) operation. Concretely, global temporal modeling (GTM) employs cross-attention [71] to model the temporal relationships of the features across all the frames from a video, including labeled and unlabeled ones. To illustrate, Zhou et al. [81] deployed the cross-attention to integrate the space-time relations of the features in the TPA VI module in the audio-visual fusion stage; likewise, Gao et al. [19] proposed the channel-attention mixer based on the cross-attention in the audio-visual fusion stage to obtain the mask features.\\n\\nTo measure the improvement by exploiting the unlabeled frames with GTM of the previous method [19, 81], we establish the baseline by discarding the unlabeled frames and only using the labeled frames for model training. We perform experiments on A VSBench-S4 dataset and compare the performance with two typical methods TPA VI [81] and A VSegFormer [19]. The results on TPA VI baseline model are shown in Fig. 1 (c), compared to the model trained with only labeled frames (w/o GTM), previous method [81] based on global temporal modeling (w/ GTM) achieves only marginal performance gain: 0.14 gain with ResNet and 0.58 gain with PVT in mIoU ($M_J$). For more metrics and more results on the A VSegFormer baseline method, please refer to the supplementary materials.\\n\\nThe results demonstrate the major issue of current methods: the underutilization of the unlabeled frames to boost the performance for the A VS task. Based on the observation, we intend to devise a more effective method to fully exploit the unlabeled frames, which is elaborated as follows.\\n\\n4. Method\\n4.1. Framework Overview\\nTechnically, our proposed framework can be established based on either FCN-based methods or transformer-based methods. Since both categories of methods are divided into three steps as elaborated in Sec. 2, our proposed framework also has three steps in the inference stage as illustrated in Fig. 2 (b). The model accepts the image $I_i$, the calculated flow $O_i$ and its audio $A_i$ as model inputs, then goes through three successive steps, to predict the target segmentation mask. The main difference from previous methods is that we harness the optical flow $O_i$ extracted from the target frame and its unlabeled neighboring frame (NF) as model...\"}"}
{"id": "CVPR-2024-187", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. Overview of our framework to exploit unlabeled frames. (a) Teacher-student network for training. Student network is optimized with $L_{sup}$ and $L_{unsup}$. $L_{sup}$ is computed with the predicted mask $f_\\\\theta(x_l)$ and its groundtruth for the labeled frame $x_l$; $L_{unsup}$ is computed between $f_\\\\theta(x_u)$ from the student and the predicted pseudo mask $f_\\\\theta(x_u')$ for the strong-augmented unlabeled image from teacher. (b) Inference pipeline of the framework. We incorporate flow as auxiliary input to exploit the motion cues within NFs.\\n\\nRegarding the exploitation of distant frames (DFs), we consider leveraging the semantic information of the distant frames and integrating the abundance of DFs to enrich the training data diversity. Even though DFs have no groundtruth annotations, we adopt the teacher-student network in the training phase, as shown in Fig. 2 (a). Different from previous methods where the unlabeled frames receive no supervision, our teacher-student network can provide supervision for the unlabeled distant frames to exploit the unlabeled frames with the weak-to-strong consistency from pseudo-labeling.\\n\\n4.2. Neighboring Frame Exploitation\\n\\nSound occurs due to object motions and air vibration, thus sound has strong associations with motions. For example, when someone speaks, it always comes along with the movement of the speaker's lips; the sound of a musical instrument usually comes along with the hand movement of the player. The motion information can provide important dynamic cues for achieving the A VS task, which is complementary to the static information that still RGB frame provides. Concretely, i). motion can assist in localizing the exact sounding object and resolve the identity ambiguity. ii). motion information can even outline the shape and contour of the sounding objects, which contributes to accurate segmentations with better fine-grained details. And the motion information of the target frame can be simply extracted by exploiting the neighboring unlabeled frames. Concretely, we use the target frame and its temporally-adjacent unlabeled frame in raw frame sequence to compute the optical flow, which serves a common way for motion estimation [13, 63]. And we leverage the optical flow by incorporating it as model input together with the target frame. Specifically, for the $i$th sampled target frame from a video to be sent into the model, we utilize the RGB frame $I_l$ and its subsequent neighboring unlabeled frame $I_n$ to calculate the optical flow $O_i$ with Gunnar Farneback algorithm [10]. Then as shown in Fig. 2 (b) we extract image and optical flow features with the image encoder and optical flow encoder separately. Following [19, 81], the image encoder can be either ResNet50 [24] or PVT-v2 [72]. We represent the extracted multi-scale image features as $F_{vi} \\\\in \\\\mathbb{R}^{h_i \\\\times w_i \\\\times C_i}$, where $(h_i, w_i) = (H, W)/2^{i+1}$ for $i = 1, \\\\ldots, 4$. For the optical flow encoder, we adapt the pretrained ResNet-18 architecture by modifying the in-channel number of the first convolution layer into 2. We denote the extracted optical flow features from the flow encoder as $F_{fi} \\\\in \\\\mathbb{R}^{h_o \\\\times w_o \\\\times C_o}$. And for the audio encoder, we adopt the VGGish [25] model pretrained on the AudioSet [20] dataset and pool the features into a vector embedding. The obtained audio features are denoted as $F_a \\\\in \\\\mathbb{R}^{C_a}$.\\n\\nBefore performing audio-visual fusion, we first employ a refinement network to fuse the extracted multi-scale image features and flow features. We first utilize the upsampling operations to ensure the refined flow features align with the visual features. Then we fuse the refined optical flow features with the visual features of scale with summation. This process can be formulated as:\\n\\n$$F_{\\\\text{refine}} = F_{vi} + \\\\Phi_{\\\\text{Upsample}}(\\\\Phi_{\\\\text{Refine}}(F_{fi})),$$\\n\\nwhere $\\\\Phi_{\\\\text{Refine}}$ is composed of multiple convolution layers.\"}"}
{"id": "CVPR-2024-187", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"After fusing the image and optical flow features, we obtain the aggregated visual features $F_{\\\\text{refine}}^i$; then we perform the multi-modal feature fusion and mask decoding to obtain the segmentation predictions, by inheriting the modules from TPA VI [81] or AVSegFormer [19].\\n\\n4.3. Distant Frame Exploitation\\n\\nDistant frames (DFs) refer to the video frames which are temporally faraway from the target labeled frame. Contrary to neighboring frames, the distant frames do not contain the motion dynamics of the target sounding objects due to the long temporal distance. However, thanks to the large visual appearance variations to the target labeled frame, these distant frames are the natural augmentations to the target labeled frames with shared semantics. And these frames can substantially enhance the data diversity if utilized for model training, thereby boosting the model generalization. Even though there are no groundtruth annotations for the DFs, modern pseudo-labeling techniques can be harnessed to provide self-supervision. Given the observation, we propose to exploit the unlabeled distant frames with a teacher-student network to train the model, inspired by recent works [51, 52, 56, 65, 67].\\n\\nSpecifically, given the training data, we first divide it into the labeled frame set and the unlabeled distant frame set. We first use the labeled frames to train the teacher for some iterations to ensure that the model has the capability to generate reliable pseudo mask labels; this step is called burn-in stage [51, 52, 56, 67]. Then we initialize the student network with the weights of the teacher; and during the training stage, the teacher and student network share the weights. The difference is that the model parameters are optimized through the student network with the supervised loss and unsupervised loss; while the teacher network with stop gradient serves to provide pseudo labels for the unlabeled frames. To this end, in each iteration after the burn-in stage, the student network is optimized with labeled frames $\\\\{ (I_\\\\ell, O_\\\\ell, A_\\\\ell), y_\\\\ell \\\\}$ and the unlabeled frames $\\\\{ (I_u, O_u, A_u) \\\\}$. For the labeled frames, the supervised loss $L_{\\\\text{sup}}$ is computed between student prediction and groundtruth mask. For the supervised loss $L_{\\\\text{sup}}$, it can be either BCE loss [81] in TPA VI or Dice loss [57] in AVSegFormer [19], which are formulated as:\\n\\n\\\\[\\nL_{\\\\text{BCE}} = -\\\\frac{1}{N} \\\\sum_{i=1}^{N} [y_i \\\\cdot \\\\log(p_i) + (1 - y_i) \\\\cdot \\\\log(1 - p_i)],\\n\\\\]\\n\\n\\\\[\\nL_{\\\\text{Dice}} = 1 - \\\\frac{2}{N} \\\\sum_{i=1}^{N} (p_i \\\\cdot y_i) / (p_i + y_i),\\n\\\\]\\n\\nwhere in Eq. (3) and Eq. (4), $y_i$ represents the ground truth label of a pixel and $p_i$ represents the predicted probability of a pixel belonging to the foreground class.\\n\\nFor the unlabeled frames $\\\\{ (I_u, O_u, A_u) \\\\}$, the input visual signals are perturbed by two operators, i.e., weak perturbation $H_w$ (e.g., flip) and strong perturbation $H_s$ (e.g., cutmix). Afterwards, we feed the weakly-augmented view $\\\\{ H_w(I_u), H_w(O_u), A_u \\\\}$ to the teacher model to predict pseudo mask label $p_w$; and we feed the strongly-augmented view $\\\\{ H_s(I_u), H_s(O_u), A_u \\\\}$ to the student model to predict the mask $p_s$. The unsupervised loss $L_{\\\\text{unsup}}$ ensures that the predictions under strong perturbations align with those under weak perturbations, which can be formulated as:\\n\\n\\\\[\\nL_{\\\\text{unsup}} = \\\\frac{1}{B_u} \\\\sum_{i=1}^{B_u} H_s(p_w, p_s),\\n\\\\]\\n\\nwhere $B_u$ is the batch size for unlabeled data. $H_s$ serves to minimize the entropy between two probability distributions:\\n\\n\\\\[\\np_w = \\\\Phi_{\\\\text{mask}}(H_w(I_i), H_w(O_i), A_i),\\np_s = \\\\Phi_{\\\\text{mask}}(H_s(I_i), H_s(O_i), A_i),\\n\\\\]\\n\\nThe overall training objective $L_{\\\\text{total}}$ is a combination of supervised loss $L_{\\\\text{sup}}$ and unsupervised loss $L_{\\\\text{unsup}}$ as:\\n\\n\\\\[\\nL_{\\\\text{total}} = L_{\\\\text{sup}} + \\\\lambda L_{\\\\text{unsup}},\\n\\\\]\\n\\nwhere $\\\\lambda$ is the weight to balance the losses.\"}"}
{"id": "CVPR-2024-187", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. Experiment\\n\\n5.1. Experimental setup\\n\\nDatasets.\\n\\nWe use the AVSBench [81] dataset, which was recently proposed for audio-visual segmentation task with segmentation mask annotations for sounding objects. This dataset has two subsets: the semi-supervised Single Sound Source Segmentation (S4) and the fully supervised Multiple Sound Source Segmentation (MS3). In S4 subset, there exists only one sounding object in the video. For each training video sample, only the first frame of the frame sequence is annotated while all five sampled frames need to be segmented in the validation and test sets. In MS3 subset, there might exist more than one sounding objects in the video frames. All five frames sampled from a 5s-long video are provided with mask annotations in both training and evaluation stages. In terms of the dataset size, S4 subset includes 3452 / 740 / 740 videos in training, validation and test sets separately, for a total of 10,852 annotated frames; MS3 subset includes 296 / 64 / 64 videos in training, validation and test sets, with 2,120 annotated frames. For the MS3 dataset, since all five frames are annotated in training set, we extract semantically relevant videos from the VGGSound dataset and select the middle frames as the source of distant frames for the MS3 dataset, totaling 12,990 in size.\\n\\nMetrics.\\n\\nWe adopt the mean Intersection-over-Union (M\\\\(_J\\\\)) and F-score (M\\\\(_F\\\\)) as our evaluation metrics following previous methods [19, 81].\\n\\nImplementation details.\\n\\nTechnically, our proposed framework can be combined with any mainstream methods. In our experiments, we verify our method based on TPA VI [81] and A VSegFormer [19], thus we follow their experimental settings such as backbone, learning rate and optimizing strategy. The input image and optical flow size is 224 \u00d7 224.\\n\\nFor the teacher-student network, the weak augmentations include resize, crop and horizontal flip; and the strong augmentations include additional color jitter, grayscale and cut-mix operations. And the loss weight \\\\(\\\\lambda\\\\) is set to 0.5. The burn-in stage lasts for 10 epochs. We train the models for 120 epochs, with one NVIDIA A100 GPU. Batch size is 24.\\n\\n5.2. Comparison with Prior Arts\\n\\nImprovement of our method over baselines.\\n\\nTo verify the effectiveness of our method, we choose two typical baseline methods: FCN-based TPA VI [81] and transformer-based A VSegFormer [19], and we apply our framework onto the baseline methods. We compare the performance between the models (Ours) and the baseline models in Tab. 1.\\n\\nAs Tab. 1 shows, our method consistently improves the performance significantly on both TPA VI [81] and A VSeg-Former [19], which indicates the effectiveness and universality of our method. For the TPA VI with ResNet baseline method, our method has significant performance gains. For instance, our method achieves 5.36 \\\\(M_J\\\\) gains on S4 subset and 6.20 \\\\(M_J\\\\) gains on MS3 subset. In terms of the AVSegFormer baseline method, even it is already a very powerful method, our method can also improve the performance upon it on both backbones. For the ResNet backbone, our method achieves 2.51 \\\\(M_J\\\\) gains on S4 subset and 6.35 \\\\(M_J\\\\) gains on MS3 subset. With PVT backbone, our method achieves new state-of-the-art performance: 83.15 \\\\(M_J\\\\) on S4 subset and 61.95 \\\\(M_J\\\\) on MS3 subset.\\n\\nComparison with Other Arts.\\n\\nWe also collect up-to-date AV methods A VSC [47], CATR [44], AuTR [49], ECMV AE [54], and SAMA-A VS [50]; and we compare the performance of these methods with our proposed method. The results are shown in Tab. 2. The comparison shows the strong competitiveness of our proposed framework when compared with so various latest methods. Our method based on A VSegFormer [19] is still the state-of-the-art method among all the methods. Moreover, on S4 subset, the original TPA VI method only has 72.8 \\\\(M_J\\\\) with ResNet, 78.7 \\\\(M_J\\\\) with PVT, which falls behind the other methods including A VSC [47], CATR [44], AuTR [49], ECMV AE [54]. However, by combining our method with the TPA VI, the model (Ours w/TPA VI) has outperformed the other methods including A VSegFormer, A VSC, CATR, AuTR and ECMV AE in almost all metrics; and it becomes the second best model except our A VSegFormer-based model \\\"ours w/ A VSegFormer\\\" under the same backbones. The results clearly reveal the effectiveness of our versatile proposed framework. Notably, our framework can also be applied on these methods [44, 47, 49, 50, 54] to further improve their performance.\"}"}
{"id": "CVPR-2024-187", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Comparison with up-to-date state-of-the-arts on both sub-sets. Our proposed methods significantly improve the competitiveness of the baseline models. (The best performance in bold and the second best is underlined; \\\"I.B.\\\" denotes image backbone.)\\n\\n| Method          | From Scratch | P.T. on S4 |\\n|-----------------|--------------|------------|\\n|                 | ResNet       | PVT        | ResNet       | PVT        |\\n| TPA VI          |              | 47.90      | 54.00        | 54.30      | 57.30      |\\n| Ours (w/ TPA VI)|              | 54.08      | 59.49        | 54.73      | 60.78      |\\n| A VSegFormer    |              | 49.53      | 58.36        | 55.78      | 61.91      |\\n| Ours (w/ A VSegFormer) |       | 55.88      | 61.95        | 59.32      | 64.47      |\\n\\nTable 3. Results of the models with different percentages of labeled training data from A VSBench S4 dataset.\\n\\nResults using less labeled training data. We also investigate the performance when utilizing training data with varying proportions (5% and 10%) of labeled data on the S4 subset. As shown in Tab. 3, our approach consistently demonstrates impressive improvements across different data proportions. For instance, using only 10% of labeled data with ResNet backbone, the performance of our model increases by nearly 10 points on MJ. This indicates that our method is also highly effective when the labeled data is limited.\\n\\nTable 4. Performance with different initialization strategies under the MS3 setting on MJ.\\n\\n| Method          | From Scratch | P.T. on S4 |\\n|-----------------|--------------|------------|\\n|                 | ResNet       | PVT        | ResNet       | PVT        |\\n| NF DF           |              | 72.80      | 54.00        | 54.30      | 57.30      |\\n|                 |              | 77.89      | 81.01        | 81.41      | 82.49      |\\n\\n(a) Results with ResNet backbone.\\n\\n(b) Results with PVT backbone.\\n\\nTable 5. Ablation study of our framework based on TPA VI baseline model on S4 subset.\\n\\n| Effectiveness of NF and DF. | | | | | |\\n|-----------------------------|---|---|---|---|\\n| Burn-in epochs for training | 5 | 10 | 20 | 30 |\\n| MJ                           | 77.72 | 78.15 | 77.50 | 77.48 |\\n| MF                           | 0.883 | 0.887 | 0.881 | 0.882 |\\n\\n(a) Burn-in epochs for training.\\n\\n| Effectiveness of NF and DF. | | | | | |\\n|-----------------------------|---|---|---|---|\\n| Unsupervised loss weight \u03bb  | 0.1 | 0.2 | 0.5 | 1.0 |\\n| MJ                           | 77.00 | 77.62 | 78.15 | 77.19 |\\n| MF                           | 0.878 | 0.883 | 0.887 | 0.881 |\\n\\n(b) Unsupervised loss weight \u03bb.\\n\\nTable 6. Effects of burn-in epochs and unsupervised loss weight.\\n\\nPre-training on the Single-source subset. Following the TPA VI [81], we conduct an investigation into the impact on the MS3 when using pretrained weights of S4. As shown in Table 4, it is evident that pretraining on the S4 dataset will indeed improve the performance on the MS3 subset for all methods including ours; and our method achieves the best performance on the MS3 subset among all methods using S4-pretrained weights.\\n\\n5.3. Ablation Study\\nIn this section, we conduct ablation studies to evaluate the components in the framework with TPA VI baseline model on the S4 subset.\\n\\nEffectiveness of NF and DF. To study the effects of neighboring frames (NFs) and distant frames (DFs) in our proposed framework, we conduct ablation studies, adjusting one component at a time based on the TPA VI baseline methods with two visual backbones. As illustrated in Tab. 5, both NF and DF demonstrate significant enhancements when applied independently, indicating that their respective contributions to the performance are both fairly considerable. And when combining the NF and DF, the model obtains the best performance across all metrics. The results demonstrate the effectiveness and complementarity of neighboring frames (NFs) and distant frames (DFs) in boosting the performance on the A VSB task, implying the great value of the abundant unlabeled frames with proper exploitation.\"}"}
{"id": "CVPR-2024-187", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Qualitative comparison between our method and AVSegFormer [19] on both subsets of AVSBench. Our method shows better segmentation performance by localising the exact sounding object, attending to the fine-grained details and being closer to groundtruths.\\n\\nWe ablate the burn-in epochs in the teacher-student training. As shown in Tab. 6 (a), 10-epoch burn-in works best. Either using the unsupervised loss too early or too late will result in a suboptimal performance. If using the unsupervised loss too early, the pseudo labels from the teacher are not reliable thus it will cause negative effects on the final model performance. If using the unsupervised loss is too late, the model will be biased toward the labeled data without utilizing the unlabeled data.\\n\\nWe ablate the weight $\\\\lambda$ for the unsupervised loss $L_{unsup}$. The results in Tab. 6 (b) show a moderate value of 0.5 achieving the best performance. When $\\\\lambda$ is set as low as 0.1, the improvement is less significant than the cases where $\\\\lambda$ is 0.2 or 0.5. However, if $\\\\lambda$ is too high such as 1.0, the model performance degrades. This is due to the model overemphasizing the unlabeled data when using large unsupervised loss weight.\\n\\n5.4. Qualitative Examples\\nIn Fig. 3, we qualitatively show some segmentation results of our method and the baseline AVSegFormer [19]. The results clearly demonstrate the advantages of our method by producing the segmentations for the sounding objects which are closer to groundtruths. As shown in the first video of Fig. 3 (a), with the assistance of flow, our method can segment the tail of the bird which is hard to find with only visual RGB frames. In the second video of Fig. 3 (a), the AVSegFormer baseline falsely segments both the Ukulele and the salient yet silent person. On the contrary, our method accurately localizes and segments only Ukulele according to the audio cues, without being distracted by the silent person. In the multi-sound scenario in Fig. 3 (b), our method also shows improved performance. In the first video, the AVSegFormer [19] baseline mistakenly segments another silent instrument as marked with red boxes; while our method leverages both the sound and the hand motion of the player to produce the segmentations for the instrument being played. In the second video, the segmentations produced by our method for both instruments have fine-grained details and are closer to groundtruth annotations.\\n\\n6. Conclusion\\nIn this paper, we have pointed out the major limitation of previous AVS methods: the underutilization of the abundant unlabeled frames. To mitigate this, we analyzed that the unlabeled frames can be divided into two categories: neighboring frame (NF) and distant frame (DF), according to the temporal characteristics. And we proposed a unified unlabeled frame exploitation (UFE) framework to harness the two kinds of unlabeled frames based on their unique traits. For NFs, we extracted the motion cues as dynamic guidance to assist in the precise localization of sounding objects; while for DFs, since they are natural semantic augmentations to the labeled frames, we utilized them to enrich the data diversity with the teacher-student training. Extensive experiments have demonstrated the significant improvement brought by the exploitation of unlabeled frames. We believe that our proposed framework serves as a strong baseline and hopefully inspires more research to value both labeled and unlabeled data to successfully tackle AVS task.\"}"}
