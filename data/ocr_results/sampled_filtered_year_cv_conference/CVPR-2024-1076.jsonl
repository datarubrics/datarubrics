{"id": "CVPR-2024-1076", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Overview of the automatic annotation pipeline. It contains 2D pose estimation, angle-based hardcase mining and hardcase-based optimization.\\n\\nThe hand and foot are close to T-pose. In this case, the ankle angle usually approaches $90^\\\\circ$, and the wrist angle tends towards $180^\\\\circ$. Based on these insights, we design our angle-based Hardcase mining algorithm as follows: For the foot, we can approximate the coordinate of mid-toe joint as:\\n\\n$$K_{\\\\text{midtoe}} = K_{\\\\text{bigtoe}} + K_{\\\\text{smalltoe}}$$\\n\\nAccording to Equation 1, we define the vectors for the foot and leg as shown in Fig. 3:\\n\\n$$V_{\\\\text{foot}} = K_{\\\\text{midtoe}} - K_{\\\\text{ankle}}$$\\n$$V_{\\\\text{leg}} = K_{\\\\text{ankle}} - K_{\\\\text{knee}}$$\\n\\nHence, we can calculate the ankle rotation using the value of $\\\\alpha_{\\\\text{ankle rotation}}$ as determined by Equation 3.\\n\\n$$\\\\alpha_{\\\\text{ankle rotation}} = \\\\arccos \\\\left( \\\\frac{V_{\\\\text{foot}} \\\\cdot V_{\\\\text{leg}}}{\\\\|V_{\\\\text{foot}}\\\\|\\\\|V_{\\\\text{leg}}\\\\|} \\\\right)$$\\n\\nWe then mine the hardcase samples based on the following criteria: 1. The confidence for each of the foot keypoints must exceed 0.65, ensuring the reliability of the 2D keypoints. 2. $|\\\\alpha_{\\\\text{ankle rotation}} - 90^\\\\circ| > 63^\\\\circ$, implying that the angular deviation of the ankle from the T-pose should exceed $63^\\\\circ$.\\n\\nAs for the hand, since our goal is to resolve the rotation issue of the wrist joint, we do not consider the precise finger postures. Therefore, during the mining and optimization of the hand-hardcase, we have selected 10 keypoints of the hand part. Then we can approximate the coordinate of hand as:\\n\\n$$K_{\\\\text{hand}} = \\\\frac{1}{5}(K_{\\\\text{thumb}} + K_{\\\\text{forefinger}} + K_{\\\\text{middlefinger}} + K_{\\\\text{ringfinger}} + K_{\\\\text{pinkyfinger}})$$\\n\\nBased on the keypoints defined in Equation 4, we define the vectors for the hand and arm as shown in Fig. 3:\\n\\n$$V_{\\\\text{hand}} = K_{\\\\text{hand}} - K_{\\\\text{wrist}}$$\\n$$V_{\\\\text{arm}} = K_{\\\\text{wrist}} - K_{\\\\text{elbow}}$$\\n\\nBased on Equation 5, we can calculate the wrist rotation as follows.\\n\\n$$\\\\alpha_{\\\\text{wrist rotation}} = \\\\arccos \\\\left( \\\\frac{V_{\\\\text{hand}} \\\\cdot V_{\\\\text{arm}}}{\\\\|V_{\\\\text{hand}}\\\\|\\\\|V_{\\\\text{arm}}\\\\|} \\\\right)$$\\n\\nThen, we select hand-hardcase samples based on the following criteria: 1. The confidence for each of the 10 hand keypoints must exceed 0.7 to ensure the reliability of the 2D keypoints 2. $|\\\\alpha_{\\\\text{wrist rotation}} - 90^\\\\circ| < 52^\\\\circ$. Due to page limits, we leave more details about hardcase mining in supplementary details.\\n\\nFoot-hardcase Optimization.\\n\\nTo correct the behavior of the model in hardcases, we need to obtain precise pseudo labels of the filtered hardcase samples. Although the classic SMPLify [5] method can accurately fit the SMPL [26] parameter $\\\\Theta$ to 2D keypoints, it tends to generate physically implausible pseudo labels. To solve the issues, the EFT method [17] employ the trained HMR model as a optimization prior. However, due to the lack of specific priors for these hardcases, it performs poorly on optimizing hardcase samples. Therefore, we propose our methods. With the following loss denoted by Equation 7, the model is trained on the hardcase dataset. Thus it can learn an underlying hardcase pose prior via a data-driven paradigm. Then we use the trained model to test on the hardcase dataset, thus obtaining our precise pseudo labels.\\n\\n$$L_{\\\\text{joint}} = \\\\lambda_{\\\\text{body}} \\\\|\\\\hat{K}_{2D_{\\\\text{body}}} - \\\\bar{K}_{2D_{\\\\text{body}}}\\\\|_1 + \\\\lambda_{\\\\text{foot}} \\\\|\\\\hat{K}_{2D_{\\\\text{foot}}} - \\\\bar{K}_{2D_{\\\\text{foot}}}\\\\|_1 + \\\\lambda_{\\\\text{smpl}} \\\\|\\\\hat{\\\\Theta} - \\\\Theta\\\\|_1.$$\"}"}
{"id": "CVPR-2024-1076", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Human mesh recovery accuracy on the HardMo dataset.\\n\\n| Method                        | MPJPE  | PA-MPJPE | PCK@0.01 | PCK@0.05 |\\n|-------------------------------|--------|----------|----------|----------|\\n| ProHMR [22]                  | 113.8  | 74.2     | 0.10     | 0.73     |\\n| CLIFF [24]                   | 93.3   | 56.2     | 0.29     | 0.90     |\\n| HMR [18]\u2021                    | 61.1   | 42.1     | 0.40     | 0.96     |\\n| HardMo-HMR \u2020                 | 46.0   | 31.5     | 0.47     | 0.97     |\\n| 4DHumans a                    | 83.1   | 52.7     | 0.16     | 0.92     |\\n| 4DHumans b                    | 36.6   | 23.0     |          | 0.48     |\\n| HardMo-4DHumans \u2020            | 29.9   | 20.2     | 0.55     | 0.98     |\\n\\n\u2020: trained with mix of HardMo and commonly used datasets.\\n\u2021: trained with InstaVariety [19].\\n\\nHere, \\\\( \\\\hat{K}_{2D}^{\\\\text{body}} \\\\) is the 2D keypoints pseudo labels of body, and \\\\( \\\\hat{K}_{2D}^{\\\\text{foot}} \\\\) indicates 2D keypoints pseudo labels of the foot. \\\\( \\\\hat{K}_{2D}^{\\\\text{body}} \\\\) and \\\\( \\\\hat{K}_{2D}^{\\\\text{foot}} \\\\) represent 2D keypoints of body and foot projected by SMPL [26] mesh.\\n\\n\\\\( \\\\hat{\\\\theta} \\\\) and \\\\( \\\\hat{\\\\beta} \\\\) are the pseudo label.\\n\\\\( \\\\bar{\\\\theta} \\\\) is the pose parameter of the SMPL model, and \\\\( \\\\bar{\\\\beta} \\\\) is the shape parameter.\\n\\n\\\\( \\\\Theta = (\\\\bar{\\\\theta}, \\\\bar{\\\\beta}) \\\\) is the SMPL parameter predicted by the model. The use of raw SMPL pseudo labels as a regularization is important. We find that using only 2D keypoints as weak supervision can damage the prior knowledge learned by the model, consequently undermining the physical plausibility of results generated by the model.\\n\\nHand-hardcase Optimization. The hand exhibits greater flexibility compared to the foot. So employing the same optimization strategy as with foot-hardcase would make it challenging for the model to learn a reasonable solution. However, utilizing SMPLify [5] can well annotate hard-case parts but could potentially compromise other parts of the body. Therefore, we propose a two-stage optimization method:\\n\\nIn the first stage, we follow the optimization process outlined in Sec. 4.2, focusing on the body and foot parts. In the second stage, we introduce a hand-based optimization approach: for each sample, we optimize the following loss parameters:\\n\\n\\\\[\\nL_{\\\\text{joint}} = \\\\lambda_{2D} \\\\| \\\\hat{K}_{2D}^{\\\\text{hand}} - \\\\bar{K}_{2D}^{\\\\text{hand}} \\\\|_1 + \\\\lambda_{\\\\theta} \\\\| \\\\hat{\\\\theta}^{0:19} - \\\\theta^{0:19} \\\\|_1.\\n\\\\]\\n\\nHere, \\\\( K_{2D}^{\\\\text{hand}} \\\\) represents 2D keypoints of the hand, and \\\\( \\\\theta^{0:19} \\\\) denotes the SMPL [26] pose parameters that exclude the hand. \\\\( \\\\hat{\\\\theta} \\\\) denotes the predicted object, and \\\\( \\\\bar{\\\\theta} \\\\) denotes pseudo-labels. By applying this term, we impose a regularization loss on the body part to prevent the hand optimization from compromising the physical plausibility of other body parts. Through this two-stage optimization process, we not only ensure the physical plausibility of the body part but also annotate hard-case parts well.\\n\\n5. Experiments\\n\\nTo validate the impact of the proposed dataset HardMo, we initially conduct experiments to evaluate existing state-of-the-art approaches. Subsequently, we introduce two innovative benchmarks, i.e., HardMo-Foot and HardMo-Hand, to investigate the efficacy of the collected data in addressing hard-case issues. Due to the page limit, we leave the validation of the effectiveness of the automatic annotation pipeline and the description of the metrics in the supplementary details.\\n\\n5.1. Impact of HardMo\\n\\nTo ensure the quality of the dataset, we first discard ineligible samples using various filtering methods. The remaining dataset is divided into training set (80%) and testing set (20%). For the test data, we use the optimization approach in Sec. 4.2 to ensure the accuracy of labels. For comprehensive evaluation, we design two evaluation settings. The first one evaluates the performance of previous models trained on the commonly used datasets. The second one is reproducing existing methods on our HardMo dataset, including HMR [18] and 4DHumans following [10].\\n\\nResults and Analysis. In Table 2, we present the comparison results on the HardMo benchmark. HardMo-HMR, trained solely on HardMo, achieving an MPJPE of 36.0 mm performance, significantly outperforms HMR [18] that is trained with InstaVariety [19] with 61.1 mm. Moreover, it is 3.2 \\\\( \\\\times \\\\) better than ProHMR [22]'s 113.8 mm and even surpasses the SOTA method, 4DHumans [10] with 36.6 mm. These results show the effectiveness of HardMo in solving domain gap issues. Furthermore, HardMo-HMR \u2020, trained on a mixed dataset about HardMo and commonly used datasets, shows a 10.0mm decrease in MPJPE compared to HardMo-HMR, which highlights the inadequacy of the commonly used datasets. Similar results are observed in the HardMo-4DHumans. What's more HMR \u2021 trained solely on the InstaVariety [19] dataset, shows a 25.1mm decrease in MPJPE compared to HardMo-HMR. This result demonstrates that compared to InstaVariety, HardMo is closer to real-world scenarios and more effective in mitigating the domain gap.\\n\\nVisible Results. In Fig. 4, we present visualization results comparing HardMo-HMR with existing methods. The results reveal that the proposed HardMo-HMR excels in handling challenging poses, outperforming ProHMR [22], and even surpassing the state-of-the-art 4DHumans [10].\\n\\n5.2. Hardcase Benchmarks\\n\\nTo show the effectiveness of our hardcase datasets on solving hardcase problems, we establish two benchmarks based on HardMo-Foot and HardMo-Hand datasets, respectively.\"}"}
{"id": "CVPR-2024-1076", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative comparison between HardMo-HMR and other leading methods. As shown in the figure, HardMo-HMR is superior in handling challenging poses.\\n\\n| Method                  | Body foot | Foot P1 | Foot P2 | Body foot | Foot P1 | Foot P2 |\\n|-------------------------|-----------|---------|---------|-----------|---------|---------|\\n| HardMo-HMR (w/o OPT)    | 38.9      | 58.3    | 23.4    | 16.2      | 0.57    | 0.43    |\\n| HardMo-HMR (w/ OPT)     | 23.6      | 34.9    | 15.4    | 10.8      | 0.71    | 0.58    |\\n| 4DHumans a [10]         | 86.7      | 143.7   | 47.2    | 41.8      | 0.16    | 0.08    |\\n| 4DHumans b [10]         | 40.0      | 88.1    | 21.1    | 26.5      | 0.59    | 0.14    |\\n| HardMo-4DHumans \u2020 (w/ OPT) | 20.9      | 29.4    | 13.5    | 9.0       | 0.68    | 0.50    |\\n| HardMo-4DHumans \u2020 (w/ OPT) | 19.8      | 29.3    | 13.0    | 9.0       | 0.70    | 0.53    |\\n\\nTable 3. Reconstruction error on HardMo-Foot. P1 is intra-class evaluation, P2 is inter-class evaluation. \u2020: trained on the mixture of HardMo-Foot and HardMo-Hand. OPT: optimized label. 4DHumans a: HMR 2.0a from [10]. 4DHumans b: HMR 2.0b from [10].\\n\\n5.2.1 Benchmark on HardMo-Foot\\n\\nWe conduct evaluations following two protocols: 1. Intra-class (P1): Training and testing are performed on the same motion classes, e.g., jazziness. To ensure no overlap between training and testing, we split each dance class, allocating 80% of the images for training and the remaining 20% for testing. 2. Inter-class (P2): Images from the three types of dance classes are used for the training set, while the other five types are allocated to the test set. Due to page limits, more details about metrics and other setup are provided in the appendix.\\n\\nResults and Analysis.\\n\\nTable 3 shows that HardMo-HMR performs much better than all existing models on HardMo-Foot P1, with an MPJPE-foot of 34.9mm, which is 7\u00d7 better than ProHMR [22] at 213.6mm, and almost 3\u00d7 better than SOTA method, 4DHumans b [10], at 88.1mm. What's more it achieves an MPJPE-body of 23.6mm, which is 5\u00d7 better than the ProHMR, 1.7\u00d7 better than 4DHumans b.\\n\\nThe above results confirm that our HardMo-Foot dataset not only effectively addresses the foot-hardcase issues but also benefits the body-part recovery. Similarly, HardMo-HMR also performs much better than all existing models on HardMo-Foot P2. These results demonstrate that after training on our dataset, the model doesn't merely learn the unique hardcase information of a particular class but indeed acquires a generalizable knowledge about foot movements. Moreover, we also perform training for HardMo-HMR without the optimized label, the comparison results demonstrating the necessity of optimized labels for foot recovery. More details are in appendix.\\n\\n5.2.2 Benchmark on HardMo-Hand\\n\\nSimilar to the HardMo-Foot, we conduct comparison experiments with existing methods on HardMo-Hand. The detailed settings are attached in the appendix.\\n\\nResults and Analysis.\\n\\nTable 4 shows that HardMo-HMR performs much better than all existing models on the HardMo-Hand dataset, with a PCK@0.01-hand of 0.36. This is seven 7\u00d7 better than ProHMR [22] at 0.03, and almost 3\u00d7 better than SOTA method, 4DHumans b [10], at 0.11. Moreover, it achieves a PCK@0.01-body of 0.54, which is almost 5\u00d7 better than the ProHMR at 0.12, and is only lower than that of 4DHumans b by a mere 0.02.\"}"}
{"id": "CVPR-2024-1076", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Qualitative Comparisons between HardMo-4DHumans and other leading methods. As shown in the figure, both ProHMR [22] and 4DHumans [10] fail to recover the correct hand-wrist posture. In contrast, after being finetuned on the hardcase subset of our HardMo dataset, HardMo-4DHumans can resolve these hardcases perfectly.\\n\\n| Method          | PCK@0.01 | PCK@0.05 |\\n|-----------------|----------|----------|\\n| ProHMR [22]     | 0.12     | 0.03     |\\n| CLIFF [24]      | 0.32     | 0.08     |\\n| HardMo-HMR      | 0.54     | 0.36     |\\n| 4DHumans [10]   | 0.15     | 0.04     |\\n| 4DHumans [10]   |          | 0.93     |\\n| HardMo-4DHumans | 0.61     |          |\\n\\nTable 4. Reconstruction error on HardMo-hand. \u2020: trained with mixture of HardMo-Foot and HardMo-Hand.\\n\\nabove results demonstrate that our HardMo-Hand dataset effectively addresses the hand-hardcase issues and benefits the body-part recovery.\\n\\nQualitative Results. In Fig. 5, we provide the qualitative results for tackling foot and hand hardcase problems. We compare HardMo-4DHumans with existing methods. The front view reveals that ProHMR [22] and 4DHumans [10] continue to struggle with severe hardcase issues. In contrast, HardMo-4DHumans has resolved these intrinsic hard-case problems perfectly.\\n\\n6. Conclusion\\nAlthough existing methods perform well on benchmarks, they often struggle in real-world scenarios like dance and martial arts. Most of them suffer from severe misalignment in these cases where they tend to recover the posture of hands and feet at a T-pose. To bridge the gap between the research and application, we present HardMo, a large-scale dataset specially designed for complex motions. HardMo contains over 7 million images with 2D keypoints and SMPL [26] annotations. To further investigate the misalignment of feet and hands, we further filter two subsets, HardMo-Foot and HardMo-Hand dataset. Each subset is accompanied by optimized SMPL annotations. To efficiently build such datasets, we develop an efficient pipeline for automatic annotation, hardcase mining, and label optimization. Extensive experiments demonstrate the efficacy of the proposed pipeline and HardMo datasets. After training on HardMo, HMR [18], an early pioneering method, can even outperform the current SOTA, 4DHumans [10], on our benchmarks.\\n\\nLimitation and Future Works. This paper only focuses on the misalignment of feet and hands. Apart from such hardcases, there still exist some other significant challenges, such as self-occlusion, that deserve further exploration. We leave this as our future work. From the data-driven perspective, we hope that our solution would enhance the mocap method in real-world scenarios and encourage more research centered around practical, real-world settings.\\n\\nAcknowledgements. This work was supported by the National Key R&D Program of China (No. 2022ZD0116500), the National Natural Science Foundation of China (No. U21B2042, No. 62320106010), the InnoHK program, and in part by the 2035 Innovation Program of CAS.\"}"}
{"id": "CVPR-2024-1076", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HardMo: A Large-Scale Hardcase Dataset for Motion Capture\\n\\nJiaqi Liao\\\\textsuperscript{1}*, Chuanchen Luo\\\\textsuperscript{2}*, Yinuo Du\\\\textsuperscript{1}, Yuxi Wang\\\\textsuperscript{2,3}, Xucheng Yin\\\\textsuperscript{5}, Man Zhang\\\\textsuperscript{1,6}, Zhaoxiang Zhang\\\\textsuperscript{2,3,4}, Junran Peng\\\\textsuperscript{5}B\\n\\n\\\\textsuperscript{1}Beijing University of Posts and Telecommunications\\n\\\\textsuperscript{2}Institute of Automation, Chinese Academy of Sciences\\n\\\\textsuperscript{3}Centre for Artificial Intelligence and Robotics, HKISI, CAS\\n\\\\textsuperscript{4}University of Chinese Academy of Sciences\\n\\\\textsuperscript{5}University of Science and Technology Beijing\\n\\\\textsuperscript{6}Qinghai University of Science and Technology\\n\\n\\\\{liaojiaqi, duyinuo99, zhangman\\\\}@bupt.edu.cn, \\\\{chuanchenluo, yuxiwang93, jrpeng4ever\\\\}@gmail.com, xuchengyin@ustb.edu.cn\\n\\n\\\\textbf{Figure 1.} Comparisons between vanilla 4DHumans [10] and our HardMo-4DHumans. The teaser comprises four sub-figures. Each sub-figure, from left to right, corresponds to (a) the input image, (b) the prediction of HardMo-4DHumans, and (c) the prediction of vanilla 4DHumans. In comparison, our HardMo-4DHumans is superior in the alignment of hand and foot posture.\\n\\n\\\\textbf{Abstract}\\n\\nRecent years have witnessed rapid progress in monocular human mesh recovery. Despite their impressive performance on public benchmarks, existing methods are vulnerable to unusual poses, which prevents them from deploying to challenging scenarios such as dance and martial arts. This issue is mainly attributed to the domain gap induced by the data scarcity in relevant cases. Most existing datasets are captured in constrained scenarios and lack samples of such complex movements. For this reason, we propose a data collection pipeline comprising automatic crawling, precise annotation, and hardcase mining. Based on this pipeline, we establish a large dataset in a short time. The dataset, named HardMo, contains 7M images along with precise annotations covering 15 categories of dance and 14 categories of martial arts. Empirically, we find that the prediction failure in dance and martial arts is mainly characterized by the misalignment of hand-wrist and foot-ankle. To dig deeper into the two hardcases, we leverage the proposed automatic pipeline to filter collected data and construct two subsets named HardMo-Hand and HardMo-Foot. Extensive experiments demonstrate the effectiveness of the annotation pipeline and the data-driven solution to failure cases. Specifically, after being trained on HardMo, HMR, an early pioneering method, can even outperform the current state of the art, 4DHumans, on our benchmarks. Dataset will be publicly available at https://ljqnb.github.io/HardMo.github.io.\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2024-1076", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison with related datasets.\\n\\n| Dataset          | Type                  | Rendered   | Scene Type | Subjects | #Frames | #Scenes | #Subjects |\\n|------------------|-----------------------|------------|------------|----------|---------|---------|-----------|\\n| AGORA [29]       | Marker/Sensor-based MoCap |             | Daily      | -        | 350     | 4,240   | 4,240     |\\n| BEDLAM [4]       | Marker/Sensor-based MoCap |             | Daily      | -        | 380     | 103     | 103       |\\n| Human3.6M [13]   | Marker/Sensor-based MoCap |             | Daily      | -        | 3.6M    | 1      | 1         |\\n| 3DPW [38]        | Marker/Sensor-based MoCap |             | Daily      | -        | 51K     | 60      | 60        |\\n| MPI-INF-3DHP [28]| Marker/Sensor-based MoCap |             | Daily      | -        | 1.3M    | 8       | 8         |\\n| AIST++ [23]      | Marker/Sensor-based MoCap |             | Dance      | -        | 10.1M   | 30      | 30        |\\n| MSCOCO [25]      | Marker-less            |             | Daily      | -        | 38K     | -       | -         |\\n| MPII [1]         | Marker-less            |             | Daily      | -        | 24,920  | 3,913   | 3,913     |\\n| HardMo (Ours)    | Marker-less            |             | Daily      | -        | 7.0M    | 350     | 350       |\\n| HardMo-Hand (Ours)| Marker-less            |             | Daily      | -        | 400K    | 320     | 320       |\\n| HardMo-Foot (Ours)| Marker-less            |             | Daily      | -        | 500K    | 180     | 180       |\\n\\n1. Introduction\\n\\nMonocular motion capture aims to recover human skeletal motions from single-view videos. As a pivotal component of computer animation, this technique is primarily employed to endow virtual characters with authentic motion, especially in the context of dance and martial arts. For example, most dance and martial arts in games (e.g., Genshin Impact) and movies (e.g., Avatar) are realized through the motion capture technique. Fueled by deep learning techniques, recent years have witnessed remarkable progress in monocular motion capture. Since the proposal of HMR [18], a series of methods [18, 22, 24, 43] have emerged and achieved great breakthroughs on public benchmarks. Specifically, the MPJPE metric on Human3.6M [13] has seen a significant decrease from 88.0 mm to 47.1 mm. Despite the promising performance on benchmarks, these methods underperform in dance and martial arts scenarios. Most existing methods are vulnerable to unusual poses in dance and martial arts scenes, revealing the vast gap between research and practical application. From a data-driven perspective, the crux lies in the domain gap between existing motion datasets and real-world scenarios. In stark contrast to daily actions, dance and martial arts are characterized by rapid and tension-filled skeletal movements. As shown in Table 1, such movements rarely appear in commonly used datasets such as Human3.6M [13], MPI-INF-3DHP [28], and COCO [25]. As a result, models trained on these standard datasets struggle to effectively handle dance and martial arts.\\n\\nRecently, 4DHumans [10] additionally employed the InstaVariety [19] dataset for training. This practice leads to a promising improvement in handling unusual poses which demonstrates the effectiveness of the data-driven paradigm. Nevertheless, the InstaVariety [19] dataset lacks diversity. It contains a limited number of dances and excludes martial arts. Thus, this dataset cannot fully bridge the domain gap. Moreover, as shown in Fig. 1, 4DHumans [10] trained on InstaVariety [19] still suffers from prediction error in two cases, i.e., foot-hardcase (incorrect foot-ankle posture) and hand-hardcase (incorrect hand-wrist posture), where hands and feet tend to be recovered to the rest pose, respectively. From a data-driven perspective, we identify three limitations that may cause the issue:\\n\\n1. Limited Diversity in Hand and Foot Postures. Synthetic datasets have an advantage in precise SMPL [26] annotations. But they mainly focus on daily actions and lack diversity in hand and foot postures.\\n2. Incorrect SMPL Annotations. Some real-world datasets, such as Human3.6M [13], have incorrect hand and foot annotations. The two body parts are annotated as a nearly rest pose.\\n3. Keypoint Annotations Lack Hand and Foot. Keypoints annotations of existing datasets are either in COCO format or in Human3.6M format. The two formats only contain body keypoints and lack hand and foot keypoints. Using such annotations to train models would lead to a lack of constraints on hand and foot joints.\\n\\nAccording to the analysis mentioned above, how to collect relevant data with precise annotations is crucial to handle domain gap and inherent hardcase issues. Considering the highly technical and artistic demands in dance and martial arts, collecting data using marked or markerless capture methods would be prohibitively expensive and might not be sufficiently comprehensive. Therefore, we propose an automatic pipeline that leverages online videos. First, we gather ample dance and martial arts videos from the Internet. Second, we employ RTM-pose [9, 14] and 4DHumans [10] to estimate 2D keypoints and raw SMPL parameters, respectively. Third, we propose an angle-based hardcase mining algorithm to identify foot-hardcase and hand-hardcase samples. Lastly, we further optimize the SMPL parameters of these hardcases, since the precise annotation is crucial for handling the issue caused by data bias. Such a pipeline can accurately annotate 1 million images within three days, showcasing its efficiency, precision, and scalability.\\n\\nBased on this pipeline, we introduce HardMo, a large-scale hardcase dataset for monocular motion capture. HardMo contains over 7 million images extracted from...\"}"}
{"id": "CVPR-2024-1076", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1,500 sequences spanning 15 categories of dance and 14 categories of martial arts. Each image is accompanied by 2D keypoints and SMPL [26] annotations. Furthermore, to specifically tackle foot-hardcase and hand-hardcase issues, we curate additional HardMo-Foot and HardMo-Hand sub-sets which contain over 500k and 400k samples of corresponding hardcases, respectively.\\n\\nIn summary, our contributions are threefold:\\n\\n\u2022 We develop an efficient and scalable pipeline for automatic annotation and hardcase mining. This system offers a potent solution to the data scarcity issue in the motion domain.\\n\\n\u2022 HardMo bridges the domain gap, containing 7 million images across over 300 different scenarios. As subsets of HardMo, HardMo-Hand and HardMo-Foot, the first of their kind, focus on solving the inherent hardcase issues.\\n\\n\u2022 Extensive experiments demonstrate the effectiveness of HardMo in addressing the domain gap and inherent hard-case issues.\\n\\n2. Related Work\\n\\n2.1. Human Mesh Recovery\\n\\nThe techniques for human mesh recovery can generally be categorized into two main approaches: optimization-based [5, 12, 30, 34, 36, 41, 42] and regression-based [3, 8, 10, 18\u201322, 24, 27, 31, 43, 44]. The predominant method in the optimization approach iteratively optimizes by fitting the pose and shape parameters of SMPL [26] based on 2D keypoints. The Optimization method typically yields highly accurate results when the quality of the 2D keypoints is reliable. However, when faced with the occlusions, and unusual movements in dances and martial arts scenes, the results of optimization can be unsatisfactory. With the rise of deep learning, regression-based methods have increasingly become mainstream. Starting with HMR [18], many techniques have been developed to improve upon its foundation. For example, SPIN [21] utilizes the regression results of HMR as the initial pose for SMPLify [5]. Py-MAF [43, 44] introduces a mesh alignment module to correct poorly performing regression results, while CLIFF [24] incorporates additional bounding box input and calculates the 2D reprojection loss on the full image. Nonetheless, despite their excellent performance on benchmarks such as 3DPW [38] and Human3.6m [13], these methods often fall short in real-world scenarios, particularly in dance and martial arts. However, after training additionally on the Instavariety [19] dataset, 4DHumans [10] has shown significant advancements and can capture some unusual poses well. Nevertheless, all the methods above still struggle with two inherent hardcase issues.\\n\\n2.2. Human Body Pose Datasets\\n\\nAs shown in Table 1, The available datasets can generally be grouped into four types: rendered datasets [4, 6, 29], marker-based mocap datasets [7, 13, 35, 38], marker-less mocap datasets [16, 23, 28, 32, 39], pseudo-label datasets [1, 2, 25, 45]. (1) Rendered datasets, such as AGORA [29] and BEDLAM [4], offer the most standard SMPL [26] labels. However, these datasets lack realism and don't include dance or martial arts scenes. (2) In contrast, motion capture datasets like Human3.6M [13] are limited to a singular scenario, with a very basic range of movements. Although there are outdoor motion capture datasets like 3DPW [38], they still don't include dance or marital arts scenes. (3) To enrich the diversity of actions, markless motion capture datasets like AIST++ [23, 37] have emerged. They capture different views of 2D images and joints and then employ triangulation techniques to get 3D joints, subsequently fitting the SMPL model to these 3D joints to produce pseudo labels. While AIST++ focuses on dance, its dataset lacks variety in scenarios and dance types. Critically the poses of hand and foot in their dataset are inaccurately represented, resembling a T-pose. (4) Given the known richness and diversity of 2D pose datasets in subjects, poses, and scenes, some methods apply pseudo labels to these 2D datasets. However, as mentioned in Sec. 1, the hand and foot of their pseudo SMPL labels often resemble a T-pose, which causes the hardcase problems.\\n\\n3. HardMo Dataset\\n\\nThe HardMo dataset is a large-scale hardcase dataset for motion capture in dance and martial arts scenes. It is distinguished from existing datasets by the emphasis on hard-cases of two scenarios close to practical deployment. The HardMo dataset contains over 7 million images with precise 2D keypoints and 3D SMPL [26] annotations. These images are collected from 1,500 sequences covering 15 categories of dance and 14 categories of martial arts. Such a dataset bridges the gap between current human mesh recovery methods and real-world applications effectively. In dance and martial arts scenes, we observe that the misalignment of hands and feet appears frequently. To deal with the two hardcases, we additionally curate a HardMo-Foot dataset and a HardMo-Hand dataset. The two datasets contain over 500K and 400K samples with unusual foot and hand pose, respectively. To ensure the efficacy of both datasets, we perform further optimization on the SMPL annotations of these samples. For better understanding, we show some examples of our dataset in Fig. 2.\\n\\n4. Automatic Annotation Pipeline\\n\\nTo process massive raw videos efficiently, we develop a pipeline for automatic annotation, hardcase mining, and label generation.\"}"}
{"id": "CVPR-2024-1076", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overview of HardMo. It contains: (a) challenging and artistic motions from diverse types of dance and martial arts, (b) Hand-hardcase with precise annotations, and (c) Foot-hardcase with precise annotations.\\n\\n4. Normal Annotation\\n\\nGiven a video sequence, we first employ YOLOv8 [15, 33] to detect bounding boxes of persons. Then, the person images are cropped accordingly for subsequent annotation. In terms of 2D keypoint annotation, we apply RTM [9, 14] to estimate the whole-body keypoints $K_{2D}$ from the cropped image including body keypoints $K_{2D\\\\text{body}} \\\\in \\\\mathbb{R}^{23 \\\\times 2}$ and hand keypoints $K_{2D\\\\text{hand}} \\\\in \\\\mathbb{R}^{42 \\\\times 2}$. We discard the facial keypoints since SMPL does not cover facial expressions. To ensure the reliability of 2D keypoint annotations, we exclude samples with an average keypoint confidence below 0.5. As for 3D annotation, we predict the SMPL $\\\\Theta$ using the state-of-the-art body mesh recovery method, 4D-Humans [10].\\n\\n4.2. Hardcase Annotation\\n\\nIn preliminary experiments, SMPL annotations failed to meet our requirement for quality in some cases. We empirically find that the failures mainly derive from extreme hand and foot posture. To delve into the effect of these hardcases, we design an algorithm to collect such samples automatically. As mentioned above, these hardcases suffer from imprecise 3D SMPL annotations. To correct their annotations, an intuitive solution is to employ some optimization methods like SMPLify [5] or EFT [17]. However, these methods damage the annotation precision of other parts and may lead to implausible poses. For this reason, we instead devise a learning-based method to refine the SMPL annotations.\\n\\nHardcase Mining.\\n\\nTo mine specific hardcase samples, we propose an angle-based mining algorithm. As mentioned in Sec. 1, the existing methods tend to recover meshes where...\"}"}
{"id": "CVPR-2024-1076", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2D human pose estimation: New benchmark and state of the art analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3686\u20133693, 2014.\\n\\n[2] Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt Schiele. Posetrack: A benchmark for human pose estimation and tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5167\u20135176, 2018.\\n\\n[3] Anurag Arnab, Carl Doersch, and Andrew Zisserman. Exploiting temporal context for 3D human pose estimation in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3395\u20133404, 2019.\\n\\n[4] Michael J Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. Bedlam: A synthetic dataset of bodies exhibiting detailed lifelike animated motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8726\u20138737, 2023.\\n\\n[5] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image. In Computer Vision \u2013 ECCV 2016. Springer International Publishing, 2016.\\n\\n[6] Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei, Daxuan Ren, Zhengyu Lin, Haiyu Zhao, Lei Yang, Chen Change Loy, and Ziwei Liu. Playing for 3D human recovery. arXiv preprint arXiv:2110.07588, 2021.\\n\\n[7] Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, et al. Humman: Multi-modal 4D human dataset for versatile sensing and modeling. In European Conference on Computer Vision, pages 557\u2013577. Springer, 2022.\\n\\n[8] Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee. Beyond static features for temporally consistent 3D human pose and shape from a video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1964\u20131973, 2021.\\n\\n[9] MMPose Contributors. Openmmlab pose estimation tool-box and benchmark. https://github.com/openmmlab/mmpose, 2020.\\n\\n[10] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa*, and Jitendra Malik*. Humans in 4D: Reconstructing and tracking humans with transformers. In International Conference on Computer Vision (ICCV), 2023.\\n\\n[11] Chunhui Gu, Chen Sun, David A Ross, Carl von Ondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: A video dataset of spatio-temporally localized atomic visual actions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6047\u20136056, 2018.\\n\\n[12] Peng Guan, Alexander Weiss, Alexandru O Balan, and Michael J Black. Estimating human shape and pose from a single image. In 2009 IEEE 12th International Conference on Computer Vision, pages 1381\u20131388. IEEE, 2009.\\n\\n[13] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3D human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence, 36(7):1325\u20131339, 2013.\\n\\n[14] Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, and Kai Chen. Rtmpose: Real-time multi-person pose estimation based on mmpose, 2023.\\n\\n[15] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. YOLO by Ultralytics, 2023.\\n\\n[16] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: A massively multiview system for social motion capture. In Proceedings of the IEEE International Conference on Computer Vision, pages 3334\u20133342, 2015.\\n\\n[17] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Example fine-tuning for 3D human pose fitting towards in-the-wild 3D human pose estimation. In 3DV, 2020.\\n\\n[18] Angjoo Kanazawa, Michael J Black, David W Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7122\u20137131, 2018.\\n\\n[19] Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jitendra Malik. Learning 3D human dynamics from video. In Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\n[20] Muhammed Kocabas, Nikos Athanasiou, and Michael J Black. Vibe: Video inference for human body pose and shape estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5253\u20135263, 2020.\\n\\n[21] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis. Learning to reconstruct 3D human pose and shape via model-fitting in the loop. In ICCV, 2019.\\n\\n[22] Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman, and Kostas Daniilidis. Probabilistic modeling for human mesh recovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11605\u201311614, 2021.\\n\\n[23] Ruilong Li, Shan Yang, David A. Ross, and Angjoo Kanazawa. Learn to dance with aist++: Music conditioned 3D dance generation, 2021.\\n\\n[24] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, and Youliang Yan. Cliff: Carrying location information in full frames into human pose and shape estimation. In European Conference on Computer Vision, pages 590\u2013606. Springer, 2022.\\n\\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, 1637.\"}"}
{"id": "CVPR-2024-1076", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[26] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1\u2013248:16, 2015.\\n\\n[27] Zhengyi Luo, S Alireza Golestaneh, and Kris M Kitani. 3d human motion estimation via motion compression and refinement. In Proceedings of the Asian Conference on Computer Vision, 2020.\\n\\n[28] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3d human pose estimation in the wild using improved cnn supervision. In 3D Vision (3DV), 2017 Fifth International Conference on. IEEE, 2017.\\n\\n[29] Priyanka Patel, Chun-Hao P Huang, Joachim Tesch, David T Hoffmann, Shashank Tripathi, and Michael J Black. Agora: Avatars in geography optimized for regression analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13468\u201313478, 2021.\\n\\n[30] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black. Expressive body capture: 3d hands, face, and body from a single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10975\u201310985, 2019.\\n\\n[31] Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Human mesh recovery from multiple shots. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1485\u20131495, 2022.\\n\\n[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.\\n\\n[33] Dillon Reis, Jordan Kupec, Jacqueline Hong, and Ahmad Daoudi. Real-time flying object detection with yolov8. arXiv preprint arXiv:2305.09972, 2023.\\n\\n[34] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas J Guibas. Humor: 3d human motion model for robust pose estimation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 11488\u201311499, 2021.\\n\\n[35] Leonid Sigal, Alexandru O Balan, and Michael J Black. Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. International journal of computer vision, 87(1-2):4\u201327, 2010.\\n\\n[36] Garvita Tiwari, Dimitrije Anti\u0107, Jan Eric Lenssen, Nikolaos Sarafianos, Tony Tung, and Gerard Pons-Moll. Pose-ndf: Modeling human pose manifolds with neural distance fields. In European Conference on Computer Vision, pages 572\u2013589. Springer, 2022.\\n\\n[37] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto. Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing. In Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019, pages 501\u2013510, Delft, Netherlands, 2019.\\n\\n[38] Timo von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and a moving camera. In European Conference on Computer Vision (ECCV), 2018.\\n\\n[39] Jiong Wang, Fengyu Yang, Wenbo Gou, Bingliang Li, Danqi Yan, Ailing Zeng, Yijun Gao, Junle Wang, and Ruimao Zhang. Freeman: Towards benchmarking 3d human pose estimation in the wild. arXiv preprint arXiv:2309.05073, 2023.\\n\\n[40] Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, Rui Liang, Wenjia Wang, Shipei Zhou, Guosen Lin, Yanwei Fu, et al. Ai challenger: A large-scale dataset for going deeper in image understanding. arXiv preprint arXiv:1711.06475, 2017.\\n\\n[41] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human and camera motion from videos in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21222\u201321232, 2023.\\n\\n[42] Andrei Zanfir, Elisabeta Marinoiu, and Cristian Sminchisescu. Monocular 3d pose and shape estimation of multiple people in natural scenes\u2014the importance of multiple scene constraints. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2148\u20132157, 2018.\\n\\n[43] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang, Yebin Liu, Limin Wang, and Zhenan Sun. Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop. In Proceedings of the IEEE International Conference on Computer Vision, 2021.\\n\\n[44] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, and Yebin Liu. Pymaf-x: Towards well-aligned full-body model regression from monocular images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\\n\\n[45] Weiyu Zhang, Menglong Zhu, and Konstantinos G Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding. In Proceedings of the IEEE international conference on computer vision, pages 2248\u20132255, 2013.\"}"}
