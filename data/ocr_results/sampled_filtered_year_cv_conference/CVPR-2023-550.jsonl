{"id": "CVPR-2023-550", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Patch is optional during inference phase.\\n\\nSample LR conditional voxel patch (17).\\n\\nSample LR voxel and category label Sample HR voxel patch and category label (18), (16), (19).\\n\\nNoisy voxel at timestep HR noisy voxel at timestep.\\n\\n\\\\[ k \\\\]\\n\\nwhere to Eq. (12), the diffusion process for SDF super-resolution Conditional diffusion-based super-resolution respectively. Note that the patch-based operation is optional.\\n\\n\\\\[ x \\\\]\\n\\ndistribution that have the same shape with voxel-shaped noises sampled from the standard Gaussian.\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[^{\\\\text{note}}\\\\] data distribution to reduce memory consumption. We de-\\n\\ning memory due to the growth of spatial complexity hin-\\n\\nPatch-based learning scheme.\\n\\nGroup normalization [72] through channel-wise scaling and group normalization [72] following [10]. AdaGN is an extension of every residual block using adaptive group normalization lay-\\n\\ners (AdaGN) following [10].\\n\\n\\\\[ z \\\\]\\n\\nembedding vector \\\\( z \\\\) is converged in Transformer [68] and embeddings in the supplementary material.\\n\\nlayer. The detailed network architecture is available in the\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ f \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ s \\\\]\\n\\n\\\\[ R \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\epsilon \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ c \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\gamma \\\\]\\n\\n\\\\[ \\\\delta \\\\]\\n\\n\\\\[ \\\\sigma \\\\]\\n\\n\\\\[ \\\\parallel \\\\]\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\hat{x} \\\\]\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\"}
{"id": "CVPR-2023-550", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Generated 3D shapes with a single-category generative model. Baseline methods generate point clouds with 2,048 points, and SDF-Diffusion produces 3D shapes in the form of T-SDF voxels, which can be converted to meshes directly through zero iso-surface finding algorithms.\\n\\nImplementation details. For both generation and super-resolution models, we share the same network structure regardless of single-category and multi-category generations (i.e., the same network architecture is shared with all of our experiments). For point cloud input, we use the point cloud encoder from the convolutional occupancy network [53] as our point cloud encoder and combine the resulting features using AdaGN. We use the Adam optimizer [28] with $\\\\beta_1 = 0.9$ and $\\\\beta_2 = 0.999$, to train all of our models. We train our method with learning rate $0.0001$ with learning rate warmup at the first epoch and reduce the learning rate when the validation loss is not decreased for 5 epochs. We fix the patch size to $32 \\\\times 32$ for training the super-resolution model regardless of the target resolution. On the other hand, we sample the full resolution of the shape at once without using patches during the inference phase. Following [16], we set diffusion step $T = 1,000$, and linear noise scheduler with $\\\\beta_0 = 0.0001$ and $\\\\beta_T = 0.02$. We reduce sampling steps to 50 using the denoising diffusion implicit model (DDIM) [65] to accelerate sampling speed. We use the exponential moving average (EMA) following [23] for the first stage model. Flip data augmentation is applied to the super-resolution model.\\n\\nDataset. For evaluation, we use the ShapeNet dataset [3] (especially version 1), which consists of 13-category of 3D shapes in mesh representations. In comparison to prior work, our method requires sampled SDF in the training and evaluation phase. For this reason, we convert meshes in the ShapeNet dataset into voxel-shaped SDF similar to the dual octree graph networks [69]. Concretely, we normalize each of the watertight mesh so that it has zero center and unit scale with $0.1$ padding. We then extract SDF voxels at multiple resolutions of $32$ for low-resolution and $64$ and $128$ for high-resolution using Mesh2SDF [70]. While sampling watertight mesh, we manipulate the level set value, which is the parameter of Mesh2SDF, to approximately double cell size (i.e., $0.03125$ for voxel resolution of $64$). We then clip each of the extracted SDF voxel with minimum and maximum thresholds as the same value with double voxel size, and we utilize SDF in the form of truncated signed distance fields (T-SDF) to decrease redundant information. After that, we sample $15K$ points from the watertight meshes at the resolution of $64$, which are used as ground truth for training point cloud-based methods and evaluation purposes. Following [69], the dataset is split into the train, validation, and test datasets. We mainly measure the quality of our method with point clouds sampled from a voxel resolution of $64$ in the validation dataset like previous methods.\\n\\nEvaluation metrics. Following the previous methods [77, 87], we use three metrics (MMD, COV, and 1-NNA [37]) based on both the Chamfer Distance (CD) and the Earth Mover's Distance (EMD) as the main evaluation metrics. MMD, COV, and 1-NNA analyze both quality and variety by quantifying the distributional similarity between the produced forms and the validation set. A detailed explanation of the evaluation method is in the supplementary material.\\n\\nComparison methods. To evaluate the generative ability of our model, we compare it with various methods, such as DDM-based techniques: point voxel diffusion (PVD) [87] and diffusion point cloud (DPM) [41], as well as flow-based technique, PointFlow [77]. As most of the previous methods have been trained using the ShapeNet dataset preprocessed by PointFlow, which only provides point clouds and not SDF, we re-implemented these methods to compare with our approach. It should be noted that since our dataset and PointFlow's are based on the different versions of ShapeNet, and different pre-processing methods, the results may slightly differ from those of the original papers.\\n\\n4.1. Single-Category Generation\\nFollowing previous works [41, 77, 87], we evaluate metrics for 3 models trained with each category (airplane, car, and chair) separately. To ensure a fair comparison with point clouds-based baseline methods that used 2,048 points to compute metrics, we sampled the same number of points from meshes reconstructed by our method.\\n\\nWe show the quantitative comparisons in Table 1. Our method, SDF-Diffusion demonstrated competitive performance in all categories and outperformed other baselines, especially in the 1-NNA (EMD) metric, which is the most important metric designed to overcome the limitations of MMD and COV [77]. In addition, EMD is a more distinctive metric [42] because CD tends to fail to compare point clouds with uneven density, and uniform point cloud density is important to represent 3D shapes well.\\n\\nSince SDF-Diffusion reconstructs continuous meshes first and then samples point clouds from the meshes, the point clouds can be evenly arranged, unlike other methods that generate point clouds directly.\\n\\nFigure 3 shows the qualitative comparisons with the\"}"}
{"id": "CVPR-2023-550", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2. Multi-Category Generation\\n\\nTo assess the generation quality of multi-category generation, we compare our model with PVD, which is a diffusion-based state-of-the-art 3D shape generation method. SDF-Diffusion can seamlessly extend to multi-category generation using the modified version of PVD (PVD* in short) that can generate plausible 3D shapes like the other approaches; especially, we can directly produce continuous 3D meshes. As shown in Fig. 4, SDF-Diffusion is a diffusion-based state-of-the-art 3D shape generation method that can be used for only single-category generation. So, we compare ours with the modified version of PVD (PVD*) that can be trained using multiple super-resolution stages, allowing us to resolve samples. Figure 5 illustrates the 3D shapes generated using SDF-Diffusion.\\n\\n4.3. Application: Shape Completion\\n\\nFor partial 3D shapes, we perform a 3D shape completion task. Without modification of network architecture, we show that our method can be applied to 3D shape completion. Specifically, the second stage of our method takes a voxel-shaped partial SDF voxel instead of a low-resolution condition. Therefore, using the same network architecture, we can seamlessly train a shape completion model by giving a partial SDF voxel as input and a complete SDF voxel as output. Hence, under this condition, we put. Therefore, using the same network architecture, we can consider the second stage of our model as another task: 3D shape completion with voxel condition.\\n\\nTable 2.\\n\\n| Category | PVD* (CD) | PVD* (EMD) | Ours (CD) | Ours (EMD) |\\n|----------|-----------|------------|-----------|------------|\\n| Airplane | 0.57      | 0.59       | 0.58      | 0.57       |\\n| Chair    | 0.62      | 0.59       | 0.60      | 0.62       |\\n| Car      | 0.58      | 0.59       | 0.59      | 0.58       |\\n| Sofa     | 0.57      | 0.59       | 0.58      | 0.57       |\\n| Bench    | 0.59      | 0.59       | 0.59      | 0.59       |\\n| Lamp     | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Display  | 0.57      | 0.59       | 0.57      | 0.57       |\\n| Vessel   | 0.59      | 0.60       | 0.59      | 0.59       |\\n| Cabinet  | 0.62      | 0.60       | 0.62      | 0.62       |\\n| Table    | 0.58      | 0.59       | 0.58      | 0.58       |\\n| Rifle"}
{"id": "CVPR-2023-550", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Visualization of generated 3D shapes at different resolutions. The first stage model generates coarse shapes in the resolution of $32^3$. By performing super-resolution using our second stage model multiple times, we can obtain improved shape detail of 3D shapes with the voxel resolutions of $64^3$ and $128^3$.\\n\\nSDF-Diffusion, as shown in Fig. 6. As you can see, we can generate various 3D shapes from given partial 3D shapes, which shows the applicability and flexibility of the proposed SDF-Diffusion.\\n\\nIn addition, we also demonstrate the generality of our SDF-Diffusion by using it for the task of 3D shape completion with a partially observed point cloud as a condition. To generate the partial point cloud input, we randomly sample 200 points from the preprocessed ShapeNet dataset using the same method employed in [82]. For out-of-distribution data, we acquire partial point clouds from depth images of the Redwood 3DScans dataset [9]. As the point cloud has a different format than the voxel feature generated by our SDF-Diffusion, we incorporate an additional layer to extract and voxelize the point cloud. Specifically, we employ the PointNet layer [55] from the encoder of the convolutional occupancy network [53], which transforms the point cloud feature into a voxel and gives this feature to each layer of SDF-Diffusion along with the AdaGN. The completed 3D shapes are visualized in Fig. 7. We show SDF-Diffusion can successfully apply unseen out-of-distribution data.\\n\\n5. Conclusion\\n\\nIn this work, we have proposed SDF-Diffusion, a novel diffusion-based SDF generation approach for 3D shapes. SDF-Diffusion composed of two-stage generates low-resolution SDF and then produces high-resolution SDF conditioned on the low-resolution one. For memory efficiency, we learn the second stage of SDF-Diffusion (i.e., super-resolution) in a patch-wise manner, which allows us to perform super-resolution several times in high-resolution. In addition, SDF-Diffusion can be seamlessly extended to multi-category 3D shape generation and 3D shape completion. Experiments demonstrate that the proposed method can generate high-quality 3D shapes in the SDF domain with superior generative power.\\n\\nAcknowledgments. This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00612, Geometric and Physical Commonsense Reasoning based Behavior Intelligence for Embodied AI, No.2022-0-00907, Development of AI Bots Collaboration Platform and Self-organizing AI and No.2020-0-01336, Artificial Intelligence Graduate School Program (UNIST)) and Artificial intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT (MSIT, Korea) & Gwangju Metropolitan City.\"}"}
{"id": "CVPR-2023-550", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and generative models for 3D point clouds. In ICML, 2018.\\n\\n[2] Fausto Bernardini, Joshua Mittleman, Holly Rushmeier, Claudio Silva, and Gabriel Taubin. The ball-pivoting algorithm for surface reconstruction. IEEE transactions on visualization and computer graphics, 5(4):349\u2013359, 1999.\\n\\n[3] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\\n\\n[4] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713, 2020.\\n\\n[5] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In ICLR, 2021.\\n\\n[6] Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731, 2016.\\n\\n[7] Zhiqin Chen, Vladimir G. Kim, Matthew Fisher, Noam Aigerman, Hao Zhang, and Siddhartha Chaudhuri. DecorGAN: 3d shape detailization by conditional refinement. In CVPR, 2021.\\n\\n[8] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In CVPR, 2019.\\n\\n[9] Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, and Vladlen Koltun. A large dataset of object scans. arXiv preprint arXiv:1602.02481, 2016.\\n\\n[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021.\\n\\n[11] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016.\\n\\n[12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021.\\n\\n[13] Lin Gao, Tong Wu, Yu-Jie Yuan, Ming-Xian Lin, Yu-Kun Lai, and Hao Zhang. TM-Net: Deep generative networks for textured meshes. ACM Trans. Graph., 2021.\\n\\n[14] Rohit Girdhar, David F. Fouhey, Mikel Rodriguez, and Abhinav Gupta. Learning a predictable and generative vector representation for objects. In ECCV, 2016.\\n\\n[15] Christian Hane, Shubham Tulsiani, and Jitendra Malik. Hierarchical surface prediction for 3D object reconstruction. 3DV, 2017.\\n\\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020.\\n\\n[17] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. JMLR, 2022.\\n\\n[18] Wenlong Huang, Brian Lai, Weijian Xu, and Zhuowen Tu. 3D volumetric modeling with introspective neural networks. AAAI, 33, 2019.\\n\\n[19] Moritz Ibing, Gregor Kobsik, and Leif Kobbelt. Octree transformer: Autoregressive 3D shape generation on hierarchically structured sequences. CoRR, 2021.\\n\\n[20] Moritz Ibing, Isaak Lim, and Leif Kobbelt. 3D shape generation with grid-based implicit functions. In CVPR, 2021.\\n\\n[21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pages 1125\u20131134, 2017.\\n\\n[22] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Proc. NeurIPS, 2022.\\n\\n[23] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, pages 8110\u20138119, 2020.\\n\\n[24] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe. Poisson surface reconstruction. In Proceedings of the fourth Eurographics symposium on Geometry processing, volume 7, 2006.\\n\\n[25] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. ACM TOG, 2013.\\n\\n[26] Hyeongju Kim, Hyeonseung Lee, Woo Hyun Kang, Joun Yeop Lee, and Nam Soo Kim. Softflow: Probabilistic framework for normalizing flow on manifolds. In NeurIPS, 2020.\\n\\n[27] Jinwoo Kim, Jaehoon Yoo, Juho Lee, and Seunghoon Hong. SETVAE: Learning hierarchical composition for generative modeling of set-structured data. In CVPR, 2021.\\n\\n[28] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[29] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2013.\\n\\n[30] Roman Klokov, Edmond Boyer, and Jakob Verbeek. Discrete point flow networks for efficient point cloud generation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, ECCV, 2020.\\n\\n[31] Wei-Jan Ko, Hui-Yu Huang, Yu-Liang Kuo, Chen-Yi Chiu, Li-Heng Wang, and Wei-Chen Chiu. RPG: learning recursive point cloud generation. CoRR, 2021.\\n\\n[32] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In ICLR, 2021.\\n\\n[33] Chun-Liang Li, Manzil Zaheer, Yang Zhang, Barnabas Poczos, and Ruslan Salakhutdinov. Point cloud GAN. CoRR, 2018.\\n\\n[34] Yiyi Liao, Katja Schwarz, Lars M. Mescheder, and Andreas Geiger. Towards unsupervised learning of generative models for 3D controllable image synthesis. In CVPR, 2022.\\n\\n[35] Or Litany, Alex Bronstein, Michael Bronstein, and Ameesh Makadia. Deformable shape completion with graph convolutional autoencoders. In CVPR, 2018.\"}"}
{"id": "CVPR-2023-550", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2023-550", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\\n\\nZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-guang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In CVPR, 2015.\\n\\nJianwen Xie, Yifei Xu, Zilong Zheng, Song-Chun Zhu, and Ying Nian Wu. Generative pointnet: Deep energy-based learning on unordered point sets for 3d generation, reconstruction and classification. In CVPR, 2021.\\n\\nJianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Learning descriptor networks for 3d shape synthesis and analysis. In CVPR, 2018.\\n\\nXingguang Yan, Liqiang Lin, Niloy J Mitra, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Shapeformer: Transformer-based shape completion via sparse representation. In CVPR, 2022.\\n\\nGuandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointflow: 3d point cloud generation with continuous normalizing flows. In ICCV, 2019.\\n\\nLior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. In NeurIPS, 2020.\\n\\nKangxue Yin, Zhiqin Chen, Hui Huang, Daniel Cohen-Or, and Hao Zhang. Logan: Unpaired shape transform in latent overcomplete space. In ACM TOG, 2019.\\n\\nKangxue Yin, Jun Gao, Maria Shugrina, Sameh Khamis, and Sanja Fidler. 3dstylenet: Creating 3d shapes with geometric and texture style variations. In ICCV, 2021.\\n\\nKangxue Yin, Hui Huang, Daniel Cohen-Or, and Hao Zhang. P2p-net: Bidirectional point displacement net for shape transform. In ACM TOG, 2018.\\n\\nXumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, and Jie Zhou. Pointr: Diverse point cloud completion with geometry-aware transformers. In ICCV, 2021.\\n\\nXiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3d shape generation. In NeurIPS, 2022.\\n\\nDongsu Zhang, Changwoon Choi, Jeonghwan Kim, and Young Min Kim. Learning to generate 3d shapes with generative cellular automata. In ICLR, 2021.\\n\\nYuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao, Yinnan Zhang, Antonio Torralba, and Sanja Fidler. Image gans meet differentiable rendering for inverse graphics and interpretable 3d neural rendering. In ICLR, 2021.\\n\\nX Zheng, Yang Liu, P Wang, and Xin Tong. Sdf-stylegan: Implicit sdf-based stylegan for 3d shape generation. In CGF, 2022.\\n\\nLinqi Zhou, Yilun Du, and Jiajun Wu. 3D shape generation and completion through point-voxel diffusion. In CVPR, 2021.\"}"}
{"id": "CVPR-2023-550", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Diffusion-Based Signed Distance Fields for 3D Shape Generation\\n\\nJaehyeok Shim, Changwoo Kang and Kyungdon Joo\\n\\nArtificial Intelligence Graduate School, UNIST\\n{jh.shim,kangchangwoo,kyungdon}@unist.ac.kr\\n\\nhttps://kitsunetic.github.io/sdf-diffusion\\n\\nFigure 1. Visualization of 3D shape generation for various categories using SDF-Diffusion.\\n\\nSDF-diffusion gradually removes the Gaussian noise and generates high-resolution 3D shapes in the form of an SDF voxel by a two-stage framework. Unlike previous methods, which use point clouds, we can generate 3D meshes without concern of complex post-processing. For visualization purposes, we show less-noise data instead of the initial Gaussian noise.\\n\\nAbstract\\n\\nWe propose a 3D shape generation framework (SDF-Diffusion in short) that uses denoising diffusion models with continuous 3D representation via signed distance fields (SDF). Unlike most existing methods that depend on discontinuous forms, such as point clouds, SDF-Diffusion generates high-resolution 3D shapes while alleviating memory issues by separating the generative process into two-stage: generation and super-resolution. In the first stage, a diffusion-based generative model generates a low-resolution SDF of 3D shapes. Using the estimated low-resolution SDF as a condition, the second stage diffusion model performs super-resolution to generate high-resolution SDF. Our framework can generate a high-fidelity 3D shape despite the extreme spatial complexity. On the ShapeNet dataset, our model shows competitive performance to the state-of-the-art methods and shows applicability on the shape completion task without modification.\\n\\n1. Introduction\\n\\nThe need for generative modeling of 3D shapes has rapidly increased due to high demand in various fields, such as computer vision, graphics, robotics, and content generation. To synthesize a high-quality 3D shape, numerous generative approaches have been actively studied, including generative adversarial networks (GAN) [1, 7, 8, 18, 20, 40, 51, 62, 71, 85], variational auto-encoders (VAE) [13, 27, 35, 45, 67], normalizing flows [26, 30, 60, 77], auto-regressive models [19, 31, 45, 46, 76], and others [61, 74, 75, 80, 84].\\n\\nRecently, denoising diffusion models (DDM) have emerged as a promising generative framework in image generation [10, 16, 22, 39, 47, 63] and speech synthesis [5, 32, 54]. DDM achieves a generative process by gradually corrupting data through a diffusion process and denoising through a learned neural network. This network can generate new realistic data by repeating the denoising process from the given pure noise. Based on the success of DDM-based generative models in various domains, several attempts are being made to apply them to the task of 3D shape generation [41, 83, 87]. They have applied DDM to generate new point cloud and have outperformed previous methods.\"}"}
{"id": "CVPR-2023-550", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"However, even though DDM-based 3D generative models have demonstrated their impressive performance, they still have limitations to being adopted for real-world problems. Most previous DDM-based 3D generative models use point clouds to represent 3D shapes, which limits their ability to express continuous surfaces of 3D shapes, unlike mesh representation commonly used in real-world applications. To resolve this issue, intricate post-processing [2, 24, 25, 52] is required to reconstruct continuous meshes from point clouds. They require various parameter tuning or additional information (e.g., surface normal at each point [52]), and a densely sampled point cloud is required to reconstruct high-quality mesh. However, existing DDM-based methods for 3D shape generation are based on sparse point clouds.\\n\\nIn this work, we introduce a new DDM-based generation framework that generates high-quality 3D shapes through signed distance fields (SDF) (see Fig. 1). We can view SDF as a function that takes an arbitrary location as input and returns a signed distance value from the input location to the nearest surface of the mesh, and the sign of the value means whether inside or outside of the shape. We sample SDF values uniformly from a 3D shape to form a voxel-shaped SDF. This form has several advantages over point clouds. It can directly reconstruct mesh through the marching cube algorithm [38] and can utilize convolutional neural network (CNN) because of its dense and fixed structure. Based on this voxel-shaped SDF representation, we propose a novel DDM-based generative model in two-stage (see Fig. 2). In the first stage, a diffusion-based SDF generation produces low-resolution SDF for coarse 3D shapes. In the second stage, we propose a diffusion-based SDF super-resolution, given the low-resolution SDF of the first stage as a condition. In particular, since the increasing resolution of the voxel comes as a significant burden in terms of computational resources (i.e., due to the curse of the dimension), we approach this problem by super-resolution in patch-based learning. Under this scheme, we can iteratively perform the second stage multiple times with the same structure and generate further higher resolution of SDF, which is a more detailed 3D shape. With various experiments, we achieve state-of-the-art in single-/multi-category 3D shape generative quality, and our method can be directly converted into 3D meshes, unlike other point cloud-based methods that require intricate post-processing.\\n\\nIn summary, our contributions are as follows:\\n\\n\u2022 We propose a novel DDM-based generation method that utilizes a voxel-shaped SDF representation to generate high-quality and continuous 3D shapes.\\n\\n\u2022 We represent a memory-efficient two-stage framework composed of low-resolution SDF generation and SDF super-resolution conditioned on the low-resolution SDF. In particular, we can iteratively perform super-resolution to generate higher-resolution SDF.\\n\\n\u2022 SDF-diffusion can be seamlessly extended into multi-category 3D shape generation and completion, which shows the flexibility of the proposed method.\\n\\n2. Related Work\\n\\n3D Shape representation. Numerous in-depth studies have been conducted on the representation of 3D shape [8, 43, 44, 48\u201350, 64, 73, 78, 79, 81]. We can categorize them into two groups: explicit representations, such as voxels [14, 15, 71], point clouds [1, 55, 56], and implicit functions, such as signed distance fields (SDF) [50, 64]. Voxels are widely adapted in various neural networks due to their intuitive structure and synergy with 3D CNN [14, 15, 71, 73]. However, it may cause a memory problem according to the increase in resolution. Point clouds, which are compact and readily available, are dominant representations with their well-suitability to learn contextual information in neural networks [1, 55, 56, 79, 81]. However, it requires post-processing to convert the synthesized point clouds into meshes for real-world applications [8, 38, 43, 44, 48\u201350, 78].\\n\\nImplicit functions view the 3D shape as a set of levels for a continuous function, which makes it possible to express a continuous surface [8, 44, 48\u201350, 64, 78]. As a result, despite their slow interference as they require numerous network operations, these methods are growing in popularity. In particular, SDF encoded with distance to the nearest surface has some advantages to express sharper reconstructions [50, 64]. Thus, we exploit SDF voxel representation to obtain sharper meshes without unnecessary procedures.\\n\\n3D shape generation. To generate 3D geometry, a variety of frameworks are applied [1, 7, 8, 13, 19, 20, 26, 27, 30, 31, 34, 46, 51, 60, 62, 67, 85]. With such a perspective, several frameworks are applied to model the distribution representing a 3D shape, such as GAN-based models [33], auto-regressive models [66], flow-based models [77], and implicit functions [43, 50].\\n\\nFor GAN-based 3D generative method, SDF-StyleGAN [86] extends StyleGAN2 [23] for 3D shape generation by proposing global and local discriminator that enables GAN discriminator on SDF. Generative methods, such as AutoSDF [45], and ShapeFormer [76], propose to utilize a voxel form of implicit representation (e.g., SDF and occupancy) for mesh generation through a two-stage. A two-stage generative strategy reduces required computational resources by compressing data into a tractable form (e.g., latent feature) rather than directly dealing with an original data structure to make a generative model concentrate semantically compressed representation and save computational resources. Motivated by VQ-GAN [12],...\"}"}
{"id": "CVPR-2023-550", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AutoSDF and ShapeFormer apply two-stage generative method and show impressive performance on conditional 3D shape generation. They utilize an auto-encoder to convert voxel into compressed latent space and generate a new shape using the autoregressive model on the latent space. However, they miss fully utilizing the advantage of the two-stage generative method because they use a regression-based auto-encoder unlike VQ-GAN, which uses GAN and perceptual loss. Unlike the 2D image generation, there are insufficient research works in perceptually plausible auto-encoding methods about 3D shapes, especially voxel structures; it has difficulties in applying a two-stage strategy to 3D shape generation.\\n\\nOn the other hand, recent research works propose a new two-stage generative method. They use a super-resolution model to up-sample low-resolution data, and the generation model learns low-resolution data distribution. Instead of training an auto-encoder, these methods utilize naturally constructed low-resolution forms. Inspired by this, our method exploits a two-stage framework using voxel-shaped SDF. Instead of learning the auto-encoder, we propose a method of generating a coarse 3D shape that is naturally compressed form in the representation of a low-resolution voxel. We then generate high-quality 3D shapes by refining the details of the coarse shape by upsampling the low-resolution voxel.\\n\\n3. Methodology\\n\\nWe propose a generative method for high-resolution 3D shapes in the form of SDF voxels based on the denoising diffusion models (SDF-Diffusion in short). To generate high-resolution samples efficiently, the proposed method involves two stages: diffusion-based SDF generation and diffusion-based patch-wise SDF super-resolution (see Fig. 2 for the overview). In the first stage, the diffusion-based SDF generation model generates a low-resolution coarse 3D shape. Given this low-resolution sample, the diffusion-based SDF super-resolution model progressively produces high-resolution samples in the second stage. In other words, we can consider the first stage as a pure diffusion-based generative model, and the second stage as a conditional diffusion-based super-resolution model. It is worth noting that, ideally, the second stage can be repeated multiple times with the same architecture, which enables us to generate more high-resolution 3D shapes.\\n\\nIn the following, we briefly explain the background on DDM in Sec. 3.1. Then, in Sec. 3.2, we describe our method for generating coarse 3D shapes using a diffusion-based technique. In Sec. 3.3, we explain how we generate high-resolution 3D shapes by super-resolving low-resolution correspondence. Training and inference processes for each diffusion model are listed in Algorithm 1 and Algorithm 2.\\n\\n3.1. Background on Denoising Diffusion Models\\n\\nDDM is a class of generative models that generates new samples from a given data distribution by learning the in-\"}"}
{"id": "CVPR-2023-550", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"directly sampled by: \\\\( \\\\sigma \\\\)\\n\\ncharacterized by: \\\\( \\\\theta \\\\)\\n\\npredicts noise from noisy input: \\\\( \\\\epsilon \\\\)\\n\\nBy repeating this process, we can eventually generate:\\n\\n\\\\[ \\\\mu \\\\]\\n\\n\\\\[ p \\\\]\\n\\n\\\\[ f \\\\]\\n\\n\\\\[ \\\\beta \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\nwhere \\\\( \\\\sigma \\\\) is a time-step dependant variance, and we\\n\\n\\\\[ \\\\beta \\\\]\\n\\n\\\\[ \\\\alpha \\\\]\\n\\n\\\\[ \\\\bar{\\\\alpha} \\\\]\\n\\n\\\\[ \\\\bar{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\bar{\\\\theta} \\\\]\\n\\n\\\\[ \\\\bar{x} \\\\]\\n\\n\\\\[ \\\\bar{I} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c} \\\\]\\n\\n\\\\[ \\\\bar{f} \\\\]\\n\\n\\\\[ \\\\bar{LR} \\\\]\\n\\n\\\\[ \\\\bar{MLP} \\\\]\\n\\n\\\\[ \\\\bar{PosEnc} \\\\]\\n\\n\\\\[ \\\\bar{c}"}
