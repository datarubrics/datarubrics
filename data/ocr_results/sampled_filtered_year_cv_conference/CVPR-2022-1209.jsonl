{"id": "CVPR-2022-1209", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multi-level Feature Learning for Contrastive Multi-view Clustering\\n\\nJie Xu\u2020, Huayi Tang\u2020, Yazhou Ren\u2217, Liang Peng\u2020, Xiaofeng Zhu2, Lifang He3\\n\\n1School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China\\n2Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China, Shenzhen 518000, China\\n3Department of Computer Science and Engineering, Lehigh University, PA 18015, USA\\n\\njiexuwork@outlook.com, tangh4681@gmail.com, yazhou.ren@uestc.edu.cn\\nlarrypengliang@gmail.com, seanzhuxf@gmail.com, lih319@lehigh.edu\\n\\nAbstract\\n\\nMulti-view clustering can explore common semantics from multiple views and has attracted increasing attention. However, existing works punish multiple objectives in the same feature space, where they ignore the conflict between learning consistent common semantics and reconstructing inconsistent view-private information. In this paper, we propose a new framework of multi-level feature learning for contrastive multi-view clustering to address the aforementioned issue. Our method learns different levels of features from the raw features, including low-level features, high-level features, and semantic labels/features in a fusion-free manner, so that it can effectively achieve the reconstruction objective and the consistency objectives in different feature spaces. Specifically, the reconstruction objective is conducted on the low-level features. Two consistency objectives based on contrastive learning are conducted on the high-level features and the semantic labels, respectively. They make the high-level features effectively explore the common semantics and the semantic labels achieve the multi-view clustering. As a result, the proposed framework can reduce the adverse influence of view-private information. Extensive experiments on public datasets demonstrate that our method achieves state-of-the-art clustering effectiveness.\\n\\n1. Introduction\\n\\nMulti-view clustering (MVC) is attracting more and more attention in recent years [22,50,52,57] as multi-view data or multi-modal data can provide common semantics to improve the learning effectiveness [3, 14, 27, 33, 36, 43]. In the literature, existing MVC methods can be roughly divided into two categories, i.e., traditional methods and deep methods. The traditional MVC methods conduct the clustering task based on traditional machine learning methods and can be subdivided into three subgroups, including subspace methods [6, 18, 24], matrix factorization methods [45, 53, 56], and graph methods [28, 55, 60]. Many traditional MVC methods have the drawbacks such as poor representation ability and high computation complexity, resulting in limited performance in the complex scenarios with real-world data [10].\\n\\nRecently, deep MVC methods have gradually become a popular trend in the community due to the outstanding representation ability [1, 2, 20, 44, 49, 50, 54]. Previous deep MVC methods can be subdivided into two subgroups, i.e.,\\n\\n1. Two-stage methods focus on separately learning the salient features from multiple views and performing the clustering task. However, Xie et al. [48] present that the clustering results can be leveraged to improve the quality of feature learning. Therefore, one-stage deep MVC methods (e.g., [39,59]) embed the feature learning with the clustering task in a unified framework to achieve end-to-end clustering.\\n\\nMulti-view data contains two kinds of information, i.e., the common semantics across all views and the view-private information for individual view. For example, a text and an image can be combined to describe common semantics, while the unrelated context in the text and the background pixels in the image are meaningless view-private information for learning common semantics. In multi-view learning, it is an always-on topic to learn common semantics and avoid the misleading of meaningless view-private information. Although important progress has been achieved by existing MVC methods, they have the following drawbacks to be addressed: (1) Many MVC methods (e.g., [39,59]) try to discover the latent cluster patterns by fusing the features of all views. However, the meaningless view-private information might be dominant in the feature fusion process, compared to the common semantics, and thus interferes with the quality of clustering. (2) Some MVC methods (e.g., [18, 21]) leverage the consistency objective on the latent features to explore the common semantics across all views. However, they usually need the reconstruction objective on the same\"}"}
{"id": "CVPR-2022-1209", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We propose a new framework of multi-level feature learning for contrastive multi-view clustering (MFLVC for short) to address the aforementioned issues, as shown in Figure 1. Our goals include (1) designing a fusion-free MVC model to avoid fusing the adverse view-private information among all views and (2) generating different levels of features for the samples in each view including low-level features, high-level features, and semantic labels/features.\\n\\nTo do this, we first leverage the autoencoder to learn the low-level features from raw features, and then obtain the high-level features and semantic labels via stacking two MLPs on low-level features. Each MLP is shared by all views and is conducive to filtering out the view-private information. Furthermore, we take the semantic labels as the anchors, which combine with the cluster information in the high-level features to improve the clustering effectiveness.\\n\\nIn this framework, the reconstruction objective is achieved by the low-level features while two consistency objectives are achieved by the high-level features and the semantic labels, respectively. Moreover, these two consistency objectives are conducted by contrastive learning, which makes the high-level features focus on mining the common semantics across all views and makes the semantic labels represent consistent cluster labels for multi-view clustering, respectively. As a result, the conflict between the reconstruction objective and two consistency objectives is alleviated.\\n\\nCompared to previous works, our contributions are listed as follows:\\n\u2022 We design a fusion-free MVC method which conducts different objectives in different feature spaces to solve the conflict between the reconstruction and consistency objectives. In this way, our method is able to effectively explore the common semantics across all views and avoid their meaningless view-private information.\\n\u2022 We propose a flexible multi-view contrastive learning framework, which can be used to simultaneously achieve the consistency objectives for the high-level features and the semantic labels. The high-level features enjoy good manifolds and represent common semantics, which enable to improve the quality of semantic labels.\\n\u2022 Our method is robust to the hyper-parameters' setting due to the well-designed framework. We conduct ablation studies in details, including the loss components and contrastive learning structures to understand the proposed model. Extensive experiments demonstrate that it achieves state-of-the-art clustering effectiveness.\"}"}
{"id": "CVPR-2022-1209", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The relaxation of $K$-means [26]. For example, Cai et al. [4] introduced a shared clustering indicator matrix for multiple views and handled a constrained matrix factorization problem. The third category of MVC methods is graph based MVC [28, 34], where graph structures are build to preserve the adjacency relationship among samples. The fourth category of MVC methods is based on deep learning framework, as known as deep MVC methods, which have been exploited increasingly and can be further roughly divided into two groups, i.e., two-stage deep MVC methods [21, 50] and one-stage deep MVC methods [20, 51, 59]. These methods utilize the excellent representation ability of deep neural networks to discover the latent cluster patterns of multi-view data.\\n\\nContrastive learning. Contrastive learning [7, 42] is an attention-getting unsupervised representation learning method, with the idea that maximizing the similarities of positive pairs while minimizing that of negative pairs in a feature space. This learning paradigm has lately achieved promising performance in computer vision, such as [29, 40]. For example, a one-stage online image clustering method was proposed in [19], which explicitly conducted contrastive learning in the instance-level and cluster-level. For multi-view learning, there are also some works based on contrastive learning [12, 21, 35, 38]. For instance, Tian et al. [38] proposed a contrastive multi-view coding framework to capture underlying scene semantics. In [12], the authors developed a multi-view representation learning method to tackle graph classification via contrastive learning. Recently, some works investigated different contrastive learning frameworks for multi-view clustering [21, 31, 39].\\n\\n3. Method\\n\\nRaw features. A multi-view dataset $\\\\{X_m \\\\in \\\\mathbb{R}^{N \\\\times D_m}\\\\}_{m=1}^M$ includes $N$ samples across $M$ views, where $x_m^i \\\\in \\\\mathbb{R}^{D_m}$ denotes the $D_m$-dimensional sample from the $m$-th view. The dataset is treated as the raw features where multiple views have $K$ common cluster patterns to be discovered.\\n\\n3.1. Motivation\\n\\nThe multi-view data usually have redundancy and random noise, so the mainstream methods always learn salient representations from raw features. In particular, autoencoder [13, 37] is a widely used unsupervised model and it can project the raw features into a customizable feature space. Specifically, for the $m$-th view, we denote $E_m(X_m; \\\\theta_m)$ and $D_m(Z_m; \\\\phi_m)$, respectively, as the encoder and the decoder, where $\\\\theta_m$ and $\\\\phi_m$ are network parameters, denote $z_m^i = E_m(x_m^i) \\\\in \\\\mathbb{R}^L$ as the $L$-dimensional latent feature of the $i$-th sample, and denote $L_m Z$ as the reconstruction loss between input $X_m$ and output $\\\\hat{X}_m \\\\in \\\\mathbb{R}^{N \\\\times D_m}$, so the reconstruction objective of all views is formulated as:\\n\\n$$L_Z = \\\\sum_{m=1}^M \\\\sum_{i=1}^N \\\\|x_m^i - D_m(E_m(x_m^i))\\\\|_2^2.$$  \\n\\nBased on $\\\\{Z_m = E_m(X_m)\\\\}_{m=1}^M$, MVC aims to mine the common semantics across all views to improve the clustering quality. To achieve this, existing MVC methods still have two challenges to be addressed: (1) Many MVC methods (e.g., [20, 59]) fuse the features of all views $\\\\{Z_m\\\\}_{m=1}^M$ to obtain a common representation for all views. In this way, the multi-view clustering task is transformed to single-view clustering task by conducting clustering directly on the fused features. However, the features of each view $Z_m$ contain the common semantics as well as the view-private information. The latter is meaningless or even misleading, which might interfere with the quality of fused features and result in poor clustering effectiveness. (2) Some MVC methods (e.g., [8, 21]) learn consistent multi-view features to explore the common semantics by conducting a consistency objective on $\\\\{Z_m\\\\}_{m=1}^M$, e.g., minimizing the distance of correlational features across all views. However, they also apply Eq. (1) to punish constraints on $\\\\{Z_m\\\\}_{m=1}^M$ to avoid the model collapse and producing trivial solutions [11, 21]. The consistency objective and the reconstruction objective are pushed on the same features, so that their conflict may limit the quality of $\\\\{Z_m\\\\}_{m=1}^M$. For example, the consistency objective aims to learn the common semantics while the reconstruction objective hopes to maintain the view-private information. Recently, contrastive learning becomes popular and can be applied to achieve the consistency objective for multiple views. For instance, Trosten et al. [39] proposed a one-stage contrastive MVC method but its feature fusion suffers from challenge (1). Lin et al. [21] presented a two-stage contrastive MVC method by learning consistent features, but it does not consider challenge (2). Additionally, many contrastive learning methods (e.g., [19, 30, 40]) mainly handle single-view data with data augmentation. Such specific structure makes it difficult be applied in multi-view scenarios.\\n\\nTo address the aforementioned challenges, we propose a new framework of multi-level feature learning for contrastive multi-view clustering (named MFLVC) as shown in Figure 1. Specially, to reduce the adverse influence of view-private information, our framework avoids the direct feature fusion and builds a multi-level feature learning model for each view. To alleviate the conflict between the consistency objective and the reconstruction objective, we propose to conduct them in different feature spaces, where the consistency objective is achieved by the following multi-view contrastive learning.\\n\\n3.2. Multi-view Contrastive Learning\\n\\nSince the features $\\\\{Z_m\\\\}_{m=1}^M$ obtained by Eq. (1) mix the common semantics with the view-private information, we treat $\\\\{Z_m\\\\}_{m=1}^M$ as low-level features and learn another\"}"}
{"id": "CVPR-2022-1209", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning semantic labels.\\n\\nIn consequence, the features of each view can be written as:\\n\\n\\\\[ \\\\{ \\\\cdot \\\\} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\nwhere \\\\( \\\\cdot \\\\) denotes the temperature parameter. In this paper, \\\\( \\\\tau \\\\) is set to the Softmax operation to output the probability, e.g. set to the Softmax operation to output the probability, and that of negative pairs should be minimized. Inspired by NT-Xent [7], the cosine distance is applied to measure the similarity between two features:\\n\\n\\\\[ \\\\log \\\\frac{\\\\| \\\\cdot \\\\|}{\\\\| \\\\cdot \\\\|} \\\\]\\n\\nand that of positive pairs should be maximized. The similarities of positive pairs and the rest of negative pairs need to be consistent. The consistency objectives \\\\( \\\\ell \\\\) are constructed as:\\n\\n\\\\[ (i, j) \\\\text{ positive feature pairs, the rest } (i, j) \\\\text{ negative label pairs.} \\\\]\\n\\nIn real-world scenarios, however, some views of a sample might have wrong cluster labels due to the misleading of other views. Therefore, we need to leverage the clustering consistency for all views. The second term of Eq. (6) is a regularization term [40], which is usually used to avoid all samples being assigned into a single cluster. In order to obtain robustness, we simultaneously learn the high-level features and the cluster assignments, respectively. We learn the clustering consistency for all views. The second part of Eq. (6) is a regularization term [40], which is usually used to avoid all samples being assigned into a single cluster.\\n\\nSpecifically, each high-level feature \\\\( h_m \\\\) is formulated as:\\n\\n\\\\[ h_m = \\\\text{fc} \\\\cdot \\\\theta m \\\\]\\n\\nwhere \\\\( \\\\theta m \\\\) is the high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make the high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we further achieve the consistency objective by contrastive learning to make high-level feature space, we"}
{"id": "CVPR-2022-1209", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the consistent cluster assignments\\n\\\\{Q_m^M\\\\}_{m=1}^M. We then treat\\n\\\\{Q_m^M\\\\}_{m=1}^M as anchors and match them with the clusters\\namong \\\\{H_m^M\\\\}_{m=1}^M. In this way, we can leverage the cluster\\ninformation contained in the high-level features to improve\\nthe clustering effectiveness of the semantic labels.\\n\\nConcretely, we adopt $K$-means [26] to obtain the clus-\\nter information of each view. For the $m$-th view, letting\\n\\\\{c_m^k^K\\\\}_{k=1}^K \\\\in \\\\mathbb{R}^H$ denote the\\n$K$ cluster centroids, we have:\\n\\n$$\\\\min_{c_m^1, c_m^2, \\\\ldots, c_m^K} \\\\sum_{X_i=1}^N \\\\sum_{X_j=1}^K (h_m^i - c_m^j)^2.$$  (8)\\n\\nThe cluster labels of all samples $p_m^i \\\\in \\\\mathbb{R}^N$ are obtained by:\\n\\n$$p_m^i = \\\\arg\\\\min_j (h_m^i - c_m^j)^2.$$  (9)\\n\\nLet $l_m^i \\\\in \\\\mathbb{R}^N$ denote the cluster labels outputted by the\\nlabel MLP, where\\n\\n$$l_m^i = \\\\arg\\\\max_j q_m^{ij},$$\\n\\nit is worth noting that\\nthe clusters represented by $p_m^i$ and $l_m^i$ are not corresponding\\nto each other. Because the clustering consistency is achieved\\nby Eq. (6) in advance,\\n\\n$$l_m^i \\\\text{ and } l_n^i \\\\text{ represent the same cluster.}$$\\n\\nTherefore, we can treat $l_m^i$ as anchors to modify\\n$p_m^i$ by the\\nfollowing maximum matching formula:\\n\\n$$\\\\min_A \\\\sum_{m=1}^M \\\\sum_{a_{mj}=1} (a_{mj} = 1),$$\\n\\ns.t.\\n\\n$$X_i=1 \\\\quad a_{mj}=1,$$\\n\\n$$X_j=1 \\\\quad a_{mj}=1,$$\\n\\n$$a_{mj} \\\\in \\\\{0, 1\\\\}, \\\\quad i, j = 1, 2, \\\\ldots, K,$$  (10)\\n\\nwhere $A \\\\in \\\\{0, 1\\\\}^{K \\\\times K}$ is the boolean matrix and\\n$M_{m} \\\\in \\\\mathbb{R}^{K \\\\times K}$ denotes the cost matrix.\\n\\nLet $\\\\tilde{M}_{m} \\\\in \\\\mathbb{R}^{K \\\\times K}$ denote the\\nmodified cluster assignments\\n\\\\hat{p}_m^i \\\\in \\\\{0, 1\\\\}^K for the\\n$i$-th sample is defined as a\\none-hot vector. The\\n$k$-th element of $\\\\hat{p}_m^i$ is 1 when\\n\\n$$k = k_1 \\\\quad a_{mks}=1,$$\\n\\n$$k, s \\\\in \\\\{1, 2, \\\\ldots, K\\\\}.$$ We then\\nfine-tune the model by cross-entropy loss:\\n\\n$$L_P = -\\\\sum_{m=1}^M \\\\hat{P}_m \\\\log Q_m,$$  (11)\\n\\nwhere $\\\\hat{P}_m = [\\\\hat{p}_m^1; \\\\hat{p}_m^2; \\\\ldots; \\\\hat{p}_m^N] \\\\in \\\\mathbb{R}^{N \\\\times K}$. In this way, we\\ncan transfer the learned semantic knowledge to improve the\\nclustering. Finally, the semantic label of the\\n$i$-th sample is:\\n\\n$$y_i = \\\\arg\\\\max_j \\\\prod_{X_m=1}^M q_m^{ij}!.$$  (12)\\n\\nOptimization. The full optimization process of MFLVC\\nis summarized in Algorithm 1. To be specific, we adopt the\\nalgorithm of mini-batch gradient descent to train the model,\\nwhich consists of multiple autoencoders, a feature MLP, and\\na label MLP. The autoencoders are initialized by Eq. (1).\\n\\n---\\n\\n| Datasets   | #Samples | #Views | #Classes |\\n|------------|----------|--------|----------|\\n| MNIST-USPS | 5,000    | 2      | 10       |\\n| BDGP       | 2,500    | 2      | 5        |\\n| CCV        | 6,773    | 3      | 20       |\\n| Fashion    | 10,000   | 3      | 10       |\\n| Caltech-2V | 1,400    | 2      | 7        |\\n| Caltech-3V | 1,400    | 3      | 7        |\\n| Caltech-4V | 1,400    | 4      | 7        |\\n| Caltech-5V | 1,400    | 5      | 7        |\\n\\nTable 1. The information of the datasets in our experiments.\"}"}
{"id": "CVPR-2022-1209", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Datasets\\n\\n**MNIST-USPS**\\n\\n**BDGP**\\n\\n**CCV**\\n\\n**Fashion**\\n\\nEvaluation metrics\\n\\n| Method        | ACC | NMI | PUR |\\n|---------------|-----|-----|-----|\\n| RMSL [18] (2019) | 0.424 | 0.318 | 0.428 |\\n| MVC-LFA [41] (2019) | 0.849 | 0.630 | 0.849 |\\n| COMIC [34] (2019) | 0.215 | 0.157 | 0.243 |\\n| CDIMC-net [44] (2020) | 0.408 | 0.405 | 0.421 |\\n| EAMC [59] (2020) | 0.768 | 0.675 | 0.768 |\\n| IMVTSC-MVI [46] (2021) | 0.578 | 0.642 | 0.639 |\\n| SiMVC [39] (2021) | 0.578 | 0.642 | 0.608 |\\n| CoMVC [39] (2021) | 0.620 | 0.676 | 0.647 |\\n| MFLVC (ours) | 0.735 | 0.837 | 0.778 |\\n\\nTable 2. Results of all methods on four datasets. Bold denotes the best results and underline denotes the second-best.\\n\\nDatasets\\n\\n**Caltech-2V** includes WM and CENTRIST;\\n\\n**Caltech-3V** includes WM, CENTRIST, and LBP;\\n\\n**Caltech-4V** includes WM, CENTRIST, LBP, and GIST;\\n\\n**Caltech-5V** includes WM, CENTRIST, LBP, GIST, and HOG.\\n\\nImplementation. All the datasets are reshaped into vectors, and the fully connected networks with the similar architecture are adopted to implement the autoencoders for all views in our MFLVC. Adam optimizer [17] is adopted for optimization. The code of MFLVC is implemented by PyTorch [32]. More implementation details are provided in https://github.com/SubmissionsIn/MFLVC.\\n\\nComparison methods. The comparison methods include classical and state-of-the-art methods, i.e., 4 traditional methods (RMSL [18], MVC-LFA [41], COMIC [34], and IMVTSC-MVI [46]) and 4 deep methods (CDIMC-net [44], EAMC [59], SiMVC [39], and CoMVC [39]).\\n\\nEvaluation metrics. The clustering effectiveness is evaluated by three metrics, i.e., clustering accuracy (ACC), normalized mutual information (NMI), and purity (PUR). The mean values of 10 runs are reported for all methods.\\n\\n4.2. Result Analysis\\n\\nThe comparison results on four datasets are shown in Table 2, where many comparison methods (e.g., RMSL and COMIC) punish multiple objectives on the same features, and CDIMC-net, EAMC, SiMVC, and CoMVC are feature fusion methods. One could find that:\\n\\n1. Our MFLVC achieves the best performance in terms of all metrics. Especially on Dataset Fashion, MFLVC outperforms the best comparison method CoMVC (i.e., 85%) by about 14% in terms of ACC. This is because our model is fusion-free and it conducts the reconstruction objective and the consistency objective in different feature spaces so that the adverse influence of view-private information can be reduced.\\n\\n2. The improvements obtained by the previous contrastive MVC method (i.e., CoMVC) are limited. Our MFLVC is also a contrastive MVC method, instead, it avoids the fusion of view-private information and its multi-level feature learning framework allows the high-level features to learn the common semantics across all views more effectively.\\n\\nTo further verify our method, we build four datasets based on Caltech and test the performance of all comparison methods. Table 3 shows the results on Caltech with different views, from which we could have the following observations:\\n\\n1. The clustering effectiveness of most methods improves with the increase of the number of views, i.e., ACC increases from 60% to 80%.\\n\\n2. Compared to 8 comparison methods, our MFLVC mostly achieves the best performance indicating its robustness.\\n\\n3. Some methods obtain bad results when increasing the number of views. For example, RMSL, COMIC, and EAMC achieve ACC about 35%, 53%, and 31% on Caltech-5V which are lower than that on Caltech-4V.\"}"}
{"id": "CVPR-2022-1209", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Visualization of low-level features (a-d) and high-level features (e-h) for the contrastive learning process.\\n\\nFigure 3. (a) The similarities of feature pairs and label pairs. (b) Convergence analysis. (c) and (d) Parameters sensitivity analysis.\\n\\n(i.e., 59%, 63%, and 35%). The reason is that the data of each view simultaneously contain useful common semantics as well as meaningless view-private information. Views contain much view-private information which might increase the difficulty of extracting their common semantics. These observations further verify the effectiveness of our method, which learns multiple levels of features so as to reduce the interference from the view-private information.\\n\\n5. Model Analysis\\n\\n5.1. Understand the Multi-level Feature Learning\\n\\nIn order to investigate the proposed multi-level feature learning, we take MNIST-USPS as an example and visualize its training process. The MNIST view is shown in Figure 2 via t-SNE [25]. It can be discovered that the cluster structures of low-level features and high-level features become clear during the training process. The clusters of low-level features are not dense. This is because the low-level features have maintained the diversity among samples by reconstruction objective. In contrast, the clusters of high-level features are dense and have better low-dimensional manifolds. Additionally, in Figure 3(a), the similarities of positive feature pairs are rising while that of negative feature pairs are decreasing. This indicates that the information learned by the high-level features is close to the common semantics across multiple views. These observations are in agreement with our motivations, i.e., the feature MLP can filter out the view-private information of multiple views so the outputted high-level features are in dense shapes. The similarities of positive label pairs are also rising which indicates that the clustering consistency of semantic labels is achieved.\\n\\nConvergence analysis. It is not difficult to discover that the objectives of $L_Z$, $L_H$, $L_Q$, and $L_P$, i.e., Eqs. (1,4,6,11) are all convex functions. As shown in Figure 3(b), the clustering effectiveness increases with the decrease of loss values, indicating that MFLVC enjoys good convergence property.\\n\\nParameter sensitivity analysis. We investigate whether hyper-parameters are needed to balance the loss components in Eq. (7), i.e., $L_Z + \\\\lambda_1 L_H + \\\\lambda_2 L_Q$. Figure 3(c) shows the mean values of NMI within 10 independent runs, which indicates that our model is insensitive to $\\\\lambda_1$ and $\\\\lambda_2$. This is because our model has a well-designed multi-level feature learning framework, by which the interference among different view-private information is reduced.\"}"}
{"id": "CVPR-2022-1209", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Ablation studies on loss components.\\n\\n| Component | ACC | NMI |\\n|-----------|-----|-----|\\n| L_Q      | 0.676 | 0.777 |\\n| L_Z      | 0.891 | 0.939 |\\n| L_H      | 0.984 | 0.962 |\\n| L_P      | 0.995 | 0.985 |\\n\\nTable 5. Ablation studies on contrastive learning structures.\\n\\n- (a) X\u2212Q: The semantic labels Q are learned directly from the input features X. This structure is similar to \\([29, 40, 58]\\) in some degree. It results in poor performance by directly extending contrastive learning to the multi-view scenarios.\\n- (b) X\u2212Z: Between X and Q, we set the low-level features Z and perform contrastive learning on Q and Z. This structure is similar to \\([19, 21, 39]\\) in some degree and the performance is also limited.\\n- (c) Z: Based on Z, we stack a feature MLP to obtain the high-level features H and perform contrastive learning on Z, H, and Q. As for (b) and (c), the reconstruction objective is also performed on Z. (b) and (c) make progress on MNIST-USPS, because the two views of MNIST-USPS are digital images and they have little view-private information to influence the learning performance. However, (b) and (c) cannot mine the common semantics well on BDGP. The reason is that the two views of BDGP are visual features and text features and they have much view-private information. It results in poor performance when performing reconstruction and consistency objectives on the same features (i.e., Z).\\n- (d) H: We perform contrastive learning only on H and Q while leaving reconstruction objective on Z. This setting obtains the best performance by performing consistency and reconstruction objectives in different feature spaces. These experiments further verified the effectiveness of our method, and confirmed that it is useful to learn representations via a multi-level feature learning structure.\\n\\n6. Conclusion\\n\\nIn this paper, we have proposed a new framework of multi-level feature learning for contrastive multi-view clustering. For each view, the proposed framework learns multiple levels of features, including low-level features, high-level features, and semantic labels in a fusion-free manner. This allows our model to learn the common semantics across all views and reduce the adverse influence of view-private information. Extensive experiments on five public datasets demonstrate that our method obtains state-of-the-art performance.\\n\\nBroader impacts. The proposed framework learned a high-level feature extractor and a label predictor, which can be applied to downstream tasks such as feature compression, unsupervised labeling, and cross-modal retrieval, etc. However, this work aims to provide a general framework and the trained model might be affected by the intrinsic bias of data especially with dirty samples. Therefore, the future works could extend our framework to other application scenarios.\\n\\nAcknowledgments\\n\\nThis work was partially supported by the National Natural Science Foundation of China (Grants No. 61806043 and No. 61876046) and the Guangxi \u201cBagui\u201d Teams for Innovation and Research, China. Lifang He was supported by the Lehigh\u2019s Accelerator Grant (No. S00010293).\"}"}
{"id": "CVPR-2022-1209", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Mahdi Abavisani and Vishal M Patel. Deep multimodal sub-space clustering networks. IEEE Journal of Selected Topics in Signal Processing, 12(6):1601\u20131614, 2018.\\n\\n[2] Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, and Du Tran. Self-supervised learning by cross-modal audio-video clustering. In NeurIPS, pages 9758\u20139770, 2019.\\n\\n[3] Yuki M Asano, Mandela Patrick, Christian Rupprecht, and Andrea Vedaldi. Labelling unlabelled videos from scratch with multi-modal self-supervision. In NeurIPS, pages 4660\u20134671, 2020.\\n\\n[4] Xiao Cai, Feiping Nie, and Heng Huang. Multi-view k-means clustering on big data. In IJCAI, pages 2598\u20132604, 2013.\\n\\n[5] Xiao Cai, Hua Wang, Heng Huang, and Chris Ding. Joint stage recognition and anatomical annotation of drosophila gene expression patterns. Bioinformatics, 28(12):i16\u2013i24, 2012.\\n\\n[6] Xiaochun Cao, Changqing Zhang, Huazhu Fu, Si Liu, and Hua Zhang. Diversity-induced multi-view subspace clustering. In CVPR, pages 586\u2013594, 2015.\\n\\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, pages 1597\u20131607, 2020.\\n\\n[8] Jiafeng Cheng, Qianqian Wang, Zhiqiang Tao, De-Yan Xie, and Quanxue Gao. Multi-view attribute graph convolution networks for clustering. In IJCAI, pages 2973\u20132979, 2020.\\n\\n[9] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR, pages 178\u2013178, 2004.\\n\\n[10] Jun Guo and Jiahui Ye. Anchors bring ease: An embarrassingly simple approach to partial multi-view clustering. In AAAI, pages 118\u2013125, 2019.\\n\\n[11] Xifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. Improved deep embedded clustering with local structure preservation. In IJCAI, pages 1753\u20131759, 2017.\\n\\n[12] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In ICML, pages 4116\u20134126, 2020.\\n\\n[13] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504\u2013507, 2006.\\n\\n[14] Di Hu, Feiping Nie, and Xuelong Li. Deep multimodal clustering for unsupervised audiovisual learning. In CVPR, pages 9248\u20139257, 2019.\\n\\n[15] Yu-Gang Jiang, Guangnan Ye, Shih-Fu Chang, Daniel Ellis, and Alexander C Loui. Consumer video understanding: A benchmark database and an evaluation of human and machine performance. In ICMR, pages 1\u20138, 2011.\\n\\n[16] Roy Jonker and Ton Volgenant. Improving the hungarian assignment algorithm. Operations Research Letters, 5(4):171\u2013175, 1986.\\n\\n[17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[18] Ruihuang Li, Changqing Zhang, Huazhu Fu, Xi Peng, Tianyi Zhou, and Qinghua Hu. Reciprocal multi-layer subspace learning for multi-view clustering. In ICCV, pages 8172\u20138180, 2019.\\n\\n[19] Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. In AAAI, pages 8547\u20138555, 2021.\\n\\n[20] Zhaoyang Li, Qianqian Wang, Zhiqiang Tao, Quanxue Gao, and Zhaohua Yang. Deep adversarial multi-view clustering network. In IJCAI, pages 2952\u20132958, 2019.\\n\\n[21] Yijie Lin, Yuanbiao Gou, Zitao Liu, Boyun Li, Jiancheng Lv, and Xi Peng. COMPLETER: Incomplete multi-view clustering via contrastive prediction. In CVPR, 2021.\\n\\n[22] Jiyuan Liu, Xinwang Liu, Yuexiang Yang, Li Liu, Siqi Wang, Weixuan Liang, and Jiangyong Shi. One-pass multi-view clustering for large-scale data. In ICCV, pages 12344\u201312353, 2021.\\n\\n[23] Jialu Liu, Chi Wang, Jing Gao, and Jiawei Han. Multi-view clustering via joint nonnegative matrix factorization. In SDM, pages 252\u2013260, 2013.\\n\\n[24] Shirui Luo, Changqing Zhang, Wei Zhang, and Xiaochun Cao. Consistent and specific multi-view subspace clustering. In AAAI, 2018.\\n\\n[25] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:2579\u20132605, 2008.\\n\\n[26] James MacQueen. Some methods for classification and analysis of multivariate observations. In BSMSP, pages 281\u2013297, 1967.\\n\\n[27] Kevis-Kokitsi Maninis, Stefan Popov, Matthias Niesser, and Vittorio Ferrari. Vid2cad: Cad model alignment using multi-view constraints from videos. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\\n\\n[28] Feiping Nie, Jing Li, and Xuelong Li. Self-weighted multi-view clustering with multiple graphs. In IJCAI, pages 2564\u20132570, 2017.\\n\\n[29] Chuang Niu and Ge Wang. SPICE: Semantic pseudo-labeling for image clustering. arXiv preprint arXiv:2103.09382, 2021.\\n\\n[30] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\\n\\n[31] Erlin Pan and Zhao Kang. Multi-view contrastive graph clustering. In NeurIPS, 2021.\\n\\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024\u20138035, 2019.\\n\\n[33] Liang Peng, Yang Yang, Zheng Wang, Zi Huang, and Heng Tao Shen. MRA-Net: Improving vqa via multi-modal relation attention network. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(1):318\u2013329, 2020.\\n\\n[34] Xi Peng, Zhenyu Huang, Jiancheng Lv, Hongyuan Zhu, and Joey Tianyi Zhou. COMIC: Multi-view clustering without parameter selection. In ICML, pages 5092\u20135101, 2019.\"}"}
{"id": "CVPR-2022-1209", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Nicolas Pielawski, Elisabeth Wetzer, Johan \u00a8Ofverstedt, Jiahao Lu, Carolina W\u00a8ahlby, Joakim Lindblad, and Nata\u02c7sa Sladoje. CoMIR: Contrastive multimodal image representation for registration. In NeurIPS, pages 18433\u201318444, 2020.\\n\\nRaeid Saqur and Karthik Narasimhan. Multimodal graph networks for compositional generalization in visual question answering. In NeurIPS, pages 3070\u20133081, 2020.\\n\\nJingkuan Song, Hanwang Zhang, Xiangpeng Li, Lianli Gao, Meng Wang, and Richang Hong. Self-supervised video hashing with hierarchical binary auto-encoder. IEEE Transactions on Image Processing, 27(7):3210\u20133221, 2018.\\n\\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV, pages 776\u2013794, 2020.\\n\\nDaniel J. Trosten, Sigurd L\u00f8kse, Robert Jenssen, and Michael Kampffmeyer. Reconsidering representation alignment for multi-view clustering. In CVPR, pages 1255\u20131265, 2021.\\n\\nWouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. SCAN: Learning to classify images without labels. In ECCV, pages 268\u2013285, 2020.\\n\\nSiwei Wang, Xinwang Liu, En Zhu, Chang Tang, Jiyuan Liu, Jingtao Hu, Jingyuan Xia, and Jianping Yin. Multi-view clustering via late fusion alignment maximization. In IJCAI, pages 3778\u20133784, 2019.\\n\\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML, pages 9929\u20139939, 2020.\\n\\nJiwei Wei, Yang Yang, Xing Xu, Xiaofeng Zhu, and Heng Tao Shen. Universal weighting metric learning for cross-modal retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\n\\nJie Wen, Zheng Zhang, Yong Xu, Bob Zhang, Lunke Fei, and Guo-Sen Xie. CDIMC-net: Cognitive deep incomplete multi-view clustering network. In IJCAI, pages 3230\u20133236, 2020.\\n\\nJie Wen, Zheng Zhang, Zhao Zhang, Lei Zhu, Lunke Fei, Bob Zhang, and Yong Xu. Unified tensor framework for incomplete multi-view clustering and missing-view inferring. In AAAI, pages 10273\u201310281, 2021.\\n\\nHan Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\\n\\nJunyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In ICML, pages 478\u2013487, 2016.\\n\\nJie Xu, Yazhou Ren, Guofeng Li, Lili Pan, Ce Zhu, and Zenglin Xu. Deep embedded multi-view clustering with collaborative training. Information Sciences, 573:279\u2013290, 2021.\\n\\nJie Xu, Yazhou Ren, Huayi Tang, Xiaorong Pu, Xiaofeng Zhu, Ming Zeng, and Lifang He. Multi-V AE: Learning disentangled view-common and view-peculiar visual representations for multi-view clustering. In ICCV, pages 9234\u20139243, 2021.\\n\\nJie Xu, Yazhou Ren, Huayi Tang, Zhimeng Yang, Lili Pan, Yang Yang, and Xiaorong Pu. Self-supervised discriminative feature learning for deep multi-view clustering. arXiv preprint arXiv:2103.15069, 2021.\\n\\nMouxing Yang, Yunfan Li, Peng Hu, Jinfeng Bai, Jian Cheng Lv, and Xi Peng. Robust multi-view clustering with incomplete information. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\\n\\nZuyuan Yang, Naiyao Liang, Wei Yan, Zhenni Li, and Shengli Xie. Uniform distribution non-negative matrix factorization for multiview clustering. IEEE Transactions on Cybernetics, pages 3249\u20133262, 2021.\\n\\nMing Yin, Weitian Huang, and Junbin Gao. Shared generative latent representation learning for multi-view clustering. In AAAI, pages 6688\u20136695, 2020.\\n\\nKun Zhan, Changqing Zhang, Junpeng Guan, and Junsheng Wang. Graph learning for multiview clustering. IEEE Transactions on Cybernetics, 48(10):2887\u20132895, 2017.\\n\\nHandong Zhao, Zhengming Ding, and Yun Fu. Multi-view clustering via deep matrix factorization. In AAAI, pages 2921\u20132927, 2017.\\n\\nGuo Zhong and Chi-Man Pun. Improved normalized cut for multi-view clustering. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\n\\nHuasong Zhong, Chong Chen, Zhongming Jin, and Xian-Sheng Hua. Deep robust clustering by contrastive learning. arXiv preprint arXiv:2008.03030, 2020.\\n\\nRunwu Zhou and Yi-Dong Shen. End-to-end adversarial-attention network for multi-modal clustering. In CVPR, pages 14619\u201314628, 2020.\\n\\nXiaofeng Zhu, Shichao Zhang, Wei He, Rongyao Hu, Cong Lei, and Pengfei Zhu. One-step multi-view spectral clustering. IEEE Transactions on Knowledge and Data Engineering, 31(10):2022\u20132034, 2018.\"}"}
