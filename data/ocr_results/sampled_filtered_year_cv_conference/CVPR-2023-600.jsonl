{"id": "CVPR-2023-600", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata\\n\\nEhsan Pajouheshgar\u2217, Yitao Xu\u2217, Tong Zhang, Sabine S\u00fcsstrunk\\nSchool of Computer and Communication Sciences, EPFL, Switzerland\\n{ehsan.pajouheshgar, yitao.xu, tong.zhang, sabine.susstrunk}@epfl.ch\\n\\nAbstract\\nCurrent Dynamic Texture Synthesis (DyTS) models can synthesize realistic videos. However, they require a slow iterative optimization process to synthesize a single fixed-size short video, and they do not offer any post-training control over the synthesis process. We propose Dynamic Neural Cellular Automata (DyNCA), a framework for real-time and controllable dynamic texture synthesis. Our method is built upon the recently introduced NCA models and can synthesize infinitely long and arbitrary-sized realistic video textures in real time. We quantitatively and qualitatively evaluate our model and show that our synthesized videos appear more realistic than the existing results. We improve the SOTA DyTS performance by $2\\\\sim4$ orders of magnitude. Moreover, our model offers several real-time video controls including motion speed, motion direction, and an editing brush tool. We exhibit our trained models in an online interactive demo that runs on local hardware and is accessible on personal computers and smartphones.\\n\\n1. Introduction\\nTextures are everywhere. We perceive them as spatially repetitive patterns. Dynamic Textures are textures that change over time and induce a sense of motion. Flames, sea waves, and fluttering branches are everyday examples. Understanding and computationally modeling dynamic textures is an intriguing problem, as these patterns are observed in most natural scenes.\\n\\nThe goal of Dynamic Texture Synthesis (DyTS) [5\u20138, 10, 12, 23, 24, 27, 29, 32] is to generate perceptually-equivalent samples of an exemplar video texture. Applications of DyTS include the creation of special effects for backdrops and video games [21], dynamic style transfer [24], and creating cinemagraphs [12].\\n\\nThe state-of-the-art (SOTA) dynamic texture synthesis approach we use the same flow visualization as Baker et al. [2]. Link to the demo: https://dynca.github.io\\n\\n3 We use \u201cvideo texture\u201d and \u201cdynamic texture\u201d interchangeably.\"}"}
{"id": "CVPR-2023-600", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appearance Loss\\n\\\\( \\\\mathcal{F} \\\\)\\nMotion Loss\\n\\\\( \\\\mathcal{S} \\\\)\\n\\\\\\\\\\nFigure 2. Overview of DyNCA. Starting from a seed state, DyNCA iteratively updates it, generating an image sequence. We extract images from this sequence and compare them with an appearance target as well as a motion target to obtain the DyNCA training objectives. After training, DyNCA can adapt to seeds of different heights and widths, and synthesize videos with arbitrary lengths. Sequentially applying DyNCA updates on the seed synthesizes dynamic texture videos in real time.\\n\\nMethods \\\\([8, 24, 27, 29, 32]\\\\) follow the same trend. They aim to find better optimization strategies to iteratively update a randomly initialized video until the synthesized video resembles the exemplar dynamic texture. Although the SOTA methods are able to synthesize acceptable quality videos, their optimization process is very slow and requires several hours to generate a single fixed-resolution short video on a high-end GPU. Moreover, these methods do not offer any post-training control over the synthesized video.\\n\\nIn this paper we propose DyNCA (Dynamic Neural Cellular Automata), a model for dynamic texture synthesis that is fast to train, and once trained, can synthesize infinitely-long, arbitrary-resolution dynamic texture videos in real time on a low-end GPU. Moreover, our method enables several real-time video editing controls, including motion direction and motion speed. Through quantitative and qualitative analysis we demonstrate that our synthesized videos achieve better quality and realism than the previous results in the literature.\\n\\nOur model builds upon the recently introduced NCA (Neural Cellular Automata) model \\\\([18, 19]\\\\). While Niklasson et al. \\\\([19]\\\\) train the NCA with the goal of synthesizing static textures only, the NCA model is able to spontaneously generate randomly moving patterns. As an inherent property of NCA, these spontaneous motions are, however, unstructured and uncontrolled. We modify the architecture and the training scheme of NCA so that it can learn to synthesize video textures that have the desired motion and appearance.\\n\\nIn short, our DyNCA model acts as a stochastic Partial Differential Equation (PDE), parameterized by a small neural network. DyNCA starts from a constant initial state called seed, and then iteratively evolves the state according to its trainable PDE update rule to generate a sequence of images. This image sequence is then evaluated against the appearance exemplar and the motion target to calculate the loss functions for the optimization of DyNCA, as illustrated in Figure 2. We allow the user to specify the desired motion either by a motion vector field or an exemplar dynamic texture video. Moreover, by using a different target for the appearance and the motion, our model can perform dynamic style transfer, as shown in Figure 1. Our contributions summarized are:\\n\\n- Our DyNCA model, once trained, can synthesize dynamic texture videos in real time.\\n- Our synthesized videos are on-par with or even better than the existing results in terms of realism and quality.\\n- After training, our DyNCA model can synthesize infinitely-long videos with arbitrary frame sizes.\\n- Our DyNCA framework enables several real-time interactive video editing controls including speed, direction, a brush tool, and local coordinate transformation.\\n- We can perform real-time dynamic style transfer by learning appearance and motion from distinct sources.\\n\\n2. Related Works\\n\\nIn the following section, we discuss the existing DyTS methods. For comparison, Table 1 summarizes the strengths (\u2713) and shortcomings (\u2717) of these methods.\"}"}
{"id": "CVPR-2023-600", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison of DyTS methods.\\n\\n| Method                  | Can synthesize videos with arbitrary frame size after training. | Can synthesize arbitrarily long videos. | Can synthesize new video samples without re-training. | Allows real-time video editing (speed, direction, and a brush tool). | Does not rely on pre-trained models to extract motion or texture information. | Has disentangled appearance and motion. | Can learn motion from vector fields. |\\n|------------------------|---------------------------------------------------------------|----------------------------------------|-----------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|----------------------------------|----------------------------------|\\n| Doretto et al. [6]     | \u2717                                                             | \u2713                                      | \u2713                                                   | \u2717                                                               | \u2717                                                               | \u2717                                | \u2717                                |\\n| Costantini et al. [5]  | \u2713                                                              | \u2713                                      | \u2717                                                   | \u2717                                                               | \u2717                                                               | \u2717                                | \u2717                                |\\n| Funke et al. [8]       | \u2717                                                             | \u2713                                      | \u2717                                                   | \u2717                                                               | \u2717                                                               | \u2717                                | \u2717                                |\\n| Tesfaldet et al. [24]  | \u2717                                                             | \u2713                                      | \u2717                                                   | \u2717                                                               | \u2713                                                               | \u2713                                | \u2717                                |\\n| Xie et al. [27]        | \u2717                                                             | \u2717                                      | \u2717                                                   | \u2717                                                               | \u2713                                                               | \u2717                                | \u2717                                |\\n| Zhang et al. [32]      | \u2717                                                             | \u2713                                      | \u2717                                                   | \u2717                                                               | \u2713                                                               | \u2713                                | \u2717                                |\\n| DyNCA (Ours)           | \u2713                                                              | \u2713                                      | \u2713                                                   | \u2713                                                               | \u2717                                                               | \u2713                                | \u2713                                |\\n\\nTo synthesize video textures, Doretto et al. [6] propose to use a linear dynamical system (LDS) in which each frame of the video is controlled by a latent variable whose evolution through time is driven by random noise. The latent variable is obtained via projection of the target texture image into a lower dimensional space by singular value decomposition (SVD). Costantini et al. [5] propose to use higher-order SVD to improve the expressivity of LDS-based models. Yuan et al. [31] introduce feedback into the LDS to improve the stability of the dynamical system. All these methods can synthesize new video textures without re-training, and potentially in real time. However, the synthesized videos are of low quality and contain artifacts.\\n\\nOur DyNCA method benefits from the best of both approaches by combining the expressivity of neural networks with PDEs. Having $2 \\\\sim 4$ orders of magnitude fewer parameters than the SOTA models, our model can synthesize realistic dynamic texture videos in real time.\\n\\n2.2. NCA for Texture Synthesis\\n\\nGilpin [11] shows that Cellular Automata models can be represented using Convolutional Neural Networks. Extending [11], Mordvinstev et al. [18] propose the Neural Cellular Automata model and show its potential in creating various self-organizing systems [17\u201320]. NCA is inspired by Turing's seminal work [25] on pattern generation, and the observation that many natural patterns stem from local interactions between tiny particles, cells, or molecules [19].\\n\\nNiklasson et al. [17,19] were the first to train NCA models to synthesize textures. While the NCA training signal originates from a static exemplar texture image, the model is able to spontaneously generate stable but randomly moving textures. We modify the architecture and training scheme of NCA to enable synthesizing structured motion. First, our DyNCA model receives supervision from a pre-trained optical-flow network, which enables it to synthesize a video texture with structured motion. Moreover, we incorporate multi-scale perception and positional encoding into the DyNCA architecture. The proposed architectural changes increase the communication range of the cells and allow the cells to be aware of global information, respectively.\\n\\n3. DyNCA Architecture\\n\\nIn the following sections, we first review the NCA model and then present the architecture of our DyNCA.\\n\\n3.1. Neural Cellular Automata (NCA)\\n\\nThe idea of NCA stems from cellular automata, in which cells live on a grid, and each cell communicates with its neighbors to determine the cell's next state. In NCA, cell states at time $t$ are represented by $S_t \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C}$ where $H \\\\times W$ is the grid size. The $C$ dimensional vector $s_{ij}$ codes the state of the cell at location $i, j$, where the first three dimensions define the RGB color of the cell.\\n\\nThe NCA starts from a constant zero-filled initial state called seed and evolves this state over time according to its trainable PDE. The update rule of this PDE consists of two parts, **Perception** and **Stochastic Update**. At the perception stage, each cell gathers information from its surrounding neighbors, forming the perception vector $z_{ij} \\\\in \\\\mathbb{R}^{4C}$, illustrated in the green box in Figure 3.\\n\\n$$z_{ij} = \\\\text{Concat}(s_{ij}, \\\\nabla_x S_{ij}, \\\\nabla_y S_{ij}, \\\\nabla^2 S_{ij})$$  \\\\hfill (1)\\n\\nNote that the convolution kernels in the perception stage are frozen during training. In the stochastic update stage, the new state of each cell is determined based on its perception vector. The update stage of NCA can be viewed as a stochastic discrete-time, discrete-space PDE:\\n\\n$$S_{t+1} = F(S_t) = S_t + \\\\partial S_t \\\\partial t \\\\Delta t \\\\partial s_{ij} \\\\partial t = \\\\text{MLP}(z_{ij}) \\\\cdot M$$  \\\\hfill (2)\\n\\nwhere MLP is a MultiLayered Perceptron with two layers and a ReLU activation function. The residual update values produced by the MLP are multiplied by a binary random variable $M$ to introduce stochasticity into the model. This ensures that the cells can work asynchronously, and also enables synthesizing new texture samples. The stochastic update stage is illustrated in the blue box in Figure 3.\\n\\nTo facilitate long-range cell communication and to allow the cells to be aware of global information, we introduce multi-scale perception and positional encoding into the vanilla NCA architecture, respectively. Our experiments in section 5 demonstrate their necessity for DyTS. The overall architecture of our DyNCA model is illustrated in Figure 3. To the best of our knowledge, we are the first to incorporate these two architectural changes in conjunction with NCA models. In the following sections, we elaborate on the proposed architectural modifications.\"}"}
{"id": "CVPR-2023-600", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Illustration of a single DyNCA step. Given an input state $S_t \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C}$ at time step $t$, each cell first perceives its neighbors on various scales with the same perception layer. The perception tensor of each scale is then upsampled and summed up, and is concatenated with the positional encoding tensor $P_{xy}$. Each cell then applies the same update rule, parameterized by a small MLP. Finally, all cells perform a stochastic residual update to determine the state of the cells in time $t + \\\\Delta t$. We use $\\\\Delta t = 1$ in our model.\\n\\n3.2. Multi-scale Perception\\n\\nIn the perception stage of vanilla NCA, each cell only receives information from its eight surrounding cells. Therefore, many timesteps are needed for far-apart cells to perceive and communicate with each other. This problem becomes more pronounced when we increase the grid size $H \\\\times W$, since the $3 \\\\times 3$ perception kernels become smaller in proportion to the image size. One straightforward solution is to increase the number of NCA steps during training to facilitate long-range cell communication. However, this simple solution increases memory usage, slows down the training, and makes the training more unstable.\\n\\nTo solve this problem, we propose Multi-Scale Perception (MSP). The idea of using multi-scale analysis predates deep learning and has shown its effectiveness in many computer vision tasks [1, 3, 14, 15]. We build a pyramid of cell states via bilinear downsampling and apply the same perception kernels at different scales. To combine the information from all scales, we upsample and sum up the perception vectors, as shown in the red box in Figure 3. Multi-scale perception increases the communication range of the cells and allows messages to pass between faraway cells in fewer steps. It also improves the stability of DyNCA and makes the training less sensitive to hyperparameters. We perform an ablation study of multi-scale perception in section 5.4 and show its importance in preserving appearance fidelity.\\n\\n3.3. Positional Encoding\\n\\nIn order to apply the $3 \\\\times 3$ depth-wise convolutions in the perception stage, one needs to adopt a padding strategy to retain the spatial dimensions. Niklasson et al. [19] use circular padding to make the cells homogeneous and to reduce spatial inductive bias. While cell homogeneity is a good assumption for generating static textures, it is not suitable for synthesizing structured motion. Our intuition is that, in physical systems, motion not only arises from local interactions between tiny particles and cells, but also from global external forces such as gravity. Hence, we allow the cells to be aware of their position and propose to use replicate padding and Cartesian Positional Encoding (CPE), which is known to provide a more consistent spatial inductive bias than zero-padding [28]. As illustrated in the purple box in Figure 3, we concatenate the output of the multi-scale perception stage with a two-channel tensor $P_{xy}$, where $P_{ij} = 2i + 1 - 2j + 1$. Our ablation study in section 5.4 shows that employing CPE drastically improves motion consistency and accuracy in the synthesized videos.\\n\\n4. DyNCA Training\\n\\nOur DyNCA model acts as a PDE and generates a sequence of images $I^g = \\\\{I^g_1, I^g_2, \\\\ldots\\\\}$. We sample the synthesized video frames $V^g = \\\\{V^g_1, V^g_2, \\\\ldots\\\\}$ from this image sequence by mapping $T_{DyNCA}$ steps to one frame, i.e. $V^g_k = I^g_{kT}$. The synthesized video is then evaluated against the target appearance and the target motion to compute the appearance loss $L_{\\\\text{appr}}$ and the motion loss $L_{\\\\text{motion}}$, respectively. This process is shown in Figure 2. Our final loss is $L_{\\\\text{DyNCA}} = L_{\\\\text{appr}} + \\\\lambda L_{\\\\text{motion}}$. The overall scheme of the proposed loss functions is illustrated in Figure 4.\"}"}
{"id": "CVPR-2023-600", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Edward H Adelson, Charles H Anderson, James R Bergen, Peter J Burt, and Joan M Ogden. Pyramid methods in image processing. RCA engineer, 29(6):33\u201341, 1984.\\n\\n[2] Simon Baker, Daniel Scharstein, JP Lewis, Stefan Roth, Michael J Black, and Richard Szeliski. A database and evaluation methodology for optical flow. International Journal of Computer Vision, 92(1):1\u201331, 2011.\\n\\n[3] Peter J Burt and Edward H Adelson. The laplacian pyramid as a compact image code. In Readings in Computer Vision, pages 671\u2013679. Elsevier, 1987.\\n\\n[4] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.\\n\\n[5] Roberto Costantini, Luciano Sbaiz, and Sabine Susstrunk. Higher order svd analysis for dynamic texture synthesis. IEEE Transactions on Image Processing, 17(1):42\u201352, 2007.\\n\\n[6] Gianfranco Doretto, Alessandro Chiuso, Ying Nian Wu, and Stefano Soatto. Dynamic textures. International Journal of Computer Vision, 51(2):91\u2013109, 2003.\\n\\n[7] Gianfranco Doretto, Eagle Jones, and Stefano Soatto. Spatially homogeneous dynamic textures. In European Conference on Computer Vision, pages 591\u2013602. Springer, 2004.\\n\\n[8] Christina M Funke, Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Synthesising dynamic textures using convolutional neural networks. arXiv preprint arXiv:1702.07006, 2017.\\n\\n[9] Leon Gatys, Alexander S Ecker, and Matthias Bethge. Texture synthesis using convolutional neural networks. Advances in Neural Information Processing Systems, 28, 2015.\\n\\n[10] PP Ghadekar and NB Chopade. Nonlinear dynamic texture analysis and synthesis model. International Journal on Recent Trends in Engineering & Technology, 11(1):475, 2014.\\n\\n[11] William Gilpin. Cellular automata as convolutional neural networks. Physical Review E, 100(3):032402, 2019.\\n\\n[12] Aleksander Holynski, Brian L. Curless, Steven M. Seitz, and Richard Szeliski. Animating pictures with eulerian motion fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5810\u20135819, June 2021.\\n\\n[13] Nicholas Kolkin, Jason Salavon, and Gregory Shakhnarovich. Style transfer by relaxed optimal transport and self-similarity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10051\u201310060, 2019.\\n\\n[14] David G Lowe. Object recognition from local scale-invariant features. In Proceedings of the Seventh IEEE International Conference on Computer Vision, volume 2, pages 1150\u20131157. IEEE, 1999.\\n\\n[15] Jonathan McCabe. Cyclic symmetric multi-scale turing patterns. In Proceedings of Bridges 2010: Mathematics, Music, Art, Architecture, Culture, pages 387\u2013390, 2010.\\n\\n[16] Alexander Mordvinstev. Fixing neural ca colors with sliced optimal transport.\\n\\n[17] Alexander Mordvintsev and Eyvind Niklasson. \u00b5nca: Texture generation with ultra-compact neural cellular automata. arXiv preprint arXiv:2111.13545, 2021.\\n\\n[18] Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, and Michael Levin. Growing neural cellular automata. Distill, 2020. https://distill.pub/2020/growing-ca.\\n\\n[19] Eyvind Niklasson, Alexander Mordvintsev, Ettore Randazzo, and Michael Levin. Self-organising textures. Distill, 6(2):e00027\u2013003, 2021.\\n\\n[20] Ettore Randazzo, Alexander Mordvintsev, Eyvind Niklasson, Michael Levin, and Sam Greydanus. Self-classifying mnist digits. Distill, 2020. https://distill.pub/2020/selforg/mnist.\\n\\n[21] Arno Sch\u00f6dl, Richard Szeliski, David H Salesin, and Irfan Essa. Video textures. In Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, pages 489\u2013498, 2000.\\n\\n[22] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\\n\\n[23] Stefano Soatto, Gianfranco Doretto, and Ying Nian Wu. Dynamic textures. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, volume 2, pages 439\u2013446. IEEE, 2001.\\n\\n[24] Matthew Tesfaldet, Marcus A Brubaker, and Konstantinos G Derpanis. Two-stream convolutional networks for dynamic texture synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6703\u20136712, 2018.\\n\\n[25] Alan Mathison Turing. The chemical basis of morphogenesis. Bulletin of Mathematical Biology, 52(1):153\u2013197, 1990.\\n\\n[26] Pierre Wilmot, Eric Risser, and Connelly Barnes. Stable and controllable neural texture synthesis and style transfer using histogram losses. CoRR, abs/1701.08893, 2017.\\n\\n[27] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthesizing dynamic patterns by spatial-temporal generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7093\u20137101, 2017.\\n\\n[28] Rui Xu, Xintao Wang, Kai Chen, Bolei Zhou, and Chen Change Loy. Positional encoding as spatial inductive bias in gans. In arxiv, December 2020.\\n\\n[29] Feng Yang, Gui-Song Xia, Liangpei Zhang, and Xin Huang. Stationary dynamic texture synthesis using convolutional neural networks. In 2016 IEEE 13th International Conference on Signal Processing (ICSP), pages 1135\u20131139. IEEE, 2016.\\n\\n[30] Xinge You, Weigang Guo, Shujian Yu, Kan Li, Jos\u00e9 C Pr\u00edncipe, and Dacheng Tao. Kernel learning for dynamic texture synthesis. IEEE Transactions on Image Processing, 25(10):4782\u20134795, 2016.\"}"}
{"id": "CVPR-2023-600", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lu Yuan, Fang Wen, Ce Liu, and Heung-Yeung Shum. Synthesizing dynamic texture with closed-loop linear dynamic system. In European Conference on Computer Vision, pages 603\u2013616. Springer, 2004.\\n\\nKaitai Zhang, Bin Wang, Hong-Shuo Chen, Xuejing Lei, Ye Wang, and C-C Jay Kuo. Dynamic texture synthesis by incorporating long-range spatial and temporal correlations. In 2021 International Symposium on Signals, Circuits and Systems (ISSCS), pages 1\u20134. IEEE, 2021.\"}"}
{"id": "CVPR-2023-600", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1. Appearance Loss\\nWe follow the texture fitting scheme proposed by Niklasson et al. [19], where the statistics of deep features of the NCA-generated images are forced to match the ones of the target texture. We use a pre-trained VGG16 network [22] to extract the deep features from the images. We denote VGG as $F_{VGG}$ and the feature map extracted from layer $l$ as $F_{l, VGG}$. Niklasson et al. [19] use the Gram matrix to define the training objective of the NCA. However, using Gram matrices can cause unstable training [26], and can result in textures with wrong colors [16,19]. Hence, we use the style loss proposed by Kolkin et al. [13], which has shown to produce better results. This loss is composed of a structure-matching term and a moment-matching term. Given a feature map of size $C \\\\times H \\\\times W$, the algorithm first creates a set of deep features with $n = H \\\\times W$ elements by flattening the feature map along the spatial dimensions. Let $X$ and $Y$ be the deep feature set of a synthesized image and target appearance image, respectively. Then the structure-matching term $L_{s}(X, Y)$ and the moment-matching term $L_{m}(X, Y)$ are defined as:\\n\\n\\\\[\\nD(A, B) = \\\\frac{1}{|A|} \\\\min_{j} \\\\left| 1 - A_i \\\\cdot B_j \\\\right| \\\\left| A_i \\\\right|_2 \\\\left| B_j \\\\right|_2\\n\\\\]\\n\\n\\\\[\\nL_{s}(X, Y) = \\\\max \\\\{ D(X, Y), D(Y, X) \\\\}, \\\\quad (3)\\n\\\\]\\n\\n\\\\[\\nL_{m}(X, Y) = \\\\frac{1}{C} \\\\left\\\\| \\\\mu_X - \\\\mu_Y \\\\right\\\\|_1 + \\\\frac{1}{C^2} \\\\left\\\\| \\\\Sigma_X - \\\\Sigma_Y \\\\right\\\\|_1, \\\\quad (4)\\n\\\\]\\n\\nwhere $D$ measures the distance between two deep feature sets, and $\\\\mu$, $\\\\Sigma$ are the mean and covariance matrix of their corresponding set, respectively. Let $X_{l,k}$ and $Y_{l,k}$ be deep VGG features extracted by $F_{conv \\\\ l}$, where $l$ and $k$ indicate the VGG block index and the synthesized image index, respectively. Our final appearance loss is then defined as:\\n\\n\\\\[\\nL_{appr} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\sum_{l=1}^{5} \\\\left( L_{s}(X_{l,k}, Y_{l,k}) + L_{m}(X_{l,k}, Y_{l,k}) \\\\right) \\\\quad (5)\\n\\\\]\\n\\n4.2. Motion Loss\\nThe motion loss guides the DyNCA to produce the desired motion. We use the pre-trained Optical-Flow Prediction Network from [24] to quantify the motion information between two frames. We denote this network as $F_{OF}$, and the feature map extracted from layer $l$ as $F_{l, OF}$. For different types of target motion sources, namely vector fields and videos, our motion loss takes two different forms, $L_{mvec}$ and $L_{mvid}$, respectively. These terms are discussed below.\\n\\n4.2.1 Motion from Vector Field\\nLet $U_t \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 2}$ be the target motion vector field. Let $U_g = F_{OF}(I_{gt1}, I_{gt2})$ be the optical-flow prediction on two synthesized images, where $t_1, t_2$ are two random indices such that $t_2 > t_1$. We propose two losses, $L_{dir}$ for matching the motion direction, defined as:\\n\\n\\\\[\\nL_{dir} = \\\\frac{1}{HW} \\\\sum_{i,j} 1 - U_{g_{ij}} \\\\cdot U_{t_{ij}} U_{g_{ij}}^2 U_{t_{ij}}^2,\\n\\\\]\\n\\n\\\\[\\nL_{norm} = \\\\frac{1}{HW} \\\\sum_{i,j} T_{t_2-t_1} U_{g_{ij}}^2 - U_{t_{ij}}^2.\\n\\\\]\\n\\nSince $t_1, t_2$ are selected randomly, we scale the norm of the $U_g$ by $T_{t_2-t_1}$ before comparing it against the target motion vector field. We define our final loss $L_{mvec}$ as:\\n\\n\\\\[\\nL_{mvec} = (1 - \\\\min \\\\{ 1, L_{dir} \\\\}) L_{norm} + \\\\gamma L_{dir}, \\\\quad (8)\\n\\\\]\\n\\nwhere $\\\\gamma$ is a constant coefficient. We set the norm-matching loss weight to $1 - \\\\min \\\\{ 1, L_{dir} \\\\}$ to guide the training process to first focus on producing motion with the correct direction before trying to match the norm of the synthesized motion with the target vector field.\"}"}
{"id": "CVPR-2023-600", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2.2 Motion from Video\\n\\nFor training DyNCA to learn the motion from a target video, we aim to match the motion between successive frames of the synthesized and target videos. We synthesize a video $V_g$ with $K$ frames, and randomly pick the same number of consecutive frames from the target video, forming $V_t$. Inspired by Tesfaldet et al. [24], we feed successive video frames to a pre-trained optical-flow network and extract deep features from the concatenation layer of the network, denoted as $F_{\\\\text{concat}}$. To compare the deep optical flow features of the target and the synthesized videos, we use the same structure-matching and moment-matching terms defined in Section 4.1. We denote the extracted deep optical-flow features as $X_k$ for the synthesized video, and $Y_k$ for the target video, where $k$ indicates the frame index. Notice that there are $K - 1$ successive frame pairs in total. We define the objective for learning motion from video textures as:\\n\\n$$L_{\\\\text{mvid}} = \\\\frac{1}{K - 1} \\\\sum_{k=1}^{K - 1} \\\\left( L_s(X_k, Y_k) + L_m(X_k, Y_k) \\\\right)$$\\n\\n(9)\\n\\n5. Experiments\\n\\nWe conduct experiments on the critical parameters of DyNCA, including $N_C$, $N_H$, $N_W$, $N_{\\\\text{FC}}$, where $N_C$, $N_H$, $N_W$ are the size of the DyNCA seed state and $N_{\\\\text{FC}}$ is the output dimensionality of the first layer of MLP in DyNCA. Concretely, we set $N_C = 12$, $N_{\\\\text{FC}} = 96$ for DyNCA-S and $N_C = 16$, $N_{\\\\text{FC}} = 128$ for DyNCA-L. Although DyNCA can adapt to arbitrary seed sizes after training, the seed resolution used during training affects the texture details. We experiment with both $128^2$ and $256^2$ spatial resolutions for the seed. We use Positional Encoding for all of the DyNCA configurations, and enable Multi-Scale Perception only when training with the $256 \\\\times 256$ seed size. Table 2 summarizes the DyNCA configurations. We perform our experiments on an Nvidia-A100 GPU, and use Adam optimizer with an initial learning rate of 0.001. We refer the reader to the supplementary for further training details.\\n\\n5.1. Dynamic Texture Synthesis\\n\\nWe present the results of synthesizing dynamic textures using DyNCA, where the target motion is either a vector field or a video, and the target appearance is a static image.\\n\\nMotion from Vector Field: We manually design 12 target vector fields, and for each target motion, we train both DyNCA-S and DyNCA-L configurations on 45 different target appearances. We provide the resulting 1080 trained models in our real-time interactive demo. We set seed size to $128 \\\\times 128$ and $T = 24$, assuming that 24 steps of DyNCA update equals one frame of the synthesized video. Figure 6 shows the optical flow of the DyNCA-S synthesized videos and the corresponding target vector fields used for training.\\n\\nMotion from Video: We train DyNCA to match the target motion from videos. For the target appearance image, we use one of the video frames or a stylistic image, to perform dynamic texture synthesis and dynamic style transfer, respectively. We train both DyNCA-S and DyNCA-L on 59 target dynamic texture videos provided in [24], setting seed size to $256 \\\\times 256$ and $T = 64$. Figure 5 provides some visual results. More results are provided in our demo and supplementary. In Table 2, we compare our DyNCA and the previous SOTA models [24, 27] in terms of the computational costs, namely performance and the number of parameters. Note that our DyNCA model is orders of magnitude more efficient in both training and synthesis time as well as the number of parameters. We refer the reader to the supplementary material for the detailed experimental setup.\\n\\n5.2. Real-time Video Editing\\n\\nDyNCA can also perform real-time video editing by utilizing the post-training NCA controls introduced in [19]. These edits include direction control, speed control, a brush tool, and local coordinate transformations. We refer the reader to our online demo at https://dynca.github.io for real-time and interactive visualization and experimentation.\\n\\nDirection Control: By rotating the Sobel convolution kernels $\\\\nabla x$ and $\\\\nabla y$ in the perception stage, we can control the direction of the motion in the synthesized video. This is done by replacing $\\\\nabla x$ and $\\\\nabla y$ with $\\\\nabla u$ and $\\\\nabla v$, where $u$ and $v = R_\\\\theta x y$ and $R_\\\\theta$ is the rotation matrix with angle $\\\\theta$. We also rotate $P_{xy}$ by angle $\\\\theta$ so that the position-dependent part of the motion magnitude also rotates accordingly.\\n\\nSpeed Control: We control the speed of the synthesized video by increasing/decreasing $T$, which determines how many DyNCA steps are used to synthesize one video frame.\\n\\nBrush Tool: The brush tool allows the users to delete pixels from the video. Since our DyNCA model exhibits the self-organization property of the original NCA [19], the model can reorganize itself and continue synthesizing realistic videos by naturally filling the deleted pixels.\\n\\nLocal Coordinate Transformation: DyNCA can create complex motion transformations by allowing each cell to use a different rotation angle $\\\\theta(i, j)$ for transforming its Sobel convolution kernels. For example, setting the rotation angle for the cell at location $(i, j)$ to $\\\\arctan(i \\\\cdot W/2 - j \\\\cdot H/2)$ will transform a rightward motion into a circular motion.\\n\\n5.3. User Study\\n\\nAlthough it is tempting to use a metric, e.g. the Gram matrix difference [9], to compare the quality of the videos synthesized by different methods, the variety of training objectives used in the existing methods makes such comparisons biased and unfair. Therefore, we conduct a similar user study as Tesfaldet et al. [24] to quantitatively evaluate and compare the realism of the videos. We show a pair of videos to the participants and ask them to choose the video that appears more realistic. We compare the videos synthesized by (DyNCA, [24], [27]) with each other and also\"}"}
{"id": "CVPR-2023-600", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dynamic Texture Synthesis: DyNCA faithfully reproduces the target appearance and target dynamics.\\n\\nDynamic Style Transfer: DyNCA can learn appearance and motion from different sources. Note that DyNCA does not simply copy the target dynamics from the video, but learns the style of the given motion and naturally adapts it to the target appearance, as shown by the optical-flow images. See our real-time interactive demo https://dynca.github.io\\n\\nTable 2. Comparison of training time, synthesis time per frame, and number of trainable parameters of different DyTS methods.\\n\\n| Method   | Training Time (s) | Synthesis Time (s) | # Parameters |\\n|----------|-------------------|--------------------|--------------|\\n| A [24]   |                  |                    |              |\\n| B [27]   |                  |                    |              |\\n| C [27]   |                  |                    |              |\\n| DyNCA-S  |                  |                    |              |\\n| DyNCA-L  |                  |                    |              |\\n\\n5.4. Ablation Study\\n\\nPositional Encoding: We ablate the Cartesian Positional Encoding (CPE) and train DyNCA-S-$128 \\\\times 2$ with different padding strategies for comparison. In Table 4, we report the average of $L_{\\\\text{dir}}$ and $L_{\\\\text{norm}}$ losses on 4 target vector fields (Diverge, Converge, Circular, and Hyperbolic shown in the last 4 columns of Figure 6) and 10 different target vector fields.\"}"}
{"id": "CVPR-2023-600", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Top: Comparison of real and synthesized video frames from different methods: (A) Tesfaldet et al. [24]; (B) Xie et al. [27] FC config; (C) Xie et al. [27] ST config.\\n\\nBottom: Pair-wise comparison results from our user study. The participants see two videos one after another in random order and are asked to choose the video that appears more realistic. Our DyNCA achieves an on-par \\\"fooling\\\" rate when compared with real videos. Yet, our synthesized videos look more realistic when compared to existing DyTS methods (60%, 54%, 80%). All error margins are $\\\\leq 2\\\\%$.\\n\\nThe results demonstrate that CPE is a necessary component for DyNCA to learn the motion from a structured target vector field, in which the motion direction and magnitude are position-dependent. We show the importance of CPE for video motion fitting in the supplementary.\\n\\nMulti-Scale Perception: MSP is key to enable training DyNCA with larger seed sizes. We train DyNCA-L-$256^2$ both with and without multi-scale perception and evaluate the results qualitatively and quantitatively. Figure 8 shows the corresponding synthesized video frames, qualitatively demonstrating that single-scale perception causes artifacts to appear in the synthesized frames. We also perform a quantitative evaluation in the supplementary, showing that training with multi-scale perception improves both the appearance $L_{app}$ and the video motion $L_{mvid}$ losses.\\n\\n6. Limitations\\n\\nDyNCA has certain limitations. In the case of vector field motion, it cannot generate a correct motion when the target appearance and the target motion are not compatible. For example, DyNCA fails to move a 45-degree oblique-line pattern in a circular manner. For video motion, we find it difficult to automatically set the motion loss weight $\\\\lambda$.\\n\\nMoreover, DyNCA cannot generate diverse motion when the target videos are violating the underlying assumption of dynamic textures, i.e. when they are not temporally homogeneous. In these cases, DyNCA suffers from overfitting to the dominant direction of the motion in the target videos. We refer to our supplementary material for more details.\\n\\n7. Conclusion\\n\\nWe propose DyNCA, a model that can, in real time, synthesize dynamic texture videos with arbitrary frame size and infinite length. Exploiting multi-scale perception and positional encoding, the cells in DyNCA can readily perform long-range communication and obtain global information. This ensures improved performance both in terms of visual quality and computational expressivity as compared to the vanilla NCA model, as we show with both qualitative and quantitative experiments. DyNCA can learn motion either from a hand-crafted vector field or a video, thus allowing for broader synthesis options. DyNCA produces more realistic video textures than the current DyTS methods, as demonstrated through a user study. DyNCA is also $\\\\sim 4$ orders of magnitude faster than the SOTA methods in synthesis time and has much fewer trainable parameters, thus facilitating real-world deployment. Lastly, DyNCA allows for several real-time and interactive video control tools that let the users control DyNCA without re-training.\\n\\nAcknowledgement. This work was supported in part by the Swiss National Science Foundation via the Sinergia grant CRSII5-180359.\"}"}
