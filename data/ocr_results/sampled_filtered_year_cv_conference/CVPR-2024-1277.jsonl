{"id": "CVPR-2024-1277", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LEAP-VO pipeline. Given a new image \\\\(I_t\\\\) received at time \\\\(t\\\\), the keypoint extractor extracts new keypoints from \\\\(I_t\\\\). Then, all the keypoints from the latest \\\\(S_{\\\\text{LP}}\\\\)-frame \\\\(I_{t-S_{\\\\text{LP}}+1}\\\\) are tracked across all other frames within the current LEAP window, followed by a track filtering step to remove outliers. Finally, the local BA module is used on the current BA window to update the camera poses and 3D positions of the extracted keypoints. The colored arrows denote the moving direction of each module when a new image is received.\\n\\n3.4. LEAP-VO\\n\\nLeveraging the anchor-based dynamic track estimations and temporal probabilistic formulation, we develop our Long-term Effective Point Tracking (LEAP) pipeline, boosting the accuracy of point tracking and simultaneously capturing global motion patterns and uncertainties of the tracks (refer to Figure 3). In this section, we demonstrate how we can build visual odometry for dynamic scenes based on the long-term tracking capabilities of LEAP. Our LEAP-VO pipeline is illustrated in Figure 4.\\n\\nKeypoint Extraction and Tracking. In our VO system, the LEAP model serves as the front-end. When a new image frame \\\\(I_t\\\\) is captured at time \\\\(t\\\\), we sample \\\\(N\\\\) new keypoints from this image to initiate point tracking. We consistently employ the distributed image gradient-based sampling method (refer to Section 3.2) for keypoint selection due to its favorable properties in feature distinctiveness and distribution. This approach also obviates the need for additional anchors during tracking. The LEAP model then tracks these keypoints across the last \\\\(S_{\\\\text{LP}}\\\\) frames, performing bidirectional tracking (both forward and backward in time). Notably, \\\\(S_{\\\\text{LP}}\\\\) can exceed the model window \\\\(S_{\\\\text{BA}}\\\\), thus enabling more expansive tracking capabilities through sequential prediction [16, 18].\\n\\nTrack Filtering. Theoretically, precise camera pose estimation between two images can be attained with just a few accurate correspondences under certain conditions. However, the quality of correspondences obtained from feature matching or optical flow can vary. Traditional VO pipelines often use RANSAC [14] to filter out incorrect matches, but their efficiency decreases with more keypoints and views. Our approach, in contrast, bypasses the sampling-based method and uses trajectory quality assessments from LEAP for more efficient and informative track filtering.\\n\\n(i) Visibility and Dynamic Track Label: The LEAP model has the capability to estimate not only the 2D point positions within a trajectory but also the visibility of each point and the dynamic track label of the entire trajectory. Thresholds \\\\(\\\\gamma_v\\\\) for visibility and \\\\(\\\\gamma_d\\\\) for dynamic track labels are set to ensure that only visible and static points are utilized in the bundle adjustment process.\\n\\n(ii) Track Uncertainty: In Section 3.3, we introduce probabilistic modeling to LEAP, allowing us to gauge confidence via distribution measurements. We select only high-confidence points, which correspond to low uncertainty measurements. Denoting our model's uncertainty estimation as \\\\(\\\\Phi\\\\), we retain points that exhibit high confidence using the criterion \\\\(\\\\Phi \\\\leq Q(\\\\gamma_u)\\\\), where \\\\(\\\\gamma_u\\\\) is the uncertainty quantile and \\\\(Q: [0, 1] \\\\rightarrow \\\\mathbb{R}\\\\) is the quantile function.\\n\\n(iii) Track Length: After filtering, we evaluate and remove tracks with insufficient observations, as bundle adjustment is more reliable with increased observations and wider baselines. Tracks with fewer than \\\\(\\\\gamma_{\\\\text{track}}\\\\) valid points are excluded from optimization cost computation due to their potential unreliability.\\n\\nSliding-window Optimization. With the point validation mask established, we proceed to define the optimization cost function for our sliding window bundle adjustment. The window size for local bundle adjustment, denoted as \\\\(S_{\\\\text{BA}}\\\\), may differ from the point tracking window size \\\\(S_{\\\\text{LP}}\\\\). Our geometric bundle adjustment aims to optimize camera poses \\\\(T\\\\) and 3D scene point positions \\\\(Q\\\\) by aligning the induced point trajectory from the projective relationship, with the estimated point trajectory from LEAP. We parameterize a 3D scene point \\\\(Q_i\\\\) by its 2D keypoint location \\\\(x_i\\\\) in image \\\\(I_i\\\\) and depth \\\\(d_i\\\\). Let LEAP\\\\(_i \\\\rightarrow j\\\\) denote the mapping of tracking keypoint from \\\\(I_i\\\\) to \\\\(I_j\\\\) using LEAP model. The\\\\(\\\\)...\"}"}
{"id": "CVPR-2024-1277", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"reprojection cost is formulated as:\\n\\n$$X_i X_j \\\\in |i - j| \\\\leq S_{BA}$$\\n\\n$$X_n_{w_i \\\\rightarrow j,n} \\\\parallel P(T_i, T_j, K, d_{i,n}) - LEAP_i \\\\rightarrow j(x_{i,n}) \\\\parallel \\\\rho,$$\\n\\nwhere $\\\\parallel \\\\cdot \\\\parallel$ is the distance metric, and the weight $w_{i \\\\rightarrow j,n}$ is derived from the track filtering results. We use the Gauss-Newton method [34] to optimize Eq. (9) for $K_{BA}$ iterations. At each iteration, we compute the camera poses update $\\\\Delta \\\\xi(k) \\\\in se(3)$ (lie-algebra corresponding to $T$), and the depth update $\\\\Delta D(k)$ for each point. The optimization can be solved efficiently with the Schur decomposition. We use the robust Huber loss function for the distance metric.\\n\\n4. Experiment\\n\\n4.1. Datasets and Metrics\\n\\nReplica. Replica [29] provides synthetic indoor environments for simulating the image-capturing process with a moving camera. We use the camera trajectories provided by [50] for the localization task, resulting in 16 camera trajectories derived from 8 static scenes. Each sequence comprises 900 frames with rendered RGB images. We use \u201cSequence 1\u201d for the visual odometry evaluation.\\n\\nMPI Sintel. The MPI Sintel [4] dataset contains dynamic image sequences from open-source 3D animated short films. These sequences consist of large and complex object motion, along with other imaging effects such as motion blur and defocus blur. Following prior works [20, 48], we assess dynamic VO performance on the MPI Sintel dataset, with each sequence containing 20 to 50 image frames.\\n\\nTartanAir Shibuya. The TartanAir Shibuya dataset [26] features dynamic scenes from two scenarios: \u201cStanding Humans\u201d and \u201cRoad Crossing\u201d. The majority of the humans remain stationary in \u201cStanding Humans\u201d sequences, while \u201cRoad Crossing\u201d sequences provide a more challenging setting where multiple humans are moving in different directions. Each sequence in the dataset includes 100 frames with more than 30 tracked moving humans.\\n\\nMetrics. For visual odometry, we compare the Absolute Translation Error (ATE), Relative Translation Error (RPE trans), and Relative Rotation Error (RPE rot) following [33, 47, 48]. ATE measures how much the estimated trajectory deviated from the ground truth trajectory, which is calculated as the root mean square error (RMSE) overall all pose transformations after scaling and alignment. \u201cRPE trans\u201d computes translation errors made over a certain distance (meter), and \u201cRPE rot\u201d computes the estimated rotation over a certain distance (degree). Both \u201cRPE trans\u201d and \u201cRPE rot\u201d are calculated on all poses and then averaged.\\n\\n| Method          | Replica (m) | MPI Sintel |\\n|-----------------|-------------|------------|\\n| Replica         |             |            |\\n| Orb-SLAM2       | 0.086       | X          |\\n| DynaSLAM        | 0.39       | X          |\\n| DROID-SLAM      | 0.267       | 0.175      |\\n| TartanVO        | 0.406       | 0.238      |\\n| DytanVO         | 0.289       | 0.131      |\\n| DPVO            | 0.257       | 0.076      |\\n\\nTable 1. Camera tracking results on Replica [29]. The statistics demonstrate that our method achieves better results than other competitive baselines.\\n\\n| Method          | MPI Sintel |\\n|-----------------|------------|\\n| Orb-SLAM2       | X          |\\n| DynaSLAM        | X          |\\n| DROID-SLAM      | 0.175      |\\n| TartanVO        | 0.238      |\\n| DytanVO         | 0.131      |\\n| DPVO            | 0.076      |\\n\\nTable 2. Camera tracking results on MPI Sintel [4]. The statistics show that our method outperforms other competitive baselines. \u2018X\u2019 denotes the tracking failure case.\\n\\n4.2. Implementation details\\n\\nWe employ CoTracker [18], a recent PIPs variant, as our base TAP model. We train our model on the TAP-Vid-Kubric [9] training set, following [18]. We load the pre-trained CoTracker weights for the image feature extractor and train LEAP for 100,000 steps using 4 NVIDIA A100 GPUs in parallel. During training, we use $K = 4$ steps for iterative refinements, $N = 256$ for the number of queries, $S = 8$ for the model window, and $S_{LP} = 12$ for the tracking window. For inter-track anchor-based attention, the number of anchors is 64. The loss weights are set to $w_1 = 1.0$. $w_2 = 0.5$. $w_3 = 0.5$. In LEAP-VO, we perform 4 steps for each bundle adjustment with the Huber Loss for distance metric. The bundle adjustment window size $S_{BA}$ is set to 15. Track filtering parameters are set at $\\\\gamma_v = 0.9$, $\\\\gamma_d = 0.9$, $\\\\gamma_u = 0.8$, $\\\\gamma_{track} = 3$. Please refer to our supplementary materials for more details.\\n\\n4.3. Visual Odometry Results\\n\\nWe compare our methods against the state-of-the-art SLAM and VO approaches, including ORB-SLAM2 [24], DynaSLAM [1], DROID-SLAM [32], TartanVO [40], DytanVO [28], and DPVO [33]. Evaluations are conducted on the static Replica dataset [29] as well as on the dynamic datasets MPI-Sintel [4] and TartanAir-Shibuya [26].\"}"}
{"id": "CVPR-2024-1277", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. ATE (m) results on TartanAir-Shibuya Sequences [26]. (\u00b7) denotes averaging on valid scenes only. We can observe that our method shows better results for ATE on most sequences, outperforming all SLAM and VO baselines on average ATE.\\n\\nFigure 5. Qualitative results of camera trajectory estimation on TartanAir-Shibuya [26]. The visualizations show that our method provides more robust and accurate camera pose trajectories, especially in hard cases (RoadCrossing 06 and RoadCrossing 07).\\n\\nWithout any moving objects. Challenges arise in two test sequences that contain prolonged occlusions, leading to the potential camera tracking loss. As illustrated in Table 1, our approach outperforms other VO methods on all three metrics, showing effectiveness under static environments and the capability to handle occlusions. Further insights are available in Section 4.5. For classical methods, ORB-SLAM2 [24] and DynaSLAM [1], we observe tracking loss and resuming via re-localization, which reduces long-term drift and shows high accuracy for camera pose tracking.\\n\\nMPI-Sintel. We also evaluate our VO camera trajectory accuracy on 14 dynamic scenes from the MPI Sintel dataset, following the setting of [48]. As shown in Table 2, our method substantially surpasses other VO baselines in dynamic scenes, thanks to our temporal uncertainty estimation and dynamic track detection. Traditional feature-based methods like ORB-SLAM2 [24] and DynaSLAM [1] sometimes fail to track camera movements in highly dynamic sequences, primarily due to outliers from dynamic object correspondences and segmentation-based only dynamic track estimation. In contrast, our approach, which considers visual, temporal, and inter-track information, demonstrates notable improvements in dynamic scene tracking accuracy.\\n\\nTartanAir-Shibuya. Following DytanVO [28], we further evaluate the VO performance using the dynamic TartanAir-Shibuya dataset. We assess the Average Trajectory Error (ATE) across seven diverse scenes, ranging from simple to challenging. As depicted in Table 3, our method consistently outperforms all other SLAM and VO baselines in the majority of these scenes. Moreover, trajectory visualizations in Figure 5 also highlight the robust camera tracking capabilities of our method in dynamic environments.\\n\\n4.4. Dynamic Track Estimation Results\\n\\nIn our LEAP formulation, we have introduced the anchor-based dynamic track estimation module to distinguish between static and dynamic point trajectories, which provides valuable information for handling dynamic scenes in visual odometry. Our method leverages both point feature information and inter-track relations to predict the dynamic track label for each trajectory. To demonstrate the performance of the proposed dynamic track estimation module, we take the LEAP model and evaluate it on novel image sequences from diverse datasets. For a comprehensive demonstration, we uniformly sample $16 \\\\times 16$ queries among the images and track them for $T = 8$ frames. The results, shown in Figure 6, illustrate that LEAP can effectively handle various scenarios, including single-object motion, complex and rapid motion, and multi-object motion with occlusion.\\n\\n4.5. Ablation Study\\n\\nTrack Filtering. To highlight the efficacy of our track filtering module in LEAP-VO, we conduct an ablation study on different masks used in track filtering. We study three types of filtering criteria: visibility, dynamic\"}"}
{"id": "CVPR-2024-1277", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Visualization of dynamic track estimation on DA VIS [19], MPI-Sintel [4], and TartanAir-Shibuya [26]. Odd columns: all point trajectories. Even columns: estimated dynamic point trajectories.\\n\\nFigure 7. Visualization of point-wise uncertainty measurements. Keypoints with the lowest 20% (left) and highest 20% (right) uncertainty are shown in green and yellow, respectively.\\n\\nTable 4 shows the impact of applying various filtering types on the dynamic MPI Sintel benchmark. Without dynamic filtering or confidence filtering, the ATE increases by 46% and 130%, respectively. Combining all three types of filtering methods altogether, we achieve the best results in all three metrics.\\n\\n| Method      | Track Filtering | Vis. Dyn. | Conf. | ATE    | RPE trans | RPE rot |\\n|-------------|-----------------|-----------|-------|--------|-----------|---------|\\n| LEAP-VO     | \u2713               | -         | -     | 0.118  | 0.078     | 1.340   |\\n|             | \u2713               | \u2713         | -     | 0.085  | 0.062     | 1.281   |\\n|             | \u2713               | -         | \u2713     | 0.054  | 0.058     | 1.274   |\\n|             | \u2713               | \u2713         | \u2713     | 0.037  | 0.055     | 1.263   |\\n\\nTable 4. Effect of the proposed track filtering method on MPI-Sintel [4]. Tracking using all criteria yields the best results.\\n\\nUncertainty Estimation. As illustrated in Figure 7, we demonstrate the LEAP's capability to provide meaningful per-point uncertainty. It models the uncertainty as distribution parameters for both the entire track and individual points. Two key observations can be found from Figure 7. Firstly, our model reliably identifies high-uncertainty points, typically located in low-texture areas or in regions with repetitive patterns. Secondly, although not explicitly supervised, the model tends to assign higher uncertainty to dynamic objects, which are inherently more challenging to track. These findings highlight the efficacy of LEAP's probabilistic formulation in handling uncertainty.\\n\\nKeypoint Extraction. Table 5 showcases the effectiveness of various keypoint extraction strategies for VO performance. Despite its distinctive features, SIFT extraction provides inferior performance due to its unpredictable keypoint distribution. In contrast, the random sampling tends to generate well-separated keypoints, leading to a performance similar to SIFT's. Our distributed image gradient-based sampling shows the best performance, underscoring the significance of feature distribution and distinctiveness.\\n\\nPolicy: Uniform Random SIFT [22] Ours\\nATE (m): 0.1511 0.0366 0.0355 0.0290\\n\\nTable 5. Comparison of keypoint extraction policies (N = 64) on TartanAir-Shibuya [26]. Our distributed image-gradient sampling method achieves better results compared to other policies.\\n\\nBaseline comparison. We evaluate the pretrained CoTracker [18] for the end VO performance. Table 6 shows that LEAP front-end outperforms CoTracker on all datasets, demonstrating the effectiveness of the proposed dynamic track estimation and temporal probability modeling.\\n\\nFront-end Replica MPI Sintel Shibuya\\nATE RPE trans RPE rot ATE RPE trans RPE rot ATE\\nCoTracker 0.301 0.031 2.043 0.126 0.071 1.366 0.160\\nLEAP 0.204 0.030 1.992 0.037 0.055 1.263 0.029\\n\\nTable 6. Baseline comparison with CoTracker front-end [18]. Our LEAP front-end achieves better results on all datasets.\\n\\n5. Conclusion\\nWe introduce Long-term Effective Any Point Tracking (LEAP), designed to tackle the limitations of previous point-tracking methods. By leveraging visual, inter-track, and temporal cues within an anchor-based dynamic track estimation module, LEAP captures global motion patterns and effectively discriminates between static and dynamic elements. Moreover, our approach incorporates a temporal probabilistic formulation into the point-tracking pipeline, emulating recursive Bayesian filtering through a learning-based refinement module. This enables our model to accurately assess the reliability of point trajectory estimates. Using LEAP as the front-end, we then develop the LEAP-VO system, featuring novel integration of visual information, global motion patterns, and track distribution for handling dynamic scenes. Additionally, our takes for LEAP-VO can be integrated with other TAP-based front-ends, offering the potential to markedly improve camera-tracking accuracy and robustness.\"}"}
{"id": "CVPR-2024-1277", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry\\n\\nWeirong Chen 1,2*\\nLe Chen 3\\nRui Wang 4\\nMarc Pollefeys 4\\n\\n1 TU Munich\\n2 Munich Center for Machine Learning\\n3 MPI for Intelligent Systems\\n4 Microsoft\\n\\nAbstract\\nVisual odometry estimates the motion of a moving camera based on visual input. Existing methods, mostly focusing on two-view point tracking, often ignore the rich temporal context in the image sequence, thereby overlooking the global motion patterns and providing no assessment of the full trajectory reliability. These shortcomings hinder performance in scenarios with occlusion, dynamic objects, and low-texture areas. To address these challenges, we present the Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively combines visual, inter-track, and temporal cues with mindfully selected anchors for dynamic track estimation. Moreover, LEAP's temporal probabilistic formulation integrates distribution updates into a learnable iterative refinement module to reason about point-wise uncertainty. Based on these traits, we develop LEAP-VO, a robust visual odometry system adept at handling occlusions and dynamic scenes. Our mindful integration showcases a novel practice by employing long-term point tracking as the front-end. Extensive experiments demonstrate that the proposed pipeline significantly outperforms existing baselines across various visual odometry benchmarks.\\n\\n1. Introduction\\nVisual odometry (VO) calculates the camera's position and movement from an image sequence. It is widely used in various fields including robotics [46], mixed reality [23], and autonomous driving [15]. The performance of VO highly depends on the accuracy of its point tracking front-end, which recovers relative camera motion between consecutive frames using projective geometry. It further requires the point correspondences to be static, as dynamic points can lead to inaccurate motion estimation and pose recovery. Despite significant advances in feature tracking and optical flow estimation, most existing VO methods mainly focus on the two-view case, i.e., computing feature correspondences between a pair of images. However, these methods are limited as they rely solely on pairwise relative motion, thus neglecting the valuable temporal information in image sequences. Consequently, they often struggle with capturing dynamic motion patterns in tracks. Another challenge arises from the need to handle occlusions. In two-view scenarios, occlusion is not explicitly modeled, leading to misidentification of occluded points as incorrect matches. To address these challenges effectively, we propose to introduce a long-term point tracker, which can recover the trajectories of specific points across the image sequence given a sparse set of these query points. Its capability of leveraging temporal context and reliably tracking queries over multiple frames even under occlusion, makes it highly suitable for VO applications (refer to Figure 1).\\n\\nRecently, Persistent Independent Particles (PIPs) [16] proposes a novel pipeline that brings the idea of iterative refinement into the long-term point tracking task, with the local correlation evidence to maintain feasible computation. While it is proficient in leveraging the temporal context present in the image sequence, it ignores the inter-track information in the image sequence and processes each query point independently. Consequently, PIPs struggles to capture long-term dynamics effectively.\\n\\n* Work done at Microsoft Mixed Reality & AI Lab Zurich for a Master's degree.\"}"}
{"id": "CVPR-2024-1277", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we present Long-term Effective Point Tracking (LEAP), aiming to estimate the reliability of predicted correspondences and track moving object trajectories in dynamic scenes. To achieve this, we harness the global motion patterns\u2014often overlooked in existing methods\u2014through an anchor-based dynamic track estimation module, leveraging visual, temporal, and inter-track information to differentiate between the static and dynamic elements. We also incorporate a temporal probabilistic approach into our LEAP pipeline to handle uncertainties and refine the distribution of point correspondences iteratively, showcasing superior performance in complex real-world situations. More importantly, we pioneer the usage of continuous motion estimation from long-term point tracking to construct a robust visual odometry system. Our LEAP-VO distinguishes itself through its thoughtful integration of visual features, track distribution, and global motion patterns, significantly enhancing both the performance and robustness of visual odometry in dynamic environments.\\n\\n2. Related Work\\n\\nLong-term Point Tracking.\\nIn the realm of long-term point tracking or Tracking Any Point (TAP), various methods have been introduced recently. PIPs [16] reframes pixel tracking as a long-term motion estimation with local information and improves tracking accuracy by associating each pixel with a trajectory that spans multiple image frames. The subsequent works enhance performance by integrating spatial context features [2], tracking multiple queries concurrently [18], and extending temporal fields and templates [49]. TAP-Net [9] adopts a different method by locating candidate correspondences through a global cost volume, improved by TAPIR\u2019s trajectory refinement [10] and BootsTAP\u2019s semi-supervised training [11]. Other approaches explore self-supervision targets to learn visual correspondence via powerful feature representation [5, 17, 36]. Recently, OmniMotion [37] provides a pure test-time optimization method that infers the point trajectory and occlusion directly from videos with optical flow predictions. However, its slow inference speed makes it unsuitable for real-time applications such as visual odometry.\\n\\nMonocular Visual Odometry.\\nVisual Odometry (VO) can be categorized into indirect (feature-based) and direct methods. Indirect methods, such as MonoSLAM [8] and ORB-SLAM [25], mostly rely on feature extraction and matching to estimate the camera pose. Direct methods, conversely, skip feature extraction and matching and utilize intensity to optimize camera pose and 3D scene point positions based on photometric errors [12, 13, 38], showing better performance on low-texture regions where keypoint tracking can easily fail. However, they are more vulnerable to illumination changes and issues with rolling shutter cameras. Advancements in deep learning have spawned end-to-end monocular VO methods in supervised [3, 28, 30, 33, 39, 40, 43, 44] and unsupervised [21, 27, 45, 51] settings. DeepVO [39] utilizes Recurrent Neural Networks to model sequential data, while SfMLearner [51] develops an unsupervised framework to learn the depth and camera motion from unlabeled monocular videos. TartanVO [40] proposes a VO model that generalizes to multiple datasets and real-world scenarios. DROID-SLAM [32] follows learning-based optimization from RAFT [31] and proposes an end-to-end system, integrating flow, confidence, and geometric optimization via iterative GRU updates. The followed-up work DPVO [33] further cuts computational complexity by replacing dense feature tracking with sparse patch tracking.\\n\\nDynamic Track Estimation.\\nIn addition to enhancing accuracy, numerous studies have focused on the detection of dynamic objects within videos. Methods based on segmentation, such as DynaSLAM [1] and RCVD [20], employ semantic cues from pre-trained models to exclude dynamic regions. However, these segmentation-based methods are constrained to predefined object categories and struggle to capture real-world motion (for example, differentiating between a stationary and a moving car). Alternatively, TartanVO [28] and ParticleSfM [48] adopt a trajectory-based motion detection approach. This method addresses dynamic scenes using estimated point track information, thereby generating reliable camera trajectories in complex motion scenarios. Nonetheless, they depend on dense optical flow estimation to capture global motion patterns. Furthermore, dynamic tracks can be detected using additional depth information for a 3D consistency check [7, 47], or by optimization based on photo consistency [35, 41, 42].\\n\\n3. Method\\n\\nOur goal is to tackle the aforementioned challenges in VO systems, particularly in handling dynamic environments, temporal occlusions, and low-texture regions. To this end, we introduce our LEAP module, which incorporates dynamic track estimation and temporal probabilistic formulation. Furthermore, we illustrate how this module serves as the cornerstone for constructing our advanced VO system.\"}"}
{"id": "CVPR-2024-1277", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is mapped to a corresponding location in a subsequent frame. However, two-view approaches often overlook the rich temporal context within the image sequence, leading to suboptimal outcomes. Recent advancements address this by revisiting the particle video concept and improving long-term point tracking through learning-based techniques [10, 16, 18]. These TAP methods can track points over multiple frames and detect occlusions, significantly boosting tracking robustness under challenging scenarios.\\n\\n**Formulation.**\\n\\nGiven a sequence of $S$ consecutive RGB images $I = [I_1, \\\\ldots, I_S]$, $I_s \\\\in \\\\mathbb{R}^{3 \\\\times H \\\\times W}$ as input, the goal of TAP is to track a set of query points across these frames. For a specific query point with the 2D pixel coordinate $x_q \\\\in \\\\mathbb{R}^2$ in frame $s_q$, TAP predicts its trajectory across all $S$ images as $X = [x_1, \\\\ldots, x_S]$, $x_s \\\\in \\\\mathbb{R}^2$, given $x_{s_q} = x_q$. Additionally, TAP estimates the visibility of each point throughout the sequence as $V = [v_1, \\\\ldots, v_S]$, where $v_s \\\\in \\\\{0, 1\\\\}$ is a binary label of point visibility indicating whether a point in frame $s$ is visible or occluded. The TAP formulation for a single query can be described as \\n\\n$$(X, V) = \\\\text{TAP}(I, x_q, s_q).$$\\n\\nPIPs [16], a notable learning-based TAP method, offers a promising solution featuring local cost volumes and iterative refinement. Initially, each image $I_s \\\\in I$ is processed through a CNN feature extractor $F$ to extract the feature map $Y_s = F(I_s)$. Point features $f_q$ are computed by bilinear sampling at the query positions $x_s$ on $Y_s$. These point features from each image are then concatenated to form a point feature tensor $F = [f_1, \\\\ldots, f_S]$. PIPs approaches the TAP problem by iteratively updating the state variables $(X, F)$, with the initial states obtained by duplicating the query positions and features. During the $k$-th iterative refinement, PIPs computes a multi-scale local cost volume $C[X_k]$ centered around the current estimated point position $X_k$. Utilizing this local evidence, PIPs predicts updates for the state variables via\\n\\n$$(\\\\Delta X, \\\\Delta F) = \\\\text{Refiner}(F_k, \\\\text{pos}(X_k - x_q), C_k[X_k]), X_{k+1} \\\\leftarrow X_k + \\\\Delta X, F_{k+1} \\\\leftarrow F_k + \\\\Delta F,$$\\n\\nwhere $\\\\text{pos}(\\\\cdot)$ refers to the positional embedding. The final point feature $F_K$ is used to predict point visibility with a simple linear projection layer $G_v$, yielding $V = G_v(F_K)$.\\n\\nRecent advances extend PIPs by replacing the MLP-based refiner with an attention-based refiner and adding spatial feature aggregation to track multiple queries together [18].\\n\\n### 3.2. Anchor-based Dynamic Track Estimation\\n\\nThe established TAP model enables us to extract long-term point trajectories and thereby recover camera motion from an image sequence. However, dynamic environments pose extra challenges for VO systems that rely on static 3D point correspondences linked to unchanging scene locations. Therefore, integrating dynamic track estimation into the VO system is crucial for real-world applications.\\n\\nWe propose an innovative method for dynamic track estimation that leverages visual, temporal, and inter-track information. While common techniques like monocular segmentation [20] excel at identifying moving objects using visual cues but can erroneously label static objects, like parked cars, as dynamic. Trajectory-based methods [48], utilizing optical flows from two or multiple frames, can differentiate static and moving objects apart but tend to ignore visual information and can be inefficient due to dense trajectories computation. Merging these two distinct methods is non-trivial; however, the formulation of the TAP pipeline, enhanced with inter-track attention, offers a viable solution.\\n\\nOur method smartly integrates visual appearance, long-term motion, and inter-track cues, combining the strengths of segmentation and trajectory-based techniques. Utilizing TAP, we extract point features $F$ and long-term trajectories $X$ from the video input. These features capture visual information and the iterative refinement module exchanges information across tracks over time for accurate labeling. However, such a method depends on the query distribution and quantities. With only one query during inference time, it cannot utilize inter-track information. Likewise, if the queries focus on a single object, the system may overlook the motions of other objects due to similar motion patterns.\\n\\n**Anchor-based Inter-track Attention.**\\n\\nTo better capture global motion patterns, we introduce additional anchors alongside the given queries, denoted as $A$, $|A| = N_a$. Ideally, the anchor set should exhibit two characteristics: (1) the anchors should be easy to track, and (2) they should be well-distributed to encapsulate global motion patterns.\\n\\nGiven the image $I_{s_q}$ from which the queries $x_q$ are extracted (we maintain the same notations for both single and multiple queries for simplicity), we first employ the Sobel Kernel to derive the image gradient $(G_x, G_y)$ for image $I_{s_q}$. We then apply an average pooling layer on gradient maps for smoothing and downscaling feature maps. Subsequently, we divide the gradient map into $k \\\\times k$ sub-regions and select top $N_a k^2$ pixels with the highest image gradient magnitude in each sub-region.\"}"}
{"id": "CVPR-2024-1277", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"each local grid. The process is demonstrated in Figure 2.\\n\\nTo predict the dynamic label of a trajectory, we track queries with additional anchors and concatenate the original query features $X$ and query trajectories $F$ with anchor features $X_A$ and anchor trajectories $F_A$. We apply a shallow MLP layer $G_d$ with average pooling over the temporal dimension. The dynamic track label $m_d$ is estimated as\\n\\n\\\\[ m_d = \\\\text{avgpool}(G_d([X_K; X_K A], [F_K; F_K A])). \\\\]\\n\\nSince newly added anchors often lack ground truth information, we employ a semi-supervised training scheme that tracks queries and anchors together to leverage inter-track attention but only calculates losses based on query predictions. This approach prevents the network from developing bias towards specific query sampling methods while learning dynamic track labels effectively.\\n\\n### 3.3. Temporal Probability Modeling\\n\\nSupervised learning applied to large-scale datasets has yielded notable advancements in learning-based point tracking methods. Nonetheless, trajectory estimation errors remain a concern, especially under challenging conditions like occlusions, motion blur, or textureless regions. Recognizing the varying reliability of trajectories, integrating an uncertainty estimation mechanism into the correspondence estimation process is beneficial.\\n\\nTo evaluate the reliability of point correspondences, we incorporate a probabilistic formulation into the TAP pipeline. Given an image sequence $I$ and a single query point $x_q$, our objective is to compute the conditional probability density of point trajectory as $p(X|I, x_q)$. Recognizing the strong correlation between points within the same track, treating each point in the track as independent becomes suboptimal. Therefore, we propose to model the distribution of point trajectories by applying two multivariate distributions, respectively for the 2D coordinates. Let $X = [a; b]$, where $a \\\\in \\\\mathbb{R}^S$ and $b \\\\in \\\\mathbb{R}^S$ represent the X and Y coordinate of all points in $X$, respectively. Assuming independence between these two coordinates, the joint probability distribution can be expressed as\\n\\n\\\\[ p(X|I, x_q) = p(a|I, x_q) \\\\cdot p(b|I, x_q). \\\\]\\n\\nWe adopt the multivariate Cauchy distribution, which is associated with its heavy tails distribution and is more stable for optimization. The probability density function (PDF) of a single coordinate is given by\\n\\n\\\\[ p(a|I, x_q) = \\\\frac{\\\\Gamma\\\\left(\\\\frac{1+S^2}{2}\\\\right)}{\\\\Gamma\\\\left(\\\\frac{1}{2}\\\\right) \\\\pi S^2 |\\\\Sigma_a|^{\\\\frac{1}{2}}} \\\\left| 1 + \\\\left( a - \\\\mu_a \\\\right)^T \\\\Sigma_a^{-1} (a - \\\\mu_a) \\\\right|^{\\\\frac{1+S^2}{2}}. \\\\]\\n\\nThe parameters $(\\\\mu_a, \\\\Sigma_a, \\\\mu_b, \\\\Sigma_b)$ represent the location and scale matrices for the respective coordinates. During inference, the uncertainty associated with each point is quantified by the sum of diagonal scale estimates at position $(s, s)$ as $\\\\phi(x_s) = \\\\sum_a [s, s] + \\\\sum_b [s, s]$.\\n\\nKernel-based Estimation. Rather than just predicting the positions of points within a trajectory, our model focuses on recovering the distribution of point trajectories by estimating its parameters. The location parameter predictions can be associated with the previous point trajectory as $[\\\\mu_a; \\\\mu_b] = X$. However, directly deriving a symmetric and positive definite scale matrix from model outputs is challenging. To address this, we employ a kernel-based approach with the linear kernel $K(x, y) = x^T y$. During each iteration, two scale matrices $(\\\\Sigma_a, \\\\Sigma_b)$ are constructed by first applying two linear projection layers to the point features, represented as $F_k a = G_a(F_k)$ and $F_k b = G_b(F_k)$. We then compute the scale matrices as $\\\\Sigma_a = K(F_a, F_a) + \\\\sigma I$, and $\\\\Sigma_b = K(F_b, F_b) + \\\\sigma I$, where $\\\\sigma$ is a small positive value and $I$ is the identity matrix. The model parameters are refined through Maximum Likelihood Estimation (MLE) using a negative log-likelihood (NLL) loss function.\\n\\nLoss Functions. We supervise the point trajectory mainly with the NLL loss, which is based on the predicted distribution parameters and the ground-truth point trajectory. The main point trajectory loss is given as\\n\\n\\\\[ L_{\\\\text{main}} = K X_k \\\\gamma K - k L_{\\\\text{NLL}}(X_k, X^*, \\\\Sigma_k a, \\\\Sigma_k b), \\\\]\\n\\nwhere $K$ is the number of iterations and $\\\\gamma = 0$. For the visibility and dynamic track label supervision, we use the cross entropy loss with the estimates $V$, $m_d$ and ground truth $V^*$, $m^*_d$ as\\n\\n\\\\[ L_{\\\\text{vis}} = (1 - V^*) \\\\log(1 - V) + V^* \\\\log V. \\\\]\\n\\n\\\\[ L_{\\\\text{dyn}} = (1 - m^*_d) \\\\log(1 - m_d) + m^*_d \\\\log m_d. \\\\]\\n\\nThe total loss is a weighted sum of three losses:\\n\\n\\\\[ L_{\\\\text{total}} = w_1 L_{\\\\text{main}} + w_2 L_{\\\\text{vis}} + w_3 L_{\\\\text{dyn}}. \\\\]\"}"}
{"id": "CVPR-2024-1277", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Berta Bescos, Jos\u00e9 M Facil, Javier Civera, and Jos\u00e9 Neira. Dynaslam: Tracking, mapping, and inpainting in dynamic scenes. IEEE Robotics and Automation Letters, 3(4):4076\u20134083, 2018.\\n\\n[2] Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yitong Dong, Yijin Li, and Hongsheng Li. Context-tap: Tracking any point demands spatial context features. arXiv preprint arXiv:2306.02000, 2023.\\n\\n[3] Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, and Andrew J Davison. Codeslam\u2014learning a compact, optimisable representation for dense visual slam. In CVPR, pages 2560\u20132568, 2018.\\n\\n[4] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical flow evaluation. In ECCV, pages 611\u2013625. Springer-Verlag, 2012.\\n\\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In CVPR, pages 9650\u20139660, 2021.\\n\\n[6] Weirong Chen, Suryansh Kumar, and Fisher Yu. Uncertainty-driven dense two-view structure from motion. IEEE Robotics and Automation Letters, 8(3):1763\u20131770, 2023.\\n\\n[7] Yuhua Chen, Cordelia Schmid, and Cristian Sminchisescu. Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera. In ICCV, pages 7063\u20137072, 2019.\\n\\n[8] Andrew J Davison, Ian D Reid, Nicholas D Molton, and Olivier Stasse. Monoslam: Real-time single camera slam. IEEE TPAMI, 29(6):1052\u20131067, 2007.\\n\\n[9] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adri\u00e0 ReyCasens, Lucas Smaira, Yusuf Aytar, Jo\u00e3o Carreira, Andrew Zisserman, and Yi Yang. Tap-vid: A benchmark for tracking any point in a video. NeurIPS, 35:13610\u201313626, 2022.\\n\\n[10] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Jo\u00e3o Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and temporal refinement. ICCV, 2023.\\n\\n[11] Carl Doersch, Yi Yang, Dilara Gokay, Pauline Luc, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ross Goroshin, Jo\u00e3o Carreira, and Andrew Zisserman. Bootstap: Bootstrapped training for tracking-any-point. arXiv preprint arXiv:2402.00847, 2024.\\n\\n[12] Jakob Engel, Thomas Sch\u00f6ps, and Daniel Cremers. LSD-slam: Large-scale direct monocular slam. In ECCV, pages 834\u2013849. Springer, 2014.\\n\\n[13] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE TPAMI, 40(3):611\u2013625, 2017.\\n\\n[14] Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381\u2013395, 1981.\\n\\n[15] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231\u20131237, 2013.\\n\\n[16] Adam W Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In ECCV, pages 59\u201375. Springer, 2022.\\n\\n[17] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as a contrastive random walk. NeurIPS, 33:19545\u201319560, 2020.\\n\\n[18] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Co-tracker: It is better to track together. arXiv preprint arXiv:2307.07635, 2023.\\n\\n[19] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In ACCV, 2018.\\n\\n[20] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In CVPR, pages 1611\u20131621, 2021.\\n\\n[21] Shunkai Li, Xin Wang, Yingdian Cao, Fei Xue, Zike Yan, and Hongbin Zha. Self-supervised deep visual odometry with online adaptation. In CVPR, pages 6339\u20136348, 2020.\\n\\n[22] David G Lowe. Object recognition from local scale-invariant features. In ICCV, pages 1150\u20131157. IEEE, 1999.\\n\\n[23] Annette Mossel and Manuel Kroeter. Streaming and exploration of dynamically changing dense 3d reconstructions in immersive virtual reality. In 2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct), pages 43\u201348. IEEE, 2016.\\n\\n[24] Ra\u00fal Mur-Artal and Juan D. Tardos. ORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras. IEEE Transactions on Robotics, 33(5):1255\u20131262, 2017.\\n\\n[25] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versatile and accurate monocular slam system. IEEE Transactions on Robotics, 31(5):1147\u20131163, 2015.\\n\\n[26] Yuheng Qiu, Chen Wang, Wenshan Wang, Mina Henein, and Sebastian Scherer. Airdos: Dynamic slam benefits from articulated objects. In ICRA, pages 8047\u20138053. IEEE, 2022.\\n\\n[27] Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim, Deqing Sun, Jonas Wulff, and Michael J Black. Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. In CVPR, pages 12240\u201312249, 2019.\\n\\n[28] Shihao Shen, Yilin Cai, Wenshan Wang, and Sebastian Scherer. Dytanvo: Joint refinement of visual odometry and motion segmentation in dynamic environments. In ICRA, pages 4048\u20134055. IEEE, 2023.\\n\\n[29] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, et al. The replica dataset: A digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019.\\n\\n[30] Zachary Teed and Jia Deng. Deepv2d: Video to depth with differentiable structure from motion. In ICLR, 2019.\"}"}
{"id": "CVPR-2024-1277", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, pages 402\u2013419. Springer, 2020.\\n\\nZachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. NeurIPS, 34:16558\u201316569, 2021.\\n\\nZachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry. NeurIPS, 36, 2024.\\n\\nBill Triggs, Philip F McLauchlan, Richard I Hartley, and Andrew W Fitzgibbon. Bundle adjustment\u2014a modern synthesis. In International workshop on vision algorithms, pages 298\u2013372. Springer, 1999.\\n\\nVadim Tschernezki, Diane Larlus, and Andrea Vedaldi. Neuraldiff: Segmenting 3d objects that move in egocentric videos. In 3DV, pages 910\u2013919. IEEE, 2021.\\n\\nNarek Tumanyan, Assaf Singer, Shai Bagon, and Tali Dekel. Dino-tracker: Taming dino for self-supervised point tracking in a single video. arXiv preprint arXiv:2403.14548, 2024.\\n\\nQianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. arXiv preprint arXiv:2306.05422, 2023.\\n\\nRui Wang, Martin Schworer, and Daniel Cremers. Stereo dso: Large-scale direct sparse visual odometry with stereo cameras. In ICCV, pages 3903\u20133911, 2017.\\n\\nSen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. In ICRA, pages 2043\u20132050. IEEE, 2017.\\n\\nWenshan Wang, Yaoyu Hu, and Sebastian Scherer. Tartanvo: A generalizable learning-based vo. In CoRL, pages 1761\u20131772. PMLR, 2021.\\n\\nTianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. D\u2082nerf: Self-supervised decoupling of dynamic and static objects from a monocular video. NeurIPS, 35:32653\u201332666, 2022.\\n\\nJiawei Yang, Boris Ivanovic, Or Litany, Xinshuo Weng, Seung Wook Kim, Boyi Li, Tong Che, Danfei Xu, Sanja Fidler, Marco Pavone, et al. Emernerf: Emergent spatial-temporal scene decomposition via self-supervision. arXiv preprint arXiv:2311.02077, 2023.\\n\\nNan Yang, Lukas von Stumberg, Rui Wang, and Daniel Cremers. D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry. In CVPR, pages 1281\u20131292, 2020.\\n\\nWeicai Ye, Xinyue Lan, Shuo Chen, Yuhang Ming, Xingyuan Yu, Hujun Bao, Zhaopeng Cui, and Guofeng Zhang. Pvo: Panoptic visual odometry. In CVPR, pages 9579\u20139589, 2023.\\n\\nZhichao Yin and Jianping Shi. Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In CVPR, pages 1983\u20131992, 2018.\\n\\nKhalid Yousif, Alireza Bab-Hadiashar, and Reza Hoseinnezhad. An overview to visual odometry and visual slam: Applications to mobile robotics. Intelligent Industrial Systems, 1(4):289\u2013311, 2015.\\n\\nZhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubenstein, Noah Snavely, and William T Freeman. Structure and motion from casual videos. In ECCV, pages 20\u201337. Springer, 2022.\\n\\nWang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In ECCV, pages 523\u2013542. Springer, 2022.\\n\\nYang Zheng, Adam W Harley, Bokui Shen, Gordon Wetzstein, and Leonidas J Guibas. Pointodyssey: A large-scale synthetic dataset for long-term point tracking. In ICCV, pages 19855\u201319865, 2023.\\n\\nShuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew J Davison. In-place scene labelling and understanding with implicit scene representation. In CVPR, pages 15838\u201315847, 2021.\\n\\nTinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In CVPR, pages 1851\u20131858, 2017.\"}"}
