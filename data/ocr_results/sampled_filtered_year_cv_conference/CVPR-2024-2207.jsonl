{"id": "CVPR-2024-2207", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Renderpeople, https://renderpeople.com/.\\n\\n[2] Stable diffusion image variations. huggingface.co/lambdalabs/stable-diffusion-image-conditioned.\\n\\n[3] Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang. Single-image 3d human digitization with shape-guided diffusion. In SIGGRAPH Asia, 2023.\\n\\n[4] Thiemo Alldieck, Mihai Zanfir, and Cristian Sminchisescu. Photorealistic monocular 3d reconstruction of humans wearing clothing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[5] Paul J Besl and Neil D McKay. Method for registration of 3-d shapes. In Sensor fusion IV: control paradigms and data structures, pages 586\u2013606. Spie, 1992.\\n\\n[6] Mikolaj Binkowski, Danica J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD gans. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.\\n\\n[7] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image. In Proceedings of the European Conference on Computer Vision (ECCV). Springer International Publishing, 2016.\\n\\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 33:1877\u20131901, 2020.\\n\\n[9] Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Yanjun Wang, Hui En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Lei Yang, and Ziwei Liu. Smpler-x: Scaling up expressive human pose and shape estimation. In Advances in Neural Information Processing Systems (NeurIPS), 2023.\\n\\n[10] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-Yee K Wong. Dreamavatar: Text-and-shape guided 3d human avatar generation via diffusion models. arXiv preprint arXiv:2304.00916, 2023.\\n\\n[11] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023.\\n\\n[12] Enric Corona, Mihai Zanfir, Thiemo Alldieck, Eduard Gabriel Bazavan, Andrei Zanfir, and Cristian Sminchisescu. Structured 3d features for reconstructing relightable and animatable avatars. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\n[14] Zijian Dong, Chen Guo, Jie Song, Xu Chen, Andreas Geiger, and Otmar Hilliges. Pina: Learning a personalized implicit neural avatar from a single rgb-d video sequence. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[15] Qiao Feng, Yebin Liu, Yu-Kun Lai, Jingyu Yang, and Kun Li. Fof: Learning fourier occupancy field for monocular real-time human reconstruction. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\n[16] R\u0131za Alp G\u00fcrler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human pose estimation in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\n[17] Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. Vid2avatar: 3d avatar reconstruction from videos in the wild via self-supervised scene decomposition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[18] Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-Mi Kang, Young-Jae Park, and Hae-Gon Jeon. High-fidelity 3d human digitization from single 2k resolution images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[19] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023.\\n\\n[20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 16000\u201316009, 2022.\\n\\n[21] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and Tony Tung. Arch++: Animation-ready clothed human reconstruction revisited. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2021.\\n\\n[22] Hsuan-I Ho, Lixin Xue, Jie Song, and Otmar Hilliges. Learning locally editable virtual humans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.\\n\\n[24] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Jiaxiang Tang, Deng Cai, and Justus Thies. TeCH: Text-guided Reconstruction of Lifelike Clothed Humans. In International Conference on 3D Vision (3DV), 2024.\\n\\n[25] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. Arch: Animatable reconstruction of clothed humans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[26] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Selfrecon: Self reconstruction your digital avatar from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\"}"}
{"id": "CVPR-2024-2207", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Avatar-craft: Transforming text into neural human avatars with parameterized shape and pose control. arXiv preprint arXiv:2303.17606, 2023.\\n\\nTianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. Instantavatar: Learning avatars from monocular video in 60 seconds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nWei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, and Anurag Ranjan. Neuman: Neural human radiance field from a single video. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.\\n\\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Proceedings of the European Conference on Computer Vision (ECCV), pages 694\u2013711. Springer, 2016.\\n\\nJohanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023.\\n\\nManuel Kaufmann, Velko Vechev, and Dario Mylonopoulos. aitviewer, 2022.\\n\\nMichael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. ACM Transactions on Graphics (TOG), 32(3):1\u201313, 2013.\\n\\nMichael Kazhdan, Matthew Bolitho, and Hugues Hoppe. Poisson surface reconstruction. In Proceedings of the fourth Eurographics symposium on Geometry processing, 2006.\\n\\nByungjun Kim, Patrick Kwon, Kwangho Lee, Myunggi Lee, Sookwan Han, Daesik Kim, and Hanbyul Joo. Chupa: Carving 3d clothed humans from skinned shape priors using 2d diffusion probabilistic models. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023.\\n\\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whithead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023.\\n\\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning (ICML), 2022.\\n\\nTingting Liao, Xiaomei Zhang, Yuliang Xiu, Hongwei Yi, Xudong Liu, Guo-Jun Qi, Yong Zhang, Xianguyu Zhu, and Zhen Lei. High-Fidelity Clothed Avatar Reconstruction from a Single Image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl von Ondr. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023.\\n\\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A skinned multi-person linear model. ACM Transactions on Graphics (TOG), 34(6):248:1\u2013248:16, 2015.\\n\\nWilliam E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction algorithm, 1998.\\n\\nCamillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-Ling Chang, Ming Yong, Juhyun Lee, Wan-Teh Chang, Wei Hua, Manfred Georg, and Matthias Grundmann. Mediapipe: A framework for perceiving and processing reality. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshop (CVPRW), 2019.\\n\\nQianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J. Black. Learning to Dress 3D People in Generative Clothing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nGal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\\n\\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (TOG), 41(4):102:1\u2013102:15, 2022.\\n\\nAlejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV). Springer, 2016.\\n\\nGeorgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black. Expressive body capture: 3d hands, face, and body from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\"}"}
{"id": "CVPR-2024-2207", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.\\n\\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In Proceedings of the International Conference on Learning Representations (ICLR), 2023.\\n\\nGuocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanem. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843, 2023.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021.\\n\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine-tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems (NeurIPS), 35, 2022.\\n\\nShunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019.\\n\\nShunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nKaiyue Shen, Chen Guo, Manuel Kaufmann, Juan Zarate, Julien Valentin, Jie Song, and Otmar Hilliges. X-avatar: Expressive human avatars. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nTianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\nDavid Svitov, Dmitrii Gudkov, Renat Bashirov, and Victor Lempitsky. Dinar: Diffusion inpainting of neural textures for one-shot human avatars. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023.\\n\\nMaxim Tatarchenko, Stephan R Richter, Ren\u00e9e Ranftl, Zhuwen Li, Vladlen Koltun, and Thomas Brox. What do single-view 3d reconstruction networks learn? In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nAyush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, W Yifan, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. Advances in neural rendering. In Annual Conference of the European Association for Computer Graphics (EUROGRAPHICS), pages 703\u2013735. Wiley Online Library, 2022.\\n\\nYating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang. Recovering 3d human mesh from monocular images: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023.\\n\\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\nZhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality assessment. In The Thirty-Seventh Asilomar Conference on Signals, Systems & Computers, 2003.\\n\\nChung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. HumanNeRF: Free-viewpoint rendering of moving people from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 16210\u201316220, 2022.\\n\\nYiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond. In Annual Conference of the European Association for Computer Graphics (EUROGRAPHICS), pages 641\u2013676. Wiley Online Library, 2022.\\n\\nYuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. ICON: Implicit Clothed humans Obtained from Normals. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nYuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J. Black. ECON: Explicit Clothed humans Optimized via Normal integration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\"}"}
{"id": "CVPR-2024-2207", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nHuichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long, Feida Zhu, Kang Du, and Min Zheng. Avatarverse: High-quality stable 3d avatar creation from text and pose. arXiv preprint arXiv:2308.03610, 2023.\\n\\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023.\\n\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\nZerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.\"}"}
{"id": "CVPR-2024-2207", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SiTH: Single-view Textured Human Reconstruction with Image-Conditioned Diffusion\\nHsuan-I Ho Jie Song Otmar Hilliges\\nDepartment of Computer Science, ETH Zurich\\nhttps://ait.ethz.ch/sith\\n\\nAI-generated Image\\nBack-view Hallucination\\nMesh Reconstruction\\n\\nReal Photo\\nBack-view Hallucination\\nMesh Reconstruction\\n\\n\u201cA woman in a white T-shirt and a blue tennis skirt\u201d\\n\\nFigure 1. Single-view textured human reconstruction. SiTH is a novel pipeline for creating high-quality and fully textured 3D human meshes from single images. We first hallucinate back-view appearances through an image-conditioned diffusion model, followed by the reconstruction of full-body textured meshes using both the front and back-view images. Our pipeline enables the creation of lifelike and diverse 3D humans from unseen photos (left) and AI-generated images (right).\\n\\nAbstract\\nA long-standing goal of 3D human reconstruction is to create lifelike and fully detailed 3D humans from single-view images. The main challenge lies in inferring unknown body shapes, appearances, and clothing details in areas not visible in the images. To address this, we propose SiTH, a novel pipeline that uniquely integrates an image-conditioned diffusion model into a 3D mesh reconstruction workflow. At the core of our method lies the decomposition of the challenging single-view reconstruction problem into generative hallucination and reconstruction subproblems. For the former, we employ a powerful generative diffusion model to hallucinate unseen back-view appearance based on the input images. For the latter, we leverage skinned body meshes as guidance to recover full-body texture meshes from the input and back-view images. SiTH requires as few as 500 3D human scans for training while maintaining its generality and robustness to diverse images. Extensive evaluations on two 3D human benchmarks, including our newly created one, highlighted our method\u2019s superior accuracy and perceptual quality in 3D textured human reconstruction.\\n\\n1. Introduction\\nWith the growing popularity of 3D and virtual reality applications, there has been increasing interest in creating realistic 3D human models. In general, crafting 3D humans is labor-intensive, time-consuming, and requires collaboration from highly skilled professionals. To bring lifelike 3D humans to reality and to support both expert and amateur creators in this task, it is essential to enable users to create textured 3D humans from simple 2D images or photos.\\n\\nReconstructing a fully textured human mesh from a single-view image presents an ill-posed problem with two major challenges. Firstly, the appearance information required for generating texture in unobserved regions is missing. Secondly, 3D information for mesh reconstruction, such as depth, surface, and body pose, becomes ambiguous in a 2D image. Previous efforts [4, 60, 79] attempted to tackle these challenges in a data-driven manner, focusing on training neural networks with image-mesh pairs. However, these approaches struggle with images featuring unseen appearances or poses, due to limited 3D human training data. More recent studies [61, 73, 79] introduced additional 3D reasoning modules to enhance robustness against unseen...\"}"}
{"id": "CVPR-2024-2207", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"poses. Yet, generating realistic and full-body textures from unseen appearances still remains an unsolved problem.\\n\\nTo address the above challenges, we propose SiTH, a novel pipeline that integrates an image-conditioned diffusion model to reconstruct lifelike 3D textured humans from monocular images. At the core of our approach is the decomposition of the challenging single-view problem into two subproblems: generative back-view hallucination and mesh reconstruction. This decomposition enables us to exploit the generative capability of pretrained diffusion models to guide full-body mesh and texture reconstruction. The workflow is depicted in Fig. 1. Given a front-view image, the first stage involves hallucinating a perceptually consistent back-view image using image-conditioned diffusion. The second stage reconstructs full-body mesh and texture, utilizing both the front and back-view images as guidance.\\n\\nMore specifically, we employ the generative capabilities of pretrained diffusion models (e.g. Stable Diffusion) to infer unobserved back-view appearances for full-body 3D reconstruction. The primary challenge in ensuring the realism of 3D meshes lies in generating images that depict spatially aligned body shapes and perceptually consistent appearances with the input images. While diffusion models demonstrate impressive generative abilities with text conditioning, they are limited in producing desired back-view images using the frontal images as image conditions. To overcome this, we adapt the network architecture to enable conditioning on frontal images and introduce additional trainable components following ControlNet to provide pose and mask control. To fully tailor this model to our task while retaining its original generative power, we carefully fine-tune the diffusion model using multi-view images rendered from 3D human scans. Complementing this generative model, we develop a mesh reconstruction module to recover full-body textured mesh from front and back-view images. We follow prior work in handling 3D ambiguity through normal and skinned body guidance.\\n\\nIt is worth noting that the models for both subproblems are trained using the same public THuman2.0 dataset, which consists of as few as 500 scans.\\n\\nTo advance research in single-view human reconstruction, we created a new benchmark based on the high-quality CustomHumans dataset and conducted comprehensive evaluations against state-of-the-art methods. Compared to existing end-to-end methods, our two-stage pipeline can recover full-body textured meshes, including back-view details, and demonstrates robustness to unseen images. In contrast to time-intensive diffusion-based optimization methods, our pipeline efficiently produces high-quality textured meshes in under two minutes. Moreover, we explored applications combining text-guided diffusion models, showing SiTH's versatility in 3D human creation. Our contributions are summarized as follows:\\n\\n- We introduce SiTH, a single-view human reconstruction pipeline capable of producing high-quality, fully textured 3D human meshes within two minutes.\\n- Through decomposing the single-view reconstruction task, SiTH can be efficiently trained with public 3D human scans and is more robust to unseen images.\\n- We establish a new benchmark featuring more diverse subjects for evaluating textured human reconstruction.\\n\\n2. Related Work\\n\\n**Single-view human mesh reconstruction.** Reconstructing 3D humans from monocular inputs has gained more popularity in research. In this context, we focus on methods that recover 3D human shapes, garments, and textures from a single image. As a seminal work, Saito et al. first proposed a data-driven method with pixel-aligned features and neural fields. Its follow-up work PIFuHD further improved this framework with high-res normal guidance. Later approaches extended this framework with additional human body priors. For instance, PaMIR and ICON utilized skinned body models to guide 3D reconstruction. ARCH, ARCH++, and CAR transformed global coordinates into the canonical coordinates to allow for reposing. PHOHRUM and S3F further disentangled shading and albedo to enable relighting. Another line of work replaced the neural representations with conventional Poisson surface reconstruction. ECON and 2K2K trained normal and depth predictors to generate front and back 2.5D point clouds. The human mesh is obtained by fusing these point clouds with body priors and 3D heuristics. However, none of these methods produce realistic full-body texture and geometry in the unobserved regions. Our pipeline addresses this problem by incorporating a generative diffusion model into the 3D human reconstruction workflow.\\n\\n**3D generation with 2D diffusion models.** Diffusion models trained with large collections of images have demonstrated unprecedented capability in creating 3D objects from text prompts. Most prior work followed an optimization workflow to update 3D representations (e.g. NeRF, SDF tetrahedron) via neural rendering and a score distillation sampling (SDS) loss. While some methods applied this workflow to human bodies, they cannot produce accurate human bodies and appearances due to the ambiguity of text-conditioning. More recent work also tried to extend this workflow with more accurate image-conditioning. However, we show that they struggle to recover human clothing details and require a long optimization time. Most related to our work is Chupa, which also decomposes its pipeline into two stages. Note that Chupa is an optimization-based approach that relies on texts.\"}"}
{"id": "CVPR-2024-2207", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. Method overview. SiTH is a two-stage pipeline composed of back-view hallucination and mesh reconstruction. The back-view hallucination module samples perceptually consistent back-view images through an iterative denoising process conditioned on the input image, UV map, and silhouette mask (Sec. 3.1). Based on the input and generated back-view images, the mesh reconstruction module recovers a full-body mesh and textures leveraging a skinned body prior as guidance (Sec. 3.2). Note that both modules in the pipeline can be trained with the same public 3D human dataset and generalize unseen images.\\n\\nDiffusion models adaptation. Foundation models [8, 13, 20, 36] trained on large-scale datasets have been shown to be adaptable to various downstream tasks. Following this trend, pretrained diffusion models [52, 56, 57, 59] have become common backbones for generative modeling. For instance, they can be customized by finetuning with a small collection of images [23, 37, 58]. ControlNet [77] introduced additional trainable plugins to enable image conditioning such as body skeletons. While these strategies have been widely adopted, none of them directly fit our objective. More relevant to our task is DreamPose [31], which utilizes DensePose [16] images as conditions to repose input images. However, it cannot handle out-of-distribution images due to overfitting. Similarly, Zero-1-to-3 [41] finetunes a diffusion model with multi-view images to allow for viewpoint control. However, we show that viewpoint conditioning is not sufficient for generating consistent human bodies. Our model addresses this issue by providing accurate body pose and mask conditions for back-view hallucination.\\n\\n3. Methodology\\n\\n3.1. Back-view Hallucination\\n\\nPreliminaries. Given an input front-view image \\\\(I_F\\\\) of size \\\\(R_H \\\\times W \\\\times 3\\\\), our goal is to infer a back-view image \\\\(I_B\\\\) of size \\\\(R_H \\\\times W \\\\times 3\\\\) which depicts unobserved body appearances. This task is under-constrained since there are multiple possible solutions to the same input images. Taking this perspective into account, we leverage a latent diffusion model (LDM) \\\\([57]\\\\) to learn a conditional distribution of back-view images given a front-view image. First, a VAE autoencoder, consisting of an encoder \\\\(E\\\\) and a decoder \\\\(D\\\\), is pretrained on a corpus of 2D natural images through image reconstruction, i.e., \\\\(\\\\tilde{I} = D(E(I))\\\\). Afterwards, an LDM learns to produce a latent code \\\\(z\\\\) within the VAE latent distribution \\\\(z = E(I)\\\\) from randomly sampled noise. To sample an image, a latent code \\\\(\\\\tilde{z}\\\\) is obtained by iteratively denoising Gaussian noise. The final image is reconstructed through the decoder, i.e., \\\\(\\\\tilde{I} = D(\\\\tilde{z})\\\\).\\n\\nImage-conditioned diffusion model. Simply applying the LDM architecture to our task is not sufficient since our goal is to learn a conditional distribution of back-view images given an input conditional image. To this end, we make several adaptations to allow for image-conditioning as shown in Fig. 3. First, we utilize the pretrained CLIP \\\\([55]\\\\) image encoder and VAE encoder \\\\(E\\\\) to extract image features from the front-view image (i.e., \\\\(I_F\\\\)). These image features are used for conditioning the LDM, ensuring the output image shares a consistent appearance with the input image. Second, we follow the idea of ControlNet \\\\([77]\\\\) and propose to use a UV map \\\\((I_B^{UV})\\\\) and a silhouette mask.\"}"}
{"id": "CVPR-2024-2207", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Training of back-view hallucination module. We employ a pretrained LDM and ControlNet architecture to enable image conditioning. To train our model, we render training pairs of conditional images $I_F$ and ground-truth images $I_B$ from 3D human scans. Given a noisy image latent $z_t$, the model predicts added noise $\\\\varepsilon$ given the conditional image $I_F$, UV map $I_B^{UV}$, and mask $I_B^{M}$ as conditions. We train the ControlNet model and cross-attention layers while keeping other parameters frozen.\\n\\nAnother challenge in training an image-conditioned LDM is data. Training the model from scratch is infeasible due to the requirement of a large number of paired images rendered from 3D textured human scans. Inspired by the concept of learning from large-scale pretraining \\\\[13,20\\\\], we build our image-conditioned LDM on top of a pretrained diffusion U-Net \\\\[57\\\\]. We utilize the finetuning strategy \\\\[37,77\\\\] to optimize cross-attention layers and ControlNet parameters while keeping most of the other parameters frozen (see Fig. 3). The design and training strategy of our image-conditioned diffusion model enables hallucinating plausible back-view images that are consistent with the frontal inputs.\\n\\nTraining and inference. To generate pairwise training images from 3D human scans, we sample camera view angles and use orthographic projection to render RGBA images from 3D scans and UV maps from their SMPL-X fits. Given a pair of images rendered by a frontal and its corresponding back camera, the first image serves as the conditional input $I_F$ while the other one is the ground-truth image $I_B$. During training, the ground-truth latent code $z_0 = E(I_B)$ is perturbed by the diffusion process in $t$ time steps, resulting in a noisy latent $z_t$. The image-conditioned LDM model $\\\\varepsilon$ aims to predict the added noise $\\\\varepsilon$ given the noisy latent $z_t$, the time step $t \\\\in [0,1000]$, the conditional image $I_F$, the silhouette mask $I_B^{M}$, and the UV map $I_B^{UV}$ (See Fig. 3).\\n\\nThe objective function for fine-tuning can be represented as:\\n\\n$$\\\\min_{\\\\theta} \\\\mathcal{L}(\\\\theta) = \\\\mathbb{E}_{I_B \\\\sim \\\\mathcal{D}, t \\\\sim \\\\mathcal{U}(0,1000)} \\\\left[ \\\\| \\\\varepsilon \\\\|^2 \\right] + \\\\lambda \\\\sum_{i=1}^{n} \\\\| \\\\theta_i \\\\|^2,$$\\n\\nwhere $\\\\lambda$ is a hyperparameter.\\n\\nAt test time, we obtain $I_B^{UV}, I_B^{M}$ from an off-the-shelf pose predictor \\\\[9\\\\] and segmentation model \\\\[36\\\\]. To infer a back-view image, we sample a latent $\\\\tilde{z}_0$ by performing the iterative denoising process starting from a Gaussian noise $z_T \\\\sim N(0, I)$. The back-view image can be obtained by:\\n\\n$$\\\\tilde{I}_B = D(\\\\tilde{z}_0) = D(f_\\\\theta(z_T, I_F, I_B^{UV}, I_B^{M})),$$\\n\\nwhere $f_\\\\theta$ is a function representing the iterative denoising process of our image-conditioned LDM (See Fig. 2 left).\\n\\n3.2. Human Mesh Reconstruction\\n\\nAfter obtaining the back-view image, our goal is to construct a full-body human mesh and its textures using the input and back-view image as guidance. We follow the literature \\\\[60,61\\\\] to model this task with a data-driven method. Given pairwise training data (i.e., front/back-view images and 3D scans), we learn a data-driven model that maps these images to a 3D representation (e.g., a signed distance field (SDF)). We define this mapping as below:\\n\\n$$\\\\mathcal{M}: \\\\mathbb{R}^H \\\\times \\\\mathbb{R}^W \\\\times \\\\mathbb{R}^3 \\\\times \\\\mathbb{R}^H \\\\times \\\\mathbb{R}^W \\\\times \\\\mathbb{R}^3 \\\\times \\\\mathbb{R}^3 \\\\to \\\\mathbb{R}^3,$$\\n\\nwhere $x$ is the 3D coordinate of a query point, and $d_x, r_x$ denote the signed distance and RGB color value at point $x$. The network components we used for learning the mapping function are depicted in Fig. 4.\"}"}
{"id": "CVPR-2024-2207", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Local feature querying.\\nTo learn a generic mapping function that is robust to unseen images, it is important that the model is conditioned solely on local image information with respect to the position of $x$. Therefore, we employ the idea of pixel-aligned feature querying \\\\[60, 61\\\\] and separate our model into two branches, i.e., color and geometry. Our model contains a normal predictor that converts the RGB image pair ($I_F, I_B$) into normal maps ($N_F, N_B$).\\n\\nTwo image feature encoders $G_d, G_r$ then extract color and geometry feature maps ($f_d, f_r$) from the images and normal maps respectively (for simplicity we describe the process for a single image and leave out the superscripts, but both front and back images are treated the same). Finally, we project the query point $x$ onto the image coordinate (Fig. 4 red points) to retrieve the local features ($f_d, x$, $f_r, x$) : \\n\\n$$f_d, x = B\\\\left(B\\\\left(G_d(N), \\\\pi(x)\\\\right), \\\\pi(x)\\\\right),$$\\n\\n$$f_r, x = B\\\\left(B\\\\left(G_r(I), \\\\pi(x)\\\\right), \\\\pi(x)\\\\right),$$\\n\\n(4)\\n\\nwhere $B$ is a local feature querying operation using bilinear interpolation and $\\\\pi(\\\\cdot)$ denotes orthographic projection.\\n\\nLocal positional embedding with skinned body prior.\\nAs mentioned in Sec. 1, a major difficulty in mesh reconstruction is 3D ambiguity where a model has to infer unknown depth information between the front and back images. To address this issue, we follow prior work \\\\[22, 73, 79\\\\] leveraging a skinned body mesh \\\\[51\\\\] for guiding the reconstruction task. This body mesh is regarded as an anchor that provides an approximate 3D shape of the human body. To exploit this body prior, we devise a local positional embedding function that transforms the query point $x$ into the local body mesh coordinate system. We look for the closest point $x_c^\\\\star$ on the body mesh (Fig. 4 blue point), i.e.,\\n\\n$$x_c^\\\\star = \\\\arg \\\\min_{x_c} k x - x_c^2,$$\\n\\n(5)\\n\\nwhere $x_c$ are points on the skinned body mesh $M$. Our positional embedding $p$ constitutes four elements: a signed distance value $d_c$ between $x_c^\\\\star$ and $x_c$, a vector $n_c = (x - x_c^\\\\star)$, the UV coordinates $u_c \\\\in [0, 1]^2$ of the point $x_c^\\\\star$, and a visibility label $v_c \\\\in \\\\{1, 0\\\\}$ that indicates whether $x_c^\\\\star$ is visible in the front/back image or neither. Finally, two separate MLPs $H_d, H_r$ take the positional embedding $p = [d_c, n_c, u_c, v_c]$ and the local texture/geometry features ($f_d, x$, $f_r, x$) as inputs to predict the final SDF and RGB values at point $x$:\\n\\n$$d_x = H_d(f_F, f_B, p),$$\\n\\n$$r_x = H_r(f_F, f_B, p).$$\\n\\n(6)\\n\\nTraining and inference.\\nWe used the same 3D dataset described in Sec. 3.1 to render training image pairs ($I_F, I_B$) from the 3D textured scans. For each training scan, query points $x$ are sampled within a 3-dimensional cube $[1, 1]^3$. For each point, we compute the ground-truth signed distance values $d$ to the scan surface, closest texture RGB values $r$, and surface normal $n$. Finally, we jointly optimized the normal predictors, the image encoders, and the MLPs in 542.\"}"}
{"id": "CVPR-2024-2207", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CAPE \\\\[45\\\\] CustomHuman \\\\[22\\\\]\\n\\nMethod\\n\\nCD: P-to-S / S-to-P (cm) # NC \u201d f-Score \u201d\\n\\nLPIPS: F / B (\u21e5 10^2) # NC \u201d f-Score \u201d\\n\\nPIFu \\\\[60\\\\] 2.368 / 3.763 0.778 33.842 2.720 2.209 / 2.582 0.805 34.881 6.073 / 8.496\\n\\nPIFuHD \\\\[61\\\\] 2.401 / 3.522 0.772 35.706 - 2.107 / 2.228 0.804 39.076 -\\n\\nPaMIR \\\\[79\\\\] 2.190 / 2.806 0.804 36.725 2.085 2.181 / 2.507 0.813 35.847 4.646 / 7.152\\n\\n2K2K \\\\[18\\\\] 2.478 / 3.683 0.782 28.700 - 2.488 / 3.292 0.796 30.186 -\\n\\nFOF \\\\[15\\\\] 2.196 / 4.040 0.777 34.227 - 2.079 / 2.644 0.808 36.013 -\\n\\nICON \\\\[73\\\\] 2.516 / 3.079 0.786 29.630 - 2.256 / 2.795 0.791 30.437 -\\n\\nECON \\\\[74\\\\] 2.475 / 2.970 0.788 30.488 - 2.483 / 2.680 0.797 30.894 -\\n\\nSiTH (Ours) 1.899 / 2.261 0.816 37.763 1.977 1.871 / 2.045 0.826 37.029 3.929 / 6.803\\n\\nTable 1. Single-view human reconstruction benchmarks. We report Chamfer distance (CD), normal consistency (NC), and f-score between ground truth and predicted meshes. To evaluate texture reconstruction quality, we compute LPIPS between the image rendering of GT and generated textures. The best and the second best methods are highlighted in bold and underlined respectively. Note that gray color denotes models trained on more commercial 3D human scans while the others are trained on the public THuman2.0 dataset.\\n\\nBoth branches with the following reconstruction losses:\\n\\n\\\\[ \\\\mathcal{L}_d = \\\\left| \\\\mathbf{d}_x \\\\right| + \\\\left| \\\\mathbf{n} \\\\cdot \\\\mathbf{r}_x \\\\mathbf{d}_x \\\\right| \\\\]\\n\\n(7)\\n\\n\\\\[ \\\\mathcal{L}_r = \\\\left| \\\\mathbf{r}_x \\\\right| \\\\]\\n\\n(8)\\n\\nNote that \\\\( \\\\mathbf{r}_x \\\\) indicates numerical finite differences for computing local normals at point \\\\( \\\\mathbf{x} \\\\) and \\\\( \\\\mathbf{n} \\\\) is a hyperparameter.\\n\\nDuring inference, we use the input image \\\\( \\\\mathbf{I}_F \\\\) and the back-view image \\\\( \\\\tilde{\\\\mathbf{I}}_B \\\\) obtained from Sec. 3.1 to reconstruct 3D mesh and textures. First, we align both images with the estimated body mesh \\\\( \\\\mathbf{M} \\\\) to ensure that image features can be properly queried around the 3D anchor. We adopt a similar strategy of SMPLify \\\\[7\\\\] to optimize the scale and the offset of the body mesh with silhouette and 2D joint errors. Finally, we perform the marching cube algorithm \\\\[43\\\\] by querying SDF and RGB values within a dense voxel grid via Eq. (8) (see Fig. 2 right).\\n\\n4. Experiments\\n\\n4.1. Experimental Setup\\n\\nDataset. Previous work relied on training data from commercial datasets such as RenderPeople \\\\[1\\\\]. While these datasets offer high-quality textured meshes, they also limit reproducibility due to limited accessibility. For fair comparisons, we follow ICON \\\\[73\\\\] by training our method on the public 3D dataset THuman2.0 \\\\[75\\\\] and using the CAPE \\\\[45\\\\] dataset for evaluation. However, we observed potential biases in the evaluation due to the low-res ground-truth meshes and image rendering defects in the CAPE dataset (for a detailed discussion, please refer to Supp-Sec. 6). Consequently, we further create a new benchmark that evaluates the baselines on a higher-quality 3D human dataset CustomHumans \\\\[22\\\\]. In the following, we provide a summary of the datasets used in our experiments:\\n\\n\u2022 THuman2.0 \\\\[75\\\\] contains approximately 500 scans of humans wearing 150 different garments in various poses. We use these 3D scans as the training data.\\n\\n\u2022 CAPE \\\\[45\\\\] contains 15 subjects in 8 types of tight outfits. The test set, provided by ICON, consists of 100 meshes. We use CAPE for the quantitative evaluation (Sec. 4.2).\\n\\n\u2022 CustomHumans \\\\[22\\\\] contains 600 higher-quality scans of 80 subjects in 120 different garments and varied poses. We selected 60 subjects for all quantitative experiments, user studies, and ablation studies. (Sec. 4.2 - Sec. 4.4)\\n\\nEvaluation protocol. We follow the evaluation protocol in OccNet \\\\[46\\\\] and ICON \\\\[73\\\\] to compute 3D metrics Chamfer distance (CD), normal consistency (NC), and f-Score \\\\[65\\\\] on the generated meshes. To evaluate reconstructed mesh texture, we report LPIPS \\\\[78\\\\] of front and back texture rendering. In user studies, 30 participants rank the meshes obtained by four different methods. We report the average ranking ranging from 1 (best) to 4 (worst).\\n\\n4.2. Single-view Human Reconstruction Benchmark evaluation. We compared SiTH with state-of-the-art single-view human reconstruction methods, including PIFu \\\\[60\\\\], PIFuHD \\\\[61\\\\], PaMIR \\\\[79\\\\], FOF \\\\[15\\\\], ICON \\\\[73\\\\], PHORHUM \\\\[4\\\\], 2K2K \\\\[18\\\\], and ECON \\\\[74\\\\] on CAPE and CustomHumans. Note that PHORHUM is only used for qualitative comparison since a different camera system is used, leading to the misalignment with ground-truth meshes. We visualize the generated mesh texture and normals in Fig. 5. Existing methods produce over-smoothed texture and normals, particularly in the back. Our method not only generates photorealistic and perceptually consistent appearances in unobserved regions but also recovers underlying geometric details like clothing wrinkles. The quantitative results are summarized in Tab. 1. It's worth noting that most methods are trained with commercial datasets (gray color in Tab. 1), while the others are trained on the public THuman2.0 dataset. To evaluate the methods leveraging a skinned body prior (i.e., PaMIR, ICON, ECON, FOF, and SiTH), we use the same pose alignment procedure in their original implementations for a fair\"}"}
{"id": "CVPR-2024-2207", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DreamPose\\n\\nControlNet + Interrogate\\n\\nZero-\\n-\\n-\\n\\nSiTH (Ours)\\n\\nInput Image\\n\\nGT References\\n\\nFigure 6. Qualitative comparison of back-view hallucination. We visualize back-view images generated by the baseline methods. Note that the three different images are sampled from different random seeds. Our results are perceptually close to the ground-truth image in terms of appearances and poses. Moreover, our method also preserves generative stochasticity for handling tiny wrinkle changes.\\n\\nChupa\\n\\nSiTH (Ours)\\n\\nInput Image\\n\\n\u201cA woman wearing a big coat and jeans\u201d\\n\\nN/A\\n\\nN/A\\n\\nMagic123\\n\\nZero-\\n-\\n-\\n\\nFigure 7. Comparison with optimization-based methods. Compared to methods that utilize diffusion models for optimization, our result is closer to the input image and contains local geometric details. Note that Chupa is not conditioned on the image, and fails to generate correct clothing from the text prompts.\\n\\ncomparison. Results in Tab. 1 show that the method using a body prior (PaMIR) outperformed the end-to-end method (PIFuHD) on tight clothing and challenging poses in CAPE. However, it falls short in handling diverse outfits in CustomHumans. Moreover, the methods trained on commercial datasets achieve better performance than those trained with public data (ICON, ECON). Notably, our method is robust across both benchmarks, achieving performance comparable to the methods trained on high-quality commercial data.\\n\\nCompared with optimization-based methods. We compared SiTH with methods that use pretrained diffusion models and a score distillation sampling loss [53] to optimize 3D meshes. In the case of Zero-1-to-3 [41], we used the input image to optimize an instant NGP [49] radiance field, and for Magic-123 [54], we provided additional text prompts to optimize an SDF tetrahedron [63]. From Fig. 7, we see that while both methods can handle full-body textures, they struggle with reasoning the underlying geometry and clothing details. It is worth noting that Zero-1-to-3 and Magic-123 require 10 minutes and 6 hours in optimization, respectively, while our method takes under 2 minutes to generate a textured mesh with a marching cube of 512^3 resolution.\\n\\n| Method      | Front Geometry | Back Geometry | Similarity |\\n|-------------|----------------|---------------|------------|\\n| ICON ECON PIFuHD Ours | 3.127 2.720 | 3.093 1.200 | 3.093 1.456 |\\n\\n| Method      | Front Texture | Back Texture | Similarity |\\n|-------------|---------------|--------------|------------|\\n| PIFu PaMIR PHOHRUM Ours | 3.067 3.450 | 3.140 1.054 | 3.307 1.093 |\\n\\n| Method      | User Preference |\\n|-------------|-----------------|\\n| Chupa Ours   | 64.0%           |\\n\\nTable 2. User study results. Top: 30 users are asked to rank the quality of surface normal images from best (1) to worst (4). We report the average ranking of each method. Middle: Similar to the first task, users are asked to rank the quality of RGB textures. Bottom: We ask users to choose the mesh with a better quality. Additionally, more similar to our method is Chupa [35], which generates front/back-view normals for mesh reconstruction. Note that Chupa is not conditioned on images and does not generate texture. Instead, we provided body poses and text prompts generated by an image-to-text interrogator [38] as their conditional inputs. From Fig. 7, it's clear that text-conditioning is less accurate than image-conditioning, and the method struggles to generate unseen clothing styles such as coats. By contrast, our method can reconstruct correct clothing geometry and texture from unseen images. We present more discussions and comparisons with optimization-based methods in Supp-Sec. 9.1.\\n\\nUser study. The above metrics may not fully capture the quality of 3D meshes in terms of realism and local details. To address this, we conducted a user study to compare the texture and geometry quality among various baselines. We invited 30 users to rank the front/back-view texture and normal renderings of 3D meshes generated by four different methods. Additionally, we asked the users to assess the similarity between the input images and the generated meshes. The results (Tab. 2) support our claim that existing methods struggle to efficiently generate desirable back-view texture/geometry from single-view images. Our method,\"}"}
{"id": "CVPR-2024-2207", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Ablation study on CustomHumans. We ablate the hallucination module and the skinned body mesh in our pipeline. Please refer to our discussion in Sec. 4.4 which leverages the generative capability of diffusion models, consistently outperforms each baseline. It also produces more preferred front-view textures and geometries, as evidenced by higher user rankings. We also conducted a user study with Chupa (in Tab. 2 bottom) which also indicates more users prefer the 3D meshes generated by our method.\\n\\n4.3. Generative Capability\\n\\nImage quality comparison. Our hallucination module is a unique and essential component that generates spatially aligned human images to guide 3D mesh reconstruction. Given that our focus is on back-view hallucination, we compare the quality of generated images with the relevant generative methods in Fig. 6. We trained a baseline Pix2PixHD [69] model, which produced smooth and blurry results on unseen images due to overfitting to 500 subjects. Another method closely related to ours is DreamPose [31], which conditions the model with DensePose images and finetunes the diffusion model with paired data. However, their model failed to handle unseen images, in contrast to our approach. While Zero-1-to-3 [41] can generalize to unseen images, their method faces challenges in generating consistent body poses given the same back-view camera. Moreover, we designed another baseline that provides ControlNet [77] for corresponding text prompts using an image-to-text interrogator [38]. However, without proper image conditioning and fine-tuning, such a method cannot generate images that faithfully match the input appearances. Our method not only addresses these issues but also handles stochastic appearances (e.g., tiny differences in wrinkles) from different random seeds. We report 2D generative evaluation metrics and more results in Supp-Sec. 8.1.\\n\\n4.4. Ablation Study\\n\\nWe conducted controlled experiments to validate the effectiveness of our proposed modules. As shown in Fig. 8, the skinned body mesh is a crucial component for 3D human reconstruction. Without this body mesh as a 3D anchor, the output mesh contains an incorrect body shape due to the depth ambiguity issue. Conversely, removing the hallucination module has minimal impact on 3D reconstruction metrics, though it slightly degrades normal consistency. However, the overall quality in both texture and geometry is incomparable with our full model (see Fig. 8 right). This is consistent with our findings in user studies, indicating that 3D metrics may not accurately reflect the perceptual quality of 3D meshes. Finally, we tested two additional variants, leveraging ground-truth body meshes and real back-view images in our full pipeline, representing the upper bound of our method. As shown in Tab. 3 bottom, this additional information notably improves the 3D metrics. These results highlight the persistent challenges in the single-view reconstruction problem, including pose ambiguity and the stochastic nature of clothing geometry. For more experiments on our design choices, please refer to Supp-Sec. 8.5.\\n\\n4.5. Applications\\n\\nInheriting the generative capability of LDM, SiTH is robust to diverse inputs, such as out-of-distribution or AI-generated images. We demonstrate a unique solution to link photo-realistic AI photos and high-fidelity 3D humans. In Fig. 9, we introduce a 3D creation workflow integrating powerful text-to-image generative models. Given a body pose, we generate a front-view image using Stable Diffusion and ControlNet using text prompts. SiTH then creates a full-body textured human from the AI-generated image.\\n\\n5. Conclusion\\n\\nWe propose an innovative pipeline designed to create fully textured 3D humans from single-view images. Our approach seamlessly integrates an image-conditioned diffusion model into the existing data-driven 3D reconstruction workflow. Leveraging the generative capabilities of the diffusion model, our method efficiently produces lifelike 3D humans from a diverse range of unseen images in under two minutes. We expect our work will advance the application of generative AI in 3D human creation.\\n\\nAcknowledgements. This work was partially supported by the Swiss SERI Consolidation Grant \\\"AI-PERCEIVE\\\".\"}"}
