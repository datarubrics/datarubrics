{"id": "CVPR-2024-1968", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\nPseudo algorithms of RankMatch.\\n\\n1: Inputs: Labeled Set $D_l = \\\\{(x_{li}, y_{li})\\\\}_{li=1}^{N_l}$, Unlabeled Set $D_u = \\\\{x_{ui}\\\\}_{ui=1}^{N_u}$ ($N_u \\\\gg N_l$)\\n\\n2: Define: Teacher Network $f_T$, Student Network $f_S$, Weak Augmentation $\\\\text{aug} \\\\cdot$, Strong Augmentation $A_{\\\\text{ug}} \\\\cdot$\\n\\n3: Output: Student Network $f_S$\\n\\n4: for each batch of $(x_{li}, y_{li}), x_{ui}$ in $D_l, D_u$ do\\n\\n5: # Labeled Data:\\n6: Calculate $L_{\\\\text{sup}}$ for $f_S$ by Equation (1) $\\\\triangleright$ Supervised Loss\\n\\n7: # Unlabeled Data:\\n8: Obtain pseudo-labels from $f_T$ by Equation (2)\\n\\n9: Calculate $L_{\\\\text{reg}}$ for $f_S$ by Equation (3) $\\\\triangleright$ Pixel-wise Consistency Regularization Loss\\n\\n10: Obtain agents for $f_T$ and $f_S$ respectively through orthogonal selection strategy\\n\\n11: Calculate the agent-level correlation by Equation (4)\\n\\n12: Transform the agent-level correlation into agent-ranking probability distribution by Equation (6)\\n\\n13: Calculate $L_{\\\\text{rank}}$ for $f_S$ by Equation (8) $\\\\triangleright$ Rank-aware Correlation Consistency Regularization Loss\\n\\n14: Gradient backward $L_{\\\\text{sup}} + L_{\\\\text{reg}} + \\\\lambda L_{\\\\text{rank}} \\\\triangleright$ Update Model\\n\\n15: end for\\n\\npermutation. That is to say, every permutation of the agents exists with some probability rather than only the permutation from largest to smallest exists. The probability of one permutation $\\\\pi \\\\in P$ ($|P| = N!$) given $c$ can be derived as:\\n\\n$$P(\\\\pi|c) = \\\\prod_{n=1}^{N} c_{\\\\pi(n)}^{c_{\\\\pi(n)}}, (6)$$\\n\\nwhere $\\\\pi(n)$ denotes the $n$th agent index of this permutation.\\n\\nFor example, suppose we have three agents: $a$, $b$ and $c$. One permutation of these three agents is $\\\\pi = (a, b, c)$. Based on the agent-level correlation $c$, we can derive the probability of permutation $\\\\pi$:\\n\\n$$P(\\\\pi|c) = c_a c_a c_b c_b c_c c_c,$$ (7)\\n\\nFrom this perspective, the ranking permutation reflects the relationship of agents. By calculating the probabilities for all $|P|$ permutations, we transform the agent-level correlation $c$ into agent-ranking probability distribution $P(P|c) \\\\in \\\\mathbb{R}^{1 \\\\times |P|}$, which has modeled the inter-agent relationship. In fact, if we calculate the full permutations for all $N$ agents, the computational overhead is indeed unacceptable. For computational efficiency, we focus on the permutations of the top-4 agents for each pixel, based on our observation that in every agent-level correlation, the top-4 agents have occupied almost all weight. Then, the rank-aware correlation consistency regularization can be obtained by:\\n\\n$$L_{\\\\text{rank}} = \\\\frac{1}{N_u} \\\\sum_{i=1}^{N_u} \\\\sum_{j=1}^{H \\\\times W} \\\\ell_{kl} \\\\prod_{w, s} P(P|c_{\\\\text{w},ij}) P(P|c_{\\\\text{s},ij}), (8)$$\\n\\nFinally, the overall loss objective of our RankMatch is derived as:\\n\\n$$L = L_{\\\\text{sup}} + L_{\\\\text{reg}} + \\\\lambda L_{\\\\text{rank}}, (9)$$\\n\\nwhere the $\\\\lambda$ is the trade-off weight.\\n\\n4. Experiments\\n4.1. Experimental Setup\\nDatasets:\\n(1) PASCAL VOC 2012 [11] is an object-centric semantic segmentation dataset, containing 20 object classes in the foreground and a background class with 1,464 and 1,449 finely annotated images for training and validation, respectively. Many researches [9, 19] augment the original training set (i.e., classic) with additional 9,118 coarsely annotated images in SBD [16] to get a blender training set.\\n(2) Cityscapes [10] is an urban scene understanding dataset consisting of 2,975 images for training and 500 images for validation. The initial 30 semantic classes are remapped into 19 classes for the semantic segmentation task.\\n\\nImplementation Details:\\nFor a fair comparison, we use ResNet-50/101 [17] pretrained on ImageNet [20] as the backbone and DeepLabv3+ [7] as the decoder. The crop size is set as $513 \\\\times 513$ for PASCAL and $801 \\\\times 801$ for Cityscapes, respectively. We adopt stochastic gradient descent (SGD) optimizer with an initial learning rate of $0.001$ for PASCAL and $0.005$ for Cityscapes. Polynomial Decay learning rate policy is applied throughout the whole training. The strong augmentation $A_{\\\\text{ug}} \\\\cdot$ contains random color jitter, grayscale and Gaussian blur. The weak augmentation $\\\\text{aug} \\\\cdot$ consists of random crop, resize and horizontal flip. The features used to construct the correlation consistency are extracted from the output of the ASPP module [6] and the channel number is $256$. We set the number of agents $N=128$ and trade-off weight $\\\\lambda=0.1$ for all experiments.\\n\\nThe model is trained for 80 epochs on PASCAL and 240 epochs on Cityscapes with a batch size of 8, using 8 $\\\\times$ RTX 3090 GPUs (memory is 24G/GPU).\"}"}
{"id": "CVPR-2024-1968", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ResNet-50 63 69 61 \u2212 \u2212 \u2212 \u2212 66 64 71 64 \u2212 \u2212 \u2212 \u2212 \u2212 73 68 68 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 70\\nResNet-101 72 45 65 \u2212 \u2212 \u2212 \u2212 75 52 \u2212 \u2212 \u2212 \u2212 73 70 68 71 74 + 67 44 55 73 76 75 77 71 73 77 70 78 \u2212 \u2212 \u2212 \u2212 72\\nResNet-50 62 76 76 75 68 + 74 ResNet-101 74 67 + 78 78 74 72 77 74 75 \u2212 \u2212 \u2212 4.2. Comparison with State-of-the-art Methods\\nTable 2. Quantitative results of different SSL methods on Pascal\\n\\nTable 1. Quantitative results of different SSL methods on Pascal\\n\\nSup.-only show the improvements over when compared to the baseline method, i.e, we consistently observe substantial performance gains.\\n\\nUnlabeled data is effectively utilized in our method. Moreover, we evaluate our method on both PASCAL (classic)\\n\\nWe investigate the effectiveness of our RankMatch.\\n\\nComparison of our method with the SOTA methods on PASCAL Results on PASCAL.\\nTable 1 and Table 2 show the comparison of our method with the SOTA methods on PASCAL blender and classic set. Compared with the supervised-only set. We report mIoU (%) under various partition protocols.\\n\\nfixSeg)\\n\\nDAW\\n\\nCSS\\n\\niMAS\\n\\nST++\\n\\nPL\\n\\nCFCG\\n\\nSup.-only\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n\\n\u2206\\n"}
{"id": "CVPR-2024-1968", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Quantitative results of different SSL methods on Cityscapes. We report mIoU (%) under various partition protocols and show the improvements over Sup.-only baseline. The best is highlighted in bold.\\n\\n| Method          | ResNet-50 | ResNet-101 |\\n|-----------------|-----------|------------|\\n|                  | 1/16(186) | 1/8(372)   |\\n|                  | 1/4(744)  | 1/2(1488)  |\\n| Sup.-only        | 63.3      | 70.2       |\\n| FixMatch [NeurIPS'20] | 72.6    | 75.7       |\\n| AEL [NeurIPS'21] | 74.0      | 76.2       |\\n| AugSeg [CVPR'23] | 73.7      | 76.4       |\\n| iMAS [CVPR'23]  | 74.3      | 77.4       |\\n| ESL [ICCV'23]   | -         | 75.1       |\\n| Co-Train [ICCV'23] | 76.3    | 77.1       |\\n| NP-SemiSeg [ICML'23] | 73.0    | 78.8       |\\n| Switch [NeurIPS'23] | -       | 76.8       |\\n| DAW [NeurIPS'23] | 75.2      | 78.0       |\\n| UniMatch [CVPR'23] | 75.0    | 77.9       |\\n| RankMatch (Ours) | 75.4      | 77.7       |\\n\\n$\\\\Delta \\\\uparrow$ indicates improvements.\\n\\nTable 4. Ablation studies of different components.\\n\\n| Component          | mIoU(92) | mIoU(1464) |\\n|--------------------|----------|------------|\\n| \u2713 Contrastive      | 67.4     | 79.3       |\\n| \u2713 Correlation      | 70.6     | 80.0       |\\n\\nTable 5. Ablation on different agent selection strategies.\\n\\n| Agent Selection   | mIoU    |\\n|--------------------|---------|\\n| All                | 69.8    |\\n| Random             | 70.2    |\\n| Top-N              | 70.6    |\\n| Orthogonal         | 71.6    |\\n\\nTable 6. Ablation on different correlation consistency.\\n\\n| Corr. Consis. | mIoU    |\\n|---------------|---------|\\n| L2            | 70.2    |\\n| CE            | 70.1    |\\n| KL            | 70.3    |\\n| Rank-aware    | 71.6    |\\n\\n4.3. Ablation Study and Analysis\\n\\nTo look deeper into our method, we perform a series of ablation studies on PASCAL classic set under 1/16(92) partition protocol with ResNet-50 to analyze our RankMatch.\\n\\nEffectiveness of Components.\\n\\nIn Table 4, we report the results of 1/16(92) and Full (1464) to clearly substantiate the effectiveness of our design. Note that, \\\"Contrastive\\\" denotes the reproduced results for U^2PL [52], a classic contrastive learning method in the semi-supervised semantic segmentation, based on our baseline (i.e., UniMatch).\\n\\nThe correlation-level consistency (\\\"Correlation\\\") without rank-aware (\\\"Rank\\\") means that treating each agent independently and straightforwardly imposing correlation-level regularization resorting to KL divergence, i.e., $L_{corr}$ in Equation (5). (1) Indeed, while contrastive learning can yield certain benefits for pixel-level consistency regularization baseline (1st row vs 2nd row), it is still inferior to correlation-level consistency regularization (2nd row vs 3rd & 4th rows). (2) By comparing the results of the 3rd and 1st rows, a naive consideration of correlation-level consistency is insufficient. With the help of our rank-aware correlation consistency, RankMatch exhibits superior abilities in most scenarios.\"}"}
{"id": "CVPR-2024-1968", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Visualization of the Lucchi dataset.\\n\\nTable 9. Quantitative results of different SSL methods on Lucchi.\\n\\n| Method     | Spe.1/32(5) | Spe.1/16(10) | Spe.1/8(20) | Sup.-only  |\\n|------------|-------------|--------------|-------------|------------|\\n| MT         | 45.7        | 57.4         | 61.8        | 71.8       |\\n| CCT        |             |              |             | 84.7       |\\n| CPS        |             |              |             | 84.5       |\\n| DualRel    |             |              |             | 85.6       |\\n| Ours       |             |              |             | 86.9       |\\n\\nConsistency reveals a significant performance improvement for pixel-level consistency regularization baseline. This observation indicates that the abundant correlations provide additional information gain for consistency regularization. (3) As the 3rd vs. 4th row shows, harnessing the relationships between agents through the construction of the agent-ranking probability distribution can yield more effective supervision signals, manifesting in further performance improvements.\\n\\nEffectiveness of Orthogonal Selection Strategy. In Table 5, we explore various strategies for agent selection, including \\\"ALL\\\" (considering all pixels in the feature map as agents), \\\"Random\\\" (randomly pick \\\\( N \\\\) pixels in the feature map as agents), \\\"Top-\\\\( N \\\\)\\\" (select top \\\\( N \\\\) pixels conditioned on the cumulative self-correlation matrix along the pixels dimension), and our proposed \\\"Orthogonal\\\". (1) Selecting all pixels as agents is not a desirable approach, as it inevitably introduces considerable noise among these pixels. This noise can adversely affect the quality of supervision signals, resulting in sub-optimal performance. (2) The strategy of \\\"Orthogonal\\\" achieves the best results, which is in line with our design purpose, that is, that representative agents can enjoy synergy with subsequent correlation-level consistency. We visualize the agent-pixel activation maps for those agents selected by \\\"Orthogonal\\\", as shown in Figure 5. It can be observed that the different agents activate different parts of the image, and resonate favorably with diverse semantic cues from the original pixels. These carefully selected agents retain as much critical information as possible in the original image, facilitating the subsequent construction of correlation consistency.\\n\\nEffectiveness of Rank-aware Correlation Consistency. To investigate the effectiveness of rank-aware correlation consistency, we compare different modeling strategies for correlation consistency in Table 6. Among them, L2, CE, and KL belong to agent-independent correlation consistency, overlooking the inherent relationships between agents and resulting in sub-optimal performance. The proposed rank-aware correlation consistency achieve the best results, indicating that modeling the relationships between agents contributes to more effective supervision signals.\\n\\nHyperparameter Evaluations. (1) As shown in Table 7, it can be observed that the performance is optimal with \\\\( N = 128 \\\\). This result aligns with intuition, as too few agents can lead to information loss from the original image while too many can introduce noise into the training. Therefore, finding a balance for \\\\( N \\\\) is crucial. (2) \\\\( \\\\lambda \\\\) controls the relative importance of the rank-aware correlation consistency loss, our model achieves much better performance when \\\\( \\\\lambda = 0.1 \\\\) as shown in Table 8.\\n\\nScalability for Other Scenarios. We extend our experimental evaluations on mitochondria segmentation [26, 27, 31, 39, 43] dataset Lucchi [29] to assess the scalability of our method. Figure 4 illustrates the images and ground truth of the Lucchi dataset, highlighting a common challenge in electron microscope images where instances are notably small and scattered. It underscores the need for more robust supervision during training within a semi-supervised framework. As depicted in Table 9, RankMatch exhibits superior performance compared to other competitive methods. Notably, our approach surpasses the specialized (\\\"spe.\\\" method DualRel [34] in the domain of electron microscopy images, underscoring the capability of our method to provide more rich and effective supervision.\\n\\n5. Conclusion\\n\\nIn this paper, we offer a fresh perspective on inter-pixel correlations to construct more safe and effective supervision signals. To this end, we develop a coherent RankMatch network, including the construction of representative agents to model inter-pixel correlation beyond regular individual pixel-wise consistency, and further unlock the potential of agents by modeling inter-agent relationships in pursuit of rank-aware correlation consistency. Extensive experimental results on challenging benchmarks show the effectiveness.\\n\\n6. Acknowledgments\\n\\nThis work was partially supported by the National Defense Basic Scientific Research Program of China (Grant JCKY2021601B013), National Nature Science Foundation of China (Grant 62021001), Youth Innovation Promotion Association CAS 2018166.\"}"}
{"id": "CVPR-2024-1968", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Eric Arazo, Diego Ortego, Paul Albert, Noel E O'Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2020.\\n\\n[2] Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. Advances in neural information processing systems, 27, 2014.\\n\\n[3] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural information processing systems, 32, 2019.\\n\\n[4] David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain adaptation. arXiv preprint arXiv:2106.04732, 2021.\\n\\n[5] Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 6912\u20136920, 2021.\\n\\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848, 2017.\\n\\n[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801\u2013818, 2018.\\n\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020.\\n\\n[9] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong Wang. Semi-supervised semantic segmentation with cross pseudo supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2613\u20132622, 2021.\\n\\n[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213\u20133223, 2016.\\n\\n[11] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88:303\u2013338, 2010.\\n\\n[12] Di Feng, Christian Haase-Sch\u00fctz, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner Wiesbeck, and Klaus Dietmayer. Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges. IEEE Transactions on Intelligent Transportation Systems, 22(3):1341\u20131360, 2020.\\n\\n[13] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004.\\n\\n[14] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.\\n\\n[15] Lan-Zhe Guo and Yu-Feng Li. Class-imbalanced semi-supervised learning with adaptive thresholding. In International Conference on Machine Learning, pages 8082\u20138094. PMLR, 2022.\\n\\n[16] Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In 2011 international conference on computer vision, pages 991\u2013998. IEEE, 2011.\\n\\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.\\n\\n[19] Hanzhe Hu, Fangyun Wei, Han Hu, Qiwei Ye, Jinshi Cui, and Liwei Wang. Semi-supervised semantic segmentation via adaptive equalization learning. Advances in Neural Information Processing Systems, 34:22106\u201322118, 2021.\\n\\n[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.\\n\\n[21] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016.\\n\\n[22] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, page 896, 2013.\\n\\n[23] Shuo Li, Yue He, Weiming Zhang, Wei Zhang, Xiao Tan, Junyu Han, Errui Ding, and Jingdong Wang. Cfcg: Semi-supervised semantic segmentation via cross-fusion and contour guidance supervision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16348\u201316358, 2023.\\n\\n[24] Yijiang Li, Xinjiang Wang, Lihe Yang, Litong Feng, Wayne Zhang, and Ying Gao. Diverse cotraining makes strong semi-supervised segmentor. arXiv preprint arXiv:2308.09281, 2023.\\n\\n[25] Chen Liang, Wenguan Wang, Jiaxu Miao, and Yi Yang. Logic-induced diagnostic reasoning for semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16197\u201316208, 2023.\"}"}
{"id": "CVPR-2024-1968", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Xiaoyu Liu, Bo Hu, Wei Huang, Yueyi Zhang, and Zhiwei Xiong. Efficient biomedical instance segmentation via knowledge distillation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 14\u201324. Springer, 2022.\\n\\nXiaoyu Liu, Wei Huang, Zhiwei Xiong, Shenglong Zhou, Yueyi Zhang, Xuejin Chen, Zheng-Jun Zha, and Feng Wu. Learning cross-representation affinity consistency for sparsely supervised biomedical instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21107\u201321117, 2023.\\n\\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431\u20133440, 2015.\\n\\nAur\u00e9lien Lucchi, Kevin Smith, Radhakrishna Achanta, Graham Knott, and Pascal Fua. Supervoxel-based segmentation of mitochondria in em image stacks with learned shape features. IEEE transactions on medical imaging, 31(2):474\u2013486, 2011.\\n\\nNaisong Luo, Yuwen Pan, Rui Sun, Tianzhu Zhang, Zhiwei Xiong, and Feng Wu. Camouflaged instance segmentation via explicit de-camouflaging. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17918\u201317927, 2023.\\n\\nNaisong Luo, Rui Sun, Yuwen Pan, Tianzhu Zhang, and Feng Wu. Electron microscopy images as set of fragments for mitochondrial segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024.\\n\\nYucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs for semi-supervised learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8896\u20138905, 2018.\\n\\nJie Ma, Chuan Wang, Yang Liu, Liang Lin, and Guanbin Li. Enhanced soft label for semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1185\u20131195, 2023.\\n\\nHuayu Mai, Rui Sun, Tianzhu Zhang, Zhiwei Xiong, and Feng Wu. Dualrel: Semi-supervised mitochondria segmentation from a prototype perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19617\u201319626, 2023.\\n\\nHuayu Mai, Rui Sun, Yuan Wang, Tianzhu Zhang, and Feng Wu. Pay attention to target: Relation-aware temporal consistency for domain adaptive video semantic segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024.\\n\\nJaemin Na, Jung-Woo Ha, Hyung Jin Chang, Dongyoon Han, and Wonjun Hwang. Switching temporary teachers for semi-supervised semantic segmentation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n\\nAvital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. Advances in neural information processing systems, 31, 2018.\\n\\nYassine Ouali, C\u00e9line Hudelot, and Myriam Tami. Semi-supervised semantic segmentation with cross-consistency training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12674\u201312684, 2020.\\n\\nYuwen Pan, Naisong Luo, Rui Sun, Meng Meng, Tianzhu Zhang, Zhiwei Xiong, and Yongdong Zhang. Adaptive template transformer for mitochondria segmentation in electron microscopy images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21474\u201321484, 2023.\\n\\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596\u2013608, 2020.\\n\\nRui Sun, Yihao Li, Tianzhu Zhang, Zhendong Mao, Feng Wu, and Yongdong Zhang. Lesion-aware transformers for diabetic retinopathy grading. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10938\u201310947, 2021.\\n\\nRui Sun, Naisong Luo, Yuwen Pan, Huayu Mai, Tianzhu Zhang, Zhiwei Xiong, and Feng Wu. Appearance prompt vision transformer for connectome reconstruction. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, pages 1423\u20131431. International Joint Conferences on Artificial Intelligence Organization, 2023. Main Track.\\n\\nRui Sun, Huayu Mai, Naisong Luo, Tianzhu Zhang, Zhiwei Xiong, and Feng Wu. Structure-decoupled adaptive part alignment network for domain adaptive mitochondria segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 523\u2013533. Springer, 2023.\\n\\nRui Sun, Huayu Mai, Tianzhu Zhang, and Feng Wu. Daw: Exploring the better weighting function for semi-supervised semantic segmentation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n\\nRui Sun, Yuan Wang, Huayu Mai, Tianzhu Zhang, and Feng Wu. Alignment before aggregation: trajectory memory retrieval network for video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1218\u20131228, 2023.\\n\\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.\\n\\nVikas Verma, Kenji Kawaguchi, Alex Lamb, Juho Kannala, Arno Solin, Yoshua Bengio, and David Lopez-Paz. Interpolation consistency training for semi-supervised learning. Neural Networks, 145:90\u2013106, 2022.\\n\\nChangqi Wang, Haoyu Xie, Yuhui Yuan, Chong Fu, and Xiangyu Yue. Space engage: Collaborative space super-vision for contrastive-based semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 931\u2013942, 2023.\"}"}
{"id": "CVPR-2024-1968", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jianfeng Wang, Daniela Massiceti, Xiaolin Hu, Vladimir Pavlovic, and Thomas Lukasiewicz. Np-semiseg: When neural processes meet semi-supervised semantic segmentation. 2023.\\n\\nXiaoyang Wang, Bingfeng Zhang, Limin Yu, and Jimin Xiao. Hunting sparsity: Density-guided contrastive learning for semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3114\u20133123, 2023.\\n\\nYuan Wang, Rui Sun, Zhe Zhang, and Tianzhu Zhang. Adaptive agent transformer for few-shot segmentation. In European Conference on Computer Vision, pages 36\u201352. Springer, 2022.\\n\\nYuchao Wang, Haochen Wang, Yujun Shen, Jingjing Fei, Wei Li, Guoqiang Jin, Liwei Wu, Rui Zhao, and Xinyi Le. Semi-supervised semantic segmentation using unreliable pseudo-labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4248\u20134257, 2022.\\n\\nYuan Wang, Naisong Luo, and Tianzhu Zhang. Focus on query: Adversarial mining transformer for few-shot segmentation. In Advances in Neural Information Processing Systems, 2023.\\n\\nYuan Wang, Rui Sun, and Tianzhu Zhang. Rethinking the correlation in few-shot segmentation: A buoys view. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7183\u20137192, 2023.\\n\\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. Advances in neural information processing systems, 33:6256\u20136268, 2020.\\n\\nYi Xu, Lei Shang, Jinxing Ye, Qi Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong Jin. Dash: Semi-supervised learning with dynamic thresholding. In International Conference on Machine Learning, pages 11525\u201311536. PMLR, 2021.\\n\\nLihe Yang, Lei Qi, Litong Feng, Wayne Zhang, and Yinghuan Shi. Revisiting weak-to-strong consistency in semi-supervised semantic segmentation. arXiv preprint arXiv:2208.09910, 2022.\\n\\nLihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao. St++: Make self-training work better for semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4268\u20134277, 2022.\\n\\nBowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jingdong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. Advances in Neural Information Processing Systems, 34:18408\u201318419, 2021.\\n\\nZhen Zhao, Sifan Long, Jimin Pi, Jingdong Wang, and Luping Zhou. Instance-specific and model-adaptive supervision for semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23705\u201323714, 2023.\\n\\nZhen Zhao, Lihe Yang, Sifan Long, Jimin Pi, Luping Zhou, and Jingdong Wang. Augmentation matters: A simple-yet-effective approach to semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11350\u201311359, 2023.\\n\\nXiaojin Jerry Zhu. Semi-supervised learning literature survey. 2005.\"}"}
{"id": "CVPR-2024-1968", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nThe key lie in semi-supervised semantic segmentation is how to fully exploit substantial unlabeled data to improve the model's generalization performance by resorting to constructing effective supervision signals. Most methods tend to directly apply contrastive learning to seek additional supervision to complement independent regular pixel-wise consistency regularization. However, these methods tend not to be preferred ascribed to their complicated designs, heavy memory footprints and susceptibility to confirmation bias. In this paper, we analyze the bottlenecks exist in contrastive learning-based methods and offer a fresh perspective on inter-pixel correlations to construct more safe and effective supervision signals, which is in line with the nature of semantic segmentation. To this end, we develop a coherent RankMatch network, including the construction of representative agents to model inter-pixel correlation beyond regular individual pixel-wise consistency, and further unlock the potential of agents by modeling inter-agent relationships in pursuit of rank-aware correlation consistency. Extensive experimental results on multiple benchmarks, including mitochondria segmentation, demonstrate that RankMatch performs favorably against state-of-the-art methods. Particularly in the low-data regimes, RankMatch achieves significant improvements.\\n\\n1. Introduction\\nSemantic segmentation, which aims to explain visual semantics at the pixel level, has achieved conspicuous achievements attributed to the recent advances in deep neural network [28] as a fundamental task in computer vision with widespread applications such as visual understanding [11], autonomous driving [12], etc. However, its data-driven nature makes it labor-intensive and time-consuming to gather massive pixel-level annotations as training data. To alleviate the data-hunger issue, considerable works [19, 44, 57] have turned their attention to semi-supervised semantic segmentation task. However, since only limited labeled data is accessible, how to fully exploit a large amount of unlabeled data to improve the model's generalization performance by resorting to constructing effective supervision signals is thus extremely challenging.\\n\\nIn previous literature, pseudo-labeling [1, 22] and consistency regularization [2, 21] have emerged as mainstream paradigms to leverage unlabeled data for semi-supervised semantic segmentation. Recently, these two paradigms are typically encapsulated into a teacher-student scheme [46] (where the teacher and student can be identical), that is,\"}"}
{"id": "CVPR-2024-1968", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the teacher network with weakly augmented perturbation view generates corresponding pseudo-labels to instruct the student network under the presence of strongly augmented perturbation view, using a form of pixel-wise consistency regularization (see Figure 1 (a)).\\n\\nAfter an in-depth analysis of the teacher-student scheme, we argue that constructing extra supervision from substantial unlabeled samples matters in semi-supervised semantic segmentation, which is intuitively sensible from the definition of the task itself; that is, empowering regular consistency regularization to adapt to dense pixel-level prediction rather than suffering from the supervision signals of limited capacity derived at the individual pixel level (Figure 1 (a)). Inspired by the recent popularity of representation learning [8, 18], it naturally comes into mind to directly apply contrastive learning [33, 48, 50, 52] to semi-supervised semantic segmentation to establish an ample set of positive/negative samples in the representation space, aiming at seeking additional supervision to complement independent pixel-wise consistency regularization.\\n\\nDespite their promising results, these methods tend not to be preferred ascribed to complicated designs and heavy memory footprint raised by the existence of ad hoc numerous positive/negative samples, inevitably compressing their capability and compromising the inherent simplicity of the teacher-student scheme. Plus, considering the absence of ground truth in unlabeled data, the determination of positive/negative samples is entirely conditioned on the model's biased predictions (erroneous pseudo-labels), leading to confirmation bias [14]. To make matters worse, the corollary of error accumulation is inevitably amplified by inbuilt low-data regimes of semi-supervised semantic segmentation, hindering the generalization ability of the model.\\n\\nIn this paper, we analyze the bottlenecks exist in contrastive learning-based methods for improving pixel-wise consistency regularization, and offer a fresh perspective on inter-pixel correlations to construct more safe and effective supervision signals for robust semi-supervised semantic segmentation. Intuitively, most methods neglect the fact that dense pixel prediction task carries rich inter-pixel information beyond basic individual pixel-wise consistency, shedding light on the possibility of closer collaboration between the inter-pixel correlation and the consistency regularization (i.e., correlation-wise consistency regularization, right part of Figure 1 (a)) to comprehensively probe unlabeled data. The main idea is, we prepend the agent-level correlation consistency through a set of representative reference points (referred to as agents) to model the inter-pixel correlation (see Figure 1 (b)). For each pixel from the weakly augmented or corresponding strongly augmented view, we can obtain the agent-level correlation (i.e., a likelihood vector) by comparing this pixel with a set of agents.\\n\\nIn essence, the agent-level correlation reflects the consensus among representative agents with a broader receptive field, thus it encodes a higher-order consistency regularization to adapt to dense pixel-level prediction. However, it is non-trivial to attain the appropriate agents without any supervision signals for training. Intuitively, the agents should resonate favorably with diverse semantic cues from the original pixels with a wide range of semantic contrast descriptions. To this end, we devise an orthogonal selection strategy to pick the most representative agents from the feature map, preserving as much critical information as possible in the original pixels. In this way, benefiting from the richer description of the data distribution in agent-level correlation, we can achieve better exploitation of the unlabeled data. Based on the above discussion, it is natural to integrate the resultant agent-level correlation into the teacher-student scheme and impose consistency constraint resorting to KL divergence, etc. (Figure 1 (b)). However, such a straightforward constraint treats each agent independently and heavily relies on strong i.i.d. assumption, hindering the potential for further optimization of the model. In fact, there exist specific relationships between agents that should also be considered in the agent-level correlation consistency regularization. For example, as shown in Figure 1, agent a and agent b are two pixels that reside in the same car while agent c situates from the road. Thus, agent a should hold a tighter relationship with agent b than agent c. To harness the inter-agent relationship modeling information for more effective supervision signals, instead of taking each agent independently, we carefully design the rank-aware correlation consistency to strive to further unlock the potential of agents by imposing the agent-level correlation rich in inter-agent relationship to be consistent between the teacher and student networks (i.e., weak and strong augmented views, Figure 1 (c)). The core idea is that we take the agent ranking as a random event rather than a deterministic permutation. For instance, the correlation between different agents and a given pixel varies, which can be regarded as the probabilities in ranking. The probability of being ranked first of the agent a is 0.3 while 0.5 of the agent b. From this perspective, the ranking permutation reflects the relationship of agents w.r.t. the pixel. In this way, for a given pixel, we consider every possible rank permutation of the agents (e.g., abc, cba, etc.), and transform the agent-level correlation into the agent-ranking probability distribution. By constraining the consistency of the agent-ranking probability distribution between teacher and student networks, the model can be guided by more effective supervision signals. Ultimately, we term our final model as RankMatch.\"}"}
{"id": "CVPR-2024-1968", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The framework of our RankMatch. The student network is guided by two sources of supervision, including the ground truth for the labeled data and the pseudo-labels generated by the teacher network for the unlabeled data. In previous consistency regularization methods, consistency is imposed at the pixel-level. While our work focuses on the rich correlations between pixels and imposes consistency constraint at the correlation-level. Furthermore, we design the ranking-aware correlation consistency for more effective supervision signals.\\n\\n(2) We develop a consistent RankMatch network, including the construction of representative agents to model inter-pixel correlation beyond regular individual pixel-wise consistency, and further unlock the potential of agents by modeling inter-agent relationships in pursuit of rank-aware correlation consistency.\\n\\n(3) Extensive experiments on three challenging benchmarks including mitochondria segmentation demonstrate that our RankMatch outperforms state-of-the-art semi-supervised semantic segmentation methods. Particularly in low-data regimes, RankMatch achieves significant improvements.\\n\\n2. Related Work\\n\\nSemi-supervised Learning. Semi-supervised learning [13, 37, 62] (SSL) is a well-studied topic and recent research can be summarized in two branches: Pseudo-labeling and consistency regularization. Pseudo-labeling [1, 5, 22, 59] methods involve training the model on unlabeled samples using pseudo-labels generated from the most up-to-date optimized model. On the other hand, consistency regularization-based [21, 46, 47, 55] methods leverage the smoothness assumption [32], encouraging the model to exhibit consistency when presented with the same example under different perturbations. Notably, recent SSL methods [3, 4, 15, 40, 56] have demonstrated the synergy between consistency regularization and Pseudo-labeling. One prominent example is FixMatch [40], which generates pseudo-labels from weakly augmented unlabeled images for strongly augmented versions of the same images. This concise yet powerful approach has gained widespread adoption in recent SSL studies.\\n\\nSemi-supervised Semantic Segmentation. Benefits from the advances in deep neural network [30, 35, 41, 42, 45, 51, 53, 54] and various kinds of semi-supervised semantic segmentation (SSSS) algorithms [25, 27, 36, 44, 60, 61] have been proposed based on the mature combination of Pseudo-labeling and consistency regularization. Most of all, UniMatch [57] taking into account the nature of semantic segmentation tasks, incorporates suitable data augmentations into FixMatch, thus evolving into a concise yet powerful SSSS baseline. On top of these fundamental designs, motivated by representation learning, a series of works [33, 48, 50, 52] have incorporated contrastive learning into SSSS, tailoring it to the characteristics of the dense prediction task. In this paper, we offer a fresh perspective on inter-pixel correlations to construct more safe and effective supervision signals for robust semi-supervised semantic segmentation.\"}"}
{"id": "CVPR-2024-1968", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"construction of the agent-level correlation to mine more reliable information in the unlabeled data. Finally, rank-aware correlation consistency regularization is devised to harness the inter-agent relationship for more effective supervision signals. In Algorithm 1, we present the pseudo algorithm of our RankMatch to clearly summarize the method.\\n\\n3.1. Preliminaries\\n\\nGiven a labeled set \\\\(D_l = \\\\{(x_{li}, y_{li})\\\\}_{i=1}^{N_l}\\\\) and an unlabeled set \\\\(D_u = \\\\{x_{ui}\\\\}_{i=1}^{N_u}\\\\), where \\\\(N_u \\\\gg N_l\\\\), semi-supervised semantic segmentation aims to train a segmentation model with limited labeled data and vast unlabeled data. As shown in Figure 2, the popular teacher-student scheme consists of a teacher network \\\\(f_T\\\\) and a student network \\\\(f_S\\\\). The student network is guided by two sources of supervision, including the ground truth for the labeled data and the pseudo-labels generated by the teacher network for the unlabeled data. The teacher network can either be identical to the student network or an exponentially moving average (EMA) version of the student network. In specific, for the labeled data, the supervised loss \\\\(L_{sup}\\\\) can be formulated as:\\n\\n\\\\[\\nL_{sup} = \\\\frac{1}{N_l} \\\\sum_{i=1}^{N_l} \\\\sum_{j=1}^{HW} \\\\ell_{ce}(y_{li}, f_S(x_{li}))\\n\\\\]\\n\\nwhere \\\\(H\\\\) and \\\\(W\\\\) represent the height and width of the input image, \\\\(\\\\ell_{ce}\\\\) denotes the standard pixel-wise cross-entropy loss. For the unlabeled data, the teacher network takes the weak augmented view \\\\(aug(x_{ui})\\\\) as input and generates pseudo-labels \\\\(\\\\hat{y}_{ui}\\\\) for the student network as:\\n\\n\\\\[\\n\\\\hat{y}_{ui} = \\\\begin{cases} \\n\\\\arg \\\\max_{f_T(aug(x_{ui}))} j, & c_{ui} > \\\\gamma \\\\\\\\\\n\\\\text{ignore index}, & \\\\text{otherwise}\\n\\\\end{cases}\\n\\\\]\\n\\nwhere \\\\(c_{ui} = \\\\max_{f_T(aug(x_{ui}))} j\\\\) represents the confidence of the teacher prediction for \\\\(j\\\\)th pixel and \\\\(\\\\gamma\\\\) denotes the confidence threshold to exclude unreliable pseudo-labels from training. As result, we can obtain the consistency regularization loss \\\\(L_{reg}\\\\) as:\\n\\n\\\\[\\nL_{reg} = \\\\frac{1}{N_u} \\\\sum_{i=1}^{N_u} \\\\sum_{j=1}^{HW} \\\\ell_{ce}(\\\\hat{y}_{ui}, f_S(Aug(x_{ui}))),\\n\\\\]\\n\\nwhere \\\\(Aug(\\\\cdot)\\\\) means the strong augmentation. By imposing consistency regularization, the model can learn reliable information from unlabeled data. The overall loss of the commonly used teacher-student scheme is \\\\(L = L_{sup} + L_{reg}\\\\).\\n\\nNote that the above consistency regularization is operated at pixel-level, still stuck in the mindset of classification task. We contend that there exist substantial inter-pixel correlations within an image inherently, which should be taken into account in consistency regularization. What follows, we detail the process of modeling inter-pixel correlation.\\n\\n3.2. Agent-level Correlation\\n\\nTo mine more reliable information in the unlabeled data, the idea arises naturally that we can impose consistency regularization at the correlation level, which is much richer than pixels. However, simply enforcing the correlations of all pixels (i.e., the self-correlation matrix) to be consistent between teacher and student is not desirable. Lots of noise in the self-correlation matrix interferes with the optimization of the model, leading to a sub-optimal result.\\n\\nIn order to better model the inter-pixel correlation for consistency regularization, we construct the agent-level correlation by comparing each pixel with a set of representative reference points (referred to as agents). Intuitively, the agents should resonate favorably with diverse semantic cues from original pixels with a wide range of semantic contrast descriptions. For this purpose, we design an orthogonal selection strategy to pick the most representative agents from the image. Specifically, we obtain the feature map \\\\(F \\\\in \\\\mathbb{R}^{C \\\\times h \\\\times w}\\\\) for an unlabeled image \\\\(x_u\\\\) extracted by the feature extractor of the segmentation model. Then, we incrementally build a set of agents \\\\(A = \\\\{f_a_i\\\\}_{i=1}^{N}\\\\) sampled from the \\\\(F\\\\) such that a new agent is maximally orthogonal (i.e., minimal cosine similarity) to the agents already selected, starting with a pixel feature at random, where \\\\(N\\\\) denotes the number of agents. This greedy strategy is dynamic, since it selects agents from the feature of the current image, preserving as much critical information as possible in the original pixels.\\n\\nIn this way, we can get the agent-level correlation \\\\(c \\\\in \\\\mathbb{R}^{1 \\\\times N}\\\\) (omit the subscript \\\\(i, j\\\\) for convenience), i.e., pixel-agent-level correlation for a given pixel feature \\\\(f\\\\) by\\n\\n\\\\[\\nc = \\\\text{softmax}(fA^T),\\n\\\\]\\n\\nwhere \\\\(A^T\\\\) refers to the matrix transpose operation. Straightforwardly, we can impose the consistency regularization between the agent-level correlation \\\\(c_w\\\\) of teacher network and the \\\\(c_s\\\\) of student network resorting to KL divergence as:\\n\\n\\\\[\\nL_{corr} = \\\\frac{1}{N_u} \\\\sum_{i=1}^{N_u} \\\\sum_{j=1}^{HW} \\\\ell_{kl}(c_{wi}, c_{si}).\\n\\\\]\\n\\nHowever, such a naive constraint treats each agent independently hindering the potential for further model optimization. In the next, we introduce rank-aware correlation consistency regularization to model the specific relationships between agents.\\n\\n3.3. Rank-aware Correlation Consistency\\n\\nTo harness the inter-agent relationship for more effective supervision signals, we carefully design the rank-aware consistency regularization. The core idea is that we take the agent ranking as a random event rather than a deterministic\"}"}
