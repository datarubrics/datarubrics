{"id": "CVPR-2022-1726", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"arg max tree, the tree-based collage generation problem formula-\\ncomponent as siamese network architecture [5] is employed to model this\\nis the parameter of tree generator.\\n\\n\\\\[\\nW \\\\alpha \\\\beta \\\\gamma \\\\delta \\\\epsilon \\\\zeta \\\\theta \\\\iota \\\\kappa \\\\lambda \\\\mu \\\\nu \\\\xi \\\\omicron \\\\rho \\\\sigma \\\\tau \\\\upsilon \\\\phi \\\\chi \\\\psi \\\\omega\\n\\\\]\\n\\n\\\\[\\nR \\\\alpha \\\\beta \\\\gamma \\\\delta \\\\epsilon \\\\zeta \\\\theta \\\\iota \\\\kappa \\\\lambda \\\\mu \\\\nu \\\\xi \\\\omicron \\\\rho \\\\sigma \\\\tau \\\\upsilon \\\\phi \\\\chi \\\\psi \\\\omega\\n\\\\]\\n\\nDifferent from Edge classifier. \\\\[\\nW \\\\alpha \\\\beta \\\\gamma \\\\delta \\\\epsilon \\\\zeta \\\\theta \\\\iota \\\\kappa \\\\lambda \\\\mu \\\\nu \\\\xi \\\\omicron \\\\rho \\\\sigma \\\\tau \\\\upsilon \\\\phi \\\\chi \\\\psi \\\\omega\\n\\\\]\\n\\nparameterized by \\\\[\\nR \\\\alpha \\\\beta \\\\gamma \\\\delta \\\\epsilon \\\\zeta \\\\theta \\\\iota \\\\kappa \\\\lambda \\\\mu \\\\nu \\\\xi \\\\omicron \\\\rho \\\\sigma \\\\tau \\\\upsilon \\\\phi \\\\chi \\\\psi \\\\omega\\n\\\\]\\n\\n\\\\[\\n\\\\text{Node classifier. A fully connected layer is utilized to model}\\n\\\\]\\n\\n\\\\[\\n\\\\text{is able to memorize a variety of subtree structure schemes,}\\n\\\\]\\n\\n\\\\[\\n\\\\text{fitting from the two-stage transformation, the fusion module}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Initialize}\\n\\\\]\\n\\n\\\\[\\n1 \\\\quad 2 \\\\quad t \\\\quad 3 \\\\quad 4 \\\\quad \\\\text{Construct probabilistic collage tree}\\n\\\\]\\n\\n\\\\[\\n5 \\\\quad t \\\\quad 6 \\\\quad \\\\text{Compute}\\n\\\\]\\n\\n\\\\[\\n7 \\\\quad \\\\theta \\\\quad 8 \\\\quad t \\\\quad 9 \\\\quad \\\\text{repeat}\\n\\\\]\\n\\n\\\\[\\n10 \\\\quad \\\\text{Optimization procedure of our model}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Algorithm 2:}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Input: } w, h, \\\\theta, \\\\text{Output: } F\\n\\\\]\\n\\n\\\\[\\n\\\\text{Sample } i, j \\\\text{ according to Algo. 1 ;}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Attention } W \\\\alpha \\\\beta \\\\gamma \\\\delta \\\\epsilon \\\\zeta \\\\theta \\\\iota \\\\kappa \\\\lambda \\\\mu \\\\nu \\\\xi \\\\omicron \\\\rho \\\\sigma \\\\tau \\\\upsilon \\\\phi \\\\chi \\\\psi \\\\omega\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax } = \\\\text{softmax }\\n\\\\]\\n\\n\\\\[\\n\\\\text{softmax } = \\\\text{softmax } = \\\\text{"}
{"id": "CVPR-2022-1726", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Experiments\\n\\nBaselines. We select three representative tree-based methods as baselines, where one is the state-of-the-art method [23], which is also the mostly related work with ours, and the other two are widely-used commercial softwares [2, 6].\\n\\nMetrics. We introduce five quantitative metrics to analyze collage results, which are commonly used in state-of-the-art works. Among them, three metrics, i.e., compactness $M_c$, ratio preservation $M_r$ and nonoverlapping constraint $M_o$, are defined identically to [23]. The other two metrics are described as follows.\\n\\n- Correlation preservation $M_n$. Gathering correlated images can facilitate informativeness [18, 23, 38, 41]. Despite Pan et al. [23] considered this end, their metric is actually both an athlete and referee due to the lack of groundtruth label. To tackle this problem, we collected an annotated dataset in Sec. 4.1. Thus, we define $M_n = \\\\frac{1}{N} \\\\sum_{i} \\\\| P_i - P_{c_i} \\\\|_2$, where $N$ is collection size, $P_i$ is the position vector of image $I$, and $P_{c_i}$ is the centroid position vector of category label $c_i$ of image $I$. All position coordinates are normalized by $w$ and $h$.\\n\\n- Saliency loss $M_s$. This metric measures saliency preservation ability. The collage mask is obtained by replacing each image in collage with the corresponding saliency mask. We define $M_s = 1 - \\\\frac{\\\\sum_{i} |S_i|}{\\\\sum_{i} |S_i|}$, where $S_i$ is the saliency mask of image $I$, $\\\\bigcup_{i} S_i$ is the collage mask and $|\\\\cdot|$ operator calculates the saliency area of mask.\\n\\n4.1. Annotated Image Collection Dataset\\n\\nCollage result for unlabeled image collection cannot support the calculation of $M_n$ and $M_s$. To encourage research works in this field to compete fairly, we collect an annotated image collection dataset, namely AIC, based on saliency detection dataset DUTS [34] which is partially collected from ImageNet [7] and has high generalization ability [35].\\n\\nFirstly we select 3402 images from DUTS to build the image collection sampling source, namely ICSS, which covers 72 categories and under each category there is at least 10 images. Subsequently we divide 72 categories into 11 themes manually (Tab. 1). The aspect ratio of images in ICSS ranges from 0.4625 to 1.9048. Each image in ICSS is labeled with category, theme and saliency mask, thus image collection sampled from ICSS is able to support the calculation of $M_n$ and $M_s$. With the idea of five-fold cross-validation, we divide the images at a ratio of 4:1 in each category into a train set and a test set, and both sets have a near-identical distribution.\\n\\nFinally, we develop an image collection sampling framework to generate AIC from ICSS. This framework requires that each image in one collection is sampled from one identical theme of ICSS. Moreover, each collection should include images from at least two categories and each category in collection should have at least two images in order to acquire effective $M_n$ value. Additionally, category distribution of each collection conforms to uniform distribution and is not biased by prior category distribution in ICSS. This framework samples train set and test set of AIC respectively from train set and test set of ICSS. As a result, AIC includes image collections with sizes of 10, 15, 20, 25, 30, 50 and 100. The train set has 562 image collections.\\n\\nTable 1. The percentage of image number under each theme of ICSS.\\n\\n| Theme   | Percentage (%) |\\n|---------|---------------|\\n| Animals | 13.22         |\\n| Food    | 3.85          |\\n| Fruits  | 11.73         |\\n| Transportation | 12.76 |\\n| Sports  | 4.94          |\\n| Office  | 6.44          |\\n| Baby Clothes | 4.29   |\\n| Houseware | 18.34     |\\n| Instrument | 9.38      |\\n| Makeup  | 1.79          |\\n| Others  | 3.26          |\\n\\nTable 2. Quantitative metric results on the train set of AIC.\\n\\n| Method | $M_r$ | $M_n$ | $M_s$ |\\n|--------|-------|-------|-------|\\n| SHP [6] | 1.522 | 0.376 | 0.239 |\\n| CLT [2] | 1.517 | 0.377 | 0.232 |\\n| VSM [23] | 1.095 | 0.335 |       |\\n| Ours ResNet-50 [14] | 1.086 | 0.284 | 0     |\\n\\nTable 3. Ablation analysis of our method on the train set of AIC. The first method removes the backbone network and sets $d_{ar}=1024$, $d_w=d_h=32$, $d_{inf}=1024$. The second method removes the information embedding in the extractor. The third method replaces the fusion module with feature average operation. The fourth method removes the scaled dot-product attention (Eq. (9)). More results are shown in the supplementary materials.\\n\\n| Method | Pre-trained | Identical theme | Size | $M_r$ | $M_n$ | $M_s$ |\\n|--------|-------------|-----------------|------|-------|-------|-------|\\n| Ours   | %            | %               | %    |       |       |       |\\n| Ours   | <            | <               | %    |       |       |       |\\n| Ours   | >            | %               | %    |       |       |       |\\n| Ours   | =            | %               | %    |       |       |       |\\n\\nTable 4. Generalization study of our method on the test set of AIC. The model is directly trained on the test set when not pre-trained. \u2018>', '<' and '=' represent cases where model is pre-trained on a collection of larger, smaller and identical size respectively.\"}"}
{"id": "CVPR-2022-1726", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. 5-scale human evaluation along with side-by-side human evaluation of collage results on the AIC. The score in 5-scale evaluation is the weighted average. \u2206 in side-by-side evaluation denotes the gap between the win rate and the lose rate.\\n\\nTable 6. The results of information conveying test. We investigate four indicators, i.e., recall, precision, accuracy and F1-score to evaluate the information conveying ability of collages.\"}"}
{"id": "CVPR-2022-1726", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. Comparison of the collage results generated by different methods on the AIC. We can see that SHP and CLT both introduce content occlusion (red dotted rectangle) into the images in collage. Despite VSM circumvents this defect, the results still contain images suffering high aspect ratio distortion (red dotted rectangle), particularly when the image collection size is large. However, our method takes advantage of the probability space to produce results closer to the global optimal. Notably employing loss function without $F_p$ of Eq. (19) to train our model leads to drastic imbalance in image area assignment in collages.\\n\\nResults, illustrated in Tab. 5, suggest that our method is substantially superior to all baselines in producing high-quality collage from human's perspective. The high Kappa scores imply that a major agreement prevails among the evaluators.\\n\\nWe further validate the effectiveness of our NNP via the information conveying test according to [22, 23]. Twenty subjects participated in the test and they were equally divided into four groups. Each group corresponds to one collage method. For each image collection, we showed participants the corresponding collage for 20 s and then asked them to perform a binary classification test, namely selecting the images that they had seen in the collage, on an image set including five groundtruth images and five negative samples (sharing the identical theme with the groundtruths). Tab. 6 shows the test results. Our collage benefits from the NNP and thus outperforms the other baselines. We find that the images selected by participants account for approximately 72%, which implies that participants are inclined to choose more images as remembered, leading to a higher recall than precision.\\n\\n5. Conclusion\\n\\nIn this paper, we present SoftCollage, a novel tree-based collage method. Our key idea is to soften the discrete tree structure into the probability space. By modeling the conditional probability distribution of collage tree via the proposed tree generator, we can formulate the collage generation as a differentiable process and optimize the layout with the gradient of criterion loss instead of the hand-crafted adjustment scheme. We demonstrate the effectiveness of our method via extensive experiments on the proposed large-scale dataset AIC. Currently, the GPU memory consumption of our model is high when the size of image collection is large. Because of the extensibility of our method in model architecture design, in the future we will explore the lightweight design and knowledge distillation of our model.\"}"}
{"id": "CVPR-2022-1726", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Similarity preserving snippet-based visualization of web search results. *IEEE transactions on visualization and computer graphics*, 20(3):457\u2013470, 2014.\\n\\n[2] Collageit. online, 2019. https://www.collageitfree.com/.\\n\\n[3] C Brian Atkins. Blocked recursive image composition. In *Proceedings of the 16th ACM international conference on Multimedia*, pages 821\u2013824, 2008.\\n\\n[4] Simone Bianco and Gianluigi Ciocca. User preferences modeling and learning for pleasing photo collage generation. *ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)*, 12(1):1\u201323, 2015.\\n\\n[5] Jane Bromley, James W Bentz, L\u00e9on Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard Sackinger, and Roopak Shah. Signature verification using a \u201csiamese\u201d time delay neural network. *International Journal of Pattern Recognition and Artificial Intelligence*, 7(04):669\u2013688, 1993.\\n\\n[6] V. Cheung. Shape collage. online, 2013. http://www.shapecollage.com/.\\n\\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In *2009 IEEE conference on computer vision and pattern recognition*, pages 248\u2013255. IEEE, 2009.\\n\\n[8] Jian Fan. Photo layout with a fast evaluation method and genetic algorithm. In *2012 IEEE International Conference on Multimedia and Expo Workshops*, pages 308\u2013313. IEEE, 2012.\\n\\n[9] Yuan Gan, Yan Zhang, Zhengxing Sun, and Hao Zhang. Qualitative photo collage by quartet analysis and active learning. *Computers & Graphics*, 88:35\u201344, 2020.\\n\\n[10] J. Geigel, A. Loui, and E. Loui. Automatic page layout using genetic algorithms for electronic albuming. *Proceedings of SPIE - The International Society for Optical Engineering*, pages 79\u201390, 2001.\\n\\n[11] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. *Journal of Machine Learning Research*, 15:315\u2013323, 2011.\\n\\n[12] Stas Goferman, Ayellet Tal, and Lihi Zelnik-Manor. Puzzle-like collage. In *Computer graphics forum*, volume 29, pages 459\u2013468. Wiley Online Library, 2010.\\n\\n[13] E. Gomez-Nieto, W. Casaca, D. Motta, I. Hartmann, G. Taubin, and L. G. Nonato. Dealing with multiple requirements in geometric arrangements. *IEEE Transactions on Visualization & Computer Graphics*, 22(3):1223\u20131235, 2016.\\n\\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 770\u2013778, 2016.\\n\\n[15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, *3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings*, 2015.\\n\\n[16] Yuan Liang, Xiting Wang, Song-Hai Zhang, Shi-Min Hu, and Shixia Liu. Photorecomposer: Interactive photo recomposition by cropping. *IEEE transactions on visualization and computer graphics*, 24(10):2728\u20132742, 2017.\\n\\n[17] Zhouhan Lin, Minwei Feng, C\u00edcero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. In *5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*. OpenReview.net, 2017.\\n\\n[18] Lingjie Liu, Hongjie Zhang, Guangmei Jing, Yanwen Guo, Zhonggui Chen, and Wenping Wang. Correlation-preserving photo collage. *IEEE transactions on visualization and computer graphics*, 24(6):1956\u20131968, 2017.\\n\\n[19] Tie Liu, Jingdong Wang, Jian Sun, Nanning Zheng, Xiaoou Tang, and Heung-Yeung Shum. Picture collage. *IEEE Transactions on Multimedia*, 11(7):1225\u20131239, 2009.\\n\\n[20] G. P. Nguyen and M. Worring. Interactive access to large image collections using similarity-based visualization. *Journal of Visual Languages & Computing*, 19(2):203\u2013224, 2008.\\n\\n[21] E. G. Nieto, W. Casaca, L. G. Nonato, and G. Taubin. Mixed integer optimization for layout arrangement. In Graphics, Patterns & Images, 2013.\\n\\n[22] Aude Oliva and Antonio Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. *International journal of computer vision*, 42(3):145\u2013175, 2001.\\n\\n[23] Xingjia Pan, Fan Tang, Weiming Dong, Chongyang Ma, Yiping Meng, Feiyue Huang, Tong-Yee Lee, and Changsheng Xu. Content-based visual summarization for image collections. *IEEE transactions on visualization and computer graphics*, 2019.\\n\\n[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. *Advances in neural information processing systems*, 32:8026\u20138037, 2019.\\n\\n[25] Carsten Rother, Lucas Bordeaux, Youssef Hamadi, and Andrew Blake. Autocollage. *ACM transactions on graphics (TOG)*, 25(3):847\u2013852, 2006.\\n\\n[26] Carsten Rother, Sanjiv Kumar, Vladimir Kolmogorov, and Andrew Blake. Digital tapestry [automatic image synthesis]. In *2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)*, volume 1, pages 589\u2013596. IEEE, 2005.\\n\\n[27] M. Shuang and W. C. Chang. Automatic creation of magazine-page-like social media visual summary for mobile browsing. In *2016 IEEE International Conference on Image Processing (ICIP)*, 2016.\\n\\n[28] Yu Song, Fan Tang, Weiming Dong, Feiyue Huang, Tong-Yee Lee, and Changsheng Xu. Balance-aware grid collage for small image collections. *IEEE Transactions on Visualization and Computer Graphics*, 2021.\\n\\n[29] Hendrik Strobelt, Marc Spicker, Andreas Stoffel, Daniel Keim, and Oliver Deussen. Rolled-out wordles: A heuristic method for overlap removal of 2d data representatives. In 3737\"}"}
{"id": "CVPR-2022-1726", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057\u20131063, 2000.\\n\\nLi Tan, Yangqiu Song, Shixia Liu, and Lexing Xie. Image-hive: Interactive content-aware image summarization. IEEE computer graphics and applications, 32(1):46\u201355, 2011.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017.\\n\\nJingdong Wang, Long Quan, Jian Sun, Xiaoou Tang, and Heung-Yeung Shum. Picture collage. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), volume 1, pages 347\u2013354. IEEE, 2006.\\n\\nLijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, and Xiang Ruan. Learning to detect salient objects with image-level supervision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 136\u2013145, 2017.\\n\\nWenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen, Haibin Ling, and Ruigang Yang. Salient object detection in the deep learning era: An in-depth survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\n\\nYichen Wei, Yasuyuki Matsushita, and Yingzhen Yang. Efficient optimization of photo collage. Microsoft Research, Redmond, WA, USA, MSRTR-2009-59, 2009.\\n\\nZhipeng Wu and Kiyoharu Aizawa. Picwall: Photo collage on-the-fly. In 2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, pages 1\u201310. IEEE, 2013.\\n\\nZhipeng Wu and Kiyoharu Aizawa. Very fast generation of content-preserved photo collage under canvas size constraint. Multimedia Tools and Applications, 75(4):1813\u20131841, 2016.\\n\\nXintong Han, Chongyang Zhang, Weiyao Lin, Mingliang Xu, and Sheng. Tree-based visualization and optimization for image collection. IEEE Transactions on Cybernetics, 46(6):1286\u2013300, 2016.\\n\\nYingzhen Yang, Yichen Wei, Chunxiao Liu, Qunsheng Peng, and Yasuyuki Matsushita. An improved belief propagation method for dynamic collage. The Visual Computer, 25(5):431\u2013439, 2009.\\n\\nZongqiao Yu, Lin Lu, Yanwen Guo, Rongfei Fan, Mingming Liu, and Wenping Wang. Content-aware photo collage using circle packing. IEEE Transactions on Visualization and Computer Graphics, 20(2):182\u2013195, 2013.\"}"}
{"id": "CVPR-2022-1726", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nImage collage task aims to create an informative and visual-aesthetic visual summarization for an image collection. While several recent works exploit tree-based algorithms to preserve image content better, all of them resort to hand-crafted adjustment rules to optimize the collage tree structure, leading to the failure of fully exploring the structure space of collage tree. Our key idea is to soften the discrete tree structure space into a continuous probability space. We propose SoftCollage, a novel method that employs a neural-based differentiable probabilistic tree generator to produce the probability distribution of correlation-preserving collage tree conditioned on deep image feature, aspect ratio and canvas size. The differentiable characteristic allows us to formulate the tree-based collage generation as a differentiable process and directly exploit gradient to optimize the collage layout in the level of probability space in an end-to-end manner. To facilitate image collage research, we propose AIC, a large-scale public-available annotated dataset for image collage evaluation. Extensive experiments on the introduced dataset demonstrate the superior performance of the proposed method. Data and codes are available at https://github.com/ChineseYjh/SoftCollage.\\n\\n1. Introduction\\n\\nImage collage aims to create a visual summarization with rich information and high aesthetic quality for a group of images. Because this task requires professional collage knowledge, amateurs have a huge demand for automatic image collage tools [16]. Therefore, many research efforts have tried to automate the process of image collage. While many works [4, 12, 18, 19, 25, 26, 33, 41] have achieved a certain level of success in improving visual perception of collage results, they brought about image artifacts [18, 25, 26, 41] and image overlapping [19, 33, 36, 40]. To tackle these defects, some tree-based algorithms [3, 8, 16, 23, 37, 38] were developed to preserve image content better. A tree-based collage is encoded as a binary tree which leads to a recursive partition of the canvas as illustrated in Fig. 2. In the tree, each leaf node corresponds to an image and each interior node corresponds to a bounding box, whose designation as a horizontal (\u201cH\u201d) or vertical (\u201cV\u201d) cut corresponds to dividing the box into two child boxes [3]. The existing tree-based methods design a two-stage procedure, where images are arranged in a standard collage tree in the first stage and the tree is mapped to the collage via a specific bijection mapping function in the second stage. Accordingly, the collage layout optimization is cast as an optimal tree structure search problem. However, all the existing works only resort to heuristic hand-crafted adjustment rules when searching the optimal tree structure, leading to the failure of fully exploring the structure space of collage tree (Fig. 3). Deep learning provides a promising way to learn a high-quality collage tree. Unfortunately, the two-stage tree-based collage generation process is undifferentiable because both stages include discrete decisions.\"}"}
{"id": "CVPR-2022-1726", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. An example of the mapping from a standard collage tree to the tree-based collage.\\n\\ncrete operations that prevent back propagation. Although recent tree-based advances [16, 23] utilized learning strategies, they only applied them to yield semantic feature in the first stage so that images with similar features clustered together. These works achieve much improvement because placing correlated images together can facilitate collage informativeness [18, 38, 41]. However, these methods still employed hand-crafted scheme to refine tree structure and failed to fully explore the solution space (Fig. 3). Recently, despite Pan et al. [23] introduced back propagation for the first time to fine-tune aspect ratio and splitting ratio, they still failed to propagate the gradients back to optimize the collage tree structure due to the undifferentiable characteristic of the tree-based process.\\n\\nIn this paper, we attack the key problem of differentiating the overall two-stage tree-based collage generation process (Fig. 1). Specifically, firstly we propose a novel neural-based differentiable probabilistic tree generator to model the first stage of tree-based procedure. Our tree generator exploits deep image feature and embedded information including aspect ratio and canvas size to construct a correlation-preserving probabilistic collage tree (PCtree), which builds a probability space via modeling the node type distribution (the cut type of the node is horizontal (\u201cH\u201d) or vertical (\u201cV\u201d)) and the edge connection distribution (the child node is on the left (\u201cL\u201d) or right (\u201cR\u201d)) (Fig. 5). Secondly, we formulate the tree generator optimization as an end-to-end framework resorting to the policy gradient technique [30], which naturally overcomes the differentiation difficulty in the second stage of tree-based procedure. Instead of the hand-crafted adjustment scheme in instance level, our optimization paradigm directly utilizes the gradient of collage criteria loss to optimize the collage tree structure in the level of probability space, which facilitates the exploration of the optimal collage structure.\\n\\nFurthermore, this field lacks a benchmark dataset with sufficient labels for quantitative evaluation. To facilitate image collage research, we propose AIC, a large-scale public-available annotated dataset for image collage evaluation. The major contributions can be summarized as follows.\\n\\n\u2022 We propose a novel neural-based probabilistic tree generator which constructs \u201csoft\u201d probabilistic tree structure to build a probability space of correlation-preserving collage tree conditioned on the deep image feature, aspect ratio and canvas size.\\n\\n\u2022 We formulate the tree-based collage generation procedure as a differentiable process for the first time, and introduce an end-to-end learning strategy to perform gradient-based structure optimization.\\n\\n\u2022 We provide a large-scale public-available benchmark dataset for evaluation of image collage method.\\n\\n\u2022 We conduct extensive experiments and user study, and show that our model outperforms the state-of-the-art methods.\\n\\n2. Related Work\\n\\nPrevious works on image collage mainly fall into two categories, i.e., parametric method and partitioning-based method. Our tree-based method belongs to the latter. Parametric methods parameterize a collage with variables including position, scale, orientation and layer index of each image and design well-defined objective functions to solve the optimal variables directly [4, 9, 12, 19, 25\u201327, 33, 36, 40]. These works either modeled the problem via a probabilistic graphical framework [19, 25, 26, 33, 36, 40] or solved the collage parameters in a heuristic manner [4, 9, 12, 27]. To preserve correlation among images, some methods exploited a feature space to acquire the correlation and projected the images into a visualization space [1, 13, 20, 21, 29, 39]. However, these methods introduce image overlapping and artifact problem.\\n\\nPartitioning-based methods partition the canvas and assign each image with a corresponding region to compose a collage [3, 8, 10, 16, 18, 23, 28, 31, 37, 38, 41]. Some works utilized V oronoi tessellation [31] and packing algorithm [18, 41] to allocate canvas space for the irregular salient region of each image, which brought about image artifacts when blending image boundaries. Hence, tree-based collage is developed to preserve image content better [3, 8, 16, 23, 28, 37, 38]. Atkins [3] first introduced tree-based collage and solved tree structure in a beam-search\"}"}
{"id": "CVPR-2022-1726", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4. The pipeline of our tree generator. Here the image collection size is four, and our feature extractor initially extracts feature of each image. Subsequently the NNP and fusion module iteratively select child nodes to yield parent feature node in a bottom-up manner until the root feature node of the probability collage tree is acquired. Finally, the edge classifier and node classifier generate $p_e$ and $p_n$ respectively. $\\sigma$ is the softmax activation.\\n\\nFan [8] employed genetic algorithm to improve [3] via designing genetic operators of collage tree. Wu and Aizawa [38] initialized tree structure in a greedy manner and adjusted the layout iteratively according to the hand-crafted distortion threshold. These tree-based methods all designed heuristic hand-crafted rules to adjust tree structure, thus failed to fully explore the solution space. Recently, Pan et al. [23] utilized back propagation to refine the aspect ratio and splitting ratio of region box in [38]. However, the gradient in [23] still fails to flow back to optimize the tree structure due to the undifferentiable characteristic of the tree-based collage generation process. Different from the prior work, we attack the key problem of differentiating the process via softening the discrete structure of collage tree, and hence our gradient can directly update all the structural details of collage tree.\\n\\n3. Approach\\n\\nProblem formulation. According to the literature, a high-quality collage should satisfy the following criteria: 1) Compact. The collage should fully utilize canvas space by blank space minimization. 2) Ratio-preserving. Image in the collage should suffer low aspect ratio distortion to retain the aesthetics. 3) Content-preserving. Image content, especially the salient region, should prevent occlusion. And image overlapping decreases the representativeness and aesthetics of the collage [23]. 4) Correlation-preserving. Recent works show that placing correlated images together facilitates informativeness of the collage [18, 23, 38, 41].\\n\\nFigure 5. Our probabilistic collage tree softens the standard collage tree structure via modeling the node type distribution as $p_n$ and edge connection distribution as $p_e$. Therefore, given an image collection $\\\\{I_i\\\\}$, canvas width $w$ and height $h$, we aim to design a tree generator $G$. This generator constructs a collage tree $\\\\tau$ in the first stage and the tree is mapped to the final collage $C$ via a mapping function $g$. Supposing we integrate the above four criteria into one criterion function $F$, our goal is to solve the optimal tree generator $G^* = \\\\arg\\\\max_{G} F(g(G(w,h,\\\\{I_i\\\\}))))$.\\n\\nOverview. To solve the above two-stage problem in an end-to-end manner, firstly we propose a \u201csoft\u201d probabilistic collage tree (PCtree) and design a differentiable tree generator to construct the PCtree. Secondly, we approximate the gradient of criterion loss to optimize our generator via back propagation. These two steps tackle the differentiation problem of the two stages respectively. In the following parts, we firstly present the PCtree, our tree generator and the tree generation algorithm in Sec. 3.1. Afterwards we introduce the model architecture of our neural generator in Sec. 3.2. Finally we present our gradient-based optimization.\"}"}
{"id": "CVPR-2022-1726", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1. Probabilistic Collage Tree Generation\\n\\nThe probabilistic collage tree (PCtree) softens the parameters of the collage layout using discrete structural parameters including the node type distribution (the cut type of the node is designated as horizontal (\u201cH\u201d) or vertical (\u201cV\u201d)) as well as the edge connection distribution (the first child node is designated as the left (\u201cL\u201d) or right (\u201cR\u201d) in the child list is designated as the left (\u201cL\u201d) or right (\u201cR\u201d) respectively, and the edge connection distribution is designated as horizontal (\u201cH\u201d) or vertical (\u201cV\u201d)).\\n\\nTo generate the PCtree, we design the probabilistic collage tree (PCtree) softens the parameters of the collage layout using discrete structural parameters including the node type distribution (the cut type of the node is designated as horizontal (\u201cH\u201d) or vertical (\u201cV\u201d)) as well as the edge connection distribution (the first child node is designated as the left (\u201cL\u201d) or right (\u201cR\u201d) respectively, and the edge connection distribution is designated as horizontal (\u201cH\u201d) or vertical (\u201cV\u201d)).\\n\\nThus, given an interior node \\\\( n \\\\) and a child node \\\\( \\\\tilde{n} \\\\), we denote the subtree of PCtree as \\\\( \\\\tau \\\\) and the likelihood of a standard collage tree is in one-to-one correspondence.\\n\\nThis module should obtain the parent feature \\\\( p \\\\) and the likelihood of a standard collage tree are in one-to-one correspondence. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a list of features, our NNP finds the pair of features with the closest Euclidean distance. The tree construction process is conducted in a greedy manner. Given a"}
