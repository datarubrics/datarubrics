{"id": "CVPR-2024-1690", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Nader Akoury, Kalpesh Krishna, and Mohit Iyyer. Syntactically supervised transformers for faster neural machine translation. ACL, 2019.\\n\\n[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433, 2015.\\n\\n[3] Yu Bao, Hao Zhou, Shujian Huang, Dongqi Wang, Lihua Qian, Xinyu Dai, Jiajun Chen, and Lei Li. latent-glat: Glancing at latent variables for parallel text generation. CoRR, 2022.\\n\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, 2020.\\n\\n[5] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling framework for object detection. International Conference on Learning Representations, 2022.\\n\\n[6] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey Hinton. A unified sequence interface for vision tasks. arXiv preprint arXiv:2206.07669, 2022.\\n\\n[7] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, 2020.\\n\\n[8] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In International Conference on Machine Learning, pages 1931\u20131942. PMLR, 2021.\\n\\n[9] Qi Dong, Zhuowen Tu, Haofu Liao, Yuting Zhang, Vijay Mahadevan, and Stefano Soatto. Visual relationship detection using part-and-sum transformers with composite queries. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 3550\u20133559, 2021.\\n\\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\n[11] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of training end-to-end vision-and-language transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\\n\\n[12] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 2020.\\n\\n[13] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. Advances in Neural Information Processing Systems, 2020.\\n\\n[14] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. EMNLP-IJCNLP, 2019.\\n\\n[15] Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning, pages 369\u2013376, 2006.\\n\\n[16] Jiatao Gu and Xiang Kong. Fully non-autoregressive neural machine translation: Tricks of the trade. ACL, 2021.\\n\\n[17] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. ICLR, 2018.\\n\\n[18] Jiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. Advances in Neural Information Processing Systems, 2019.\\n\\n[19] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv, 2015.\\n\\n[20] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\\n\\n[21] Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam Shazeer. Fast decoding in sequence models using discrete latent variables. In International Conference on Machine Learning. PMLR, 2018.\\n\\n[22] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr: Modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.\\n\\n[23] Justin Lazarow, Kwonjoon Lee, Kunyu Shi, and Zhuowen Tu. Learning instance occlusion for panoptic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10720\u201310729, 2020.\\n\\n[24] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\\n\\n[25] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 2021.\\n\\n[26] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. ACL/IJCNLP, 2021.\\n\\n[27] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, 2020.\\n\\n[28] Jinglin Liu, Yi Ren, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao, and Tie-Yan Liu. Task-level curriculum learning for...\"}"}
{"id": "CVPR-2024-1690", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"non-autoregressive neural machine translation. IJCAI, 2021.\\n\\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Motaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022.\\n\\nJunhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille. Deep captioning with multimodal recurrent neural networks (m-rnn). arXiv preprint arXiv:1412.6632, 2014.\\n\\nJunhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11\u201320, 2016.\\n\\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019.\\n\\nGiovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, Rishita Anubhai, Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. Structured prediction as translation between augmented natural languages. arXiv preprint arXiv:2101.05779, 2021.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 2020.\\n\\nQiu Ran, Yankai Lin, Peng Li, and Jie Zhou. Guiding non-autoregressive neural machine translation decoding with reordering information. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv, 2015.\\n\\nRaphael Shu, Jason Lee, Hideki Nakayama, and Kyunghyun Cho. Latent-variable non-autoregressive neural machine translation with deterministic inference using a delta posterior. In Proceedings of the aaai conference on artificial intelligence, 2020.\\n\\nMitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 2018.\\n\\nMitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. Insertion transformer: Flexible sequence generation via insertion operations. In International Conference on Machine Learning. PMLR, 2019.\\n\\nZhiqing Sun, Zhuohan Li, Haoqing Wang, Di He, Zi Lin, and Zhihong Deng. Fast structured decoding for sequence models. Advances in Neural Information Processing Systems, 2019.\\n\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017.\\n\\nChunqi Wang, Ji Zhang, and Haiqing Chen. Semi-autoregressive neural machine translation. EMNLP, 2018.\\n\\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pages 23318\u201323340. PMLR, 2022.\\n\\nWenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. arXiv, 2021.\\n\\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv, 2021.\\n\\nYisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu. A survey on non-autoregressive generation for neural machine translation and beyond. arXiv, 2022.\\n\\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang. Crossing the format boundary of text and boxes: Towards unified vision-language modeling. arXiv, 2021.\\n\\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In European Conference on Computer Vision, pages 69\u201385. Springer, 2016.\\n\\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv, 2021.\\n\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\\n\\nZhaoyang Zhang, Yantao Shen, Kunyu Shi, Zhaowei Cai, Jun Fang, Siqi Deng, Hao Yang, Davide Modolo, Zhuowen Tu, and Stefano Soatto. Musketeer (all for one, and one for all): A generalist vision-language model with task explanation prompts, 2023.\\n\\nChunting Zhou, Graham Neubig, and Jiatao Gu. Understanding knowledge distillation in non-autoregressive machine translation. ICLR, 2019.\"}"}
{"id": "CVPR-2024-1690", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Non-autoregressive Sequence-to-Sequence Vision-Language Models\\nKunyu Shi Qi Dong Luis Goncalves Zhuowen Tu Stefano Soatto\\nAWS AI Labs\\n{kunyus, qdon, luisgonc, ztu, soattos}@amazon.com\\n\\nAbstract\\nSequence-to-sequence vision-language models are showing promise, but their applicability is limited by their inference latency due to their autoregressive way of generating predictions. We propose a parallel decoding sequence-to-sequence vision-language model, trained with a Query-CTC loss, that marginalizes over multiple inference paths in the decoder. This allows us to model the joint distribution of tokens, rather than restricting to conditional distribution as in an autoregressive model. The resulting model, NARVL, achieves performance on-par with its state-of-the-art autoregressive counterpart, but is faster at inference time, reducing from the linear complexity associated with the sequential generation of tokens to a paradigm of constant time joint inference.\\n\\n1. Introduction\\nSequence-to-sequence autoregressive Transformers [12, 34, 42] are deep neural network architectures that map a sequence of tokens, each representing a segment of text as a vector, onto another sequence, typically representing the same sequence shifted forward by one. Such models can handle a variety of tasks [24, 33, 34], whereby the input (query) text could be a sentence in natural language, and the output (target) the same sentence in a different language (translation), or the answer to a question expressed in the input (question-answering, QA), the name of an entity or class, etc. The Transformer architecture\u2019s versatile and unified design has led to the development of all-in-one (AIO) models, such that multiple tasks can be approached as a sequence-to-sequence translation problem.\\n\\nVision-Language AIO Models [29, 44, 48], including sequence-to-sequence, have proven successful at mapping multimodal inputs, typically images and strings of text, to textual outputs that encode tasks expressible as a string of text, such as visual question answering (VQA), visual grounding (VG), visual entailment (VE), and image captioning (IC). These auto-regressive sequence-to-sequence models face the inference cost issue, since they tend to be unwieldy and need to be executed $T$ times to generate an output sequence of length $T$.\\n\\nNon-autoregressive methods are proposed in some recent Visual-language AIO models [22], which formulate sequence-to-sequence mapping as a bipartite matching problem. This approach excels in tasks where visual information is key, such as object grounding and detection. However, it\u2019s less effective of handling language-focused tasks like Visual Question Answering and Image Captioning. This discrepancy may stem from the nature of the tasks: in object detection/grounding, tokens are orderless and each token correlates to distinct objects or boxes, leading to a weaker inter-object correlation compared to the stronger inter-word correlation in sentences where tokens are ordered. Consequently, the set-to-set, order-independent translation method is more suitable for visual tasks than for language-oriented ones.\\n\\nMain hypothesis:\\nWe hypothesize that a transformer-based architecture could leverage the homogeneity of the input and output spaces, while enabling more flexible output spaces. In particular, we are interested in the possibility of performing joint decoding of a sequence in one step, rather than step-by-step. We test whether such an architecture could achieve performance comparable to the autoregressive baseline at significantly reduced inference cost.\"}"}
{"id": "CVPR-2024-1690", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To test this hypothesis, we develop a new Visual-language AIO model, turning a Transformer-based autoregressive one-step prediction model into a joint predictor of the target tokens, as explained in Sect. 3. In Sect. 4 we show that such a model, which we name NARVL, can be used for the multiple visual-language tasks of interest (VQA, captioning, entailment, grounding). As shown in Fig 1, NARVL achieves comparable performance to a state-of-the-art autoregressive model, with significant speed advantage ranging from 1.4 to 12.7 times.\\n\\nNARVL is made possible by re-purposing of the decoder of an autoregressive Transformer model, and the model has a layer of learnable query tokens (LQT) that are fixed at inference time and learned during fine-tuning. NARVL is enabled by Query-CTC (Q-CTC) loss, a variant of the CTC loss used in audio and language [15] but never applied to the visual domain, where the ordinary empirical cross-entropy loss (CE) is marginalized with respect to generative variability in the prediction. Whereas in the language domain the multiple decoding hypotheses stem from the output of the encoder, in vision this is limiting, since input and output spaces are heterogeneous. Therefore, we modify the CTC loss to marginalize not with respect to decoding paths, but with respect to paths from the sequential learnable query tokens of order indexes to the predicted tokens.\\n\\nOur key contributions can therefore be summarized as follows: (i) we propose a new sequence-to-sequence non-autoregressive all-in-one vision language model, that generates sequences in parallel. (ii) We introduce Query-CTC loss to train this architecture, inspired by the CTC loss used in audio recognition and language, that leverages the sequential learnable query tokens to generate multiple generative paths, and marginalizes the resulting population in the ordinary cross-entropy loss. We show that (iii) the resulting architecture is competitive with state-of-the-art autoregressive architecture in multiple vision-language tasks, at a significantly reduced inference time, since the model is executed once at inference time, rather than sequentially for as many steps as tokens in the output layer.\\n\\n2. Related Work\\n\\nSequence to Sequence Generation. Many NLP tasks share a common problem setting where the input consists of sequences of words with the output being targeted sequences. Therefore, the sequence to sequence formulation becomes a prototypical setting for many tasks in NLP [41], which can be readily solved by an autoregressive (AR) model. Beyond recurrent neural networks (RNNs), the decoder in Transformers [42] also adopts the autoregressive strategy in both training and prediction. On the vision side, considering an image as a sequence of tokens was popularized by the Vision Transformer (ViT) [10]. Transformers [42] based approaches pix2seq [5, 6] formulate object detection, instance segmentation, and human poses as sequence generation problem, which differs a lot from approaches designed specifically for individual tasks [4, 9, 23]. Furthermore, [44, 52] unify more tasks with seq2seq models. Recently, vision-language tasks have received increasing attention [30], including visual questioning and answering [2], and visual grounding [49] that have also been tackled by AR models for sequence to sequence generation.\\n\\nNon-autoregressive Sequence Generation. [17] proposes non-autoregressive (NAR) Transformer model for machine translation that generates the translated sequence in parallel. The main challenge in NAR is to capture the inter-token dependency [16] since the predictions are made conditionally independent. To improve inter-token dependency, in [18, 38\u201340, 43, 47], the model architectures are modified to conduct local NAR only, or light AR layers are added at the end of the decoder. In [1, 3, 21, 35, 37], hand-crafted or learnable latent variables are incorporated. Knowledge distillation [53] has proven effective to reduce complexity of training and increase robustness. In [14, 28], curriculum learning has been applied to simplify the learning task.\\n\\nSet-to-Set Prediction. In the seminal Detection Transformers (DETR) [4], a set of object queries are turned into a set of detected objects. A key component is the Hungarian matching step that deterministically assigns object queries to the ground-truth objects in training. Broadly speaking, we can also view the decoder in DETR as performing non-autoregressive set generation, which works well for order-less objects but may not be directly applicable to text sequence generation in vision-language models.\\n\\nNon-autogressive Vision-Language Models. Inspired by [17], we develop NARVL, a non-autogressive vision-language model and illustrate the effectiveness of NARVL on a recent autogressive model, OFA [44]. The Connectionist Temporal Classification (CTC) loss [15] and knowledge distillation [53] have been adopted in NARVL to implement a NAR solution. CTC is designed to align the two sequences with different lengths, and it marginalizes over all possible monotonic alignments. It assumes output always longer than target, and the final output is decoded by collapsing repetitive tokens. The variant of the CTC loss we introduce is formalized in Eq. 1, and NARVL described in Sect. 3.\\n\\n3. Method\\n\\nWe co-opt a pre-trained sequence-to-sequence autoregressive model (OFA [44]) and turn it from a one-step predictor ($p(y_{t+1}|x_1, \\\\ldots x_T, y_1, \\\\ldots y_t)$) to a flexible task-dependent joint encoding of the target query given heterogeneous inputs $p(y_1, \\\\ldots, y_N|x_1, \\\\ldots x_T; q)$, where $x_i$ are token embeddings of images and text and $y$ are output token embeddings. The overview of the proposed NARVL is shown in Figure 2.\\n\\nSpecifically, we embed an image with a convolutional\"}"}
{"id": "CVPR-2024-1690", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The overview of NARVL.\\n\\nNARVL borrows the encoder from OFA [44], where the embedding sequence of input text and image CNN (ResNet) feature are concatenated in the input token sequence. Unlike the standard transformer decoder that generates outputs sequentially, conditioning on the generated sequence, our non-autoregressive decoder takes a sequence of tokens that are learnable weights, and generates outputs for all tokens in parallel. As the output sequence length is unknown, we set the number of of learnable query tokens to a value (hyperparameter) larger than the largest target sequence length. The loss used, Q-CTC, is described in Eq. 1.\\n\\nThis constant layer of Learnable Query Tokens (LQT), is chosen during training as the output of a module with learnable parameters, including the number of tokens, as a hyperparameter, bounded from below by a function of the length of the ground truth output sequence (target tokens) in the training set.\\n\\nDiscussion\\n\\nThe design of NARVL decoder is different from the existing non-autoregressive Transformer decoder proposed in Natural Language processing [16\u201318] (see Figure 3(b)), which utilizes the outputs of the encoder as the inputs to the decoder. However, unlike translation, the sequence lengths of input and output sequences are not strongly correlated in vision-language tasks. The NARVL encoder-decoder design is also different from set-to-set decoders [4, 22], and our learnable queries for the decoder is a sequence rather than a set. For each learnable query, we add absolute position embeddings in the bias of attention layers, to force the structure of output sequences.\\n\\n3.2. NARVL with Query Connectionist Temporal Classification Loss\\n\\nNARVL is trained with a variant of the CTC loss [15] used in audio and language, which consists of a cross-entropy marginalized over a distribution of predictions. In our case, the distribution of prediction corresponds to multiple inference paths in the decoder from the redundant learnable query tokens, to the smaller sequence of output tokens.\\n\\n3.2.1 Q-CTC for Visual-Language Translation\\n\\nSuppose we have a training set of \\\\( n \\\\) image-text pairs \\\\( D = \\\\{ (I_i, T_i, Y_i) \\\\} \\\\) where \\\\( I, T, Y \\\\) specify the input images, input texts and target outputs, respectively. Note that the output sequences can be text sequences or location sequences. The model generates \\\\( Y \\\\) according to the inputs \\\\( I, T \\\\).\\n\\nIn order to formalize the expression of the Q-CTC loss, we call the Learnable Query Tokens (LQT) \\\\( q \\\\in \\\\mathbb{R}^{D \\\\times N} \\\\), the outputs of the encoder \\\\( x \\\\in \\\\mathbb{R}^{D \\\\times L} \\\\), the output tokens are \\\\( z \\\\in \\\\mathbb{R}^{D \\\\times N} \\\\), and the ground truth tokens \\\\( y \\\\in \\\\mathbb{R}^{D \\\\times T} \\\\), where \\\\( N > T \\\\). Let the decoder vocabulary denote \\\\( D_d \\\\) containing \\\\( d \\\\) possible target tokens including the whole valid vocabulary tokens and one blank token \\\\( - \\\\).\\n\\nLet \\\\( \\\\hat{z}_i(x; q) \\\\) denote the \u201cpath\u201d from the query set to the output token, meaning the value of \\\\( z \\\\) as a (deterministic) function of the encoding \\\\( x \\\\) for a given set of LQTs \\\\( q \\\\). We denote the ensemble of paths \\\\( p(z|x; q) = \\\\delta(z - \\\\hat{z}_i(x; q)) \\\\), where \\\\( \\\\delta \\\\) is Dirac function and \\\\( q \\\\) are learnable parameters. The ordinary cross-entropy loss would be\\n\\n\\\\[\\nL_{\\\\text{CE}}(\\\\theta) = -\\\\sum_i \\\\log \\\\left( \\\\sum_j e^{f_y(j)(z_i)} \\\\right)\\n\\\\]\\n\\nwhere \\\\( f_Y(j)(\\\\cdot) \\\\) computes the logits of \\\\( x_i \\\\) on target token class \\\\( y_i \\\\), and \\\\( \\\\theta \\\\) are the learnable parameters.\"}"}
{"id": "CVPR-2024-1690", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Existing non-autoregressive Transformer decoders for language tasks\\n\\nCross-attention\\nFFN\\nsampling and paste\\n\\nThe outputs of Transformer Encoder...\\n\\n(a) Standard Auto-regressive Transformer decoder\\n\\n(b) Existing non-autoregressive Transformer decoders for audio and language tasks\\n\\n(c) The proposed non-autoregressive Transformer decoder in NARVL\\n\\nFigure 3. Comparison of various design of Transformer decoder. (a) Standard Auto-regressive Transformer decoder; (b) The existing non-autoregressive Transformer decoders for audio and language tasks; (c) The proposed non-autoregressive Transformer decoder in NARVL.\\n\\nDuring training, an AR decoder (a) uses teacher forcing with causal masks, where tokens can only attend to previous tokens, while all tokens can attend to each other in the decoders of (b) and (c). NARVL decoder has dedicated query tokens inputs, instead of using the outputs of the encoder as inputs in (b). This design avoids the large latency of the decoder due to the long output sequence from the encoder.\\n\\nGiven the vocabulary size $d$ and output sequence length $N$, there are $N^d$ possible output sequences. Among all output sequences, the Q-CTC selects the valid output sequences, and maximizes the probability over all valid paths.\\n\\nCollapse operation, denoted as $B$, removes all blank tokens, and merge continuous repetitive tokens between two blank tokens. For example, $B(-a bag on a table) = B(a bag bag on a a table) = a bag on a table$. There are many possible sequences that can be collapsed to the correct target sequence, which are valid alignments. When we calculate the loss, we marginalize over the set of valid paths. We denote the collapsed valid sequences as $\\\\tilde{p}(z|x;k;q)$.\\n\\nBefore $x$ is instantiated at inference time, $z_i$ are random variables, functions of $x$, which we need to marginalize in the loss, obtaining\\n\\n$$L_{Q-CTC}(\\\\theta, q) = -\\\\sum_{k} \\\\sum_{z_i} \\\\tilde{p}(z_i|x_k;q) \\\\log ef_{y_i}(z_i(x_k;q)) P_{y_j}(z_i(x_k;q))$$\\n\\n(1)\\n\\nwhere $k$ is the number of all training samples. Note that this loss is different from the CTC loss, where the marginalization only depends on $x$, not $q$. Our loss is a function of $\\\\theta$ as well as the learnable parameters used to produce $q$, in addition to any other hyperparameters shared with the ordinary autoregressive model.\\n\\n3.2.2 Knowledge Distillation\\n\\nOptimising Q-CTC is more challenging in visual-language domain than language domain, because the image-text input sequence has less structure than pure language inputs, and the model is required to generate well-structured sequences from less order sequences. Due to the large solution space and the lack of inter-token dependency between decoder tokens, non-autoregressive models can have lower performance compared to auto-regressive models. To reduce the solution space in model training, we exploit two simple knowledge distillation mechanisms[19] in training. Specifically, we train a standard Transformer with an auto-regressive loss, and set this model as the teacher model, which has significantly less variations in ground-truth and makes learning targets more deterministic. We observe that this knowledge distillation benefits the task of long sequence outputs such as image captioning, and the detailed discussion is in Section 4.3. We also observe that the model initialization weights are non-trivial for training Q-CTC loss, and we use OFA pretrained weights to initialize NARVL and finetune it in various down-stream tasks.\\n\\n3.2.3 Model inference\\n\\nDuring the inference, for each output token, we use the text token in the vocabulary with the largest probability as prediction. We follow the same path collapse rules used in training to remove - and repetitive tokens to obtain the final predicted sequence.\\n\\n4. Experiments\\n\\nWe perform experiments on various vision-language tasks, and make performance and speed comparisons to the state-of-the-art models to show the effectiveness of the proposed method. Figure 4 summarizes the tasks we experiment on.\\n\\n4.1. Implementation details\\n\\nWe implement NARVL starting from the official OFA[44] code, which is written in the fairseq library [32]. To make a...\"}"}
{"id": "CVPR-2024-1690", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. We test the proposed NARVL on various vison-language tasks, including Visual Question Answering (VQA), Visual grounding (VG), Image Captioning (IC) and Visual Entailment (VE). The inputs and outputs of each tasks are illustrated here, and all types outputs are unified within the sequence formulation.\\n\\nWe compare accuracy and speed between AR (autoregressive) and our proposed NAR (non-autoregressive) models on various vision-language tasks and results are shown in Table 1. It shows that the NAR model consistently outperforms the AR model on visual grounding and has significantly higher execution speed (with 2.4 to 12.7 times speed up on various tasks). As visual entailment requires shorter output sequences, the speedup is smaller than for other tasks. The significant speedups are on VQA and Captioning datasets, because the length of output sequence is longer than that of the grounding task. We will analyse the accuracy and inference speed for each downstream tasks in next section.\\n\\n4.3. Benchmark Performance\\n\\nRefCOCO, RefCOCO+, RefCOCOg results. Following the metric used previous works, we report Acc@0.5 numbers. We compare NARVL to other methods in Table 2. Our proposed NARVL model achieves state-of-the-art performance.\"}"}
{"id": "CVPR-2024-1690", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Accuracy and speed comparisons of AR (autoregressive) and our proposed NAR (non-autoregressive) models on various vision-language tasks. Our NAR model is trained under the exact same settings of model size, parameters and training schedule as the AR model for fair comparison. The NAR model consistently outperforms the AR model on visual grounding and has significantly higher execution speed. As visual entailment requires shorter output sequences, the speedup is smaller than for other tasks. We observe significant speedups on VQA and Captioning datasets, but with a measurable performance drop. Beam search is used in all AR models and greedy decoding is used in NAR models (beam search can be applied and are studied in 10, but not adopted due to its sequential nature.). All models reported here are in base size. Inference wall clock time is measured in ms.\\n\\n(a) Visual Grounding\\n\\n| Method | Val | TestA | TestB | Time | Val | TestA | TestB | Time |\\n|--------|-----|-------|-------|------|-----|-------|-------|------|\\n| AR     | 88.15 | 90.08 | 83.45 | 133.6/1 | 81.67 | 86.40 | 74.49 | 133.1/1 |\\n| NAR    | 88.78 | 90.63 | 84.67 | 54.8/2.4 | 82.35 | 87.15 | 74.74 | 54.6/2.4 |\\n\\n(b) Visual Entailment\\n\\n| Method | Dev | Test | Time | BLEU@4 | METEOR | CIDEr | SPICE | Time |\\n|--------|-----|------|------|--------|--------|-------|-------|------|\\n| AR     | 89.0 | 89.0 | 68.0/1 | 77.48 | 77.58 | 645.1/1 | 41.0 | 30.9 | 138.2/1 |\\n| NAR    | 89.0 | 89.0 | 48.7/1.4 | 75.69 | 75.75 | 50.7/7.2 | 36.4 | 28.7 | 123.1 |\\n\\nPerformance on all subsets of RefCOCO, RefCOCO+ and RefCOCOg datasets. As shown in Table 1, our NAR model consistently outperforms the AR model on all subsets of the three datasets and has significantly faster speed. Take RefCOCO dataset as an example, our NAR model has on average 0.83 higher accuracy compared to the AR model, and is 2.4 times faster (54.8 ms vs 133.6 ms). The speedup is introduced by the parallel nature of NARVL decoder, and we argue the accuracy improvements come from the bi-directional attention of our parallel decoder, as opposed to uni-directional attention in autoregressive decoder, where the later generated coordinate tokens are not available in attention operation of the early token generation.\\n\\nTable 2. Results on visual grounding datasets: RefCOCO, RefCOCO+ and RefCOCOg, and comparisons to previous works. *Weights are not released and the model was reproduced by us using the official released training scripts. Difference to the reported results in [44] might due to randomness in checkpoint picking. We do not perform knowledge distillation for this task.\\n\\n| Method | Val | TestA | TestB | Speed (ms) | Val | Test | Speed (ms) |\\n|--------|-----|-------|-------|------------|-----|------|------------|\\n| UNITER[7] | 81.41 | 87.04 | 74.17 | - | UNITER[7] | 75.77 | - |\\n| VILLA[13] | 82.39 | 87.48 | 74.84 | - | VILLA[13] | 76.71 | - |\\n| MDETR[22] | 86.75 | 89.58 | 81.41 | 73.5 | MDETR[22] | 80.89 | 73.5 |\\n| UNICORN[48] | 88.29 | 90.42 | 83.06 | 266.4 | UNICORN[48] | 83.93 | 266.4 |\\n| OFA* [44] | 91.24 | 93.41 | 87.16 | 284.9 | OFA* [44] | 87.70 | 284.9 |\\n| NARVL tiny (ours) | 80.40 | 84.64 | 73.8 | 30.7 | NARVL tiny | 88.01 | 30.7 |\\n| NARVL base (ours) | 88.78 | 90.63 | 84.67 | 54.9 | NARVL base | 91.1 | 54.9 |\\n| NARVL huge (ours) | 91.8 | 94.24 | 88.01 | 150.8 | NARVL huge | 91.8 | 150.8 |\\n\\nTable 3. SNLI-VE visual entailment results. NARVL shows on par performance to previous state-of-the-art. Knowledge distillation is not used in this task.\\n\\n| Method | Dev | Test | Time |\\n|--------|-----|------|------|\\n| UNITER[7] | 73.8 | 74.0 | |\\n| VinVL[51] | 76.5 | 76.6 | |\\n| UNIMO[26] | 75.0 | 75.3 | |\\n| ALBEF[25] | 75.8 | 76.0 | |\\n| METER[11] | 77.7 | 77.6 | |\\n| VLMo[45] | 79.9 | 80.0 | |\\n| SimVLM[46] | 80.0 | 80.3 | |\\n| Florence[50] | 80.2 | 80.4 | |\\n| OFA [44] | 91.0 | 91.2 | |\\n| NARVL base (ours) | 89.0 | 89.0 | |\\n| NARVL huge (ours) | 91.1 | 91.1 | |\\n\\nSNLI-VE results\\n\\nWe compare our NAR model and AR model in Table 3. Our model has average inference time of 48.73 ms, which is 1.4 times faster than the AR model with 68.03 ms. The effective target sequence excluding EOS has only 1 token, which means decoder attention in essence is the same for NAR and AR model, and we see exactly the same performance. As the output sequence (label-EOS) for this dataset is shorter than other dataest, the relative speedup from NAR model is smaller. Our NARVL shows on par performance as the current state-of-art OFA model on both validation and test set, as shown in Table 3 along with comparisons to other previous methods.\\n\\nVQA results\\n\\nComparisons to the AR model on VQA are shown in 1. NAR model performance is on average 1.81 lower than the AR model, while it has gigantic speed up of 12.7 times (50.7 ms vs 645.1 ms). Our model shows competitive results and more comparisons to previous methods can be found in Table 4. The default OFA model uses a slow\"}"}
{"id": "CVPR-2024-1690", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"all-candidate decoding method, and we additionally report the results with beam search decoding. It is noteworthy that OFA-VQA was trained with trie-based auto-regressive decoding that predicts next node on a Trie in each step, and the speedup would be smaller over standard AR models. In this task, one question might have multiple valid answers e.g. \u201cWhat's the color of the shirt?\u201d can be answered with \u201cRed and White\u201d or \u201cWhite and Red\u201d. This might create confusions for NARVL, which leads to incorrect predictions like \u201cWhite and White\u201d, and here we use knowledge distillation from an AR model of the same size to make the answers used in training more deterministic.\\n\\nTable 4. Results on VQA. We report base version of NARVL with knowledge distillation. *beam search version is released in the official OFA github that is much faster than the original allcnd version reported in the OFA paper, and we present results from both models here.\\n\\n| Method         | test-dev | test-std | Speed (ms) |\\n|----------------|----------|----------|------------|\\n| UNITER[7]      | 73.8     | 74.0     | -          |\\n| UNIMO[26]      | 75.0     | 75.3     | -          |\\n| ALBEF[25]      | 75.8     | 76.0     | -          |\\n| METER[11]      | 77.7     | 77.6     | -          |\\n| VLMo[45]       | 79.9     | 80.0     | -          |\\n| SimVLM[46]     | 80.0     | 80.3     | -          |\\n| Florence[50]   | 80.2     | 80.4     | -          |\\n| OFA huge Allcan[44] | 82.0 | 82.0     | -          |\\n| OFA base Beam Search* | 77.48 | 77.58    | 645.1      |\\n| OFA base Allcan | 78.0     | 78.1     | 15415.3    |\\n| NARVL-KD base (ours) | 75.59 | 75.75    | 50.76      |\\n| NARVL-KD huge (ours) | 79.59 | 79.39    | 81.53      |\\n\\nMSCOCO Image Captioning results. Comparisons in Table 9 demonstrate the superiority of the Q-CTC loss function in the context of NARVL, where we observe a huge improvement on all metrics with Q-CTC loss. On top of the Q-CTC version of model, we utilize knowledge distillation from AR model and obtained another set of massive performance boost, as shown in Tab 9. Comparison results of our NAR model to the AR model are shown in Tab 1, and we see huge speed advantage of the NAR model and performance advantage of the AR model. More comparisons to previous methods are shown in Table 5, where all models shown are trained without CIDEr reinforcement learning optimization, and NARVL shows competitive results.\\n\\n5. Ablation experiments\\n\\nSequential learnable queries in decoder\\nWe proposed the learnable query token as the decoder input, which is shown in Figure 3. This design differs from the existing non-autoregressive Transformer for sequence generation, which uses the outputs of the encoder as the inputs for the decoder [16]. We compare these two designs on RefCOCO dataset and MSCOCO Image Captioning dataset, and the results are shown in Table 6 and Table 7, respectively. We observe that our learnable query token design has both accuracy and speed advantage over the encoder output design.\\n\\nTable 6. Comparisons of two types of decoder input design on RefCOCO dataset. The learnable query token design is faster and more accurate.\\n\\n| Decoder Input Val | TestA | TestB | Speed (ms) |\\n|-------------------|-------|-------|------------|\\n| Output of Encoder | 87.78 | 89.89 | 83.24 82.8 |\\n| Learnable Query Tokens (ours) | 88.78 | 90.63 | 84.67 54.9 |\\n\\nTable 7. Comparisons of two types of decoder input design on MSCOCO Image Captioning dataset. The learnable query token design has equal or better performance with faster inference speed.\\n\\n| Decoder Input         | BLEU@4 | METEOR | CIDEr | SPICE | Speed (ms) |\\n|-----------------------|--------|--------|-------|-------|------------|\\n| Output of Encoder      | 36.4   | 28.6   | 121.8 | 22.0  | 58.7       |\\n| Learnable Query Tokens | 36.4   | 28.7   | 123.1 | 22.5  | 51.2       |\\n\\nQ-CTC loss vs Cross-entropy loss\\nWe compare Q-CTC loss and standard cross-entropy loss (CE) in NARVL, and the results are in Table 8. With cross entropy loss, the first \\\\( n \\\\) input queries are supervised to predict the output sequence, where \\\\( n \\\\) is the number of tokens in the target sequence. Q-CTC loss performs significantly better than CE loss as it assigns proper penalty to the model, while CE penalizes the model severely even just one token position shift.\\n\\nTable 8. Comparing Q-CTC loss and CE loss with NARVL on Captioning. The base model is used in the experiments and knowledge distillation is not used for both losses.\\n\\n| Loss    | BLEU@4 | METEOR | CIDEr | SPICE | Speed (ms) |\\n|---------|--------|--------|-------|-------|------------|\\n| CE      | 17.54  | 18.57  | 66.85 | 11.89 | 52.05      |\\n| Q-CTC   | 26.69  | 24.06  | 93.24 | 17.38 | 54.03      |\\n| Difference | +9.15 | +5.49 | +26.39 | +5.49 | +2.0       |\\n\\nKnowledge distillation\\nWe study the effect of knowledge distillation on VQA datasets, and found it improves the performance by 1.48/1.35 on test-dev/test-std splits, as shown in Tab 9.\\n\\nThe various lengths of query tokens\\nWe study the effect on performance and speed of our method when we change the number of input queries to the decoder and decoding\"}"}
{"id": "CVPR-2024-1690", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9. Effect of knowledge distillation on VQA and Captioning datasets. The models with \u201cKD\u201d are the distilled models.\\n\\n| Method   | BLEU@4 | METEOR | CIDEr | SPICE | Speed (ms) |\\n|----------|--------|--------|-------|-------|------------|\\n| NARVL    | 26.7   | 24.1   | 93.2  | 17.4  | 54.0       |\\n| NARVL-KD | 36.4   | 28.7   | 123.1 | 22.5  | 51.2       |\\n| Difference | +9.7  | +4.6   | +29.9 | +5.1  | -2.8       |\\n\\n(a) MSCOCO Image Captioning\\n\\n| Method   | Test-dev | Test-std | Speed (ms) |\\n|----------|----------|----------|------------|\\n| NARVL    | 74.21    | 74.4     | 51.52      |\\n| NARVL-KD | 75.69    | 75.75    | 50.76      |\\n| Difference | +1.48  | +1.35   | -0.76      |\\n\\n(b) VQA methods. Ablation experiments are done on MSCOCO Image Captioning dataset, and the results are shown in Table 10. If the input length is smaller than the target sequence, the decoder won\u2019t be able to generate complete sequence, which leads to reduced performance for number of values that are too small, while too large number of input queries makes it harder for the model to decide the input output token correspondence. Naturally larger number of queries leads to increased inference time, while increasing the number of input queries from 10 to 1000 only leads to an increase from 51.32 ms to 60.40 ms, demonstrating the amazing ability of long sequence scalability.\\n\\nTable 10. Ablation experiments for the number of queries on Image Captioning dataset. All experiments are done with base size model. The models used in query number ablations only and 1 epoch training.\\n\\n| Number of Queries | BLEU@4 | METEOR | CIDEr | SPICE | Speed (ms) |\\n|-------------------|--------|--------|-------|-------|------------|\\n| 10                | 29.43  | 24.89  | 96.90 | 18.51 | 51.32      |\\n| 20                | 33.18  | 27.10  | 110.35| 20.48 | 51.72      |\\n| 100               | 32.47  | 26.98  | 108.76| 20.42 | 51.96      |\\n| 500               | 32.36  | 26.79  | 107.34| 19.94 | 53.82      |\\n| 1000              | 31.91  | 26.60  | 105.90| 19.89 | 60.40      |\\n\\nBeam search in NARVL\\n\\nWe also experiment with greedy and beam search decoding, and observe slightly better performance from beam search. Due to the increased inference time and autoregressive nature of beam search decoding, it\u2019s not adopted in previous experiments.\\n\\nTable 11. Ablation experiments of Beam Search in NARVL. Test it on Image Captioning dataset. All experiments are done with base size model.\\n\\n| Decoding Method | BLEU@4 | METEOR | CIDEr | SPICE | Speed (ms) |\\n|-----------------|--------|--------|-------|-------|------------|\\n| Greedy          | 36.38  | 28.70  | 123.15| 22.46 | 51.18      |\\n| Beam Search 5    | 36.85  | 28.74  | 124.03| 22.51 | 68.95      |\\n| Beam Search 20   | 36.91  | 28.71  | 124.15| 22.51 | 70.22      |\\n| Beam Search 100  | 37.00  | 28.71  | 124.25| 22.51 | 122.91     |\\n\\nThe model scale\\n\\nWe investigate the performance and inference speed of AR and NAR models of different model sizes, and the comparison is illustrated in Figure 5. We observe that the inference latency increases when the model becomes large on both AR and NAR model, but AR model has larger latency change compared to AR models. It is worthy to note that NAR huge has the similar performance as AR huge, meanwhile its inference speed is closed to the AR base model.\\n\\nFigure 5. The comparison of accuracy and inference speed with NAR (non-autoregressive) and AR (autoregressive) models for varying model sizes: Tiny, Base and Huge. Speed is measured in wall clock time. NAR models significantly outperform their AR counterparts under the same inference time budget on the RefCOCO validation set.\\n\\n6. Limitations\\n\\nOptimization of the NAR models is difficult, and the best performance we get on Image Captioning and VQA datasets rely on distillation from an autoregressive model, which is inconvenient in practice as one needs to train two models. The most common failure case comes from the conditional independence nature between the decoder tokens, and the model might end up merging multiple possible valid sequences into a incorrect output sequence.\\n\\n7. Conclusion\\n\\nWe have introduced NARVL, an All-in-One non-autoregressive model for various visual-language tasks, and the proposed NARVL repurposes the autoregressive decoder into a more flexible encoding that can be tailored to different hypothesis spaces using a layer of learnable query tokens (LQTs). These tokens are, in turn, used to define Query-CTC loss, akin to losses used in language modeling, but augmented to incorporate LQTs. This innovation is key to enabling the flexibility of letting the task drive the design of the hypothesis space, which we deem critical for heterogeneous input and output spaces as expected in the visual domain but absent in language.\"}"}
