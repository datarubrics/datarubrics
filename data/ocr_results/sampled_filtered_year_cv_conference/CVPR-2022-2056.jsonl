{"id": "CVPR-2022-2056", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. The process of the denoising operation.\\n\\nTwo potentially non-aligned sequences from each of them, denoted as \\\\( v_p \\\\in \\\\mathbb{R}^{T \\\\times d_v} \\\\) and \\\\( q_p \\\\in \\\\mathbb{R}^{T \\\\times d_q} \\\\), \\\\( T(\\\\cdot) \\\\) represents the sequence length (audio or video), and \\\\( d(\\\\cdot) \\\\) represents the dimension, respectively. Inspired by the multi-modal transformer in MulT [51], we attend to interactions between multi-modal sequences across distinct time steps and latently adapt streams from audio modality to visual modality. We assume the input of the cross-modal attention is a sequence of queries \\\\( Q = v_p W_Q \\\\), keys \\\\( K = q_p W_K \\\\) and values \\\\( V = q_p W_V \\\\). The cross-modal attention is calculated by\\n\\n\\\\[\\n\\\\text{Attention}_{a \\\\rightarrow v}(Q, K, V) = \\\\text{Softmax}(Q^\\\\top K \\\\sqrt{d_k}) V^\\\\top,\\n\\\\]\\n\\nwhere \\\\( d_k \\\\) is the query dimensions and Softmax operation is performed on every row. We employ multi-head attention layers [53], which consists of \\\\( H \\\\) paralleled cross-modal attention layers. Finally, we obtain cross-modal representation \\\\( f \\\\in \\\\mathbb{R}^{T \\\\times d_v} \\\\).\\n\\nNoise problem can be derived from acquisition (pause, environment noise, etc.) and alignment. For the acquisition noises, we use a pre-trained MFCC model [5] to extract acoustic features, which is widely used in automatic speech and speaker recognition. There is a large gap between video and audio representations. The joint representations reflect important information considering multi-modal alignment. Attention is an importance estimate in the time domain, while the frequency domain can reflect another granularity of importance estimate. For multi-modal tasks, the importance of the frequency band requires consideration of interactive alignment information, and the high frequency part is more likely to be noisy information unrelated to alignment. For example, the noise can be parts of the audio irrelevant to the content of the video, or parts of the video irrelevant to the audio features. To deal with the noise problem, we adopt 2D Discrete Wavelet Transform (DWT) for the joint representation. The reason for using DWT instead of DFT is that DFT is more likely to lose useful information at high frequencies, resulting in a decrease in actual performance.\\n\\nThe proposed algorithm is a hybrid approach that uses spatial and transform-domain information. Wavelet transform decomposes a signal into its sub-bands using a series of high-pass and low-pass filters. As noise is generally categorized as a high-frequency component, it is easier to separate it from the signal using wavelet transform. The decomposition of frequency content depends on the number of levels of DWT. The cross-modal representations \\\\( f \\\\in \\\\mathbb{R}^{T \\\\times d_v} \\\\) serve as the input signal. The DWT separates filtering operations on rows and columns. \\\\( A_{j,u} \\\\) and \\\\( C_{k_j,u} \\\\) denote scaling and wavelet coefficients at scale \\\\( j \\\\) for the given signal \\\\( f \\\\) where \\\\( k = 1, 2, 3 \\\\). We'll be working with separable orthonormal filters so 2D filters can be expressed as a product between low pass filter \\\\( h \\\\) and high pass filter \\\\( g \\\\). The coefficients at scale \\\\( j \\\\) can be obtained from coefficients at scale \\\\( j + 1 \\\\). We can obtain \\\\( A_{j,u} \\\\) and \\\\( C_{k_j,u} \\\\) as follows.\\n\\n\\\\[\\nA_{j,u} = \\\\sqrt{2} \\\\sum_{u} h(l - 2u) A_{j+1,l};\\n\\\\]\\n\\n\\\\[\\nC_{1_j,u} = \\\\sqrt{2} \\\\sum_{u} g(l - 2u) A_{j+1,l};\\n\\\\]\\n\\n\\\\[\\nC_{2_j,u} = \\\\sqrt{2} \\\\sum_{u} g(h l - 2u) A_{j+1,l};\\n\\\\]\\n\\n\\\\[\\nC_{3_j,u} = \\\\sqrt{2} \\\\sum_{u} g(g l - 2u) A_{j+1,l}.\\n\\\\]\\n\\nTo implement the filter bank, we use two-stage filter banks. In the first stage, rows of two-dimensional signal are convolved with \\\\( h, g \\\\) filters and then we downsample columns by 2. In the next stage, columns are convolved with the filters \\\\( h, g \\\\) and we keep only even indexed rows. A \\\\( n \\\\times d_v \\\\) cross-modal signal is transformed into four \\\\( n^2 \\\\times d_v^2 \\\\) signal after the two stages. Next, we perform threshold quantization on the high-frequency coefficients \\\\( C_{k_j,u} \\\\) of wavelet decomposition. For the high-frequency coefficients (in three directions) of each layer from layer 1 to layer \\\\( N \\\\), a threshold value is selected for threshold quantization. We adopt VisuShrink threshold \\\\( \\\\alpha \\\\) with a soft threshold function. For \\\\( \\\\phi = \\\\max |C'_{k_j,u}| \\\\), the filter operations can be represented as follows (\\\\( k \\\\in [1, 2, 3] \\\\)).\\n\\n\\\\[\\nC'_{k_j,u}[x, y] = \\\\text{sgn}(C_{k_j,u}[x, y])(C_{k_j,u}[x, y] - \\\\alpha \\\\phi)^+,\\n\\\\]\\n\\nThen, we perform the wavelet reconstruction of the signal. The wavelet is reconstructed according to the low frequency coefficients of the \\\\( N \\\\)-th layer of wavelet decomposition and the high-frequency coefficients from the 1st layer to the \\\\( N \\\\)-th layer after quantization.\"}"}
{"id": "CVPR-2022-2056", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $f_n$ is the denoised joint representation. The $f_n$ serves as the input of the following feed-forward layers and layer norm. Finally, we obtain the encoded representation $E$.\\n\\n4.4. Self Attention-Free Decoder\\n\\nDue to the fusion between the audio and video signal, joint representations are more suitable for processing in the frequency domain. We adopt the decoder without the self-attention layers to achieve speedups. Inspired by [30], each layer consists of a Fourier mixing sublayer followed by a feed-forward sublayer. We replace the self-attention sublayer of each transformer decoder layer with a Fourier sublayer. A 2D DFT is applied to its embedding input. One 1D DFT is along the sequence dimension, $F_{seq}$, and one 1D DFT is along the hidden dimension $F_{hidden}$.\\n\\n$$y = R(F_{seq}(R(F_{hidden}(x))))$$\\n\\nWe only keep the real part of the result. After the Fourier layers, we employ the multi-head attention layers. The object prediction $O$ is obtained.\\n\\n4.5. Training of Wnet\\n\\nAmong the whole model, the loss function includes the mask loss, the box loss and the mutual loss.\\n\\n$$L = \\\\lambda_1 L_{mask} + \\\\lambda_2 L_{box} + L_{mutual},$$\\n\\nwhere $\\\\lambda_1, \\\\lambda_2$ aim to adjust the three losses. The mask loss for supervising the predictions is defined as a combination of the Dice [36] and Focal [32] loss:\\n\\n$$L_{mask}(m_i, m_{\\\\sigma}(i)) = \\\\frac{1}{T} \\\\sum_{t=0}^{T} [L_{Dice}(m_{i,t}, m_{\\\\sigma}(i),t) + L_{Focal}(m_{i,t}, m_{\\\\sigma}(i),t)],$$\\n\\nwhere $m$ is the predicted mask, $m_{\\\\sigma}$ is the target mask and $T$ is the number of frames in the video. $L_{box}$ scores the bounding boxes. We use a linear combination of the sequence level L1 loss and the generalized IOU [43] loss.\\n\\n$$L_{box}(b_i, b_{\\\\sigma}(i)) = \\\\frac{1}{T} \\\\sum_{t=0}^{T} [L_{iou}(b_{i,t}, b_{\\\\sigma}(i),t) + ||b_{i,t} - b_{\\\\sigma}(i),t||_1],$$\\n\\nWe use KL divergence [60] to maximize the mutual information between the cross-modal representation $f$ and the encoded representation $E$.\\n\\n$$L_{mutual}(E(i, j)||f(i, j)) = \\\\sum_{E(i, j)} (log E(i, j) f(i, j))$$\\n\\nwhere $i$ stands for the sequence and $j$ stands for the dimension. $E$ and $f$ are sent to the softmax function before computing KL divergence. The KL divergence is used to pull in the distance between the cross-modal representation and the encoded representation. Thus, the audio guidance is strengthened, which avoids the DWT operation filtering too many audio factors.\\n\\n5. Experiments\\n\\n5.1. Performance Criteria\\n\\nWe evaluate the performance of our Wnet method based on two widely-used evaluation criteria for audio-guided video semantic segmentation following [40]. Given the testing video sequence $v$ and audio query $q$ with the ground-truth masks $G$, we denote the generated masks from our Wnet method by $S$. We employ the Jaccard index defined as the intersection-over-union of the generated segmentation and the ground-truth mask ($J = \\\\frac{S \\\\cap G}{S \\\\cup G}$). From a contour-based perspective, one can interpret $S$ as a set of closed contours $c(S)$ delimiting the spatial extent of the mask. Therefore, one can compute the contour-based precision and recall $P_c$ and $R_c$ between the contour points of $c(S)$ and $c(G)$. We adopt F-measure as a trade-off between the two ($F = \\\\frac{2 \\\\cdot P_c \\\\cdot R_c}{P_c + R_c}$).\\n\\n5.2. Implementation Details\\n\\n**Visual Feature Extraction.** We use a ResNet-50 backbone to extract visual features, which has the same settings as DETR [6]. And it is then fed into a 2D convolution with kernel size 1 to map the model dimension and each frame is concatenated to form the clip level feature.\\n\\n**Acoustic Feature Extraction.** We use a 39-dimensional MFCC to represent its acoustic feature. Then, we use 1D convolution to further extract features and map them to the corresponding dimensions of the model following the implementation by Tsai et al. [51]. The kernel size of 1D convolution is 1.\\n\\n| Model       | J F J | FURvos+ [48] | PAM+ [38] | VisTR+ [56] | Wnet (Ours) |\\n|-------------|-------|--------------|-----------|-------------|-------------|\\n|             | 37.1% | 39.2%        | 38.2%     | 38.6%       | 43.0%       |\\n|             | 38.6% | 38.9%        | 38.8%     | 38.0%       | 45.0%       |\\n|             | 39.5% | 38.8%        | 43.0%     | 45.0%       | 44.0%       |\\n\\nTable 2. The comparison of different method for audio-guided semantic segmentation on AVOS.\\n\\n| Dataset | J F J |\\n|---------|-------|\\n| RVOS    | 43.0% |\\n| A2D     | 49.8% |\\n| J-HMDB  | 65.6% |\\n\\nTable 3. The results of different datasets in the AVOS dataset. In this table, we use the same dataset for training and testing. Note: JHMDB-Sentences is only for evaluation, not training, so it is directly evaluated using the checkpoint trained on A2D-Sentences.\"}"}
{"id": "CVPR-2022-2056", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A brown turtle is swimming up in the water. An ATV being ridden down a path.\\n\\nFigure 4. Visualization of Wnet and VisTR+ on the AVOS.\\n\\n\\\\[ \\\\mathbf{C}_1, u_0 \\\\mathbf{C}_1, u_1 \\\\mathbf{C}_1, u_2 \\\\]\\n\\nFilter\\n\\n\\\\[ \\\\mathbf{A}_1, u \\\\]\\n\\nReconstructed Signal\\n\\n\\\\[ \\\\mathbf{C}_2, u_0 \\\\mathbf{C}_1, u_1 \\\\mathbf{C}_2, u_2 \\\\]\\n\\nFigure 5. Visualization of DWT-based denoised features.\\n\\nDataset Processing. Our dataset (AVOS) contains three parts (RVOS, A2D and J-HMDB). For the RVOS part, we use the same videos in Youtube-VIS [59], as VisTR [56]. The mask annotations in validation and test set are unavailable, so we divide the training set into the training, validation and test set in our experiments. However, we also offer the audio queries for original validation and test set.\\n\\nModel Setting. We adopt a 2-layer, 8-head multi-head cross-attention [51] module with the width of 3 to fuse visual and audio features. Between the attention layer and the feed-forward layer, a wavelet transform filter layer is used to remove noise from joint representations. For the transformer decoder, we use Fourier transform [30] instead of the self-attention layer. After obtaining the prediction of the decoder and the encoder, for each corresponding frame, we send them to an attention module to obtain the attention map, which is not multiplied by the value. Then it will be fused with the backbone features and the memory to get the mask features for each instance of each frame, following the same practice with VisTR [56]. We expand the number of frames per video to 36 for end-to-end training, and applied 36 query slots for 36 objects throughout the video. Finally, we use three Conv3d layers and GroupNorm layers [57] with ReLU activation. The Conv3d layers have the kernel size of 3, padding of 2 and dilation of 2. And we use a last Conv3d layer with the kernel size of 1 to obtain the mask. More details are in supplementary material.\\n\\n5.3. Performance Comparisons\\n\\nWe compare our proposed method with other existing methods for the problem as follows:\\n\\n- **VisTR+** is the extension of the transformer-based video instance segmentation algorithm [56], where the cross-modal attention layer is added to fuse the two modalities. For the VisTR+, we use the Hungarian loss as [56]. For our Wnet, we use the box and mask loss.\\n\\n- **URVOS+** is the extension of the unified referring video segmentation network [48], where the MFCC layer [5] is added to encode the audio inputs.\\n\\n- **PAM+** is the extension of the polar relative positional encoding mechanism [38], where the MFCC layer [5] is added to encode the audio inputs.\\n\\nTab. 2 and Tab. 3 presents the performance on AVOS. We exceed VisTR+, URVOS+ and PAM+ with 5.0%, 5.9% and 4.4% in the region similarly. Wnet has an absolute improvement of 5.5%, 5.8% and 6.1% for the contour accuracy. These comparisons mean that the audio-guided video object segmentation is quite different from the text-guided task. There is also a vast difference between the recordings collected from the natural environment and those generated from text-to-speech model. Therefore, it is not suitable to treat audio-guided segmentation as the combination of automatic speech recognition and text-based segmentation.\\n\\nThe visualization of Wnet on the AVOS test dataset is shown in Fig. 4, with each row containing images sampled from the same video. The comparison between Wnet and VisTR+ shows our efficiency in the audio-guided models.\"}"}
{"id": "CVPR-2022-2056", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. The results for different components.\\n\\n| Model                  | J  | F  | J & F |\\n|------------------------|----|----|-------|\\n| Base                   | 39.3% | 41.6% | 40.4% |\\n| Base + AFD             | 41.2% | 42.5% | 41.8% |\\n| Base + AFD + MIM       | 41.8% | 43.0% | 42.4% |\\n| Base + AFD + MIM + DWT | 42.9% | 44.0% | 43.5% |\\n\\nNote: We use self-attention layers to replace DWT layers in Base, Base+AFD and Base+AFD+MIM.\\n\\nTable 5. The results for different wavelet basis, mentioned in [42].\\n\\n| Wavelet Basis | J  | F  |\\n|---------------|----|----|\\n| Daubechies    | 42.9% | 41.9% | 41.7% |\\n| Symlets       | 42.6% | 42.9% | 42.7% |\\n| Coiflets      | 41.3% | 41.5% | 41.2% |\\n| Meyer         | 40.7% | 40.5% | 40.3% |\\n\\nTable 6. The results for DWT.\\n\\nFor the low pass and high-low pass, we use the hard threshold function. For the high pass, we use the soft threshold function.\\n\\n| Model          | J  | F  | J & F |\\n|----------------|----|----|-------|\\n| Low Pass       | 41.8% | 43.3% | 42.5% |\\n| [0.0, 0.9]     | 42.1% | 43.3% | 42.7% |\\n| [0.0, 0.8]     | 40.3% | 41.3% | 40.6% |\\n| High-Low Pass  | 42.1% | 43.7% | 42.9% |\\n| [0.008, 0.9]   | 42.8% | 43.2% | 43.1% |\\n| High Pass      | 42.9% | 44.0% | 43.5% |\\n| [0.01, 1]      | 42.3% | 43.8% | 43.1% |\\n| [0.008, 1]     | 42.1% | 43.7% | 42.9% |\\n| [0.006, 1]     | 42.0% | 43.6% | 42.8% |\\n\\nTable 7. The results for threshold function selection. Take high pass [0.008, 1] for example.\\n\\n| Function Selection       | J  | F  |\\n|--------------------------|----|----|\\n| Hard Function             | 42.1% | 41.8% | 42.0% |\\n| Soft Function             | 42.9% | 44.0% | 43.5% |\\n\\nTable 8. The results for J selection. For different J, the number of the high frequency coefficient matrix is 3J.\\n\\n| J | Number of Coefficient Matrix | J  | F  | J & F |\\n|---|-----------------------------|----|----|-------|\\n| 1 | 3 (C_k^1, u); C_k^2, u)     | 42.9% | 44.0% | 43.5% |\\n| 2 | 6 (C_k^1, u); C_k^2, u; C_k^3, u) | 41.0% | 41.7% | 41.4% |\\n\\nTable 9. The comparison of average inference latency among Wnet and audio-text-segmentation model. The evaluation is conducted on a server with 1 NVIDIA 3090Ti GPU, 12 Intel Xeon CPU. The batch size is set to 1.\\n\\n| Method      | J  | Latency | Speedup |\\n|-------------|----|---------|---------|\\n| Wnet        | 42.9% | 0.0014s | 2.10 \u00d7 |\\n| ASR+RVOS    | 38.4% | 0.0032s | 1.00 \u00d7 |\\n\\nWnet can segment small objects from the nature environment, while the VisTR+ performs poorly under this circumstance. Furthermore, the visualization of the DWT process in Fig. 5 shows the denoising performance.\\n\\n5.4. Ablation Study\\n\\nIn the ablation study, we fine-tune parameters on the validation set. We take the audio-guided RVOS dataset (parts of AVOS dataset) as the example. About the model components. As shown in Tab. 4, we conduct the experiments to verify the effectiveness of our model design, including the DWT-Based denoising (DWT), mutual information maximum (MIM) and self attention-free decoder (AFD). We use the self-attention layers in the models without the DWT layers. The full model achieves better results than the model (w/o. DWT). It suggests that the DWT layers can filter the noise generated in the audio-video fusion and improve subsequent segmentation results.\\n\\nAbout the wavelet basis. Tab. 5 shows the comparison of different wavelet bases. Results verify that the Daubechies wavelet basis is proper for the discrete joint representations.\\n\\nAbout the threshold parameter selection. We conduct the experiments under high-pass filters and low-pass filters with different threshold parameter selections. Results in Tab. 6 shows that we can get the best performance under the low-pass filters (0.008) with the soft function.\\n\\nAbout the threshold function selection. Two common threshold functions are the hard and the soft function. The hard threshold method can preserve the local features such as the edge of the signal well, while the soft threshold method is relatively smooth. As Tab. 7 shown, we choose the soft function for our model.\\n\\nAbout the J selection. Tab. 8 shows the results for the J selection. The performance will be worse when the order is increasing. We select J = 1 for our model, with 1 low frequency and 3 high-frequency coefficients.\\n\\nAbout the audio-text-segment model. The audio-text-segment model means that we use an ASR model first and then employ the latter referring segmentation model. Tab. 9 shows the results for comparison of Wnet and audio-text-segment model, in terms of speed and quality. We achieve the better performance in these two factors.\\n\\n6. Conclusion\\n\\nIn this paper, we present the problem of open-ended audio-guided video semantic segmentation, which can be applied in video analysis, video editing, virtual human and so on, from the viewpoint of end-to-end denoising encoder-decoder network learning. We propose the wavelet-based encoder network to learn the cross-modal representations of the video contents with audio-form queries. Then, a self attention-free decoder network is developed to generate the target masks with frequency-domain transforms. In addition, we construct the first large-scale audio-guided video semantic segmentation dataset. The extensive experiments show the effectiveness of our method.\\n\\nAcknowledgments. This work was supported by the National Natural Science Foundation of China (Grant No.61836002, No.62020106007, No.62072150, No.62072397) and Zhejiang Natural Science Foundation (LR19F020006).\"}"}
{"id": "CVPR-2022-2056", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wnet: Audio-Guided Video Object Segmentation via Wavelet-Based Cross-Modal Denoising Networks\\n\\nWenwen Pan 1,\u2020, Haonan Shi 1,\u2020, Zhou Zhao 1,* , Jieming Zhu 2, Xiuqiang He 2, Zhigeng Pan 3, Lianli Gao 4, Jun Yu 5, Fei Wu 1,6, Qi Tian 7\\n\\n1 Zhejiang University, 2 Huawei Noah\u2019s Ark Lab, 3 Hangzhou Normal University, 4 The University of Electronic Science and Technology of China, 5 Hangzhou Dianzi University, 6 Shanghai Institute for Advanced Study of Zhejiang University, 7 Huawei Cloud & AI\\n\\n{wenwenpan, shihn, zhaozhou}@zju.edu.cn, jiemingzhu@ieee.org, hexiuqiang1@huawei.com, zgpan@hznu.edu.cn, lianli.gao@uestc.edu.cn, yujun@hdu.edu.cn, wufei@cs.zju.edu.cn, tian.qi1@huawei.com\\n\\nAbstract\\n\\nAudio-Guided video object segmentation is a challenging problem in visual analysis and editing, which automatically separates foreground objects from the background in a video sequence according to the referring audio expressions. However, existing referring video object segmentation works mainly focus on the guidance of text-based referring expressions, due to the lack of modeling the semantic representations of audio-video interaction contents. In this paper, we consider the problem of audio-guided video semantic segmentation from the viewpoint of end-to-end denoising encoder-decoder network learning. We propose the wavelet-based encoder network to learn the cross-modal representations of the video contents with audio-form queries. Specifically, we adopt the multi-head cross-modal attention layers to explore the potential relations of video and query contents. A 2-dimension discrete wavelet transform is merged into the transformer encoder to decompose the audio-video features. Next, we maximize mutual information between the encoded features and multi-modal features after cross-modal attention layers to enhance the audio guidance. Then, a self attention-free decoder network is developed to generate the target masks with frequency-domain transforms. In addition, we construct the first large-scale audio-guided video semantic segmentation dataset. The extensive experiments show the effectiveness of our method.\\n\\n1. Introduction\\n\\nReferring video object segmentation aims to segment video objects referred by given language expressions, which has attracted wide attention due to its applicability to many practical problems including video analysis and video editing [33, 35, 49, 50, 61]. Currently, most referring video object segmentation approaches mainly focus on the guidance of text-guided referring expressions [18, 19, 31, 33, 35, 49, 61, 63], which can learn the multi-modal representation from the interaction network layer, and then generate the object masks to the given text references. The existing works have achieved promising performance in text-based video object segmentation, but they may still be ineffectively applied to the audio-guided video object segmentation due to the lack of modeling the semantic representation of audio-video interaction contents.\\n\\nThe audio-guided video analysis is a simulation of human cognition, comparing with the text-guided analysis [44]. Humankind use speech exclusively long before the invention of writing. People also learn and use language in the real world, as to collaborate, describe and relate their visual environment, talk about each other, and so on. Furthermore, in the natural scene, audio interaction is more convenient and common than text interaction. Although audio...\"}"}
{"id": "CVPR-2022-2056", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"inputs can be converted to text inputs through ASR models \\\\[3, 4, 46\\\\], the process will produce unavoidable losses. Since Harwath and Glass\u2019s collection of spoken captions for Flickr8k \\\\[14\\\\], more works address cognitive and linguistic questions \\\\[8, 10\u201312\\\\]. Other work addresses applied tasks, including multi-modal retrieval \\\\[22\\\\], cross-modality alignment \\\\[13, 27\\\\], retrieving speech in different languages using images as a pivot modality \\\\[1, 26, 39\\\\], and speech-to-speech retrieval \\\\[1, 39\\\\]. Our work focuses on the audio-guided video object segmentation tasks, shown as Fig. 1. The audio guidance often contains rich semantic information, such as the accent, emotion and speed. These extra factors can facilitate the object segmentation. The same object can correspond to different pronunciations, while the same pronunciation can point to different objects. Thus, the simple extension of the existing segmentation works based on text-based guidance is difficult for modeling the semantic representation of audio-video interaction contents. Inspired by MulT \\\\[51\\\\], we use multi-head cross-modal attention layers to fuse the video embeddings and audio embeddings. Different from the MulT model \\\\[51\\\\], we extend dimensions of inputs and apply it to large-scale natural language datasets. The cross-modal transformers referred to text embeddings are all removed.\\n\\nOne other bottleneck is the noise problem, derived from acquisition noise and fusing noise \\\\[7\\\\]. For the acquisition noises, we use a pre-trained MFCC model \\\\[5\\\\] to extract acoustic features, which is widely used in automatic speech and speaker recognition. In this paper, we focus on the processing of fusing noise. There is a large gap between video and audio representations. The joint representations reflect important information considering multi-modal alignment. Audio and video features have different redundant parts (i.e. irrelevant phonemes and pixels), likewise termed as noise. These noises are difficult to handle only by convolution operations and attention mechanisms in the time domain. As mentioned in \\\\[29\\\\], noises are likely to concentrate at high frequencies. Recently, Fnet \\\\[30\\\\] has been proposed to learn the frequency-domain-level representation with Fourier transforms for recognition tasks, while it only aims to speed up the encoder architectures but fails to obtain improvement in performances. Low-pass filtering on Fourier analysis cannot effectively distinguish the high-frequency parts of the required signal from the high-frequency interference caused by noise. If the low-pass filtering is too narrow, parts of the required signal are treated as noise and its morphological information is erased, which leads to the distortion of the original signal \\\\[45\\\\].\\n\\nMotivated by this, we integrate the 2-dimension discrete wavelet (DWT) transform into the transformer encoder, which replaces self-attention layers with DWT layers. The DWT denoising has proved its effectiveness in image denoising \\\\[25, 45, 52\\\\], but has not been used in multi-modal representation yet to our knowledge. We are the first to devise the DWT-transformer for the audio-visual joint representation to filter the noise and outliers, as a priori. The layers of the whole transformer encoder are reduced, which obtains a sizable performance boost in terms of speed and model consumption. Inspired by the AMDIM \\\\[2\\\\], we maximize mutual information between the encoded features and multi-modal features after the cross-modal attention to enhance the audio guidance.\\n\\nThe main contributions of this paper are as follows: (i) Unlike the previous studies, we study the problem of audio-guided video object segmentation from the viewpoint of end-to-end denoising encoder-decoder network learning. (ii) We propose the wavelet-based encoder network to learn the cross-modal representations of the video contents with audio-form queries. (iii) We construct a large-scale dataset for audio-guided video object segmentation and validate the effectiveness of our proposed method through extensive experiments.\\n\\n2. Related Work\\n\\n2.1. Referring Expression object Segmentation\\n\\nThe referring expression segmentation task has attracted increasing research interest \\\\[18, 19, 31, 33, 35, 49, 61, 63\\\\] in recent years. Hu et al. \\\\[18\\\\] formulate this task as an image-region-wise classification problem. Li et al. \\\\[31\\\\] employ multi-scale image features from multiple convolutional layers. Qiu et al. \\\\[41\\\\] further enhance visual features and introduce an adversarial mechanism. Some works \\\\[33, 35, 49, 50, 61\\\\] make more interactions between the image and natural language query. Furthermore, the attention module \\\\[61, 63\\\\] is introduced to the segmentation task. To enhance the accuracy, further works successfully model the dependencies of cross-modal information \\\\[20\\\\], informative words of the expression \\\\[21\\\\] and localization information of the referent instances \\\\[24\\\\]. Moreover, Luo et al. \\\\[34\\\\] achieve a joint learning of referring expression comprehension and segmentation. \\\\[28\\\\] extends technologies to video data and incorporated temporal coherency. For the video data, existing methods commonly employ dynamic convolutions \\\\[9, 54\\\\] to adaptively generate convolutional filters, or leverage cross-modal attention \\\\[38, 55, 62\\\\] to compute the correlations among input visual and linguistic embeddings. However, these works cannot handle the noise problem of audio-video joint representations.\\n\\n2.2. Speech-Based Video Analysis\\n\\nComparing with the text-guided video analysis, audio-guided analysis is a more precise simulation of human cognition to the world \\\\[44\\\\]. Actually, people use speech exclusively long before the invention of writing. Harwath et al. \\\\[14\\\\] collect spoken captions for Flickr8k, and then much...\"}"}
{"id": "CVPR-2022-2056", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. The statistics of the A VOS dataset.\\n\\n|            | RVOS | A2D | J-HMDB | Total |\\n|------------|------|-----|--------|-------|\\n| Number of Audio | 11,226 | 6,656 | 929    | 18,811 |\\n\\nResearch begins to attach importance to this task. Some works emphasize the cognitive and linguistic questions, such as understanding how different learned layers correspond to visual stimuli, learning linguistic units or how visually grounded representations can help understand lexical competition in phonemic processing. Ramon Sanabria et al. propose dual encoder models that can be used for efficient multimodal retrieval. However, these works take less consideration of video object segmentation.\\n\\n3. Audio-Guided-VOS Dataset (A VOS)\\n\\nThere are previous works that constructed referring segmentation datasets for videos. Gavrilyuk et al. extended the A2D and J-HMDB datasets with natural sentences. Seo et al. constructed the first large-scale referring video object segmentation dataset called RVOS. To facilitate audio-based video object segmentation, we have constructed a large-scale audio-guided dataset, Audio-Guided-VOS (A VOS), with referring audio expressions as Tab. 1. A VOS is the extension of RVOS, A2D and J-HMDB. We select the three datasets for their rich scene information. To obtain audio annotations, we employ 36 speakers to read the sentences totally. To ensure the recording quality, all the speakers are required to read proficiently, do not stammer, stuck and other situations. The sampling rate is 44,100K or above, the sampling number is 16 bits, and the speaking speed is 100-150 words per minute. Speaking speed should be normal speaking speed, or TV announcer speaking speed. The word accuracy of text files and audio files is not less than 99% under manual checking. The average length of each recording is 5 to 6 seconds, about 28 hours in total. Moreover, we have run two rounds of inspections. We not only correct the pronunciation in the recordings, but also correct grammar and spelling errors in the original texts. The ratio of the training set, the test set and the validation set is 75:15:10.\\n\\n4. Proposed Method\\n\\nWe present a video sequence as $v = \\\\{v_i\\\\}_{i=1}^n$, where $v_i$ is the pre-extracted visual feature of the $i$-th frame and $n$ is the frame number of the video. Each video is associated with an audio query, denoted by $q = \\\\{q_i\\\\}_{i=1}^m$ where $q_i$ is the feature of the $i$-th frame and $m$ is the frame number of the audio. The goal of the audio-guided video object segmentation is to predict binary segmentation masks $S = \\\\{S_i \\\\in \\\\{0, 1\\\\}^W \\\\times H\\\\}_{i=1}^n$.\\n\\n4.1. Analysis on Wavelet Transform\\n\\nAs to the convolutional neural network, each convolutional layer is composed of several convolutional units, and the parameters of each convolutional unit are optimized by the back propagation algorithm. Convolution operations aim to extract different features of inputs, represented as follows.\\n\\n$$W(\\\\tau) = \\\\int_{-\\\\infty}^{\\\\infty} f(t) g(\\\\tau - t) dt$$  \\\\hspace{1cm} (1)\\n\\nThe convolution kernel in the convolution layer is relatively fixed. Audio-video joint representations contain rich time-frequency characteristics, which are more suitable for window functions that vary in the time-frequency domain. The wavelet can be represented as follows.\\n\\n$$W(a, \\\\tau) = \\\\frac{1}{\\\\sqrt{a}} \\\\int_{-\\\\infty}^{\\\\infty} f(t) \\\\psi(t - \\\\tau a) dt$$  \\\\hspace{1cm} (2)\\n\\nwhere scaling function $\\\\psi_{a,\\\\tau}(t) = a^{-1/2} \\\\psi(t - \\\\tau a)$ and $a$ is the scale, which is inversely proportional to the frequency. The operation of the traditional convolution layer and wavelet have the commonality. The difference is $g(\\\\tau - t)$ and $\\\\psi(t - \\\\tau a)$. Audio and video features have different redundant parts (i.e. irrelevant phoneme and pixel), termed as noises. The noises from the video and audio inputs are distributed among most features after the cross-modal attention. These noises are difficult to handle only by convolution operations in the time domain. As mentioned in [29], noises are likely to concentrate at high frequencies. Fnet [30] proposes to use Fourier sublayers to replace the self-attention layers. However, low-pass filtering on Fourier analysis cannot effectively distinguish the high-frequency part of the required signal from the high-frequency interference caused by noise. Wavelet can well retain the peak value and mutation part of the useful signal required in the original signal. It has good time-frequency localization characteristics and can be expressed linearly as:\\n\\n$$W_x = W_f + W_e$$  \\\\hspace{1cm} (3)\\n\\nwhere $W_e$ is the wavelet coefficients controlled by noise. We can use threshold quantization to reconstruct denoising joint representations. Furthermore, we can obtain improvements by replacing self-attention layers with the DWT layers in terms of the model consumption and speed.\\n\\n4.2. Overview\\n\\nAs Fig. 2 illustrated, our model can be divided into five modules: visual encoder, audio encoder, transformer encoder, transformer decoder and segmentation module.\\n\\nVisual Encoder. We employ ResNet-50 as our backbone network to extract visual features from an input frame.\"}"}
{"id": "CVPR-2022-2056", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. The whole framework for our segmentation model.\\n\\nTo include spatial information of the visual feature, we augment 3-dimensional spatial coordinates following [56], denoted as \\\\( v = \\\\{ v_i \\\\}_{n_i=1} \\\\). The output of the backbone is \\\\( B \\\\in \\\\mathbb{R}^{n \\\\times c \\\\times H \\\\times W} \\\\). \\\\( c \\\\) represents the original video dimension. A 1\u00d71 convolution is used to reduce the dimension to \\\\( \\\\mathbb{R}^{n \\\\times d \\\\times H \\\\times W} \\\\). We then flatten the dimensions as \\\\( d \\\\times (n \\\\times H \\\\times W) \\\\).\\n\\nAudio Encoder. MFCCs are the features widely used in automatic speech and speaker recognition. Following [37], we capture information about lower frequencies than higher frequencies by non-linear scaling and thus acts as a human ear. A set of MFCCs is encoded as a multi-hot vector and projected onto an embedding space using the 1D convolution, denoted as \\\\( q = \\\\{ q_i \\\\}_{m_i=1} \\\\).\\n\\nTransformer Encoder and Decoder. We devise a transformer encoder-decoder framework for our audio-guided video object segmentation model. The model is in end-to-end manners. The transformer encoder is employed to learn the cross-modal representations of the video contents with audio-form queries. We first apply the layer normalization to the visual features and audio features, respectively. Next, we devise a wavelet-based cross-modal module to fuse the two modalities and achieve denoised joint representations. Each encoder layer consists of a multi-head attention module [53] and a fully connected feed-forward network. Then, we maximize the mutual information between the cross-modal representations and the encoded representations. During this stage, the temporal order is the same as the order of the initial input.\\n\\nThe transformer decoder aims to generate the top pixel features that can represent the target object of each frame. Motivated by Fnet [30], we also replace the self-attention sublayers with simple linear transformations. The self-attention-free decoder can better handle audio-video encoding. Besides the Fourier layers, we follow the standard architecture of the transformer, using multi-headed encoder-decoder attention mechanisms. Following [56], the decoder then takes a small fixed number of learned positional embeddings (object queries) as inputs, and attends to the encoder output. The overall predictions follow the input frame order. We remove all self-attention layers in the transformer encoder-decoder framework to reduce model computation. Details of the transformer are in 4.3 and 4.4.\\n\\nObject Sequence Segmentation. The module aims to predict the mask sequence for the target object. We capture the object predictions \\\\( O \\\\), backbone features \\\\( B \\\\) and encoded feature maps \\\\( E \\\\) from the previous layers, shown in Fig. 2. First, we employ an attention module to calculate the similarity map between \\\\( O \\\\) and \\\\( E \\\\). Following [56], we only compute the features of its corresponding frame. Next, we fuse the similarity map, \\\\( B \\\\) and \\\\( E \\\\) of the corresponding frames, following the DETR [6]. \\\\( B \\\\in \\\\mathbb{R}^{n \\\\times c \\\\times H \\\\times W} \\\\), \\\\( E \\\\in \\\\mathbb{R}^{d \\\\times n \\\\times (H \\\\times W)} \\\\), \\\\( O \\\\in \\\\mathbb{R}^{n \\\\times d} \\\\), where \\\\( n \\\\) denotes the frame numbers, \\\\( c \\\\) and \\\\( d \\\\) denote the dimension. Then, we use a deformable convolution as the last layer of the fusion. Thus, the mask features for the target object of different frames are achieved. Finally, the 3D convolution, which three 3D convolutional layers and group normalization layers [57] with ReLU activation function, is employed to obtain the mask sequence.\\n\\n4.3. DWT-Based Transformer Encoder\\n\\nComparing with the text-guided semantic segmentation, audio-based segmentation suffers from severe noise problems [44], derived from acquisition noise and fusing noise. For the acquisition noises, we use a pre-trained MFCC models [37] to extract acoustic features. For the fusing noise, we propose a DWT-based transformer encoder to realize multi-modal encoding and joint feature denoising.\"}"}
{"id": "CVPR-2022-2056", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Emmanuel Azuh, David Harwath, and James R. Glass. Towards bilingual lexicon discovery from visually grounded speech audio. In Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019, pages 276\u2013280. ISCA, 2019.\\n\\n[2] Philip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 15509\u201315519, 2019.\\n\\n[3] Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning of discrete speech representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\\n\\n[4] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\\n\\n[5] Lallouani Bouchakour and Mohamed Debyeche. Mfccs and gabor features for improving continuous arabic speech recognition in mobile communication modified. In Proceedings of the 3rd International Conference on Advanced Aspects of Software Engineering, ICAASE 2018, Constantine, Algeria, December 1-2, 2018, volume 2326 of CEUR Workshop Proceedings, pages 115\u2013121. CEUR-WS.org, 2018.\\n\\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, volume 12346 of Lecture Notes in Computer Science, pages 213\u2013229. Springer, 2020.\\n\\n[7] Grzegorz Chrupala. Visually grounded models of spoken language: A survey of datasets, architectures and evaluation techniques. CoRR, abs/2104.13225, 2021.\\n\\n[8] Grzegorz Chrupala, Lieke Gelderloos, and Afra Alishahi. Representations of language in a model of visually grounded speech signal. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 613\u2013622. Association for Computational Linguistics, 2017.\\n\\n[9] Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees G. M. Snoek. Actor and action video segmentation from a sentence. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 5958\u20135966. IEEE Computer Society, 2018.\\n\\n[10] Lieke Gelderloos and Grzegorz Chrupala. From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning. In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16, 2016, Osaka, Japan, pages 1309\u20131319. ACL, 2016.\\n\\n[11] David Harwath and James R. Glass. Towards visually grounded sub-word speech unit discovery. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019, pages 3017\u20133021. IEEE, 2019.\\n\\n[12] David Harwath, Wei-Ning Hsu, and James R. Glass. Learning hierarchical discrete linguistic units from visually-grounded speech. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\\n\\n[13] David Harwath, Adri`a Recasens, D`\u0131dac Sur`\u0131s, Galen Chuang, Antonio Torralba, and James R. Glass. Jointly discovering visual objects and spoken words from raw sensory input. Int. J. Comput. Vis., 128(3):620\u2013641, 2020.\\n\\n[14] David F. Harwath and James R. Glass. Deep multimodal semantic embeddings for speech and images. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2015, Scottsdale, AZ, USA, December 13-17, 2015, pages 237\u2013244. IEEE, 2015.\\n\\n[15] William N. Havard, Jean-Pierre Chevrot, and Laurent Bessacier. Word recognition, competition, and activation in a model of visually grounded speech. In Proceedings of the 23rd Conference on Computational Natural Language Learning, CoNLL 2019, Hong Kong, China, November 3-4, 2019, pages 339\u2013348. Association for Computational Linguistics, 2019.\\n\\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770\u2013778. IEEE Computer Society, 2016.\\n\\n[17] Bertrand Higy, Lieke Gelderloos, Afra Alishahi, and Grzegorz Chrupala. Discrete representations in neural models of spoken language. CoRR, abs/2105.05582, 2021.\\n\\n[18] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Segmentation from natural language expressions. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I, volume 9905 of Lecture Notes in Computer Science, pages 108\u2013124. Springer, 2016.\\n\\n[19] Ronghang Hu, Marcus Rohrbach, Subhashini Venugopalan, and Trevor Darrell. Utilizing large scale vision and text datasets for image segmentation from referring expressions. CoRR, abs/1608.08305, 2016.\\n\\n[20] Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, and Huchuan Lu. Bi-directional relationship inferring network for referring image segmentation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 4423\u20134432. IEEE, 2020.\\n\\n[21] Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, and Bo Li. Referring image understanding. CoRR, abs/1606.04659, 2016.\"}"}
{"id": "CVPR-2022-2056", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2022-2056", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian D. Reid, and Silvio Savarese. Generalized intersection over union: A metric and a loss for bounding box regression. In *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019*, Long Beach, CA, USA, June 16-20, 2019, pages 658\u2013666. Computer Vision Foundation / IEEE, 2019.\\n\\nRamon Sanabria, Austin Waters, and Jason Baldridge. Talk, don\u2019t write: A study of direct speech-based image retrieval. *CoRR*, abs/2104.01894, 2021.\\n\\nGayatri Saripalli, Priyank H. Prajapati, and Anand D. Darji. CSD optimized DWT filter for ECG denoising. In *2020 24th International Symposium on VLSI Design and Test (VDAT)*, Bhubaneswar, India, July 23-25, 2020, pages 1\u20136. IEEE, 2020.\\n\\nSteffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training for speech recognition. In *Interspeech 2019*, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019, pages 3465\u20133469. ISCA, 2019.\\n\\nSebastiaan Scholten, Danny Merkx, and Odette Scharenborg. Learning to recognise words using visually grounded speech. In *IEEE International Symposium on Circuits and Systems, ISCAS 2021*, Daegu, South Korea, May 22-28, 2021, pages 1\u20135. IEEE, 2021.\\n\\nSeonguk Seo, Joon-Young Lee, and Bohyung Han. URVOS: unified referring video object segmentation network with a large-scale benchmark. In *Computer Vision - ECCV 2020 - 16th European Conference*, Glasgow, UK, August 23-28, 2020, Proceedings, Part XV, volume 12360 of *Lecture Notes in Computer Science*, pages 208\u2013223. Springer, 2020.\\n\\nHengcan Shi, Hongliang Li, Fanman Meng, and Qingbo Wu. Key-word-aware network for referring expression image segmentation. In *Computer Vision - ECCV 2018 - 15th European Conference*, Munich, Germany, September 8-14, 2018, Proceedings, Part VI, volume 11210 of *Lecture Notes in Computer Science*, pages 38\u201354. Springer, 2018.\\n\\nHengcan Shi, Hongliang Li, Qingbo Wu, and King Ngan Ngan. Query reconstruction network for referring expression image segmentation. *IEEE Trans. Multim.*, 23:995\u20131007, 2021.\\n\\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. In *Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019*, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 6558\u20136569. Association for Computational Linguistics, 2019.\\n\\nNaveed ur Rehman, Ubaid ur Rehman, Syed Zain Abbas, Anum Asif, and Anum Javed. Translation invariant DWT based denoising using goodness of fit test. In *IEEE Statistical Signal Processing Workshop, SSP 2016*, Palma de Mallorca, Spain, June 26-29, 2016, pages 1\u20135. IEEE, 2016.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017*, December 4-9, 2017, Long Beach, CA, USA, pages 5998\u20136008, 2017.\\n\\nHao Wang, Cheng Deng, Fan Ma, and Yi Yang. Context modulated dynamic networks for actor and action video segmentation with language queries. In *The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020*, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 12152\u201312159. AAAI Press, 2020.\\n\\nHao Wang, Cheng Deng, Junchi Yan, and Dacheng Tao. Asymmetric cross-guided attention network for actor and action video segmentation from natural language query. In *2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019*, Seoul, Korea (South), October 27 - November 2, 2019, pages 3938\u20133947. IEEE, 2019.\\n\\nYuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021*, virtual, June 19-25, 2021, pages 8741\u20138750. Computer Vision Foundation / IEEE, 2021.\\n\\nYuxin Wu and Kaiming He. Group normalization. *Int. J. Comput. Vis.*, 128(3):742\u2013755, 2020.\\n\\nChenliang Xu, Shao-Hang Hsieh, Caiming Xiong, and Jason J. Corso. Can humans fly? action understanding with multiple classes of actors. In *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015*, Boston, MA, USA, June 7-12, 2015, pages 2264\u20132273. IEEE Computer Society, 2015.\\n\\nLinjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In *2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019*, Seoul, Korea (South), October 27 - November 2, 2019, pages 5187\u20135196. IEEE, 2019.\\n\\nXue Yang, Xiaojiang Yang, Jirui Yang, Qi Ming, Wentao Wang, Qi Tian, and Junchi Yan. Learning high-precision bounding box for rotated object detection via kullback-leibler divergence. *CoRR*, abs/2106.01883, 2021.\\n\\nLinwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-modal self-attention network for referring image segmentation. In *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019*, Long Beach, CA, USA, June 16-20, 2019, pages 10502\u201310511. Computer Vision Foundation / IEEE, 2019.\\n\\nLinwei Ye, Mrigank Rochan, Zhi Liu, Xiaoqin Zhang, and Yang Wang. Referring segmentation in images and videos with cross-modal self-attention network. *CoRR*, abs/2102.04762, 2021.\\n\\nLicheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L. Berg. Mattnet: Modular attention network for referring expression comprehension.\"}"}
{"id": "CVPR-2022-2056", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
