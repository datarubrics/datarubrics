{"id": "CVPR-2023-1167", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning Visual Representations via Language-Guided Sampling\\n\\nMohamed El Banani Karan Desai Justin Johnson\\nUniversity of Michigan\\n{mbanani,kdexd,justincj}@umich.edu\\n\\nAbstract\\n\\nAlthough an object may appear in numerous contexts, we often describe it in a limited number of ways. Language allows us to abstract away visual variation to represent and communicate concepts. Building on this intuition, we propose an alternative approach to visual representation learning: using language similarity to sample semantically similar image pairs for contrastive learning. Our approach diverges from image-based contrastive learning by sampling view pairs using language similarity instead of hand-crafted augmentations or learned clusters. Our approach also differs from image-text contrastive learning by relying on pre-trained language models to guide the learning rather than directly minimizing a cross-modal loss. Through a series of experiments, we show that language-guided learning yields better features than image-based and image-text representation learning approaches.\\n\\n1. Introduction\\n\\nConsider the images in Fig. 1, is the center image more similar to its left or right neighbor? Despite the difference in background and pose, it is clear that the right pair captures the same concept: a flying snow owl. Nevertheless, a self-supervised image model will judge the left pair as more similar. Human perception and language abstract away appearance differences to capture conceptual similarity rather than just visual similarity. Ideally, we could learn visual features that capture conceptual similarity and generalize effectively to other visual tasks. In this work, we show how language can be a proxy for conceptual similarity; allowing us to sample better pairs for contrastive learning and train more generalizable visual models.\\n\\nImage-only contrastive learning uses visual similarity as a proxy for conceptual similarity. This is based on the observation that discriminative approaches can discover inter-class similarity\u2014e.g., cheetahs are similar to lions\u2014without requiring explicit annotations [106]. The core idea is to train a discriminative model where each instance is treated as a separate class, and the model is trained to map augmented views of the same image to similar features [12\u201315,106]. While successful, instance discrimination ignores the similarity between different instances as it assumes all other images are unrelated. Later work focused on inter-image relationships by estimating clusters [3,9,10] or finding nearest neighbors [28]. However, those relationships are estimated using visual embeddings; resulting in visually, rather than conceptually, similar pairs.\\n\\nLanguage similarity is a strong proxy for semantic relationships. Consider the example in Fig. 1; images that depict the same concept are often described similarly. Radford et al. [76] propose language-image contrastive learning by mapping images and text to a shared representation space and achieve impressive generalization capabilities. However, it is unclear whether forcing models to map onto a shared space is optimal for visual learning. Although linguistic and visual similarity might align for similar instances, it is unclear whether all distances in one space should map exactly to the other. Instead of learning a joint vision-and-language representations, we argue that it is better to use linguistic similarity to guide visual learning.\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2023-1167", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To this end, we propose language-guided contrastive learning: a simple adaptation to contrastive learning that uses language models to find conceptually-similar image pairs for visual learning. Our approach is motivated by the observation that language models, despite never training on visual data, can still be used to sample caption pairs that belong to conceptually similar images, as seen in Fig. 2. Such sampled images exhibit desirable variations in pose, lightning, and context which are very different from hand-crafted augmentations which can be ill-suited to downstream tasks [108] or too focused on background textures [81]. We use the sampled pairs instead of image augmentations within standard self-supervised visual learning approaches such as SimCLR [12], SimSiam [15], and SLIP [67]. Our approach departs from image-only contrastive learning by relying on conceptually-similar image pairs rather than visually similar augmentations or cluster-assignment. We also depart from image-text pre-training by allowing the model to be guided by language similarity rather than learning a joint embedding space.\\n\\nWe conduct a series of controlled experiments to analyze our approach and compare it to commonly used representation learning paradigms on generalization to downstream classification tasks. In controlled settings, our approach outperforms all baselines on linear probe and few-shot classification on a range of downstream classification datasets. Our analysis suggests that while learning multi-modal joint embeddings can result in good representations, it is better to use one modality to guide the training of the other. Furthermore, we find that our approach is robust to the specific choice of sampling strategy or language model. Our code and pre-trained models are available at https://github.com/mbanani/lgssl.\\n\\n2. Related Work\\n\\nVisual Representation Learning aims to learn visual embedding spaces that capture semantics, with a typical focus on learning from scalable data sources. Broadly speaking, there are two general approaches: generative and discriminative. Generative approaches hypothesize that a model that can capture the image distribution will learn semantically relevant features [26, 31, 37, 70, 98, 115]. In contrast, discriminative approaches posit that differentiating between images will give rise to better features. This idea can be traced by to early work on metric learning [18] and dimensionality reduction [35], and is clearly seen for supervised classification models [84]. More recently, Wu et al. [106] proposed treating each image as a separate class and using augmented images as class instances to relieve the need for human annotation. This was followed by papers that simplified this approach [12 \u2013 14, 38] and proposed non-contrastive variants [15, 34]. While those approaches have been successful, the utility of augmentation-based self-supervised learning has been questioned [68, 108] with follow-up work proposing the use of objectness [66, 75] and saliency [81] to alleviate some of those concerns. While we share the goal of visual representation learning, we question the reliance on image augmentations for training and propose using language models to learn for conceptually-similar images.\\n\\nLanguage-supervised vision pre-training aims to learn visual representations from language data. Early work of Li et al. [57] trained n-gram models using YFCC [93] images and user-tag metadata. While some works learn joint vision-and-language representations for tasks like visual question answering [2, 33, 45, 118], visual reasoning [50, 89, 113], and retrieval [72, 112], we are interested in using language to learn better visual representations [23, 23, 76, 80, 88]. Early works used language modeling as a pretext task for visual learning [23, 80], but contrastive approaches quickly gained more popularity due to their relative simplicity and generalization capabilities [47, 76]. Follow-up work extended the contrastive formulation to learn dense features [109, 111] or used additional self-supervised losses to improve performance and data efficiency [21, 56, 59, 67]. While we share the motivation of using language for visual learning, we focus on learning visual representations by using linguistic guidance from pre-trained language models.\\n\\nLeveraging structure in the data. This is commonly done in dense feature learning, where optical flow [36, 46, 82, 101] or 3D transformations [29, 44, 83, 90, 105] provide natural associations between image patches. For images, prior approaches used class names [51, 79], class hierarchies [58, 110], meta data [32, 48, 57] or clustering [3, 9, 10, 94, 117] to improve learning and inference. Within contrastive learning, clustering has been a popular choice for leveraging dataset structure. The intuition is that natural clusters emerge in feature spaces that can provide an additional training signal or useful pseudo-labels. While such approaches work well on curated datasets (e.g., ImageNet) where the label set provides an estimate of the number of clusters, it struggles with imbalanced and uncurated data [4]. Other approaches sample nearest neighbors as a feature-driven within-domain augmentation [28, 59]. While these approaches differ in how they extract inter-instance relationships, they all use within-domain feature similarity to sample positive pairs or clusters and hence do not leverage the rich cross-modal relationships. Closest to our work is Han et al. [36] who propose a co-training [6] scheme for jointly learning image and optical flow representations. We share their motivation of using similarity in one space (language) to learn in another (vision). Furthermore, instead of relying on co-training on the same dataset, we extract distances from a text-only language model, allowing us to leverage unaligned data.\"}"}
{"id": "CVPR-2023-1167", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Supervised Visual Sampling\\n\\nFigure 2. Language sampling yields semantically-similar and visually-diverse image pairs. We sample the three nearest neighbors using a self-supervised visual model \\\\[13\\\\], an ImageNet supervised model \\\\[27\\\\], and a self-supervised language model \\\\[78\\\\]. While visual sampling yields visually similar pairs, language sampling yields semantically relevant and visually diverse images. We argue that the combination of semantic consistency and visual diversity are better for learning generalizable features.\\n\\n3. Method\\n\\nThe goal of this work is to learn visual representations that can generalize to other datasets. We extend image-only contrastive learning beyond hand-crafted augmentations and visually-sampled clusters to learn from conceptually similar images. Through learning to associate images that depict the same visual concept, models can learn visual invariances that more closely capture human semantics.\\n\\nTo achieve this, we propose sampling image pairs that have similar captions using a pre-trained sentence encoder \\\\[78\\\\] and using them for contrastive learning. This work does not propose a new model or loss but rather a novel way of sampling image views that is applicable to a variety of approaches and losses for learning visual representations.\\n\\n3.1. Learning from Conceptual Similarity\\n\\nInstance discrimination has been the dominant task for visual representation learning. Its core intuition is that visual similarity is a good proxy for semantic similarity. The standard approach generates positive view pairs using image augmentations and maximizes their embedding similarity, with or without negative views. While there has been a large number of contrastive learning approaches, view pair generation has largely remained the same. Other methods use visual feature similarity to learn prototypes \\\\[3, 9, 10\\\\] or sample previously seen instances \\\\[28\\\\] for contrastive learning. While these approaches extend beyond instances and consider relations in the dataset, they still rely on visual similarity to generate their contrastive pairs. This limits the visual invariances that they can learn \\\\[108\\\\].\\n\\nWe propose training models to identify the same visual concept instead of the same instance. Our key observation is simple: images that have similar captions often depict similar concepts regardless of the actual appearance similarity. This can be clearly seen in Fig. 2. Nearest neighbors in visual representation space depict objects in similar scenes and poses, with self-supervised models showing some color invariances due to color augmentation. Conversely, similarly captioned images depict objects in different colors, poses, and contexts. This makes language-sampled images an excellent source for visual representation learning as they implicitly capture human-like visual invariances.\\n\\n3.2. Sampling Image Pairs using Language\\n\\nGiven a captioned image dataset, we want to sample image pairs that have very similar captions. While caption similarity may be a good proxy for conceptual similarity, measuring caption similarity is a challenge on its own. Traditional metrics such as BLEU \\\\[71\\\\] and CIDER \\\\[96\\\\] rely on n-gram overlap, which can be too sensitive to phrasing and sentence structure. This makes them ill-suited for our needs. Other metrics such as SPICE \\\\[1\\\\] account for such variety by comparing parse trees; however, they still can not account for different wording choices. Inspired by advances in language models as well as approaches like BERTScore \\\\[116\\\\] and CLIPScore \\\\[43\\\\], we use a pre-trained sentence encoder to compute caption similarity.\\n\\nSentence encoders are trained to extract sentence-level features \\\\[52, 61, 78\\\\]. We use SBERT \\\\[78\\\\], which fine-tunes a pre-trained language model to allow it to better capture semantic similarity using feature cosine distance.\"}"}
{"id": "CVPR-2023-1167", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Contrasting Contrastive Formulations. While image-only and image-text contrastive learning directly extract views from the instance, nearest-neighbor methods rely on a memory bank of previously extracted features for training. In contrast, our approach samples nearest neighbors in caption embedding space using a pretrained language model and use the associated image for contrastive learning.\\n\\nSBERT is trained in two stages: first, a language backbone is trained using a standard self-supervised task such as masked [25, 65] or permuted [87] language modeling; second, the language modeled is fine-tuned via contrastive learning on a large combined dataset of 1 billion sentence pairs. Fine-tuning the model in a contrastive way simplifies downstream usage as it allows features to be compared directly using cosine similarity. We use an SBERT [78] model with an MPNet [87] backbone. However, we find that our formulation is not sensitive to the choice of language encoder, as shown in Tab. 5c.\\n\\nFinally, we sample the nearest neighbors for all captions in the language embedding space. We leverage modern similarity search libraries [49] to perform the nearest neighbor search quickly, despite the large dataset size. For example, nearest neighbor sampling runs in under 3 hours for RedCaps (12 million instances) on 4 GPUs, with 43 minutes spent on feature extraction and 117 minutes on nearest neighbor search. Furthermore, we find that we could further reduce the complexity of the sampling by only searching within subsets of the data as shown in Appendix E.\\n\\n3.3. Language-Guided Visual Learning\\n\\nOur approach is applicable to several representation learning methods as it only changes the training view pairs. We focus on contrastive learning since its fairly minimal setting allows us to analyze the impact of language guidance with minimal confounding factors. We train SimCLR with the language-sampled pairs and refer to it as LGSimCLR. We also evaluate the impact of language guidance on SimSiam [15] and SLIP [67], and find that they can similarly benefit from language guidance. We only use random cropping for image augmentations since language-sampled pairs are naturally augmented versions of each other and find that additional augmentations are not helpful. For LGSLIP, we match their setup by applying the CLIP loss only between the source's image and caption, ignoring an additional loss between the nearest neighbor image and its caption.\\n\\n4. Experiments\\n\\nOur experiments evaluate the efficacy of learning visual features from conceptually similar images. We hypothesize that a model trained with language guidance will learn useful visual invariances and better generalize to downstream tasks. We are interested in answering these questions: Does language guidance improve generalization over other pretraining approaches? Does language guidance generalize to other datasets and pre-training approaches? How can language be used for visual pre-training?\\n\\n4.1. Experimental Setup\\n\\nWe formulate our experimental setup to compare the efficacy of different learning signals. We train models with language-guided sampling and compare them with image-only self-supervised models and image-text contrastive models. We are interested in conducting controlled experiments for a fair comparison.\\n\\nRecent work in self-supervised learning has demonstrated the impressive impact of scaling [12, 76, 114]. While such work has shown impressive performance, it has complicated the evaluation as different models are trained on different pretext tasks on different datasets using varying amounts of compute and training recipes. Furthermore, replication is difficult, if not impossible, due to the unavailability of training data or prohibitive compute requirements. Fortunately, several papers report results that indicate that performance patterns often hold at smaller scales [10, 12, 21, 67, 76]. Hence, we conduct our experiments at a scale that allows us to perform a comprehensive evaluation and permits replication by others.\\n\\nWe conduct our experiments with a standard backbone [39] on publicly available datasets [11, 24, 85]. To account for variation in training recipes, we retrain all methods from scratch using the same training recipe. We scale down experiments to a level that permits fair comparisons and replication. We also provide system-level comparisons in Tab. 4 and scaling results in App. D.\"}"}
{"id": "CVPR-2023-1167", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Training details:\\nWe use a ResNet-50 backbone and train all models using the AdamW optimizer with a learning rate of $10^{-3}$ and a weight decay of $10^{-2}$. We use a cosine learning scheduler with 5000 warm-up steps. Models are trained using a batch size of 512 for 250k steps; this corresponds to 10.5 epochs on RedCaps. We use a constant number of steps to permit meaningful comparisons between models trained on different datasets.\\n\\nEvaluation setup:\\nWe evaluate all approaches using linear probe and fewshot classification on 15 classification datasets inspired by. We use the linear probe evaluation proposed by and learn a single linear layer using logistic regression. We sweep over a range of cost values and choose the value with the best validation performance. We retrain a classifier on both train and validation splits and report test performance. We also evaluate all approaches on fewshot classification to understand their generalization ability. We use a weighted kNN classifier on frozen support features inspired by prior work showing its effectiveness for fewshot classification. Please see Appendices A and B for more details on evaluation datasets and tasks.\\n\\nBaselines:\\nWhile there have been many proposed visual representation learning approaches, they can be grouped into several key directions that differ in the pretext task. We focus our comparison on a few representative approaches to explore the impact of the learning signal. We overview the baselines here and provide more details in Appendix C. Many of our baselines are variants of contrastive learning as shown in Fig. 3. Contrastive approaches operate over paired source and target feature embeddings: $z_s$ and $z_t$. The goal is to maximize the similarity between the paired embeddings and minimize it with respect to all other embeddings. Given a batch size $N$ and embedding dimension $F$, $z_s$, $z_t \\\\in \\\\mathbb{R}^{N \\\\times F}$. The contrastive loss is:\\n\\n$$L(z_s, z_t) = \\\\log \\\\exp(\\\\text{sim}(z_s, z_t) / \\\\tau) \\\\sum_{k=1}^{N} \\\\exp(\\\\text{sim}(z_s, z_t) / \\\\tau),$$\\n\\nwhere $\\\\tau$ is a scaling parameter and $\\\\text{sim}(\\\\cdot, \\\\cdot)$ is cosine similarity. Contrastive approaches primarily differ in how the embeddings are computed.\\n\\nImage-Only Contrastive Learning contrasts features extracted from two randomly augmented views of the same image to perform instance discrimination. We use SimCLR as a representative approach due to its simplicity and strong performance.\\n\\nImage-Text Contrastive Learning learns by contrasting features extracted from images and their captions. Unlike image-only approaches, this approach can learn semantics from the captions. Radford et al. first proposed this approach and has had several follow-ups that augment it with additional self-supervised losses. We use CLIP and SLIP due to their simplicity.\\n\\nNearest Neighbor Contrastive Learning contrasts source embeddings with retrieved embeddings from a memory bank. The target features are used to retrieve the nearest neighbor embedding from a memory bank of previous batches. Dwibedi et al. proposed this approach for image-only contrastive learning, while Li et al. proposed adapting this loss for language embeddings. We use NNCLR as Visual NNCLR and DeCLIP with the CLIP and the language NNS losses as Language NNCLR.\\n\\nImage-Only Non-Contrastive Learning deviates from the typical contrastive setup by learning without negative samples. We use SimSiam as a representative approach due to its simplicity and strong performance.\\n\\nCluster-based Contrastive Learning learn by contrasting image features with learned prototypes. Prototypes are estimated via clustering or learned jointly with the feature encoder. Caron et al. report that different cluster-based approaches perform similarly when provided with the same algorithmic advances. We use an adapted SwA V without the multi-crop augmentation strategy as it is equally applicable to other methods. We also compare against a pretrained SwA V checkpoint in Tab. 4.\\n\\n4.2. Results\\nWe train all approaches with a ResNet-50 backbone on RedCaps and report results in Tabs. 1 and 3. Our model outperforms all baselines with a significant margin for both evaluations. We analyze the results below through a series of questions.\\n\\nDoes language-guided sampling provide better training pairs than image augmentations? LGSimCLR greatly outperforms SimCLR despite using the same learning objective. By using language sampled pairs instead of image augmentations, LGSimCLR learns stronger invariances. We find that the largest gains arise in fine-grained datasets: Cars, CUB, and Food101. The performance gains can be explained by considering the critique of Xiao et al.: the training augmentations dictate the invariances learned by SimCLR as shown in nearest neighbor samples in Fig. 2. Consider the third row of Fig. 2, while language sampling depicts three Aston Martin cars in different spots, visual nearest neighbors are sports cars in different poses and colors, closely resembling the flip and color augmentations used for training. Similarly in the first row of Fig. 2, visual nearest neighbors depict owls from different species in similar poses, while language sampling retrieves three great horned owls from different viewpoints. These trends are further amplified when features are used directly for fewshot classification. Language guidance allows us to capture relationships that go beyond visual similarity by training on image pairs that capture human semantics.\"}"}
{"id": "CVPR-2023-1167", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Linear Probe Evaluations.\\n\\nWe train ResNet-50 models on RedCaps and report performance of a linear probe using frozen features on 15 downstream tasks. Models are split based on whether or not they require caption images for training. LGSimCLR outperforms all previous approaches with strong performance gains for fine-grained classification datasets.\\n\\n| Model          | Food-101 | CIFAR-10 | CIFAR-100 | CUB | SUN397 | Cars | Aircraft | DTD | Pets | Caltech-101 | Flowers | STL-10 | EuroSAT | RESISC45 | PCAM | Mean |\\n|----------------|----------|----------|-----------|-----|--------|------|----------|-----|------|-------------|---------|--------|---------|----------|------|------|\\n| SwA V          | 63.6     | 81.3     | 57.5      | 21.6| 47.5   | 22.9 | 35.4     | 68.1| 61.1 | 70.5        | 78.0    | 87.7   | 79.9    | 84.3     | 63.6 |      |\\n| SimSiam        | 64.1     | 79.9     | 56.1      | 28.2| 48.3   | 29.5 | 41.2     | 66.2| 69.1 | 73.6        | 83.6    | 85.7   | 94.4    | 82.1     | 65.7 |      |\\n| Visual NNCLR   | 65.4     | 82.8     | 60.2      | 26.6| 50.0   | 26.6 | 40.9     | 68.0| 65.2 | 75.4        | 83.5    | 88.5   | 95.3    | 82.2     | 66.3 |      |\\n| SimCLR         | 69.0     | 82.9     | 61.6      | 30.6| 52.6   | 33.7 | 43.7     | 69.8| 70.5 | 74.1        | 86.9    | 88.0   | 95.4    | 84.6     | 68.5 |      |\\n| Language NNCLR | 81.2     | 83.1     | 61.9      | 48.6| 56.5   | 45.1 | 37.2     | 68.8| 78.1 | 82.0        | 90.2    | 93.4   | 92.5    | 81.1     | 72.0 |      |\\n| CLIP           | 80.9     | 84.7     | 62.7      | 50.4| 57.4   | 45.8 | 36.7     | 67.6| 79.8 | 84.0        | 91.0    | 93.5   | 93.9    | 82.2     | 72.9 |      |\\n| SLIP           | 77.7     | 87.2     | 67.0      | 42.4| 58.1   | 48.7 | 45.2     | 72.3| 79.5 | 82.7        | 92.1    | 92.7   | 95.6    | 85.5     | 74.0 |      |\\n| LGSimCLR (Ours)| 83.2     | 87.8     | 69.0      | 59.3| 60.3   | 62.3 | 53.4     | 71.2| 81.8 | 89.4        | 95.9    | 94.0   | 95.6    | 88.0     | 78.2 |      |\\n\\nFigure 4. Nearest Neighbor methods are limited by the memory bank size. Even with a large memory bank, the nearest embedding can still be unrelated to the source image while language sampling provides us with conceptually similar pairs.\\n\\nCan we just sample nearest neighbors from previous batches? LGSimCLR outperforms NNCLR despite both relying on nearest neighbors. NNCLR uses the nearest feature embedding from a memory bank in the same modality. The quality of their retrieved samples is limited by the size of the memory bank. To demonstrate this, we visualize the nearest neighbors retrieved by NNCLR for different memory bank sizes in Fig. 4. We find that the retrieval quality is poor even for larger queues. Interestingly, we note that NNCLR also underperforms SimCLR on RedCaps, despite performing better on ImageNet. We posit that ImageNet\u2019s curated distribution explains this: a queue of 16k will most probably contain instances from each class, resulting in both visually and conceptually similar retrievals. Additionally, the quality of nearest neighbors is affected by the sampling feature space; features that are only trained on image augmentations will have limited invariances as shown in Fig. 2. We further explore the impact of sampling space on training in Sec. 4.3.\\n\\nTable 2. Language-guided contrastive learning outperforms image-text contrastive learning, regardless of text encoder.\\n\\n| Objective | Text Encoder | Linear Fewshot |\\n|-----------|--------------|----------------|\\n| Image-Text| Randomly-Initialized | 72.9 | 77.5 |\\n|           | Frozen SBERT   | 71.8 | 77.1 |\\n| Image-Image| Frozen CLIP (RedCaps) | 78.3 | 82.4 |\\n|           | Frozen SBERT   | 78.2 | 82.5 |\\n\\nCan cluster-based approaches learn better features? Similar to nearest-neighbor sampling, clustering is performed using visual similarity. Furthermore, it is based on an estimated number of clusters in the training dataset. Although this can be determined for ImageNet due to its known class structure, the number of clusters in an arbitrary uncurated dataset is unknown. This results in a large performance drop, as seen in Tab. 1 and Tab. 3. On the other hand, sampling related pairs assumes no global structure within the data and hence is able to better capture inter-instance similarity. This results in nearest-neighbor sampling outperforming clustering and both being outperformed by contrastive learning and language-guided contrastive learning.\\n\\nShould we use language for guidance or supervision? Our experiments indicate that LGSimCLR outperforms both CLIP and SLIP. We consider two possible explanations: (a) SBERT extracts better language embeddings than CLIP can learn from the data, or (b) language-guided contrastive learning is a better training objective than image-text contrastive learning. To evaluate this, we compare four models in Tab. 2. The first two models use CLIP\u2019s training objective: the first model uses a randomly initialized language encoder, similar to CLIP. The second model uses a frozen SBERT model as the language encoder and only trains the projection layers. The second two models use LGSimCLR\u2019s training objective but sample pairs using a\"}"}
{"id": "CVPR-2023-1167", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Few-Shot Evaluations.\\n\\nWe train ResNet-50 models on RedCaps and report 5-way, 5-shot classification performance. We observe that language results in huge performance gains as shown by the performance of CLIP and LGSimCLR. Furthermore, the use of any augmentations hurts performance as seen by SLIP's drop in performance.\\n\\n| Model                  | Food-101 | CIFAR-10 | CIFAR-100 | CUB | SUN397 | Cars | Aircraft | DTD | Pets | Caltech-101 | Flowers | STL-10 | EuroSAT | RESISC45 | Mean |\\n|------------------------|----------|----------|-----------|-----|--------|------|----------|-----|------|-------------|---------|--------|---------|----------|------|\\n| SwA V                  | 64.5     | 54.0     | 61.8      | 45.8| 84.9   | 36.5 | 34.1     | 74.8| 66.5 | 78.1        | 75.5    | 72.6   | 80.4    | 72.9     | 64.5 |\\n| SimSiam                | 63.9     | 49.9     | 57.2      | 49.5| 84.5   | 39.3 | 37.9     | 75.7| 67.8 | 79.7        | 81.5    | 69.6   | 80.6    | 79.4     | 65.5 |\\n| Visual NNCLR           | 65.6     | 54.1     | 61.7      | 45.8| 85.3   | 37.9 | 34.9     | 75.2| 67.3 | 81.1        | 75.4    | 74.3   | 83.6    | 76.7     | 65.6 |\\n| SimCLR                 | 66.9     | 45.7     | 51.0      | 51.5| 87.1   | 44.0 | 38.4     | 77.6| 70.1 | 80.0        | 86.9    | 69.6   | 83.5    | 81.3     | 66.7 |\\n| Language NNCLR         | 89.3     | 65.3     | 73.4      | 78.6| 90.8   | 68.4 | 40.4     | 75.2| 78.8 | 90.9        | 94.3    | 89.6   | 75.2    | 71.9     | 77.3 |\\n| CLIP                   | 88.9     | 64.6     | 73.1      | 78.3| 90.9   | 69.7 | 40.7     | 75.7| 77.5 | 91.6        | 94.7    | 89.8   | 75.3    | 74.8     | 77.5 |\\n| SLIP                   | 81.5     | 63.5     | 70.8      | 63.1| 91.3   | 62.9 | 42.1     | 79.6| 76.4 | 88.4        | 92.2    | 83.4   | 82.7    | 80.8     | 75.6 |\\n| LGSimCLR (Ours)        | 90.3     | 66.3     | 75.5      | 83.1| 92.7   | 77.6 | 50.6     | 81.1| 84.1 | 95.4        | 97.6    | 86.5   | 85.0    | 89.0     | 82.5 |\\n\\nWe find that image-image contrastive learning yields better visual features for both setups. While CLIP does not benefit from an SBERT backbone, LGSimCLR benefits from sampling using a language encoder trained on the same dataset. This suggests that learning joint embeddings results in worse visual features than language-guided learning.\\n\\nSystem-level comparisons: We compare LGSimCLR with publicly-available checkpoints of prior approaches; see Appendix C for details. We emphasize that while the experiments reported in Tabs. 1 and 3 were done in a controlled setup (same batch size, training data, optimizer), the system level comparisons are trained on different datasets with different training recipes and enhancements to further boost performance; e.g., large batch sizes, longer training, multi-crop augmentation. Furthermore, it has been shown that models trained on ImageNet implicitly benefit from its curated nature [4, 67]. Nevertheless, our approach still outperforms prior self-supervised approaches. We fall short of CLIP's ResNet-50 due to its training scale; 64\u21e5larger batch, 32\u21e5larger dataset, and 75\u21e5. We also observe that ImageNet-supervised ResNet-50 achieves better few-shot performance. Examining the performance breakdown in Tab. 10, we find the improvement mainly comes from CIFAR10, CIFAR100, and Pets. We posit that this can be explained by ImageNet's class structure: mostly pets with a large overlap with CIFAR's classes.\\n\\n4.3. Analysis\\n\\nWe now analyze language-guided contrastive learning by evaluating the impact of pre-training data, the choice of embedding space, and the pretext task. By understanding the impact of those choices, we can better understand what the model is learning.\\n\\nTable 4. ResNet-50 System Level Comparisons.\\n\\nWe outperform prior self-supervised approaches despite them benefiting from ImageNet's curation for training and using larger batch sizes. CLIP outperforms us due to the scale of its training.\\n\\n| Approach   | Batch # | Img Updates | Dataset | Linear Fewshot |\\n|------------|---------|-------------|---------|----------------|\\n| Supervised | [104]   | 1024        | ImageNet| 78.0           |\\n| SimSiam    | [15]    | 512         | ImageNet| 72.9           |\\n| SimCLR     | [13]    | 4096        | ImageNet| 75.4           |\\n| MoCo       | [16]    | 4096        | ImageNet| 77.7           |\\n| SwA V      | [10]    | 4096        | ImageNet| 78.2           |\\n| CLIP       | [76]    | 32768       | CLIP    | 81.8           |\\n| LGSimCLR   | 512     | 1.3\u21e510^8    | RedCaps | 78.2           |\\n\\nApproach generality: We extend language guidance to other contrastive approaches: SimSiam and SLIP. We observe that language guidance uniformly improves performance for all methods, as shown in Tab. 5a. Furthermore, the difference between SimCLR and SLIP shrinks when adding language guidance. This suggests that language guidance provides the model with similar semantics to the ones learned from an image-text contrastive loss, resulting in diminished gains from the additional image-text loss.\\n\\nImpact of training dataset: We train our model on four datasets: CC3M [85], CC12M [11], RedCaps-2020, and RedCaps [24]. In Tab. 5b, we observe that larger datasets result in stronger performance, indicating that our approach could scale well with even larger datasets. Furthermore, we observe that RedCaps results in better performance than Conceptual Captions. This may be attributed to the higher quality of captions in RedCaps; while the alt-text captions CC3M and CC12M can be short and contain image metadata, RedCaps captions are diverse, longer, and more descriptive. This allows our model to sample more interesting visual pairs that capture more visual diversity. We provide qualitative results in Appendix F to support this.\"}"}
{"id": "CVPR-2023-1167", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Analysis Experiments.\\n\\nWe conduct a series of analysis experiments to understand language-guided contrastive learning. The results indicate that language sampling is beneficial to several formulations and scales well with larger datasets. Furthermore, while language sampling consistently results in good pairs for training, visual sampling only helps if it has access to semantics through supervision.\\n\\nImpact of sampling space:\\nThe idea of using offline nearest-neighbor sampling does not require a specific language model or even a specific modality. We explore other choices for embedding space: four sentence encoders and two image models. In our experiments, we use SBERT\u2019s MPNet model [78, 87]; the highest performing SBERT model for sentence similarity. We compare it to two other sentence transformers: a smaller SBERT model, MiniLM [100], and the language encoder from CLIP [76]. We also compared against a bag-of-words (BoW) sentence encoder that uses FastText [164] embeddings. Results are in Tab. 5c. While we expected that using CLIP for sampling would improve performance due to its multimodal training, we were surprised that MiniLM also improved performance despite its lower performance on language tasks. We find that pairs obtained using a BoW model result in a weaker performance which might hint at the importance of contextual sentence embeddings. Nevertheless, the BoW-sampled pairs still result in higher performance than all the other baselines on RedCaps.\\n\\nWe also consider training with pairs sampled using two visual models: ImageNet-supervised ResNet-50 [104] and ImageNet-trained SimCLR [163]. We find that using a visual model for sampling is only beneficial if the visual model captures semantic relations; e.g., through supervised training. Using a self-supervised language model results in a strong drop in performance relative to the other sampling spaces. Nevertheless, it still allows the model to achieve better performance than using a self-supervised visual approach on the same data. This indicates that while language is a better modality to use, \u201csample-guided\u201d contrastive learning can still achieve a stronger performance than only using self-supervised learning.\\n\\nLimitations:\\nWe observe a few limitations in our approach. Image captions can be noisy, vague, and often omit obvious relations in the image [5]. While this broadly affects image-language models, it can result in us retrieving unrelated image pairs. For example, captions like \u201cI found this in the garden\u201d or \u201cPhoto from our family trip\u201d could describe a large range of images, some of which are unrelated. We expand on this in Appendix F. Image descriptions also depend on the context and the perceiver; e.g., a tourist and an art curator will describe artwork in very different ways. We observe that descriptions in topic-focused subreddits (e.g., r/birdpics and r/woodworking) are more specific than in generic subreddits (e.g., r/itookapicture and r/pics). Our experiments in Appendix E support this observation. Since a caption only captures one aspect of the image, sampled pairs can be similar for a variety of reasons. Allowing the model to condition the feature extraction or similarity calculation on captions could alleviate this issue.\\n\\n5. Conclusion\\nWe propose using language to find conceptually similar images for contrastive learning. This is based on a simple observation: people describe an object in similar ways even when it appears in different contexts. We use pre-trained language models to sample similar captions and use the captioned images for contrastive learning. We hypothesize that using language guidance instead of image augmentations would result in learning more human-like invariances. We evaluate our approach on multiple train and test datasets and find that it outperforms previous self-supervised and image-text contrastive models. Our analysis demonstrates the utility of using nearest-neighbor instances for training and the superiority of language sampling over other approaches for unlabeled datasets. Our findings align with prior work that critiques the use of image augmentations [81, 108] and shows the utility of cross-modal guidance [36] and intra-instance relationships [28, 51]. Our results demonstrate the potential of incorporating language guidance in contrastive learning. We hope that future work will explore scaling up our approach to larger and more diverse datasets, as well as modeling approaches that further integrate language into the learning process.\\n\\nAcknowledgments:\\nWe thank Richard Higgins, Ashkan Kazemi, and Santiago Castro for many helpful discussions, as well as David Fouhey, Ziyang Chen, Chenhao Zheng, Fahad Kamran, and Dan-dan Shan for their feedback on early drafts. This project was funded under the Ford-UM Alliance partnership; we thank Alireza Rahimpour, Devesh Upadhyay, and Ali Hassani from Ford Research for their support and discussion.\"}"}
{"id": "CVPR-2023-1167", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. SPICE: Semantic propositional image caption evaluation. In Proceedings of European Conference on Computer Vision (ECCV), 2016.\\n\\n[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In Proceedings of IEEE International Conference on Computer Vision (ICCV), 2015.\\n\\n[3] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In International Conference on Learning Representations, 2019.\\n\\n[4] Mido Assran, Randall Balestriero, Quentin Duval, Florian Bordes, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, and Nicolas Ballas. The hidden uniform cluster prior in self-supervised learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2023.\\n\\n[5] Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. Are elephants bigger than butterflies? reasoning about sizes of objects. In AAAI Conference on Artificial Intelligence, 2016.\\n\\n[6] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of Annual Conference on Computational learning theory, 1998.\\n\\n[7] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606, 2016.\\n\\n[8] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative components with random forests. In European Conference on Computer Vision, 2014.\\n\\n[9] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In Proceedings of the European conference on computer vision (ECCV), pages 132\u2013149, 2018.\\n\\n[10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, 2020.\\n\\n[11] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\n[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework for Contrastive Learning of Visual Representations. In Proceedings of the International Conference on Machine Learning (ICML), 2020.\\n\\n[13] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised models are strong semi-supervised learners. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, 2020.\\n\\n[14] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.\\n\\n[15] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\n[16] Xinlei Chen*, Saining Xie*, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021.\\n\\n[17] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105(10):1865\u20131883, Oct 2017.\\n\\n[18] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with application to face verification. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2005.\\n\\n[19] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.\\n\\n[20] Adam Coates, Andrew Ng, and Honglak Lee. An Analysis of Single Layer Networks in Unsupervised Feature Learning. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), 2011. https://cs.stanford.edu/~acoates/papers/coatesleengaistats2011.pdf.\\n\\n[21] Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, and Jing Shao. Democratizing contrastive language-image pre-training: A CLIP benchmark of data, model, and supervision. In ICML Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward, 2022.\\n\\n[22] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.\\n\\n[23] Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11162\u201311173, 2021.\\n\\n[24] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. RedCaps: Web-curated image-text data created by the people, for the people. In NeurIPS Datasets and Benchmarks, 2021.\\n\\n[25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics \u2013 Human Language Technologies (NAACL HLT), 2018.\\n\\n[26] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of IEEE International Conference on Computer Vision (ICCV), 2015.\"}"}
{"id": "CVPR-2023-1167", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.\\n\\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 9588\u20139597, October 2021.\\n\\nMohamed El Banani, Luya Gao, and Justin Johnson. Unsupervised r&r: Unsupervised point cloud registration via differentiable rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7129\u20137139, 2021.\\n\\nLi Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Pattern Recognition Workshop, 2004.\\n\\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.\\n\\nYunchao Gong, Qifa Ke, Michael Isard, and Svetlana Lazebnik. A multi-view embedding space for modeling internet images, tags, and their semantics. In IJCV, 2014.\\n\\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\nJean-Bastien Grill, Florian Strub, Florent Altch`e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent\u2014a new approach to self-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2006.\\n\\nTengda Han, Weidi Xie, and Andrew Zisserman. Self-supervised co-training for video representation learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.\\n\\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadam, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of IEEE International Conference on Computer Vision (ICCV), 2021.\\n\\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.\\n\\nJi Hou, Saining Xie, Benjamin Graham, Angela Dai, and Matthias Nie\u00dfner. Pri3d: Can 3d priors help 2d representation learning? In Proceedings of IEEE International Conference on Computer Vision (ICCV), 2021.\\n\\nDrew A Hudson and Christopher D Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nAllan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as a contrastive random walk. Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Proceedings of the International Conference on Machine Learning (ICML), 2021.\\n\\nJustin Johnson, Lamberto Ballan, and Li Fei-Fei. Love thy neighbors: Image annotation by exploiting image metadata. In Proceedings of IEEE International Conference on Computer Vision (ICCV), 2015.\\n\\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 2019.\\n\\nSahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.\\n\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning.\"}"}
{"id": "CVPR-2023-1167", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2023-1167", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2023-1167", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3733\u20133742, 2018.\\n\\nJ. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.\\n\\nTete Xiao, Xiaolong Wang, Alexei A Efros, and Trevor Darrell. What should not be contrastive in contrastive learning. In International Conference on Learning Representations, 2020.\\n\\nJiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nZhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis DeCoste, Wei Di, and Yizhou Yu. Hd-cnn: hierarchical deep convolutional neural networks for large scale visual recognition. In Proceedings of IEEE International Conference on Computer Vision (ICCV), 2015.\\n\\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. FILIP: Fine-grained interactive language-image pre-training. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.\\n\\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL, 2014.\\n\\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In Proceedings of European Conference on Computer Vision (ECCV), 2016.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.\\n\\nMingkai Zheng, Fei Wang, Shan You, Chen Qian, Changshui Zhang, Xiaogang Wang, and Chang Xu. Weakly supervised contrastive learning. In Proceedings of IEEE International Conference on Computer Vision (ICCV), 2021.\\n\\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\"}"}
