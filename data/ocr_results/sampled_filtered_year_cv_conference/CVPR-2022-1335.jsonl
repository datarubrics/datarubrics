{"id": "CVPR-2022-1335", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"One Loss for Quantization: Deep Hashing with Discrete Wasserstein Distributional Matching\\nKhoa D. Doan, Peng Yang, Ping Li\\nCognitive Computing Lab\\nBaidu Research\\n10900 NE 8th St. Bellevue, WA 98004, USA\\n{khoadoan106, pengyang01, pingli98}@gmail.com\\n\\nAbstract\\nImage hashing is a principled approximate nearest neighbor approach to find similar items to a query in a large collection of images. Hashing aims to learn a binary-output function that maps an image to a binary vector. For optimal retrieval performance, producing balanced hash codes with low-quantization error to bridge the gap between the learning stage's continuous relaxation and the inference stage's discrete quantization is important. However, in the existing deep supervised hashing methods, coding balance and low-quantization error are difficult to achieve and involve several losses. We argue that this is because the existing quantization approaches in these methods are heuristically constructed and not effective to achieve these objectives. This paper considers an alternative approach to learning the quantization constraints. The task of learning balanced codes with low quantization error is reformulated as matching the learned distribution of the continuous codes to a pre-defined discrete, uniform distribution. This is equivalent to minimizing the distance between two distributions. We then propose a computationally efficient distributional distance by leveraging the discrete property of the hash functions. This distributional distance is a valid distance and enjoys lower time and sample complexities. The proposed single-loss quantization objective can be integrated into any existing supervised hashing method to improve code balance and quantization error. Experiments confirm that the proposed approach substantially improves the performance of several representative hashing methods.\\n\\n1. Introduction\\nAn important challenge associated with massive image datasets is to efficiently and effectively search for images containing semantically similar content in these datasets. Hashing is a principled approximate nearest neighbor search approach with applications in many domains, ranging from text or image retrieval [21, 46] to spam or duplicate-scene detection [8, 37]. Hashing approaches learn binary encoding of the original images so that the \\\"candidate\\\" subset of images can be efficiently discovered from the binary-coding space. Binary codes are efficient to store and the high cost of pairwise distance calculations in the high-dimensional space is reduced to the significantly lower cost of discrete Hamming distance calculations. A Hamming-distance calculation only requires a bit-wise XOR and a bit-count operation, which can be efficiently computed in most conventional systems.\\n\\nTo ensure that the retrieved images are relevant, hashing methods learn hash functions that preserve the pairwise similarity of the images in the discrete space. Supervised hashing methods additionally leverage the annotated similarity to learn the hash functions and achieve superior retrieval performance compared to unsupervised hashing methods [4, 5, 18, 39, 47, 49, 53, 56, 59]. Since discrete optimization is intractable, these methods solve a relaxed problem that replaces the discrete constraint with a continuous output. The continuous output is \\\"quantized\\\" to obtain the binary during inference. Such a relaxation results in a discrepancy between the discrete and continuous optimizations that must be compensated for in the learning process. Two important criteria to consider are quantization error and coding balance [21, 51, 52]. Quantization error is the information loss when the discrete function is represented by a continuous function. Quantization error penalizes the cases of assigning \\\"very\\\" similar data points to binary codes with large distances [52]. Coding balance, on the other hand, encourages a uniform distribution of the images into the binary codes, which helps reduce the time complexity of the retrieval operations in the worst and average scenarios [23].\\n\\nExisting supervised hashing methods, especially those that are based on neural networks, include one or more penalty terms, besides the similarity-preserving loss, to force the continuous output as discrete as possible. However, these relaxation schemes still introduce non-trivial...\"}"}
{"id": "CVPR-2022-1335", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"quantization error and coding unbalanced, which eventually leads to sub-optimal hash codes [51, 52]. Multiple penalty terms also lead to longer model trainings due to the time-consuming hyperparameter-tuning step.\\n\\nThis paper proposes a faster and more performant quantization approach for the deep supervised hashing methods. First, we empirically show that low-quantization error and balance coding induce a uniform discrete distribution. The ultimate goal of hash-function learning is to project data into this uniform discrete distribution. Thus, we formulate the quantization objective as minimizing the distributional distance between the relaxed, continuous hash distribution and this uniform discrete distribution. The proposed formulation has two advantages: (i) achieving low-quantization error and coding balance is easier with this formulation and (ii) low-quantization error and coding balance are simultaneously optimized in a unified formulation, thus reducing the number of hyperparameters to tune. Our main contributions are as follows:\\n\\n\u2022 We achieve low quantization error and coding balance by minimizing the single-loss distributional distance between the learned hash distribution and the uniform discrete distribution. This new quantization objective can be used in conjunction with any existing deep supervised hashing methods to further improve their retrieval performances and reduce their time-consuming hyperparameter tuning processes.\\n\\n\u2022 We propose a low computation- and sample-complexity Sliced-Wasserstein-based distributional distance. The proposed distance, called HSWD, is theoretically a valid distance with better computational efficiency than other Wasserstein-distance types, including the original Sliced Wasserstein Distance.\\n\\n\u2022 We demonstrate the efficiency and effectiveness of the proposed quantization technique in several well-known deep supervised hashing methods on various widely used real-world datasets using both quantitative and qualitative performance analysis.\\n\\nPaper Organization: The rest of the paper is organized as follows. We review the related work in Section 2. In Section 3, we present the empirical analysis of the quantization error and coding balance, and the details of the proposed methodology. We evaluate the effectiveness of the proposed quantization approach in Section 4. Finally, Section 5 presents remarks and concludes this paper. We present more details about experimental settings and results as well as supporting proofs in the supplementary material.\\n\\n2. Related Work\\n\\nIn this section, we first discuss the prior works in deep supervised hashing and the quantization constraints. Then we discuss the Wasserstein distances, which are related to the computational approach proposed in the paper.\\n\\n2.1. Image Hashing\\n\\nHashing has been intensively investigated in both theory and practice, with applications ranging from retrieval [46] to compressed sensing [33] and feature learning [40]. In the image retrieval domain, existing data-dependent hashing methods can be organized into two categories: shallow hashing and deep hashing. Shallow hashing methods rely on hand-crafted features and learn linear hash functions and feature extraction techniques [21, 32, 43, 52]. Along with the rapid development of deep neural networks, deep hashing methods [6, 42, 54, 55] combine representation learning and hash-function learning into an end-to-end model and have demonstrated significant performance improvements over the hand-crafted shallow hashing approaches. Hashing methods can also be broadly classified into (data dependent) unsupervised [11,13,19,21,24,25,27,28,41,46,52,55] and supervised methods [4\u20136, 18, 39, 47, 53, 56, 57]. Supervised hashing methods demonstrate superior performance over the unsupervised ones by utilizing the labeled similarity information in the training data to learn the hash functions. Note that there is also a popular line of hashing works on \u201cdata independent\u201d (unsupervised) hashing with quantizations, including (i) quantized random projections [7, 20, 36, 38, 50]; (ii) quantized stable random projections [33]; (iii) quantized random Fourier features [40, 58]; (iv) b-bit minwise hashing [3, 35] and b-bit consistent weighted sampling [29, 34, 44], etc.\\n\\nSince learning the discrete hash function is computationally intractable, existing deep hashing methods approximate the discrete hash function with a continuous-output one, by replacing the discrete-output constraint with a \\\\text{tanh} activation [6, 39, 57, 59]. To learn the hash functions, these methods optimize the similarity-preserving loss and quantization loss, simultaneously. The similarity-preserving loss tries to preserve the input-space similarity structure in the discrete space while the quantization loss minimizes the discrepancy between the discrete and continuous optimizations. It has been shown that an effective quantization approach is crucial in improving the retrieval performance [16, 36, 52]. However, such quantization objective is often ignored or not effectively learned via multiple losses (>3) in the existing deep supervised hashing methods. This paper studies the effectiveness of quantization objectives in these approaches and proposes an alternative, more effective single-loss quantization objective. Note that, we consider our work to be orthogonal to OrthoHash [26] although OrthoHash also aims to reduce the number of losses (but only in the point-wise supervised image hashing). Our approach does not require Batch Normalization for coding balance as in OrthoHash, can be used in the point-wise or pair-wise...\"}"}
{"id": "CVPR-2022-1335", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"setting, and can be incorporated into any existing hashing methods to improve its quantization effectiveness.\\n\\n2.2. Wasserstein Distances\\n\\nWasserstein distances have been extensively studied in the literature and especially in recent applications such as GANs [2, 15, 22]. One advantage of the Wasserstein distances over other types of distributional distances, such as Kullback\u2013Leibler (KL) or Jensen-Shannon (JS) divergence, is that they consider the geometry of the data and are well-defined even for distributions without overlapping supports [1]. However, estimating the Wasserstein distance is a challenging task. Computing the Wasserstein-2 distance from its primal domain is computationally intractable [2]. Computing the Wasserstein-2 distance via the Optimal-Transport (OT) formulation has also been explored [14,16], but the OT's high computational cost is prohibitive. Employing the Kantorovich-Rubinstein dual requires estimating a Lipschitz continuous function to calculate the distance. Parameterizing the Lipschitz function with a neural network is challenging [9]. Furthermore, the dual approach results in a minimax optimization, which requires non-trivial modifications to learning algorithms such as those in hashing. Note that, estimating the KL or JS divergence also requires minimax optimization [17].\\n\\nWasserstein-2 distance (also KL and JS) requires an exponential number of samples to reliably estimate the distance [10]. In contrast, a variant of the Wasserstein distance, called Sliced Wasserstein Distance (SWD), approximates the Wasserstein distance by averaging the one-dimensional Wasserstein distances of the data points when they are projected onto many random, one-dimensional directions [30]. SWD has a polynomial sample complexity [9]. The SWD estimation has a computational cost of $O(LN \\\\log(nd))$, where $L$ is the number of random directions, $N$ is the number of samples, and $d$ is the dimension of the data. Nevertheless, in the high dimensional space, it becomes very likely that many random, one-dimensional directions do not lie on the manifold of the data. In other words, in several of these directions, the projected distances are close to zero. Consequently, in practice, the number of random directions $L$ is often large. For example, in [10], for a mini-batch size of 64, SWD needs $L = 10,000$ projections. To address this problem, some works find the best directions and estimate the Wasserstein distance using these directions [9, 12, 13].\\n\\n3. Distributional Matching for Hash Function Learning\\n\\n3.1. Hash Function Learning\\n\\nWe focus on the general supervised setting of learning a discrete hash function with a neural network. In this setting, a training dataset consists of $N$ images, $X = \\\\{x\\\\}_{N}$, and a corresponding labeling function $Y$. $Y$ can be pointwise (i.e., assigning labels to each image) or pair-wise (i.e., assigning a similarity score to a pair of images) and is predefined. The labeling function essentially describes the semantic similarity of the input. The goal of deep supervised hashing is to learn a discrete hash function $H: x \\\\rightarrow c$ that preserves the semantic similarity of the input. Without loss of generality, we consider the point-wise labeling case and the $\\\\theta$ is the parameter of the hash function. The optimization objective can be defined as:\\n\\n$$\\\\min_{\\\\theta} L_s (H(x), Y(x)), \\\\text{ s.t. } H(x) \\\\in \\\\{-1, 1\\\\}^m$$\\n\\nwhere $m$ is the number of bits. In this objective function, $L_s$ preserves the semantic similarity of the input, defined via $Y$. In practice, such a discrete optimization, especially when $H$ is a neural network, is intractable. To this end, the discrete $H$ is relaxed with a continuous function $h: x \\\\rightarrow [-1, 1]^m$ with a regularization. The general, continuously-relaxed objective function can be written as,\\n\\n$$\\\\min_{\\\\theta} L_s (h(x), Y(x)) + \\\\lambda L_q (h(x))$$\\n\\nwhere the $h(x)$ is usually modeled with a tanh activation function at the output layer. The term $L_q$ denotes the quantization objective, which minimizes the gap between the discrete and continuous solutions.\\n\\n3.2. Quantization and Code-balance\\n\\nThe quantization objective is very important for learning a hash function with a good retrieval performance [51, 52]. This objective typically includes three constraints: quantization error, bit balance, and bit uncorrelation. Low quantization error alleviates the loss that occurs in assigning \\\"very\\\" similar data points to the codes with large Hamming distances. Bit balance ensures that each bit has the same chance of being $-1$ or $1$, while bit uncorrelation encourages each bit to represent the orthogonal feature of the data. Together, bit-balance and bit-uncorrelation essentially characterize \\\"code-balance\\\", i.e., the condition where training data points are uniformly assigned to each hash code. Code balance is important because it helps minimize the time complexity of both the worst and average cases in retrieval and improve the retrieval quality [23].\\n\\nConsider Figure 1, where the data, from four classes, is projected on a two-dimensional space (Figure 1(a)). The goal is to learn a two-bit hash function. In Figure 1(c), since the bits are correlated, it is not possible to avoid the collision of data points from different classes in the discrete, hashing space. When the hash function has a high quantization error (Figure 1(d)), some similar data points near the boundary are likely to be assigned to two different hash codes, which results in a higher false-negative rate in retrieval. The optimal hash function, as seen in Figure 1(b), keeps data points from the same class close together, which results in a better retrieval performance.\"}"}
{"id": "CVPR-2022-1335", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. A visual illustration of the optimal function (Figure (b)) and learned hash function with poor code balance (Figure (c)) and quantization constraint (Figure (d)). Figure (a) shows the original data clusters from four classes.\\n\\n(a) HashNet (mAP: 0.7208)\\n\\n(b) HashNet-C (mAP: 0.8500)\\n\\n(c) CSQ (mAP: 0.8280)\\n\\n(d) CSQ-C (mAP: 0.8521)\\n\\nFigure 2. Visualization of the learned 2-bit hashing space (i.e. learn a 2-bit hash function) using the HashNet and DSDH methods. HashNet-C and DSDH-C replace the quantization in HashNet and DSDH, respectively, with the proposed HSWD quantization approach.\\n\\nPulls the data points toward the corners (low quantization error) and equally divided the data into four quadrants (code balance) while preserving the semantic similarity of the data (data from different classes occupy different quadrants).\\n\\nWhile the quantization objective is important, existing deep supervised approaches do not effectively learn this objective. Consider the case of learning a two-bit hash function on CIFAR-10 using data from four classes. Figures 2(a) and 2(c) show the quantization results of two representative supervised hashing methods, HashNet [6] and CSQ [57], respectively. HashNet suffers from high quantization error (unbounded continuous output) and bit correlation (several samples from class 1 and class 3 occupy the hash code (1, 1)). CSQ is slightly better than HashNet but cannot balance well between preserving semantic similarity and the quantization objective. Our proposed quantization constraint in HashNet-C and CSQ-C, which revises the corresponding quantization objective in HashNet and CSQ, respectively, learns hash codes with better balance and lower quantization error. This leads to improvements in the retrieval performance (from 0.7208 and 0.8280 to 0.8500 and 0.8521 for HashNet and CSQ, respectively).\\n\\nIn the next section, we will describe the proposed quantization objective, i.e., a single distributional-distance minimization between the learned continuous hash codes and a fixed, uniform discrete distribution. This approach can be easily adapted to existing methods and facilitates these methods to learn better hash functions. Our approach reduces the number of quantization hyperparameters to one, significantly reducing the training process.\\n\\n3.3. Distributional Quantization Distance\\n\\nAs discussed, following the uniform discrete distribution (Figure 1(b)) results in better coding balance and low quantization error. Under this distribution (denoted as $B$), a sample $b \\\\in \\\\{-1, 1\\\\}^m$ can be drawn as follows: for each dimension, independently and randomly samples a value of $-1$ or $1$ with equal probability. We can see that the samples from $B$ exhibit coding balance: each bit is balanced because its value has an equal chance of being $-1$ or $1$, and the bits are uncorrelated since each bit is sampled independently. We propose to minimize the discrepancy between the learned hash distribution and $B$ to improve the coding balance in the learned distribution. This approach also reduces the quantization error since the learned continuous codes will gradually become binary. Formally, we minimize the following quantization objective:\\n\\n$$L_q(h(X)) = D(h(X)||B) \\\\tag{3}$$\\n\\nwhere $D$ denotes the distributional distance function. Note that the constraint is applied on the output space of $h$ (thus, $h(X)$), instead on an individual sample ($h(x)$). Nonetheless...\"}"}
{"id": "CVPR-2022-1335", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"less, minimizing D is a non-trivial task, especially when the hashing space is high dimensional and the distribution density of the output space that we are learning cannot be estimated. Furthermore, for a choice of a distribution distance, its estimation should be efficient to compute to be used in practical applications.\\n\\nSome well-known divergences, such as Kullback\u2013Leibler (KL) or Jensen-Shannon (JS) divergence, generally require the ability to estimate the learned output distribution of $h(x)$. While we can use an additional network (i.e., similar to the discriminators in GANs [22]) to estimate KL (or JS), learning the neural network with the minimax optimization is computationally expensive. KL and JS are also not well-defined for distributions with non-overlapping supports [1]. To remedy this issue, we first consider the Wasserstein-2 distance. To improve the computation efficiency of the Wasserstein-2 distance, we propose a Sliced-based Wasserstein-2 distance that is more computationally efficient and effective to estimate. The quantization objective $D$ using the Wasserstein-2 distance can be formulated as follows:\\n\\n$$ D(\\\\mu, \\\\nu) = \\\\inf_{\\\\gamma \\\\in \\\\Pi(\\\\mu, \\\\nu)} \\\\int p(z, b) \\\\left| z - b \\\\right|^2 \\\\, dz \\\\, db $$\\n\\nwhere $\\\\Pi(\\\\mu, \\\\nu)$ is the set of all transportation plans $\\\\gamma$ such that their marginal distributions are $\\\\mu$ and $\\\\nu$. $\\\\mu$ and $\\\\nu$ define the output distribution of the continuous hash function $h$ and the distribution $B$, respectively. Computing the infimum in Equation (4) is difficult since the distribution induced by $z = h(x)$ is not fixed or unknown while employing the Kantorovich-Rubinstein duality to optimize Equation (4) requires modeling a Lipschitz function using an additional neural network [2]. Fortunately, Wasserstein distance has an elegant yet closed-form solution for one-dimensional continuous measures. Denoting $q_\\\\mu$ and $q_\\\\nu$ as the density functions of $\\\\mu$ and $\\\\nu$, respectively, the Wasserstein-2 distance between one-dimensional measures $\\\\mu$ and $\\\\nu$ is given by:\\n\\n$$ W(\\\\mu, \\\\nu) = \\\\int_{\\\\mathbb{R}} \\\\left| F^{-1}_\\\\mu(w) - F^{-1}_\\\\nu(w) \\\\right|^2 \\\\, dw $$\\n\\nwhere $F_\\\\mu(w) = \\\\int_{-\\\\infty}^w q_\\\\mu(\\\\rho) \\\\, d\\\\rho$ and $F_\\\\nu(w) = \\\\int_{-\\\\infty}^w q_\\\\nu(\\\\rho) \\\\, d\\\\rho$ are the cumulative distribution functions. Inspired by the efficiency of estimating the one-dimensional Wasserstein-2 distance, we propose to find a family of one-dimensional representations, e.g., through the linear projections, and approximate the Wasserstein-2 distance as a function of these one-dimensional marginals:\\n\\n$$ D(h(X), B) \\\\approx \\\\frac{1}{L} \\\\sum_{l=1}^{L} W(\\\\omega^T_l h(X), \\\\omega^T_l B) $$\\n\\nwhere $\\\\omega^T_l h(X)$ and $\\\\omega^T_l B$ are the projections of the output samples of $h$ and samples from $B$ onto a one-dimensional direction defined by $\\\\omega_l$ (a slice). Typically, $\\\\omega_l$ is drawn from a uniform distribution on the unit sphere. This is also known as the Sliced-Wasserstein distance (SWD) [10, 30]. While this approach has successful applications in a variety of tasks [10, 30], the random nature of the slices could lead to several non-informative directions where the sliced distances along these directions are close to 0. Consequently, a large number $L$ of random directions is needed to approximate the sliced-Wasserstein distance, which increases the computational complexity of the estimation.\\n\\nTo remedy this issue, we avoid the random projections and select directions that contain discriminant information of the two data sources. We utilize the observation that the objective $L_s$ aims to learn discriminative projections, via $h$, of the data points onto the discrete space. Intuitively, since each output dimension of the hash function describes a discriminative feature of the space, projecting the data into this dimension can capture meaningful separation of the two distributions. Empirically, we can show that the averaged one-dimensional Wasserstein distance along these dimensions, denoted as HSWD, captures better separations of the data than SWD. Specifically, we provide the estimated HSWD and SWD (using 10,000 projections in CIFAR-10) when the model is trained to minimize SWD (Figures 3(a) and 3(c)), or HSWD (Figures 3(b) and 3(d)) as the quantization objective. As we can observe, HSWD describes better separations of the two distributions (i.e., higher distances). Minimizing SWD does not effectively bring the two distributions closer in some cases (e.g., HSWD increases in Figure 3(a), indicating the separations even increase in some...\"}"}
{"id": "CVPR-2022-1335", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"directions). However, minimizing HSWD effectively forces $h(X)$ to be closer to $B$, as both HSWD and SWD decrease in Figures 3(b) and 3(d). More importantly, the number of projections, which is also the number of dimensions of $h(X)$, is fixed and generally small ($m \\\\leq 128$ in most applications). The one-dimensional distances along these dimensions are exactly the distances along the coordinate axis of the discrete space. Thus, HSWD is defined, as follows:\\n\\n$$D(h(X), B) \\\\approx \\\\sum_{l=1}^{m} [W(h(X)_l, :), B_l, :)^2]^{1/2} \\\\quad (7)$$\\n\\nwhere $h(X)_l, :$ and $B_l, :$ are the one-dimensional samples along the dimension $l$ of $h(X)$ and $B$, respectively. Note that, using the discrete dimensions as projected directions is similar to the works in [13] for the unsupervised study.\\n\\nWe can show that HSWD is a valid distance.\\n\\n**Theorem 1.** The proposed distance Sliced Wasserstein calculation in Theorem 1, denoted as HSWD, is a valid distance function of probability measures in this space.\\n\\n**Remark 1.** The proposed HSWD is similar to the Max Sliced Wasserstein Distance (M-SWD), which has successful applications [9, 30]. However, different from M-SWD, HSWD does not require a separate discriminative network to estimate the distance.\\n\\nHSWD enjoys a better computational efficiency than SWD ($O(mN \\\\log(Nd))$ compared to $O(LN \\\\log(Nd))$). In most problems, SWD requires a large number ($L \\\\gg N$) of random directions, typically between 1000 to 10,000, to provide a reliable estimate of the distance [9,45]. In HSWD, the number of directions is fixed to the dimension of the hashing space $m$, which is typically between 16 to 128 for many image hashing applications.\\n\\n### 4. Experiments\\n\\nWe present the experimental results to demonstrate the effectiveness of the proposed quantization method over the existing quantization approaches using extensive combinations of representative datasets, deep supervised hashing methods, and hash-function architectures.\\n\\n#### 4.1. Datasets Used\\n\\nWe utilize the following datasets:\\n\\n- **CIFAR10**: This dataset consists of 60,000 natural images categorized uniformly into 10 labels. We randomly select 100 and 500 images from each label for the query set and the training set, respectively. The remaining images form the retrieval set. Hence, there are 1000, 5000, and 54000 images in the query, training, and retrieval sets, respectively.\\n\\n- **NUS-WIDE**: This dataset contains 269,648 images, each of which belongs to at least one of 21 concepts. We randomly select 5000 images for the query set, with the remaining images used as the retrieval set. 10,000 images randomly selected in the retrieval set are used for training.\\n\\n- **COCO**: This dataset contains 123,287 images, labeled with at least 1 out of 80 semantic concepts. Similarly, the query set is randomly constructed with 5000 images, with the remaining images used as the retrieval set (10,000 randomly selected images in this set are used for training).\\n\\n#### 4.2. Evaluation Metrics\\n\\nFor evaluating the performance of the proposed model, we follow the standard evaluation mechanism that is widely accepted for the problem of image hashing - the precision@R (P@R) and mean average precision (mAP).\\n\\n#### 4.3. Comparison Methods\\n\\nWe compare the performance of the proposed quantization approach in various representative deep supervised image hashing methods: DSDH [39], HashNet [6], Greedy-Hash [49], DCH [5], CSQ [57], and DBDH [59]. Note that the performance comparisons between the methods are beyond the scope of this paper and we only focus on the performance improvement when the proposed quantization approach is incorporated in these methods.\\n\\nFor each method, report the retrieval performance using the original quantization proposal and our quantization approach using SWD (using the suffix -S) and HSWD (using the suffix -C). For example, for the CSQ method, we report CSQ (original algorithm), CSQ-S (CSQ with SWD quantization), and CSQ-C (CSQ with HSWD quantization).\\n\\n#### 4.4. Implementation Details\\n\\nFor a fair comparison, we use the same underlying deep hash function (e.g., VGG11) for the original algorithm and those with the proposed quantization approaches. We perform a hyperparameter selection step for each method and report the average performances for the best configuration across multiple runs with different random initializations.\\n\\n#### 4.5. Retrieval Performance\\n\\nIn this section, we measure the performance of the proposed quantization approach and the original quantization proposal for each selected deep supervised hashing method. Table 1 shows the mAP results in our experiments for learning the 16-bit, 32-bit, and 64-bit hash functions with the VGG11 backbone [48]. We also report the P@1000 results for learning the 16-bit and 32-bit hash functions with the AlexNet backbone [31] in Table 2. For each of the proposed\"}"}
{"id": "CVPR-2022-1335", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. mAP for different numbers of bits on the three image datasets. The blue value (in bold) along the mAP value of each proposed approach shows the relative improvement over the original algorithm, while the italicized value indicates no improvement.\\n\\n| Method    | CIFAR-10       | NUS-WIDE       | COCO          |\\n|-----------|----------------|----------------|---------------|\\n|           | 16 bits | 32 bits | 64 bits | 16 bits | 32 bits | 64 bits | 16 bits | 32 bits | 64 bits |\\n| DSDH      | 0.7909  | 0.8072  | 0.8278  | 0.8270  | 0.8455  | 0.8640  | 0.7331  | 0.7853  | 0.8074  |\\n| DSDH-S    | 0.8187  | 0.8439  | 0.8517  | 0.8282  | 0.8461  | 0.8712  | 0.7330  | 0.8030  | 0.8404  |\\n| DSDH-C    | 0.8531  | 0.8620  | 0.8658  | 0.8433  | 0.8631  | 0.8749  | 0.7424  | 0.8032  | 0.8408  |\\n| HashNet   | 0.6922  | 0.8311  | 0.8566  | 0.7728  | 0.8336  | 0.8654  | 0.6899  | 0.7666  | 0.8098  |\\n| HashNet-S | 0.8131  | 0.8573  | 0.8749  | 0.8062  | 0.8438  | 0.8713  | 0.7215  | 0.7764  | 0.8189  |\\n| HashNet-C | 0.7939  | 0.8467  | 0.8691  | 0.8002  | 0.8437  | 0.8791  | 0.7202  | 0.7789  | 0.8202  |\\n| GreedyHash| 0.8223  | 0.8474  | 0.8646  | 0.7802  | 0.8081  | 0.8328  | 0.6533  | 0.7219  | 0.7561  |\\n| GreedyHash-S| 0.8280 | 0.8497  | 0.8653  | 0.7815  | 0.8083  | 0.8390  | 0.6668  | 0.7291  | 0.7618  |\\n| GreedyHash-C| 0.8375 | 0.8536  | 0.8722  | 0.8159  | 0.8179  | 0.8477  | 0.6637  | 0.7299  | 0.7712  |\\n| DCH       | 0.8302  | 0.8432  | 0.8558  | 0.8015  | 0.8061  | 0.8040  | 0.7578  | 0.7792  | 0.7723  |\\n| DCH-S     | 0.8372  | 0.8515  | 0.8602  | 0.8058  | 0.8079  | 0.8067  | 0.7657  | 0.7831  | 0.7803  |\\n| DCH-C     | 0.8446  | 0.8596  | 0.8711  | 0.8159  | 0.8145  | 0.8155  | 0.7702  | 0.7892  | 0.7807  |\\n| CSQ       | 0.8069  | 0.8291  | 0.8366  | 0.7992  | 0.8384  | 0.8596  | 0.6783  | 0.7550  | 0.8146  |\\n| CSQ-S     | 0.8401  | 0.8555  | 0.8554  | 0.8044  | 0.8495  | 0.8626  | 0.7036  | 0.7765  | 0.8234  |\\n| CSQ-C     | 0.8457  | 0.8558  | 0.8652  | 0.8054  | 0.8511  | 0.8701  | 0.6989  | 0.7752  | 0.8255  |\\n| DBDH      | 0.7660  | 0.8223  | 0.8492  | 0.8305  | 0.8552  | 0.8666  | 0.7202  | 0.7826  | 0.8042  |\\n| DBDH-S    | 0.8458  | 0.8587  | 0.8603  | 0.8387  | 0.8577  | 0.8680  | 0.7461  | 0.7996  | 0.8336  |\\n| DBDH-C    | 0.8466  | 0.8593  | 0.8668  | 0.8395  | 0.8633  | 0.8760  | 0.7389  | 0.7889  | 0.8308  |\\n\\nTable 2. P@1000 for 16 and 32 bits on CIFAR-10 and NUS-WIDE. The blue value (in bold) along the P@1000 value of each proposed approach shows the relative improvement over the original algorithm, while the italicized value indicates no improvement.\\n\\n| Method    | CIFAR-10       | NUS-WIDE       |\\n|-----------|----------------|----------------|\\n|           | 16 bits | 32 bits | 16 bits | 32 bits |\\n| DSDH      | 0.8252  | 0.8406  | 0.8117  | 0.8294  |\\n| DSDH-S    | 0.8526  | 0.8543  | 0.8162  | 0.8312  |\\n| DSDH-C    | 0.8645  | 0.8739  | 0.8195  | 0.8391  |\\n| HashNet   | 0.6193  | 0.8613  | 0.7581  | 0.8158  |\\n| HashNet-S | 0.8470  | 0.8755  | 0.7743  | 0.8199  |\\n| HashNet-C | 0.7698  | 0.8715  | 0.7456  | 0.8078  |\\n| GreedyHash| 0.8561  | 0.8616  | 0.7601  | 0.8009  |\\n| GreedyHash-S| 0.8583 | 0.8656 | 0.7657 | 0.7973 |\\n| GreedyHash-C| 0.8517 | 0.8700 | 0.7630 | 0.7931 |\\n| DCH       | 0.8621  | 0.8568  | 0.7843  | 0.7898  |\\n| DCH-S     | 0.8622  | 0.8761  | 0.7846  | 0.7923  |\\n| DCH-C     | 0.8654  | 0.8635  | 0.7893  | 0.7914  |\\n| CSQ       | 0.8510  | 0.8571  | 0.7903  | 0.8285  |\\n| CSQ-S     | 0.8661  | 0.8732  | 0.8034  | 0.8318  |\\n| CSQ-C     | 0.8670  | 0.8688  | 0.8007  | 0.8353  |\\n| DBDH      | 0.8440  | 0.8421  | 0.8122  | 0.8323  |\\n| DBDH-S    | 0.8626  | 0.8675  | 0.8177  | 0.8388  |\\n| DBDH-C    | 0.8658  | 0.8731  | 0.8135  | 0.8380  |\\n\\nAs we can observe in Tables 1 and 2, the proposed quantization approaches achieve significant improvement in the retrieval results across different supervised hashing methods. Specifically, the SWD variants result in up to over 10% improvement in CIFAR-10, 4% in NUS-WIDE, and almost 5% in COCO. Similarly, the HSWD variants also consistently improve the retrieval results in these supervised hashing methods. As mentioned previously, the primary advantage of HSWD over SWD is its computational efficiency. We conjecture that the superior performance of the proposed approach is due to the following reasons:\\n\\n\u2022 The quantization constraints in the existing deep supervised hashing methods are less effective to minimize the gaps between the relaxed hash function $h(x)$ and the optimal, discrete hash function $H(x)$. The observation in Figure 2 also confirms our discussion.\\n\\n\u2022 The proposed quantization approaches can help learn better-quantized hash functions. Specifically, it is more effective to optimize the combination of the similarity preserving loss $L_s$ and $L_q$ when $L_q$ is one of our proposed approaches.\\n\\n\u2022 Finally, low-quantization error and code balance are crucial in improving the retrieval performance in the existing deep supervised hashing methods. Additional empirical results for this relationship are presented in Section 4.7.2.\\n\\n4.6. Computational Efficiency\\n\\nWe compare the training time per epoch of the proposed quantization approaches when learning 64-bit hash functions. For each method, we capture the average running time (in seconds) of the original algorithm and the algorithm with SWD- and HSWD-quantizations. For SWD, we report the corresponding running time of the number of projections that results in the optimal performance. We observe that the optimal number of projections ranges from 1000 to 10,000. Note that for HSWD, the number of projections is fixed and equals the size of the hash codes.\"}"}
{"id": "CVPR-2022-1335", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Averaged running time per epoch across different supervised hashing methods (in seconds). The blue values are the relative running-time decreases.\\n\\n| Dataset   | Original | SWD | HSWD |\\n|-----------|----------|-----|------|\\n| CIFAR-10  | 19.4     | 24.2| 17.1 |\\n| NUS-WIDE  | 58.3     | 71.2| 50.1 |\\n| COCO      | 55.6     | 68.1| 49.5 |\\n\\nTable 3 report the average running times across different deep hashing methods. We can observe that HSWD is more computationally efficient than SWD because of its fewer projections and the omission of the matrix-multiplication operation that projects the data points into random directions. Compared to the original quantizations, HSWD quantization is also faster since each original quantization (in $L^q$) usually comprises of several losses.\\n\\n4.7. Qualitative Analysis\\n\\n4.7.1 Hash Code Visualization\\n\\nFigure 4. Two-dimensional t-SNE visualizations of the quantized 16-bit hash codes learned by different quantization approaches in CSQ and DCH on CIFAR-10.\\n\\nWe present the visualization of the hash codes generated by the learned hash functions for the different quantization approaches, using the CSQ and DCH methods, in Figure 4. We project the hash codes into 2-dimensional embeddings using the t-SNE method. As Figure 4 displays, the learned hash codes of the methods with SWD and HSWD quantizations exhibit better inter-cluster separation and intra-cluster closeness, which makes their retrieval performance superior. For example, for the CSQ's result in Figure 4(a), we can observe that the samples from classes 2 and 3 are closer to each other. For CSQ-S and CSQ-C, the samples from these two classes are more separated.\\n\\n4.7.2 Analysis of Effective Quantization and Performance Improvement\\n\\n| CSQ   | HashNet | DBDH |\\n|-------|---------|------|\\n| CSQ-S | HashNet-S | DBDH-S | 0.82 |\\n| CSQ-C | HashNet-C | DBDH-C | 0.83 |\\n\\nFigure 5. Quantization Error (angle, in radian, between continuous codes and corresponding hash codes) and Bit Entropy (CIFAR10).\\n\\nThe proposed method replaces several quantization losses with a single loss (SWD/HSWD), allowing efficient and effective quantization in deep supervised hashing methods. In Figure 5, we show that the proposed quantization approaches simultaneously achieve low-quantization and balanced bits. As we can observe in this figure, when the quantization error is reduced (as in the proposed approaches), the retrieval performance increases. Furthermore, the proposed approaches also consistently achieve highly balanced bits (i.e., high bit entropies). Higher balanced bits also lead to better performance.\\n\\n5. Conclusions\\n\\nIn this paper, we proposed a novel single-loss quantization objective for the deep supervised image hashing problem. Different from the existing quantization approaches, our model selects an optimal uniform discrete distribution and directly minimizes the distribution distance between the output distribution of the hash function and this uniform distribution. We considered the Sliced Wasserstein Distance as the choice of the distributional distance for its low-sample complexity and easier estimation. However, the Sliced Wasserstein Distance can require projections into several random directions that do not contain useful information about the separation of the data. By studying the properties of the distributions under consideration, we proposed a variant of SWD, called HSWD, and showed that it only requires a small number of informative directions. HSWD can achieve significant computational gains than SWD. Our experiments validate that the proposed quantization approach can improve the performance of several representative deep supervised-hashing methods on several datasets. Our work makes one leap towards leveraging an efficient, robust quantization approach for deep supervised hashing and we envision that our model will serve as a motivation for improving other related hashing applications.\"}"}
{"id": "CVPR-2022-1335", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Mart\u00edn Arjovsky and L\u00e9on Bottou. Towards principled methods for training generative adversarial networks. In Proceedings of the 5th International Conference on Learning Representations (ICLR), Toulon, France, 2017. 3, 5\\n\\n[2] Mart\u00edn Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 214\u2013223, Sydney, Australia, 2017. 3, 5\\n\\n[3] Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. Syntactic clustering of the web. In Proceedings of the World Wide Web (WWW), pages 1157 \u2013 1166, Santa Clara, CA, 1997. 2\\n\\n[4] Yue Cao, Bin Liu, Mingsheng Long, and Jianmin Wang. Hashgan: Deep learning to hash with pair conditional wasserstein gan. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1287\u20131296, Salt Lake City, UT, 2018. 1, 2\\n\\n[5] Yue Cao, Mingsheng Long, Bin Liu, and Jianmin Wang. Deep cauchy hashing for hamming space retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1229\u20131237, Salt Lake City, UT, 2018. 1, 2, 6, 7\\n\\n[6] Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Philip S Yu. Hashnet: Deep learning to hash by continuation. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 5608\u20135617, Venice, Italy, 2017. 2, 4, 5, 6, 7\\n\\n[7] Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings on 34th Annual ACM Symposium on Theory of Computing (STOC), pages 380\u2013388, Montreal, Canada, 2002. 2\\n\\n[8] Ondrej Chum, James Philbin, and Andrew Zisserman. Near duplicate image detection: min-hash and tf-idf weighting. In Proceedings of the British Machine Vision Conference (BMVC), pages 1\u201310, 2008. 1\\n\\n[9] Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao, David A. Forsyth, and Alexander G. Schwing. Max-sliced wasserstein distance and its use for gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 10648\u201310656, Long Beach, CA, 2019. 3, 6\\n\\n[10] Ishan Deshpande, Ziyu Zhang, and Alexander G. Schwing. Generative modeling using the sliced wasserstein distance. In Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3483\u20133491, Salt Lake City, UT, 2018. 3, 5\\n\\n[11] Thanh-Toan Do, Anh-Dzung Doan, and Ngai-Man Cheung. Learning to hash with binary deep neural network. In Computer Vision - ECCV 2016 - Proceedings of the 14th European Conference on Computer Vision (ECCV), Part V, pages 219\u2013234, 2016. 2\\n\\n[12] Khoa D Doan, Yingjie Lao, and Ping Li. Backdoor attack with imperceptible input and latent modification. Advances in Neural Information Processing Systems (NeurIPS), 2021. 3\\n\\n[13] Khoa D Doan, Saurav Manchanda, Sarkhan Badirli, and Chandan K Reddy. Image hashing by minimizing discrete component-wise wasserstein distance. arXiv preprint arXiv:2003.00134, 2020. 2, 3, 6\\n\\n[14] Khoa D. Doan, Saurav Manchanda, Suchismit Mahapatra, and Chandan K. Reddy. Interpretable graph similarity computation via differentiable optimal alignment of node embeddings. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 665\u2013674, Virtual Event, Canada, 2021. 3\\n\\n[15] Khoa D Doan, Saurav Manchanda, Fengjiao Wang, Sathiya Keerthi, Avradeep Bhowmik, and Chandan K Reddy. Image generation via minimizing fr\u00e9chet distance in discriminator feature space. arXiv preprint arXiv:2003.11774, 2020. 3\\n\\n[16] Khoa D Doan and Chandan K Reddy. Efficient implicit unsupervised text hashing using adversarial autoencoder. In Proceedings of The Web Conference (WWW), pages 684\u2013694, Taipei, 2020. 2, 3\\n\\n[17] Khoa D Doan, Pranjul Yadav, and Chandan K Reddy. Adversarial factorization autoencoder for look-alike modeling. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 2803\u20132812, 2019. 3\\n\\n[18] Tiezheng Ge, Kaiming He, and Jian Sun. Graph cuts for supervised binary coding. In European Conference on Computer Vision (ECCV), pages 250\u2013264, Zurich, Switzerland, 2014. Springer. 1, 2\\n\\n[19] Kamran Ghasedi Dizaji, Feng Zheng, Najmeh Sadoughi, Yanhua Yang, Cheng Deng, and Heng Huang. Unsupervised deep generative adversarial hashing network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3664\u20133673, 2018. 2\\n\\n[20] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. J. ACM, 42(6):1115\u20131145, 1995. 2\\n\\n[21] Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval. IEEE Trans. Pattern Anal. Mach. Intell., 35(12):2916\u20132929, 2013. 1, 2\\n\\n[22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014. 3, 5\\n\\n[23] Junfeng He, Shih-Fu Chang, Regunathan Radhakrishnan, and Claus Bauer. Compact hashing with joint optimization of search accuracy and time. In CVPR 2011, pages 753\u2013760. IEEE, 2011. 1, 3\\n\\n[24] Kaiming He, Fang Wen, and Jian Sun. K-means hashing: An affinity-preserving quantization method for learning binary compact codes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2938\u20132945, 2013. 2\"}"}
{"id": "CVPR-2022-1335", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jae-Pil Heo, Youngwoon Lee, Junfeng He, Shih-Fu Chang, and Sung-Eui Yoon. Spherical hashing. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2957\u20132964. IEEE, 2012.\\n\\nJiun Tian Hoe, Kam Woh Ng, Tianyu Zhang, Chee Seng Chan, Yi-Zhe Song, and Tao Xiang. One loss for all: Deep hashing with a single cosine similarity based learning objective. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nChen Huang, Chen Change Loy, and Xiaoou Tang. Unsupervised learning of discriminative attributes and visual representations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5175\u20135184, 2016.\\n\\nShanshan Huang, Yichao Xiong, Ya Zhang, and Jia Wang. Unsupervised triplet hashing for fast image retrieval. In Proceedings of the on Thematic Workshops of ACM Multimedia 2017, pages 84\u201392. ACM, 2017.\\n\\nSergey Ioffe. Improved consistent sampling, weighted min-hash and L1 sketching. In The 10th IEEE International Conference on Data Mining (ICDM), pages 246\u2013255, Sydney, AU, 2010.\\n\\nSoheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized sliced wasserstein distances. Advances in Neural Information Processing Systems, 32:261\u2013272, 2019.\\n\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), volume 25, pages 1106\u20131114, Lake Tahoe, Nevada, United States, 2012.\\n\\nBrian Kulis and Trevor Darrell. Learning to hash with binary reconstructive embeddings. In Advances in Neural Information Processing Systems (NIPS), pages 1042\u20131050, Vancouver, British Columbia, Canada, 2009.\\n\\nPing Li. Binary and multi-bit coding for stable random projections. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1430\u20131438, Fort Lauderdale, FL, 2017.\\n\\nPing Li. Linearized GMM kernels and normalized random Fourier features. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 315\u2013324, 2017.\\n\\nPing Li and Arnd Christian K\u00f6nig. b-bit minwise hashing. In Proceedings of the 19th International Conference on World Wide Web (WWW), pages 671\u2013680, Raleigh, NC, 2010.\\n\\nPing Li, Michael Mitzenmacher, and Anshumali Shrivastava. Coding for random projections. In Proceedings of the 31th International Conference on Machine Learning (ICML), pages 676\u2013684, Beijing, China, 2014.\\n\\nPing Li, Anshumali Shrivastava, Joshua Moore, and Arnd Christian K\u00f6nig. Hashing algorithms for large-scale learning. In Advances in Neural Information Processing Systems (NIPS), pages 2672\u20132680, Granada, Spain, 2011.\\n\\nPing Li and Martin Slawski. Simple strategies for recovering inner products from coarsely quantized random projections. In Advances in Neural Information Processing Systems (NIPS), pages 4567\u20134576, Long Beach, CA, 2017.\\n\\nQi Li, Zhenan Sun, Ran He, and Tieniu Tan. Deep supervised discrete hashing. In Neural Information Processing Systems (NIPS), pages 2479\u20132488, Long Beach, CA, 2017.\\n\\nXiaoyun Li and Ping Li. One-sketch-for-all: Non-linear random features from compressed linear measurements. In Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 2647\u20132655, Virtual Event, 2021.\\n\\nKevin Lin, Jiwen Lu, Chu-Song Chen, and Jie Zhou. Learning compact binary descriptors with unsupervised deep neural networks. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1183\u20131192, Las Vegas, NV, 2016.\\n\\nKevin Lin, Huei-Fang Yang, Jen-Hao Hsiao, and Chu-Song Chen. Deep learning of binary hash codes for fast image retrieval. In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 27\u201335, Boston, MA, 2015.\\n\\nWei Liu, Jun Wang, Rongrong Ji, Yu-Gang Jiang, and Shih-Fu Chang. Supervised hashing with kernels. In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2074\u20132081, Providence, RI, 2012.\\n\\nMark Manasse, Frank McSherry, and Kunal Talwar. Consistent weighted sampling. Technical Report MSR-TR-2010-73, Microsoft Research, 2010.\\n\\nKhai Nguyen, Nhat Ho, Tung Pham, and Hung Bui. Distributational sliced-wasserstein and applications to generative modeling. In Proceedings of the 9th International Conference on Learning Representations (ICLR), Virtual Event, Austria, 2021.\\n\\nRuslan Salakhutdinov and Geoffrey Hinton. Semantic hashing. International Journal of Approximate Reasoning, 50(7):969\u2013978, 2009.\\n\\nFumin Shen, Chunhua Shen, Wei Liu, and Heng Tao Shen. Supervised discrete hashing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 37\u201345, Boston, MA, 2015.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), San Diego, CA, 2015.\\n\\nShupeng Su, Chao Zhang, Kai Han, and Yonghong Tian. Greedy hash: Towards fast optimization for accurate hash coding in CNN. In Advances in Neural Information Processing Systems (NeurIPS), pages 806\u2013815, Montr\u00e9al, Canada, 2018.\\n\\nSantosh S. Vempala. The Random Projection Method. American Mathematical Society, 2004.\\n\\nJingdong Wang, Ting Zhang, Jingkuan Song, Nicu Sebe, and Heng Tao Shen. A survey on learning to hash. IEEE Trans. Pattern Anal. Mach. Intell., 40(4):769\u2013790, 2018.\\n\\nYair Weiss, Antonio Torralba, and Robert Fergus. Spectral hashing. In Advances in neural information processing systems (NIPS), pages 1753\u20131760, Vancouver, Canada, 2008.\"}"}
{"id": "CVPR-2022-1335", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rongkai Xia, Yan Pan, Hanjiang Lai, Cong Liu, and Shuicheng Yan. Supervised hashing for image retrieval via image representation learning. In Carla E. Brodley and Peter Stone, editors, Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence (AAAI), pages 2156\u20132162, Qu\u00b4ebec City, Canada, 2014.\\n\\nErkun Yang, Cheng Deng, Tongliang Liu, Wei Liu, and Dacheng Tao. Semantic structure-based unsupervised deep hashing. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI), pages 1064\u20131070, Stockholm, Sweden, 2018.\\n\\nErkun Yang, Tongliang Liu, Cheng Deng, Wei Liu, and Dacheng Tao. Distillhash: Unsupervised deep hashing by distilling data pairs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2946\u20132955, Long Beach, CA, 2019.\\n\\nHuei-Fang Yang, Kevin Lin, and Chu-Song Chen. Supervised learning of semantics-preserving hash via deep convolutional neural networks. IEEE Trans. Pattern Anal. Mach. Intell., 40(2):437\u2013451, 2018.\\n\\nLi Yuan, Tao Wang, Xiaopeng Zhang, Francis EH Tay, Zequn Jie, Wei Liu, and Jiashi Feng. Central similarity quantization for efficient image and video retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3083\u20133092, Seattle, WA, USA, 2020.\\n\\nJian Zhang, Avner May, Tri Dao, and Christopher R \u00b4e. Low-precision random fourier features for memory-constrained kernel approximation. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1264\u20131274, Naha, Okinawa, Japan, 2019.\\n\\nXiangtao Zheng, Yichao Zhang, and Xiaoqiang Lu. Deep balanced discrete hashing for image retrieval. Neurocomputing, 403:224\u2013236, 2020.\"}"}
