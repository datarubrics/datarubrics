{"id": "CVPR-2023-2182", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo Garcia-Hernando, Aron Monszpart, Victor Adrian Prisacariu, Daniyar Turmukhambetov, and Eric Brachmann. Map-free visual relocalization: Metric pose relative to a single image. Proceedings of the European Conference on Computer Vision (ECCV), 2022. 2, 5, 6, 7\\n\\n[2] Daniel Barath, Luca Cavalli, and Marc Pollefeys. Learning to find good models in RANSAC. In CVPR, pages 15744\u201315753, 2022. 1, 2, 5, 8\\n\\n[3] Daniel Barath and Jiri Matas. Graph-cut RANSAC. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6733\u20136741, 2018. 2\\n\\n[4] Daniel Barath, Jiri Matas, and Jana Noskova. MAGSAC: marginalizing sample consensus. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10197\u201310205, 2019. 2\\n\\n[5] Daniel Barath, Jana Noskova, Maksym Ivashechkin, and Jiri Matas. MAGSAC++, a fast, reliable and accurate robust estimator. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1304\u20131312, 2020. 1, 2, 3, 6, 7, 8\\n\\n[6] Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, and Krystian Mikolajczyk. Key.Net: Keypoint detection by handcrafted and learned CNN filters. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5836\u20135844, 2019. 2\\n\\n[7] Axel Barroso-Laguna, Yurun Tian, and Krystian Mikolajczyk. ScaleNet: A shallow architecture for scale estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12808\u201312818, 2022. 2\\n\\n[8] Axel Barroso-Laguna, Yannick Verdie, Benjamin Busam, and Krystian Mikolajczyk. HDD-Net: Hybrid detector descriptor with mutual interactive learning. In Proceedings of the Asian Conference on Computer Vision, 2020. 2\\n\\n[9] Eric Brachmann, Martin Humenberger, Carsten Rother, and Torsten Sattler. On the limits of pseudo ground truth in visual camera re-localisation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6218\u20136228, 2021. 8\\n\\n[10] Eric Brachmann and Carsten Rother. Neural-guided RANSAC: Learning where to sample model hypotheses. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4322\u20134331, 2019. 2\\n\\n[11] Ruojin Cai, Bharath Hariharan, Noah Snavely, and Hadar Averbuch-Elor. Extreme rotation estimation using dense correlation volumes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14566\u201314575, 2021. 2, 5\\n\\n[12] Luca Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten Sattler, and Marc Pollefeys. AdaLAM: Revisiting handcrafted outlier detection. arXiv preprint arXiv:2006.04250, 2020. 2\\n\\n[13] Luca Cavalli, Marc Pollefeys, and Daniel Barath. NeFSAC: Neurally filtered minimal samples. In ECCV, 2022. 1, 2\\n\\n[14] Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang Bai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learning to match features with seeded graph matching network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6301\u20136310, 2021. 2\\n\\n[15] Ondrej Chum and Jiri Matas. Matching with PROSAC\u2014progressive sample consensus. In 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05), volume 1, pages 220\u2013226. IEEE, 2005. 1, 2\\n\\n[16] Ondrej Chum, Ji\u0159\u00ed Matas, and Josef Kittler. Locally optimized RANSAC. In Joint Pattern Recognition Symposium, pages 236\u2013243. Springer, 2003. 2\\n\\n[17] Ondrej Chum, Tomas Werner, and Jiri Matas. Two-view geometry estimation unaffected by a dominant plane. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), volume 1, pages 772\u2013779. IEEE, 2005. 2\\n\\n[18] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. ScanNet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828\u20135839, 2017. 3, 5, 7\\n\\n[19] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperPoint: Self-supervised interest point detection and description. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 224\u2013236, 2018. 1, 2, 3, 5, 6\\n\\n[20] Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and Xiao Liu. TransMVS-Net: Global context-aware multi-view stereo network with transformers. In CVPR, pages 8585\u20138594, 2022. 2\\n\\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2\\n\\n[22] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-Net: A trainable CNN for joint detection and description of local features. arXiv preprint arXiv:1905.03561, 2019. 2\\n\\n[23] Hongyi Fan, Joe Kileel, and Benjamin Kimia. On the instability of relative pose estimation and RANSAC's role. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8935\u20138943, 2022. 1, 2\\n\\n[24] Mohammed E Fathy, Ashraf S Hussein, and Mohammed F Tolba. Fundamental matrix estimation: A study of error criteria. Pattern Recognition Letters, 32(2):383\u2013391, 2011. 4\\n\\n[25] Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381\u2013395, 1981. 1, 6, 7\\n\\n[26] Debabrata Ghosh and Naima Kaabouch. A survey on image mosaicing techniques. Journal of Visual Communication and Image Representation, 34:1\u201311, 2016. 1\\n\\n[27] Vitor Guizilini, Rares Ambrus, Dian Chen, Sergey Zakharov, and Adrien Gaidon. Multi-frame self-supervised depth with 8987\"}"}
{"id": "CVPR-2023-2182", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. 4, 6\\n\\n[29] Yihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu. Epipolar transformers. In CVPR, pages 7779\u20137788, 2020. 2, 3\\n\\n[30] Maksym Ivashechkin, Daniel Barath, and Ji\u0159\u00ed Matas. VSAC: Efficient and accurate estimator for h and f. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15243\u201315252, 2021. 2\\n\\n[31] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, and Kwang Moo Yi. COTR: Correspondence transformer for matching across images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6207\u20136217, 2021. 2\\n\\n[32] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas, Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Image matching across wide baselines: From paper to practice. International Journal of Computer Vision, 129(2):517\u2013547, 2021. 2, 3, 6\\n\\n[33] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156\u20135165. PMLR, 2020. 5\\n\\n[34] Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding, Francis X Creighton, Russell H Taylor, and Mathias Unberath. Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers. In ICCV, pages 6197\u20136206, 2021. 2, 5\\n\\n[35] Zhengqi Li and Noah Snavely. MegaDepth: Learning single-view depth prediction from internet photos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2041\u20132050, 2018. 3, 5\\n\\n[36] David G Lowe. Object recognition from local scale-invariant features. In Proceedings of the seventh IEEE international conference on computer vision, volume 2, pages 1150\u20131157. Ieee, 1999. 1, 2\\n\\n[37] Jiri Matas, Ondrej Chum, Martin Urban, and Tom\u00e1\u0161 Pajdla. Robust wide-baseline stereo from maximally stable extremal regions. Image and vision computing, 22(10):761\u2013767, 2004. 1\\n\\n[38] Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Repeatability is not enough: Learning affine regions via discriminability. In Proceedings of the European Conference on Computer Vision (ECCV), pages 284\u2013300, 2018. 2\\n\\n[39] Jorge J Mor\u00e9. The levenberg-marquardt algorithm: implementation and theory. In Numerical analysis, pages 105\u2013116. Springer, 1978. 6\\n\\n[40] Omid Poursaeed, Guandao Yang, Aditya Prakash, Qiuren Fang, Hanqing Jiang, Bharath Hariharan, and Serge Belongie. Deep fundamental matrix estimation without correspondences. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pages 0\u20130, 2018. 2\\n\\n[41] Rahul Raguram, Ondrej Chum, Marc Pollefeys, Ji\u0159\u00ed Matas, and Jan-Michael Frahm. USAC: A universal framework for random sample consensus. IEEE transactions on pattern analysis and machine intelligence, 35(8):2022\u20132038, 2012. 1, 6\\n\\n[42] Ren\u00e9 Ranftl and Vladlen Koltun. Deep fundamental matrix estimation. In Proceedings of the European conference on computer vision (ECCV), pages 284\u2013299, 2018. 2\\n\\n[43] Jerome Revaud, Philippe Weinzaepfel, C\u00e9sar De Souza, Noe Pion, Gabriela Csurka, Yohann Cabon, and Martin Humenberger. R2D2: repeatable and reliable detector and descriptor. arXiv preprint arXiv:1906.06195, 2019. 2\\n\\n[44] Chris Rockwell, Justin Johnson, and David F. Fouhey. The 8-point algorithm as an inductive bias for relative pose prediction by vitS. In T3DV, 2022. 2\\n\\n[45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015. 4\\n\\n[46] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4938\u20134947, 2020. 1, 2, 3, 5, 6\\n\\n[47] Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Efficient & effective prioritized matching for large-scale image-based localization. IEEE transactions on pattern analysis and machine intelligence, 39(9):1744\u20131756, 2016. 1\\n\\n[48] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4104\u20134113, 2016. 1, 2\\n\\n[49] Charles V. Stewart. MINPRAN: A new robust estimator for computer vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(10):925\u2013938, 1995. 2\\n\\n[50] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021. 1, 2, 3, 4, 5, 6, 8\\n\\n[51] Weiwei Sun, Wei Jiang, Eduard Trulls, Andrea Tagliasacchi, and Kwang Moo Yi. ACNe: Attentive context normalization for robust permutation-equivariant learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11286\u201311295, 2020. 2\\n\\n[52] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. YFCC100M: The new data in multimedia research. Communications of the ACM, 59(2):64\u201373, 2016. 6\\n\\n[53] Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Balntas, and Krystian Mikolajczyk. HyNet: Learning local descriptor with hybrid similarity measure and triplet loss. Advances in Neural Information Processing Systems, 33:7401\u20137412, 2020. 2\\n\\n[54] Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen, and Vassileios Balntas. SOSNet: Second order similarity. Advances in Neural Information Processing Systems, 33:7401\u20137412, 2020. 2\"}"}
{"id": "CVPR-2023-2182", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"regularization for local descriptor learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11016\u201311025, 2019.\\n\\n[55] Wei Tong, Jiri Matas, and Daniel Barath. Deep MAGSAC++. arXiv preprint arXiv:2111.14093, 2021.\\n\\n[56] Philip HS Torr and David W Murray. Outlier detection and motion segmentation. In Sensor Fusion VI, volume 2059, pages 432\u2013443. SPIE, 1993.\\n\\n[57] Philip Hilaire Torr, Slawomir J Nasuto, and John Mark Bishop. NAPSAC: High noise, high dimensional robust estimation\u2014it's in the bag. In British Machine Vision Conference (BMVC), volume 2, page 3, 2002.\\n\\n[58] Philip HS Torr and Andrew Zisserman. MLESAC: A new robust estimator with application to estimating image geometry. Computer vision and image understanding, 78(1):138\u2013156, 2000.\\n\\n[59] Philip H. S. Torr. Bayesian model estimation and selection for epipolar geometry and generic manifold fitting. International Journal of Computer Vision, 50(1):35\u201361, 2002.\\n\\n[60] Prune Truong, Martin Danelljan, Luc Van Gool, and Radu Timofte. Learning accurate dense correspondences and when to trust them. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5714\u20135724, 2021.\\n\\n[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\n[62] Yannick Verdie, Kwang Yi, Pascal Fua, and Vincent Lepetit. TILDE: A temporally invariant learned detector. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5279\u20135288, 2015.\\n\\n[63] Xiaofeng Wang, Zheng Zhu, Fangbo Qin, Yun Ye, Guan Huang, Xu Chi, Yijia He, and Xingang Wang. MVSTER: Epipolar transformer for efficient multi-view stereo. arXiv preprint arXiv:2204.07346, 2022.\\n\\n[64] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit, Mathieu Salzmann, and Pascal Fua. Learning to find good correspondences. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2666\u20132674, 2018.\\n\\n[65] Kwang Moo Yi, Yannick Verdie, Pascal Fua, and Vincent Lepetit. Learning to assign orientations to feature points. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 107\u2013116, 2016.\\n\\n[66] Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou, Tianwei Shen, Yurong Chen, Long Quan, and Hongen Liao. Learning two-view correspondences and geometry using order-aware network. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5845\u20135854, 2019.\\n\\n[67] Chen Zhao, Zhiguo Cao, Chi Li, Xin Li, and Jiaqi Yang. NM-Net: Mining reliable neighbors for robust feature correspondences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 215\u2013224, 2019.\\n\\n[68] Qunjie Zhou, Torsten Sattler, Marc Pollefeys, and Laura Leal-Taixe. To learn or not to learn: Visual localization from essential matrices. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 3319\u20133326. IEEE, 2020.\"}"}
{"id": "CVPR-2023-2182", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Two-view Geometry Scoring Without Correspondences\\n\\nAxel Barroso-Laguna\\nEric Brachmann\\nVictor Adrian Prisacariu\\nGabriel Brostow\\nDaniyar Turmukhambetov\\n\\n1 Niantic\\n2 University of Oxford\\n3 University College London\\n\\nwww.github.com/nianticlabs/scoring-without-correspondences\\n\\nAbstract\\n\\nCamera pose estimation for two-view geometry traditionally relies on RANSAC. Normally, a multitude of image correspondences leads to a pool of proposed hypotheses, which are then scored to find a winning model. The inlier count is generally regarded as a reliable indicator of \\\"consensus\\\". We examine this scoring heuristic, and find that it favors disappointing models under certain circumstances.\\n\\nAs a remedy, we propose the Fundamental Scoring Network (FSNet), which infers a score for a pair of overlapping images and any proposed fundamental matrix. It does not rely on sparse correspondences, but rather embodies a two-view geometry model through an epipolar attention mechanism that predicts the pose error of the two images. FSNet can be incorporated into traditional RANSAC loops.\\n\\nWe evaluate FSNet on fundamental and essential matrix estimation on indoor and outdoor datasets, and establish that FSNet can successfully identify good poses for pairs of images with few or unreliable correspondences. Besides, we show that naively combining FSNet with MAGSAC++ scoring approach achieves state of the art results.\\n\\n1. Introduction\\n\\nHow to determine the relative camera pose between two images is one of the cornerstone challenges in computer vision. Accurate camera poses underpin numerous pipelines such as Structure-from-Motion, odometry, SLAM, and visual relocalization, among others [26, 37, 47, 48, 56]. Much of the time, an accurate fundamental matrix can be estimated by existing means, but the failures are prevalent enough to hurt real-world tasks, and are hard to anticipate [23]. Where are the mistakes coming from?\\n\\nTraditional approaches first detect then describe a set of interest points in each image, and establish correspondences between the two sets while possibly filtering them, e.g., checking for mutual nearest neighbors or applying Lowe's ratio test [36]. Then, random subsets of correspondences are sampled and a 5-point or 7-point algorithm is used to estimate many essential or fundamental matrix hypotheses, respectively, (i.e., two-view geometry models). A RANSAC [25] loop iterates over the generated hypotheses, and ranks them. Conventionally, the ranking is scored by counting inliers, i.e., the number of correspondences within a threshold of that two-view geometry hypothesis. Finally, the top-ranked hypothesis is further refined by using all inlier correspondences.\\n\\nAs the research in robust model estimation advances [2, 5, 13, 15, 41, 58], the different stages of the pipeline are being revisited, e.g., local feature detection and description is learned with neural networks, outlier correspondences are filtered with learned models, hypotheses are sampled more efficiently, or the inlier threshold is optimized. Although the latest matching pipelines produce very accurate and robust correspondences [19, 46, 50], correspondence-based scoring methods are still sensitive to the ratio of inliers, number of correspondences, or the accuracy of the keypoints.\\n\\nReference frame\\nDestination frame\\nF-Mat Hypothesis\\nMAGSAC++\\nFSNet\\n\\nFigure 1. Example where SuperPoint-SuperGlue [19, 46] correspondences are highly populated by outliers, but there are still enough inliers to produce a valid fundamental matrix hypothesis. In such scenarios with unreliable correspondences, current top scoring methods fail (MAGSAC++ [5]), while our proposed FSNet model, a correspondence-free scoring approach, is able to pick out the best fundamental matrix.\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2023-2182", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Incorrect two-view geometry estimation can lead to invalid merges in 3D reconstruction models, bad localization services, or more expensive steps when finding outliers in pose graphs.\\n\\nA second family of approaches emerged in recent years, where a neural network is trained to directly regress two-view geometry from the input images. Thus, such approaches replace all the components of the RANSAC pipeline. This can be a viable approach when two views are extremely difficult, even when they do not overlap. However, challenging scenarios, e.g., wide-baseline, or large illumination changes, can lead to incorrect predictions. Typically, poses directly regressed this way have fewer catastrophic relative pose predictions, but they have difficulty in estimating precise geometry.\\n\\nOn the other hand, correspondence-based hypotheses can be very precise, if estimated correspondences are of sufficiently high quality. Our approach uses correspondences to generate model hypotheses, but does not use correspondences to score them during the RANSAC loop.\\n\\nWe propose a fundamental matrix scoring network that leverages epipolar geometry to compare features of the images in a dense manner. We refer to our method as the Fundamental Scoring Network, or FSNet for short. Inspired by the success of Vision Transformers, and detector-free matchers, we define an architecture that incorporates the epipolar geometry into an attention layer, and hence the quality of the fundamental matrix hypothesis conditions the coherence of the computed features.\\n\\nFigure 1 shows an example where correspondences are highly populated by outliers. However, there are still enough inliers to generate a good fundamental matrix, and FSNet was able to select it from the hypothesis pool.\\n\\nOur contributions are 1) an analysis of the causes of scoring failures, as well as more insights into the traditional RANSAC approach of relative pose estimation; 2) FSNet, a network that predicts angular translation and rotation errors for a given image pair and a fundamental matrix hypothesis; 3) an image order-invariant design of FSNet that outputs the same values for (Image A, Image B, F) and (Image B, Image A, \\\\( F^T \\\\)) inputs; 4) a solution that can be combined with state-of-the-art methods to cope with current failure cases.\\n\\n2. Related Work\\n\\nEstablishing correspondences. Previous correspondence estimation built around SIFT has largely been superseded by learned methods. Keypoint detectors, patch-based descriptors, joint detector-descriptors, or shape estimators are some of the steps that have benefited from data-driven techniques. To find correspondences, the mutual nearest neighbors and Lowe's ratio test approach has also been revisited, and learned matchers have pushed forward the matching capability.\\n\\nComplementary to matchers, additional filtering methods learn to detect and reject outlier correspondences. Once correspondences are established, the RANSAC loop finds the best hypothesis among the pool. Multiple works aim at sampling an all-inlier correspondence minimal-set sooner than RANSAC. Combining that with early termination techniques and detection of degenerate configurations can significantly improve the results and run-time. More recently, alternative methods have been proposed to improve upon the classical detect-then-describe approach, e.g., detector-free matchers, or direct relative-pose regressor networks.\\n\\nModel quality. Model quality research focuses mainly on improving the heuristics for classical inlier counting. LO-RANSAC applies a local optimization step to promising models generated from the RANSAC sampling. GC-RANSAC extended previous local optimization and uses graph techniques to infer spatial structures and mask out outliers. MAGSAC++ proposes an iterative inlier counting score over a range of inlier thresholds, which reduces the sensitivity to the inlier-outlier threshold parameter. MQ-Net combines the inlier counting score with a neural network that predicts the quality of a hypothesis from correspondence residuals.\\n\\nTransformers in vision. Since its introduction, the transformer architecture has become the standard in natural language processing due to its performance and simple design. Transformers are getting attention in the vision community, and have been applied successfully to image matching, multi-view stereo, or depth estimation, among others. Moreover, different works have been proposed to guide the cross-attention mechanism with epipolar supervision, where the most popular strategies use the epipolar attention to limit.\"}"}
{"id": "CVPR-2023-2182", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. MAA@10\u00b0 vs number of correspondences. We show that the number of correspondences has a high impact on the precision of the fundamental matrix. The latest matchers already filter out the correspondence outliers [46], and naturally, the number of correspondences correlates with the difficulty of correctly matching two images. Thus, we see that the number of correspondences is a good indicator to decide when to trust the correspondences.\\n\\n3. Analysis\\n\\nIn this section we explore shortcomings of the traditional RANSAC-based pose estimation approach. First, we show that correspondence-based scoring of fundamental matrix hypotheses is the major reason for RANSAC returning incorrect relative poses. Next, we show that the low number of correspondences often leads to scoring function failures motivating correspondence-free approach. To predict fundamental matrix quality without correspondence-based heuristics, we need a good training signal. We consider alternatives to correspondence-based scoring functions that can be used to quantify the quality of fundamental matrix hypotheses. For our analyses, we use SuperPoint-SuperGlue (SP-SG) [19, 46] correspondences with MAGSAC++ [5], a top-performing publicly available feature extraction, feature matching and robust estimation model. We opt for a top performing combination instead of a classical baseline, i.e., SIFT with RANSAC, to analyse a more realistic use case. We then mine image pairs with low overlapping views from validation splits of ScanNet [18] and MegaDepth [35] datasets as in [46, 50] and study MAGSAC++'s behavior for 500 iterations, i.e., 500 fundamental matrix hypotheses were generated and scored.\\n\\nWhere do wrong solutions come from?\\n\\nIn Figure 2, we show the number of times a good fundamental matrix (fundamental matrix with pose error < 10\u00b0) was selected. We also count failures: (i) pre-scoring, i.e., no good fundamental matrix is among hypotheses, (ii) degeneracy cases, e.g., inlier correspondences can be explained with a homography, (iii) scoring failures, where bad fundamental matrix was chosen by MAGSAC++ heuristic while a good fundamental matrix was among the 500 hypotheses but had lower score. As seen in the figure, 42% (ScanNet) and 23% (MegaDepth) of the image pairs generate a valid fundamental matrix but the scoring method is not able to select it.\\n\\nWhy are wrong solutions selected?\\n\\nInlier heuristics find fewer inliers for a good fundamental matrix than a bad fundamental matrix. So, what leads to a low number of inliers for a good fundamental matrix? Intuitively, feature matching methods return fewer correspondences when the image pair is difficult to match, due to repetitive patterns, lack of textures, or small visual overlap, among the possible reasons. In such scenarios, correspondences can also be highly contaminated by outliers, making it even harder to select the correct fundamental matrix. Figure 3 shows the MAA@10\u00b0 as in [32] w.r.t the number of correspondences generated by SP-SG and MAGSAC++. We see the strong correlation between the number of correspondences and the accuracy of the selected model. Furthermore, the SuperGlue correspondences in this experiment are already of high-quality, as bad correspondences were filtered out. In supplementary materials, we show that loosening the correspondence filtering criteria, and hence, increasing the number of correspondences, does not lead to improved results. This observation motivates our correspondence-free fundamental matrix hypothesis scoring approach, which leads to improvements in relative pose estimation task when correspondences are not reliable.\\n\\nHow should we select good solutions?\\n\\nIf we want to train a model that predicts fundamental matrix quality what should this model predict? As we are interested in ranking hypotheses, or potentially discard all of them if they are all wrong, the predicted value should correlate with the error in the relative pose or the mismatch of epipolar constraint. The quality of relative pose is measured for rotation and translation separately. The angle of rotation between the estimated and the ground truth rotation matrices provides the rotation error. As fundamental and essential matrices\\n\\nTable 1. Error criteria evaluation.\\n\\n|          | GT Pose error | SED | RE1 | Epi. Distance|\\n|----------|---------------|-----|-----|-------------|\\n| ScanNet  | 0.75          | 0.57| 0.74| 0.57        |\\n| MegaDepth| 0.72          | 0.48| 0.41| 0.40        |\\n\\nFundamental matrices are generated with SP-SG [19, 46] and MAGSAC++ [5], and evaluated under densely projected correspondences with ground-truth camera poses and depth maps on our validation set. GT Pose error uses directly the error associated to the fundamental matrix to rank them, being the upper-bound of the scoring function.\"}"}
{"id": "CVPR-2023-2182", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. FSNet architecture has four components. 1. A CNN feature extractor computes the feature maps \\\\( f_A \\\\) and \\\\( f_B \\\\) from input images \\\\( A \\\\) and \\\\( B \\\\) at 1/4 of input resolution (Section 4.1). 2. The extracted features are then processed by the transformer block, which contains \\\\( N \\\\) self and cross-attention layers. The transformer outputs \\\\( \\\\hat{f}_A \\\\) and \\\\( \\\\hat{f}_B \\\\), which are stored and reused for every \\\\( F_i \\\\) hypothesis (Section 4.2). 3. The epipolar cross-attention layer applies cross-attention along the epipolar lines, and embeds \\\\( F_i \\\\) into the feature maps \\\\( f_{Ai} \\\\) and \\\\( f_{Bi} \\\\). Epipolar attention is done every two positions, reducing the final feature maps to an 1/8 of the input resolution (Section 4.3). 4. The pose regressor applies a ResNet and a 2D average pooling block, and outputs \\\\( v_{A \\\\rightarrow B_i} \\\\) and \\\\( v_{B \\\\rightarrow A_i} \\\\). The vectors are combined through a max pooling operator, and the final MLP layer predicts the \\\\( e_R \\\\) and \\\\( e_t \\\\) errors associated to \\\\( F_i \\\\) (Section 4.4).\\n\\nThere are multiple error criteria [24] for fitting fundamental matrices. Reprojection error (RE) measures the minimum distance correction needed to align the correspondences with the epipolar geometry. Symmetric epipolar (SED) measures geometric distance of each point to its epipolar line. Finally, Sampson (RE1) distance is a first-order approximation of RE. The benefit of these metrics is that the quality of fundamental matrices is measured with one scalar value.\\n\\nSo, how do we choose the best error criteria as training signal for FSNet? Should it be relative pose error, or one of the fundamental matrix error criteria? Let us assume that we have access to multiple oracle models, one oracle for each of different error criteria in Table 1. So, one oracle model that predicts the relative pose error perfectly, one oracle model that predicts SED error perfectly, etc. Which of these oracle models provides the best fundamental matrix scoring approach? To simulate evaluation of these oracle models, we use SP-SG and MAGSAC++ to mine fundamental matrix hypotheses from validation datasets, and use the ground truth depth maps and camera poses to generate dense correspondences between image pairs to exactly compute all error criteria. In Table 1, we show the MAA@10\u00b0 of different scoring criteria when evaluating mined fundamental matrices using oracle models in ScanNet and MegaDepth image pairs. As can be seen in the table, all the fundamental matrix error criteria under-perform compared to relative pose error metrics, hence FSNet should be supervised by the relative rotation and translation errors.\"}"}
{"id": "CVPR-2023-2182", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use an \\\\( L \\\\)-multi-head attention transformer architecture following [50], and alternate between self and cross-attention blocks to exploit the similarities within and across the feature maps. We denote the transformed features as \\\\( \\\\hat{f}_A \\\\) and \\\\( \\\\hat{f}_B \\\\). Following the transformer nomenclature, some features are used to compute the query (Q), and potentially different features are used to compute the key (K) and the value (V). Q retrieves information from V based on the attention weight computed from the product of Q and K. In the self-attention layer, the same feature map builds Q, K, and V, meanwhile, in the cross-attention layer, Q is computed from a different feature map than K and V. We interleave the self and cross-attention block \\\\( N \\\\) times.\\n\\nSelf-attention and Cross-attention layers. To limit the computational complexity of our transformer block, we use a Linear Transformer [33] as in [50]. Linear Transformer reduces the computational complexity of the original Transformer from \\\\( O(N^2) \\\\) to \\\\( O(N) \\\\) by making use of the associativity property of matrix products and replacing the exponential similarity kernel with a linear dot-product kernel. Specifically, in a self-attention layer, the input feature map \\\\( f' \\\\) is used to compute Q, K, and V. We concatenate the result of the attention layer with the input \\\\( f' \\\\) feature map and pass it through a two-layer MLP. The output of the MLP is then added to \\\\( f' \\\\) and passed to the next block. In the cross-attention layer, we repeat the previous process but compute Q from one feature map and K and V from the second. We found that positional encoding for attention layers did not improve our results, similarly to findings in [34], and hence, we do not use positional encodings in FSNet.\\n\\n4.3. Epipolar Cross-attention\\n\\nUp to this point, attended feature maps, \\\\( \\\\hat{f}_A \\\\) and \\\\( \\\\hat{f}_B \\\\), are cached and reused. Given that FSNet computes the score for every \\\\( F_i \\\\), this design assures a more practical scenario where the overhead of computing additional fundamental matrix scores is small.\\n\\nFor every fundamental matrix hypothesis \\\\( F_i \\\\), our epipolar cross-attention mechanism embeds \\\\( F_i \\\\) together with feature maps \\\\( \\\\hat{f}_A \\\\) and \\\\( \\\\hat{f}_B \\\\). Every position \\\\( p_A = [u, v] \\\\) in feature map \\\\( \\\\hat{f}_A \\\\) has a corresponding epipolar line in \\\\( \\\\hat{f}_B \\\\) defined as \\\\( l_{A} \\\\rightarrow_{B} uv = F_i \\\\bar{p}_A \\\\), where \\\\( \\\\bar{p}_A \\\\) refers to the homogeneous coordinates of \\\\( p_A \\\\) and \\\\( F_i \\\\) is a scaled \\\\( F \\\\) by a factor of \\\\( \\\\frac{1}{4} \\\\). As we consider potentially hundreds of hypotheses, the resolution of feature maps impacts run-time speed. So, we opt to define query points, \\\\( p_A = [u, v] \\\\), with a step sampling of two. This reduces even further the final feature map to a resolution of \\\\( \\\\frac{1}{8} \\\\) of the input image. So, for every feature \\\\( \\\\hat{f}_A \\\\) \\\\( uv \\\\in \\\\hat{f}_A \\\\) we sample \\\\( \\\\hat{f}_B \\\\) at \\\\( D \\\\) equidistant locations along the epipolar line \\\\( l_{A} \\\\rightarrow_{B} uv \\\\). We start sampling where the epipolar line meets the feature map (from left to right) and use bilinear interpolation to produce \\\\( D \\\\) features \\\\( \\\\hat{f}_B_{l_{uv}} \\\\). If sampling positions fall outside the image plane, or the epipolar line never crosses the image, we zero pad the features. Thus, we build feature volume \\\\( \\\\hat{f}_B_{i} \\\\in [C, D, \\\\frac{W}{8}, \\\\frac{H}{8}] \\\\) from feature map \\\\( \\\\hat{f}_B \\\\) and \\\\( F_i \\\\).\\n\\nWe use \\\\( \\\\hat{f}_A \\\\) to compute Q, and \\\\( \\\\hat{f}_B_{i} \\\\) volume to obtain the K and V, and perform attention along the epipolar candidate points. Finally, we use Q, K, and V to obtain epipolar transformed features \\\\( f_A \\\\). For order-invariance, we also compute \\\\( f_B \\\\) by repeating these operations for \\\\((\\\\hat{f}_B, \\\\hat{f}_A)\\\\) pair of feature maps and \\\\( F \\\\).\\n\\n4.4. Pose Error Regressor\\n\\nAs seen in Figure 4, the pose error regressor uses a ResNet block to extract features from \\\\( f_A \\\\) and \\\\( f_B \\\\). Following [1,11], we apply a 2D average pooling that results in two 1D vectors, \\\\( v_{A \\\\rightarrow B} \\\\) and \\\\( v_{B \\\\rightarrow A} \\\\), with size \\\\( C' \\\\). Both 1D vectors are then merged by a max pooling operator, such that different order of the input images always produce the same feature vector \\\\( v \\\\). An MLP layer then regresses the angular translation and rotation errors, \\\\( e_t \\\\) and \\\\( e_R \\\\), associated to \\\\( F_i \\\\).\\n\\n4.5. Loss Function\\n\\nContrary to previous binary [2] or multi-class [11] formulation of the pose error, we experimentally found that FSNet is more accurate when regressing independently the translation and rotation errors (see Section 5.3). The supervision we use is angular errors, hence, the predicted error, \\\\( e_i \\\\), and the ground truth error, \\\\( \\\\bar{e}_i \\\\), are bounded between \\\\([0^\\\\circ, 180^\\\\circ]\\\\). Directly using a simple L1 loss (\\\\( l = |\\\\bar{e}_i - e_i| \\\\)) would treat all error ranges equally. However, we are interested in accurate error estimation of all good fundamental matrices, while not requiring being as precise when the fundamental matrices have high pose errors. Hence, we propose to use a soft clamping of the ground truth error as well as the network prediction, such as:\\n\\n\\\\[\\nL = |g(\\\\bar{e}_i) - g(e_i)| + |g(\\\\bar{e}_R) - g(e_R)|,\\n\\\\]\\n\\nwhere \\\\( g(x) \\\\) refers to the \\\\( \\\\tanh(\\\\frac{x}{t_s}) \\\\) function, and \\\\( t_s \\\\) is the scaling factor that adjusts the (soft) threshold after which the accuracy of the angular errors is not important.\\n\\n4.6. Implementation Details\\n\\nWe train indoor and outdoor FSNet models on ScanNet [18] and MegaDepth [35] datasets. To generate training and validation sets, we first extract SP-SG [19, 46] correspondences using appropriate indoor and outdoor pre-trained models. We then draw minimal subsets of correspondences randomly and extract 500 two-view hypotheses for every image pair. For each hypothesis, we compute the angular translation (\\\\( e_t \\\\)) and rotation (\\\\( e_R \\\\)) errors using the ground truth extrinsic and intrinsic parameters. During training, we ensure that the ground truth hypothesis is among the 500\"}"}
{"id": "CVPR-2023-2182", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2. Fundamental and essential matrix estimation on ScanNet\\n\\nWe compute the MAA@10\u00b0 and Median error (\u00b0) metrics in the test split of ScanNet, and divide the pairs of images based on the number of SP-SG correspondences to highlight the benefits of FSNet, which results in 3,522 (0-100) and 1,478 (100-Inf) image pairs. In fundamental and essential estimation, we see that when number of correspondences is small (0-100), FSNet provides a more robust solution than competitors. In the overall split (All), FSNet obtains more precise rotation errors for the fundamental estimation tasks, while outperforming in all metrics in essential estimation. Moreover, we also show how FSNet and MAGSAC++ can be easily combined to obtain more reliable approaches.\\n\\n| Method                  | MAA@10\u00b0 | Median (\u00b0) | R / t / max(R, t) | eR / et / max(eR, et) |\\n|-------------------------|---------|------------|-------------------|----------------------|\\n| Fundamental             |         |            |                   |                      |\\n| RANSAC [25]             | 0.15    | 0.07       | 0.04              | 17.97 / 30.28        |\\n| MAGSAC++ [5]            | 0.28    | 0.12       | 0.08              | 9.19 / 24.38         |\\n| FSNet (F)               | 0.29    | 0.19       | 0.12              | 7.98 / 13.30         |\\n| FSNet (F + E)           | 0.35    | 0.24       | 0.17              | 6.52 / 12.05         |\\n| w/ Corresp. filter      | 0.29    | 0.19       | 0.12              | 7.98 / 13.30         |\\n| w/ Candidate filter     | 0.33    | 0.18       | 0.13              | 7.48 / 14.91         |\\n| Essential               |         |            |                   |                      |\\n| EssNet [68]             | -       | -          | -                 | 0.01 / 0.02 / 0.01   |\\n| Map-free [1]            | -       | -          | -                 | 0.39 / 0.13 / 0.09   |\\n| RANSAC [25]             | 0.27    | 0.16       | 0.13              | 9.78 / 16.64         |\\n| MAGSAC++ [5]            | 0.29    | 0.19       | 0.14              | 8.90 / 16.00         |\\n| FSNet (E)               | 0.36    | 0.25       | 0.18              | 6.34 / 10.51         |\\n| FSNet (F + E)           | 0.35    | 0.24       | 0.17              | 6.70 / 10.65         |\\n| w/ Corresp. filter      | 0.36    | 0.25       | 0.18              | 6.34 / 10.51         |\\n| w/ Candidate filter     | 0.33    | 0.22       | 0.16              | 7.71 / 13.41         |\\n\\nThis section presents results for different fundamental and essential hypothesis scoring methods. We first compute correspondences with SP-SG [19, 46], and then use the universal framework USAC [41] to generate hypotheses with a uniformly random minimal sampling. Besides the inlier counting of RANSAC [25], we also use the state-of-the-art MAGSAC++ scoring function [5], and compare them both to FSNet scoring method. To control for randomness, we use the same set of hypotheses generated for each image pair for the evaluation of all methods. We refine all the selected hypotheses by applying LSQ fitting followed by the Levenberg-Marquardt [39] optimization to the correspondences that agree with the best hypothesis model.\\n\\nTo evaluate the scoring methods, we decompose the hypothesis (fundamental or essential matrix) into rotation matrix and translation vectors, and compute the angular errors (eR and et) w.r.t ground truth rotation and translation. We report the mean Average Accuracy (mAA), which thresholds, accumulates, and integrates the errors up to a maximum threshold (in our experiments 10\u00b0) [32]. The integration over the accumulated values gives more weight to accurate poses and discards any camera pose with a relative angular error above the maximum threshold. Besides the\"}"}
{"id": "CVPR-2023-2182", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Correct F-Mats selection (%)\\n\\n| Angle | RANSAC | MAGSAC++ | FSNet (F) | FSNet (E) |\\n|-------|--------|----------|-----------|-----------|\\n| 5\u00b0    | 0.12 / 0.11 | 0.24 / 0.17 | 0.21 / 0.19 | 0.30 / 0.31 |\\n| 10\u00b0   | 0.55 / 0.21 | 0.80 / 0.44 | 0.71 / 0.35 | 0.82 / 0.21 |\\n| 20\u00b0   | 3.72 / 17.75 | 1.39 / 5.42 | 2.08 / 7.22 | 1.32 / 4.78 |\\n\\nTable 3. Fundamental and essential matrix estimation on PhotosphereTourism dataset. We show the mAA (10\u00b0) and median error when using SP-SG correspondences. Contrary to indoor datasets, in the outdoor scenario, current feature extractors provide very accurate and robust features, and in the test split only 1.5% of the image pairs returned fewer than 100 correspondences. Nevertheless, combinations of FSNet and MAGSAC++ provide the most reliable methods for fundamental and essential matrix estimation.\\n\\n5.2. Outdoor Pose Estimation\\n\\nIn Table 3, we show the mAA and median errors of the fundamental and essential matrix estimation tasks on the PhotoTourism test set. Analogous to the indoor evaluation, we split the image pairs based on the number of SP-SG correspondences. Contrary to the indoor scenario, there are only 1.5% of the image pairs in the test set that have fewer than 100 SP-SG correspondences. This is in line with the analysis of Figure 2, where scoring failures are not as common in outdoor dataset as in indoor dataset. There are multiple explanations of these phenomena: (i) walls and floors are mostly untextured surfaces which lead to fewer reliable correspondences for indoor images, (ii) indoor surfaces are closer to the camera, so small camera motion can result in large parts of the scene not being visible from both...\"}"}
{"id": "CVPR-2023-2182", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Median (\u00b0)\\n\\n| Supervision | RE1          | RE1          | RE1          |\\n|-------------|--------------|--------------|--------------|\\n|             | 0.23 / 0.12 | 0.28 / 0.14  | 8.53 / 19.42 |\\n| CE as in [2] | 0.27 / 0.18 | 0.34 / 0.20  | 6.94 / 13.21 |\\n|             | 0.25 / 0.16 | 0.32 / 0.18  | 7.52 / 14.07 |\\n| FSNet (Soft-L1) | 0.29 / 0.19 | 0.36 / 0.21  | 6.52 / 12.05 |\\n| MAGSAC++ [5] | 0.28 / 0.12 | 0.38 / 0.17  | 6.35 / 17.89 |\\n\\nTable 4. Loss function and generalization ablation results on the fundamental matrix estimation task in the test set of ScanNet.\\n\\nSupervising the training of FSNet directly with the camera pose error provides the best results, while the CE approach outperforms L1 norm and Sampson distance (RE1) prediction models.\\n\\nIn the fundamental estimation task, FSNet scoring approach obtains the most accurate poses in the low number of correspondences split (0-100), while still outperforming RANSAC inlier counting approach in the full test set. In the essential estimation task, even though we observe that correspondence-based scoring approaches, i.e., RANSAC or MAGSAC++, outperform FSNet, they still benefit from combining their predictions with FSNet.\\n\\n5.3. Understanding FSNet\\n\\nLoss supervision ablation. In Table 4, we experimentally verify our analysis of supervision signals in Section 3. We show that FSNet trained with Eq. 1 outperforms FSNet trained using RE1 supervision (best epipolar constraint error criteria among oracle models in Table 1) and FSNet trained using L1 norm without soft clamping. We also train FSNet using binary cross-entropy (CE), where fundamental matrix hypothesis \\\\( F \\\\) is deemed correct if \\\\( e < 10\u00b0 \\\\) and incorrect otherwise. We follow [2] and incorporate the network uncertainty in the cross-entropy loss; please see the supplementary materials for details.\\n\\nFailures. Figure 6 shows failure cases for FSNet hypothesis selection. As FSNet needs to run on hundreds of hypotheses in a reasonable time, the network is designed to work on low resolution images. Unfortunately, at low resolution, matching features can seem degenerate. Looking into the failures, we observe that the fundamental matrices selected by FSNet have a valid epipolar geometry, i.e., epipolar lines in image \\\\( B \\\\) cross the surrounding regions of corresponding points from image \\\\( A \\\\), and hence, the low resolution features \\\\((W/8, H/8) = (32, 32)\\\\) given to the pose error regressor lack the precision to produce a good estimate.\\n\\nTime analysis. Given a pair of images and 500 hypotheses, FSNet scores and ranks them in 0.97s on a machine with one V100 GPU. Running the feature extraction and transformer subnetworks takes 138.28ms. This is precomputed once and reused for every hypothesis. The hypotheses are queried in batches, so the epipolar cross-attention and pose error regressor subnetworks run in 167.13ms for a batch of 100 fundamental matrices. Besides, strategies such as Candidate filter can reduce FSNet time by only scoring promising hypotheses, e.g., for fundamental estimation it only adds 150ms on top of MAGSAC++ runtime (71ms).\\n\\n6. Conclusions\\n\\nWe introduce FSNet, a correspondence-free two-view geometry scoring method. Our experiments show that our learned model achieves SOTA results, performing exceptionally well in challenging scenarios with few or unreliable correspondences. These results suggest exciting avenues for future research. We mention some of them here.\\n\\nNaive combinations of FSNet with traditional inlier counting lead to even higher reliability. This suggests that more sophisticated combinations of scoring approaches could lead to further improvements. Alternatively, a better network design may improve prediction accuracy, which could supersede inlier counting in all scenarios.\\n\\nThe first stages of our pipeline (feature extraction, self and cross-attention blocks) form a subnetwork that precomputes feature maps for each image pair. This subnetwork architecture is similar to the latest feature extraction methods, e.g. [50]. This means that feature map precomputation, local feature extraction, feature matching, and hypothesis generation can all be performed by this subnetwork. Training the whole network on two tasks: estimating good correspondences and two-view geometry scoring could lead to synergistic improvements on both tasks.\"}"}
