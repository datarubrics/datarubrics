{"id": "CVPR-2024-984", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Data settings for the three learning paradigms.\\n\\n| Paradigm                     | #Training Images and Label Information | #Testing Images | Common Challenge | Full |\\n|------------------------------|----------------------------------------|-----------------|------------------|------|\\n| 300W CariFace ArtiFace       | 3,148 Labeled                          | 554             | 135              | 689  |\\n| DA (300W \u2192 CariFace)         | \u2014                                      | \u2014               | \u2014                | \u2014    |\\n| DA (300W \u2192 ArtiFace)         | \u2014                                      | \u2014               | \u2014                | \u2014    |\\n| GZSL (Unseen ArtiFace)       | 3,148 Labeled                          | 554             | 135              | 689  |\\n| GZSL (Unseen CariFace)       | 3,148 Labeled                          | 554             | 135              | 689  |\\n| Oracle                       | 3,148 Labeled                          | 554             | 135              | 689  |\\n\\nFigure 4. Illustrations of typical samples in the 300W, CariFace, and ArtiFace datasets, each of which is annotated with landmarks.\\n\\n\u2022 ArtiFace Dataset.\\n  ArtiFace [55] contains 160 artistic portraits of 16 artists, which covers diverse artwork styles ranging from Renaissance to Comics. Each face in the datasets is annotated with 68 landmarks. Typical faces in the datasets and their landmarks are shown in Fig. 4. We can find that the faces in the three datasets have distinguished styles, which correspond to three different domains. In particular, compared to 300W [38], CariFace [4] exhibits abstract and exaggerated patterns, leading to large representation variations. ArtiFace [55] not only has larger variations across different artistic categories but also differs greatly in terms of the aspect of facial scales, orientations, locations, and so on.\\n\\n4.2. Learning Paradigms and Baselines\\n\\nGiven the above datasets, we consider the following three learning paradigms:\\n\\n\u2022 Domain Adaptation (DA).\\n  Given labeled 300W faces and unlabeled stylized faces from CariFace or ArtiFace, we learn a face landmarker based on various domain adaptation methods.\\n\\n\u2022 Generalized Zero-shot Learning (GZSL).\\n  In the challenging GZSL setting, we learn a face landmarker based on the above DA-based methods and test it in an unseen face domain (e.g., learning the landmarker on labeled 300W and unlabeled CariFace and testing on ArtiFace).\\n\\n\u2022 Oracle.\\n  In this setting, the labeled faces of all three datasets are accessible, and we can learn the face landmarker by classic supervised learning.\\n\\nFor a fair comparison, in each learning paradigm, we set the architecture of the face landmarker based on the SLPT in [52]. Ideally, we would like to learn landmarkers in the DA and GZSL settings, making its performance comparable to the oracle. In the oracle setting, we can learn the face landmarker directly via classic supervised learning (SL), i.e.,\\n\\n$$\\\\min_{\\\\theta} \\\\mathbb{P}(X, Y) \\\\sim D \\\\| f_{\\\\theta}(X) - Y \\\\|_2^2 \\\\mathbb{F}.$$ \\n\\nIn the DA and GZSL settings, besides minimizing the landmark estimation errors, we can apply various image style transfer and domain adaptation methods, e.g., RevGrad [12], CycleGAN [59], BDL [25], AdaptSegNet [45] and FDA [54], to impose domain adaptation regularization during training. These methods work as the baselines of our method.\\n\\nGiven the landmarkers learned by various methods, we evaluate them with the standard metric, Normalized Mean Error (NME). In Tab. 1, we show the training and testing data settings in the above three learning paradigms. Following existing work [9, 42, 52], we further split the 689 testing faces in 300W into 554 faces in common scenarios and 135 faces in challenging scenarios. The NMEs for the common, challenge, and full scenarios are recorded.\\n\\n4.3. Numerical and Visual Comparisons\\n\\nIn Tab. 2, we show the performance of various learning methods in different settings, demonstrating the effectiveness and superiority of our method. In particular, existing DA methods often fail to improve the generalization power of model a lot in face landmarking tasks \u2014 their performance in target and unseen domains is inferior to that in the oracle setting, with a significant gap on NME. A potential reason for this phenomenon is that these methods focus on the adaptation of image domain and the landmark-related loss is not dominant in their learning processes. As a result, instead of learning the face landmarker, they make more efforts to optimize the parameters of other modules (e.g., the neural network-based face stylization modules and discriminators) during training.\"}"}
{"id": "CVPR-2024-984", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2. Comparisons for various methods on their NMEs. In DA and GZSL settings, the best results are bold.\\n\\n| Learning Paradigm | Learning Method | 300W | CariFace | ArtiFace | Average | NME |\\n|-------------------|----------------|------|----------|----------|---------|-----|\\n| DA (300W \u2192 CariFace) | SL+RevGrad [12] | 2.84 | 5.58 | 3.38 | 12.19 | 5.16 | 7.83 |\\n| DA (300W \u2192 ArtiFace) | SL+CycleGAN [59] | 2.74 | 5.43 | 3.27 | 12.11 | 4.70 | 7.70 |\\n| DA (300W \u2192 ArtiFace) | SL+BDL [25] | 3.28 | 6.17 | 3.84 | 13.63 | 5.65 | 8.77 |\\n| GZSL (Unseen ArtiFace) | SL+ASN [45] | 2.92 | 5.26 | 3.38 | 12.21 | 4.75 | 7.80 |\\n| GZSL (Unseen CariFace) | SL+FDA [54] | 2.89 | 5.18 | 3.34 | 12.66 | 4.60 | 8.07 |\\n| **Ours** | SL+RevGrad [12] | **2.79** | **4.91** | **3.20** | **7.70** | **3.95** | **5.46** |\\n\\nTable 3. The impacts of different model architectures on the NME performance of our method. In each setting, the best results are bold.\\n\\n| Learning Paradigm | Learning Method | 300W | CariFace | ArtiFace | Average | NME |\\n|-------------------|----------------|------|----------|----------|---------|-----|\\n| DA (300W \u2192 CariFace) | SBR [9] | 3.61 | 6.36 | 4.15 | 8.00 | 5.09 | 6.11 |\\n| DA (300W \u2192 CariFace) | HRNet [47] | 2.93 | 5.29 | 3.40 | 7.73 | 4.33 | 5.60 |\\n| GZSL (Unseen ArtiFace) | SLPT [52] (Default) | 2.79 | 4.91 | 3.20 | 7.70 | 3.95 | 5.46 |\\n| DA (300W \u2192 ArtiFace) | SBR [9] | 3.70 | 6.82 | 4.32 | 11.35 | 5.32 | 8.04 |\\n| DA (300W \u2192 ArtiFace) | HRNet [47] | 3.04 | 5.44 | 3.51 | 9.85 | 4.30 | 6.86 |\\n| GZSL (Unseen CariFace) | SLPT [52] (Default) | 2.90 | 5.14 | 3.34 | 10.93 | 3.93 | 7.34 |\\n\\nSource-only RevGrad CycleGAN BDL Ours\\n\\n(a) DA (300W \u2192 CariFace)\\n\\n(b) DA (300W \u2192 ArtiFace)\\n\\nFigure 5. Visual comparisons for various methods in the two DA settings. We only highlight points on the inner lips in the enlarged region of the mouth in (a), as well as the eyes and the sides of the cheeks, excluding points on the eyebrows in (b).\\n\\nDifferent from the baselines, the performance of our model in the target domain (e.g., CariFace and ArtiFace) is improved consistently, while the performance in the source domain does not degrade a lot. In both DA and GZSL settings, our method outperforms the baselines in most situations. Especially in the GZSL settings, the landmarkers\\n\\n2430\\n\\nFigure 6. Visual comparisons for various methods in the GZSL (Unseen ArtiFace) setting. The zoomed-in area in (a) highlights the noses and facial contours, while that in (b) concentrates on the upper part of faces.\"}"}
{"id": "CVPR-2024-984", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The number of iterations\\n2.6\\n2.8\\n3.0\\n3.2\\n3.4\\n\\n(a) 300W (Common)\\n\\n(b) 300W (Challenge)\\n\\n(c) 300W (Full)\\n\\n(d) CariFace (Target)\\n\\n(e) ArtiFace (Unseen)\\n\\nFigure 7. Comparisons for different optimization strategies on NME performance in the GZSL (Unseen ArtiFace) setting.\\n\\nTable 4. Comparison with SOTA landmark detectors on NME.\\n\\n| Learning Method | Train on 300W | | Backbone |\\n|-----------------|--------------|-----------------|----------|\\n| | Our method | | OP SPIGA STAR SLPT |\\n| | | | STAR SLPT |\\n| | 10.46 11.23 10.97 11.05 | | 7.62 7.70 |\\n| | 5.55 5.19 5.30 4.56 | | 4.86 3.93 |\\n\\nObtained by our method show encouraging generalization power, which achieves lower NME than the baselines in the unseen domain (i.e., ArtiFace). Overall, the NME of our method in the source domain (i.e., 300W) is comparable to that achieved in the oracle setting. For the target even unseen domains, our method reduces the gap to the oracle.\\n\\nBesides the numerical comparisons, we provide some visualization results obtained by different methods in Figs. 5 and 6. For a complete comparison, the results of the SLPT trained only on 300W are shown in the figures as well, which corresponds to learning a face landmarker only based on the data in the source domain. We can find that by applying our learning method, the face landmarker obtains better landmarks, which aligns with the ground truth with smaller errors, particularly for the landmarks of face contour, nose, mouth, and eyebrow.\\n\\nTo further verify the generalization of our method, we select three more state-of-the-art landmarkers, including OpenPose (OP) [7], SPIGA [36] and STAR [57] for numerical comparisons, and incorporate STAR as the backbone model of our method. Tab. 4 demonstrates that our learning method is applicable for various backbone models (e.g., STAR and SLPT), and integrating existing landmarkers with our learning method is an effective and competitive approach to enhance their generalizability.\\n\\n4.4. Analytic Experiments\\n\\n4.4.1 Joint v.s. Alternating Optimization\\n\\nAs aforementioned, learning the face landmarker and the warping field model jointly makes it much easier to fall into undesired local optimum or unstable saddle points. To verify this claim, we apply the joint optimization strategy, i.e., solving (3) by optimizing \\\\( \\\\theta \\\\) and \\\\( \\\\gamma \\\\) jointly in each gradient descent step and compare its performance with ours. In Fig. 7, we show the best NME achieved by this joint optimization strategy and the NME achieved by our alternating optimization method in each iteration. We can find that the joint optimization strategy tends to overfit the source domain (300W), leading to power generlizability in the target and unseen domains. On the contrary, with the increase of iterations, the NMEs of our method on the target and unseen domains decrease consistently and become lower than those of the joint optimization after several iterations.\\n\\n4.4.2 Impacts of Model Architectures\\n\\nBesides the optimization strategy, the model architecture also has an impact on the model performance. By default, we implement the face landmarker as the SLPT model. In this experiment, we further explore the performance of other model architectures, including SBR [9] and HR-Net [47]. Tab. 3 shows the performance of different model architectures in DA and GZSL settings. We can find that the SLPT-based face landmarker works better than the two competitors in this experiment. A potential reason for this phenomenon is that both SBR and HRNet are heatmap-based face landmarkers. Given an input facial image, they output a heatmap indicating the distribution of landmarks rather than a set of deterministic landmark coordinates. We have to first detect the landmarks from the heatmap and then pass them through the warping field model. Accordingly, in the face warper optimization step (i.e., solving (4)), the landmarker and the warping field model cannot be trained in an end-to-end way because the backpropagation of the gradient becomes inapplicable. As a result, we have to update \\\\( \\\\theta \\\\) and \\\\( \\\\gamma \\\\) alternatively, leading to suboptimal performance.\\n\\n4.4.3 Effects on Loss Term\\n\\nIn the context of our model\u2019s loss function, the image gradient field serves as its input. It has been observed through experiments that utilizing distinct gradient operators leads to the generation of varying image gradient fields, which in 2431\"}"}
{"id": "CVPR-2024-984", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Quantitative comparison on CariFace dataset.\\n\\n| Operator | Gray Spatial Laplacian | Canny | Sobel |\\n|----------|------------------------|-------|-------|\\n| NME      | 8.282                  | 7.870 | 8.005 |\\n\\nTable 6. Comparisons for different losses on NME.\\n\\n| Type of Loss | MSE | Perceptual | w.o. Grad MSE | Grad MSE |\\n|--------------|-----|------------|---------------|----------|\\n| 300W         | 3.08| 2.89       | 2.96          | 2.79     |\\n| Common       | 5.43| 5.13       | 5.00          | 4.91     |\\n| Challenge    | 3.54| 3.33       | 3.36          | 3.20     |\\n| Full         | 8.26| 8.23       | 9.48          | 7.70     |\\n| DA on Caricature | 4.12| 4.07   | 4.06          | 3.95     |\\n| GZSL on ArtiFace | 5.12| 5.07   | 5.06          | 4.95     |\\n\\nThe rationality of the warping field model implemented in our work. In particular, applying different warping field models in our training process, we visualize their warping results in Fig. 8. We can find that although the face landmarker with the polyharmonic interpolation model leads to a very simple face warper, it outperforms many existing neural network-based image warping methods, such as AutoToon [15] and CariGANs [5] for facial manipulation, and the optical flow methods like RAFT [44]. These methods either require one-to-one correspondence information between source and target images or assume the deformation between the two images to be slight, making them unsuitable for our problem, especially for the stylized faces with significant nonrigid deformations.\\n\\n5. Conclusion\\n\\nIn this paper, we propose a simple but effective method for learning a generalizable face landmarker applicable to facial images with different styles. Given labeled real human faces and unlabeled stylized faces, our method learns the face landmarker under the guidance of conditional face warping, demonstrating the usefulness of the warping information. An alternating optimization framework is proposed to learn the face landmarker together with the warping field model. Experiments demonstrate the effectiveness of our method. Especially in the generalized zero-shot learning scenarios, our method achieves encouraging landmarking accuracy in unseen face domains.\\n\\nAcknowledgements\\n\\nThis work was supported in part by the National Natural Science Foundation of China (62102031), the foundation of Key Laboratory of Artificial Intelligence, Ministry of Education, P.R. China, and the Young Scholar Program (XSQD-202107001) from Beijing Institute of Technology.\"}"}
{"id": "CVPR-2024-984", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generalizable Face Landmarking Guided by Conditional Face Warping\\n\\nJiayi Liang 1\\nHaotian Liu 1*\\nHongteng Xu 2, 3\\nDixin Luo 1, 4\u2020\\n\\n1School of Computer Science and Technology, Beijing Institute of Technology, Beijing\\n2Gaoling School of Artificial Intelligence, Renmin University of China, Beijing\\n3Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing\\n4Key Laboratory of Artificial Intelligence, Ministry of Education, Shanghai\\n\\n{jiayi.liang, haotianliu, dixin.luo}@bit.edu.cn, hongtengxu@ruc.edu.cn\\n\\nAbstract\\nAs a significant step for human face modeling, editing, and generation, face landmarking aims at extracting facial keypoints from images. A generalizable face landmarker is required in practice because real-world facial images, e.g., the avatars in animations and games, are often stylized in various ways. However, achieving generalizable face landmarking is challenging due to the diversity of facial styles and the scarcity of labeled stylized faces. In this study, we propose a simple but effective paradigm to learn a generalizable face landmarker based on labeled real human faces and unlabeled stylized faces. Our method learns the face landmarker as the key module of a conditional face warper. Given a pair of real and stylized facial images, the conditional face warper predicts a warping field from the real face to the stylized one, in which the face landmarker predicts the ending points of the warping field and provides us with high-quality pseudo landmarks for the corresponding stylized facial images. Applying an alternating optimization strategy, we learn the face landmarker to minimize\\n\\n1. Introduction\\nFace landmarking seeks to extract human facial keypoints (e.g., eyes, nose, facial contour, and so on) from facial images. This task is important for many applications in the field of computer vision and graphics, such as face recognition [1, 28, 33, 43, 53], face stylization [5], and 3D face reconstruction [11, 26, 37]. Currently, many open-source and commercial face landmarkers [19, 51, 58] have been developed and achieved encouraging performance in this task. Most existing face landmarkers are designed and trained for landmarking real human faces, while the rapid development of AIGC applications, such as artistic character creation and cartoon generation [2, 15], leads to a massive increase in demands for landmarking stylized facial images. Unfortunately, as shown in Fig. 1, existing face landmarkers often fail to landmark stylized facial images. Even if applying state-of-the-art domain adaptation strategies [12, 25, 27, 59, 61], the generalizability of the learned landmarkers in the stylized facial image domain is still unsatisfactory. Essentially, traditional face landmarkers work...\"}"}
{"id": "CVPR-2024-984", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The scheme of our proposed method for learning a generalizable face landmarker.\\n\\nwell on real human faces because of the relatively stable geometry of real human faces and the sufficient labeled facial images. These two conditions, however, become questionable when landmarking stylized faces \u2014 the stylized facial images often have various facial styles, and manually landmarking such stylized faces is much more time-consuming than landmarking real faces. As a result, learning a generalizable face landmarker becomes a challenging task.\\n\\nIn this paper, we propose a simple but effective paradigm for learning a generalizable face landmarker, overcoming the challenges caused by the diversity of facial styles and the scarcity of labeled stylized faces. As illustrated in Fig. 2, given labeled real human facial images and unlabeled stylized facial images, we learn a face landmarker embedded in a conditional face warper. The face warper aims to deform real human faces according to stylized facial images, generating warped faces and corresponding warping fields. The face landmarker, as the key module of the warper, predicts the ending points of the warping fields and thus provides us with pseudo landmarks for the stylized facial images. The warping field is parametrized by a polyharmonic interpolation model. Under the guidance of the conditional face warping, we learn the face landmarker in an alternating optimization framework: The face landmarker is updated to minimize the discrepancy between the stylized faces and the corresponding warped real human faces and minimize the prediction errors of both real and pseudo landmarks. In the first step, the face landmarker is learned associated with the warping field model, while in the second step, the face landmarker is updated with proximal regularization.\\n\\nThe extensive experiments in various face landmarking tasks demonstrate the effectiveness of learning method. The impacts of different loss functions and data settings on our learning method are analyzed through detailed ablation studies. Experimental results show that our learning method results in a face landmarker that is generalizable to the facial images with different styles, which outperforms representative domain adaptation methods consistently in various stylized face landmarking tasks.\\n\\n2. Related Work\\n\\n2.1. Face Landmarking\\n\\nGiven facial images, the early face landmarking methods learn regression models to predict landmark coordinates directly [6, 32]. The models are often parameterized by neural networks like Transformer [46], capturing facial attributes that have been proven to be crucial for landmark prediction [24]. The coordinate regression is achieved in a coarse-to-fine framework [58], leading to a cascading landmarking pipeline. Recently, to make the landmark prediction robust to the variations in pose, scale, and occlusion, DAN [22] introduces the novel use of heatmaps and extracts features from the entire face rather than local patches around landmarks. SBR [9] utilizes the registration of synthesized images to provide supervisory signals for training. Adaptive Wing Loss [48] is proposed to address the imbalance between foreground pixels and background pixels by analysis of the main drawbacks of different loss functions. HRNet [47] produces high-resolution maps by connecting and exchanging information via merging multi-scale picture features across many branches.\\n\\nAs aforementioned, most existing face landmarkers are learned for real human faces, which can not be adapted directly to stylized faces (e.g., cartoon and artistic faces). Essentially, treating labeled real human faces as a source domain and unlabeled stylized faces as a target domain, we can learn a generalizable face landmark by solving a domain adaptation (DA) problem [30, 31]. Accordingly, many DA techniques [27, 49] have potential in our problem, including the classic metric learning-based methods (e.g., CORAL [41], contrastive domain discrepancy [17], and maximum mean discrepancy [16]) and the recent adversarial learning-based methods [12, 35, 56, 59]. However, in the following content, we will show that directly applying these DA techniques often fails to achieve generalizable face landmarking because of the significant gap between the source and target face domains.\\n\\n2.2. Face Warping\\n\\nFace warping is a technique that involves geometrically deforming source facial images to specified target shapes. The key step of this task is predicting a warping field between the source and target images that captures the shifts of image pixels. To achieve this aim, DST [20] and FoA [55] find matching keypoints between source and target images.\"}"}
{"id": "CVPR-2024-984", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"man facial image, conditioned on $X$, mapping from the pixel in (1), the vector $y$ respond to the parameters of the warping field. As shown face domains, respectively. Each $w$ in the stylized facial image, and of our face landmarker. Given a stylized face $X$, face landmarker into a conditional face warper and learn it to both $X$ face landmarks from facial images and moreover, general-\\n\\nIn this study, we take the SLPT model \\\\[52\\\\] as the backbone alternating optimization framework, as illustrated in Fig. 2. Associated with a parametric warping field predictor in an face landmarker into a conditional face warper and learn it to both $X$ face landmarks from facial images and moreover, general- \\n\\nthe model parameter. The model should be able to predict $X$ face landmarks from facial images and moreover, general-\\n\\nthe model parameter. The model should be able to predict $X$ face landmarks from facial images and moreover, general- \\n\\nthese methods require the paired images to be similar to \\n\\nToon \\\\[15\\\\], RAFT \\\\[44\\\\], and their variants \\\\[29\\\\]. However, \\n\\ndirectly based on paired images, e.g., Flownet \\\\[10\\\\], Auto-\\n\\nods learn neural networks to predict dense warping fields \\nand then generate a dense warping field through data inter-\\n\\nand then generate a dense warping field through data inter-\\n\\nCorrelation, we develop the proposed learning paradigm. \\n\\nCompared to face stylization \\\\[13, 18, 39, 40\\\\], face warp-\\n\\nbetween the faces in different domains. \\n\\nting is a relatively easier task because it only considers the \\n\\ndeformation of shapes while ignoring the transfer of tex-\\n\\nrelevant to face landmarking, in which the warping field \\n\\ntures. However, it should be noted that this task is more \\n\\nTo achieve this aim, we embed the \\n\\nDenote $X$ as the image space and $Y$ as the landmark space, and each $y$ represents an image, and each $i,\\\\gamma$ face landmark coordinates, where $y_i = [\\\\hat{y}_i, b_i]$. At the same time, we can sample a labeled real \\n\\n$\\\\omega_k \\\\in \\\\mathcal{K}$, we have \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times \\\\{\\\\hat{U}_i\\\\}$ records \\n\\n$\\\\mathcal{L} = \\\\{\\\\hat{L}_i\\\\} \\\\times \\\\{\\\\hat{V}_i\\\\} \\\\times"}
{"id": "CVPR-2024-984", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Illustrations of conditional face warping results. Taking a cartoon face as the target, our model warps real human faces accordingly. The red dots indicate real human face landmarks, and green dots indicate cartoon and warped face landmarks.\\n\\n\\\\[ \\\\gamma \\\\]\\n\\nJointly often falls into an undesired local optimum even an unstable saddle point. To mitigate this issue, we propose an alternating optimization framework. In principle, we can decompose the optimization problem in (3) into the following two subproblems and solve them iteratively.\\n\\n- **Face Warper Optimization:** The first subproblem corresponds to the optimization of the face warper, i.e., \\\\( \\\\theta(1) \\\\), \\\\( \\\\gamma(1) \\\\) = \\\\( \\\\text{arg min}_{\\\\theta, \\\\gamma} \\\\sum_{i,j} \\\\| \\\\nabla c_{X}(L_j) - \\\\nabla X(U_i) \\\\|_2^2 + \\\\sum_{i,j} \\\\| b_Y(U_i) - Y(L_j) \\\\|_2^2 \\\\).\\n\\n  In this subproblem, we only care about whether the real human faces can be warped as the stylized faces with high accuracy, so the term of landmarking error is ignored. We solve this problem by Adam [21]: in each step, we update \\\\( \\\\theta \\\\) and \\\\( \\\\gamma \\\\) based on a batch of randomly-sampled face pairs.\\n\\n- **Proximal Face Landmarker Optimization:** Given \\\\( \\\\theta(1) \\\\) and the predicted landmarks (i.e., \\\\( b_Y(U_i) = f_{\\\\theta(1)}(X(U_i)) \\\\) for \\\\( i = 1, ..., N \\\\)), we can treat \\\\( \\\\theta(1) \\\\) as the initial variable and optimize it with a proximal regularizer:\\n\\n  \\\\[ \\\\theta(2) = \\\\text{arg min}_{\\\\theta} \\\\sum_{j=1}^{N_L} \\\\| f_{\\\\theta}(X(L_j)) - Y(L_j) \\\\|_2^2 + \\\\sum_{i=1}^{N_U} \\\\| f_{\\\\theta}(X(U_i)) - b_Y(U_i) \\\\|_2^2 \\\\|_2^2 \\\\{ \\\\text{Pseudo landmarking error in the target domain} \\\\} \\\\]\\n\\n  Here, the second term in (5) measures the estimation errors of the pseudo landmarks achieved in the previous step. Essentially, it works as a proximal regularizer, ensuring that the optimized landmarks \\\\( f_{\\\\theta(2)}(X(U_i)) \\\\) is not too far away from the previous estimation \\\\( b_Y(U_i) \\\\). Similarly, we can solve this problem by Adam [21] as well.\\n\\nFig. 3 shows the warping effect on real human faces achieved by solving (4). In Fig. 3, the first row shows the real human faces with landmarks and the target stylized face, and the second row shows the warping results, in which the green dots are predicted landmarks. These results empirically demonstrate the rationality of our alternating optimization framework. In particular, we can find that solving (4) leads to reasonable warping results, which are similar to the target stylized face on shape. The similarity on face shape indicates that the predicted landmarks can be treated as reliable pseudo labels of the stylized face, which can be used to construct the proximal regularizer that penalizing the pseudo landmarking errors in the target domain.\\n\\nRepeating the above two steps till converge, we obtain the target face landmarker that is generalizable for both real and stylized faces. Algorithm 1 shows the learning scheme.\\n\\n### Algorithm 1\\n\\n**Proposed learning scheme of face landmarker**\\n\\n**Require:**\\nLabeled real faces \\\\( D(L) \\\\) and unlabeled stylized faces \\\\( D(U) \\\\). The number of iterations (i.e., \\\\( M \\\\)). Epochs for the subproblems (i.e., \\\\( L_1 \\\\) and \\\\( L_2 \\\\)).\\n\\n1. Initialize \\\\( \\\\{ \\\\gamma(0) \\\\} \\\\) with a pretrained model on \\\\( D(L) \\\\) and \\\\( \\\\{ \\\\theta(0) \\\\} \\\\) randomly.\\n2. for \\\\( m = 0, ..., M - 1 \\\\) do\\n3. Sample a batch \\\\( \\\\{ X(U_i), X(L_i), Y(L_i) \\\\}_{i=1}^{N_i} \\\\).\\n4. Face warper optimization:\\n5. Take \\\\( \\\\theta(2m), \\\\gamma(m) \\\\) as the initialization, then solve (4) by Adam with \\\\( L_1 \\\\) epochs and obtain \\\\( \\\\theta(2m+1) \\\\).\\n6. Proximal face landmarker optimization:\\n7. Take \\\\( \\\\theta(2m+1) \\\\) as the initialization, then solve (5) by Adam with \\\\( L_2 \\\\) epochs and obtain \\\\( \\\\theta(2m+2) \\\\).\\n8. end for\\n9. return Output a generalizable face landmarker \\\\( f_{\\\\theta(2M)} \\\\).\\n\\n4. Experiment\\n\\nWe apply our learning method to learn a face landmarker and test it on landmarking faces with various styles. Extensive experiments, including comparisons with baselines and analytic ablation studies, demonstrate the effectiveness of our learning method and the generalizability of the corresponding face landmarker. All the experiments are conducted on a single NVIDIA 3090 GPU. Representative experimental results are shown below. More experimental results and implementation details are given in the supplementary material.\\n\\n4.1. Dataset\\n\\nIn this study, we conduct experiments based on the following three commonly-used face datasets.\\n\\n- **300W Dataset.** 300W [38] is comprised of five well-known real human face datasets including LFPW [3], AFW [60], HELEN [23], XM2VTS [34], and IBUG [38].\\n\\n- **CariFace Dataset.** CariFace [4] is created by searching and selecting thousands of various caricatures from different celebrities on the Internet.\"}"}
{"id": "CVPR-2024-984", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Olalekan Agbolade, Azree Nazri, Razali Yaakob, Abdul Azim Abd Ghani, and Yoke Kqueen Cheah. Landmark-based homologous multi-point warping approach to 3d facial recognition using multiple datasets. PeerJ Computer Science, 6:e249, 2020.\\n\\n[2] Sadegh Aliakbarian, Pashmina Cameron, Federica Bogo, Andrew Fitzgibbon, and Thomas J. Cashman. Flag: Flow-based 3d avatar generation from sparse observations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13253\u201313262, 2022.\\n\\n[3] Peter N. Belhumeur, David W. Jacobs, David J. Kriegman, and Neeraj Kumar. Localizing parts of faces using a consensus of exemplars. In CVPR 2011, 2011.\\n\\n[4] Hongrui Cai, Yudong Guo, Zhuang Peng, and Juyong Zhang. Landmark detection and 3d face reconstruction for caricature using a nonlinear parametric model, 2020.\\n\\n[5] Kaidi Cao, Jing Liao, and Lu Yuan. Carigans: Unpaired photo-to-caricature translation. arXiv preprint arXiv:1811.00222, 2018.\\n\\n[6] Xudong Cao, Yichen Wei, Fang Wen, and Jian Sun. Face alignment by explicit shape regression. International journal of computer vision, 107:177\u2013190, 2014.\\n\\n[7] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7291\u20137299, 2017.\\n\\n[8] Forrester Cole, David Belanger, Dilip Krishnan, Aaron Sarna, Inbar Mosseri, and William T Freeman. Synthesizing normalized faces from facial identity features. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3703\u20133712, 2017.\\n\\n[9] Xuanyi Dong, Shoou-I Yu, Xinshuo Weng, Shih-En Wei, Yi Yang, and Yaser Sheikh. Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 360\u2013368, 2018.\\n\\n[10] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 2758\u20132766, 2015.\\n\\n[11] Pengfei Dou, Shishir K Shah, and Ioannis A Kakadiaris. End-to-end 3d face reconstruction with deep neural networks. In proceedings of the IEEE conference on computer vision and pattern recognition, pages 5908\u20135917, 2017.\\n\\n[12] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180\u20131189. PMLR, 2015.\\n\\n[13] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576, 2015.\\n\\n[14] C. A. Glasbey and K. V. Mardia. A review of image-warping methods. Journal of Applied Statistics, 25(2):155\u2013171, 2002.\\n\\n[15] Julia Gong, Yannick Hold-Geoffroy, and Jingwan Lu. Auto-toon: Automatic geometric warping for face cartoon generation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 360\u2013369, 2020.\\n\\n[16] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Sch\u00f6lkopf, and Alex Smola. A kernel method for the two-sample-problem. Advances in neural information processing systems, 19, 2006.\\n\\n[17] Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4893\u20134902, 2019.\\n\\n[18] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.\\n\\n[19] Vahid Kazemi and Josephine Sullivan. One millisecond face alignment with an ensemble of regression trees. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1867\u20131874, 2014.\\n\\n[20] Sunnie S. Kim, Nicholas I. Kolkin, Jason Salavon, and Gregory Shakhnarovich. Deformable style transfer, 2020.\\n\\n[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[22] Marek Kowalski, Jacek Naruniec, and Tomasz Trzcinski. Deep alignment network: A convolutional neural network for robust face alignment. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2017.\\n\\n[23] Vuong Le, Jonathan Brandt, Zhe Lin, Lubomir Bourdev, and Thomas S. Huang. Interactive Facial Feature Localization, page 679\u2013692. 2012.\\n\\n[24] Hui Li, Zidong Guo, Seon-Min Rhee, Seungju Han, and Jae-Joon Han. Towards accurate facial landmark detection via cascaded transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4176\u20134185, 2022.\\n\\n[25] Yunsheng Li, Lu Yuan, and Nuno Vasconcelos. Bidirectional learning for domain adaptation of semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6936\u20136945, 2019.\\n\\n[26] Feng Liu, Dan Zeng, Qijun Zhao, and Xiaoming Liu. Joint face alignment and 3d face reconstruction. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V, pages 545\u2013560. Springer, 2016.\\n\\n[27] Shaolei Liu, Xiaoyuan Luo, Kexue Fu, Manning Wang, and Zhijian Song. A learnable self-supervised task for unsupervised domain adaptation on point cloud classification and segmentation. Frontiers of Computer Science, 17(6):176708, 2023.\"}"}
{"id": "CVPR-2024-984", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 212\u2013220, 2017.\\n\\nXiao-Chang Liu, Yong-Liang Yang, and Peter Hall. Learning to warp for style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3702\u20133711, 2021.\\n\\nMingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97\u2013105. PMLR, 2015.\\n\\nMingsheng Long, Han Zhu, Jianmin Wang, and Michael Jordan. Unsupervised domain adaptation with residual transfer networks. Advances in neural information processing systems, 29, 2016.\\n\\nJiangjing Lv, Xiaohu Shao, Junliang Xing, Cheng Cheng, and Xi Zhou. A deep regression architecture with two-stage re-initialization for high performance facial landmark detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3317\u20133326, 2017.\\n\\nIacopo Masi, Stephen Rawls, G\u00e9rard Medioni, and Prem Natarajan. Pose-aware face recognition in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4838\u20134846, 2016.\\n\\nKieron Messer, Jiri Matas, Josef Kittler, Juergen Luettin, Gilbert Maitre, et al. Xm2vtsdb: The extended m2vts database. In Second international conference on audio and video-based biometric person authentication, pages 965\u2013966. Citeseer, 1999.\\n\\nZhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Multi-adversarial domain adaptation. In Thirty-second AAAI conference on artificial intelligence, 2018.\\n\\nAndr\u00e9s Prados-Torreblanca, Jos\u00e9 M Buenaposada, and Luis Baumela. Shape preserving facial landmarks with graph attention networks. arXiv preprint arXiv:2210.07233, 2022.\\n\\nJoseph Roth, Yiying Tong, and Xiaoming Liu. Unconstrained 3d face reconstruction. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2606\u20132615, 2015.\\n\\nChristos Sagonas, Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 300 faces in-the-wild challenge: The first facial landmark localization challenge. In 2013 IEEE International Conference on Computer Vision Workshops, 2014.\\n\\nAhmed Selim, Mohamed Elgharib, and Linda Doyle. Painting style transfer for head portraits using convolutional neural networks. ACM Transactions on Graphics (ToG), 35(4):1\u201318, 2016.\\n\\nYichun Shi, Debayan Deb, and Anil K Jain. Warpgan: Automatic caricature generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10762\u201310771, 2019.\\n\\nBaochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. Domain adaptation in computer vision applications, pages 153\u2013171, 2017.\\n\\nKe Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nYaniv Taigman, Ming Yang, Marc\u2019Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to human-level performance in face verification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1701\u20131708, 2014.\\n\\nZachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II, pages 402\u2013419. Springer, 2020.\\n\\nYi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7472\u20137481, 2018.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nJingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 43(10):3349\u20133364, 2020.\\n\\nXinyao Wang, Liefeng Bo, and Li Fuxin. Adaptive wing loss for robust face alignment via heatmap regression. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.\\n\\nKunhong Wu, Fan Jia, and Yahong Han. Domain-specific feature elimination: multi-source domain adaptation for image classification. Frontiers of Computer Science, 17(4):174705, 2023.\\n\\nRuizheng Wu, Xiaodong Gu, Xin Tao, Xiaoyong Shen, Yu-Wing Tai, et al. Landmark assisted cyclegan for cartoon face generation. arXiv preprint arXiv:1907.01424, 2019.\\n\\nWayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai, and Qiang Zhou. Look at boundary: A boundary-aware face alignment algorithm. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2129\u20132138, 2018.\\n\\nJiahao Xia, Weiwei Qu, Wenjian Huang, Jianguo Zhang, Xi Wang, and Min Xu. Sparse local patch transformer for robust face alignment and landmarks inherent relation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4052\u20134061, 2022.\\n\\nJiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen, Hongdong Li, and Gang Hua. Neural aggregation network for video face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition.\"}"}
{"id": "CVPR-2024-984", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yanchao Yang and Stefano Soatto. FDA: Fourier domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4085\u20134095, 2020.\\n\\nJordan Yaniv, Yael Newman, and Ariel Shamir. The face of art: landmark detection and geometric style in portraits. ACM Transactions on Graphics (TOG), 38(4):1\u201315, 2019.\\n\\nJing Zhang, Zewei Ding, Wanqing Li, and Philip Ogunbona. Importance weighted adversarial nets for partial domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8156\u20138164, 2018.\\n\\nZhenglin Zhou, Huaxia Li, Hong Liu, Nanyang Wang, Gang Yu, and Rongrong Ji. Star loss: Reducing semantic ambiguity in facial landmark detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15475\u201315484, 2023.\\n\\nZhou, Erjin and Fan, Haoqiang and Cao, Zhimin and Jiang, Yuning and Yin, Qi. Extensive Facial Landmark Localization with Coarse-to-Fine Convolutional Network Cascade. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops, 2013.\\n\\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.\\n\\nXiangxin Zhu and Deva Ramanan. Face detection, pose estimation, and landmark localization in the wild. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2879\u20132886. IEEE, 2012.\\n\\nYi Zhu, Xindong Wu, Jipeng Qiang, Yunhao Yuan, and Yun Li. Representation learning via an integrated autoencoder for unsupervised domain adaptation. Frontiers of Computer Science, 17(5):175334, 2023.\"}"}
