{"id": "CVPR-2022-361", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stage 3\\n\\n---------\\n\\nStage 3-1\\n\\nStage 3-2\\n\\nStage 3-3\\n\\nStage 3-4\\n\\nStage 3-5\\n\\nStage 3-6\\n\\nStage 3-7\\n\\nStage 3-8\\n\\nStage 3-9\\n\\nStage 3-10\\n\\nStage 3-11\\n\\nStage 3-12\\n\\nStage 3-13\\n\\nStage 3-14\\n\\nStage 3-15\\n\\nStage 3-16\\n\\nStage 3-17\\n\\nStage 3-18\\n\\nStage 3-19\\n\\nStage 3-20\\n\\nStage 3-21\\n\\nStage 3-22\\n\\nStage 3-23\\n\\nStage 3-24\\n\\nStage 3-25\\n\\nStage 3-26\\n\\nStage 3-27\\n\\nStage 3-28\\n\\nStage 3-29\\n\\nStage 3-30\\n\\nStage 3-31\\n\\nStage 3-32\\n\\nStage 3-33\\n\\nStage 3-34\\n\\nStage 3-35\\n\\nStage 3-36\\n\\nStage 3-37\\n\\nStage 3-38\\n\\nStage 3-39\\n\\nStage 3-40\\n\\nStage 3-41\\n\\nStage 3-42\\n\\nStage 3-43\\n\\nStage 3-44\\n\\nStage 3-45\\n\\nStage 3-46\\n\\nStage 3-47\\n\\nStage 3-48\\n\\nStage 3-49\\n\\nStage 3-50\\n\\nStage 3-51\\n\\nStage 3-52\\n\\nStage 3-53\\n\\nStage 3-54\\n\\nStage 3-55\\n\\nStage 3-56\\n\\nStage 3-57\\n\\nStage 3-58\\n\\nStage 3-59\\n\\nStage 3-60\\n\\nStage 3-61\\n\\nStage 3-62\\n\\nStage 3-63\\n\\nStage 3-64\\n\\nStage 3-65\\n\\nStage 3-66\\n\\nStage 3-67\\n\\nStage 3-68\\n\\nStage 3-69\\n\\nStage 3-70\\n\\nStage 3-71\\n\\nStage 3-72\\n\\nStage 3-73\\n\\nStage 3-74\\n\\nStage 3-75\\n\\nStage 3-76\\n\\nStage 3-77\\n\\nStage 3-78\\n\\nStage 3-79\\n\\nStage 3-80\\n\\nStage 3-81\\n\\nStage 3-82\\n\\nStage 3-83\\n\\nStage 3-84\\n\\nStage 3-85\\n\\nStage 3-86\\n\\nStage 3-87\\n\\nStage 3-88\\n\\nStage 3-89\\n\\nStage 3-90\\n\\nStage 3-91\\n\\nStage 3-92\\n\\nStage 3-93\\n\\nStage 3-94\\n\\nStage 3-95\\n\\nStage 3-96\\n\\nStage 3-97\\n\\nStage 3-98\\n\\nStage 3-99\\n\\nStage 3-100\\n\\nStage 3-101\\n\\nStage 3-102\\n\\nStage 3-103\\n\\nStage 3-104\\n\\nStage 3-105\\n\\nStage 3-106\\n\\nStage 3-107\\n\\nStage 3-108\\n\\nStage 3-109\\n\\nStage 3-110\\n\\nStage 3-111\\n\\nStage 3-112\\n\\nStage 3-113\\n\\nStage 3-114\\n\\nStage 3-115\\n\\nStage 3-116\\n\\nStage 3-117\\n\\nStage 3-118\\n\\nStage 3-119\\n\\nStage 3-120\\n\\nStage 3-121\\n\\nStage 3-122\\n\\nStage 3-123\\n\\nStage 3-124\\n\\nStage 3-125\\n\\nStage 3-126\\n\\nStage 3-127\\n\\nStage 3-128\\n\\nStage 3-129\\n\\nStage 3-130\\n\\nStage 3-131\\n\\nStage 3-132\\n\\nStage 3-133\\n\\nStage 3-134\\n\\nStage 3-135\\n\\nStage 3-136\\n\\nStage 3-137\\n\\nStage 3-138\\n\\nStage 3-139\\n\\nStage 3-140\\n\\nStage 3-141\\n\\nStage 3-142\\n\\nStage 3-143\\n\\nStage 3-144\\n\\nStage 3-145\\n\\nStage 3-146\\n\\nStage 3-147\\n\\nStage 3-148\\n\\nStage 3-149\\n\\nStage 3-150\\n\\nStage 3-151\\n\\nStage 3-152\\n\\nStage 3-153\\n\\nStage 3-154\\n\\nStage 3-155\\n\\nStage 3-156\\n\\nStage 3-157\\n\\nStage 3-158\\n\\nStage 3-159\\n\\nStage 3-160\\n\\nStage 3-161\\n\\nStage 3-162\\n\\nStage 3-163\\n\\nStage 3-164\\n\\nStage 3-165\\n\\nStage 3-166\\n\\nStage 3-167\\n\\nStage 3-168\\n\\nStage 3-169\\n\\nStage 3-170\\n\\nStage 3-171\\n\\nStage 3-172\\n\\nStage 3-173\\n\\nStage 3-174\\n\\nStage 3-175\\n\\nStage 3-176\\n\\nStage 3-177\\n\\nStage 3-178\\n\\nStage 3-179\\n\\nStage 3-180\\n\\nStage 3-181\\n\\nStage 3-182\\n\\nStage 3-183\\n\\nStage 3-184\\n\\nStage 3-185\\n\\nStage 3-186\\n\\nStage 3-187\\n\\nStage 3-188\\n\\nStage 3-189\\n\\nStage 3-190\\n\\nStage 3-191\\n\\nStage 3-192\\n\\nStage 3-193\\n\\nStage 3-194\\n\\nStage 3-195\\n\\nStage 3-196\\n\\nStage 3-197\\n\\nStage 3-198\\n\\nStage 3-199\\n\\nStage 3-200\\n\\nStage 3-201\\n\\nStage 3-202\\n\\nStage 3-203\\n\\nStage 3-204\\n\\nStage 3-205\\n\\nStage 3-206\\n\\nStage 3-207\\n\\nStage 3-208\\n\\nStage 3-209\\n\\nStage 3-210\\n\\nStage 3-211\\n\\nStage 3-212\\n\\nStage 3-213\\n\\nStage 3-214\\n\\nStage 3-215\\n\\nStage 3-216\\n\\nStage 3-217\\n\\nStage 3-218\\n\\nStage 3-219\\n\\nStage 3-220\\n\\nStage 3-221\\n\\nStage 3-222\\n\\nStage 3-223\\n\\nStage 3-224\\n\\nStage 3-225\\n\\nStage 3-226\\n\\nStage 3-227\\n\\nStage 3-228\\n\\nStage 3-229\\n\\nStage 3-230\\n\\nStage 3-231\\n\\nStage 3-232\\n\\nStage 3-233\\n\\nStage 3-234\\n\\nStage 3-235\\n\\nStage 3-236\\n\\nStage 3-237\\n\\nStage 3-238\\n\\nStage 3-239\\n\\nStage 3-240\\n\\nStage 3-241\\n\\nStage 3-242\\n\\nStage 3-243\\n\\nStage 3-244\\n\\nStage 3-245\\n\\nStage 3-246\\n\\nStage 3-247\\n\\nStage 3-248\\n\\nStage 3-249\\n\\nStage 3-250\\n\\nStage 3-251\\n\\nStage 3-252\\n\\nStage 3-253\\n\\nStage 3-254\\n\\nStage 3-255\\n\\nStage 3-256\\n\\nStage 3-257\\n\\nStage 3-258\\n\\nStage 3-259\\n\\nStage 3-260\\n\\nStage 3-261\\n\\nStage 3-262\\n\\nStage 3-263\\n\\nStage 3-264\\n\\nStage 3-265\\n\\nStage 3-266\\n\\nStage 3-267\\n\\nStage 3-268\\n\\nStage 3-269\\n\\nStage 3-270\\n\\nStage 3-271\\n\\nStage 3-272\\n\\nStage 3-273\\n\\nStage 3-274\\n\\nStage 3-275\\n\\nStage 3-276\\n\\nStage 3-277\\n\\nStage 3-278\\n\\nStage 3-279\\n\\nStage 3-280\\n\\nStage 3-281\\n\\nStage 3-282\\n\\nStage 3-283\\n\\nStage 3-284\\n\\nStage 3-285\\n\\nStage 3-286\\n\\nStage 3-287\\n\\nStage 3-288\\n\\nStage 3-289\\n\\nStage 3-290\\n\\nStage 3-291\\n\\nStage 3-292\\n\\nStage 3-293\\n\\nStage 3-294\\n\\nStage 3-295\\n\\nStage 3-296\\n\\nStage 3-297\\n\\nStage 3-298\\n\\nStage 3-299\\n\\nStage 3-300\\n\\nStage 3-301\\n\\nStage 3-302\\n\\nStage 3-303\\n\\nStage 3-304\\n\\nStage 3-305\\n\\nStage 3-306\\n\\nStage 3-307\\n\\nStage 3-308\\n\\nStage 3-309\\n\\nStage 3-310\\n\\nStage 3-311\\n\\nStage 3-312\\n\\nStage 3-313\\n\\nStage 3-314\\n\\nStage 3-315\\n\\nStage 3-316\\n\\nStage 3-317\\n\\nStage 3-318\\n\\nStage 3-319\\n\\nStage 3-320\\n\\nStage 3-321\\n\\nStage 3-322\\n\\nStage 3-323\\n\\nStage 3-324\\n\\nStage 3-325\\n\\nStage 3-326\\n\\nStage 3-327\\n\\nStage 3-328\\n\\nStage 3-329\\n\\nStage 3-330\\n\\nStage 3-331\\n\\nStage 3-332\\n\\nStage 3-333\\n\\nStage 3-334\\n\\nStage 3-335\\n\\nStage 3-336\\n\\nStage 3-337\\n\\nStage 3-338\\n\\nStage 3-339\\n\\nStage 3-340\\n\\nStage 3-341\\n\\nStage 3-342\\n\\nStage 3-343\\n\\nStage 3-344\\n\\nStage 3-345\\n\\nStage 3-346\\n\\nStage 3-347\\n\\nStage 3-348\\n\\nStage 3-349\\n\\nStage 3-350\\n\\nStage 3-351\\n\\nStage 3-352\\n\\nStage 3-353\\n\\nStage 3-354\\n\\nStage 3-355\\n\\nStage 3-356\\n\\nStage 3-357\\n\\nStage 3-358\\n\\nStage 3-359\\n\\nStage 3-360\\n\\nStage 3-361\\n\\nStage 3-362\\n\\nStage 3-363\\n\\nStage 3-364\\n\\nStage 3-365\\n\\nStage 3-366\\n\\nStage 3-367\\n\\nStage 3-368\\n\\nStage 3-369\\n\\nStage 3-370\\n\\nStage 3-371\\n\\nStage 3-372\\n\\nStage 3-373\\n\\nStage 3-374\\n\\nStage 3-375\\n\\nStage 3-376\\n\\nStage 3-377\\n\\nStage 3-378\\n\\nStage 3-379\\n\\nStage 3-380\\n\\nStage 3-381\\n\\nStage 3-382\\n\\nStage 3-383\\n\\nStage 3-384\\n\\nStage 3-385\\n\\nStage 3-386\\n\\nStage 3-387\\n\\nStage 3-388\\n\\nStage 3-389\\n\\nStage 3-390\\n\\nStage 3-391\\n\\nStage 3-392\\n\\nStage 3-393\\n\\nStage 3-394\\n\\nStage 3-395\\n\\nStage 3-396\\n\\nStage 3-397\\n\\nStage 3-398\\n\\nStage 3-399\\n\\nStage 3-400\\n\\nStage 3-401\\n\\nStage 3-402\\n\\nStage 3-403\\n\\nStage 3-404\\n\\nStage 3-405\\n\\nStage 3-406\\n\\nStage 3-407\\n\\nStage 3-408\\n\\nStage 3-409\\n\\nStage 3-410\\n\\nStage 3-411\\n\\nStage 3-412\\n\\nStage 3-413\\n\\nStage 3-414\\n\\nStage 3-415\\n\\nStage 3-416\\n\\nStage 3-417\\n\\nStage 3-418\\n\\nStage 3-419\\n\\nStage 3-420\\n\\nStage 3-421\\n\\nStage 3-422\\n\\nStage 3-423\\n\\nStage 3-424\\n\\nStage 3-425\\n\\nStage 3-426\\n\\nStage 3-427\\n\\nStage 3-428\\n\\nStage 3-429\\n\\nStage 3-430\\n\\nStage 3-431\\n\\nStage 3-432\\n\\nStage 3-433\\n\\nStage 3-434\\n\\nStage 3-435\\n\\nStage 3-436\\n\\nStage 3-437\\n\\nStage 3-438\\n\\nStage 3-439\\n\\nStage 3-440\\n\\nStage 3-441\\n\\nStage 3-442\\n\\nStage 3-443\\n\\nStage 3-444\\n\\nStage 3-445\\n\\nStage 3-446\\n\\nStage 3-447\\n\\nStage 3-448\\n\\nStage 3-449\\n\\nStage 3-450\\n\\nStage 3-451\\n\\nStage 3-452\\n\\nStage 3-453\\n\\nStage 3-454\\n\\nStage 3-455\\n\\nStage 3-456\\n\\nStage 3-457\\n\\nStage 3-458\\n\\nStage 3-459\\n\\nStage 3-460\\n\\nStage 3-461\\n\\nStage 3-462\\n\\nStage 3-46"}
{"id": "CVPR-2022-361", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep trackers have an intrinsic requirement for strict translation invariance, which is evitable to destroy the translation invariance. Thus, deep trackers, such as TransT [5] and STARK [50], provide more expressive features while their padding is inevitable to destroy the translation invariance. Therefore, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial-aware sampling training strategy to keep the translation invariance. Theoretically, padding in SBT can be removed completely or only exists in patch embedding for easy implementation. Moreover, the flattened feature shape. To obtain the correlation feature\\n\\n\\\\[ \\\\tilde{x}^{2} = \\\\text{Softmax}(\\\\text{Inter}) \\\\]\\n\\nis the weight matrix and bias vector \\\\[ W \\\\times z \\\\], \\\\[ b \\\\].\\n\\nAttn \\\\[ x, z \\\\] is the correlation operation. Modern backbones [17, 48] can provide more expressive features while their padding is inevitable to destroy the translation invariance. Thus, deep trackers need to adopt spatial"}
{"id": "CVPR-2022-361", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Comparison on the GOT-10k [19] test set.\\n\\n| Tracker        | AUC \u2191 | Prec \u2191 |\\n|----------------|-------|--------|\\n| SiamRPN++      | 61.1  | 61.5   |\\n| ATOM           | 65.2  | 66.0   |\\n| DiMP           | 67.1  | 67.2   |\\n| AutoMatch      | 68.8  | 60.2   |\\n| Siam TransT    | 69.9  | 70.4   |\\n| DTT s50        | 72.5  | 72.9   |\\n| Siam TransT s101 | 74.0  | 74.3   |\\n\\nTable 4. Comparison on the LaSOT [13] test set.\\n\\n| Tracker        | AUC \u2191 | Prec \u2191 |\\n|----------------|-------|--------|\\n| Ocean          | 61.6  | 63.4   |\\n| ATOM           | 71.7  | 69.7   |\\n| SiamMask       | 76.6  | 76.6   |\\n| SuperDiMP      | 76.8  | 76.1   |\\n| STM DET50      | 78.1  | 68.5   |\\n| AlphaRef       | 77.3  | 77.3   |\\n| Siam TransT    | 80.4  | 80.8   |\\n| DTT s50        | 82.5  | 82.5   |\\n| Siam TransT s101 | 84.0  | 84.0   |\\n\\nTable 5. Results on VOT2020. We use AlphaRefine [51] to generate masks for VOT benchmark.\\n\\n| Tracker          | Acc. \u2191 | Rob. \u2191 | EAO \u2191 |\\n|------------------|--------|--------|-------|\\n| DiMP             | 0.693  | 0.754  | 0.430 |\\n| SiamFC++         | 0.462  | 0.734  | 0.271 |\\n| DualTFR          | 0.624  | 0.648  | 0.321 |\\n| TransT           | 0.492  | 0.745  | 0.305 |\\n| STARK-s50        | 0.751  | 0.574  | 0.308 |\\n| SBT-light        | 0.679  | 0.787  | 0.441 |\\n| SBT-small        | 0.754  | 0.777  | 0.482 |\\n| SBT-base         | 0.754  | 0.749  | 0.462 |\\n| SBT-large        | 0.761  | 0.789  | 0.497 |\\n\\nTable 6. Comparison on the TrackingNet test set.\\n\\n| Tracker          | AUC \u2191 | Norm.Prec \u2191 |\\n|------------------|-------|-------------|\\n| SiamRPN++        | 74.0  | 80.1        |\\n| ATOM             | 75.4  | 80.0        |\\n| DiMP             | 80.1  | 84.9        |\\n| SiamTransT s50   | 81.4  | 86.7        |\\n| SBT-light        | 79.6  | 85.0        |\\n| SBT-small        | 80.3  | 85.1        |\\n| SBT-base         | 81.9  | 74.5        |\\n| SBT-large        | 82.2  | 87.1        |\\n\\n5.3. Correlation-Aware Feature for Other Trackers\\n\\nWe replace the backbone in four typical trackers with SBT, which are named as Correlation-Aware Trackers.\\n\\n6. Experiments\\n\\nThis section describes the implementation details, comparisons to the state-of-the-art (sota) trackers and improvements in CATs. Exploration studies are also provided.\\n\\n6.1. Implementation Details\\n\\nImageNet pre-training. We firstly train 4-stage SBT with classification head on the ImageNet [33]. Similar to the network for image classification, our model structure and data flow is one-stream. The setting mostly follows [35] and [43]. We employ the AdamW [27] optimizer for 300 epochs. The input image is resized to 224 \u00d7 224 and the augmentation and regularization strategies of [35] are adopted.\\n\\nFinetune on tracking. Next, the pre-trained weights are used to initialize our tracking model. By arranging EoC-SA/EoC-CA blocks, the model is still one-stream in structure but two-stream in data flow. For each image pair, we compute standard cross-entropy loss for the classification and GIoU [32] loss and $L_1$ loss for the regression. We use 8 tesla V100 GPUs and set the batch size to be 160. The template and search image size are set to 128 \u00d7 128 and 256 \u00d7 256. The sample pairs of each epoch is 50,000, and the total epoch is 600. The learning rate is set to be $10^{-4}$ for the head, and $10^{-5}$ for the rest and it decays by a factor of $10$ at the 200th, 400th epoch. The training datasets include the train subsets of LaSOT [13], GOT-10K [19], COCO2017 [25], and TrackingNet [29]. Other settings are the same with [5, 46]. Details are in supplement.\\n\\nTable 7. Improvements of CATs over baselines on GOT-10k [19] and DA VIS17 [31] benchmarks.\\n\\n| Tracker        | Box-Level Tracker Params(M) | Flops(G) |\\n|----------------|-----------------------------|----------|\\n|                 | AUC \u2191 | Norm.Prec \u2191 |       |\\n| SiamFC++       | 13.9  | 19.8       | 69.5   |\\n| SiamFC++-CA    | 16.3  | 14.1 (5.7 \u2193) | 74.8   | 54.5 (6.6 \u2191) |\\n| DiMP           | 26.1  | -          | 71.7   | 61.1     |\\n| DiMP-CA        | 26.3  | -          | 74.1 (2.4 \u2191) | 56.8 (7.2 \u2191) |\\n| STARK          | 23.3  | 11.5       | 76.1   | 61.2     |\\n| STARK-CA       | 23.6  | 8.7 (2.8 \u2193) | 77.8 (1.7 \u2191) | 62.7 (1.5 \u2191) |\\n\\n| Tracker        | Pixel-Level Tracker Params(M) | Flops(G) |\\n|----------------|-------------------------------|----------|\\n|                 | J/F Mean                      |          |\\n| STM            | 24.5  | -          | 69.2   | 74.0   |\\n| STM-CA         | 25.1  | -          | 72.8 (3.6 \u2191) | 75.6 (1.6 \u2191) |\\n\\nTesting. For SBT/Siamese, we adopt fixed template as [46]. For DCF/STM, the inputs are firstly fused with template by SBT.\\n\\n6.2. Comparison to State-of-the-Art Trackers\\n\\nGOT-10K. GOT-10K [19] is a large-scale benchmark which has the zero overlap of object classes between training and testing. We follow the official policy without extra training data. As shown in Tab. 3 and Fig. 5, in a fair comparison scenario, our base and large version outperform other top-performing trackers such as STARK-st101, TransT, TrSiam, and DiMP, verifying the strong generalization to unseen objects. Our light and small version also achieve competitive results with much smaller size.\\n\\nOTB100/VOT2020/LaSOT. We refer the reader to [13, 21, 45] for detailed descriptions of datasets. In challenging short-term benchmarks (VOT2020 and OTB100), Tab. 5 shows that SBT-small achieves competitive result, which is better than SuperDiMP. After increasing the model variants, SBT-base obtains an EAO of 0.515, being superior to other top-performing trackers. With a much simpler pipeline, our SBT-large is even closed to the winner of VOT2020 challenge RPT [28]. Fig. 5 shows our base and large version achieves sota results in OTB. In long-term benchmark LaSOT, with the comparable model size and no online update, SBT-base outperforms the recent strong Transformer-based methods (STARK-s50 and TransT).\\n\\n6.3. Improvement over Baselines\\n\\nBox-Level tracking. In Tab. 7, our correlation-aware features improve other tracking pipelines with comparable model size and less computation burden.\\n\\nPixel-Level tracking. In multi-object video object segmentation (VOS) benchmark DA VIS17 [31], STM-CA improves other tracking pipelines with comparable model size and less computation burden.\"}"}
{"id": "CVPR-2022-361", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8. Ablation studies on GOT-10k [19]. S3B6 denotes the third stage 6th block. Fea.Ext denotes the feature extraction network. Corr.Emd denotes whether network embeds correlation into extraction layers. Fea.Cor denotes the feature correlation method. For DCF, we integrate SBT-base to ECO [10].\\n\\nFigure 6. (a), (b), (c) denote three trackers (refer to Sec. 6.4). The first sub-figure indicates the average true positive rate and average negative numbers of negative objects. The other sub-figures denote the T-SNE and classification maps.\\n\\n6.4. Exploration Study\\n\\nWe further explore the characteristics of our SBT feature by training it on the GOT-10k [19] training split.\\n\\nCorrelation-embedded structure. As shown in Tab. 8, correlation-embedded SBT (\u2466, \u2467, \u2468) significantly improves the tracking performance on all correlation cases (\u2463, \u2464, \u2465). Comparing to the layer-wise aggregation, correlation-embedded trackers outperform the CNN-based trackers or attention-based trackers (65.9% of \u2467 Vs. 60.1% of \u2463, 65.0% of \u2466 Vs. 61.5% of \u2464, 39.2% of \u2468 Vs. 30.3% of \u2462). It clearly verifies that SBT structure is more effective on multi-level feature utilization. We also prove that CA works better than DW-Corr in feature correlation (60.1% of \u2463 Vs. 61.5% of \u2464).\\n\\nTarget-dependent feature embedding. We further explore the features of three different settings in two folds: one is to maintain spatial location information while another is to classify the target from distractor objects. We begin by training three models with Cls head only to localize the target: (a) Correlation-embedded tracker. (b) Siamese correlation with SBT. (c) Siamese correlation with ResNet-50. We select the five hard videos from OTB [44] benchmark. The search image randomly jitters around target. We only evaluate the Cls map for localization. In Fig. 6, the true positive rate of target ground-truth indicates that (a), (b) can preserve more spatial information than CNN (c). The T-SNE/Cls map also show the target-dependent characteristic of (a) features. The average negative objects (largest connected components) of (a) is higher than (b) which indicates that correlation-embedded is critical to filter out distractors.\\n\\nBenefits from pre-training. Comparing to the existing trackers [5, 46, 50], our tracking model except prediction heads can be directly benefit from the ImageNet [33] pre-trained weights. As shown in Fig. 8 (a), there is a significant correlation between the number of pre-trained blocks and tracking performance. We also investigate the impacts of model variants of SBT. In Fig. 8 (b), the SBT tracking model prefers consistent block numbers for pre-training. We also observe that SBT converges faster and the stabilized IoU value rises with more pre-trained model weights.\\n\\n7. Conclusion\\n\\nIn this work, we are the first to propose a target-dependent feature network for VOT. Our SBT greatly simplifies the tracking pipeline and converges much faster than recent Transformer-based trackers. Then, we conduct a systematic study on SBT tracking both experimentally and theoretically. Extensive experiments demonstrate that our method achieves sota results and can be applied to other tracking pipelines as dynamic feature network.\\n\\nAcknowledgment\\n\\nThis work was supported by NSFC (No.61773117 and No.62006041) and the National Key Research and Development Program of China under Grant 2021YFC2802002.\"}"}
{"id": "CVPR-2022-361", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] https://github.com/visionml/pytracking. 3, 7\\n\\n[2] Luca Bertinetto, Jack Valmadre, Jo\u00e3o F Henriques, Andrea Vedaldi, and Philip H S Torr. Fully-convolutional siamese networks for object tracking. In ECCVW, 2016. 2\\n\\n[3] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu Timofte. Learning discriminative model prediction for tracking. In ICCV, 2019. 1, 2, 7\\n\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. 4\\n\\n[5] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu. Transformer tracking. In CVPR, pages 8126\u20138135, 2021. 1, 2, 3, 6, 7, 8\\n\\n[6] Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang, and Rongrong Ji. Siamese box adaptive network for visual tracking. In CVPR, 2020. 6\\n\\n[7] Siyuan Cheng, Bineng Zhong, Guorong Li, Xin Liu, Zhenjun Tang, Xianxian Li, and Jing Wang. Learning to filter: Siamese relation network for robust tracking. In CVPR, pages 4421\u20134431, 2021. 2\\n\\n[8] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv preprint arXiv:2104.13840, 2021. 4\\n\\n[9] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882, 2021. 4\\n\\n[10] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. ECO: Efficient convolution operators for tracking. In CVPR, 2017. 8\\n\\n[11] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Atom: Accurate tracking by overlap maximization. In CVPR, 2019. 2, 7\\n\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2020. 3, 4\\n\\n[13] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling. LaSOT: A high-quality benchmark for large-scale single object tracking. In CVPR, 2019. 7\\n\\n[14] Heng Fan and Haibin Ling. Siamese cascaded region proposal networks for real-time visual tracking. In CVPR, 2019. 2\\n\\n[15] Dongyan Guo, Yanyan Shao, Ying Cui, Zhenhua Wang, Liyan Zhang, and Chunhua Shen. Graph attention tracking. In CVPR, pages 9543\u20139552, 2021. 2\\n\\n[16] Dongyan Guo, Jun Wang, Ying Cui, Zhenhua Wang, and Shengyong Chen. SiamCAR: Siamese fully convolutional classification and regression for visual tracking. In CVPR, 2020. 2\\n\\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 3, 6\\n\\n[18] Jo\u00e3o F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed tracking with kernelized correlation filters. In ICVS, 2008. 1, 2\\n\\n[19] Lianghua Huang, Xin Zhao, and Kaiqi Huang. GOT-10k: A large high-diversity benchmark for generic object tracking in the wild. TPAMI, 2019. 1, 4, 7, 8\\n\\n[20] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity benchmark for generic object tracking in the wild. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019. 7\\n\\n[21] Matej Kristan, Aleks Leonardis, Ji\u0159\u00ed Matas, Michael Felsberg, Roman Pflugfelder, Joni-Kristian K\u00e4m\u00e4r\u00e4inen, Martin Danelljan, Luka \u010cehovin Zajc, Alan Luke\u017ei\u010d, Ondrej Drbohlav, et al. The eighth visual object tracking vot2020 challenge results. In ECCVW, 2020. 7\\n\\n[22] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. Siamrpn++: Evolution of siamese visual tracking with very deep networks. In CVPR, 2019. 1, 2, 3, 6, 7\\n\\n[23] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance visual tracking with siamese region proposal network. In CVPR, pages 8971\u20138980, 2018. 1, 2\\n\\n[24] Xin Li, Chao Ma, Baoyuan Wu, Zhenyu He, and Ming-Hsuan Yang. Target-aware deep tracking. In CVPR, pages 1369\u20131378, 2019. 2\\n\\n[25] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 7\\n\\n[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021. 3, 4, 6\\n\\n[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 7\\n\\n[28] Ziang Ma, Linyuan Wang, Haitao Zhang, Wei Lu, and Jun Yin. Rpt: Learning point set representation for siamese visual tracking. arXiv preprint arXiv:2008.03467, 2020. 7\\n\\n[29] Matthias M\u00fcller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, and Bernard Ghanem. Trackingnet: A large-scale dataset and benchmark for object tracking in the wild. In ECCV, 2018. 7\\n\\n[30] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In ICCV, 2019. 7\\n\\n[31] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In CVPR, pages 724\u2013732, 2016. 7\\n\\n[32] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: A metric and a loss for bounding box regression. In CVPR, 2019. 7\"}"}
{"id": "CVPR-2022-361", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, and Michael Bernstein. ImageNet Large scale visual recognition challenge. *IJCV*, 2015.\\n\\n[34] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In *CVPR*, 2015.\\n\\n[35] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In *International Conference on Machine Learning*, pages 10347\u201310357. PMLR, 2021.\\n\\n[36] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. *Journal of machine learning research*, 9(11), 2008.\\n\\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *NIPS*, 2017.\\n\\n[38] Guangting Wang, Chong Luo, Xiaoyan Sun, Zhiwei Xiong, and Wenjun Zeng. Tracking by instance detection: A meta-learning approach. In *CVPR*, 2020.\\n\\n[39] Guangting Wang, Chong Luo, Zhiwei Xiong, and Wenjun Zeng. Spm-tracker: Series-parallel matching for real-time visual object tracking. In *CVPR*, 2019.\\n\\n[40] Ning Wang, Wengang Zhou, Jie Wang, and Houqiang Li. Transformer meets tracker: Exploiting temporal context for robust visual tracking. In *CVPR*, pages 1571\u20131580, 2021.\\n\\n[41] Qiang Wang, Zhu Teng, Junliang Xing, Jin Gao, Weiming Hu, and Stephen Maybank. Learning attentions: residual attentional siamese network for high performance online visual tracking. In *CVPR*, pages 4854\u20134863, 2018.\\n\\n[42] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and Philip H. S. Torr. Fast online object tracking and segmentation: A unifying approach. In *CVPR*, 2019.\\n\\n[43] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions, 2021.\\n\\n[44] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object tracking: A benchmark. In *CVPR*, 2013.\\n\\n[45] Yi Wu, Jongwoo Lim, and Ming Hsuan Yang. Object tracking benchmark. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 37(9):1834\u20131848, 2015.\\n\\n[46] Fei Xie, Chunyu Wang, Guangting Wang, Yang Wankou, and Wenjun Zeng. Learning tracking representations via dual-branch fully transformer networks. In *ICCVW*, 2021.\\n\\n[47] Fei Xie, Wankou Yang, Bo Liu, Kaihua Zhang, Guangting Wang, and Wangmeng Zuo. Learning spatio-appearance memory network for high-performance visual tracking. *ICCVW*, 2021.\\n\\n[48] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In *CVPR*, 2017.\\n\\n[49] Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, and Gang Yu. SiamFC++: towards robust and accurate visual tracking with target estimation guidelines. In *AAAI*, 2020.\\n\\n[50] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and Huchuan Lu. Learning spatio-temporal transformer for visual tracking. *ICCV*, 2021.\\n\\n[51] Bin Yan, Xinyu Zhang, Dong Wang, Huchuan Lu, and Xiaoyun Yang. Alpha-refine: Boosting tracking performance by precise bounding box estimation. *CVPR*, 2021.\\n\\n[52] Bin Yu, Ming Tang, Linyu Zheng, Guibo Zhu, Jinqiao Wang, Hao Feng, Xuetao Feng, and Hanqing Lu. High-performance discriminative tracking with transformers. In *ICCV*, 2021.\\n\\n[53] Bin Yu, Ming Tang, Linyu Zheng, Guibo Zhu, Jinqiao Wang, Hao Feng, Xuetao Feng, and Hanqing Lu. High-performance discriminative tracking with transformers. In *ICCV*, pages 9856\u20139865, 2021.\\n\\n[54] Yuechen Yu, Yilei Xiong, Weilin Huang, and Matthew R Scott. Deformable siamese attention networks for visual object tracking. In *CVPR*, pages 6728\u20136737, 2020.\\n\\n[55] Lichao Zhang, Abel Gonzalez-Garcia, Joost van de Weijer, Martin Danelljan, and Fahad Shahbaz Khan. Learning the model update for siamese trackers. In *ICCV*, 2019.\\n\\n[56] Qinglong Zhang and Yubin Yang. Rest: An efficient transformer for visual recognition. *arXiv preprint arXiv:2105.13677*, 2021.\\n\\n[57] Zhipeng Zhang, Yihao Liu, Xiao Wang, Bing Li, and Weiming Hu. Learn to match: Automatic matching network design for visual tracking. 2021.\\n\\n[58] Zhipeng Zhang and Houwen Peng. Deeper and wider siamese networks for real-time visual tracking. In *CVPR*, 2019.\\n\\n[59] Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, and Weiming Hu. Ocean: Object-aware anchor-free tracking. In *ECCV*, 2020.\\n\\n[60] Linyu Zheng, Ming Tang, Yingying Chen, Jinqiao Wang, and Hanqing Lu. Learning feature embeddings for discriminant model based tracking. In *ECCV*, 2020.\\n\\n[61] Jinghao Zhou, Peng Wang, and Haoyang Sun. Discriminative and robust online learning for siamese visual tracking. In *AAAI*, volume 34, pages 13017\u201313024, 2020.\"}"}
{"id": "CVPR-2022-361", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Correlation-Aware Deep Tracking\\n\\nFei Xie\u2020*, Chunyu Wang\u2021, Guangting Wang\u2021, Yue Cao\u2021, Wankou Yang\u2020, Wenjun Zeng\u2021\\nSoutheast University, China\\nMicrosoft Research Asia\\n\\njaffe0319@gmail.com, chnuwa@microsoft.com, flylight@mail.ustc.edu.cn\\nyuecao@microsoft.com, wkyang@seu.edu.cn, wezeng@microsoft.com\\n\\nAbstract\\nRobustness and discrimination power are two fundamental requirements in visual object tracking. In most tracking paradigms, we find that the features extracted by the popular Siamese-like networks cannot fully discriminatively model the tracked targets and distractor objects, hindering them from simultaneously meeting these two requirements. While most methods focus on designing robust correlation operations, we propose a novel target-dependent feature network inspired by the self-/cross-attention scheme. In contrast to the Siamese-like feature extraction, our network deeply embeds cross-image feature correlation in multiple layers of the feature network. By extensively matching the features of the two images through multiple layers, it is able to suppress non-target features, resulting in instance-varying feature extraction. The output features of the search image can be directly used for predicting target locations without extra correlation step. Moreover, our model can be flexibly pre-trained on abundant unpaired images, leading to notably faster convergence than the existing methods. Extensive experiments show our method achieves the state-of-the-art results while running at real-time. Our feature networks also can be applied to existing tracking pipelines seamlessly to raise the tracking performance.\\n\\n1. Introduction\\nVisual object tracking (VOT) is a long-standing topic in computer vision. There are two fundamental yet competing goals in VOT: on one hand, it needs to recognize the target undergoing large appearance variations; on the other hand, it needs to filter out the distractors in the background which may be very similar to the target.\\n\\nMost appearance-based approaches address this challenge in two perspectives: the first is to learn a more expressive feature embedding space by Siamese-like extraction; the second is to develop a more robust correlation operation, such as Siamese cropping, online filter learning and Transformer-based fusion. Since the modern backbones become the mainstream choice in deep era, most trackers devote to the correlation operation, hoping to discriminate targets from distractors given their features. Despite their great success, few of these tracking paradigms notice that the two competing goals may put the feature network into a target-distractor dilemma, bringing much difficulties to the correlation step. The underlying reasons are three folds: 1) The Siamese encoding process is unaware of the template and search images, which weakens the instance-level discrimination of learned embeddings. 2) There is no explicit modelling for the backbone to learn the decision boundary that separates the two competing goals, leading to a sub-optimal embedding space. 3) Each training video only annotates one single object while arbitrary objects including distractors can be tracked during inference. This gap is further enlarged by 2). Our key insight is that feature extraction should have dynamic instance-varying behaviors to generate \u201cappropriate\u201d\\n\\nFigure 1. Comparison with the state-of-the-arts on GOT-10k [19]. We visualize the AO performance with respect to the model size. All reported trackers follow the official GOT-10k test protocol. Our SBT tracker achieves superior results while multiple trackers (with suffix \u201cCA\u201d) can benefit from our correlation-aware features.\"}"}
{"id": "CVPR-2022-361", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. (a1) standard Siamese-like feature extraction; (a2) our target-dependent feature extraction; (b1) correlation step, such as Siamese cropping correlation [23], DCF [11] and Transformer-based correlation [5]; (b2) our pipeline removes separated correlation step; (c) prediction stage; (d1)/(d2) are the TSNE [36] visualizations of search features in (a1)/(a2) when feature networks go deeper.\\n\\nTo this end, we present a novel dynamic feature network on top of the attention scheme [37]. As shown in Fig. 2 (a2), our Single Branch Transformer (SBT) network allows the features of the two images to deeply interact with each other at the stage of feature extraction. Intuitively, the cross-attention weights gradually filter out target-irrelevant features layer by layer while the self-attention weights enrich the feature representations for better matching. Thus, the feature extraction process is target-dependent and asymmetrical for image pair, allowing the network to achieve a win-win scenario: it differentiates the target from similar distractors while preserving the coherent characteristics among dissimilar targets. The effectiveness of features from SBT is validated in Fig. 2 (d2). The features belonging to the target (green) become more and more separated from the background (pink) and distractors (blue) while the search features from Siamese extraction are totally target-unaware.\\n\\nThe overall framework of SBT is shown in Fig. 3. It has three model stages on top of Extract-or-Correlation (EoC) blocks. The patch embedding produces embeddings for the template and search images. Then the embeddings are fed to the stacked EoC blocks. There are two variants of EoC, i.e., EoC-SA and EoC-CA, which use Self-Attention (SA) and Cross-Attention (CA) as its core operator, respectively. The EoC-SA block fuses features within the same image while the EoC-CA block mixes features across images. The output features of the search image are directly fed to the prediction heads to obtain a spatial score map and a size embedding map. Our key technical innovation is introducing one single stream for template and search image pair processing that jointly extract or correlate through homogeneous attention-based blocks. Thus, SBT can be pre-trained on abundant unpaired images such as ImageNet [33], leading to a fast convergence in the fine-tune on tracking.\\n\\nExtensive experiments are conducted to compare different SBT network designs. Based on the insights, we summarize a number of general principals. Our method achieves superior performance and improves Siamese, DCF and Transformer-based trackers as can be seen in Fig. 1. The main contributions of this work are as follows:\\n\\n\u2022 We present a novel tracking framework which allows the features of the search and template image to be deeply fused for tracking. It further improves existing popular tracking pipelines. To our best, we are the first to propose a specialized target-dependent feature network for VOT.\\n\\n\u2022 We conduct a systematic study on SBT tracking both experimentally and theoretically, and summarize several general principles for following works.\\n\\nThe rest of the paper is organized as follows. We discuss related work in Sec. 2. The SBT framework is presented in Sec. 3. Then, we conduct empirical studies and theoretical analysis on SBT in Sec. 4 and Sec. 5, respectively. Finally, we provide extensive experimental results in Sec. 6 and conclude the paper in Sec. 7.\\n\\n2. Related Work\\n\\nVisual Tracking. The Siamese network [2] based trackers have drawn great attention in recent years. By introducing the powerful backbones [22, 58] and elaborated prediction networks [16, 23, 49], Siamese trackers obtain superior performance. However, the offline target matching with a shallow correlation structure [2] lacks of discriminative power towards distractors. Then, the dedicated modifications rise, including attention mechanism [15, 41, 54], online module [59, 61], cascaded frameworks [7, 14, 39], update mechanism [55] and target-aware model fine-tuning [24,38]. Despite the improvements, most of them bring much complexity to the Siamese tracking pipeline. Instead, our target-dependent feature network can upgrade the original network seamlessly. Moreover, our feature network formulates a novel and conceptually simple tracking pipeline by removing the separated correlation step in Siamese trackers. Discriminative Correlation Filter (DCF) tracker [18] learns a target model by solving least-squares based regression online. It is further improved by fast gradient algorithm [11], end-to-end learning [3, 60] and CNN-based size...\"}"}
{"id": "CVPR-2022-361", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. (a) architecture of our proposed Single Branch Transformer for tracking. Different from Siamese, DCF and Transformer-based methods, it does not have a standalone module for computing correlation. Instead, it embeds correlation in all Cross-Attention layers which exist at different levels of the networks. The fully fused features of the search image are directly fed to Classification Head (Cls Head) and Regression Head (Reg Head) to obtain localization and size embedding maps. (b) shows the structure of a Extract-or-Correlation (EoC) block. (c) shows the difference of EoC-SA and EoC-CA. PaE denotes patch embedding. LN denotes layer normalization.\\n\\n3.1. Patch Embedding\\n\\nOur model takes two images as input, comprising a template image $z \\\\in \\\\mathbb{R}^{3 \\\\times H_z \\\\times W_z}$ and a larger search image $x \\\\in \\\\mathbb{R}^{3 \\\\times H_x \\\\times W_x}$. In general, $z$ is centered on the target object while $x$ represents a larger region in the subsequent frame which contains the target. In the Patch Embedding (Pa.E) stage, the two images are fed to a convolutional layer $\\\\phi_0^p$ with kernel size $7 \\\\times 7$ and stride $4$, followed by a layer normalization (LN) layer. It embeds the images into feature maps of $f_0^z$ and $f_0^x$, respectively.\\n\\n$$f_0^z, f_0^x = LN(\\\\phi_0^p(z)), LN(\\\\phi_0^p(x)),$$\\n\\nwhere $f_0^z \\\\in \\\\mathbb{R}^{C_0 \\\\times H_z \\\\times W_z}$, $f_0^x \\\\in \\\\mathbb{R}^{C_0 \\\\times H_x \\\\times W_x}$ and $C_0$ is the number of channels.\\n\\n3.2. Extract-or-Correlation Block\\n\\nEoC block which can simultaneously implement Self-Attention (SA) and Cross-Attention (CA) is the main building block. Intuitively, they gradually fuse features from the same and different images, respectively. It is known that computing attention globally among all tokens leads to quadratic complexity. To address this, there are a number of works which attempt to reduce the computation cost. We present a general formulation for different efficient attention methods. On top of the formulation, we describe our SA and CA operations.\\n\\nLet $\\\\chi(\\\\cdot)$ denote a function that reshapes/arranges a feature maps into the desired form. The function varies for different methods. We compute the $q, k, v$ features as:\\n\\n$$q_i = [\\\\chi_q(f_i)]^T \\\\omega_q, i \\\\in \\\\{z, x\\\\},$$\\n$$k_i = [\\\\chi_k(f_i)]^T \\\\omega_k, i \\\\in \\\\{z, x\\\\},$$\\n$$v_i = [\\\\chi_v(f_i)]^T \\\\omega_v, i \\\\in \\\\{z, x\\\\},$$\\n\\nwhere $\\\\{\\\\omega_q, \\\\omega_k, \\\\omega_v\\\\}$ represent linear projections.\"}"}
{"id": "CVPR-2022-361", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. The left part compares different factors of SBT including attention computation methods (ATTN), position encoding methods (PE), patch embedding methods (PaE), number of model parameters and flops. The right part compares the rest of factors based on A (described in the left part) such as the feature dimensions (DIM) and the number of blocks (BLK), as well as the stride of the feature maps in each stage. All models unless explained follow the same setting: training from scratch, interleaved EoC-SA/EoC-CA block in the third stage, 128 \u00d7 128 for template image and 256 \u00d7 256 search image.\\n\\n| Setting | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |\\n|---------|---|---|---|---|---|---|---|---|\\n| DIM(1,2) | [64, 128] | [64, 128] | [64, 128] | [64, 128] | [32, 64] |\\n| ATTN | VG | SL | SRG | SRG | SRG | VL/SRG |\\n| DIM(3,4) | [320] | [320, 512] | [320, 512] | [512] | [320] | [320] | [320] |\\n| PE | Abs Rel | Cond | Cond | Cond Rel | Cond | Cond | Cond | Cond |\\n| BLK | [3, 4, 10] | [4, 2, 6, 1] | [2, 2, 6, 2] | [2, 2, 4] | [3, 4, 10] | [2, 4, 6, 1] | [3, 4, 12] | [3, 4, 10] |\\n| PaE | H | H | H | H | H | H | H | H |\\n| STR | [4, 2, 1] | [4, 2, 1, 1] | [4, 2, 1, 1] | [4, 2, 1] | [4, 1, 2] | [4, 2, 1, 1] | [4, 2, 2] | [4, 2, 1] |\\n| Param.(M) | 22.5 | 40.2 | 23.9 | 20.1 | 21.3 | 21.0 | 19.6 | 21.3 |\\n| Param.(M) | 21.3 | 18.6 | 21.1 | 20.5 | 20.8 | 19.3 | 20.8 | 15.1 |\\n| Flops(G) | 35.1 | 36.5 | 20.2 | 18.9 | 19.6 | 19.3 | 17.5 | 19.6 |\\n| Flops(G) | 19.6 | 19.3 | 22.5 | 19.2 | 24.4 | 24.7 | 12.1 | 14.5 |\\n| AO | 47.5 | 56.4 | 63.7 | 61.7 | 63.5 | 63.1 | 60.1 | 63.5 |\\n| AO | 63.5 | 57.4 | 60.9 | 56.7 | 63.3 | 60.6 | 52.2 | 56.2 |\\n\\n1. Setting A does not have hierarchical structure, so we adopt 4 downsampling ratio at the beginning and drops the classification token.\\n2. For Setting A2, we set the same image size (224 \u00d7 224) for template and search image for simplicity.\\n3. H1 denotes the Setting A1 splits an input image into non-overlapping patches (4 \u00d7 4).\\n4. H2 denotes a linear layer to change dimensions after patch split.\\n5. For model settings with total network stride 16, we increase the search image size to 320 \u00d7 320 for a fair comparison.\\n\\nKey and value features. The resolution of the query features is not changed. Then it computes global attention as VG. The method largely reduces the computational overhead. The Vanilla Local window attention (VL) [8] splits feature tokens in groups based on their spatial locations and only computes attention within each group. Swin Transformer [26] further adds a Shift window mechanism to vanilla Local attention (SL) for global modelling.\\n\\nSince the target object may appear anywhere in the search image, it is not practical to use local attention methods for CA. In our work, we use SRG to implement SA and CA. More discussions are in Sec. 4. The following equation shows how we compute SA or CA:\\n\\n$$\\\\tilde{f}_{ij} = \\\\text{Softmax}(q_i k_j^T \\\\sqrt{d_h}) v_j, \\\\quad i, j \\\\in \\\\{z, x\\\\},$$\\n\\n(3)\\n\\nIn SA, i and j are from the same source (either z or x) and the resulting feature update is:\\n\\n$$f_z := f_z + \\\\tilde{f}_{zz}, \\\\quad f_x := f_x + \\\\tilde{f}_{xx},$$\\n\\n(4)\\n\\nIn CA, it mixes the features from different sources:\\n\\n$$f_z := f_z + \\\\tilde{f}_{zx}, \\\\quad f_x := f_x + \\\\tilde{f}_{xz}.$$ \\n\\n(5)\\n\\nWe can see that the correlation between the two images is deeply embedded in feature extraction seamlessly.\\n\\nEoC block also consists of two LN layers and a 2-layer MLP as shown in Fig 3 (b).\\n\\n3.3. Position Encoding\\n\\nFor majority methods [4, 12, 26], the encoding is generated by the sinusoidal functions with Absolute coordinates (Abs) or Relative distances (Rel) between tokens. Being much simpler, Conditional positional encoding [9, 43, 56] (Cond) generates dynamic encoding by convolutional layers. In our model, we add a 3 \u00d7 3 depth-wise convolutional layer $\\\\phi_{pe}$ to MLP before GELU as conditional PE.\\n\\n3.4. Direct Prediction\\n\\nDifferent from the existing tracking methods, we directly add a classification head $\\\\Phi_{cls}$ and regression head $\\\\Phi_{reg}$ on top of the search feature $\\\\hat{f}_x$ from SBT $\\\\Omega$ without additional correlation operations:\\n\\n$$\\\\hat{f}_x = \\\\Omega(z, x), \\\\quad y_{reg} = \\\\Phi_{reg}(\\\\hat{f}_x), \\\\quad y_{cls} = \\\\Phi_{cls}(\\\\hat{f}_x),$$\\n\\n(6)\\n\\nwhere $y_{reg}, y_{cls}$ denote the target regression and classification results to estimate the location and shape of the target.\\n\\nWe implement $\\\\Phi_{reg}$ and $\\\\Phi_{cls}$ by stacking multiple Mix-MLP Blocks (MMB) which can jointly model the dependency between the spatial and channel dimensions of the input features $\\\\hat{f}_i$ in the $i$th MMB:\\n\\n$$\\\\hat{f}_i = \\\\phi_{sp}(\\\\text{RS}(\\\\phi_{cn}(\\\\text{RS}(\\\\hat{f}_{i-1})))),$$\\n\\n(7)\\n\\nwhere $\\\\phi_{sp}$ and $\\\\phi_{cn}$ consist of a linear layer followed by RELU activation. $\\\\text{RS}$ represents reshape. $\\\\phi_{cn}$ is applied to features along the channel dimension, and the weights are shared for all spatial locations. In contrast, the operator $\\\\phi_{sp}$ is shared for all channels.\\n\\n4. Empirical Study of SBT Instantiations\\n\\nIn this section, we conduct empirical studies on SBT variants by raising a number of questions. As efficient attention computing is vital to the SBT, we firstly ablate other network factors including hierarchical structure, position encoding and patch embedding. As shown in Tab. 1, it is obvious that hierarchical structure performs much better than single stage (Setting A1 vs. A2 to A7). Conditional PE only surpasses the relative PE by 4 points (Setting A5 vs. A6). The difference between PE methods is rather small, indicating that PE does not have key impacts on performance.\"}"}
