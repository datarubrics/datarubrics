{"id": "CVPR-2024-2454", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. In International conference on machine learning, pages 284\u2013293. PMLR, 2018.\\n\\n[2] Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the cross-entropy method. Annals of operations research, 134:19\u201367, 2005.\\n\\n[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. IEEE, 2009.\\n\\n[4] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9185\u20139193, 2018.\\n\\n[5] Yinpeng Dong, Shouwei Ruan, Hang Su, Caixin Kang, Xingxing Wei, and Jun Zhu. Viewfool: Evaluating the robustness of visual recognition to adversarial viewpoints. Advances in Neural Information Processing Systems, 35:36789\u201336803, 2022.\\n\\n[6] Yinpeng Dong, Jun Zhu, Xiao-Shan Gao, et al. Isometric 3d adversarial examples in the physical world. Advances in Neural Information Processing Systems, 35:19716\u201319731, 2022.\\n\\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\n[8] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5501\u20135510, 2022.\\n\\n[9] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\\n\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\n[11] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.\\n\\n[12] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering. ACM Transactions on Graphics, 39(6), 2020.\\n\\n[13] Bo Li, Xiaoyang Xie, Xingxing Wei, and Tang Wenting. Ship detection and classification from optical remote sensing images: A survey. Chinese Journal of Aeronautics, 34(3):145\u2013163, 2021.\\n\\n[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022.\\n\\n[15] Maosen Li, Cheng Deng, Tengjiao Li, Junchi Yan, Xinbo Gao, and Heng Huang. Towards transferable targeted attack. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 641\u2013649, 2020.\\n\\n[16] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.\\n\\n[17] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. In International Conference on Learning Representations, 2017.\\n\\n[18] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.\\n\\n[19] William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages 347\u2013353. 1998.\\n\\n[20] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\\n\\n[21] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7210\u20137219, 2021.\\n\\n[22] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2574\u20132582, 2016.\\n\\n[23] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1\u201315, 2022.\\n\\n[24] Muzammal Naseer, Salman Khan, Munawar Hayat, Fahtad Shahbaz Khan, and Fatih Porikli. On generating transferable targeted perturbations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7708\u20137717, 2021.\\n\\n[25] Andrew Nealen, Takeo Igarashi, Olga Sorkine, and Marc Alexa. Laplacian mesh optimization. In Proceedings of the 4th international conference on Computer graphics and interactive techniques in Australasia and Southeast Asia, pages 381\u2013389, 2006.\\n\\n[26] Scott Oslund, Clayton Washington, Andrew So, Tingting Chen, and Hao Ji. Multiview robust adversarial stickers for 924520.\"}"}
{"id": "CVPR-2024-2454", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"arbitrary objects in the physical world.\\n\\nJournal of Computational and Cognitive Engineering, 1(4):152\u2013158, 2022.\\n\\n[27] Waseem Rawat and Zenghui Wang. Deep convolutional neural networks for image classification: A comprehensive review. Neural Computation, 29(9):2352\u20132449, 2017.\\n\\n[28] Shouwei Ruan, Yinpeng Dong, Hang Su, Jianteng Peng, Ning Chen, and Xingxing Wei. Towards viewpoint-invariant visual recognition via adversarial training. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4709\u20134719, 2023.\\n\\n[29] Shouwei Ruan, Yinpeng Dong, Hang Su, Jianteng Peng, Ning Chen, and Xingxing Wei. Improving viewpoint robustness for visual recognition via adversarial training. arXiv preprint arXiv:2307.11528, 2023.\\n\\n[30] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510\u20134520, 2018.\\n\\n[31] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\\n\\n[32] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5459\u20135469, 2022.\\n\\n[33] Naufal Suryanto, Yongsu Kim, Hyoeun Kang, Harashta Tatimma Larasati, Youngyeo Yun, Thi-Thu-Huong Le, Hunmin Yang, Se-Yoon Oh, and Howon Kim. Dta: Physical camouflage attacks using differentiable transformation network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15305\u201315314, 2022.\\n\\n[34] Naufal Suryanto, Yongsu Kim, Harashta Tatimma Larasati, Hyoeun Kang, Thi-Thu-Huong Le, Yoonyoung Hong, Hunmin Yang, Se-Yoon Oh, and Howon Kim. Active: Towards highly transferable 3D physical camouflage for universal and robust vehicle evasion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4305\u20134314, 2023.\\n\\n[35] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\\n\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826, 2016.\\n\\n[37] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105\u20136114. PMLR, 2019.\\n\\n[38] Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Erruie Ding, Jingdong Wang, and Gang Zeng. Delicate textured mesh recovery from nerf via adaptive surface refinement. arXiv preprint arXiv:2303.02091, 2023.\\n\\n[39] Simen Thys, Wiebe Van Ranst, and Toon Goedem\u00e9. Fooling automated surveillance cameras: adversarial patches to attack person detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 0\u20130, 2019.\\n\\n[40] Donghua Wang, Tingsong Jiang, Jialiang Sun, Weien Zhou, Zhiqiang Gong, Xiaoya Zhang, Wen Yao, and Xiaoqian Chen. Fca: Learning a 3D full-coverage vehicle camouflage for multi-view physical adversarial attack. In Proceedings of the AAAI conference on artificial intelligence, pages 2414\u20132422, 2022.\\n\\n[41] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3D mesh models from single RGB images. In Proceedings of the European conference on computer vision (ECCV), pages 52\u201367, 2018.\\n\\n[42] Zhibo Wang, Hongshan Yang, Yunhe Feng, Peng Sun, Hengchang Guo, Zhifei Zhang, and Kui Ren. Towards transferable targeted adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20534\u201320543, 2023.\\n\\n[43] Xingxing Wei, Ying Guo, and Jie Yu. Adversarial sticker: A stealthy attack method in the physical world. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):2711\u20132725, 2023.\\n\\n[44] Xingxing Wei, Ying Guo, Jie Yu, and Bo Zhang. Simultaneously optimizing perturbations and positions for black-box adversarial patch attacks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(7):9041\u20139054, 2023.\\n\\n[45] Xingxing Wei, Yao Huang, Yitong Sun, and Jie Yu. Unified adversarial patch for cross-modal attacks in the physical world. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4445\u20134454, 2023.\\n\\n[46] Xingxing Wei, Yao Huang, Yitong Sun, and Jie Yu. Unified adversarial patch for visible-infrared cross-modal attacks in the physical world. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\\n\\n[47] Xingxing Wei, Jie Yu, and Yao Huang. Physically adversarial infrared patches with learnable shapes and locations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12334\u201312342, 2023.\\n\\n[48] Xingxing Wei, Jie Yu, and Yao Huang. Infrared adversarial patches with learnable shapes and locations in the physical world. International Journal of Computer Vision, pages 1\u201317, 2023.\\n\\n[49] Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, and Mingyan Liu. Meshadv: Adversarial meshes for visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6898\u20136907, 2019.\\n\\n[50] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L. Yuille. Improving transferability of adversarial examples with input diversity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6898\u20136907, 2019.\"}"}
{"id": "CVPR-2024-2454", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Xiao Yang, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Boosting transferability of targeted adversarial examples via hierarchical generative networks. In European Conference on Computer Vision, pages 725\u2013742. Springer, 2022.\\n\\nXiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi Xie, Yu-Wing Tai, Chi-Keung Tang, and Alan L Yuille. Adversarial attacks beyond the image space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4302\u20134311, 2019.\\n\\nZhengyu Zhao, Zhuoran Liu, and Martha Larson. On success and simplicity: A second look at transferable targeted attacks. Advances in Neural Information Processing Systems, 34:6115\u20136128, 2021.\\n\\nZhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection with deep learning: A review. IEEE transactions on neural networks and learning systems, 30(11):3212\u20133232, 2019.\"}"}
{"id": "CVPR-2024-2454", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In essence, such a dual optimization strategy facilitates the embedding of adversarial perturbations at both the foundational feature level and the more sophisticated decision-making echelons of the neural network, thus yielding more transferable 3D adversarial examples while also ensuring naturalness. It is also noteworthy that though merely optimizing the geometric shape proves challenging for achieving transferable targeted 3D attacks, optimizing the texture in conjunction with adjustments to the geometric shape (i.e., mesh vertex coordinates) can enhance transferability to a certain extent, as verified in Sec. 4.2.1. However, fine-tuning the geometric parameters in the grid-based NeRF space will lead to significant deformation, and ensuring naturalness by restricting the distance to the original mesh\u2019s geometry in such a space would require repeatedly using the marching cubes [19] technique to convert geometric parameters into mesh during optimization. This incurs a substantial time cost. Therefore, we make a compromise that explicitly modifies the mesh vertex coordinates.\\n\\nTo sum up, we target the grid-based NeRF\u2019s inner parameters: \\\\( (\\\\Theta^G_{\\\\text{tex}}, \\\\Theta^M_{\\\\text{tex}}) \\\\) instead of vertex colors \\\\( T \\\\) in appearance, accompanying the optimization to mesh\u2019s vertex coordinates, thus the objective function (3.2) becomes:\\n\\n\\\\[\\n\\\\min_{V^*, \\\\Theta^*_{G\\\\text{tex}}, \\\\Theta^*_{M\\\\text{tex}}} E_{v \\\\in V_L} (\\\\hat{I}_v(M_{\\\\text{adv}}), y^*) + \\\\beta \\\\cdot R(M_{\\\\text{adv}}, M) .\\n\\\\]\\n\\n3.4. Regularization for Naturalness\\n\\nTo ensure the perceptible naturalness of 3D adversarial examples, we impose constraints to both appearance and geometry. For the constraint of appearance, we evaluate the disparity between images rendered under various viewpoints from the adversarial textured mesh \\\\( M_{\\\\text{adv}} \\\\) and those rendered from the initial clean textured mesh \\\\( M \\\\), which is achieved by calculating the squared distance between these two sets of images. This appearance-related constraint, denoted as \\\\( R_{\\\\text{rgb}} \\\\), ensures that the adversarial examples, while effective, do not deviate significantly in appearance from the original mesh and can be formulated as follows:\\n\\n\\\\[\\nR_{\\\\text{rgb}}(M_{\\\\text{adv}}, M) = \\\\frac{1}{N} \\\\sum_{v \\\\in V} (\\\\hat{I}_v(M_{\\\\text{adv}}) - \\\\hat{I}_v(M))^2 ,\\n\\\\]\\n\\nwhere \\\\( N \\\\) is the sampled number for one epoch.\\n\\nThen, referring to the constraint in geometry, we incorporate the Chamfer distance \\\\( R_{\\\\text{cd}} \\\\) to make vertices of \\\\( M_{\\\\text{adv}} \\\\) not far from their initial positions in \\\\( M \\\\), the Laplacian Smoothing Loss \\\\( R_{\\\\text{lap}} \\\\) [25] for preventing self-intersecting, and the Mesh Edge Length Loss \\\\( R_{\\\\text{edge}} \\\\) [41] for the smoothness of the mesh surface. Above all, the whole regularization \\\\( R \\\\) could be represented as follows:\\n\\n\\\\[\\nR(M_{\\\\text{adv}}, M) = \\\\lambda_1 R_{\\\\text{rgb}}(M_{\\\\text{adv}}, M) + \\\\lambda_2 R_{\\\\text{cd}}(M_{\\\\text{adv}}, M) + \\\\lambda_3 R_{\\\\text{lap}}(M_{\\\\text{adv}}) + \\\\lambda_4 R_{\\\\text{edge}}(M_{\\\\text{adv}}) ,\\n\\\\]\\n\\nwhere \\\\( \\\\lambda_1, \\\\lambda_2, \\\\lambda_3 \\\\) and \\\\( \\\\lambda_4 \\\\) are hyperparameters that represent weights of \\\\( R_{\\\\text{rgb}}, R_{\\\\text{cd}}, R_{\\\\text{lap}}, \\\\) and \\\\( R_{\\\\text{edge}} \\\\), respectively.\\n\\n3.5. Physical Attack\\n\\nTo realize physically feasible 3D adversarial examples, we need to ensure their robustness against complex transformations in the physical world, including 3D rotations, affine projections, color discrepancies, etc. A prevalent technique that we utilize here is the Expectation Over Transformation (EOT) algorithm [1], which optimizes the adversarial example across a distribution of varying transformations in both 2D and 3D spaces. Specifically, we effectively merge diverse 3D transformations, such as pose, distance, and viewpoint shifts, during rendering with 2D transformations such as contrast and blurring on rendered images. With EOT, the rendered images for optimization in Sec. 3.3 can be as below:\\n\\n\\\\[\\n\\\\hat{I}_v(M_{\\\\text{adv}}) = t(S(V^*, T^*, F, \\\\rho(v))) ,\\n\\\\]\\n\\nwhere \\\\( t \\\\in T, \\\\rho \\\\in Q, v \\\\in V \\\\), where \\\\( t \\\\) and \\\\( \\\\rho \\\\) represent the randomly sampled transformations from \\\\( T \\\\) and \\\\( Q \\\\). \\\\( T \\\\) and \\\\( Q \\\\) are the sets of various transformations in the 2D space and the 3D space, respectively.\\n\\n4. Experiments\\n\\nIn this section, we present the experimental results both in the digital world and the physical world to demonstrate the effectiveness of the proposed method.\\n\\n4.1. Experiment Settings\\n\\nDatasets. We use the IM3D [28] dataset in our experiments, which contains 1K typical 3D objects from 100 ImageNet [3] categories. For our experimental validation, we choose 100 objects randomly from 30 categories.\\n\\nVictim models. Two typical classifier models, ResNet-101 [10] and DenseNet-121 [11] are chosen as surrogate models to perform attacks. Other models to test transferability consist of the CNN-based ResNet-50 [10], ResNet-152 [10], VGG-16 [31], VGG19 [31], Inception-v3 [36], EfficientNet-B0 [37], MobileNet-V2 [30] and the Transformer-based Swin-B [18], ViT-B/16 [7]. For testing the cross-render transferability, we choose two commercial rendering software: MeshLab and Blender. For testing the cross-task transferability, we choose two other tasks: zero-shot detection [16] and image caption [14] due to their non-limitation to types of objects.\\n\\nEvaluation metrics. To quantitatively evaluate the effectiveness of our proposed TT3D, we measure the attack success rate (ASR). Specifically, given that our method is a targeted attack, we judge by whether rendered images can be predicted as the target label by victim models. For each 3D object, we render 100 images using random rendering...\"}"}
{"id": "CVPR-2024-2454", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. The ASR(%) of 3d adversarial examples generated by different methods including the enhanced mesh-based, the mlp-only, the grid-only, and our dual optimization method under random viewpoints against ResNet-50 (RN-50), ResNet-101 (RN-101), ResNet-152 (RN-152), VGG-16, VGG-19, Inception-v3 (Inc-v3), DenseNet-121 (DN-121), EfficientNet-B0 (EN-B0), MobileNet-v2 (MN-v2), Swin-B, and VIT-B/16. The adversarial examples are learned against the surrogate models ResNet-101 and DenseNet-121.\\n\\n4.2. Attack performance in the Digital World\\n\\n4.2.1 Basic Results\\n\\nEffectiveness of the proposed method. To verify the effects of our proposed method, we compare the attack performance with different methods. Among them, for a fair comparison with mesh-based optimization method [49], we enhance it to optimize both the vertex colors and the coordinates. Specifically, this method is based on the classical method MeshAdv [49] and to further accentuate the limitations of this method, we put no limitations on the color modifications. Tab. 2 shows the attack success rates (%) in various common classifier models, where we find that mesh-based optimization exhibits nearly no transferability due to its tendency to overfit. In contrast, the dual optimization targeting both MLP and Grid parameters demonstrates a substantial improvement in transferability. Then, to explore the effects of different parameters in grid-based NeRF space, we also test the attack performance of MLP-only optimization and Grid-only optimization. Experimental results show that MLP-only optimization shows reasonable success, indicating its effectiveness in manipulating model-specific features. However, Grid-only optimization presents better transferability, likely due to its ability to capture spatial relationships more effectively. The combination of both, as evidenced by the highest success rates across victim models in Tab. 2, suggests that this dual optimization method effectively integrates detailed textural features and complex decision-making processes, proving the necessity and superiority of the dual optimization strategy.\\n\\nBetter visual naturalness. In TT3D, while achieving excellent attack effectiveness, our method also exhibits visual naturalness compared to mesh-based optimization methods. As illustrated in Fig. 3, 3D objects optimized using mesh-based methods display nonsensical noise for appearance. In contrast, our TT3D, benefiting from targeting on both the foundation feature level and decision layers, could generate adversarial perturbations that are more semantically informative and show greater resemblance to the original ones. More specific details, including quantitative metrics, are provided in the Supplementary Materials.\\n\\n4.2.2 Cross-render Transferability\\n\\nAs illustrated in Fig. 4, we observe that there exist variations in the rendering results of different renderers. To further validate the effectiveness of our method, we conduct transferability tests across various renderers, including two commercial-grade rendering software. As shown in Tab. 3, our 3D adversarial examples still perform well, even when faced with completely unknown rendering systems. This demonstrates the robustness of our method, as there is no significant performance degradation across different rendering environments, highlighting the broad applicability and resilience of our adversarial examples.\\n\\nZero-shot detector. We utilize the state-of-the-art model of Liu et al. [16] for zero-shot detection. In our tests, we...\"}"}
{"id": "CVPR-2024-2454", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. The ASR(%) of 3D adversarial examples with different renderers against RN-50, RN-101, RN-152, VGG-16, VGG-19, Inc-v3, DN-121, EN-B0, MN-v2, Swin-B, and ViT-B/16. The adversarial examples are learned against surrogate models RN-101 and DN-121.\\n\\n| Source Model | Methods | Victim Model | RN-50 | RN-101 | RN-152 | VGG-16 | VGG-19 | Inc-v3 | DN-121 | EN-B0 | MN-v2 | Swin-B | ViT-B/16 |\\n|--------------|---------|--------------|-------|--------|--------|--------|--------|--------|--------|-------|-------|--------|----------|\\n| ResNet-101   | Tex     | RN-50        | 76.96 | 91.60  | 78.52  | 52.00  | 49.51  | 53.31  | 72.96  | 55.73 | 53.09 | 64.40  | 54.50    |\\n|              | Tex+Geo | RN-50        | 78.85 | 88.98  | 78.96  | 58.29  | 51.54  | 55.14  | 89.14  | 58.71 | 57.69 | 69.80  | 55.84    |\\n| DenseNet-121 | Tex     | RN-50        | 65.71 | 46.13  | 52.13  | 46.60  | 40.85  | 36.44  | 94.28  | 37.49 | 38.72 | 47.62  | 35.58    |\\n|              | Tex+Geo | RN-50        | 70.98 | 51.61  | 59.59  | 57.59  | 50.25  | 43.81  | 96.74  | 40.59 | 45.06 | 55.15  | 36.49    |\\n\\nTable 4. The ASR(%) of tex-only and tex+geo(sub) optimization methods against RN-50, RN-101, RN-152, VGG-16, VGG-19, Inc-v3, DN-121, EN-B0, MN-v2, Swin-B, and ViT-B/16. The adversarial examples are learned against the surrogate models RN-101 and DN-121.\\n\\n4.2.3 Cross-task Transferability\\n\\nTo verify the transferability of our adversarial examples across different tasks, we select zero-shot detection and image caption for testing due to their broad applicability.\\n\\nImage caption. For image captioning, we employ the BLIP model from Li et al. [14], still maintaining the white background setup. We present the rendered adversarial images to the BLIP model and deem the test successful if the generated caption includes the target label. Due to testing costs, we randomly selected three viewpoints for 100 different 3D objects. The results demonstrate that our method still achieves a success rate of 32.33% in this task, as evidenced by some successful examples shown in Fig. 5.\\n\\n4.3. Additional Results and Ablation Study\\n\\nThe effects of additional vertex optimization. To validate the additional vertex optimization mentioned in Sec. 3.2 that could serve as an auxiliary means to further enhance texture-focused optimization performance, we conduct an ablation study on it. As shown in Tab. 4, it is evident that adding the geometric changes of the mesh vertex coordinates while optimizing texture features in the grid-based parameter space can, to a certain extent, enhance the transferability of the attack. Additionally, we find that solely altering the geometric structure with requirements for natural\"}"}
{"id": "CVPR-2024-2454", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Visual examples of printed 3D adversarial objects towards the target label under different backgrounds (B-1, B-2, B-3) and viewpoints in the physical world. The first row is learned against ResNet-101 and the second row is learned against DenseNet-121.\\n\\n\\\\[ \\\\text{adv, } \\\\beta = 10^3 \\\\quad \\\\text{(ASR = 98%)} \\\\]\\n\\\\[ \\\\text{adv, } \\\\beta = 10^2 \\\\quad \\\\text{(ASR = 100%)} \\\\]\\n\\\\[ \\\\text{clean adv, } \\\\beta = 10^4 \\\\quad \\\\text{(ASR = 76%)} \\\\]\\n\\n# Appearance\\n# Geometry\\nStove             Bread\\n\\nFigure 7. An visual example of the change in appearance and geometry with \\\\( \\\\beta \\\\) and its corresponding attack success rate.\\n\\nTo verify the feasibility of TT3D in the physical world, we first utilize 3D printing techniques to print 3D adversarial objects. Then, considering that the backgrounds in the physical world are diverse, unlike the white background of the optimization process, we not only validate the digital-to-physical transferability of TT3D under different viewpoints, but also add three randomly selected, distinct backgrounds B-1, B-2, and B-3 to evaluate its robustness against backgrounds. For the number of testing objects, we print 20 3D objects, including 10 for the surrogate ResNet-101 and 10 for the surrogate DenseNet-121. The specific process involves:\\n\\n1) placing the 3D adversarial objects on the surface;\\n2) slowly circling them with a smartphone for about 360\u00b0, excluding the bottom part, which lasts approximately 20 seconds per object in each setting, capturing 10 frames per second, resulting in a total of 200 frames;\\n3) calculating the attack success rate, which is determined by the proportion of successful frames.\\n\\nThe experimental results are presented in Fig. 6 and Tab. 5, demonstrating the robust effectiveness of our method in various scenarios from the physical world. Further experiments on transferability tests in the physical world are available in Supplementary Materials.\\n\\n|                | ResNet-101 | DenseNet-121 |\\n|----------------|------------|--------------|\\n| B-1            | 81.30      | 79.30        |\\n| B-2            | 76.45      | 83.25        |\\n| B-3            | 91.55      | 84.50        |\\n\\nTable 5. The ASR(%) of printed adversarial meshes against the RN-101 and DN-121 under different backgrounds: B-1, B-2, B-3 and various viewpoints in the real world.\\n\\n5. Conclusion\\nIn this paper, we propose a novel 3D attack framework TT3D that could rapidly reconstruct multi-view images into transferable targeted 3D adversarial examples, effectively filling the gap in 3D transferable targeted attacks. Besides, TT3D leverages a dual optimization strategy in the grid-based NeRF space, which significantly improves the black-box transferability meanwhile ensuring visual naturalness. Extensive experiments further demonstrate TT3D's strong cross-model transferability and adaptability in various renders and vision tasks, with real-world applicability confirmed through 3D printing techniques.\\n\\nAcknowledgement\\nThis work was supported in part by the Project of the National Natural Science Foundation of China (Nos. 62076018, U2341228, 62276149) and in part by the Fundamental Research Funds for the Central Universities.\"}"}
{"id": "CVPR-2024-2454", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Transferable Targeted 3D Adversarial Attack in the Physical World\\n\\nYao Huang, Yinpeng Dong, Shouwei Ruan, Xiao Yang, Hang Su, Xingxing Wei\\n\\n1 Institute of Artificial Intelligence, Beihang University, Beijing 100191, China\\n2 Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China\\n3 RealAI\\n\\n{yhuang, shouweiruan, xxwei}@buaa.edu.cn\\n{dongyinpeng, suhangss}@mail.tsinghua.edu.cn\\n{yangxiao19}@mails.tsinghua.edu.cn\\n\\nAbstract\\n\\nCompared with transferable untargeted attacks, transferable targeted adversarial attacks could specify the misclassification categories of adversarial samples, poses a greater threat to security-critical tasks. In the meanwhile, 3D adversarial samples, due to their potential of multi-view robustness, can more comprehensively identify weaknesses in existing deep learning systems, possessing great application value. However, the field of transferable targeted 3D adversarial attacks remains vacant. The goal of this work is to develop a more effective technique that could generate transferable targeted 3D adversarial examples, filling the gap in this field. To achieve this goal, we design a novel framework named TT3D that could rapidly reconstruct from few multi-view images into transferable targeted 3D textured meshes. While existing mesh-based texture optimization methods compute gradients in the high-dimensional mesh space and easily fall into local optima, leading to unsatisfactory transferability and distinct distortions, TT3D innovatively performs dual optimization towards both feature grid and Multi-layer Perceptron (MLP) parameters in the grid-based NeRF space, which significantly enhances black-box transferability while enjoying naturalness. Experimental results show that TT3D not only exhibits superior cross-model transferability but also maintains considerable adaptability across different renders and vision tasks. More importantly, we produce 3D adversarial examples with 3D printing techniques in the real world and verify their robust performance under various scenarios.\\n\\n1. Introduction\\n\\nDespite the unprecedented performance of deep neural networks (DNNs) in numerous tasks, including image classification [27] and object detection [13, 54], these models are vulnerable to adversarial examples [4, 5, 9, 20, 22, 29, 35, 39, 43\u201348, 50]. These adversarial examples are shown to have good transferability across models [4, 17, 50], emerging as a key concern since they can evade black-box models in practical applications. The majority of transferable attacks are untargeted, aiming to induce misclassification of the target model. However, transferable targeted attacks, which mislead DNNs to produce predetermined misclassifications, can lead to a more severe threat in real-world applications. Crafting such transferable targeted attacks is particularly challenging because they not only need to achieve a specific misclassification but also must avoid overfitting to ensure effective attacks across various models, which requires further investigation.\\n\\nWhile transferable targeted adversarial attacks in the 2D domain have been extensively studied [15, 24, 42, 53], the...\"}"}
{"id": "CVPR-2024-2454", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. A comparison among different 3D attack methods regarding target vision tasks, used data, 3D attack types, transferability, whether conducting targeted attacks, naturalness, and whether conducting attacks in the physical world.\\n\\n- **Target Task**: Classify, Detect, Classify+Detect\\n- **Data**: 3D model, Few Multi-view images\\n- **3D Attack Type**: Texture, Geometry/Texture, Texture+Geometry\\n- **Transferability**: Revealed, Revealed, Revealed, Revealed, Revealed, Revealed, Revealed\\n- **Naturalness**: Revealed, Revealed, Revealed, Revealed, Revealed, Revealed, Revealed\\n- **Physical Attack**: Revealed, Revealed, Revealed, Revealed, Revealed, Revealed, Revealed\\n\\nThe exploration of transferable targeted attacks in the 3D domain remains vacant. Compared with 2D adversarial attacks, 3D attacks possess greater practical value in the real world due to the potential of consistent attacks across various viewpoints. As illustrated in Tab. 1, existing 3D attack methods [1, 6, 33, 34, 40, 49, 52] could ensure multi-view effectiveness by modifying pre-existing meshes' textures or geometry due to meshes' 3D consistency. But, they struggle to simultaneously ensure transferability and targeted attack, especially for physical attacks. In addition, it is also difficult to maintain the naturalness of these adversarial examples.\\n\\nBased on the above discussions, this paper aims to generate transferable and natural 3D adversarial examples for physically targeted attacks. However, there exist two challenges to meet our goal:\\n\\n1. **Prior mesh-based optimization methods involve direct alterations to vertex colors in the high-dimensional mesh space, easily falling into the overfitting and thus leading to unsatisfactory transferability.** Thus, the first challenge is how to design an optimization method that deviates from the mesh space, avoiding overfitting for better transferability.\\n\\n2. **Existing methods struggle to simultaneously ensure attack performance and naturalness, whose attack process easily accompanies the extremely unnatural phenomena, such as visual anomalies in appearance or distortion or fragmentation of the 3D mesh,** etc. Therefore, how to balance the attack performance and the visual naturalness is another challenge.\\n\\nTo meet the above challenges, we design a novel framework called **TT3D** that could rapidly reconstruct a transferable targeted 3D adversarial textured mesh with guaranteed naturalness. To be specific, driven by the recent advancement in multi-view reconstruction techniques (i.e., grid-based NeRF [23]), we first reconstruct an initial 3D mesh from its multi-view images, but instead of directly manipulating in the mesh space, we innovatively perform adversarial fine-tuning of appearance rendering parameters in the grid-based NeRF space. Such a method not only successfully avoids overfitting but also eliminates previous methods' reliance on the pre-existing 3D mesh, greatly lowering the cost. The technical details can be found as follows:\\n\\n1. **Firstly, to enhance the transferability, we design a dual optimization strategy for adversarial fine-tuning.** This optimization strategy simultaneously targets the parameters of the appearance feature grid and the corresponding MLP (feature grids and MLP are both components of grid-based NeRF, detailed in Sec. 3.1), thereby effectively perturbing at both the foundational feature level and the more advanced decision-making layers. Additionally, it's worth mentioning that, though solely altering the geometric structure is challenging for achieving transferable targeted attacks, we observe that incorporating changes in geometric information under a texture optimization-focused method, can enhance the performance (shown in Sec. 4.2.1). Thus, we add the optimization of vertex coordinates into our methodology.\\n\\n2. **Secondly, to ensure the naturalness of the 3D adversarial textured mesh, we implement constraints on appearance and geometry during optimization.** These include measures for visual consistency and vertex proximity, along with specialized loss functions to prevent mesh deformities and ensure surface smoothness. For 3D adversarial objects' robustness in the real world, we also introduce an EOT that could effectively integrate various transformations in both 3D and 2D spaces to more realistically simulate real-world conditions.\\n\\nA comparison of our TT3D's performance in transferable targeted attacks with the typical mesh-based optimization method is shown in Fig. 1.\\n\\nThe contributions of this paper are as follows:\\n\\n- **We propose a novel framework called TT3D for generating transferable targeted 3D adversarial examples,** which is the first work to fill a critical gap in this field and eliminate the previous reliance on pre-existing 3D meshes, expanding the feasible region of 3D attacks.\\n\\n- **We design a dual optimization strategy within the grid-based NeRF space,** which effectively perturbs at both the foundational feature level and the more sophisticated decision-making echelons of the neural network, and ensures the naturalness in the meanwhile.\\n\\n- **Experimental results demonstrate that our 3D adversarial object can easily be misclassified as a given label across various victim models, renders, and tasks.** Moreover, we produce 3D adversarial objects in the real world by utilizing 3D printing technology and validate their robust performance under various settings, like different viewpoints and backgrounds.\"}"}
{"id": "CVPR-2024-2454", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. An overview of our TT3D framework. We first utilize 3D multi-view reconstruction technology, i.e., grid-based NeRF with marching cubes techniques to obtain the initial clean 3D mesh. Then, we perform adversarial fine-tuning in the textual parameter space of grid-based NeRF instead of directly altering the texture $T$, supplemented with geometric perturbations at vertex positions $V$. To ensure the naturalness simultaneously, we add constraints to the distance between the 3D adversarial samples and the initial ones in terms of both texture and geometric structure when performing optimization.\\n\\n2. Related Work\\n\\n2.1. Transferable Targeted Adversarial Attack\\n\\nTransferable targeted adversarial attacks in the 2D domain have been extensively studied. For instance, Li et al. [15] improve the transferability of targeted attacks by addressing \u2018noise curing\u2019 through self-adaptive gradient magnitudes and metric learning. Zhao et al. [53] demonstrate that simple logit loss-based attacks, without extensive resources, can achieve unexpected effectiveness in targeted transferability, challenging traditional, resource-intensive methods. The works of Wang et al. [42] and Naseer et al. [24] further advance the field, introducing sophisticated approaches for capturing class distributions and aligning image distributions. Moreover, Yang et al. [51] also propose a hierarchical generative network-based approach for crafting targeted transfer-based adversarial examples, showcasing the potential of generative models in this domain. However, the above works only focus on the 2D domain while the realm of transferable targeted 3D adversarial attacks remains vacant, posing a necessity for investigations.\\n\\n2.2. 3D Adversarial Attack\\n\\nSince Athalye et al. [1] verified the existence of 3D adversarial examples, plenty of generation methods [26, 33, 34, 40, 49, 52] have emerged. Among them, except for MeshAdv [49], which has attempted to change the mesh geometry for 3D attacks but still fails to perform transferable targeted attacks, the rest methods [26, 33, 34, 40, 52], including MeshAdv (optional), are based on modifying the texture of the 3D mesh. We can split texture-based attack methods into two categories. For methods like [26, 49, 52], they directly alter vertex colors in the high-dimensional mesh space, leading to unavoidable overfitting. On the other hand, works like [33, 34, 40] optimize a texture map and render it onto a specified 3D model (e.g., a car), enhancing transferability to some extent. However, their transferability is limited to non-targeted attacks in object detection, lacking comparability. Moreover, most of these attacks are confined to the digital realm, and physical attacks often come with visual unnaturalness. To address these issues, our work, without the reliance on 3D models, can effectively perform an 3D attack that not only ensures attack performance (i.e., transferable targeted 3D attacks feasible even in real-world scenarios), but also could meet demands for naturalness.\\n\\n3. Methodology\\n\\nIn this section, we detail the proposed TT3D framework for generating transferable targeted 3D adversarial examples, as presented in Fig. 2. TT3D adopts grid-based NeRF [23] to model 3D objects from multi-view images and perform optimization. As below, we first introduce the background knowledge of grid-based NeRF and then present the problem formulation and our optimization strategy.\\n\\n3.1. Preliminary\\n\\nGiven a set of multi-view images, vanilla NeRF [21] encodes a real-world object into a continuous volumetric radius field $F: (x, d) \\\\rightarrow (c, \\\\sigma)$. $F$ is approximated by a multi-layer perceptron (MLP), which takes a 3D location as input and outputs a vector containing the color and density at that location.\"}"}
{"id": "CVPR-2024-2454", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and a unit-norm viewing direction \\\\( \\\\mathbf{d} \\\\in \\\\mathbb{R}^3 \\\\) as inputs, and outputs a volume density \\\\( \\\\sigma \\\\in \\\\mathbb{R}^+ \\\\) and emitted RGB color \\\\( \\\\mathbf{c} \\\\in [0, 1]^3 \\\\). Despite its efficacy, vanilla NeRF often faces challenges in terms of computational efficiency. In addressing the limitations of traditional methods, grid-based NeRF techniques \\\\cite{23, 32, 38} employ two distinct 3D grids, namely \\\\( G_{\\\\text{geo}} \\\\) and \\\\( G_{\\\\text{tex}} \\\\), to explicitly represent geometric and textural information of objects. This structured approach, utilizing feature grids, maps a 3D point \\\\( \\\\mathbf{x} \\\\) to corresponding feature vectors \\\\( f_{\\\\text{geo}} \\\\) and \\\\( f_{\\\\text{tex}} \\\\) for both geometry and texture, which are expressed as:\\n\\n\\\\[\\n f_{\\\\text{geo}}(\\\\mathbf{x}) = G_{\\\\text{geo}}(\\\\mathbf{x}; \\\\Theta_{G_{\\\\text{geo}}}),\\n\\\\]\\n\\n\\\\[\\n f_{\\\\text{tex}}(\\\\mathbf{x}) = G_{\\\\text{tex}}(\\\\mathbf{x}; \\\\Theta_{G_{\\\\text{tex}}}),\\n\\\\]\\n\\n(1)\\n\\nwhere \\\\( \\\\Theta_{G_{\\\\text{geo}}} \\\\) and \\\\( \\\\Theta_{G_{\\\\text{tex}}} \\\\) denote the sets of feature vectors within \\\\( G_{\\\\text{geo}} \\\\) and \\\\( G_{\\\\text{tex}} \\\\), respectively.\\n\\nTo derive the final rendering attributes, i.e., volume density \\\\( \\\\sigma \\\\) and emitted color \\\\( \\\\mathbf{c} \\\\), these feature vectors \\\\( f_{\\\\text{geo}} \\\\) and \\\\( f_{\\\\text{tex}} \\\\) are then processed through two shallow MLPs: \\\\( M_{\\\\text{geo}} \\\\) and \\\\( M_{\\\\text{tex}} \\\\), formulated as:\\n\\n\\\\[\\n \\\\sigma \\\\leftarrow M_{\\\\text{geo}}(f_{\\\\text{geo}}(\\\\mathbf{x}); \\\\Theta_{M_{\\\\text{geo}}}),\\n\\\\]\\n\\n\\\\[\\n \\\\mathbf{c} \\\\leftarrow M_{\\\\text{tex}}(f_{\\\\text{tex}}(\\\\mathbf{x}); \\\\Theta_{M_{\\\\text{tex}}}),\\n\\\\]\\n\\n(2)\\n\\nwhere \\\\( \\\\Theta_{M_{\\\\text{geo}}} \\\\) and \\\\( \\\\Theta_{M_{\\\\text{tex}}} \\\\) are weights of \\\\( M_{\\\\text{geo}} \\\\) and \\\\( M_{\\\\text{tex}} \\\\).\\n\\nIn this paper, by leveraging grid-based NeRF \\\\cite{23} with Marching Cubes \\\\cite{19}, we first efficiently reconstruct 3D mesh as an accurate geometry and continue adversarial fine-tuning within the parameter space of grid-based NeRF.\\n\\n3.2. Problem Formulation\\n\\nFor the 3D adversarial attack task, we aim to develop an effective method that can generate transferable targeted 3D adversarial samples and maintain their visual naturalness. Similar to \\\\cite{1}, we also propose to craft 3D adversarial examples as textured mesh, which can fully leverage 3D printing techniques for physically realizable adversarial attacks.\\n\\nSpecifically, we define the reconstructed mesh representation of the 3D object as \\\\( M = (V, T, F) \\\\), where \\\\( V \\\\in \\\\mathbb{R}^{n \\\\times 3} \\\\) is the \\\\( \\\\text{xyz} \\\\) coordinates of \\\\( n \\\\) vertices, \\\\( T \\\\in \\\\mathbb{R}^{n \\\\times 3} \\\\) is the \\\\( \\\\text{rgb} \\\\) value of vertices, and \\\\( F \\\\in \\\\mathbb{Z}^{m \\\\times 3} \\\\) is the set of \\\\( m \\\\) triangle faces which encodes each triangle with the indices of vertices. Here, we do not pose changes to the mesh topology \\\\( F \\\\) to prevent serious distortions of geometric structures, and instead, we turn to \\\\( V \\\\) and \\\\( T \\\\).\\n\\nIn this paper, we focus on the challenging transferable targeted 3D attacks against image classification models under different views. Given a surrogate classifier \\\\( f: \\\\mathbb{X} \\\\rightarrow \\\\mathbb{Y} \\\\), the goal of the attack is to generate a 3D adversarial example \\\\( M_{\\\\text{adv}} = (V^*, T^*, F) \\\\) for the original one \\\\( M \\\\) with color and vertex perturbations. Then, with a differentiable render function \\\\cite{12} and a random viewpoint \\\\( v \\\\), we render the \\\\( M_{\\\\text{adv}} \\\\) into the corresponding 2D image \\\\( \\\\hat{I}_v(M_{\\\\text{adv}}) \\\\), which can be misclassified by other common classifiers as the target class \\\\( y^* (\\\\neq y) \\\\). In general, the perturbation should be small to make our 3D adversarial example natural in the physical world. Thus, the optimization problem of crafting 3D adversarial examples can be formulated as\\n\\n\\\\[\\n \\\\min_{V^*, T^*} E_{v \\\\in V} L_{f}(\\\\hat{I}_v(M_{\\\\text{adv}}), y^*) + \\\\beta \\\\cdot R(M_{\\\\text{adv}}, M),\\n\\\\]\\n\\n(3)\\n\\nwhere \\\\( \\\\hat{I}_v(M_{\\\\text{adv}}) = S(V^*, T^*, F, v) \\\\) represents the rendered image of \\\\( M_{\\\\text{adv}} \\\\) under the viewpoint \\\\( v \\\\) and \\\\( V \\\\) is the feasible distribution. \\\\( L_{f} \\\\) is the cross-entropy loss \\\\cite{2} that facilitates the misclassification of \\\\( \\\\hat{I}_v(M_{\\\\text{adv}}) \\\\) to \\\\( y^* \\\\), \\\\( R \\\\) is the regularization for minimizing a perceptibility distance between \\\\( M_{\\\\text{adv}} \\\\) and \\\\( M \\\\) and \\\\( \\\\beta \\\\) is a balancing hyperparameter between these two losses.\\n\\nTo ensure the feasibility of our 3D adversarial example in the real world, we also introduce an EOT scheme (as detailed in Sec. 3.5) to help bridge the gap from \u201cdigital\u201d to \u201cphysical\u201d. However, the mesh-based optimization by following the objective function (3.2) needs to calculate gradients in high-dimensional mesh space due to thousands of points in each 3D object. It will easily distort the natural textural appearance of the textured mesh and is trapped into the overfitting with unsatisfactory transferability.\\n\\n3.3. Dual Optimization in NeRF Parameter Space\\n\\nIn this section, we aim to deviate from the existing mesh-based optimization regime and perform the optimization trajectory in the parameter space of grid-based NeRF \\\\cite{23} as a regularization for escaping from overfitting. Specifically, as introduced in Sec. 3.1, grid-based NeRF stores appearance feature vectors in a structured feature grid \\\\( G_{\\\\text{tex}}(\\\\cdot; \\\\Theta_{G_{\\\\text{tex}}}) \\\\) and processes them into emitted colors through a shallow MLP \\\\( M_{\\\\text{tex}}(\\\\cdot; \\\\Theta_{M_{\\\\text{tex}}}) \\\\). Actually, the clean textural parameter set \\\\( T \\\\), representing the colors of all vertices in the initial 3D textured mesh \\\\( M \\\\), are estimated based on the aforementioned mechanism, expressed as follows:\\n\\n\\\\[\\n T = \\\\{ c | c = M_{\\\\text{tex}}(f_{\\\\text{tex}}(x); \\\\Theta_{M_{\\\\text{tex}}}), \\\\forall x \\\\in V \\\\},\\n\\\\]\\n\\n(4)\\n\\nwhere \\\\( f_{\\\\text{tex}}(x) \\\\) is the textual feature vector of \\\\( x \\\\) and mapped by \\\\( G_{\\\\text{tex}}(\\\\cdot; \\\\Theta_{G_{\\\\text{tex}}}) \\\\) following Eq. (1).\\n\\nExisting mesh-based optimization methods, which primarily perform direct alterations to the vertex colors in \\\\( T \\\\), struggle with the computing complexity, unsatisfactory transferability, and naturalness inherent in explicit mesh spaces. To prevent this, our method innovatively utilizes the previously established color estimation mechanism, i.e., Eq. (4) to perform adversarial fine-tuning in the space of grid-based NeRF, indirectly generating adversarial textures. Moreover, we undertake a dual optimization, concurrently targeting both the parameters of the appearance feature grid, \\\\( \\\\Theta_{G_{\\\\text{tex}}} \\\\), and those of the corresponding MLP, \\\\( \\\\Theta_{M_{\\\\text{tex}}} \\\\), which effectively integrates the manipulation of detailed textural features contained within the grids with the nuanced processing capabilities inherent in the MLPs.\"}"}
