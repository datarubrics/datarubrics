{"id": "CVPR-2022-698", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delivering into high quality object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6154\u20136162, 2018.\\n\\n[2] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks meet squeeze-excitation networks and beyond. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 0\u20130, 2019.\\n\\n[3] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efficient object detection models with knowledge distillation. Advances in neural information processing systems, 30, 2017.\\n\\n[4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\\n\\n[5] Xing Dai, Zeren Jiang, Zhao Wu, Yiping Bao, Zhicheng Wang, Si Liu, and Erjin Zhou. General instance distillation for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7842\u20137851, 2021.\\n\\n[6] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian. Centernet: Keypoint triplets for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6569\u20136578, 2019.\\n\\n[7] Jianyuan Guo, Kai Han, Yunhe Wang, Han Wu, Xinghao Chen, Chunjing Xu, and Chang Xu. Distilling object detectors via decoupled features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2154\u20132164, 2021.\\n\\n[8] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.\\n\\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[10] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1921\u20131930, 2019.\\n\\n[11] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\\n\\n[12] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3588\u20133597, 2018.\\n\\n[13] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132\u20137141, 2018.\\n\\n[14] Zijian Kang, Peizhen Zhang, Xiangyu Zhang, Jian Sun, and Nanning Zheng. Instance-conditional knowledge distillation for object detection. arXiv preprint arXiv:2110.12724, 2021.\\n\\n[15] Quanquan Li, Shengying Jin, and Junjie Yan. Mimicking very efficient network for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6356\u20136364, 2017.\\n\\n[16] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2117\u20132125, 2017.\\n\\n[17] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.\\n\\n[18] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.\\n\\n[19] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21\u201337. Springer, 2016.\\n\\n[20] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32:8026\u20138037, 2019.\\n\\n[21] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.\\n\\n[22] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28:91\u201399, 2015.\\n\\n[23] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.\\n\\n[24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.\\n\\n[25] Ruoyu Sun, Fuhui Tang, Xiaopeng Zhang, Hongkai Xiong, and Qi Tian. Distilling object detectors with task adaptive regularization. arXiv preprint arXiv:2006.13108, 2020.\\n\\n[26] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9627\u20139636, 2019.\"}"}
{"id": "CVPR-2022-698", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1365\u20131374, 2019.\\n\\nTao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. Distilling object detectors with fine-grained feature imitation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4933\u20134942, 2019.\\n\\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7794\u20137803, 2018.\\n\\nSanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. CBAM: Convolutional block attention module. In Proceedings of the European conference on computer vision (ECCV), pages 3\u201319, 2018.\\n\\nSaining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492\u20131500, 2017.\\n\\nZe Yang, Shaohui Liu, Han Hu, Liwei Wang, and Stephen Lin. Reppoints: Point set representation for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9657\u20139666, 2019.\\n\\nJunho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4133\u20134141, 2017.\\n\\nSergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016.\\n\\nLinfeng Zhang and Kaisheng Ma. Improve object detection with feature-based knowledge distillation: Towards accurate and efficient detectors. In International Conference on Learning Representations, 2020.\"}"}
{"id": "CVPR-2022-698", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nKnowledge distillation has been applied to image classification successfully. However, object detection is much more sophisticated and most knowledge distillation methods have failed on it. In this paper, we point out that in object detection, the features of the teacher and student vary greatly in different areas, especially in the foreground and background. If we distill them equally, the uneven differences between feature maps will negatively affect the distillation. Thus, we propose Focal and Global Distillation (FGD). Focal distillation separates the foreground and background, forcing the student to focus on the teacher's critical pixels and channels. Global distillation rebuilds the relation between different pixels and transfers it from teachers to students, compensating for missing global information in focal distillation. As our method only needs to calculate the loss on the feature map, FGD can be applied to various detectors. We experiment on various detectors with different backbones and the results show that the student detector achieves excellent mAP improvement. For example, ResNet-50 based RetinaNet, Faster R-CNN, RepPoints and Mask R-CNN with our distillation method achieve 40.7%, 42.0%, 42.0% and 42.1% mAP on COCO2017, which are 3.3, 3.6, 3.4 and 2.9 higher than the baseline, respectively. Our codes are available at https://github.com/yzd-v/FGD.\\n\\n1. Introduction\\n\\nRecently, deep learning has achieved great success in various domains [8, 9, 22, 24]. To get better performance, we usually use a larger backbone, which needs more compute resources and inferences more slowly. To get over this, knowledge distillation has been proposed [11]. Knowledge distillation is a method to inherit the information from a large teacher network to a compact student network and achieve strong performance without extra cost during inference time. However, most distillation methods [10, 27, 33, 34] are designed for image classification, which lead to trivial improvements for object detection. It is well acknowledged that the extreme foreground-background distillation is a method to inherit the information from a large teacher network to a compact student network and achieve strong performance without extra cost during inference time.\"}"}
{"id": "CVPR-2022-698", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"background class imbalance is a key point in object detection [17]. The imbalanced ratio also harms the distillation for object detection. There are some efforts for this problem. Chen et al. [3] distributes a weight to suppress the background. Mimick [15] distills the positive area proposed by region proposal network of the student. FGFI [28] and TADF [25] use the fine-grained and Gaussian Mask to select the distillation area, respectively. Defeat [7] distills the foreground and background separately. However, where is the key area for distillation is still not clear.\\n\\nIn order to explore the difference between the features of students and teachers, we do the visualization of the spatial and channel attention. As the Fig. 1 shows, the difference between student's attention and teacher's attention in the foreground is quite significant, while that in the background is relatively small. This may lead to different difficulties in learning the foreground and background. In this paper, we further explore the influence of the foreground and background in knowledge distillation on object detection. We design experiments by decoupling the foreground and background in the distillation. Surprisingly, as shown in Tab. 1, the performance of distillation on the foreground and background together is the worst, even worse than only using foreground or background. This phenomenon suggests that the uneven differences in the feature map can negatively affect distillation. Besides, as shown in Fig. 1, the attention between each channel is also very different. Thinking one step deeper, not only are there negative influences between the foreground and the background, but also between the pixels and the channels. Therefore, we propose focal distillation. While separating the foreground and background, focal distillation also calculates the attention of different pixels and channels in teacher's feature, allowing the student to focus on teacher's crucial pixels and channels.\\n\\nHowever, just focusing on key information is not enough. It is well known that global context also plays an important role in detection. A lot of relation modules have been successfully applied into detection, such as non-local [29], GcBlock [2], relation network [12], which have greatly improved the performance of detectors. In order to compensate for the missing global information in focal distillation, we further propose global distillation. In global distillation, we utilize GcBlock to extract the relation between different pixels and then distill them from teachers to students.\\n\\nAs we analyzed above, we propose Focal and Global Distillation (FGD), combining focal distillation and global distillation, as shown in Fig. 2. All loss functions are only calculated on features, so that FGD can be used directly on various detectors, including two-stage models, anchor-based one-stage models and anchor-free one-stage models. Without bells and whistles, we achieve state-of-the-art performances in object detection with FGD. In a nutshell, the contributions of this paper are:\\n\\n- We present that the pixels and channels that teacher and student pay attention to are quite different. If we distill the pixels and channels without distinguishing them, it will result in a trivial improvement.\\n- We propose focal and global distillation, which enables the student not only to focus on the teacher's critical pixels and channels, but also to learn the relation between pixels.\\n- We verify the effectiveness of our method on various detectors via extensive experiments on the COCO [18], including one-stage, two-stage, anchor-free methods, achieving state-of-the-art performance.\\n\\n2. Related Work\\n\\n2.1. Object Detection\\n\\nObject detection is a fundamental and challenging task in computer vision. The CNN-based detection networks with high performance are divided into two-stage [1, 8, 22], anchor-based one-stage [17, 19, 21] and anchor-free one-stage detectors [6, 26, 32]. One-stage detectors get the classification and bounding box of targets on feature maps directly. In contrast, two-stage detectors utilize RPN and RCNN head to achieve better results but cost more time. Prior anchor boxes provide one-stage models with proposals to detect targets. However, the number of anchor boxes is far more than targets, which brings extra computation. While anchor-free detectors show a way to predict the key point and location of targets directly. Although there are different detection heads, their inputs are all features. Therefore, our feature-based knowledge distillation method can be applied in almost all detectors.\\n\\n2.2. Knowledge Distillation\\n\\nKnowledge distillation is a method of model compression without changing the network structure. It is first proposed by Hinton et al. [11], which uses the output as soft labels to transfer the dark knowledge from a large teacher network to a small student network for the classification task. Moreover, FitNet [23] proves that the semantic information from intermediate is also helpful to guide the student model. There have been many works [10, 27, 33, 34] that improve the student classifiers significantly.\\n\\nRecently, some works have successfully applied knowledge distillation to detectors. Chen et al. [3] first apply knowledge distillation to detection by distilling knowledge on the neck feature, the classification head, and the regression head. Nevertheless, distilling the whole feature may introduce much noise because of the imbalance between the foreground and background. Li et al. [15] choose the features sampled from RPN to calculate distillation loss.\"}"}
{"id": "CVPR-2022-698", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. An illustration of FGD, including focal distillation and global distillation. Focal distillation not only separates the foreground and the background, but also enables the student network to better pay attention to the important information in the teacher network\u2019s feature map. Global distillation bridges the gap between the global context of the student and the teacher.\\n\\nWang et al. [28] propose the fine-grained mask to distill the regions calculated by ground-truth bounding boxes. Sun et al. [25] utilize the Gaussian Mask to cover the ground-truth for distillation. Such methods lack the distillation for the background. Without distinguishing the foreground and background, GID [5] distills the areas where the performance of the student and teacher is different. Guo et al. [7] shows that both the foreground and background play important roles for distillation, and distilling them separately benefits the student more. Both of their methods distill the knowledge from the background and get significant results. However, they treat all the pixels and channels equally.\\n\\nFKD [35] uses attention masks and Non-local module [29] to guide the student and distills the relation, respectively. However, it distills the foreground and background together.\\n\\nThe critical problem of distillation for detection is to select the valuable area for distillation. The previous distillation methods treat all the pixels and channels equally [5,7,25,28] or distill all the areas [35] together. Most methods lack the distillation of the global context information.\\n\\nIn this paper, we use ground-truth boxes to separate the images, and then use the attention masks from the teacher to select crucial parts for distillation. In addition, we capture the global relations between different pixels and distill them to the student, which brings another improvement.\\n\\n3. Method\\n\\nMost detectors have used FPN [16] to utilize the multi-scale semantic information. The features from FPN fuse different levels of semantic information from the backbone and are used to predict directly. Transferring the knowledge of these features from the teacher has significantly improved the performance of the student. Generally, the distillation of the features can be formulated as:\\n\\n$$L_{\\\\text{fea}} = \\\\frac{1}{CHW} \\\\sum_{k=1}^{C} \\\\sum_{i=1}^{H} \\\\sum_{j=1}^{W} (F_{T}^{k,i,j} - f(F_{S}^{k,i,j}))^2$$\\n\\nwhere $F_{T}$ and $F_{S}$ denote the feature from the teacher and student, respectively, and $f$ is the adaptation layer to reshape the $F_{S}$ to the same dimension as $F_{T}$. $H$, $W$ denote the height and width of the feature and $C$ is the channel.\\n\\nHowever, such methods treat all the parts equally and lack the distillation of the global relations between different pixels. To get over the above problems, we propose FGD, which includes focal and global distillation, as shown in Fig. 2. Here we will introduce our method in detail.\\n\\n3.1. Focal Distillation\\n\\nFor the foreground and background imbalance, we propose focal distillation to separate the images and guide the student to focus on crucial pixels and channels. The comparison of the distillation areas can be seen in Fig. 3. Firstly we set a binary mask $M$ to separate the background and foreground:\\n\\n$$M_{i,j} = \\\\begin{cases} 1, & \\\\text{if } (i, j) \\\\in r_0, \\\\\\\\ 0, & \\\\text{otherwise} \\\\end{cases}$$\"}"}
{"id": "CVPR-2022-698", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\text{where } r \\\\text{ denotes the ground-truth boxes and } i, j \\\\text{ are the horizontal and vertical coordinates of the feature map, respectively. If } (i, j) \\\\text{ falls in the ground truth, then } M_{i,j} = 1, \\\\text{ otherwise it is 0.} \\\\]\\n\\nThe targets with larger-scale will occupy more loss because they own more pixels, which will influence the distillation of the small targets. And the ratios of foreground to background vary greatly in different images. Therefore, in order to treat different targets equally and balance the loss of foreground and background, we set a scale mask \\\\( S \\\\) as:\\n\\n\\\\[\\nS_{i,j} = \\\\begin{cases} \\n1 \\\\cdot H_r \\\\cdot W_r, & \\\\text{if } (i, j) \\\\in r \\\\\\\\\\n0, & \\\\text{otherwise}\\n\\\\end{cases}\\n\\\\]\\n\\n\\\\[ N_{bg} = H \\\\cdot \\\\sum_{i=1}^{H} W \\\\cdot \\\\sum_{j=1}^{W} (1 - M_{i,j}) \\\\]\\n\\nwhere \\\\( H_r \\\\) and \\\\( W_r \\\\) denote the height and width of the ground-truth box \\\\( r \\\\). If a pixel belongs to different targets, we choose the smallest box to calculate the \\\\( S \\\\).\\n\\nSENet \\\\[13\\\\] and CBAM \\\\[30\\\\] show that focusing on crucial pixels and channels helps CNN-based models get better results. Zagoruyko et al. \\\\[34\\\\] use a simple way to get the spatial attention mask and improve the performance of distillation. In this paper, we apply a similar method to select focal pixels and channels, and then get corresponding attention masks. We calculate the absolute mean values on different pixels and different channels, respectively:\\n\\n\\\\[\\nG_S(F) = \\\\frac{1}{C} \\\\cdot \\\\sum_{c=1}^{C} |F_c|\\n\\\\]\\n\\n\\\\[\\nG_C(F) = \\\\frac{1}{HW} \\\\cdot \\\\sum_{i=1}^{H} \\\\sum_{j=1}^{W} |F_{i,j}|\\n\\\\]\\n\\nwhere \\\\( H, W, C \\\\) denote the feature's height, width, and channel. \\\\( G_S \\\\) and \\\\( G_C \\\\) are the spatial and channel attention map. Then the attention mask can be formulated as:\\n\\n\\\\[\\nA_S(F) = \\\\frac{1}{H \\\\cdot W \\\\cdot \\\\text{softmax}(G_S(F))} \\\\]\\n\\n\\\\[\\nA_C(F) = \\\\frac{1}{C \\\\cdot \\\\text{softmax}(G_C(F))} \\\\]\\n\\nwhere \\\\( T \\\\) is the temperature hyper-parameter proposed by Hinton et al. \\\\[11\\\\] to adjust the distribution.\\n\\nThere are significant differences between the masks of the student and teacher. In training process, we use the teacher's masks to guide the student. With the binary mask \\\\( M \\\\), scale mask \\\\( S \\\\), attention mask \\\\( A_S \\\\) and \\\\( A_C \\\\), we propose the feature loss \\\\( L_{\\\\text{fea}} \\\\) as follows:\\n\\n\\\\[\\nL_{\\\\text{fea}} = \\\\alpha \\\\cdot \\\\sum_{k=1}^{C} \\\\sum_{i=1}^{H} \\\\sum_{j=1}^{W} M_{i,j} S_{i,j} A_{S_{i,j}} A_{C_{i,j}} F_{T_{k,i,j}} - f(F_{S_{k,i,j}}) \\\\]\\n\\n\\\\[\\n+ \\\\beta \\\\cdot \\\\sum_{k=1}^{C} \\\\sum_{i=1}^{H} \\\\sum_{j=1}^{W} (1 - M_{i,j}) S_{i,j} A_{S_{i,j}} A_{C_{i,j}} F_{T_{k,i,j}} - f(F_{S_{k,i,j}}) \\\\]\\n\\nwhere \\\\( A_S \\\\) and \\\\( A_C \\\\) denote the spatial and channel attention mask of the teacher detector, respectively. \\\\( F_T \\\\) and \\\\( F_S \\\\) denote the feature maps of the teacher detector and student detector, respectively. \\\\( \\\\alpha \\\\) and \\\\( \\\\beta \\\\) are the hyper-parameters to balance the loss between foreground and background.\\n\\nBesides, we use attention loss \\\\( L_{\\\\text{at}} \\\\) to force the student detector to mimic the spatial and channel attention mask of the teacher detector, which is formulated as:\\n\\n\\\\[\\nL_{\\\\text{at}} = \\\\gamma \\\\cdot (l(A_{S_t}, A_{S_s}) + l(A_{C_t}, A_{C_s}))\\n\\\\]\\n\\nwhere \\\\( t \\\\) and \\\\( s \\\\) denote the teacher and student. \\\\( l \\\\) denotes \\\\( L_1 \\\\) loss and \\\\( \\\\gamma \\\\) is a hyper-parameter to balance the loss.\\n\\nThe focal loss \\\\( L_{\\\\text{focal}} \\\\) is the sum of feature loss \\\\( L_{\\\\text{at}} \\\\) and attention loss \\\\( L_{\\\\text{at}} \\\\):\\n\\n\\\\[\\nL_{\\\\text{focal}} = L_{\\\\text{fea}} + L_{\\\\text{at}}\\n\\\\]\\n\\n3.2. Global Distillation\\n\\nThe relation \\\\[2,12,29\\\\] between different pixels has valuable knowledge and is utilized to improve the performance for detection tasks. And in Sec. 3.1, we utilize Focal Distillation to separate the images and force the student focus on crucial parts. However, such distillation cuts off the relation between foreground and background. So here we propose Global Distillation, which aims to extract the global relation between different pixels from the feature maps and distill it from the teacher to the student.\\n\\nAs shown in Fig. 4, we utilize GcBlock \\\\[2\\\\] to capture the global relation information in a single image and force...\"}"}
{"id": "CVPR-2022-698", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. The Global Distillation with GcBlock. The inputs are the feature maps from the teacher's neck and student's neck, respectively.\\n\\nthe student detector to learn the relation from the teacher detector. The global loss $L_{\\\\text{global}}$ is as follows:\\n\\n$$L_{\\\\text{global}} = \\\\lambda \\\\cdot X_R F_T - R F_S$$\\n\\n$$R(F) = F + W_v^2(\\\\text{ReLU}(LN(W_v^1(N_p X_j = 1 e W_k F_j P N_p m = 1 e W_k F_M F))))$$\\n\\nwhere $W_k$, $W_v^1$, and $W_v^2$ denote convolutional layers, LN denotes the layer normalization, $N_p$ is the number of pixels in the feature and $\\\\lambda$ is a hyper-parameter to balance the loss.\\n\\n3.3. Overall loss\\n\\nTo sum up, we train the student detector with the total loss as follows:\\n\\n$$L = L_{\\\\text{original}} + L_{\\\\text{focal}} + L_{\\\\text{global}}$$\\n\\nwhere $L_{\\\\text{original}}$ is the original loss for detectors.\\n\\nThe distillation loss is calculated just on feature maps, which can be obtained from the neck of the detectors. So it can be easily applied to different detectors.\\n\\n4. Experiments\\n\\n4.1. Dataset\\n\\nWe evaluate our knowledge distillation method on COCO dataset [18], which contains 80 object classes. We use the 120k train images for training and 5k val images for testing for all the experiments. The performances of different detectors are evaluated in Average Precision and Average Recall.\\n\\n| Method       | mAP  | AP  | mAP  | AP  |\\n|--------------|------|-----|------|-----|\\n| RetinaNet-Res101(T) | 38.9 | 21.0 | 42.8 | 52.4 |\\n| RetinaNet-Res50(S)   | 37.4 | 20.6 | 40.7 | 49.7 |\\n| FGFI [28]           | 38.6 | 21.4 | 42.5 | 51.5 |\\n| GID [5]             | 39.1 | 22.8 | 43.1 | 52.3 |\\n| Ours                | 39.6 | 22.9 | 43.7 | 53.6 |\\n| Ours \u2020              | 39.7 | 22.0 | 43.7 | 53.6 |\\n| RCNN-Res101(T)      | 39.8 | 22.5 | 43.6 | 52.8 |\\n| RCNN-Res50(S)       | 38.4 | 21.5 | 42.1 | 50.3 |\\n| FGFI [28]           | 39.3 | 22.5 | 42.3 | 52.2 |\\n| GID [5]             | 40.2 | 22.7 | 44.0 | 53.2 |\\n| Ours                | 40.4 | 22.8 | 44.5 | 53.5 |\\n| Ours \u2020              | 40.5 | 22.6 | 44.7 | 53.2 |\\n| FCOS-Res101(T)      | 40.8 | 24.2 | 44.3 | 52.4 |\\n| FCOS-Res50(S)       | 38.5 | 21.9 | 42.8 | 48.6 |\\n| GID [5]             | 42.0 | 25.6 | 45.8 | 54.2 |\\n| Ours                | 42.1 | 27.0 | 46.0 | 54.6 |\\n| Ours \u2020              | 42.7 | 27.2 | 46.5 | 55.5 |\\n\\nTable 2. Results of different distillation methods with different detection frameworks on COCO dataset.\\n\\nT and S mean the teacher and student detector, respectively. FGFI can only be applied to an anchor-based detector. \u2020 means using inheriting strategy. We train the FCOS with tricks including GIoULoss, norm-on-bbox and center-sampling which is the same as GID.\\n\\n4.2. Details\\n\\nWe conduct experiments on different detection frameworks, including two-stage models [22], anchor-based one-stage models [17], and anchor-free one-stage models [26, 32]. Besides, we verify our method on the Mask RCNN [8] and get significant improvement for instance segmentation.\\n\\nKang et al. [14] propose inheriting strategy which initializes the student with the teacher's neck and head parameters and gets better results. Here we use this strategy to initialize the student which has the same head structure as the teacher. All the experiments are conducted with mmdet [4] with Pytorch [20].\\n\\nFGD uses $\\\\alpha$, $\\\\beta$, $\\\\gamma$, $\\\\lambda$ to balance the loss of foreground and background in Eq. (9), attention loss in Eq. (10) and global loss in Eq. (12), respectively. And $T = 0.5$ is used to adjust the attention distribution for all the experiments. We adopt the hyper-parameters $\\\\{\\\\alpha = 5 \\\\times 10^{-5}, \\\\beta = 2.5 \\\\times 10^{-5}, \\\\gamma = 5 \\\\times 10^{-5}, \\\\lambda = 5 \\\\times 10^{-7}\\\\}$ for all the two-stage models, $\\\\{\\\\alpha = 1 \\\\times 10^{-3}, \\\\beta = 5 \\\\times 10^{-4}, \\\\gamma = 1 \\\\times 10^{-3}, \\\\lambda = 5 \\\\times 10^{-6}\\\\}$ for all the anchor-based one-stage models, $\\\\{\\\\alpha = 1.6 \\\\times 10^{-3}, \\\\beta = 8 \\\\times 10^{-4}, \\\\gamma = 8 \\\\times 10^{-3}, \\\\lambda = 8 \\\\times 10^{-6}\\\\}$ for all the anchor-free one-stage models. We train all the detectors for 24 epochs with SGD optimizer, which the momentum is 0.9 and the weight decay is 0.0001.\"}"}
{"id": "CVPR-2022-698", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 3. Results of more detectors with stronger teacher detectors on COCO dataset.\\n\\n| Detector              | mAP  | AP  | mAP  | AP  |\\n|-----------------------|------|-----|------|-----|\\n| RetinaNet-Res50       | 37.4 | 20.6| 40.7 | 49.7|\\n| FKD [35]             | 39.6 (+2.2) | 22.7 | 43.3 | 52.5 |\\n| Ours                 | 40.4 (+3.0) | 23.4 | 44.7 | 54.1 |\\n| Ours\u2020                | 40.7 (+3.3) | 22.9 | 45.0 | 54.7 |\\n| Cascade Mask RCNN-Res50 | 39.2 | 22.9 | 42.6 | 51.2 |\\n| FKD [35]             | 41.5 (+3.1) | 23.5 | 45.0 | 55.3 |\\n| Ours                 | 42.0 (+3.6) | 23.8 | 46.4 | 55.5 |\\n| Ours\u2020                | 42.0 (+3.4) | 24.0 | 45.7 | 55.6 |\\n| RepPoints-Res50      | 38.6 | 22.5 | 42.2 | 50.4 |\\n| FKD [35]             | 40.6 (+2.0) | 23.4 | 44.6 | 53.0 |\\n| Ours                 | 41.3 (+2.7) | 24.5 | 45.2 | 54.0 |\\n| Ours\u2020                | 42.0 (+3.4) | 24.0 | 45.7 | 55.6 |\\n\\n4.3. Main Results\\n\\nOur method can be applied to different detection frameworks easily, so we first conduct experiments on three popular detectors, including a two-stage detector (Faster RNN), an anchor-based one-stage detector (RetinaNet) and an anchor-free detector (FCOS). We compare with other two knowledge distillation methods [5, 28] for object detection. In the experiments, we choose the detectors with ResNet-50 [9] as the students and the identical detectors with ResNet-101 as the teachers. As shown in Tab. 2, our distillation method surpasses the other two state-of-the-art methods. All the student detectors gain significant AP improvements with the knowledge transferred from teacher detectors, e.g. the RetinaNet based ResNet-50 gets 2.3 mAP improvement on COCO dataset. Furthermore, in this Res101-Res50 setting, the student detectors even outperform the teacher detectors by training with FGD.\\n\\n4.4. Distillation of more detectors with stronger students and teachers\\n\\nOur method can also be applied between heterogeneous backbones, e.g. the ResNeXt [31] based teacher detector distills the ResNet based student detector. Here we conduct experiments on more detectors and use stronger backbone-based teacher detectors. And we compare the results with FKD [28], which is another effective and general distillation method. As shown in Tab. 3, all the student detectors achieve significant improvements on both AP and AR. Besides, comparing the results with Tab. 2, we find that student detectors perform better with stronger teacher detectors, e.g. Retina-Res50 model achieves 40.7 and 39.7 mAP with ResNeXt101 and ResNet101 based teacher, respectively. The comparisons show that student detectors get the better feature by mimicking the feature maps of stronger backbones-based teacher detectors.\\n\\n4.5. Better feature with FGD\\n\\nAs shown in Tab. 2 and Tab. 3, initializing the student with the teacher's neck and head parameters brings another improvement, which indicates the student gets a similar feature with the teacher. So in this subsection, we visualize and compare the spatial attention mask and channel attention mask from teacher detector, student detector and student detector with FGD, which is shown in Fig. 5. Comparing the attention masks between the teacher and student, we can see they have a big difference in the distribution of pixels and channels before distillation, e.g. the teacher detector focuses more on the fingers and has a larger weight in channel 241. However, after training with FGD, the student detector...\"}"}
{"id": "CVPR-2022-698", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Visualization of the spatial and channel attention mask from different detectors. Each pixel in the channel attention mask means a channel.\\n\\nTeacher detector: RetinaNet-ResNeXt101.\\nStudent detector: RetinaNet-ResNet50\\n\\nThe student detector has a similar distribution of pixels and channels with the teacher detectors, which means the student focuses on the same parts as the teacher. This also explains how FGD helps the student detector perform better. Based on a similar feature, the student detector gets significant improvements and even outperforms the teacher detector.\\n\\n4.6. Analysis\\n\\n4.6.1 Sensitivity study of different losses\\nIn this paper, we transfer the focal knowledge and global knowledge from the teacher to the student. In this subsection, we conduct experiments of focal loss ($L_{\\\\text{focal}}$) and global loss ($L_{\\\\text{global}}$) to investigate their influences on the student with RetinaNet. As shown in Tab. 4, both the focal loss and global loss lead to significant AP and AR improvements. Furthermore, considering targets with different sizes, we find $L_{\\\\text{focal}}$ benefits more to the large size targets and $L_{\\\\text{global}}$ benefits more to the small and medium targets. Besides, when combining $L_{\\\\text{focal}}$ and $L_{\\\\text{global}}$, we achieve 40.4 mAP and 56.7 mAR, which indicates the focal loss and global loss are complementary to each other.\\n\\n4.6.2 Sensitivity study of focal distillation\\nIn focal distillation, we use the ground-truth boxes to separate the images and guide the student with the teacher's attention masks. In this subsection, we explore the effectiveness of focal distillation.\\n\\nAs shown in Tab. 1, we find distilling just on foreground or background both lead significant improvements. Here we analyze different error types to investigate their effectiveness, which is shown in Fig. 6. With the knowledge from background, student detectors reduce the false-positive predictions and get higher mAP. In comparison, the foreground's distillation helps students detect more targets and reduce the false-negative predictions. In conclusion, the results show that both foreground and background are crucial and have different functions for the student detectors.\\n\\nIn this paper, we utilize the spatial and channel attention mask of the teacher to guide the student to focus on crucial parts. Here we conduct experiments with RetinaNet to explore the effects of each mask, which is shown in Tab. 5. Each attention mask improves the performance, especially the spatial attention mask which brings 2.6 mAP gains and 2.2 mAR gains. And the combination of two masks gets the best result. The experiments show both the attention masks help the student perform better.\"}"}
{"id": "CVPR-2022-698", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Figure 6.** Different error types analyses of foreground and background distillation.\\n\\n- **FN**: false negative prediction;\\n- **BG**: background false positive prediction;\\n- **Oth**: classification errors;\\n- **Sim**: wrong class but correct supercategory;\\n- **Loc**: localization errors\\n\\n**Table 5.** Ablation study of the spatial and channel attention mask.\\n\\n| Method               | ReinaNet ResNeXt101-Res50 | Spatial attention | Channel attention |\\n|----------------------|---------------------------|-------------------|------------------|\\n|                      |                           | \u2713                 | \u2713                |\\n|                      |                           |                   |                  |\\n\\n| mAP                  | 37.4                      | 40.0              | 39.7             |\\n|----------------------|---------------------------|-------------------|------------------|\\n| AP S                 | 20.0                      | 22.3              | 22.0             |\\n|                      |                           |                  |                  |\\n| AP M                 | 40.7                      | 44.0              | 43.5             |\\n|                      |                           |                  |                  |\\n| AP L                 | 49.7                      | 53.6              | 53.4             |\\n|                      |                           |                  |                  |\\n| mAR                  | 53.9                      | 56.1              | 55.8             |\\n| AR S                 | 33.1                      | 36.5              | 35.7             |\\n|                      |                           |                  |                  |\\n| AR M                 | 57.7                      | 60.2              | 59.9             |\\n|                      |                           |                  |                  |\\n| AR L                 | 70.2                      | 72.1              | 71.8             |\\n|                      |                           |                  |                  |\\n\\n**Table 6.** Comparison of different global relation methods on Faster RCNN ResNeXt101-Res50.\\n\\n| Methods          | baseline | Non-Local | GcBlock |\\n|------------------|----------|-----------|---------|\\n| mAP S            | 38.4     | 39.8      | 41.5    |\\n| mAR S            | 21.5     | 22.7      | 23.4    |\\n| mAP M            | 42.1     | 43.1      | 46.0    |\\n| mAR M            | 50.3     | 52.3      | 55.3    |\\n\\n**Table 7.** Ablation study of temperature hyper-parameter $T$ on RetinaNet ResNeXt101-Res50.\\n\\n| $T$  | mAP | mAR |\\n|------|-----|-----|\\n| 0.3  | 40.1| 56.4|\\n| 0.5  | 40.4| 56.7|\\n| 0.8  | 40.4| 56.6|\\n| 1.0  | 40.2| 56.5|\\n| 1.2  | 40.0| 56.4|\\n\\n**4.6.3 Sensitivity study of global distillation**\\n\\nIn global distillation, we rebuild the relation between different pixels to compensate for the missing global information in focal distillation and transfer it from the teacher detector to the student detector. In this subsection, we distill the student just using the global distillation with GcBlock [2] or Non-local module [29] on Faster RCNN, which is shown in Tab. 6. The results show both two relation methods extract effective global information and bring the student effective improvement, especially the GcBlock which brings 3.1 mAP improvement.\\n\\n**4.6.4 Sensitivity study of $T$**\\n\\nIn Eq. (7) and Eq. (8), we use the temperature hyper-parameter $T$ to adjust the pixels and channels distribution of the feature map. The gap between pixels and channels becomes wider and smaller when $T < 1$ and $T > 1$, respectively. Here we conduct several experiments to investigate the influence of $T$. As shown in Tab. 7, when $T = 0.5$, the student gains 0.2 mAP and 0.2 mAR improvement compared with $T = 1$, which means distillation without distribution adjustment. With $T = 0.5$, the pixels and channels of high value are emphasized more and this helps the student detector focus on such crucial parts more and perform better. It is also observed the worst result is just a 0.4 mAP drop compared with the best result, indicating our method is not sensitive to the hyper-parameter $T$.\\n\\n**5. Conclusion**\\n\\nIn this paper, we point out the student detector needs to pay attention to both the crucial parts and global relations from the teacher. Then we propose Focal and Global Distillation (FGD) to guide the student detectors. Extensive experiments on various detectors prove that our method is simple and efficient. Furthermore, our method is just based on the feature so that FGD can be applied to two-stage detectors, anchor-based one-stage detectors, and anchor-free one-stage detectors easily. The analysis shows that the student gets a really similar feature with the teacher and initializing the student with the teacher's parameters can bring another improvement. However, our understanding of how to get a better head is preliminary and left as future works.\\n\\n**Acknowledgement.** This work was supported by the NSFC project Grant No.U1833101, the SZSTI project Grant No.JCYJ20190809172201639 and Grant NO.WDZC20200820200655001.\"}"}
