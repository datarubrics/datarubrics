{"id": "CVPR-2024-1675", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neural Refinement for Absolute Pose Regression with Feature Synthesis\\n\\nShuai Chen\\nYash Bhalgat\\nXinghui Li\\nJia-Wang Bian\\nKejie Li\\nZirui Wang\\nVictor Adrian Prisacariu\\n\\n1 Active Vision Lab, University of Oxford\\n2 Visual Geometry Group, University of Oxford\\n\\nAbstract\\n\\nAbsolute Pose Regression (APR) methods use deep neural networks to directly regress camera poses from RGB images. However, the predominant APR architectures only rely on 2D operations during inference, resulting in limited accuracy of pose estimation due to the lack of 3D geometry constraints or priors. In this work, we propose a test-time refinement pipeline that leverages implicit geometric constraints using a robust feature field to enhance the ability of APR methods to use 3D information during inference. We also introduce a novel Neural Feature Synthesizer (NeFeS) model, which encodes 3D geometric features during training and directly renders dense novel view features at test time to refine APR methods. To enhance the robustness of our model, we introduce a feature fusion module and a progressive training strategy. Our proposed method achieves state-of-the-art single-image APR accuracy on indoor and outdoor datasets. Code will be released at https://github.com/ActiveVisionLab/NeFeS.\\n\\n1. Introduction\\n\\nCamera relocalization is a crucial task that allows machines to understand their position and orientation in 3D space. It is an essential prerequisite for applications such as augmented reality, robotics, and autonomous driving, where the accuracy and efficiency of pose estimation are important. Recently, Absolute Pose Regression (APR) methods [21\u201323] have been shown to be effective in directly estimating camera pose from RGB images using convolutional neural networks. The simplicity of APR's architecture offers several potential advantages over classical geometry-based methods [5, 43, 45], involving end-to-end training, cheap computation cost, and low memory demand.\\n\\nLatest advances in APR, particularly the use of novel view synthesis (NVS) [10, 11, 29, 32, 33, 49] to generate new images from random viewpoints as data augmentation during training, have significantly improved the pose regression performance. Despite this, state-of-the-art (SOTA) (a) Before Pose Refinement (b) After Pose Refinement Figure 1. Our pose refinement ($R$) improves (coarse) pose predictions from other methods using novel feature synthesis to achieve pixel-wise alignment. Top left / right: 3D plots of predicted (green) and ground-truth (red) camera positions. Bottom left / right: alignment between rendered features and query image.\\n\\nAPRs still have the following limitations: (i) They predict the pose of a query image by passing it through a CNN, which typically disregards geometry at inference time. This causes APR networks to struggle to generalize to viewpoints that the training data fails to cover [46]; (ii) The unlabeled data, often sampled from the validation/testing set, used for finetuning the APR network [8, 10, 11] may not be universally available in real-life circumstances, and this semi-supervised finetuning is also time-consuming.\\n\\nTo address these limitations, we propose a novel test-time refinement pipeline for APR methods. Unlike prior works that explore extended Kalman filters [34], pose graph optimization [8], or pose auto-encoders [49], our method integrates an implicit representation based geometric refinement into an end-to-end learning framework, where gradients can be backpropagated to the APR network. We test our proposed method across different APR architectures to...\"}"}
{"id": "CVPR-2024-1675", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"demonstrate its robustness and effectiveness. Furthermore, we propose a Neural Feature Synthesizer (NeFeS) network to encode the 3D geometry of a scene implicitly into an MLP. NeFeS renders dense features from novel viewpoints for refinement. To ensure the robustness of feature rendering, we introduce a Feature Fusion module into NeFeS that combines the rendered color and features and is trained in a progressive manner. Our method leverages prior literature on volume rendering to inherently constrain geometric consistency during test time using implicit 3D neural feature fields. As such, our approach occupies a middle ground between APR and methods informed by geometry.\\n\\nWe summarize our main contributions as follows: First, we propose a test-time refinement pipeline that greatly improves the pose-estimation accuracy of any APR model without using additional unlabeled data and exhibits a new single-frame APR SOTA performance on standard benchmarks. Second, we propose a Neural Feature Synthesizer (NeFeS) network that implicitly encodes 3D geometric features. NeFeS refines an initial pose by rendering a dense feature map and making the comparison with the query image feature. Third, we propose a progressive training strategy and a Feature Fusion module to improve the robustness of the rendering ability of the NeFeS model.\\n\\n2. Related Work\\n\\nAbsolute Pose Regression (APR). APR methods have been widely studied due to their simple and lightweight formulation that allows the camera pose to be directly regressed using an end-to-end neural network. PoseNet [21\u201323] introduced the first APR solution using GoogLeNet-backbone, followed by various architectures like the hourglass network [31], attention layers [50, 51, 61], separated translation and rotation prediction [35, 63], or LSTM [60]. To further improve APR accuracy, some works utilize sequential information. These approaches incorporate temporal constraints such as visual odometry [8, 39, 58], motion [34], temporal filtering [15], and multitasking [39]. Recent APR methods also benefit from novel view synthesis, where one line of approaches focuses on generating large amounts of extra photo-realistic synthetic data [11, 33, 38] via randomly sampled virtual camera poses. However, generating high-quality offline synthetic data may take several days [33] for each scene. Other approaches [10, 11] use NeRF [29, 32] as a direct matching module to perform unlabeled finetuning [8] using extra images without ground-truth pose annotation. However, finetuning takes significant time and assumes that extra unlabeled data can be easily obtained. While the aforementioned works enhance APR training, we focus on improving generic APR methods during test time. Unlike prior works that only exam means for test-time refinement on a single specific APR architecture, such as extended Kalman filters [34], pose graph optimization [8], or pose auto-encoders [49], our method exhibits strong flexibility to be adapted to a wide range of APR architectures on both camera positions and orientations, achieving state-of-the-art results without extra unlabeled data. Notably, classical geometry-based techniques [4\u20136, 28, 41, 43\u201345] that require explicit feature correspondence search [16, 17, 26, 42, 54, 55] also employ test-time refinement to improve localization accuracy. For example, [28, 41, 43] build upon image retrievals and pre-computed SfM model to perform standard geometric refinement via neural network-based feature matcher, PnP+RANSAC, or dense feature metric-alignment. Our method, however, offers end-to-end neural feature refinement via implicit representation, enhancing existing APR models without external storage, pre-computed data, or manual tuning.\\n\\nNeural Radiance and Feature Fields. Neural Radiance Fields (NeRF) [32] revolutionized novel view image synthesis and 3D surface reconstruction. NeRF's implicit 3D representation and differentiable volume rendering enable self-supervised optimization from RGB images, avoiding costly 3D annotations. iNeRF [64] showed that NeRF can be inverted for pose optimization. Recent approaches such as BARF [27] and its counterparts [3, 14, 62] simultaneously train NeRF by treating camera poses as learnable parameters in simple, non-360\u00b0 scenes. Parallel works, NICE-SLAM [65] and iMAP [53], use NeRF for dense geometry and real-time camera tracking. Direct-PN [10] uses NeRF as a direct matching module to compute the photometric errors and propagate the error gradients back to the pose regression network. DFNet [11] extends this method to outdoor scenarios with robust feature extraction. LENS [33] uses NeRF to generate a synthetic training dataset based on manually tuned scene bounds and parameters. Recently, NeRF models have been extended to directly predict and render feature fields alongside density and appearance fields. Typically, these feature fields are learned by supervision from a 2D feature extractor using volumetric rendering. [2, 24, 57] showed that these 3D feature fields outperform 2D baselines [9, 12, 25] on downstream tasks such as 2D object retrieval or 3D segmentation. CLIP-Fields [48] established feature fields as scene memory for robot navigation. This work explores distilled neural feature fields for camera relocalization, highlighting their role in test-time pose refinement.\\n\\n3. Method\\n\\nIn this section, we present a detailed outline of our approach. Sec. 3.1 provides a high-level overview of our refinement framework. Sec. 3.2 describes the architecture and training details of our proposed NeFeS network along with its two components: Exposure-adaptive Affine Color Transformation (ACT) and Feature Fusion module.\"}"}
{"id": "CVPR-2024-1675", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Illustration of the pose refinement pipeline. The query image is processed by a pose estimator $F$, typically an absolute pose regressor, to obtain a coarse camera pose $\\\\hat{P}$. Our novel feature synthesizer $N$ renders a dense feature map $f_{rend}$ based on $\\\\hat{P}$. Simultaneously, the feature extractor $G$ extracts the feature map $f_G$ from the query image. We then compute the feature-metric error between $f_{rend}$ and $f_G$, denoted as $L_{feature}$. This error is backpropagated to update either the parameters of $F$ or the pose $\\\\hat{P}$ directly.\\n\\n3.1. Refinement Framework for APR\\n\\nGiven a query image $I$, an absolute pose regression (APR) network $F$ directly regresses the camera pose $\\\\hat{P}$ of $I$: $\\\\hat{P} = F(I)$. The network is typically trained with ground truth image-pose pairs. While APR-based methods are much more efficient than geometry-based methods since they require only a single forward pass of the network, the quality of their predictions is often significantly worse than those of geometry-based methods due to the lack of any 3D geometry-based reasoning [46].\\n\\nIn contrast to prior APR research, which attempts to improve APR by adding constraints to the training loss or making architectural changes to the backbone network, we propose an alternative method to refine the results of APR methods by backpropagating a feature-metric error at inference time. Our method has three major components (see Fig. 2): (1) a pretrained APR network, denoted as $F$, which provides an initial pose; (2) a differentiable novel feature synthesizer $N$ that directly renders dense feature maps given a camera pose; (3) an off-the-shelf feature extractor $G$ that extracts the dense feature map of the query image. In our implementation, the feature extraction module from [11] is employed as the feature extractor $G$.\\n\\nThe refinement procedure is as follows: (i) The query image $I$ is passed through the pretrained APR model $F$ to predict a coarse camera pose $\\\\hat{P}$. (ii) The feature synthesizer $N$ renders a dense feature map $f_{rend} \\\\in \\\\mathbb{R}^{n \\\\times c}$ given the coarse camera pose $\\\\hat{P}$, where $n = h \\\\times w$, and $h$ and $w$ are the spatial dimensions of the feature map $f_{rend}$. (iii) At the same time, the feature extractor $G$ extracts a feature map $f_G = G(I)$ from the query image, where $f_G \\\\in \\\\mathbb{R}^{n \\\\times c}$. (iv) The pose $\\\\hat{P}$ is iteratively refined by minimizing the feature cosine similarity loss $L_{feature}$ between $f_{rend}$ and $f_G$:\\n\\n$$L_{feature} = \\\\sum_{i=1}^{c} \\\\frac{1 - \\\\langle f_{rend}, i, f_G, i \\\\rangle}{\\\\|f_{rend}, i\\\\|_2 \\\\cdot \\\\|f_G, i\\\\|_2} (1)$$\\n\\nNote: We treat the $n$ dimension as the feature rather than the $c$ dimension.\\n\\nOur method can be regarded as post-processing to the initial pose $\\\\hat{P}$. We do not save the updated weights of the APR method since we restart from the initial state when given a new query image.\\n\\n3.2. Neural Feature Synthesizer\\n\\nWe propose a Neural Feature Synthesizer (NeFeS) model that directly renders dense feature maps of a given viewpoint to refine the predictions of an underlying APR network. Similar to NeRF-W [29], our NeFeS architecture uses a base MLP module with static and transient heads that predict the static and transient density ($\\\\sigma(s)$ and $\\\\sigma(\\\\tau)$) and view-dependent color ($c(s)$ and $c(\\\\tau)$) respectively, given an input 3D position ($x$) and viewing direction ($d$). We use the frequency encoding [32, 59] to encode all 3D positions and view directions. The transient head models the colors of the 3D points using an isotropic normal distribution and predicts a view-dependent variance value ($\\\\beta^2$) for the transient color distribution. To render the color of a given pixel, the original volume rendering formulation in NeRF [32] is augmented to include the transient colors and densities, and the color of a given image-pixel ($\\\\hat{C}(r)$) is computed as a composite of the static and transient components. Here, $r$ denotes the ray (corresponding to the pixel) on which points are sampled to compute the volume rendering quadrature approximation [30]. The variances of sampled points along\"}"}
{"id": "CVPR-2024-1675", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. The architecture of our proposed NeFeS model. The query 3D position $x$ is fed to the network after positional encoding $\\\\text{PE} \\\\cdot$. The network then splits into two heads: the static head and the transient head. Given a viewing direction $d$, the rendered color map is generated by fusing static RGB value $c_s^i$, the transient RGB value $c_t^i$ and their corresponding density values $\\\\sigma_s^i$ and $\\\\sigma_t^i$, while the rendered feature map is formed only by static features $f_i$ and density $\\\\sigma_s^i$. In addition, the color map adopts exposure-adaptive ACT to compensate for exposure differences between images. The final feature map $\\\\hat{F}_{\\\\text{fusion}}$ is the concatenation of rendered RGB and feature map processed by the feature fusion module.\\n\\nWe expand the output of the static MLP to also predict features for an input 3D position. The output dimension is $N_c + N_f$, where $N_f$ features are predicted along with RGB values. The per-pixel features are rendered using the same volume rendering quadrature approximation [30]:\\n\\n$$\\\\hat{F}_f(r) = N \\\\sum_{i=1}^{T_i} 1 - \\\\exp(-\\\\sigma_s^i \\\\delta_i f_i),$$\\n\\n(2)\\n\\nwhere $f_i$ and $\\\\sigma_s^i$ are the feature and density predicted by the static MLP for a sampled point on the ray, and $\\\\delta_i$ is the distance between sampled quadrature points $i$ and $i + 1$.\\n\\nFig. 3. The architecture of our proposed NeFeS model. We propose two crucial components in the rendering pipeline of our NeFeS architecture that ensure the robustness of our rendered features.\\n\\n**Exposure-adaptive ACT.** In the context of camera relocalization, testing images may differ in exposure or lighting from training sequences. To address this, DFNet [11] proposed using the luminance histogram of the query image as a latent code input to the color prediction head of the NeRF MLP. However, since our NeFeS outputs both colors and features simultaneously, we find this approach perturbs the feature output values and causes instability. Ideally, the feature descriptors should be able to maintain local invariance even under varying exposure. Inspired by Urban Radiance Fields (URF) [40], we propose to use an exposure-adaptive Affine Color Transformation (ACT) which is a $3 \\\\times 3$ matrix $K$ and a $3$-dimensional bias vector $b$ predicted by a $4$-layer MLP with the query image's luminance histogram $y_I$. Unlike URF, which uses a pre-determined exposure code, we use the query image's histogram embedding for accurate appearance rendering of unseen testing images. The final per-pixel color $\\\\hat{C}(r)$ is computed using the affine transformation as $\\\\hat{C}(r) = K \\\\hat{C}_{\\\\text{rend}}(r) + b$, where $\\\\hat{C}_{\\\\text{rend}}(r)$ is the rendered per-pixel color obtained using the static and transient MLPs.\\n\\n**Feature Fusion Module**\\n\\nWe propose a Feature Fusion module to fuse the rendered colors and features to produce the final feature map. The rendered colors and features are concatenated and fed into the fusion module consisting of three $3x3$ convolutions, followed by a $5x5$ convolution and a batch normalization layer. During inference, we render colors and features for all $H \\\\times W$ image pixels and the resulting $H \\\\times W \\\\times (N_c + N_f)$ tensor is processed by the module. Note, for efficiency during training, we sample $S \\\\times S$ regions to render and apply the loss to those pixels each iteration. We use $H$ to represent the fusion module. The final output feature result is:\\n\\n$$\\\\hat{F}_{\\\\text{fusion}}(R) = H(\\\\hat{C}(R), \\\\hat{F}_f(R))$$\\n\\n(3)\\n\\nwhere $R$ is the sampled region as described above.\\n\\nWe experimentally find that the fusion module produces more robust features than the input rendered features $\\\\hat{F}_f$. We refer readers to the supplementary for detailed ablations.\\n\\n**Training the Feature Synthesizer**\\n\\nThe high-level concept of training the NeFeS is motivated by feature field distillation proposed in [24], which essentially distills the 2D backbone features into a 3D NeRF model. However, 2D features in our NeFeS need to be closely related to the direct matching formulation [10, 20]. In this work, we use.\"}"}
{"id": "CVPR-2024-1675", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the trained 2D feature extractor from [11] to produce the feature labels due to its effectiveness in generating domain invariant features.\\n\\nLoss Functions.\\n\\nThe total loss used to train our NeFeS model consists of a photometric loss $L_{\\\\text{rgb}}$ and two $l_1$-based feature-metric losses:\\n\\n$$L = L_{\\\\text{rgb}} + \\\\lambda_1 L_f + \\\\lambda_2 L_{\\\\text{fusion}}. \\\\quad (4)$$\\n\\nThe photometric loss is defined as the negative log-likelihood of a normal distribution with variance $\\\\beta(r)$:\\n\\n$$L_{\\\\text{rgb}}(r) = \\\\frac{1}{2\\\\beta(i(r))C(r) - \\\\hat{C}(r)} + \\\\frac{1}{2} \\\\log \\\\beta(r) + \\\\lambda_s K \\\\sum_{k=1}^{K} \\\\sigma(\\\\tau_k) \\\\quad (5)$$\\n\\nwhere $r$ is the ray direction corresponding to an image pixel, $C_i(r)$ and $\\\\hat{C}_i(r)$ are the ground-truth and rendered pixel colors. The third term in Eq. (5) is a sum of the transient densities of all the points on ray $r$ and is used to ensure that transient densities are sparse.\\n\\nThe feature losses are simply $l_1$ losses:\\n\\n$$L_f = \\\\sum_r \\\\| \\\\hat{F}_f(r) - F_{\\\\text{img}}(I, r) \\\\|_1, \\\\quad (6)$$\\n\\nand\\n\\n$$L_{\\\\text{fusion}} = \\\\sum_r \\\\| \\\\hat{F}_{\\\\text{fusion}}(r) - F_{\\\\text{img}}(I, r) \\\\|_1. \\\\quad (7)$$\\n\\nwhere $F_{\\\\text{img}}(I, \\\\cdot)$ are the features extracted from the training images using the pre-trained 2D feature extractor [11]. Note that, $L_f$ is applied to the rendered features $\\\\hat{F}_f$ and $L_{\\\\text{fusion}}$ is applied to the fused features $\\\\hat{F}_{\\\\text{fusion}}$. We experimentally find that using $l_1$ gives more robust features than $l_2$ and cosine feature loss for the test time refinement.\\n\\nProgressive Training.\\n\\nWe propose using a progressive schedule to train the NeFeS model. We first train the color and density part of the network for $T_1$ epochs to bootstrap the correct 3D geometry for the network. For these epochs, only $L_{\\\\text{rgb}}$ is used. Then we add $L_f$ with weight $\\\\lambda_1$ for the next $T_2$ epochs to train the feature part of the static MLP. Since the ground-truth features may not be fully multi-view consistent, we apply stop-gradients to the predicted density for the feature rendering branch. And finally, we add the feature fusion loss $L_{\\\\text{fusion}}$ with weight $\\\\lambda_2$ for the last $T_3$ epochs. Since the feature fusion module takes both RGB images and 2D features as input, we randomly sample $N_{\\\\text{crop}}$ patches of $S \\\\times S$ regions of the image and features to increase training efficiency. According to our experiments, this progressive training schedule leads to better convergence and performance. In addition, we apply semantic filtering to improve the network training results. Specifically, we use an off-the-shelf panoptic segmentation method [13] to mask out temporal objects in the scene such as people and moving vehicles.\\n\\n3.3. Direct Pose Refinement\\n\\nWhile our method is primarily designed to optimize APR, it is also possible to directly optimize camera pose parameters. We explore this feature by showing a possible scenario wherein the source of the pose estimation is either a black box or cannot be optimized (e.g. the initial camera pose comes from image retrieval). In these settings, we can set up our proposed method to directly refine the camera poses. Specifically, given an estimated camera pose $\\\\hat{P} = [R|t]$, where $R$ is rotation and $t$ is the translation component, our method optimizes the camera poses using tangent space backpropagation. Additionally, we found that using two different learning rates for the translation and rotation parts helps achieve faster and more stable convergence for camera pose refinement. This is different from the standard convention used in [3, 14, 27, 62]. We refer our readers to supplementary material for more details.\\n\\n4. Experiments\\n\\nWe implement our method in PyTorch [37]. Implementation details about the NeFeS architecture and progressive training scheduling can be found in the supplementary.\\n\\n4.1. Evaluation on Cambridge Landmarks\\n\\nWe evaluate our proposed refinement method on Cambridge Landmarks [23], which is a popular outdoor dataset used for benchmarking pose regression methods. The dataset contains handheld smartphone images of scenes with large exposure variations and covers an area of $875 \\\\text{m}^2$ to $5600 \\\\text{m}^2$. The training sequences contain 200-1500 samples, and test sets are captured from different sequences. For each test image, we refine the model using the approach in Sec. 3.1 for $m = 50$ iterations.\\n\\nWe first test our method on top of an open-sourced SOTA single-frame APR method. Tab. 1 summarizes the results of our method and existing APR methods. Our method achieves the best accuracy across all four scenes when coupled with the DFNet. In Sec. 4.3, we demonstrate the performance of our method with other APR approaches. Particularly, our method improves DFNet by as much as 70.6% compared to its scene average results. All the per-scene performances from the other compared methods are taken from their papers, except for MS-Transformer+PAE, which only reports the scene average median errors. We encourage our readers to check out supplementary for more thorough comparisons and ablations to the Cambridge Landmarks dataset.\\n\\n2 We use the LieTorch [56] library for this.\\n\\n3 Supplementary: Implementation Details\"}"}
{"id": "CVPR-2024-1675", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparisons on Cambridge Landmarks. We compare our proposed test-time refinement method with single-frame APR methods. The subscript of NeFeS denotes the number of optimization iterations used for APR refinement. We report the median position and orientation errors in m/\u00b0. The best results are highlighted in bold.\\n\\n| Methods       | Chess | Fire | Heads | Office | Pumpkin | Kitchen | Stairs | Average |\\n|---------------|-------|------|-------|--------|---------|---------|--------|---------|\\n| PoseNet       | 0.10  | 0.27 | 0.18  | 0.17   | 0.19    | 0.22    | 0.35   | 0.21    |\\n| MapNet        | 0.13  | 0.33 | 0.19  | 0.25   | 0.28    | 0.32    | 0.43   | 0.28    |\\n| MS-Transformer| 0.11  | 0.23 | 0.13  | 0.18   | 0.17    | 0.16    | 0.29   | 0.18    |\\n| PAE           | 0.13  | 0.24 | 0.14  | 0.19   | 0.17    | 0.18    | 0.30   | 0.19    |\\n| DFNet         | 0.03  | 0.06 | 0.04  | 0.06   | 0.07    | 0.07    | 0.12   | 0.06    |\\n| DFNet + NeFeS | 0.02  | 0.02 | 0.02  | 0.02   | 0.02    | 0.02    | 0.05   | 0.02    |\\n\\nTable 2. Comparisons on 7-Scenes dataset. We compare the proposed refinement method with previous single-frame APR methods. We evaluate all methods with SfM ground truth poses provided in [7], measured in median translational error (m) and rotational error (\u00b0). Numbers in bold represent the best performance.\\n\\n4.2. Evaluation on 7-Scenes\\nWe further evaluate our method on Microsoft 7-Scenes dataset [19, 52], which includes seven indoor scenes ranging in size from 1m\u00b3 to 18m\u00b3 and up to 7000 training images for each scene. We sub-sampled the large training sequences in the dataset to 1000 images for training our NeFeS model. The original 'ground truth' (GT) poses are obtained from RGB-D SLAM (dSLAM) [36]. However, we observe imperfection in the GT poses due to the asynchronous data between the RGB and depth sequences, and this results in low-quality NeRF renderings, as shown in Fig. 4. Thus, we use alternative 'ground truth' provided by [7] for our experiments. The authors [7] demonstrate that their camera poses reconstructed by COLMAP [47], a structure-from-Motion (SfM) library, are more accurate for image-based relocalization. We refer the reader to our supplementary for more details about the GT poses.\\n\\nFor a fair comparison, we use the SfM poses to re-train baseline APR methods using their official code, except for PoseNet, in which we use the open-sourced code from [10]. We trained each APR method 3-4 times to select the best-performing model. We use DFNet + NeFeS 50 with the same settings as in Cambridge for our pose refinement experiment. We achieve state-of-the-art results (59%+ better in scene average) in all scenes by running 50 optimization steps (see Tab. 2). The results by using the dSLAM ground-truth poses are included in the supplementary, where we also achieve SOTA accuracy.\"}"}
{"id": "CVPR-2024-1675", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 3. Pose refinement on different APR architectures.\\n\\n| Dataset      | NetVlad Optimize Pose | Ours                      |\\n|--------------|-----------------------|---------------------------|\\n| 7-Scenes     | 0.32m/13.72\u00b0          | 0.14m/3.97\u00b0               |\\n| Cambridge    | 3.18m/7.74\u00b0           | 1.15m/1.30\u00b0               |\\n\\nOur refinement method can effectively improve pose estimation results for different APR architectures.\\n\\n### Table 4. Pose refinement on NetVlad.\\n\\n| Dataset      | NetVlad Optimize Pose | Ours                      |\\n|--------------|-----------------------|---------------------------|\\n| 7-Scenes     | 0.06m/1.93\u00b0           | 0.02m/0.79\u00b0               |\\n| Cambridge    | 1.19m/2.90\u00b0           | 0.35m/0.77\u00b0               |\\n\\nOur method also works on poses initialized with non-APR-based methods, such as NetVlad image retrieval. Since the initial pose error is relatively large, we refine the poses with 100 iterations.\\n\\n### Table 5. Pose refinement vs. APR refinement.\\n\\n| Dataset      | NetVlad Optimize Pose | Ours                      |\\n|--------------|-----------------------|---------------------------|\\n| 7-Scenes     | 0.06m/1.93\u00b0           | 0.04m/1.16\u00b0               |\\n| Cambridge    | 1.19m/2.90\u00b0           | 0.66m/1.39\u00b0               |\\n\\nWe study on the optimization over APR vs. the pose. Both methods can effectively optimize poses. However, optimizing APR can obtain lower errors than optimizing poses given the same number of iterations.\"}"}
{"id": "CVPR-2024-1675", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Experiments on pose refinement bounds of our method in indoor and outdoor scenes. Each plot shows errors before (x-axis) and after (y-axis) refinement when ground-truth pose is perturbed by varying magnitudes. Dashed green line is $y = x$. Points below this line indicate a reduction in pose error using our refinement method.\\n\\n(a) NeFeS Architecture Ablation\\nMethod Shop Facade\\nInitial Pose Error 0.67m/2.21\u00b0\\nRefine w/ NeFeS (ours) 0.15m/0.53\u00b0\\n- Exposure-adaptive ACTA\\n- Feature Fusion 0.37m/1.62\u00b0\\n\\n(b) Training Scheduling Ablation\\nMethod Shop Facade\\nCombined 0.17m/0.80\u00b0\\nProgressive 0.15m/0.53\u00b0\\n\\nTable 6.\\n(a) Ablation on NeFeS architecture.\\n(b) Ablation on the proposed training scheduling.\\n\\nFigure 6. Plots of rotation and translation errors against the number of iteration on Cambridge: Shop Facade scene.\\n\\nA noteworthy insight from our architecture design is that the superior pose estimation accuracy is attributed to integrating both Exposure-adaptive ACT and the Feature Fusion module. The feature fusion module can smooth potential noises (outliers) from directly-rendered 3D features.\\n\\nIn Tab. 6b, we compare our progressive training scheduling with the combined scheduling, where all three loss terms have been enabled simultaneously since the beginning of the training. The results reveal that the progressive training scheduling results in better accuracy, providing further support for our design decisions.\\n\\n4.7. Number of Iterations vs. Accuracy Trade-off\\nIn Fig. 6, we plot the relationship between the number of optimization iterations and pose error. Both translation and rotation errors reduce significantly within the first 10-20 iterations. Hence, given a computational budget, an operating point can be (optionally) chosen w.r.t. the performance-time trade-off. The errors start to plateau around 50 steps. Although we can achieve even lower errors with more iterations, we think using 50 steps strikes a balance between accuracy and efficiency, and explains how we set our previous experiments.\\n\\n4.8. Spatial vs Channel-wise Normalization\\nAs described in Sec. 3.1, we empirically find spatial-wise normalized features yield higher accuracy than channel-wise normalized features when computing the feature cosine similarity loss $L_{\\\\text{feature}}$, evidenced by Tab. 7. This is likely due to our spatially sensitive dense direct matching, akin to methods like Direct Sparse Odometry [18]. In contrast, channel-wise normalization can introduce inconsistencies among neighboring pixels.\\n\\nTable 7. Performance comparison between using Channel-wise vs. Spatial-wise normalization in loss $L_{\\\\text{feature}}$ during refinement.\\n\\n5. Conclusion\\nWe tackle the camera relocalization problem and improve absolute pose regression (APR) methods by proposing a test-time refinement method. In particular, we design a novel model named Neural Feature Synthesizer (NFS), which can encode 3D geometric features. Given an estimated pose, the NFS renders a dense feature map, compares it with the query image features, and back-propagates the error to refine estimated camera poses from APR methods.\\n\\nIn addition, we propose a progressive learning strategy and a feature fusion module to improve the feature robustness of the NFS model. The experiments demonstrate that our method can greatly improve the accuracy of APR methods. Our method provides a promising direction for improving the accuracy of APR methods.\"}"}
{"id": "CVPR-2024-1675", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: CNN architecture for weakly supervised place recognition. In CVPR, 2016.\\n\\n[2] Yash Bhalgat, Iro Laina, Jo\u00e3o F Henriques, Andrew Zisserman, and Andrea Vedaldi. Contrastive lift: 3D object instance segmentation by slow-fast contrastive fusion. In NeurIPS, 2023.\\n\\n[3] Wenjing Bian, Zirui Wang, Kejie Li, Jiawang Bian, and Victor Adrian Prisacariu. NoPe-NeRF: Optimising neural radiance field with no pose prior. In CVPR, 2023.\\n\\n[4] Eric Brachmann and Carsten Rother. Learning Less is More - 6D Camera Localization via 3D Surface Regression. In CVPR, 2018.\\n\\n[5] Eric Brachmann and Carsten Rother. Visual camera relocalization from RGB and RGB-D images using DSAC. IEEE TPAMI, 2021.\\n\\n[6] Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. DSAC - Differentiable RANSAC for Camera Localization. In CVPR, 2017.\\n\\n[7] Eric Brachmann, Martin Humenberger, Carsten Rother, and Torsten Sattler. On the limits of pseudo ground truth in visual camera relocalisation. In ICCV, 2021.\\n\\n[8] S. Brahmbhatt, J. Gu, K. Kim, J. Hays, and J. Kautz. Geometry-Aware Learning of Maps for Camera Localization. In CVPR, 2018.\\n\\n[9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.\\n\\n[10] Shuai Chen, Zirui Wang, and Victor Prisacariu. DirectPoseNet: Absolute pose regression with photometric consistency. In 3DV, 2021.\\n\\n[11] Shuai Chen, Xinghui Li, Zirui Wang, and Victor Prisacariu. DFNet: Enhance absolute pose regression with direct feature matching. In ECCV, 2022.\\n\\n[12] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In CVPR, 2022.\\n\\n[13] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In CVPR, 2022.\\n\\n[14] Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and Simon Lucey. Garf: Gaussian activated radiance fields for high fidelity reconstruction and pose estimation. In ECCV, 2022.\\n\\n[15] Ronald Clark, Sen Wang, Andrew Markham, Niki Trigoni, and Hongkai Wen. Vidloc: A deep spatio-temporal model for 6-dof video-clip relocalization. In CVPR, 2017.\\n\\n[16] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. In CVPRW, 2018.\\n\\n[17] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-net: A trainable CNN for joint detection and description of local features. In CVPR, 2019.\\n\\n[18] J. Engel, V. Koltun, and D. Cremers. DSO: Direct sparse odometry. In IEEE TPAMI, 2017.\\n\\n[19] Ben Glocker, Shahram Izadi, Jamie Shotton, and Antonio Criminisi. Real-time RGB-D camera relocalization. In ISMAR, 2013.\\n\\n[20] M. Irani and P. Anandan. All about direct methods. In Workshop Vis. Algorithms: Theory Pract., 1999.\\n\\n[21] A. Kendall and R. Cipolla. Modelling uncertainty in deep learning for camera relocalization. In ICRA, 2016.\\n\\n[22] A. Kendall and R. Cipolla. Geometric loss functions for camera pose regression with deep learning. In CVPR, 2017.\\n\\n[23] A. Kendall, M. Grimes, and R. Cipolla. Posenet: A convolutional network for real-time 6-dof camera relocalization. In ICCV, 2015.\\n\\n[24] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing NeRF for editing via feature field distillation. In NeurIPS, 2022.\\n\\n[25] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. In ICLR, 2022.\\n\\n[26] Xinghui Li, Kai Han, Shuda Li, and Victor Prisacariu. Dual-resolution correspondence networks. In NeurIPS, 2020.\\n\\n[27] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In ICCV, 2021.\\n\\n[28] Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson, and Marc Pollefeys. Pixel-Perfect Structure-from-Motion with FeatureMetric Refinement. In ICCV, 2021.\\n\\n[29] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. In CVPR, 2021.\\n\\n[30] N. Max. Optical models for direct volume rendering. In IEEE Transactions on Visualization and Computer Graphics, 1995.\\n\\n[31] I. Melekhov, J. Ylioinas, J. Kannala, and E. Rahtu. Image-based localization using hourglass networks. In ICCVW, 2017.\\n\\n[32] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\\n\\n[33] Arthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bogdan Stanciulescu, and Arnaud de La Fortelle. LENS: Localization enhanced by NeRF synthesis. In CoRL, 2021.\\n\\n[34] Arthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bogdan Stanciulescu, and Arnaud de La Fortelle. Coordinet: uncertainty-aware pose regressor for reliable vehicle localization. In WACV, 2022.\\n\\n[35] T. Naseer and W. Burgard. Deep regression for monocular camera-based 6-dof global localization in outdoor environments. In IROS, 2017.\"}"}
{"id": "CVPR-2024-1675", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"R. Newcombe, A. Davison, S. Izadi, P. Kohli, O. Hilliges, J. Shotton, D. Molyneaux, S. Hodges, D. Kim, and A. Fitzgibbon. Kinectfusion: Real-time dense surface mapping and tracking. In ISMAR, 2011.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 32, 2019.\\n\\nPulak Purkait, Cheng Zhao, and Christopher Zach. Synthetic view generation for absolute pose regression and image synthesis. In BMVC, 2018.\\n\\nN. Radwan, A. Valada, and W. Burgard. Vlocnet++: Deep multitask learning for semantic visual localization and odometry. In IEEE Robotics and Automation Letters, 2018.\\n\\nKonstantinos Rematas, Andrew Liu, Pratul P Srinivasan, Jonathan T Barron, Andrea Tagliasacchi, Thomas Funkhouser, and Vittorio Ferrari. Urban radiance fields. In CVPR, 2022.\\n\\nPaul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In CVPR, 2019.\\n\\nPaul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In CVPR, 2020.\\n\\nPaul-Edouard Sarlin, Ajaykumar Unagar, M\u00e5ns Larsson, Hugo Germain, Carl Toft, Viktor Larsson, Marc Pollefeys, Vincent Lepetit, Lars Hammarstrand, Fredrik Kahl, and Torsten Sattler. Back to the Feature: Learning robust camera localization from pixels to pose. In CVPR, 2021.\\n\\nT. Sattler, B. Leibe, and L. Kobbelt. Improving image-based localization by active correspondence search. In ECCV, 2012.\\n\\nT. Sattler, B. Leibe, and L. Kobbelt. Efficient & Effective Prioritized Matching for Large-Scale Image-Based Localization. In IEEE TPAMI, 2017.\\n\\nT. Sattler, Q. Zhou, M. Pollefeys, and L. Leal-Taixe. Understanding the limitations of cnn-based absolute camera pose regression. In CVPR, 2019.\\n\\nJohannes Lutz Sch\u00f6nherr and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016.\\n\\nNur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, and Arthur Szlam. Clip-fields: Weakly supervised semantic fields for robotic memory. In Workshop on Language and Robotics at CoRL, 2022.\\n\\nYoli Shavit and Yosi Keller. Camera pose auto-encoders for improving pose regression. In ECCV, 2022.\\n\\nYoli Shavit, Ron Ferens, and Yosi Keller. Paying attention to activation maps in camera pose regression. In arXiv preprint arXiv:2103.11477, 2021.\\n\\nYoli Shavit, Ron Ferens, and Yosi Keller. Learning multi-scene absolute pose regression with transformers. In ICCV, 2021.\\n\\nJamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression forests for camera relocalization in rgb-d images. In CVPR, 2013.\\n\\nEdgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew Davison. iMAP: Implicit mapping and positioning in real-time. In ICCV, 2021.\\n\\nJiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021.\\n\\nHajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Akihiko Torii. InLoc: Indoor Visual Localization with Dense Matching and View Synthesis. CVPR, 2018.\\n\\nZachary Teed and Jia Deng. Tangent space backpropagation for 3d transformation groups. In CVPR, 2021.\\n\\nVadim Tschernezki, Iro Laina, Diane Larlus, and Andrea Vedaldi. Neural Feature Fusion Fields: 3D distillation of self-supervised 2D image representations. In 3DV, 2022.\\n\\nA. Valada, N. Radwan, and W. Burgard. Deep auxiliary learning for visual localization and odometry. In ICRA, 2018.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.\\n\\nF. Walch, C. Hazirbas, L. Leal-Taixe, T. Sattler, S. Hilsenbeck, and D. Cremers. Image-based localization using lstms for structured feature correlation. In ICCV, 2017.\\n\\nBing Wang, Changhao Chen, Chris Xiaoxuan Lu, Peijun Zhao, Niki Trigoni, and Andrew Markham. Atloc: Attention guided camera localization. In AAAI, 2020.\\n\\nZirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. NeRF\u2212\u2212: Neural radiance fields without known camera parameters. arXiv preprint arXiv:2102.07064, 2021.\\n\\nJ. Wu, L. Ma, and X. Hu. Delving Deeper into Convolutional Neural Networks for Camera Relocalization. In ICRA, 2017.\\n\\nLin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Inverting neural radiance fields for pose estimation. In arxiv arXiv:2012.05877, 2020.\\n\\nZihan Zhu, Songyou Peng, Viktor Larsson, Weidi Xie, Hujun Bao, Zhaopeng Cui, Martin R. Oswald, and Marc Pollefeys. Nice-slam: Neural implicit scalable encoding for slam. In CVPR, 2022.\"}"}
