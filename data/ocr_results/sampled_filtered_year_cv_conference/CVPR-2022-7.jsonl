{"id": "CVPR-2022-7", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3D Scene Painting via Semantic Image Synthesis\\n\\nJaebong Jeong\u2217\\nJanghun Jo\\nSunghyun Cho\\nJaesik Park\\n\\nPOSTECH\\nGSAI & CSE\\n\\n{jbjeong, jhjo432, s.cho, jaesik.park}@postech.ac.kr\\n\\nAbstract\\n\\nWe propose a novel approach to 3D scene painting using a configurable 3D scene layout. Our approach takes a 3D scene with semantic class labels as input and trains a 3D scene painting network that synthesizes color values for the input 3D scene. We exploit an off-the-shelf 2D semantic image synthesis method to teach the 3D painting network without explicit color supervision. Experiments show that our approach produces images with geometrically correct structures and supports scene manipulation, such as the change of viewpoint, object poses, and painting style. Our approach provides rich controllability to synthesized images in the aspect of 3D geometry.\\n\\n1. Introduction\\n\\nCreating realistic 3D scenes becomes crucial due to the increasing demands of unobserved content for virtual realism. However, it is regarded as a challenging problem because many components depend on human labor or manual capture of real scenes. In particular, painting a scene is hard for a human, and it takes extra effort if we want to change the style after the creation. There are some attempts to focus on automatic 3D scene painting to resolve the issues. For instance, given a 3D scene and reference image, they find a texture from a texture database, which has a similar color distribution to the reference image. However, making a large-scale texture set is a burden due to the variety of scene geometry.\\n\\nWe propose an automatic painting approach for 3D scene creation in this work. For 3D scene painting, our approach learns a 3D scene painting network that takes a 2D semantic map and 3D coordinate map as input and produces a 2D image with realistic RGB colors. Training a 3D scene painting network, on the other hand, requires numerous colored 3D scenes with semantic labels for supervision, which are hard to acquire. To overcome this, we propose to utilize techniques already developed for 2D image synthesis. Specifically, given a 3D scene with object-wise semantic labels, we render 2D maps of 3D coordinates and semantic labels. The rendered maps are then fed to a realistic image synthesis module to synthesize 2D RGB images, which are used as pseudo-ground-truth labels for training our 3D scene painting network to produce realistic colors. As our network is conditioned on 3D coordinates, it produces consistent colors under the change of viewpoint.\\n\\n\u2217Part of this work was done while the first author was a research intern at Microsoft Research Asia.\"}"}
{"id": "CVPR-2022-7", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The proposed method has a few merits compared to previous works on 3D scene painting or texture mapping. First, our approach can generate a quality texture of a scene with the aid of a generative adversarial loss. Second, our approach can change the style of a scene by simply manipulating a style vector. Third, our approach allows a user detailed control of scene layouts, and change of the viewpoint or the positions of objects for image rendering.\\n\\nThe proposed scene painting network can be regarded as a geometrically conditioned image synthesis from an image synthesis perspective. Concurrent image synthesis approaches utilize a rough guide, such as image class, semantic labels, attributes, poses, voxelized scenes, or viewing-directions. On the other hand, our approach opens a new research direction for cases when 3D scenes are provided.\\n\\nWe apply our approach to various indoor scenes. Our qualitative and quantitative experimental results verify that our method is highly controllable and produces high-quality colored scenes and images.\\n\\nOur contributions can be summarized as follows:\\n\\n\u2022 We propose a novel approach for automatic 3D scene painting, which is based on a novel 3D scene painting network that produces realistic RGB colors from 3D coordinates and semantic class labels.\\n\\n\u2022 Our approach can learn 3D scene painting without ground-truth colored 3D scenes by combining off-the-shelf 2D semantic image synthesis and 2D renderings of 3D semantic labels and coordinates.\\n\\n\u2022 Our approach allows detailed control over scene layouts and change of the viewpoint and object poses.\\n\\n2. Related Work\\n\\nSemantic image synthesis.\\n\\nSince the emergence of generative adversarial networks (GANs) and conditional GANs, a number of approaches have been proposed that utilize 2D semantic label maps to control the image synthesis process. Wang et al. proposed a coarse-to-fine generator and a multi-scale discriminator to achieve high-resolution image synthesis. Park et al. proposed spatially-adaptive normalization (SP ADE) layers. Sch\u00f6nfeld et al. utilize a semantic segmentation network as a discriminator. Zhu et al. proposed a group convolution-based network. Ntavelis et al. proposed a method for image editing using semantic labels. While these approaches show astonishing results, these approaches are limited to the synthesis of 2D images that are neither multi-view consistent nor conditioned by 3D geometries. On the other hand, we tackle the problem of realistic 3D scene coloring, where we aim to produce multi-view consistent images with realistic colors for a given 3D scene.\\n\\nMaterial suggestion.\\n\\nMaterial suggestion methods aim to automatically assign texture maps to input 3D meshes by searching an external database. They require a large-scale database of textured 3D models or images with 3D material annotations, but both of which are expensive to acquire. We aim to automatically generate a realistic 3D scene coloring with a deep generative model.\\n\\nImage synthesis for 3D scenes.\\n\\nThere have been various attempts to incorporate 3D information for image generation, such as novel view synthesis, texture synthesis, and 3D-aware generative models. Novel view synthesis approaches aim at generating images of novel viewpoints from a single input image or multiple images. However, it is hard to synthesize an image that is largely deviated from the original viewing directions or to allow the manipulation of objects, such as the adjustment of objects' poses. Most previous texture synthesis approaches for 3D scenes aim at generating textures of a single 3D object. Grigorev et al. and Huang et al. proposed image synthesis methods conditioned on the arbitrary viewpoint for an object. Henderson et al. proposed a method for synthesizing a textured 3D mesh. Martin-Brualla et al. presented a compact representation for reconstructing thin 3D structures by combining a coarse shape collection with their learned textures. Oechsle et al. and Schwarz et al. proposed a texture field and a radiance field, respectively, both of which is a mapping function from a 3D point to a color value. Unfortunately, these methods focus on a single 3D object, and it is not trivial to extend them to handle 3D scenes having multiple objects of different classes. Recently, Liao et al. introduced a generative model for textures of multiple 3D objects. However, their approach is limited to a small number of simple objects due to the complexity of the approach. Liu et al. proposed a pipeline that uses camera trajectories and generates an outdoor natural image sequence of an infinite length. However, as their approach relies on depth prediction, handling indoor scenes with heavy occlusions and various thin structures gets hard.\\n\\nRecently, GANcraft introduces an approach that generates geometrically consistent and realistic outdoor images. Similarly to ours, GANcraft trains a 3D-aware GAN using pseudo-ground-truth data and synthesizes images conditioned by 3D geometries. However, GANcraft uses the voxel representation, which is limited in representing fine geometric details. Moreover, to synthesize each image, it relies on frame-by-frame image synthesis procedure that needs to render a voxel-based feature map to synthesize an image for each viewpoint. In contrast, our approach is designed for indoor scenes with fine geometric details, and assigns a color value for each 3D coordinate in a scene, so it does not require frame-by-frame synthesis. In addition, since our approach assigns a color value for each 3D coordinate, it can be directly used for scene manipulation.\"}"}
{"id": "CVPR-2022-7", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit representation. Another relevant work to ours is implicit representation-based approaches. These approaches use implicit representations or continuous representations, a class of learnable functions that map a coordinate to a particular type of signal, e.g., color and voxel occupancy. Occupancy Networks \\\\(29\\\\) and DeepSDF \\\\(37\\\\) reconstruct a 3D model by introducing an implicit function that takes a 3D coordinate as input and predicts the 3D occupancy of that position. Oechsle et al. \\\\(35\\\\) learn a function that maps a 3D coordinate to color. NeRF \\\\(30\\\\) takes a 3D coordinate and a viewing direction as input and predicts a novel-view image. Schwarz et al. \\\\(42\\\\) designed a generative model based on NeRF. Anokhin et al. \\\\(1\\\\) propose an image generator that independently calculates the color value at each pixel given a random vector and a 2D coordinate of that pixel. Sitzmann et al. \\\\(44\\\\) showed that periodic activation functions could improve the representational performance. Oechsle et al. \\\\(35\\\\) learn a function that maps a 3D coordinate to a color value. Peng et al. \\\\(39\\\\) reconstructs a whole scene using a convolutional neural network that predicts occupancy. Encoding the coordinates is known to yield successful results for the implicit mapping from a coordinate to the desired output. NeRF \\\\(30\\\\) found that the sinusoidal positional encoding improves the representation power. Other approaches \\\\(1, 46\\\\) show that mapping Fourier features \\\\(40\\\\) enables to learn high-frequency functions. These advances greatly inspire our approach.\\n\\nTo effectively learn consistent color information of an input 3D scene from independently generated pseudo images, our method is designed with an implicit function that maps a 3D coordinate to an RGB color. To enhance the image quality, we adopt positional encoding. Nevertheless, our method allows scene-level image synthesis and object manipulation for complex 3D scenes in contrast to prior work.\\n\\n3. Method\\n\\nThis section introduces a problem definition for 3D scene painting and our pipeline that benefits from conditional image synthesis. Figure 2 shows an overview of our 3D scene painting framework.\\n\\n3.1. Problem Definition\\n\\nThe objective of our approach is to colorize a 3D scene. Specifically, a 3D scene \\\\(S\\\\) consists of 3D meshes of objects \\\\(M_i\\\\) with their semantic class labels \\\\(l_i\\\\), i.e., \\\\(S = \\\\{(M_i, l_i)\\\\}_{1 \\\\leq i \\\\leq n(S)}\\\\), where \\\\(n(S)\\\\) is the number of mesh models in the scene. For a given 3D scene \\\\(S\\\\), our goal is to generate a realistic RGB color \\\\(c_j \\\\in \\\\mathbb{R}^3\\\\) for each point \\\\(p_j\\\\) on the surfaces of the meshes in \\\\(S\\\\). In addition, we incorporate a style vector \\\\(z\\\\) to give ability to manipulate the color distribution of the 3D scene.\\n\\n3.2. Data Preparation\\n\\nLearning to paint 3D scenes requires color supervision, while it is not easy to access an extensive collection of 3D scenes with realistic colors. Instead, to train our scene painting network without direct supervision, we synthesize pseudo-ground-truth labels using an off-the-shelf conditional image synthesis method based on semantic segmentation maps. In the following, we describe our training data generation process.\\n\\nLabel map rendering. Given a 3D scene \\\\(S\\\\), we sample a set of multiple viewpoints. Then, from each viewpoint, we render a depth map \\\\(D \\\\in \\\\mathbb{R}^{H \\\\times W}\\\\) and a label map \\\\(L \\\\in \\\\mathbb{R}^{H \\\\times W}\\\\), where \\\\(H\\\\) and \\\\(W\\\\) are the height and width of the map, respectively. We transform \\\\(L\\\\) to \\\\(L' \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C}\\\\) by converting the class label at each pixel to a one-hot vector, where \\\\(C\\\\) is the number of class labels. By using the camera parameters of the current viewpoint, we convert the depth map to a coordinate map \\\\(P \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 3}\\\\) whose pixel values indicate the world coordinates in the 3D scene. In this manner, we render the input 3D scene from multiple viewpoints and acquire various pairs of a coordinate map and a label map that is denoted as \\\\(\\\\{(P_v, L'_v)\\\\}_{1 \\\\leq v \\\\leq V}\\\\), where \\\\(V\\\\) is the number of viewpoints.\\n\\nPseudo ground-truth generation. We then generate pseudo-ground-truth images \\\\(\\\\{I'_v\\\\}\\\\) from generated label maps \\\\(\\\\{L'_v\\\\}\\\\) using an off-the-shelf image synthesis network as a pseudo image generator \\\\(G'\\\\) where \\\\(I'_v \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 3}\\\\). For \\\\(G'\\\\), we use OASIS \\\\(41\\\\) trained on the ADE20K dataset \\\\(52\\\\), a state-of-the-art image synthesis network that produces a realistic 2D image from a semantic label map and a style vector \\\\(z\\\\). Exploiting \\\\(G'\\\\), we generate pseudo ground-truth images \\\\(\\\\{I'_v\\\\}\\\\) as \\\\(I'_v = G'(L', z)\\\\). By sampling random \\\\(z\\\\) from the normal distribution, we gather images of various styles for each scene. While the pseudo-ground-truth images \\\\(I'_v\\\\) are realistic-looking thanks to the advances in the 2D image synthesis, they are not multi-view consistent because they are generated independently. Nonetheless, our scene painting network can still learn multi-view consistent image generation thanks to its network architecture and learning strategy, which will be explained in the following.\\n\\n3.3. Scene Painting Network\\n\\nOur 3D scene painting network \\\\(G\\\\) generates colors for the points in a 3D scene for a given 3D coordinate map, a label map, and a style vector. Specifically, \\\\(G\\\\) performs 3D scene coloring as \\\\(I = G(X, W(z))\\\\) where \\\\(I \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 3}\\\\) is a resulting 2D RGB image, \\\\(X\\\\) is a concatenation of a positional encoding map \\\\(\\\\gamma(P)\\\\) and \\\\(L'\\\\), and \\\\(W\\\\) is a learnable style mapping network inspired by StyleGANv2 \\\\(22\\\\). \\\\(F\\\\) is the sum of the number of channels in \\\\(\\\\gamma(P)\\\\) and \\\\(L'\\\\). We utilize sinusoidal function-based positional encoding \\\\(\\\\gamma(\\\\cdot)\\\\) \\\\(30\\\\). For each 3D world coordinate \\\\(x = [x, y, z]\\\\) in \\\\(P\\\\), we feed it through \\\\(\\\\gamma(\\\\cdot)\\\\) for 2264 times.\"}"}
{"id": "CVPR-2022-7", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overview of the proposed method. Given a 3D scene, we render a coordinate map $P$ and a semantic label map $L'$ from arbitrary viewpoints. Our scene painting network $G$ is trained with pseudo images generated by $G'$. $G$ generates color from the feature map $X = \\\\gamma(P); L')$, which is a concatenation of $\\\\gamma(P)$ and $L'$ in the channel dimension. $\\\\gamma(\\\\cdot)$ is an element-wise positional encoding function. $G$ and $G'$ share the style vector $z$, and $z$ passes the style mapping network $W$. The generated image $I$ is compared with the pseudo image $I'$ by $\\\\mathcal{L}_{rec}$. The segmentation-based discriminator classifies the authenticity of the generated images $I$ and pseudo images $I'$. Note that $G$ generates color for each 3D point and $\\\\mathcal{L}_{rec}$ is defined for each 3D point. After training, the generated colors are mapped to the 3D scene, and we get the colored scene.\\n\\nwhich is normalized into $[-1, 1]$, $\\\\gamma(\\\\cdot)$ produces a $(6T+3)$-dimensional encoding vector where $T$ is a hyperparameter.\\n\\nThe scene painting network $G$ consists of nine fully-connected layers that independently assign a color to each coordinate. The layers use style adaptive weight modulation adopted from StyleGANv2. The detailed architecture is provided in the supplement. We train $G$ with our pseudo-ground-truth images by optimizing a reconstruction loss $\\\\mathcal{L}_{rec}$ and an adversarial loss $\\\\mathcal{L}_{adv}$. We define the reconstruction loss $\\\\mathcal{L}_{rec}$ as a combination of an $L1$, $L2$, and VGG perceptual loss, i.e.,:\\n\\n$$\\\\mathcal{L}_{rec} = \\\\frac{1}{KB\\\\sum_b} \\\\sum_{b=1}^B H \\\\times W \\\\sum_{i=1}^H (A_{bi} \\\\hat{I}_{bi} - I'_{bi})^2 + \\\\lambda L_2 A_{bi} \\\\hat{I}_{bi} - I'_{bi},$$\\n\\nwhere $K = BHW$ is a normalization factor, $B$ is the mini-batch size, and $H$ and $W$ are the height and width of a generated image, respectively . $I_{bi}$ and $I'_{bi}$ are the $b$-th generated image and pseudo image in the current mini-batch, and $I_{bi}$ indicates the $i$-th pixel of $I_{bi}$. $A_{bi}$ is the $b$-th adaptive weight map, which will be explained later. $\\\\mathcal{L}_{VGG}$ is a perceptual loss between $I$ and $I'$. $\\\\lambda L_2$ is a scalar weight and we use $\\\\lambda L_2 = 10$.\\n\\nPseudo images from different viewpoints may have different textures (Figure 3(b)) and artifacts (Figure 7(a)), which may eventually lead to artifacts in the final 3D scene coloring results. The reconstruction loss adopts the adaptive weight map to discard such inconsistent parts of pseudo images during training. The adaptive weight map $A \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 3}$ is defined as:\\n\\n$$A_{i} = \\\\exp(-||I'_{i} - \\\\mu(I'_{i}, L_{i})||_2^2/\\\\sigma^2), \\\\mu(I'_{i}, L_{i}) = \\\\sum_{j=1}^{H \\\\times W} I'_{j}[L_{j} = L_{i}] \\\\sum_{j=1}^{H \\\\times W} 1[L_{j} = L_{i}],$$\\n\\nwhere $\\\\mu(I'_{i}, L_{i}) \\\\in \\\\mathbb{R}^3$ is a label-wise mean vector of pseudo image pixel values whose semantic class is $L_{i}$, and $\\\\sigma = 1$ is a scalar that controls smoothness of the Gaussian function. Minimizing the reconstruction loss $\\\\mathcal{L}_{rec}$ tends to mix clashing colors from pseudo images of different viewpoints, and it makes $G$ to produce blurry images as a trivial solution. To alleviate this, we adopt adversarial learning to guide $G$ to synthesize more realistic-looking images with detailed textures. We adopt the semantic segmentation-based adversarial learning, where a discriminator learns to classify each pixel of an image into $(C+1)$ semantic classes. The additional class accounts for one fake class. Our framework uses a label map $L'$ as ground truth for the discriminator.\\n\\nThe adversarial loss $\\\\mathcal{L}_{adv}$ is defined as:\\n\\n$$\\\\mathcal{L}_{adv} = -\\\\frac{1}{KB\\\\sum_c} \\\\sum_{b=1}^B \\\\sum_{c=1}^{C} \\\\alpha_c H \\\\times W \\\\sum_{i=1}^H L'_{bi};c \\\\log D(I_{bi});c,$$\\n\\nwhere $\\\\alpha_c$ is a weight for each class to resolve the class imbalance problem, $D$ is a semantic segmentation-based discriminator, and $L'_{bi};c$ is the $c$-th element of the class label represented as a one-hot vector at the $i$-th pixel of the image.\"}"}
{"id": "CVPR-2022-7", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Ivan Anokhin, Kirill Demochkin, T aras Khakhulin, Gleb Sterkin, V ictor Lempitsky , and Denis Korzhenkov . Image generators with conditionally-independent pixel synthesis. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 14278\u201314287, 2021.\\n\\n[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.\\n\\n[3] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A. Efros. Everybody dance now . Proceedings of the International Conference on Computer Vision (ICCV), pages 5932\u20135941, 2019.\\n\\n[4] Angel X. Chang, Angela Dai, Thomas A. Funkhouser, Ma-tieh Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from RGB-D data in indoor environments. In International Conference on 3D Vision (3DV), pages 667\u2013676. IEEE Computer Society, 2017.\\n\\n[5] Angel X. Chang, T . Funkhouser, L. Guibas, P . Hanra-ghan, Qixing Huang, Zimo Li, S. Savarese, M. Savva, Shuran Song, H. Su, J. Xiao, L. Yi, and F . Y u. Shapenet: An information-rich 3d model repository . arXiv, abs/1512.03012, 2015.\\n\\n[6] Kang Chen, Kun Xu, Yizhou Y u, Tian-Yi W ang, and Shi-min Hu. Magic decorator: automatic material suggestion for indoor digital scenes. ACM Trans. Graph., 34(6):232:1\u2013232:11, 2015.\\n\\n[7] Sungjoon Choi, Qian-Yi Zhou, Stephen D. Miller, and Vladlen Koltun. A large dataset of object scans. arXiv, abs/1602.02481, 2016.\\n\\n[8] Blender Online Community . Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.\\n\\n[9] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-ber, Thomas A. Funkhouser, and Matthias Nie\u00dfner. Scan-net: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 2432\u20132443. IEEE Computer Society, 2017.\\n\\n[10] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Bin-qiang Zhao, Stephen J. Maybank, and Dacheng T ao. 3d-future: 3d furniture shape with texture. Int. J. Comput. V is., 129(12):3313\u20133337, 2021.\\n\\n[11] Ian J. Goodfellow , Jean Pouget-Abadie, M. Mirza, Bing Xu, David W arde-Farley , Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NeurIPS), 2014.\\n\\n[12] Artur Grigorev , Artem Sevastopolsky , Alexander V akhitov , and V ictor Lempitsky . Coordinate-based texture inpainting for pose-guided human image generation. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\n[13] A. Handa, V. P \u02d8atr\u02d8aucean, S. Stent, and R. Cipolla. Scenenet: An annotated model generator for indoor scene understand-ing. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pages 5737\u20135743, 2016.\\n\\n[14] Zekun Hao, Arun Mallya, Serge Belongie, and Ming-Y u Liu. Gancraft: Unsupervised 3d neural rendering of minecraft worlds. In Proceedings of the International Conference on Computer Vision (ICCV), pages 14072\u201314082, October 2021.\\n\\n[15] P . Henderson, V agia Tsiminaki, and Christoph H. Lampert. Leveraging 2d data to learn textured 3d mesh generation. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 7495\u20137504, 2020.\\n\\n[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilib-rium. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, 2017.\\n\\n[17] J. Huang, J. Thies, A. Dai, A. Kundu, C. Jiang, L. J. Guibas, M. Nie\u00dfner, and T . Funkhouser. Adversarial texture opti-mization from rgb-d scans. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 1556\u20131565, 2020.\\n\\n[18] Phillip Isola, Jun-Y an Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adver-sarial networks. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 5967\u20135976, 2017.\\n\\n[19] Arjun Jain, Thorsten Thorm \u00a8ahlen, T obias Ritschel, and Hans-Peter Seidel. Material memex: automatic material sug-gestions for 3d objects. ACM Trans. Graph., 31(6):143:1\u2013143:8, 2012.\\n\\n[20] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Proceedings of the European Conference on Computer V ision (ECCV), 2016.\\n\\n[21] Minguk Kang and Jaesik Park. Contragan: Contrastive learn-ing for conditional image generation. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 21357\u201321369, 2020.\\n\\n[22] T ero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 8107\u20138116, 2020.\\n\\n[23] Diederik P . Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv, abs/1412.6980, 2015.\\n\\n[24] Marc Levoy , Kari Pulli, Brian Curless, Szymon M. Rusinkiewicz, David Koller, Lucas Pereira, Matt Ginzton, Sean E. Anderson, James Davis, Jeremy Ginsberg, Jonathan Shade, and Duane Fulk. The digital michelangelo project: 3d scanning of large statues. Proceedings of the 27th annual conference on Computer graphics and interactive tech-niques, 2000.\\n\\n[25] Y . Liao, K. Schwarz, L. Mescheder, and A. Geiger. Towards unsupervised learning of generative models for 3d control-lable image synthesis. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 5870\u20135879, 2020.\"}"}
{"id": "CVPR-2022-7", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Andrew Liu, Ameesh Makadia, Richard Tucker, Noah Snavely, Varun Jampani, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from a single image. In Proceedings of the International Conference on Computer Vision (ICCV), pages 14438\u201314447. IEEE, 2021.\\n\\nAndrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models. In ICML Workshop on Deep Learning for Audio, Speech and Language Processing, 2013.\\n\\nRicardo Martin-Brualla, R. Pandey, Sofien Bouaziz, M. Brown, and D. Goldman. Gelato: Generative latent textured objects. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\\n\\nLars M. Mescheder, Michael Oechsle, M. Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 4455\u20134465, 2019.\\n\\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, J. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\\n\\nTakeru Miyato and Masanori Koyama. cGANs with projection discriminator. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.\\n\\nMichael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 11448\u201311459, 2021.\\n\\nEvangelos Ntavelis, Andres Romero, Iason Kastanis, Luc Van Gool, and Radu Timofte. Sesame: Semantic editing of scenes by adding, manipulating or erasing objects. In Proceedings of the European Conference on Computer Vision (ECCV), pages 394\u2013411, Cham, 2020.\\n\\nAugustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier GANs. In Proceedings of the International Conference on Learning Representations (ICLR), volume 70 of Proceedings of Machine Learning Research, pages 2642\u20132651, 06\u201311 Aug 2017.\\n\\nMichael Oechsle, Lars M. Mescheder, M. Niemeyer, Thilo Strauss, and Andreas Geiger. Texture fields: Learning texture representations in function space. Proceedings of the International Conference on Computer Vision (ICCV), pages 4530\u20134539, 2019.\\n\\nE. Park, J. Yang, E. Yumer, D. Ceylan, and A. C. Berg. Transformation-grounded image generation network for novel 3d view synthesis. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 702\u2013711, 2017.\\n\\nJeong Joon Park, P. Florence, J. Straub, Richard A. Newcombe, and S. Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 165\u2013174, 2019.\\n\\nT. Park, Ming-Yu Liu, T. Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 2332\u20132341, 2019.\\n\\nSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Proceedings of the European Conference on Computer Vision (ECCV), pages 523\u2013540, 2020.\\n\\nA. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems (NeurIPS), 2007.\\n\\nEdgar Sch\u00f6pf, Vadim Sushko, Dan Zhang, Juergen Gall, Bernt Schiele, and Anna Khoreva. You only need adversarial supervision for semantic image synthesis. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.\\n\\nKatja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 20154\u201320166, 2020.\\n\\nWei Shen and Rujie Liu. Learning residual images for face attribute manipulation. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 1225\u20131233, 2017.\\n\\nVincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 7462\u20137473, 2020.\\n\\nJulian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June You, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard A. Newcombe. The replica dataset: A digital replica of indoor spaces. arXiv, abs/1906.05797, 2019.\\n\\nMatthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nT. Wang, Ming-Yu Liu, Jun-Yan Zhu, A. Tao, J. Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 8798\u20138807, 2018.\"}"}
{"id": "CVPR-2022-7", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European Conference on Computer Vision (ECCV), pages 432\u2013448, 2018.\\n\\nX. Xu, Y. Chen, and J. Jia. View independent generative adversarial network for novel view synthesis. In Proceedings of the International Conference on Computer Vision (ICCV), pages 7790\u20137799, 2019.\\n\\nDingdong Yang, Seunghoon Hong, Yunseok Jang, Tiangchen Zhao, and Honglak Lee. Diversity-sensitive conditional generative adversarial networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.\\n\\nHan Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. In Proceedings of the International Conference on Machine Learning (ICML), volume 97 of Proceedings of Machine Learning Research, pages 7354\u20137363, 09\u201315 Jun 2019.\\n\\nB. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 5122\u20135130, 2017.\\n\\nTinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A. Efros. View synthesis by appearance flow. In Proceedings of the European Conference on Computer Vision (ECCV), pages 286\u2013301, 2016.\\n\\nJ. Zhu, Y. Guo, and H. Ma. A data-driven approach for furniture and indoor scene colorization. IEEE Transactions on Visualization and Computer Graphics, 24:2473\u20132486, 2018.\\n\\nZ. Zhu, Z. Xu, A. You, and X. Bai. Semantically multi-modal image synthesis. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 5466\u20135475, 2020.\"}"}
{"id": "CVPR-2022-7", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"b-th label map, which is either 0 or 1. Similarly, \\\\( D(i) \\\\) is the \\\\( c \\\\)-th element at the \\\\( i \\\\)-th pixel of the discriminator output \\\\( D(I) \\\\). The discriminator \\\\( D \\\\) is trained using a loss defined as:\\n\\n\\\\[\\nL_D = \\\\lambda_{adv} \\\\left( \\\\sum_{b=1}^{K_B} \\\\sum_{c=1}^{C} \\\\sum_{i=1}^{H \\\\times W} L'_{bi;c} \\\\log D(I'_{b;i;c} - \\\\sum_{b=1}^{K_B} \\\\sum_{i=1}^{H \\\\times W} \\\\log D(I_{b;i;C+1})) \\\\right). \\\\tag{4}\\n\\\\]\\n\\nThe total loss for training \\\\( G \\\\) is then defined as:\\n\\n\\\\[\\nL_G = L_{rec} + \\\\lambda_{adv} L_{adv},\\n\\\\]\\n\\nwhere \\\\( \\\\lambda_{adv} \\\\) is a scalar weight.\\n\\n### 3.4. Texture Mapping\\n\\nAfter training, we create a texture map of each mesh model \\\\( M_i \\\\) in the scene \\\\( S \\\\) using coordinate maps \\\\( P_v \\\\) and the corresponding generated images \\\\( I_v \\\\). Specifically, we retrieve 3D points appearing at each coordinate map and collect color values generated by the scene painting network. Then, using the texture coordinates corresponding to the 3D points, we reconstruct texture maps of meshes. This step gives us texture-mapped meshes of the scene, so we can quickly render arbitrary viewpoints of the colored scenes, and the rearrangement of objects is straightforward.\\n\\n### 4. Experiments\\n\\n#### 4.1. Implementation details\\n\\n**Network and training.** We use the MLP architecture mentioned in Sec. 3.3 with the positional encoding parameter \\\\( T = 4 \\\\). We use leaky-ReLU \\\\([27]\\\\) with a negative slope of 0.2 for the activation functions in our network. For training, we used the Adam \\\\([23]\\\\) optimizer with \\\\( \\\\beta_1 = 0.9, \\\\beta_2 = 0.999 \\\\), and the learning rate of \\\\( 1 \\\\times 10^{-4} \\\\). We set the batch size as 8. For the adversarial loss, we set \\\\( \\\\lambda_{adv} = 0.1 \\\\). We train our scene painting network for 40,000 iterations.\\n\\n**Data preparation.** For our experiments, we utilize 3D models and their arrangements from the SceneNet \\\\([13]\\\\] dataset, as it provides complete object meshes, which enable scene editing, unlike other 3D datasets such as Replica \\\\([45]\\\\), MatterPort3D \\\\([4]\\\\), and ScanNet \\\\([9]\\\\). For qualitative experiments, we use four scenes in the SceneNet dataset: bedroom, kitchen, living room, and office, each of which provides 1.1k, 0.8k, 1k, and 1.8k training frames, respectively, and 49, 48, 64, and 64 test frames, respectively. We separately train the scene painting network for each scene. As SceneNet has no semantic labels on the objects, we assign each 3D model a class label using the 150 labels in the ADE20k \\\\([52]\\\\] dataset. We use Blender \\\\([8]\\\\) to render label maps and depth maps from multiple viewpoints.\\n\\n#### 4.2. Qualitative results\\n\\n**Image quality and view consistency.** In Figure 3, we visually compare the quality and view consistency of our results with images generated by OASIS \\\\([41]\\\\). As shown in Figure 3(b), OASIS produces unnatural textures, especially in the scene with complex geometry and large homogeneous regions. In contrast, our method produces geometrically valid and consistent images (Figure 3(c)).\\n\\n**Scene style control.** Our approach is readily able to control the style of a scene by changing the style vector \\\\( z \\\\). We can change the style during the test time, so it does not involve re-training the network. Figure 4 shows example scenes generated with different style vectors.\\n\\n**3D scene editing.** Our approach directly assigns texture colors to each mesh in a 3D scene. Thus, once texture colors are assigned, a 3D scene can be easily edited using ordinary 3D operations in contrast to other 3D image synthesis techniques \\\\([14, 26]\\\\). Figure 5 shows such an example of 3D scene editing where we first synthesized texture maps for a 3D scene using our scene painting network, edited the 3D scene using Blender \\\\([8]\\\\), and rendered new images with the synthesized textures.\\n\\n#### 4.3. Evaluation metrics\\n\\nWe evaluate our method using the following metrics.\\n\\n**Frechet Inception Distance (FID).** We measure FID \\\\([16]\\\\) between the generated images and real images from the ADE20k dataset \\\\([52]\\\\). A smaller value indicates that the distribution of generated images is closer to that of real images.\\n\\n**Mean Intersection-over-Union (mIoU).** Our method generates a 2D image from a rendered 2D semantic label map. To measure how semantically faithful a generated image is for a given input semantic label map, we measure the mIoU between the input semantic label map and the semantic segmentation result of a generated image predicted by a pre-trained semantic segmentation network \\\\([48]\\\\). A higher mIoU score indicates that the generated image is more faithful in terms of semantic labels.\\n\\n**View consistency (VC).** We also measure the consistency of images generated by different viewpoints. To measure the view consistency among images generated by different viewpoints, we collect color values of 3D points from the images. Then, we compare the color values of 3D points.\"}"}
{"id": "CVPR-2022-7", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. We compared images generated by our method and the images produced by OASIS on the three scenes. For each scene, two images from two different views were compared. (a) Semantic label map, (b) Pseudo images produced by OASIS, and (c) Our results.\\n\\nFigure 4. Examples of style manipulation using our scene painting network.\\n\\nIn a local neighborhood in the 3D space. Specifically, we define a local neighborhood $N_p$ as a cell in the 3D space whose center is the 3D point $p$, and define $C(N_p)$ as a set of pixel colors in $p$. Then, we define the VC metric as:\\n\\n$$\\\\sum_{N_p \\\\in N} \\\\max_{y \\\\in C(N_p)} \\\\max_{y' \\\\in C(N_p)} |y - y'|^2$$\\n\\nwhere $y$ and $y'$ are color values in the neighborhood $N_p$.\\n\\n$N = \\\\{N_p | n(C(N_p)) > 2\\\\}$ is the set of the local neighborhoods having two or more corresponding pixels. A lower VC score indicates higher view consistency.\\n\\n4.4. Ablation Study\\n\\nWe conduct ablation studies on the scene painting network architecture, dimensions of positional encoding, and loss function. In this ablation study, we use the configuration described in the following as our baseline. We use the MLP architecture mentioned in Sec. 3.3 with positional encoding $T = 4$. We use the reconstruction loss $L_{rec}$ without adaptive weight $A$ and adversarial loss with pseudo images with $\\\\lambda_{adv} = 1$. We utilize early stopping and the number of training iterations of 10,000 or 40,000 to avoid overfitting the discriminator. Unless otherwise mentioned, we use this setting in the quantitative experiments.\\n\\nArchitecture of the scene painting network. We conduct an experiment on two architectures. Previous methods based on implicit representations [14, 32] adopt CNNs to improve their performance. In this study, we also examine whether adopting a CNN can improve the generation quality of our framework. In Table 1, MLP+CNN is an MLP architecture followed by four $3 \\\\times 3$ convolution layers. A residual connection is added to the output of each convolution layer. The number of training iterations is 10,000. The results indicate there exists a trade-off between the single image quality and view consistency. As architecture has more CNN layers, the receptive field gets larger, and the view consistency gets worse. MLP+CNN achieves better mIoU and FID than MLP for the single image quality. However, we choose the MLP for the scene painting network because the change of colors of CNN layers is noticeable (Figure 6(a), (b), (c)).\"}"}
{"id": "CVPR-2022-7", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Colored meshes generated with our method, and scene editing examples. (From left to right) the original scene, removed chairs, rearranged chairs, more bottles on the table, and different style of the table.\\n\\nTable 1. Comparison on scene painting network architectures.\\n\\n| Architecture | Measure | mIoU (\u2191) | FID (\u2193) | VC (\u2193) |\\n|--------------|---------|----------|---------|--------|\\n| MLP+CNN      |         | 0.461    | 111.69  | 37.51  |\\n| MLP          |         | 0.391    | 132.93  | 1.23   |\\n\\nTable 2. Evaluation on various $T$ of positional encoding.\\n\\n| Training iter. | Measure | mIoU (\u2191) | FID (\u2193) | VC (\u2193) |\\n|----------------|---------|----------|---------|--------|\\n| 0              |         | 0.290    | 174.39  | 0.76   |\\n| 2              |         | 0.396    | 151.23  | 0.92   |\\n| 4              |         | 0.391    | 132.93  | 1.23   |\\n| 4              | 40,000  |          |         |        |\\n| 6              |         | 0.386    | 155.02  | 3.99   |\\n| 10             | 10,000  |          |         |        |\\n| 10             | 40,000  |          |         |        |\\n\\nTo study the effect of $T$ on the generation quality, we test various values for $T$ in Table 2. The scene painting network produces blurry images without positional encoding ($T = 0$). We found that a large $T$ catches the high-frequency details in the pseudo images when the number of training iterations is large enough to reach the convergence of the reconstruction loss. However, when the number of training iterations is large, artifacts of pseudo images emerge in the generated images (Figure 7(a), (d)). An extensive $T$ causes a grid-like artifact in generated images when the number of training iterations is insufficient to converge the reconstruction loss (Figure 7(b)).\\n\\nThe mIoU values are similar except for $T = 0$ when the number of iterations is 10,000. The mIoU and FID improve when the networks are trained for 40,000 iterations, yet the resulting images have significant artifacts that are not shown in mIoU and FID (Figure 6(d), (e)). The setting with the number of training iterations of 10,000 and $T = 4$ shows the best FID.\\n\\nLoss function. We experiment to determine the best configuration for the loss functions. Table 3 shows the effects of $L_{rec}$, adversarial loss $L_{adv}$ with pseudo images (GAN-p), adaptive weight $A$, and $\\\\lambda_{adv}$. While the first row shows that employing only $L_{rec}$ achieves the lowest VC, it is because using only $L_{rec}$ results in blurry images (Figure 7(b)). We achieve better mIoU and FID scores when combining $L_1$, $L_2$, VGG perceptual losses, and GAN-p. GAN-p enhances perceptual quality, as evidenced by improved mIoU when looking at the first and fourth rows. In this experiment, we also evaluate the adversarial learning with real indoor images from the ADE20k dataset [52], which is denoted as (GAN-r) in the table. The table shows that GAN-r does not improve the quality because the domain gap between the real images and labels from the 3D scene is enormous, making the training difficult. As mentioned earlier, a large number of iterations causes a GAN artifact, and we resolve this by balancing the $\\\\lambda_{adv}$. In the sixth row, when $\\\\lambda_{adv} = 0$.1, training progresses for significant iterations without artifact, and the result shows the better mIoU and FID compared with the fourth row. In the seventh row, when using adaptive weight $A$, the result shows the better mIoU and FID compared with the fourth row. Finally, the last row shows the best configuration.\\n\\nWe also show the measures of OASIS [41], which is our pseudo image generator (Figure 7(a)). We generate pseudo images with the same labels used in the other experiments and evaluate the measures with those pseudo images. Although OASIS\u2019s perceptual quality is superior to ours, it is difficult for OASIS to create coherent images for different viewpoints. We also emphasize that our goal differs significantly from the image synthesis methods. It is unfair to compare our approach against previous ones solely based on conventional metrics.\"}"}
{"id": "CVPR-2022-7", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Images that are generated with different architectures. (a), (b) and (c) are generated by MLP + 3 \u00d7 3 CNN architecture from different viewpoints. (a) and (b) show the color shift of the table, and (b) and (c) show the color shift of the floor. (d) shows the result of MLP architecture with 10k training iterations. (e) shows the result of MLP architecture with 40k iterations.\\n\\nFigure 7. Examples of artifacts. (a) A pseudo image generated from OASIS. (b) $T = 4$, 40k iter., and no adversarial loss. (c) $T = 10$ and 10k iter. (d) $T = 10$ and 40k iter. (e) $T = 4$, 40k iter., and adaptive weight $A$, and $\\\\lambda_{adv} = 0$.\\n\\nFigure 8. Comparison between the averaging approach and ours.\\n\\nComparison with averaging. An alternative to generating view-consistent images from pseudo-ground-truth images would be simply averaging colors from pseudo-ground-truth images for each 3D point. We compare our approach with the averaging approach. The results of the averaging approach show noisy results (Fig. 8). It is because averaging requires many training images without holes. On the other hand, our approach does not suffer from such a problem because it learns a function from a 3D coordinate to a color rather than simply memorizing color for each coordinate.\\n\\n5. Conclusion\\n\\nThis paper proposed a novel 3D scene coloring approach synthesizing the color of configurable 3D scene layout and a training scheme that does not require direct supervision from colored 3D scenes. Given 3D coordinate and semantic label maps, our scene painting network synthesizes view-consistent colored scenes. Our method ensures the view consistency of synthesis, which is not addressed in the semantic image synthesis method. In addition, our method can be used to generate the color of a scene containing multiple objects, allowing users to modify scene color and configuration using 3D graphics tools. An interesting future direction would be to make the pipeline faster and support explainable adjustments of scene styles (make the scenes brighter, darker, or warmer).\\n\\nLimitation\\n\\nOur approach has several limitations. While our approach can produce coherent images and be combined with any image generation approach, our results depend on the quality of the pseudo image generator. Our approach also shows limited diversity in generated images due to the limited diversity of a pre-trained OASIS model as discussed by Yang et al. The example is shown in Fig. 4, where the bed cover is consistently red regardless of the different style vectors. The adaptive weight map assumes each class has a single color, which may lead to over-smoothing in some areas. Besides, our approach hardly generates transparent objects such as a window or viewpoint-dependent appearances. This limitation stems from the input representation. The network assigns one color to each 3D coordinate, so it cannot model the view-dependent lighting effect. Finally, our approach requires training of a new network for a new scene.\\n\\nAcknowledgements\\n\\nThis work was partly supported by IITP grants funded by the Korea government (MSIT) (2019-0-01906: Artificial Intelligence Graduate School Program (POSTECH) and 2020-0-01649: High-Potential Individuals Global Training Prog.) and by Ministry of Culture, Sports and Tourism (R2021040136: Development of meta-verse contents based on XR & AI).\"}"}
