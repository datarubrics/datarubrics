{"id": "CVPR-2024-159", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. The pipeline of our Image-Text Anchor Retrieval (ITAR) module. We search for the most similar image-text pairs in the candidate set to obtain the rich semantic image-text anchors related to the downstream task for regularizing the finetuning process.\\n\\nImage-Caption Contrastive Learning. We cast \\\\((x, t_{\\\\text{cap}})\\\\) as text-compensated anchors and utilize a contrastive loss function \\\\(L_{\\\\text{Cap}}\\\\) similar to Eq. (1) to align them within the feature space, thereby maintaining the semantic consistency between images and texts. The text-compensated anchors prevent overfitting by ensuring that the conventional finetuning process in Eqs. (1) and (2) does not pull the embedding of the image \\\\(x_i\\\\) too close to the text embedding of its corresponding class prompt \\\\(t_c\\\\), regularizing the finetuning process of CLIP with auxiliary semantic supervision.\\n\\n4.2.2 Image-Text Anchor Retrieval\\n\\nFurthermore, we propose to search for image-text pairs according to the downstream task as auxiliary anchors. We construct a candidate set that is similar to the pretraining data of CLIP, originally containing rich semantic information. The pretrained CLIP model possesses an exceptional cross-modal retrieval capacity, which we employ to find image-text pairs with abundant semantics for regularization. These retrieved image-text-pair anchors are aligned using a contrastive loss function to preserve the original feature space during the finetuning process.\\n\\nCandidate Set Construction. We construct the web-scale image-text dataset CC3M \\\\([33]\\\\) as a candidate set \\\\(S_{\\\\text{can}} = \\\\{(x_{\\\\text{can}}i, t_{\\\\text{can}}i)\\\\}\\\\), which is employed to search for rich-semantic image-text pairs. The candidate set closely resembles the pretraining data of CLIP and encompasses abundant semantic information for maintaining the original feature space. We leverage the cross-modal retrieval capacity of CLIP to obtain image-text-pair anchors relevant to the downstream task. Specifically, we extract the embeddings of the images \\\\(x_i\\\\) in the downstream dataset as \\\\(f(x_i)\\\\), as well as the embeddings of the texts \\\\(t_{\\\\text{can}}\\\\) in the candidate set as \\\\(g(t_{\\\\text{can}})\\\\) for preparation. These embeddings are utilized for retrieval and can be precomputed offline.\\n\\nImage-Text Pair Retrieval. In practice, only a small subset of the candidate set \\\\(S_{\\\\text{can}}\\\\) is relevant to the downstream task and can be employed to preserve the original feature space of CLIP. We propose to search for image-text pairs from the candidate set \\\\(S_{\\\\text{can}}\\\\) using KNN search. Specifically, as illustrated in Fig. 3, we designate each image \\\\(x_i\\\\) as the query and find the most similar image-text pairs from the candidate set \\\\(S_{\\\\text{can}}\\\\) by calculating the similarity between the image embedding \\\\(f(x_i)\\\\) and the text embeddings \\\\(g(t_{\\\\text{can}})\\\\), which can be formulated as follows,\\n\\n\\\\[\\nk = \\\\arg \\\\max (f(x_i) \\\\cdot g(t_{\\\\text{can}})),\\n\\\\]\\n\\n(4)\\n\\nwhere \\\\(k\\\\) represents the index of retrieved image-text pair in the candidate set \\\\(S_{\\\\text{can}}\\\\). The retrieval process can be efficiently executed using existing libraries, such as Faiss \\\\([18]\\\\). Subsequently, we utilize the retrieved image-text pairs \\\\(S_{\\\\text{ret}} = \\\\{(x_{\\\\text{ret}}k, t_{\\\\text{ret}}k)\\\\}\\\\) with rich semantics as auxiliary anchors to regularize the finetuning process.\\n\\nImage-Text Contrastive Learning. We denote \\\\((x_{\\\\text{ret}}, t_{\\\\text{ret}})\\\\) as retrieved image-text-pair anchors and employ a contrastive loss function \\\\(L_{\\\\text{Ret}}\\\\), similar to Eq. (1), to preserve the original feature space of CLIP. The retrieved image-text pairs exhibit rich semantics related to the downstream task and serve as auxiliary supervision during finetuning. These two types of image-text anchors with abundant semantic information are complementary to each other and are utilized as additional contrastive supervision to regularize the finetuning process of CLIP. The image encoder and text encoder of CLIP are finetuned together with the following loss function,\\n\\n\\\\[\\nL = L_{\\\\text{CL}} + L_{\\\\text{Cap}} + L_{\\\\text{Ret}}.\\n\\\\]\\n\\n(5)\\n\\n5. Experiments\\n\\nOverview. We assess the effectiveness of our proposed Anchor-based Robust Finetuning (ARF) approach by comparing it with several baselines and providing implementation details for reproducibility. The evaluation of maintaining out-of-distribution (OOD) generalization capabilities is divided into two sections. In Section 5.1, we present results for domain shift, which was the focus of the original robust finetuning methods. Subsequently, in Section 5.2, we show results for our extended scenario (i.e., zero-shot learning). Furthermore, we conduct an ablation study in Section 5.3 to evaluate the efficacy of our approach and showcase qualitative examples of two types of anchors in Section 5.4.\\n\\nBaselines. We compare our ARF with two conventional ways of finetuning pretrained models to downstream tasks using cross-entropy: linear probing (LP) and end-to-end fully finetuning (FT). Additionally, we investigate recent advancements of robust finetuning, such as LP-FT \\\\([22]\\\\),\"}"}
{"id": "CVPR-2024-159", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Methods | ID | Im-V2 | Im-Sketch | ImageNet-A | ImageNet-R | ObjectNet | Avg. OOD |\\n|---------|----|-------|-----------|------------|------------|----------|----------|\\n| CLIP    | 68.3 | 61.9 | 77.7 | 50.0 | 48.3 | 54.2 | 58.4 |\\n| LP      | 79.9 | 69.8 | 70.8 | 46.4 | 46.9 | 50.4 | 56.9 |\\n| FT      | 81.3 | 71.2 | 66.1 | 37.8 | 46.1 | 51.6 | 54.6 |\\n| LP-FT   | 81.7 | 72.1 | 73.5 | 47.6 | 50.3 | 54.4 | 59.6 |\\n| FLYP    | 82.6 | 73.0 | 71.4 | 48.1 | 49.6 | 54.7 | 59.4 |\\n| ARF     | 82.7 | 72.8 | 75.6 | 50.3 | 51.8 | 55.8 | 61.3 |\\n\\nTable 1. Domain shift results (%) of state-of-the-art conventional finetuning and robust finetuning approaches on ImageNet and DomainNet benchmarks. The numbers represent the top-1 accuracy. We employ ImageNet and DomainNet-Real as the finetuning datasets, while the others serve as domain shift evaluation datasets. The best results are marked in **Black**.\\n\\nFigure 4. The ID and domain shift performance of our ARF compared with several baselines through linear interpolation of the finetuned model weights with the original model weights following Wise-FT [39]. The performance curves of ARF surpass (positioned in the upper right) those of the baselines on ImageNet, resulting in improved ID and domain shift accuracy.\\n\\nWhich involves an initial linear probing followed by fully finetuning, and FLYP [12], wherein finetuning is conducted in a pretraining-like manner.\\n\\nImplementation Details. We employ a batch size of 512 for finetuning on ImageNet [7] and DomainNet [29] with 10 epochs. We utilize a learning rate of $10^{-5}$ and a weight decay parameter of $0.1$. ViT-B/16 [10] is utilized as the image encoder of CLIP for finetuning. Domain shift and zero-shot learning benchmarks are exclusively used for evaluation. We apply the same prompt templates as those employed in CLIP [30] and WiseFT [39] for training and inference.\\n\\n5.1. Evaluation Under Domain Shift Benchmarks. We evaluate domain shift performance on two widely employed benchmarks, namely, ImageNet [7] and DomainNet [29]. FMoW [20] and iWILDCam [3] are excluded from our evaluation due to the poor domain shift performance of the original CLIP. In the first benchmark, we finetune CLIP using ImageNet [7] for in-distribution evaluation and assess its performance on five distinct variants of ImageNet with domain shift: ImageNet-V2 [32], ImageNet-Sketch [37], ImageNet-A [15], ImageNet-R [14] and ObjectNet [2]. We follow the training protocol outlined in FLYP [12] and employ the same ImageNet prompt templates used by CLIP during inference. For the second benchmark, we utilize the standard domain shift dataset DomainNet [29] for evaluation. We finetune CLIP using DomainNet-Real for in-distribution performance and assess its generalization ability to four domain shift splits: Clipart, Infograph, Painting, and Sketch.\\n\\nQuantitative Results. We compare the performance of our ARF with several baselines on domain shift benchmarks, as detailed in Table 1. On the ImageNet dataset, our ARF exhibits a slight performance advantage over conventional finetuning methods and other robust finetuning methods in the in-distribution (ID) test dataset. The true strength of our ARF lies in its capacity to generalize to domain shift scenarios, achieving state-of-the-art performance with an average accuracy of 61.3% across five domain shift test datasets. Notably, our ARF performs significantly better than other finetuning methods, approaching or even surpassing the performance of CLIP on ImageNet-R, ImageNet-A, ImageNet-Sketch, and ObjectNet, which have large domain differences. In the case of DomainNet, we also observe that our ARF demonstrates state-of-the-art performance of 65.2% on the domain shift test dataset without sacrificing accuracy on in-distribution (ID) data. Our ARF outperforms CLIP on domain shift scenarios by 1.1%, whereas other finetuning methods either fail to surpass it or achieve only marginal improvements. These results indicate the effectiveness of the two types of anchors in our ARF. They effectively prevent the overfitting of the original feature space of CLIP to the downstream class prompts after finetuning, preserving the OOD generalization capability of CLIP for handling domain shift scenarios.\\n\\nWeight Ensembling Curves. Wise-FT [39] demonstrates that a simple linear interpolation between the weights of the pretrained and finetuned models yields the optimal performance for both ID and domain shift. Therefore, we...\"}"}
{"id": "CVPR-2024-159", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Zero-shot learning results (%) of state-of-the-art conventional finetuning and robust finetuning methods on numerous recognition tasks. The numbers represent the top-1 accuracy. We employ ImageNet as the finetuning dataset while the others serve as zero-shot learning evaluation datasets. The best results are marked in Black.\\n\\n| Method           | ImageNet | Zero-Shot Avg. OOD |\\n|------------------|----------|--------------------|\\n| CLIP             | 68.3     | 89.3               |\\n| FT               | 81.3     | 78.8               |\\n| LP-FT            | 81.7     | 84.0               |\\n| FLYP             | 82.6     | 87.6               |\\n| ARF              | 82.7     | 88.6               |\\n\\nTable 3. Ablation study for Text-Compensated Anchor Generation (TCAG) module and Image-Text Anchor Retrieval (ITAR) module of our ARF. The baseline only conducts visual-language contrastive learning for finetuning like FLYP [12].\\n\\n5.2. Evaluation Under Zero-shot Learning Benchmarks.\\n\\nWe evaluate zero-shot learning performance using a diverse benchmark that encompasses a range of recognition tasks. To ensure a fair comparison, we employ the standard test split for inference, and the CLIP model is finetuned on ImageNet [7]. As for the evaluation of zero-shot learning, we utilize fine-grained object classification tasks such as OxfordPets [28], StanfordCars [21], Flowers102 [27] and Food101 [4]; as well as specific recognition tasks such as UCF101 [35] for action recognition, FGV-Aircraft [26] for aircraft classification, DTD [6] for texture classification, SUN397 [40] for scene recognition and EuroSAT [13] for satellite image classification. Additionally, we also evaluate our ARF on the general object classification dataset Caltech101 [11].\\n\\nQuantitative Results.\\n\\nWe present the results of our ARF and several baseline approaches on various zero-shot learning recognition tasks, as displayed in Table 2. It can be observed that previous finetuning methods exhibit substantial improvements in accuracy on the ImageNet test data after finetuning with ImageNet training data, compared to the original CLIP. However, their performance significantly deteriorates in zero-shot learning recognition on the categories that were not contained in the finetuning data. In contrast, our ARF optimally maintains the zero-shot recognition capability without compromising the performance on ImageNet. The experimental results demonstrate that our ARF effectively regularizes the finetuning process with auxiliary semantic supervision, preserving the OOD generalization capability of CLIP for handling zero-shot learning scenarios.\\n\\n5.3. Ablation Study\\n\\nTwo Types of Anchors.\\n\\nTo evaluate the effectiveness of our ARF, we conduct an ablation study to analyze the influence of the two types of anchors, as shown in Table 3. It can be observed that the Text-Compensated Anchor Generation (TCAG) module significantly improves domain shift and zero-shot learning accuracy by 1.3% and 5.7% over the baseline, respectively. These gains demonstrate that the rich semantic text descriptions generated by the pretrained captioner provide effective auxiliary supervision for the images to alleviate overfitting on class prompts. For retrieved image-text-pair anchors, we evaluate the influence of the Image-Text Anchor Retrieval (ITAR) module, which can improve domain shift and zero-shot learning accuracy over the baseline by 0.8% and 5.0% after finetuning on ImageNet, respectively. These results reveal that the rich semantic image-text pairs, retrieved from the candidate set similar to the pretraining data of CLIP according to the downstream task, are beneficial for regularizing the finetuning process. The two modules, working together, boost performance over the baseline by 1.9% on domain shift and 7.0% on zero-shot learning. The experimental results suggest\\n\\n26925\"}"}
{"id": "CVPR-2024-159", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Visualization examples of captions generated by a pretrained captioner (e.g., BLIP2 [24]) and retrieved image-text pairs relative to the downstream task from the CC3M dataset [33]. The downstream task images and generated captions serve as text-compensated anchors for regularization. The retrieved image-text pairs function as auxiliary anchors for maintaining the feature space.\\n\\n| Captioner | ImageNet | Zero-Shot | ID Domain Shift | Avg. Acc |\\n|-----------|----------|-----------|----------------|----------|\\n| BLIP      | 82.1     | 60.1      | 54.0           |          |\\n| BLIP2     | 82.7      | 61.3      | 55.6           |          |\\n| BLIP2 + Vicuna | 82.5       | 61.2      | 56.5           |          |\\n\\nTable 4. Ablation study for the quality of generated captions. We evaluate the effectiveness of pretrained image captioners (i.e., BLIP [23] and BLIP2 [24]) and further rewrite the text descriptions using Large Language Models (e.g., Vicuna [5]) in our ARF.\\n\\nThe Quality of Captions. To assess the impact of caption quality in our ARF, we examine two pretrained image captioners (i.e., BLIP [23] and BLIP2 [24]) and further rewrite the text descriptions using Large Language Models (e.g., Vicuna [5]). As illustrated in Table 4, employing captions generated by BLIP2 results in a 1.2% improvement in domain shift performance and a 1.6% enhancement in zero-shot learning performance compared to using captions generated by BLIP. These gains demonstrate that more accurate text descriptions with rich semantics are beneficial for regularizing the finetuning process. Additionally, we utilize Vicuna [5] to rewrite the captions for increased diversity and richer semantic information. Since the text descriptions generated by BLIP2 are already sufficiently accurate, there is no improvement in ID and domain shift scenarios. However, the rich semantic knowledge accessed from Vicuna boosts zero-shot learning by 0.9%. These results indicate the effectiveness of auxiliary information from LLMs, which warrants further exploration.\\n\\n5.4. Qualitative Examples of Anchors\\n\\nIn Fig. 5, we provide visualization examples to facilitate an understanding of how our Anchor-based Robust Finetuning (ARF) works. The text descriptions (i.e., captions) generated by a pretrained captioner (e.g., BLIP2 [24]) accurately describe the images, thus serving as effective anchors that supply rich information for maintaining the semantic consistency between images and texts. The image-text pairs, retrieved from the candidate set, bear a close resemblance to the pretraining data of CLIP and are related to the downstream task. This contributes auxiliary semantic knowledge to preserve the OOD generalization capabilities of CLIP.\\n\\n6. Conclusion\\n\\nIn this study, we extend previous robust finetuning to a more challenging setting: preserving out-of-distribution (OOD) generalization capabilities in both domain shift and zero-shot learning during finetuning. We argue that the diminished OOD generalization results from the overly simplified finetuning target, which provides only class information. Consequently, we propose an Anchor-based Robust Finetuning (ARF) approach to regularize the finetuning process with auxiliary contrastive supervision. This approach incorporates a Text-Compensated Anchor Generation module and an Image-Text Anchor Retrieval module to generate image-text-pair anchors with rich semantic information and align these anchors with contrastive loss. Extensive experiments demonstrate the effectiveness of our approach.\\n\\nAcknowledgement\\n\\nThis work was supported by the National Nature Science Foundation of China under grants 62306214 and 62325111.\"}"}
{"id": "CVPR-2024-159", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Anchor-based Robust Finetuning of Vision-Language Models\\nJinwei Han, Zhiwen Lin, Zhongyisun Sun, Yingguo Gao, Ke Yan, Shouhong Ding, Yuan Gao, Gui-Song Xia\\n\\n1 School of Computer Science, Wuhan University\\n2 YouTu Lab, Tencent\\n3 Electronic Information School, Wuhan University\\n\\n{hanjinwei, guisong.xia}@whu.edu.cn, {sunzy12315, ethan.y.gao}@gmail.com\\nxavier.lin@foxmail.com, {yingguogao, kerwinyan, ericshding}@tencent.com\\n\\nAbstract\\nWe aim at finetuning a vision-language model without hurting its out-of-distribution (OOD) generalization. We address two types of OOD generalization, i.e., i) domain shift such as natural to sketch images, and ii) zero-shot capability to recognize the category that was not contained in the finetune data. Arguably, the diminished OOD generalization after finetuning stems from the excessively simplified finetuning target, which only provides the class information, such as \\\"a photo of a [CLASS].\\\" This is distinct from the process in that CLIP was pretrained, where there is abundant text supervision with rich semantic information. Therefore, we propose to compensate for the finetune process using auxiliary supervision with rich semantic information, which acts as anchors to preserve the OOD generalization. Specifically, two types of anchors are elaborated in our method, including i) text-compensated anchor which uses the images from the finetune set but enriches the text supervision from a pretrained captioner, ii) image-text-pair anchor which is retrieved from the dataset similar to pretraining data of CLIP according to the downstream task, associating with the original CLIP text with rich semantics. Those anchors are utilized as auxiliary semantic information to maintain the original feature space of CLIP, thereby preserving the OOD generalization capabilities. Comprehensive experiments demonstrate that our method achieves in-distribution performance akin to conventional finetuning while attaining new state-of-the-art results on domain shift and zero-shot learning benchmarks.\\n\\n1. Introduction\\nMaintaining out-of-distribution (OOD) generalization is essential for a pretrained model to ensure applicability across diverse circumstances (e.g., domain shift and zero-shot learning), even after adaptation to downstream tasks. In recent years, contrastive vision-language pretrained models, such as CLIP [30] and ALIGN [17], have exhibited exceptional OOD generalization capabilities in the aforementioned situations. Although it is desirable to finetune these models using task-specific labeled data to improve performance on downstream tasks [8, 31], they often experience a significant degradation of OOD generalization [22, 39].\\n\\nAs illustrated in Fig. 1(a), OOD generalization encompasses domain shift, such as from natural to sketch images, and zero-shot capability to recognize categories not present in the finetuning data. Numerous methods have been proposed to preserve the OOD generalization of CLIP during the finetuning process. Prompt learning [1, 42, 43] optimizes a set of learnable vectors using a limited number of in-distribution examples. The method consists of three main steps: i) selecting a subset of in-distribution data (finetune set), ii) retrieving the related auxiliary data (auxiliary set), and iii) finetuning the CLIP model using the selected subset and auxiliary data. This process leverages the auxiliary data to fine-tune the CLIP model without degrading its OOD generalization capability.\\n\\nFigure 1. Motivation illustration. (a) The out-of-distribution generalization (i.e., domain shift and zero-shot learning) capabilities of CLIP degrade significantly after finetuning on downstream tasks. (b) Images with generated texts and retrieved image-text pairs serve as two types of anchors to regularize the finetuning process of CLIP with auxiliary semantic information.\"}"}
{"id": "CVPR-2024-159", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"labeled images while keeping the entire pretrained parameters of CLIP frozen. Although these approaches demonstrate potential in addressing zero-shot prediction, their performance on downstream tasks is insufficient for practical needs. Robust finetuning \\\\cite{12, 22, 36, 39} leverages all available data and employs a fully-tuned process to achieve high accuracy on in-distribution tasks without compromising performance under domain shift in OOD, while zero-shot capability remains neglected. In this study, we extend robust finetuning to a more challenging scenario, aiming to preserve OOD generalization capabilities in both domain shift and zero-shot learning. For instance, as shown in Fig. 1(a), our goal is to improve CLIP's capacity to recognize various types of animals while maintaining its original ability to classify animals from different domains (e.g., sketch) and other categories of animals (e.g., horse).\\n\\nOur work is based on the observation that the feature space in CLIP, trained on large-scale image-text pairs, encompasses open-vocabulary semantic knowledge and thus demonstrates exceptional OOD generalization capabilities. The conventional supervised finetuning paradigm primarily minimizes a cross-entropy loss on an image classifier with class labels. FLYP \\\\cite{12} casts downstream class labels as text prompts (e.g., \u201ca photo of a [CLASS]\u201d) and optimizes the contrastive loss to align the image embeddings with the prompt embeddings, using only the class information.\\n\\nWe argue that the decline in OOD generalization stems from the semantic-scarce supervision containing only class labels during the finetuning process. Such an oversimplified finetuning target is distinct from the abundant text supervision employed in the pretraining of CLIP, leading to degraded OOD generalization. Specifically, image features originally possessing rich semantics tend to collapse into a single class center after finetuning. As a result, the feature space of the finetuned model shifts towards fitting the downstream dataset without the necessity of preserving rich semantics. Finally, as a consequence, the original feature space of CLIP deteriorates significantly, impairing its OOD generalization capabilities.\\n\\nBased on the above analysis, we propose an Anchor-based Robust Finetuning (ARF) approach that regularizes the finetuning process of CLIP with auxiliary contrastive supervision. As illustrated in Fig. 1(b), our approach incorporates two types of anchors for maintaining the original feature space of CLIP. Specifically, one of them is the text-compensated anchor, which is derived from using the image from the finetune set while enriching the semantic text from a pretrained captioner, such as BLIP2 \\\\cite{24}. This is achieved by a carefully designed Text-Compensated Anchor Generation (TCAG) module. The other one is the image-text-pair anchor retrieved from a dataset similar to the pretraining data of CLIP according to the downstream task, whose text originally exhibits rich semantics. We implement this using the Image-Text Anchor Retrieval (ITAR) module. These two types of image-text anchors are complementary to each other and are utilized as auxiliary supervision to regularize the finetuning process of CLIP, ensuring that the image features do not converge too close to the class prompt while retaining the original feature space of CLIP. Extensive experiments demonstrate that our ARF achieves in-distribution performance comparable to finetuning while attaining new state-of-the-art OOD generalization results on domain shift and zero-shot learning benchmarks.\\n\\nOur main contributions are summarized as follows:\\n\\n\u2022 We extend robust finetuning to a more challenging setting, aiming to preserve OOD generalization capabilities in both domain shift and zero-shot learning.\\n\\n\u2022 We propose Anchor-based Robust Finetuning (ARF), using both text-compensated anchor and retrieved image-text-pair anchor to regularize the finetuning process.\\n\\n\u2022 Extensive experiments reveal that our ARF attains new state-of-the-art performance on domain shift and zero-shot learning benchmarks while achieving in-distribution performance akin to finetuning.\\n\\n2. Related Work\\n\\n2.1. Vision-Language Contrastive Learning\\n\\nVision-language models \\\\cite{17, 30, 34} primarily focus on establishing a joint embedding space for cross-modal learning by aligning web-scale images and texts. Recent advancements in the contrastive vision-language pretraining paradigm, particularly CLIP \\\\cite{30}, have demonstrated exceptional out-of-distribution (OOD) generalization capabilities across various downstream tasks (e.g., domain shift and zero-shot learning). Numerous subsequent studies have been proposed to further enhance contrastive image-text pretraining, such as masked language/image modeling \\\\cite{9, 25}, hard sample mining \\\\cite{38}, and retrieval augmentation \\\\cite{16, 41}. BLIP \\\\cite{23} aims to improve image-text pretraining for unified vision-language understanding and generation, while BLIP2 \\\\cite{24} incorporates Large Language Models (LLMs) to further improve generalization. Although these methods have indeed enhanced vision-language pretraining, finetuning remains necessary to improve performance on downstream tasks. A recent study \\\\cite{8} demonstrates that CLIP attains superior or at least competitive performance on downstream tasks after finetuning, compared to conventional supervised pretraining models. ViFi-CLIP \\\\cite{31} implicitly models temporal cues and effectively finetunes image-level CLIP representations for videos. However, the OOD generalization capabilities significantly degrade after finetuning, impairing the applicability of CLIP across diverse circumstances.\"}"}
{"id": "CVPR-2024-159", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2. Finetuning for Generalization\\nMaintaining the OOD generalization capabilities of CLIP during the finetuning process has been extensively investigated through various approaches. Prompt learning [19, 42, 43] incorporates a small number of learnable prompt vectors to finetune CLIP, using a limited set of labeled images while keeping the pretrained model weights fixed. Although these methods demonstrate potential in addressing zero-shot prediction, their performance on downstream tasks remains unsatisfactory. Robust finetuning [12, 22, 36, 39] leverages all available data and implements a fully-tuned process to achieve high accuracy on in-distribution tasks without sacrificing performance under domain shift in OOD. Wise-FT [39] ensembles the weights of finetuned and original models, yielding remarkable gains in domain shift. LP-FT [22] initially trains the classification layer and then finetunes the entire network. This two-stage process substantially mitigates distortion of pretrained features and improves generalization. FLYP [12] demonstrates that a straightforward method of mimicking contrastive pretraining consistently outperforms finetuning approaches, while TPGM [36] automatically learns the constraint imposed on each layer for finetuning regularization. Nonetheless, the zero-shot learning capability is neglected by robust finetuning, leading to a substantial degradation of OOD generalization. In light of these observations, we propose a more practical and challenging setting to finetune CLIP on downstream tasks without compromising both domain shift and zero-shot learning generalization capabilities.\\n\\n3. Preliminaries\\n3.1. Summary of Notations\\nWe summarize the notations used in the following sections. Specifically, we use the uppercase calligraphy font to denote a specific set, e.g., a dataset \\\\( S \\\\), which consists of input images and their corresponding class information. The input images are denoted by \\\\( X \\\\), while the class information is either represented by the class labels \\\\( Y \\\\), or the text descriptions \\\\( T \\\\) encompassing the class information, e.g., \\\\( S = \\\\{X, Y\\\\} \\\\) or \\\\( S = \\\\{X, T\\\\} \\\\). We denote the set of all the classes by \\\\( C \\\\). We use the lowercase font to denote a sample, e.g., \\\\( x \\\\) is an image sample.\\n\\n3.2. Contrastive Vision-Language Models\\nIn this paper, we adapt the contrastive vision-language pretraining model CLIP [30] to downstream tasks, which comprises an image encoder \\\\( f(\\\\cdot) \\\\) and a text encoder \\\\( g(\\\\cdot) \\\\). During the pretraining phase, image-text pairs are sampled from a web-scale training dataset \\\\( S_{\\\\text{train}} = \\\\{(x_{\\\\text{train}}^i, t_{\\\\text{train}}^i)\\\\} \\\\), where each text description \\\\( t_{\\\\text{train}}^i \\\\) encompasses rich semantics with abundant class information. The categories \\\\( C_{\\\\text{train}} \\\\) cover open-vocabulary semantic knowledge. Each image \\\\( x_{\\\\text{train}}^i \\\\) and text \\\\( t_{\\\\text{train}}^i \\\\) is mapped to an image embedding \\\\( f(x_{\\\\text{train}}^i) \\\\) and a text embedding \\\\( g(t_{\\\\text{train}}^i) \\\\), respectively. Subsequently, a contrastive loss function \\\\( L_{\\\\text{CL}} \\\\) is utilized to align the image embedding \\\\( f(x_{\\\\text{train}}^i) \\\\) with the corresponding text embedding \\\\( g(t_{\\\\text{train}}^i) \\\\), which is formulated as,\\n\\n\\\\[\\nL_{\\\\text{CL}} = -\\\\frac{1}{B} \\\\sum_{i=1}^{B} \\\\log \\\\frac{\\\\exp(\\\\frac{f(x_{\\\\text{train}}^i) \\\\cdot g(t_{\\\\text{train}}^i)}{\\\\tau})}{\\\\sum_{j=1}^{B} \\\\exp(\\\\frac{f(x_{\\\\text{train}}^j) \\\\cdot g(t_{\\\\text{train}}^j)}{\\\\tau})} - \\\\frac{1}{B} \\\\sum_{i=1}^{B} \\\\log \\\\frac{\\\\exp(\\\\frac{g(t_{\\\\text{train}}^i) \\\\cdot f(x_{\\\\text{train}}^i)}{\\\\tau})}{\\\\sum_{j=1}^{B} \\\\exp(\\\\frac{g(t_{\\\\text{train}}^j) \\\\cdot f(x_{\\\\text{train}}^j)}{\\\\tau})},\\n\\\\]\\n\\nwhere \\\\( B \\\\) represents the number of image-text pairs in the minibatch, and \\\\( \\\\tau \\\\) denotes the temperature parameter used to scale the pairwise similarities in the loss function.\\n\\nWe consider image recognition as evaluation, that is, we evaluate the performance on a test set \\\\( S_{\\\\text{test}} = \\\\{(x_{\\\\text{test}}^i, y_{\\\\text{test}}^i)\\\\} \\\\) with categories \\\\( C_{\\\\text{test}} \\\\). CLIP utilizes predefined text prompts \\\\( t_{\\\\text{test}}^c \\\\) as inputs for the text encoder, which describe each class \\\\( c \\\\in C_{\\\\text{test}} \\\\), such as, text prompt: a photo of a [CLASS], where \\n\\n\\\\[\\n\\\\text{[CLASS]}\\n\\\\]\\n\\nis the class name. The output text embeddings \\\\( g(t_{\\\\text{test}}^c) \\\\) are employed as classification weights during evaluation. Given an image \\\\( x_{\\\\text{test}}^i \\\\) in \\\\( S_{\\\\text{test}} \\\\), it is fed into the image encoder of CLIP to obtain the corresponding image embedding \\\\( f(x_{\\\\text{test}}^i) \\\\). The predicted label \\\\( p_{\\\\text{test}}^i \\\\) is calculated as,\\n\\n\\\\[\\np_{\\\\text{test}}^i = \\\\arg \\\\max_{c \\\\in C_{\\\\text{test}}} (f(x_{\\\\text{test}}^i) \\\\cdot g(t_{\\\\text{test}}^c)),\\n\\\\]\\n\\nwhich implies that the class exhibiting the highest similarity between the image embedding \\\\( f(x_{\\\\text{test}}^i) \\\\) and text embeddings \\\\( g(t_{\\\\text{test}}^c) \\\\) of \\\\( C_{\\\\text{test}} \\\\) classes is selected as the classification result.\\n\\nWe finetune the CLIP model by mimicking contrastive pretraining, as described in FLYP [12]. Specifically, class labels are formulated as text prompts in Eq. (2) and the contrastive loss function in Eq. (1) is utilized to align image embeddings with text prompt embeddings. We follow the same evaluation process as used in CLIP [30].\\n\\n4. Method\\n4.1. Problem Setup\\nGiven a pretrained CLIP model, we aim to finetune it on the in-distribution dataset \\\\( S_{\\\\text{id}} = \\\\{(x_{\\\\text{id}}^i, y_{\\\\text{id}}^i)\\\\} \\\\) sampled from the distribution \\\\( P_{\\\\text{id}} \\\\) with classes \\\\( C_{\\\\text{id}} \\\\), where each image \\\\( x_{\\\\text{id}}^i \\\\) has a\"}"}
{"id": "CVPR-2024-159", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The pipeline of our proposed Anchor-based Robust Finetuning (ARF) comprises a Text-Compensated Anchor Generation (TCAG) module and an Image-Text Anchor Retrieval (ITAR) module. TCAG generates a caption for each image in the finetuning dataset utilizing a pretrained captioner as a text-compensated anchor with rich semantics. ITAR searches for image-text pairs from a candidate set similar to the data on which CLIP was pretrained, ensuring the presence of rich semantics in the image-text-pair anchor. We retrieve those samples related to our downstream tasks. A contrastive loss function, as used in CLIP, is employed for image-text alignment.\\n\\nLabel $y_{id} \\\\in Y_{id}$. The adapted model should perform at least as well as conventional finetuning methods on the test set from the same distribution $P_{id}$ and with the same categories $C_{id}$ as the training data. Simultaneously, we also work towards preserving the out-of-distribution (OOD) generalization capabilities in both domain shift and zero-shot learning scenarios. In the domain shift situation, we evaluate the performance on the domain shift dataset $S_{ds} = \\\\{(x_{ds}, y_{ds})\\\\}$. The test data are sampled from a different domain $P_{ds}$ but share the same categories as the in-distribution data where we have $P(X_{id}) \\\\neq P(X_{ds})$ while $P(Y|X_{id}) = P(Y|X_{ds})$.\\n\\nAs for zero-shot learning, we have the downstream dataset as $S_{zsl} = \\\\{(x_{zsl}, y_{zsl})\\\\}$, which composes of test image $x_{zsl}$ from a different category $y_{zsl} \\\\in Y_{zsl}$ where $C_{zsl} \\\\cap C_{id} = \\\\emptyset$.\\n\\nIn other words, as illustrated in Fig. 1(a), consider fine-tuning on real cat images, if we use sketch cat images for testing, then it is a domain shift problem; while if we test on horse images whose category is not included in the fine-tune set, then it is a zero-shot learning situation. We aim to achieve high performance on the in-distribution testing set, while also preserving the OOD generalization for both domain shift and zero-shot learning scenarios.\\n\\n4.2. Anchor-based Robust Finetuning\\n\\nOverview. As shown in Fig. 2, our Anchor-based Robust Finetuning (ARF) approach finetunes both the image encoder and text encoder of CLIP using contrastive loss and incorporates two distinct modules to regularize the finetuning process. Specifically, in the Text-Compensated Anchor Generation (TCAG) module, we leverage a pretrained image captioner to generate a caption as text-compensated information for each image and align them using contrastive loss. In the Image-Text Anchor Retrieval (ITAR) module, we search for image-text pairs from a dataset similar to CLIP's pretraining data, which are related to the downstream task. These samples, originally containing rich semantic information, serve as additional anchors during fine-tuning. These two types of anchors complement each other and preserve the original feature space of CLIP to ensure the OOD generalization after adaptation.\\n\\n4.2.1 Text-Compensated Anchor Generation\\n\\nIt is essential to note that merely employing the contrastive training loss between images $x$ and class prompts $t_{c}$ in Eq. (1) can lead to overfitting the excessively simplified finetuning target, as class prompts $t_{c}$ contain only class information shown in Eq. (2). This semantic-scarce supervision is distinct from the abundant text supervision exploited in the pretraining of CLIP and can result in the degradation of the original feature space.\\n\\nCaption Generation. To mitigate the aforementioned issue, we propose to employ a pretrained image captioner, such as BLIP2 [24], to generate a text description $t_{cap}$ (i.e., caption) as compensated rich-semantic information for each image $x_{i}$, thereby preventing the overfitting on class prompts $t_{c}$. The generated captions $t_{cap}$ encompass various descriptive words with more abundant semantics compared to class prompts $t_{c}$, similar to the texts utilized in CLIP's pretraining. The images $x$ and corresponding captions $t_{cap}$ form the text-compensated anchors for regularization.\"}"}
{"id": "CVPR-2024-159", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Shuanghao Bai, Min Zhang, Wanqi Zhou, Siteng Huang, Zhirong Luan, Donglin Wang, and Badong Chen. Prompt-based distribution alignment for unsupervised domain adaptation. arXiv:2312.09553, 2023.\\n\\n[2] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In NeurIPS, 2019.\\n\\n[3] Sara Beery, Elijah Cole, and Arvi Gjoka. The iwildcam 2020 competition dataset. arXiv:2004.10340, 2020.\\n\\n[4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 - mining discriminative components with random forests. In ECCV, 2014.\\n\\n[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-hao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. https://vicuna.lmsys.org, 2023.\\n\\n[6] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014.\\n\\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.\\n\\n[8] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Shuyang Gu, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. CLIP itself is a strong fine-tuner: Achieving 85.7% and 88.0% top-1 accuracy with vit-b and vit-l on imagenet. arXiv:2212.06138, 2022.\\n\\n[9] Xiaoyi Dong, Yinglin Zheng, Jianmin Bao, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Maskclip: Masked self-distillation advances contrastive language-image pretraining. In CVPR, 2023.\\n\\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\\n\\n[11] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshops, 2004.\\n\\n[12] Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models. In CVPR, 2023.\\n\\n[13] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Remote Sensing, 2019.\\n\\n[14] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadowath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021.\\n\\n[15] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, 2021.\\n\\n[16] Ahmet Iscen, Mathilde Caron, Alireza Fathi, and Cordelia Schmid. Retrieval-enhanced contrastive vision-text models. arXiv:2306.07196, 2023.\\n\\n[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021.\\n\\n[18] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. IEEE Trans. Big Data, 2021.\\n\\n[19] Muhammad Uzair Khattak, Hanoona Abdul Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. CVPR, 2023.\\n\\n[20] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran S. Haque, Sara M. Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In ICML, 2021.\\n\\n[21] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV Workshops, 2013.\\n\\n[22] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pre-trained features and underperform out-of-distribution. In ICLR, 2022.\\n\\n[23] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022.\\n\\n[24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv:2301.12597, 2023.\\n\\n[25] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. CVPR, 2023.\\n\\n[26] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew B. Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv:1306.5151, 2013.\\n\\n[27] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In ICVGIP, 2008.\\n\\n[28] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012.\\n\\n[29] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, 2019.\"}"}
{"id": "CVPR-2024-159", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021.\\n\\nHanoona Abdul Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Fine-tuned CLIP models are efficient video learners. CVPR, 2023.\\n\\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, 2019.\\n\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.\\n\\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillem Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. FLA V A: A foundational language and vision alignment model. In CVPR, 2022.\\n\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. arXiv:1212.0402, 2012.\\n\\nJunjiao Tian, Xiaoliang Dai, Chih-Yao Ma, Zecheng He, Yen-Cheng Liu, and Zsolt Kira. Trainable projected gradient method for robust fine-tuning. In CVPR, 2023.\\n\\nHaohan Wang, Songwei Ge, Zachary C. Lipton, and Eric P. Xing. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019.\\n\\nHaonan Wang, Minbin Huang, Runhui Huang, Lanqing Hong, Hang Xu, Tianyang Hu, Xiaodan Liang, and Zhenguo Li. Boosting visual-language models by exploiting hard samples. arXiv:2305.05208, 2023.\\n\\nMitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. In CVPR, 2022.\\n\\nJianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.\\n\\nChenwei Xie, Siyang Sun, Xiong Xiong, Yun Zheng, Deli Zhao, and Jingren Zhou. Ra-clip: Retrieval augmented contrastive language-image pre-training. In CVPR, 2023.\\n\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, 2022.\\n\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. IJCV, 2022.\"}"}
