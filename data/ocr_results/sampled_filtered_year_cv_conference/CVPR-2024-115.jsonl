{"id": "CVPR-2024-115", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the above ABD-R module, unlabeled images are manipulated to create new samples by replacing the potentially degraded regions of one model making erroneous predictions, thereby preventing the degradation of the other model\u2019s training process. For the unlabeled data, it also reduces the likelihood of one model sharing the same definition with Eq. (5).\\n\\nTo achieve controlled effects of the mixed perturbations for cross-supervision, as shown in Eq. (18). For strong augmented image, the corresponding logits and confidence scores are represented as $Z$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$ confidence score of a patch is $\\\\text{Ind}_u$. The patch of one augmented image is divided into $K$ patches, the corresponding logits and confidence scores are represented as $Z$. The selected indices for the top $1$"}
{"id": "CVPR-2024-115", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gaussian warming up: gradually increase with iterations increases based on the where $\\\\lambda$\\n\\n\\\\[ Y = [24], \\\\text{ respectively.} \\\\]\\n\\nwhere $P$  \\n\\nL\\n\\nThe ACDC dataset [4] comprises 200 an-\\n\\n4.1. Dataset and Evaluation Metrics  \\n\\n4. Experiments\\n\\nP\\n\\nSimilarly, $L$ the original augmented labeled images, $L$ newly generated labeled samples.\\n\\n4.2. Implementation Details\\n\\nP\\n\\nThe unsupervised loss $L$ The loss $L$ The loss function $L$ The loss function $L$ $L$ The loss $L$ $L$ The loss $L$ $L$ The loss $L$\\n\\n4.3. Comparison with State-of-the-Art Methods\\n\\n$P$\\n\\nFor testing. Following previous approaches, four evaluation\\n\\n$\\\\text{ACDA dataset:}$\\n\\n4075\\n\\nmetrics and achieves a new state-of-the-art performance. It improvement compared to BCP. Considering the 10% labeled\\n\\n$\\\\text{Prominent comparison}$\\n\\nproposed approach. Specifically, we observe a 20.75% DSC improve-\\n\\nwhich highlights the flexibility and scalability of our ap-\\n\\ni.e. the ACDC dataset. By incorporating our method into BCP,\\n\\nnewly released methods, it achieved high performance on\\n\\nline Cross Teaching by a large margin. BCP is one of the\\n\\nperformance compared to recent methods, surpassing the base-\\n\\nthe 5% label ratio, our ABD achieves state-of-the-art per-\\n\\nthe 10% labeled ratios. For\\n\\nTable 1 shows the average performance on\\n\\nCross Teaching [23] and BCP [2].\\n\\n$\\\\text{BCP}$\\n\\nWe evaluate our approach on two baselines: Cross Teach-\\n\\nWe add the input perturbation using\\n\\nweak and strong data augmentation to provide two kinds of\\n\\nincluding 8 labeled slices and 8 unlabeled slices. The networks are trained with a batch size of 16,  \\n\\n$K$  \\n\\n256\\n\\nused on these two student networks following the above de-\\n\\nK\\n\\nDuring training, all inputs are cropped to\\n\\n$224$\\n\\ninputs. For weak data augmentation, random rotation and\\n\\nflipping are used. For strong data augmentations, colorjit-\\n\\n$\\\\text{bc}$\\n\\n\\\\[ \\\\theta ] \\\\text{and} \\\\[ s.c \\\\]\\n\\nare the predictions in Eq (2). $\\\\text{$w,c$ are the predictions in Eq (2).}$\\n\\n$f$ which are the predictions from $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{aug}$ $\\\\text{semi}$ $\\\\text{abd}$ $\\\\text{"}
{"id": "CVPR-2024-115", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparisons with other methods on the ACDC test set, \u201cOurs -ABD (Cross Teaching)\u201d and \u201cOurs -ABD (BCP)\u201d represent the baseline is Cross Teaching [23] and BCP [2] respectively.\\n\\n| Method                  | Scans used | Metrics |\\n|-------------------------|------------|---------|\\n|                         | Labeled    | Unlabeled | DSC | Jaccard | 95HD | ASD | \u2193 |\\n| U-Net (MICCAI'2015) [27]| 3(5%)      | 0        | 47.83 | 37.01  | 31.16 | 12.62 |\\n|                         | 7(10%)     | 0        | 79.41 | 68.11  | 9.35  | 2.70  |\\n|                         | 70(All)    | 0        | 91.44 | 84.59  | 4.30  | 0.99  |\\n| DTC (AAAI'2021) [21]    | 3(5%)      | 67(95%)  | 56.90 | 45.67  | 23.36 | 7.39  |\\n| URPC (MICCAI'2021) [22] | 55.87      | 44.64    |       |        |       |       |\\n| MC-Net (MICCAI'2021) [36]|           |          |       |        |       |       |\\n| SS-Net (MICCAI'2022) [37]|           |          |       |        |       |       |\\n| SCP-Net (MICCAI'2023) [44]|           |          |       |        |       |       |\\n| Cross Teaching (Reported) (MIDL'2022) [23]|           |          |       |        |       |       |\\n| BCP (CVPR'2023) [2]     | 87.59      | 78.67    | 1.90  | 0.67   |       |\\n| Ours -ABD (Cross Teaching)|           |          |       |        |       |       |\\n|                         |            |          |       |        |       |       |\\n| Ours -ABD (BCP)         | 88.96      | 80.70    | 1.57  | 0.52   |       |\\n\\nTable 2. Comparisons with state-of-the-art semi-supervised segmentation methods on the PROMISE12 test set.\\n\\n| Method | Scans used | Metrics |\\n|--------|------------|---------|\\n|        | Labeled    | Unlabeled | DSC | ASD | \u2193 |\\n| U-Net  | 7(20%)     | 0        | 60.88 | 13.87 |\\n|        | 35(100%)   | 0        | 84.76 | 1.58  |\\n| CCT    | 7(20%)     | 28(80%)  | 71.43 | 16.61 |\\n| URPC   |            |          | 63.23 | 4.33  |\\n| SS-Net |            |          | 62.31 | 4.36  |\\n| SLC-Net|            |          | 68.31 | 4.69  |\\n| SCP-Net|            |          | 77.06 | 3.52  |\\n| Ours-ABD|           |          | 82.06 | 1.33  |\\n| Ours-ABD| 3(10%)    | 32(90%)  | 81.81 | 1.46  |\\n\\nDemonstrates that ABD mitigates the uncontrolled effects caused by the mixture of multiple perturbations and validates the mixed perturbations could expand the upper bound of consistency learning.\\n\\nEffectiveness of each module in ABD: Table 3 demonstrates the effectiveness of each module in ABD by progressively adding ABD-R and ABD-I. It can be observed that the introduction of input perturbation (IP) decreases the performance of the baseline, indicating that the mixed perturbation easily leads to out-of-control. Incorporating ABD-R and ABD-I brings increased performance. By incorporating both ABD-R and ABD-I into the baseline, the model has the best result, reaching 88.52% DSC and surpassing 86.40% DSC and 8.60% ASD.\"}"}
{"id": "CVPR-2024-115", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Visualization of segmentation results on ACDC dataset with 10% labeled data. (a) Ground-truth. (b) Cross Teaching results. (c) Ours-ABD (Cross Teaching) results. (d) BCP results. (e) Ours-ABD (BCP) results. Best viewed in color on the screen.\\n\\nTable 3. Effectiveness of ABD-R and ABD-I modules. \u201cbase\u201d means baseline is the Cross Teaching. \u201cIP\u201d means adding perturbation to the input.\\n\\n|       | Base | IP  | ABD-R | ABD-I |\\n|-------|------|-----|-------|-------|\\n| DSC   |      | 86.45 | 86.25 | 87.42 | 87.20 |\\n| Jaccard |      | 77.02 | 76.69 | 78.37 | 78.07 |\\n| 95HD  |      | 6.30  | 5.44  | 5.23  | 6.06  |\\n| ASD   |      | 1.86  | 1.72  | 1.68  | 1.96  |\\n| \u2713     | \u2713    | \u2713    | \u2713     | \u2713     | \u2713     |\\n\\nThe baseline is improved by 2.07%. The improvement indicates that the two modules are complementary which is consistent with the design targets. ABD-R enhances the upper limit of consistency learning for mixed perturbations, and ABD-I enables the model to learn potentially uncontrollable regions, thereby the combination can learn more semantics.\\n\\nTable 4. Influence of the displacement strategies in ABD-R.\\n\\n| Strategy       | DSC   | Jaccard | 95HD  | ASD   |\\n|----------------|-------|---------|-------|-------|\\n| Random         | 86.55 | 77.07   | 6.13  | 1.74  |\\n| Same           | 87.22 | 78.04   | 5.61  | 1.50  |\\n| Same+Reliable  | 87.38 | 78.06   | 4.56  | 1.69  |\\n| Reliable       | 87.42 | 78.37   | 5.23  | 1.68  |\\n\\nTable 5. Ablation study of patch number $K$.\\n\\n| $K$ | DSC   | Jaccard | 95HD  | ASD   |\\n|-----|-------|---------|-------|-------|\\n| 4   | 87.39 | 78.35   | 5.06  | 1.69  |\\n| 16  | 88.52 | 79.97   | 5.06  | 1.43  |\\n| 64  | 87.54 | 78.57   | 5.52  | 1.88  |\\n\\nTable 6. Effectiveness of the strong data augmentation.\\n\\n| Cutout Colorjitter Blur | DSC   | Jaccard | 95HD  | ASD   |\\n|-------------------------|-------|---------|-------|-------|\\n| \u2713                       | 88.23 | 79.53   | 5.90  | 1.40  |\\n| \u2713                       | 88.03 | 79.34   | 7.15  | 1.76  |\\n| \u2713                       | 87.76 | 78.76   | 7.28  | 1.61  |\\n| \u2713 \u2713                     | 88.52 | 79.97   | 5.06  | 1.43  |\\n| \u2713 \u2713 \u2713                   | 87.83 | 79.02   | 6.14  | 1.87  |\\n\\nChoice of Strong Augmentation: Table 6 illustrates the influence of three different augmentation methods: colorjitter, blur, and cutout. Through comparison, combining colorjitter and cutout produces better performance.\\n\\n5. Conclusion\\n\\nIn this paper, we proposed an adaptive bidirectional displacement (ABD) for semi-supervised medical image segmentation. Our key idea is to mitigate the constraints of mixed perturbations on consistency learning, thereby enhancing the upper limit of consistency learning. To achieve this, we designed two novel modules in our ABD: an ABD-R module reduces the uncontrolled regions in unlabeled samples and captures comprehensive semantic information from input perturbations, and an ABD-I module enhances the learning capacity to uncontrollable regions in labeled samples to compensate for the deficiencies of ABD-R. With the cooperation of two modules, our method achieves state-of-the-art performance and is easily embedded into different methods. In the future, we will design a patch adaptive displacement strategy to tackle more complicated cases.\\n\\nAcknowledgment: This work was supported by National Natural Science Foundation of China (No. 62301613), the Taishan Scholar Program of Shandong (No. tsqn202306130), the Shandong Natural Science Foundation (No. ZR2023QF046), Qingdao Postdoctoral Applied Research Project (No. QDBSH20230102091) and Independent Innovation Research Project of China University of Petroleum (East China) (No. 22CX06060A).\"}"}
{"id": "CVPR-2024-115", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Wenjia Bai, Ozan Oktay, Matthew Sinclair, Hideaki Suzuki, Martin Rajchl, Giacomo Tarroni, Ben Glocker, Andrew King, Paul M Matthews, and Daniel Rueckert. Semi-supervised learning for network-based cardiac mr image segmentation. In Medical Image Computing and Computer-Assisted Intervention, pages 253\u2013260. Springer, 2017.\\n\\n[2] Yunhao Bai, Duowen Chen, Qingli Li, Wei Shen, and Yan Wang. Bidirectional copy-paste for semi-supervised medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11514\u201311524, 2023.\\n\\n[3] Hritam Basak and Zhaozheng Yin. Pseudo-label guided contrastive learning for semi-supervised medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19786\u201319797, 2023.\\n\\n[4] Olivier Bernard, Alain Lalande, Clement Zotti, Frederick Cervenansky, Xin Yang, Pheng-Ann Heng, Irem Cetin, Karim Lekadir, Oscar Camara, Miguel Angel Gonzalez Ballester, et al. Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem solved? IEEE transactions on medical imaging, 37(11):2514\u20132525, 2018.\\n\\n[5] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural information processing systems, 32, 2019.\\n\\n[6] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang. Swin-unet: Unet-like pure transformer for medical image segmentation. In European conference on computer vision, pages 205\u2013218. Springer, 2022.\\n\\n[7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.\\n\\n[8] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong Wang. Semi-supervised semantic segmentation with cross pseudo supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2613\u20132622, 2021.\\n\\n[9] Jie Du, Xiaoci Zhang, Peng Liu, and Tianfu Wang. Coarse-refined consistency learning using pixel-level features for semi-supervised medical image segmentation. IEEE Journal of Biomedical and Health Informatics, 2023.\\n\\n[10] Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei Huang, Luo Si, et al. Galaxy: A generative pre-trained model for task-oriented dialog with semi-supervised learning and explicit policy injection. In Proceedings of the AAAI conference on artificial intelligence, number 10, pages 10749\u201310757, 2022.\\n\\n[11] Wei Huang, Chang Chen, Zhiwei Xiong, Yueyi Zhang, Xiaoyan Sun, and Feng Wu. Semi-supervised neuron segmentation via reinforced consistency learning. IEEE Transactions on Medical Imaging, 41(11):3016\u20133028, 2022.\\n\\n[12] Rushi Jiao, Yichi Zhang, Le Ding, Rong Cai, and Jicong Zhang. Learning with limited annotations: a survey on deep semi-supervised learning for medical image segmentation. arXiv preprint arXiv:2207.14191, 2022.\\n\\n[13] Zhanghan Ke, Di Qiu, Kaican Li, Qiong Yan, and Rynson WH Lau. Guided collaborative training for pixel-wise semi-supervised learning. In Computer Vision\u2013ECCV, pages 429\u2013445. Springer, 2020.\\n\\n[14] Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathematical statistics, 22(1):79\u201386, 1951.\\n\\n[15] Xin Lai, Zhuotao Tian, Li Jiang, Shu Liu, Hengshuang Zhao, Liwei Wang, and Jiaya Jia. Semi-supervised semantic segmentation with directional context-aware consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1205\u20131214, 2021.\\n\\n[16] Xiaomeng Li, Lequan Yu, Hao Chen, Chi-Wing Fu, Lei Xing, and Pheng-Ann Heng. Transformation-consistent self-ensembling model for semisupervised medical image segmentation. IEEE Transactions on Neural Networks and Learning Systems, 32(2):523\u2013534, 2020.\\n\\n[17] Yanwen Li, Luyang Luo, Huangjing Lin, Hao Chen, and Pheng-Ann Heng. Dual-consistency semi-supervised learning with uncertainty quantification for covid-19 lesion segmentation from ct images. In Medical Image Computing and Computer Assisted Intervention, pages 199\u2013209. Springer, 2021.\\n\\n[18] Chen Liang, Wenguan Wang, Jiaxu Miao, and Yi Yang. Logic-induced diagnostic reasoning for semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16197\u201316208, 2023.\\n\\n[19] Geert Litjens, Robert Toth, Wendy Van De Ven, Caroline Hoeks, Sjoerd Kerkstra, Bram Van Ginneken, Graham Vincent, Gwenael Guillard, Neil Birbeck, Jindang Zhang, et al. Evaluation of prostate segmentation algorithms for mri: the promise12 challenge. Medical image analysis, 18(2):359\u2013373, 2014.\\n\\n[20] Jinhua Liu, Christian Desrosiers, and Yuanfeng Zhou. Semi-supervised medical image segmentation using cross-model pseudo-supervision with shape awareness and local context constraints. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 140\u2013150. Springer, 2022.\\n\\n[21] Xiangde Luo, Jieneng Chen, Tao Song, and Guotai Wang. Semi-supervised medical image segmentation through dual-task consistency. In Proceedings of the AAAI conference on artificial intelligence, number 10, pages 8801\u20138809, 2021.\\n\\n[22] Xiangde Luo, Wenjun Liao, Jieneng Chen, Tao Song, Yinan Chen, Shichuan Zhang, Nianyong Chen, Guotai Wang, and Shaoting Zhang. Efficient semi-supervised gross target volume of nasopharyngeal carcinoma segmentation via uncertainty rectified pyramid consistency. In Medical Image Computing and Computer Assisted Intervention, pages 318\u2013329. Springer, 2021.\\n\\n[23] Xiangde Luo, Minhao Hu, Tao Song, Guotai Wang, and Shaoting Zhang. Semi-supervised medical image segmentation via cross teaching between cnn and transformer.\"}"}
{"id": "CVPR-2024-115", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[24] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565\u2013571. IEEE, 2016.\\n\\n[25] Yassine Ouali, C\u00e9line Hudelot, and Myriam Tami. Semi-supervised semantic segmentation with cross-consistency training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12674\u201312684, 2020.\\n\\n[26] Pengzhen Ren, Changlin Li, Hang Xu, Yi Zhu, Guangrun Wang, Jianzhuang Liu, Xiaojun Chang, and Xiaodan Liang. Viewco: Discovering text-supervised segmentation masks via multi-view semantic consistency. arXiv preprint arXiv:2302.10307, 2023.\\n\\n[27] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention, pages 234\u2013241. Springer, 2015.\\n\\n[28] Suman Sedai, Bhavna Antony, Ravneet Rai, Katie Jones, Hiroshi Ishikawa, Joel Schuman, Wollstein Gadi, and Rahil Garnavi. Uncertainty guided semi-supervised segmentation of retinal layers in OCT images. In Medical Image Computing and Computer Assisted Intervention, pages 282\u2013290. Springer, 2019.\\n\\n[29] Yucheng Shu, Hengbo Li, Bin Xiao, Xiuli Bi, and Weisheng Li. Cross-mix monitoring for medical image segmentation with limited supervision. IEEE Transactions on Multimedia.\\n\\n[30] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596\u2013608, 2020.\\n\\n[31] Nima Tajbakhsh, Laura Jeyaseelan, Qian Li, Jeffrey N Chiang, Zhihao Wu, and Xiaowei Ding. Embracing imperfect datasets: A review of deep learning solutions for medical image segmentation. Medical Image Analysis, 63:101693, 2020.\\n\\n[32] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.\\n\\n[33] Vikas Verma, Kenji Kawaguchi, Alex Lamb, Juho Kannala, Arno Solin, Yoshua Bengio, and David Lopez-Paz. Interpolation consistency training for semi-supervised learning. Neural Networks, 145:90\u2013106, 2022.\\n\\n[34] Yan Wang, Yuyin Zhou, Wei Shen, Seyoun Park, Elliot K Fishman, and Alan L Yuille. Abdominal multi-organ segmentation with organ-attention networks and statistical fusion. Medical Image Analysis, 55:88\u2013102, 2019.\\n\\n[35] Huisi Wu, Zhaoze Wang, Youyi Song, Lin Yang, and Jing Qin. Cross-patch dense contrastive learning for semi-supervised segmentation of cellular nuclei in histopathologic images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11666\u201311675, 2022.\\n\\n[36] Yicheng Wu, Minfeng Xu, Zongyuan Ge, Jianfei Cai, and Lei Zhang. Semi-supervised left atrium segmentation with mutual consistency training. In Medical Image Computing and Computer Assisted Intervention, pages 297\u2013306. Springer, 2021.\\n\\n[37] Yicheng Wu, Zhonghua Wu, Qianyi Wu, Zongyuan Ge, and Jianfei Cai. Exploring smoothness and class-separation for semi-supervised medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 34\u201343. Springer, 2022.\\n\\n[38] Mou-Cheng Xu, Yu-Kun Zhou, Chen Jin, Stefano B Blumberg, Frederick J Wilson, Marius deGroot, Daniel C Alexander, Neil P Oxtoby, and Joseph Jacob. Learning morphological feature perturbations for calibrated semi-supervised segmentation. In International Conference on Medical Imaging with Deep Learning, pages 1413\u20131429. PMLR, 2022.\\n\\n[39] Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao. St++: Make self-training work better for semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4268\u20134277, 2022.\\n\\n[40] Lihe Yang, Lei Qi, Litong Feng, Wayne Zhang, and Yinghuan Shi. Revisiting weak-to-strong consistency in semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7236\u20137246, 2023.\\n\\n[41] Xiaosu Yang, Jiya Tian, Yaping Wan, Mingzhi Chen, Lingna Chen, and Junxi Chen. Semi-supervised medical image segmentation via cross-guidance and feature-level consistency dual regularization schemes. Medical Physics, 2023.\\n\\n[42] Shuo Zhang, Jiaojiao Zhang, Biao Tian, Thomas Lukasiewicz, and Zhenghua Xu. Multi-modal contrastive mutual learning and pseudo-label re-learning for semi-supervised medical image segmentation. Medical Image Analysis, 83:102656, 2023.\\n\\n[43] Yichi Zhang, Rushi Jiao, Qingcheng Liao, Dongyang Li, and Jicong Zhang. Uncertainty-guided mutual consistency learning for semi-supervised medical image segmentation. Artificial Intelligence in Medicine, 138:102476, 2023.\\n\\n[44] Zhenxi Zhang, Ran Ran, Chunna Tian, Heng Zhou, Xin Li, Fan Yang, and Zhicheng Jiao. Self-aware and cross-sample prototypical learning for semi-supervised medical image segmentation. arXiv preprint arXiv:2305.16214, 2023.\\n\\n[45] Xiangyu Zhao, Zengxin Qi, Sheng Wang, Qian Wang, Xuehai Wu, Ying Mao, and Lichi Zhang. Rcps: Rectified contrastive pseudo supervision for semi-supervised medical image segmentation. arXiv preprint arXiv:2301.05500, 2023.\\n\\n[46] Zhen Zhao, Lihe Yang, Sifan Long, Jimin Pi, Luping Zhou, and Jingdong Wang. Augmentation matters: A simple-yet-effective approach to semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}"}
{"id": "CVPR-2024-115", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[47] Ke Zheng, Junhai Xu, and Jianguo Wei. Double noise mean teacher self-ensembling model for semi-supervised tumor segmentation. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1446\u20131450. IEEE, 2022.\\n\\n[48] Mingkai Zheng, Shan You, Lang Huang, Fei Wang, Chen Qian, and Chang Xu. Simmatch: Semi-supervised learning with similarity matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14471\u201314481, 2022.\\n\\n[49] Kaiyang Zhou, Chen Change Loy, and Ziwei Liu. Semi-supervised domain generalization with stochastic stylematch. International Journal of Computer Vision, pages 1\u201311, 2023.\\n\\n[50] Qianyu Zhou, Zhengyang Feng, Qiqi Gu, Guangliang Cheng, Xuequan Lu, Jianping Shi, and Lizhuang Ma. Uncertainty-aware consistency regularization for cross-domain semantic segmentation. Computer Vision and Image Understanding, 221:103448, 2022.\\n\\n[51] Yuliang Zou, Zizhao Zhang, Han Zhang, Chun-Liang Li, Xiao Bian, Jia-Bin Huang, and Tomas Pfister. Pseudoseg: Designing pseudo labels for semantic segmentation. arXiv preprint arXiv:2010.09713, 2020.\"}"}
{"id": "CVPR-2024-115", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adaptive Bidirectional Displacement for Semi-Supervised Medical Image Segmentation\\n\\nHanyang Chi\\nJian Pang\\nBingfeng Zhang\\nWeifeng Liu\\n\\n1 China University of Petroleum (East China)\\n{chihanyang, jianpang}@s.upc.edu.cn, {bingfeng.zhang, liuwf}@upc.edu.cn\\n\\nAbstract\\n\\nConsistency learning is a central strategy to tackle unlaabelled data in semi-supervised medical image segmentation (SSMIS), which enforces the model to produce consistent predictions under the perturbation. However, most current approaches solely focus on utilizing a specific single perturbation, which can only cope with limited cases, while employing multiple perturbations simultaneously is hard to guarantee the quality of consistency learning. In this paper, we propose an Adaptive Bidirectional Displacement (ABD) approach to solve the above challenge. Specifically, we first design a bidirectional patch displacement based on reliable prediction confidence for unlabeled data to generate new samples, which can effectively suppress uncontrollable regions and still retain the influence of input perturbations. Meanwhile, to enforce the model to learn the potentially uncontrollable content, a bidirectional displacement operation with inverse confidence is proposed for the labeled images, which generates samples with more unreliable information to facilitate model learning. Extensive experiments show that ABD achieves new state-of-the-art performances for SSMIS, significantly improving different baselines. Source code is available at https://github.com/chy-upc/ABD.\\n\\n1. Introduction\\n\\nMedical image segmentation derives from computer tomography (CT) or magnetic resonance imaging (MRI), which is crucial for various clinical applications [34, 45]. Obtaining a large medical dataset with precise annotation to train segmentation models is challenging, as reliable annotations can only be provided by experts, which constrains the development of medical image segmentation algorithms and poses substantial challenges for further research and implementation [31, 43]. To mitigate the burden of manual annotation and address these challenges, semi-supervised medical image segmentation (SSMIS) [2, 28, 35, 42] is emerging as a practical approach to encourage segmentation models to learn from readily available unlabelled data in conjunction with limited labeled examples.\\n\\nMost recent approaches in SSMIS employ consistency learning [9, 11, 15, 44] to make the decision boundary of the learned model located within the low-density boundary [12]. By ensuring consistent features or predictions under diverse perturbations, this strategy becomes one of the most effective solutions for learning from unlabelled data. According to the perturbation differences, consistency learning approaches can be divided into three categories: 1) Input perturbations, which mainly produce different inputs to the same model [5, 29, 41], e.g., weak and strong data augmentation for the given image. 2) Feature perturbations [17, 25, 47], which mainly include feature noise, feature dropout, and context masking. 3) Network perturbations [8, 13], which focus on using different network architectures.\"}"}
{"id": "CVPR-2024-115", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tectures for the same input. To ensure the stability of consistency learning, most previous approaches [16, 32, 38, 51] solely utilize one of the above perturbations, restricting the performance of the consistency learning and leading to imprecise decision boundary since the specific single perturbation can only handle limited cases, as shown in Fig. 1(a).\\n\\nUtilizing mixed or multiple perturbations presents a direct solution to solve the above problem. However, once added multiple perturbations, the consistency learning process is easily out-of-control, leading to restricted learning quality. For example, Cross Pseudo Supervision (CPS) [8] is a widely-used technique in consistency learning that primarily applies network perturbation to produce two discriminate predictions for further consistency learning. If we directly add input perturbation by using weak and strong augmentation to the input training data, consistency learning will be ineffective. As shown in Fig. 1(b), when the original input is replaced with weak and strong augmentation inputs, the CPS model incorrectly classifies the background as the foreground with consistency learning mechanism, indicating decreased performance when mixed perturbations are introduced.\\n\\nTo tackle the aforementioned challenges, we propose Adaptive Bidirectional Displacement (ABD) for SSMIS, as shown in Fig. 1(c). Specifically, for each unlabelled image, the input perturbation is firstly applied to produce a pair of input images, e.g., weak augmentation image and strong augmentation image. Then we generate two confidence matrices (confidence rank maps) based on the predictions from the above two perturbed images. The confidence matrix assesses the certainty of predicted pixels belonging to various categories, thereby reflecting the model's reliability to different perturbations. Then, for any augmented image, its region with the lowest confidence rank is displaced with the region from the other augmented image that has the most similar output distribution with highly confident scores. We refer to this as an adaptive bidirectional displacement with reliable confidence (ABD-R). In this way, the newly generated image can remove the uncontrolled region and obtain complementary and approximate semantic information from another augmented image, which ensures consistent predictions of the model across different perturbations. Meanwhile, to enforce the model to learn those potentially uncontrollable regions, we incorporate inverse confidence for labeled data as an additional adaptive bidirectional displacement (ABD-I). For any labeled augmented image, the image regions with the highest confidence scores are displaced with the regions from another augmented image having the lowest confidence scores. This operation will strengthen the model to tackle uncontrollable regions.\\n\\nCombining these two strategies, our approach performs a novel input perturbation method, which can be directly applied to existing consistency learning approaches. Extensive experiments show that ABD achieves new state-of-the-art (SOTA) performances for SSMIS, significantly improving different baseline performances.\\n\\nWe summarize our main contributions as follows:\\n\\n\u2022 We observed that the combination of different perturbations leads to instability in consistency learning. To address this issue, we propose Adaptive Bidirectional Displacement to enable the generation of semantically complementary data by replacing model inadaptable regions with credible regions, which assists the model in effectively correcting erroneous predictions and enhances the quality of consistency learning.\\n\\n\u2022 To take full advantage of labeled data, we propose an enhanced Adaptive Bidirectional Displacement that incorporates inverse confidence for labeled data to enforce the model to tackle those potentially uncontrollable regions.\\n\\n\u2022 The proposed method can be easily plug-and-play, which can be embedded into different approaches and enhance their performance. Extensive experiments are conducted to validate the feasibility of the method, resulting in significant improvements compared to previous SOTA approaches on different datasets.\\n\\n2. Related Work\\n\\n2.1. Consistency Learning in Semi-Supervised Medical Image Segmentation\\n\\nConsistency learning has been widely used in recent approaches for SSMIS. It aims to improve the performance of models by promoting consistent predictions for unlabeled data under different perturbations. According to the perturbation differences, consistency learning has three categories: input perturbation, feature perturbation, and network perturbation. Input perturbation is achieved through producing different inputs. For instance, Huang et al. [11] introduced cutout content loss and slice misalignment as perturbations in the input. In contrast, ST++ [39] involved strong augmentation on unlabeled images and directly utilizes the augmented samples for re-training. PseudoSeg [51] was similar to FixMatch [30] in leveraging pseudo-labels generated from weakly augmented images to supervise the predictions of strongly augmented images. BCP [2] encouraged the mixing of labeled and unlabeled images on the input level, enabling unlabeled data to learn comprehensive and general semantic information from labeled data in both directions. Meanwhile, feature perturbation and network perturbation are achieved by producing different features or outputs for the same input. Specifically, for feature perturbation, Mismatch [38] introduced morphological feature perturbation, which is based on classic morphological operations. For network perturbation, CPS [8] encouraged consistent predictions from different initialized networks with an input image. Mean Teacher [32] utilized\"}"}
{"id": "CVPR-2024-115", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overview of our adaptive bidirectional displacement framework. (A) For the unlabeled data, one image is subjected to weak and strong augmentations, resulting in two images that are separately input to two networks for cross-supervision. Then, based on the Anderson's patches in the images are bidirectionally displaced, resulting in the formation of new samples \\\\( X_{us} \\\\rightarrow w \\\\) and \\\\( X_{uw} \\\\rightarrow s \\\\). These new samples are further fed into the networks for cross-supervision. (B) For the labeled images, they are also subjected to both weak and strong augmentations, and their predictions are supervised by the labels. Afterward, based on the Anderson's inverse bidirectional patch displacement is performed on the images, resulting in the generation of new samples \\\\( X_{ls} \\\\rightarrow w \\\\) and \\\\( X_{lw} \\\\rightarrow s \\\\). Similarly, the labels undergo the same operation, leading to the creation of new labels \\\\( Y_{ls} \\\\rightarrow w \\\\) and \\\\( Y_{lw} \\\\rightarrow s \\\\). The new samples are then fed into the network, and their predictions are supervised by the new labels. Note that ABD-R and ABD-I are two parallel modules during training.\"}"}
{"id": "CVPR-2024-115", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Overview\\n\\nThe overall framework of our approach is shown in Fig. 2, which can be divided into the following steps:\\n\\n1. For the input labeled image, we first apply input perturbation to generate two different samples, e.g., a weak augmentation image and a strong augmentation image, both of which are input to the model with network perturbation: the weak augmentation image is input to one network and the strong augmentation image is input to the other network to generate two corresponding predictions, using the provided label as supervision.\\n\\n2. For an unlabeled image, two predictions are obtained following the above pipeline. Then using our proposed ABD-R, the corresponding confidence matrices are used to perform bidirectional displacement between two augmented unlabeled images to generate new input samples.\\n\\n3. Meanwhile, to enforce learning from the potentially uncontrollable regions, the ABD-I strategy based on inverse confidence is applied to labeled data to generate two new samples with the corresponding labels.\\n\\n4. Finally, all generated new samples are input to the model to produce the corresponding predictions. The predictions of unlabeled data are used for cross-supervision, and the predictions of labeled data are supervised by the newly generated labels.\\n\\n3.3. Adaptive Bidirectional Displacement with Reliable Confidence\\n\\nUnder various perturbations, the segmentation model produces unreliable predictions for unlabeled data. Imposing alignment using these predictions can render consistent learning ineffective. A direct solution is to remove the regions related to unreliable prediction. In practice, we first generate the confidence matrix based on the model's predictions. The confidence matrix measures the confidence of each predicted pixel belonging to different categories, it reflects the model's reliability to different perturbations. Utilizing the confidence matrix, our ABD-R can generate a new training sample that exhibits more reliable regions.\\n\\nSuppose $X_{uw} \\\\in \\\\mathbb{R}^{3 \\\\times H \\\\times W}$ is an unlabeled medical image with weak augmentation. $X_{us} \\\\in \\\\mathbb{R}^{3 \\\\times H \\\\times W}$ is the same unlabeled medical image but with strong augmentation. After passing the model, we generate their corresponding outputs:\\n\\n$$\\\\text{logits}_{uw} = f_{\\\\theta_1}(X_{uw}), \\\\text{logits}_{us} = f_{\\\\theta_2}(X_{us})$$\\n\\n(1)\\n\\nwhere $f_{\\\\theta_1}$ and $f_{\\\\theta_2}$ are two networks, which usually have different architecture or initialization to build the network perturbation. \\\\(\\\\text{logits}_{uw}\\\\) and \\\\(\\\\text{logits}_{us}\\\\) are the logits outputs that correspond to $X_{uw}$ and $X_{us}$, respectively. Applying softmax to these logits yields the corresponding prediction probability scores:\\n\\n$$P_{uw} = \\\\text{softmax}(\\\\text{logits}_{uw}), P_{us} = \\\\text{softmax}(\\\\text{logits}_{us})$$\\n\\n(2)\\n\\nwhere $P_{uw}$ is the prediction of $X_{uw}$ and $P_{us}$ is the prediction of $X_{us}$. After upsampling them to the same height and width with input, we can generate $P_{uw} \\\\in \\\\mathbb{R}^{C \\\\times H \\\\times W}$ and $P_{us} \\\\in \\\\mathbb{R}^{C \\\\times H \\\\times W}$. $C$ is the number of classes.\\n\\nThen we divide $X_{uw}$ into $K$ patches, each with a size of $k \\\\times k$, denoting as $X_{uw} = \\\\{X_{u,jw}\\\\}_{K,j=1}$. where $X_{u,jw} \\\\in \\\\mathbb{R}^{k \\\\times k}$ and $K = \\\\frac{H}{k} \\\\times \\\\frac{W}{k}$. For any patch $X_{u,jw}$ ($j$ is the index of the patch), the corresponding logits scores is:\\n\\n$$Z_{u,jw,c} = \\\\frac{1}{k \\\\times k} \\\\sum_{m=1}^{k \\\\times k} \\\\text{logits}_{u,jw,c}(m)$$\\n\\n(3)\\n\\nwhere $Z_{u,jw,c}$ represents the average logits score of the $j$-th patch for the class $c$ and $c \\\\in \\\\{1, 2, \\\\ldots, C\\\\}$, $\\\\text{logits}_{u,jw,c}(m)$ is the logit score of the class $c$ for the $m$-th pixel in the $j$-th patch. We regard $Z_{u,jw}$ as the output distribution for the $j$-th patch of $X_{uw}$.\\n\\nMeanwhile, the corresponding confidence score for the $j$-th patch is computed as follows:\\n\\n$$A_{u,jw} = \\\\frac{1}{k \\\\times k} \\\\max_{c \\\\in C}(P_{u,jw,c}(m))$$\\n\\n(4)\\n\\nwhere $j$ is the index of the corresponding patch, $P_{u,jw,c}(m)$ is the probability of the class $c$ for the $m$-th pixel in the patch. $\\\\max(\\\\cdot)$ is a maximum operator to select the highest confidence score for each pixel. $A_{u,jw}$ is the average confidence score for the $j$-th patch, which measures the reliability of the corresponding patch.\\n\\nThen the index of the patch with the lowest confidence for $X_{uw}$ is computed as:\\n\\n$$\\\\text{ind}_{uw-min} = \\\\arg\\\\min_{j \\\\in K} A_{u,jw}$$\\n\\n(5)\\n\\nSimultaneously, using the confidence map $A_{uw}$, the top $n$ highest confidence patches in $X_{uw}$ are selected, and the corresponding index set is represented as:\\n\\n$$\\\\text{Ind}_{uw-top} = \\\\{\\\\text{ind}_{uw-max}^1, \\\\text{ind}_{uw-max}^2, \\\\ldots, \\\\text{ind}_{uw-max}^n\\\\}$$\\n\\n(6)\\n\\nwhere $\\\\text{Ind}_{uw-top}$ is an index set that includes the indices of the selected top $n$ highest confidence patches. $\\\\text{ind}_{uw-max}^1$ to $\\\\text{ind}_{uw-max}^n$.\"}"}
