{"id": "CVPR-2022-1196", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Emre Aksan, Manuel Kaufmann, Peng Cao, and Otmar Hilliges, \u201cA Spatio-temporal Transformer for 3D Human Motion Prediction,\u201d in 2021 Int. Conf. on 3D Vision (3DV), 2021.\\n\\n[2] Sadegh Aliakbarian, Fatemeh Saleh, Lars Petersson, Stephen Gould, et al., \u201cContextually Plausible and Diverse 3D Human Motion Prediction,\u201d in 2021 IEEE/CVF Int. Conf. on Computer Vision (ICCV), 2021.\\n\\n[3] Timothy D Barfoot, State Estimation for Robotics. 2021.\\n\\n[4] Emad Barsoum, John Kender, and Zicheng Liu, \u201cHP-GAN: Probabilistic 3D Human Motion Prediction via GAN,\u201d in 2018 IEEE/CVF Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW), 2018.\\n\\n[5] Apratim Bhattacharyya, Bernt Schiele, and Mario Fritz, \u201cAccurate and Diverse Sampling of Sequences Based on a \u201cBest of Many\u201d Sample Objective,\u201d in 2018 IEEE/CVF Conf. on Computer Vision and Pattern Recognition, 2018.\\n\\n[6] Christopher Bingham, \u201cAn antipodally symmetric distribution on the sphere,\u201d The Annals of Statistics, 1974.\\n\\n[7] Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, et al., \u201cPyro: Deep Universal Probabilistic Programming,\u201d Journal of Machine Learning Research, 2018.\\n\\n[8] Christopher M Bishop, \u201cMixture Density Networks,\u201d Tech. Rep., 1994.\\n\\n[9] Nat Dilokthanakul, Pedro A. M. Mediano, Marta Garnelo, Matthew C. H. Lee, et al., \u201cDeep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders,\u201d in Int. Conf. on Learning Representations, 2017.\\n\\n[10] Ronald Fisher, \u201cDispersion on a Sphere,\u201d Proceedings of the Royal Society of London. Series A, 1953.\\n\\n[11] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Jitendra Malik, \u201cRecurrent Network Models for Human Dynamics,\u201d in 2015 IEEE Int. Conf. on Computer Vision (ICCV), 2015.\\n\\n[12] Partha Ghosh, Jie Song, Emre Aksan, and Otmar Hilliges, \u201cLearning Human Motion Models for Long-Term Predictions,\u201d in 2017 Int. Conf. on 3D Vision (3DV), 2017.\\n\\n[13] Igor Gilitschenski, Gerhard Kurz, Simon J. Julier, and Uwe D. Hanebeck, \u201cEfficient Bingham filtering based on saddlepoint approximations,\u201d in 2014 Int. Conf. on Multisensor Fusion and Information Integration for Intelligent Systems (MFI), 2014.\\n\\n[14] Igor Gilitschenski, Roshni Sahoo, Wilko Schwarting, Alexander Amini, et al., \u201cDeep Orientation Uncertainty Learning Based on a Bingham Loss,\u201d in Int. Conf. on Learning Representations (ICLR), 2020.\\n\\n[15] Jared Glover, \u201cThe Quaternion Bingham Distribution, Detection, and Dynamic Manipulation,\u201d Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge, 2014.\\n\\n[16] F Sebastian Grassia, \u201cPractical Parameterization of Rotations using the Exponential Map,\u201d Tech. Rep., 1998.\\n\\n[17] Liang-Yan Gui, Yu-Xiong Wang, Xiaodan Liang, and Jos\u00e9 M. F. Moura, \u201cAdversarial Geometry-Aware Human Motion Prediction,\u201d in Proceedings of the European Conf. on Computer Vision (ECCV), 2018.\\n\\n[18] Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and R. Venkatesh Babu, \u201cDeLiGAN: Generative Adversarial Networks for Diverse and Limited Data,\u201d in 2017 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\n[19] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, et al., \u201c-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,\u201d in Int. Conf. on Learning Representations, 2016.\\n\\n[20] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu, \u201cHuman3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments,\u201d IEEE Trans. on Pattern Analysis and Machine Intelligence, 2014.\\n\\n[21] Boris Ivanovic and Marco Pavone, \u201cThe Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatio-temporal Graphs,\u201d in 2019 IEEE/CVF Int. Conf. on Computer Vision (ICCV), 2019.\\n\\n[22] Boris Ivanovic, Edward Schmerling, Karen Leung, and Marco Pavone, \u201cGenerative Modeling of Multimodal Multi-Human Behavior,\u201d in IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2018.\\n\\n[23] Ashesh Jain, Amir R. Zamir, Silvio Savarese, and Ashutosh Saxena, \u201cStructural-RNN: Deep Learning on Spatio-Temporal Graphs,\u201d in 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\n[24] Eric Jang, Shixiang Gu, and Ben Poole, \u201cCategorical Reparameterization with Gumbel-Softmax,\u201d in Int. Conf. on Learning Representations, 2017.\\n\\n[25] Diederik P Kingma and Max Welling, \u201cAuto-Encoding Variational Bayes,\u201d in arXiv preprint, 2013.\\n\\n[26] Thomas N. Kipf and Max Welling, \u201cSemi-Supervised Classification with Graph Convolutional Networks,\u201d in Int. Conf. on Learning Representations, 2017.\\n\\n[27] Vineet Kosaraju, Amir Sadeghian, Roberto Mart\u00edn-Mart\u00ednez, Ian Reid, et al., \u201cSocial-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks,\u201d in Advances in Neural Information Processing Systems, 2019.\\n\\n[28] Jogendra Nath Kundu, Maharshi Gor, and R. Venkatesh Babu, \u201cBiHMP-GAN: Bidirectional 3D Human Motion Prediction GAN,\u201d in AAAI Conf. on Artificial Intelligence, 2018.\\n\\n[29] Gerhard Kurz, Igor Gilitschenski, and Uwe D Hanebeck, \u201cEfficient Evaluation of the Probability Density Function of a Wrapped Normal Distribution,\u201d in IEEE ISIF Workshop on Sensor Data Fusion, 2014.\\n\\n[30] Gerhard Kurz, Igor Gilitschenski, Florian Pfaff, Lukas Drude, et al., \u201cStatistics and Filtering Using libDirectional,\u201d Journal of Statistical Software Directional, 2017.\\n\\n[31] Muriel Lang, \u201cApproximation of probability density functions on the Euclidean group parametrized by dual quaternions,\u201d Ph.D. dissertation.\\n\\n[32] Andreas M. Lehrmann, Peter V. Gehler, and Sebastian Nowozin, \u201cEfficient Nonlinear Markov Models for Human Motion,\u201d in 2014 IEEE Conf. on Computer Vision and Pattern Recognition, 2014.\\n\\n[33] Maosen Li, Siheng Chen, Yangheng Zhao, Ya Zhang, et al., \u201cDynamic Multiscale Graph Neural Networks for 3D Skeleton Based Human Motion Prediction,\u201d in 2020 IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[34] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, et al., \u201cAMASS: Archive of Motion Capture as Surface Shapes,\u201d in Int. Conf. on Computer Vision, 2019.\\n\\n[35] Wei Mao, Miaomiao Liu, and Mathieu Salzmann, \u201cHistory Repeats Itself: Human Motion Prediction via Motion Attention,\u201d in European Conf. on Computer Vision, 2020.\\n\\n[36] Wei Mao, Miaomiao Liu, Mathieu Salzmann, and Hongdong Li, \u201cLearning Trajectory Dependencies for Human Motion Prediction,\u201d in 2019 IEEE/CVF Int. Conf. on Computer Vision (ICCV), 2019.\\n\\n[37] F. Landis Markley, Yang Cheng, John Crassidis, and Yaakov Oshman, \u201cAveraging Quaternions,\u201d in Journal of Guidance, Control, and Dynamics, 2007.\\n\\n[38] Julieta Martinez, Michael J. Black, and Javier Romero, \u201cOn Human Motion Prediction Using Recurrent Neural Networks,\u201d in 2017 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\n[39] Emanuel Parzen, \u201cOn Estimation of a Probability Density Function and Mode,\u201d Ann. Math. Statist., 1962.\\n\\n[40] Dario Pavllo, Christoph Feichtenhofer, Michael Auli, and David Grangier, \u201cModeling Human Motion with Quaternion-based Neural Networks,\u201d Int. Journal of Computer Vision, 2019.\"}"}
{"id": "CVPR-2022-1196", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Vladimir Pavlovic, James M. Rehg, and John Maccormick, \u201cLearning Switching Linear Models of Human Motion,\u201d in Advances in Neural Information Processing Systems, 2000.\\n\\nValentin Peretroukhin, Matthew Giamou, David M. Rosen, W. Nicholas Greene, et al., \u201cA Smooth Representation of Belief over SO(3) for Deep Rotation Learning with Uncertainty,\u201d in Proceedings of Robotics: Science and Systems (RSS), 2020.\\n\\nTim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone, \u201cTrajectron++: Dynamically-Feasible Trajectory Forecasting with Heterogeneous Data,\u201d in European Conf. on Computer Vision (ECCV), 2020.\\n\\nYichuan Charlie Tang and Ruslan Salakhutdinov, \u201cMultiple Futures Prediction,\u201d in Advances in neural information processing systems, 2019.\\n\\nYongyi Tang, Lin Ma, Wei Liu, and Wei-Shi Zheng, \u201cLong-Term Human Motion Prediction by Modeling Motion Context and Enhancing Motion Dynamics,\u201d in Int. Joint Conf. on Artificial Intelligence, 2018.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, et al., \u201cAttention Is All You Need,\u201d in Advances in Neural Information Processing Systems, 2017.\\n\\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, et al., \u201cGraph Attention Networks,\u201d in Int. Conf. on Learning Representations, 2017.\\n\\nJacob Walker, Kenneth Marino, Abhinav Gupta, and Martial Hebert, \u201cThe Pose Knows: Video Forecasting by Generating Pose Futures,\u201d in Int. Conf. on Computer Vision, 2017.\\n\\nJack M. Wang, David J. Fleet, and Aaron Hertzmann, \u201cGaussian process dynamical models for human motion,\u201d IEEE Trans. on Pattern Analysis and Machine Intelligence, 2008.\\n\\nTianlong Wu, Feng Chen, and Yun Wan, \u201cGraph Attention LSTM Network: A New Model for Traffic Flow Forecasting,\u201d in 2018 5th Int. Conf. on Information Science and Control Engineering (ICISCE), 2018.\\n\\nXinchen Yan, Akash Rastogi, Ruben Villegas, Kalyan Sunkavalli, et al., \u201cMT-V AE: Learning Motion Transformations to Generate Multimodal Human Dynamics,\u201d in European Conf. on Computer Vision, 2018.\\n\\nYe Yuan and Kris Kitani, \u201cDiverse Trajectory Forecasting with Determinantal Point Processes,\u201d in Int. Conf. on Learning Representations, 2020.\\n\\nYe Yuan and Kris Kitani, \u201cDLow: Diversifying Latent Flows for Diverse Human Motion Prediction,\u201d in Proceedings of the European Conf. on Computer Vision (ECCV), 2020.\\n\\nYan Zhang, Michael J. Black, and Siyu Tang, \u201cWe are More than Our Joints: Predicting how 3D Bodies Move,\u201d in Proceedings of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, 2021.\\n\\nJie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, et al., \u201cGraph Neural Networks: A Review of Methods and Applications,\u201d in AI Open, 2020.\"}"}
{"id": "CVPR-2022-1196", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Sampled: The model's sampled output, where \\\\( z \\\\) and \\\\( y \\\\) are sampled sequentially according to \\n\\\\[\\n  z \\\\sim p(\\\\cdot | x), \\\\quad y \\\\sim p(\\\\cdot | x, z)\\n\\\\]\\n\\n3. Most Likely Mode (ML-Mode): The model's deterministic and most-likely single output. The high-level latent behavior mode and output trajectory are the modes of their respective distributions, where \\n\\\\[\\n  z_{\\\\text{mode}} = \\\\arg \\\\max_z p(\\\\cdot | x), \\\\quad y = \\\\arg \\\\max_y p(\\\\cdot | x, z_{\\\\text{mode}})\\n\\\\]\\n\\n4. Weighted Mean (W-Mean): The mean of all latent modes weighted by their probability. Following \\\\[37\\\\], it is given as the normalized largest eigenvector of \\n\\\\[\\n  QQ^T\\n\\\\]\\nwhere \\\\( Q \\\\) is a matrix of stacked quaternion column vectors, where \\n\\\\[\\n  q_n = \\\\arg \\\\max_y p(\\\\cdot | x, z = n), \\\\quad Q = [\\\\cdots q_n \\\\cdots]\\n\\\\]\\n\\n6. Experiments\\nWhile desiderata (III) and (V) in Sec. 1 are explicitly fulfilled by the embedded human structure and the use of \\\\( \\\\mathbb{S}^\\\\mathbb{O} \\\\odot \\\\mathbb{R}^3 \\\\) as output distribution, we conduct both quantitative and qualitative experiments to show that our method also succeeds in the remaining desiderata.\\n\\nWe, therefore, structure our experiments as follows:\\nFirst, we show that Motron performs best in capturing humans' probabilistic and diverse nature (II) by introducing a new probabilistic metric to the field of human motion prediction. Secondly, we highlight the learned multimodal structure (I) by evaluating deterministic outputs of high likelihood modes. Later we also give a visualization of how these modes manifest in distinct motions. Finally, we show that our approach can handle incomplete data (IV) in the form of occluded joints.\\n\\nDatasets.\\nWe provide quantitative experimentation results on two datasets; namely the Human 3.6 Million (H3.6M) \\\\[20\\\\] dataset and the Archive of Motion Capture as Surface Shapes (AMASS) \\\\[34\\\\]. AMASS is a unified collection of 18 motion capture datasets totaling 13944 motion-sequences from 460 subjects performing a large variety of actions. In comparison, the H3.6M dataset consists of 240 sequences from 8 subjects performing 15 actions each.\\n\\n6.1. Probabilistic Evaluation\\nMetrics.\\nProbabilistic approaches have been compared on the basis of a variety of metrics in position space, most prominent are Best-of-N metrics where \\\\( N = 50 \\\\) motions are sampled from the model; the best metric value of these is reported. Such metrics, however, only present limited insights on a model's output distribution as only a single motion of an arbitrary number \\\\( N \\\\) is evaluated. To fully assess an algorithm's probabilistic capabilities, we propose an alternative evaluation methodology for probabilistic algorithms where we measure the ability to accurately capture and reproduce the underlying uncertainty distribution of motions. To this point we adopt the \\\\( \\\\text{KDE-NLL} \\\\) metric \\\\[21\\\\] to assess the method's Negative Log-Likelihood (NLL) by fitting a probability distribution, using Kernel Density Estimate (KDE) \\\\[39\\\\], to output samples. Although Motron can compute its own log-likelihood, we apply the same evaluation methodology to maintain a directly comparable performance measure. As the NLL is unbounded, we clip it to a maximum value of \\\\( 20 \\\\) (\\\\( \\\\llap{\\\\sim} 2 \\\\times 10^{-7} \\\\% \\\\)) in order to prevent single outliers from dominating. Still, to be comparable, we additionally follow the evaluation methodology of Yuan et al. \\\\[53\\\\]: We report the Average Pairwise Distance (APD) as a measure of sample diversity as well as the Best-of-N metrics Average Displacement Error (ADE), and Final Displacement Error (FDE) as measures of quality. Further, we report their Multi-Modal ADE and FDE metrics (MMADE and MMFDE). \\\"Similar\\\" motions are grouped by using an arbitrary distance threshold at \\\\( t = 0 \\\\) and the average metric over all these grouped motions is reported. As close poses at a single instance can, however, belong to entirely different motions (Appendix C), the KDE-NLL reports a more holistic representation of a model's probabilistic capabilities.\\n\\nProbabilistic Baselines.\\nWe focus on the current state-of-the-art algorithm (1) DLow \\\\[53\\\\] to compare our desiderata side to side. DLow uses an adapted VAE algorithm to generate samples without collapsing to a single mode. For the standard probabilistic experiment methodology, we further report methods based on CVAE: (2) Pose-Knows \\\\[48\\\\] and (3) MT-VAE \\\\[51\\\\]; GAN based (4) DeLiGAN \\\\[18\\\\] and diversity promoting methods (5) Best-of-Many \\\\[5\\\\], (6) GM-VAE \\\\[9\\\\], and (7) DSF \\\\[52\\\\]. We were not able to compare to LCP-VAE \\\\[2\\\\] for a lack of open accessible source code.\\n\\nEvaluation Methodology.\\nIn order to compare to other probabilistic methods, which output motions solely in position space, we use the forward kinematic of the respective test subject to convert our joint configuration samples to joint positions. Predicting in configuration space and using human's forward kinematic to produce motions in position space, ensures our motions to be kinematic feasible. However, they bring a disadvantage compared to approaches directly outputting joints positions, common for probabilistic approaches, as they are not constrained by a rigid bone structure (Appendix D). For the KDE-NLL metric, we sample \\\\( N = 1000 \\\\) motions from each method and fit a KDE for each future timestep. For the \\\\( N = 50 \\\\) metrics (APD, ADE, FDE, MMADE, MMFDE) we use 50 motions, sampled from our intermediate output distribution over \\\\( \\\\dot{q} \\\\) and \\\"integrated\\\"; these motions are transformed into position space using the respective subjects forward kinematics.\\n\\nWe train the model on 0.5 seconds of history and predict a two second horizon. Generally, we can dynamically change the prediction horizon online thanks to the flexible decoder structure during inference.\\n\\nResults.\\nFig. 3 shows that Motron clearly outperforms the current state-of-the-art algorithm DLow in representing the underlying motion distribution of the human subject.\"}"}
{"id": "CVPR-2022-1196", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Negative Log Likelihood of the ground truth in the fitted KDE distribution of samples from each model. Lower is better. Samples from DLow are over-confidently wrong for early prediction timesteps.\\n\\n| Model         | APD     | ADE    | FDE    |\\n|---------------|---------|--------|--------|\\n| DSF           | 9.330   | 0.493  | 0.592  |\\n| DeLiGAN       | 6.509   | 0.483  | 0.534  |\\n| GMV AE        | 6.769   | 0.461  | 0.555  |\\n| Best-of-Many  | 6.265   | 0.448  | 0.533  |\\n| MT-V AE       | 0.403   | 0.457  | 0.595  |\\n| Pose-Knows    | 6.723   | 0.461  | 0.560  |\\n| Ours          | 3.453   | 0.252  | 0.350  |\\n\\nTable 1. Best of $N=5$ evaluation against probabilistic algorithms on a prediction horizon of one and two seconds on H3.6M dataset.\\n\\nProject and therefore having a lower NLL over all prediction timesteps. This result is supported by the Best-of-N metrics over one and two seconds in Table 1. We show significantly better results than DLow on ADE and FDE. Notably, this is achieved while using less diverse motion samples (lower APD) indicating that our samples are concentrated around likely motions. MMADE and MMFDE values for different thresholds are presented in Figure 6 in Appendix C. For smaller thresholds we outperform DLow while for higher thresholds, where possibly uncorrelated motions are evaluated together as displayed in Figure 8 in Appendix C, we unsurprisingly achieve lower scores as our approach does not over-diversify its predictions.\\n\\n6.2. Deterministic Evaluation\\n\\nWe report the Mean Angle Error (MAE-L2) as the Euclidean distance of the stacked (ZYX-)Euler angles as well as the Mean Per Joint Position Error (MPJPE) which is calculated using the human's skeleton forward kinematic. Those are the standard evaluation metrics in deterministic motion prediction. To better understand the influence of the learned latent multi-modality we apply a Best-of-N evaluation for the MAE-L2 and MPJPE metric. Here we report the value for the motion with the best average metric value over all prediction timesteps originating from the $N$ modes' means with the highest probability $p(z|x)$.\\n\\nDeterministic Baselines.\\n\\nWe evaluate against the following deterministic approaches: (1) Zero Velocity: All joints keep their current state at prediction time throughout the entire prediction horizon. (2) GRU sup.: Simple encoder-decoder structure using GRUs for variable history and prediction horizon and exponential maps as data representation. (3) Quaternet: Encoder-Decoder architecture using GRUs and quaternion data representation. (4) HistRepItself: Transformer encoder and fixed prediction horizon graph convolution decoder. Data is pre-processed using the Discrete Cosine Transform (DCT) on the time dimension and the model predicts a residual before the output is transformed back using the inverse DCT. (5) ST-Transformer: Decoupled temporal and spatial self-attention.\\n\\nFor the H3.6M dataset we state the reported values of (2) - (3) and re-run the evaluation of (4) as they originally did not account for $2\\\\pi$ angle discontinuity. For the AMASS dataset we re-train (4) HistRepItself to match our test split. We were not able to compare to some other methods for a lack of open source code (AGED) or their computational complexity (DMGNN - 62 Million parameter).\\n\\nEvaluation Methodology.\\n\\nIt has become common to benchmark on 8 fixed sequences per action of a single test subject on the H3.6M dataset. This has been shown to be un-representative. Thus, we report results on 8 (33, 35, 38, 40) (see Appendix F) and 256 (35, 40) samples per action on the H3.6M data to be comparable to past methods. For the AMASS dataset, we report results on the official test split, consisting of the Transitions and SSM dataset. While prior authors have argued that the Transitions dataset is not suitable for evaluating prediction algorithms for their change of action within sequences, we argue that such behavior can happen in real-world applications. We subsample the H3.6M dataset to 25 HZ and the AMASS dataset to 20 HZ as most of the included datasets have been recorded with a framerate divisible by 20 but not by 25. When comparing to ST-Transformer (Tab. 4) we follow their evaluation methodology. For the H3.6M dataset, we report metric values previously published on both 8 and 256 samples per action. As the AMASS dataset has been released recently, there are not many published results, yet. Thus, we retrain the current best state-of-the-art algorithm HistRepItself and re-train it on the official test split. We train the model on two seconds of history and predict one second into the future.\\n\\nResults.\\n\\nWe evaluate our approach on common single sample metrics against fully deterministic approaches. Table 2 summarizes the results on the H3.6M dataset. Even though we don't explicitly optimize for a deterministic output, our Weighted-Mean output outperforms all other state-of-the-art algorithms. We want to point out that we introduced the W-Mean output configuration solely for these metrics commonly used in deterministic evaluation. This allows us to point to the shortcomings of both, deterministic algorithms and their corresponding evaluation metrics: They produce motions which represent the average of all likely motions given the motion history. This average motion, however, may represent a unlikely or even infeasible motion. This (unintentional) behavior is followed by our W-Mean output configuration. Thus, we want to emphasize that while we outperform other algorithms on specific metrics we advise against...\"}"}
{"id": "CVPR-2022-1196", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Average angle error on 256 samples per action on the H3.6M test dataset. A break down by actions and the results on the MPJPE metric can be found in Appendix F.\\n\\nTable 3. Average angle error on 10,000 samples from the AMASS test set. The MPJPE metric can be found in Appendix F.\\n\\nTable 4. Average angle error using the evaluation procedure in [1]. using the W-Mean output configuration for actual applications.\\n\\nThe lower rows of Tab. 2 and Tab. 3 supports that our approach captures multimodality by committing to a specific motion per mode; thereby attributing appropriate probability mass to less likely motions. As such, the most likely deterministic mode (ML-Mode) commits to the mode which is best explained by the data. However, on average it accumulates a higher error compared to W-Mean as it is expected to represent a \u201cwrong\u201d motion with probability $p = \\\\max_i (\\\\pi_i)$ (see Sec. 5). Looking at the set of likely deterministic mode outputs (BoN in Tab. 2 and Tab. 3), it becomes clear that one of the motion modes performs exceptionally better than the mean output of deterministic regressors. This behavior is further visualized for a single example in Sec. 6.4.\\n\\nNotably, Motron can capture multimodality and reason probabilistically about humans\u2019 future motion while being computationally more efficient than a deterministic regressor. With 1.7 Million parameters we need half the computational power than HistRepItself (3.4M parameters) and are significantly more efficient compared to the current state-of-the-art algorithm DLow with 7.3 Million parameters. Other graph based models, such as [33], even use 62 Million parameters.\\n\\n6.3. Ablation Study\\n\\nIn this section, we will show the influence of our contributions on the model\u2019s performance as well as justify our design choices quantitatively.\\n\\nTable 5. Negative Log Likelihood (NLL) and MAE (L2) performance using different number of latent modes on H3.6M dataset.\\n\\nTable 6. Negative Log Likelihood (NLL) and MAE (L2) performance on ablated model structures on H3.6M dataset.\\n\\nNumber of latent modes. We ablate the number of latent discrete states which manifest as motion modes in the model\u2019s output (Tab. 5). Four and five modes show the best performance. We use five modes for our approach to give the model more expressiveness when necessary.\\n\\nInfluence of contributions. To show the influence of our contributions on the model\u2019s performance we ablate them individually in Tab. 6. We first remove the Typed Graph weight sharing scheme and subsequently replace it with One-Hot encoded type information which can recover some performance. Next, we replace the Concentrated Gaussian distribution in SO(3) with a standard Multivariate Normal (Gaussian) Mixture Model (GMM) and subsequently with a Bingham distribution. Unlike our $N_{SO}(3)$, the MVN is not designed to handle rotations and their special characteristics. It, therefore, has worse results in all metrics. The Bingham distribution, in contrast, is designed for rotations but does not support the composition of rotations. Thus, we can not use differential quaternions as intermediate output making the learning task more complex. Also, samples from a Bingham can only be approximated using computationally and memory expensive algorithms (e.g. Metropolis-Hasting). Finally, we enable gradient flow through the latent variable (see Appendix A and Sec. 5), which has a minor negative impact.\\n\\n6.4. Qualitative Results\\n\\nFig. 4 shows the capability of our method to capture the multimodal nature of human motions. Displayed is an exemplary motion where the human lands from a jump and rapidly pulls his arms down. In the beginning $t = 500$ ms, the different latent modes capture different possible speeds of the downwards arm movement. While the most likely mode anticipates a slower downwards movement than performed, the second most likely mode captures the true motion closely. Further, less likely modes capture even faster motions as well as movements where the arms are pulled more in front of the body. Notably, reasonably small uncertainties...\"}"}
{"id": "CVPR-2022-1196", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative visualization of the prediction distribution on a single sample from the AMASS dataset. The human lands from a jump and pulls both arms downwards. (a)\u2013(c), (e) Ground Truth poses. (d) 5 modes of our prediction distribution. Each mode is weighted by its probability where opacity indicates high confidence in a particular mode. (f)\u2013(j) Mean of each of the five output modes at t = 1 s.\\n\\nTowards the end t = 1000 ms, the expressiveness and multimodality of the modes can be experienced as, for example, Mode 5 captures the possibility of a consecutive second jump.\\n\\nAnother important quality for robotic applications is the ability to work with imperfect data. To simulate occlusions during training we apply Node Dropout. For the occluded nodes, we set a random number of continuous states leading to t = 0 to the neutral quaternion. The unique capabilities of our model here are shown in Fig. 5. In this instance, we artificially occlude all joint's data of the left leg. This leads to high variance but reasonable sampled predictions during early timesteps. More importantly, the model can understand and output its uncertainty: The closed form standard deviation of the parametric $\\\\mathbb{N}(\\\\pi, \\\\text{SO}(3))$ output distribution is adequately higher compared to the distribution with perfect data. For increasing prediction time, the model uses the influence between nodes learned in the Typed Graph layers to produce reasonable motions even for the occluded nodes and adjusts its relative confidence reasonably.\\n\\n7. Conclusion\\n\\nIn this work, we present Motron, a probabilistic human motion forecasting approach which uniquely provides the information plethora of a probabilistic approach and the accuracy of a deterministic model. Its predictions respect rigid skeleton constraints, all while producing full parametric motion distributions, which can be especially useful in downstream robotic applications. It achieves state-of-the-art prediction performance in a variety of metrics on standard and new real-world human motion datasets. Further, to the best of the authors' knowledge, it is the first method that demonstrates its ability to deal with occluded data while reasoning about its own uncertainty.\\n\\nFigure 5. Handling of imperfect data. All joints of the left leg are artificially occluded. Top: Visualization of prediction samples with and without occlusion. Bottom: Mean standard deviation of parametric output distribution of left hip and knee. The occluded data is addressed by the model adjusting its own uncertainty.\\n\\nLimitations. The approach is yet limited by the upstream data provider. While the motion capture system utilized to record the datasets used here, provides the required accuracy to calculate the joint rotations via inverse kinematics, the authors anticipate this being a challenge when relying on vision-based algorithms for human poses detection. Further, our approach's tendency to less diverse predictions can become problematic with regards to new unseen behaviors where our approach would be overly confident.\\n\\nFuture Directions include incorporating Motron's human behavior predictions in downstream robotic planning, decision making, and control frameworks, as well as exploring options to tightly couple upstream vision algorithms.\"}"}
{"id": "CVPR-2022-1196", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nAutonomous systems and humans are increasingly sharing the same space. Robots work side by side or even hand in hand with humans to balance each other's limitations. Such cooperative interactions are ever more so sophisticated. Thus, the ability to reason not just about a human's center of gravity position, but also its granular motion is an important prerequisite for human-robot interaction. Though, many algorithms ignore the multimodal nature of humans or neglect uncertainty in their motion forecasts. We present Motron, a multimodal, probabilistic, graph-structured model, that captures human's multimodality using probabilistic methods while being able to output deterministic maximum-likelihood motions and corresponding confidence values for each mode. Our model aims to be tightly integrated with the robotic planning-control-interaction loop; outputting physically feasible human motions and being computationally efficient. We demonstrate the performance of our model on several challenging real-world motion forecasting datasets, outperforming a wide array of generative/variational methods while providing state-of-the-art single-output motions if required. Both using significantly less computational power than state-of-the-art algorithms.\\n\\n1. Introduction\\n\\nThe key desideratum of autonomous systems is to provide added-value to humans while ensuring safety. Traditionally, safety aspects limit such robots to low-risk tasks with minimal human interaction. An understanding of humans and their distribution of feasible anticipated movement is key to develop safe, risk-aware human-interactive autonomous systems. Such systems could operate in closer proximity to humans, performing tasks involving higher levels of interaction, providing enhanced added value. However, capturing the complexity of human motion in a computational model is challenging due to the multitude of continuous movement possibilities (multimodality), even within fixed boundaries of physical limitations. Traditionally, over-conservative systems rely solely on those constraints to ensure safety, while predictive single-motion-output methods discard the potential of many high-level features in favor of a single possible motion.\\n\\nIn contrast to such deterministic regressors, Monte-Carlo method models aim to produce samples of human future motion. However, for different reasons (e.g., simulation purposes), they prioritize generative diversity over representing actual plausible motions. For robotic use cases, we aim for samples to represent the underlying distribution of possible motions; or even better a parametric description thereof (Fig. 1).\\n\\nBoth, single-output and sampled, human motion predictions have been developed independently to maximize their strengths in accuracy and diversity. Still, many of them have been developed without directly accounting for real-world robotic use cases; they settle for diversity instead of accuracy, ignore physical boundaries, and are computationally too expensive. We target robotic use cases, where predictions are used in a control loop. As such, a model has to capture the underlying distribution of possible human motions without being over-confident or over-diverse within an imminent time horizon; combining and balancing the desiderata of both research strands. In consequence, we are interested in developing a human motion prediction model that (I) represents the inherent multimodal structure of human motion; (II) accurately captures humans' probabilistic\"}"}
{"id": "CVPR-2022-1196", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and diverse nature while obeying rigid skeleton constraints; (III) by directly incorporating structural information of the human skeleton; (IV) while being able to deal with imperfect data; (V) outputting the maximum amount of information for the use of subsequent systems.\\n\\nTo achieve those desiderata, our contribution is three-fold: First, we describe a new efficient way to model graph-structured problems where nodes have a fixed semantic class, usable for generic graph-structured problems. Secondly, we present \\\\textbf{Motron}, which uniquely uses a probabilistic output structure based on the Concentrated Gaussian distribution in SO3 and a parallel weight sharing approach incorporating the skeleton's structure. It is designed to mirror the multimodal and uncertain nature of humans. Motron's flexible output structure is designed to serve downstream robotic modules such as motion planning, decision making, and control. Finally, we evaluate our model on the fulfillment of our desiderata using single-output metrics and metrics based on samples; combining both research strands. We outperform an extensive selection of Monte-Carlo based motion prediction methods while showing state-of-the-art performance on single-output evaluation procedures using a variety of metrics and datasets. Our contributions are substantiated by a thorough ablation study. We further show that Motron can deal with occluded data often present in real-world applications by including the additional uncertainty in its output.\\n\\n2. Related Work\\n\\nHuman Motion Forecasting. The release of the H3.6M dataset \\\\cite{20} in 2014 stimulated a wealth of research in the field of human motion prediction. While early works were based on traditional approaches such as Markov Models \\\\cite{32,70} or Gaussian Processes \\\\cite{49} the advances of deep learning has taken over recent approaches. Algorithms now are largely based on single-motion-output regressors \\\\cite{11,12,38,40}; improving performance by incorporating human skeleton structural information into their architectures \\\\cite{23,33}. Li et al. \\\\cite{33} merge nodes to create meta graphs of different scale to extract higher level information. Two concepts of capturing temporal influences have emerged: Recurrent Neural Networks (RNNs) and transforming time-series to a frequency domain \\\\cite{1,17,35,54}, where the latter is not agnostic to varying history horizons. Recently, Transformer \\\\cite{46} architectures have been explored overcoming RNNs shortcomings in capturing longer time series \\\\cite{35,45}. While generative and variational approaches have emerged as state-of-the-art in trajectory forecasting \\\\cite{27,43,44} for their plethora of captured information, they largely exist in parallel to research on single-output regressors in human motion forecasting. The field is split in approaches using Generative Adversarial Networks (GANs) \\\\cite{4,17,18,28} and (Conditional) Variational Autoencoders ((C)V AEs) \\\\cite{52\u201354}. Of these, similar to the field of trajectory prediction, adapted versions of the V AE frameworks show better results \\\\cite{53}.\\n\\nSurprisingly, these two fields of single-output and probabilistic motion prediction are highly disentangled, following their respective distinct experimentation protocols. In single-motion-output setups, it is, for example, common to predict one future second. In contrast, previous probabilistic works set their focus on producing plausible diverse motion samples over a longer prediction horizon (2s); at which point the true human motion is fraught with uncertainty. By slight abuse of terminology, we will distinguish the two common types of motion prediction algorithms in the remainder of this work. Monte-Carlo based models which can produce samples from a distribution of future motions will be referenced as \\\\textbf{probabilistic} models while single-motion-output models will be referenced as \\\\textbf{deterministic} models for their lack of uncertainty awareness.\\n\\nDirectional Probabilistic Learning. In robotic applications, such as filtering, the use of directional statistics for rotational systems has been proven useful \\\\cite{13,15,29,30}. The most common distributions for modeling rotational uncertainty are the Bingham \\\\cite{6}, the von Mises\u2013Fisher \\\\cite{10}, the Projected Gaussian \\\\cite{31}, and using a Concentrated Gaussian in SO(3)\\\\cite{3, Chapter 7.3.1}. Advances in deep probabilistic learning, however, are mainly focused on learning distributions in vector space \\\\cite{7,8,43}. Thus, of the rotational distributions, only the Bingham distribution has been applied to probabilistic deep learning \\\\cite{14,42}. In \\\\cite{14}, Gilitschenski et al. directly learn the parameters of a Bingham distribution representing the orientation of an object in an image.\\n\\nGraph Neural Networks. Besides other concepts like message passing, convolution, and aggregation \\\\cite{55} the concept of attention, first introduced for temporal dependencies \\\\cite{46}, has been applied to graph-structured problems by Veli\u02c7ckovi\u02c7c et al.\\\\cite{47} as Graph Attention Networks (GAT). Recently those GATs have been included in temporal networks such as LSTMs by replacing the linear transformations in each RNN cell with a Graph Attention Layer \\\\cite{50}. Within human motion forecasting Graph Convolution Networks (GCNs) \\\\cite{35,36} have shown good performance. Salzmann et al.\\\\cite{43} model dependencies between different entities for trajectory prediction using a sequential message passing algorithm. This, however, becomes computational infeasible for larger number of node types.\"}"}
{"id": "CVPR-2022-1196", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To represent multimodality, we define our output structures as a Concentrated Gaussian Mixture Model in SO(3). We get \\n\\n\\\\[\\nR \\\\sim \\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nusing the first order Baker-Campbell-Hausdorff approximation. Without approximation, we have\\n\\n\\\\[\\nR = \\\\exp(x) \\\\cdot \\\\bar{R}\\n\\\\]\\n\\nwhere \\\\( \\\\bar{R} \\\\) is a 'large', noise-free, nominal rotation, \\\\( \\\\exp \\\\) is the inverse of the exponential map, and\\n\\n\\\\[\\nx \\\\sim \\\\mathcal{N}(0, \\\\Gamma)\\n\\\\]\\n\\nwhere \\\\( \\\\Gamma \\\\) is the Gaussian normalization constant, \\\\( \\\\ln \\\\) is the skew-symmetric Lie algebra operator.\\n\\nThe distribution's p.d.f. is given as\\n\\n\\\\[\\np(x | \\\\bar{R}, \\\\Gamma) = \\\\prod_i \\\\mathcal{N}(x_i | 0, \\\\Gamma_i)\\n\\\\]\\n\\n\\\\( i \\\\) runs over all dimensions. The distribution's entropy is\\n\\n\\\\[\\nH = -\\\\int p(x | \\\\bar{R}, \\\\Gamma) \\\\ln p(x | \\\\bar{R}, \\\\Gamma) dx = \\\\frac{1}{2} \\\\log |\\\\det(\\\\Gamma)| + \\\\frac{1}{2} dim - \\\\frac{1}{2}\\n\\\\]\\n\\nwhere \\\\( dim \\\\) is the dimensionality of the state. For deterministic and stationary rotation, the entropy is\\n\\n\\\\[\\nH = \\\\frac{1}{2} \\\\log |\\\\det(\\\\Gamma)| + \\\\frac{1}{2} dim - \\\\frac{1}{2}\\n\\\\]\\n\\nand all properties of the distribution \\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We define the equivalent to a linear, skew-symmetric Lie algebra operator, and all properties of the distribution\\n\\n\\\\[\\n\\\\mathcal{N}(\\\\bar{R}, \\\\Gamma)\\n\\\\]\\n\\nare determined by the Gaussian normalization constant \\\\( \\\\Gamma \\\\) which is the inverse of the exponential map. We"}
{"id": "CVPR-2022-1196", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. Motron\\n\\nOur model is visualized in Fig. 2. From a high-level perspective, we combine latent discrete variables with probabilistic mixture distribution as output structure to model the diverse nature of human motion while embedding the skeleton graph structure directly into the learning and inference procedure by using only Typed Graph components. This model extends core concepts of probabilistic, multi-modal deep learning towards the application of human motion prediction. We call our model Motron.\\n\\nGraph Structure Embodiment. We enable efficient use of graph-structured information by imbuing the architecture from end to end. Thus, the architecture is fully described by the two building blocks TG-Linear and TG-GRU; there is no fully connected influence between hidden node states. This leads to natural modeling of the information flow, where weights are shared between symmetric joints as well as a reduction in model parameters of about 40%.\\n\\nModeling Motion History. Starting from the input representation $x = s(t_H:t)$, the model needs to encode a node's current state and its history. To encode the observed history of the node, its current and previous states are fed into a TG-GRU network. The final output is encoded to the model's hidden state $h$ by a TG-Linear layer.\\n\\nExplicitly Accounting for Multimodality. Motron explicitly handles multimodality by leveraging a probabilistic latent variable architecture. It produces the target distribution $p(y|x)$ by introducing a discrete latent variable $z$, $p(z|x) = \\\\frac{1}{N} \\\\sum_i f_{\\\\text{TG Linear}}(h)_i$, (12) which encodes high-level latent behavior and allows for $p(y|x)$ to be expressed as $p(y|x,z)\\\\sum z p(z|x)$, (13) where $\\\\pi$ and $\\\\varrho$ are deep neural network weights that parameterize their respective distributions. Unlike latent variables in probabilistic variational (e.g. (C)V AEs), this latent variable is not explicitly encouraged to learn a representation for the decoder (e.g. using KL-divergence), but implicitly enables the decoder to differentiate between modes.\\n\\nProducing Motions. The sampled latent variable $z$ is concatenated with the hidden representation vector $h$ and fed into our TG-GRU decoder. Each TG-GRU cell outputs the parameters $\\\\bar{R}, \\\\bar{\\\\rho}$ of a $N_{SO}(3)$ for each node. Using $\\\\varpi_i = p(z = i|x)$ we produce the mixture model $\\\\hat{q} \\\\sim N_{SO}(3)$ over differential quaternions as an intermediate output distribution. Thus, $z$ being discrete is necessary as it enables us to rethink Eq. (13) as a $N_{SO}(3)$ with mixture distribution $p(z|x)$. It also aids in interpretability, as one can visualize the high-level behaviors resulting from each $z$ by sampling.\\n\\nAll of our source code, trained models, and data can be found online at https://github.com/TUM-AAS/motron-cvpr22.\"}"}
