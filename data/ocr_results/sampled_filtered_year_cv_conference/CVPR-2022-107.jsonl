{"id": "CVPR-2022-107", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alignment-Uniformity aware Representation Learning\\nfor Zero-shot Video Classification\\n\\nShi Pu\\nKaili Zhao\\nMao Zheng\\n\\n1 Tencent AI Platform Department\\n2 Beijing University of Posts and Telecom.\\n\\n{shipu, moonzheng}@tencent.com kailizhao@bupt.edu.cn\\n\\nAbstract\\nMost methods tackle zero-shot video classification by aligning visual-semantic representations within seen classes, which limits generalization to unseen classes. To enhance model generalizability, this paper presents an end-to-end framework that preserves alignment and uniformity properties for representations on both seen and unseen classes. Specifically, we formulate a supervised contrastive loss to simultaneously align visual-semantic features (i.e., alignment) and encourage the learned features to distribute uniformly (i.e., uniformity). Unlike existing methods that only consider the alignment, we propose uniformity to preserve maximal-info of existing features, which improves the probability that unobserved features fall around observed data. Further, we synthesize features of unseen classes by proposing a class generator that interpolates and extrapolates the features of seen classes. Besides, we introduce two metrics, closeness and dispersion, to quantify the two properties and serve as new measurements of model generalizability. Experiments show that our method significantly outperforms SoTA by relative improvements of 28.1% on UCF101 and 27.0% on HMDB51. Code is available.\\n\\n1. Introduction\\nMimicking human capability to recognize things never seen before, zero-shot video classification (ZSVC) only trains models on videos of seen classes and makes predictions on unobserved ones. Correspondingly, existing ZSVC models map visual and semantic features into a unified representation, and hope the association can be generalized to unseen classes. However, these methods learn associated representations within limited classes, thus facing the following two critical problems: (1) semantic-gap: manifolds inconsistency between visual and semantics features, and (2) domain-shift: the representations learned from training sets are biased when applied to the target sets due to disjoint classes between two groups. In ZSVC, these two problems cause side effects on model generalizability.\\n\\nReviewing the literature, we observe that most methods focus on tackling the semantic-gap by learning alignment-aware representations, which ensure visual and semantic features of the same class close. To improve the alignment, MSE loss, ranking loss, and center loss are commonly used to optimize the similarity between visual and semantic features. Apart from the loss, improvements for alignment are attributed mainly to the designs of architectures. For instance, first project global visual features to local object attributes, then optimize similarity between the attributes and final semantics. In contrast, URL, Action2Vec, and TARN directly align visual and final semantic features, which are improved via attention modules. Since video features are hard to learn, the above methods utilize pre-trained models to extract visual features. The recent model benefits from the efficient R(2+1)D module in video classification and achieves the state-of-the-art (SoTA) results in ZSVC. However, the SoTA neglects to learn semantic features; thus, it is still...\"}"}
{"id": "CVPR-2022-107", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"not a true end-to-end (e2e) framework for visual-semantic feature learning. We claim that e2e is critical for alignment since fixed visual/semantic features will bring obstacles to adjusting one to approach another.\\n\\nNoteworthily, the latest MUFI \\\\( ^{35} \\\\) and ER \\\\( ^{6} \\\\) get down to addressing the domain-shift problem by involving more semantic information, thus consuming extra resources. In particular, MUFI \\\\( ^{35} \\\\) augments semantics by training multi-stream models on multiple datasets. ER \\\\( ^{6} \\\\) expands class names by annotating amount of augmented words crawled from the website. Freeing complex models or additional annotations, we will design a compact model that preserves maximal semantic info of existing classes while synthesizing features of unseen classes.\\n\\nTo tackle the two problems with one stone, we present an end-to-end framework that jointly preserves alignment and uniformity properties for representations on both seen and unseen classes. Here, alignment ensures closeness of visual-semantic features; uniformity encourages the features to distribute uniformly (maximal-info preserving), which improves the possibility that unseen features stand around seen features, mitigating the domain-shift implicitly.\\n\\nSpecifically, we formulate a supervised contrastive loss as a combination of two separate terms: one regularizes alignment of features within classes, and the other guides uniformity between semantic clusters. To alleviate the domain-shift explicitly, we generate new features of unseen synthetic classes by our class generator that interpolates and extrapolates features of seen classes. In addition, we introduce closeness and dispersion scores to quantify the two properties and provide new measurements of model generalizability. Fig. 1 illustrates the representations of our method and the SoTA alternative \\\\( ^{3} \\\\). We train the two models on ten classes sampled from Kinetics-700 \\\\( ^{5} \\\\) and map features on 3D hyperspheres. We observe that our representation shows better closeness within classes and preserves more dispersion between semantic clusters. Experiments validate that our method significantly outperforms SoTA by relative improvements of 28.1% on UCF101 and 27.0% on HMDB51.\"}"}
{"id": "CVPR-2022-107", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Architecture of AURL: From left to right, we map a video sequence $I$ and the class name set $Y$ to a unified representation $(fv(g(I))), fs(c(Y)))$. During training, to learn representations of seen classes, we introduce LS to preserve alignment and uniformity properties. For synthetic unseen classes, we introduce LUS to learn the two properties on synthetic visual-semantic features $(\\\\cdot, Z)$. To synthesize features of unseen classes, we first utilize LC to learn visual centers $W$, then propose Class Generator to transform $W$ and existing semantics $fs(c(Y)))$ into the representation $(\\\\cdot, Z)$. During inference, we perform an NNS strategy to obtain the final class.\\n\\nTends MoCo to video understanding. [53] introduces feature transformation on existing samples to obtain broader and diversified positives and negatives, thus enhancing discrimination. However, the above models learn instance pairs from the same domain, e.g., images or videos. Instead, CLIP [36] exploits features of two domains (i.e., images and texts) guided by a contrastive loss. Motivated by CLIP, the latest models in ZSVC, MUFI [35] and ER [6], extend the self-supervised contrastive loss to a fully-supervised loss that contrasts visual and semantic features. However, MUFI and ER neglect the difference and similarities of the self-supervised and supervised contrastive losses. Considering alignment and uniformity properties, we build connections between the two losses and analyze the advantages of the supervised loss. Besides, MUFI and ER both require extra resources to improve model generalizability. We propose a compact model using a class generator to explicitly synthesize new features of synthetic unseen classes.\\n\\n3. Alignment-Uniformity aware Representation Learning (AURL)\\n\\nThis section describes Alignment-Uniformity aware Representation Learning (AURL) involving a unified architecture, loss functions, class generator, and two novel metrics, followed by its training and inference strategy, and then discusses similarities and differences against alternatives.\\n\\n3.1. Architecture\\n\\nFig. 2 shows the AURL architecture. Given complete $K$ class names $Y = \\\\{ y_1, ..., y_K \\\\}$, and an input video $I$ of class $y_i \\\\in Y$ (e.g., playing basketball), we by end-to-end learn visual and semantic embeddings. We introduce R(2+1)D [43] as the backbone to generate visual features $g(I)$, and utilize a video projector $fv$ to implement 3-layer MLP projection (2 fc+bn+ReLU and 1 fc+bn), thus obtain visual embeddings $fv(g(I)) \\\\in \\\\mathbb{R}^d$. Parallelly, we perform Word2vec [30] to extract the initial word embeddings $c(Y)$, then learn semantic embeddings $fs(c(Y))) \\\\in \\\\mathbb{R}^{K \\\\times d}$ by a word projector $fs$ that has one fc (#node=512) and the 3-layer MLP projection. For convenience, we note $fv(g(I))$ and $fs(c(y_i))$ of $i$-th class as $v_i$ and $s_i$ for the below discussions. Compared with SoTA methods [3,35] that only learn video parts, our AURL end-to-end trains the backbone, video and word projectors, providing more feature flexibility under the regularization of loss functions.\\n\\n3.2. Alignment-uniformity aware Loss\\n\\nCan alignment and uniformity properties be preserved in supervised contrastive loss?\\n\\nFor self-supervised learning, [8] claims that contrastive loss [7,33] (see Eq. 1 and supplementary) preserves alignment and uniformity properties. Alignment indicates that positive samples should be mapped to nearby features and thus be invariant to unneeded noises. Uniformity [47] means feature vectors should be roughly uniformly distributed on the unit hypersphere, thus bringing better generalization to downstream tasks.\\n\\n\\\\[\\nL_{self} = \\\\log \\\\left( \\\\frac{\\\\exp(\\\\text{sim}(f, f^+))}{\\\\mathbb{P}_f \\\\exp(\\\\text{sim}(f, f^-))} \\\\right).\\n\\\\] (1)\\n\\nHere, $(f, f^+), (f, f^-)$ are positive and negative pairs of images/videos, $N$ is negative set; sim means a similarity function (thus in $[-1, +1]$); and $\\\\theta$ is a temperature parameter.\"}"}
{"id": "CVPR-2022-107", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"bound in Eq. reformulate max\\n\\n\\\\[\\n\\\\text{maximize } \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\nwhere\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\nas follows (the full derivation in\\n\\ntriplet loss \\\\[\\nL_{\\\\text{sup}}(v, y, s) = \\\\text{max}(0, -1 + \\\\text{sim}(v, y) - \\\\text{sim}(v, s))\\n\\\\]\\n\\nwe show that\\n\\nthe similarity and difference between the two losses. Here,\\n\\nglect the alignment and uniformity properties, but also miss\\n\\nalso utilize supervised contrastive loss, not only do they ne-\\n\\ndenominator. Even recent work MUFI \\\\[\\nL_{\\\\text{sup}}(v, y, s) = \\\\frac{1}{|Y|} \\\\sum_{y \\\\in Y} \\\\sum_{s \\\\neq y} \\\\exp \\\\left( \\\\text{sim}(v, y) - \\\\text{sim}(v, s) \\\\right)\\n\\\\]\\n\\nobserve that\\n\\nmuch as possible. To sum up, our\\n\\nusing a LogSumExp function, thus spreading features as\\n\\nmaximize the distances between features of different classes\\n\\nthe same class will be aligned. The uniformity term tends to\\n\\nvors sim\\n\\nPlus function and LSE is LogSumExp. Since Eq.\\n\\n\\\\[\\nL_{\\\\text{sup}}(v, y, s) = \\\\frac{1}{|Y|} \\\\sum_{y \\\\in Y} \\\\sum_{s \\\\neq y} \\\\exp \\\\left( \\\\text{sim}(v, y) - \\\\text{sim}(v, s) \\\\right)\\n\\\\]\\n\\nand the complete class set is\\n\\nv\\n\\nWe derive upper bounds of\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\nwhere, SP\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\nand\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL = \\\\frac{\\\\text{similarities}}{\\\\text{differences}}\\n\\\\]\\n\\n\\\\[\\nL"}
{"id": "CVPR-2022-107", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Illustration of Class Generator: (a) Synthetic classes \u2022 are generated by linearly combining features of seen classes (4 with colors); N means test classes. (b) Feature transformation w/ different \\\\( \\\\tau \\\\) synthesizes semantics covering various size of regions.\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\vec{z} &= \\\\|W \\\\vec{m}\\|, \\\\\\\\\\n\\\\|s Y \\\\| &= [s y_1, \\\\ldots, s y_D],\\n\\\\end{align*}\\n\\\\]\\n\\nHere, \\\\( \\\\vec{m} \\\\in \\\\mathbb{R}^{K_u \\\\times d} \\\\) and \\\\( \\\\vec{s} \\\\) separately represent the synthetic visual centers and semantic features of unseen classes; \\\\( K_u \\\\) represents the number of unseen classes and \\\\( d \\\\) is the feature dimension. Besides, we apply normalization on \\\\( W \\\\) and \\\\( s Y \\\\) (both are \\\\( D \\\\times d \\\\) matrix) because learning on a unit hypersphere helps model optimization [47]. \\\\( D \\\\) is a hyper-parameter that means how many classes are sampled for unseen-class generator. The matrix \\\\( M \\\\in \\\\mathbb{R}^{K_u \\\\times D} \\\\) is used for inter- and extra-polations, whose elements are randomly sampled from a uniform distribution \\\\( U(\\\\tau, 1) \\\\), and \\\\( 1 < \\\\tau < 1 \\\\). \\\\( \\\\tau \\\\) is another hyper-parameter that controls the distributed range of the synthetic points.\\n\\nIt is worth noting that the settings of hyper-parameters \\\\( D \\\\) and \\\\( \\\\tau \\\\) are non-trivial. For \\\\( D \\\\), we prefer \\\\( D \\\\leq d \\\\), i.e, the number of seen classes should be larger than the dimension of a hypersphere. Because for a full rank matrix \\\\( W \\\\), a linear combination of the column vector of \\\\( W \\\\) can express any vector on the transformed space. We aim to generate as diverse unseen classes as possible to improve the possibility that the synthesized points can cover the classes in the test set. Thus, in experiments, we will select features of all seen classes for feature transformation. For \\\\( \\\\tau \\\\), we choose positive values for interpolation where the synthetic clusters locate inside of seen points (see Fig. 3(b)), and gradually enlarge the cluster regions by decreasing \\\\( \\\\tau \\\\) where the negative value is for extrapolation. Fig. 3(a) illustrates our Class Generator with \\\\( D = 10, d = 3, \\\\tau = 0.5 \\\\) on the Kinetics-700 dataset [18]. We can see our transformation not only provides unseen classes but also approaches test classes (e.g., UCF101 dataset [40]).\\n\\n3.4. Closeness and Dispersion\\n\\nTo quantify the alignment and uniformity, we introduce two metrics: closeness and dispersion. Closeness measures the mean distance of features within the same class, reflecting the alignment of visual and semantic features.\\n\\n\\\\[\\n\\\\text{Closeness} = \\\\frac{1}{K} \\\\sum_{y} \\\\frac{1}{N_y} \\\\sum_{i=1}^{N_y} (\\\\cos(v_{yi}, s_{yi}))^2,\\n\\\\]\\n\\nwhere, \\\\( N_{yi} \\\\) is the # of training videos of class \\\\( y_i \\\\). Besides, to evaluate the uniformity/separation of semantic clusters, we adopt minimal distances among all clusters to compute dispersion. Here, we consider all visual features within the same class as a semantic cluster instead of using one single semantic vector \\\\( s y_i \\\\). For example, \\\\( \\\\bar{v}_{yi} \\\\) is the mean of visual features of the class \\\\( y_i \\\\), and indicates one semantic cluster.\\n\\n\\\\[\\n\\\\text{Dispersion} = \\\\frac{1}{K} \\\\sum_{y} \\\\min_{y_k} (\\\\cos(\\\\bar{v}_{yi}, \\\\bar{v}_{yk}))^2.\\n\\\\]\\n\\nThe experiments in Sec. 4.2 show that models tested with higher accuracy preserve the lower closeness and higher dispersion in representations. We conclude our two metrics can serve as new measurements of model generalizability.\\n\\n3.5. Training & Inference\\n\\nTraining: We end-to-end train visual and semantic features and jointly learn the contrastive loss \\\\( L_{\\\\text{contrast}} \\\\) and classification loss \\\\( L_C \\\\), thus obtaining the following overall loss:\\n\\n\\\\[\\nL_{AURL} = L_S + L_{US} + L_C.\\n\\\\]\\n\\nWe will justify our end-to-end training is critical for alignment and uniformity properties, and validate our compact model with \\\\( L_{AURL} \\\\) outperforms SoTA alternatives.\\n\\nInference: we train AURL on source dataset \\\\( I \\\\) with \\\\( K \\\\) seen classes \\\\( Y = \\\\{y_1, \\\\ldots, y_K\\\\} \\\\), and evaluate the model on target dataset \\\\( I_t \\\\) with \\\\( T \\\\) unseen classes \\\\( Y_t = \\\\{y_{t1}, \\\\ldots, y_{tT}\\\\} \\\\). In this paper, we follow the strict problem setting in [3], which requires training classes \\\\( Y \\\\) have no overlap with test classes \\\\( Y_t \\\\). Mathematically, we re-write the requirement as:\\n\\n\\\\[\\n\\\\forall y_2 \\\\in Y, \\\\min_{y_t \\\\in Y_t} (1 - \\\\cos(c_y, c_{yt})) > \\\\tau,\\n\\\\]\\n\\nwhere \\\\( c_i \\\\) means Word2vec features of class \\\\( i \\\\), \\\\( \\\\tau \\\\) is the distance threshold. We utilize the Nearest Neighbor Search 19972.\"}"}
{"id": "CVPR-2022-107", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparisons between AURL and alternative methods.\\n\\n| Methods | ET | AUL | ERF | UCG | SLR |\\n|---------|----|-----|-----|-----|-----|\\n| SoTA    | 3  | \u21e5   | \u21e5   | \u21e5   | \u21e5   |\\n| MUFI    | 35 | \u21e5   | \u21e5   | \u21e5   | \u21e5   |\\n| ER      | 6  | \u21e5   | \u21e5   | \u21e5   | \u21e5   |\\n| AURL (ours) | X | X | X | X | X |\\n\\n*ET*: end-to-end trainable, *AUL*: alignment-uniformity learning, *ERF*: extra resources free, *UCG*: unseen class generator, *SLR*: strict label requirement.\\n\\n3.6. Comparisons with Related Work\\n\\nThe closest studies to our AURL are SoTA [3], MUFI [35], and ER [6]. Table 1 summarizes their similarities and differences. SoTA only utilizes MSE loss to regularize feature alignment within seen classes, limiting model generalizability. MUFI and ER both implicitly increase semantic info to improve the generalization. MUFI trains multi-stream models across multiple datasets. ER crawls and annotates a number of web words to expand existing class names. Unlike that MUFI and ER both require extra resources, our AURL is a compact model to utilize the uniformity that helps preserve maximal info of existing features, and introduce a class generator to synthesize more semantics explicitly. Even MUFI and ER adopt the supervised contrastive loss, they neglect how alignment and uniformity properties affect ZSVC. Besides, our AURL follows the strict label requirement (in Eq. 10) that classes of training and test sets are far away from each other, which manifests the nature of ZSVC. At last, compared with ER that only trains the last fc layers, AURL utilizes a true e2e training strategy that is critical to realize the two properties.\\n\\n4. Experiments\\n\\n4.1. Settings\\n\\nDatasets: We train our AURL on the Kinetics-700 dataset [18] and evaluate it on UCF101 [40] and HMDB51 [21] datasets. The Kinetics-700 provides download links of YouTube videos annotated with 700 categories of human actions. We collect 555,774 videos using these links. The UCF101 contains 13,320 videos with 101 actions and the HMDB51 has 6,767 videos annotated with 51 actions.\\n\\nTraining protocol: For fair comparisons with the SoTA [3], we select the training videos in Kinetics-700 whose classes have non-overlap with UCF101 and HMDB51 as described in Eq. 11, and set the same $\\\\tau = 0.05$, thus obtaining 662 classes. AURL is inductive zero-shot learning, thus does not include any test data during training.\\n\\nEvaluation protocol: Existing ZSVC methods adopt various evaluation protocols to report experimental results. For complete comparisons, we perform three protocols: 1, 3, and N test splits. 1 test split reports an accuracy on all videos of UCF101 or HMDB51 set. 3 test splits reports an average accuracy by averaging 3 accuracies that are separately evaluated on 3 test sets provided by the UCF101 or HMDB51. N test splits also reports the average by averaging N accuracies that are obtained by running N (10 in our method) times testing, in each, m classes are randomly selected (m=50 for UCF101 and m=25 for HMDB51).\\n\\nImplementation details: We adopt one or multiple video clips as one input video of models. We follow the same SoTA settings [3] (i.e., 1 or 25 video clips and 16 frames/clip with size of $1 \\\\times 16 \\\\times 112 \\\\times 112 \\\\times 3$) for fair comparisons. If multiple video clips are used, we take the mean of multiple visual embeddings as the representative embedding. Besides, if a class name contains multiple words, we average the corresponding Word2vec features to represent the class prototype. For the AURL architecture, we set feature dimension of the R(2+1)D backbone as 512 (i.e., $g(I) \\\\rightarrow \\\\mathbb{R}^{512}$) and dimension of Word2vec as 300 (i.e., $c(Y) \\\\rightarrow \\\\mathbb{R}^{K \\\\times 300}$), and set the number of nodes in 3-layer MLP of the projector as 2048, 2048, and 2048 separately. During training, we empirically set $K$ as 662, as 10, $D$ as 662, and $\\\\epsilon$ as 0. We deploy the training on 8 Nvidia Tesla V100 GPUs. We set batch size as 256 and synchronize all batch normalization across GPUs following [4, 7]. We implement experiments using PyTorch and Horovod. SGD is our optimizer and a learning rate of 0.05 with a cosine decay schedule [7, 26] is adopted. Then, we set the weight decay as 0.0001 and the SGD momentum as 0.9. The number of training iterations is 58,500 which takes 45 hours.\\n\\n4.2. Ablation Study\\n\\nTo analyze AURL, we performed extensive ablations that were trained on the Kinetics-700 and evaluated on UCF101 and HMDB51 using 1 video clip and 1 test split of evaluation protocols. Table 2 summarizes the quantitative results. Fig. 4 visualizes the visual-semantic representation of ablations by sampling 10 classes from Kinetics-700 dataset and setting the features as 3-D for better visualization. We will justify: (1) our model that preserves alignment-uniformity properties performs better than the SoTA method [3] that focuses on alignment only; (2) end-to-end (e2e) training is critical to realize the two properties; (3) our AURL involving the class generator performs the best. From the justifications, we will show that our closeness and dispersion metrics can serve as new measurements of model generalizability. Here, we take the architecture of the SoTA [3] as our Base model (i.e., base backbone + fc only for video parts).\"}"}
{"id": "CVPR-2022-107", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Ablations: The representations of ablations w/ MSE loss, our LS, e2e training, and the AURL. \u2022 and 4 represent visual and semantic features separately; colors are for different classes. Here, we randomly sample 10 classes from Kinetics-700 for visualization.\\n\\nPer (1), we compare Base w/ MSE (i.e., the SoTA) and Base w/ LS. From the accuracy in Table 2, Base w/ LS largely improves the results by (14.8%, 35.1! 40.3) on UCF101 and by (13.6%, 21.3! 24.2) on HMDB. Comparing the learned representation in Fig. 4(a) w/ MSE and (b) w/ LS, we observe the semantic clusters of (b) spread more than (a), but the alignment within classes gets worse, for example, visual and semantic features are not calibrated for classes \\\"skipping rope\\\" and \\\"abseiling\\\". Similarly, we find the same trend in closeness and dispersion metrics shown in Fig. 4 and Table 2, where closeness gets worse (0.029! 0.065, 0.30! 0.45) but dispersion becomes much better (0.330! 0.414, 0.09! 0.29). We can see our Base w/ LS preserving higher uniformity in the trained representation can achieve better generalization when making inference on the test set, even scarify a little alignment.\\n\\nPer (2), we involve e2e training strategy (i.e., base backbone + video projector for video parts; word projector for semantic parts) to the Base w/ LS, and get the Base w/ LS + e2e, which further improves the accuracy from 40.3 to 43.2 on UCF-101 and from 24.2 to 26.2 on HMDB. Not surprisingly, we observe the alignment is tuned better and uniformity is maintained in good quality, thus obtaining a better trade-off. Referring to Fig. 4(b) and (c), we see Base w/ LS + e2e encourages better uniformity that semantic clusters are relatively distributed uniformly across the hypersphere while achieves a satisfying alignment that visual and semantic features are apparently aligned (see classes \\\"skipping rope\\\" and \\\"abseiling\\\" again for comparisons). The similar trends also occur in closeness and dispersion metrics, i.e., (0.024 and 0.340; 0.30 and 0.29) in Fig. 4 and Table 2. We conclude that e2e is critical for adjusting features to meet the regularizations of alignment and uniformity.\\n\\nPer (3), we apply LS to unseen classes coupling with the class generator (CG), i.e., our AURL. Compared AURL with Base w/ LS + e2e, AURL steadily improves 2.8% on UCF and 4.6% on HMDB, achieving the best accuracy. Quantitatively, closeness and dispersion reach the best scores, such as (0.29, 0.32) in Table 2 and (0.020, 0.354) in Fig. 4. From the representation of Fig. 4(d), we see the semantic clusters cover most regions of the hypersphere, which improves the possibility that unseen features fall around existing points, thus bringing a better generalization. Furthermore, we remove CG from AURL (i.e., AURL w/o CG) to validate the effectiveness of the class generator. Comparing AURL and AURL w/o CG, we find that the performances of AURL w/o CG on UCF and HMDB both decrease, and the accuracy on HMDB even degrades lower than Base w/ LS + e2e. Thus, we conclude the CG is a critical module to enhance the generalization.\\n\\nLast but not least, from the above justifications, we summarize that our models consistently improve the accuracy by involving the proposed modules; closeness/dispersion measured on the learned representations have agreements with the accuracy evaluated on test sets, providing model evaluations even prior to making inference.\\n\\n4.3. Comparisons with the Closest SoTA\\n\\nThe closest SoTA to our AURL is the recent work [3], which utilizes a compact model that achieves the SoTA results even under a strict setting (i.e., Eq. 10). Table 1 summarizes the similarity and difference between the SoTA and our AURL. Table 3 shows the comprehensive comparisons quantitatively. We reported the SoTA results using the same settings and the authors' released code. For comprehensive comparisons, we include various evaluation protocols including Pre-training, Video clips, and Test splits. Pre-training means that SoTA fine-tunes the pre-trained models on the SUN dataset [50]. From the comparisons, we see our\"}"}
{"id": "CVPR-2022-107", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Comparisons with the closest SoTA \\\\cite{3} on both UCF and HMDB datasets. Red numbers indicate the best.\\n\\n| Method | Pre-training | Video clips | Test splits | UCF \\\\(\\\\text{top-1}\\\\) | UCF \\\\(\\\\text{top-5}\\\\) | HMDB \\\\(\\\\text{top-1}\\\\) | HMDB \\\\(\\\\text{top-5}\\\\) |\\n|--------|-------------|-------------|-------------|----------------|----------------|----------------|----------------|\\n| SoTA \\\\cite{3} | 1 | 10 | | 45.6 | 73.1 | 28.1 | 51.8 |\\n| AURL | 1 | 10 | | 55.1 | 79.3 | 34.3 | 65.1 |\\n| SoTA \\\\cite{25} | 25 | 10 | | 49.2 | 77.0 | 32.6 | 57.1 |\\n| AURL | 25 | 10 | | 58.0 | 82.0 | 39.0 | 69.5 |\\n| SoTA \\\\cite{11} | 1 | 1 | | 36.8 | 61.7 | 23.0 | 41.3 |\\n| AURL | 1 | 1 | | 44.4 | 70.0 | 27.4 | 53.2 |\\n\\nAURL consistently surpasses the SoTA under each evaluation protocol. Specifically, the smallest improvements happen at (25 Video clips, 1 Test splits) by (17.6, 16.5)% improvements on UCF top-1 and HMDB top-1, and the largest comes at (1 Video clip, 10 Test splits) by (28.1, 27.0)% increases on UCF top-1 and HMDB top-1.\\n\\nTo conclude, AURL outperforms the SoTA by a large margin.\\n\\n4.4. Comparisons with the Alternatives\\n\\nTable 4 shows the comparisons with the alternatives. The SoTA \\\\cite{3} and our AURL with ? mean the two methods follow the strict label requirement in Eq. 10. From the results, we observe that our AURL surpasses all the alternatives in various challenging situations. Specifically, we summarize the challenges: first, fewer test splits are harder testing situations, e.g., for SAE \\\\cite{27}, 3 vs. 10 splits corresponds to 32.8 vs. 40.4 on UCF; second, strict label requirement (\\\\(\\\\times\\\\)) serves more difficult situations, e.g., our AURL \\\\(\\\\times\\\\) w/ 10 (the more) test splits achieves even worse results than AURL w/ 3 splits; third, some methods acquire extra training datasets (e.g., Kinetics + extra 5 datasets trained in MUFI \\\\cite{35}), additional semantic classes (e.g., web words used in ER \\\\cite{6}), and even training videos sampled from the same domain as the test set (e.g., tr/te are both UCF or HMDB in TARN \\\\cite{2}, Act2Vec \\\\cite{14}, PSGNN \\\\cite{13}, and ER \\\\cite{6}), which provide more difficulties to be competed against for other methods. Correspondingly, we find the superiority of our AURL as below: (1) AURL w/ 1 (the fewest) test split outperforms most methods w/ 3 or 50 splits, e.g., (46.8, 31.7) of AURL vs. (36.3, -) of OPCL and (43.0, 32.6) of PSGNN on (UCF, HMDB) dataset; (2) AURL \\\\(\\\\times\\\\) w/ more strict requirements but w/o extra datasets competes against all the SoTA alternatives, e.g., (58.0, 39.0) of AURL \\\\(\\\\times\\\\) vs. (51.8, 35.3) of ER, and (56.3, 31.0) of MUFI on (UCF, HMDB) dataset.\\n\\nTo sum up, our AURL reaches the new SoTA in ZSVC.\\n\\nTable 4. Comparisons with SoTA alternatives on both UCF and HMDB datasets. Results of alternatives were obtained from original papers, and the higher, the better. Red and blue numbers indicate the best and second best. \\\\(\\\\times\\\\) means using \\\\(\\\\times\\\\) = 0.05 in Eq. 10.\\n\\n| Method | Test splits | Train dataset | UCF \\\\(\\\\text{top-1}\\\\) | HMDB \\\\(\\\\text{top-1}\\\\) |\\n|--------|-------------|---------------|----------------|----------------|\\n| SoTA \\\\cite{3} | 1 | Kinetics | 37.6 | 26.9 |\\n| AURL | 1 | Kinetics | 46.8 | 31.7 |\\n| Obj2act \\\\cite{16} | 3 | - | -30.3 | -15.6 |\\n| SAE \\\\cite{27} | 3 | - | -32.8 | - |\\n| OPCL \\\\cite{13} | 3 | - | -36.3 | - |\\n| MUFI \\\\cite{35} | 3 | Kinetics + | 56.3 | 31.0 |\\n| AURL | 3 | Kinetics | 60.9 | 40.4 |\\n| TARN \\\\cite{2} | 30 | UCF | 23.2 | 19.5 |\\n| Act2Vec \\\\cite{14} | - | UCF | 22.1 | 23.5 |\\n| SAE \\\\cite{27} | 10 | - | -40.4 | - |\\n| PSGNN \\\\cite{13} | 50 | UCF | 43.0 | 32.6 |\\n| OPCL \\\\cite{13} | 10 | - | -47.3 | - |\\n| SoTA \\\\cite{3} | 10 | Kinetics | 48.0 | 26.7 |\\n| DASZL \\\\cite{19} | 10 | - | -48.9 | - |\\n| ER \\\\cite{6} | 50 | UCF | 51.8 | 35.3 |\\n| AURL | 10 | Kinetics | 58.0 | 39.0 |\\n\\nFinally, we conduct AURL \\\\(\\\\times\\\\) with pre-extracted features (trained only on video and word projectors). We observe AURL w/ pre-extracted features achieves comparable performance with the e2e AURL \u2013 (59.5, 38.2) vs. (58.0, 39.0) on UCF and HMDB. This suggests AURL can achieve high performance without carefully finetuning video features.\\n\\nLimitations and possible solutions. Even our AURL achieves promising results, there are still two problems to be concerned. (1) Uniformity of visual features within classes could be included to further increase info-preserving, introducing contrastive learning between videos may be a possible solution. (2) It will be helpful to study how the class generator affects the overall optimization during training.\\n\\n5. Conclusion\\n\\nThis paper learns representation awareness of both alignment and uniformity properties for seen and unseen classes. We reformulate a supervised contrastive loss to jointly align visual-semantic features and encourage semantic clusters to distribute uniformly. To explicitly synthesize features of unseen semantics, we propose a class generator that performs feature transformation on features of seen classes. Besides, we introduce closeness and dispersion to quantify the two properties, providing new measurements for generalization. Extensive ablations justify the effectiveness of each module in our model. Comparisons with the SoTA alternatives validate our model reaches the new SoTA results.\\n\\nAcknowledgement. Research was supported by Natural Science Foundation of China under grant 62076036.\"}"}
{"id": "CVPR-2022-107", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A large-scale video classification benchmark. *arXiv:1609.08675*, 2016.\\n\\n[2] Mina Bishay, Georgios Zoumpourlis, and Ioannis Patras. Tarn: Temporal attentive relation network for few-shot and zero-shot action recognition. *arXiv:1907.09021*, 2019.\\n\\n[3] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In *CVPR*, 2020.\\n\\n[4] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. *arXiv:2006.09882*, 2020.\\n\\n[5] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In *CVPR*, 2017.\\n\\n[6] Shizhe Chen and Dong Huang. Elaborative rehearsal for zero-shot action recognition. In *ICCV*, 2021.\\n\\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In *ICML*, 2020.\\n\\n[8] Ting Chen, Calvin Luo, and Lala Li. Intriguing properties of contrastive losses. *arXiv preprint arXiv:2011.02803*, 2020.\\n\\n[9] Cunxiao Du, Zhaozheng Chen, Fuli Feng, Lei Zhu, Tian Gan, and Liqiang Nie. Explicit interaction model towards text classification. In *AAAI*, 2019.\\n\\n[10] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In *ICCV*, 2019.\\n\\n[11] Yanwei Fu, Timothy M Hospedales, Tao Xiang, and Shao-gang Gong. Transductive multi-view zero-shot learning. *TPAMI*, pages 2332\u20132345, 2015.\\n\\n[12] Chuang Gan, Tianbao Yang, and Boqing Gong. Learning attributes equals multi-source domain generalization. In *CVPR*, 2016.\\n\\n[13] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. Learning to model relationships for zero-shot video classification. *TPAMI*, pages 3476\u20133491, 2020.\\n\\n[14] Meera Hahn, Andrew Silva, and James M Rehg. Action2vec: A crossmodal embedding approach to action learning. *arXiv:1901.00484*, 2019.\\n\\n[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In *CVPR*, 2020.\\n\\n[16] Mihir Jain, Jan C Van Gemert, Thomas Mensink, and Cees GM Snoek. Objects2action: Classifying and localizing actions without any video example. In *ICCV*, 2015.\\n\\n[17] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal and motion encoding for action recognition. In *ICCV*, 2019.\\n\\n[18] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. *arXiv:1705.06950*, 2017.\\n\\n[19] Tae Soo Kim, Jonathan Jones, Michael Peven, Zihao Xiao, Jin Bai, Yi Zhang, Weichao Qiu, Alan Yuille, and Gregory D Hager. Daszl: Dynamic action signatures for zero-shot learning. In *AAAI*, 2021.\\n\\n[20] Haofei Kuang, Yi Zhu, Zhi Zhang, Xinyu Li, Joseph Tighe, S\u00f6ren Schwertfeger, Cyrill Stachniss, and Mu Li. Video contrastive learning with global context. *arXiv:2108.02722*, 2021.\\n\\n[21] Hildegard Kuehne, Hueihan Jhuang, Estibaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In *ICCV*, 2011.\\n\\n[22] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In *ICCV*, 2019.\\n\\n[23] Rongcheng Lin, Jing Xiao, and Jianping Fan. Nextvlad: An efficient neural network to aggregate frame-level features for large-scale video classification. In *ECCV Workshops*, 2018.\\n\\n[24] Jingen Liu, Benjamin Kuipers, and Silvio Savarese. Recognizing human actions by attributes. In *CVPR*, 2011.\\n\\n[25] Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Tong Lu. Teinet: Towards an efficient architecture for video recognition. In *AAAI*, 2020.\\n\\n[26] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. *arXiv:1608.03983*, 2016.\\n\\n[27] Pascal Mettes and Cees GM Snoek. Spatial-aware object embeddings for zero-shot localization and classification of actions. In *ICCV*, 2017.\\n\\n[28] Pascal Mettes, William Thong, and Cees GM Snoek. Object priors for classifying and localizing unseen actions. *IJCV*, pages 1954\u20131971, 2021.\\n\\n[29] Antoine Miech, Ivan Laptev, and Josef Sivic. Learnable pooling with context gating for video classification. *arXiv:1706.06905*, 2017.\\n\\n[30] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. *arXiv:1301.3781*, 2013.\\n\\n[31] Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, and Wei Liu. Videomoco: Contrastive video representation learning with temporally adversarial examples. In *CVPR*, 2021.\\n\\n[32] Ofir Press and Lior Wolf. Using the output embedding to improve language models. *arXiv:1608.05859*, 2016.\\n\\n[33] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal contrastive video representation learning. In *CVPR*, 2021.\\n\\n[34] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d residual networks. In *ICCV*, 2017.\\n\\n[35] Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xiao-Ping Zhang, Dong Wu, and Tao Mei. Boosting video representation learning with multi-faceted integration. In *CVPR*, 2021.\"}"}
{"id": "CVPR-2022-107", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv:2103.00020, 2021.\\n\\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In CVPR, 2015.\\n\\nKaren Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In NIPS, 2014.\\n\\nKihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NeurlIPs, 2016.\\n\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv:1212.0402, 2012.\\n\\nSwathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz. Gate-shift networks for video action recognition. In CVPR, 2020.\\n\\nDu Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3D convolutional networks. In ICCV, 2015.\\n\\nDu Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In CVPR, 2018.\\n\\nFeng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu. Additive margin softmax for face verification. IEEE Signal Processing Letters, pages 926\u2013930, 2018.\\n\\nFeng Wang, Xiang Xiang, Jian Cheng, and Alan Loddon Yuille. Normface: L2 hypersphere embedding for face verification. In Proceedings of the 25th ACM international conference on Multimedia, 2017.\\n\\nLimin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, 2016.\\n\\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML, 2020.\\n\\nNicolai Wojke and Alex Bewley. Deep cosine metric learning for person re-identification. In WACV, 2018.\\n\\nYongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning\u2014a comprehensive evaluation of the good, the bad and the ugly. TPAMI, pages 2251\u20132265, 2018.\\n\\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.\\n\\nTimothy M. Hospedales Xu, Xun and Shaogang Gong. Multi-task zero-shot action recognition with prioritised data augmentation. In ECCV, 2016.\\n\\nChenrui Zhang and Yuxin Peng. Visual data synthesis via gan for zero-shot video classification. In IJCV, 2018.\\n\\nRui Zhu, Bingchen Zhao, Jingen Liu, Zhenglong Sun, and Chang Wen Chen. Improving contrastive learning by visualizing feature transformation. arXiv:2108.02982, 2021.\\n\\nYi Zhu, Yang Long, Yu Guan, Shawn Newsam, and Ling Shao. Towards universal representation for unseen action recognition. In CVPR, 2018.\"}"}
