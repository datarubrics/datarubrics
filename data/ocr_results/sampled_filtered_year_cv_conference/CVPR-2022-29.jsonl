{"id": "CVPR-2022-29", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"quantum system from the initial Hamiltonian to the final Hamiltonian. Current quantum annealers are not able to completely isolate external noise from the process, which affects the quality of the solution.\\n\\nTo obtain a useful solution, during the annealing process, the system must have a non-negligible probability of staying in the lowest energy state. If the system jumps to a higher energy state, it will fail in solving the QUBO (19) optimally. The spectral gap is the minimum gap between the lowest and the second lowest (higher) energy states, which affects the probability of staying in the lowest energy state; see [24, 31] for details. We will investigate the spectral gap issue in Sec. D of the supplementary material.\\n\\nWhy quantum annealing?\\n\\nThe above issues limit the scale of problems and quality of solutions attainable with current quantum annealers. However, quantum technology is advancing steadily, and the vision community should be prepared for potential breakthroughs, as like-minded colleagues are also advocating [7, 8, 19, 34, 63, 65]. Moreover, our main algorithm combines quantum and classical computation to leverage the strengths of both paradigms.\\n\\n5. Main algorithm\\n\\nAlg. 1 presents our overall algorithm. At its core, our algorithm aims to solve (VC), i.e., find the minimum outlier set, but by incrementally generating the hyperedges $A \\\\subseteq E$.\\n\\nOther main characteristics of the algorithm are:\\n\\n\u2022 At each iteration, the QUBO (19) based on the current hyperedges $A$ is solved using quantum annealing.\\n\\n\u2022 The penalty $\\\\lambda$ for (19) decays following a schedule defined by hyperparameters $\\\\bar{\\\\lambda}$, $\\\\gamma$ and $\\\\bar{M}$ (Step 6).\\n\\n\u2022 Hyperedges are sampled from a candidate vertex set $V'$, which is updated based on the current results (Sec. 5.2).\\n\\nThe algorithm terminates with the best estimate $z_{\\\\text{best}}$ of the minimum outlier set and the sampled hyperedges $A$.\\n\\nIn the following, we show how the outputs of Alg. 1 can be used to derive an error bound for consensus maximization, and the rationale of our hyperedge sampling technique.\\n\\n5.1. Error bound\\n\\nConsider the relaxation of (15) $\\\\text{LP}(A) = \\\\min_{z \\\\in [0, 1]^N} \\\\|z\\\\|_1$, s.t. $A^T z \\\\geq 1$ (20), which is a linear program. We must have that $\\\\text{LP}(A) \\\\leq I(A) \\\\leq I(E)$.\\n\\nDue to the factors in Sec. 4.3, the solution $z$ by the quantum annealer on (19) can be suboptimal. Given the best solution $z_{\\\\text{best}}$, if the $I_{\\\\text{best}} = V \\\\setminus C z_{\\\\text{best}}$ is a consensus set, by Claim 2, $z_{\\\\text{best}}$ is a vertex cover of (VC). We must have that $\\\\text{LP}(A) \\\\leq I(E) \\\\leq \\\\|z_{\\\\text{best}}\\\\|_1$.\\n\\nAlgorithm 1\\n\\nHybrid Quantum-Classical Robust Fitting.\\n\\nNote: Only Step 8 invokes the quantum annealer.\\n\\nRequire: Data $D = \\\\{p_i\\\\}_{i=1}^N$, inlier threshold $\\\\epsilon$, maximum iterations $M$, penalty $\\\\lambda$ with decay parameters $\\\\bar{\\\\lambda}$, $\\\\gamma$, $\\\\bar{M}$.\\n\\n1: Initialise hyperedge set $A \\\\leftarrow \\\\emptyset$, candidate vertices $V' \\\\leftarrow V$, best outlier set $z_{\\\\text{best}} \\\\leftarrow 1_N$.\\n2: for $m = 1$ to $M$ do\\n3: \\\\hspace{1em} $a(m) \\\\leftarrow$ Active set of $V'$ (see Sec. 5.2).\\n4: \\\\hspace{1em} $A \\\\leftarrow A \\\\cup \\\\{C a(m)\\\\}$.\\n5: \\\\hspace{1em} if $m \\\\mod \\\\bar{M} = 0$ then\\n6: \\\\hspace{2em} $\\\\lambda \\\\leftarrow \\\\max(\\\\lambda/\\\\gamma, \\\\bar{\\\\lambda})$.\\n7: \\\\hspace{1em} end if\\n8: \\\\hspace{1em} $v = [z_1 ... z_t] \\\\leftarrow$ Solve (19) using quantum annealing\\n9: \\\\hspace{1em} if $f(V \\\\setminus C z) = 0$ then\\n10: \\\\hspace{2em} $I \\\\leftarrow V \\\\setminus C z$ (found a consensus set).\\n11: \\\\hspace{2em} if $\\\\|z_{\\\\text{best}}\\\\|_1 < \\\\|z\\\\|_1$ then\\n12: \\\\hspace{3em} $z_{\\\\text{best}} \\\\leftarrow z$.\\n13: \\\\hspace{2em} end if\\n14: \\\\hspace{1em} $V' \\\\leftarrow C z \\\\cup \\\\{\\\\text{random subset of } I\\\\}$ (see Sec. 5.2).\\n15: \\\\hspace{1em} else\\n16: \\\\hspace{2em} $V' \\\\leftarrow V \\\\setminus C z$ (see Sec. 5.2).\\n17: \\\\hspace{1em} end if\\n18: end for\\n19: return Outlier set estimate $z_{\\\\text{best}}$ and hyperedge set $A$.\\n\\nUsing the fact that $|I^*| = N - I(E)$, we thus have $|I^*| - |I_{\\\\text{best}}| \\\\leq \\\\|z_{\\\\text{best}}\\\\|_1 - \\\\text{LP}(A)$.\\n\\nIf the RHS is 0, then $I_{\\\\text{best}}$ is the globally optimal solution.\\n\\n5.2. Heuristic for sampling hyperedges\\n\\nRecall that a hyperedge is an infeasible basis. A simple way to generate hyperedges is to randomly sample $\\\\delta$-subsets from $V$ until we find an infeasible subset, which will not be efficient. To improve efficiency, our sampling technique maintains a candidate vertex set $V' \\\\subseteq V$ where $f(V') = 1$, and takes the active set $S$ of $V'$ [3, 30], where $g(S) = g(V')$, as a hyperedge. Intuitively, the active set of $V'$ is a basis with equal value with $V'$. To generate diverse hyperedges, two strategies are employed to maintain $V'$:\\n\\n\u2022 Take $V' = V \\\\setminus C z$ if it is not a consensus set (Step 16);\\n\u2022 If a new consensus set $I = V \\\\setminus C z$ is found (Step 10), set $V'$ as the union of $C z$ with a random subset of $I$ (Step 14).\\n\\n5.3. Hyperparameter selection\\n\\nThe penalty value and decay schedule play important roles in Alg. 1 to quickly find a consensus set and tighten the error bound, see Sec. E in supplementary material for details. The precise values for the parameters used and/or investigated in our experiments will be provided in Sec. 6.\"}"}
{"id": "CVPR-2022-29", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. Experiments\\n\\n6.1. Synthetic data\\n\\nWe first examine the performance of D-Wave Advantage (version 1.1) [23] on our robust fitting formulation via synthetic data. We generated 2D points\\n\\n\\\\[ D = \\\\{ (a_i, b_i) \\\\} \\\\]\\n\\nfor 1D linear regression \\\\((x \\\\in \\\\mathbb{R}^1)\\\\) with residual\\n\\n\\\\[ r_i(x) = |a_i x - b_i|, \\\\]\\n\\nwhere \\\\(0 \\\\leq a_i, b_i \\\\leq 1\\\\). For a randomly chosen ground truth \\\\(x\\\\), a proportion of the points are corrupted with Gaussian noise of \\\\(\\\\sigma_{in} = 0.1\\\\) to form inliers, with the rest by Gaussian noise of \\\\(\\\\sigma_{out} = 1.5\\\\) to simulate outliers.\\n\\nDue to the cost of accessing the QPU, our results in this subsection were not derived from many data repetitions. However, each QPU input instance was invoked with 10,000 anneals, which typically consumed \\\\(\\\\approx 1.3\\\\) seconds. Also, see Sec. F in the supp. material for details on embedding (19) (e.g., number of qubits, runtime) onto the QPU.\\n\\nComparison between CPU and QPU.\\n\\nWe first compare CPU and QPU performance on our QUBO (19) (i.e., independent of Alg. 1), with \\\\(A\\\\) containing all hyperedges \\\\(E\\\\).\\n\\nThe CPU solver used was Gurobi [35], which solves (19) exactly via exhaustive search, hence practical only for small instances. Fig. 2 plots the number of outliers \\\\(\\\\|z\\\\|_1\\\\) (lower is better) optimised by the solvers as a function of\\n\\n- Penalty \\\\(\\\\lambda \\\\in [0.1, 100]\\\\), with \\\\(N = 50\\\\), outlier ratio \\\\(0.2\\\\).\\n- Outlier ratio \\\\(\\\\in [0.1, 0.6]\\\\), with \\\\(N = 20\\\\), \\\\(\\\\lambda = 1.0\\\\).\\n- \\\\(N \\\\in [10, 100]\\\\), with \\\\(\\\\lambda = 1.0\\\\), outlier ratio \\\\(0.2\\\\).\\n\\nAs expected (see Sec. 4.3), the gap in quality between the QPU solution and the \u201cground truth\u201d provided by Gurobi increased with the examined parameters, indicating that the QPU is more reliable on \u201ceasier\u201d instances of (19).\\n\\nMain algorithm\\n\\nFig. 3 illustrates running Alg. 1 on synthetic 1D linear regression instances with \\\\(N = 20, 50, 100\\\\) points, each with outlier ratio \\\\(0.2\\\\). The QUBO subroutine (19) in the main algorithm was solved using the QPU with \\\\(\\\\lambda = 1.0\\\\) (no \\\\(\\\\lambda\\\\) decay was done). The values \\\\(\\\\|z\\\\|_1\\\\) and \\\\(LP(A)\\\\) were plotted as a function of the size of \\\\(A\\\\), i.e., number of hyperedges. The results mainly illustrate the feasibility of solving robust fitting using quantum annealing.\\n\\nComparing to simulated annealing\\n\\nIn the context of Alg. 1, we compared quantum annealing (QA) and simulated annealing (SA) [37] (on CPU with 10,000 anneals) in solving the QUBO subroutine (Line 8 of Alg. 1). A synthetic 1D linear regression instance with \\\\(N = 20\\\\) and outlier ratio \\\\(0.2\\\\) was generated. The penalty \\\\(\\\\lambda\\\\) was set to \\\\(0.5\\\\) (no \\\\(\\\\lambda\\\\) decay was done). Fig. 4 shows the runtime of QA and SA across the iterations of Alg. 1 (for QA, the cost of embedding (19) onto the QPU was excluded; again, see Sec. F in the supp. material for details), and the Hamming distance between the \\\\(z\\\\)'s found by the methods in each iteration.\"}"}
{"id": "CVPR-2022-29", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We compared our method to i) random sampling methods: RANSAC (RS) [32], LO-RANSAC (LRS) [20], and Fixing LO-RANSAC (FLRS) [43], ii) deterministic algorithms: Exact penalty (EP) [41], and Iterative biconvex optimization (IBCO) [14], and iii) quantum robust fitting (QRF) [19]. Each method was run 100 times and average results were reported. All experiments were conducted on a system with 2.6 GHz processor and 16 GB of RAM.\\n\\n6.2.1 Fundamental matrix estimation.\\nWe evaluated our method on linearised fundamental matrix fitting [61, Chapter 4], where $x \\\\in \\\\mathbb{R}^8$. We used inlier threshold $\\\\epsilon = 0.03$ for the algebraic residual (convex in $x$, hence also quasiconvex), and penalty parameters $\\\\lambda = 1.0$, $\\\\gamma = 0.5$, $\\\\bar{M} = 50$, and $\\\\bar{\\\\lambda} = 0.01$. We used three image pairs from VGG [72] (Castle, Valbonne, and Zoom) and three image pairs from sequence 00 of KITTI odometry [33] (frame indices 104-108, 198-201, and 738-742). In each pair, SIFT features [48] were extracted and matched using VLFeat [1]; Lowe's second nearest neighbour test was also applied to prune matches.\\n\\nFig. 5 shows the intermediate outputs of Alg. 1-F on Castle, KITTI 198-201 and KITTI 738-742, particularly the lower bound of the solution. See Sec. G in the supplementary material for the plots for the other image pairs.\\n\\nTable 1 compares our method with the others. Overall, the quality of our method was comparable to the others, with Alg. 1-F providing higher quality and tighter bound than Alg. 1-E. Note that only our method returned error bounds (Sec. 5.1), which allowed to deduce that Alg. 1-F found consensus sets that were close to the optimum. As expected, the fastest methods were the random sampling approaches. Our method was much slower than the others, mainly due to the usage of SA. However, our experiments in Sec. 6.1 shows that QA can improve the speed of SA up to a factor of 10 without affecting solution quality (see Fig. 4). Hence, we expect Alg. 1 to be more competitive as quantum annealer capacity improves. Fig. 6 qualitatively illustrates the results of Alg. 1-E.\\n\\n6.2.2 Multi-view triangulation\\nPoints 134 & 534 from Nikolai, points 1 & 14 from Linkoping, and points 3 & 132 from Tower [29] were used. In this task, 3D coordinates of those 3D points ($x \\\\in \\\\mathbb{R}^3$) were estimated using reprojection error (which is quasiconvex [38]) under outliers. The inlier threshold and penalty were respectively $\\\\epsilon = 1$ pixels and $\\\\lambda = 5$. The decay parameters were $\\\\gamma = 0.5$, $\\\\bar{M} = 50$, and $\\\\bar{\\\\lambda} = 0.03$.\\n\\nFig. 7 shows the intermediate outputs of Alg. 1-F on Nikolai point 134, Linkoping point 1 and Tower point 132, particularly the lower bound of the solution. See Sec. G in the supplementary material for the plots for the other points. Interestingly, the results show that it was more difficult to find a tight lower bound here (especially Nikolai point 134 and Linkoping point 1). This could be due to numerical inaccuracies in solving the minimax problem (3) for quasiconvex residuals [3, 30], which affected the efficacy of hyperedge sampling. Table 2 shows the quantitative results; a similar conclusions as that of Table 1 can be drawn. In particular, note that only our method was able to provide error bounds; in the case of Tower point 132, the global solution was provably found by the algorithm (gap is zero).\\n\\n7. Weaknesses and conclusions\\nThere are two main shortcomings: First, Alg. 1 was validated on an actual quantum computer only for small scale synthetic data (for reasons covered in Sec. 4.3). To fully realise the potential of the algorithm, testing with real data on a quantum computer is needed. Second, our results reveal that the hyperedge sampling procedure is also crucial to Alg. 1. Developing a more effective way of sampling hyperedges is an interesting research direction.\"}"}
{"id": "CVPR-2022-29", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Fundamental matrix estimation results. Alg. 1 employed simulated annealing (quantum annealing could be faster by \\\\(10\\\\) times).\\n\\nOnly Alg. 1 amongst all methods compared here returned error bounds.\\n\\nTable 2. Multi-view triangulation results. Alg. 1 employed simulated annealing (quantum annealing could be faster by \\\\(10\\\\) times). Only Alg. 1 amongst all methods compared here returned error bounds.\\n\\nConclusions\\n\\nOur work illustrates the potential of quantum annealing for robust fitting. It outperforms (in simulation) the only other quantum approach in robust fitting [19] as well as offers an error bound to mitigate the weakness of current QPU. We hope that our work helps trigger further development on applying quantum computers in robust fitting and computer vision applications.\\n\\nAcknowledgement\\n\\nThis work was supported by Australian Research Council ARC DP200101675, and D. Suter acknowledges funding under Australian Research Council grant DP200103448.\"}"}
{"id": "CVPR-2022-29", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Hybrid Quantum-Classical Algorithm for Robust Fitting\\n\\nAnh-Dzung Doan\\nMichele Sasdelli\\nDavid Suter\\nTat-Jun Chin\\n\\n1 School of Computer Science, The University of Adelaide, South Australia\\n2 Centre for AI&ML, School of Science, Edith Cowan University, Western Australia\\n\\nAbstract\\nFitting geometric models onto outlier contaminated data is provably intractable. Many computer vision systems rely on random sampling heuristics to solve robust fitting, which do not provide optimality guarantees and error bounds. It is therefore critical to develop novel approaches that can bridge the gap between exact solutions that are costly, and fast heuristics that offer no quality assurances. In this paper, we propose a hybrid quantum-classical algorithm for robust fitting. Our core contribution is a novel robust fitting formulation that solves a sequence of integer programs and terminates with a global solution or an error bound. The combinatorial subproblems are amenable to a quantum annealer, which helps to tighten the bound efficiently. While our usage of quantum computing does not surmount the fundamental intractability of robust fitting, by providing error bounds our algorithm is a practical improvement over randomised heuristics. Moreover, our work represents a concrete application of quantum computing in computer vision. We present results obtained using an actual quantum computer (D-Wave Advantage) and via simulation.\\n\\n1. Introduction\\nImperfections in sensing and processing in computer vision inevitably generate data that contain outliers. Therefore, it is necessary for vision pipelines to be robust against outliers in order to mitigate their harmful effects. In 3D vision, where a major goal is to recover the scene structure and camera motion, a basic task is to fit a geometric model onto noisy and outlier prone measurements. This is often achieved through the consensus maximisation framework [18]: given $N$ data points $D = \\\\{p_i\\\\}_{i=1}^N$ and a target geometric model parametrised by $x \\\\in \\\\mathbb{R}^d$, let $P_N$ be the power set of index set $\\\\{1, \\\\ldots, N\\\\}$. We aim to solve\\n\\n$$\\\\max_{I \\\\in P_N, x \\\\in \\\\mathbb{R}^d} |I| \\\\text{ s.t. } r_i(x) \\\\leq \\\\epsilon \\\\ \\\\forall \\\\ i \\\\in I,$$\\n\\nwhere $r_i(x)$ is the residual of point $p_i$ w.r.t. $x$, and $\\\\epsilon$ is a given inlier threshold. The form of $r_i(x)$ depends on the specific geometric model (more details in Sec. 3). A candidate solution $(I, x)$ consists of a consensus set $I$ and its \\\"witness\\\" (an estimate) $x$, where the points in $I$ are the inliers of $x$. Problem (1) seeks the maximum consensus set $I^*$, whose witness $x^*$ is a robust estimate of the model. Many computer vision systems employ random sampling heuristics, i.e., RANSAC [32] and its variants (e.g., [5, 6, 20, 57, 68, 69]), for consensus maximisation. The basic idea is to repeatedly fit the model on randomly sampled minimal subsets of $D$, and return the $\\\\tilde{x}$ with the largest consensus set $\\\\tilde{I}$. Such heuristics can only approximate (1) and generally do not provide optimality guarantees or error characterisation, e.g., a tight bound on the discrepancy $|I^*| - |\\\\tilde{I}|$. Moreover, $\\\\tilde{x}$ is subject to randomness, and post-processing or reruns are often executed to vet the result. Unfortunately, consensus maximisation is provably intractable [4, 16], hence there is little hope in finding efficient algorithms that can solve it exactly. While there has been active research into globally optimal algorithms [13, 17, 44, 54, 55], such techniques are realistic only for small input instances (small $d$, $N$ and/or number of outliers [17]).\\n\\nBridging the gap between exact algorithms that are costly and randomised heuristics that offer no quality assurances is an important research direction in robust fitting with practical ramifications. Towards this aim, deterministic approximate algorithms [14, 41, 42, 59, 73] eschew exhaustive search (e.g., branch-and-bound) and randomisation, and instead adopt deterministic subroutines such as convex optimisation, proximal splitting, etc. These methods avoid the vagaries of random sampling, and some can even guarantee convergence [41, 42, 59]. However, none of them provide error bounds. Indeed, complexity results [4,16] also preclude efficient approximate solutions with error bounds. Partly buoyed by the dominance of deep learning in computer vision, learning-based solutions to robust geometric fitting have been developed [10, 60, 70]. Such techniques leverage statistics in large datasets to learn a mapping from...\"}"}
{"id": "CVPR-2022-29", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the input instance to the desired solution. Despite showing promising results in benchmark datasets, learning methods do not provide optimality guarantees and error bounds. Whether the learned model can generalise is also a concern.\\n\\nTo summarise, existing algorithms for robust fitting, particularly those targeted at consensus maximisation, have yet to satisfactorily solve the problem. It is thus worthwhile to investigate novel approaches based on new insights.\\n\\nOur contributions\\n\\nWe propose a new approach that leverages quantum computing for consensus maximisation. Our core contribution is a consensus maximisation algorithm that iteratively solves a sequence of integer programs and terminates with either $x^*$ or a suboptimal solution $\\\\tilde{x}$ with a known error bound $|I^*| - |\\\\tilde{I}| \\\\leq \\\\rho$. The integer programs are amenable to a quantum annealer [64, Chap. 8], which is utilised to tighten the bound efficiently. Since our method employs convex subroutines and random sampling, it is a hybrid quantum-classical algorithm [11, 36, 40, 56].\\n\\nWe will present results using an actual quantum computer, the D-Wave Advantage [23], as well as simulation. While our technique does not yet outperform state-of-the-art algorithms, in part due to the limitations of current quantum technology, our work represents a concrete application of quantum computing in computer vision. We hope to inspire future efforts on this topic in the community.\\n\\n2. Related work\\n\\nIn Sec. 1, we have provided an overview of robust fitting and recent algorithmic advances. We thus focus our survey on quantum computing in computer vision. Many quantum methods have been proposed for image processing [15, 27, 71, 74], image recognition [26, 50, 51], and object detection [45]. Also, several methods explored the tasks of classification and training a deep neural network [39, 52, 63]. Recently, Golyanik and Theobalt [34] proposed a practical quantum algorithm for rotation estimation to align two point sets. Their basic idea is to discretise rotation matrices to formulate the problem to the quadratic unconstrained binary optimization (QUBO), which can be solved by quantum annealers. Benkner et al. [7] proposed to solve the graph matching problem through formulating the quadratic assignment problem (QAP) to the QUBO using the penalty approach. They conducted the experiment and provided analysis on the quantum computer D-Wave 2000Q. However, the limitation of quantum computers precluded them from solving large problems. To address this issue, instead of enforcing a penalty to QAP, Q-Match [65] was proposed to iteratively select and solve subproblems of QAP, which allows D-Wave annealers to efficiently deal with large problems. Another interesting work is QuantumSync [8], which addresses the synchronisation problem in the context of multi-image matching. This work carefully formulated the synchronisation problem to the QUBO, which was then validated on D-Wave Advantage.\\n\\nThe closest work to ours is [19], who proposed a quantum solution for robust fitting. However, there are non-trivial differences: first, [19] estimates per-point influences (a measure of outlyingness) [66, 67] for outlier removal instead of consensus maximisation. Second, their algorithm is based on the gate computing model, which is fundamentally different from the quantum annealing approach adopted in our work. Third, the results in [19] are only based on simulation; we will compare against [19] on this basis in Sec. 6.\\n\\n3. Reformulating consensus maximisation\\n\\nIn this section, we describe our novel reformulation for consensus maximisation and relevant theoretical results, before presenting the usage of quantum annealing in Sec. 4 and the overall algorithm in Sec. 5.\\n\\n3.1. Preliminaries\\n\\nFollowing [19], we consider residuals $r_i(x)$ that are quasiconvex, which encapsulates many geometric models of interest in computer vision [38]. Formally, if the set $\\\\{x \\\\in \\\\mathbb{R}^d | r_i(x) \\\\leq \\\\alpha\\\\}$ is convex for all $\\\\alpha \\\\geq 0$, then $r_i(x)$ is quasiconvex. Note that assuming quasiconvex residuals does not reduce the computational hardness of consensus maximisation [16].\\n\\nFor $C \\\\in \\\\mathbb{P}_N$, define the minimax problem\\n\\\\begin{equation}\\n    g(C) = \\\\min_{x \\\\in \\\\mathbb{R}^d} \\\\max_{i \\\\in C} r_i(x).\\n\\\\end{equation}\\nFor quasiconvex $r_i(x)$, (3) is a quasiconvex program [3, 30], which is polynomial-time solvable. Note that $g(C) \\\\leq \\\\epsilon$ implies that $C$ is a consensus set, since all the points in $C$ are within error $\\\\epsilon$ to the extremiser of (3).\\n\\nDefine the \\\"feasibility test\\\" $f(C) = \\\\begin{cases} 0 & \\\\text{if } g(C) \\\\leq \\\\epsilon; \\\\\\\\ 1 & \\\\text{otherwise}\\\\end{cases}$.\\nAny $C$ such that $f(C) = 0$ implies that $C$ is a consensus set. Problem (1) can thus be restated as\\n\\\\begin{equation}\\n    \\\\max_{I \\\\in \\\\mathbb{P}_N} |I|, \\\\quad \\\\text{s.t.} \\\\quad f(I) = 0,\\n\\\\end{equation}\\nwith the witness $x$ for any feasible $I$ obtainable through computing $g(I)$ to evaluate $f(I)$.\\n\\nGiven a consensus set $I$ with witness $x$, the points in the complement $O = \\\\{1, \\\\ldots, N\\\\} \\\\setminus I$ are the outliers to $x$. The \\\"dual\\\" of problem (5) is therefore\\n\\\\begin{equation}\\n    \\\\min_{O \\\\in \\\\mathbb{P}_N} |O|, \\\\quad \\\\text{s.t.} \\\\quad f(\\\\{1, \\\\ldots, N\\\\} \\\\setminus O) = 0,\\n\\\\end{equation}\\ne.g., find the model with the least number of outliers.\"}"}
{"id": "CVPR-2022-29", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Definition 1\\n(True inliers and true outliers).\\n\\nLet $I^*$ be the maximum consensus set and $O^* = \\\\{1, \\\\ldots, N\\\\} \\\\setminus I^*$. We call $I^*$ the \\\"true inliers\\\" and $O^*$ the \\\"true outliers\\\".\\n\\nProperty 1 (Monotonicity).\\nFor (3) with quasiconvex residuals, given subsets $P, Q, R \\\\in \\\\mathbb{P}_N$ with $P \\\\subseteq Q \\\\subseteq R$, we have $g(P) \\\\leq g(Q) \\\\leq g(R)$. By extension, we also have that $f(P) \\\\leq f(Q) \\\\leq f(R)$. See [3, 30] for more details.\\n\\nIntuitively, adding points to a feasible subset can only potentially make it infeasible; the converse cannot be true. This leads to the following crucial concept.\\n\\nDefinition 2 (Basis).\\nA basis $B \\\\subset \\\\{1, \\\\ldots, N\\\\}$ is a subset such that $g(B') < g(B)$ for every $B' \\\\subset B$.\\n\\nIntuitively, removing any point from a basis $B$ will cause the minimax value of the subset to shrink.\\n\\nProperty 2 (Combinatorial dimension).\\nThe combinatorial dimension $\\\\delta$ of minimax problem (3) is the upper bound on the size of bases [3,30]. For quasiconvex $r_i(x)$, $\\\\delta = 2d+1$.\\n\\nClaim 1. If basis $B$ is infeasible, i.e., $f(B) = 1$, then $|B \\\\cap O^*| \\\\geq 1$, i.e., an infeasible basis $B$ contains at least one true outlier.\\n\\nProof. See Sec. A in supplementary material.\\n\\n3.2. Hypergraph vertex cover\\nDefine the binary $N$-vector $z = [z_1, \\\\ldots, z_N] \\\\in \\\\{0, 1\\\\}^N$, where the set of indices corresponding to nonzero $z_i$'s are $C_z = \\\\{i \\\\in \\\\{1, \\\\ldots, N\\\\} | z_i = 1\\\\}$.\\n\\nThe outlier minimisation problem (6) can be reexpressed as\\n\\\\[\\n\\\\min_{z \\\\in \\\\{0, 1\\\\}^N} \\\\|z\\\\|_1, \\\\quad \\\\text{s.t.} \\\\quad f(\\\\{1, \\\\ldots, N\\\\} \\\\setminus C_z) = 0,\\n\\\\]\\nwhere $z_i = 1$ implies the $i$-th point is removed as an outlier.\\n\\nLet $\\\\{b(k)\\\\}_{k=1}^K$ be $K$ binary $N$-vectors that correspond to all infeasible bases of the problem, i.e., for each $k$, $f(C_{b(k)}) = 1$, $\\\\|b(k)\\\\|_1 \\\\leq \\\\delta$, where the latter appeals to the combinatorial dimension (Property 2). Also, the number of infeasible bases $K = O(N^{\\\\delta})$. Define the hypergraph $H$ with vertex set $V$ and hyperedge set $E$ respectively as\\n\\\\[\\nH = \\\\{V, E\\\\}, \\\\quad V = \\\\{1, \\\\ldots, N\\\\}, \\\\quad E = \\\\{C_{b(k)}\\\\}_{k=1}^K.\\n\\\\]\\nRecall that hypergraphs are a generalisation of graphs, where a hyperedge can be incident with more than two vertices [2]. In our hypergraph (11), each hyperedge connects vertices that form an infeasible basis; see Fig. 1.\\n\\nClaim 2. A subset $I \\\\subseteq V$ is a consensus set iff it is an independent set of hypergraph $H$.\\n\\nProof. See Sec. B in supplementary material.\\n\\nClaim 2 proves that finding the maximum consensus set $I^*$ is equivalent to finding the maximum independent set of $H$. Since the complement of an independent set is a vertex cover, it justifies to minimise the vertex cover\\n\\\\[\\n\\\\min_{z \\\\in \\\\{0, 1\\\\}^N} \\\\|z\\\\|_1 \\\\quad \\\\text{s.t.} \\\\quad b_T^k z \\\\geq 1, \\\\quad \\\\forall k = 1, \\\\ldots, K,\\n\\\\]\\nwhich is an 0-1 integer linear program (ILP). Setting $z_i = 1$ implies removing the $i$-th vertex, and the constraints ensure that all hyperedges are \\\"covered\\\", i.e., at least one vertex in each hyperedge is removed (cf. Claim 1).\\n\\nThe hypergraph formalism has been applied previously in geometric fitting [2,46,47,58]. However, the target problem in [2, 46, 47, 58] was higher order clustering (e.g., via hypergraph cuts), which is very distinct from our aims. Formulation (VC) is impractical for two reasons:\\n\u2022 Hypergraph vertex cover is intractable;\\n\u2022 The number of hyperedges in $H$ is exponential.\\n\\nHowever, the form (VC) is amenable to a quantum annealer, as we will show in Sec. 4. To deal with the number of hyperedges, we propose a hybrid quantum-classical algorithm in Sec. 5 that incrementally generates hyperedges.\\n\\n4. Quantum solution\\nWe first provide a basic introduction to quantum annealing, before describing our quantum treatment of (VC).\\n\\n4.1. Quantum annealing\\nA quantum annealer solves optimisation problems through energy minimisation of a physical system. A Hamiltonian defines the energy profile of a quantum system, which is composed of a number of interacting qubits. The system's state is initialised at the lowest energy of the initial Hamiltonian and annealed such that the its final state gives the desired solution. At the end of the annealing, the Hamiltonian can be obtained from the following model\\n\\\\[\\nX_n Q_{nn} q_n + \\\\sum_{n<m} X_{nm} Q_{nm} q_n q_m = q^T Q q.\\n\\\\]\\nThe measurement collapses the $N$-qubit quantum state into $q = [q_1, q_2, \\\\ldots, q_N]$, where $q_n \\\\in \\\\{0, 1\\\\}$, $Q \\\\in \\\\mathbb{R}^{N \\\\times N}$. The elements of $Q$ define the couplings between qubits and their biases; see [64, Chap. 8] for more details.\\n\\nA quantum annealer solves a problem of the form\\n\\\\[\\n\\\\min_{q \\\\in \\\\{0, 1\\\\}^N} q^T Q q,\\n\\\\]\"}"}
{"id": "CVPR-2022-29", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. (a) Suppose we fit a line\\n\\nmaximum consensus size is $H$\\n\\nEquivalent hypergraph\\n\\nedges $E$\\n\\nA we first generalise (VC). Let\\n\\n4.2. Hypergraph vertex cover as QUBO\\n\\nsurement; see Sec. 4.3 on practical limitations.\\n\\nA roever, since $q$ (quantum tunnelling), and $N$\\n\\nscribed above, may\\n\\nmay be a subset of the hypergraph $H$.\\n\\nmaximum independent set, and the minimum outlier set\\n\\n9\\n\\nClaim 2 implies the maximum\\n\\nequality constraints can be expressed in matrix form\\n\\nTo simplify description of the main algorithm in Sec. 5,\\n\\nWe can recover (VC) from (15) by setting $A$\\n\\nTo formulate (15) as a QUBO, we first convert the in-\\n\\nWe frame the discussion here in the context of SOTA\\n\\nSecond, although there are\\n\\nFurther algebraic manipulation is required to remove the\\n\\nChallenges\\n\\nProblem (19) is an application of the\\n\\nquantum annealer\u2014the D-Wave Advantage [23].\\n\\n4.3. Practical considerations and limitations\\n\\nIn Sec. C in the supp. material for details. In the following,\\n\\n1 constant\\n\\nwith $\\\\lambda$\\n\\npenalty parameter $\\\\lambda > 0$\\n\\nrules out a fully connected model, i.e., the\\n\\ntopology of quantum processing unit (QPU)\\n\\nwhich precludes the usage of large penalty parameters.\\n\\nembedding step [9, 12, 62] is required to map the QUBO onto\\n\\nbedding step \\n\\nAs alluded in Sec. 4.1, the annealing process \u201cgradually\u201d\\n\\n> 5000 qubits in D-Wave Advan-\\n\\nSecond, although there are\\n\\nFurther algebraic manipulation is required to remove the\\n\\nChallenges\\n\\nProblem (19) is an application of the\\n\\nquantum annealer\u2014the D-Wave Advantage [23].\\n\\n4.3. Practical considerations and limitations\\n\\nIn Sec. C in the supp. material for details. In the following,\\n\\n1 constant\\n\\nwith $\\\\lambda$\\n\\npenalty parameter $\\\\lambda > 0$\\n\\nrules out a fully connected model, i.e., the\\n\\ntopology of quantum processing unit (QPU)\\n\\nwhich precludes the usage of large penalty parameters.\\n\\nembedding step [9, 12, 62] is required to map the QUBO onto\\n\\nbedding step \\n\\nAs alluded in Sec. 4.1, the annealing process \u201cgradually\u201d\\n\\n> 5000 qubits in D-Wave Advan-\"}"}
{"id": "CVPR-2022-29", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] VLFeat. https://www.vlfeat.org/. Accessed: 2021-11-02.\\n[2] Sameer Agarwal, Jongwoo Lim, Lihi Zelnik-Manor, Pietro Perona, David Kriegman, and Serge Belongie. Beyond pairwise clustering. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005.\\n[3] Nina Amenta, Marshall Bern, and David Eppstein. Optimal point placement for mesh smoothing. Journal of Algorithms, 1999.\\n[4] Pasquale Antonante, Vasileios Tzoumas, Heng Yang, and Luca Carlone. Outlier-robust estimation: Hardness, minimally-tuned algorithms, and applications. IEEE Transactions on Robotics, 2021.\\n[5] Daniel Barath and Ji\u0159\u00ed Matas. Graph-cut ransac. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2018.\\n[6] Daniel Barath, Jana Noskova, Maksym Ivashechkin, and Jiri Matas. Magsac++, a fast, reliable and accurate robust estimator. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020.\\n[7] Marcel Seelbach Benkner, Vladislav Golyanik, Christian Theobalt, and Michael Moeller. Adiabatic quantum graph matching with permutation matrix constraints. In International Conference on 3D Vision, 2020.\\n[8] Tolga Birdal, Vladislav Golyanik, Christian Theobalt, and Leonidas J Guibas. Quantum permutation synchronization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\\n[9] Tomas Boothby, Andrew D King, and Aidan Roy. Fast clique minor generation in chimera qubit connectivity graphs. Quantum Information Processing, 2016.\\n[10] Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. Dsac-differentiable ransac for camera localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\\n[11] Sergey Bravyi, Alexander Kliesch, Robert Koenig, and Eugene Tang. Hybrid quantum-classical algorithms for approximate graph coloring. arXiv preprint arXiv:2011.13420, 2020.\\n[12] Jun Cai, William G Macready, and Aidan Roy. A practical heuristic for finding graph minors. arXiv preprint arXiv:1406.2741, 2014.\\n[13] Zhipeng Cai, Tat-Jun Chin, and Vladlen Koltun. Consensus maximization tree search revisited. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.\\n[14] Zhipeng Cai, Tat-Jun Chin, Huu Le, and David Suter. Deterministic consensus maximization with biconvex programming. In Proceedings of the European Conference on Computer Vision, 2018.\\n[15] Simona Caraiman and Vasile Manta. Image processing using quantum computing. In 16th International Conference on System Theory, Control and Computing, 2012.\\n[16] Tat-Jun Chin, Zhipeng Cai, and Frank Neumann. Robust fitting in computer vision: Easy or hard? In Proceedings of the European Conference on Computer Vision, 2018.\\n[17] Tat-Jun Chin, Pulak Purkait, Anders Eriksson, and David Suter. Efficient globally optimal consensus maximisation with tree search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015.\\n[18] Tat-Jun Chin and David Suter. The maximum consensus problem: recent algorithmic advances. Synthesis Lectures on Computer Vision, 2017.\\n[19] Tat-Jun Chin, David Suter, Shin-Fang Ch\u2019ng, and James Quach. Quantum robust fitting. In Proceedings of the Asian Conference on Computer Vision, 2020.\\n[20] Ond\u0159ej Chum, Ji\u0159\u00ed Matas, and Josef Kittler. Locally optimized ransac. In Joint Pattern Recognition Symposium. Springer, 2003.\\n[21] Creative Commons. Attribution-NonCommercial-ShareAlike 3.0 License. https://creativecommons.org/licenses/by-nc-sa/3.0/. Accessed: 2021-11-16.\\n[22] D-Wave Systems, Inc. Error Sources for Problem Representation. https://docs.dwavesys.com/docs/latest/c_qpu_ice.html. Accessed: 2021-11-08.\\n[23] D-Wave Systems, Inc. QPU-specific physical properties: Advantage system1.1. https://docs.dwavesys.com/docs/latest/_downloads/9a1b594d84370df6c0a09d00a5b72661/09-1237A-A_QPU_Properties_Advantage_system1_1.pdf. Accessed: 2021-11-02.\\n[24] D-Wave Systems, Inc. What is Quantum Annealing? https://docs.dwavesys.com/docs/latest/c_gs_2.html. Accessed: 2021-11-09.\\n[25] Nike Dattani, Szilard Szalay, and Nick Chancellor. Pegasus: The second connectivity graph for large-scale quantum annealing hardware. arXiv preprint arXiv:1901.07636, 2019.\\n[26] Aditya Dendukuri, Blake Keeling, Arash Fereidouni, Joshua Burbridge, Khoa Luu, and Hugh Churchill. Defining quantum neural networks via quantum time evolution. Quantum Techniques in Machine Learning, 2019.\\n[27] Aditya Dendukuri and Khoa Luu. Image processing in quantum computers. Quantum Techniques in Machine Learning, 2019.\\n[28] John E Dorband. Extending the d-wave with support for higher precision coefficients. arXiv preprint arXiv:1807.05244, 2018.\\n[29] Olof Enqvist, Fredrik Kahl, and Carl Olsson. Non-sequential structure from motion. In IEEE International Conference on Computer Vision Workshops (ICCV Workshops), 2011.\\n[30] David Eppstein. Quasiconvex programming. Combinatorial and Computational Geometry, 2005.\\n[31] Edward Farhi, Jeffrey Goldstone, Sam Gutmann, and Michael Sipser. Quantum computation by adiabatic evolution. arXiv preprint quant-ph/0001106, 2000.\\n[32] Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 1981.\"}"}
{"id": "CVPR-2022-29", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In IEEE conference on computer vision and pattern recognition, 2012.\\n\\nVladislav Golyanik and Christian Theobalt. A quantum computational approach to correspondence problems on point sets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.\\n\\nGurobi. https://www.gurobi.com/. Accessed: 2021-11-02.\\n\\nLaszlo Gyongyosi and Sandor Imre. A survey on quantum computing technology. Computer Science Review, 2019.\\n\\nJoseph T. Iosue. https://qubovert.readthedocs.io/en/stable/. Accessed: 2021-11-02.\\n\\nFredrik Kahl and Richard Hartley. Multiple-view geometry under the $L_\\\\infty$-norm. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008.\\n\\nAmir Khoshaman, Walter Vinci, Brandon Denis, Evgeny Andriyash, Hossein Sadeghi, and Mohammad H Amin. Quantum variational autoencoder. Quantum Science and Technology, 2018.\\n\\nDonggyu Kim, Pureum Noh, Hyun-Yong Lee, and Eun-Gook Moon. Advancing hybrid quantum-classical algorithms via mean-operators. arXiv preprint arXiv:2107.07527, 2021.\\n\\nHuu Le, Tat-Jun Chin, and David Suter. An exact penalty method for locally convergent maximum consensus. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\\n\\nHuu Le, Anders Eriksson, Michael Milford, Thanh Toan Do, Tat Jun Chin, and David Suter. Non-smooth m-estimator for maximum consensus estimation. In Proceedings of the British Machine Vision Conference, 2018.\\n\\nKarel Lebeda, Ji\u0159\u00ed Matas, and Ond\u0159ej Chum. Fixing the locally optimized ransac\u2013full experimental evaluation. In British Machine Vision Conference, 2012.\\n\\nHongdong Li. Consensus set maximization with guaranteed global optimality for robust geometry estimation. In IEEE 12th International Conference on Computer Vision, 2009.\\n\\nJunde Li and Swaroop Ghosh. Quantum-soft qubo suppression for accurate object detection. In European Conference on Computer Vision, 2020.\\n\\nShuyuan Lin, Guobao Xiao, Yan Yan, David Suter, and Hanzi Wang. Hypergraph optimization for multi-structural geometric model fitting. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019.\\n\\nHairong Liu and Shuicheng Yan. Efficient structure detection via random consensus graph. In IEEE Conference on Computer Vision and Pattern Recognition, 2012.\\n\\nDavid G Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer vision, 2004.\\n\\nHarmut Neven, Vasil S Denchev, Marshall Drew-Brook, Jiayong Zhang, William G Macready, and Geordie Rose. Nips 2009 demonstration: Binary classification using hardware implementation of quantum annealing. Quantum, 2009.\\n\\nHartmut Neven, Vasil S Denchev, Geordie Rose, and William G Macready. Qboost: Large scale classifier training with adiabatic quantum optimization. In Asian Conference on Machine Learning, 2012.\\n\\nNga TT Nguyen and Garrett T Kenyon. Image classification using quantum inference on the d-wave 2x. In IEEE International Conference on Rebooting Computing, 2018.\\n\\nV Nguyen, SB Orbell, Dominic T Lennon, Hyungil Moon, Florian Vigneau, Leon C Camenzind, Liuqi Yu, Dominik M Zumb\u00a8uhl, G Andrew D Briggs, Michael A Osborne, et al. Deep reinforcement learning for efficient measurement of quantum devices. npj Quantum Information, 2021.\\n\\nJorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media, 2006.\\n\\nCarl Olsson, Olof Enqvist, and Fredrik Kahl. A polynomial-time bound for matching and registration with outliers. In IEEE Conference on Computer Vision and Pattern Recognition, 2008.\\n\\nAlvaro Parra Bustos and Tat-Jun Chin. Guaranteed outlier removal for rotation search. In Proceedings of the IEEE International Conference on Computer Vision, 2015.\\n\\nAlejandro Perdomo-Ortiz, Marcello Benedetti, John Realpe-G\u00b4omez, and Rupak Biswas. Opportunities and challenges for quantum-assisted machine learning in near-term quantum computers. Quantum Science and Technology, 2018.\\n\\nTrung Pham, Tat-Jun Chin, Jin Yu, and David Suter. Simultaneous sampling and multi-structure fitting with adaptive reversible jump mcmc. Advances in Neural Information Processing Systems, 2011.\\n\\nPulak Purkait, Tat-Jun Chin, Alireza Sadri, and David Suter. Clustering with hypergraphs: the case for large hyperedges. IEEE transactions on pattern analysis and machine intelligence, 2016.\\n\\nPulak Purkait, Christopher Zach, and Anders Eriksson. Maximum consensus parameter estimation by reweighted $\\\\ell_1$ methods. In International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition, 2018.\\n\\nRen \u00b4e Ranftl and Vladlen Koltun. Deep fundamental matrix estimation. In Proceedings of the European conference on computer vision, 2018.\\n\\nRichard Hartley and Andrew Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2004.\\n\\nNeil Robertson and Paul D Seymour. Graph minors. xiii. the disjoint paths problem. Journal of combinatorial theory, Series B, 1995.\\n\\nMichele Sasdelli and Tat-Jun Chin. Quantum annealing formulation for binary neural networks. International Conference on Digital Image Computing Techniques and Applications, 2021.\\n\\nWolfgang Scherer. Mathematics of Quantum Computing. Springer, 2019.\\n\\nMarcel Seelbach Benkner, Zorah L \u00a8ahner, Vladislav Golyanik, Christof Wunderlich, Christian Theobalt, and Michael Moeller. Q-match: Iterative shape matching via quantum annealing. In International Conference on Computer Vision, 2021.\"}"}
{"id": "CVPR-2022-29", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"David Suter, Ruwan Tennakoon, Erchuan Zhang, Tat-Jun Chin, and Alireza Bab-Hadiashar. Monotone boolean functions, feasibility/infeasibility, lp-type problems and maxcon. arXiv preprint arXiv:2005.05490, 2020.\\n\\nRuwan Tennakoon, David Suter, Erchuan Zhang, Tat-Jun Chin, and Alireza Bab-Hadiashar. Consensus maximisation using influences of monotone boolean functions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\\n\\nPhilip HS Torr and Andrew Zisserman. Mlesac: A new robust estimator with application to estimating image geometry. Computer vision and image understanding, 2000.\\n\\nQuoc Huy Tran, Tat-Jun Chin, Wojciech Chojnacki, and David Suter. Sampling minimal subsets with large spans for robust estimation. International journal of computer vision, 2014.\\n\\nGiang Truong, Huu Le, David Suter, Erchuan Zhang, and Syed Zulqarnain Gilani. Unsupervised learning for robust fitting: A reinforcement learning approach. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\\n\\nSalvador E Venegas-Andraca and Sougato Bose. Storing, processing, and retrieving an image using quantum mechanics. In Quantum Information and Computation. International Society for Optics and Photonics, 2003.\\n\\nVisual Geometry Group. https://www.robots.ox.ac.uk/~vgg/data/. Accessed: 2021-11-02.\\n\\nFei Wen, Rendong Ying, Zheng Gong, and Peilin Liu. Efficient algorithms for maximum consensus robust fitting. IEEE Transactions on Robotics, 2019.\\n\\nFei Yan, Abdullah M Iliyasu, and Salvador E Venegas-Andraca. A survey of quantum image representations. Quantum Information Processing, 2016.\"}"}
