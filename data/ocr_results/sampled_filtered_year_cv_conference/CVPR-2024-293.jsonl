{"id": "CVPR-2024-293", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Embedding guidance.\\n\\nAs a means to enhance the cost aggregation process, we additionally leverage the embeddings $D_L$ and $D_V$ to provide spatial structure or contextual information of the inputs. Intuitively, we aim to guide the process with embeddings, based on the assumption that visually or semantically similar input tokens, e.g., color or category, have similar matching costs, inspired by cost volume filtering [20, 48] in stereo matching literature [46]. Accordingly, we redefine Eq. 2 and Eq. 3 as:\\n\\n$$F'(i, n) = T_{sa}(F(i, n); P_V(D_V)),$$\\n\\n$$F''(i, :) = T_{ca}(F'(i, :); P_L(D_L)),$$\\n\\n(4)\\n\\nwhere $[\u00b7]$ denotes concatenation, $P_V$ and $P_L$ denote linear projection layer, $D_V \\\\in \\\\mathbb{R}^{(H \\\\times W) \\\\times d}$, and $D_L \\\\in \\\\mathbb{R}^{N_C \\\\times d}$, where $d$ denotes the feature dimension. Notably, we only provide the embeddings to query and key as we find this is sufficient for embedding guidance.\\n\\nEfficient fine-tuning of CLIP\\n\\nWhile we aim to fully adapt CLIP to the downstream task through fine-tuning its image and text encoders, fine-tuning such foundation models can scale up to hundreds of millions of parameters, being computationally expensive and memory-intensive. On the other hand, freezing some of its layers, not only would be more efficient but also can help CLIP preserve its original embedding space, allowing it to be more robust to overfitting. To this end, we extensively investigate which layers should be frozen within CLIP [11], among examining various approaches for fine-tuning pre-trained models. We provide a detailed analysis of our exploration in Sec. 4.4.\\n\\n4. Experiments\\n\\n4.1. Datasets and Evaluation\\n\\nWe train our model on the COCO-Stuff [3], which has 118k densely annotated training images with 171 categories, following [30]. We employ the mean Intersection-over-Union (mIoU) as the evaluation metric for all experiments. For the evaluation, we conducted experiments on two different sets of datasets [12, 40, 65]: a commonly used in-domain datasets [14], and a multi-domain evaluation set [1] containing domain-specific images and class labels.\\n\\nDatasets for standard benchmarks.\\n\\nFor in-domain evaluation, we evaluate our model on ADE20K [65], PASCAL VOC [12], and PASCAL-Context [40] datasets. ADE20K has 20k training and 2k validation images, with two sets of categories: A-150 with 150 frequent classes and A-847 with 847 classes [9]. PASCAL-Context contains 5k training and validation images, with 459 classes in the full version (PC-459) and the most frequent 59 classes in the PC-59 version. PASCAL VOC has 20 object classes and a background class, with 1.5k training and validation images. We report PAS-20 using 20 object classes. We also report the score for PAS-20, which defines the \u201cbackground\u201d as classes present in PC-59 but not in PAS-20, as in Ghiasi et al. [14].\\n\\nDatasets for multi-domain evaluation.\\n\\nWe conducted a multi-domain evaluation on the MESS benchmark [1], specifically designed to stress-test the real-world applicability of open-vocabulary models with 22 datasets. The benchmark includes a wide range of domain-specific datasets from fields such as earth monitoring, medical sciences, engineering, agriculture, and biology. Additionally, the benchmark contains a diverse set of general domains, encompassing driving scenes, maritime scenes, paintings, and body parts. We report the average scores for each domain in the main text for brevity. For the complete results and details of the 22 datasets, please refer to the supplementary material.\\n\\n4.2. Implementation Details\\n\\nWe train the CLIP image encoder and the cost aggregation module with per-pixel binary cross-entropy loss. We set $d_F = 128$, $N_B = 2$, $N_U = 2$ for all of our models. We implement our work using PyTorch [41] and Detectron2 [53]. AdamW [35] optimizer is used with a learning rate of $2 \\\\cdot 10^{-4}$ for our model and $2 \\\\cdot 10^{-6}$ for the CLIP, with weight decay set to $10^{-4}$. The batch size is set to 4. We use 4 NVIDIA RTX 3090 GPUs for training. All of the models are trained for 80k iterations.\\n\\n4.3. Main Results\\n\\nResults of standard benchmarks.\\n\\nThe evaluation of standard open-vocabulary semantic segmentation benchmarks is shown in Table 1. Overall, our method significantly outperforms all competing methods, including those [14, 30] that leverage additional datasets [6, 42] for further performance improvements. To ensure a fair comparison, we categorize the models based on the scale of the vision-language models (VLMs) they employ. First, we present results for models that use VLMs of comparable scale to ViT-B/16 [11], and our model surpasses all previous methods, even achieving performance that matches or surpasses those using the ViT-L/14 model as their VLM [57]. For models employing the ViT-L/14 model as their VLM, our model demonstrates remarkable results, achieving a 16.0 mIoU in the challenging A-847 dataset and a 23.8 mIoU in PC-459. These results represent a 29% and 52% increase, respectively, compared to the previous state-of-the-art. We also present qualitative results of PASCAL-Context with 459 categories in Fig. 4, demonstrating the efficacy of our proposed approach in comparison to the current state-of-the-art methods [9, 30, 56].\\n\\nResults of multi-domain evaluation.\\n\\nIn Table 2, we present the qualitative results obtained from the MESS benchmark [1]. This benchmark assesses the real-world applicability of the models.\"}"}
{"id": "CVPR-2024-293", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Quantitative evaluation on standard benchmarks. The best-performing results are presented in bold, while the second-best results are underlined. Improvements over the second-best are highlighted in green.\u2020: Re-implementation trained on full COCO-Stuff.\\n\\nTable 2. Quantitative evaluation on MESS. MESS includes a wide range of domain-specific datasets, which pose significant challenges due to their substantial domain differences from the training dataset. We report the average score for each domain. Please refer to the supplementary material for the results of all 22 datasets.\\n\\nRandom is the result of uniform distributed prediction which represents the lower-bound, while Best supervised represents the upper-bound performance for the datasets.\\n\\nTable 3. Quantitative comparison between feature and cost aggregation. Cost aggregation acts as an effective alternative to direct fine-tuning of CLIP image encoder. F.T.: Fine-Tuning.\\n\\n4.4. Analysis and Ablation Study\\n\\nComparison between feature and cost aggregation. We provide quantitative and qualitative comparison of two aggregation baselines, feature aggregation, and cost aggregation, in Table 3. For both of baseline architectures, we simply apply the upsampling decoder and note that both methods share most of the architecture, but differ in whether they aggregate the concatenated features or aggregate the cosine similarity between image and text embeddings of CLIP. For (I) and (III), we freeze the encoders of CLIP and only optimize the upsampling decoder. Subsequently, in (II) and (IV), we fine-tune the encoders of CLIP on top of...\"}"}
{"id": "CVPR-2024-293", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative comparison to SAN [57]. We visualize the results of PC-459 dataset in (a-c). For (d-f), we visualize the results from the MESS benchmark [1] across three domains: underwater (top), human parts (middle), and agriculture (bottom).\\n\\nFigure 5. Qualitative comparison between feature and cost aggregation. Our approach (d) successfully segments the previously unseen class, such as \u201cbirdcage,\u201d whereas approach (c) fails.\\n\\nComponent analysis. Table 4 shows the effectiveness of the main components within our architecture through quantitative results. First, we introduce the baseline models in (I) and (II), identical to the fine-tuned baseline models from Table 3. We first add the proposed spatial and class aggregations to the cost aggregation baseline in (III) and (IV), (V) and (VI).\\n\\n| Components                | A-847 | PC-459 | A-150 | PC-59 | PAS-20 | PAS-20 |\\n|---------------------------|-------|--------|-------|-------|--------|--------|\\n| (I) Feature Agg.          | 5.6   | 12.8   | 23.6  | 58.1  | 96.3   | 77.7   |\\n| (II) Cost Agg.            | 14.7  | 23.2   | 35.3  | 60.3  | 96.7   | 78.9   |\\n| (III) Spatial agg.        | 14.9  | 23.1   | 35.9  | 60.3  | 96.7   | 79.5   |\\n| (IV) Class agg.           | 14.7  | 21.5   | 36.6  | 60.6  | 95.5   | 80.5   |\\n| (V) Spatial and Class agg. | 15.5  | 23.2   | 37.0  | 62.3  | 96.7   | 81.3   |\\n| (VI) Embedding guidance   | 16.0  | 23.8   | 37.9  | 63.3  | 97.0   | 82.5   |\\n\\nTable 4. Ablation study for CAT-Seg. We conduct ablation study by gradually adding components to the cost aggregation baseline.\\n\\nMethods\\n\\nTable 5. Ablation study of upsampling decoder. CLIP with ViT-B is used for ablation. In (V), we interleave the spatial and class aggregations. Lastly, we add the proposed embedding guidance to (V), which becomes our final model.\\n\\nAs shown, we stress the gap between (I) and (II), which supports the findings presented in Fig. 5. Given that PAS-20 shares most of its classes with the training datasets [56], the performance gap between (I) and (II) is minor. However, for challenging datasets such as A-847 or PC-459, the difference is notably significant, validating our cost aggregation framework for its generalizability. We also highlight that as we incorporate the proposed spatial and class aggregation techniques, our approach (V) outperforms (II), demonstrating the effectiveness of our design. Finally, (VI) shows that our embedding guidance further improves performance across all the benchmarks. Furthermore, we provide quantitative results of adopting the upsampling decoder in Table 5. The results show consistent improvements across all the benchmarks.\\n\\nAnalysis on fine-tuning of CLIP. In this section, we analyze the effects and methods of fine-tuning of the encoders of CLIP. In Table 6, we report the results of different approaches, which include the variant (I): without fine-tuning.\"}"}
{"id": "CVPR-2024-293", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Methods\\n\\nTable 6. Analysis of fine-tuning methods for CLIP.\\n\\nWe additionally note the number of learnable parameters of CLIP and memory consumption during training. Our method not only outperforms full fine-tuning, but also requires smaller computation.\\n\\nUnseen Classes: arcade machine | lamp\\nSeen Classes: sky | person | sea\\n\\n(a) CLIP\\n\\n(b) Fine-tuned CLIP\\n\\nFigure 6. Effects of fine-tuning CLIP. We show the t-SNE [50] visualization of CLIP image embeddings based on its predictions. In contrast to (a), we observe well-grouped clusters in (b), showing the adaptation of CLIP to segmentation for both seen and unseen classes.\\n\\nMethods\\n\\nTable 7. Training on various datasets. CLIP with ViT-B is used for all methods. Our model demonstrates remarkable generalization capabilities even on relatively smaller datasets. The scores evaluated on the same dataset used for training are colored in gray.\\n\\nTable 8. Efficiency comparison. All results are measured with a single RTX 3090 GPU.\\n\\n5. Conclusion\\n\\nIn conclusion, we introduce a cost aggregation framework for open-vocabulary semantic segmentation, aggregating the cosine-similarity scores between image and text embeddings of CLIP. Through our CAT-Seg framework, we fine-tune the encoders of CLIP for its adaptation for the downstream task of segmentation. Our method surpasses the previous state-of-the-art in standard benchmarks and also in scenarios with a vast domain difference. The success in diverse domains underscores the promise and potential of our cost aggregation framework in advancing the field of open-vocabulary semantic segmentation.\"}"}
{"id": "CVPR-2024-293", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation\\n\\nSeokju Cho 1, \u2217\\nHeeseong Shin 1, \u2217\\nSunghwan Hong 1\\nAnurag Arnab 2\\nPaul Hongsuck Seo 1, \u2020\\nSeungryong Kim 1, \u2020\\n\\n1 Korea University\\n2 Google Research\\n\\n{seokju.cho, hsshin98, sung.hwan, phseo, seungryong.kim}@korea.ac.kr\\naarnab@google.com\\n\\n1. Introduction\\n\\nOpen-vocabulary semantic segmentation aims to assign each pixel in an image to a class label from an unbounded range, defined by text descriptions. To handle the challenge of associating an image with a wide variety of text descriptions, pre-trained vision-language foundation models, e.g., CLIP [43] and ALIGN [22], have drawn attention as they exerted strong open-vocabulary recognition capabilities achieved through training on extensive image-text datasets. Nonetheless, these foundation models primarily receive image-level supervision during training, which introduces a notable disparity when applying them to the pixel-level segmentation tasks [66].\\n\\nTo address this gap, recent works [9, 14, 30, 55\u201357, 60] have reformulated the task into a region-level problem by utilizing mask proposal generators. While this partially bridges the discrepancy between the pre-training and the downstream task, a discernible gap persists between the conceptualization of regions and the entire image for CLIP.\\n\\nIn this work, we investigate methods to transfer the holistic understanding capability of images to the pixel-level task of segmentation. While a straightforward approach would be to fine-tune the encoders of CLIP, existing methods struggle in such attempt [57, 60, 66] as they encounter significant overfitting problems to the seen classes. This results in the misalignment of the joint embedding space for unseen classes, as the CLIP features undergo decoder modules for aggregating them into segmentation masks, hence losing their alignment. Consequently, most methods [9, 14, 30, 55\u201357, 60] opt for freezing the encoders of CLIP instead, remaining the challenge underexplored.\\n\\nIn this regard, we extend the exploration of adapting CLIP for open-vocabulary semantic segmentation and introduce a novel cost-based framework. We propose to aggregate the cosine similarity between image and text embeddings of CLIP, i.e., the matching cost, drawing parallels to the visual correspondence literature [26]. Surprisingly, we find that fine-tuning CLIP upon this framework effectively aggregates the cost volume considering its multi-modal nature of being established between image and text embeddings. Furthermore, we examine various methods for efficiently fine-tuning CLIP.\"}"}
{"id": "CVPR-2024-293", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Intuitively adapts CLIP to the downstream task of segmentation for both seen and unseen classes, as shown in Fig. 1. Noticing this, we delve into better aggregating the cost volume between image and text for segmentation.\\n\\nIntuitively, the cost volume can be viewed as rough semantic masks grounded to their respective classes, as illustrated in Fig. 2. Subsequently, these rough masks can be further refined to obtain accurate predictions, being the cost aggregation process. In light of this, we aim to effectively aggregate the cost volume and configure the process into spatial and class aggregation, regarding its multi-modal nature from being established between image and text. Furthermore, by observing the effectiveness of fine-tuning CLIP for its adaptation to semantic segmentation, we explore various methods to facilitate this process efficiently.\\n\\nWe analyze our cost aggregation framework to be advantageous in two aspects for adapting CLIP to dense prediction:\\n\\n1. the robustness of cost aggregation against overfitting, and\\n2. the direct construction of the cost volume from image and text embeddings of CLIP. For cost aggregation, the aggregation layers operate upon similarity scores, preventing them from overfitting to the features [4, 32, 47]. Moreover, as opposed to existing methods where they often employ decoder layers upon the image embeddings of CLIP [60, 66], we do not introduce additional layers that can potentially project the embeddings to a different embedding space.\\n\\nOur framework, dubbed CAT-Seg, combines our cost aggregation-based framework consisting of spatial and class aggregation, with our optimal approach for fine-tuning the encoders of CLIP. We achieve state-of-the-art results on every standard open-vocabulary benchmark with large margins, gaining +3.6 mIoU in A-847 and +8.1 mIoU in PC-459 compared to the recent state-of-the-art. Not only CAT-Seg is effective, but is also efficient both for training and inference compared to region-text methods, being over $\\\\times 3.7$ faster for inference. Furthermore, even in the extreme scenario [1] where the domain of the image and text description differs significantly from the training dataset, our model outperforms existing state-of-the-art methods with a large margin, paving the way for various domain-specific applications.\\n\\nWe summarize our contribution as follows:\\n\\n\u2022 We propose a cost aggregation-based framework for open-vocabulary semantic segmentation, effectively adapting CLIP to the downstream task of segmentation by fine-tuning its encoders.\\n\u2022 To aggregate the image-text cost volume, we consist of our framework with spatial and class aggregation to reason the multi-modal cost volume and explore various methods to enhance our cost aggregation framework.\\n\u2022 Our framework, named CAT-Seg, establishes state-of-the-art performance for standard open-vocabulary benchmarks.\"}"}
{"id": "CVPR-2024-293", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we refrain from adding external layers to CLIP and achieve fine-tuning of its encoders by aggregating the cost volume, which is obtained solely from the embeddings of CLIP.\\n\\nFine-tuning vision-language models. Along with the advance of large-scale vision-language models, e.g. CLIP, numerous attempts have been made to adapt CLIP to various downstream tasks [52]. CoOp [68] and CoCoOp [67] learn prompt tokens instead of optimizing the full model. Another stream of work is CLIP-Adapter [13] and TIP-Adapter [63], where they aggregate the image and text embeddings from CLIP through adapter layers instead of tuning the encoder itself. However, such methods mainly focus on few-shot settings rather than zero-shot evaluation.\\n\\nWe explore end-to-end fine-tuning of CLIP for zero-shot pixel-level prediction, which has failed in numerous attempts [57, 60, 66].\\n\\nCost aggregation. Cost aggregation [5, 8, 15, 19, 21, 26, 47, 58] is a popular technique adopted for the process of establishing correspondence between visually or semantically similar images [7, 15, 18, 26, 58] by reducing the impact of errors and inconsistencies in the matching process. A matching cost, an input to cost aggregation, is typically constructed between dense features extracted from a pair of images [44], and often cosine-similarity [32, 44] is used.\\n\\nIn this work, we view the cosine-similarity score between image and text embeddings of CLIP from the viewpoint of establishing the matching cost volume. Especially, we find the robustness of the cost aggregation layers to be favorable to open-vocabulary semantic segmentation, as these layers operate upon the similarity scores rather than the embeddings itself [32, 47]. However, our approach diverges from traditional methods as the cost volume obtained from CLIP is inherently multi-modal, originating from both image and text modalities. This contrasts with conventional cost aggregation techniques [7, 15, 18, 26, 58]. Consequently, we explore methods to effectively aggregate the multi-modal cost volume.\\n\\n3. Methodology\\n\\nGiven an image \\\\( I \\\\) and a set of candidate class categories \\\\( C = \\\\{ T(n) \\\\} \\\\) for \\\\( n = 1, \\\\ldots, N_C \\\\), where \\\\( T(n) \\\\) denotes textual description of the \\\\( n \\\\)-th category and \\\\( N_C \\\\) is the number of classes, open-vocabulary semantic segmentation assigns a class label for each pixel in image \\\\( I \\\\). Different from classical semantic segmentation tasks [16, 17, 24, 34, 59, 61, 69], open-vocabulary segmentation is additionally challenged by varying \\\\( C \\\\), given as free-form text description.\\n\\nIn this section, we describe our cost-based approach for open-vocabulary semantic segmentation. In specific, we refine the cosine-similarity scores from image and text embedding of CLIP, as illustrated in Fig. 2. The process of refining the cosine-similarity scores, or cost aggregation [26], was initially developed for the image correspondence problem and specifically designed to process an image-to-image cost volume. Consequently, traditional cost aggregation methods leverage image-specific priors, such as the assumption of local smoothness of images [27, 38, 39] for aggregating the cost volume.\\n\\nOn the other hand, we aim to aggregate the image-to-text cost volume, hence need to consider the multi-modality of the cost volume and the respective characteristics of each modality. In this regard, as shown in Fig. 3, we break down the aggregation stage into two separate modules, i.e., spatial and class aggregation, reasonably addressing the unique challenges presented by the task of open-vocabulary semantic segmentation. This includes aspects such as handling varying numbers of classes during inference and guaranteeing the permutation invariance between classes. Specifically, we perform spatial aggregation followed by class aggregation and alternate both aggregations. In the following section, we describe the cost aggregation process in detail, as well as introduce additional techniques for enhancing the cost aggregation framework.\\n\\n3.1. Cost Computation and Embedding\\n\\nGiven an image \\\\( I \\\\) and a set of classes \\\\( C \\\\), we extract the dense image embeddings \\\\( D_V = \\\\Phi_V(I) \\\\in \\\\mathbb{R}^{(H \\\\times W) \\\\times d} \\\\) and the text embeddings \\\\( D_L = \\\\Phi_L(T) \\\\in \\\\mathbb{R}^{N_C \\\\times d} \\\\), where \\\\( \\\\Phi_V(\\\\cdot) \\\\) and \\\\( \\\\Phi_L(\\\\cdot) \\\\) denotes the image and text encoders of CLIP respectively. For extracting dense CLIP image embeddings, we follow the method described in [66], wherein we modify the last attention layer of the image encoder to eliminate the pooling effect. We use the image and text embeddings \\\\( D_V(i) \\\\) and \\\\( D_L(n) \\\\), where \\\\( i \\\\) denotes 2D spatial positions of the image embedding and \\\\( n \\\\) denotes an index for a class, to compute a cost volume \\\\( C \\\\in \\\\mathbb{R}^{(H \\\\times W) \\\\times N_C} \\\\) by cosine similarity [44]. Formally, this is defined as:\\n\\n\\\\[\\nC(i, n) = D_V(i) \\\\cdot D_L(n) / \\\\| D_V(i) \\\\| / \\\\| D_L(n) \\\\| .\\n\\\\]\\n\\nTo enhance the processing of cost in high-dimensional feature space, we feed the cost volume to a single convolution layer that processes each cost slice \\\\( C(:, n) \\\\in \\\\mathbb{R}^{(H \\\\times W) \\\\times 1} \\\\) independently to obtain initial cost volume embedding \\\\( F \\\\in \\\\mathbb{R}^{(H \\\\times W) \\\\times N_C \\\\times d_F} \\\\), where \\\\( d_F \\\\) is the cost embedding dimension, as shown in Fig. 3.\\n\\n3.2. Spatial Cost Aggregation\\n\\nFor spatial aggregation, we aim to consider the characteristics of images within the image-text cost volume, such as spatial smoothness within the image. Specifically, we apply spatial aggregation for each class, respectively. Considering that we pursue the holistic understanding of images of CLIP to effectively transfer to segmentation, we adopt Transformer [33, 51] over CNNs for its global [51]\"}"}
{"id": "CVPR-2024-293", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Overview of CAT-Seg.\\nOur cost aggregation framework consists of spatial aggregation and class aggregation, followed by an upsampling decoder. Please refer to the supplementary material for a detailed illustration.\\n\\nor semi-global [18, 33] receptive fields. In practice, we employ Swin Transformer [33] for computational efficiency.\\n\\nWe define this process as follows:\\n\\n\\\\[ F'(\\\\cdot, n) = T_{sa}(F(\\\\cdot, n)) \\\\]\\n\\nwhere \\\\( F(\\\\cdot, n) \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times d_F} \\\\), and \\\\( T_{sa}(\\\\cdot) \\\\) denotes a pair of two consecutive Swin transformer blocks for spatial aggregation, where the first block features self-attention within a local window, followed by the second block with self-attention within shifted window. Note that we treat \\\\( d_F \\\\) as channel dimensions for each token, and attention is computed within individual classes separately. Intuitively, we can roughly relate the process of spatial aggregation to the bottom row of Fig. 2, where the cost volume for \\\"sofa\\\" is well-refined after aggregation, and the noise in the background region is suppressed.\\n\\n3.3. Class Cost Aggregation\\nSubsequent to spatial aggregation, class aggregation is applied to consider the text modality, explicitly capturing relationships between different class categories. We also consider the unique challenges of open-vocabulary semantic segmentation of handling varying numbers of categories \\\\( C \\\\) while being invariant to their ordering. To address these challenges, we employ a Transformer [51] layer without position embedding for aggregation, as this can achieve both of the aforementioned criteria. This process is defined as:\\n\\n\\\\[ F''(i, \\\\cdot) = T_{ca}(F'(i, \\\\cdot)) \\\\]\\n\\nwhere \\\\( F'(i, \\\\cdot) \\\\in \\\\mathbb{R}^{N_C \\\\times d_F} \\\\), and \\\\( T_{ca}(\\\\cdot) \\\\) denotes a transformer block for class aggregation. In contrast to spatial aggregation, we instead employ a linear transformer [25] as we do not need to consider spatial structure of the input tokens in this aggregation, as well as benefitting from the linear computational complexity with respect to the number of the tokens. The class aggregation process can be related to the top row of Fig. 2, where the aggregated cost volume depicts its prediction to only chairs and excluding the sofa, as both classes are given together for reasoning.\\n\\n3.4. CAT-Seg Framework\\nUpon the aggregated cost volume through spatial and class aggregation, we further enhance our methodology by incorporating an upsampling and aggregation process to derive semantic segmentation predictions. Additionally, drawing insights from state-of-the-art cost aggregation techniques [7, 8, 18], we refine our cost aggregation strategy by leveraging guidance derived from the embeddings of CLIP. Finally, we examine various methods to fine-tune the encoders of CLIP, in pursuit of effectively, yet efficiently adapting CLIP for open-vocabulary semantic segmentation.\\n\\nAltogether, we introduce Cost Aggregation approach for open-vocabulary semantic Segmentation (CAT-Seg). We describe the upsampling decoder, embedding guidance, and our fine-tuning approach in detail in the subsequent sections. For detailed illustrations of the architecture for each component, please refer to the supplementary materials.\\n\\nUpsampling decoder. Similar to FPN [31], we employ bilinear upsampling on the aggregated cost volume and concatenate it with the corresponding level of feature map extracted from CLIP, followed by a convolutional layer with a 3 \u00d7 3 kernel of fixed size. We iterate this process \\\\( N \\\\) times, generating a high-resolution output which is fed into the prediction head for final inference. To extract the high-resolution feature map, we avoid using an additional feature backbone that would introduce a heavy computational burden. Instead, similarly to [29], we extract these maps from the middle layers of the CLIP image encoder. Specifically, we extract the feature map from the output of intermediate layers of CLIP ViT [11] and then upsample them using a single learnable transposed convolution layer. This approach allows us to efficiently leverage the well-learned representations of CLIP for obtaining detailed predictions. For additional details, refer to the supplementary materials.\"}"}
{"id": "CVPR-2024-293", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Benedikt Blumenstiel, Johannes Jakubik, Hilde K\u00fchne, and Michael V\u00f6ssing. What a mess: Multi-domain evaluation of zero-shot semantic segmentation. arXiv preprint arXiv:2306.15521, 2023.\\n\\n[2] Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick P\u00e9rez. Zero-shot semantic segmentation. Advances in Neural Information Processing Systems, 32, 2019.\\n\\n[3] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1209\u20131218, 2018.\\n\\n[4] Changjiang Cai, Matteo Poggi, Stefano Mattoccia, and Philippos Mordohai. Matching-space stereo networks for cross-domain generalization. In 2020 International Conference on 3D Vision (3DV), pages 364\u2013373. IEEE, 2020.\\n\\n[5] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5410\u20135418, 2018.\\n\\n[6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\\n\\n[7] Seokju Cho, Sunghwan Hong, Sangryul Jeon, Yunsung Lee, Kwanghoon Sohn, and Seungryong Kim. Cats: Cost aggregation transformers for visual correspondence. Advances in Neural Information Processing Systems, 34:9011\u20139023, 2021.\\n\\n[8] Seokju Cho, Sunghwan Hong, and Seungryong Kim. Cats++: Boosting cost aggregation with convolutions and transformers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\\n\\n[9] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11583\u201311592, 2022.\\n\\n[10] Zheng Ding, Jieke Wang, and Zhuowen Tu. Open-vocabulary panoptic segmentation with maskclip. arXiv preprint arXiv:2208.08984, 2022.\\n\\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\n[12] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88:303\u2013308, 2009.\\n\\n[13] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, pages 1\u201315, 2023.\\n\\n[14] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXVI, pages 540\u2013557. Springer, 2022.\\n\\n[15] Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, and Hongsheng Li. Group-wise correlation stereo network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3273\u20133282, 2019.\\n\\n[16] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu Qiao. Adaptive pyramid context network for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7519\u20137528, 2019.\\n\\n[17] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.\\n\\n[18] Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation with 4d convolutional swin transformer for few-shot segmentation. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXIX, pages 108\u2013126. Springer, 2022.\\n\\n[19] Sunghwan Hong, Jisu Nam, Seokju Cho, Susung Hong, Sangryul Jeon, Dongbo Min, and Seungryong Kim. Neural matching fields: Implicit representation of matching fields for visual correspondence. arXiv preprint arXiv:2210.02689, 2022.\\n\\n[20] Asmaa Hosni, Christoph Rhemann, Michael Bleyer, Carsten Rother, and Margrit Gelautz. Fast cost-volume filtering for visual correspondence and beyond. PAMI, 2012.\\n\\n[21] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer: A transformer architecture for optical flow. arXiv preprint arXiv:2203.16194, 2022.\\n\\n[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.\\n\\n[23] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXIII, pages 709\u2013727. Springer, 2022.\\n\\n[24] Zhenchao Jin, Tao Gong, Dongdong Yu, Qi Chu, Jian Wang, Changhu Wang, and Jie Shao. Mining contextual information beyond image for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7231\u20137241, 2021.\\n\\n[25] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156\u20135165. PMLR, 2020.\\n\\n[26] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry.\"}"}
{"id": "CVPR-2024-293", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"End-to-end learning of geometry and context for deep stereo regression. In Proceedings of the IEEE international conference on computer vision, pages 66\u201375, 2017.\\n\\n[27] Junha Lee, Seungwook Kim, Minsu Cho, and Jaesik Park. Deep hough voting for robust global registration. In Proceedings of the IEEE/CVF international conference on computer vision, pages 15994\u201316003, 2021.\\n\\n[28] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Ren\u00e9 Ranftl. Language-driven semantic segmentation. arXiv preprint arXiv:2201.03546, 2022.\\n\\n[29] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part IX, pages 280\u2013296. Springer, 2022.\\n\\n[30] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. arXiv preprint arXiv:2210.04150, 2022.\\n\\n[31] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2117\u20132125, 2017.\\n\\n[32] Biyang Liu, Huimin Yu, and Guodong Qi. Graftnet: Towards domain generalized stereo matching with a broad-spectrum and task-oriented feature. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13012\u201313021, 2022.\\n\\n[33] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.\\n\\n[34] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431\u20133440, 2015.\\n\\n[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\\n\\n[36] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\n\\n[37] George A Miller. WordNet: An electronic lexical database. MIT press, 1998.\\n\\n[38] Juhong Min and Minsu Cho. Convolutional hough matching networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2940\u20132950, 2021.\\n\\n[39] Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6941\u20136952, 2021.\\n\\n[40] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 891\u2013898, 2014.\\n\\n[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\\n\\n[42] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V, pages 647\u2013664. Springer, 2020.\\n\\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\\n\\n[44] Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. Convolutional neural network architecture for geometric matching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6148\u20136157, 2017.\\n\\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, 2022.\\n\\n[46] Daniel Scharstein and Richard Szeliski. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. International journal of computer vision, 47:7\u201342, 2002.\\n\\n[47] Xiao Song, Guorun Yang, Xinge Zhu, Hui Zhou, Zhe Wang, and Jianping Shi. Adastereo: A simple and efficient approach for adaptive stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10328\u201310337, 2021.\\n\\n[48] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In CVPR, 2018.\\n\\n[49] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek, and Herv\u00e9 J\u00e9gou. Three things everyone should know about vision transformers. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXIV, pages 497\u2013515. Springer, 2022.\\n\\n[50] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\\n\\n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\"}"}
{"id": "CVPR-2024-293", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7959\u20137971, 2022.\\n\\n[53] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019.\\n\\n[54] Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt Schiele, and Zeynep Akata. Semantic projection network for zero-and few-label semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8256\u20138265, 2019.\\n\\n[55] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2955\u20132966, 2023.\\n\\n[56] Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, and Xiang Bai. A simple baseline for open-vocabulary semantic segmentation with pre-trained vision-language model. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXIX, pages 736\u2013753. Springer, 2022.\\n\\n[57] Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai. Side adapter network for open-vocabulary semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2945\u20132954, 2023.\\n\\n[58] Gengshan Yang, Joshua Manela, Michael Happold, and Deva Ramanan. Hierarchical deep stereo matching on high-resolution images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5515\u20135524, 2019.\\n\\n[59] Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, and Nong Sang. Context prior for scene segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12416\u201312425, 2020.\\n\\n[60] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional clip. arXiv preprint arXiv:2308.02487, 2023.\\n\\n[61] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VI, pages 173\u2013190. Springer, 2020.\\n\\n[62] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, and Lei Zhang. A simple framework for open-vocabulary segmentation and detection. arXiv preprint arXiv:2303.08131, 2023.\\n\\n[63] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930, 2021.\\n\\n[64] Hang Zhao, Xavier Puig, Bolei Zhou, Sanja Fidler, and Antonio Torralba. Open vocabulary scene parsing. In Proceedings of the IEEE International Conference on Computer Vision, pages 2002\u20132010, 2017.\\n\\n[65] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302\u2013321, 2019.\\n\\n[66] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In ECCV, pages 696\u2013712. Springer, 2022.\\n\\n[67] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16816\u201316825, 2022.\\n\\n[68] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.\\n\\n[69] Tianfei Zhou, Wenguan Wang, Ender Konukoglu, and Luc Van Gool. Rethinking semantic segmentation: A prototype view. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2582\u20132593, 2022.\\n\\n[70] Ziqin Zhou, Bowen Zhang, Yinjie Lei, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adapting clip for zero-shot semantic segmentation. arXiv preprint arXiv:2212.03588, 2022.\\n\\n[71] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15116\u201315127, 2023.\"}"}
