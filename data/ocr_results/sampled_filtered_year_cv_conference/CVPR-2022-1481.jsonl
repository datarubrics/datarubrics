{"id": "CVPR-2022-1481", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RAMA: A Rapid Multicut Algorithm on GPU\\n\\nAhmed Abbas Paul Swoboda\\nMax Planck Institute for Informatics, Saarland Informatics Campus\\n\\nAbstract\\nWe propose a highly parallel primal-dual algorithm for the multicut (a.k.a. correlation clustering) problem, a classical graph clustering problem widely used in machine learning and computer vision. Our algorithm consists of three steps executed recursively: (1) Finding conflicted cycles that correspond to violated inequalities of the underlying multicut relaxation, (2) Performing message passing between the edges and cycles to optimize the Lagrange relaxation coming from the found violated cycles producing reduced costs and (3) Contracting edges with high reduced costs through matrix-matrix multiplications.\\n\\nOur algorithm produces primal solutions and lower bounds that estimate the distance to optimum. We implement our algorithm on GPUs and show resulting one to two orders-of-magnitudes improvements in execution speed without sacrificing solution quality compared to traditional sequential algorithms that run on CPUs. We can solve very large scale benchmark problems with up to $O(10^8)$ variables in a few seconds with small primal-dual gaps. Our code is available at https://github.com/pawelswoboda/RAMA.\\n\\n1. Introduction\\nDecomposing a graph into meaningful clusters is a fundamental problem in combinatorial optimization. The multicut problem [15] (also known as correlation clustering [10]) is a popular approach to decompose a graph into an arbitrary number of clusters based on affinites between nodes. The multicut problem and its extensions such as higher order multicut [27, 32], lifted multicut [30], (asymmetric) multiway cut [14, 36], lifted disjoint paths [21] and joint multicut and node labeling [41] have found numerous applications in machine learning, computer vision, biomedical image analysis, data mining and beyond. Examples include unsupervised image segmentation [4, 5, 7, 58], instance-separating semantic segmentation [2, 33], multiple object tracking [21, 53], cell tracking [25], articulated human body pose estimation [22], motion segmentation [31], image and mesh segmentation [30], connectomics [6, 13, 48] and many more.\\n\\nMulticut and its extensions are NP-hard to solve [10, 18]. Since large problem instances with millions or even billions of variables typically occur, powerful approximative algorithms have been developed [11, 12, 30, 39, 52]. However, even simple heuristics such as [30] require very large running times for very large instances. In particular, some instances, such as those investigated in [48] could not be solved in acceptable time (hence ad-hoc decomposition techniques were used). In other scenarios very fast running times are essential, e.g., when multicut is used in end-to-end training [2, 49]. Hence, the need for parallelization arises, preferably on GPUs. The parallelism offered by GPUs is typically difficult to exploit due to irregular data structures and the inherently sequential nature of most combinatorial optimization algorithms. This makes design of combinatorial optimization algorithms challenging on GPUs. An additional benefit of running our algorithms on GPU is that memory transfers between CPU and GPU are avoided when used in a deep learning pipeline.\\n\\nOur contribution is a new primal-dual method that can be massively parallelized and run on GPU. This results in faster runtimes than previous multicut solvers while still computing solutions which are similar or better than CPU based solvers in terms of objective. Yet, our approach is rooted in solving a principled polyhedral relaxation and yields both a primal solution and a dual lower bound. In particular, finding primal solutions and approximate dual solving is interleaved such that both components of our algorithm can profit from each other. In more detail, our algorithmic contribution can be categorized as follows:\\n\\n- **Primal:** Edge Contraction:\\n  Finding a primal solution depends similarly as in [30] on contracting edges that are highly likely to end up in the same component of the final clustering. To this end we propose to use a linear algebra approach by expressing edge contractions as sparse matrix-matrix multiplications. This allows us to accelerate edge contraction by exploiting highly parallel matrix-matrix multiplication GPU primitives.\\n\\n- **Dual:** Lagrange Relaxation & Message Passing:\\n  To find good edge contraction candidates, we consider approximately solving a polyhedral relaxation by searching for conflicted cycles, adding them to a Lagrange relaxation.\"}"}
{"id": "CVPR-2022-1481", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and updating the resulting Lagrange multipliers iteratively by message passing. We propose a new message passing scheme that is both massively parallelizable yet yields monotonic increases w.r.t. the dual objective, speeding up the scheme of [52] by orders of magnitude.\\n\\nRecursive Primal-Dual: We interleave the above operations of finding and solving a Lagrange relaxation and contracting edges, yielding the final graph decomposition. Hence, our algorithm goes beyond classical polyhedral approaches [26, 44, 52] that only consider the original graph.\\n\\nOn the experimental side we obtain primal solutions that are of comparable or better quality to those obtained by established high-quality heuristics [30, 38] in a fraction of the execution time but with additional dual lower bounds that help in estimating the quality of the solutions. We perform experiments on 2D and 3D instance segmentation problems for scene understanding [17] and connectomics [48] containing up to \\\\( O(10^8) \\\\) variables.\\n\\n2. Related Work\\n\\nPreprocessing and Inprocessing: For fixing variables to their optimal values and shrinking the problem before or during optimization, persistency or partial optimality methods have been proposed in [3, 37, 38]. These methods apply a family of criteria that, when passed, prove that any solution can be improved if its values do not coincide with the persistently fixed variables.\\n\\nPrimal heuristics: For obtaining primal solutions without optimality guarantees or estimates on the distance to optimum, a large number of methods have been proposed with different execution time/solution quality trade-offs. The first heuristic for multicut, the classical Kernighan&Lin move-making algorithm was originally proposed in [29] and slightly generalized in [30]. The algorithm consists of trying various moves such as joining two components, moving a node from one component to the next etc and performing sequences of moves that decrease the objective. The faster but simpler greedy additive edge contraction (GAEC) heuristic, a move making algorithm that only can join individual components, was proposed in [30]. It is used in [30] to initialize the more complex Kernighan&Lin algorithm. Variants involving different join selection strategies were proposed in [28]. The greedy edge fixation algorithm [30] generalizes GAEC in that it can additionally mark edges as cut, constraining their endpoints to be in distinct components. The more involved Cut, Glue & Cut (CGC) move-making heuristic [12] works by alternating bipartitioning of the graph and exchanging nodes in pairs of clusters. The latter operation is performed by computing a max-cut on a planar subgraph via reduction to perfect matching. CGC was extended to a more general class of possible \u201cfusion moves\u201d in [11]. A parallel algorithm for the simpler problem of unweighted correlation clustering problem was given in [46]. A comparative survey of some of the above primal heuristics is given in [40].\\n\\nLP-based algorithms: For obtaining dual lower bounds that estimate the distance to the optimum or even certify optimality of a solution a number of LP-relaxation based algorithms have been proposed. These algorithms can be used inside branch and bound and their computational results can be used to guide primal heuristics to provide increasingly better solutions. Quite surprisingly, it has been shown by [26, 32] that multicut problems of moderately large sizes can be solved with commercial integer linear programming (ILP) solvers like Gurobi [19] in a cutting plane framework in reasonable time to global optimality. Column generation based on solving perfect matching subproblems has been proposed in [42, 58]. Still, the above approaches break down when truly large scale problems need to be solved, since the underlying LP-relaxations are still solved by traditional LP-solvers that do not scale linearly with problem size and are not explicitly adapted to the multicut problem. Additionally, violated inequality separation (cutting planes) requires solving weighted shortest path problems which is not possible in linear time. The message passing algorithm [50] approximately solves a dual LP-relaxation faster than traditional LP-solvers and has faster separation routines than those of primal LP-solvers as well, thereby scaling to larger problems. An even faster, but less powerful, approximate cycle packing algorithm was proposed in [38].\\n\\nOther efficient clustering Methods: The mutex watershed [57] and its generalizations [9] are closely related to the greedy additive edge fixation heuristic for multicut [40]. The corresponding algorithms can be executed faster than their multicut counterparts on CPU, but are sequential. Fast GPU schemes [8] were proposed for agglomerative clustering. Last, spectral clustering can be implemented on GPU with runtime gains [24, 43]. All these approaches however are not based on any energy minimization problem, hence do not come with the theoretical benefits that an optimization formulation offers.\\n\\n3. Method\\n\\nA decomposition (or clustering) of a graph \\\\( G = (V, E) \\\\) is a partition \\\\( \\\\{V_1, \\\\ldots, V_k\\\\} \\\\) of the node set such that \\\\( V_1 \\\\cup \\\\ldots \\\\cup V_k = V \\\\) and \\\\( V_i \\\\cap V_j = \\\\emptyset \\\\) for all \\\\( i \\\\neq j \\\\). The cut \\\\( \\\\delta(V_1, \\\\ldots, V_k) \\\\) induced by a decomposition is the subset of edges that straddle distinct clusters. Such edges are said to be cut. See Figure 1 for an illustration of a cut into three components.\"}"}
{"id": "CVPR-2022-1481", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Decomposition of a graph into three components (green). The corresponding cut consists of the dashed edges straddling distinct components (red).\\n\\nThe associated minimum cost multicut problem is defined by an additional edge cost vector $c \\\\in \\\\mathbb{R}^E$. For any edge $uv \\\\in E$, negative costs $c_{uv} < 0$ favour the nodes $u$ and $v$ to be in distinct components. Positive costs $c_{uv} > 0$ favour these nodes to lie in the same component. The minimum cost multicut problem is\\n\\n$$\\\\min_{y \\\\in M_G} \\\\langle c, y \\\\rangle,$$\\n\\nwhere $y_{uv}$ for edge $uv \\\\in E$ is 1 (resp. 0) if $u$ and $v$ belong to distinct (resp. same) components.\\n\\nBelow we detail the key components of our algorithm: Starting from a graph where each node is a cluster, primal updates consist of edge contractions that iteratively merge clusters by join operations. Dual updates optimize a Lagrange relaxation via message passing to obtain better edge costs and lower bound. Primal and dual updates are interleaved to yield our primal-dual multicut algorithm. We additionally detail how each operation can be done in a highly parallel manner.\\n\\n3.1. Primal: Parallel Edge Contraction\\n\\nThe idea of edge contraction algorithms is to iteratively choose edges with large positive costs. Such edges prefer their endpoints to be in the same component, hence they are contracted and end up in the same cluster. Edge contraction is performed until no contraction candidates are found. The special case of greedy additive edge contraction (GAEC) [30] chooses in each iteration an edge with maximum edge weight for contraction and stops if each edge in the contracted graph has negative weight. The following Lemma describes the operation of edge contraction.\\n\\n**Lemma 1.** Let an undirected graph $G = (V, E, c)$ and a set of edges $S \\\\subseteq E$ to contract be given. Also let $G' = (V', E', c')$ be the graph obtained after edge contraction.\\n\\n(a) The corresponding surjective contraction mapping $f: V \\\\to V'$ mapping node set $V$ onto the contracted node set $V'$ is up to isomorphism uniquely defined by $f(u) = f(v) \\\\iff \\\\exists uv$-path $(V, S)$. The contracted edge set is given by $E' = \\\\{f(u)f(v) : f(u) \\\\neq f(v), uv \\\\in E\\\\}$.\\n\\n(b) The edge weights for contracted edges are $c'_{ij} = \\\\sum_{uv \\\\in E: f(u) = i, f(v) = j} c_{uv}$, $\\\\forall ij \\\\in E'$.\\n\\n**Lemma 1(a)** relates the contraction mapping $f$ with the set $S$ of edges to contract. If two nodes have the same value in $f$ then there must be a path between them in graph $(V, S)$.\\n\\nMoreover, the edges whose end points are not contracted are preserved in $E'$. **Lemma 1(b)** provides the costs of contracted edges that are obtained by summing the costs of parallel edges. An illustration of the lemma is given in Figure 2.\\n\\nIn order to perform edge contraction fast we will use a linear algebraic representation that will allow to use highly parallel (sparse) matrix-matrix multiplication.\\n\\n**Definition 2 (Adjacency Matrix).** Given a weighted graph $G = (V, E, c)$ its (symmetric) adjacency matrix $A \\\\in \\\\mathbb{R}^{V \\\\times V}$ is defined by $A_{uv} = (c_{uv}, uv \\\\in E)0, \\\\text{otherwise}.$\\n\\nWe will perform edge contraction with the help of an edge contraction matrix defined as follows.\\n\\n**Definition 3 (Edge Contraction Matrix).** Given a weighted graph $G = (V, E, c)$ and an edge set $S \\\\subset E$ to contract, let $f$ be the contraction mapping and $V'$ the contracted node set. The edge contraction matrix $K_S \\\\in \\\\{0, 1\\\\}^{V \\\\times V'}$ is defined as $(K_S)_{uu'} = (1, f(u) = u')0, \\\\text{otherwise}$.\\n\\n**Lemma 4.** Given a weighted graph $G = (V, E, c)$, an edge set $S \\\\subseteq E$ to contract and an associated edge contraction mapping $f$, (a) the adjacency matrix of the contracted graph is equal to $K_S^\\\\top AK_S - \\\\text{diag}(K_S^\\\\top AK_S)$, where $\\\\text{diag}(\\\\cdot)$ is the diagonal part of a matrix, (b) it holds for the diagonal entry $(K_S^\\\\top AK_S)_{u'u'} = \\\\sum_{uv \\\\in E: f(u) = f(v)} c_{uv}$.\"}"}
{"id": "CVPR-2022-1481", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma 4(a) provides a way to compute the contracted graph in parallel by sparse matrix-matrix multiplication. Lemma 4(b) allows to efficiently judge whether the newly formed clusters decrease the multicut objective. Specifically if the diagonal contains all positive terms then the corresponding multicut objective will also decrease after contraction.\\n\\nA primal update iteration is given in Algorithm 1 that performs edge contraction as in Lemma 4(a).\\n\\nAlgorithm 1: Parallel-Edge-Contraction\\n\\nData: Graph \\\\( G = (V, E, c) \\\\)\\n\\nResult: Contracted Graph \\\\( G' = (V', E', c') \\\\), contraction mapping \\\\( f: V \\\\rightarrow V' \\\\)\\n\\n1. Compute contraction set \\\\( S \\\\subseteq E \\\\)\\n2. Compute adjacency matrix \\\\( A \\\\) from \\\\( G \\\\)\\n3. Construct contraction mapping \\\\( f: V \\\\rightarrow V' \\\\)\\n4. Construct contraction matrix \\\\( K \\\\)\\n5. \\\\( A' = K^\\\\top S A K - \\\\text{diag}(K^\\\\top S A K) \\\\)\\n6. Compute contracted graph \\\\( G' = (V', E', c') \\\\) from \\\\( A' \\\\)\\n\\nFinding contraction edge set \\\\( S \\\\): A vital step for ensuring a good primal update is selecting the edge set \\\\( S \\\\) for contraction in Algorithm 1. On one hand, we would like to choose edges in a conservative manner to avoid erroneous contractions. On the other hand, we need to contract as much edges as possible for efficiency. We propose two approaches allowing us to be at the sweet spot for both criterion as follows.\\n\\nMaximum matching: Perform a fast maximum matching on the positive edges in using a GPU version of the Luby-Jones handshaking algorithm \\\\[16\\\\] and select the matched edges for contraction.\\n\\nMaximum spanning forest without conflicts: Compute a maximum spanning forest on the positive edges with a fast GPU version of Bor\u016fvka's algorithm \\\\[55\\\\] to find initial contraction candidates. Afterwards, iterate over all negative edges \\\\( ij \\\\), find the unique path between \\\\( i \\\\) and \\\\( j \\\\) in the forest (if it exists) and remove the smallest positive edge. We make use of GPU connected components \\\\[23\\\\] to check for presence of these paths and to compute the final contraction mapping.\\n\\nBoth of the above strategies ensure that the resulting join operation decreases the multicut objective. We first find contraction edges via maximum matching. If not enough edges are found (i.e., fewer than \\\\( 0.1 |V| \\\\)), we switch to the spanning forest based approach. Note that if we chose only one largest positive edge for contraction, Algorithm 1 specializes to GAEC \\\\[30\\\\]. Since our algorithm depends upon many simultaneous edge contractions for efficiency, we do not use this strategy.\\n\\n3.2. Dual: Conflicted Cycles & Message passing\\n\\nSolving a dual of multicut problem (2) can help in obtaining a lower bound on the objective value and also yields a reparametrization of the edge costs which can help in better primal updates. Our dual algorithm works on the cycle relaxation for the multicut problem \\\\[15\\\\]. We present for its solution massively parallel inequality separation routines to search for the most useful violated constraints and efficient dual block coordinate ascent procedure for optimizing the resulting relaxation.\\n\\n3.2.1 Cycle Inequalities & Lagrange Relaxation\\n\\nSince the multicut problem is NP-hard \\\\[10, 18\\\\], we cannot hope to obtain a feasible polyhedral description of \\\\( \\\\text{conv}(M_G) \\\\). A good relaxation for most practical problems is given in terms of cycle inequalities. Given a cycle \\\\( C = \\\\{e_1, \\\\ldots, e_l\\\\} \\\\subseteq E \\\\), a feasible multicut must either not contain any cut edge or should contain at least two cut edges. This constraint is expressed as\\n\\n\\\\[\\n\\\\forall C \\\\in \\\\text{cycles}(G) : \\\\forall e \\\\in C: y_e \\\\leq \\\\bigcap_{e' \\\\in C \\\\setminus \\\\{e\\\\}} y_{e'}.\\n\\\\]\\n\\nCycle inequalities (3) give us a polyhedral relaxation of the multicut problem (2), our algorithm will operate on a Lagrangean decomposition that was proposed in \\\\[50\\\\]. It consists of two types of subproblems joined together via Lagrange variables: (i) edge subproblems for each edge \\\\( e \\\\in E \\\\) and (ii) triangle subproblems (i.e., cycles of length 3) for a subset of triangles \\\\( T \\\\subset E_3 \\\\). Triangulation of cycles of length more than three is done to get triangles defining the same polyhedral relaxation as the one with all possible cycle inequalities (3) without loss of generality \\\\[15\\\\]. We define the set of feasible multicuts on triangle graphs as\\n\\n\\\\[\\nM_T = \\\\{(0, 0, 0), (1, 1, 0), (1, 0, 1), (0, 1, 1), (1, 1, 1)\\\\}.\\n\\\\]\\n\\nWhile cycle inequalities (3) give us a polyhedral relaxation of the multicut problem (2), our algorithm will operate on a Lagrangean decomposition that was proposed in \\\\[50\\\\]. It consists of two types of subproblems joined together via Lagrange variables: (i) edge subproblems for each edge \\\\( e \\\\in E \\\\) and (ii) triangle subproblems (i.e., cycles of length 3) for a subset of triangles \\\\( T \\\\subset E_3 \\\\).\"}"}
{"id": "CVPR-2022-1481", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ c_{\\\\lambda t} \\\\in \\\\mathbb{R}^3 \\\\] for triangle \\\\( t = \\\\{ij, jk, ki\\\\} \\\\in T \\\\) are\\n\\\\[ c_{\\\\lambda uv} = c_{\\\\lambda uv} + \\\\sum_{t \\\\in T: \\\\lambda t, \\\\lambda t, \\\\lambda t} c_{\\\\lambda t,uv} \\\\] (6a)\\n\\\\[ c_{\\\\lambda t} = - (\\\\lambda t,ij, \\\\lambda t, jk, \\\\lambda t, ki) \\\\] (6b)\\n\\n\\\\( LB(\\\\lambda) \\\\) in (5) is a lower bound on the cost of the optimum multicut for any \\\\( \\\\lambda \\\\). The optimum objective value of (5) equals that of the polyhedral relaxation [52].\\n\\n3.2.2 Cycle Inequality Separation\\nFor the dual problem (5) one would need to enumerate all possible cycle inequalities (3). However, as mentioned in [38] we can restrict only to conflicted cycles of \\\\( G \\\\) for efficiency without loosening the relaxation. A cycle is called a conflicted cycle if it contains exactly one repulsive edge.\\n\\n**Definition 5 (Conflicted cycles)**\\nLet the set of attractive edges in \\\\( E \\\\) be \\\\( E^+ = \\\\{ c_e > 0 : \\\\forall e \\\\in E \\\\} \\\\) and repulsive edges \\\\( E^- = \\\\{ c_e < 0 : \\\\forall e \\\\in E \\\\} \\\\). Then conflicted cycles of \\\\( G \\\\) is the set \\\\( \\\\{ C \\\\in \\\\text{cycles}(G) : |C \\\\cap E^-| = 1 \\\\} \\\\).\\n\\n**Lemma 6.**\\nThe search for conflicted cycles can be performed in parallel for each \\\\( ij \\\\in E^- \\\\) by finding shortest path w.r.t. hop distance between \\\\( i \\\\) and \\\\( j \\\\) in the graph \\\\( G = (V, E^+) \\\\) making good use of parallelization capabilities of GPUs.\\n\\n3.2.3 Dual Block Coordinate Ascent (DBCA)\\nDBCA (a.k.a. message passing) was studied in [50] for multicut. However, the resulting message passing schemes are not easily parallelizable. The underlying reason for the inherent sequential nature of these schemes is that the effectiveness of the proposed message passing operations depend on the previous ones being executed. We propose a message passing scheme for multicut that is invariant to the message passing schedule, hence allowing parallel computation.\\n\\nAs in [50], our scheme iteratively improves the lower bound (5) by message passing between edges and triangles. For each message passing operation we need to compute min-marginals, i.e. the difference of optimal costs on subproblems obtained by fixing a specified variable to \\\\( 1 \\\\) and \\\\( 0 \\\\).\\n\\nFor edge costs the min-marginal is just the reparametrized edge cost. For triangle subproblems it is given as follows\\n\\n**Definition 7 (Marginalization for triangle subproblems)**\\nLet \\\\( t \\\\in T \\\\) be a triangle containing an edge \\\\( e \\\\).\\n\\\\[ m_{t \\\\rightarrow e}(c_{\\\\lambda t}) = \\\\min_{y \\\\in \\\\{0, 1\\\\}} y \\\\in M_T \\\\langle c_{\\\\lambda t}, y \\\\rangle - \\\\min_{y \\\\in \\\\{0, 1\\\\}} y \\\\in M_T \\\\langle c_{\\\\lambda t}, y \\\\rangle \\\\] (7)\\nis called min-marginal for triangle \\\\( t \\\\) and edge \\\\( e \\\\).\\n\\nThe message passing algorithm iteratively sets min-marginal to zero first for edge subproblems and then for triangles described in Algorithm 2. By sending back and forth messages the subproblems communicate their local optima and ultimately the min-marginals converge towards agreement (i.e. their corresponding edge labels \\\\( y \\\\) are consistent). In [52] it was shown that each such operation is non-decreasing in the dual objective value, yielding an overall monotonic convergence. Message are passed from edges to triangles in lines 2-5. After this step the reparametrized edge costs \\\\( c_{\\\\lambda e} \\\\) become zero. We perform multiple triangle to edge message passing updates (line 8-13) similar to the way it was done in [54] that distribute messages uniformly among all triangles which contain that edge. After this operation min-marginals for \\\\( c_{\\\\lambda t} \\\\) become zero.\\n\\n**Algorithm 2:** Parallel-Message-Passing\\n---\\n**Data:** Graph \\\\( G = (V, E, c) \\\\), triangles \\\\( T \\\\), Lagrange multipliers \\\\( \\\\lambda \\\\).\\n**Result:** Updated Lagrange multipliers \\\\( \\\\lambda \\\\)\\n\\n1. for \\\\( e \\\\in E \\\\) in parallel do\\n2. \\\\( \\\\alpha = c_{\\\\lambda e} \\\\)\\n3. for \\\\( t \\\\in T: e \\\\in t \\\\) do\\n4. \\\\( \\\\lambda_{t,e} - = \\\\alpha \\\\)\\n5. end\\n6. end\\n\\n// Messages from triangles to edges\\n7. for \\\\( t = \\\\{ij, jk, ki\\\\} \\\\in T \\\\) in parallel do\\n8. \\\\( \\\\lambda_{t,ij} + = \\\\frac{1}{3} m_{t \\\\rightarrow ij}(c_{\\\\lambda t}) \\\\)\\n9. \\\\( \\\\lambda_{t,ik} + = \\\\frac{1}{2} m_{t \\\\rightarrow ik}(c_{\\\\lambda t}) \\\\)\\n10. \\\\( \\\\lambda_{t,jk} + = m_{t \\\\rightarrow jk}(c_{\\\\lambda t}) \\\\)\\n11. \\\\( \\\\lambda_{t,ij} + = \\\\frac{1}{2} m_{t \\\\rightarrow ij}(c_{\\\\lambda t}) \\\\)\\n12. \\\\( \\\\lambda_{t,ik} + = m_{t \\\\rightarrow ik}(c_{\\\\lambda t}) \\\\)\\n13. \\\\( \\\\lambda_{t,ij} + = m_{t \\\\rightarrow ij}(c_{\\\\lambda t}) \\\\)\\n14. end\\n\\nConvergence of Message Passing.\\nAlgorithm 2 converges towards fixed points, similar to other DBCA schemes for graphical models [34, 35, 54, 56]. These fixed points are characterized with the help of arc consistency and need not coincide with the optimal dual solution, but are typically close to them. Below, we characterize these fixed points.\\n\\n**Definition 8 (Locally Optimal Solutions)**\\nDefine the locally optimal solutions for edge \\\\( e \\\\in E \\\\) as\\n\\\\[ c_{\\\\lambda e} : = \\\\{ x \\\\in \\\\{0, 1\\\\} : x \\\\cdot c_{\\\\lambda e} = \\\\min(0, c_{\\\\lambda e}) \\\\} \\\\] (8)\\nand similarly for triangle \\\\( t \\\\in T \\\\) as\\n\\\\[ c_{\\\\lambda t} : = \\\\{ x \\\\in M_T : \\\\langle c_{\\\\lambda t}, x \\\\rangle = \\\\min_{x' \\\\in M_T} \\\\langle c_{\\\\lambda t}, x' \\\\rangle \\\\} \\\\] (9)\"}"}
{"id": "CVPR-2022-1481", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Message passing\\n\\nEdge contraction\\n\\nFigure 3. Example iteration of our primal-dual multicut solver on a graph with repulsive and attractive edges. Width of the edges indicate abs. cost. First we detect conflicted cycles and triangulate to get triangles (indicated by \u27f3). Next, dual update reparametrizes edge costs which resolves the conflicted cycles. Lastly a primal update is done by contracting attractive edges.\\n\\nDefine the projection of triangle solutions onto one of its edges as $\\\\Pi_e(c_{\\\\lambda t}) := \\\\{x \\\\in \\\\{0, 1\\\\}: \\\\exists x' \\\\in c_{\\\\lambda t} \\\\text{ s.t. } x'e = x\\\\}$ (10)\\n\\nDefinition 9 (Arc-Consistency).\\n\\nLagrange multipliers $\\\\lambda$ are arc-consistent if $\\\\Pi_e(c_{\\\\lambda t}) = c_{\\\\lambda e}$ for all $t \\\\in T$ and $e \\\\subseteq t$.\\n\\nHowever, note that arc-consistency is not necessary for dual optimality. A necessary criterion is edge-triangle agreement.\\n\\nDefinition 10 (Edge-Triangle Agreement).\\n\\nLagrange multipliers are in edge-triangle agreement if there exist non-empty subsets $\\\\xi_e \\\\subseteq c_{\\\\lambda e}$ for all $e \\\\in E$ and $\\\\xi_t \\\\subseteq c_{\\\\lambda t}$ for all $t \\\\in T$ such that $\\\\xi$ is arc-consistent, i.e. $\\\\xi_e = \\\\Pi_e(\\\\xi_t)$ for all $t \\\\in T$ and $e \\\\subseteq t$.\\n\\nIn words, edge-triangle agreement signifies that there exists a subset (also called kernel in [56]) of locally optimal solutions that are arc-consistent.\\n\\nTheorem 11.\\n\\nAlgorithm 2 converges to edge-triangle agreement.\\n\\n3.3. Primal-Dual Updates\\n\\nWhile the two building blocks of our multicut solver i.e. edge contraction and cycle separation with message passing can be used in isolation to compute a primal solution and lower bound, we propose an interleaved primal-dual solver in Algorithm 3.\\n\\nIn each iteration we separate cycles and perform message passing to get reparameterized edge costs. We use these reparametrized edge costs to perform parallel edge contraction. This interleaved process continues until no edge contraction candidate can be found. Such scheme has the following benefits\\n\\nBetter edge contraction costs: The reparametrization in line 6 produces edge costs $c_{\\\\lambda e}$ that are more indicative of whether an edge is contracted or not in the final solution thus yielding better primal updates in line 8. In case Algorithm 3:\\n\\nPrimal-Dual Multicut\\n\\nData:\\n\\nGraph $G = (V, E, c)$\\n\\nResult:\\n\\nContraction mapping $f: V \\\\rightarrow V'$\\n\\n// Init. each node as a separate cluster\\n\\n1 $f = V \\\\rightarrow V$, $f(v) = v \\\\forall v \\\\in V$\\n\\n2 while $G$ has positive edges without conflicts do\\n\\n// Find conflicted cycles (Lemma. 6)\\n\\n3 $T = \\\\text{Cycle-Separation}(G)$\\n\\n4 for iter = 1, ..., k do\\n\\n5 $\\\\lambda = \\\\text{Parallel-Message-Pass.}(G, T)$ // Reparametrize edge costs\\n\\n6 $c_e = c_{\\\\lambda e} \\\\forall e \\\\in E$ by (6a)\\n\\n7 end\\n\\n8 $G, f' = \\\\text{Parallel-Edge-Contract.}(G)$ // Update contraction mapping\\n\\n9 $f(v) = f'(f(v)) \\\\forall v \\\\in V$\\n\\n10 end\\n\\nthe relaxation (5) is tight, the sign of $c_{\\\\lambda e}$ perfectly predicts whether an edge $e$ is separating two clusters or is inside one.\\n\\nBetter cycle separation: For fast execution times we stop cycle separation for cycles greater than a given length ($5$ in our case). Since cycle separation is performed again after edge contraction, this corresponds to finding longer cycles in the original graph. Such approach alleviates the need to perform a more exhaustive and time-consuming initial search.\\n\\nNote that a valid lower bound can be obtained from Algorithm 3 by recording (5) after cycle separation and message passing on the original graph.\\n\\n4. Experiments\\n\\nWe evaluate solvers on multicut problems for neuron segmentation for connectomics in the fruit-fly brain [48] and unsupervised image segmentation on Cityscapes [17]. We use a single NVIDIA Volta V100 (16GB) GPU for our solvers unless otherwise stated and an AMD EPYC 7702 for CPU solvers. Our solvers are implemented using the CUDA [45] and Thrust [20] GPU programming frameworks.\\n\\nDatasets\\n\\nWe have chosen three datasets containing the largest multicut problem instances we are aware of. The instances are available in [51].\\n\\nConnectomics-SP: Contains neuron segmentation problems from the fruit-fly brain [48]. The raw data is taken from the CREMI-challenge [1] acquired by [59] and converted to multiple multicut instances by [48]. For this conversion [48] also reduced the problem size by creating super-pixels. The majority of these instances are different crops of one global problem. There are $38198$\"}"}
{"id": "CVPR-2022-1481", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"small (400,000\u2013600,000 edges), 3 medium (4\u20135 million edges) and 5 large (28\u2013650 million edges) multicut instances. For the largest problem we use NVIDIA RTX 8000 (48GB) GPU.\\n\\nConnectomics-Raw: We use the 3 test volumes (sample A+, B+, C+) from the CREMI-challenge [1] segmenting directly on the pixel level without conversion to superpixels. Conversion to multicut instances is carried out using [47]. We report results on two types of instances: (i) The three full problems where the underlying volumes have size $1250 \\\\times 1250 \\\\times 125$ with around 700 million edges and (ii) six cropped problems created by halving each volume and creating the corresponding multicut instances each containing almost 340 million edges. For all these instances we use NVIDIA RTX 8000 (48GB) GPU.\\n\\nCityscapes: Unsupervised image segmentation on 59 high resolution images ($2048 \\\\times 1024$) taken from the validation set [17]. Conversion to multicut instances is done by computing the edge affinities produced by [2] on a grid graph with 4-connectivity and additional coarsely sampled longer range edges. Each instance contains approximately 2 million nodes and 9 million edges.\\n\\nAlgorithms: As baseline methods we have chosen, to our knowledge, the fastest primal heuristics from the literature.\\n\\nGAEC [30]: The greedy additive edge contraction corresponds to Algorithm 2 with choosing a single highest edge to contract. We use our own CPU implementation that is around 1.5 times faster than the one provided by the authors.\\n\\nKLj [30]: The Kernighan&Lin with joins algorithm performs local move operations which can improve the objective. To avoid large runtimes the output of GAEC is used for initialization.\\n\\nGEF [40]: The greedy edge fixation algorithm is similar to GAEC but additionally visits negative valued (repulsive) edges and adds non-link constraints between their endpoints.\\n\\nBEC [28]: Balanced edge contraction, a variant of GAEC which chooses edges to contract based on their cost normalized by the size of the two endpoints.\\n\\nICP [38]: The iterated cycle packing algorithm searches for cycles and greedily solves a packing problem that approximately solves the multicut dual (5).\\n\\nP: Our purely primal Algorithm 1 using the maximum matching and spanning forest based edge contraction strategy.\\n\\nPD: Our primal-dual Algorithm 3 which additionally makes use of the dual information. We find conflicted cycles up to length 5 on original graph and up to a length of 3 for later iterations on contracted graphs.\\n\\nFigure 4. Comparison of primal solutions from Cityscapes dataset. Our purely primal algorithm (P) is 30\u00d7 faster than GAEC [30] and GEF [40], although with worse objective values. Incorporating dual information enables our solvers (PD, PD+) to even surpass the sequential solvers in objective while being faster by an order of magnitude. Error bars mark the 0.25, 0.75-quantile. (KLj not shown due to high runtime).\\n\\nFigure 5. Comparison of lower bounds from Cityscapes dataset. Our parallel message passing scheme (D) is more than an order of magnitude faster than ICP [38] and gives slightly better lower bounds. Error bars mark the 0.25, 0.75-quantile.\\n\\nFigure 6. Runtime scaling comparison computed on different crops of CREMI test data showing that RAMA scales very well compared to GAEC [30] w.r.t. increasing problem sizes.\\n\\nPD+: Variant of PD which always considers conflicted cycles up to a length 5 for reparametrization which can lead to even better primal solutions although with higher runtime.\\n\\nD: Our dual cycle separation algorithm followed by message passing on the original graph via Algorithm 2 producing lower bounds.\"}"}
{"id": "CVPR-2022-1481", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison of results on all datasets. (C: cost, t(s): time in seconds, \u2020: timed out, *: out of GPU memory). We report average primal and dual costs and runtime over instances within each category. In terms of primal solutions our primal-dual solvers (PD, PD+) achieve objectives close to or better than sequential solvers while being substantially faster especially on larger instances. Moreover, our parallel message passing approach (D) gives better lower bounds than ICP with up to two orders of magnitude reduction in runtime.\\n\\nDiscussion\\n\\nResults on all datasets are given in Table 1. On the Connectomics-SP dataset we attain primal objectives very close to those produced by GAEC [30] but faster by more than an order of magnitude on large instances. For the Cityscapes and Connectomics-Raw datasets we achieve even better primal solutions than sequential algorithms by incorporating dual information while also being substantially faster. Our best solver (PD+) is more than $10^4$ times faster than KLj [30] and produces better solutions.\\n\\nDistributions of runtimes and primal resp. dual objectives for all instances of Cityscapes are shown in Figures 4 and 5. We compare the scaling behaviour of our solver w.r.t increasing instance sizes in Figure 6 showing that RAMA scales much more efficiently than GAEC. An example visual comparison of results is given in Figure 7 in Appendix.\\n\\nLastly, our dual algorithm (D) produces speedups of up to two orders of magnitude and better lower bounds compared to the serial ICP [38], except on the full instances of Connectomics-Raw where we run out of GPU memory.\\n\\nRuntime breakdown\\n\\nRuntime breakdown of our PD algorithm is given in Table 2. Most of the time is spent in finding conflicted cycles which we found to be challenging to implement on GPU while keeping runtime and memory consumption low. Future improvements offer a potential for even better results and speedups by finding longer cycles more efficiently.\\n\\n5. Conclusion\\n\\nWe have demonstrated that multicut, an important combinatorial optimization problem for machine learning and computer vision, can be effectively parallelized on GPU. Our approach produces better solutions than state of the art efficient heuristics on grid graphs and comparable ones on super-pixel graphs while being faster by one to two orders-of-magnitude. We believe that performance gap on super-pixel graphs is due to a graph structure containing much more (and longer) conflicted cycles. Since our implementation can only find cycles of length up to 5, better implementations that can efficiently handle longer cycles might yield further improvements.\\n\\nIn contrast to CPU algorithms, where execution speed is the limiting factor, for our GPU algorithm, comparatively smaller amount of GPU-memory limits application to even larger instances. We hope that our work will enable more compute intensive applications of multicut, where until now the slower serial CPU codepath has hindered its adoption.\\n\\n6. Acknowledgments\\n\\nWe would like to thank Shweta Mahajan and Jan-Hendrik Lange for insightful discussions and Constantin Pape, Adrian Wolny and Anna Kreshuk for their suggestions regarding the experiments. We also thank all anonymous reviewers for their valuable feedback.\"}"}
{"id": "CVPR-2022-1481", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] CREMI MICCAI Challenge on circuit reconstruction from Electron Microscopy Images. https://cremi.org.\\n\\n[2] Ahmed Abbas and Paul Swoboda. Combinatorial Optimization for Panoptic Segmentation: A Fully Differentiable Approach. Advances in Neural Information Processing Systems, 34, 2021.\\n\\n[3] Amir Alush and Jacob Goldberger. Ensemble segmentation using efficient integer linear programming. IEEE transactions on pattern analysis and machine intelligence, 34(10):1966\u20131977, 2012.\\n\\n[4] Amir Alush and Jacob Goldberger. Break and conquer: Efficient correlation clustering for image segmentation. In International Workshop on Similarity-Based Pattern Recognition, pages 134\u2013147. Springer, 2013.\\n\\n[5] Bjoern Andres, J\u00f6rg H. Kappes, Thorsten Beier, Ullrich K\u00f6the, and Fred A. Hamprecht. Probabilistic image segmentation with closedness constraints. In ICCV, 2011.\\n\\n[6] Bjoern Andres, Thorben Kr\u00f6ger, Kevin L. Briggman, Winfried Denk, Natalya Korogod, Graham Knott, Ullrich K\u00f6the, and Fred A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In ECCV, 2012.\\n\\n[7] Bjoern Andres, Julian Yarkony, BS Manjunath, Steffen Kirchhoff, Engin Turetken, Charless C Fowlkes, and Hanspeter Pfister. Segmenting planar superpixel adjacency graphs wrt non-planar superpixel affinity graphs. In International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition, pages 266\u2013279. Springer, 2013.\\n\\n[8] Bas Fagginger Auer and Rob H Bisseling. Graph coarsening and clustering on the GPU. Graph Partitioning and Graph Clustering, 588:223, 2012.\\n\\n[9] Alberto Bailoni, Constantin Pape, Steffen Wolf, Thorsten Beier, Anna Kreshuk, and Fred A Hamprecht. A generalized framework for agglomerative clustering of signed graphs applied to instance segmentation. arXiv preprint arXiv:1906.11713, 2019.\\n\\n[10] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine learning, 56(1-3):89\u2013113, 2004.\\n\\n[11] Thorsten Beier, Bj\u00f6rn Andres, Ullrich K\u00f6the, and Fred A Hamprecht. An efficient fusion move algorithm for the minimum cost lifted multicut problem. In European Conference on Computer Vision. Springer, 2016.\\n\\n[12] Thorsten Beier, Thorben Kroeger, J\u00f6rg H Kappes, Ullrich Kothe, and Fred A Hamprecht. Cut, glue & cut: A fast, approximate solver for multicut partitioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014.\\n\\n[13] Thorsten Beier, Constantin Pape, Nasim Rahaman, Timo Prange, Stuart Berg, Davi D Bock, Albert Cardona, Graham W Knott, Stephen M Plaza, Louis K Scheffer, et al. Multicut brings automated neurite segmentation closer to human performance. Nature methods, 14(2):101, 2017.\\n\\n[14] Sunil Chopra and Mendu R Rao. On the multiway cut polyhedron. Networks, 21(1):51\u201389, 1991.\\n\\n[15] Sunil Chopra and Mendu R Rao. The partition problem. Mathematical Programming, 59(1-3):87\u2013115, 1993.\\n\\n[16] Jonathan Cohen and Patrice Castonguay. Efficient graph matching and coloring on the GPU. In GTC. NVIDIA, 2012.\\n\\n[17] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.\\n\\n[18] Erik D Demaine, Dotan Emanuel, Amos Fiat, and Nicole Immorlica. Correlation clustering in general weighted graphs. Theoretical Computer Science, 361(2-3):172\u2013187, 2006.\\n\\n[19] LLC Gurobi Optimization. Gurobi optimizer reference manual, 2019.\\n\\n[20] Jared Hoberock and Nathan Bell. Thrust: A parallel template library, 2010. Version 1.7.0.\\n\\n[21] Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn, and Paul Swoboda. Lifted disjoint paths with application in multiple object tracking. In International Conference on Machine Learning, pages 4364\u20134375. PMLR, 2020.\\n\\n[22] Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin, Siyu Tang, Evgeny Levinkov, Bjoern Andres, and Bernt Schiele. Arttrack: Articulated multi-person tracking in the wild. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\\n\\n[23] Jayadharini Jaiganesh and Martin Burtscher. A high-performance connected components implementation for GPUs. In Proceedings of the 27th International Symposium on High-Performance Parallel and Distributed Computing, pages 92\u2013104, 2018.\\n\\n[24] Yu Jin and Joseph F JaJa. A high performance implementation of spectral clustering on CPU-GPU platforms. In Parallel and Distributed Processing Symposium Workshops, 2016 IEEE International, pages 825\u2013834. IEEE, 2016.\\n\\n[25] Florian Jug, Evgeny Levinkov, Corinna Blasse, Eugene W Myers, and Bjoern Andres. Moral lineage tracing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.\\n\\n[26] J\u00f6rg Hendrik Kappes, Markus Speth, Bj\u00f6rn Andres, Gerhard Reinelt, and Christoph Schn\u00f6rr. Globally optimal image partitioning by multicuts. In International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition. Springer, 2011.\\n\\n[27] J\u00f6rg Hendrik Kappes, Markus Speth, Gerhard Reinelt, and Christoph Schn\u00f6rr. Higher-order segmentation via multicuts. Computer Vision and Image Understanding, 143:104\u2013119, 2016.\\n\\n[28] Amirhossein Kardoost and Margret Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In Asian Conference on Computer Vision, pages 74\u201389. Springer, 2018.\\n\\n[29] Brian W Kernighan and Shen Lin. An efficient heuristic procedure for partitioning graphs. Bell system technical journal, 49(2):291\u2013307, 1970.\\n\\n[30] Margret Keuper, Evgeny Levinkov, Nicolas Bonneel, Guillaume Lavou\u00e9, Thomas Brox, and Bjorn Andres. Efficient decomposition of image and mesh graphs by lifted multicut. In Proceedings of the IEEE International Conference on Computer Vision, 2015.\"}"}
{"id": "CVPR-2022-1481", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Margret Keuper, Siyu Tang, Bjoern Andres, Thomas Brox, and Bernt Schiele. Motion segmentation & multiple object tracking by correlation co-clustering. IEEE transactions on pattern analysis and machine intelligence, 42(1):140\u2013153, 2018.\\n\\nSungwoong Kim, Sebastian Nowozin, Pushmeet Kohli, and Chang D Yoo. Higher-order correlation clustering for image segmentation. In Advances in neural information processing systems, 2011.\\n\\nAlexander Kirillov, Evgeny Levinkov, Bjoern Andres, Bogdan Savchynskyy, and Carsten Rother. Instancecut: from edges to instances with multicut. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\\n\\nVladimir Kolmogorov. Convergent tree-reweighted message passing for energy minimization. In International Workshop on Artificial Intelligence and Statistics, pages 182\u2013189. PMLR, 2005.\\n\\nVladimir Kolmogorov. A new look at reweighted message passing. IEEE transactions on pattern analysis and machine intelligence, 37(5):919\u2013930, 2014.\\n\\nThorben Kroeger, J\u00f6rg H Kappes, Thorsten Beier, Ullrich Koethe, and Fred A Hamprecht. Asymmetric cuts: Joint image labeling and partitioning. In German Conference on Pattern Recognition. Springer, 2014.\\n\\nJan-Hendrik Lange, Bjoern Andres, and Paul Swoboda. Combinatorial persistency criteria for multicut and max-cut. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6093\u20136102, 2019.\\n\\nJan-Hendrik Lange, Andreas Karrenbauer, and Bjoern Andres. Partial optimality and fast lower bounds for weighted correlation clustering. In International Conference on Machine Learning, 2018.\\n\\nEvgeny Levinkov, Alexander Kirillov, and Bjoern Andres. A comparative study of local search algorithms for correlation clustering. In German Conference on Pattern Recognition. Springer, 2017.\\n\\nEvgeny Levinkov, Jonas Uhrig, Siyu Tang, Mohamed Omran, Eldar Insafutdinov, Alexander Kirillov, Carsten Rother, Thomas Brox, Bernt Schiele, and Bjoern Andres. Joint graph decomposition & node labeling: Problem, algorithms, applications. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\\n\\nJovita Lukasik, Margret Keuper, Maneesh Singh, and Julian Yarkony. A benders decomposition approach to correlation clustering. In 2020 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC) and Workshop on Artificial Intelligence and Machine Learning for Scientific Applications (AI4S). IEEE, 2020.\\n\\nMaxim Naumov and Timothy Moon. Parallel spectral graph partitioning. tech. rep., NVIDIA tech. rep, 2016.\\n\\nSebastian Nowozin and Stefanie Jegelka. Solution stability in linear programming relaxations: Graph partitioning and unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 769\u2013776, 2009.\\n\\nNVIDIA, P\u00e9ter Vingelmann, and Frank H.P. Fitzek. CUDA, release: 11.2, 2021.\\n\\nXinghao Pan, Dimitris Papailiopoulos, Samet Oymak, Benjamin Recht, Kannan Ramchandran, and Michael I Jordan. Parallel correlation clustering on big graphs. In Advances in Neural Information Processing Systems, 2015.\\n\\nConstantin Pape. torch-em. https://github.com/constantinpape/torch-em, 2021.\\n\\nConstantin Pape, Thorsten Beier, Peter Li, Viren Jain, Davi D Bock, and Anna Kreshuk. Solving large multicut problems for connectomics via domain decomposition. In Proceedings of the IEEE International Conference on Computer Vision, 2017.\\n\\nJie Song, Bjoern Andres, Michael J Black, Otmar Hilliges, and Siyu Tang. End-to-end learning for graph decomposition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.\\n\\nPaul Swoboda and Bjoern Andres. A message passing algorithm for the minimum cost multicut problem. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\\n\\nPaul Swoboda, Andrea Hornakova, Paul Roetzer, and Ahmed Abbas. Structured Prediction Problem Archive. arXiv preprint arXiv:2202.03574, 2022.\\n\\nPaul Swoboda, Jan Kuske, and Bogdan Savchynskyy. A dual ascent framework for lagrangean decomposition of combinatorial problems. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\\n\\nSiyu Tang, Mykhaylo Andriluka, Bjoern Andres, and Bernt Schiele. Multiple people tracking by lifted multicut and person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\\n\\nSiddharth Tourani, Alexander Shekhovtsov, Carsten Rother, and Bogdan Savchynskyy. MPLP++: Fast, parallel dual block-coordinate ascent for dense graphical models. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.\\n\\nW Hwu Wen-mei. GPU Computing Gems Jade Edition. Elsevier, 2011.\\n\\nTomas Werner. A linear programming approach to max-sum problem: A review. IEEE transactions on pattern analysis and machine intelligence, 29(7):1165\u20131179, 2007.\\n\\nSteffen Wolf, Alberto Bailoni, Constantin Pape, Nasim Rahaman, Anna Kreshuk, Ullrich Koethe, and Fred A Hamprecht. The mutex watershed and its objective: Efficient, parameter-free graph partitioning. IEEE transactions on pattern analysis and machine intelligence, 2020.\\n\\nJulian Yarkony, Alexander Ihler, and Charless C Fowlkes. Fast planar correlation clustering for image segmentation. In European Conference on Computer Vision. Springer, 2012.\\n\\nZhihao Zheng, J Scott Lauritzen, Eric Perlman, Camenzind G Robinson, Matthew Nichols, Daniel Milkie, Omar Torrens, John Price, Corey B Fisher, Nadiya Sharifi, et al. A complete electron microscopy volume of the brain of adult Drosophila melanogaster. Cell, 174(3):730\u2013743, 2018.\"}"}
