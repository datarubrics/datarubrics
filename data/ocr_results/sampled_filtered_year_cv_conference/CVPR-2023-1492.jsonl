{"id": "CVPR-2023-1492", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Yaakov Bar-Shalom, Fred Daum, and Jim Huang. The probabilistic data association filter. IEEE Control Systems Magazine, 29(6):82\u2013100, 2009.\\n\\n[2] Sumit Basu, Irfan Essa, and Alex Pentland. Motion regularization for model-based head tracking. In Proceedings of 13th International Conference on Pattern Recognition, volume 3, pages 611\u2013616. IEEE, 1996.\\n\\n[3] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime tracking. In 2016 IEEE international conference on image processing (ICIP), pages 3464\u20133468. IEEE, 2016.\\n\\n[4] Guillem Bras\u00f3 and Laura Leal-Taix\u00e9. Learning a neural solver for multiple object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6247\u20136257, 2020.\\n\\n[5] Jiarui Cai, Mingze Xu, Wei Li, Yuanjun Xiong, Wei Xia, Zhuowen Tu, and Stefano Soatto. Memot: multi-object tracking with memory. In CVPR, 2022.\\n\\n[6] Jinkun Cao, Jiangmiao Pang, Xinshuo Weng, Rawal Khirodkar, and Kris M Kitani. Object tracking by hierarchical part-whole attention.\\n\\n[7] Jinkun Cao, Hongyang Tang, Hao-Shu Fang, Xiaoyong Shen, Cewu Lu, and Yu-Wing Tai. Cross-domain adaptation for animal pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9498\u20139507, 2019.\\n\\n[8] Jinkun Cao, Hao Wu, and Kris Kitani. Track targets by dense spatio-temporal position encoding. arXiv preprint arXiv:2210.09455, 2022.\\n\\n[9] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, Dea Wang, Peter Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d tracking and forecasting with rich maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8748\u20138757, 2019.\\n\\n[10] Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. Training and testing low-degree polynomial data mappings via linear svm. Journal of Machine Learning Research, 11(4), 2010.\\n\\n[11] Wongun Choi. Near-online multi-target tracking with aggregated local flow descriptor. In Proceedings of the IEEE international conference on computer vision, pages 3029\u20133037, 2015.\\n\\n[12] Peng Chu, Jiang Wang, Quanzeng You, Haibin Ling, and Zicheng Liu. Transmot: Spatial-temporal graph transformer for multiple object tracking. arXiv preprint arXiv:2104.00194, 2021.\\n\\n[13] Peng Dai, Renliang Weng, Wongun Choi, Changshui Zhang, Zhangping He, and Wei Ding. Learning a proposal classifier for multiple object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2443\u20132452, 2021.\\n\\n[14] Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taix\u00e9. Mot20: A benchmark for multi object tracking in crowded scenes. arXiv preprint arXiv:2003.09003, 2020.\\n\\n[15] David Duvenaud. Automatic model construction with Gaussian processes. PhD thesis, University of Cambridge, 2014.\\n\\n[16] Hao-Shu Fang, Jinkun Cao, Yu-Wing Tai, and Cewu Lu. Pairwise body-part attention for recognizing human-object interactions. In Proceedings of the European conference on computer vision (ECCV), pages 51\u201367, 2018.\\n\\n[17] Juergen Gall, Angela Yao, Nima Razavi, Luc Van Gool, and Victor Lempitsky. Hough forests for object detection, tracking, and action recognition. IEEE transactions on pattern analysis and machine intelligence, 33(11):2188\u20132202, 2011.\\n\\n[18] Damien Garreau, Wittawat Jitkrittum, and Motonobu Kanagawa. Large sample analysis of the median heuristic. arXiv preprint arXiv:1707.07269, 2017.\\n\\n[19] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021.\\n\\n[20] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231\u20131237, 2013.\\n\\n[21] Nicolas Franco Gonzalez, Andres Ospina, and Philippe Calvez. Smat: Smart multiple affinity metrics for multiple object tracking. In International Conference on Image Analysis and Recognition, pages 48\u201362. Springer, 2020.\\n\\n[22] Fredrik Gustafsson, Fredrik Gunnarsson, Niclas Bergman, Urban Forssell, Jonas Jansson, Rickard Karlsson, and P-J Nordlund. Particle filters for positioning, navigation, and tracking. IEEE Transactions on signal processing, 50(2):425\u2013437, 2002.\\n\\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[24] David V Hinkley. On the ratio of two correlated normal random variables. Biometrika, 56(3):635\u2013639, 1969.\\n\\n[25] Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn, and Paul Swoboda. Lifted disjoint paths with application in multiple object tracking. In International Conference on Machine Learning, pages 4364\u20134375. PMLR, 2020.\\n\\n[26] Andrea Hornakova, Timo Kaiser, Paul Swoboda, Michal Rol\u00ednek, Bodo Rosenhahn, and Roberto Henschel. Making higher order mot scalable: An efficient approximate solver for lifted disjoint paths. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6330\u20136340, 2021.\\n\\n[27] Andrew H Jazwinski. Stochastic processes and filtering theory. Courier Corporation, 2007.\\n\\n[28] Simon J Julier and Jeffrey K Uhlmann. New extension of the kalman filter to nonlinear systems. In Signal processing, sensor fusion, and target recognition VI, volume 3068, pages 182\u2013193. International Society for Optics and Photonics, 1997.\\n\\n[29] Simon J Julier and Jeffrey K Uhlmann. Unscented filtering and nonlinear estimation. Proceedings of the IEEE, 92(3):401\u2013422, 2004.\\n\\n[30] Rudolf Emil Kalman et al. Contributions to the theory of optimal control. Bol. soc. mat. mexicana, 5(2):102\u2013119, 1960.\\n\\n[31] Kris M Kitani, Brian D Ziebart, James Andrew Bagnell, and Martial Hebert. Activity forecasting. In European conference on computer vision, pages 201\u2013214. Springer, 2012.\"}"}
{"id": "CVPR-2023-1492", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jonathan Ko and Dieter Fox. Gp-bayesfilters: Bayesian filtering using gaussian process prediction and observation models. Autonomous Robots, 27(1):75\u201390, 2009.\\n\\nParth Kothari, Sven Kreiss, and Alexandre Alahi. Human trajectory forecasting in crowds: A deep learning perspective. IEEE Transactions on Intelligent Transportation Systems, 2021.\\n\\nErich L Lehmann and George Casella. Theory of point estimation. Springer Science & Business Media, 2006.\\n\\nWei Li, Yuanjun Xiong, Shuo Yang, Mingze Xu, Yongxin Wang, and Wei Xia. Semi-tcl: Semi-supervised track contrastive representation learning. arXiv preprint arXiv:2107.02396, 2021.\\n\\nChao Liang, Zhipeng Zhang, Yi Lu, Xue Zhou, Bing Li, Xiyong Ye, and Jianxiao Zou. Rethinking the competition between detection and reid in multi-object tracking. arXiv preprint arXiv:2010.12138, 2020.\\n\\nJonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taix\u00e9, and Bastian Leibe. Hota: A higher order metric for evaluating multi-object tracking. International journal of computer vision, 129(2):548\u2013578, 2021.\\n\\nGerard Maggiolino, Adnan Ahmad, Jinkun Cao, and Kris Kitani. Deep oc-sort: Multi-pedestrian tracking by adaptive re-identification. arXiv preprint arXiv:2302.11813, 2023.\\n\\nTim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer: Multi-object tracking with transformers. arXiv preprint arXiv:2101.02702, 2021.\\n\\nStan Melax, Leonid Keselman, and Sterling Orsten. Dynamics based 3d skeletal hand tracking. In Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, pages 184\u2013184, 2013.\\n\\nJonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taix\u00e9, and Bastian Leibe. Hota: A higher order metric for evaluating multi-object tracking. International journal of computer vision, 129(2):548\u2013578, 2021.\\n\\nLawrence Rabiner and Biinghwang Juang. An introduction to hidden markov models. IEEE ASSP Magazine, 3(1):4\u201316, 1986.\\n\\nAkshay Rangesh, Pranav Maheshwari, Mez Gebre, Siddhesh Mhatre, Vahid Ramezani, and Mohan M Trivedi. Trackmpnn: A message passing graph neural architecture for multi-object tracking. arXiv preprint arXiv:2101.04206, 2021.\\n\\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779\u2013788, 2016.\\n\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\\n\\nHamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: A metric and a loss for bounding box regression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 658\u2013666, 2019.\\n\\nToby Sharp, Cem Keskin, Duncan Robertson, Jonathan Taylorlor, Jamie Shotton, David Kim, Christoph Rhemann, Ido Leichter, Alon Vinnikov, Yichen Wei, et al. Accurate, robust, and flexible real-time hand tracking. In Proceedings of the 33rd annual ACM conference on human factors in computing systems, pages 3633\u20133642, 2015.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\\n\\nGerald L Smith, Stanley F Schmidt, and Leonard A McGee. Application of statistical filter theory to the optimal estimation of position and velocity on board a circumlunar vehicle. National Aeronautics and Space Administration, 1962.\\n\\nDaniel Stadler and Jurgen Beyerer. Improving multiple pedestrian tracking by track management and occlusion handling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10958\u201310967, 2021.\\n\\nPeize Sun, Jinkun Cao, Yi Jiang, Zehuan Yuan, Song Bai, Kris Kitani, and Ping Luo. Dancetrack: Multi-object tracking in uniform appearance and diverse motion. arXiv preprint arXiv:2111.14690, 2021.\\n\\nPeize Sun, Jinkun Cao, Yi Jiang, Rufeng Zhang, Enze Xie, Zehuan Yuan, Changhu Wang, and Ping Luo. Transtrack: Multiple object tracking with transformer. arXiv preprint arXiv:2012.15460, 2020.\\n\\nRamana Sundararaman, Cedric De Almeida Braga, Eric Marchand, and Julien Pettre. Tracking pedestrian heads in dense crowd. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3865\u20133875, 2021.\\n\\nPavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien Gaidon. Learning to track with object permanence. In ICCV, pages 10860\u201310869, 2021.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nGaoang Wang, Renshu Gu, Zuozhu Liu, Weijie Hu, Mingli Song, and Jenq-Neng Hwang. Track without appearance: Learn box and tracklet embedding with local and global motion patterns for vehicle tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9876\u20139886, 2021.\\n\\nShuai Wang, Hao Sheng, Yang Zhang, Yubin Wu, and Zhang Xiong. A general recurrent tracking framework without real data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9695\u20139695.\"}"}
{"id": "CVPR-2023-1492", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[61] Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 13708\u201313715. IEEE, 2021.\\n\\n[62] Christopher Williams and Carl Rasmussen. Gaussian processes for regression. Advances in neural information processing systems, 8, 1995.\\n\\n[63] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In 2017 IEEE international conference on image processing (ICIP), pages 3645\u20133649. IEEE, 2017.\\n\\n[64] Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, and Junsong Yuan. Track to detect and segment: An online multi-object tracker. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12352\u201312361, 2021.\\n\\n[65] Yu Xiang, Alexandre Alahi, and Silvio Savarese. Learning to track: Online multi-object tracking by decision making. In Proceedings of the IEEE international conference on computer vision, pages 4705\u20134713, 2015.\\n\\n[66] Yuliang Xiu, Jiefeng Li, Haoyu Wang, Yinghong Fang, and Cewu Lu. Pose flow: Efficient online pose tracking. arXiv preprint arXiv:1802.00977, 2018.\\n\\n[67] Yihong Xu, Yutong Ban, Guillaume Delorme, Chuang Gan, Daniela Rus, and Xavier Alameda-Pineda. Transcenter: Transformers with dense queries for multiple-object tracking. arXiv preprint arXiv:2103.15145, 2021.\\n\\n[68] Bin Yan, Yi Jiang, Peize Sun, Dong Wang, Zehuan Yuan, Ping Luo, and Huchuan Lu. Towards grand unification of object tracking. In ECCV. Springer, 2022.\\n\\n[69] Fangao Zeng and et al. Motr: End-to-end multiple-object tracking with transformer. In ECCV. Springer, 2022.\\n\\n[70] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Byte-track: Multi-object tracking by associating every detection box. arXiv preprint arXiv:2110.06864, 2021.\\n\\n[71] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. International Journal of Computer Vision, 129(11):3069\u20133087, 2021.\\n\\n[72] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-iou loss: Faster and better learning for bounding box regression. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 12993\u201313000, 2020.\\n\\n[73] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\\\"ahenb\\\"uhl. Tracking objects as points. In ECCV, pages 474\u2013490. Springer, 2020.\\n\\n[74] Xingyi Zhou, Dequan Wang, and Philipp Kr\\\"ahenb\\\"uhl. Objects as points. arXiv preprint arXiv:1904.07850, 2019.\\n\\n[75] Xingyi Zhou, Tianwei Yin, Vladlen Koltun, and Philipp Kr\\\"ahenb\\\"uhl. Global tracking transformers. In CVPR, 2022.\"}"}
{"id": "CVPR-2023-1492", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Example of how Observation-centric Re-Update (ORU) reduces the error accumulation when a track is broken. The target is occluded between the second and the third time step and the tracker finds it back at the third step. Yellow boxes are the state observations by the detector. White stars are the estimated centers without ORU. Yellow stars are the estimated centers fixed by ORU. The gray star on the fourth step is the estimated center without ORU and fails to match observations. The pipeline with less noise and more robustness over occlusion and non-linear motion. The key is to design the tracker as observation-centric instead of estimation-centric. If a track is recovered from being untracked, we use an Observation-centric Re-Update (ORU) strategy to counter the accumulated error during the untracked period. OC-SORT also adds an Observation-Centric Momentum (OCM) term in the association cost. Please refer to Algorithm 1 in Appendix for the pseudo-code of OC-SORT. The pipeline is shown in Fig. 2.\\n\\n4.1. Observation-centric Re-Update (ORU)\\n\\nIn practice, even if an object can be associated again by SORT after a period of being untracked, it is probably lost again because its KF parameters have already deviated far away from the correct due to the temporal error magnification. To alleviate this problem, we propose Observation-centric Re-Update (ORU) to reduce the accumulated error. Once a track is associated with an observation again after a period of being untracked (\\\"re-activation\\\"), we backcheck the period of its being lost and re-update the parameters of KF. The re-update is based on \\\"observations\\\" from a virtual trajectory. The virtual trajectory is generated referring to the observations on the steps starting and ending the untracked period. For example, by denoting the last-seen observation before being untracked as $z_{t_1}$ and the observation triggering the re-association as $z_{t_2}$, the virtual trajectory is denoted as $\\tilde{z}_{t} = T_{\\\\text{virtual}}(z_{t_1}, z_{t_2}, t_1 < t < t_2)$. Then, along the trajectory of $\\\\tilde{z}_{t}$ ($t_1 < t < t_2$), we run the loop of predict and re-update. The re-update operation is\\n\\n$$\\nK_{t} = P_{t | t-1} H_{t}^{\\\\top} (H_{t} P_{t | t-1} H_{t}^{\\\\top} + R_{t})^{-1} \\\\hat{x}_{t | t-1} = \\\\hat{x}_{t | t-1} + K_{t}(\\\\tilde{z}_{t} - H_{t} \\\\hat{x}_{t | t-1})\\n$$\\n\\n$$\\nP_{t | t} = (I - K_{t} H_{t}) P_{t | t-1}\\n$$\\n\\nAs the observations on the virtual trajectory match the motion pattern anchored by the last-seen and the latest associated real observations, the update will not suffer from the error accumulated through the dummy update anymore. We call the proposed process Observation-centric Re-Update. It serves as an independent stage outside the predict-update loop and is triggered only a track is re-activated from a period of having no observations.\\n\\n4.2. Observation-Centric Momentum (OCM)\\n\\nIn a reasonably short time interval, we can approximate the motion as linear. And the linear motion assumption also asks for consistent motion direction. But the noise prevents us from leveraging the consistency of direction. To be precise, to determine the motion direction, we need the object state on two steps with a time difference $\\\\Delta t$. If $\\\\Delta t$ is small, the velocity noise would be significant because of the estimation's sensitivity to state noise. If $\\\\Delta t$ is big, the noise of direction estimation can also be significant because of the temporal error magnification and the failure of linear motion assumption. As state observations have no problem of temporal error magnification that state estimations suffer from, we propose to use observations instead of estimations to reduce the noise of motion direction calculation and introduce the term of velocity consistency to help the association. With the new term, given $N$ existing tracks and $M$ detected observations, the association cost is\\n\\n$$\\nc(x, y) = \\\\sqrt{(x_{\\\\text{det}} - x_{\\\\text{track}})^2 + (y_{\\\\text{det}} - y_{\\\\text{track}})^2} + \\\\alpha \\\\sqrt{(\\\\theta_{\\\\text{det}} - \\\\theta_{\\\\text{track}})^2 + (\\\\phi_{\\\\text{det}} - \\\\phi_{\\\\text{track}})^2 + (\\\\psi_{\\\\text{det}} - \\\\psi_{\\\\text{track}})^2}\\n$$\\n\\nFigure 4. Calculation of motion direction difference in OCM. The green line indicates an existing track and the dots are the observations on it. The red dots are the new observations to be associated. The blue link and the yellow link form the directions of $\\\\theta_{\\\\text{track}}$ and $\\\\theta_{\\\\text{intention}}$ respectively. The included angle is the difference of direction $\\\\Delta \\\\theta$. The included angle is the difference of direction $\\\\Delta \\\\theta$. ci\"}"}
{"id": "CVPR-2023-1492", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The association cost matrix is formulated as:\\n\\n$$C(\\\\hat{X}, Z) = C_{IoU}(\\\\hat{X}, Z) + \\\\lambda C_v(Z, Z)$$\\n\\nwhere \\\\(\\\\hat{X} \\\\in \\\\mathbb{R}^{N \\\\times 7}\\\\) is the set of object state estimations and \\\\(Z \\\\in \\\\mathbb{R}^{M \\\\times 5}\\\\) is the set of observations on the new time step. \\\\(\\\\lambda\\\\) is a weighting factor. \\\\(Z\\\\) contains the trajectory of observations of all existing tracks. \\\\(C_{IoU}(\\\\cdot, \\\\cdot)\\\\) calculates the negative pairwise IoU (Intersection over Union) and \\\\(C_v(\\\\cdot, \\\\cdot)\\\\) calculates the consistency between the directions of: i) linking two observations on an existing track (\\\\(\\\\theta_{\\\\text{track}}\\\\)) and ii) linking a track's historical observation and a new observation (\\\\(\\\\theta_{\\\\text{intention}}\\\\)). \\\\(C_v\\\\) contains all pairs of \\\\(\\\\Delta \\\\theta = |\\\\theta_{\\\\text{track}} - \\\\theta_{\\\\text{intention}}|\\\\).\\n\\nIn our implementation, we calculate the motion direction in radians, namely:\\n\\n$$\\\\theta = \\\\arctan\\\\left(\\\\frac{v_1 - v_2}{u_1 - u_2}\\\\right)$$\\n\\nwhere \\\\((u_1, v_1)\\\\) and \\\\((u_2, v_2)\\\\) are the observations on two different time steps. The calculation is also illustrated in Figure 4.\\n\\nFollowing the assumptions of noise distribution mentioned before, we can derive a closed-form probability density function of the distribution of the noise in the direction estimation. The derivation is explained in detail in Appendix A. By analyzing the property of this distribution, we reach a conclusion that, under the linear-motion model, the scale of the noise of direction estimation is negatively correlated to the time difference between the two observation points, i.e., \\\\(\\\\Delta t\\\\). This suggests increasing \\\\(\\\\Delta t\\\\) to achieve a low-noisy estimation of \\\\(\\\\theta\\\\). However, the assumption of linear motion typically holds only when \\\\(\\\\Delta t\\\\) is small enough. Therefore, the choice of \\\\(\\\\Delta t\\\\) requires a trade-off.\\n\\nBesides ORU and OCM, we also find it empirically helpful to check a track\u2019s last presence to recover it from being lost. We thus apply a heuristic Observation-Centric Recovery (OCR) technique. OCR will start a second attempt of associating between the last observation of unmatched tracks to the unmatched observations after the usual association stage. It can handle the case of an object stopping or being occluded for a short time interval.\\n\\n5. Experiments\\n\\n5.1. Experimental Setup\\n\\nDatasets. We evaluate our method on multiple multi-object tracking datasets including MOT17 [41], MOT20 [14], KITTI [20], DanceTrack [54] and CroHD [56]. MOT17 [41] and MOT20 [14] are for pedestrian tracking, where targets mostly move linearly, while scenes in MOT20 are more crowded. KITTI [20] is for pedestrian and car tracking with a relatively low frame rate of 10 FPS. CroHD is a dataset for head tracking in the crowd and the results on it are included in the appendix. DanceTrack [54] is a recently proposed dataset for human tracking. For the data in DanceTrack, object localization is easy, but the object motion is highly non-linear. Furthermore, the objects have a close appearance, severe occlusion, and frequent crossovers. Considering our goal is to improve tracking robustness under occlusion and non-linear object motion, we would emphasize the comparison on DanceTrack.\\n\\nImplementations. For a fair comparison, we directly apply the object detections from existing baselines. For MOT17, MOT20, and DanceTrack, we use the publicly available YOLOX [19] detector weights by ByteTrack [70]. For KITTI [20], we use the detections from PermaTrack [57] publicly available in the official release. For ORU, we generate the virtual trajectory during occlusion with the constant-velocity assumption. Therefore, Eq. 6 is adopted as:\\n\\n$$\\\\tilde{z}_t = z_{t_1} + \\\\Delta t - t_1 \\\\cdot (z_{t_2} - z_{t_1})$$\\n\\nfor \\\\(t_1 < t < t_2\\\\). For OCM, the velocity direction is calculated using the observations three time steps apart, i.e., \\\\(\\\\Delta t = 3\\\\). The direction difference is measured by the absolute difference of angles in radians. We set \\\\(\\\\lambda = 0\\\\) in Eq. 8. Following the common practice of SORT, we set the detection confidence threshold at 0.4 for MOT20 and 0.6 for other datasets. The IoU threshold during association is 0.3.\\n\\nMetrics. We adopt HOTA [37] as the main metric as it maintains a proper balance between the accuracy of object detection and association [37]. We also emphasize AssA to evaluate the association performance. IDF1 is also used for association performance evaluation. Other metrics we report, such as MOTA, are highly related to detection performance. It is fair to use these metrics only when all methods use the same detections for tracking, which is referred to as \u201cpublic tracking\u201d as reported in Appendix C.\\n\\n5.2. Benchmark Results\\n\\nHere we report the benchmark results on multiple datasets. We put all methods that use the shared detection results in the blue blocks at the bottom of each table. MOT17 and MOT20. We report OC-SORT\u2019s performance on MOT17 and MOT20 in Table 1 and Table 2 using private detections. To make a fair comparison, we use the same detection as ByteTrack [70]. OC-SORT achieves performance comparable to other state-of-the-art methods. Our gains are especially significant in MOT20 under severe pedestrian occlusion, setting a state-of-the-art HOTA of 62.1. As our method is designed to be simple for better generalization, we do not use adaptive detection thresholds as in ByteTrack. Also, ByteTrack uses more detections of low-confidence to achieve higher MOTA scores but we keep the detection confidence threshold the same as on other datasets, which is the common practice in the community. We inherit the linear interpolation on the two datasets by baseline methods for a fair comparison. To more clearly discard the variance from the detector, we also perform public tracking on MOT17 and MOT20, which is reported in Table 12 and Table 13 in [9691].\"}"}
{"id": "CVPR-2023-1492", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Results on MOT17-test with the private detections. ByteTrack and OC-SORT share detections.\\n\\n| Tracker   | HOTA  | MOTA  | IDF1 | FP(10^4) | FN(10^4) | IDs  | Frag | AssA | AssR |\\n|-----------|-------|-------|------|----------|----------|------|------|------|------|\\n| FairMOT   | 59.3  | 73.7  | 72.3 | 2.75     | 11.7     | 3,303| 8,073| 58.0 | 63.6 |\\n| TransCt   | 54.5  | 73.2  | 62.2 | 2.31     | 12.4     | 4,614| 9,519| 49.7 | 54.2 |\\n| TransTrk  | 54.1  | 75.2  | 63.5 | 5.02     | 8.64     | 3,603| 4,872| 47.9 | 57.1 |\\n| GRTU      | 62.0  | 74.9  | 75.0 | 3.20     | 10.8     | 1,812| 1,824| 62.1 | 65.8 |\\n| QDTrack   | 53.9  | 68.7  | 66.3 | 2.66     | 14.7     | 3,378| 8,091| 52.7 | 57.2 |\\n| MOTR      | 57.2  | 71.9  | 68.4 | 2.11     | 13.6     | 2,115| 3,897| 55.8 | 59.2 |\\n| PermaTr   | 55.5  | 73.8  | 68.9 | 2.90     | 11.5     | 3,699| 6,132| 53.1 | 59.8 |\\n| TransMOT  | 61.7  | 76.7  | 75.1 | 3.62     | 9.32     | 2,346| 7,719| 59.9 | 66.5 |\\n| GTR       | 59.1  | 75.3  | 71.5 | 2.68     | 11.0     | 2,859|-     | 61.6 | -    |\\n| DST-Tracker | 60.1 | 75.2  | 72.3 | 2.42     | 11.0     | 2,729|-     | 62.1 | -    |\\n| MeMOT     | 56.9  | 72.5  | 69.0 | 2.72     | 11.5     | 2,724|-     | 55.2 | -    |\\n| UniCorn   | 61.7  | 77.2  | 75.5 | 5.01     | 7.33     | 5,379|-     | -    | -    |\\n| ByteTrack | 63.1  | 80.3  | 77.3 | 2.55     | 8.37     | 2,196| 2,277| 62.0 | 68.2 |\\n| OC-SORT   | 63.2  | 78.0  | 77.5 | 1.51     | 10.8     | 1,950| 2,040| 63.2 | 67.5 |\\n\\nTable 2. Results on MOT20-test with private detections. ByteTrack and OC-SORT share detections.\\n\\n| Tracker   | HOTA  | MOTA  | IDF1 | FP(10^4) | FN(10^4) | IDs  | Frag | AssA | AssR |\\n|-----------|-------|-------|------|----------|----------|------|------|------|------|\\n| FairMOT   | 54.6  | 61.8  | 67.3 | 10.3     | 8.89     | 5,243| 7,874| 54.7 | 60.7 |\\n| TransCt   | 43.5  | 58.5  | 49.6 | 6.42     | 14.6     | 4,695| 9,581| 37.0 | 45.1 |\\n| Semi-TCL  | 55.3  | 65.2  | 70.1 | 6.12     | 11.5     | 4,139| 8,508| 56.3 | 60.9 |\\n| CSTrack   | 54.0  | 66.6  | 68.6 | 2.54     | 14.4     | 3,196| 7,632| 54.0 | 57.6 |\\n| GSDT      | 53.6  | 67.1  | 67.5 | 3.19     | 13.5     | 3,131| 9,875| 52.7 | 58.5 |\\n| TransMOT  | 61.9  | 77.5  | 75.2 | 3.42     | 8.08     | 1,615| 2,421| 60.1 | 66.3 |\\n| MeMOT     | 54.1  | 63.7  | 66.1 | 4.79     | 13.8     | 1,938|-     | 55.0 | -    |\\n| ByteTrack | 61.3  | 77.8  | 75.2 | 2.62     | 8.76     | 1,223| 1,460| 59.6 | 66.2 |\\n| OC-SORT   | 62.1  | 75.5  | 75.9 | 1.80     | 10.8     | 913  | 1,198| 62.0 | 67.5 |\\n\\nTable 3. Results on DanceTrack test set. Methods in the blue block share the same detections.\\n\\n| Tracker   | HOTA  | DetA  | AssA | MOTA  | IDF1  | IDs  | Frag |\\n|-----------|-------|-------|------|-------|-------|------|------|\\n| CenterTrack | 41.8 | 78.1  | 22.6 | 86.8  | 35.7  |      |      |\\n| FairMOT   | 39.7  | 66.7  | 23.8 | 82.2  | 40.8  |      |      |\\n| QDTrack   | 45.7  | 72.1  | 29.2 | 83.0  | 44.8  |      |      |\\n| TransTrk  | 45.5  | 75.9  | 27.5 | 88.4  | 45.2  |      |      |\\n| TraDes    | 43.3  | 74.5  | 25.4 | 86.2  | 41.2  |      |      |\\n| MOTR      | 54.2  | 73.5  | 40.2 | 79.7  | 51.5  |      |      |\\n| GTR       | 48.0  | 72.5  | 31.9 | 84.7  | 50.3  |      |      |\\n| DST-Tracker | 51.9 | 72.3  | 34.6 | 84.9  | 51.0  |      |      |\\n| SORT      | 47.9  | 72.0  | 31.2 | 91.8  | 50.8  |      |      |\\n| DeepSORT  | 45.6  | 71.0  | 29.7 | 87.8  | 47.9  |      |      |\\n| ByteTrack | 47.3  | 71.6  | 31.4 | 89.5  | 52.5  |      |      |\\n| OC-SORT   | 54.6  | 80.4  | 40.2 | 89.6  | 54.6  |      |      |\\n| OC-SORT + Linear Interp | 55.1 | 80.4  | 40.4 | 92.2  | 54.9  |      |      |\\n\\nAppendix C. OC-SORT still outperforms the existing state-of-the-art in public tracking settings.\\n\\nDanceTrack. To evaluate OC-SORT under challenging non-linear object motion, we report results on the DanceTrack in Table 3. OC-SORT sets a new state-of-the-art, outperforming the baselines by a great margin under non-linear object motions. We compare the tracking results of SORT and OC-SORT under extreme non-linear situations in Fig. 1 and more samples are available in Fig. 8 in Appendix E. We also visualize the output trajectories by OC-SORT and SORT on randomly selected DanceTrack video clips in Fig. 9 in Appendix E. For multi-object tracking in occlusion and non-linear motion, the results on DanceTrack are strong evidence of the effectiveness of OC-SORT.\\n\\nKITTI. In Table 4 we report the results on the KITTI dataset. For a fair comparison, we adopt the detector weights by PermaTr [57] and report its performance in the table as well. We run OC-SORT given the shared detections. As initializing SORT's track requires continuous tracking across several frames (\u201cminimum hits\u201d), we observe that the results not recorded during the track initialization make a significant difference. To address this problem, we perform offline head padding (HP) post-processing by writing these entries back after finishing the online tracking stage. The results of the car category on KITTI show an essential shortcoming of the default implementation version of OC-SORT that it chooses the IoU matching for the association. When the objects move fast or the frame rate is low, the IoU of bounding boxes between consecutive frames can be very low or even zero. This issue does not come from the intrinsic design of OC-SORT and is widely observed when using IoU as the association cue. Adding other cues [49, 72, 73] and appearance similarity [38, 63] have been demonstrated [63] effective to solve this. In contrast to the relatively inferior car tracking performance, OC-SORT improves pedestrian tracking performance to a new state-of-the-art. Using the same detections, OC-SORT achieves a large performance gap over PermaTr with 10x faster speed. The results on multiple benchmarks have demonstrated the effectiveness and efficiency of OC-SORT. We note that we use a shared parameter stack across datasets. Carefully tuning the parameters can probably further boost the performance. For example, the adaptive detection threshold is proven useful in previous work [70]. Besides the association accuracy, we also care about the inference speed. Given off-the-shelf detections, OC-SORT runs at 793 FPS on an Intel i9-9980XE CPU @ 3.00GHz. Therefore, OC-SORT...\"}"}
{"id": "CVPR-2023-1492", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Results on KITTI-test. Our method uses the same detections as PermaTr [57].\\n\\n| Tracker | HOTA  | MOTA  | AssA | IDs  | Frag |\\n|---------|-------|-------|------|------|------|\\n| IMMDP   | 68.66 | 82.75 | 69.76| 211  | -    |\\n| SMAT    | 71.88 | 83.64 | 72.13| 198  | 294  |\\n| TrackMPNN | 72.30 | 87.33 | 70.63| 481  | 237  |\\n| MPNTrack| -     | -     | -    | -    | -    |\\n| CenterTr| 73.02 | 88.83 | 71.18| 254  | 227  |\\n| LGM     | 73.14 | 87.60 | 72.31| 448  | 164  |\\n| TuSimple | 71.55 | 86.31 | 71.11| 292  | 218  |\\n| PermaTr | 77.42 | 90.85 | 77.66| 275  | 271  |\\n| OC-SORT | 74.64 | 87.81 | 74.52| 257  | 318  |\\n| OC-SORT + HP | 76.54 | 90.28 | 76.39| 250  | 280  |\\n\\nTable 5. Ablation on MOT17-val and DanceTrack-val.\\n\\n| Component | MOT17-val | DanceTrack-val |\\n|-----------|-----------|----------------|\\n| OCM       | 64.9      | 47.8           |\\n| OCR       | 66.3      | 48.5           |\\n| ORU       | 66.4      | 52.1           |\\n\\nTable 6. Ablation on the trajectory hypothesis in ORU.\\n\\n| Hypothesis  | MOT17-val | DanceTrack-val |\\n|-------------|-----------|----------------|\\n| Const. Speed| 66.5      | 52.1           |\\n| GPR         | 63.1      | 49.5           |\\n| Linear Regression | 64.3 | 49.3 |\\n| Const. Acceleration | 66.2 | 51.3 |\\n\\nTable 7. Influence from the value of $\\\\Delta t$ in OCM.\\n\\n| $\\\\Delta t$ | MOT17-val | DanceTrack-val |\\n|------------|-----------|----------------|\\n| 1          | 66.1      | 51.3           |\\n| 2          | 66.3      | 52.2           |\\n| 3          | 66.5      | 52.1           |\\n| 6          | 66.0      | 52.1           |\\n\\n5.3. Ablation Study\\n\\nComponent Ablation. We ablate the contribution of proposed modules on the validation sets of MOT17 and DanceTrack in Table 5. The splitting of the MOT17 validation set follows a popular convention [73]. The results demonstrate the effectiveness of the proposed modules in OC-SORT.\\n\\nThe results show that the performance gain from ORU is significant on both datasets but OCM only shows significant help on DanceTrack dataset where object motion is more complicated and the occlusion is heavy. The ablation study proves the effectiveness of our proposed method to improve tracking robustness in occlusion and non-linear motion.\\n\\nVirtual Trajectory in ORU. For simplicity, we follow the naive hypothesis of constant speed to generate a virtual trajectory in ORU. There are other alternatives like constant acceleration, regression-based fitting such as Linear Regression (LR) or Gaussian Process Regression (GPR), and Near Constant Acceleration Model (NCAM) [27]. The results of comparing these choices are shown in Table 6. For GPR, we use the RBF kernel $k(x, x') = \\\\exp\\\\left(-\\\\frac{|x - x'|^2}{2}\\\\right)$.\\n\\nWe provide more studies on the kernel configuration in Appendix B. The results show that local hypotheses such as Constant Speed/Acceleration perform much better than global hypotheses such as LR and GPR. This is probably because, as virtual trajectory generation happens in an online fashion, it is hard to get a reliable fit using only limited data points on historical time steps.\\n\\n$\\\\Delta t$ in OCM. There is a trade-off when choosing the time difference $\\\\Delta t$ in OCM (Section 4). A large $\\\\Delta t$ decreases the noise of velocity estimation but is also likely to discourage approximating object motion as linear. Therefore, we study the influence of varying $\\\\Delta t$ in Table 7. Our results agree with our analysis that increasing $\\\\Delta t$ from $\\\\Delta t = 1$ can boost performance. But increasing $\\\\Delta t$ higher than a best-practice value instead hurts the performance because of the difficulty of maintaining the approximation of linear motion.\\n\\n6. Conclusion\\n\\nWe analyze the popular motion-based tracker SORT and recognize its intrinsic limitations from using Kalman filter. These limitations significantly hurt tracking accuracy when the tracker fails to gain observations for supervision\u2014likely caused by unreliable detectors, occlusion, or fast and non-linear target object motion. To address these issues, we propose Observation-Centric SORT (OC-SORT).\\n\\nOC-SORT is more robust to occlusion and non-linear object motion while keeping simple, online, and real-time. In our experiments on diverse datasets, OC-SORT significantly outperforms the state-of-the-art. The gain is especially significant for multi-object tracking under occlusion and non-linear object motion.\\n\\nAcknowledgement. We thank David Held, Deva Ramanan, and Siddharth Ancha for the discussion about the theoretical modeling of OC-SORT. We thank Yuda Song for the help with the analysis of OCM error distribution. We also would like to thank Zhengyi Luo, Yuda Song, and Erica Weng for the detailed feedback on the paper writing. We also thank Yifu Zhang for sharing his experience with ByteTrack. This project was sponsored in part by NSF NRI award 2024173.\"}"}
{"id": "CVPR-2023-1492", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Observation-Centric SORT:\\nRethinking SORT for Robust Multi-Object Tracking\\n\\nJinkun Cao\\nJiangmiao Pang\\nXinshuo Weng\\nRawal Khirodkar\\nKris Kitani\\n\\n1 Carnegie Mellon University\\n2 Shanghai AI Laboratory\\n3 Nvidia\\n\\nAbstract\\nKalman filter (KF) based methods for multi-object tracking (MOT) make an assumption that objects move linearly. While this assumption is acceptable for very short periods of occlusion, linear estimates of motion for prolonged time can be highly inaccurate. Moreover, when there is no measurement available to update Kalman filter parameters, the standard convention is to trust the priori state estimations for posteriori update. This leads to the accumulation of errors during a period of occlusion. The error causes significant motion direction variance in practice. In this work, we show that a basic Kalman filter can still obtain state-of-the-art tracking performance if proper care is taken to fix the noise accumulated during occlusion. Instead of relying only on the linear state estimate (i.e., estimation-centric approach), we use object observations (i.e., the measurements by object detector) to compute a virtual trajectory over the occlusion period to fix the error accumulation of filter parameters. This allows more time steps to correct errors accumulated during occlusion. We name our method Observation-Centric SORT (OC-SORT). It remains Simple, Online, and Real-Time but improves robustness during occlusion and non-linear motion. Given off-the-shelf detections as input, OC-SORT runs at 700+ FPS on a single CPU. It achieves state-of-the-art on multiple datasets, including MOT17, MOT20, KITTI, head tracking, and especially DanceTrack where the object motion is highly non-linear. The code and models are available at https://github.com/noahcao/OC_SORT.\\n\\n1. Introduction\\nWe aim to develop a motion model-based multi-object tracking (MOT) method that is robust to occlusion and non-linear motion. Most existing motion model-based algorithms assume that the tracking targets have a constant velocity within a time interval, which is called the linear motion assumption. This assumption breaks in many practical scenarios, but it still works because when the time interval is small enough, the object's motion can be reasonably approximated as linear. In this work, we are motivated by the fact that most of the errors from motion model-based tracking methods occur when occlusion and non-linear motion happen together. To mitigate the adverse effects caused, we first rethink current motion models and recognize some limitations. Then, we propose addressing them for more robust tracking performance, especially in occlusion.\\n\\nAs the main branch of motion model-based tracking, filtering-based methods assume a transition function to predict the state of objects on future time steps, which are called state \\\"estimations\\\". Besides estimations, they leverage an observation model, such as an object detector, to derive the state measurements of target objects, also called \\\"observations\\\". Observations usually serve as auxiliary information to help update the posteriori parameters of the filter. The trajectories are still extended by the state estimations. Among this line of work, the most widely used one is SORT [3], which uses a Kalman filter (KF) to estimate object states and a linear motion function as the transition function.\\n\\nFigure 1. Samples from the results on DanceTrack [54]. SORT and OC-SORT use the same detection results. On the third frame, SORT encounters an ID switch for the backflip target while OC-SORT tracks it consistently.\"}"}
{"id": "CVPR-2023-1492", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"function between time steps. However, SORT shows insufficient tracking robustness when the object motion is non-linear, and no observations are available when updating the filter posteriori parameters.\\n\\nIn this work, we recognize three limitations of SORT.\\n\\nFirst, although the high frame rate is the key to approximating the object motion as linear, it also amplifies the model's sensitivity to the noise of state estimations. Specifically, between consecutive frames of a high frame-rate video, we demonstrate that the noise of displacement of the object can be of the same magnitude as the actual object displacement, leading to the estimated object velocity by KF suffering from a significant variance. Also, the noise in the velocity estimate will accumulate into the position estimate by the transition process. Second, the noise of state estimations by KF is accumulated along the time when there is no observation available in the update stage of KF. We show that the error accumulates very fast with respect to the time of the target object's being untracked. The noise's influence on the velocity direction often makes the track lost again even after re-association. Last, given the development of modern detectors, the object state by detections usually has lower variance than the state estimations propagated along time steps by a fixed transition function in filters. However, SORT is designed to prolong the object trajectories by state estimations instead of observations.\\n\\nTo relieve the negative effect of these limitations, we propose two main innovations in this work. First, we design a module to use object state observations to reduce the accumulated error during the track's being lost in a backcheck fashion. To be precise, besides the traditional stages of predict and update, we add a stage of re-update to correct the accumulated error. The re-update is triggered when a track is re-activated by associating to an observation after a period of being untracked. The re-update uses virtual observations on the historical time steps to prevent error accumulation. The virtual observations come from a trajectory generated using the last-seen observation before untracked and the latest observation re-activating this track as anchors. We name it Observation-centric Re-Update (ORU).\\n\\nBesides ORU, the assumption of linear motion provides the consistency of the object motion direction. But this cue is hard to be used in SORT's association because of the heavy noise in direction estimation. But we propose an observation-centric manner to incorporate the direction consistency of tracks in the cost matrix for the association. We name it Observation-Centric Momentum (OCM). We also provide analytical justification for the noise of velocity direction estimation in practice.\\n\\nThe proposed method, named as Observation-centric SORT or OC-SORT in short, remains simple, online, real-time and significantly improves robustness over occlusion and non-linear motion. Our contributions are summarized as the following:\\n\\n1. We recognize, analytically and empirically, three limitations of SORT, i.e. sensitivity to the noise of state estimations, error accumulation over time, and being estimation-centric;\\n2. We propose OC-SORT for tracking under occlusion and non-linear motion by fixing SORT's limitations. It achieves state-of-the-art performance on multiple datasets in an online and real-time fashion.\\n\\n2. Related Works\\n\\nMotion Models. Many modern MOT algorithms [3, 11, 63, 70, 73] use motion models. Typically, these motion models use Bayesian estimation [34] to predict the next state by maximizing a posterior estimation. As one of the most classic motion models, Kalman filter (KF) [30] is a recursive Bayes filter that follows a typical predict-update cycle. The true state is assumed to be an unobserved Markov process, and the measurements are observations from a hidden Markov model [44]. Given that the linear motion assumption limits KF, follow-up works like Extended KF [52] and Unscented KF [28] were proposed to handle non-linear motion with first-order and third-order Taylor approximation. However, they still rely on approximating the Gaussian prior assumed by KF and require motion pattern assumption. On the other hand, particle filters [22] solve the non-linear motion by sampling-based posterior estimation but require exponential order of computation. Therefore, these variants of Kalman filter and particle filters are rarely adopted in the visual multi-object tracking and the mostly adopted motion model is still based on Kalman filter [3].\\n\\nMulti-object Tracking. As a classic computer vision task, visual multi-object tracking is traditionally approached from probabilistic perspectives, e.g. joint probabilistic association [1]. And modern video object tracking is usually built upon modern object detectors [46, 48, 74]. SORT [3] adopts the Kalman filter for motion-based multi-object tracking given observations from deep detectors. DeepSORT [63] further introduces deep visual features [23, 51] into object association under the framework of SORT. Re-identification-based object association [42, 63, 71] has also become popular since then but falls short when scenes are crowded and objects are represented coarsely (e.g. enclosed by bounding boxes), or object appearance is not distinguishable. More recently, transformers [58] have been introduced to MOT [8, 39, 55, 69] to learn deep representations from both visual information and object trajectories. However, their performance still has a significant gap between state-of-the-art tracking-by-detection methods in terms of both accuracy and time efficiency.\"}"}
{"id": "CVPR-2023-1492", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Rethink the Limitations of SORT\\n\\nIn this section, we review Kalman filter and SORT [3]. We recognize some of their limitations, which are significant with occlusion and non-linear object motion. We are motivated to improve tracking robustness by fixing them.\\n\\n3.1. Preliminaries\\n\\nKalman filter (KF) [30] is a linear estimator for dynamical systems discretized in the time domain. KF only requires the state estimations on the previous time step and the current measurement to estimate the target state on the next time step. The filter maintains two variables, the posteriori state estimate $x$, and the posteriori estimate covariance matrix $P$.\\n\\nIn the task of object tracking, we describe the KF process with the state transition model $F$, the observation model $H$, the process noise $Q$, and the observation noise $R$. At each step $t$, given observations $z_t$, KF works in an alternation of predict and update stages:\\n\\n**predict**\\n\\n$$\\\\hat{x}_{t|t-1} = F_t \\\\hat{x}_{t-1|t-1}$$\\n\\n$$P_{t|t-1} = F_t P_{t-1|t-1} F_t^T + Q_t$$\\n\\n**update**\\n\\n$$K_t = P_{t|t-1} H_t^T (H_t P_{t|t-1} H_t^T + R_t)^{-1}$$\\n\\n$$\\\\hat{x}_{t|t} = \\\\hat{x}_{t|t-1} + K_t (z_t - H_t \\\\hat{x}_{t|t-1})$$\\n\\n$$P_{t|t} = (I - K_t H_t) P_{t|t-1}$$\\n\\n(1)\\n\\nThe stage of predict is to derive the state estimations on the next time step. Given a measurement of target states on the next step, the stage of update aims to update the posteriori parameters in KF. Because the measurement comes from the observation model $H$, it is also called \u201cobservation\u201d in many scenarios.\\n\\nSORT [3] is a multi-object tracker built upon KF. The KF's state $x$ in SORT is defined as\\n\\n$$x = [u, v, s, r, \\\\dot{u}, \\\\dot{v}, \\\\dot{s}]^T$$\\n\\nwhere $(u,v)$ is the 2D coordinates of the object center in the image. $s$ is the bounding box scale (area) and $r$ is the bounding box aspect ratio. The aspect ratio $r$ is assumed to be constant. The other three variables, $\\\\dot{u}$, $\\\\dot{v}$ and $\\\\dot{s}$ are the corresponding time derivatives. The observation is a bounding box $z = [u, v, w, h, c]^T$ with object center position $(u,v)$, object width $w$, and height $h$ and the detection confidence $c$ respectively. SORT assumes linear motion as the transition model $F$ which leads to the state estimation as\\n\\n$$u_{t+1} = u_t + \\\\dot{u}_t \\\\Delta t, \\\\quad v_{t+1} = v_t + \\\\dot{v}_t \\\\Delta t.$$  \\n\\n(2)\\n\\nTo leverage KF (Eq 1) in SORT for visual MOT, the stage of predict corresponds to estimating the object position on the next video frame. And the observations used for the update stage usually come from a detection model. The update stage is to update Kalman filter parameters and does not directly edit the tracking outcomes.\\n\\nWhen the time difference between two steps is constant during the transition, e.g., the video frame rate is constant, we can set $\\\\Delta t = 1$. When the video frame rate is high, SORT works well even when the object motion is non-linear globally, (e.g., dancing, fencing, wrestling) because the motion of the target object can be well approximated as linear within short time intervals. However, in practice, observations are often absent on some time steps, e.g., the target object is occluded in multi-object tracking. In such cases, we cannot update the KF parameters by the update operation as in Eq. 1 anymore. SORT uses the priori estimations directly as posterior. We call this \u201cdummy update\u201d namely\\n\\n$$\\\\hat{x}_{t|t} = \\\\hat{x}_{t|t-1}, \\\\quad P_{t|t} = P_{t|t-1}.$$  \\n\\n(3)\\n\\nThe philosophy behind such a design is to trust estimations when no observations are available to supervise them. We thus call the tracking algorithms following this scheme \u201cestimation-centric\u201d. However, we will see that this estimation-centric mechanism can cause trouble when non-linear motion and occlusion happen together.\\n\\n3.2. Limitations of SORT\\n\\nIn this section, we identify three main limitations of SORT which are connected. This analysis lays the foundation of our proposed method.\\n\\n3.2.1 Sensitive to State Noise\\n\\nNow we show that SORT is sensitive to the noise from KF's state estimations. To begin with, we assume that the estimated object center position follows $u \\\\sim N(\\\\mu_u, \\\\sigma^2_u)$ and $v \\\\sim N(\\\\mu_v, \\\\sigma^2_v)$, where $(\\\\mu_u, \\\\mu_v)$ is the underlying true position. Then, if we assume that the state noises are independent on different steps, by Eq.2, the object speed between two time steps, $t \\\\rightarrow t + \\\\Delta t$, is\\n\\n$$\\\\dot{u} = u_{t+1} - u_t, \\\\quad \\\\dot{v} = v_{t+1} - v_t,$$\\n\\n(4)\\n\\nmaking the noise of estimated speed $\\\\delta \\\\dot{u} \\\\sim N(0, \\\\sigma^2_u (\\\\Delta t)^2)$, $\\\\delta \\\\dot{v} \\\\sim N(0, \\\\sigma^2_v (\\\\Delta t)^2)$. Therefore, a small $\\\\Delta t$ will amplify the noise. This suggests that SORT will suffer from the heavy noise of velocity estimation on high-frame-rate videos. The analysis above is simplified from the reality. In practice, velocity won\u2019t be determined by the state on future time steps. For a more strict analysis, please refer to Appendix G.\\n\\nMoreover, for most multi-object tracking scenarios, the target object displacement is only a few pixels between consecutive frames. For instance, the average displacement is 1.93 pixels and 0.65 pixels along the image width and height for the MOT17 [41] training dataset. In such a case, even if the estimated position has a shift of only a single pixel, it causes a significant variation in the estimated speed. In general, the variance of the speed estimation can be of the same magnitude as the speed itself or even greater. This will not make a massive impact as the shift is only of few pixels.\"}"}
{"id": "CVPR-2023-1492", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The pipeline of our proposed OC-SORT. The red boxes are detections, orange boxes are active tracks, blue boxes are untracked tracks, and dashed boxes are the estimates from KF. During association, OCM is used to add the velocity consistency cost. The target #1 is lost on the frame $t+1$ because of occlusion. But on the next frame, it is recovered by referring to its observation of the frame $t$ by OCR. It being re-tracked triggers ORU from $t$ to $t+2$ for the parameters of its KF.\\n\\n3.2.2 Temporal Error Magnification\\n\\nFor analysis above in Eq. 4, we assume the noise of the object state is i.i.d on different time steps (this is a simplified version, a more detailed analysis is provided in Appendix G). This is reasonable for object detections but not for the estimations from KF. This is because KF's estimations always rely on its estimations on previous time steps. The effect is usually minor because KF can use observation in update to prevent the posteriori state estimation and covariance, i.e.\\n\\n$$\\\\hat{x}_t|_t \\\\text{ and } P_t|_t,$$ deviating from the true value too far away. However, when no observations are provided to KF, it cannot use observation to update its parameters. Then it has to follow Eq. 3 to prolong the estimated trajectory to the next time step. Consider a track is occluded on the time steps between $t$ and $t+T$ and the noise of speed estimate follows\\n\\n$$\\\\delta \\\\dot{u}_t \\\\sim N(0, \\\\sigma_u^2) \\\\text{ and } \\\\delta \\\\dot{v}_t \\\\sim N(0, \\\\sigma_v^2)$$\\n\\nfor SORT. On the step $t+T$, state estimation would be\\n\\n$$u_{t+T} = u_t + T \\\\dot{u}_t, \\\\quad v_{t+T} = v_t + T \\\\dot{v}_t,$$\\n\\nwhose noise follows\\n\\n$$\\\\delta \\\\dot{u}_{t+T} \\\\sim N(0, T^2 \\\\sigma_u^2) \\\\text{ and } \\\\delta \\\\dot{v}_{t+T} \\\\sim N(0, T^2 \\\\sigma_v^2).$$\\n\\nSo without the observations, the estimation from the linear motion assumption of KF results in a fast error accumulation with respect to time. Given $\\\\sigma_v$ and $\\\\sigma_u$ is of the same magnitude as object displacement between consecutive frames, the noise of final object position $(u_{t+T}, v_{t+T})$ is of the same magnitude as the object size. For instance, the size of pedestrians close to the camera on MOT17 is around $50 \\\\times 300$ pixels. So even assuming the variance of position estimation is only 1 pixel, 10-frame occlusion can accumulate a shift in final position estimation as large as the object size. Such error magnification leads to a major accumulation of errors when the scenes are crowded.\\n\\n3.2.3 Estimation-Centric\\n\\nThe aforementioned limitations come from a fundamental property of SORT that it follows KF to be estimation-centric. It allows update without the existence of observations and purely trusts the estimations. A key difference between state estimations and observations is that we can assume that the observations by an object detector in each frame are affected by i.i.d. noise $\\\\delta z \\\\sim N(0, \\\\sigma_z^2)$ while the noise in state estimations can be accumulated along the hidden Markov process. Moreover, modern object detectors use powerful object visual features [48, 51]. It makes that, even on a single frame, it is usually safe to assume $\\\\sigma_z < \\\\sigma_u$ and $\\\\sigma_z < \\\\sigma_v$ because the object localization is more accurate by detection than from the state estimations through linear motion assumption. Combined with the previously mentioned two limitations, being estimation-centric makes SORT suffer from heavy noise when there is occlusion and the object motion is not perfectly linear.\\n\\n4. Observation-Centric SORT\\n\\nIn this section, we introduce the proposed Observation-Centric SORT (OC-SORT). To address the limitations of SORT discussed above, we use the momentum of the object moving into the association stage and develop a...\"}"}
