{"id": "CVPR-2024-981", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Webar/beauty demo app. 6\\n\\n[2] Rameen Abdal, Hsin-Ying Lee, Peihao Zhu, Menglei Chai, Aliaksandr Siarohin, Peter Wonka, and Sergey Tulyakov. 3davatargan: Bridging domains for personalized editable avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4552\u20134562, 2023.\\n\\n[3] ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli Shechtman, and Zhixin Shu. Rignerf: Fully controllable neural 3d portraits. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 20364\u201320373, 2022.\\n\\n[4] Ziqian Bai, Feitong Tan, Zeng Huang, Kripasindhu Sarkar, Danhang Tang, Di Qiu, Abhimitra Meka, Ruofei Du, Ming-song Dou, Sergio Orts-Escolano, et al. Learning personalized high quality volumetric head avatars from monocular rgb videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16890\u201316900, 2023.\\n\\n[5] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20919\u201320929, 2023.\\n\\n[6] V Blanz and T Vetter. A morphable model for the synthesis of 3d faces. In 26th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH 1999), pages 187\u2013194. ACM Press, 1999.\\n\\n[7] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392\u201318402, 2023.\\n\\n[8] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[9] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In Proceedings of the European Conference on Computer Vision, pages 333\u2013350. Springer, 2022.\\n\\n[10] Anton Cherepkov, Andrey Voynov, and Artem Babenko. Navigating the gan parameter space for semantic image editing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3671\u20133680, 2021.\\n\\n[11] deepfakes. faceswap. https://github.com/deepfakes/faceswap, 2023. Accessed: 2023-10-10.\\n\\n[12] deepfakes. roop. SomdevSangwan, 2023. Accessed: 2023-10-10.\\n\\n[13] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4690\u20134699, 2019.\\n\\n[14] Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart. Learning an animatable detailed 3d face model from in-the-wild images. ACM Transactions on Graphics (ToG), 40(4):1\u201313, 2021.\\n\\n[15] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias Nie\u00dfner. Dynamic neural radiance fields for monocular 4d facial avatar reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8649\u20138658, 2021.\\n\\n[16] Xuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong, Yudong Guo, and Juyong Zhang. Reconstructing personalized semantic facial nerf models from monocular video. ACM Transactions on Graphics (TOG), 41(6):1\u201312, 2022.\\n\\n[17] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin Huang. Expressive text-to-image generation with rich text. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7545\u20137556, 2023.\\n\\n[18] Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. arXiv preprint arXiv:2303.12789, 2023.\\n\\n[19] Erik Harkonen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering interpretable gan controls. Advances in neural information processing systems, 33:9841\u20139850, 2020.\\n\\n[20] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2328\u20132337, 2023.\\n\\n[21] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023.\\n\\n[22] Wentao Jiang, Si Liu, Chen Gao, Jie Cao, Ran He, Jiashi Feng, and Shuicheng Yan. PSGAN: Pose and expression robust spatial-aware gan for customizable makeup transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5194\u20135202, 2020.\\n\\n[23] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.\\n\\n[24] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8110\u20138119, 2020.\\n\\n[25] Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. Advances in Neural Information Processing Systems, 34:852\u2013863, 2021.\"}"}
{"id": "CVPR-2024-981", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier Romero. Learning a model of facial shape and expression from 4d scans. ACM Trans. Graph., 36(6):194\u20131, 2017.\\n\\n2. Tingting Li, Ruihe Qian, Chao Dong, Si Liu, Qiong Yan, Wenwu Zhu, and Liang Lin. Beautygan: Instance-level facial makeup transfer with deep generative adversarial network. In Proceedings of the 26th ACM international conference on Multimedia, pages 645\u2013653, 2018.\\n\\n3. K-E Lin, Alex Trevithick, Keli Cheng, Michel Sarkis, Mohsen Ghafoorian, Ning Bi, Gerhard Reitmayr, and Ravi Ramamoorthi. Pvp: Personalized video prior for editable dynamic portraits using stylegan. In Computer Graphics Forum, page e14890. Wiley Online Library, 2023.\\n\\n4. Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja Fidler. Editgan: High-precision semantic image editing. Advances in Neural Information Processing Systems, 34:16331\u201316345, 2021.\\n\\n5. Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, and Yi Jin. Freedrag: Point tracking is not you need for interactive point-based image editing. arXiv preprint arXiv:2307.04684, 2023.\\n\\n6. Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, 33:15651\u201315663, 2020.\\n\\n7. Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, and Ali Mahdavi-Amiri. Sked: Sketch-guided text-based 3d editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14607\u201314619, 2023.\\n\\n8. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\\n\\n9. Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023.\\n\\n10. Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. arXiv preprint arXiv:2201.05989, 2022.\\n\\n11. Thao Nguyen, Anh Tuan Tran, and Minh Hoai. Lipstick ain\u2019t enough: beyond color matching for in-the-wild makeup transfer. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 13305\u201313314, 2021.\\n\\n12. Thu Nguyen-Phuoc, Gabriel Schwartz, Yuting Ye, Stephen Lombardi, and Lei Xiao. Alteredavatar: Stylizing dynamic 3d avatars with fast style adaptation. arXiv preprint arXiv:2305.19245, 2023.\\n\\n13. Mohit Mendiratta Pan, Mohamed Elgharib, Kartik Teetia, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt, et al. Avatarstudio: Text-driven editing of 3d dynamic human head avatars. arXiv preprint arXiv:2306.00547, 2023.\\n\\n14. Xingang Pan, Ayush Tewari, Thomas Leimk\u00fchler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.\\n\\n15. Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.\\n\\n16. Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2085\u20132094, 2021.\\n\\n17. Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Ume, Mr Dpfks, Carl Shift Facenheim, Luis RP, Jian Jiang, et al. Deepfacelab: Integrated, flexible and extensible face-swapping framework. arXiv preprint arXiv:2005.05535, 2020.\\n\\n18. Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based editing of real images. ACM Transactions on graphics (TOG), 42(1):1\u201313, 2022.\\n\\n19. Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance Fields without Neural Networks. In CVPR, 2022.\\n\\n20. Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, Boyao Zhou, Hongwen Zhang, and Yebin Liu. Control4d: Dynamic portrait editing by learning 4d gan from 2d diffusion-based editor. arXiv preprint arXiv:2305.20082, 2023.\\n\\n21. Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for semantic face editing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9243\u20139252, 2020.\\n\\n22. Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. arXiv preprint arXiv:2306.14435, 2023.\\n\\n23. Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5459\u20135469, 2022.\\n\\n24. Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, and Yebin Liu. IDE-3D: Interactive disentangled editing for high-resolution 3D-aware portrait synthesis. arXiv preprint arXiv:2205.15517, 2022.\\n\\n25. Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, and Yebin Liu. Next3d: Generative neural texture rasterization for 3D-aware head avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20991\u201321002, 2023.\"}"}
{"id": "CVPR-2024-981", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ping Tan, and Yinda Zhang. Volux-gan: A generative model for 3D face synthesis with HDRI relighting. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1\u20139, 2022.\\n\\nJunshu Tang, Bo Zhang, Binxin Yang, Ting Zhang, Dong Chen, Lizhuang Ma, and Fang Wen. 3DFaceshop: Explicitly controllable 3D-aware portrait generation. IEEE Transactions on Visualization and Computer Graphics, 2023.\\n\\nZiyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, and Michael Zollhofer. Learning compositional radiance fields of dynamic human heads. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5704\u20135713, 2021.\\n\\nYue Wu, Yu Deng, Jiaolong Yang, Fangyun Wei, Qifeng Chen, and Xin Tong. Anifacegan: Animatable 3D-aware face image generation for video avatars. Advances in Neural Information Processing Systems, 35:36188\u201336201, 2022.\\n\\nWeihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):3121\u20133138, 2022.\\n\\nYinghao Xu, Sida Peng, Ceyuan Yang, Yujun Shen, and Bolei Zhou. 3D-aware image synthesis via learning structural and textural representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18430\u201318439, 2022.\\n\\nYuelang Xu, Lizhen Wang, Xiaochen Zhao, Hongwen Zhang, and Yebin Liu. Avatarmav: Fast 3D head avatar reconstruction using motion-aware neural voxels. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201310, 2023.\\n\\nBangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Learning object-compositional neural radiance field for editable scene rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13779\u201313788, 2021.\\n\\nBangbang Yang, Chong Bao, Junyi Zeng, Hujun Bao, Yinda Zhang, Zhaopeng Cui, and Guofeng Zhang. Neumesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing. In European Conference on Computer Vision, pages 597\u2013614. Springer, 2022.\\n\\nBangbang Yang, Yinda Zhang, Yijin Li, Zhaopeng Cui, Sean Fanello, Hujun Bao, and Guofeng Zhang. Neural rendering in a room: amodal 3D understanding and free-viewpoint rendering for the closed scene composed of pre-captured objects. ACM Transactions on Graphics (TOG), 41(4):1\u201310, 2022.\\n\\nChenyu Yang, Wanrong He, Yingqing Xu, and Yang Gao. Elegant: Exquisite and locally editable gan for makeup transfer. In European Conference on Computer Vision, pages 737\u2013754. Springer, 2022.\\n\\nXingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), pages 499\u2013507. IEEE, 2022.\\n\\nWeicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 339\u2013351, 2023.\\n\\nDeheng Zhang, Clara Fernandez-Labrador, and Christopher Schroers. Coarf: Controllable 3D artistic style transfer for radiance fields. In 2024 International Conference on 3D Vision (3DV). IEEE, 2024.\\n\\nHao Zhang, Yanbo Xu, Tianyuan Dai, Tai Chi-Keung Tang, et al. Fdnerf: Semantics-driven face reconstruction, prompt editing and relighting with diffusion models. arXiv preprint arXiv:2306.00783, 2023.\\n\\nXiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and Jonathan T Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ACM Transactions on Graphics (TOG), 40(6):1\u201318, 2021.\\n\\nBoming Zhao, Bangbang Yang, Zhenyang Li, Zuoyue Li, Guofeng Zhang, Jiashu Zhao, Dawei Yin, Zhaopeng Cui, and Hujun Bao. Factorized and controllable neural re-rendering of outdoor scene for photo extrapolation. In Proceedings of the 30th ACM International Conference on Multimedia, pages 1455\u20131464, 2022.\\n\\nYufeng Zheng, Victoria Fernandez Abrevaya, Marcel C Buehler, Xu Chen, Michael J Black, and Otmar Hilliges. Imavatar: Implicit morphable head avatars from videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13545\u201313555, 2022.\\n\\nYufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J Black, and Otmar Hilliges. Pointavatar: Deformable point-based head avatars from videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21057\u201321067, 2023.\\n\\nJiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In European conference on computer vision, pages 592\u2013608. Springer, 2020.\\n\\nJiapeng Zhu, Ceyuan Yang, Yujun Shen, Zifan Shi, Bo Dai, Deli Zhao, and Qifeng Chen. Linkgan: Linking gan latents to pixels for controllable image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7656\u20137666, 2023.\\n\\nPeihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Sean: Image synthesis with semantic region-adaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5104\u20135113, 2020.\\n\\nZihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R Oswald, and Marc Pollefeys. Nice-slam: Neural implicit scalable encoding for slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12786\u201312796, 2022.\"}"}
{"id": "CVPR-2024-981", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and Guanbin Li. Dreameditor: Text-driven 3D scene editing with neural fields. arXiv preprint arXiv:2306.13455, 2023.\\n\\nWojciech Zielonka, Timo Bolkart, and Justus Thies. Towards metrical reconstruction of human faces. In European Conference on Computer Vision, pages 250\u2013269. Springer, 2022.\\n\\nWojciech Zielonka, Timo Bolkart, and Justus Thies. Instant volumetric head avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4574\u20134584, 2023.\\n\\nzllrunning. face-makeup.pytorch. https://github.com/zllrunning/face-makeup.PyTorch, 2023. Accessed: 2023-10-10.\"}"}
{"id": "CVPR-2024-981", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. We quantitatively compare with the PVP [28], Roop [12], Next3D [50] by user study and image identity similarity [13].\\n\\n|                  | PVP   | Roop  | Next3D | Ours  |\\n|------------------|-------|-------|--------|-------|\\n| Editing preservation | \u2191 29.44% | 23.33% | 5.56%  | 41.67% |\\n| Identity preservation | \u2191 31.11% | 21.11% | 5.56%  | 42.22% |\\n| Temporal consistency | \u2191 30.00% | 23.89% | 4.44%  | 41.67% |\\n| Overall           | \u2191 29.44% | 23.33% | 3.89%  | 43.33% |\\n| Image identity similarity | 0.8373 | 0.8704 | 0.8547 | 0.8845 |\\n\\nFigure 3. We compare geometry editing with PVP [28], Roop [12], Next3D [50] on INSTA [76] and NeRFBlendshape [16] avatars. The \u201cReference Animation\u201d denotes the image of the original avatar under the same expression with the rendered edited view.\"}"}
{"id": "CVPR-2024-981", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We further show more geometry editing results of our method on avatars in various representations in Fig. 4. Our method supports a convenient way to adjust the size of diverse facial features, such as eyes, mouths, jaw, etc., by editing a single rendered image from the avatars. The edits on 2D images are successfully lifted onto the avatar and rendered across different viewpoints and expression. Please refer to the supplementary Sec. C.3 for detailed visualizations of modified head geometry.\\n\\nTexture Editing.\\n\\nWe then show our capability in texture editing. We utilize Photoshop, an online makeup app WebBeauty [1], and text-driven editing method InstaPix2Pix [7] to modify the texture on 2D renderings. We show comparisons with PVP [28], Roop [12] and Next3D [50] on four distinct heads avatars (INSTA [76] for the upper three and NeRFBlendshape [16] for bottom one) in Fig. 5. Roop [12] is ineffective in transferring non-human-face-like texture, thus failing in all examples. PVP [28] only transfers partial or blurry textures, and also causes shifts across the expression and head poses. Next3D [50] successfully uplifts the texture editing in 2D images sharply. However, it still suffers from the identity shift issue in the lower two heads and a blurred pattern in the upper two heads in Fig. 5. In contrast, our method faithfully paints the complicated texture following the edited image and preserves the identity of the original avatar and consistency.\"}"}
{"id": "CVPR-2024-981", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. We compare texture editing with PVP [28], Roop [12], Next3D [50] on INSTA [76] and NeRFBlendshape [16] avatars. The \u201cReference Animation\u201d denotes the image of the original avatar under the same expression with the rendered edited view.\\n\\nFigure 6. Analysis of our effectiveness with the na\u00efve baselines that can accomplish the single-view avatar editing.\\n\\nWe show extensive texture editing results in three avatar representations in Fig. 7. Our method supports a wide range of texture editing, including global style transfer (\u201cAdd a clown makeup\u201d via text-driven editing), semantic-driven editing (changing the hair color), and free-form sketch (painting on the face). Please refer to the supplementary Sec. C.1/2 for hybrid editing and face reenactment results.\\n\\n4.3. Quantitative Comparison\\n\\nUser Study. We conducted a user study to validate our method further quantitatively. Following the evaluation protocol of Avatarstudio [38], users are required to watch the rendered videos of different methods side by side and answer each question by picking up one of the methods. For each group of editing results, we will ask four questions the same as Avatarstudio [38] on editing preservation, identity preservation, temporal consistency and overall performance in Tab. 1. We collected statistics from 30 participants in 12 groups of edit results. Results are reported in Tab. 1. Our method exhibits the best editing ability while keeping the best consistency across different facial expressions for geometry and texture editing. Moreover, our method performs the best in keeping non-edited parts untouched and making the human heads still recognizable after being edited e.g., identity preservation metric in Tab. 1. Please refer to the supplementary Sec. B.4 for more details.\\n\\nImage Identity Similarity Evaluation. Following the VoLux-GAN [51], we further evaluate the cross-view identity consistency in our edit results. We take 7 groups of geometry editing results and 7 groups of texture editing results, and render the 350 images of edited avatars with different viewpoints and expressions. We calculate the cosine similarities between each rendered image and the single-view 2D edited image and average the similarities on all rendered images as metrics. As reported in Tab. 1, our method outperforms all baselines in recovering desired editing effect and retaining identity consistency.\\n\\n4.4. Native Editing Capability of Avatar\\n\\nIn this section, we analyze the editing capability from the original avatar model and show the necessity of our design. Comparison to 3DMM-based Geometry Editing. One intuitive way to support geometry editing is updating the underlying 3DMM geometry [14]. Here, we investigate this method and verify its capability. Specifically, we run state-of-the-art single image-based 3DMM reconstruction method [75], and update the 3DMM shape parameter of the avatar model with the estimated one. Note that such an approach...\\n\\n...\"}"}
{"id": "CVPR-2024-981", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. We show our texture editing results using the 2D editing method with text-prompt, pattern painting and makeup drawing on INSTA [76] and NeRFBlendshape [16], Next3D [50] avatars.\\n\\nThe approach is only available for those avatars that use 3DMM explicitly, and we take INSTA [76] as an example and show the results in Fig. 6 (a). The 3DMM fitting tends to fail when the edited face is out-of-distribution, so the magnitude of editing may not be correct (e.g. the face shape). Changing the 3DMM parameter could also result in blurry rendering from the pre-trained avatar model, which cannot be trivially fixed even after fine-tuning the rendering decoder.\\n\\nComparison to Fine-tuning for Texture Editing. Texture editing could be done by fine-tuning the avatar with the one-shot edited image. We test this method on avatars from NeRFBlendShape [16] and show the results in Fig. 6 (b). While large structured changes, e.g., hair color, can be edited, detailed editing is largely ignored. Please refer to the supplementary Sec. C.4 for more ablation studies.\\n\\n5. Conclusion\\n\\nWe have proposed a novel generic editing approach that allows users to edit various volumetric head avatar representations from a single image, where an expression-aware modification generator lifts the editing to the 3D avatar while maintaining consistency across multiple expression and viewpoints. As a limitation, we cannot add additional objects (e.g., hat) or modify the hairstyle as shown in our supplementary Sec. C.5, which may be improved by learning extra specialized geometry addition and hair modification generators.\\n\\nAcknowledgment: This work was partially supported by the NSFC (No. 62102356) and Ant Group.\"}"}
{"id": "CVPR-2024-981", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image\\n\\nChong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui\\n\\n1 State Key Lab of CAD&CG, Zhejiang University\\n2 Google\\n3 ETH Z\u00fcrich\\n4 ByteDance\\n\\nFigure 1. We propose a generic approach to edit 3D avatars in various volumetric representations (NeRFBlendShape [16], INSTA [76], Next3D [50]) from a single perspective using 2D editing methods with drag-style, text-prompt and pattern painting. Our editing results are consistent across multiple facial expression and camera viewpoints.\\n\\nAbstract\\n\\nRecently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM-driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints.\\n\\nProject page: https://zju3dv.github.io/geneavatar/.\\n\\n1. Introduction\\n\\nRecently various volumetric representations [3, 4, 15, 16, 57, 68, 69, 76] have achieved remarkable success in reconstructing personalized, animatable, and photorealistic head avatars using implicit [15, 16, 57, 68, 69] or explicit [3, 4, 76] conditioning of 3D Morphable Models (3DMM) [6]. A popular demand, once with a created avatar model, is to edit the avatar, e.g., for face shape, facial makeup, or apply artistic effects, for the downstream applications, e.g., in virtual/augmented reality.\\n\\nIdeally, the desired editing functionality on the animatable avatar should have the following properties. (1) Adaptive: The editing method should be applicable across various volumetric avatar representations. This is particularly valuable in light of the growing diversity of avatar frameworks [16, 50, 76]. (2) User-friendly: The editing should be user-friendly and intuitive. Preferably, the editing of geometry and texture of the 3D avatar could be accomplished on a single-perspective rendered image. (3) Faithful: The editing results should be consistent across various facial expression and camera viewpoints. (4) Flexible: Both intensive editing (e.g., global appearance transfer following style prompts) and delicate local editing (e.g., dragging to enlarge eyes or ears) should be supported as illustrated in Fig. 1.\\n\\nHowever, 3D-aware avatar editing is still underexplored in both geometry and texture. One plausible way is to perform 3D editing via animatable 3D GAN [50, 52, 54], but the editing results may not be consistently reflected when expression and camera viewpoint change. Alternatively, the editing can be done on the generated 2D video using 2D personalized StyleGAN [28]; however, the identity shift is often observed. Some face-swapping methods [11, 12, 42] are capable of substituting the face in a video with another face derived from a reference image or video; however, they do not support texture editing and local geometry editing.\\n\\nTo this end, we propose GeneAvatar \u2013 a generic approach to support fine-grained 3D editing in various volumetric avatar representations from a single perspective.\"}"}
{"id": "CVPR-2024-981", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"leveraging 2D editing methods, such as drag-based methods \\\\[30, 34, 39, 47\\\\], text-driven methods \\\\[7, 17, 20, 40, 41\\\\], or image editing tools like Photoshop (see Fig. 1). We adopt a novel editing framework that formulates the editing as predicting expression-aware 3D modification fields applied in the geometry and texture space of the volumetric avatars, which makes editing independent with the original representation as long as they are in parametric-driven radiance field, e.g., 3DMM-based neural avatar. Second, to ensure that the 2D image editing can be faithfully transferred into the 3D space, we propose to learn a generative model for modification fields, which produces 3DMM conditional modification fields from a compact latent space. Given the rendered avatar image and its edited counterpart, we conduct auto-decoding optimization on this generative model to search for the latent code that best explains the editing, obtaining consistent 3DMM conditional modification fields across various viewpoints and expression. Third, inspired by the spirit of learning from the pre-trained large-scale generative model \\\\[7, 18, 32, 74\\\\], we design a novel distillation scheme to learn the expression-dependent modification from a 3DMM-based GAN \\\\[50\\\\] and 2D face editing tools \\\\[22, 27, 36\\\\]. The scheme addresses the issue of insufficient real training data (i.e., avatars with a wide range of geometry and texture changes). Besides, we develop several techniques to enhance the editing effects, including the implicit latent space guidance to stabilize the initialization and convergence of learning, and a segmentation-based loss reweight strategy for fine-grained texture inversion.\\n\\nThe contributions of our paper are summarized as follows.\\n\\n1) We propose a generic avatar editing approach that can be applied to various 3DMM driving head avatars in the neural radiance field. To achieve this, we design a novel expression-aware modification generative model, which lifts the geometry and texture editing from a single image to a consistent 3D modification field.\\n\\n2) To bootstrap the training of the modification generator with limited real paired training data, we design a distillation scheme to learn the expression-dependent geometry and texture modification from the large-scale head avatar generative model \\\\[50\\\\] and 2D face texture editing tools \\\\[22, 27, 36\\\\], and develop several techniques, including implicit guidance in latent space to improve training convergence, and a loss reweight strategy based on segmentation for fine-grained texture inversion.\\n\\n3) Extensive experiments on various head avatar representations demonstrate that our method delivers high-quality editing results and the editing effects are consistent under different viewpoints and expression.\\n\\n2. Related Work\\n\\n2D Head Avatar Editing. The manipulation of 2D head avatars has made significant strides in recent years. Various GAN-based methods \\\\[23\u201325\\\\] can result in precise and high-resolution human face editing by leveraging image space semantic information \\\\[29, 72\\\\] or controlling latent space explorations \\\\[10, 19, 46, 70\\\\]. Some approaches \\\\[22, 27, 36, 61\\\\] focus on the task of makeup transferring by exploiting the GAN to learn the transferring ability from a large unaligned makeup and non-makeup face datasets. The drag-based GAN editing approach \\\\[39, 71\\\\] gained vast popularity due to providing a user-friendly editing way. PVP \\\\[28\\\\] uses a monocular video to fine-tune StyleGAN \\\\[23, 24\\\\] to obtain the personalized image generator and provide various editing functions. The diffusion models \\\\[7, 21\\\\] also show the capability of achieving fine-grained face editing with text prompt and other conditional input. For editing an avatar in a video, lots of face-swapping methods \\\\[11, 12, 42\\\\] have emerged to provide high-quality and properly aligned face-swapping results. However, these approaches typically suffer from multi-view consistency and identity preservation.\\n\\n3D Head Avatar Editing. Neural Radiance Field \\\\[33\\\\] has exhibited great reconstruction and rendering qualities in SLAM \\\\[62, 73\\\\], scene editing \\\\[5, 58\u201360, 64\\\\] and relighting \\\\[63, 66, 67\\\\], especially promoting the emergence of many 3D avatar reconstruction \\\\[4, 16, 53, 68, 69, 76\\\\] and generation \\\\[50, 52, 54\\\\]. Some methods \\\\[2, 49, 56, 65\\\\] exploit the powerful editing ability of GAN to edit a 3D static head portrait. However, they cannot be trivially extended to the dynamic avatars. The methods \\\\[37, 38, 45\\\\] focus on style transfer of the avatar using text prompt or style image but reach a poor identity-preserving. We propose a novel 3D avatar editing approach with an expression-aware modification generative model, which can be applied to various 3DMM-based volumetric avatars and render consistent novel views with fine-grained editing across multiple viewpoints and expression while preserving identity of person.\\n\\n3. Method\\n\\nAs shown in Fig. 2, given a volumetric head avatar, we edit the avatar using a single-view image and synthesize consistent novel views across multiple expression and viewpoints. To achieve this goal, we propose a novel expression-aware modification generator to generate 3D modification fields, which can be seamlessly integrated into various representations and animated with facial expression (see Sec. 3.2). Furthermore, to bootstrap the training with limited pairwise data, we propose a novel expression-aware distillation scheme to learn the expression-dependent modifications from large-scale generative models \\\\[7, 50\\\\] (see Sec. 3.3). During the editing process, given a single edited image of a 3D avatar, we perform an auto-decoding optimization to lift 2D editing effect to the 3D space (see Sec. 3.4).\"}"}
{"id": "CVPR-2024-981", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. We use an expression-aware generative model that accepts a modification latent code $z_g$/$z_t$ and 3DMM coefficients and outputs a modification field of a tri-plane structure. The modification field modifies the geometry and texture of the template avatar by deforming the sample points $x$ and blending the color $c_o$ with the modification color $c_\\\\Delta$ respectively. We lift the 2D editing effect to 3D using an auto-decoding optimization and synthesize novel views across different expression.\\n\\n3.2. Expression-aware Modification Generator\\n\\nTo enable the modification animated with the facial expressions, we follow the architecture of 3DMM-based 3D GAN [50] to build our expression-aware modification generator. As shown in Fig. 2, our generator consists of a geometry generator $G_\\\\Delta g$ and a texture generator $G_\\\\Delta t$.\\n\\n$G_\\\\Delta g$ encodes the expression-dependent geometry modification by deforming the query points in the edited space to the original template space under each expression.\\n\\n$G_\\\\Delta t$ encodes the expression-dependent modification color of query points under each expression:\\n\\n$$x' = G_\\\\Delta g(x, z_g, v), (c_\\\\Delta, \\\\beta_\\\\Delta) = G_\\\\Delta t(x, z_t, v),$$\\n\\nwhere $x$ is the query points in the edited space, and $x'$ is the deformed point in the space of original avatar under current expression $e$. $c_\\\\Delta$ is the modification color and $\\\\beta_\\\\Delta$ determines the blending weights with the original color. $z_g$, $z_t$ are the geometry and texture modification latent code respectively, where $z_g$, $z_t \\\\in \\\\mathbb{R}^{1024}$. They control the generation of modification feature maps in the UV space. $v$ is the 3DMM mesh vertices [26] that condition the current expression $e$. Each mesh vertex has a neural feature that is retrieved in the modification feature map using pre-defined UV mapping. We rasterize the vertex features to the three axis-aligned planes to generate the tri-plane feature. The modification information of the query point $x$ is first collected by bilinear interpolation on the tri-plane feature and then decoded by the neural feature decoder [8]. Since the modification is defined as decoupled fields without relying on the original field, our generated modification field can be integrated into various volumetric avatar representations and be animated following the facial expression.\\n\\n3.3. Expression-dependent Modification Learning\\n\\nTo learn the proposed expression-aware modification, we need extensive training data on avatars with a wide range of geometry and texture changes, which is hard to obtain in practice. Following the spirit of learning high-fidelity editing ability from the large-scale generative model [7, 18, 32, 74], we propose a novel expression-aware distillation scheme to deal with insufficient real training data. We leverage the ability of 3DMM-based 3D GAN [50] and 2D face texture editing tools to generate facial editing data, which encompasses a wide range of geometry and texture editing across various expression and viewpoints.\\n\\nGeometry Distillation. We use the teacher 3DMM-based 3D GAN $G_n$ to synthesize two volumetric avatars with different geometry (an original avatar $F$ and an edited avatar $F'$) by modifying the 3DMM shape parameter of the original avatar. This provides the paired editing data for our generator to learn how to modify the geometry of the avatar while maintaining consistency across various expression and viewpoints. Specifically, we randomly sample the latent code $z_n$ in the latent space of $G_n$ as well as 3DMM shape parameter $\\\\beta$, expression parameter $\\\\psi$, and pose parameter $\\\\theta$. $\\\\beta$, $\\\\psi$ are sampled from a normal distribution whose absolute mean and standard deviation are within $[0, 1]$. $\\\\theta$ are a group of rotation vectors that have random directions within a unit sphere and magnitude within $[-6, 6]$ degrees. Then, we sample an edit vector $\\\\beta_\\\\Delta$ from a uniform distribution.\"}"}
{"id": "CVPR-2024-981", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"U(\u22123, 3) and apply it to the original shape parameter by\\n\\\\[ \\\\beta' = \\\\beta \\\\Delta + \\\\beta. \\\\]\\nThese hyperparameters w.r.t. 3DMM coefficients sampling are selected empirically to maintain the shape definition of the human head. Please refer to our supplementary Sec. B.2 for more details on 3DMM sampling.\\n\\nThe original avatar \\\\( F \\\\) and paired edited avatar \\\\( F' \\\\) are generated by\\n\\\\[ F = G_n(z_n, \\\\beta, \\\\psi, \\\\theta), \\\\quad F' = G(z_n, \\\\beta', \\\\psi, \\\\theta). \\\\]\\nDuring training, we will apply our modification generator to modify the geometry of \\\\( F \\\\) such that \\\\( F \\\\) and \\\\( F' \\\\) render the face with the same geometry.\\n\\nTexture Distillation.\\nWe distill the capabilities of fine-grained texture editing from 2D face editing algorithms by generating texture-modified avatar \\\\( F' \\\\) with the teacher 3DMM GAN \\\\([50]\\\\). Specifically, we sample an original avatar \\\\( F = G_n(z_n, \\\\beta, \\\\psi, \\\\theta) \\\\) from the teacher generator and render the image of its positive face. A segmentation-based 2D face texture editing algorithm (SBA) \\\\([77]\\\\) and two makeup transfer algorithms (MTA) \\\\([22, 27, 36]\\\\) are referred to in the distillation. We randomly choose one of them to edit the texture of the rendered face image. For SBA, we define several editable semantic regions of the face. A subset of these regions is selected randomly for texture painting using hues randomly sampled from the HSV color spectrum. For MTA, we randomly choose a makeup image as a reference from the open-sourced makeup dataset \\\\([22, 27, 36]\\\\) and transfer the reference makeup to the rendered face image. The makeup dataset \\\\([36]\\\\) contains complex makeups, such as blushes and makeup jewelry, which allow our generator to learn complicated texture editing patterns. Then, we perform the PTI inversion \\\\([43]\\\\) on the texture-modified face image to lift the 2D texture editing to 3D space and obtain a texture-modified avatar \\\\( F' \\\\).\\n\\nModification Learning.\\nFollowing the training style of StyleGAN \\\\([24]\\\\), we sample a modification latent code \\\\( z_{g/t} \\\\in \\\\mathbb{R}^{1024} \\\\) in \\\\( Z \\\\) latent space for each paired editing data. We do not fully sample a 1024-dimensional modification code but sample a reduced code \\\\( \\\\bar{z} \\\\) from a standard normal distribution and concatenate it with the latent code \\\\( z_n \\\\) of the original avatar \\\\( F \\\\) that is sampled from the teacher model, i.e., \\\\( z_{g/t} = (z_n, \\\\bar{z}) \\\\), \\\\( z_n, \\\\bar{z} \\\\in \\\\mathbb{R}^{512} \\\\). This design is regarded as implicit code guidance that decently integrates knowledge from the teacher model to facilitate the model convergence.\\n\\n\\\\( z_n \\\\) encodes the facial appearances of avatar \\\\( F \\\\), serving as a reference to the superimposition of the modification onto the avatar \\\\( F \\\\). Note that during inference, we do not require the concatenation of latent code from the teacher model and directly optimize the full modification code from the edited image using an auto-decoding manner. Our generator generates the modification field following Eq. (2) where \\\\( v \\\\) is decoded from the 3DMM parameters of the avatar \\\\( F \\\\) using FLAME model \\\\([26]\\\\), i.e.,\\n\\\\[ v = E(\\\\beta, \\\\psi, \\\\theta). \\\\]\\nTo apply the modification field, we feed the deformed query points \\\\( x' \\\\) to the original avatar \\\\( F \\\\) to obtain the density and color, and composite the color with modification color by:\\n\\\\[ c = (1 - \\\\beta \\\\Delta) \\\\ast c_o + \\\\beta \\\\Delta \\\\ast c, \\\\quad (\\\\sigma, c_o) = F(x', d). \\\\]\\nThen, we perform volume rendering on the density \\\\( \\\\sigma \\\\) and color \\\\( c \\\\) using Eq. (1) to render the modified image \\\\( \\\\hat{I}_e \\\\) of avatar \\\\( F \\\\). We use the photometric loss to supervise the modified image with the rendered image \\\\( \\\\hat{I}'_e \\\\) from the edited avatar \\\\( F' \\\\) under the same camera parameters.\\n\\\\[ L = ||\\\\hat{I}_e - \\\\hat{I}'_e||^2_2. \\\\]\\n\\nDuring training, we sample multiple viewpoints and 3DMM expression parameters \\\\( \\\\psi \\\\) for each editing pair \\\\((F, F')\\\\) to enhance the spatial consistency under different expressions.\"}"}
