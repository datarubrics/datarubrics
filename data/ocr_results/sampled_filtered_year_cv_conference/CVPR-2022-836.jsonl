{"id": "CVPR-2022-836", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparisons (in terms of ADE/FDE) on ETH/UCY. Our teacher network (STT) trained according to the standard protocol shows comparable results w.r.t. the competitors, while our student network (STT + DTO) shows similar performance despite its knowledge gap.\\n\\n5.2. Comparison with the State-of-the-art\\n\\nSince our proposal is deterministic (i.e., it gives a single future sample), we leave aside stochastic methods [15, 22, 34] and compare our model to the following state-of-the-art deterministic solutions:\\n\\n\u2022 Constant Velocity Model (CVM) [36]: a simple but effective baseline that estimates future positions by considering solely the latest two timesteps;\\n\u2022 ST-GAT [20]: graph-based attention network using LSTMs to model temporal correlations. We consider the 1V-1 version, i.e., without variety loss and with one output sample per input;\\n\u2022 Ind-TF [14]: vanilla transformer without explicit interactions modelling;\\n\u2022 SR-LSTM [52]: LSTM-based network integrating a system of motion gates that refines cells\u2019 hidden states using neighbourhood information;\\n\u2022 STAR [48]: encoder-decoder architecture based on a transformer network to model temporal information and spatial interactions. We consider its deterministic version obtained removing the Gaussian noise.\\n\\nTab. 1 and Tab. 2 report our results: when trained according to the common protocol (8 observations \u2013 12 predictions), our teacher network (STT \u2013 8 obs) performs comparably to state-of-the-art approaches. Notably, the student network (STT + DTO \u2013 2 obs, last row of Tab. 1 and Tab. 2) shows remarkable results: it approaches the teacher on all the datasets, suggesting that the last two observations are an informative summary of the input trajectory. Notably, trivial strategies using only two observations do not achieve the accuracy of our approach: both the CVM and training from scratch with short sequences (STT \u2013 2 obs) deliver higher errors. Instead, our training procedure successfully bridges the huge informative gap simulated at inference time.\\n\\nIt is worth noting that two observations as input do not necessarily generate straight lines as output: in this case, our approach would have achieved results in line with SDD Lyft.\\n\\nTable 2. ADE/FDE results on SDD and Lyft.\\n\\n| Sampling strategy | ADE | FDE |\\n|-------------------|-----|-----|\\n| VRNN-1 one sample | 0.73 | 1.49 |\\n| VRNN-20 argmin KL(q\u2225p) | 0.75 | 1.51 |\\n| VRNN-20 argmin MSE(\u00b7,GT) | 0.58 | 1.17 |\\n| STT (ours) one sample | 0.63 | 1.26 |\\n\\nTable 3. Comparison between our approach and the V-RNN [11].\"}"}
{"id": "CVPR-2022-836", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. On the SDD's scenarios, a comparison (ADE / FDE) between teacher (STT) and student (STT + DTO) on both ground-truth and tracked trajectories.\\n\\nConditional prior). In this regard, Tab. 3 proposes a comparison between our proposal and a V-RNN model with different sampling strategies: remarkably, only the (unviable) criterion based on ground-truth trajectories yields higher accuracy w.r.t. DTO.\\n\\n5.3. Towards an \\\"in-the-wild\\\" evaluation: a tracker in the middle\\n\\nAs outlined in Sec. 1, on-line scenarios cannot rely on human intervention to correct detection and re-identification errors at inference time. Based on this motivation, we advocate for employing shorter temporal horizons while estimating future trajectories (2 time steps in place of the common 8 ones), since a tracker can still provide reliable predictions for so short fragments. To shed light on this point, we conduct an experiment on the Stanford Drone Dataset: more precisely, we focus on input trajectories and replace ground-truth associations with Deep SORT's output, which is a tracking-by-detection algorithm that leverages a deep metric for modelling appearance. For each scene, we extract all the detections contained in the observation history; then, we run this tracker on the obtained detections and select as our new observation sequence the most similar tracklet to the ground-truth one. For the sake of simplicity, we restrict our analysis to examples that are successfully followed for at least 8 time steps (hence, we discard cases suffering from identity switches).\\n\\nIn this setting, we evaluate the performance of teacher (namely, STT fed with 8-length tracklets) and student networks (namely, STT trained via DTO fed with 2-length tracklets). As reported in Tab. 4, while DTO appears not advantageous in ideal scenarios (i.e. using ground-truth observations), switching to a fully-automatic inference (i.e. using tracked trajectories) turns the table: in almost all SDD scenes, our model trained via DTO experiences a diminished degradation in performance w.r.t. the teacher. Its worsening is mainly due to errors accumulation occurring on long sequences: as depicted in Fig. 3a, tracker's errors\\n\\nFigure 3. a) histograms of displacements errors between ground-truth trajectories and their estimations provided by Deep SORT; b) spread of the attention coefficients assigned by the decoder to each encoder state.\\n\\n(measured as ADE between ground-truth and re-tracked input trajectories) spread on higher values for longer tracklets. By contrast, when restraining the model to just a few observations, the average association error tends to be lower, thus affecting less the downstream forecasting model. Additionally, Tab. 4 draws a comparison between DTO and two baselines that use only 2 observations: STT trained from scratch with 2 observations and the Constant Velocity Model (CVM). As reported, the effectiveness of DTO in a fully-automatic context is not merely due to the use of few time steps, but, more interestingly, also derives from the exploitation of our knowledge distillation paradigm.\\n\\n5.4. Why Distilling the Observations works\\n\\nResults reported above suggest that information about future positions can be often recovered by looking solely at the most recent observations. This finding is also investigated by Sch\u00f6ller et al. [36], who report that forecasting methods retain only partial input data. Furthermore, Becker et al. [5] show that the contribution of the latest time step is 80.3% while for the second-latest is only 8.3%.\"}"}
{"id": "CVPR-2022-836", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To verify if this behaviour also affects our spatio-temporal transformer, Fig. 3b reports an analysis of the values assumed by the coefficients in the encoder-decoder self-attention, i.e., the coefficients that represent the contribution of each encoder state to the decoding of future positions. Similarly to [36], we observe that, while earlier steps exert an (albeit small) influence, subsequent states provide a higher contribution. In this regard, we conjecture that the robustness of DTO resides in how the model handles the earlier information: at training time, initial time steps are not drastically discarded (as would happen when training from scratch on fewer steps) but, instead, the student learns to dispense with their limited informative content.\\n\\n5.5. On the \u201clength-shift\u201d problem\\n\\nWe also argue that exploiting longer sequences overly binds the model to the amount of data considered at training time. To prove our intuition, we investigate how models behave when the number of input time steps changes at evaluation time: as shown in Fig. 4, reducing the number of past observations results in a sudden and huge performance drop, even for small variations as removing a single time step. This behaviour \u2013 which we refer as \u201clength-shift problem\u201d \u2013 is a common trait among different splits (8\u201312, 7\u201312, etc.) and architectures. This issue could reduce the applicability of these models when limited or partial annotations are available. For this reason, in the following, we explore several strategies that attempt to mitigate this issue: among all, DTO appears the most promising paradigm.\\n\\nAddressing the length-shift problem.\\n\\nThe na\u00efve approach for dealing with this problem is to directly train a forecasting model using fewer time steps (i.e., the same number of observations expected at inference time). However, as reported in Sec. 5.2 and Tab. 5, this choice does not allow the model to extract valuable motion patterns. To this end, we explore a second strategy, training a single instance of our STT with a variable number of observations (from 2 to 8 time steps). As reported in Tab. 5 (Variable observations), this strategy brings no benefits: we conjecture that the model learns an average set of motion features, thus granting predictions that are less sensitive to changes in the number of time steps; however, it is far from extracting the set of features that is optimal for each specific input length. A third approach (Past generation) reckons on an auxiliary network to fill the input sequence with a set of generated observations: namely, when the number of observations is less than the one used at training time, we employ a secondary model that predicts the missing part of the input trajectory, which is then concatenated to available positions and fed to the primary forecasting model. This represents a step forward, but still delivers unsatisfactory results: we conjecture that the main limitation of this approach regards the amount of noise injected by the auxiliary module, which then spreads to the model that generates future locations. Finally, we found particularly beneficial the supervision inherent with our distillation strategy. While the student can collect novel motion patterns from few observations, the teacher guides the student to gradually improve its performance. This way, the student learns to handle the earlier information, which is crucial for predicting future positions accurately.\"}"}
{"id": "CVPR-2022-836", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. A qualitative comparison between predicted trajectories generated by our teacher and its obs distilled student (on Stanford Drone Dataset). In (a) and (b), our student generates more realistic samples, while in (c) and (d), due to more complex dynamics, few observations are not enough to forecast positions close to the ground-truth trajectories.\\n\\n5.6. On the \u201cdomain shift\u201d problem \u2013 Knowledge Transfer\\n\\nRegarding the generalization capabilities of the student, we discuss here its higher degree of robustness to domain-shifts (i.e., a change in the underlying data distribution between training and test sets). We expect that exploiting only a few observations limits an excessive specialization on the dataset-specific statistics, thus granting a superior strength to distribution shifts. We validate our claim by investigating the following experimental setting: we use SDD as training set and then test both teacher and student networks on novel scenarios, embodied by the test sets of ETH and Lyft.\\n\\nAs reported in Tab. 6, the presence of DTO favours knowledge transfer and outperforms its distillation-less counterpart. Moreover, in some cases, this strategy even outperforms the results provided by the upper bound, i.e., when there is a match between source and target datasets (e.g., ETH \u2192 ETH). On the one hand, we conjecture that this is due to the fact that Stanford Drone collects more representative instances of motion dynamics: indeed, the complexity of the learned motion patterns eases the network effort when evaluated on more straightforward scenarios, such as ETH. On the other hand, our framework proves to be beneficial against shifts, thus providing a solution that addresses scenarios with limited available labels.\\n\\n5.7. Qualitative analysis\\n\\nIn some cases (Fig. 5a and 5b), the gap occurring between our networks does not impact the corresponding predictions. In presence of complex dynamics, e.g., pronounced turns (see Fig. 5c and 5d), the student incurs some issues: here, observing only few positions does not provide enough information to grasp such sophisticated dynamics.\\n\\n| Train. Set | DTO | # of training/evaluation obs | ETH | \u2713 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 | \u2713 | \u2717 |\\n|-----------|-----|-----------------------------|-----|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| ETH       |     |                             |     |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n| \u2713         |     |                             | 0.72 | 0.68 | 0.66 | 0.60 | 0.58 | 0.57 | 0.54 |   |   |   |   |   |   |   |   |   |   |   |   |\\n|\u2717         | SDD | 0.71 | 0.59 | 0.57 | 0.58 | 0.57 | 0.57 | 0.55 |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n| \u2713         | SDD | 0.66 | 0.57 | 0.58 | 0.54 | 0.56 | 0.54 | 0.55 |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|\u2717         |      |                             | 0.31 | 0.30 | 0.28 | 0.27 | 0.26 | 0.26 | 0.24 |   |   |   |   |   |   |   |   |   |   |   |   |\\n|\u2717         | SDD | 0.70 | 0.53 | 0.41 | 0.41 | 0.44 | 0.46 | 0.41 |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|\u2713         | SDD | 0.35 | 0.30 | 0.30 | 0.42 | 0.36 | 0.28 | 0.34 |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|       |      |                             | 0.31 | 0.30 | 0.28 | 0.27 | 0.26 | 0.26 | 0.24 |   |   |   |   |   |   |   |   |   |   |   |   |\\n|       | SDD | 0.70 | 0.53 | 0.41 | 0.41 | 0.44 | 0.46 | 0.41 |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|       | SDD | 0.35 | 0.30 | 0.30 | 0.42 | 0.36 | 0.28 | 0.34 |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n\\n(a) Performance on the test set of ETH.\\n(b) Performance on the test set of Lyft.\\n\\nTable 6. Results of transfer knowledge among different datasets (ADE). We highlight in bold the highest results obtained in case of dataset-shift (i.e., second and third rows of each of the two tables).\\n\\n6. Conclusion\\n\\nThis paper proposes an in-depth analysis of the evaluation protocol usually employed to assess trajectory prediction models. We conceive a novel training strategy to train a transformer-based architecture to deal with scenarios where only a few observations are available. Our teacher-student paradigm reduces the information gap experienced by the student, thus providing a practical and viable inference scheme for on-line scenarios. We also investigate issues that could emerge at inference time. Our experiments suggest that our strategy also enables a better knowledge transfer capability across different training scenarios.\\n\\nAcknowledgments.\\n\\nFunded by the PREVUE \u201cPRediction of activities and Events by Vision in an Urban Environment\u201d project (CUP E94I19000650001), PRIN National Research Program, Italian Ministry for Education, University and Research (MIUR).\"}"}
{"id": "CVPR-2022-836", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How many Observations are Enough?\\n\\nKnowledge Distillation for Trajectory Forecasting\\n\\nAlessio Monti\\nAngelo Porrello\\nSimone Calderara\\nPasquale Coscia\\nLamberto Ballan\\nRita Cucchiara\\n\\n1 University of Modena and Reggio Emilia, Italy\\n2 University of Padova, Italy\\n\\nAbstract\\n\\nAccurate prediction of future human positions is an essential task for modern video-surveillance systems. Current state-of-the-art models usually rely on a \u201chistory\u201d of past tracked locations (e.g., 3 to 5 seconds) to predict a plausible sequence of future locations (e.g., up to the next 5 seconds). We feel that this common schema neglects critical traits of realistic applications: as the collection of input trajectories involves machine perception (i.e., detection and tracking), incorrect detection and fragmentation errors may accumulate in crowded scenes, leading to tracking drifts. On this account, the model would be fed with corrupted and noisy input data, thus fatally affecting its prediction performance.\\n\\nIn this regard, we focus on delivering accurate predictions when only few input observations are used, thus potentially lowering the risks associated with automatic perception. To this end, we conceive a novel distillation strategy that allows a knowledge transfer from a teacher network to a student one, the latter fed with fewer observations (just two ones). We show that a properly defined teacher supervision allows a student network to perform comparably to state-of-the-art approaches that demand more observations. Besides, extensive experiments on common trajectory forecasting datasets highlight that our student network better generalizes to unseen scenarios.\\n\\n1. Introduction\\n\\nPedestrian trajectory forecasting deals with predicting future paths through the exploitation of individual trajectory information and mutual influence between pedestrians. This task has several practical applications in advanced surveillance systems [24], behavioral analysis [32], intrusion detection [39], smart vehicles and autonomous systems [4,37].\\n\\nWhile several recent works focused on novel deep-network architectures tailored for this task [14, 15, 20, 35, 49, 52], we believe the inference phase of a trajectory predictor has not been thoroughly addressed and investigated yet. Typically, data-driven models are trained and evaluated on large public datasets of tracked trajectories refined with a human intervention to correct missed detections and identity switches. However, this process is unfeasible at inference time: therefore, the input trajectories required to condition the prediction have to be automatically extracted by a tracking system.\\n\\nIn this regard, the widely adopted 8-12 protocol [1,15,20,22,34,48,52] (i.e., 8 input time steps and 12 ones for prediction), which require data collected at 2.5 FPS, does not provide a large margin of correction for the above situations. In real-time applications, a visual tracking system may provide inaccurate observations for sequences of such length [12]: occlusions, false detections and non-rigid shape deformations pose non-trivial issues.\\n\\nTo overcome the aforementioned limitations, one potential solution is to reduce the length of the input trajectories to the extent we can minimize the tracking associations errors. Based on this intuition, this work proposes an approach based on Knowledge Distillation [18], which recovers a reliable proxy of the same information obtained with more input observations. We show that it allows for an effective inference schema, requiring fewer samples than the training one. We also demonstrate that properly conditioning a model on shorter input trajectories provides more room to generalize across different experimental settings.\\n\\nFrom a technical perspective, this work implements our idea by deploying a teacher-student paradigm [18]: a student network is trained to mimic a teacher\u2019s behaviour using less input observations. Each network devises a transformer-based architecture that accounts for both spatial and temporal interactions through an attention mechanism. To deal with a limited number of observations, we propose a distillation procedure that acts on both encoder and decoder stacks of the transformer architecture. Finally, our objective function takes into account ground-truth data and distillation losses to conveniently match teacher and student internal representations.\"}"}
{"id": "CVPR-2022-836", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We remark the following contributions:\\n\\ni) to the best of our knowledge, this is the first attempt to in-depth analyze the effectiveness (at inference time) of the evaluation protocol commonly employed by current trajectory prediction models;\\n\\nii) we introduce a novel distillation strategy to reduce the length of the input trajectories while keeping the prediction accurate;\\n\\niii) we explore the student's capabilities in adapting and transferring its knowledge to scenarios exhibiting different levels of complexity in terms of human dynamics and scene interactions. Indeed the experiments highlights that it is possible to construct a solid trajectory forecasting system that at inference time is fed only with just 2 observations (i.e. last two) per-pedestrian. This is possible only through the thoughtful exploitation of the global knowledge that can be inferred from training data and distilled into the inference model.\\n\\n2. Related Work\\n\\nSocial models. Modelling human-human interactions plays a fundamental part in predicting plausible trajectories. Pioneering works take advantage of hand-crafted relations, energy-based features or rule-based models [3, 9, 17, 27, 41, 45], which fail to adapt to scene changes and model complex crowd dynamics. In recent years, data-driven approaches received increasing attention: in their seminal work, Alahi et al. [1] capture these interactions by aggregating the hidden states of neighbouring agents with a dedicated grid-based \u201cSocial Pooling\u201d. Gupta et al. [15] improved this mechanisms which is extended to all the agents involved in the scene by max-pooling their hidden states. Such modules have also been extended with attention-based mechanisms [2], while other works propose architectures combining social pooling with context information (e.g. scene semantic, groups or head poses) [4, 8, 16, 26, 34].\\n\\nRecent improvements in graph machine learning [21, 25, 28, 43] motivated the adoption of such flexible structures to model agents relationships. Several solutions [6, 20, 22, 38, 40, 44] consider agents as graph nodes whose features are represented by their hidden states. This solution enables the use of message-passing mechanisms and grants the possibility to aggregate information at each node with powerful Graph Neural Networks (GNNs) like Graph Attention Networks [43]. Zhang et al. [52] similarly treat the pedestrian space as a fully-connected graph but design a custom message-passing solution that integrates a motion gate to perform a feature selection based on pedestrian movements. Finally, Yu et al. [48] only rely on attention mechanisms to predict future locations exploiting recent advances in transformer-based architectures [42].\\n\\nKnowledge distillation. Knowledge distillation has been firstly investigated as an approach for model compression [10, 18]: a small model (student) has to mimic the behaviour of an over-parameterized one (teacher). As a result, the student manifest a smaller memory footprint without experiencing a large drop in the overall performance. [31] aims to reduce both student and teacher feature maps; [18] suggests to match the soft-targets before the final classification layer; [50] matches the features of attention regions. In this work, we employ knowledge distillation in a different fashion. Inspired by [13, 51], our aim is not to compress a model yet to improve its performance. This procedure is usually referred to as self-distillation, since the student network shares the same architecture of its teacher. Similarly to [7, 29], our approach sets up asymmetric networks: the student is encouraged to overcome its knowledge gap by following the guide of its teacher, eventually boosting its performance. This is done in the specific context of trajectory forecasting, and we demonstrate that knowledge distillation can lead to effective predictions even when the model has access to very few observations.\\n\\n3. Model\\n\\nTrajectory forecasting is usually defined as a time-series prediction problem [32]. The task is particularly challenging because:\\n\\ni) human motion is inherently multi-modal, and\\n\\nii) agents simultaneously interact with each other and with static scene elements.\\n\\nTo meet these two points, we design a novel approach modelling both temporal and spatial relationships occurring between agents. Specifically, this section describes how we extend the original Transformer [42] architecture to deal with trajectory forecasting.\\n\\n3.1. Vanilla Transformer for Trajectory Prediction\\n\\nTo deal with sequence-to-sequence tasks, transformers follow the well established encoder-decoder paradigm. Instead of relying on internal recurrent layers, input sequences are processed as a whole through a purely attentive mechanism. Self-attention aims to discover relationships between every pair of elements in the sequence: this reduces the risk of forgetting past information and allows the network to learn long-range dependencies [42].\\n\\nFrom a technical perspective, each embedding $e_t$ (from time step $t = 1$ up to $t = T$) is linearly projected into a triplet of vectors: a query $q_t$, a key $k_t$ and a value $v_t$. Then, transformers exploit the dot product between queries and keys to compute attention coefficients (scaled dot-product attention), the latter being used to weight the corresponding values and provide the final output. This operation is performed $h$ times (heads) on different linear projections of $Q$, $K$ and $V$ to attend information from several representations at different positions.\"}"}
{"id": "CVPR-2022-836", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Spatio-Temporal Transformer (STT)\\n\\nAs in [42], our proposal firstly attends on the temporal axis with a scaled dot-product attention module. This way, it is able to recover temporal dependencies across different time steps and capture characteristic motion patterns of the monitored agents. However, this vanilla sequence-to-sequence model does not explicitly account for high-level spatio-temporal structure, i.e., no interactions are considered. For this reason, the output of our temporal attention is fed to a second self-attention module that acts on the spatial axis. While in the temporal attention queries, keys and values refer to different time steps of a specific agent, here $Q$, $K$ and $V$ refer to the embeddings of all the agents at a fixed time step. This way, each agent can additionally attend on the information of its neighbours, recovering useful spatial information. Fig. 1 shows a visual representation of our encoder architecture (the same applies to the decoder).\\n\\nRelation with previous works.\\n\\nWhile the approach discussed in [14] consists of a transformer network treating each pedestrian separately (thus handling only temporal information), our self-attention mechanism takes into consideration also spatial interactions between pedestrians. This is somewhat similar to what has been devised by the authors of [48], who equipped a spatio-temporal transformer with an auxiliary memory retaining representations of previous predictions. However, our approach significantly differs in the design of the decoder: while [48] adopts a fully connected layer, we stay close to the original transformer [42] and mirror the encoder into the decoder.\\n\\n4. Distilling the Observations (DTO)\\n\\nOur goal is to set up a model capable of accurately predicting future positions when only a few observations are available: this way, we can address the inference-time shortcomings outlined in Sec. 1. More specifically, we devise a two-fold approach (depicted in Fig. 2):\\n\\n- Firstly (Sec. 4.1), we train a teacher network to estimate trajectories given 8-length observation sequences;\\n- Secondly (Sec. 4.2), we freeze its parameters and attempt to transfer its predictive capability to a student network. Importantly, the latter is forced to operate with an information gap, i.e., using only a small fraction of available inputs (e.g., two last observations).\\n\\n4.1. Teacher training\\n\\nTo train our teacher network, we follow the standard protocol and consider 8 observation time steps and 12 prediction time steps. The network is trained by teacher forcing, i.e., when predicting the next time step, the decoder is conditioned on past ground-truth samples rather than its own predictions. We mark the beginning of the prediction sequence with a start token and mask the information related to the future time steps. Mean Squared Error (MSE) between predictions and ground-truth positions is used as loss function while training our teacher network:\\n\\n$$L_{GT} = \\\\frac{1}{P} \\\\sum_{p=0}^{P-1} (x_p[:]-\\\\hat{x}_p[:])^2,$$\\n\\nwhere $P$ is the number of pedestrians and $x_p[:](\\\\hat{x}_p[:])$ represents the sequence of ground-truth (predicted) positions of a pedestrian $p$ at time $t$.\\n\\nBy contrast, the inference procedure resembles an auto-regressive model. The decoder forecasts the first future position using the last hidden state of the encoder stack and an input sequence initially composed only by the start token. At each step, the predicted position $\\\\hat{x}_t$ is concatenated to the current input sequence: this partial sequence is fed again to the decoder to predict the next position $\\\\hat{x}_{t+1}$.\\n\\n4.2. Student training\\n\\nTo preserve teacher's predictive capabilities given only few observations, our training strategy relies on transferring the knowledge lying in the entire input sequence: to achieve this, we act on both encoder and decoder stacks.\\n\\nEncoder distillation. Firstly, we force the student encoder to mimic the behaviour of its teacher's counterpart. Given the information gap between the two networks, the higher the transfer occurring at this level, the higher the capability of the encoder to infer the missing information from the (few) spatio-temporal interactions it observes. Technically,\"}"}
{"id": "CVPR-2022-836", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. A comprehensive picture of our framework, termed Distilling the Observations (DTO), which provides a training strategy for obtaining accurate trajectory predictions when only few observations are available.\\n\\nDecoder distillation. At the same time, we focus on matching the function space spanned by both teacher and student decoders. We pursue our goal relying on two terms: on one hand, we match the activations prior to the fully connected layer that give the final prediction, i.e., $\\\\hat{x}_{p,t} = FC(o_{T,p})$. On the other hand, as proposed in [46], we exploit the self-attention coefficients $A_{T,p,[:]}$ of the last decoder layer as an additional learning guidance. The corresponding objective function is defined as follows:\\n\\n$$L_{DD} = \\\\frac{1}{P} \\\\sum_{P=0}^{P-1} o_{T,p,[:]} - o_{S,p,[:]}^2 + A_{T,p,[:]} - A_{S,p,[:]}^2.$$  \\n\\nOverall objective. Finally, the student objective consists of a weighted sum of the prediction loss, which takes into account ground-truth positions, and the distillation losses:\\n\\n$$L = \\\\alpha L_{GT} + \\\\beta L_{ED} + \\\\gamma L_{DD},$$\\n\\nwhere $\\\\alpha$, $\\\\beta$ and $\\\\gamma$ are three hyperparameters balancing the contribution of each term.\\n\\n5. Experiments\\n\\nMetrics. We consider two standard error metrics in our comparisons: the Average Displacement Error (ADE) and the Final Displacement Error (FDE) [27]. While the ADE indicates the average Euclidean distance between all the predicted time steps and the ground-truth ones, the FDE expresses instead only the error regarding the final position.\\n\\n5.1. Datasets\\n\\nETH/UCY. As usually done [1, 15], we stitch together two scenes from ETH [27] (ETH and Hotel) and three scenes from UCY [23] (Univ, Zara-1, Zara-2). The resulting dataset contains more than 1500 pedestrians, taking linear and non-linear paths in outdoor scenarios. We follow the common leave-one-scene-out protocol, training on 4 scenes and testing on the remaining one.\\n\\nStanford Drone Dataset (SDD). [30] is a large scale dataset collected by a drone monitoring crowded university campus scenarios. It contains multiple interacting agents (e.g., pedestrians, cyclists, cars) and is composed of a large diversity of urban scenes (e.g., intersections and parks) where people exhibit complex dynamics. We split the SDD World Plane Human-Human dataset [33] into train (70%), val (10%) and test (20%) sets, respectively.\\n\\nLyft Prediction Dataset. [19] is one of the largest collection of traffic agent motion data. It includes tracks of cars, pedestrians and other traffic agents recorded by cameras and lidar sensors of Lyft autonomous fleet. We split the reduced version of this dataset (1000 agents) in train (70%), val (10%) and test (20%) sets.\"}"}
{"id": "CVPR-2022-836", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. Social LSTM: Human trajectory prediction in crowded spaces. In Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\n[2] Javad Amirian, Jean-Bernard Hayet, and Julien Pettr\u00e9. Social Ways: Learning multi-modal distributions of pedestrian trajectories with GANs. In Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019.\\n\\n[3] Gianluca Antonini, Michel Bierlaire, and Mats Weber. Discrete choice models of pedestrian walking behavior. Transportation Research Part B: Methodological, 2006.\\n\\n[4] Federico Bartoli, Giuseppe Lisanti, Lamberto Ballan, and Alberto Del Bimbo. Context-aware trajectory prediction. In Proc. of the IAPR International Conference on Pattern Recognition (ICPR), 2018.\\n\\n[5] Stefan Becker, Ronny Hug, Wolfgang H\u00fcbner, and Michael Arens. RED: A simple but effective baseline predictor for the trajnet benchmark. In Proc. of the European Conference on Computer Vision Workshops, 2018.\\n\\n[6] Alessia Bertugli, Simone Calderara, Pasquale Coscia, Lamberto Ballan, and Rita Cucchiara. AC-VRNN: Attentive Conditional-vrnn for multi-future trajectory prediction. Computer Vision and Image Understanding, 2021.\\n\\n[7] Shweta Bhardwaj, Mukundhan Srinivasan, and Mitesh M Khapra. Efficient video classification using fewer frames. In Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\n[8] Niccol\u00f2 Bisagno, Bo Zhang, and Nicola Conci. Group LSTM: Group trajectory prediction in crowded scenarios. In Proc. of the European Conference on Computer Vision Workshops, 2018.\\n\\n[9] Federico Bolelli, Stefano Allegretti, and Costantino Grana. One DAG to rule them all. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\n\\n[10] Cristian Bucil\u0103, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proc. of the ACM International Conference on Knowledge Discovery and Data Mining (KDD), 2006.\\n\\n[11] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2015.\\n\\n[12] P. Dendorfer, H. Rezatofighi, A. Milan, J. Shi, D. Cremers, I. Reid, S. Roth, K. Schindler, and L. Leal-Taixe. Mot20: A benchmark for multi-object tracking in crowded scenes. arXiv preprint, 2020.\\n\\n[13] Tommaso Furlanello, Zachary C Lipton, Michael Tschan nen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In Proc. of the International Conference on Machine Learning (ICML), 2018.\\n\\n[14] Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio Galasso. Transformer networks for trajectory forecasting. In Proc. of the IAPR International Conference on Pattern Recognition (ICPR), 2020.\\n\\n[15] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social GAN: Socially acceptable trajectories with generative adversarial networks. In Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\n[16] Irtiza Hasan, Francesco Setti, Theodore Tsesmelis, Alessio Del Bue, Fabio Galasso, and Marco Cristani. MX-LSTM: mixing tracklets and vislets to jointly forecast trajectories and head poses. In Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\n[17] Dirk Helbing and Peter Moln\u00e1r. Social force model for pedestrian dynamics. Physical review E, 1995.\\n\\n[18] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In Proc. of the NeurIPS Deep Learning and Representation Learning Workshop, 2015.\\n\\n[19] J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, A. Jain, S. Omari, V. Iglovikov, and P. Ondruska. One thousand and one hours: Self-driving motion prediction dataset. https://level5.lyft.com/dataset/, 2020.\\n\\n[20] Yingfan Huang, Huikun Bi, Zhaoxin Li, Tianlu Mao, and Zhaoqi Wang. ST-GAT: Modeling spatial-temporal interactions for human trajectory prediction. In Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.\\n\\n[21] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In Proc. of the International Conference on Learning Representations (ICLR), 2017.\\n\\n[22] Vineet Kosaraju, Amir Sadeghian, Roberto Mart\u00edn-Mart\u00edn, Ian Reid, Hamid Rezatofighi, and Silvio Savarese. Social-BiGAT: Multimodal trajectory forecasting using bicycle-GAN and graph attention networks. In Proc. of Advances in Neural Information Processing Systems (NeurIPS), 2019.\\n\\n[23] A. Lerner, Y. Chrysanthou, and Dani Lischinski. Crowds by example. Computer Graphics Forum, 26, 2007.\\n\\n[24] Yuanman Li, Rongqin Liang, Wei Wei, Wei Wang, Jiantao Zhou, and Xia Li. Temporal pyramid network with spatial-temporal attention for pedestrian trajectory prediction. IEEE Transactions on Network Science and Engineering, 2021.\\n\\n[25] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks. In Proc. of the International Conference on Learning Representations (ICLR), 2016.\\n\\n[26] Matteo Lisotto, Pasquale Coscia, and Lamberto Ballan. Social and scene-aware trajectory prediction in crowded spaces. In Proc. of the IEEE/CVF International Conference on Computer Vision Workshops, 2019.\\n\\n[27] Stefano Pellegrini, Andreas Ess, Konrad Schindler, and Luc Van Gool. You'll never walk alone: Modeling social behavior for multi-target tracking. In Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV), 2009.\\n\\n[28] Angelo Porrello, Davide Abati, Simone Calderara, and Rita Cucchiara. Classifying signals on irregular domains via conv65616561.\"}"}
{"id": "CVPR-2022-836", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[26] Rice, E. L., & Rice, S. V. (1995). *Mathematical statistics and data analysis*. Duxbury Press.\\n\\n[27] Rice, S. V., & Rice, E. L. (2007). *Mathematical statistics and data analysis*. Duxbury Press.\\n\\n[28] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[29] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[30] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[31] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[32] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[33] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[34] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[35] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[36] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[37] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[38] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[39] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[40] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[41] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[42] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[43] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[44] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[45] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[46] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[47] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[48] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[49] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[50] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[51] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\\n\\n[52] Tippett, L. H. C. (1937). The control of biological experiments. *The Eugenics Review*, 29(1), 76-82.\"}"}
