{"id": "CVPR-2023-601", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Recent methods for synthesizing novel views from monocular videos of dynamic scenes\u2014like HyperNeRF \\\\[50\\\\] and NSFF \\\\[35\\\\]\u2014struggle to render high-quality views from long videos featuring complex camera and scene motion. We present a new approach that addresses these limitations, illustrated above via an application to DoF video stabilization, where we apply our approach and prior methods on a 30-second, shaky video clip, and compare novel views rendered along a smoothed camera path (left). On a dynamic scenes dataset (right) \\\\[75\\\\], our approach significantly improves rendering fidelity, as indicated by synthesized images and LPIPS errors computed on pixels corresponding to moving objects (yellow numbers). Please see the supplementary video for full results.\\n\\nAbstract\\n\\nWe address the problem of synthesizing novel views from a monocular video depicting a complex dynamic scene. State-of-the-art methods based on temporally varying Neural Radiance Fields (aka dynamic NeRFs) have shown impressive results on this task. However, for long videos with complex object motions and uncontrolled camera trajectories, these methods can produce blurry or inaccurate renderings, hampering their use in real-world applications. Instead of encoding the entire dynamic scene within the weights of MLPs, we present a new approach that addresses these limitations by adopting a volumetric image-based rendering framework that synthesizes new viewpoints by aggregating features from nearby views in a scene motion\u2013aware manner. Our system retains the advantages of prior methods in its ability to model complex scenes and view-dependent effects, but also enables synthesizing photo-realistic novel views from long videos featuring complex scene dynamics with unconstrained camera trajectories. We demonstrate significant improvements over state-of-the-art methods on dynamic scene datasets, and also apply our approach to in-the-wild videos with challenging camera and object motion, where prior methods fail to produce high-quality renderings.\\n\\n1. Introduction\\n\\nComputer vision methods can now produce free-viewpoint renderings of static 3D scenes with spectacular quality. What about moving scenes, like those featuring people or pets? Novel view synthesis from a monocular video of a dynamic scene is a much more challenging dynamic scene reconstruction problem. Recent work has made progress towards synthesizing novel views in both space and time, thanks to new time-varying neural volumetric representations like HyperNeRF \\\\[50\\\\] and Neural Scene Flow Fields (NSFF) \\\\[35\\\\], which encode spatiotemporally varying scene content volumetrically within a coordinate-based multi-layer perceptron (MLP).\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\\n\\n4273\"}"}
{"id": "CVPR-2023-601", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that prevent their application to casual, in-the-wild videos. Local scene flow\u2013based methods like NSFF struggle to scale to longer input videos captured with unconstrained camera motions: the NSFF paper only claims good performance for 1-second, forward-facing videos \\\\cite{35}. Methods like HyperNeRF that construct a canonical model are mostly constrained to object-centric scenes with controlled camera paths, and can fail on scenes with complex object motion.\\n\\nIn this work, we present a new approach that is scalable to dynamic videos captured with 1) long time duration, 2) unbounded scenes, 3) uncontrolled camera trajectories, and 4) fast and complex object motion. Our approach retains the advantages of volumetric scene representations that can model intricate scene geometry with view-dependent effects, while significantly improving rendering fidelity for both static and dynamic scene content compared to recent methods \\\\cite{35,50}, as illustrated in Fig. 1.\\n\\nWe take inspiration from recent methods for rendering static scenes that synthesize novel images by aggregating local image features from nearby views along epipolar lines \\\\cite{39,64,70}. However, scenes that are in motion violate the epipolar constraints assumed by those methods. We instead propose to aggregate multi-view image features in scene motion\u2013adjusted ray space, which allows us to correctly reason about spatio-temporally varying geometry and appearance.\\n\\nWe also encountered many efficiency and robustness challenges in scaling up aggregation-based methods to dynamic scenes. To efficiently model scene motion across multiple views, we model this motion using motion trajectory fields that span multiple frames, represented with learned basis functions. Furthermore, to achieve temporal coherence in our dynamic scene reconstruction, we introduce a new temporal photometric loss that operates in motion-adjusted ray space. Finally, to improve the quality of novel views, we propose to factor the scene into static and dynamic components through a new IBR-based motion segmentation technique within a Bayesian learning framework.\\n\\nOn two dynamic scene benchmarks, we show that our approach can render highly detailed scene content and significantly improves upon the state-of-the-art, leading to an average reduction in LPIPS errors by over 50% both across entire scenes, as well as on regions corresponding to dynamic objects. We also show that our method can be applied to in-the-wild videos with long duration, complex scene motion, and uncontrolled camera trajectories, where prior state-of-the-art methods fail to produce high quality renderings. We hope that our work advances the applicability of dynamic view synthesis methods to real-world videos.\\n\\n2. Related Work\\n\\nNovel view synthesis.\\n\\nClassic image-based rendering (IBR) methods synthesize novel views by integrating pixel information from input images \\\\cite{58}, and can be categorized according to their dependence on explicit geometry. Light field or lumigraph rendering methods \\\\cite{9,21,26,32} generate new views by filtering and interpolating sampled rays, without use of explicit geometric models. To handle sparser input views, many approaches \\\\cite{7,14,18,23,24,26,30,52,54,55} leverage pre-computed proxy geometry such as depth maps or meshes to render novel views.\\n\\nRecently, neural representations have demonstrated high-quality novel view synthesis \\\\cite{12,17,38,40,46,48,59-62,72,81}. In particular, Neural Radiance Fields (NeRF) \\\\cite{46} achieves an unprecedented level of fidelity by encoding continuous scene radiance fields within multi-layer perceptrons (MLPs). Among all methods building on NeRF, IBRNet \\\\cite{70} is the most relevant to our work. IBRNet combines classical IBR techniques with volume rendering to produce a generalized IBR module that can render high-quality views without per-scene optimization. Our work extends this kind of volumetric IBR framework designed for static scenes \\\\cite{11,64,70} to more challenging dynamic scenes. Note that our focus is on synthesizing higher-quality novel views for long videos with complex camera and object motion, rather than on generalization across scenes.\\n\\nDynamic scene view synthesis.\\n\\nOur work is related to geometric reconstruction of dynamic scenes from RGBD \\\\cite{5,15,25,47,68,83} or monocular videos \\\\cite{31,44,78,80}. However, depth- or mesh-based representations struggle to model complex geometry and view-dependent effects. Most prior work on novel view synthesis for dynamic scenes requires multiple synchronized input videos \\\\cite{1,3,6,27,33,63,69,76,82}, limiting their real-world applicability. Some methods \\\\cite{8,13,22,51,71} use domain knowledge such as template models to achieve high-quality results, but are restricted to specific categories \\\\cite{41,56}. More recently, many works propose to synthesize novel views of dynamic scenes from a single camera. Yoon et al. \\\\cite{75} render novel views through explicit warping using depth maps obtained via single-view depth and multi-view stereo. However, this method fails to model complex scene geometry and to fill in realistic and consistent content at disocclusions. With advances in neural rendering, NeRF-based dynamic view synthesis methods have shown state-of-the-art results \\\\cite{16,35,53,66,74}. Some approaches, such as Nerfies \\\\cite{49} and HyperNeRF \\\\cite{50}, represent scenes using a deformation field mapping each local observation to a canonical scene representation. These deformations are conditioned on time \\\\cite{53} or a per-frame latent code \\\\cite{49,50,66}, and are parameterized as translations \\\\cite{53,66} or rigid body motion fields \\\\cite{49,50}. These methods can handle long videos, but are mostly limited to object-centric scenes with relatively small object motion and controlled camera paths. Other methods represent scenes as time-varying NeRFs \\\\cite{19,20,35,67,74}.\\n\\nIn particular, NSFF uses neural scene flow fields that can...\"}"}
{"id": "CVPR-2023-601", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Rendering via motion-adjusted multi-view feature aggregation.\\n\\nGiven a sampled location $x_i$ at time $i$ along a target ray $r$, we estimate its motion trajectory, which determines the 3D correspondence of $x_i$ at nearby time $j \\\\in N(i)$, denoted $x_i \\\\rightarrow j$. Each warped point is then projected into its corresponding source view. Image features $f_j$ extracted along the projected curves are aggregated and fed to the ray transformer with time embedding $\\\\gamma(x_i)$, producing per-sample color and density $(c_i, \\\\sigma_i)$. The final pixel color $\\\\hat{C}_i$ is then synthesized by volume rendering $(c_i, \\\\sigma_i)$ along $r$.\\n\\nCapture fast and complex 3D scene motion for in-the-wild videos [35]. However, this method only works well for short (1-2 second), forward-facing videos.\\n\\n3. Dynamic Image-Based Rendering\\n\\nGiven a monocular video of a dynamic scene with frames $(I_1, I_2, \\\\ldots, I_N)$ and known camera parameters $(P_1, P_2, \\\\ldots, P_N)$, our goal is to synthesize a novel viewpoint at any desired time within the video. Like many other approaches, we train per-video, first optimizing a model to reconstruct the input frames, then using this model to render novel views.\\n\\nRather than encoding 3D color and density directly in the weights of an MLP as in recent dynamic NeRF methods, we integrate classical IBR ideas into a volumetric rendering framework. Compared to explicit surfaces, volumetric representations can more readily model complex scene geometry with view-dependent effects.\\n\\nThe following sections introduce our methods for scene-motion-adjusted multi-view feature aggregation (Sec. 3.1), and enforcing temporal consistency via cross-time rendering in motion-adjusted ray space (Sec. 3.2). Our full system combines a static model and a dynamic model to produce a color at each pixel. Accurate scene factorization is achieved via segmentation masks derived from a separately trained motion segmentation module within a Bayesian learning framework (Sec. 3.3).\\n\\n3.1. Motion-adjusted feature aggregation\\n\\nWe synthesize new views by aggregating features extracted from temporally nearby source views. To render an image at time $i$, we first identify source views $I_j$ within a temporal radius $r$ frames of $i$, $j \\\\in N(i) = [i-r, i+r]$. For each source view, we extract a 2D feature map $F_j$ through a shared convolutional encoder network to form an input tuple $\\\\{I_j, P_j, F_j\\\\}$.\\n\\nTo predict the color and density of each point sampled along a target ray $r$, we must aggregate source view features while accounting for scene motion. For a static scene, points along a target ray will lie along a corresponding epipolar line in a neighboring source view, hence we can aggregate potential correspondences by simply sampling along neighboring epipolar lines [64, 70]. However, moving scene elements violate epipolar constraints, leading to inconsistent feature aggregation if motion is not accounted for. Hence, we perform motion-adjusted feature aggregation, as shown in Fig. 3. To determine correspondence in dynamic scenes, one straightforward idea is to estimate a scene flow field via an MLP [35] to determine a given point's motion-adjusted 3D location at a nearby time. However, this strategy is computationally infeasible in a volumetric IBR framework due to recursive unrolling of the MLPs.\\n\\nMotion trajectory fields. Instead, we represent scene motion using motion trajectory fields described in terms of learned basis functions. For a given 3D point $x$ along target ray $r$ at time $i$, we encode its trajectory coefficients with an MLP $G_{MT}$:\\n\\n$$\\\\{\\\\phi_l(x_i)\\\\}_{l=1}^L = G_{MT}(\\\\gamma(x_i), \\\\gamma(i))$$\\n\\n(1)\\n\\nwhere $\\\\phi_l \\\\in \\\\mathbb{R}^3$ are basis coefficients (with separate coefficients for $x$, $y$, and $z$, using the motion basis described below) and $\\\\gamma$ denotes positional encoding. We choose $L=6$ bases and 16 linearly increasing frequencies for the encoding $\\\\gamma$, based on the assumption that scene motion tends to be low frequency [80].\\n\\nWe also introduce a global learnable motion basis $\\\\{h_l\\\\}_{l=1}^L$, $h_l \\\\in \\\\mathbb{R}$, spanning every time step $i$ of the input video, which is optimized jointly with the MLP. The motion trajectory of $x$ is then defined as $\\\\Gamma_{x,i}(j) = \\\\sum_{l=1}^L h_l \\\\phi_l(x_i)$, and thus, the relative displacement between $x$ and its 3D correspondence $x_i \\\\rightarrow j$ at time $j$ is computed as $\\\\Delta x_{i,j} = \\\\Gamma_{x,i}(j) - \\\\Gamma_{x,i}(i)$.\\n\\nWith this motion trajectory representation, finding 3D correspondences for a query point $x$ in neighboring views requires just a single MLP query, allowing efficient multi-view feature aggregation within our volume rendering framework.\\n\\nWe initialize the basis $\\\\{h_l\\\\}_{l=1}^L$ with the DCT basis as proposed by Wang et al. [67], but fine-tune it along with other components during optimization, since we observe that a fixed DCT basis can fail to model a wide range of real-world motions (see third column of Fig. 4).\\n\\nUsing the estimated motion trajectory of $x$ at time $i$, we denote $x$'s corresponding 3D point at time $j$ as $x_i \\\\rightarrow j = x_i + \\\\Delta x_{i,j}$. We project each warped point $x_i \\\\rightarrow j$ into its source view $I_j$ using camera parameters $P_j$, and extract color and feature vector $f_j$ at the projected 2D pixel location.\"}"}
{"id": "CVPR-2023-601", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We then use standard NeRF volume rendering without utilizing or accurately reconstructing scene motion. Therefore, to recover a consistent scene with physically plausible motion, we enforce temporal coherence of the scene via cross-time rendering. This idea is that the scene at two neighboring times is represented completely separate models for each time instance, but fail to render correct novel views. This can happen because the representation has the capacity to reconstruct those views, but the reconstruction is not accurate. To enforce temporal consistency in our optimized representation via cross-time rendering, we render a view at time \\\\( i \\\\) given the source scene model from a nearby time \\\\( j \\\\), which we refer to as cross-time rendering. For each nearby view \\\\( k \\\\), we extract \\\\( C \\\\) colors and densities. A pixel color \\\\( \\\\hat{C} \\\\) is computed by volume rendering of \\\\( C \\\\)\\n\\n\\\\[\\n\\\\hat{C}(r) = \\\\sum_{C} \\\\rho(r) \\\\omega(r) \\\\hat{C}(r)\\n\\\\]\\n\\nwhere \\\\( \\\\rho(r) \\\\) and \\\\( \\\\omega(r) \\\\) are the per-sample color and density (transformer network with time embedding \\\\( \\\\gamma \\\\)). A pixel color \\\\( \\\\hat{C} \\\\) is a motion disocclusion weight computed \\\\( \\\\hat{C}(r) \\\\) through volume rendering to form a color \\\\( \\\\hat{C}(r) \\\\) at time \\\\( j \\\\) via \\n\\n\\\\[\\n\\\\hat{C}(r) = \\\\sum_{C} \\\\rho(r) \\\\omega(r) \\\\hat{C}(r)\\n\\\\]\\n\\nNote that when \\\\( \\\\hat{C}(r) \\\\) is a motion disocclusion weight computed \\\\( \\\\hat{C}(r) \\\\) via a motion-disocclusion-aware RGB reconstruction loss: \\n\\n\\\\[\\n\\\\hat{C}(r) = \\\\sum_{C} \\\\rho(r) \\\\omega(r) \\\\hat{C}(r)\\n\\\\]\\n\\nComputing \\\\( \\\\hat{C}(r) \\\\) exactly as described for a \\\"straight\\\" ray \\\\( r \\\\) through weighted average pooling \\\\( \\\\hat{C}(r) \\\\) to produce a single \\\\( \\\\hat{C}(r) \\\\) for the ray from this sequence of \\\\( r \\\\) is fed to a shared MLP whose output features are aggregated \\\\( \\\\hat{C}(r) \\\\) along which we aggregate image features \\\\( f \\\\) to form a reconstruction loss \\n\\n\\\\[\\nL_i(C) = \\\\sum_{r} \\\\sum_{j} (\\\\hat{C}(r) - C_j(r))^2\\n\\\\]\\n\\nIn particular, we enforce temporal photometric consistency in the first column of Fig. 3. The resulting set of source features across neighbor views \\\\( j \\\\) are fed to the ray transformer with time embedding \\\\( \\\\gamma \\\\) from every sampled location along \\\\( r \\\\). These features are then composited \\\\( \\\\hat{C}(r) \\\\) and treated as if they lie along a ray at time \\\\( j \\\\) and then predicted per-sample colors and densities \\n\\n\\\\[\\n\\\\hat{C}(r) = \\\\sum_{C} \\\\rho(r) \\\\omega(r) \\\\hat{C}(r)\\n\\\\]\\n\\nA single ray is fed to uncontrolled camera paths. Therefore, we follow the ideas of NSFF during optimization. Static content is represented with a time-invariant model, which renders in the same way as the time-varying model, but aggregates multi-frame representations. Dynamic content is described by NSFF \\\\( \\\\hat{C}(r) \\\\), and high-quality content for static scene regions, since the representation has the capacity to reconstruct those views, but the reconstruction is not accurate. We can then compare \\n\\n\\\\[\\n\\\\hat{C}(r) = \\\\sum_{C} \\\\rho(r) \\\\omega(r) \\\\hat{C}(r)\\n\\\\]\\n\\nas observed in NSFF , synthesizing novel views using a small temporal window is insufficient to recover complete high-quality content for static scene regions, since the representation has the capacity to reconstruct those views, but the reconstruction is not accurate. We can then compare \\n\\n\\\\[\\n\\\\hat{C}(r) = \\\\sum_{C} \\\\rho(r) \\\\omega(r) \\\\hat{C}(r)\\n\\\\]\\n\\nand high-quality content for static scene regions, since the representation has the capacity to reconstruct those views, but the reconstruction is not accurate. We can then compare \\n\\n\\\\[\\n\\\\hat{C}(r) = \\\\sum_{C} \\\\rho(r) \\\\omega(r) \\\\hat{C}(r)\\n\\\\]\\n\\nand high-quality content for static scene regions, since the representation has the capacity to reconstruct those views, but the reconstruction is not accurate. We can then compare \\n\\n\\\\[\\n\\\\hat{C}(r) = \\\\sum_{C} \\\\rho(r) \\\\omega(r) \\\\hat{C}(r)\\n\\\\]\"}"}
{"id": "CVPR-2023-601", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative ablations. From left to right, we show rendered novel views (top) and depths (bottom) from our system (a) without enforcing temporal consistency, (b) aggregating image features with scene flow fields instead of motion trajectories, (c) representing motion trajectory with a fixed DCT basis instead of a learned one, and (d) with full configuration. Simpler configurations significantly degrade rendering quality as indicated by PSNR calculated over the regions of moving objects.\\n\\nFigure 5. Motion segmentation. We show full rendering $\\\\hat{B}_{\\\\text{full}}$ (top) and motion segmentation overlaid with rendered dynamic content $\\\\alpha_{\\\\text{dy}} \\\\odot \\\\hat{B}_{\\\\text{dy}}$ (bottom). Our approach segments challenging dynamic elements such as the moving shadow, swing, and swaying bushes.\\n\\nThe full reconstructed image is then composited pixelwise from the outputs of the two models:\\n\\n$$\\\\hat{B}_{\\\\text{full}}(r) = \\\\alpha_{\\\\text{dy}}(r) \\\\hat{B}_{\\\\text{dy}}(r) + (1 - \\\\alpha_{\\\\text{dy}}(r)) \\\\hat{B}_{\\\\text{st}}(r).$$\\n\\nTo segment moving objects, we assume the observed pixel color is uncertain in a heteroscedastic aleatoric manner, and model the observations in the video with a Cauchy distribution with time dependent confidence $\\\\beta_{\\\\text{dy}}$. By taking the negative log-likelihood of the observations, our segmentation loss is written as a weighted reconstruction loss:\\n\\n$$L_{\\\\text{seg}} = \\\\sum r \\\\log (\\\\beta_{\\\\text{dy}}(r) + ||\\\\hat{B}_{\\\\text{full}}(r) - C_i(r)||^2 \\\\beta_{\\\\text{dy}}(r)).$$\\n\\nBy optimizing the two models using Eq. 7, we obtain a motion segmentation mask $M_i$ by thresholding $\\\\alpha_{\\\\text{dy}}$ at 0.5. We do not require an alpha regularization loss as in NeRF-W to avoid degeneracies, since we naturally include such an inductive basis by excluding skip connections from network $D$, which leads $D$ to converge more slowly than the static IBR model. We show our estimated motion segmentation masks overlaid on input images in Fig. 5.\\n\\nSupervision with segmentation masks. We initialize our main time-varying and time-invariant models with masks $M_i$ as in Omnimatte, by applying a reconstruction loss to renderings from the time-varying model in dynamic regions, and to renderings from the time-invariant model in static regions:\\n\\n$$L_{\\\\text{mask}} = \\\\sum r (1 - M_i(r)) \\\\rho(\\\\hat{C}_{\\\\text{st}}(r), C_i(r)) + \\\\sum r M_i(r) \\\\rho(\\\\hat{C}_{\\\\text{dy}}(r), C_i(r)).$$\\n\\nWe perform morphological erosion and dilation on $M_i$ to obtain masks of dynamic and static regions respectively in order to turn off the loss near mask boundaries. We supervise the system with $L_{\\\\text{mask}}$ and decay the weights by a factor of 5 for dynamic regions every 50K optimization steps.\"}"}
{"id": "CVPR-2023-601", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.4. Regularization\\n\\nAs noted in prior work, monocular reconstruction of complex dynamic scenes is highly ill-posed, and using photometric consistency alone is insufficient to avoid bad local minima during optimization \\\\[19, 35\\\\]. Therefore, we adopt regularization schemes used in prior work \\\\[2, 35, 73, 75\\\\], which consist of three main parts\\n\\n\\\\[ L_{\\\\text{reg}} = L_{\\\\text{data}} + L_{\\\\text{MT}} + L_{\\\\text{cpt}}. \\\\]\\n\\n\\\\(L_{\\\\text{data}}\\\\) is a data-driven term consisting of \\\\(\\\\ell_1\\\\) monocular depth and optical flow consistency priors using the estimates from Zhang et al. \\\\[80\\\\] and RAFT \\\\[65\\\\].\\n\\n\\\\(L_{\\\\text{MT}}\\\\) is a motion trajectory regularization term that encourages estimated trajectory fields to be cycle-consistent and spatial-temporally smooth.\\n\\n\\\\(L_{\\\\text{cpt}}\\\\) is a compactness prior that encourages the scene decomposition to be binary via an entropy loss, and mitigates floaters through distortion losses \\\\[2\\\\]. We refer readers to the supplement for more details.\\n\\nIn summary, the final combined loss used to optimize our main representation for space-time view synthesis is:\\n\\n\\\\[ L = L_{\\\\text{pho}} + L_{\\\\text{mask}} + L_{\\\\text{reg}}. \\\\] (9)\\n\\n4. Implementation details\\n\\nData.\\n\\nWe conduct numerical evaluations on the Nvidia Dynamic Scene Dataset \\\\[75\\\\] and UCSD Dynamic Scenes Dataset \\\\[37\\\\]. Each dataset consists of eight forward-facing dynamic scenes recorded by synchronized multi-view cameras. We follow prior work \\\\[35\\\\] to derive a monocular video from each sequence, where each video contains 100\u2013250 frames. We removed frames that lack large regions of moving objects. We follow the protocol from prior work \\\\[35\\\\] that uses held-out images per time instance for evaluation. We also tested the methods on in-the-wild monocular videos, which feature more challenging camera and object motions \\\\[20\\\\].\\n\\nView selection.\\n\\nFor the time-varying, dynamic model we use a frame window radius of \\\\(r = 3\\\\) for all experiments. For the time-invariant model representing static scene content, we use separate strategies for the dynamic scenes benchmarks and for in-the-wild videos. For the benchmarks, where camera viewpoints are located at discrete camera rig locations, we choose all nearby distinct viewpoints whose timestep is within 12 frames of the target time. For in-the-wild videos, naively choosing the nearest source views can lead to poor reconstruction due to insufficient camera baseline. Thus, to ensure that for any rendered pixel we have sufficient source views for computing its color, we select source views from distant frames. If we wish to select \\\\(N_{\\\\text{vs}}\\\\) source views for the time-invariant model, we sub-sample every \\\\(2r_{\\\\text{max}}\\\\) frames from the input video to build a candidate pool, where for a given target time \\\\(i\\\\), we only search source views within \\\\([i - r_{\\\\text{max}}, i + r_{\\\\text{max}}]\\\\) frames. We estimate \\\\(r_{\\\\text{max}}\\\\) using the method of Li et al. \\\\[34\\\\] based on SfM point co-visibility and camera relative baseline. We then construct a final set of source views for the model by choosing the top \\\\(N_{\\\\text{vs}}\\\\) frames in the candidate pool that are the closest to the target view in terms of camera baseline. We set \\\\(N_{\\\\text{vs}} = 16\\\\).\\n\\nGlobal spatial coordinate embedding\\n\\nWith local image feature aggregation alone, it is hard to determine density accurately on non-surface or occluded surface points due to inconsistent features from different source views, as described in NeuRay \\\\[39\\\\]. Therefore, to improve global reasoning for density prediction, we append a global spatial coordinate embedding as an input to the ray transformer, in addition to the time embedding, similar to the ideas from \\\\[64\\\\]. Please see supplement for more details.\\n\\nHandling degeneracy through virtual views.\\n\\nPrior work \\\\[35\\\\] observed that optimization can converge to bad local minimal if camera and object motions are close to colinear, or object motions are too fast to track. Inspired by \\\\[36\\\\], we synthesize images at eight randomly sampled nearby viewpoints for every input via depths estimated by \\\\[80\\\\]. During rendering, we randomly sample a virtual view as additional source image. We only apply this technique to in-the-wild videos since it improves optimization stability and rendering quality for such inputs, whereas we don't observe improvement on the benchmarks due to disentangled camera-object motions as described by \\\\[20\\\\].\\n\\nTime interpolation.\\n\\nOur approach also allows for time interpolation by performing scene motion\u2013based splatting, as introduced by NSFF \\\\[35\\\\]. To render at a specified target fractional time, we predict the volumetric density and color at two nearby input times by aggregating local image features from their corresponding set of source views. The predicted color and density are then splatted and linearly blended via the scene flow derived from our motion trajectories, and weighted according to the target fractional time index.\\n\\nSetup.\\n\\nWe estimate camera poses using COLMAP \\\\[57\\\\]. For each ray, we use a coarse-to-fine sampling strategy with 128 per-ray samples as in Wang et al. \\\\[70\\\\]. A separate model is trained from scratch for each scene using the Adam optimizer \\\\[29\\\\]. The network architecture for two main representations is a variant of that of IBRNet \\\\[70\\\\]. We reconstruct the entire scene in Euclidean space without special scene parameterization. Optimizing a full system on a 10 second video takes around two days using 8 Nvidia A100s, and rendering takes roughly 20 seconds for a \\\\(768 \\\\times 432\\\\) frame.\"}"}
{"id": "CVPR-2023-601", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our rendered views\\nDVS HyperNeRF\\nNSFF Ours\\nGT\\n\\nFigure 6. Qualitative comparisons on the Nvidia dataset [75].\\n\\nMethods Full Dynamic Only\\nSSIM \u2191 PSNR \u2191 LPIPS \u2193 SSIM \u2191 PSNR \u2191 LPIPS \u2193\\nNerfies [49] 0.823 24.32 0.096 0.595 18.45 0.234\\nHyperNeRF [50] 0.859 25.10 0.095 0.618 19.26 0.212\\nDVS [19] 0.943 30.64 0.075 0.866 26.57 0.096\\nNSFF [35] 0.952 31.75 0.034 0.851 25.83 0.115\\nOurs 0.983 36.47 0.014 0.909 28.01 0.042\\n\\nTable 2. Quantitative evaluation on the UCSD dataset [37].\\n\\nWe compare our approach to state-of-the-art monocular view synthesis methods. Specifically, we compare to three recent canonical space\u2013based methods, Nerfies [49] and HyperNeRF [50], and to two scene flow\u2013based methods, NSFF [35] and Dynamic View Synthesis (DVS) from Gao et al. [19]. For fair comparisons, we use the same depth, optical flow and motion segmentation masks used for our approach as inputs to other methods.\\n\\nFollowing prior work [35], we report the rendering quality of each method with three standard error metrics: peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and perceptual similarity via LPIPS [77], and calculate errors both over the entire scene (Full) and restricted to moving regions (Dynamic Only).\\n\\n5.2. Quantitative evaluation\\nQuantitative results on the two benchmark datasets are shown in Table 1 and Table 2. Our approach significantly improves over prior state-of-the-art methods in terms of all error metrics. Notably, our approach improves PSNR over the second best methods by 2dB and 4dB on each of the two datasets. Our approach also reduces LPIPS error, a major indicator of perceptual quality compared with real images [77], by over 50%. These results suggest that our framework is much more effective at recovering highly detailed scene contents.\\n\\n5.3. Qualitative evaluation\\nDynamic scenes dataset.\\nWe provide qualitative comparisons between our approach and three prior state-of-the-art methods [19, 35, 50] on the test views from two datasets in Fig. 6 and Fig. 7. Prior dynamic-NeRF methods have difficulty rendering details of moving objects, as seen in the excessively blurred dynamic content including the texture of balloons, human faces, and clothing. In contrast, our approach synthesizes photo-realistic novel views of both static and dynamic scene content and which are closest to the ground truth images.\\n\\nIn-the-wild videos.\\nWe show qualitative comparisons on in-the-wild footage of complex dynamic scenes. We show comparisons with Dynamic-NeRF based methods in Fig. 8, and Fig. 9 shows comparisons with point cloud rendering using depths [80]. Our approach synthesizes photo-realistic...\"}"}
{"id": "CVPR-2023-601", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We show results on 10-second videos of complex dynamic scenes. The leftmost column shows the start and end frames of each video; on the right we show novel views at intermediate times rendered from our approach and prior state-of-the-art methods [19, 35, 50].\\n\\nFigure 9. From left to right, we show inputs and corresponding novel views rendered from explicit depth warping with Zhang et al. [80], and from our approach.\\n\\n| Methods   | Full Dynamic Only | Only Dynamic |\\n|-----------|-------------------|--------------|\\n| SSIM      | \u2191                 | \u2191            |\\n| PSNR      | \u2191                 | \u2191            |\\n| LPIPS     | \u2193                 | \u2193            |\\n\\nTable 3. Ablation study on the Nvidia Dataset. See Sec. 5.2 for detailed descriptions of each configuration.\\n\\nOn the other hand, explicit depth warping produces holes at regions near disocclusions and out of field of view. We refer readers to the supplementary video for full comparisons.\\n\\n6. Discussion and conclusion\\n\\nLimitations. Our method is limited to relatively small viewpoint changes compared with methods designed for static or quasi-static scenes. Our method is not able to handle small fast moving objects due to incorrect initial depth and optical flow estimates (left, Fig. 10). In addition, compared to prior dynamic NeRF methods, the synthesized views are not strictly multi-view consistent, and rendering quality of static content depends on which source views are selected (middle, Fig. 10). Our approach is also sensitive to degenerate motion patterns from in-the-wild videos, in which object and camera motion is mostly colinear, but we show heuristics to handle such cases in the supplemental. Moreover, our method is able to synthesize dynamic contents only appearing at distant times (right, Fig. 10).\\n\\nConclusion. We presented a new approach for space-time view synthesis from a monocular video depicting a complex dynamic scene. By representing a dynamic scene within a volumetric IBR framework, our approach overcomes limitations of recent methods that cannot model long videos with complex camera and object motion. We have shown that our method can synthesize photo-realistic novel views from in-the-wild dynamic videos, and can achieve significant improvements over prior state-of-the-art methods on the dynamic scene benchmarks.\\n\\nAcknowledgements. Thanks to Andrew Liu, Richard Bowen and Lucy Chai for the fruitful discussions, and thanks to Rick Szeliski and Ricardo Martin-Brualla for helpful proofreading.\"}"}
{"id": "CVPR-2023-601", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, and Srinivasa Narasimhan. 4D visualization of dynamic events from unconstrained multi-view videos. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 5366\u20135375, 2020.\\n\\n[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded anti-aliased neural radiance fields. In Proc. Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[3] Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, and Tobias Ritschel. X-fields: Implicit neural view-, light- and time-image interpolation. ACM Trans. Graphics, 39(6), 2020.\\n\\n[4] Sai Bi, Zexiang Xu, Pratul P. Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Milovs Havsan, Yannick Hold-Geoffroy, D. Kriegman, and R. Ramamoorthi. Neural reflectance fields for appearance acquisition. ArXiv, abs/2008.03824, 2020.\\n\\n[5] Aljaz Bozic, Michael Zollhofer, Christian Theobalt, and Matthias Niessner. Deepdeform: Learning non-rigid RGB-D reconstruction with semi-supervised data. In Proc. Computer Vision and Pattern Recognition (CVPR), June 2020.\\n\\n[6] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erickson, Peter Hedman, Matthew Duvall, Jason Dourgarian, Jay Busch, Matt Whalen, and Paul Debevec. Immersive light field video with a layered mesh representation. ACM Trans. Graph., 39(4), July 2020.\\n\\n[7] Chris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen. Unstructured lumigraph rendering. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 425\u2013432, 2001.\\n\\n[8] Joel Carranza, Christian Theobalt, Marcus A Magnor, and Hans-Peter Seidel. Free-viewpoint video of human actors. ACM transactions on graphics (TOG), 22(3):569\u2013577, 2003.\\n\\n[9] Jin-Xiang Chai, Xin Tong, Shing-Chow Chan, and Heung-Yeung Shum. Plenoptic sampling. In Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '00, page 307\u2013318, USA, 2000. ACM Press/Addison-Wesley Publishing Co.\\n\\n[10] Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, and Michel Barlaud. Two deterministic half-quadratic regularization algorithms for computed imaging. In Proceedings of 1st International Conference on Image Processing, volume 2, pages 168\u2013172. IEEE, 1994.\\n\\n[11] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. MVSNeRF: Fast generalizable radiance field reconstruction from multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14124\u201314133, 2021.\\n\\n[12] Inchang Choi, Orazio Gallo, Alejandro Troccoli, Min H Kim, and Jan Kautz. Extreme view synthesis. In Proc. Int. Conf. on Computer Vision (ICCV), pages 7781\u20137790, 2019.\\n\\n[13] Edilson De Aguiar, Carsten Stoll, Christian Theobalt, Naveed Ahmed, Hans-Peter Seidel, and Sebastian Thrun. Performance capture from sparse multi-view video. In ACM SIGGRAPH 2008 papers, pages 1\u201310. 2008.\\n\\n[14] Paul E Debevec, Camillo J Taylor, and Jitendra Malik. Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 11\u201320, 1996.\\n\\n[15] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip L. Davidson, Sean Ryan Fanello, Adarsh Kowdle, Sergio Orts, Christoph Rhemann, David Kim, Jonathan Taylor, Pushmeet Kohli, Vladimir Tankovich, and Shahram Izadi. Fusion4D: real-time performance capture of challenging scenes. ACM Trans. Graphics, 35:114:1\u2013114:13, 2016.\\n\\n[16] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenenbaum, and Jiajun Wu. Neural radiance flow for 4d view synthesis and video processing. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 14304\u201314314. IEEE Computer Society, 2021.\\n\\n[17] John Flynn, Michael Broxton, Paul Debevec, Matthew Duvall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and Richard Tucker. DeepView: View synthesis with learned gradient descent. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 2367\u20132376, 2019.\\n\\n[18] John Flynn, Ivan Neulander, James Philbin, and Noah Snavely. DeepStereo: Learning to predict new views from the world\u2019s imagery. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 5515\u20135524, 2016.\\n\\n[19] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5712\u20135721, 2021.\\n\\n[20] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: A reality check. arXiv preprint arXiv:2210.13445, 2022.\\n\\n[21] Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F Cohen. The lumigraph. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 43\u201354, 1996.\\n\\n[22] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-Escolano, Rohit Pandey, Jason Dourgarian, et al. The re-lightables: Volumetric performance capture of humans with realistic relighting. ACM Transactions on Graphics (ToG), 38(6):1\u201319, 2019.\\n\\n[23] Peter Hedman, Tobias Ritschel, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Trans. Graphics, 37(6):1\u201315, 2018.\\n\\n[24] Peter Hedman, Tobias Ritschel, George Drettakis, and Gabriel Brostow. Scalable inside-out image-based rendering. ACM Transactions on Graphics (TOG), 35(6):1\u201311, 2016.\\n\\n[25] Matthias Innmann, Michael Zollh\u00f6fer, Matthias Niessner, Christian Theobalt, and Marc Stamminger. VolumeDeform: Real-time volumetric non-rigid reconstruction. In Proc. European Conf. on Computer Vision (ECCV), 2016.\\n\\n[26] Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ramamoorthi. Learning-based view synthesis for light field cameras. ACM Trans. Graphics, 35(6):1\u201310, 2016.\"}"}
{"id": "CVPR-2023-601", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Takeo Kanade, Peter Rander, and PJ Narayanan. Virtualized reality: Constructing virtual worlds from real scenes. IEEE Multimedia, 4(1):34\u201347, 1997.\\n\\nYoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. ACM Transactions on Graphics (TOG), 40(6):1\u201312, 2021.\\n\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.\\n\\nJohannes Kopf, Michael F Cohen, and Richard Szeliski. First-person hyper-lapse videos. ACM Transactions on Graphics (TOG), 33(4):1\u201310, 2014.\\n\\nJohannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1611\u20131621, 2021.\\n\\nMarc Levoy and Pat Hanrahan. Light field rendering. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 31\u201342, 1996.\\n\\nTianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3D video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5521\u20135531, 2022.\\n\\nZhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, and William T Freeman. Learning the depths of moving people by watching frozen people. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 4521\u20134530, 2019.\\n\\nZhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nZhengqi Li, Qianqian Wang, Noah Snavely, and Angjoo Kanazawa. Infinitenature-zero: Learning perpetual view generation of natural scenes from single images. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part I, pages 515\u2013534. Springer, 2022.\\n\\nKai-En Lin, Lei Xiao, Feng Liu, Guowei Yang, and Ravi Ramamoorthi. Deep 3D mask volume for view synthesis of dynamic scenes. In Proc. Int. Conf. on Computer Vision (ICCV), 2021.\\n\\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, 33:15651\u201315663, 2020.\\n\\nYuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng Wang, Christian Theobalt, Xiaowei Zhou, and Wenping Wang. Neural rays for occlusion-aware image-based rendering. In CVPR, 2022.\\n\\nStephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. ACM Trans. Graphics, 38(4):65, 2019.\\n\\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. SMPL: A skinned multi-person linear model. ACM transactions on graphics (TOG), 34(6):1\u201316, 2015.\\n\\nErika Lu, F. Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, D. Salesin, W. Freeman, and M. Rubinstein. Layered neural rendering for retiming people in video. ACM Trans. Graphics (SIGGRAPH Asia), 2020.\\n\\nErika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman, William T Freeman, and Michael Rubinstein. Omnimatte: Associating objects and their effects in video. In Proc. Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nXuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ACM Trans. Graph., 39(4), July 2020.\\n\\nRicardo Martin-Brualla, N. Radwan, Mehdi S. M. Sajjadi, J. Barron, A. Dosovitskiy, and Daniel Duckworth. NeRF in the wild: Neural radiance fields for unconstrained photo collections. In Proc. Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Proc. European Conf. on Computer Vision (ECCV), 2020.\\n\\nRichard A Newcombe, Dieter Fox, and Steven M Seitz. DynamicFusion: Reconstruction and tracking of non-rigid scenes in real-time. In Proc. Computer Vision and Pattern Recognition (CVPR), 2015.\\n\\nMichael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3D representations without 3D supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3504\u20133515, 2020.\\n\\nKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5865\u20135874, 2021.\\n\\nKeunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. HyperNeRF: A higher-dimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228, 2021.\\n\\nSida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021.\\n\\nEric Penner and Li Zhang. Soft 3D reconstruction for view synthesis. ACM Trans. Graphics, 36(6):1\u201311, 2017.\\n\\nAlbert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-NeRF: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10318\u201310327, 2021.\\n\\nGernot Riegler and V. Koltun. Free view synthesis. Proc. European Conf. on Computer Vision (ECCV), 2020.\"}"}
{"id": "CVPR-2023-601", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gernot Riegler and Vladlen Koltun. Stable view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12216\u201312225, 2021.\\n\\nShunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2304\u20132314, 2019.\\n\\nJohannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 4104\u20134113, 2016.\\n\\nHarry Shum and Sing Bing Kang. Review of image-based rendering techniques. In Visual Communications and Image Processing 2000, volume 4067, pages 2\u201313. SPIE, 2000.\\n\\nVincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in Neural Information Processing Systems, 33:7462\u20137473, 2020.\\n\\nVincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie\u00dfner, Gordon Wetzstein, and Michael Zollhofer. Deep-voxels: Learning persistent 3D feature embeddings. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 2437\u20132446, 2019.\\n\\nVincent Sitzmann, Michael Zollh\u00f6fer, and Gordon Wetzstein. Scene representation networks: Continuous 3D-structure-aware neural scene representations. In Neural Information Processing Systems, pages 1119\u20131130, 2019.\\n\\nPratul P Srinivasan, Richard Tucker, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng, and Noah Snavely. Pushing the boundaries of view extrapolation with multiplane images. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 175\u2013184, 2019.\\n\\nTimo Stich, Christian Linz, Georgia Albuquerque, and Marcus Magnor. View and time interpolation in image space. In Computer Graphics Forum, volume 27, pages 1781\u20131787. Wiley Online Library, 2008.\\n\\nMohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Light field neural rendering. In Proc. Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nZachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In Proc. European Conf. on Computer Vision (ECCV), 2020.\\n\\nEdgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh\u00f6fer, Christoph Lassner, and Christian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12959\u201312970, 2021.\\n\\nChaoyang Wang, Ben Eckart, Simon Lucey, and Orazio Gallo. Neural trajectory fields for dynamic novel view synthesis. arXiv preprint arXiv:2105.05994, 2021.\\n\\nChaoyang Wang, Xueqian Li, Jhony Kaesemodel Pontes, and Simon Lucey. Neural prior for trajectory estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6532\u20136542, 2022.\\n\\nLiao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and Lan Xu. Fourier plenoctrees for dynamic radiance field rendering in real-time. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13524\u201313534, 2022.\\n\\nQianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 4690\u20134699, 2021.\\n\\nChung-Yi Weng, Brian Curless, Pratul P Srinivasan, Jonathan T Barron, and Ira Kemelmacher-Shlizerman. HumanNeRF: Free-viewpoint rendering of moving people from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16210\u201316220, 2022.\\n\\nSuttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, and Supasorn Suwajanakorn. NeX: Real-time view synthesis with neural basis expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8534\u20138543, 2021.\\n\\nTianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. D2NeRF: Self-supervised decoupling of dynamic and static objects from a monocular video. arXiv preprint arXiv:2205.15838, 2022.\\n\\nWenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free-viewpoint video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9421\u20139431, 2021.\\n\\nJae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz. Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 5336\u20135345, 2020.\\n\\nJiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, and Jingyi Yu. Editable free-viewpoint video using a layered neural representation. ACM Transactions on Graphics (TOG), 40(4):1\u201318, 2021.\\n\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proc. Computer Vision and Pattern Recognition (CVPR), pages 586\u2013595, 2018.\\n\\nZhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William T Freeman. Structure and motion from casual videos. In European Conference on Computer Vision, pages 20\u201337. Springer, 2022.\\n\\nZhoutong Zhang, Forrester Cole, Zhengqi Li, Noah Snavely, and William Freeman. Structure and motion from casual videos. In Proc. European Conf. on Computer Vision (ECCV), 2022.\\n\\nZhoutong Zhang, Forrester Cole, Richard Tucker, William T Freeman, and Tal Dekel. Consistent depth of moving objects in video. ACM Transactions on Graphics (TOG), 40(4):1\u201312, 2021.\\n\\nTinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: learning view synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4283\u20134292, 2021.\"}"}
{"id": "CVPR-2023-601", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"thesis using multiplane images.\\n\\nACM Trans. Graphics, 37:1 \u2013 12, 2018.\\n\\n[82] C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele, Simon Winder, and Richard Szeliski. High-quality video view interpolation using a layered representation. ACM Trans. Graphics, 23(3):600\u2013608, 2004.\\n\\n[83] Michael Zollh\u00f6fer, Matthias Niessner, Shahram Izadi, Christoph Rehmann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian Theobalt, et al. Real-time non-rigid reconstruction using an RGB-D camera. ACM Trans. Graphics, 33(4):156, 2014.\"}"}
