{"id": "CVPR-2024-996", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The target sensor (b) gives detailed geometry of the target object. The environment sensor (c) gives coarse spatial information around the target object. The trajectory sensor (d) is located around the human. Every set of two objects, we can infer their relationships in three types from their bounding boxes: horizontal proximity, vertical proximity, and support as mentioned in Sec. 3; for every set of three objects, we can infer if one of them is in between of the other two objects. As detection results do not contain object poses, we do not construct allocentric relations (e.g., \u201cthe shelf that is behind the sofa\u201d). Then, we build a scene graph, where each object is assigned as a node in the graph, and edges between nodes represent the relationships between objects. By converting 3D scenes into text, we can apply ChatGPT to extract meaningful insights from the data.\\n\\nLeveraging ChatGPT to localize the target object. A simple approach is to directly input the entire scene graph into ChatGPT and ask it to select the target objects. However, scenes may contain many objects, resulting in extremely long textual descriptions. We observe that ChatGPT is often confused in such settings and fails to respond with the right answer. To narrow the search space, we first employ ChatGPT to recognize objects that correlate to the provided text. We then exclude the unrelated objects from the scene graph, focusing solely on those with pertinent information, enabling us to pinpoint the target object effectively. This approach has the advantage of reducing the number of objects that need to be considered, making it easier for ChatGPT to identify the target object.\\n\\nAs shown in Fig. 3, we construct two prompts in sequence. Take \u201csit on the chair that is in the middle of the board and the end table\u201d as an example, we first need to narrow down the object category search space. In order to find which type of objects we care about, we construct the first prompt that asks ChatGPT to find out target objects and anchor objects. Target object is defined as the final object that we want the agent to interact with, which in this case, is the \u201ctable.\u201d Anchor objects are the objects that help with determining the target object, for there might be many chairs in one scene. Based on the target object \u201cchair\u201d and anchor objects \u201cboard\u201d and \u201cend table\u201d, we can filter out all the unrelated objects in the scene, only keeping chairs, the board, and the end table in our scene graph. Next, according to the simplified scene graph, we can describe object relations in text: every edge in the scene graph could be converted to an edge sentence like \u201cchair 4 is far from the end table 0\u201d. Converting all edges to such edge sentences gives a full description of the current scene. Finally, we construct a second prompt by asking ChatGPT to infer the target object from the accumulated edge sentences.\\n\\n4.2. Diffusion-based trajectory generation\\n\\nGiven the localized object from ChatGPT, we first generate the trajectory based on the instructions and then synthesize local human poses. Trajectory is defined as a sequence of characters\u2019 translations and orientations. As suggested by [51], not every point of the scene is relevant to the final human motions. Inspired by NSM [70] and ManipNet [92], we employ volumetric sensors (as shown in Fig. 4) around the target object to represent the scene.\\n\\nObject-centric scene representation. Denote the target object center location as $c_o = (c_x, c_y, c_z)$. We transform the point cloud of the scene $S$ to an object-centered coordinate axis centered at $c_o$. To recognize the surrounding geometry of the target object, we create volumetric sensors called Environment Sensor and Target Sensor.\\n\\nEnvironment sensor. The environment sensor is centered at $c_o$ in a cubic shape with a volume of $4 \\\\times 4 \\\\times 4$ m$^3$, containing $8 \\\\times 8 \\\\times 8$ cubic voxels as shown in Fig. 4 (c). It is constructed by collecting all occupancies $o_s$, center positions $c_v$, and normal directions $n_v$ of each voxel to form a feature vector $E$. Like [70], the scene occupancy $o_s \\\\in \\\\mathbb{R}^1$ in each voxel is defined based on the given scene mesh:\\n\\n$$o_s = \\\\begin{cases} 1 & \\\\text{if } d_s < 0, \\\\\\\\ 0 & \\\\text{if } d_s > a_s, \\\\\\\\ 1 - d_s & \\\\text{otherwise} \\\\end{cases}$$\\n\\nwhere $d_s$ is the signed distance between the scene mesh and the voxel center, and $a_s$ is the voxel edge length. $n_v$ is the normal direction of the closest scene point to the voxel center. The environment sensor provides coarse scene geometry around the target object.\\n\\nTarget sensor. To capture the detailed geometry of the target object, we further build a target sensor. As we already\"}"}
{"id": "CVPR-2024-996", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"obtain the 3D bounding box of the target object in Sec. 4.1, we crop the point clouds according to the bounding box and construct an $8 \\\\times 8 \\\\times 8$ cubic volumes that cover the bounding box, as shown in Fig. 4 (b). Target sensor $T$ is in the same form as the environment sensor $E$, except the target sensor has a different voxel size, as visualized in Fig. 4 (b) and (c).\\n\\nGiven the constructed object-centric representations, we follow [57] to employ a transformer decoder architecture [79], which enables arbitrary length motions. As for the text input, we use CLIP [62] text encoder to encode input text to the text feature $L$. The time-step $t$ is injected into the decoder in sinusoidal position embeddings form [79].\\n\\nIn summary, the condition for this generation model is\\n\\n$$C_t = \\\\{L, E, T\\\\},$$\\n\\nwhere $L$ is the text feature, $E$ is the environment sensor, and $T$ is the target sensor. All the conditions are projected to the same dimension $D = 512$ by MLPs and summed with positional embeddings to form tokens. We use the simple objective described in Eq. 2 to train the trajectory generation model $G_r$ to generate the trajectory $r_1: N$.\\n\\n### 4.3. Diffusion-based motion completion\\n\\nGiven the trajectory from Sec. 4.2, the next step is to complete the whole motion. Based on the generated trajectories, we construct a Trajectory Sensor $O$ to explicitly reason about the interaction between the character and scenes.\\n\\n**Trajectory sensor.** The trajectory sensor is also a volumetric sensor which is similar to the form of the environment sensor but is designed for ego-centric perception [95] as shown in Fig. 4 (d). Specifically, trajectory sensors are positioned around characters in each frame. This sensor $O_i$ is centered at the predicted root position of the $i$-th frame and faces to the $i$-th frame's predicted root orientation, containing $8 \\\\times 8 \\\\times 8$ cubic voxels that store scene occupancy. The occupancy calculation is the same as Eq. 3.\\n\\nAnother transformer-based conditional diffusion model is used to synthesize local poses along trajectories. The condition is defined as:\\n\\n$$C_m = \\\\{L, E, T, O_1, ..., O_N\\\\},$$\\n\\nwhere $L$, $E$, and $T$ have the same meanings in Eq. 4. Simple objective described in Eq. 2 is used to train the motion generation model $G_m$ to generate global orientation $\\\\gamma_1: N$ and local poses $\\\\theta_1: N$ with length $N$.\\n\\n### 4.4. Implementation details\\n\\nFollowing [38, 57, 80, 81], we use separate diffusion models for the trajectory generation and the motion generation. Because the HUMANISE dataset [83] contains a relatively small number (51 minutes) of pure motion samples from the AMASS dataset (3772 minutes) [49], we first pre-train the models on the whole AMASS dataset for 200 epochs and then fine-tune on the HUMANISE dataset for 200 epochs to improve the motion quality. During pretraining, the text feature $L$ is set to all zeros. Both models are trained with the AdamW optimizer [40], using a learning rate of $0.0001$ on a single Nvidia RTX 3090 GPU. The batch size is set to 128. The version of ChatGPT is gpt-3.5-turbo. More details can be found in the supplementary material.\\n\\n### 5. Experiments\\n\\n#### 5.1. Evaluation metrics\\n\\nWe evaluate the generated motions in three aspects: scene-conditional, action-conditional, and pure motion quality.\\n\\n**Scene-conditional motion quality.** To evaluate how the generated motion is aligned with the scene, we calculate the body-to-goal distance (goal dist.) [83] to measure how accurately the character interacts with target objects. However, goal dist. does not consider the consistency of the entire motion and scene (e.g., sitting on the sofa with incorrect orientation). To compensate for goal dist., a human perceptual study (indicated by scene score) is performed by randomly sampling 20 scenarios for each model, where ten workers are required to score each sample.\\n\\n**Action-conditional motion quality.** To measure how the generated motion is aligned with the text, we follow [17] to evaluate action recognition accuracy (accuracy), diversity, and multimodality of the results. Calculating these metrics relies on a pre-trained action recognition model [87] and we train the recognition model on the HUMANISE dataset.\\n\\n**Pure motion quality.** We evaluate the realism of generated motions using the metric suggested by [17], namely Frechet Inception Distance (FID). A lower FID indicates that generated motions are closer to the groundtruth motions. We also perform a human perceptual study (indicated by quality score) to measure pure motion quality.\\n\\n### 5.2. Generating human motion from text and scene\\n\\nOur experiments are conducted on the HUMANISE dataset [83]. Following the previous setting [83], there are $16.5k$ motion sequences in $543$ scenes for training and $3.1k$ motion sequences in $100$ scenes for testing. We compare our method with four baselines: (1) MDM$^\\\\star$ [75]: a diffusion-based motion generator. (2) GMD$^\\\\star$ [38]: GMD proposes various techniques for enhancing the control and quality of MDM. We employ the single-stage setting of GMD$^\\\\star$. Similar to HUMANISE, we use a point-transformer to provide scene features for MDM$^\\\\star$ and GMD$^\\\\star$. (3) GMD$^\\\\star$HC: we provide object centers predicted by HUMANISE (denoted by HC) to guide the motion generation process in GMD by adding a proximity loss to encourage the motion to be close to the predicted object center. The proximity loss is defined as:\\n\\n$$\\\\text{proximity loss} = \\\\sum_{i=1}^{N} \\\\sum_{j=1}^{K} \\\\|O_i - T_j\\\\|^2,$$\\n\\nwhere $O_i$ and $T_j$ are the predicted object center and the target pose, respectively.\"}"}
{"id": "CVPR-2024-996", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Quantitative results on the HUMANISE dataset.\\n\\n| Method       | Goal dist. | Scene score | Accuracy | Diversity | Multimodality | Quality score |\\n|--------------|------------|-------------|----------|-----------|---------------|---------------|\\n| Real         | 0.014      | -           | 99.1%    | 4.82      | 2.28          | 0.00          |\\n| MDM          | 2.048      | 1.99        | 96.0%    | 4.83      | 2.57          | 0.16          |\\n| GMD HC       | 1.229      | 2.50        | 96.6%    | 4.91      | 2.28          | 0.24          |\\n| HUMANISE HC  | 1.130      | 2.49        | 96.4%    | 4.93      | 2.43          | 0.23          |\\n| Ours         | 0.384      | 3.54        | 97.1%    | 4.78      | 2.31          | 0.12          |\\n\\nFigure 5. Qualitative results. We compare our method with groundtruth and four baselines (please refer to Sec. 5.2) given the same text descriptions. Our method synthesizes motions that interact with the object precisely as the groundtruth data while the baselines fail.\\n\\n5.3. Ablation study\\n\\nAblation of main components. Four variants are constructed to explore the effect of our design choice. (1) \\\"w/o localization\\\": the localization module is removed and trajectories and motions are generated directly using scene point clouds. Scene features are given as the same form of MDM. (2) \\\"w/o object-centric\\\": we remove our object-centric representation and predict motions in the scene coordinate. The scene features of our sensors are still employed. (3) \\\"w/o two-stage\\\": trajectories and motions are generated together. (4) \\\"w/o diffusion\\\": we employ cVAE instead of the diffusion model as the architecture. (5) \\\"w/o pretrain\\\": the models are not pre-trained on the AMASS dataset. As shown in Tab. 2, the localization enhances the precise interaction ability and the object-centric representation improves motion quality. Although the (\\\"w/o two-stage\\\") is more efficient and has competitive performance metrics comparing to our method, it is prone to collapse directly into the target position, lacking gradual transition and realism.\\n\\nThe design choice of object localization. For object localization, we compare our method with two baselines and five variants. (1) HUMANISE: predicts the object center in the auxiliary task. (2) BUTD-DETR: is a 3D visual grounding method. (3) \\\"ours w/o two-stage\\\": we employ a one-stage question-answering process. (4) \\\"ours w/o few-shot\\\": the few-shot examples are not provided. (5) \\\"ours using text matching\\\": we compute CLIP similarities instead of using ChatGPT. (6) \\\"ours using LLaMA\\\": ChatGPT is replaced by LLaMA 2. (7) \\\"ours using Mistral\\\": ChatGPT is replaced by Mistral 7B, an open-source LLM. The prompts for LLaMA 2 and Mistral\"}"}
{"id": "CVPR-2024-996", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Variants Scene Action Pure goal dist.\u2193 accuracy\u2191 diversity\u2192 mm\u2192 FID\u2193\\n\\nReal 0.014 99.1% 4.82 2.28 0.00\\nw/o localization 0.592 74.5% 3.71 2.75 3.14\\nw/o object-centric 0.413 90.6% 4.13 2.63 1.01\\nw/o two-stage 0.385 97.2% 4.93 2.33 0.14\\nw/o diffusion 0.390 40.0% 3.69 3.86 3.61\\nw/o pretrain 0.392 82.6% 3.88 2.37 2.50\\n\\nTable 2. Ablation of main components. We compare our method with five variants (please refer to Sec. 5.3). Among them, mm indicates multimodality. Bold indicates the best results. Underline indicates the second best.\\n\\n7B are slightly adjusted. We evaluate these variants under two scenarios: predicted detection [47] and groundtruth detection. The metrics include the accuracy (acc.) and the distance from predicted centers to the object center (center dist.). \\\"acc.\\\" is the percentage of times with an IoU (Intersection over Union) higher than the threshold (0.25) following [34]. The results are shown in Tab. 3. Since the results of Mistral are only slightly behind of ChatGPT, ChatGPT can be replaced by Mistral for better reproducibility, but cannot be replaced by the text matching method. Moreover, if groundtruth detection is provided, our method can achieve even better results.\\n\\nThe design choice of sensor density. Please refer to the supplementary material.\\n\\n5.4. Generalization\\nTo validate the generalization ability of our method, we run our method directly on the unseen PROX dataset [22] without fine-tuning. We provide illustrations in Fig. 6. With our localization method based on ChatGPT and motion generation method based on volumetric sensors, our pipeline can easily generalize to other datasets. Please refer to the supplementary material for more results.\\n\\n6. Discussion\\nWe have demonstrated that our approach could synthesize motions with better quality and more precise interactions. However, restricted by the dataset, the duration of motions is relatively short (60% are around 1-3s), and most texts follow the template [1] without detailed descriptions. We only tackle static scenes and extending our method to interact with moving objects is a future direction. We acknowledge that leveraging LLMs has several problems. ChatGPT may fail when the detector misses the objects and the behavior of ChatGPT is regulated by prompts [37]. Despite we have shown in Tab. 3 that ChatGPT can be replaced by an open source LLM Mistral 7B, using LLMs instead of classical parsing approaches is less efficient in the inference stage.\\n\\nFigure 6. Qualitative results of our method on the PROX dataset. We run our method on the scenes from the PROX dataset without fine-tuning. Results show that our method is capable to generalize to unseen scenes and objects.\\n\\n7. Conclusion\\nIn this work, we introduce a novel method for generating motion from text in a scene. To tackle this problem, we propose a two-step approach. The first step involves 3D visual grounding, where we identify the target object. In the second step, concentrating on the target object, we build a diffusion-based motion generation method. Our approach offers several advantages, including improved 3D visual grounding accuracy and motion quality.\\n\\nAcknowledgement\\nThis work was partially supported by the NSFC (No. 62172364), Ant Group and Information Technology Center and State Key Lab of CAD&CG, Zhejiang University.\"}"}
{"id": "CVPR-2024-996", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generating Human Motion in 3D Scenes from Text Descriptions\\n\\nZhi Cen\\nHuaijin Pi\\nSida Peng\\nZehong Shen\\nMinghui Yang\\nShuai Zhu\\nHujun Bao\\nXiaowei Zhou\\n\\n1 Zhejiang University\\n2 Ant Group\\n\\nFigure 1. Generating human motions in 3D scenes from text descriptions.\\n\\nAbstract\\n\\nGenerating human motions from textual descriptions has gained growing research interest due to its wide range of applications. However, only a few works consider human-scene interactions together with text conditions, which is crucial for visual and physical realism. This paper focuses on the task of generating human motions in 3D indoor scenes given text descriptions of the human-scene interactions. This task presents challenges due to the multi-modality nature of text, scene, and motion, as well as the need for spatial reasoning. To address these challenges, we propose a new approach that decomposes the complex problem into two more manageable sub-problems: (1) language grounding of the target object and (2) object-centric motion generation. For language grounding of the target object, we leverage the power of large language models. For motion generation, we design an object-centric scene representation for the generative model to focus on the target object, thereby reducing the scene complexity and facilitating the modeling of the relationship between human motions and the object. Experiments demonstrate the better motion quality of our approach compared to baselines and validate our design choices. Code will be available at link.\\n\\n1. Introduction\\n\\nHuman motion generation has been a long-standing problem due to its broad range of applications such as game development, virtual reality, and movie production. Recently, this area witnessed a paradigm shift from avatar animation given rich user input [29] to learning-based motion generation from high-level language prompts, e.g. text descriptions about the desired motion [2, 3, 15, 18, 19, 39, 74, 75]. However, most prior works on text-driven motion synthesis do not consider human-scene interactions [39, 56, 74, 75] while the scene context and physical constraints of the environment largely define the fidelity of the generated human motions.\\n\\nIn this paper, we focus on generating motions from text descriptions in 3D indoor scenes. Specifically, given a 3D scan of the target scene and a text description of a human action that interacts with the scene, we aim to generate natural human motions that are consistent with the text description. This problem presents several challenges, primarily due to the multi-modality nature of text, scene, and human motion. In contrast to previous methods [2, 3, 56, 74, 75] focusing solely on textual descriptions of how human moves, our task also includes texts that additionally describe the spatial details in the given scene (e.g., sit on the armchair near the desk). Therefore, this task requires spatial reasoning.\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2024-996", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ing skills, where the model should build a text-object mapping to locate a specific object in 3D scenes aligned with a natural language description. In addition, the generated motions should also be coherent with scene contexts. As a pioneer work, HUMANISE builds a conditional variational autoencoder (cVAE) with separate encoders of scenes and texts for multi-modality understanding. To enable spatial reasoning ability, they introduce auxiliary tasks of directly regressing object centers to learn 3D visual grounding in an implicit manner. But they do not explicitly utilize the predicted centers and thus the induc-tive bias of visual grounding cannot be fully incorporated. In addition, HUMANISE encodes the entire scene with a single point transformer. Directly generating motions with such a model is challenging, as 3D point clouds are inherently noisy and complex, leading to the inability to locate the target object. As suggested by, not every point of the scene is relevant to the final human motions. Therefore, it is also necessary to develop a more targeted approach that focuses on the relevant parts of the scene to improve the quality of motion generation.\\n\\nTo tackle the problems mentioned above, we propose a novel approach that exploits the power of the large language models (LLMs). Our key idea is to address the challenging task of generating motion in a scene based on textual cues by breaking it down into two smaller problems: (1) language grounding of objects in 3D scenes and (2) generating motions with a focus on the target object. For language grounding, rather than directly learning text-object mapping, we propose to formulate it as question answering and utilize the large prior knowledge of LLMs. Specifically, we first construct scene graphs of 3D scenes and generate their textual descriptions. Then we employ ChatGPT to analyze the relationship between scene descriptions and input instructions and respond with 3D visual grounding answers. Experiments prove the effectiveness of this strategy.\\n\\nFor motion generation, we design an object-centric representation to help the generative model focus on the target object. Specifically, we convert point clouds around the target object into volumetric sensors to build object-centric representation. Then we employ diffusion models to synthesize human motions given object-centric representation and texts. Compared with original scene point clouds which might have various scales, object-centric representation is more compact and robust to scales as objects in the same category are of similar size. Therefore, this representation reduces scene complexity and facilitates the modeling of the relationship between motions and objects.\\n\\nWe conducted thorough comparative and ablation experiments on the HUMANISE dataset. The results demonstrate that our method outperforms the baseline, reflected in more accurate object grounding results and better motions that align with the textual descriptions and scenes. We further show that our approach can generalize to the PROX dataset without any fine-tuning.\\n\\n2. Related work\\n\\n2.1. Motion synthesis\\n\\nDeep learning for motion synthesis. Deep learning methods for motion synthesis have attracted increasing attention in recent years. Various techniques, including MLP, mixture of experts (MoE), and recurrent neural network (RNN), are employed to tackle this task. To generate diverse motion results, previous works explore cVAE, GANs, normalizing flow, and diffusion models.\\n\\nText-driven motion generation. Recently, there has been a growing interest in text-driven motion generation. This task takes natural language as input and synthesizes human motions that align with instructions. KIT-ML is the first benchmark of this task. Some works further annotate the AMASS dataset with action labels and text. To tackle this task, propose to learn a shared latent space for text and motion. employs transformer VAE to generate diverse results. achieves better performance by discrete representation with VQ-VAE. successfully apply diffusion models in this direction. further explore latent diffusion models. Based on, introduce sparse spatial control. also investigate long sequence motion generation. Other existing works leverage Large Language Models in the motion domain. construct compositional motions by combining different body parts with LLM. regard motions as a kind of language and finetune LLMs with motion tokens.\\n\\nScene-aware motion generation. This direction is to generate human motions in a 3D scene. To synthesize walking and sitting motions, construct volumetric sensors to encode object information and the surroundings of the character. extends it to synthesize motions with diverse sitting and lying styles. Furthermore, proposes to control sitting styles by hand contacts. To improve the performance, employs a hierarchical framework that generates goal poses, milestones, and motion sequentially. synthesize whole-body (body and hand fingers) grasping motions with small objects. explore interactions with dynamic objects including manipulation and carrying. explore human-object interactions with physically simulated characters. These works mainly focus on one or two objects while others consider a more complex scene input (e.g., point clouds of the scene including walls and floors). adopt hierarchical frameworks and generate trajectory and...\"}"}
{"id": "CVPR-2024-996", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stage 1: Locating the target object Stage 2: Generating human motions\\n\\n(a) Input scene (b) Located object (c) Human trajectories (d) Human motions\\n\\n\\\"Walk to the desk that is farthest from the laptop.\\\"\\n\\nFigure 2. Overview of our two-stage pipeline. In the first stage, given an input scene and a text description (a), we use ChatGPT to locate the target object (b). In the second stage, human motions are synthesized by first producing human trajectories (c) and then generating local poses (d).\\n\\nText-driven scene-aware motion generation\\n\\nOnly a few works consider text and scenes simultaneously [38, 83, 101]. Although [101] enables text control of sitting styles by [100], the text descriptions in our setting are used to select an object in a cluttered scene. [38] could avoid obstacles during walking while our task needs to handle various actions. The most relevant work to us is HUMANISE [83], which employs a transformer VAE architecture with a two-stream condition module for text and scenes. To accurately localize target objects, they design auxiliary tasks like directly regressing object centers. In contrast to HUMANISE, we propose a two-stage pipeline where we first localize the target object with the help of ChatGPT [52] and then generate human motion using the object-centric representation.\\n\\n2.2. 3D visual grounding.\\n\\nIn recent years, visual grounding in 3D scenes has been explored [1, 11, 76] and also tackled in 3D question answering [13, 89]. Given the point cloud of 3D scenes, this task [1] requires models to locate the target object according to text instructions. Most works follow a two-stage scheme where multiple bounding boxes [9, 66, 102] or segmentation results [31, 90] are first predicted and then selecting the object according to language descriptions [32]. [88] employs 2D semantics. [34] combines bottom-up [47] and top-down [7] detection methods. [48] designs a single-stage pipeline by progressively selecting key points. [20] extends [32] with the help of GPT [6] to generate multi-view text inputs. More recently, [30] proposes a neuro-symbolic framework with large language-to-code models [10]. Most works only localize a single object and [97] could localize a flexible number of objects. Different from previous works which directly handle point clouds or multi-view images, we convert the scene into textual descriptions and leverage large language models to infer the target object. Like [30], in this work, we also leverage large language models for object localization.\\n\\n3. Problem setup and preliminaries\\n\\nIn this section, we discuss the definition of the task and preliminaries. We aim to generate human motion that is consistent with both the text description and the given scene.\\n\\nText descriptions.\\n\\nThe text description follows the template in Sr3D [1] (e.g., \\\"sit on the chair in the center of the desk and bookshelf\\\"). There are four actions (walk, sit, stand up, and lie). The target represents the object that the agents need to interact with, and the anchor objects help to determine the target. A certain type of target furniture category usually has many instances in one scene, while the anchor furniture should be unique. To specify the exact target object, there are five types of spatial relations [1] between the target and the anchor: horizontal proximity, vertical proximity, between, allocentric, and support.\\n\\nScene representations.\\n\\nThe scene is denoted as a point cloud of \\\\( N \\\\) points: \\\\( S \\\\in \\\\mathbb{R}^{N \\\\times 6} \\\\), containing the position and normal direction information of each point.\\n\\nMotion representations.\\n\\nThe output motion sequence is represented as a sequence of SMPL-X [54] body meshes \\\\( M \\\\).\\n\\nSMPL-X is a parametric human body model. In this work, body parameters include body shape parameters \\\\( \\\\beta \\\\in \\\\mathbb{R}^{10} \\\\), global translation \\\\( r \\\\in \\\\mathbb{R}^3 \\\\), 6D global orientation \\\\( \\\\gamma \\\\in \\\\mathbb{R}^6 \\\\), and 6D pose parameters of \\\\( J \\\\) joints \\\\( \\\\theta \\\\in \\\\mathbb{R}^{J \\\\times 6} \\\\) [104]. Following [83], \\\\( \\\\beta \\\\) is treated as a condition to model the effect of body shape, and we omit it for ease of notation. Note that start position and pose are also generated by the model.\\n\\nDiffusion models.\\n\\nThe diffusion [27] is defined as a Markov noising process \\\\( \\\\{ x_t \\\\} \\\\) \\\\( t=0 \\\\) that follows \\\\( q(x_t | x_0) \\\\), where \\\\( x_0 \\\\sim q(x_0) \\\\) is the data and \\\\( x_t \\\\) is the noised data at step \\\\( t \\\\).\"}"}
{"id": "CVPR-2024-996", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Room contains:\\nchair, backpack, monitor, end table, table, ottoman, board, printer.\\n\\nInstruction:\\nYou are an assistant that helps people find objects in a room ...\\n\\nFew-shot examples:\\n{example_1};{example_2}.\\n\\nQuestion:\\nBased on the text, infer the target object and anchor objects. Please answer in the following format ...\\n\\nStage 1: Narrowing down object search space\\n\\nText description:\\nsit on the chair that is in the middle of the board and the end table.\\n\\nObject relations (derived from the simplified scene graph from stage 1):\\nchair 0 is near the end table; chair 2 is far from the board; ...\\nchair 0 is in between of the end table and the board;\\n\\nStage 2: Infering the target object\\n\\nResponse:\\ntarget: chair 0.\\n\\nFigure 3.\\nPipeline of localizing the target object.\\n\\nIn stage 1, given the input text description and detected object bounding boxes (bbx), we construct the first prompt asking ChatGPT the categories of target objects and anchor objects. Based on the response, the scene graph can be simplified. In stage 2, we construct the second prompt with inputs and results from stage 1, including object relations derived from the simplified scene graph. The second prompt is designed for asking ChatGPT to infer the target object. Finally, we can get the target object bounding box from the response of ChatGPT.\\n\\nThe formal definition is:\\n\\n$$q(x_t | x_0) = \\\\mathcal{N}(x_t; \\\\sqrt{\\\\bar{\\\\alpha}_t} x_0, (1 - \\\\bar{\\\\alpha}_t) I), \\\\quad (1)$$\\n\\nwhere $\\\\bar{\\\\alpha}_t$ are constants with monotonically decreasing schedule. When $\\\\bar{\\\\alpha}_t$ is small enough, we can approximate $x_T \\\\sim \\\\mathcal{N}(0, I)$. In our context, we use conditional diffusion models like [64, 75]. The training loss is defined as:\\n\\n$$L = \\\\mathbb{E}_{t \\\\in [1, T], x_0 \\\\sim q(x_0)} [\\\\| x_0 - G(x_t, t, C) \\\\|] , \\\\quad (2)$$\\n\\nwhere $G$ is the generative model, and $C$ is the condition.\\n\\n4. Method\\n\\nThe overview of our method is shown in Fig. 2. In Sec. 4.1, we leverage ChatGPT to localize the target object given a 3D scene and a textual description. Based on the accurate localization, we can focus on the target object and employ an object-centric generation pipeline to separately synthesize trajectories (Sec. 4.2) and motions (Sec. 4.3). Implementation details are discussed in Sec. 4.4.\\n\\n4.1. Language grounding of objects in 3D scenes\\n\\nScene-aware motion generation from textual descriptions requires the scene-understanding ability and build the relationship between scenes and texts. Since the instructions describe how the character moves and interacts with a single target object, the majority of the scene might bear negligible relevance to the final motions [51]. Motivated by this, we propose to locate the target object to identify the most pertinent information. Inspired by the recent progress of LLM [42, 52, 53], ChatGPT [52] is utilized to find the specific objects in the given text. We first obtain textual descriptions of scenes by building scene graphs. Then we feed them with text instructions to ChatGPT with specially designed prompts and parse the response to get target objects.\\n\\nSpatial scene graph extraction.\\n\\nTo utilize LLM, the initial step involves converting a 3D scene into text. This is achieved by building a spatial scene graph. We utilize a pre-trained 3D object detection model in [47] to provide 3D box proposals. Subsequently, we follow the approach of [1] to obtain the relationships between objects. Specifically, for\"}"}
{"id": "CVPR-2024-996", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In ECCV, 2020.\\n\\n[2] Chaitanya Ahuja and Louis-Philippe Morency. Language2pose: Natural language grounded pose forecasting. In 3DV, 2019.\\n\\n[3] Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and G\u00a8ul Varol. TEACH: Temporal Action Compositions for 3D Humans. In 3DV, 2022.\\n\\n[4] Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and G\u00a8ul Varol. SINC: Spatial composition of 3D human motions for simultaneous action generation. ICCV, 2023.\\n\\n[5] German Barquero, Sergio Escalera, and Cristina Palmero. Belfusion: Latent diffusion for behavior-driven human motion prediction. In ICCV, 2023.\\n\\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.\\n\\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\\n\\n[8] Yu-Wei Chao, Jimei Yang, Weifeng Chen, and Jia Deng. Learning to sit: Synthesizing human-chair interactions via hierarchical control. In AAAI, 2021.\\n\\n[9] Dave Zhenyu Chen, Angel X Chang, and Matthias Nie\u00dfner. Scanrefer: 3d object localization in rgb-d scans using natural language. In ECCV, 2020.\\n\\n[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint, 2021.\\n\\n[11] Yixin Chen, Qing Li, Deqian Kong, Yik Lun Kei, Song-Chun Zhu, Tao Gao, Yixin Zhu, and Siyuan Huang. Yourefit: Embodied reference understanding with language and gesture. In ICCV, 2021.\\n\\n[12] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt. Mofusion: A framework for denoising-diffusion-based motion synthesis. In CVPR, 2023.\\n\\n[13] Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Neural Modular Control for Embodied Question Answering. In CoRL, 2018.\\n\\n[14] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Jitendra Malik. Recurrent network models for human dynamics. In ICCV, 2015.\\n\\n[15] Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, and Philipp Slusallek. Synthesis of compositional animations from textual descriptions. In ICCV, 2021.\\n\\n[16] Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, and Philipp Slusallek. Imos: Intent-driven full-body motion synthesis for human-object interactions. In Computer Graphics Forum, 2023.\\n\\n[17] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Action2motion: Conditioned generation of 3d human motions. In ACM MM, 2020.\\n\\n[18] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In CVPR, 2022.\\n\\n[19] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In ECCV, 2022.\\n\\n[20] Zoey Guo, Yiwen Tang, Ray Zhang, Dong Wang, Zhigang Wang, Bin Zhao, and Xuelong Li. Viewrefer: Grasp the multi-view knowledge for 3d visual grounding. In ICCV, 2023.\\n\\n[21] F \u00b4elix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. Robust motion in-betweening. ACM Trans. Graph., 2020.\\n\\n[22] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael J. Black. Resolving 3d human pose ambiguities with 3d scene constraints. In ICCV, 2019.\\n\\n[23] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael J. Black. Stochastic scene-aware motion prediction. In ICCV, 2021.\\n\\n[24] Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, and Xue Bin Peng. Synthesizing physical character-scene interactions. In SIGGRAPH, 2023.\\n\\n[25] Chengan He, Jun Saito, James Zachary, Holly Rushmeier, and Yi Zhou. Nemf: Neural motion fields for kinematic animation. In NeurIPS, 2022.\\n\\n[26] Gustav Eje Henter, Simon Alexanderson, and Jonas Beskow. Moglow: Probabilistic and controllable motion synthesis using normalising flows. ACM Trans. Graph., 2020.\\n\\n[27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.\\n\\n[28] Daniel Holden, Jun Saito, and Taku Komura. A deep learning framework for character motion synthesis and editing. ACM Trans. Graph., 2016.\\n\\n[29] Daniel Holden, Taku Komura, and Jun Saito. Phase-functioned neural networks for character control. ACM Trans. Graph., 2017.\\n\\n[30] Joy Hsu, Jiayuan Mao, and Jiajun Wu. Ns3d: Neuro-symbolic grounding of 3d objects and relations. In CVPR, 2023.\\n\\n[31] Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graph neural networks for referring 3d instance segmentation. AAAI, 2021.\\n\\n[32] Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-view transformer for 3d visual grounding. In CVPR, 2022.\"}"}
{"id": "CVPR-2024-996", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusion-based generation, optimization, and planning in 3D scenes. In CVPR, 2023.\\n\\nAyush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Katherine Fragkiadaki. Bottom up top down detection transformers for language grounding in images and point clouds. In ECCV, 2022.\\n\\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7B. arXiv preprint arXiv:2310.06825, 2023.\\n\\nBiao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as a foreign language. arXiv preprint arXiv:2306.14795, 2023.\\n\\nSai Shashank Kalakonda, Shubh Maheshwari, and Ravi Kiran Sarvadevabhatla. Action-gpt: Leveraging large-scale language models for improved and generalized action generation. In ICME, 2023.\\n\\nKorrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang. Guided motion diffusion for controllable human motion synthesis. In ICCV, 2023.\\n\\nJihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-form language-based motion synthesis & editing. In AAAI, 2023.\\n\\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv e-prints, 2014.\\n\\nDiederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv e-prints, 2013.\\n\\nTakeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022.\\n\\nHanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi, and Xinchao Wang. Priority-centric human motion generation in discrete latent space. In ICCV, 2023.\\n\\nPeizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, and Olga Sorkine-Hornung. Ganimator: Neural motion synthesis from a single sequence. ACM Trans. Graph., 2022.\\n\\nQuanzhou Li, Jingbo Wang, Chen Change Loy, and Bo Dai. Task-oriented human-object interactions generation with implicit neural representations. arXiv preprint, 2023.\\n\\nHung Yu Ling, Fabio Zinno, George Cheng, and Michiel Van De Panne. Character controllers using motion vaes. ACM Trans. Graph., 2020.\\n\\nZe Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong. Group-free 3D object detection via transformers. In ICCV, 2021.\\n\\nJunyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing Ren, Hao Shen, Huaxia Xia, and Si Liu. 3D-SPS: Single-stage 3D visual grounding via referred point progressive selection. In CVPR, 2022.\\n\\nNaureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass: Archive of motion capture as surface shapes. In ICCV, 2019.\\n\\nJulieta Martinez, Michael J. Black, and Javier Romero. On human motion prediction using recurrent neural networks. In CVPR, 2017.\\n\\nAymen Mir, Xavier Puig, Angjoo Kanazawa, and Gerard Pons-Moll. Generating continual human motion in diverse 3D scenes. arXiv preprint, 2023.\\n\\nOpenAI. Openai: Introducing chatgpt. https://openai.com/blog/chatgpt, 2022.\\n\\nOpenAI. GPT-4 technical report, 2023.\\n\\nGeorgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from a single image. In CVPR, 2019.\\n\\nMathis Petrovich, Michael J. Black, and G\u00fcl Varol. Action-conditioned 3D human motion synthesis with transformer VAE. In ICCV, 2021.\\n\\nMathis Petrovich, Michael J. Black, and G\u00fcl Varol. TEMOS: Generating diverse human motions from textual descriptions. In ECCV, 2022.\\n\\nHuaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In ICCV, 2023.\\n\\nMatthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. Big data, 2016.\\n\\nAbhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and Michael J. Black. Babel: Bodies, action and behavior with English labels. In CVPR, 2021.\\n\\nYijun Qian, Jack Urbanek, Alexander G Hauptmann, and Jungdam Won. Breaking the limits of text-conditioned 3D motion synthesis with elaborative descriptions. In ICCV, 2023.\\n\\nSigal Raab, Inbal Leibovitch, Peizhuo Li, Kfir Aberman, Olga Sorkine-Hornung, and Daniel Cohen-Or. Modi: Unconditional motion synthesis from diverse data. In CVPR, 2023.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\\n\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv e-prints, 2022.\\n\\nAli Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In NeurIPS, 2019.\\n\\nJunha Roh, Karthik Desingh, Ali Farhadi, and Dieter Fox. Languagerefer: Spatial-language model for 3D visual grounding. In CoRL, 2022.\"}"}
{"id": "CVPR-2024-996", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.\\n\\nYonatan Shafir, Guy Tevet, Roy Kapon, and Amit Haim Bermano. Human motion diffusion as a generative prior. arXiv preprint, 2023.\\n\\nKihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In NeurIPS, 2015.\\n\\nSebastian Starke, He Zhang, Taku Komura, and Jun Saito. Neural state machine for character-scene interactions. ACM Trans. Graph., 2019.\\n\\nSebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Zamir. Local motion phases for learning multi-contact character movements. ACM Trans. Graph., 2020.\\n\\nSebastian Starke, Ian Mason, and Taku Komura. Deep-phase: Periodic autoencoders for learning motion phase manifolds. ACM Trans. Graph., 2022.\\n\\nOmid Taheri, Vasileios Choutas, Michael J. Black, and Dimitrios Tzionas. Goal: Generating 4d whole-body motion for hand-object grasping. In CVPR, 2022.\\n\\nGuy Tevet, Brian Gordon, Amir Hertz, Amit Haim Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In ECCV, 2022.\\n\\nGuy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In ICLR, 2023.\\n\\nJesse Thomason, Mohit Shridhar, Yonatan Bisk, Chris Paxton, and Luke Zettlemoyer. Language grounding with 3d objects. In CoRL, 2022.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint, 2023.\\n\\nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2017.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\\n\\nJiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiaolong Wang. Synthesizing long-term 3d human motion and interaction in 3d scenes. In CVPR, 2021.\\n\\nJingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin, and Bo Dai. Towards diverse and natural scene-aware 3d human motion synthesis. In CVPR, 2022.\\n\\nXi Wang, Gen Li, Yen-Ling Kuo, Muhammed Kocabas, Emre Aksan, and Otmar Hilliges. Reconstructing action-conditioned human-object interactions using commonsense knowledge priors. In 3DV, 2022.\\n\\nZan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. Humanise: Language-conditioned human motion generation in 3d scenes. In NeurIPS, 2022.\\n\\nYan Wu, Jiahao Wang, Yan Zhang, Siwei Zhang, Otmar Hilliges, Fisher Yu, and Siyu Tang. Saga: Stochastic whole-body grasping with contact. In ECCV, 2022.\\n\\nYiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. Omnicontrol: Control any joint at any time for human motion generation. arXiv preprint, 2023.\\n\\nSirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan Gui. Interdiff: Generating 3d human-object interactions with physics-informed diffusion. In ICCV, 2023.\\n\\nSijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for skeleton-based action recognition. In AAAI, 2018.\\n\\nZhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo Luo. Sat: 2d semantics assisted training for 3d visual grounding. In ICCV, pages 1856\u20131866, 2021.\\n\\nShuquan Ye, Dongdong Chen, Songfang Han, and Jing Liao. 3d question answering. IEEE Transactions on Visualization and Computer Graphics, 2022.\\n\\nZhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng Wang, Zhen Li, and Shuguang Cui. Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. In ICCV, 2021.\\n\\nHe Zhang, Sebastian Starke, Taku Komura, and Jun Saito. Mode-adaptive neural networks for quadruped motion control. ACM Trans. Graph., 2018.\\n\\nHe Zhang, Yuting Ye, Takaaki Shiratori, and Taku Komura. Manipnet: Neural manipulation synthesis with a hand-object spatial representation. ACM Trans. Graph., 2021.\\n\\nJianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Generating human motion from textual descriptions with discrete representations. In CVPR, 2023.\\n\\nMingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint, 2022.\\n\\nXiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Vladimir Guzov, and Gerard Pons-Moll. Couch: Towards controllable human-chair interactions. In ECCV, 2022.\\n\\nYan Zhang and Siyu Tang. The wanderings of odysseus in 3d scenes. In CVPR, 2022.\\n\\nYiming Zhang, ZeMing Gong, and Angel X Chang. Multi3drefer: Grounding text description to multiple 3d objects. In ICCV, 2023.\\n\\nYaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. arXiv preprint, 2023.\\n\\nHengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In ICCV, 2021.\\n\\nKaifeng Zhao, Shaofei Wang, Yan Zhang, Thabo Beeler, and Siyu Tang. Compositional human-scene interaction synthesis with semantic control. In ECCV, 2022.\\n\\nKaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, and Siyu Tang. Synthesizing diverse human motions in 3d indoor scenes. In ICCV, 2023.\"}"}
{"id": "CVPR-2024-996", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-transformer: Relation modeling for visual grounding on point clouds. In ICCV, 2021.\\n\\nYang Zheng, Yanchao Yang, Kaichun Mo, Jiaman Li, Tao Yu, Yebin Liu, C Karen Liu, and Leonidas J Guibas. Gimo: Gaze-informed human motion prediction in context. In ECCV, 2022.\\n\\nYi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, and Li Hao. On the continuity of rotation representations in neural networks. In CVPR, 2019.\"}"}
