{"id": "CVPR-2023-224", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. arXiv preprint arXiv:1705.10823, 2017.\\n\\n[2] Moshe Bar, Karim S Kassam, Avniel Singh Ghuman, Jasmine Boshyan, Annette M Schmid, Anders M Dale, Matti S H\u00e4m\u00e4l\u00e4inen, Ksenija Marinkovic, Daniel L Schacter, Bruce R Rosen, et al. Top-down facilitation of visual recognition. Proceedings of the National Academy of Sciences, 103(2):449\u2013454, 2006.\\n\\n[3] Lukas Biewald. Experiment tracking with weights and biases, 2020. Software available from wandb.com.\\n\\n[4] Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, et al. Accounting for variance in machine learning benchmarks. Proceedings of Machine Learning and Systems, 3:747\u2013769, 2021.\\n\\n[5] Janez Dem\u0161ar. Statistical comparisons of classifiers over multiple data sets. The Journal of Machine Learning Research, 7:1\u201330, 2006.\\n\\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.\\n\\n[7] James J DiCarlo, Davide Zoccolan, and Nicole C Rust. How does the brain solve visual object recognition? Neuron, 73(3):415\u2013434, 2012.\\n\\n[8] Rotem Dror, Gili Baumer, Marina Bogomolov, and Roi Reichart. Replicability analysis for natural language processing: Testing significance with multiple datasets. Transactions of the Association for Computational Linguistics, 5:471\u2013486, 2017.\\n\\n[9] Ronald Aylmer Fisher. Statistical methods for research workers. In Breakthroughs in Statistics, pages 66\u201370. Springer, 1992.\\n\\n[10] Milton Friedman. The use of ranks to avoid the assumption of normality implicit in the analysis of variance. Journal of the American Statistical Association, 32(200):675\u2013701, 1937.\\n\\n[11] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Texture and art with deep neural networks. Current Opinion in Neurobiology, 46:178\u2013186, 2017.\\n\\n[12] Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Partial success in closing the gap between human and machine vision. NeurIPS, 34, 2021.\\n\\n[13] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.\\n\\n[14] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020.\\n\\n[15] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021.\\n\\n[16] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. CVPR, 2021.\\n\\n[17] Katherine Hermann, Ting Chen, and Simon Kornblith. The origins and prevalence of texture bias in convolutional neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, NeurIPS, 2020.\\n\\n[18] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017.\\n\\n[19] Ronald L Iman and James M Davenport. Approximations of the critical region of the Friedman statistic. Communications in Statistics-Theory and Methods, 9(6):571\u2013595, 1980.\\n\\n[20] Seogkyu Jeon, Kibeom Hong, Pilhyeon Lee, Jewook Lee, and Hyeran Byun. Feature stylization and domain-aware contrastive learning for domain generalization. In ACM International Conference on Multimedia, pages 22\u201331, 2021.\\n\\n[21] Nikolai Kalischek, Jan D Wegner, and Konrad Schindler. In the light of feature distributions: moment matching for neural style transfer. In CVPR, 2021.\\n\\n[22] Juwon Kang, Sohyun Lee, Namyup Kim, and Suha Kwak. Style neophile: Constantly seeking novel styles for domain generalization. In CVPR, 2022.\\n\\n[23] Nikolaus Kriegeskorte. Deep neural networks: a new framework for modeling biological vision and brain information processing. Annual Review of Vision Science, 1:417\u2013446, 2015.\\n\\n[24] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015.\\n\\n[25] Erich Leo Lehmann. Testing statistical hypotheses, volume 2. Springer, 1986.\\n\\n[26] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artistier domain generalization. In ICCV, 2017.\\n\\n[27] Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, and Cihang Xie. Shape-texture debiased neural network training. arXiv preprint arXiv:2010.05981, 2020.\\n\\n[28] Nikos K Logothetis and David L Sheinberg. Visual object recognition. Annual Review of Neuroscience, 19(1):577\u2013621, 1996.\\n\\n[29] Wes McKinney. Data structures for statistical computing in python. In Python in Science Conference, 2010.\\n\\n[30] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, 2021.\\n\\n[31] Peter Bjorn Nemenyi. Distribution-free multiple comparisons. Princeton University, 1963.\\n\\n[32] Jerzy Neyman and Egon S Pearson. On the use and interpretation of certain test criteria for purposes of statistical inference: Part i. Biometrika, pages 175\u2013240, 1928.\"}"}
{"id": "CVPR-2023-224", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Oren Nuriel, Sagie Benaim, and Lior Wolf. Permuted adain: Reducing the bias towards global statistics in image classification. In CVPR, 2021.\\n\\nThe pandas development team. pandas-dev/pandas: Pandas, Feb. 2020.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 32, 2019.\\n\\nBaifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, and Jingdong Wang. Informative dropout for robust representation learning: A shape-bias perspective. In ICML, 2020.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\\n\\nVladimir N Vapnik. An overview of statistical learning theory. IEEE Transactions on Neural Networks, 10(5):988\u2013999, 1999.\\n\\nJacques Wainer. A Bayesian Bradley-Terry model to compare multiple ML algorithms on multiple data sets. arXiv preprint arXiv:2208.04935, 2022.\\n\\nYue Wang, Lei Qi, Yinghuan Shi, and Yang Gao. Feature-based style randomization for domain generalization. IEEE Transactions on Circuits and Systems for Video Technology, 2022.\\n\\nGeoffrey I Webb. Multiboosting: A technique for combining boosting and wagging. Machine Learning, 40(2):159\u2013196, 2000.\\n\\nYuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289\u2013315, 2007.\\n\\nTong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. Annals of Statistics, 33(4):1538\u20131579, 2005.\"}"}
{"id": "CVPR-2023-224", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiasBed \u2013 Rigorous Texture Bias Evaluation\\n\\nNikolai Kalischek\\nRodrigo Caye Daudt\\nTorben Peters\\nReinhard Furrer\\nJan D. Wegner\\nKonrad Schindler\\n\\nPhotogrammetry and Remote Sensing, ETH Z\u00fcrich\\nInstitute for Mathematics, University of Zurich\\nInstitute for Computational Science, University of Zurich\\n\\nAbstract\\nThe well-documented presence of texture bias in modern convolutional neural networks has led to a plethora of algorithms that promote an emphasis on shape cues, often to support generalization to new domains. Yet, common datasets, benchmarks and general model selection strategies are missing, and there is no agreed, rigorous evaluation protocol. In this paper, we investigate difficulties and limitations when training networks with reduced texture bias. In particular, we also show that proper evaluation and meaningful comparisons between methods are not trivial. We introduce BiasBed, a testbed for texture- and style-biased training, including multiple datasets and a range of existing algorithms. It comes with an extensive evaluation protocol that includes rigorous hypothesis testing to gauge the significance of the results, despite the considerable training instability of some style bias methods. Our extensive experiments, shed new light on the need for careful, statistically founded evaluation protocols for style bias (and beyond). E.g., we find that some algorithms proposed in the literature do not significantly mitigate the impact of style bias at all. With the release of BiasBed, we hope to foster a common understanding of consistent and meaningful comparisons, and consequently faster progress towards learning methods free of texture bias. Code is available at https://github.com/D1noFuzi/BiasBed\\n\\n1. Introduction\\nVisual object recognition is fundamental to our daily lives. Identifying and categorizing objects in the environment is essential for human survival, indeed our brain is able to assign the correct object class within a fraction of a second, independent of substantial variations in appearance, illumination and occlusion [28]. Recognition mainly takes place along the ventral pathway [7], i.e., visual perception induces a hierarchical processing stream from local patterns to complex features, in feed-forward fashion [28]. Signals are filtered to low frequency and used in parallel also in a top-down manner [2], emphasizing the robustness and invariance to deviations in appearance.\\n\\nInspired by our brain's visual perception, convolutional neural architectures build upon the hierarchical intuition, stacking multiple convolutional layers to induce feed-forward learning of basic concepts to compositions of complex objects. Indeed, early findings suggested that neurons in deeper layers are activated by increasingly complex shapes, while the first layers are mainly tailored towards low-level features such as color, texture and basic geometry. However, recent work indicates the opposite: convolutional neural networks (CNNs) exhibit a strong bias to base their decision on texture cues [11\u201313, 17], which heavily influences their performance, in particular under domain shifts, which typically affect local texture more than global shape.\\n\\nThis inherent texture bias has led to a considerable body of work that tries to minimize texture and style bias and shift towards \u201chuman-like\u201d shape bias in object recognition. Common to all approaches is the principle of incorporating adversarial texture cues into the learning pipeline \u2013 either directly in input space [13, 15, 27] or implicitly in feature space [20,22,30,33]. Perturbing the texture cues in the input forces the neural network to make \u201ctexture-free\u201d decisions, thus focusing on the objects' shapes that remain stable during training. While texture cues certainly boost performance in fine-grained classification tasks, where local texture patterns may indicate different classes, they can cause serious harm when the test data exhibit a domain shift w.r.t. training. In this light, texture bias can be seen as a main reason for degrading domain generalization performance, and various algorithms have been developed to improve generalization to new domains with different texture properties (respectively image styles), e.g., [13, 15, 20, 22, 27, 30, 33, 40].\\n\\nHowever, while a considerable number of algorithms have been proposed to address texture bias, they are neither evaluated on common datasets nor with common evaluation metrics. Moreover they often employ inconsistent model selection criteria or do not report at all how model selection...\"}"}
{"id": "CVPR-2023-224", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is done. With the present paper we promote the view that:\\n\\n\u2022 the large domain shifts induced by texture-biased train-\\ning cause large fluctuations in accuracy, which call for\\na particularly rigorous evaluation;\\n\\n\u2022 model selection has so far been ignored in the litera-\\nture; together with the high training volatility, this may\\nhave lead to overly optimistic conclusions from spuri-\\nous results;\\n\\n\u2022 in light of the difficulties of operating under drastic do-\\nmain shifts, experimental results should include a no-\\ntion of uncertainty in the evaluation metrics.\\n\\nMotivated by these findings, we have implemented\\nBias-\\nBed, an open-source PyTorch \\\\cite{35} testbed that comes with\\nsix datasets of different texture and shape biases, four ad-\\nversarial robustness datasets and eight fully implemented\\nalgorithms. Our framework allows the addition of new al-\\ngorithms and datasets with few lines of code, including full\\nflexibility of all parameter settings. In order to tackle the\\npreviously discussed limitations and difficulties for evalu-\\nating such algorithms, we have added a carefully designed\\nevaluation pipeline that includes training multiple runs and\\nemploying multiple model selection methods, and we re-\\nport all results based on sound statistical hypothesis tests\\n\u2013 all run with a single command. In addition to our novel\\nframework, the present paper makes the following contribu-\\ntions:\\n\\n\u2022 We highlight shortcomings in the evaluation protocols\\nused in recent work on style bias, including the ob-\\nservation that there is a very high variance in the per-\\nformance of different runs of the same algorithm, and\\neven between different checkpoints in the same run\\nwith similar validation scores.\\n\\n\u2022 We develop and openly release a testbed that rig-\\norously compares different algorithms using well-\\nestablished hypothesis testing methods. This testbed\\nincludes several of the most prominent algorithms and\\ndatasets in the field, and is easily extensible.\\n\\n\u2022 We observe in our results that current algorithms on\\ntexture-bias datasets fail to surpass simple ERM in a\\nstatistically significant way, which is the main moti-\\nvation for this work and for using rigorous hypothesis\\ntests for evaluating style bias algorithms.\\n\\nIn Sec. 2, we provide a comprehensive overview of exist-\\ning work on reducing texture bias. Furthermore we describe\\nthe main forms of statistical hypothesis testing. We con-\\ntinue in Sec. 3 with a formal introduction to biased learning,\\nand systematically group existing algorithms into families\\nwith common properties. In Sec. 4, we investigate previous\\nevaluation practices in detail, discuss their limitations, and\\ngive a formal introduction to hypothesis testing. Sec. 5 de-\\nscribes our proposed BiasBed evaluation framework in de-\\ntail, while Sec. 6 presents experiments, followed by a dis-\\ncussion (Sec. 7), conclusions (Sec. 8) and a view towards\\nthe broader impact of our work (Sec. 9).\\n\\n2. Related work\\nTexture and style bias.\\n\\nThe seminal work of Geirhos et\\nal. \\\\cite{13} showed that modern neural networks are heavily\\nbiased towards local patterns, commonly referred to as tex-\\nture or style.\\\\footnote{In this paper we prefer the word \\\"texture\\\", but interchangeably use \\\"style\\\" as part of already set names and expressions, as in \\\"style transfer.\\\"} The finding that, contrary to earlier hypothe-\\nses \\\\cite{23, 24}, the network output is dominated by domain-\\nspecific texture rather than generic, large-scale shape cues\\nhelps to explain their poor ability to generalize to unseen\\nimage domains. As a consequence, several authors have\\ntried to address the issue under the umbrella term\\nDomain Generalization \\\\cite{20, 22, 30}, although\\ntexture debias-\\ning would be a more accurate description of their approach:\\nthey take advantage of neural style transfer ideas \\\\cite{11, 21}\\nand alter the texture features, thereby forcing the model to\\nrely more on shape features. To that end, Geirhos\\net al. \\\\cite{13} put forward a new\\nstylized ImageNet dataset. That dataset\\ncontains images that are blended with a random texture im-\\ngage to generate a diverse set of \\\"texture-independent\\\" sam-\\nples, such that the contained texture is not informative about\\nthe class label anymore. However, enforcing shape bias\\ntends to deteriorate performance on the source domain,\\n\\\\textit{i.e.} the domain trained on, \\\\textit{e.g.} ImageNet. To balance in-domain\\nand out-of-domain (OOD) performance, Li\\net al. \\\\cite{27} pro-\\npose debiased learning by linearly interpolating the target\\nlabels between the texture class and the content class.\\n\\nInspired by neural style transfer, a whole palette of work\\n\\\\cite{20, 22, 30, 33, 40} deals with changing the texture statistics\\nin feature space. The authors of \\\\cite{33} randomly swap mean\\nand standard deviation along the spatial dimension in cer-\\ntain network layers, while \\\\cite{40} add learnable encoders and\\nnoise variables for each statistic to align them over different\\nlayers. In \\\\cite{30} an additional adversarial content randomiza-\\ntion block tricks the network into predicting the right style\\ndespite changed content. Jeon\\net al. \\\\cite{20} sample new style\\nvectors by defining Gaussians over the channel dimension.\\n\\nEvaluation frameworks.\\n\\nComparing algorithms based\\non some relevant quantitative performance metric has be-\\ncome the norm in the deep learning community. A common\\nstrategy for domain generalization, which by definition re-\\nquires two or more different datasets, is to evaluate each\\nalgorithm on the held-out testing portion of every dataset.\\nFrom the outcomes it is typically concluded that methods\\nwhich, on average, perform better is superior. To account\"}"}
{"id": "CVPR-2023-224", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Samples of datasets used in our experiments. These datasets (a-f) aim to capture different facets of what is informally known as \u201ctexture bias\u201d. They attempt to decouple large scale features, such as image structure and object shapes, from small scale texture features. While humans can easily recognize the objects in these images in most cases, convolutional neural networks often struggle to do so with the same accuracy that they achieve with natural images; samples from (g) and (h) can be used to test for adversarial robustness.\\n\\nfor variance in learning procedures [4], a test bed similar to ours [14] trains multiple models per algorithm and uses a mean of means $\\\\mu_A$ over different datasets as the final metric, i.e., method A is considered better than method B if $\\\\mu_A > \\\\mu_B$. Clearly, this conclusion is not necessarily justified from a statistical viewpoint [8], as it does not distinguish between true impact and random effects [4].\\n\\nIn fact, statistical testing offers a formal theory for comparing multiple algorithms over multiple datasets [32], a tool that is routinely used in scientific fields like physics or medicine. Hypothesis tests are employed to answer the question whether two (or more) algorithms differ significantly, given performance estimates on one or more datasets. Depending on assumptions about the scores, the comparison may require a parametric or a non-parametric test. In machine learning (ML) the requirements for parametric tests like the repeated-measures ANOVA [9] are typically not met, e.g., scores may not be normally and spherically distributed [5]. Non-parametric tests like the Friedman test [10] require fewer assumptions and are more generally applicable. A recent line of work [39] proposed a Bayesian form of the Bradley-Terry model to determine how confident one can be that one algorithm is better than another.\\n\\n3. Biased learning\\nShape-biased learning tries to minimize the error that stems from distribution shifts between the training and test domains. Formally, we train a neural network $g$ on a training dataset $\\\\{ (x_i, y_i) \\\\}_{i \\\\in [n]}$, where the value $x_i \\\\in X$ of a random variable $X$ is the input to the network and $y_i \\\\in Y$ of a random variable $Y$ is the corresponding ground truth. Let $P(X)$ be the source domain distribution. In shape-biased learning however, we test on samples $x \\\\in X'$ such that $P(X) \\\\neq P(X')$. In our setting the distribution shift mainly results from changes in texture, e.g., going from photos to cartoons or sketches. Shape-biased learning is a special case of domain generalization [26], which also encompasses distribution shifts due to factors other than texture.\\n\\nExisting literature can be grouped into two families, input-augmented [13,15,27] and feature-augmented [20,22,30,33,40] methods. The former create style-randomized versions of the training data, whereas the latter apply stylization to the latent representations inside a neural network. The methodology to transfer style remains the same for both. Given two images $I, I' \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 3}$ and a pretrained network (e.g., VGG-19 [37]), feature statistics at suitable network layers are computed for both images. The most common approach is to use first and second moments as in [18]. Let $F_l(x) \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C_l}$ be the $l$-th feature map with input $x$, $\\\\mu_{F_l}(x) \\\\in \\\\mathbb{R}^{C_l}$ its channel-wise mean (averaged over $H \\\\times W$) and $\\\\sigma_{F_l}(x) \\\\in \\\\mathbb{R}^{C_l}$ its channel-wise standard deviation.\"}"}
{"id": "CVPR-2023-224", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"aged across the spatial dimensions) and\\n\\\\[ \\\\sigma_F l(x) \\\\in \\\\mathbb{R} \\\\]\\nthe corresponding standard deviation. Then style transfer boils down to [18]\\n\\\\[ F_{\\\\text{new}}(x) = \\\\sigma_F l(x) - \\\\mu_{F l}(x) \\\\sigma_F l(x) + \\\\mu_{F l}(x), \\\\quad (1) \\\\]\\nwhere \\\\( x, x' \\\\) are two different samples. In case of input augmentation, the (stylized) encodings are propagated through a decoder to generate images, either offline [13] or on the fly [27]. This process is often applied to features at various representation levels to capture textures at different scales. Feature augmentation methods swap the statistics within their classification network and directly output the corresponding class predictions.\\n\\n4. Evaluation practices\\n\\nWe first give an overview of current evaluation practices and point out limitations when moving out of domain. Then follows an introduction to formal hypothesis testing.\\n\\n4.1. Current practice\\n\\nExisting literature about biased learning, and also about domain generalization in a broader sense, shares common evaluation practices. Algorithms are scored on different OOD datasets in terms of an evaluation metric, usually classification accuracy, and simply averaged across datasets. From a statistical perspective, this seemingly obvious practice has several deficiencies.\\n\\nFirst, and most importantly, often only a single point estimate of the metric is reported. However, deep learning algorithms are trained in highly stochastic fashion. Performance varies between independently trained models, since they almost certainly correspond to different local minima of the loss function. This effect is often particularly strong in the presence of domain shifts (cf. Fig. 2). It is evident that best practice would be to, at the very least, train multiple models with the same data and report their mean accuracy and its standard deviation. In the context of texture bias this becomes all the more important, as the variability between (and also within) training runs tends to be high.\\n\\nSecond, neural network training is an iterative process without a natural, unambiguous stopping criterion. Which iteration to regard as the \u201cfinal\u201d model to be evaluated is therefore invariably based on some model selection rule. Common strategies are to pick the one with the best in-domain validation score, to use a fixed iteration count, or to declare convergence based on some early-stopping rule [1, 42, 43]. Looking at the texture and shape literature, we find a lack of information about the chosen strategy. This makes a fair comparison all but impossible: texture bias schemes tend to exhibit unusually high fluctuations between training epochs, such that a different model selection rule may reverse the relative performance of two methods. This concerns all tested methods and various datasets (see Fig. 2 and Tab. 1). In this regard, we emphasize the efforts of [14], who defined a set of strategies to account for performance fluctuations and encourage multiple training runs to collect the necessary statistics for fair comparisons over numerous datasets. A diverse spectrum of image domains helps to mitigate over-fitting towards particular domains and favours generic mechanisms over dataset-specific heuristics. To obtain statistically sound conclusions it is, however, not enough to simply average scores over different datasets, as done in [14]. Naturally, different datasets possess different characteristics and as such are potentially not commensurable, so that simple averaging becomes meaningless [41]. We instead propose a rigorous, statistically founded hypothesis testing framework to compensate for the differences.\\n\\n4.2. Hypothesis testing\\n\\nThis section serves as background for our experiments. We emphasize that it summarizes well-established textbook knowledge, e.g. [25]. The general aim is to compare algorithms \\\\( a_1, a_2, \\\\ldots, a_n \\\\) on \\\\( m \\\\) datasets \\\\( d_1, d_2, \\\\ldots, d_m \\\\). We are interested in comparing the algorithms based on an evaluation metric \\\\( M \\\\), where \\\\( c_{ij} = M(a_i, d_j) \\\\) is the (averaged) score of algorithm \\\\( a_i \\\\) on dataset \\\\( d_j \\\\) (e.g., accuracy). Based on \\\\( c_{ij} \\\\), we want to decide if the algorithms are statistically different [5]. We assume that the underlying metric can be expressed additively by an effect due to the algorithm and an effect due to the dataset, and \\\\( c_{ij} \\\\) is an estimate of this underlying metric. We define the null hypothesis and alternative hypothesis as following:\\n\\n\\\\[ H_0: \\\\forall ik \\\\gamma(a_i) = \\\\gamma(a_k) \\\\]\\n\\\\[ H_1: \\\\exists ik \\\\gamma(a_i) \\\\neq \\\\gamma(a_k), \\\\quad (2) \\\\]\\nwhere \\\\( \\\\gamma(a_i) \\\\) is the effect of algorithm \\\\( a_i \\\\). In case of \\\\( n = 2 \\\\) and \\\\( m = 1 \\\\), we have a two-sample setting and with a little abuse of notation, we can write \\\\( d = M(a_1, d) - M(a_2, d) \\\\) to test \\\\( H_0: \\\\gamma(a_1) = \\\\gamma(a_2) \\\\) versus \\\\( H_1: \\\\gamma(a_1) \\\\neq \\\\gamma(a_2) \\\\).\\n\\nThe decision whether to reject the null hypothesis is based on the p-value, which is defined as the probability of obtaining a more extreme result than the one observed, assuming that \\\\( H_0 \\\\) is true:\\n\\n\\\\[ p = P(D > d | H_0) + P(D < d | H_0), \\\\quad (3) \\\\]\\nwhere \\\\( D \\\\) is the test statistic associated to our observed difference \\\\( d \\\\). Beforehand, we must choose a (user-defined) significance level \\\\( \\\\alpha \\\\) and reject \\\\( H_0 \\\\) if \\\\( p < \\\\alpha \\\\). The setup gives rise to two (inevitable) error types: Type I error when rejecting \\\\( H_0 \\\\) although it is true, and Type II error when not rejecting \\\\( H_0 \\\\) although it is false. Here, the Type I error probability is equal to \\\\( \\\\alpha \\\\).\\n\\nGenerally, comparing multiple algorithms on multiple datasets follows a two-step procedure. First, an omnibus test is performed (e.g., ANOVA) to check for differences across all algorithms or datasets. If significant differences are found, pairwise comparisons are conducted to identify specific differences. This approach allows for a more detailed understanding of the performance characteristics of each algorithm and dataset, enabling researchers to make informed decisions about which methods are most suitable for particular tasks or domains.\"}"}
{"id": "CVPR-2023-224", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Accuracies of four different methods on three datasets. For each method, ten independent instances are trained and evaluated after every epoch. Lines denote average performance across the ten runs, shaded areas denote their standard deviation, highlighting significant performance fluctuations that should be taken into account.\\n\\nA test is applied to the hypothesis of all algorithms performing equally well, i.e., $H_0$ cannot be rejected. If $H_0$ can be rejected, a follow-up post-hoc test inspects each pair of algorithms to pinpoint where differences exist. A recommended omnibus test in the context of ML is the Friedman test. It ranks all algorithms separately in each domain (e.g., dataset), and uses the average rank $R_i$ to calculate the Friedman statistic:\\n\\n$$\\\\chi^2_F = \\\\frac{12}{mn(n+1)} \\\\sum_{i=1}^n R_i^2 - \\\\frac{n(n+1)^2}{4},$$\\n\\n(4)\\n\\nwhich follows approximately a $\\\\chi^2$ distribution with $(n-1)$ degrees of freedom. The Iman-Davenport extension to the Friedman statistic compensates for too conservative decisions and is defined as\\n\\n$$F_F = \\\\left(\\\\frac{m-1}{m} \\\\chi^2_F \\\\right) \\\\frac{m(n-1)}{(n-1)(m-1)} - \\\\chi^2_F,$$\\n\\n(5)\\n\\nwhich is approximately distributed according to an $F$-distribution with $n-1$ and $(m-1)(n-1)$ degrees of freedom. If the $p$-value is below the pre-defined significance threshold $\\\\alpha$, the Nemenyi post-hoc test compares pairs of differences. The Nemenyi test statistic for algorithms $a_i$ and $a_j$ is defined as\\n\\n$$z = \\\\frac{|R_i - R_j|}{r \\\\frac{m(n+1)}{6}},$$\\n\\n(6)\\n\\nFor large values of $z$ the difference is significant. Note that the two-step procedure is necessary to maintain the overall Type I error probability, which is not the case when testing all pairs within a two-sample framework at level $\\\\alpha$.\\n\\n5. BiasBed framework\\n\\nWe introduce BiasBed, a highly flexible and generic test suite to implement, train and evaluate existing algorithms in a rigorous manner. We currently include seven texture biasing algorithms, ten datasets, different model selection strategies and an omnibus hypothesis test and a post-hoc hypothesis test. Adding existing and new algorithms, other datasets or running a hyperparameter search for one algorithm is a matter of a few lines. In fact, our framework provides a full integration with Weights and Biases [3] providing the full range of logging, visualization and reports.\\n\\nAlgorithms. BiasBed currently supports several algorithms that are tailored towards shape biased learning. Our baseline algorithm is a plain ResNet-50 trained with standard empirical risk minimization [38]. The same algorithm can be trained on Stylized ImageNet to duplicate the approach by Geirhos et al. [13]. Additional algorithms are: Shape-Texture Debiased Neural Network (Debiased, [27]), Style-Agnostic Network (SagNet, [30]), Permuted Adaptive Instance Normalization (pAdaIN, [33]), Informative Dropout (InfoDrop, [36]) and Deep Augmentation (DeepAug, [15]). We show in the Appendix how to easily add more algorithms besides the ones listed here.\\n\\nDatasets. Besides the standard dataloader for ImageNet1K [6], BiasBed contains dataloaders for five additional texture-bias datasets and four robustness datasets. Stylized ImageNet [13] and Cue-Conflict [12] are stylized versions of ImageNet (1000 classes) and 16-class ImageNet, respectively, where the latter particularly uses texture cues for stylization. Datasets that contain pure shape information are Edges, Sketches and Silhouettes from [12]. We further extend BiasBed by including the following robustness datasets: ImageNet-A [16], ImageNet-R [15] and two versions of DeepAug [15].\"}"}
{"id": "CVPR-2023-224", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Model selection.\\n\\nIn Section 4, we have seen that when testing on out-of-distribution datasets, where performance fluctuations during training are rather high, the criterion used to pick the final model can have severe impacts on the final quality metrics. Therefore, we have implemented three model selection methods in BiasBed to unify evaluation.\\n\\n\u2022 Best-epoch is an oracle method that chooses the best test set score over all epochs after training for a fixed number of epochs. Importantly, this method is cherry-picking and should be discouraged to use. We include it here to highlight the severe effects of model selection on evaluation.\\n\\n\u2022 Last-n-epochs averages the test set scores over the last n epochs, to mitigate the fluctuations observed in the OOD regime. I.e., once the training has converged the model is evaluated after every training epoch, and a representative average score is reported.\\n\\n\u2022 Best-training-validation chooses a single test set score according that achieves the best in-domain performance on the validation set. This method is the most rigorous one, but leads to the highest variance between independent test runs. To decrease that variance, more runs are needed.\\n\\nEvaluation.\\n\\nBiasBed includes a full, rigorous evaluation protocol that can be run with a single command. It collects the result according to the defined model selection method, algorithms and datasets, and computes the averaged scores. In a second step, the Friedman test with Iman-Davenport extension [19] is applied with a possible post-hoc Nemenyi test to identify significant differences. All results are reported back in a Pandas dataframe [29, 34] or optionally in LATEX tables (as those in Section 6).\\n\\n6. Experiments\\n\\nWe run experiments for all implemented algorithms and extensively compare different model selection methods, highlighting the need for a common, explicit protocol. We use hypothesis testing to properly compare across algorithms. The following results are all generated by running a single command for each algorithm in BiasBed.\\n\\n6.1. Model selection\\n\\nWe quantitatively elaborate on the findings of Fig. 2. We train each algorithm ten times and test on Silhouette, Edge and Cue-Conflict after every epoch during training. In Table 2 we report the best accuracy over all epochs averaged across runs versus the last score received, averaged across all runs. Clearly, all algorithms drop severely in performance when one does not use an oracle model selection method, highlighting the need to properly define criteria how to select the model to be used. We emphasize that this step should always be part of an evaluation.\\n\\n| Algorithm | Silhouette | Edge | Cue-Conflict |\\n|-----------|------------|------|--------------|\\n| ERM       | 50.8 / 47.3 | 32.6 / 22.6 | 26.4 / 22.2 |\\n| SagNet    | 48.1 / 43.3 | 32.7 / 25.3 | 21.3 / 20.0 |\\n| pAdaIN    | 48.0 / 43.8 | 29.0 / 22.3 | 24.0 / 21.5 |\\n| InfoDrop  | 51.2 / 47.8 | 31.7 / 19.0 | 26.6 / 22.8 |\\n\\nTable 1. Comparison of best epoch (oracle) vs checkpoint with best validation accuracy for selected algorithms and datasets.\\n\\n6.2. Results\\n\\nEvaluation. We report results choosing the best validation performance for all algorithms and datasets in Table 2 and choosing the last 30 epochs in Table 3. All results are once more gathered across multiple independent runs to account for randomness in training. In particular, we do not include an average column across dataset scores per algorithm, as discussed in Section 4. Note that in three cases data used for training was augmented in a similar way as the dataset used for testing, i.e. there is no domain shift between train and test distribution. Naturally, performance is significantly higher. These results are underlined to emphasize this fact.\\n\\nHypothesis testing. We conduct the Friedman test on the scores of Table 2. We conduct hypothesis tests in two ways: once with all datasets, and another more strict setting where we reject datasets which were created using the same methods as for training any of the algorithms. We chose a significance level of $\\\\alpha = 0.05$ and test for significance. The returned F-statistic is 7.46 with an uncorrected p-value of 0.000002 when using all datasets and $F = 4.98$, $p = 0.00045$ in the strict setting. In both cases, $p < \\\\alpha$ and therefore we reject $H_0$. In Table 4 and Table 5 we report all pairwise comparisons of algorithms using the Nemenyi post-hoc test for both settings.\\n\\n7. Discussion\\n\\nOur results, found in Tabs. 1, 2, 3, and most importantly Tab. 4 and Tab. 5, allow us to draw several conclusions that support the usage of the formal hypothesis testing framework for this evaluation.\\n\\nInput augmentation and feature augmentation. The considered algorithms can be grouped into two main families: those that focus on augmenting the inputs that go into the network, and those that focus on the activations inside the network. We observe in Tab. 2 and Tab. 3 that algorithms that perform input augmentation (Stylized ERM, Debiased and both DeepAug ERMs) tend to have larger differences in performance than those that do not, indicating that augmenting the inputs is a crucial step in improving performance.\"}"}
{"id": "CVPR-2023-224", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Results choosing the best validation accuracy per run. Columns are grouped according to texture bias and adversarial robustness.\\n\\n| Algorithm                        | ImageNet1k | ImageNet-R | DeepAug (CAE) | DeepAug (EDSR) |\\n|----------------------------------|------------|------------|---------------|---------------|\\n| Stylized ERM [13]               | 73.3       | 72.9       | 1.5           | 2.0           |\\n| pAdaIN [33]                     | 73.3       | 72.9       | 1.5           | 2.0           |\\n| ERM [38]                        | 73.3       | 72.9       | 1.5           | 2.0           |\\n\\nTable 3. Results choosing the last 30 epochs per run. Columns are grouped according to texture bias and adversarial robustness.\\n\\n| Algorithm                        | ImageNet1k | ImageNet-R | DeepAug (CAE) | DeepAug (EDSR) |\\n|----------------------------------|------------|------------|---------------|---------------|\\n| Stylized ERM [13]               | 73.3       | 72.9       | 1.5           | 2.0           |\\n| pAdaIN [33]                     | 73.3       | 72.9       | 1.5           | 2.0           |\\n| ERM [38]                        | 73.3       | 72.9       | 1.5           | 2.0           |\\n\\nThe specific score is underlined.\"}"}
{"id": "CVPR-2023-224", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4. Full post-hoc Nemenyi test based on validation. For this hypothesis test we use scores from all datasets.\\n\\n| Algorithm          | ERM        | pAdaIN     | SagNet     | InfoDrop   | Stylized ERM | Debiased | DAug. ERM (CAE) | DAug. ERM (EDSR) |\\n|--------------------|------------|------------|------------|------------|--------------|----------|-----------------|------------------|\\n| ERM [38]           | 1.0        | 0.9        | 0.9        | 0.9        | 0.9          | 0.211    | 0.053           | 0.478            |\\n| pAdaIN [33]        | 1.0        | 0.9        | 0.642      | 0.012      | 0.002        | 0.053    |                 |                  |\\n| SagNet [30]        | 1.0        | 0.9        | 0.806      | 0.03       | 0.005        | 0.111    |                 |                  |\\n| InfoDrop [36]      | 1.0        | 0.9        | 0.254      | 0.069      | 0.533        |          |                 |                  |\\n| Stylized ERM [13]  | 1.0        | 0.642      | 0.303      | 0.9        |              |          |                 |                  |\\n| Debiased           | 1.0        | 0.9        | 0.9        |            |              |          |                 |                  |\\n| DAug. ERM (CAE) [15]| 1.0        | 0.9        | 0.9        |            |              |          |                 |                  |\\n| DAug. ERM (EDSR) [15]| 1.0      |            |            |            |              |          |                 |                  |\\n\\nTable 5. Partial post-hoc Nemenyi test based on validation. This hypothesis test is based solely on datasets that have a true distribution shift wrt. the training data.\\n\\n| Algorithm          | ERM        | pAdaIN     | SagNet     | InfoDrop   | Stylized ERM | Debiased | DAug. ERM (CAE) | DAug. ERM (EDSR) |\\n|--------------------|------------|------------|------------|------------|--------------|----------|-----------------|------------------|\\n| ERM [38]           | 1.0        | 0.897      | 0.9        | 0.9        | 0.9          | 0.505    | 0.241           | 0.897            |\\n| pAdaIN [33]        | 1.0        | 0.9        | 0.897      | 0.364      | 0.024        | 0.005    | 0.149           |                  |\\n| SagNet [30]        | 1.0        | 0.9        | 0.897      | 0.241      | 0.086        | 0.636    |                 |                  |\\n| InfoDrop [36]      | 1.0        | 0.9        | 0.505      | 0.241      | 0.897        |          |                 |                  |\\n| Stylized ERM [13]  | 1.0        | 0.9        | 0.766      | 0.9        |              |          |                 |                  |\\n| Debiased           | 1.0        | 0.9        | 0.9        |            |              |          |                 |                  |\\n| DAug. ERM (CAE) [15]| 1.0        | 0.9        | 0.9        |            |              |          |                 |                  |\\n| DAug. ERM (EDSR) [15]| 1.0      |            |            |            |              |          |                 |                  |\\n\\nThe two methods are on par. It is possible that using more datasets would allow us to identify for which algorithms, if any, there is a statistically significant difference, but even with the rather many runs in our test bed, no statistically supported difference has been observed yet, and we cannot confidently establish a correct ranking.\\n\\nInterpretation of hypothesis testing. When using hypothesis tests, one needs to be precise w.r.t. the conclusions drawn from the results. In particular, rejecting the null hypothesis means that if the null hypothesis were correct, we would almost certainly (with significance \\\\( \\\\alpha \\\\)) not observe such an extreme difference. However, even if the significance level is high we cannot conclude superiority of one algorithm over another for an unseen domain. The significance of the difference is only established under the experimental setting, i.e., within the datasets and algorithms used. Conversely, being unable to reject the null hypothesis should not be misinterpreted as a proof of equality, but only as an inability to rule out equality with the given data.\\n\\n8. Conclusion\\n\\nWhen analyzing existing methods tailored towards texture-free learning, common datasets, evaluation protocols and reports are missing. In this work, we introduced BiasBed to alleviate the aforementioned limitations. In particular, we have seen that model selection methods play a critical role in OOD testing and fair comparisons are only possible if algorithms are evaluated in a rigorous fashion. Hypothesis testing can fill this gap by providing statistically sound comparisons. Moreover, one must be careful to draw the right conclusions, e.g., low significance of a difference does not necessarily mean that two methods perform on par, but may also indicate that there are too few observations to make a confident statement about their difference. Our framework provides the necessary tools to implement, test and compare existing and new algorithms. Our intention is not to make negative claims or invalidate any particular approach. Rather, we hope to encourage the community to leverage existing statistical expertise and ensure fair and rigorous quantitative evaluations that drive forward the field.\\n\\n9. Broader impact\\n\\nThe aim of the work presented in this paper is to provide a solid foundation for future research on style bias of neural networks. Such hypothesis testing framework are commonplace in other fields where the effects of different experiments can only be observed in noisy results, such as many areas of physics, medicine and psychology. We hope that the presented framework will be used in the future by other researchers through the openly released codebase to find and validate novel algorithms that mitigate texture bias. Furthermore, the proposed methodology \u2013 and codebase \u2013 can be used to perform rigorous testing and comparisons of algorithms for other computer vision and machine learning problems where results are notoriously hard to validate, such as domain adaptation and domain generalization. It is our belief that setting a higher bar and expecting rigorous testing from authors who propose novel methods will, in the long run, improve the quality of research and advance the field.\"}"}
