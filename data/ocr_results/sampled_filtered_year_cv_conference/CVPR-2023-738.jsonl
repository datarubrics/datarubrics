{"id": "CVPR-2023-738", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulo, Elisa Ricci, and Barbara Caputo. Modeling the background for incremental learning in semantic segmentation. In CVPR, June 2020.\\n\\n[2] Sungmin Cha, Beomyoung Kim, YoungJoon Yoo, and Taesup Moon. Ssul: Semantic segmentation with unknown label for exemplar-based class-incremental learning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, NeurIPS, volume 34, pages 10919\u201310930. Curran Associates, Inc., 2021.\\n\\n[3] Feilong Chen, Duzhen Zhang, Minglun Han, Xiu-Yi Chen, Jing Shi, Shuang Xu, and Bo Xu. VLP: A survey on vision-language pre-training. Int. J. Autom. Comput., 20(1):38\u201356, 2023.\\n\\n[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834\u2013848, 2018.\\n\\n[5] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, pages 801\u2013818, 2018.\\n\\n[6] Yang Chen, Xiaoyan Sun, and Yaochu Jin. Communication-efficient federated deep learning with layerwise asynchronous model update and temporally weighted aggregation. IEEE Transactions on Neural Networks and Learning Systems, 31(10):4229\u20134238, 2020.\\n\\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248\u2013255, 2009.\\n\\n[8] Jiahua Dong, Yang Cong, Gan Sun, Zhen Fang, and Zhengming Ding. Where and how to transfer: Knowledge aggregation-induced transferability perception for unsupervised domain adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u20131, 2021.\\n\\n[9] Jiahua Dong, Yang Cong, Gan Sun, Bineng Zhong, and Xiaowei Xu. What can be transferred: Unsupervised domain adaptation for endoscopic lesions segmentation. In CVPR, pages 4022\u20134031, June 2020.\\n\\n[10] Jiahua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu, Xiao Wang, and Qi Zhu. Federated class-incremental learning. In CVPR, June 2022.\\n\\n[11] Arthur Douillard, Yifu Chen, Arnaud Dapogny, and Matthieu Cord. Plop: Learning without forgetting for continual semantic segmentation. In CVPR, pages 4040\u20134050, June 2021.\\n\\n[12] Mark Everingham, Luc Gool, Christopher K. Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. Int. J. Comput. Vision, 88(2):303\u2013338, June 2010.\\n\\n[13] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. NeurIPS, 33:3557\u20133568, 2020.\\n\\n[14] Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, and Feng Liu. Is out-of-distribution detection learnable? In NeurIPS, 2022.\\n\\n[15] Zhen Fang, Jie Lu, Feng Liu, Junyu Xuan, and Guangquan Zhang. Open set domain adaptation: Theoretical bound and algorithm. IEEE Transactions on Neural Networks and Learning Systems, 32(10):4309\u20134322, 2021.\\n\\n[16] Lidia Fantauzzo, Eros Fan\u00ec, Debora Caldarola, Antonio Tavera, Fabio Cermelli, Marco Ciccone, and Barbara Caputo. Feddrive: Generalizing federated learning to semantic segmentation in autonomous driving. In Proceedings of the 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2022.\\n\\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, June 2016.\\n\\n[18] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NeurIPS Workshop, 2015.\\n\\n[19] Qirong Ho, James Cipar, Henggang Cui, Jin Kyu Kim, Seunghak Lee, Phillip B. Gibbons, Garth A. Gibson, Gregory R. Ganger, and Eric P. Xing. More effective distributed ml via a stale synchronous parallel parameter server. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201913, page 1223\u20131231, 2013.\\n\\n[20] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In ICML, pages 5132\u20135143. PMLR, 2020.\\n\\n[21] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.\\n\\n[22] Matthias De Lange, Xu Jia, Sarah Parisot, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. Unsupervised model personalization while preserving privacy and scalability: An open problem. In CVPR, pages 14463\u201314472, 2020.\\n\\n[23] Guangrui Li, Guoliang Kang, Wu Liu, Yunchao Wei, and Yi Yang. Content-consistent matching for domain adaptive semantic segmentation. In ECCV, pages 440\u2013456. Springer, 2020.\\n\\n[24] Guangrui Li, Guoliang Kang, Yi Zhu, Yunchao Wei, and Yi Yang. Domain consensus clustering for universal domain adaptation. In CVPR, 2021.\\n\\n[25] Wenqi Li, Fausto Milletar\u00ec, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maximilian Baust, Yan Cheng, S\u00e9bastien Ourselin, M Jorge Cardoso, et al. Privacy-preserving federated brain tumour segmentation. In International workshop on machine learning in medical imaging, pages 133\u2013141. Springer, 2019.\\n\\n[26] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on non-iid features via local batch normalization. In ICLR, 2021.\"}"}
{"id": "CVPR-2023-738", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935\u20132947, 2018.\\n\\nBoyi Liu, Lujia Wang, Ming Liu, and Cheng-Zhong Xu. Federated imitation learning: A novel framework for cloud robotic systems with heterogeneous sensor data. IEEE Robotics and Automation Letters, 5(2):3509\u20133516, 2020.\\n\\nQuande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann Heng. Feddg: Federated domain generalization on medical image segmentation via episodic learning in continuous frequency space. In CVPR, pages 1013\u20131023, 2021.\\n\\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, pages 3431\u20133440, 2015.\\n\\nDavid Lopez-Paz and Marc Aurelio Ranzato. Gradient episodic memory for continual learning. In NeurIPS, volume 30, 2017.\\n\\nArun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In CVPR, June 2018.\\n\\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 24:109\u2013165, 1989.\\n\\nDavid Lopez-Paz and Marc Aurelio Ranzato. Gradient episodic memory for continual learning. In NeurIPS, volume 30, 2017.\\n\\nAnit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and Virginia Smith. On the convergence of federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.\\n\\nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In NeurIPS, volume 30, 2017.\\n\\nFu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Foster: Feature boosting and compression for class-incremental learning. In ECCV, 2022.\\n\\nHongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. In ICLR, 2020.\\n\\nLixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu. Addressing class imbalance in federated learning. In AAAI, volume 35, pages 10165\u201310173, 2021.\\n\\nLixu Wang, Shichao Xu, Ruiqi Xu, Xiao Wang, and Qi Zhu. Non-transferable learning: A new approach for model ownership verification and applicability authorization. In ICLR.\\n\\nKun Wei, Da Chen, Yuhong Li, Xu Yang, Cheng Deng, and Dacheng Tao. Incremental embedding learning with disentangled representation translation. IEEE Transactions on Neural Networks and Learning Systems, 2022.\\n\\nKun Wei, Cheng Deng, and Xu Yang. Lifelong zero-shot learning. In IJCAI, pages 551\u2013557, 2020.\\n\\nHaibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation in non-iid federated learning. In ICLR, 2021.\\n\\nQiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. ACM Trans. Intell. Syst. Technol., 10(2), jan 2019.\\n\\nLu Yu, Xialei Liu, and Joost Van de Weijer. Self-training for class-incremental semantic segmentation. IEEE Transactions on Neural Networks and Learning Systems, 2022.\\n\\nMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In ICML, pages 7252\u20137261. PMLR, 2019.\\n\\nChang-Bin Zhang, Jia-Wen Xiao, Xialei Liu, Ying-Cong Chen, and Ming-Ming Cheng. Representation compensation networks for continual semantic segmentation. In CVPR, pages 7053\u20137064, June 2022.\\n\\nJie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Shouhong Ding, Chunhua Shen, and Chao Wu. Dense: Data-free one-shot federated learning. In NeurIPS.\\n\\nJie Zhang, Bo Li, Chen Chen, Lingjuan Lyu, Shuang Wu, Shouhong Ding, and Chao Wu. Delving into the adversarial robustness of federated learning. arXiv preprint arXiv:2302.09479, 2023.\\n\\nJie Zhang, Bo Li, Jianghe Xu, Shuang Wu, Shouhong Ding, Lei Zhang, and Chao Wu. Towards efficient data free black-box adversarial attack. In CVPR, pages 15115\u201315125, 2022.\\n\\nYonggang Zhang, Mingming Gong, Tongliang Liu, Gang Niu, Xinmei Tian, Bo Han, Bernhard Sch\u00f6lkopf, and Kun Zhang. Causaladv: Adversarial robustness through the lens of causality. 2022.\\n\\nYonggang Zhang, Xinmei Tian, Ya Li, Xinchao Wang, and Dacheng Tao. Principal component adversarial example. IEEE Transactions on Image Processing, 29:4804\u20134815, 2020.\\n\\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, pages 5122\u20135130, 2017.\"}"}
{"id": "CVPR-2023-738", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When pseudo label \\\\( \\\\hat{Y} \\\\) where learned from the \\\\( \\\\bar{\\\\Gamma} \\\\) in the \\\\( t \\\\)-th segmentation task, we denote gradient propagation means in Eq. (5) to balance heterogeneous relation between old and new classes, and consider intrinsic relations between old and new classes. As a result, given mini-batch samples \\\\( x_{ijt} \\\\) of \\\\( \\\\bar{\\\\Gamma} \\\\)-th pixel in \\\\( \\\\bar{\\\\Gamma} \\\\)-th background label space of the \\\\( k \\\\)-th channel dimensions of one-hot pseudo labels \\\\( \\\\hat{Y} \\\\) for each epoch empirically as training process. We use the first \\\\( \\\\hat{Y} \\\\) instead of an individual sample to better characterize under-distribution of \\\\( \\\\bar{\\\\Gamma} \\\\) within local client \\\\( S \\\\).\\n\\nTo address heterogeneous forgetting speeds of different old classes, we expect gradient scalar forgetting of old classes changes dynamically as continual learning tasks, we employ gradient propagation means of different old tasks to surmount intra-client heterogeneous forgetting speeds brought by background shift.\\n\\nGiven a confident pseudo labels \\\\( \\\\hat{Y} \\\\), we first obtain its probability \\\\( P(\\\\hat{Y}) \\\\) of the \\\\( \\\\bar{\\\\Gamma} \\\\)-th pixel with respect to the \\\\( \\\\bar{\\\\Gamma} \\\\)-th segmentation task, we denote gradient propagation \\\\( \\\\partial_{\\\\hat{Y}} \\\\) of pixel classifier in \\\\( \\\\bar{\\\\Gamma} \\\\)-th class at the \\\\( r,t \\\\)-th pixel in \\\\( \\\\bar{\\\\Gamma} \\\\)-th channel dimensions of one-hot pseudo labels \\\\( \\\\hat{Y} \\\\) as follows:\\n\\n\\\\[\\n\\\\hat{Y} = \\\\begin{cases} \\\\hat{Y} \\\\in Y_{\\\\bar{\\\\Gamma}}, & \\\\text{if } \\\\eta \\\\in \\\\bar{\\\\Gamma} \\\\\\\\ \\\\emptyset, & \\\\text{otherwise,} \\\\end{cases}\\n\\\\]\\n\\nwhere the quantity of pixels belonging to background and \\\\( \\\\bar{\\\\Gamma} \\\\) is initialized as \\\\( 50\\\\% \\\\) for all old \\\\( \\\\eta \\\\).\\n\\nFor a given sample \\\\( \\\\{x_{ijt}, y_{ijt}\\\\} \\\\), we employ \\\\( \\\\hat{Y} \\\\) for heterogeneous forgetting compensation. Specifically, the loss \\\\( L \\\\) is background label space of the \\\\( k \\\\)-th class-balanced selection proportion \\\\( \\\\rho \\\\), and evaluates gradient scalar \\\\( r,t \\\\) adds adding \\\\( \\\\rho \\\\) for all old \\\\( \\\\eta \\\\). The value of \\\\( \\\\rho \\\\) is initialized as \\\\( \\\\hat{Y} \\\\). The gradient propagation means in Eq. (5) reflect gradient scalar \\\\( \\\\partial_{\\\\hat{Y}} \\\\) of \\\\( \\\\bar{\\\\Gamma} \\\\) forgetting-balanced semantic compensation loss as follows:\\n\\n\\\\[\\nFS = \\\\begin{cases} \\\\hat{Y} \\\\in Y_{\\\\bar{\\\\Gamma}}, & \\\\text{if } \\\\rho \\\\cdot D(\\\\hat{Y}) \\\\cdot \\\\theta \\\\in \\\\bar{\\\\Gamma} \\\\\\\\ \\\\emptyset, & \\\\text{otherwise,} \\\\end{cases}\\n\\\\]\\n\\nThe intrinsic relations between old and new classes are employed to perform knowledge distillation [18] on an important role in tackling intra-client heterogeneous forgetting among different old classes.\\n\\n5.1 Forgetting-Balanced Semantic Compensation\\n\\n4.2. Forgetting-Balanced Semantic Compensation\\n\\n4.3. Forgetting-Balanced Relation Consistency Loss (Section 4.3) to surmount intra-client heterogeneous forgetting compensation. These confident pseudo labels provide strong guidance for old classes within local client \\\\( S \\\\). The value of \\\\( \\\\rho \\\\) is initialized as \\\\( 50\\\\% \\\\), and abbreviate this variant as \\\\( FS \\\\). The gradient propagation means in Eq. (5) reflect gradient scalar \\\\( \\\\partial_{\\\\hat{Y}} \\\\) of \\\\( \\\\bar{\\\\Gamma} \\\\) forgetting-balanced semantic compensation loss as follows:\\n\\n\\\\[\\nFS = \\\\begin{cases} \\\\hat{Y} \\\\in Y_{\\\\bar{\\\\Gamma}}, & \\\\text{if } \\\\rho \\\\cdot D(\\\\hat{Y}) \\\\cdot \\\\theta \\\\in \\\\bar{\\\\Gamma} \\\\\\\\ \\\\emptyset, & \\\\text{otherwise,} \\\\end{cases}\\n\\\\]\\n\\nThe intrinsic relations between old and new classes are employed to perform knowledge distillation [18] on an important role in tackling intra-client heterogeneous forgetting among different old classes.\"}"}
{"id": "CVPR-2023-738", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparisons of mIoU (%) on Pascal-VOC 2012 dataset [12] under the setting of 15-1 with overlapped foregrounds.\\n\\n| Method                  | mIoU (%) |\\n|-----------------------|----------|\\n| LWF [27] + FL         | 62.8     |\\n| MiB [1] + FL          | 56.9     |\\n| ILT [36] + FL         | 66.2     |\\n| RCIL [53] + FL        | 62.8     |\\n| PLOP [11] + FL        | 85.6     |\\n| Finetuning + FL       | 85.5     |\\n|                    | 0.2      |\\n\\nTable 2. Comparisons of mIoU (%) on Pascal-VOC 2012 dataset [12] under the setting of 4-4 with overlapped foregrounds.\\n\\n| Method                  | mIoU (%) |\\n|-----------------------|----------|\\n| LWF [27] + FL         | 61.4     |\\n| MiB [1] + FL          | 55.7     |\\n| ILT [36] + FL         | 63.4     |\\n| RCIL [53] + FL        | 61.2     |\\n| PLOP [11] + FL        | 84.9     |\\n| Finetuning + FL       | 84.8     |\\n|                    | 0.2      |\"}"}
{"id": "CVPR-2023-738", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3. Comparisons of mIoU (%) on Pascal-VOC 2012 dataset [12] under the setting of 8-2 with overlapped foregrounds.\\n\\n| Class ID | mIoU | Finetuning + FL | LWF [27] + FL | ILT [36] + FL | MiB [1] + FL | PLOP [11] + FL | RCIL [53] + FL | FBL (Ours) |\\n|----------|------|----------------|--------------|-------------|-------------|---------------|---------------|-------------|\\n| 0        | 70.1 | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 13.6        |\\n| 1        | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 3.0         |\\n| 2        | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 6.0         |\\n| 3        | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 3.0         |\\n| 4        | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 5        | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 4.0         |\\n| 6        | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 7        | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 2.0         |\\n| 8        | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 2.0         |\\n| 9        | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 10       | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 11       | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 12       | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 13       | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 14       | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 15       | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 16       | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 17       | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 18       | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 19       | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n| 20       | 0.0  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 1.0         |\\n\\nTable 4. Comparisons of mIoU (%) on ADE20k dataset [59] under the setting of 100-10 with overlapped foregrounds.\\n\\n| Class ID: 0-10 | mIoU | Finetuning + FL | LWF [27] + FL | ILT [36] + FL | MiB [1] + FL | PLOP [11] + FL | RCIL [53] + FL | FBL (Ours) |\\n|---------------|------|----------------|--------------|-------------|-------------|---------------|---------------|-------------|\\n| 11-20         | 0.8  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 10.3        |\\n| 21-30         | 0.8  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 9.4         |\\n| 31-40         | 0.8  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 8.6         |\\n| 41-50         | 0.8  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 8.3         |\\n| 51-60         | 0.8  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 8.2         |\\n| 61-70         | 0.8  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 7.9         |\\n| 71-80         | 0.8  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 7.7         |\\n| 81-90         | 0.8  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 7.5         |\\n| 91-100        | 0.8  | 0.0            | 0.0          | 0.0         | 0.0         | 0.0           | 0.0           | 7.3         |\\n\\nEssential to tackle inter-client heterogeneous forgetting via considering Non-IID distributions across local clients.\\n\\n4.5. Optimization Procedure\\n\\nAt the beginning of each global round in each incremental task, all local clients employ Eq. (11) to calculate the average entropy of local data, and then some of local clients are randomly selected by global server $S_g$ to conduct local training at each round. After these chosen clients utilize task transition monitor to accurately recognize new classes, they automatically store the global model learned at the last global round as the old model $\\\\Theta_{t-1}$ to generate confident pseudo labels for old classes via Eq. (2), and optimize local model $\\\\Theta_{r,t}$ via $L_{obj}$ in Eq. (10). Finally, the updated local models $\\\\Theta_{r,t}$ of selected local clients are aggregated as $\\\\Theta_{r+1,t}$ by $S_g$ for the next round training. The supplementary material provides optimization procedure of our FBL model.\\n\\n5. Experiments\\n\\n5.1. Implementation Details\\n\\nWe utilize two benchmark datasets: Pascal-VOC 2012 [12] and ADE20k [59] under various experimental settings to analyze effectiveness of our FBL model. For fair comparisons with baseline ISS methods [1, 11, 27, 36, 53] under the FISS settings, we follow them to set exactly the same incremental tasks and class order, while using the identical segmentation backbone (i.e., Deeplab-v3 [4] with ResNet-101 [17] pretrained on ImageNet dataset [7]). As claimed in [1, 11, 53], background pixels in the current task may belong to old classes or new classes from future tasks (i.e., background has some overlap with new foreground classes in the future tasks).\\n\\nIn the FISS, we consider more challenging settings by assigning more incremental segmentation tasks with overlapped foregrounds. On Pascal-VOC 2012 [12], 15-1, 4-4, and 8-2 settings with overlapped foregrounds respectively consist in 15 classes followed by 1 classes 5 times ($T = 6$), learning 4 classes followed by 4 classes 4 times ($T = 5$), and 8 classes followed by 2 classes 6 times ($T = 7$).\\n\\nLikewise, on ADE20k [59], 100-10 setting with overlapped foregrounds means 100 classes followed by 10 classes 5 times ($T = 6$).\\n\\nWe employ SGD optimizer with initial learning rate as $1.0 \\\\times 10^{-2}$ to train the first base task and $1.0 \\\\times 10^{-3}$ to learn incremental tasks. Considering the limitation of GPU overhead, we set initial local clients as 10, and add 4 new local clients for each task. We choose 4 local clients randomly to perform local training with 6 epochs for VOC [12] and 12 epoches for ADE20k [59]. On VOC dataset [12], we randomly select 40% images for each client in each segmentation task under 15-1 setting; otherwise, we randomly sample 50% classes from current label space $Y_t$, and assign 60% samples from these classes to selected local clients under the 4-4 and 8-2 settings. For the 100-10 setting in ADE20k [59], we randomly choose 70% classes from $Y_t$, and distribute them to selected clients. Following ISS methods [1, 11, 27, 36, 53], we employ mean Intersection over Union (mIoU) as metric, and evaluate mIoU of all classes after learning the last segmentation task (i.e., $t = T$). This metric evaluates the effectiveness to address heterogeneous forgetting and the ability to segment new classes continually.\\n\\n5.2. Comparison Performance\\n\\nExperiments on Pascal-VOC 2012 [12] and ADE20k [59] are introduced to analyze superiority of our model under various settings of FISS, as shown in Tables 1 \u223c 4. Our model achieves large improvements over existing ISS methods [1, 11, 27, 36, 53] about 1.5% \u223c 51.4% mIoU under various FISS settings. It illustrates the effectiveness of our model against other ISS methods to learn a global continual model.\"}"}
{"id": "CVPR-2023-738", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Visualization of some qualitative comparison results on Pascal-VOC 2012 [12] under the overlapped 4-4 setting in the FISS.\\n\\nFigure 4. Visualization of some pseudo labels on Pascal-VOC 2012 [12] under the 4-4 setting with overlapped foregrounds.\\n\\nTable 5. Ablation studies on Pascal-VOC 2012 [12] under the FISS.\\n\\n| Variants          | VOC 4-4 [12] | VOC 8-2 [12] |\\n|-------------------|--------------|--------------|\\n|                  | AP | FSC | FRC | mIoU | Imp. | AP | FSC | FRC | mIoU | Imp. |\\n| Ours-w/oAPL       |\u2717  | \u2713  | \u2713   | 41.3 | 34.3 | 40.0| 41.4 | 33.8 | 40.0| 3.9 |\\n| Ours-w/oFSC       |\u2713  | \u2717  | \u2713   | 41.3 | 34.3 | 40.0| 41.4 | 33.8 | 40.0| 3.9 |\\n| Ours-w/oFRC       |\u2713  | \u2713  | \u2717   | 32.6 | 30.8 | 32.3| 31.6 | 6.8  | 29.2| 6.5 |\\n| FBL (Ours)        |\u2713  | \u2713  | \u2713   | 45.8 | 35.8 | 43.9| 38.5 | 8.3  | -   | -   |\\n\\nTable 6. Task-wise comparisons of mIoU (%) on Pascal-VOC 2012 dataset [12] under the setting of overlapped 4-4 (T = 5).\\n\\n| Task ID | t=1 (Base) | t=2 | t=3 | t=4 | t=5 |\\n|---------|------------|-----|-----|-----|-----|\\n| Finetuning + FL | 70.4 | 43.1 | 21.3 | 19.0 | 9.1 |\\n| LWF [27] + FL     | 70.4 | 59.8 | 38.7 | 39.1 | 23.8 |\\n| ILT [36] + FL     | 70.4 | 56.4 | 36.9 | 35.3 | 22.7 |\\n| MiB [1] + FL      | 70.4 | 64.8 | 52.8 | 47.2 | 33.0 |\\n| PLOP [11] + FL    | 70.4 | 54.2 | 38.3 | 29.4 | 28.1 |\\n| RCIL [53] + FL    | 70.5 | 60.3 | 40.1 | 36.8 | 32.4 |\\n| FBL (Ours)        | 70.4 | 66.6 | 53.6 | 49.6 | 43.9 |\\n\\nSome visualization results on Pascal-VOC 2012 [12] under the 4-4 setting are shown in Figure 3, which verifies the effectiveness of our model to address the FISS problem.\\n\\n5.3. Ablation Studies\\nTo analyze the effectiveness of each module in our model, Table 5 presents ablation experiments under various FISS settings. Ours-w/oAPL, Ours-w/oFSC and Ours-w/oFRC indicate the results of our model without adaptive class-balanced pseudo labeling (denoted as APL), forgetting-balanced semantic compensation loss $L_{FS}$ (denoted as FSC) and forgetting-balanced relation consistency loss $L_{FR}$ (denoted as FRC), where Ours-w/oAPL uses constant probability threshold for all old classes to replace adaptive class-specific entropy threshold. When compared with Ours, all ablation variants severely degrade $3\\\\%\\\\sim11.6\\\\%$ mIoU. It verifies the importance of all modules to address the heterogeneous forgetting. The proposed APL module can effectively tackle background shift via confident pseudo labels, and some confident pseudo labels are visualized in Figure 4.\\n\\n5.4. Analysis of Task-Wise Comparisons\\nAs presented in Table 6, we introduce task-wise comparison results to analyze the effectiveness of our model to address FISS settings. Our model outperforms baseline ISS methods [1, 11, 27, 36, 53] for most task-wise comparisons under the overlapped 4-4 setting. The proposed FBL model encourages local clients to learn a global incremental segmentation model cooperatively under privacy preservation. Comparisons in Table 6 show large mIoU improvements of our model to address the FISS problem over other ISS methods. When segmenting new foreground classes consecutively, our model can effectively tackle intra-client and inter-client heterogeneous forgetting on different old classes.\\n\\n6. Conclusion\\nIn this work, we propose a Federated Incremental Semantic Segmentation (FISS) problem, and develop a novel Forgetting-Balanced Learning (FBL) model to address intra-client and inter-client heterogeneous forgetting on old classes. To tackle intra-client heterogeneous forgetting, we design a forgetting-balanced semantic compensation loss and a forgetting-balanced relation consistency loss, under the guidance of adaptive class-balanced pseudo labeling. Meanwhile, we propose a task transition monitor to address inter-client heterogeneous forgetting. It can automatically recognize new classes and store the latest old global model for distillation. Comparison results demonstrate the superiority of our model to tackle the FISS problem. In the future, we will consider using only few samples of new classes to address intra-client and inter-client forgetting.\"}"}
{"id": "CVPR-2023-738", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Federated Incremental Semantic Segmentation\\n\\nJiahua Dong 1, 2, 3*, Duzhen Zhang 4*, Yang Cong 1, 2\u2020, Wei Cong 1, 2, 3, Henghui Ding 4, Dengxin Dai 4\\n\\n1 State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, China.\\n2 Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, 110169, China.\\n3 University of Chinese Academy of Sciences, Beijing, 100049, China.\\n4 ETH Z\u00fcrich, Z\u00fcrich, 8092, Switzerland.\\n\\n{dongjiahua1995, congyang81, congwei45, henghui.ding}@gmail.com, dai@vision.ee.ethz.ch\\n\\nAbstract\\n\\nFederated learning-based semantic segmentation (FSS) has drawn widespread attention via decentralized training on local clients. However, most FSS models assume categories are fixed in advance, thus heavily undergoing forgetting on old categories in practical applications where local clients receive new categories incrementally while have no memory storage to access old classes. Moreover, new clients collecting novel classes may join in the global training of FSS, which further exacerbates catastrophic forgetting. To surmount the above challenges, we propose a Forgetting-Balanced Learning (FBL) model to address heterogeneous forgetting on old classes from both intra-client and inter-client aspects. Specifically, under the guidance of pseudo labels generated via adaptive class-balanced pseudo labeling, we develop a forgetting-balanced semantic compensation loss and a forgetting-balanced relation consistency loss to rectify intra-client heterogeneous forgetting of old categories with background shift. It performs balanced gradient propagation and relation consistency distillation within local clients. Moreover, to tackle heterogeneous forgetting from inter-client aspect, we propose a task transition monitor. It can identify new classes under privacy protection and store the latest old global model for relation distillation. Qualitative experiments reveal large improvement of our model against comparison methods. The code is available at https://github.com/JiahuaDong/FISS.\\n\\n1. Introduction\\n\\nFederated learning (FL) [13, 20, 22, 44] is a remarkable decentralized training paradigm to learn a global model across local clients without accessing their private data. Under privacy preservation, it has achieved rapid development in semantic segmentation [4, 8, 30] by training on multiple decentralized local clients to alleviate the constraint of data island that requires enormous finely-labeled pixel annotations [25]. As a result, federated learning-based semantic segmentation (FSS) [28, 29] significantly economizes annotation costs in data-scarce scenarios via training a global segmentation model on private data of different clients [29].\\n\\nHowever, existing FSS methods [16, 25, 28, 29] unrealistically assume that the learned foreground classes are static and fixed over time, which is impractical in real-world dynamic applications where local clients receive streaming data of new categories consecutively. To tackle this issue, existing...\"}"}
{"id": "CVPR-2023-738", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FSS methods [28, 29, 35] typically enforce local clients to store all samples of previously-learned old classes, and then learn a global model to segment new categories continually via FL. Nevertheless, it requires large computation and memory overhead as new classes arrive continuously, limiting the application ability of FSS methods [16, 28]. If local clients have no memory to store old classes, existing FSS methods [16, 29] significantly degrade segmentation behavior on old categories (i.e., catastrophic forgetting [40, 47, 48]) when learning new classes incrementally. In addition, the pixels labeled as background in the current learning task may belong to old classes from old tasks or new foreground classes from future tasks. This phenomenon is also known as background shift [11, 36] that heavily aggravates heterogeneous forgetting speeds on old categories. More importantly, in practical scenarios, new local clients receiving new categories incrementally may join in global FL training irregularly, thus further exacerbating catastrophic forgetting to some extent.\\n\\nTo surmount the above real-world scenarios, we propose a novel practical problem called Federated Incremental Semantic Segmentation (FISS), where local clients collect new categories consecutively according to their preferences, and new local clients collecting unseen novel classes participate in global FL training irregularly. In the FISS settings, the class distributions are non-independent and identically distributed (Non-IID) across different clients, and training data of old classes is unavailable for all local clients. FISS aims to train a global incremental segmentation model via collaborative FL training on local clients while addressing catastrophic forgetting. In this paper, we use medical lesions segmentation [25, 29] as an example to better illustrate FISS, as shown in Figure 1. Hundreds of hospitals, as well as newly joined ones, collect unseen/new medical lesions continuously in clinical diagnosis. Considering privacy preservation, it is desired for these hospitals to learn a global segmentation modal via FL without accessing each other's data [44, 56].\\n\\nA naive solution for FISS problem is to directly integrate incremental semantic segmentation [1, 11, 53] and FL [19, 50] together. Nevertheless, such a trivial solution requires global server to have strong human prior about which and when local clients can collect new categories, so that global model learned in the latest old task can be stored by local clients to address forgetting on old classes via knowledge distillation [18, 43]. Considering privacy preservation in the FISS, this privacy-sensitive prior knowledge cannot be shared between local clients and global server. As a result, this naive solution severely suffers from intra-client heterogeneous forgetting on different old classes caused by background shift [1, 11, 36, 53], and inter-client heterogeneous forgetting across different clients brought by Non-IID class distributions.\\n\\nTo overcome the above-mentioned challenges, we develop a novel Forgetting-Balanced Learning (FBL) model, which alleviates heterogeneous forgetting on old classes from intra-client and inter-client perspectives. Specifically, to tackle intra-client heterogeneous forgetting caused by background shift, we propose an adaptive class-balanced pseudo labeling to adaptively generate confident pseudo labels for old classes. Under the guidance of pseudo labels, we propose a forgetting-balanced semantic compensation loss to rectify different forgetting of old classes with background shift via considering balanced gradient propagation of local clients. In addition, a forgetting-balanced relation consistency loss is designed to distill underlying category-relation consistency between old and new classes for intra-client heterogeneous forgetting compensation. Moreover, considering addressing heterogeneous forgetting from inter-client aspect, we develop a task transition monitor to automatically identify new classes without any human prior, and store the latest old model from global perspective for relation consistency distillation. Experiments on segmentation datasets reveal large improvement of our model over comparison methods. We summarize the main contributions of this work as follows:\\n\\n- We propose a novel practical problem called Federated Incremental Semantic Segmentation (FISS), where the major challenges are intra-client and inter-client heterogeneous forgetting on old categories caused by intra-client background shift and inter-client Non-IID distributions.\\n- We propose a Forgetting-Balanced Learning (FBL) model to address the FISS problem via surmounting heterogeneous forgetting from both intra-client and inter-client aspects. As we all know, in the FL field, this is a pioneer attempt to explore a global continual segmentation model.\\n- We develop a forgetting-balanced semantic compensation loss and a forgetting-balanced relation consistency loss to tackle intra-client heterogeneous forgetting across old classes, under the guidance of confident pseudo labels generated via adaptive class-balanced pseudo labeling.\\n- We design a task transition monitor to surmount inter-client heterogeneous forgetting by accurately recognizing new classes under privacy protection and storing the latest old model from global aspect for relation distillation.\\n\\n2. Related Work\\n\\nFederated Learning (FL) [26, 34, 44, 49] aggregates local-client model parameters to optimize a global model under privacy protection. [41] enforces local model to approximate the global ones via a proximal term. To minimize computation cost, [6] employs a layer-wise parameter aggregation strategy. Inspired by above FL [13, 44, 52] methods, [25, 28, 29] apply FL to semantic segmentation [5, 30], which has achieved rapid developments in medical analysis [9, 35] and autonomous driving [16]. [38] considers adversarial framework [57, 58] to tackle domain adaptation problem [15, 23, 24, 46] in the FL field. [10] proposes a federated class-incremental learning model via considering...\"}"}
{"id": "CVPR-2023-738", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"After getting global knowledge \\\\( \\\\text{ISS} \\\\) \\\\([1, 36, 53]\\\\), we consider background shift in the FISS setting. Denote global server as \\\\( \\\\text{S} \\\\) \\\\( \\\\Theta \\\\) model trained on private train-data of \\\\( \\\\text{ISS} \\\\) \\\\([1,32]\\\\). Incremental Semantic Segmentation \\\\( \\\\text{ISS} \\\\) \\\\([1,36,53]\\\\) to Federated Incremental Semantic Segmentation \\\\( \\\\text{FISS} \\\\) \\\\([11,37,53]\\\\), due to unavailable training data of future learning tasks as background \\\\([11,37]\\\\). In the FISS, at the \\\\( l \\\\)-th segmentation task, the latest global \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( r,t \\\\) is transmitted from global \\\\( \\\\text{S} \\\\) \\\\( \\\\text{SE} \\\\) \\\\( X \\\\) \\\\( B \\\\) \\\\( \\\\text{XB} \\\\) \\\\( \\\\text{B} \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\) \\\\( \\\\text{SE} \\\\) \\\\( B \\\\"}
{"id": "CVPR-2023-738", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The determination of pseudo label $P_{\\\\text{\\\\theta}}$ and $P_{\\\\text{\\\\theta}}^{\\\\text{\\\\{new\\\\}}}$ to alleviate heterogeneous forgetting across different old classes, other foreground classes: $\\\\mathcal{K}$ class-balanced pseudo labels of $\\\\frac{\\\\{\\\\text{old}\\\\}}{\\\\text{old}}$ and $\\\\mathcal{K}$ as adaptively determined as continual learning process. These class-specific entropy threshold for each old class, which are balance to mine pseudo labels for old classes via introducing pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold to select pseudo labels for all classes, our FBL model considers class that only utilize a constant probability threshold"}
