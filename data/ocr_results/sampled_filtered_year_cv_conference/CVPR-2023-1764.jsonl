{"id": "CVPR-2023-1764", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Template-based Correspondence Module (TCM). To get the implicit style supervision of one view, the content domain view $I$ is passed to a semantic extractor to obtain its semantic feature $F_I$. Then a search-replacement process is conducted to replace the reference feature according to the correspondence in the content domain. The resulting guidance feature $F_G$ serves as implicit supervision to optimize the radiance field $\\\\omega_{NP}$.\\n\\n$$F_G(i,j) = F_I(i^*, j^*)_{SR}, \\\\quad (9)$$\\n\\nwhere $(i^*, j^*) = \\\\arg \\\\min_i i', j' \\\\text{dist}(F_I(i,j), F_I(i', j'))_{IR}$. (10)\\n\\nHere we use $\\\\text{dist}(a, b)$ to denote the distance between two feature vectors $a$ and $b$, which is proved to be effective [22, 45] by taking the form of cosine distance when evaluating semantic features.\\n\\nFinally, for a stylized view $S$ rendered from the NP radiance field $\\\\omega_{NP}$ under the camera pose $\\\\phi$, we conduct implicit feature-level supervision by forcing its semantic feature $\\\\hat{F}_S$ to imitate the aforesaid guidance feature $F_G$.\\n\\n3.4. Ref-NPR Optimization\\n\\nWith the previously discussed collection $\\\\Gamma_R$ and guidance feature $F_G$ as supervision, we optimize Ref-NPR and obtain the stylized scene representation $\\\\omega_{NP}$.\\n\\nIn each training iteration, we sample a subset from $\\\\Gamma_R$ and denote the set of reference-dependent rays as $N_s$. For each sampled reference ray $r_k \\\\in N_s$, $\\\\hat{C}_R(\\\\hat{r}_k)$ is the assigned color of the corresponding pseudo-ray $\\\\hat{r}_k$, as defined in Eq. (6), and we further denote $\\\\hat{C}_{NP}(r_k)$ to be the stylized color rendered from $\\\\omega_{NP}$. This explicit supervision is formulated as the reference loss $L_{ref} = \\\\frac{1}{|N_s|} \\\\sum_{r_k \\\\in N_s} \\\\|\\\\hat{C}_{NP}(r_k) - \\\\hat{C}_R(\\\\hat{r}_k)\\\\|^2_2$. (11)\\n\\nAs for implicit supervision, the discrepancy between $F_G$ and $\\\\hat{F}_S$ should be minimized, as discussed in Sec. 3.3. To maintain the original content structure during such implicit stylization, we also minimize the mean squared distance between content feature $F_I$ and stylized feature $\\\\hat{F}_S$, according to [11]. This implicit supervision is formulated as the feature loss $L_{feat} = \\\\frac{1}{N} \\\\sum_{i,j} \\\\text{dist}(F_G, \\\\hat{F}_S) + \\\\lambda' \\\\|F_I - \\\\hat{F}_S\\\\|^2_2$, (12) where $\\\\lambda'$ is a balancing factor.\\n\\nHowever, as discussed in [22, 45], optimizing the cosine distance between feature vectors cannot effectively eliminate color mismatches. To address this issue, we transfer the average color in a patch by a coarse color-matching loss $L_{color} = \\\\frac{1}{N} \\\\sum_{i,j} \\\\|\\\\bar{C}(i,j)_{NP} - \\\\bar{C}(i^*, j^*)_{IR}\\\\|^2_2$, (13) in which $\\\\bar{C}(i,j)_{NP}$ is the $\\\\omega_{NP}$-rendered average color of the patch at feature-level index $(i, j)$, and $\\\\bar{C}(i^*, j^*)_{IR}$ is the average color of reference patch matched by minimizing feature distance, as described in Eq. (10). Since semantic features are extracted at the image level, considering the memory limitation caused by back-propagation, we follow the gradient cache strategy in [45] and optimize $\\\\omega_{NP}$ patch-wisely. Ultimately, the overall objective is $L_{NP} = \\\\lambda_f L_{feat} + \\\\lambda_r L_{ref} + \\\\lambda_c L_{color}$, where $\\\\lambda(\\\\cdot)$ are the balancing factors.\\n\\nOnce $\\\\omega_{NP}$ is optimized, we may consider it to be a normal radiance field and render stylized novel views with arbitrary camera poses.\\n\\n4. Experiments\\n\\n4.1. Implementation Details\\n\\nRef-NPR is based on the ARF codebase [45] and uses Plenoxels [10] as the radiance field for scene representation. We follow Plenoxels's training scheme to obtain the photorealistic radiance field $\\\\omega_P$. As we do not expect a view-dependent color change in stylized scenes, following [16, 45], we discard view-dependent rendering and apply a view-independent fitting on training views for two epochs before optimizing $\\\\omega_{NP}$. Then, content domain views $I$ are rendered from $\\\\omega_P$ after this view-independent training. In addition, to keep the same geometrical structure of the scene, we do not optimize the density function $\\\\sigma(r(t_i))$ in $\\\\omega_{NP}$ [10, 45].\\n\\nIn $\\\\mathbb{R}^3$, we set reference dictionary $D$ as a cube containing $256^3$ voxels. To parallelize the registration, we store at most 8 rays at each entry $D(x,y,z)$. The angle constraint of directions in Eq. (6) is empirically set to $\\\\cos(\\\\theta) > 0.6$ for ray registration. For each training step, we set the number of pseudo-ray samples to be $|N_s| = 10^6$, with half of them from $\\\\phi_R$ and the other half from rays registered in correlated views. In TCM, we use VGG16 [35] as the semantic feature extractor. We concatenate features that\"}"}
{"id": "CVPR-2023-1764", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Qualitative comparisons in novel-view stylization. For each example, we provide the reference view (top) and its corresponding stylized reference view (bottom) on the left. We compare Ref-NPR with other methods in Synthetic (a) [28], LLFF (b) [27], zoomed-in on the flower and the occluded regions, and T&T (c) [21]. Semantic consistencies are highlighted.\\n\\nhave passed through the activation layers in stages 3 and 4 ($\\\\text{relu}^*3$, $\\\\text{relu}^*4$), and use them for $L_{\\\\text{feat}}$. Balancing factors among loss terms are set to $\\\\lambda' = 5 \\\\times 10^{-3}$, $\\\\lambda_c = 5$, and $\\\\lambda_r = \\\\lambda_f = 1$. We train each scene on one NVIDIA 3090 GPU for 10 epochs. Before training the final 3 epochs, for a smoother content update, we replace $\\\\omega_P$ with a frozen $\\\\omega_{NP}$ as the content view generator in TCM, and minimize $L_{\\\\text{ref}}$ and $L_{\\\\text{feat}}$ only, with $\\\\lambda_f = 0$.\\n\\n4.2. Datasets\\n\\nWe evaluate the performance of Ref-NPR on three standard datasets. Synthetic [28] is a well-defined synthetic dataset for 3D objects. Stylizing the views of 3D models with a foreground mask using a full-image style reference is a challenging yet meaningful task, which is missed in many existing scene stylization methods [15, 16, 30, 45]. To avoid overflow of the stylized view on the mask boundary, we apply a 2D morphological erosion on the foreground mask in $R^3$. LLFF [27] is a real-world high-resolution dataset for novel view synthesis, and we choose a resolution that is 4\u00d7 downsampled, following [10, 45]. Tanks and Temples (T&T) [21] is a 360\u00b0 scene dataset for novel view synthesis and scene reconstruction with 200 to 300 high-resolution training views for each scene. In $R^3$, we register only the rays in Plenoxels\u2019s foreground model [10] apart from the reference view to make the dictionary more compact. We follow the official training and testing splits for all datasets.\"}"}
{"id": "CVPR-2023-1764", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3. Comparisons\\n\\nWe compare our Ref-NPR with two recent scene stylization methods: ARF [45] and a reimplemented SNeRF on Plenoxels [10, 30]. Besides, we include a reference-based video stylization method by Texler et al. [38] for a more comprehensive comparison.\\n\\nQualitative comparison.\\n\\nWe present qualitative comparisons with state-of-the-art video and scene stylization methods in Fig. 5. The method of Texler et al. [38] produces novel views with proper low-level color distribution but suffers from limited correspondence in the scene, leading to flickering effects as shown in Fig. 5(a). ARF [45] and SNeRF [30] maintain geometric consistency in novel views but lack content-style correlation, whether the style reference is manually created (Fig. 5(a-b)) or generated by a 2D-neural stylization method [22] in Fig. 5(c). In contrast, Ref-NPR significantly improves both geometric and semantic-style consistency in each example. Additionally, Ref-NPR benefits from TCM to fill occluded regions with perceptually reasonable visual contents. More detailed comparisons can be found in the supplementary materials.\\n\\nQuantitative comparison.\\n\\nIn order to evaluate the quality of reference-based stylization, we adopt a metric used by CCPL [42] that measures the consistency between the style reference image $S_R$ and the top 10 nearest test novel views using LPIPS [47]. Additionally, we report the long-range cross-view consistency ability as a stability metric, following the approach of SNeRF. We also evaluate the robustness of the methods by running them iteratively and measuring the PSNR between rendered results. Further details on the evaluation metrics are provided in the appendix.\\n\\nThe results presented in Tab. 1 demonstrate that Ref-NPR achieves higher perceptual similarity and cross-view geometric consistency than state-of-the-art scene stylization methods. Moreover, we conduct a user study to evaluate the perceptual quality of the stylization sequences. We apply ten stylization sequences of four shuffled methods and collect 33 responses for each sequence based on the visual quality. The user study results are presented in Tab. 2.\\n\\n4.4. Ablation Studies\\n\\nWe perform several ablation studies to investigate the effectiveness of the different components of Ref-NPR. First, we conduct a module-wise ablation to evaluate the contributions of the supervision components in $R_3$ and TCM. Our results in Fig. 6 (a-d) demonstrate that the removal of any of these components leads to a degradation in the quality of the stylized images. For instance, removing the pseudo-ray supervision $L_{\\\\text{ref}}$ (a) or supervising on the reference view only (b) results in a loss of detailed textures. While discarding the feature supervision $L_{\\\\text{feat}}$ (c) causes a photo-realistic colorization in occluded regions. Moreover, neglecting the color supervision $L_{\\\\text{color}}$ (d) can cause color mismatches in occluded regions. The combination of $R_3$ and TCM in the full Ref-NPR model compensates for these shortcomings and leverages the strengths of each module.\\n\\nWe also validate the effectiveness of implicit supervision in TCM. As depicted in Fig. 7, we observed that without\"}"}
{"id": "CVPR-2023-1764", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. Ablation study on TCM. Benefiting from multi-view correspondence, TCM can effectively fill meaningful styles in occluded regions.\\n\\nTCM, the occluded regions often failed to obtain desired correspondence due to the lack of supervision in the semantic feature space. TCM effectively matches features within the same content domain. This demonstrates the crucial role of TCM in achieving accurate and consistent stylization.\\n\\n5. Discussions\\n\\nAdapt to general style transfer.\\n\\nWe have demonstrated the ability of Ref-NPR to use arbitrary style images as reference for style transfer. In particular, as shown in Fig. 8, we generate three reference views of the same scene using different 2D stylization methods [11, 20, 22] and feed them into Ref-NPR to render three sets of stylized novel views, each preserving the characteristics of the corresponding style reference. This extension allows Ref-NPR to be more flexible in working with various style reference images and provides a more controllable solution than other scene stylization methods [15, 16, 30, 45].\\n\\nFigure 8. The pipeline of Ref-NPR naturally extends to arbitrary style reference. Images are cropped for a better presentation. Method-related style textures are highlighted.\\n\\nMulti-reference. To accommodate the stylization of large-scale scenes, it is essential to extend Ref-NPR to handle multiple style references. This can be achieved by registering rays using all stylized reference views in $\\\\mathbb{R}^3$ and expanding the capacity of styles and content features in TCM. An example of multi-reference input in the Playground scene [21] is shown in Fig. 9. Using two additional stylized views, Ref-NPR achieves better feature matching and richer style content.\\n\\nFigure 9. Multi-reference results of a 360\u00b0 scene.\\n\\nLimitations.\\n\\nRef-NPR is a versatile method. However, it may not perform well when no meaningful semantic correspondence is found in the reference view, as depicted in Fig. 10. Additionally, feature matching may also fail in stylizing objects with intricate geometric structures.\\n\\nFigure 10. Failure cases of Ref-NPR. Feature matching may fail when (a) stylizing intricate geometric structures, or (b) stylizing a large scene with a single reference.\\n\\n6. Conclusion\\n\\nThis paper introduces Ref-NPR, a new framework for controllable non-photorealistic 3D scene stylization based on radiance fields. Ref-NPR can generate high-quality semantic correspondence and geometrically consistent stylizations for novel views by utilizing a stylized reference view. The proposed framework can potentially enhance the efficiency of human creativity in professional visual content creation.\"}"}
{"id": "CVPR-2023-1764", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nCurrent 3D scene stylization methods transfer textures and colors as styles using arbitrary style references, lacking meaningful semantic correspondences. We introduce Reference-Based Non-Photorealistic Radiance Fields (Ref-NPR) to address this limitation. This controllable method stylizes a 3D scene using radiance fields with a single stylized 2D view as a reference. We propose a ray registration process based on the stylized reference view to obtain pseudo-ray supervision in novel views. Then we exploit semantic correspondences in content images to fill occluded regions with perceptually similar styles, resulting in non-photorealistic and continuous novel view sequences. Our experimental results demonstrate that Ref-NPR outperforms existing scene and video stylization methods regarding visual quality and semantic correspondence. The code and data are publicly available on the project page at https://ref-npr.github.io.\\n\\n1. Introduction\\n\\nIn the past decade, there has been a rising demand for stylizing and editing 3D scenes and objects in various fields, including augmented reality, game scene design, and digital artwork. Traditionally, professionals achieve these tasks by creating 2D reference images and converting them into stylized 3D textures. However, establishing direct cross-modal correspondence is challenging and often requires significant time and effort to obtain stylized texture results similar to the 2D reference schematics.\\n\\nA critical challenge in the 3D stylization problem is to ensure that stylized results are perceptually similar to the given style reference. Benefiting from radiance fields [2, 10, 28, 29, 37, 46], recent novel-view stylization methods [5, 8, 15, 16, 30, 45] greatly facilitated style transfer from an arbitrary 2D style reference to 3D implicit representations. However, these methods do not provide explicit control over the generated results, making it challenging to specify the regions where certain styles should be applied and ensure the visual quality of the results. On the other hand, reference-based video stylization methods allow for the controllable generation of stylized novel views with better semantic correspondence between content and style reference, as demonstrated in works like [17, 38]. However, these methods may diverge from the desired style when stylizing a frame sequence with unseen content, even with the assistance of stylized keyframes.\\n\\nTo address the aforementioned limitations, we propose...\"}"}
{"id": "CVPR-2023-1764", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a new paradigm for stylizing 3D scenes using a single stylized reference view. Our approach, called Reference-Based Non-Photorealistic Radiance Fields (Ref-NPR), is a controllable scene stylization method that takes advantage of volume rendering to maintain cross-view consistency and establish semantic correspondence in transferring style across the entire scene.\\n\\nRef-NPR utilizes stylized views from radiance fields as references instead of arbitrary style images to achieve both flexible controllability and multi-view consistency. A reference-based ray registration process is designed to project the 2D style reference into 3D space by utilizing the depth rendering of the radiance field. This process provides pseudo-ray supervision to maintain geometric and perceptual consistency between stylized novel views and the stylized reference view. To obtain semantic style correspondence in occluded regions, Ref-NPR performs template-based feature matching, which uses high-level semantic features as implicit style supervision. The correspondence in the content domain is utilized to select style features in the given style reference, which are then used to transfer style globally, especially in occluded regions. By doing so, Ref-NPR generates the entire stylized 3D scene from a single stylized reference view.\\n\\nRef-NPR produces visually appealing stylized views that maintain both geometric and semantic consistency with the given style reference, as presented in Fig. 1. The generated stylized views are perceptually consistent with the reference while also exhibiting high visual quality across various datasets. We have demonstrated that Ref-NPR, when using the same stylized view as reference, outperforms state-of-the-art scene stylization methods [30, 45] both qualitatively and quantitatively.\\n\\nIn summary, our paper makes three contributions. Firstly, we introduce a new paradigm for stylizing 3D scenes that allows for greater controllability through the use of a stylized reference view. Secondly, we propose a novel approach called Ref-NPR, consisting of a reference-based ray registration process and a template-based feature matching scheme to achieve geometrically and perceptually consistent stylizations. Finally, our experiments demonstrate that Ref-NPR outperforms state-of-the-art scene stylization methods such as ARF and SNeRF both qualitatively and quantitatively. More comprehensive results and a demo video can be found in the supplementary material and on our project page.\\n\\n2. Related Works\\n\\n2.1. Stylization in 2D\\n\\nArbitrary style transfer is a well-studied problem in Non-Photorealistic Rendering (NPR) [12, 23]. Gatys et al. [11] first introduced the idea of representing image style as high-level features extracted from pre-trained deep neural networks. Since then, various parametric image style transfer methods [4, 18, 20, 24, 25] have been developed to generate high-quality stylized images efficiently. Video stylization methods that use arbitrary style input [3, 7, 32, 41, 42] mainly focus on maintaining temporal coherence to achieve continuous stylized frames. Nonetheless, these stylization methods lack interpretability and controllability, even when using stylized keyframes as reference.\\n\\nExample-based stylization methods use multiple style references to stylize images while ensuring semantic correspondences between them. Methods such as [14, 26] use multi-level semantic feature matching to establish dense correspondences between the content image and the style reference with similar semantics. To facilitate this feature matching, some methods use explicit alignments such as warping or content-aligned stylizing [17, 33, 34, 38]. These methods offer controllability by allowing editing of the style reference, but they are not suitable for stylizing novel views as they cannot correctly stylize unseen regions using 2D reference-based methods.\\n\\n2.2. Stylization in 3D\\n\\nReference-based 3D stylization methods have achieved promising results without the use of radiance fields, as seen in previous works such as Texture Map [1], Texture Field [31], StyLit [9, 36], and StyleProp [13]. However, these methods have their limitations. For instance, Texture Field is trained on a restricted ShapeNet dataset, limiting the stylization of 3D objects within the trained categories. StyLit, on the other hand, treats each view of a 3D object as multi-channel 2D guidance and applies texture mapping, resulting in flickering artifacts due to a lack of geometric prior. Although StyleProp utilizes multi-view correspondence maps to obtain stylized novel views, it can only work on viewing directions around the reference view.\\n\\nStylizing radiance fields has recently emerged as a popular topic in computer vision research, driven by the growing popularity of radiance fields [5, 8, 28]. Huang et al. [15] pioneered the application of scene stylization on implicit 3D representations, followed by StylizedNeRF [16], CLIP-NeRF [39], NeRF-Art [40], INS [8], SNeRF [30], and ARF [45], which focus on various degrees, like text-driven stylization in CLIP feature space, stylization using unified representation, memory efficiency, and stylization quality. However, these methods lack explicit correspondence modeling, leading to uncontrollable stylized novel views that differ perceptually from the style reference. To address this issue, we propose Ref-NPR. This reference-guided controllable scene stylization method generates stylized novel views with geometric and semantic consistency with a given stylized view reference.\"}"}
{"id": "CVPR-2023-1764", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The workflow of Ref-NPR. Given a pre-trained photorealistic radiance field $\\\\omega_{P}$, we can provide a stylized reference view $S_R$ to obtain reference-based supervisions $L_{\\\\text{ref}}$, $L_{\\\\text{feat}}$, and $L_{\\\\text{color}}$. Those loss constraints are used to optimize a non-photorealistic (NP) radiance field $\\\\omega_{NP}$. During inference, with such NP radiance field, stylized results $S_T$ could be rendered from an arbitrary set of camera poses $\\\\phi_T$, corresponding to the original views $I_T$ rendered by $\\\\omega_{P}$.\\n\\n3.1. Preliminary: Radiance Field Rendering\\n\\nVolume rendering [19, 28] uses camera rays to sample a 3D radiance field $\\\\omega$ to render images. A camera ray $r(t) = o + td$ is defined by an origin $o \\\\in \\\\mathbb{R}^3$ and a direction $d \\\\in \\\\mathbb{R}^3$ pointing towards the center of a pixel in the image. The radiance field is sampled at $N$ points along the ray, denoted by $\\\\{r(t_i) | i = 1, \\\\ldots, N, t_i < t_{i+1}\\\\}$. At each sample point, the radiance field returns a density value $\\\\sigma_i = \\\\sigma(r(t_i))$ and a view-dependent color $c_i = c(r(t_i), d)$. The accumulated pixel color $\\\\hat{C}(r)$ of the ray $r$ is estimated in the discrete context, as formulated in [28]:\\n\\n$$\\\\hat{C}(r) = \\\\sum_{i=1}^{N} T_i (1 - \\\\exp(-\\\\sigma_i (t_{i+1} - t_i))) c_i,$$\\n\\nwhere $T_i = \\\\exp(-\\\\sum_{j=1}^{i-1} \\\\sigma_j (t_{j+1} - t_j))$, which estimates the accumulated transmittance along the ray from $t_1$ to $t_i$. During training, the radiance field is optimized by directly minimizing the discrepancy between the predicted pixel color $\\\\hat{C}(r)$ and the ground truth pixel color $C(r)$ for each ray, which is denoted by $L_\\\\omega = \\\\sum_{r} \\\\| \\\\hat{C}(r) - C(r) \\\\|_2^2$. Once the radiance field is optimized, depth estimation can be performed by mapping the ray $r$ to an exact 3D position in the scene. This is achieved by setting a threshold $\\\\sigma_z$ on the accumulated density $T_i$ calculated in Equation 2. The first sample that exceeds this threshold is interpreted as the intersection point between the ray and the scene. The length of the ray $r$ is then defined as the distance between the camera origin $o$ and the intersection point, denoted by $l(r) = \\\\min \\\\{t_i | i - 1 \\\\sum_{j=1}^{i-1} \\\\sigma_j (t_{j+1} - t_j) \\\\geq \\\\sigma_z \\\\}$.\\n\\nThen we write the intersection point corresponding to ray $r$ as $x(r) = o + l(r) d$, which is our desired mapping.\"}"}
{"id": "CVPR-2023-1764", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reference Ray Registration.\\n\\nTo create a reference dictionary $D$ for a stylized reference view $S_R$ with camera pose $\\\\phi_R$, we estimate pseudo-depths from $\\\\omega_P$. Then, we apply a ray registration process to train camera poses and acquire a collection $\\\\Gamma_R$ consisting of pseudo-rays and their assigned colors.\\n\\n### 3.2. Reference Ray Registration\\n\\nRef-NPR differs from existing scene stylization methods [16, 30, 45] by incorporating an additional objective of establishing semantic consistency between the stylized reference view and novel views. This objective is also pursued by some scene modeling methods [6, 43, 44], which use 3D information such as depth to enhance rendering quality. Notably, precise 3D positions and camera parameters enable pixel-wise correspondence between views with a closed-form solution. This is utilized in many methods, such as homographic warping. In Ref-NPR, the radiance field $\\\\omega_P$ is used to estimate pseudo-depth information based on Eq. (4).\\n\\n3.3. Template-Based Semantic Correspondence\\n\\nWhile $R_3$ is effective in generating pseudo-ray supervision around the given reference camera pose $\\\\phi_R$, it struggles to register reference rays to occluded regions under such a camera, especially for 360\u00b0 scenes in [21, 28]. To overcome this limitation, we leverage the common assumption in scene stylization that the semantic correspondence in the entire scene should be consistent before and after stylization. Based on this, we use the stylized reference view $S_R$ based on source content $I_R$ to establish a content-style mapping as a template. We then broadcast such reference style to novel views with semantically similar content. To achieve this, we introduce a Template-based Correspondence Module (TCM) that utilizes this content-style correspondence to construct a semantic correlation within the content domain.\\n\\nAs illustrated in Fig. 4, for each content domain view $I$ rendered from $\\\\omega_P$ under some certain camera pose $\\\\phi \\\\in \\\\Phi$, we obtain its high-level semantic feature map $F_I$ from a pre-trained semantic feature extractor (e.g., VGG16 [35]). Similarly, we denote the extracted feature maps for content reference $I_R$ and style reference $S_R$ by $F_{I_R}$ and $F_{S_R}$, respectively, and use a superscript to index each element in the feature maps. We therefore construct our desired guidance feature $F_G$ for further supervision, described by a search-and-replace process on 2D position $(i, j)$. \\n\\n$$ F_G = \\\\{ F_{I_{\\\\phi_R}}_{(i, j)} = F_{S_R}^{(i, j)} | \\\\phi \\\\in \\\\Phi, \\\\phi \\\\neq \\\\emptyset \\\\} $$\"}"}
{"id": "CVPR-2023-1764", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Sai Bi, Nima Khademi Kalantari, and Ravi Ramamoorthi. Patch-based optimization for image-based texture mapping. ACM Trans. Graph., 36(4), 2017.\\n\\n[2] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In ECCV, 2022.\\n\\n[3] Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang Hua. Coherent online video style transfer. In ICCV, pages 1105\u20131114, 2017.\\n\\n[4] Tian Qi Chen and Mark W. Schmidt. Fast patch-based style transfer of arbitrary style. ArXiv, abs/1612.04337, 2016.\\n\\n[5] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-Sheng Lai, and Wei-Chen Chiu. Stylizing 3d scene via implicit representation and hypernetwork. In CVPR, pages 1475\u20131484, 2022.\\n\\n[6] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised NeRF: Fewer views and faster training for free. In CVPR, June 2022.\\n\\n[7] Yingying Deng, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma, and Changsheng Xu. Arbitrary video style transfer via multi-channel correlation. In AAAI, volume 35, pages 1210\u20131217, 2021.\\n\\n[8] Zhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia Xu, and Zhangyang Wang. Unified implicit neural stylization. In ECCV, pages 636\u2013654. Springer, 2022.\\n\\n[9] Jakub Fi\u0161er, Ond\u0159ej Jamri\u0161ka, Michal Luk\u00e1\u010d, Eli Shechtman, Paul Asente, Jingwan Lu, and Daniel \u0160ykora. Stylit: illumination-guided example-based stylization of 3d renderings. ACM Trans. Graph., 35(4):1\u201311, 2016.\\n\\n[10] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, 2022.\\n\\n[11] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In CVPR, pages 2414\u20132423, 2016.\\n\\n[12] Bruce Gooch and Amy Gooch. Non-photorealistic rendering. AK Peters/CRC Press, 2001.\\n\\n[13] Filip Hauptfleisch, Ondrej Texler, Aneta Texler, Jaroslav Kriv\u00e1nek, and Daniel \u0160ykora. Styleprop: Real-time example-based stylization of 3d models. In Computer Graphics Forum, volume 39, pages 575\u2013586. Wiley Online Library, 2020.\\n\\n[14] Mingming He, Jing Liao, Dongdong Chen, Lu Yuan, and Pedro V Sander. Progressive color transfer with dense semantic correspondences. ACM Trans. Graph., 38(2):1\u201318, 2019.\\n\\n[15] Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh Singh, and Ming-Hsuan Yang. Learning to stylize novel views. In ICCV, 2021.\\n\\n[16] Yi-Hua Huang, Yue He, Yu-Jie Yuan, Yu-Kun Lai, and Lin Gao. Stylizednerf: Consistent 3d scene stylization as stylized nerf via 2d-3d mutual learning. In CVPR, 2022.\\n\\n[17] Ond\u0159ej Jamri\u0161ka, \u02c7S\u00e1rka Sochorov\u00e1, Ond\u0159ej Texler, Michal Luk\u00e1\u010d, Jakub Fi\u0161er, Jingwan Lu, Eli Shechtman, and Daniel \u0160ykora. Stylizing video by example. ACM Trans. Graph., 38(4):1\u201311, 2019.\\n\\n[18] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, pages 694\u2013711. Springer, 2016.\\n\\n[19] James T Kajiya and Brian P V on Herzen. Ray tracing volume densities. ACM Trans. Graph., 18(3):165\u2013174, 1984.\\n\\n[20] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, pages 4401\u20134410, 2019.\\n\\n[21] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Trans. Graph., 36(4), 2017.\\n\\n[22] Nicholas Kolkin, Michal Kucera, Sylvain Paris, Daniel Sykora, Eli Shechtman, and Greg Shakhnarovich. Neural neighbor style transfer. arXiv e-prints, pages arXiv\u20132203, 2022.\\n\\n[23] Jan Eric Kyprianidis, John Collomosse, Tinghuai Wang, and Tobias Isenberg. State of the art: A taxonomy of artistic stylization techniques for images and video. TVCG, 19(5):866\u2013885, 2012.\\n\\n[24] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature transforms. NeurIPS, 30, 2017.\\n\\n[25] Yijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, and Jan Kautz. A closed-form solution to photorealistic image stylization. In ECCV, pages 453\u2013468, 2018.\\n\\n[26] Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, and Sing Bing Kang. Visual attribute transfer through deep image analogy. ACM Trans. Graph., 36(4):120, 2017.\\n\\n[27] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Trans. Graph., 2019.\\n\\n[28] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\\n\\n[29] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013102:15, July 2022.\\n\\n[30] Thu Nguyen-Phuoc, Feng Liu, and Lei Xiao. Snerf: stylized neural implicit representations for 3d scenes. arXiv preprint arXiv:2207.02363, 2022.\\n\\n[31] Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, and Andreas Geiger. Texture fields: Learning texture representations in function space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4531\u20134540, 2019.\\n\\n[32] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox. Artistic style transfer for videos and spherical images. IJCV, 126(11):1199\u20131219, 2018.\\n\\n[33] Ahmed Selim, Mohamed Elgharib, and Linda Doyle. Painting style transfer for head portraits using convolutional neural networks. ACM Trans. Graph., 35(4):1\u201318, 2016.\"}"}
{"id": "CVPR-2023-1764", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"YiChang Shih, Sylvain Paris, Connelly Barnes, William T. Freeman, and Fr\u00e9do Durand. Style transfer for headshot portraits. ACM Trans. Graph., 2014.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, ICLR, 2015.\\n\\nDaniel S\u00fdkora, Ond\u0159ej Jamriska, Ondrej Texler, Jakub Fi\u0161er, Michal Luk\u00e1\u010d, Jingwan Lu, and Eli Shechtman. Styleblit: Fast example-based stylization with local guidance. In Computer Graphics Forum, volume 38, pages 83\u201391. Wiley Online Library, 2019.\\n\\nMatthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P. Srinivasan, Jonathan T. Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In CVPR, pages 8248\u20138258, June 2022.\\n\\nOnd\u0159ej Texler, David Futschik, Michal Kucer, Ond\u0159ej Jamriska, \u0160\u00e1rka Sochorov\u00e1, Menglei Chai, Sergey Tulyakov, and Daniel S\u00fdkora. Interactive video stylization using few-shot patch-based training. ACM Trans. Graph., 39(4):73, 2020.\\n\\nCan Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Clip-nerf: Text-and-image driven manipulation of neural radiance fields. In CVPR, pages 3835\u20133844, 2022.\\n\\nCan Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Nerf-art: Text-driven neural radiance fields stylization. arXiv preprint arXiv:2212.08070, 2022.\\n\\nWenjing Wang, Shuai Yang, Jizheng Xu, and Jiaying Liu. Consistent video style transfer via relaxation and regularization. TIP, 29:9125\u20139139, 2020.\\n\\nZijie Wu, Zhen Zhu, Junping Du, and Xiang Bai. Ccpl: Contrastive coherence preserving loss for versatile style transfer. In ECCV, 2022.\\n\\nDejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang Wang. Sinnerf: Training neural radiance fields on complex scenes from a single image. In ECCV, 2022.\\n\\nQiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In CVPR, pages 5438\u20135448, 2022.\\n\\nKai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, and Noah Snavely. Arf: Artistic radiance fields. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXI, pages 717\u2013733. Springer, 2022.\\n\\nKai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv:2010.07492, 2020.\\n\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.\"}"}
