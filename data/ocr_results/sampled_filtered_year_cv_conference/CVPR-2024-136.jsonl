{"id": "CVPR-2024-136", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Text-to-4D synthesis with AYG.\\n\\nVarious samples shown in two views each. Dotted lines denote deformation field dynamics.\\n\\nWe disentangle optimization into first synthesizing a static 3D Gaussian-based object $\\\\theta$, and then learning the deformation field $\\\\Phi$ to add scene dynamics.\\n\\nStage 1: 3D Synthesis (Fig. 3). We first use MV-Dream's multiview image prior to generate a static 3D scene via score distillation (Supp. Mat. for details). Since MV-Dream on its own would generate objects in random orientations, we enforce a canonical pose by combining MV-Dream's gradients with those of regular SD, while augmenting the text-conditioning for SD with directional texts \\\"front view\\\", \\\"side view\\\", \\\"back view\\\" and \\\"overhead view\\\" [62]. Formally, we can derive a score distillation gradient (see Sec. 3.3) by minimizing the reverse Kulback-Leibler divergence (KLD) from the rendering distribution to the product of the composed MVDream and SD model distributions.\\n\\n$$KL_{q_{\\\\theta}}\\\\{z_{c_{i}}\\\\}^{4}, \\\\{z_{\\\\tilde{c}_{j}}\\\\}_{K}^{P_{\\\\alpha 3D}}(\\\\{z_{c_{i}}\\\\}^{4}) \\\\sum_{j=1}^{K} p_{\\\\beta_{im}}(\\\\tilde{z}_{\\\\tilde{c}_{j}})$$\\n\\nSimilar to Poole et al. [62] (App. A.4). Here, $p_{3D}(\\\\{z_{c_{i}}\\\\}^{4})$ represents the MVDream-defined multiview image distribution over four diffused renderings from camera views $c_{i}$, denoted as the set $\\\\{z_{c_{i}}\\\\}^{4}$ (we omit the diffusion time $t$ subscript for brevity). Moreover, $p_{im}(\\\\tilde{z}_{\\\\tilde{c}_{j}})$ is the SD-based general image prior and $\\\\{\\\\tilde{z}_{\\\\tilde{c}_{j}}\\\\}_{K}$ is another set of $K$ diffused scene renderings. In principle, the renderings for SD and MVDream can be from different camera angles $c_{i}$ and $\\\\tilde{c}_{j}$, but in practice we choose $K=4$ and use the same renderings. Furthermore, $\\\\alpha$ and $\\\\beta$ are adjustable temperatures of the distributions $p_{3D}$ and $p_{im}$, and $q_{\\\\theta}$ denotes the distribution over diffused renderings defined by the underlying 3D scene representation $\\\\theta$, which is optimized through the differentiable rendering. We also use the Gaussian densification method discussed in Sec. 2 (see Supp. Material).\\n\\nStage 2: Adding Dynamics for 4D Synthesis (Fig. 2). While in stage 1, we only optimize the 3D Gaussians, in stage 2, the main 4D stage, we optimize (only) the deformation field $\\\\Phi$ to capture motion and extend the static 3D scene to a dynamic 4D scene with temporal dimension $\\\\tau$.\\n\\nTo this end, we compose the text-to-image and text-to-video DMs and formally minimize a reverse KLD of the form:\\n\\n$$KL_{q_{\\\\Phi}}\\\\{z_{c_{i}}_{\\\\tau_{i}}\\\\}^{F}, \\\\{\\\\tilde{z}_{\\\\tilde{c}_{j}}_{\\\\tau_{j}}\\\\}_{M}^{P_{\\\\gamma_{vid}}}\\\\{z_{c_{i}}_{\\\\tau_{i}}\\\\}^{F}_{M} \\\\sum_{j=1}^{M} p_{\\\\kappa_{im}}(\\\\tilde{z}_{\\\\tilde{c}_{j}}_{\\\\tau_{j}})$$\\n\\nwhere $p_{vid}(\\\\{z_{c_{i}}_{\\\\tau_{i}}\\\\}^{F})$ is the video DM-defined distribution.\"}"}
{"id": "CVPR-2024-136", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AYG is able to autoregressively extend dynamic 4D sequences, combine sequences with different text-guidance, and create looping animations, returning to the initial pose (also see Supp. Video).\\n\\n\\\\[ F = \\\\{ z_c, \\\\tau \\\\} \\\\]\\n\\nover 4D scene renderings \\\\( F \\\\) taken at times \\\\( \\\\tau \\\\) and camera angles \\\\( c \\\\) (\\\\( F = 16 \\\\) for our model). Similar to before, \\\\( M \\\\) additional renderings are given to the SD-based general image prior, and \\\\( \\\\gamma \\\\) and \\\\( \\\\kappa \\\\) are temperatures. The renderings \\\\( \\\\{ \\\\tilde{z}, \\\\tilde{c}_j, \\\\tilde{\\\\tau}_j \\\\} \\\\) fed to regular SD can be taken at different times \\\\( \\\\tilde{\\\\tau}_j \\\\) and cameras \\\\( \\\\tilde{c}_j \\\\) than the video model frames, but in practice \\\\( M = 4 \\\\) and we use three random renderings as well as the 8th middle frame among the ones given to the video model.\\n\\n\\\\( q\\\\Phi \\\\) defines the distribution over renderings by the 4D scene with the learnable deformation field parameters \\\\( \\\\Phi \\\\). We could render videos from the 4D scene with a fixed camera, but in practice dynamic cameras, i.e. varying \\\\( c \\\\), help to learn more vivid 4D scenes, similar to Singer et al. [79]. Moreover, following Singer et al. [79], our video DM is conditioned on the frame rate (fps) and we choose the times \\\\( 0 \\\\leq \\\\tau_i \\\\leq 1 \\\\) accordingly by sampling fps \\\\( \\\\in \\\\{4, 8, 12\\\\} \\\\) and the starting time.\\n\\nWe render videos from the 4D scene and condition the video DM with the sampled fps. This helps generating not only sufficiently long but also temporally smooth 4D animations, as different fps correspond to long-term and short-term dynamics. Therefore, when rendering short but high fps videos they only span part of the entire length of the 4D sequence. Also see Supp. Material.\\n\\nOptimizing the deformation field while supervising both with a video and image DM is crucial. The video DM generates temporal dynamics, but text-to-video DMs are not as robust as general text-to-image DMs. Including the image DM during this stage ensures stable optimization and that high visual frame quality is maintained (ablations in Sec. 4).\\n\\nA crucial advantage of the disentangled two stage design is that AYG's main 4D synthesis method\u2014the main innovation of this work\u2014could in the future in principle also be applied to 3D objects originating from other generation systems or even to synthetic assets created by digital artists.\\n\\n3.3. AYG's Score Distillation in Practice\\n\\nAbove, we have laid out AYG's general synthesis framework. The full stage 2 score distillation gradient including CFG can be expressed as (stage 1 proceeds analogously)\\n\\n\\\\[\\n\\\\nabla \\\\Phi L_{AYG}^{SDS} = E_{t, \\\\epsilon_{vid}, \\\\epsilon_{im}} w(t) \\\\left( \\\\hat{\\\\epsilon}_{vid}(Z, v, t) - \\\\hat{\\\\epsilon}_{vid}(Z, t) \\\\right) + \\\\hat{\\\\epsilon}_{vid}(Z, v, t) - \\\\epsilon_{vid} \\\\| \\\\delta_{vidgen} \\\\| + \\\\omega_{im} \\\\hat{\\\\epsilon}_{im}(\\\\tilde{Z}, v, t) - \\\\hat{\\\\epsilon}_{im}(\\\\tilde{Z}, t) \\\\| \\\\delta_{imgen} \\\\| \\\\partial \\\\{ x \\\\} \\\\partial \\\\Phi, (3)\\n\\\\]\\n\\nwhere \\\\( Z := \\\\{ z_c, \\\\tau_i \\\\} F \\\\), \\\\( \\\\tilde{Z} := \\\\{ \\\\tilde{z}, \\\\tilde{c}_j, \\\\tilde{\\\\tau}_j \\\\} M \\\\), \\\\( \\\\omega_{vid/im} \\\\) are the CFG scales for the video and image DMs, \\\\( \\\\hat{\\\\epsilon}_{vid}(Z, v, t) \\\\) and \\\\( \\\\hat{\\\\epsilon}_{im}(\\\\tilde{Z}, v, t) \\\\) are the corresponding denoiser networks and \\\\( \\\\epsilon_{vid} \\\\) and \\\\( \\\\epsilon_{im} \\\\) are the diffusion noises (an analogous SDS gradient can be written for stage 1). Moreover, \\\\( \\\\{ x \\\\} \\\\) denotes the set of all renderings from the 4D scene through which the SDS gradient is backpropagated, and which are diffused to produce \\\\( Z \\\\) and \\\\( \\\\tilde{Z} \\\\). Recently, ProlificDreamer [92] proposed a scheme where the control variates \\\\( \\\\epsilon_{vid/im} \\\\) above are replaced by DMs that model the rendering distribution, are initialized from the DMs guiding the synthesis (\\\\( \\\\hat{\\\\epsilon}_{vid}(Z, v, t) \\\\) and \\\\( \\\\hat{\\\\epsilon}_{im}(\\\\tilde{Z}, v, t) \\\\) here), and are then slowly fine-tuned on the diffused renderings (\\\\( Z \\\\) or \\\\( \\\\tilde{Z} \\\\) here). This means that at the beginning of optimization the terms \\\\( \\\\delta_{vid/im} \\\\) in Eq. (3) would be zero. Inspired by this observation and aiming to avoid ProlificDreamer's cumbersome fine-tuning, we instead propose to simply set \\\\( \\\\delta_{vid/im} = 0 \\\\) entirely and optimize with\\n\\n\\\\[\\n\\\\nabla \\\\Phi L_{AYG}^{CSD} = E_{t, \\\\epsilon_{vid}, \\\\epsilon_{im}} w(t) \\\\left( \\\\hat{\\\\epsilon}_{vid}(Z, v, t) - \\\\hat{\\\\epsilon}_{vid}(Z, t) \\\\right) \\\\| \\\\delta_{vidcls} \\\\| + \\\\omega_{im} \\\\hat{\\\\epsilon}_{im}(\\\\tilde{Z}, v, t) - \\\\hat{\\\\epsilon}_{im}(\\\\tilde{Z}, t) \\\\| \\\\delta_{imcls} \\\\| \\\\partial \\\\{ x \\\\} \\\\partial \\\\Phi, (4)\\n\\\\]\\n\\nwhere we absorbed \\\\( \\\\gamma \\\\) and \\\\( \\\\kappa \\\\) into \\\\( \\\\omega_{vid/im} \\\\). Interestingly, this exactly corresponds to the concurrently proposed classifier score distillation (CSD) [102], which points out that the above two terms \\\\( \\\\delta_{vid/im} \\\\) in Eq. (4) correspond to implicit classifiers predicting \\\\( v \\\\) from the video or images, respectively. CSD then uses only \\\\( \\\\delta_{vid/im} \\\\) for score distillation, resulting in improved performance over SDS. We discovered that scheme independently, while aiming to inherit ProlificDreamer's strong performance. Supp. Material for details.\"}"}
{"id": "CVPR-2024-136", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We show four 4D frames for different times and camera angles (also see Supp. Video).\\n\\n### 3.4. Scaling Align Your Gaussians\\n\\nTo scale AYG and achieve state-of-the-art text-to-4D performance, we introduce several further novel techniques.\\n\\n#### Distribution Regularization of 4D Gaussians\\n\\nWe developed a method to stabilize optimization and ensure realistic learnt motion. We calculate the means $\\\\nu_\\\\tau$ and diagonal covariances $\\\\Gamma_\\\\tau$ of the entire set of 3D Gaussians (using their means $\\\\mu_i$) at times $\\\\tau$ along the 4D sequence (Fig. 4). Defining a Normal distribution $N(\\\\nu_\\\\tau, \\\\Gamma_\\\\tau)$ with these means and covariances, we regularize with a modified version of the Jensen-Shannon divergence $\\\\text{JSD}(N(\\\\nu_0, \\\\Gamma_0) \\\\| N(\\\\nu_\\\\tau, \\\\Gamma_\\\\tau))$ between the 3D Gaussians at the initial and later frames (see Supp. Material). This ensures that the mean and the diagonal covariance of the distribution of the Gaussians stay approximately constant and encourages AYG to generate meaningful and complex dynamics instead of simple global translations and object size changes.\\n\\n#### Extended Autoregressive Generation\\n\\nBy default, AYG produces relatively short 4D sequences, which is due to the guiding text-to-video model, which itself only generates short video clips (see Blattmann et al. [7]). To overcome this limitation, we developed a method to autoregressively extend the 4D sequences. We use the middle 4D frame from a first sequence as the initial frame of a second sequence, optimizing a second deformation field, optionally using a different text prompt. As the second sequence is initialized from the middle frame of the first sequence, there is an overlap interval with length $5$ of the total length of each sequence. When optimizing for the second deformation field, we smoothly interpolate between the first and second deformation fields for the overlap region (Fig. 5). Specifically, we define\\n\\n$$\\\\Delta_{\\\\text{interpol}} \\\\Phi_{12} = (1 - \\\\chi(\\\\tau)) \\\\Delta \\\\Phi_1 + \\\\chi(\\\\tau) \\\\Delta \\\\Phi_2$$\\n\\nwhere $\\\\chi$ is a linear function with $\\\\chi(\\\\tau_{0.5}) = 0$ and $\\\\chi(\\\\tau_{1.0}) = 1$, $\\\\tau_{0.5}$ and $\\\\tau_{1.0}$ represent the middle and last time frames of the first sequence, $\\\\Delta_{\\\\text{interpol}} \\\\Phi_{12}$ is the interpolated deformation field, and $\\\\Delta \\\\Phi_1$ (kept fixed) and $\\\\Delta \\\\Phi_2$ are the deformation fields of the first and second sequence, respectively. We additionally minimize\\n\\n$$L_{\\\\text{Interpol-Reg.}} = ||\\\\Delta \\\\Phi_1 - \\\\Delta_{\\\\text{interpol}} \\\\Phi_{12}||^2$$\\n\\nwithin the overlap region to regularize the optimization process of $\\\\Delta \\\\Phi_2$. For the non-overlap regions, we just use the corresponding $\\\\Delta \\\\Phi_i$.\\n\\nWithout it, we obtained abrupt, unrealistic transitions.\\n\\n#### Motion Amplification\\n\\nWhen a set of 4D scene renderings is given to the text-to-video model, it produces a (classifier) score distillation gradient for each frame $i$. We expect most motion when the gradient for each frame points into a different direction. With that in mind, we propose a motion amplification technique. We post-process the video model's individual frame scores $\\\\delta_{\\\\text{vid}}\\\\text{cls}_i (i \\\\in \\\\{1, \\\\ldots, F\\\\})$ as\\n\\n$$\\\\delta_{\\\\text{vid}}\\\\text{cls}_i \\\\rightarrow \\\\delta_{\\\\text{vid}}\\\\text{cls}_i + \\\\omega_{\\\\text{ma}} \\\\delta_{\\\\text{vid}}\\\\text{cls}_i - \\\\delta_{\\\\text{vid}}\\\\text{cls}_i,$$\\n\\nwhere $\\\\delta_{\\\\text{vid}}\\\\text{cls}_i$ is the average score over the $F$ video frames and $\\\\omega_{\\\\text{ma}}$ is the motion amplifier scale. This scheme is inspired by CFG and reproduces regular video model scores for $\\\\omega_{\\\\text{ma}}=1$. For larger $\\\\omega_{\\\\text{ma}}$, the difference between the individual frames' scores and the average is amplified, thereby encouraging larger frame differences and more motion.\\n\\n#### View Guidance\\n\\nIn AYG's 3D stage, for the text-to-image model we use a new view guidance. We construct an additional implicit classifier term $\\\\omega_{\\\\text{vg}} \\\\hat{\\\\epsilon}_{\\\\text{im}}(z, v_{\\\\text{aug}}, t) - \\\\hat{\\\\epsilon}_{\\\\text{im}}(z, v, t)$, where $v_{\\\\text{aug}}$ denotes the original text prompt $v$ augmented with directional texts such as \\\"front view\\\" (see Sec. 3.2) and $\\\\omega_{\\\\text{vg}}$ is the guidance scale. View guidance amplifies the effect of directional text prompt augmentation.\\n\\n#### Negative Prompting\\n\\nWe also use negative prompting guidance during both the 3D and 4D stages. During the 4D stage, we use \\\"low motion, static statue, not moving, no motion\\\" to encourage AYG to generate more dynamic and vivid 4D scenes. Supp. Material for 3D stage and details.\\n\\n### 4. Experiments\\n\\n#### Text-to-4D\\n\\nIn Fig. 6, we show text-to-4D sequences generated by AYG (hyperparameters and details in Supp. Material).\"}"}
{"id": "CVPR-2024-136", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Ablation study by user study on synthesized 4D scenes with 30 text prompts. For each pair of numbers, the left number is the percentage that the full AYG model is preferred and the right number indicates preference percentage for ablated model as described in left column. The numbers do not add up to 100 and the difference is due to users voting \u201cno preference\u201d (details in Supp. Material).\\n\\nAYG can generate realistic, expressive, detailed and vivid dynamic 4D scenes (4D scenes can be rendered at varying speeds and frame rates). Importantly, our method demonstrates zero-shot generalization capabilities to creative text prompts corresponding to scenes that are unlikely to be found in the diffusion models\u2019 training images and videos. More results in Supp. Material and on project page.\\n\\nTo compare AYG to MAV3D [79], we performed a comprehensive user study where we took the 28 rendered videos from MAV3D\u2019s project page and compared them to corresponding generations from AYG with the same text prompts (Table 1). We asked the users to rate overall quality, 3D appearance and text alignment, as well as motion amount, motion text alignment and motion realism (user study details in Supp. Material). AYG outperforms MAV3D on all metrics, achieving state-of-the-art text-to-4D performance (we also evaluated R-Precision [32, 58] on a larger prompt set used by MAV3D [78, 79], performing on par, see Supp. Mat.; however, R-Precision is a meaningless metric to evaluate dynamic scenes). Qualitative comparisons are shown in Fig. 8 (more in Supp. Mat.). We see that AYG produces more detailed 4D outputs. Note that MAV3D uses an extra background model, while AYG does not. Adding a similar background model would be easy but is left to future work.\\n\\nAblation Studies. Next, we performed an ablation study on AYG\u2019s different components. We used a set of 30 text prompts and generated 4D scenes for versions of AYG with missing or modified components, see Table 2. Using the same categories as before, we asked users to rate preference of our full method vs. the ablated AYG variants. Some components have different effects with respect to 3D appearance and motion, but we generally see that all components matter significantly in terms of overall quality, i.e., for all ablations our full method is strongly preferred over the ablated AYG versions. This justifies AYG\u2019s design. A thorough discussion is presented in the Supp. Material, but we highlight some relevant observations. We see that our novel JSD-based regularization makes a major difference, and we also observe that the motion amplifier indeed has a strong effect for \u201cMotion Amount\u201d. Moreover, our compositional approach is crucial. Running the 4D stage without image DM feedback produces much worse 3D and overall quality. Also the decomposition into two stages is important\u2014carrying out 4D synthesis without initial 3D stage performs poorly.\\n\\nTemporally Extended 4D Synthesis and Large Scene Composition. In Fig. 7, we show autoregressively extended text-to-4D results with changing text prompts (also see Supp. Video). AYG can realistically connect different 4D sequences and generate expressive animations with changing dynamics and behavior. We can also create sequences that loop endlessly by enforcing that the last frame of a later sequence matches the first frame of an earlier one and suppressing the deformation field there (similar to how we enforce zero deformation at $\\\\tau = 0$ in Sec. 3.1). Finally, due to the explicit nature of the dynamic 3D Gaussians, AYG\u2019s 4D representation, multiple animated 4D objects can be easily composed into larger scenes, each shape with its own deformation field defining its dynamics. We show this in Fig. 1, where each dynamic object in the large scene is generated, except for the ground plane. These capabilities, not shown by previous work [79], are particularly promising for practical content creation applications.\\n\\n5. Conclusions\\n\\nWe presented Align Your Gaussians for expressive text-to-4D synthesis. AYG builds on dynamic 3D Gaussian Splatting with deformation fields as well as score distillation with multiple composed diffusion models. Novel regularization and guidance techniques allow us to achieve state-of-the-art dynamic scene generation and we also show temporally extended 4D synthesis as well as the composition of multiple dynamic objects within a larger scene. AYG has many potential applications for creative content creation and it could also be used in the context of synthetic data generation. For example, AYG would enable synthesis of videos and 4D sequences with exact tracking labels, useful for training discriminative models. AYG currently cannot easily produce topological changes of the dynamic objects. Overcoming this limitation would be an exciting avenue for future work. Other directions include scaling AYG beyond object-centric generation and personalized 4D synthesis. The initial 3D object could be generated from a personalized diffusion model (e.g. DreamBooth3D [66, 71]) or with image-to-3D methods [29, 42, 44, 45, 64] and then animated with AYG.\"}"}
{"id": "CVPR-2024-136", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models\\n\\nHuan Ling 1,2,3 *  \\nSeung Wook Kim 1,2,3 *  \\nAntonio Torralba 4  \\nSanja Fidler 1,2,3  \\nKarsten Kreis 1  \\n\\n1 NVIDIA  \\n2 Vector Institute  \\n3 University of Toronto  \\n4 MIT\\n\\nProject page:  \\nhttps://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/\\n\\nFigure 1. Text-to-4D synthesis with Align Your Gaussians (AYG). Top: Different dynamic 4D sequences. Dotted lines represent dynamics of deformation field. Bottom: Multiple dynamic 4D objects are composed within a large dynamic scene; two time frames shown.\\n\\nAbstract\\n\\nText-guided diffusion models have revolutionized image and video generation and have also been successfully used for optimization-based 3D object synthesis. Here, we instead focus on the underexplored text-to-4D setting and synthesize dynamic, animated 3D objects using score distillation methods with an additional temporal dimension. Compared to previous work, we pursue a novel compositional generation-based approach, and combine text-to-image, text-to-video, and 3D-aware multiview diffusion models to provide feedback during 4D object optimization, thereby simultaneously enforcing temporal consistency, high-quality visual appearance and realistic geometry. Our method, called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with deformation fields as 4D representation. Crucial to AYG is a novel method to regularize the distribution of the moving 3D Gaussians and thereby stabilize the optimization and induce motion. We also propose a motion amplification mechanism as well as a new autoregressive synthesis scheme to generate and combine multiple 4D sequences for longer generation. These techniques allow us to synthesize vivid dynamic scenes, outperform previous work qualitatively and quantitatively and achieve state-of-the-art text-to-4D performance. Due to the Gaussian 4D representation, different 4D animations can be seamlessly combined, as we demonstrate. AYG opens up promising avenues for animation, simulation and digital content creation as well as synthetic data generation.\\n\\n1. Introduction\\n\\nGenerative modeling of dynamic 3D scenes has the potential to revolutionize how we create games, movies, simulation and digital content creation as well as synthetic data generation.\\n\\n*Equal contribution.\"}"}
{"id": "CVPR-2024-136", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Text-to-4D synthesis with AYG. We generate dynamic 4D scenes via score distillation. We initialize the 4D sequence from a static 3D scene (generated first, Fig. 3), which is represented by 3D Gaussians with means $\\\\mu_i$, scales $\\\\sigma_i$, opacities $\\\\eta_i$ and colors $\\\\ell_i$. Consecutive rendered frames $x_{c j \\\\tau_j}$ from the 4D sequence at times $\\\\tau_j$ and camera positions $c_j$ are diffused and fed to a text-to-video diffusion model [7] (green arrows), which provides a distillation gradient that is backpropagated through the rendering process into a deformation field $\\\\Delta(x, y, z, \\\\tau)$ (dotted lines) that captures scene motion. Simultaneously, random frames $\\\\tilde{x}_{c j \\\\tau_j}$ are diffused and given to a text-to-image diffusion model [70] (red arrows) whose gradients ensure that high visual quality is maintained frame-wise.\\n\\nRelations, animations and entire virtual worlds. Many works have shown how a wide variety of 3D objects can be synthesized via score distillation techniques [10, 11, 31, 41, 52, 62, 79, 85, 88, 92, 109], but they typically only synthesize static 3D scenes, although we live in a moving, dynamic world. While image diffusion models have been successfully extended to video generation [1, 7, 22, 28, 78, 90, 91, 107], there is little research on similarly extending 3D synthesis to 4D generation with an additional temporal dimension.\\n\\nWe propose Align Your Gaussians (AYG), a novel method for 4D content creation. In contrast to previous work [79], we leverage dynamic 3D Gaussians [36] as backbone 4D representation, where a deformation field [59, 63] captures scene dynamics and transforms the collection of 3D Gaussians to represent object motion. AYG takes a compositional generation-based perspective and leverages the combined gradients of latent text-to-image [70], text-to-video [7] and 3D-aware text-to-multiview-image [76] diffusion models in a score distillation-based synthesis framework. A 3D-aware multiview diffusion model and a regular text-to-image model are used to generate an initial high-quality 3D shape. Afterwards, we compose the gradients of a text-to-video and a text-to-image model; the gradients of the text-to-video model optimize the deformation field to capture temporal dynamics, while the text-to-image model ensures that high visual quality is maintained for all time frames (Fig. 2). To this end, we trained a dedicated text-to-video model; it is conditioned on the frame rate and can create useful gradients both for short and long time intervals, which allows us to generate long and smooth 4D sequences.\\n\\nWe developed several techniques to ensure stable optimization and learn vivid dynamic 4D scenes in AYG: We employ a novel regularization method that uses a modified version of the Jensen-Shannon divergence to regularize the locations of the 3D Gaussians such that the mean and variance of the set of 3D Gaussians is preserved as they move. Furthermore, we use a motion amplification method that carefully scales the gradients from the text-to-video model and enhances motion. To extend the length of the 4D sequences or combine different dynamic scenes with changing text guidance, we introduce an autoregressive generation scheme which interpolates the deformation fields of consecutive sequences. We also propose a new view-guidance method to generate consistent 3D scenes for initialization of the 4D stage, and we leverage the concurrent classifier score distillation method [102].\\n\\nWe find that AYG can generate diverse, vivid, detailed and 3D-consistent dynamic scenes (Fig. 1), achieving state-of-the-art text-to-4D performance. We also show long, autoregressively extended 4D scenes, including ones with varying text guidance, which has not been demonstrated before. A crucial advantage of AYG's 4D Gaussian backbone representation is that different 4D animations can trivially be combined and composed together, which we also show.\\n\\nWe envision broad applications in digital content creation, where AYG takes a step beyond the literature on text-to-3D and captures our world's rich dynamics. Moreover, AYG can generate 4D scenes with exact tracking labels for free, a promising feature for synthetic data generation.\\n\\nContributions. (i) We propose AYG, a system for text-to-4D content creation leveraging dynamic 3D Gaussians with deformation fields as 4D representation. (ii) We show how to tackle the text-to-4D task through score distillation within a new compositional generation framework, combining 2D, 3D, and video diffusion models. (iii) To scale AYG, we introduce a novel regularization method and a new motion amplification technique. (iv) Experimentally, we achieve state-of-the-art text-to-4D performance and generate high-quality, diverse, and dynamic 4D scenes. (v) For the first time, we also show how our 4D sequences can be extended in time with a new autoregressive generation scheme and even creatively composed in large scenes.\"}"}
{"id": "CVPR-2024-136", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Background\\n\\n3D Gaussian Splatting [36] represents 3D scenes by $N$ 3D Gaussians with positions $\\\\mu_i$, covariances $\\\\Sigma_i$, opacities $\\\\eta_i$ and colors $\\\\ell_i$ (Fig. 2). Rendering corresponds to projection of the 3D Gaussians onto the 2D camera's image plane, producing 2D Gaussians with projected means $\\\\hat{\\\\mu}_i$ and covariances $\\\\hat{\\\\Sigma}_i$. The color $C(p)$ of image pixel $p$ can be calculated through point-based volume rendering [111] as\\n\\n$$C(p) = \\\\sum_{i=1}^{N} \\\\ell_i \\\\alpha_i \\\\prod_{j=1}^{1} (1 - \\\\alpha_j),$$\\n\\nwhere $j$ iterates over the Gaussians along the ray through the scene from pixel $p$ until Gaussian $i$. To accelerate rendering, the image plane can be divided into tiles, which are processed in parallel. Initially proposed for 3D scene reconstruction, 3D Gaussian Splatting uses gradient-based thresholding to densify areas that need more Gaussians to capture fine details, and unnecessary Gaussians with low opacity are pruned every few thousand optimization steps.\\n\\nDiffusion Models and Score Distillation Sampling.\\n\\nDiffusion-based generative models (DMs) [18, 27, 57, 80, 81] use a forward diffusion process that gradually perturbs data, such as images or entire videos, towards entirely random noise, while a neural network is learnt to denoise and reconstruct the data. DMs have also been widely used for score distillation-based generation of 3D objects [62]. In that case, a 3D object, represented for instance by a neural radiance field (NeRF) [54] or 3D Gaussians [36], like here, with parameters $\\\\theta$ is rendered from different camera views and the renderings $x$ are diffused and given to a text-to-image DM. In the score distillation sampling (SDS) framework, the DM's denoiser is then used to construct a gradient that is backpropagated through the differentiable rendering process $g$ into the 3D scene representation and updates the scene representation to make the scene rendering look more realistic, like images modeled by the DM. Rendering and using DM feedback from many different camera perspectives then encourages the scene representation to form a geometrically consistent 3D scene.\\n\\nThe SDS gradient [62] is\\n\\n$$\\\\nabla_{\\\\theta} L_{SDS}(x = g(\\\\theta)) = E_{t, \\\\epsilon} w(t) (\\\\hat{\\\\epsilon} \\\\phi(z_t, v, t) - \\\\epsilon) \\\\frac{\\\\partial x}{\\\\partial \\\\theta},$$\\n\\nwhere $x$ denotes the 2D rendering, $t$ is the time up to which the diffusion is run to perturb $x$, $w(t)$ is a weighting function, and $z_t$ is the perturbed rendering. Further, $\\\\hat{\\\\epsilon} \\\\phi(z_t, v, t)$ is the DM's denoiser neural network that predicts the diffusion noise $\\\\epsilon$. It is conditioned on $z_t$, the diffusion time $t$ and a text prompt $v$ for guidance. Classifier-free guidance (CFG) [26] typically amplifies the text conditioning.\"}"}
{"id": "CVPR-2024-136", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AYG's JSD-based regularization of the evolving 4D Gaussians (see Sec. 3.4) calculates the 3D mean $\\\\nu^\\\\tau$ and diagonal covariance matrix $\\\\Gamma^\\\\tau$ of the set of dynamic 3D Gaussians at different times $\\\\tau$ of the 4D sequence and regularizes them to not vary too much.\\n\\nAYG outperforms MAV3D qualitatively and quantitatively and synthesizes significantly higher-quality 4D scenes. Our novel compositional generation-based approach contributes to this, which MAV3D does not pursue. Finally, instead of regular SDS, used by MAV3D, in practice AYG employs classifier score distillation [102] (see Sec. 3.4).\\n\\n3. Align Your Gaussians\\n\\nIn Sec. 3.1, we present AYG's 4D representation, and in Sec. 3.2, we introduce its compositional generation framework with multiple DMs. In Sec. 3.3, we lay out AYG's score distillation framework in practice, and in Sec. 3.4, we discuss several novel methods and extensions to scale AYG.\\n\\n3.1. AYG's 4D Representation\\n\\nAYG's 4D representation combines 3D Gaussian Splatting [36] with deformation fields [59, 63] to capture the 3D scene and its temporal dynamics in a disentangled manner. Specifically, each 4D scene consists of a set of $N_{3D}$ Gaussians as in Sec. 2. Following Kerbl et al. [36], we also use two degrees of spherical harmonics to model view-dependent effects, that is, directional color, and thereby improve the 3D Gaussians' expressivity. Moreover, we restrict the 3D Gaussians' covariance matrices to be isotropic with scales $\\\\sigma^i$. We made this choice as our 3D Gaussians move as a function of time and learning expressive dynamics is easier for spherical Gaussians. We denote the collection of learnable parameters of our 3D Gaussians as $\\\\theta$.\\n\\nThe scene dynamics are modeled by a deformation field $\\\\Delta \\\\Phi(x, y, z, \\\\tau) = (\\\\Delta x, \\\\Delta y, \\\\Delta z)$, defined through a multi-layer perceptron (MLP) with parameters $\\\\Phi$. Specifically, for any 3D location $(x, y, z)$ and time $\\\\tau$, the deformation field predicts a displacement $(\\\\Delta x, \\\\Delta y, \\\\Delta z)$. The 3D Gaussians smoothly follow these displacements to represent a moving and deforming 4D scene (Fig. 2). Note that in practice we preserve the initial 3D Gaussians for the first frame, i.e., $\\\\Delta \\\\Phi(x, y, z, 0) = (0, 0, 0)$, by setting $\\\\Delta \\\\Phi(x, y, z, \\\\tau) = (\\\\xi(\\\\tau) \\\\Delta x, \\\\xi(\\\\tau) \\\\Delta y, \\\\xi(\\\\tau) \\\\Delta z)$ where $\\\\xi(\\\\tau) = \\\\tau^0$ such that $\\\\xi(0) = 0$ and $\\\\xi(1) = 1$. Following Luiten et al. [50], we regularize the deformation field so that nearby Gaussians deform similarly (\\\"rigidity regularization\\\", see Supp. Mat.).\\n\\nApart from the intuitive decomposition into a backbone 3D representation and a deformation field to model dynamics, a crucial advantage of AYG's dynamic 3D Gaussian-based representation is that different dynamic scenes, each with its own set of Gaussians and deformation field, can be easily combined, thereby enabling promising 3D dynamic content creation applications (see Fig. 1). This is due to the explicit nature of this representation, in contrast to typical NeRF-based representations. Moreover, learning 4D scenes with score distillation requires many scene renderings. This also makes 3D Gaussians ideal due to their rendering efficiency [36]. Note that early on we also explored MAV3D's HexPlane- and NeRF-based 4D representation [79], but we were not able to achieve satisfactory results.\\n\\n3.2. Text-to-4D as Compositional Generation\\n\\nWe would like AYG's synthesized dynamic 4D scenes to be of high visual quality, be 3D-consistent and geometrically correct, and also feature expressive and realistic temporal dynamics. This suggests to compose different text-driven DMs during the distillation-based generation to capture these different aspects.\\n\\n(i) We use the text-to-image model Stable Diffusion (SD) [70], which has been trained on a broad set of imagery and provides a strong general image prior.\\n\\n(ii) We also utilize the 3D-aware text-conditioned multi-view DM MVDream [76], which generates multi-view images of 3D objects, was fine-tuned from SD on the object-centric 3D dataset Objaverse [15, 16] and provides a strong 3D prior. It defines a distribution over four multiview-consistent images corresponding to object renderings from four different camera perspectives $c_1, \\\\ldots, c_4$.\\n\\nMoreover, we train a text-to-video DM, following VideoLDM [7], but with a larger text-video dataset (HDVG-130M [90] and Webvid-10M [4]) and additional conditioning on the videos' frame rate (see Supp. Material for details). This video DM provides temporal feedback when rendering 2D frame sequences from our dynamic 4D scenes. All used DMs are latent DMs [70, 86], which means that in practice we first encode renderings of our 4D scenes into the models' latent spaces, calculate score distillation gradients there, and backpropagate them through the models' encoders. All DMs leverage the SD 2.1 backbone and share the same encoder. To keep the notation simple, we do...\"}"}
{"id": "CVPR-2024-136", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\\n\\nTianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics. arXiv preprint arXiv:2311.12198, 2023.\\n\\nDejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360\u00b0 Views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nYinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, and Kai Zhang. DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model. arXiv preprint arXiv:2311.09217, 2023.\\n\\nZeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths. arXiv preprint arXiv:2305.18295, 2023.\\n\\nGengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, and Hanbyul Joo. BANMo: Building Animatable 3D Neural Models from Many Casual Videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nZiyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction. arXiv preprint arXiv:2309.13101, 2023.\\n\\nTaoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors. arXiv preprint arxiv:2310.08529, 2023.\\n\\nXin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, and Xiaojuan Qi. Text-to-3D with Classifier Score Distillation. arXiv preprint arXiv:2310.19415, 2023.\\n\\nYe Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. PhysDiff: Physics-Guided Human Motion Diffusion Model. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\\n\\nXiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. LION: Latent Point Diffusion Models for 3D Shape Generation. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\nYuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Animate124: Animating One Image to 4D Dynamic Scene. arXiv preprint arXiv:2311.14603, 2023.\\n\\nYufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Karsten Kreis, Otmar Hilliges, and Shalini De Mello. A Unified Approach for Text- and Image-guided 4D Scene Generation. arXiv preprint arxiv:2311.16854, 2023.\\n\\nDaquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. MagicVideo: Efficient Video Generation With Latent Diffusion Models. arXiv preprint arXiv:2211.11018, 2023.\\n\\nLinqi Zhou, Yilun Du, and Jiajun Wu. 3D Shape Generation and Completion Through Point-Voxel Diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.\\n\\nJunzhe Zhu and Peiye Zhuang. HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance. arXiv preprint arXiv:2305.18766, 2023.\\n\\nWojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael Zollh\u00f6fer, Justus Thies, and Javier Romero. Drivable 3D Gaussian Avatars. arXiv preprint arxiv:2311.08581, 2023.\\n\\nM. Zwicker, H. Pfister, J. van Baar, and M. Gross. EWA volume splatting. In Proceedings Visualization, 2001. VIS \u201901, 2001.\\n\\n8588\"}"}
{"id": "CVPR-2024-136", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation. arXiv preprint arXiv:2304.08477, 2023.\\n\\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer Normalization. arXiv preprint arXiv:1607.06450, 2016.\\n\\n[3] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B. Lindell. 4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling. arXiv preprint arXiv:2311.17984, 2023.\\n\\n[4] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.\\n\\n[5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karhunen, and Ming-Yu Liu. eDiff-I: Text-to-Image Diffusion Models with Ensemble of Expert Denoisers. arXiv preprint arXiv:2211.01324, 2022.\\n\\n[6] Miguel \u00c1ngel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander T Toshev, Zhuoyuan Chen, Laurent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin Dehghan, and Joshua M. Susskind. GAUDI: A Neural Architect for Immersive 3D Scene Generation. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\n[7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[8] Hongrui Cai, Wanquan Feng, Xuetao Feng, Yan Wang, and Juyong Zhang. Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), 2022.\\n\\n[9] Ang Cao and Justin Johnson. HexPlane: A Fast Representation for Dynamic Scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\\n\\n[11] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, and Guosheng Lin. IT3D: Improved Text-to-3D Generation with Explicit View Synthesis. arXiv preprint arXiv:2308.11473, 2023.\\n\\n[12] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3D using Gaussian Splatting. arXiv preprint arXiv:2309.16585, 2023.\\n\\n[13] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes. arXiv preprint arXiv:2311.13384, 2023.\\n\\n[14] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, and Devi Parikh. Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack. arXiv preprint arXiv:2309.15807, 2023.\\n\\n[15] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl von Drach, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-XL: A Universe of 10M+ 3D Objects. arXiv preprint arXiv:2307.05663, 2023.\\n\\n[16] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A Universe of Annotated 3D Objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[17] C. Deng, C. Jiang, C. R. Qi, X. Yan, Y. Zhou, L. Guibas, and D. Anguelov. NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[18] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion Models Beat GANs on Image Synthesis. In Advances in Neural Information Processing Systems, 2021.\\n\\n[19] Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Grathwohl. Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. In Proceedings of the 40th International Conference on Machine Learning, 2023.\\n\\n[20] Lincong Feng, Muyu Wang, Maoyu Wang, Kuo Xu, and Xiaoli Liu. MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry and Texture. arXiv preprint arXiv:2311.10123, 2023.\\n\\n[21] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, Yu Sun, Li Chen, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model With Knowledge-Enhanced Mixture-of-Denoising-Experts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\"}"}
{"id": "CVPR-2024-136", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\\n\\nRohit Girdhar, Mannat Singh, Andrew Brown, Quentin Dupont, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning. arXiv preprint arXiv:2311.10709, 2023.\\n\\nJiatao Gu, Shuangfei Zhai, Yizhe Zhang, Josh Susskind, and Navdeep Jaitly. Matryoshka Diffusion Models. arXiv preprint arXiv:2310.15111, 2023.\\n\\nYuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. arXiv preprint arXiv:2307.04725, 2023.\\n\\nJonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.\\n\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen Video: High Definition Video Generation with Diffusion Models. arXiv preprint arXiv:2210.02303, 2022.\\n\\nYicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large Reconstruction Model for Single Image to 3D. arXiv preprint arXiv:2311.04400, 2023.\\n\\nEmiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple Diffusion: End-to-End Diffusion for High Resolution Images. In Proceedings of the 40th International Conference on Machine Learning (ICML), 2023.\\n\\nYukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, and Lei Zhang. DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation. arXiv preprint arXiv:2306.12422, 2023.\\n\\nA. Jain, B. Mildenhall, J. T. Barron, P. Abbeel, and B. Poole. Zero-Shot Text-Guided Object Generation with Dream Fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nYanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao. Consistent4D: Consistent 360\u00b0 Dynamic Object Generation from Monocular Video. arXiv preprint arxiv:2311.02848, 2023.\\n\\nNikolai Kalischek, Torben Peters, Jan D. Wegner, and Konrad Schindler. Tetrahedral Diffusion Models for 3D Shape Generation. arXiv preprint arXiv:2211.13220, 2022.\\n\\nOren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani Lischinski. Noise-Free Score Distillation. arXiv preprint arXiv:2310.17590, 2023.\\n\\nBernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions on Graphics, 42(4), 2023.\\n\\nIsaac Kerlow. The Art of 3D Computer Animation and Effects. Wiley Publishing, 4th edition, 2009.\\n\\nLevon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators. arXiv preprint arXiv:2303.13439, 2023.\\n\\nSeung Wook Kim, Bradley Brown, Kangxue Yin, Karsten Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio Torralba, and Sanja Fidler. NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nYixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching. arXiv preprint arXiv:2311.11284, 2023.\\n\\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-Resolution Text-to-3D Content Creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nMinghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion. arXiv preprint arXiv:2311.07885, 2023.\\n\\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B. Tenenbaum. Compositional Visual Generation with Composable Diffusion Models. In Computer Vision \u2013 ECCV 2022, 2022.\\n\\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl von Ondruck. Zero-1-to-3: Zero-shot One Image to 3D Object. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\\n\\nYuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. SyncDreamer: Generating Multiview-consistent Images from a Single-view Image. arXiv preprint arXiv:2309.03453, 2023.\\n\\nZhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzezahrai, Liam Paull, and Weiyang Liu. MeshDiffusion: Score-based Generative 3D Mesh Modeling. In International Conference on Learning Representations (ICLR), 2023.\\n\\nXiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, and Wenping Wang. Wonder3D: Single Image to 3D using Cross-Domain Diffusion. arXiv preprint arXiv:2310.15008, 2023.\"}"}
{"id": "CVPR-2024-136", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2024-136", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
