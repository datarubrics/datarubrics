{"id": "CVPR-2024-1059", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data\\n\\nMengqi Zhang\\\\textsuperscript{1} \u2217 Yang Fu\\\\textsuperscript{1} \u2217 Zheng Ding\\\\textsuperscript{1} Sifei Liu\\\\textsuperscript{2} Zhuowen Tu\\\\textsuperscript{1} Xiaolong Wang\\\\textsuperscript{1}\\n\\n\\\\textsuperscript{1}UC San Diego\\\\textsuperscript{2} NVIDIA\\n\\nStable Diffusion\\n\\nFigure 1. (i) Left: Hand-object synthesis with Stable Diffusion model; (ii) Right: HOIDiffusion generates high-quality hand-object interaction images conditioned on physical structures and detailed text description. The model disentangles the geometry from appearance, exhibiting high generation diversity. Each row: We can fix the structure and control the style based on text inputs; Each column: We can fix the style and control the structure based on 3D structural inputs.\\n\\nAbstract\\n\\n3D hand-object interaction data is scarce due to the hardware constraints in scaling up the data collection process. In this paper, we propose HOIDiffusion for generating realistic and diverse 3D hand-object interaction data. Our model is a conditional diffusion model that takes both the 3D hand-object geometric structure and text description as inputs for image synthesis. This offers a more controllable and realistic synthesis as we can specify the structure and style inputs in a disentangled manner. HOIDiffusion is trained by leveraging a diffusion model pre-trained on large-scale natural images and a few 3D human demonstrations. Beyond controllable image synthesis, we adopt the generated 3D data for learning 6D object pose estimation and show its effectiveness in improving perception systems.\\n\\nProject page: https://mq-zhang1.github.io/HOIDiffusion.\\n\\n\\\\*Equal Contribution.\"}"}
{"id": "CVPR-2024-1059", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"State-of-the-art diffusion models such as Dall-E2 [44] and Stable Diffusion [46] can generate realistic images given text instructions, but they still fail quite often when it comes to capturing the details of how fingers are placed around the object. As shown on the left side of Figure 1, Stable Diffusion might not be able to output physically or geometrically plausible interactions and sometimes there are more than five fingers in a hand. Moreover, it is still unclear how to configure image outputs beyond text instructions and make the output correspond to 3D shapes and poses.\\n\\nIn this paper, we propose to generate 3D hand-object interaction data, i.e., realistic images come with 3D ground-truths at the same time. Beyond realistic generation, we also enable controllable synthesis where the users can specify the geometry configuration and appearance in a disentangled manner. We achieve this by introducing a two-stage framework: We first synthesize the 3D geometric structure (shape and pose) of the hand and the object, and then we train a diffusion model conditioned on both the 3D structure and the text (indicating the style) to synthesize the corresponding RGB image. We visualize some synthesis results on the right side of Figure 1. In each row we generate images with the same 3D structural configuration but with different object and background styles; in each column, we fix the styles and synthesize images with different geometric structures.\\n\\nIn the first stage of our framework, we generate a human grasp based on a given 3D object model. We apply the pre-trained GrabNet [55] for this task, which takes the object mesh as inputs for a Variational AutoEncoder and predicts different grasp poses as outputs. In the second stage, we train a diffusion model conditioning the hand-object geometric configurations. We fine-tune the pre-trained Stable Diffusion model [46] with a few human demonstrations from the DexYCB dataset [6]. We convert the hand-object geometry to estimated surface normals, segmentation, and hand keypoint 2D projection as conditional inputs for the new diffusion model, specifying the structure of the image to generate. The diffusion model will also take text inputs for specifying appearance. During training, we apply a background regularization strategy to reduce the bias brought by DexYCB which comes with the same clean background. The fine-tuned diffusion model leverages both the rich appearance information from the pre-trained model and the geometry information from the new conditional variables.\\n\\nIn our experiments, our method outperforms previous approaches on hand-object image synthesis with more physically plausible interactions. The disentangled design provides flexible control of geometry and appearance. Interestingly, the model shows a strong generalization ability to different text prompts when changing the foreground and background appearance. We use several metrics to evaluate generation fidelity to real datasets and visual alignment with provided prompts. The results show an improvement compared to baselines. Additionally, with a generated dataset with both images and corresponding 3D geometry using our pipeline, we can use it to train an object pose estimator as a downstream application. Our experiments indeed show such realistic synthesized data is very helpful in improving the perception metrics.\\n\\n2. Related work\\nHand-Object Interaction Dataset. The understanding of hand-object interactions has been a long-standing problem [1, 18, 39, 40, 54]. More recently, the data-driven approaches [5, 12, 20, 21, 31, 38] have shown significant advancement in hand-object shape reconstruction and pose estimation. At the heart of this progress, is the collection of hand-object interaction data. To obtain 3D annotations, existing datasets [13, 19, 20, 55, 61] collect videos with attached sensors or mocap markers to track hand pose or utilize optimized algorithms to facilitate annotations. 2D annotations are also provided with manual labeling [6]. However, these approaches are time-consuming and not scalable. Recently, more data collection pipelines [16, 32, 42, 60] have taken advantage of detection and segmentation techniques to automatically acquire annotations. However, even though this method eases the difficulty, estimations may not be precise enough, and manual annotations might still be required for accurate 3D annotations. The diversity of the data is also relatively small given the repetitive patterns in videos. In this paper, we propose a new effective data generation method facilitated by generative models for hand-object interaction images with full 3D annotations.\\n\\nHand Grasp Generation. Hand grasp generation given an object model [2, 8, 9, 16, 26, 27, 63, 65] is of vital importance in our method to provide an ending pose for grasping trajectory. Most approaches estimate or further refine the grasping hand pose by predicting the contact map between hands and objects [3, 16, 26]. Other methods [13, 27, 55] predict the MANO parameter hand representation introduced in [47] with variational autoencoder or implicit function architectures. Additionally, the hand parameter prediction can be integrated as a component of the whole human body [56]. By taking advantage of these grasp predictors, we are able to obtain satisfying ending poses for hand trajectory generation.\\n\\nDiffusion Models. Diffusion models [25, 46, 53] which learn to denoise images from Gaussian distributions, emerge recently and perform photo-realistic image synthesis with more stable training process compared to other generative models [15, 28]. Many successive advancements occur in this field [11, 36, 37, 44, 50]. More relevantly, special tokens are introduced [14, 49] to fine-tune the model, enabling personalized text-to-image generation. With these\"}"}
{"id": "CVPR-2024-1059", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Pipeline. We propose a two-stage pipeline to synthesize hand-object-interaction data. During the first stage, we utilize a pretrained GrabNet to output 3D hand poses given by a single object model. Then in the second stage, we use those 3D hand poses along with segmentation maps, normal maps and skeletons to conditionally generate high-quality HOI data. Researchers [4, 59] start investigating the possibility of utilizing diffusion models as a new source of data for classification or segmentation tasks. Besides using text inputs, conditional generation with a given layout or scratch achieves impressive performance after the appearance of CoAdapter [33] and ControlNet [64]. These works unveiled the large potential of diffusion models to control or edit images according to users' demands. However, these models still suffer from generating realistic hand-object-interaction images, such as not being object-agnostic, inaccurate geometry, and generating hands with missing fingers or unnatural poses. In our work, we focus on how to utilize physical conditions, including normal, hand skeleton projection and segmentation to construct a 3D-aware model for scalable hand-object-image generation with flexible geometry and appearance control.\\n\\n3. Method\\n\\nIn order to generate scalable hand-object-interaction images, three expectations need to be satisfied: (i) The model should generate realistic images that are consistent with the geometric description of the specified object. (ii) It should retain the prompt-editing capabilities inherent in the stable diffusion model while incorporating controllable conditions. (iii) The model should have a better generalization ability to synthesize images of unseen instances or categories.\\n\\nTo meet the above requirements, we propose a two-stage approach. For the first stage, our goal is to establish the conditions, primarily the hand-grasping trajectory, for the subsequent stage. Specifically, we utilized a generalizable VAE model trained on large-scale 3D physical data to obtain the ending pose and interpolated the trajectory using spherical linear interpolation. For the second stage, we extracted multiple geometry structures either from the generated grasping trajectory or from images with natural hand-grasping actions through rendering and off-the-shelf estimators to fine-tune a controllable Stable Diffusion, which enables precise pose control at inference. Furthermore, a background regularization strategy is introduced in our pipeline to mitigate edit ability degradation brought by finetuning. The entire pipeline is shown in Figure 2.\\n\\n3.1. Preliminary\\n\\nDenoising diffusion model [25] is a kind of new generative model with competitive performance and more stable training. It consists of two main processes: diffusion and denoising. In the forward process, randomly sampled noises are added to original images, which can be mathematically simplified as\\n\\n\\\\[ q(x_t | x_0) = N(x_t; \\\\sqrt{\\\\alpha_t} x_0, (1 - \\\\alpha_t) I), \\\\]\\n\\nwhere \\\\( \\\\alpha_t \\\\) is hyperparameters control noise scheduling, and \\\\( \\\\alpha_t = \\\\frac{Q_t}{s} \\\\). After \\\\( T \\\\) steps, the distribution shifted from image space to approximate standard Gaussian distribution.\\n\\nAnd U-Net model is trained to predict the added noise. During inference, an image is initialized from the Gaussian distribution and the model removes the noise within \\\\( T \\\\) steps, with each step formulated as\\n\\n\\\\[ \\\\hat{\\\\epsilon}_t = f_\\\\theta(x_t, t), \\\\]\\n\\nwhere \\\\( f_\\\\theta \\\\) is the U-Net model. The estimated image at the next time step can be derived and written as\\n\\n\\\\[ x_{t-1} = \\\\sqrt{\\\\alpha_t} (x_t - \\\\hat{\\\\epsilon}_t) + \\\\sigma_t z, \\\\]\\n\\nThe simpler version of training loss is:\\n\\n\\\\[ |\\\\| \\\\epsilon - f_\\\\theta(\\\\sqrt{\\\\alpha_t} x_0 + \\\\sqrt{1 - \\\\alpha_t} \\\\epsilon, t) \\\\| |^2. \\\\]\\n\\nTherefore diffusion models are able to generate photorealistic images.\"}"}
{"id": "CVPR-2024-1059", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### 3.2. Hand Grasping Trajectory Generation\\n\\nIn the first stage of our framework, given a randomly transformed object model, we require a hand-grasping trajectory to effectively reach the object. To this end, we adopted an interpolation method using GrabNet, a model for generating hand grasps conditioned on BPS (Body Part Segmentation) [41] derived from object models. Through extensive training on a large dataset, it has learned to accurately map contact between hands and objects, showcasing strong generalization capabilities for unseen scenarios. Utilizing GrabNet, we can determine the final grasp hand MANO parameters, which include joint pose and hand shape. Given the ending pose and the object's position, we can approximate an initial starting point\u2014positioned vertically above the contact surface and within a certain distance\u2014using zeroed MANO parameters. Subsequently, we employ spherical linear interpolation between this starting point and the ending pose to create a smooth hand-grasping trajectory. As shown in Figure 2 Stage 1, the grasping parameters can be obtained along the trajectory and we could acquire different ground-truth annotations such as segmentation, mask, and depth maps through rendering.\\n\\n### 3.3. Hand-Object-Interaction image Synthesis\\n\\nOur Hand-Object-Interaction image synthesis module mainly consists of two components: structure control and appearance regularization, to disentangle the geometry and appearance separately.\\n\\n**Structure Control**\\n\\nGiven the hand-grasp trajectory, we extract multiple geometric conditions and leverage the advanced Stable Diffusion models to generate realistic images that are consistent with the given conditions. To precisely control the hand-object image generation, three structure conditions are distilled to control the generation. i) To synthesize realistic hands without missing fingers which is the problem encountered in original Stable Diffusion, hand position information is essentially required. Instead of directly using MANO parameter vectors as guidance, we projected the skeleton information onto the image space as visual control, which can be denoted as \\\\( s_i \\\\). ii) Additionally, to mitigate the inter-disturbance of hand object areas and degradation in performance brought by occlusion, hand-object segmentation \\\\( m_i \\\\) is used to provide clear boundaries to separate areas, and coarse object shape priors. iii) Finally, we also apply an estimated normal map \\\\( n_i \\\\) to seize the surface geometry with lighting. The forward process defined in Equation 1 can now be defined as\\n\\n\\\\[\\n\\\\hat{\\\\epsilon}_i(t) = f_{\\\\theta}(x_{i0}, t, [s_i, m_i, n_i]).\\n\\\\]\\n\\nWith the above controls, the structure information could be disentangled from the appearance, and thus during the inference, we could seamlessly synthesize an accurate image with new poses.\\n\\n**Appearance Regularization**\\n\\nWith the above component, we are capable of synthesizing images aligned with diverse condition geometries during inference. However, a notorious drawback of fine-tuning is its tendency to converge or overfit quickly to the training dataset's style, thereby significantly reducing image diversity. To mitigate this problem and fully harness the capabilities of text-to-image diffusion models for flexible style transformation via prompts, we introduce an appearance regularization method combined with classifier-free guidance [24] as shown in Figure 3. Specifically, in addition to using the original Hand-Object Interaction (HOI) training dataset, we synthesize batches of high-quality scenery images with the pre-trained text-to-image diffusion model. The prompts for these images are generated by the large language model ChatGPT, forming what we refer to as a \u201cbackground buffer\u201d. During training, we intermittently utilize these background images for regularization, ensuring it does not detrimentally impact performance. Meanwhile, the paired blank conditions are applied as classifier-free guidance, corresponding to the background region in HOI data. The objective becomes:\\n\\n\\\\[\\nL = E_{x_{0}, x_r, \\\\epsilon, \\\\epsilon_r} \\\\left[ ||\\\\epsilon - f_{\\\\theta}(\\\\sqrt{\\\\alpha_t} x_{0} + \\\\sqrt{1-\\\\alpha_t} \\\\epsilon, t) ||^2 + w_r ||\\\\epsilon_r - f_{\\\\theta}(\\\\sqrt{\\\\alpha_t} x_r + \\\\sqrt{1-\\\\alpha_t} \\\\epsilon_r, t) ||^2 \\\\right].\\n\\\\]\\n\\nwhere \\\\( w_r \\\\) is the regularization weight, which is set to 1 in our experiments. \\\\( x_r, \\\\epsilon_r \\\\) are input from the background buffer and corresponding added noise. In addition to the buffer, we also use the large multimodal model LLaVA [30] to caption our training images with detailed descriptions of foreground appearance and background, forcing the model aware of diverse texts with the assistance of the CLIP [43] text encoder.\\n\\n### 4. Experiment\\n\\n#### 4.1. Implementation Details\\n\\nTo achieve scalable synthesis and improve generalization ability, the training dataset is required to encompass diverse backgrounds and comprehensive annotations. For these purposes, the DexYCB dataset is selected for its high-quality images from varied viewpoints of hand gestures, with human demonstrations and diverse background settings. To prevent overfitting and preserve the text-driven editing ability of diffusion models, we adopt the learning rate of \\\\( 10^{-5} \\\\) during training. About 50,000 steps prove sufficient to achieve satisfactory generation quality. When utilizing images from background buffers, we provide valuable skeleton projection and mask conditions, corresponding to the background regions in HOI training data images. This design also can be viewed as a classifier-free guidance that drives the model aware of the condition control effects. The entire training process costs approximately 12 hours on eight A100 GPUs.\"}"}
{"id": "CVPR-2024-1059", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We inject three conditional encoders into the stable diffusion model. We utilize both the HOI datasets and high-quality background images to train HOIDiffusion. The background images are synthesized using the scenery prompts. The texts sent to the model are output by LLaVA for detailed description.\\n\\n4.2. Evaluation and Baseline Comparison\\n\\nBaselines\\n\\nWe compare the results with four baseline models customized for our setting: (1) a fine-tuned LDM [46] without condition modules, working as an unconditional generation task; (2) a modified DreamBooth [49] fine-tuned model, with proposed specific token agnostically applied to all object categories, complemented by regularization data from our trained conditional stable diffusion; (3) Affordance Diffusion [62]; (4) ControlNet with same multiple condition input. It is important to mention that the Affordance Diffusion model is only used for comparison on contact recall evaluation, with quantitative results reported from the original paper.\\n\\nImage Synthesis Quality\\n\\nTo assess the generation quality, we adopt commonly used metrics FID [23] to evaluate the fidelity of our generated images to real datasets. Additionally, we also present the comparison results with baselines on the Inception Score [51] and sFID [35]. Our reference batch consists of 50k randomly selected images from the training dataset in total. For the sample batch, we synthesize hand-grasping images corresponding to all randomly rotated object models, some of which are unseen during training. The total sample size is 5k. We refer readers to Table 1 for a comprehensive overview of the comparison.\\n\\nAppearance Alignment\\n\\nFurthermore, to demonstrate the flexible control over object and background appearance, we use another metrics CLIPScore [22] originally designed for image captioning to automatically evaluate the alignment level between generated images and corresponding prompts. We randomly sample 50 instances from all object models with fixed structure conditions. Multiple appearance descriptions generated by ChatGPT are applied to each instance. The results are shown in Table 1. Notably, our method demonstrates superior overall quality of generated images, with higher fidelity to real data and improved alignment with appearance-controlling texts.\\n\\nHand Pose Evaluation\\n\\nFor hand-object-interaction images, hand grasping status, and pose precision are of vital importance for real-world applications. A fundamental criterion for synthesized images is the geometric consistency between our generated hands and the provided 2D skeleton projection, disregarding the depth dimension. On top of that, these grasping images sometimes serve as visual demonstrations for various downstream tasks, requiring the exactly accurate contact status between the hand and object in the ending pose image along the grasping trajectory. Consequently, our model is evaluated with the baselines on two key perspectives: Hand contact recall and hand re-inference accuracy. Specifically, we adopt a contact evaluation setup utilized in Affordance Diffusion [62]. An off-the-shelf hand-object detector [52] is used to classify the image\u2019s in-contact status. Furthermore, to evaluate the re-inference accuracy, we estimate the MANO parameters of hands in images through a widely used single-view hand pose estimator [48], from which we derive the predicted hand joint positions. The percentage of correct keypoints (PCK) is used to measure the accuracy of predicted keypoints representing the hand poses in our data. We evaluate...\"}"}
{"id": "CVPR-2024-1059", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative results on different structures.\\n\\nGenerated images with the same background description but different physical conditions (object shape, poses, and hand skeletons). With plain prompts, HOIDiffusion could generate more realistic images similar to the style in training datasets.\\n\\n| Method          | Hand Contact Recall % | PCK     |\\n|-----------------|------------------------|---------|\\n| LDM [46]        | 79.12                  | 0.15    |\\n| DreamBooth [49] | 79.12                  | 0.10    |\\n| ControlNet [64] | 86.00                  | 0.67    |\\n| Affordance Diffusion [62] | 73.00                 | -       |\\n| HOIDiffusion (Ours) | 92.31                 | 0.85    |\\n\\nTable 2. Evaluation metrics from hand perspective.\\n\\nHand Contact Recall is to evaluate whether the hand-ending pose is in close contact with the object. PCK is used to measure the accuracy of generated images' keypoints at the pose precision on a subset of daily objects. As delineated in Table 2, benefited from the geometry guidance, our method manifests the capability to synthesize images depicting accurate grasps and firm contacts, outperforming previous methods in achieving higher mean contact recall and PCK.\\n\\n4.3. Geometry and Appearance Disentangle\\n\\nThe most important design in our model is to disentangle the physical geometries from textures. Through this design, we have observed the remarkable ability of our model to control the generation process with novel object shapes and previously unseen text descriptions. During training, our model learns extensive shape priors from masks and normal map conditions, hence acquiring the capability to transfer to unseen object instances seamlessly. Furthermore, our model preserves the robust text editing ability, enabling flexible style transformation over both background and object appearance. In this subsection, we primarily focus on and showcase the qualitative results of structure and style manipulation.\\n\\nGeometry Manipulation\\n\\nIn Figure 4, we present the generated paired images of four daily seen objects: mug, can, bottle, and bowl, given varying instance shapes, poses, and hand skeletons. The left column of each pair is rendered image in 3D space, from which physical conditions are extracted. Normal maps are obtained from an estimated depth provided by the depth estimator MiDaS [45]. The skeleton and segmentation are also concurrently obtained during rendering. Corresponding generated images are displayed in the right column. All provided prompts follow the format: \u201cA hand is grasping a [object].\u201d The results exhibit an overall generation style in a laboratory or stereo environment, consistent with the realistic appearance style in the training dataset. From Figure 1, it is also evident that HOI images generated from our model are more closely aligned with the required geometry than baseline methods.\\n\\nBackground and object appearance control\\n\\nIn this section, we explore the text editing ability with fixed geometry. Through background regularization and classifier-free guidance, our model exhibits the ability to depict diverse background contents, retaining control over appearance using text prompts. We investigate the qualitative performance of various text controls under identical physical conditions, shown in Figure 5. Each column represents one distinct background description. Notably, the generated images exhibit high fidelity to the provided prompts, maintaining the layout and structure unchanged. This demonstrates the ability of HOIDiffusion to effectively disentangle the appearance from geometry structures, thus enabling flexible style transformation without geometry distortion. This is essential in data construction, ensuring the precise alignment with input geometry and diverse range of visual appearances.\\n\\n4.4. Applications\\n\\nVideo Generation\\n\\nThe real hand-object interaction datasets often exhibit data in video format, a complete fetching process to the object. Collecting these video clips is a considerable challenge. Some datasets [6, 60] comprising almost 8526\"}"}
{"id": "CVPR-2024-1059", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"on the beach on the grass in the snow on the moon with night sky in the mountain\\n\\nFigure 5. Synthesized images with diverse background descriptions.\\n\\nIn addition to real-style synthesis, our model also allows users to generate according to their preferences such as science fiction or general landscapes.\\n\\nmillions of images only contain no more than 10k videos, thus video data is much more valuable. In experiments, we observe the significant divergence in generated images between adjacent frames despite the similar provided conditions and texts. This divergence results in dramatic flickering in directly concatenated videos. To this end, we leverage zero-shot video generation techniques in the diffusion model to establish inter-consistency among frames. To be more specific, the original self-attention layers in the U-Net are refactored to cross-attention modules between an anchor frame and current frames. This adjustment establishes the awareness of the previous appearance style, ensuring video consistency. In our experiments, we set the middle frame as an anchor, and with all frames attend to both the anchor image and themselves. This approach effectively mitigates the flickering issue, synthesizing relatively smooth hand-grasping trajectories. The video clip samples are shown in Figure 6.\\n\\nDownstream tasks\\nAnother interesting method to evaluate the performance of HOIDiffusion, is to apply it in downstream tasks as a data augmentation method or new data source. In this section, we explore the potential for improving categorical object 6D pose estimation tasks. Most models in this task are trained on dataset NOCS [58], consisting of both synthetic and real data with annotations. Some models [7, 57] implicitly predict normalized object coordinate space, and then utilize Umeyama algorithms to parse the transformation matrix. Others [29] explicitly predict the rotation, translation, and scale parameters. Despite differing in module design, all these methods leverage RGB image encoders to obtain the visual features. An interesting observation is that the synthesized images directly rendered from object models appear too artificial, potentially affecting the performance of the RGB encoder. Inspired by this, we substitute the synthesized images with our generated HOI images in the same poses, anticipating the reality brought by our data could help enhance model performance. We exhibit the results in Table 3. We choose two representative object pose estimators for evaluation. The \\\"original\\\" model in the table refers to training using an unchanged NOCS dataset, and \\\"our\\\" model is trained using our mixed data.\\n\\n5. Ablation Study\\nTwo indispensable components of our design are precise structural control and appearance regularization, effectively improving model performance on geometry consistency and diversity.\\n\\nStructural Control\\nIn our approach, there are three crucial structure conditions provided to the model and we investigate the importance brought separately by these three modules. Results are presented in Table 4. Essentially, each condition serves a distinct purpose: the normal map guides the model in perceiving surface textures with lighting, which is essential for maintaining geometry consistency; hand keypoint projection precisely depicts the pose of hand joints, preventing the model from synthesizing multiple fingers or distorted hands; hand-object segmentation provides a clear boundary between different regions, avoiding interference between hand and object areas. As presented in the results, our full model outperforms other incomplete versions in quantitative evaluation.\\n\\nAppearance Regularization\\nTable 5 and Figure 7 demonstrate the significance of appearance regularization in our module. Without regularization, the finetuned model\"}"}
{"id": "CVPR-2024-1059", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Zero-shot video generation of hand grasping trajectory. Images along the same line represent the sequential motion of reaching an object. By leveraging temporal-level cross-attention, the frame flickering problem is mitigated.\\n\\nTable 3. Quantitative evaluation on NOCS. We use SPD and DualPoseNet and change the synthesized images in the dataset with our generated images for training. Our performance improves on all metrics with DualPoseNet and all cm metrics with SPD which demonstrates the good quality of our images and can be utilized for downstream tasks.\\n\\nTable 4. Ablation study on structural control. FID evaluation on 1,000 images of different types of missing modules to demonstrate the necessity of all physical conditions. Our method outperforms all others.\\n\\nTable 5. CLIPScore evaluation. Consistency is evaluated between provided prompts and generated images for different backgrounds and instances.\\n\\nquickly converges to the style in training datasets, mostly in a laboratory/studio environment, as depicted in the first line of Figure 7. This convergence impairs the model\u2019s ability to generate diverse images, which is essential for data generation. By incorporating appearance regularization using data generated from the pretrained model, HOIDiffusion mitigates the drift to a fixed style and improves the overall text-editing ability.\\n\\n6. Conclusion\\nIn this paper, we propose HOIDiffusion with precise appearance and structure control. We did experiments on geometry and appearance manipulation, and evaluated the performance using FID, IS, and hand contact recall. The results demonstrate better performance compared to baseline models. We apply the generated data for object 6D pose estimation and show its effectiveness in possibilities to improve perception systems.\"}"}
{"id": "CVPR-2024-1059", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Luca Ballan, Aparna Taneja, J\u00fcrgen Gall, Luc Van Gool, and Marc Pollefeys. Motion capture of hands in action using discriminative salient points. In European Conference on Computer Vision, pages 640\u2013653. Springer, 2012.\\n\\n[2] Samarth Brahmbhatt, Ankur Handa, James Hays, and Dieter Fox. Contactgrasp: Functional multi-finger grasp synthesis from contact. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2386\u20132393. IEEE, 2019.\\n\\n[3] Samarth Brahmbhatt, Chengcheng Tang, Christopher D Twigg, Charles C Kemp, and James Hays. Contactpose: A dataset of grasps with object contact and hand pose. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIII, pages 361\u2013378. Springer, 2020.\\n\\n[4] Max F Burg, Florian Wenzel, Dominik Zietlow, Max Horn, Osama Makansi, Francesco Locatello, and Chris Russell. A data augmentation perspective on diffusion models and retrieval. arXiv preprint arXiv:2304.10253, 2023.\\n\\n[5] Zhe Cao, Ilija Radosavovic, Angjoo Kanazawa, and Jitendra Malik. Reconstructing hand-object interactions in the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12417\u201312426, 2021.\\n\\n[6] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj S Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al. Dexycb: A benchmark for capturing hand grasping of objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9044\u20139053, 2021.\\n\\n[7] Kai Chen and Qi Dou. Sgpa: Structure-guided prior adaptation for category-level 6d object pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2773\u20132782, 2021.\\n\\n[8] Sammy Christen, Muhammed Kocabas, Emre Aksan, Jemin Hwangbo, Jie Song, and Otmar Hilliges. D-grasp: Physically plausible dynamic grasp synthesis for hand-object interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20577\u201320586, 2022.\\n\\n[9] Enric Corona, Albert Pumarola, Guillem Alenya, Francesc Moreno-Noguer, and Gr\u00e9goire Rogez. Ganhand: Predicting human grasp affordances in multi-object scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5031\u20135041, 2020.\\n\\n[10] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In Proceedings of the European conference on computer vision (ECCV), pages 720\u2013736, 2018.\\n\\n[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.\\n\\n[12] Bardia Doosti, Shujon Naha, Majid Mirbagheri, and David J Crandall. Hope-net: A graph-based model for hand-object pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6608\u20136617, 2020.\\n\\n[13] Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael J Black, and Otmar Hilliges. Arctic: A dataset for dexterous bimanual hand-object manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12943\u201312954, 2023.\\n\\n[14] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.\\n\\n[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.\\n\\n[16] Patrick Grady, Chengcheng Tang, Christopher D Twigg, Minh V o, Samarth Brahmbhatt, and Charles C Kemp. Contactopt: Optimizing contact to improve grasps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1471\u20131481, 2021.\\n\\n[17] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.\\n\\n[18] Henning Hamer, J\u00fcrgen Gall, Thibaut Weise, and Luc Van Gool. An object-dependent hand pose prior from sparse training data. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 671\u2013678. IEEE, 2010.\\n\\n[19] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vincent Lepetit. Honnotate: A method for 3d annotation of hand and object poses. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3196\u20133206, 2020.\\n\\n[20] Yana Hasson, Gul Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J Black, Ivan Laptev, and Cordelia Schmid. Learning joint reconstruction of hands and manipulated objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11807\u201311816, 2019.\\n\\n[21] Yana Hasson, Bugra Tekin, Federica Bogo, Ivan Laptev, Marc Pollefeys, and Cordelia Schmid. Leveraging photometric consistency over time for sparsely supervised hand-object reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 571\u2013580, 2020.\\n\\n[22] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\\n\\n[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. arXiv preprint arXiv:1701.04712, 2017.\"}"}
{"id": "CVPR-2024-1059", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[24] Jonathan Ho and Tim Salimans.Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\\n\\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.\\n\\n[26] Hanwen Jiang, Shaowei Liu, Jiashun Wang, and Xiaolong Wang. Hand-object contact consistency reasoning for human grasps generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11107\u201311116, 2021.\\n\\n[27] Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael J Black, Krikamol Muandet, and Siyu Tang. Grasping field: Learning implicit representations for human grasps. In 2020 International Conference on 3D Vision (3DV), pages 333\u2013344. IEEE, 2020.\\n\\n[28] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\\n\\n[29] Jiehong Lin, Zewei Wei, Zhihao Li, Songcen Xu, Kui Jia, and Yuanqing Li. Dualposenet: Category-level 6d object pose and size estimation using dual pose network with refined learning of pose consistency. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3560\u20133569, 2021.\\n\\n[30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.\\n\\n[31] Shaowei Liu, Hanwen Jiang, Jiarui Xu, Sifei Liu, and Xiaolong Wang. Semi-supervised 3d hand-object poses estimation with interactions in time. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14687\u201314697, 2021.\\n\\n[32] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi. Hoi4d: A 4d egocentric dataset for category-level human-object interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21013\u201321022, 2022.\\n\\n[33] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 4296\u20134304, 2024.\\n\\n[34] Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, and Xiaolong Wang. A-sdf: Learning disentangled signed distance functions for articulated shape representation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13001\u201313011, 2021.\\n\\n[35] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021.\\n\\n[36] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\\n\\n[37] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.\\n\\n[38] Markus Oberweger, Paul Wohlhart, and Vincent Lepetit. Generalized feedback loop for joint hand-object pose estimation. IEEE transactions on pattern analysis and machine intelligence, 42(8):1898\u20131912, 2019.\\n\\n[39] Iason Oikonomidis, Nikolaos Kyriazis, and Antonis A Argyros. Full dof tracking of a hand interacting with an object by modeling occlusions and physical constraints. In 2011 International Conference on Computer Vision, pages 2088\u20132095. IEEE, 2011.\\n\\n[40] Paschalis Panteleris, Nikolaos Kyriazis, and Antonis A Argyros. 3d tracking of human hands in interaction with unknown objects. In BMVC, pages 123\u20131, 2015.\\n\\n[41] Sergey Prokudin, Christoph Lassner, and Javier Romero. Efficient learning on point clouds with basis point sets. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4332\u20134341, 2019.\\n\\n[42] Yuzhe Qin, Yueh-Hua Wu, Shaowei Liu, Hanwen Jiang, Ruihan Yang, Yang Fu, and Xiaolong Wang. Dexmv: Imitation learning for dexterous manipulation from human videos. In European Conference on Computer Vision, pages 570\u2013587. Springer, 2022.\\n\\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\\n\\n[44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\\n\\n[45] Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):1623\u20131637, 2020.\\n\\n[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.\\n\\n[47] Javier Romero, Dimitrios Tzionas, and Michael J Black. Embodied hands: Modeling and capturing hands and bodies together. arXiv preprint arXiv:2201.02610, 2022.\\n\\n[48] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmocap: A monocular 3d whole-body pose estimation system via regression and integration. In IEEE International Conference on Computer Vision Workshops, 2021.\\n\\n[49] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500\u201322510, 2023.\"}"}
{"id": "CVPR-2024-1059", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.\\n\\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.\\n\\nDandan Shan, Jiaqi Geng, Michelle Shu, and David F Fouhey. Understanding human hands in contact at internet scale. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9869\u20139878, 2020.\\n\\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.\\n\\nSrinath Sridhar, Franziska Mueller, Michael Zollh\u00f6fer, Dan Casas, Antti Oulasvirta, and Christian Theobalt. Real-time joint tracking of a hand manipulating an object from rgb-d input. In European Conference on Computer Vision, pages 294\u2013310. Springer, 2016.\\n\\nOmid Taheri, Nima Ghorbani, Michael J Black, and Dimitrios Tzionas. Grab: A dataset of whole-body human grasping of objects. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IV 16, pages 581\u2013600. Springer, 2020.\\n\\nOmid Taheri, Vasileios Choutas, Michael J Black, and Dimitrios Tzionas. Goal: Generating 4d whole-body motion for hand-object grasping. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13263\u201313273, 2022.\\n\\nMeng Tian, Marcelo H Ang Jr, and Gim Hee Lee. Shape prior deformation for categorical 6d object pose and size estimation. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\\n\\nHe Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2642\u20132651, 2019.\\n\\nWeijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, and Chunhua Shen. Datasetdm: Synthesizing data with perception annotations using diffusion models. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nLixin Yang, Kailin Li, Xinyu Zhan, Fei Wu, Anran Xu, Liu Liu, and Cewu Lu. Oakink: A large-scale knowledge repository for understanding hand-object interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20953\u201320962, 2022.\\n\\nRuolin Ye, Wenqiang Xu, Zhendong Xue, Tutian Tang, Yanfeng Wang, and Cewu Lu. H2o: A benchmark for visual human-human object handover analysis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15762\u201315771, 2021.\\n\\nYufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello, Stan Birchfield, Jiaming Song, Shubham Tulsiani, and Sifei Liu. Affordance diffusion: Synthesizing hand-object interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22479\u201322489, 2023.\\n\\nHe Zhang, Yuting Ye, Takaaki Shiratori, and Taku Kornura. Manipnet: Neural manipulation synthesis with a hand-object spatial representation. ACM Transactions on Graphics (ToG), 40(4):1\u201314, 2021.\\n\\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.\\n\\nTianqiang Zhu, Rina Wu, Xiangbo Lin, and Yi Sun. Toward human-like grasp: Dexterous grasping via semantic representation of object-hand. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15741\u201315751, 2021.\"}"}
