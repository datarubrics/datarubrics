{"id": "CVPR-2022-2001", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation\\n\\nXiangtai Li\\nWenwei Zhang\\nJiangmiao Pang\\nKai Chen\\nGuangliang Cheng\\nYunhai Tong\\nChen Change Loy\\n\\n1 School of Artificial Intelligence, Key Laboratory of Machine Perception (MOE), Peking University\\n2 S-Lab, Nanyang Technological University\\n3 CUHK-SenseTime Joint Lab, the Chinese University of Hong Kong\\n4 SenseTime Research\\n5 Shanghai AI Lab\\n\\n{lxtpku, yhtong}@pku.edu.cn\\n{wenwei001, ccloy}@ntu.edu.sg\\npangjiangmiao@gmail.com\\n{chenkai, chengguangliang}@sensetime.com\\n\\nAbstract\\nThis paper presents Video K-Net, a simple, strong, and unified framework for fully end-to-end video panoptic segmentation. The method is built upon K-Net, a method that unifies image segmentation via a group of learnable kernels. We observe that these learnable kernels from K-Net, which encode object appearances and contexts, can naturally associate identical instances across video frames. Motivated by this observation, Video K-Net learns to simultaneously segment and track \\\"things\\\" and \\\"stuff\\\" in a video with simple kernel-based appearance modeling and cross-temporal kernel interaction. Despite the simplicity, it achieves state-of-the-art video panoptic segmentation results on Citscapes-VPS and KITTI-STEP without bells and whistles. In particular on KITTI-STEP, the simple method can boost almost 12% relative improvements over previous methods. We also validate its generalization on video semantic segmentation, where we boost various baselines by 2% on the VSPW dataset. Moreover, we extend K-Net into clip-level video framework for video instance segmentation where we obtain 40.5% for ResNet50 backbone and 51.5% mAP for Swin-base on YouTube-2019 validation set. We hope this simple yet effective method can serve as a new flexible baseline in video segmentation.\\n\\n1 Introduction\\nVideo Panoptic Segmentation (VPS) aims at segmenting and tracking every pixel of input video clips [20,22,57]. As a fundamental technique to scene understanding, it has received increasing attention in recent years due to its wide applications in many vision systems including autonomous driving and robot navigation [12, 16]. By definition, VPS is an extension of Panoptic Segmentation (PS) [24] into the video domain with the goal of unifying Video Semantic Segmentation (VSS) [33, 45, 73] and Video Instance Segmentation (VIS) [3, 64] into a single task.\\n\\nExisting studies for video panoptic segmentation can be mainly divided into top-down methods [20,22] and bottom-up approaches [42,57]. Top-down methods try to solve VPS as a multi-task learning problem via performing semantic segmentation, instance segmentation and multiple object tracking individually. VPSNet [22] is proposed to learn to fuse and warp features. In particular, it applies an optical flow network [14] to align features and attention modules to fuse features [60]. However, these approaches [20, 22] involve many hyper-parameters and ad-hoc designs, leading to a complex system. Bottom-up methods [10, 42, 57] first perform semantic segmentation and then predict near frames' centers to localize and track each instance where both features are connected by an ASPP module [7] (Fig. 1-(b)). Despite being simpler than top-down approaches, they rely on several post-processing components (i.e., NMS for center grouping, offline mask tracking).\\n\\nRecently, inspired by the design of object queries in Detection Transformer (DETR) [5], new efforts [11, 51, 68] emerge to explore unified solutions to image panoptic segmentation. In particular, MaskFormer [11] and K-Net [68] unify 'things' and 'stuff' segmentations by dynamic kernels within the mask classification paradigm. The aforementioned studies in image segmentation motivate us to simplify cumbersome pipelines in video panoptic segmentation. It is noteworthy that several studies [32, 46, 67] in Multi Object Tracking (MOT) also adopt the query design to achieve end-to-end learning. However, most of them introduce extra tracking queries or boxes to handle each tracked instance.\\n\\nIn this paper, we present Video K-Net, a fully end-to-end framework for video panoptic segmentation. It is observed that the learnable kernels from K-Net [68], which encode object appearances and contexts, can naturally associate identical instances across video frames. Motivated by this observation, Video K-Net learns to simultaneously segment and track \\\"things\\\" and \\\"stuff\\\" in a video with simple kernel-based appearance modeling and cross-temporal kernel interaction. Despite the simplicity, it achieves state-of-the-art video panoptic segmentation results on Citscapes-VPS and KITTI-STEP without bells and whistles. In particular on KITTI-STEP, the simple method can boost almost 12% relative improvements over previous methods. We also validate its generalization on video semantic segmentation, where we boost various baselines by 2% on the VSPW dataset. Moreover, we extend K-Net into clip-level video framework for video instance segmentation where we obtain 40.5% for ResNet50 backbone and 51.5% mAP for Swin-base on YouTube-2019 validation set. We hope this simple yet effective method can serve as a new flexible baseline in video segmentation.\"}"}
{"id": "CVPR-2022-2001", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. An illustration of previous top-down based VPS method (a), bottom-up based VPS method (b) and the proposed Video K-Net (c). Unlike previous approaches [22, 42] that perform panoptic segmentation and object tracking with independent modules, our method unifies panoptic segmentation and instance level tracking via kernels in a simpler framework.\\n\\nTo this end, we design a simple embedding head with a modified contrastive learning loss to obtain better temporal association embeddings. We directly take the dynamic kernels as inputs and output the kernel association embeddings. Such design avoids extra tracking query design. The embedding loss forces the kernels to be more distinctive. Then we also link the kernels between adjacent frames for better and more consistent tracking results. Moreover, to jointly learn more consistent temporal mask classification and prediction, we also propose a temporal kernel fusion module to fuse the kernels with corresponding kernel features in a more efficient manner. Fusing at kernel level avoids computation cost at feature levels (Attention in Fig. 1-(a) and Cascaded ASPP in Fig. 1-(b)). In this way, as shown in Fig. 1-(c), the VPS task can be seen as a kernel linking problem and can significantly reduce the pipeline complexity without extra tracking query and RoI features. The tracking is performed in an online manner. Moreover, it only adds a few extra GFLOPS compared to original K-Net (2.3% GFLOPs). Different from previous works that use RoI heads [22, 36], our kernel based embedding learning avoids box cropping and results in better association performance.\\n\\nVideo K-Net obtains consistent and significant improvements over the strong baseline K-Net with 3.5% STQ on KITTI-STEP and 4% VPQ on Cityscapes-VPS. In particular, our method boosts almost 12% relative improvements over previous bottom up baseline Motion-Deeplab [57]. Video K-Net achieves new state-of-the-art results on two VPS datasets including KITTI-STEP [57] and Cityscapes-VPS [22]. Moreover, as shown in Fig. 2, Video K-Net achieves the best trade-off between accuracy and GFLOPS on the two datasets. We further validate the effectiveness of kernel fusion on VSS task with VSPW dataset [33] and we boost previous baselines [8, 70] via considerable margins on two different metrics. Moreover, we also extend the Video K-Net into clip-level processing via extra temporal kernel fusion module for VIS task. We achieve 40.5% AP using ResNet50 backbone which lead to 4.0% improvements over previous VisTR [54] with less GFLOPs. In summary, extensive experiments and analysis demonstrate that Video K-Net can serve as a new baseline for future research on unified video segmentation tasks.\\n\\n2. Related Work\\n\\nPanoptic Segmentation. The goal of this task is to unify the semantic segmentation and instance segmentation into one framework with a single metric named Panoptic Quality (PQ) [24]. Since then, lots of works [9, 10, 23, 26, 27, 39, 52, 61, 62, 65] have been proposed to solve this task with various approaches. However, most methods separate thing and stuff segmentation as individual tasks. Recently, starting from DETR [5], query based approaches [11, 51, 68] unify both things and stuff segmentation as a set prediction problem. In particular, K-Net [68] unify segmentation tasks...\"}"}
{"id": "CVPR-2022-2001", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in the view of dot product between kernel and feature maps.\\n\\nVideo Semantic/Instance Segmentation. Video Semantic Segmentation, a direct extension of semantic segmentation to the video scenario, requires to predict a semantic label to every pixel in each video frame. Several approaches [19, 45, 73] have been proposed in the literature mainly to model the temporal association such as optical flow warping or attention. Recently, the VSPW dataset [33] is proposed to evaluate large scale video semantic segmentation in the wild. Video Instance Segmentation (VIS) [64] extends instance segmentation into video and it aims to simultaneously classify, segment and track object instances in a given video sequence. Several methods [3, 28] are proposed to link instance-wise feature in video. Recently, several works [21, 54] extend DETR into VIS. Multi-Object Tracking and Segmentation (MOTS) task [50] is proposed to evaluate Multi-Object Tracking along with instance segmentation. However, both VIS and MOTS have limited scale distribution and much fewer objects in the scene.\\n\\nVideo Panoptic Segmentation. Video Panoptic Segmentation (VPS) [20, 22] requires generating the instance tracking IDs along with panoptic segmentation results across video clips. Kim et al. [22] use Cityscapes video sequences for 6 frames out of each short 30 frame clip and mainly focus on short-term tracks. They proposed Video Panoptic Quality (VPQ) for evaluation. Several works [22, 59] are proposed to solve this task respectively. VIP-Deeplab [42] extends the Panoptic-Deeplab [10] with next frame center map prediction. Then they use such predicted maps for offline tracking. Since Cityscapes VPS only contains short-term clips, to allow long term VPS, STEP dataset [57] is proposed. They propose a new metric named Segmentation and Tracking Quality (STQ) that decouples the segmentation and tracking error. They also provide several baselines for reference. Our Video K-Net is verified to work well on both short- and long-term videos.\\n\\nObject Tracking. One of the major tasks in VPS is object tracking. Many studies adopt the tracking-by-detection paradigm [4, 25, 40, 43, 44, 63, 72] and they divide the task into two subtasks where an object detector finds all objects and then a tracking algorithm is employed to associate them. There are also several works [2, 36, 38, 56, 69, 71] that detect and track objects at the same time. There are also tracking methods using object queries. For VPS, VIP-DeepLab [42] performs object tracking by clustering all instance pixels. By contrast, our method directly extends and links kernels into kernel association embeddings and avoids such complex post-grouping steps.\\n\\n3. Method\\n\\nIn this section, we will overview the image baseline K-Net. Then we present several toy experiments on using K-Net in video without extra tracking components. Motivated by that, we propose a simple yet effective approach to learn the kernel association embeddings from kernels and introduce two improvements via linking and fusing kernels on K-Net. Finally, we detail the entire framework including both training and inference.\\n\\n3.1. Using K-Net on Video\\n\\nOverview of K-Net. K-Net [68] formulates different image segmentation tasks (semantic, instance, panoptic) into a unified framework via a group of learnable kernels, where each kernel is responsible for generating a mask for either a potential instance or a stuff class. Started with a set of randomly initialized convolutional kernels, the goal of K-Net is to learn kernels in accordance to the segmentation targets (thing or stuff masks). To distinguish and separate various instances, a cascaded decoder is proposed to refine the learned kernels iteratively. In particular, the authors propose a kernel update strategy that enables each kernel to be dynamic and conditional on its meaningful area of the input features. As shown in the top-left of Fig. 4, it mainly contains three steps: 1, group feature assembling to obtain corresponding kernel features (in grey color box). 2, adaptive feature update where the assembled kernel features are used to update the learned kernel (in yellow color box). 3, kernel interaction where the learned kernels exchange contextual information via self attention layers [48] (in brown color box). Then the final dynamic kernels are used to perform mask classification via fully connected layers. The mask segmentation is done via inner product between the\"}"}
{"id": "CVPR-2022-2001", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4. An illustration of our proposed Video K-Net. Our method is based on K-Net [68] (in blue dashed box), which is the top-left part of the figure. Video K-Net adds Kernel Fusion at the start phase of the last stage. The Kernel Linking is performed on the output of dynamic kernels. The Embedding Head is appended at the output of kernel linking and takes kernel outputs from both sampled frames. Updated kernels and input position-aware features. Then these predictions can be trained in an end-to-end manner with bipartite matching. The above process is repeated several times and the last stage outputs are chosen as the final outputs.\\n\\nToy Experiments on using K-Net on Video. Before detailing the proposed Video K-Net, we present several toy experiments to show the motivation of our work. As mentioned in the previous section, refining kernels is the key for making K-Net work. We first depict kernel indexes of a trained K-Net on Cityscapes video sequence [12] in Fig. 3. We use K-Net directly into video segmentation task. We argue that the updated kernels contains discriminative information, which can be directly used to track the instance in video even without adding extra tracking heads. The insights are as follows: 1, Each output instance mask corresponds with one specific kernel. 2, Each kernel absorbs the position-aware features during the adaptive feature update where the instance aware information has already merged into each kernel. Thus the same instance can be decoded from the previous kernel. In Fig. 3, we present one visual examples on Cityscapes-VSP. We train the K-Net directly on both datasets without adding tracking components. As shown in that figure, in three frames, several kernels are well linked such as person and car. Moreover, we take a further step by directly using the learned kernels as appearance embedding for tracking association. In particular, we use the Quasi-Dense Tracker [36] which is a pure appearance based tracker. We replace the appearance embeddings with our learned kernels from the last stage. As shown in Tab. 1, surprisingly, we found the performance of original K-Net is already good enough to perform tracking and even achieves better results than Uni-track [55] that comes with extra appearance network and motion prediction module [58]. These findings motivate us to dive into the design of linking kernels along the temporal dimension.\\n\\n3.2. Extending K-Net into Video\\n\\nDespite the competitive performance of the original K-Net, it shows several failure cases, for instance, the fast motion object shown in Fig 3. Thus we design three improvements on K-Net including the learning of kernel association embeddings via a modified contrastive learning loss, learning to link tracking kernels, and learning to fuse kernels, respectively.\\n\\nAs shown in Fig. 4, given a key image $I_{\\\\text{key}}$ for training, we randomly select a reference image $I_{\\\\text{ref}}$ from its temporal neighborhood. The neighbor distance is constrained by a window size ranging from $[-2, 2]$ in our experiments by default. Then we inference the K-Net to obtain the two kernels: $K_{\\\\text{key}}$ and $K_{\\\\text{ref}}$.\\n\\nLearning Kernel Association Embeddings. Our method is motivated by the recent work Quasi-Dense [36] in MOT. Instead of using RoI boxes for feature embedding extraction, we use the kernels directly. We add an extra lightweight embedding head after the original K-Net decoder, to extract embedding features for each kernel. The embedding head is implemented via several fully connected layers. We adopt mask-based assignment for Quasi-Dense learning. A kernel embedding is defined as positive to an object if their corresponding masks have an IoU higher than $\\\\alpha_1$, or negative if their corresponding masks have an IoU lower than $\\\\alpha_2$. The values of $\\\\alpha_1$ and $\\\\alpha_2$ are set as 0.7 and 0.3 in our experiments. The matching of kernels on two sampled frames is positive if the two regions are associated...\"}"}
{"id": "CVPR-2022-2001", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"with the same object and negative otherwise. Instead of following the original design [36] that uses hundreds of proposals during training, we only optimize kernels that are matched with ground truth masks. This is because most kernels (unmatched) are not accurate and may cause noise during training, leading to inferior results.\\n\\nAssume there are \\\\( V \\\\) matched kernels on the key frame as training samples and \\\\( K \\\\) matched kernels on the reference frame as contrastive targets where both \\\\( V \\\\) and \\\\( K \\\\) are always fewer than kernel number \\\\( N \\\\). As shown in red boxes of the Fig. 4, these kernels are sampled from \\\\( K \\\\) key and \\\\( K \\\\) ref, respectively.\\n\\n\\\\[\\n\\\\text{L}_{\\\\text{track}} = -X_k + \\\\log \\\\exp (v \\\\cdot k + ) + P_k - \\\\exp (v \\\\cdot k - ) ,\\n\\\\]\\n\\nwhere \\\\( v, k^+, k^- \\\\) are kernel embeddings of the training sample, its positive target, and negative targets in \\\\( K \\\\). In addition, following previous work [36], we also adopt L2 loss as an auxiliary loss.\\n\\n\\\\[\\n\\\\text{L}_{\\\\text{aux}} = (v \\\\cdot k ||v|| \\\\cdot ||k|| - c)^2 ,\\n\\\\]\\n\\nwhere \\\\( c \\\\) is 1 if the match of two samples is positive and 0 otherwise. In comparison to the original work, our modification makes the loss calculation process sparse, benefiting the training process and achieving better results. (See the experiment part in Sec. 4.3). In summary, we term this procedure as learning Kernel Association Embedding (KAE).\\n\\nLearning to Link Kernels. Beyond supervision, we take a further step to link kernels \\\\( K_{\\\\text{key}} \\\\) and \\\\( K_{\\\\text{ref}} \\\\) during training and inference. This forces the kernels to perform interaction along the temporal dimension. We adopt one self-attention layer (MHSA: Multi Head Self Attention) with a Feed Forward Network (FFN) [48] to learn the correspondence among each query to obtain the updated queries, allowing the full correlation among queries. This process is shown as follows:\\n\\n\\\\[\\nK_l = \\\\text{FFN} (\\\\text{MHSA} (K_{\\\\text{key}}, K_{\\\\text{ref}}, K_{\\\\text{ref}}) + K_{\\\\text{key}}) ,\\n\\\\]\\n\\nwhere the query, key and value are \\\\( K_{\\\\text{key}}, K_{\\\\text{ref}}, \\\\) and \\\\( K_{\\\\text{ref}} \\\\), respectively. In this way, kernels from the reference frame are propagated into key frame via the affinity matrix between kernels. Then the linked kernel \\\\( K_l \\\\) is sent to the tracking head. During the training, this operation is only optimized with matched thing kernels. During the inference stage, this operation is performed on final preserved kernels in panoptic segmentation map where other thing kernels are dropped.\\n\\nLearning to Fuse Kernels. The previous linking step may focus on the tracking consistency while ignoring the segmentation consistency. To address this issue, we propose to fuse kernels in between frames on K-Net. In particular, as shown in Fig. 4, we use input kernels of the last stage from two frames. Then we perform kernel updating where we use kernel features of the current frame to update the previous kernel (Step-2 from original K-Net). Then we adopt one self-attention layer with feed forward layers [48] to fuse the previous kernels into the current frame. The updating step is essential. Directly fusing kernels leads to bad results since large motion and scale variation often occur in the video scene. More details can be found in Sec. 4.3. In summary, through the above steps, we link kernels of K-Net into the video domain with a little extra computation cost.\\n\\n3.3. Video K-Net Architecture\\n\\nNetwork Architecture. By default, we adopt the panoptic segmentation setting of K-Net. The kernels are composed of instance kernels and semantic kernels for thing and stuff mask prediction. We adopt semantic FPN [23] for producing high resolution feature map with extra positional encoding as in [5, 48]. The kernel linking operation is appended at the last stage of the K-Net decoder. The kernel fusion is placed at the beginning part. The lightweight tracking head is appended after the decoder. Note that backbone, neck, update head, and embedding head are shared across frames. For VSS task, we directly append the head of Video K-Net at the end of existing semantic segmentation network [8].\\n\\nLoss Function and Training. Compared to the original K-Net, we add the tracking loss at the end of K-Net loss. The total loss function for all kernels is shown as\\n\\n\\\\[\\nL = \\\\lambda_{\\\\text{cls}} L_{\\\\text{cls}} + \\\\lambda_{\\\\text{ce}} L_{\\\\text{ce}} + \\\\lambda_{\\\\text{dice}} L_{\\\\text{dice}} + \\\\lambda_{\\\\text{track}} L_{\\\\text{track}} + \\\\lambda_{\\\\text{aux}} L_{\\\\text{aux}} ,\\n\\\\]\\n\\nwhere \\\\( L_{\\\\text{cls}} \\\\) is Focal loss [29] for classification, and \\\\( L_{\\\\text{ce}} \\\\) and \\\\( L_{\\\\text{dice}} \\\\) are Cross Entropy (CE) loss and Dice loss [34, 53] for segmentation, respectively. \\\\( L_{\\\\text{track}} \\\\) and \\\\( L_{\\\\text{aux}} \\\\) are the tracking loss for instance kernels only. We adopt Hungarian assignment strategy used in [5] for target assignment for end-to-end training. It builds a one-to-one mapping between the predicted instance masks and the ground-truth instances based on the masked-based matching costs. During training, we found that applying the segmentation loss on both key frame and reference frames leads to better results.\\n\\nInference. We use K-Net to generate panoptic segmentation results where we paste thing and stuff masks in a mixed order. We use the kernel embeddings from the learned embedding head as association features. We take these features as the input of Quasi-Dense Tracker [36] where the bidirectional softmax is calculated to associate the instances between two frames in a online manner. Noted that we only track the preserved instance masks from panoptic segmentation maps to save computation cost. We also compare our tracking algorithm with the original Quasi-Dense Tracker [36] using the boxes for appearance modeling.\\n\\nClip-level Training and Inference for VIS. We also extend our Video K-Net into Clip-level training and inference pipeline for Video Instance Segmentation [64].\"}"}
{"id": "CVPR-2022-2001", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"lar, we add three kernel fusion layers on the top of K-Net outputs to jointly fuse both kernels and the corresponding kernel features along the temporal dimension. We use the mean of temporal kernels to represent each object for each clip and assign instance id to each instance in each frame via kernel index [54].\\n\\n4. Experiment\\n\\n4.1. Experiment Setup\\n\\nDataset. We carry out experiments on three video-level datasets: KITTI-STEP, Cityscapes-VPS, and VSPW. KITTI-STEP has 21 and 29 sequences for training and testing, respectively. The training sequences are further split into a training set (12 sequences) and a validation set (9 sequences). Cityscapes-VPS contains 400 training, 50 validation, and 50 test videos. Following previous work [22], all 30 frames are predicted during the inference, and only 6 frames with ground truth are evaluated. Both Cityscapes-VPS and KITTI-STEP have the same class number with Cityscapes dataset [12]. However, the definition of thing and class is different. For VSPW dataset and YouTube-VIS dataset, refer to the appendix file.\\n\\nImplementation Details. We implement our models in PyTorch [37] with MMDetection toolbox [6]. We use the distributed training framework with 8 GPUs. Each mini-batch has one image per GPU. Following previous work, we first pretrain the image baseline on Cityscapes dataset [12] where we adopt the same pretraining setting [10, 57] for fair comparison. For STEP pretraining, we change the class distribution of Cityscapes dataset into STEP format to avoid overfitting on STEP. Both ResNet [18] and Swin Transformer [30] are adopted as the backbone networks and other layers use Xavier initialization [17]. The optimizer is AdamW [31] with weight decay 0.0001. We adopt full image size for random crop in both pretraining and training process. For the large Swin Transformer backbone [30], we also pretrain our model on Mapillary dataset [35]. More details of pretraining and finetuning can be found in the appendix file. Due to diversity of dataset, for VSPW and YouTube-VIS dataset, refer to the appendix file.\\n\\nEvaluation Metrics. For VPS task, as discussed in STEP [57], different metrics result in a significant gap even on the same VPS dataset since different metrics emphasize different properties. We adopt two widely used metrics: Video Panoptic Quality (VPQ) and Segmentation and Tracking Quality (STQ). The former mainly focuses on mask proposal level as PQ [24] with different window sizes and threshold parameters while the latter emphasizes pixel level segmentation and tracking without any thresholds. STQ contains geometric mean of two items: Segmentation Quality (SQ) and Association Quality (AQ) where $\\\\text{STQ} = \\\\left(\\\\text{SQ} \\\\times \\\\text{AQ}\\\\right)^{1/2}$. The former evaluates the pixel-level tracking while the latter evaluates the pixel-level segmentation results in a video clip. Due to the interpretability [57] of STQ, we adopt it for ablation studies. We also report VPQ for the benchmark comparison on both datasets. For VSS task, we report the Mean Intersection over Union (mIoU) and mean Video Consistency (mVC) [33] for reference.\\n\\n4.2. Main Results\\n\\nResults on KITTI-STEP. As shown in Tab. 2, our method achieves 0.71 STQ and 0.46 VPQ using ResNet50 as backbone and achieves the new state-of-the-art results among previous works. After applying a stronger backbone [30], we obtain about 3% gains on STQ and 7% gains on VPQ. This suggests strong feature representation leads to better segmentation-level prediction than pixel-level prediction. Compared with VPSNet [22] and Motion-Deeplab [57], our method does not need post processing steps or extra optical flow to warp features. Compared with Motion-Deeplab [57], our method outperforms it by 13% and 7% on validation set and test set respectively. The results suggest the suitability of our framework to serve as a new and stronger baseline. We find 0.4% noise on STEP validation set.\\n\\nResults on Cityscape-VPS. Tab. 3 compares our method with previous works on Cityscapes-VPS datasets. For ResNet50, our method outperforms VPSNet [22] by 0.3%. Moreover, we follow the same pretraining procedure of ViP-Deeplab [42] and achieve 62.2% VPQ. Our method with Swin-Transformer base achieves better results than ViP-Deeplab, which is trained with stronger data augmentation [13]. Note that we find 0.4% noise for this dataset. Moreover, we do not use the kernel fusion on this dataset due to the low frame rate of Cityscapes-VPS.\\n\\nResults on VSPW. We further carry out experiments on VSPW dataset for VSS task to prove the generalization of Video K-Net. All the methods are re-implemented on our codebase and achieve higher results than the original.\"}"}
{"id": "CVPR-2022-2001", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 3. Results on Cityscapes-VPS validation set\\n\\n$k$ is temporal window size in [22]. All the methods use the single scale inference without other augmentations in the test stage. In each cell, we report $V_{PQ}^c$, $V_{PQ}^{\\\\text{thing}}$ and $V_{PQ}^{\\\\text{stuff}}$ in order. There is about 0.4% noise on this dataset where we report the average results (three times).\\n\\n| Method                  | Backbone       | $k = 0$ | $k = 5$ | $k = 10$ | $k = 15$ | Average |\\n|-------------------------|----------------|---------|---------|----------|----------|---------|\\n| VPSNet [22]             | ResNet50       | 65.0    | 59.0    | 69.4     | 57.6     | 57.5    |\\n| SiamTrack [59]          | ResNet50       | 64.6    | 58.3    | 69.1     | 57.6     | 57.3    |\\n| ViP-Deeplab [42]        | WideResNet41   | 68.2    | N/A     | N/A      | 61.3     | 57.5    |\\n| ViP-Deeplab [42]        | WideResNet41+RFP+AutoAug [13] | 69.2    | N/A     | N/A      | 62.3     | 59.2    |\\n| Video K-Net             | ResNet50       | 65.6    | 57.4    | 71.5     | 57.7     | 57.8    |\\n| Video K-Net             | Swin-base [30] | 69.2    | 63.6    | 73.3     | 62.0     | 61.2    |\\n| Video K-Net             | Swin-base+RFP [41] | 70.8    | 63.2    | 76.3     | 63.1     | 62.2    |\\n\\n### Table 4. Results on VSPW validation set\\n\\n$mV_C$ means that a clip with $c$ frames is used. All methods use the same setting for fair comparison.\\n\\n| Method                  | Backbone       | $mIoU$ | $mV_C$ |\\n|-------------------------|----------------|--------|--------|\\n| DeepLabv3+ [8]          | ResNet101      | 35.7   | 83.5   |\\n| PSPNet+ [70]            | ResNet101      | 36.5   | 84.4   |\\n| TCB(PSPNet) [33]        | ResNet101      | 37.5   | 86.9   |\\n| Video K-Net (Deeplabv3+) | ResNet101      | 37.9   | 87.0   |\\n| Video K-Net (PSPNet)    | ResNet101      | 38.0   | 87.2   |\\n\\n### Table 5. Video instance segmentation AP (%) on the YouTube-VIS-2019 [64] validation dataset. The compared methods are listed by publication date.\\n\\n| Method                  | backbone       | AP 50 | AP 75 | AR 1 | AR 10 |\\n|-------------------------|----------------|-------|-------|------|-------|\\n| FEELVOS [49]            | ResNet50       | 26.9  | 42.0  | 29.7 | 29.9  |\\n| MaskTrack R-CNN [64]    | ResNet50       | 30.3  | 51.1  | 32.6 | 31.0  |\\n| MaskProp [3]            | ResNet-50      | 40.0  | 42.9  | 42.9 | 42.9  |\\n| MaskProp [3]            | ResNet101      | 42.5  | 45.6  | 45.6 | 45.6  |\\n| STEm-Seg [1]            | ResNet50       | 30.6  | 50.7  | 33.5 | 31.6  |\\n| STEm-Seg [1]            | ResNet101      | 34.6  | 55.8  | 37.9 | 34.4  |\\n| CompFeat [15]           | ResNet50       | 35.3  | 56.0  | 38.6 | 33.1  |\\n| CompFeat [15]           | ResNet101      | 34.6  | 55.8  | 37.9 | 34.4  |\\n| VisTR [54]              | ResNet50       | 34.4  | 55.7  | 36.5 | 33.5  |\\n| VisTR [54]              | ResNet101      | 35.3  | 57.0  | 36.2 | 34.3  |\\n| Video K-Net             | ResNet50       | 40.5  | 63.5  | 44.5 | 40.7  |\\n| Video K-Net             | Swin-base [30] | 51.4  | 77.2  | 56.1 | 49.0  |\\n\\n### Ablation Study\\n\\n#### Ablation Study on Each Components.\\n\\nIn Tab. 6a, we first perform ablation studies on the effectiveness of each component. Adding KAE yields 2.8% STQ improvements with more significant gains on AQ (about 2.4%). It shows that directly learning kernel association is a key factor for tracking. Adding Kernel Linking module (KL) further improves AQ by 1.0%. Finally, appending kernel fusing results in 0.7% STQ improvement and 1.4% improvement on SQ. The AQ decreases a little mainly because both KL and KF are fighting for tracking quality and segmentation quality, respectively.\\n\\n#### Needs of Extra Appearance Embedding.\\n\\nIn Tab. 6b, we present detailed comparison with recent box based appearance embedding and mask based appearance embedding. In particular, we re-implement their methods on our baseline K-Net. We also use the same embedding loss functions: Equ. 1 and Equ. 2 for fair comparison. From the table, our simple design leads to the best result. From the last row of Tab. 6b, even combing both the appearance embedding and kernels, there is no clear gain. We implement this by adding mask-grouped appearance features from backbone and kernel embeddings together. That indicates kernels have already encoded the discriminative information, which is consistent with our toy experiments results: the pure kernel information is good enough for tracking.\\n\\n#### Effect of Sampling Examples in Association.\\n\\nPang et al. [36] employ RPN to generate large samples to improve the tracking results. However, we find that directly using this method into our framework leads to inferior results because most kernels are not used to generate masks with Hungarian assignment strategy during the training stage. Adding more samples brings noise to the segmentation prediction shown in Tab. 6c, causing a marked decrease in SQ metric. Thus we adopt to assign training kernel samples that are matched with ground truth masks. The matched kernels are more robust during training.\\n\\n#### Ablation on Link Stage.\\n\\nIn Tab. 6d, we present ablation on the stage of linking kernels where we find linking kernels at the last stage achieves the best results. Early fusing of kernels also leads to bad results since initial kernels are not very accurate. This results in bad misalignment on both kernel and kernel features.\\n\\n#### Effect on Training Settings.\\n\\nMoreover, in Tab. 6e, we find joint training with both frames $I^\\\\text{key}$ and $I^\\\\text{ref}$ performs better than only training $I^\\\\text{key}$. This is mainly because both the proposed Kernel Linking and Kernel Fusion need to learn to group similar kernels and separate the different kernels.\\n\\n#### Ablation on Kernel Fusing Design.\\n\\nIn Tab. 6f, we find the importance of Kernel Updating module in Kernel Fusing. Since previous features are not aligned to the current frame,\"}"}
{"id": "CVPR-2022-2001", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6. Ablation studies and comparison analysis on KITTI-STEP validation set. All the experiments use ResNet-50 as backbone.\\n\\n(a) Ablation Study on Each Components.\\n\\n| Method  | STQ  | SQ  |\\n|---------|------|-----|\\n| baseline | 67.5 | 65.5 | 68.9 |\\n| K-Net   | 69.3 | 69.0 | 69.8 |\\n| \u2713       | \u2713    | \u2713   |\\n| 70.2    | 71.2 | 69.7 |\\n| \u2713       | \u2713    | \u2713   |\\n| 70.9    | 70.8 | 71.2 |\\n\\n(b) Needs of Appearance Embeddings\\n\\n| Method               | STQ  | AQ  |\\n|----------------------|------|-----|\\n| RoI-Align [36]       | 68.8 | 69.1 |\\n| Mask-Emb [59]        | 67.3 | 68.1 |\\n| Ours                 | 70.8 | 70.9 |\\n| Ours + Mask-Emb [59] | 70.3 | 70.8 |\\n\\n(c) Effect of sampling in association.\\n\\n| Method               | STQ  | AQ  | SQ  |\\n|----------------------|------|-----|-----|\\n| K-Net                | 67.5 | 65.5 | 68.9 |\\n| GT-based (ours)      | 69.3 | 69.0 | 69.8 |\\n| sampling in [36]     | 63.1 | 62.1 | 64.3 |\\n\\n(d) Ablation Study on Linking and Fusing Stage.\\n\\n| Stage | STQ  | AQ  | SQ  |\\n|-------|------|-----|-----|\\n| 3     | 70.9 | 70.8 | 71.2 |\\n| 2     | 68.5 | 68.2 | 69.3 |\\n| 1     | 66.9 | 63.4 | 67.3 |\\n\\n(e) Ablation Study on Training Settings\\n\\n| Settings       | STQ  | AQ  | SQ  |\\n|----------------|------|-----|-----|\\n| joint training | 70.9 | 70.8 | 71.2 |\\n| only train the key frame | 70.1 | 70.1 | 69.8 |\\n\\n(f) Ablation Study on Kernel Fusing Settings\\n\\n| Settings       | STQ  | AQ  | SQ  |\\n|----------------|------|-----|-----|\\n| K-Net          | 67.5 | 65.5 | 68.9 |\\n| w Update       | 70.9 | 70.8 | 71.2 |\\n| w/o Update     | 67.1 | 66.2 | 68.3 |\\n\\nBaseline Our Video K-Net\\n\\nFigure 5. Visualization on improvements over baseline methods. Tracking improvement in red boxes and segmentation improvement in yellow boxes. Best viewed in color and by zooming in.\\n\\nFigure 6. Visual Examples of our Video K-Net. The left is on Cityscapes-VPS validation set while the right is on KITTI-STEP validation set (video sequences from top to down). The same instances are shown with the same color. Best view it on screen.\\n\\nwe use current-frame features to re-weight previous kernels to filter out noise and outliers in previous kernel.\\n\\n4.4. Visualization and More Analysis\\n\\nVisualization over Baseline.\\n\\nIn Fig. 5, we visualize the improvements over the K-Net baseline. The red boxes show the tracking consistency while the yellow boxes show the segmentation consistency. Our Video K-Net achieves better results on both tracking and segmentation qualities.\\n\\nMore Qualitative Results.\\n\\nIn Fig. 6, we present several visual results on Citscapes-VPS and KITTI-STEP datasets. We find most foreground objects (person and car) can be well tracked and segmented. More visual examples can be found in the appendix file.\\n\\nParameter and GFLOPs\\n\\nCompared with K-Net baseline, our method only adds about 2.3% GFLOPs and 1.8% parameters given 800 x 1333 image inputs.\\n\\nLimitation and Future Work.\\n\\nOne limitation of Video K-Net is that we only explore one frame during both training and inference. This setting is mainly for fair comparison with other works [22, 57]. Exploring the multiple frames information via kernels will be our future work.\\n\\n5. Conclusion\\n\\nWe present Video K-Net, a simple, strong and unified system for fully end-to-end video panoptic segmentation. We propose to learn the kernel association embeddings directly from the encoded 'thing' kernels. Then we link and fuse the kernels to jointly improve both tracking and segmentation results. Despite simplicity, our method achieves state-of-the-art results on two VPS datasets including Cityscapes-VPS and KITTI-STEP. In particular, compared to previous works, our method has a much simpler pipeline but achieves better results along with less computation cost. We also show the generalization ability on the VSPW dataset and competitive results on YouTub-VIS dataset. We hope our proposed Video K-Net would be a simple baseline and benefit dense video segmentation field.\\n\\nAcknowledgement.\\n\\nThis study is partly supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). It is also partially supported by the NTU NAP grant. This research is also supported by the National Key Research and Development Program of China under Grant No. 2020YFB2103402. We also thank for the GPU resource provided by SenseTime Research.\"}"}
{"id": "CVPR-2022-2001", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Ali Athar, Sabarinath Mahadevan, Aljo\u0161a O\u0161ep, Laura Leal-Taix\u00e9, and Bastian Leibe. Stem-seg: Spatio-temporal embeddings for instance segmentation in videos. In ECCV, 2020.\\n\\n[2] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. Tracking without bells and whistles. In ICCV, pages 941\u2013951, 2019.\\n\\n[3] Gedas Bertasius and Lorenzo Torresani. Classifying, segmenting, and tracking object instances in video with mask propagation. In CVPR, 2020.\\n\\n[4] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime tracking. In ICIP, pages 3464\u20133468. IEEE, 2016.\\n\\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\\n\\n[6] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\\n\\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv:1706.05587, 2017.\\n\\n[8] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.\\n\\n[9] Yifeng Chen, Guangchen Lin, Songyuan Li, Omar Bourahla, Yiming Wu, Fangfang Wang, Junyi Feng, Mingliang Xu, and Xi Li. Banet: Bidirectional aggregation network with occlusion handling for panoptic segmentation. In CVPR, 2020.\\n\\n[10] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation. In CVPR, 2020.\\n\\n[11] Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. arXiv, 2021.\\n\\n[12] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.\\n\\n[13] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.\\n\\n[14] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In CVPR, 2015.\\n\\n[15] Yang Fu, Linjie Yang, Ding Liu, Thomas S. Huang, and Humphrey Shi. Compfeat: Comprehensive feature aggregation for video instance segmentation. AAAI, 2021.\\n\\n[16] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, 2012.\\n\\n[17] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010.\\n\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\n[19] Ping Hu, Fabian Caba, Oliver Wang, Zhe Lin, Stan Sclaroff, and Federico Perazzi. Temporally distributed networks for fast video semantic segmentation. CVPR, 2020.\\n\\n[20] Juana Valeria Hurtado, Rohit Mohan, Wolfram Burgard, and Abhinav Valada. Mopt: Multi-object panoptic tracking. arXiv preprint arXiv:2004.08189, 2020.\\n\\n[21] Sukjun Hwang, Miran Heo, Seoung Wug Oh, and Seon Joo Kim. Video instance segmentation using inter-frame communication transformers. NIPS, 2021.\\n\\n[22] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon. Video panoptic segmentation. In CVPR, 2020.\\n\\n[23] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Panoptic feature pyramid networks. In CVPR, 2019.\\n\\n[24] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll\u00e1r. Panoptic segmentation. In CVPR, 2019.\\n\\n[25] Laura Leal-Taix\u00e9, Cristian Canton-Ferrer, and Konrad Schindler. Learning by tracking: Siamese cnn for robust target association. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 33\u201340, 2016.\\n\\n[26] Jie Li, Allan Raventos, Arjun Bhargava, Takaaki Tagawa, and Adrien Gaidon. Learning to fuse things and stuff. arXiv:1812.01192, 2018.\\n\\n[27] Yanwei Li, Hengshuang Zhao, Xiaojuan Qi, Liwei Wang, Zeming Li, Jian Sun, and Jiaya Jia. Fully convolutional networks for panoptic segmentation. CVPR, 2021.\\n\\n[28] Huaijia Lin, Ruizheng Wu, Shu Liu, Jiangbo Lu, and Jiaya Jia. Video instance segmentation with a propose-reduce paradigm. ICCV, 2021.\\n\\n[29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In ICCV, 2017.\\n\\n[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. ICCV, 2021.\\n\\n[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2017.\"}"}
{"id": "CVPR-2022-2001", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[33] Jiaxu Miao, Yunchao Wei, Yu Wu, Chen Liang, Guangrui Li, and Yi Yang. Vspw: A large-scale dataset for video scene parsing in the wild. In CVPR, 2021.\\n\\n[34] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-Net: Fully convolutional neural networks for volumetric medical image segmentation. In 3DV, 2016.\\n\\n[35] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In ICCV, 2017.\\n\\n[36] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasi-dense similarity learning for multiple object tracking. In CVPR, June 2021.\\n\\n[37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.\\n\\n[38] Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei Fu. Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking. In ECCV, 2020.\\n\\n[39] Lorenzo Porzi, Samuel Rota Bulo, Aleksander Colovic, and Peter Kontschieder. Seamless scene segmentation. In CVPR, 2019.\\n\\n[40] Lorenzo Porzi, Markus Hofinger, Idoia Ruiz, Joan Serrat, Samuel Rota Bulo, and Peter Kontschieder. Learning multi-object tracking and segmentation from automatic annotations. In CVPR, pages 6846\u20136855, 2020.\\n\\n[41] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution. In CVPR, 2021.\\n\\n[42] Siyuan Qiao, Yukun Zhu, H. Adam, A. Yuille, and Liang-Chieh Chen. Vip-deeplab: Learning visual perception with depth-aware video panoptic segmentation. CVPR, 2021.\\n\\n[43] Samuel Schulter, Paul Vernaza, Wongun Choi, and Manmohan Chandraker. Deep network flow for multi-object tracking. In CVPR, pages 6951\u20136960, 2017.\\n\\n[44] Sarthak Sharma, Junaid Ahmed Ansari, J Krishna Murthy, and K Madhava Krishna. Beyond pixels: Leveraging geometry and shape cues for online multi-object tracking. In ICRA, pages 3508\u20133515, 2018.\\n\\n[45] Evan Shelhamer, Kate Rakelly, Judy Hoffman, and Trevor Darrell. Clockwork convnets for video semantic segmentation. In ECCV, pages 852\u2013868. Springer, 2016.\\n\\n[46] Peize Sun, Jinkun Cao, Yi Jiang, Rufeng Zhang, Enze Xie, Zehuan Yuan, Changhu Wang, and Ping Luo. Transtrack: Multiple-object tracking with transformer. arXiv preprint arXiv:2012.15460, 2020.\\n\\n[47] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, pages 402\u2013419. Springer, 2020.\\n\\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\\n\\n[49] Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, and Liang-Chieh Chen. Feelvos: Fast end-to-end embedding learning for video object segmentation. In CVPR, 2019.\\n\\n[50] Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, and Bastian Leibe. Mots: Multi-object tracking and segmentation. In CVPR, 2019.\\n\\n[51] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end panoptic segmentation with mask transformers. arXiv preprint arXiv:2012.00759, 2020.\\n\\n[52] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020.\\n\\n[53] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and Lei Li. Solo: Segmenting objects by locations. In ECCV, 2020.\\n\\n[54] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In CVPR, 2021.\\n\\n[55] Zhongdao Wang, Hengshuang Zhao, Ya-Li Li, Shengjin Wang, Philip HS Torr, and Luca Bertinetto. Do different tracking tasks require different appearance models? NeurIPS, 2021.\\n\\n[56] Zhongdao Wang, Liang Zheng, Yixuan Liu, and Shengjin Wang. Towards real-time multi-object tracking. arXiv preprint arXiv:1909.12605, 2019.\\n\\n[57] M. Weber, J. Xie, M. Collins, Yukun Zhu, P. Voigtlaender, H. Adam, B. Green, A. Geiger, B. Leibe, D. Cremers, Aljosa Osep, L. Leal-Taix\u00e9, and Liang-Chieh Chen. Step: Segmenting and tracking every pixel. NIPS, 2021.\\n\\n[58] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In ICIP, 2017.\\n\\n[59] Sanghyun Woo, Dahun Kim, Joon-Young Lee, and In So Kweon. Learning to associate every segment for video panoptic segmentation. In CVPR, 2021.\\n\\n[60] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. In ECCV, pages 3\u201319, 2018.\\n\\n[61] Yangxin Wu, G. Zhang, Hang Xu, Xiaodan Liang, and Liang Lin. Auto-panoptic: Cooperative multi-component architecture search for panoptic segmentation. NIPS, 2020.\\n\\n[62] Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min Bai, Ersin Yumer, and Raquel Urtasun. Upsnet: A unified panoptic segmentation network. In CVPR, 2019.\\n\\n[63] Jiarui Xu, Yue Cao, Zheng Zhang, and Han Hu. Spatial-temporal relation networks for multi-object tracking. In ICCV, pages 3988\u20133998, 2019.\\n\\n[64] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In ICCV, 2019.\"}"}
{"id": "CVPR-2022-2001", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yibo Yang, Hongyang Li, Xia Li, Qijie Zhao, Jianlong Wu, and Zhouchen Lin. Sognet: Scene overlap graph network for panoptic segmentation. In AAAI, 2020.\\n\\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\\n\\nFangao Zeng, Bin Dong, Tiancai Wang, Cheng Chen, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. arXiv preprint arXiv:2105.03247, 2021.\\n\\nWenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change Loy. K-net: Towards unified image segmentation. CoRR, abs/2106.14855, 2021.\\n\\nZheng Zhang, Dazhi Cheng, Xizhou Zhu, Stephen Lin, and Jifeng Dai. Integrated object detection and tracking with tracklet-conditioned detection. arXiv preprint arXiv:1811.11167, 2018.\\n\\nHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017.\\n\\nXingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. ECCV, 2020.\\n\\nJi Zhu, Hua Yang, Nian Liu, Minyoung Kim, Wenjun Zhang, and Ming-Hsuan Yang. Online multi-object tracking with dual matching attention networks. In ECCV, pages 366\u2013382, 2018.\\n\\nXizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature flow for video recognition. In CVPR, 2017.\"}"}
