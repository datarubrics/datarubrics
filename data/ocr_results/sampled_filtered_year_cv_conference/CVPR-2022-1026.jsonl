{"id": "CVPR-2022-1026", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and adjoint nuFFT \\\\[11, 35\\\\].\\n\\n\u03b2 is an end-to-end mapping network, taking intermediate images as the input and removing residual artifacts and noises. In our study, the loss \\\\(\\\\ell\\\\) uses a hybrid loss function combining both \\\\(l^1\\\\) loss and Structural Similarity Index Measurement (SSIM) loss \\\\[49\\\\] to characterize both the pixel-wise and structural similarity between the output of the mapping network and the fully sampled MR images \\\\(I\\\\). In addition, because the physical constraints to regularize k-space trajectory are undifferentiable in network training, an element-wise soft shrinkage loss function \\\\[25\\\\] \\\\(S\\\\) is introduced. More specifically, \\\\(S(b, c)\\\\) returns zero when input \\\\(b\\\\) is in the range of \\\\([-c, c]\\\\), otherwise returns the difference between \\\\(b\\\\) and \\\\(c\\\\). This loss function penalizes the training to ensure the imaging gradient and the gradient slew rate can converge into the maximum allowed values.\\n\\n\\\\(\\\\lambda_1\\\\) and \\\\(\\\\lambda_2\\\\) are weighting factors to balance the loss terms in this optimization function.\\n\\n4. Experiments\\n\\n4.1. In-vivo Image Datasets\\n\\nExperiments were conducted to evaluate the proposed method using the image datasets from fastMRI database \\\\[52\\\\], which contains knee and brain MR images acquired with different MR sequences. For the knee images, a subset of knee data on 80 subjects was randomly selected for training \\\\((N=60)\\\\), validation \\\\((N=5)\\\\), and testing \\\\((N=15)\\\\). The knee image dataset was acquired using a 2D coronal proton density-weighted fast spin-echo sequence with the following imaging parameters: echo train length 4, matrix size 320\u00d7320, in-plane resolution 0.5mm\u00d70.5mm, slice thickness 3mm, repetition time (TR) ranging between 2200 and 3000ms, and echo time (TE) ranging between 27 and 34ms. The k-space data were fully sampled using a 16-channel knee coil array at 1.5T and 3.0T MR scanners. For the brain images, a subset of the brain data on 65 subjects was randomly selected for training \\\\((N=50)\\\\), validation \\\\((N=5)\\\\), and testing \\\\((N=10)\\\\). The brain datasets include 2D axial T1-weighted images without (AXT1) and with (AXT1POST) the administration of contrast agent. These fully sampled brain images were acquired using multi-channel brain coil arrays at 1.5T and 3.0T MR scanners based on the standard clinical protocol. Because the image size varies across different subjects, the images were unified into a 256x256 matrix size via cropping the central region of images that were obtained by taking the inverse Fourier Transform of the fully sampled k-space data. To demonstrate the capability of our proposed method for optimizing both Cartesian and Non-Cartesian trajectories, we investigated initializing the trajectory optimization using three different k-space sampling patterns, including Cartesian, spiral (in Appendix), and radial, respectively. Acceleration was implemented using a reduced number of shots, including Figure 2.\\n\\nTrajectory comparison between fixed and learned radial trajectory with 16 spokes, optimized on the brain AXT1POST datasets. Top: initial radial trajectory. Middle: learned radial trajectory without physical constraints. Bottom: learned radial trajectory with physical constraints. The corresponding PSF is shown for each trajectory.\\n\\n\\\\(N = 4, 8, 16, \\\\text{ and } 32\\\\), respectively. For the radial initialization, a spoke at 0 degrees was first created, and then the other \\\\(N - 1\\\\) spokes were built by linearly rotating \\\\(\\\\pi/N\\\\) degrees. Each shot was designed to contain 1000 sampling points corresponding to a readout time of 4ms at a dwell time of 4\\\\(\\\\mu\\\\)s. The maximum imaging gradient was 50mT/m, and the maximum slew rate was 200T/m/s. To reduce the length of the integral in the ODE, each shot was further divided into 100 segments with equal length, and one control point in each segment (resulting in a total of 100 control points per shot) was used as the initial state to predict the remaining 900 sampling points using the described ODE solver in Eq. (3).\\n\\nOur proposed models were designed using PyTorch \\\\[33\\\\]. More architecture details are provided in appendix. The ODE solver was implemented using TorchDiffeq \\\\[9\\\\] package, from which a Runge-Kutta of order five of Dormand-Prince-Shampine solver \\\\[8\\\\] was applied given its good computing performance, accuracy, and efficiency in our initial experiment. Because of the GPU memory limit, the multi-channel images were combined into one image using a root-sum-of-squares reconstruction (RSS) \\\\[38\\\\] and then used as the input of the U-Net \\\\[39\\\\] reconstruction network. The Adam optimizer \\\\[20\\\\] was applied to train our networks, and the learning rates were set to 0.01 and 0.001 for the neural ODE and the U-Net, respectively. The framework was trained for a total of 100 epochs. To facilitate...\"}"}
{"id": "CVPR-2022-1026", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Demonstration of trajectory optimization difference between different image datasets.\\n\\nThe neural ODE convergence, the networks were optimized without the constraints at the first 25 epochs. Then, physical constraints were added to fine-tune the learned trajectory by empirically setting $\\\\lambda_1$ and $\\\\lambda_2$ to 0 for Eq. (15). All the training and testing were performed using an NVIDIA Tesla K80 GPU with 11GB GDDR5 RAM.\\n\\n4.2. Evaluation of Reconstruction\\n\\nImage reconstruction was evaluated qualitatively and quantitatively. Quantitative metrics calculate the difference between the reconstructed and fully sampled reference images in the testing datasets. Two metrics were used, focusing on different aspects of the reconstruction quality. The peak signal-to-noise ratio (PSNR) was used to assess the overall reconstructed image errors. The SSIM was used to estimate the overall image similarity to the reference images. A paired nonparametric Wilcoxon signed-rank test with statistical significance defined as a $p < 0.05$ was used to evaluate the group-wise differences between methods.\\n\\n4.2.1 Analysis, Visualization, and Evaluation\\n\\nThe learned k-space trajectories using radial trajectory as the initial sampling pattern were demonstrated in Figure 2. Both the initial trajectory and the learned trajectory are presented. The learned trajectories tend to densely sample the central k-space region with higher information density. The learned trajectories are divergent at the peripheral k-space region since the trajectories prefer to acquire appropriate high-frequency components, which are typically sparse in the k-space.\\n\\nThe radial imaging use straight-line sampling pattern to acquire k-space. In Figure 2, the wavy pattern was formed based on the radial spoke, resulting in a dense sampling of the central k-space. Unlike the radial trajectory, where the sampling density falls off rapidly away from the k-space center, the learned trajectory provides a new strategy to sample the entire k-space with a balanced and uniform sampling density distribution. The influence of the physical constraints on the optimized trajectories is also demonstrated in the middle of Figure 2. In these examples, the trajectories were only optimized on image loss from the first term of Eq. (15). While the unconstrained trajectories attempted to rapidly explore large sampling space for covering more k-space information, the resulted trajectories tend to traverse with abrupt turns, leading to irregular non-smooth sampling patterns which are difficult to be implemented in MRI scanners. However, the learned trajectories under physical constraints can produce a hardware-friendly waveform for practical implementation.\\n\\nThe point-spread function (PSF) is also demonstrated for each corresponding trajectory in Figure 2. Compared with the fixed PSF, the learned trajectory can lead to a PSF with reduced side lobes and more homogeneous sampling of the neighboring pixels. This can result in reduced structural and aliasing imaging artifacts in the undersampled images.\\n\\nThe context-awareness of the trajectory optimization is also investigated by comparing the optimized trajectories for datasets with different anatomies. Figure 3 illustrates the learned trajectories for brain and knee datasets, respectively, using an initialization of the same Cartesian trajectory. There are differences between the exemplified k-space for the brain and knee due to the difference of the imaged objects. The learned trajectories can realize this feature and correctly characterize the difference of the k-space density distribution. More specifically, the learned trajectory has more fluctuation and coverage for the scattered knee k-space than the more centralized brain k-space.\\n\\nThe reconstructed images from the learned trajectories were demonstrated in Figure 4. These images were compared with the images directly reconstructed using fixed trajectory. The framework was slightly modified by removing the trajectory optimization network and only training the end-to-end reconstruction U-Net using the standard supervised learning approach. The qualitative evaluation of brain images proves the improved image reconstruction using our proposed method. As illustrated in Figure 4, the reconstructed images from learned trajectories are consistently better than those from the fixed trajectories for each type. More specifically, the learned radial trajectories provided improved reconstruction performance compared to their fixed counterparts in Figure 4 for the brain images at the AXT1POST sequence (More results in Appendix). Notably, the intermediate images directly obtained from the RSS reconstruction were shown at the top of Figure 4.\"}"}
{"id": "CVPR-2022-1026", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reference Fixed Learned Fixed Learned\\n\\nFigure 4. MRI reconstruction comparison between fixed (Fixed) and learned (Learned) radial trajectory with 32 spokes for brain AXT1POST images.\\n\\nThe leftmost column shows the ground truth fully sampled image. Compared to this, the learned trajectory provides accurate image contract recovery and detailed blood vessel reconstruction superior to the fixed trajectory both before and after the reconstruction, as indicated by the red arrows.\\n\\nTable 1. Quantitative comparison on brain AXT1 images at different acceleration levels (Acc. Level).\\n\\n| Acc. Level | Cartesian Radial Spiral | Fixed | Learned | Fixed | Learned |\\n|------------|-------------------------|-------|---------|-------|---------|\\n| PSNR       | SSIM                    | PSNR  | SSIM    | PSNR  | SSIM    |\\n| 4-Shots    | 29.75 \u00b1 0.78           | 0.81  | \u00b1 0.02  | 30.11 | \u00b1 0.81  |\\n| 24.24 \u00b1 0.83 | 0.65 \u00b1 0.02  |       | 24.98 \u00b1 0.72 |\\n| 26.94 \u00b1 0.72 | 0.74 \u00b1 0.01  |       | 29.36 \u00b1 0.67 |\\n| 8-Shots    | 30.54 \u00b1 0.78           | 0.82  | \u00b1 0.01  | 30.55 | \u00b1 0.75  |\\n| 25.71 \u00b1 0.89 | 0.69 \u00b1 0.02  |       | 28.98 \u00b1 0.58 |\\n| 29.44 \u00b1 0.98 | 0.81 \u00b1 0.01  |       | 30.99 \u00b1 0.64 |\\n| 16-Shots   | 31.06 \u00b1 0.85           | 0.83  | \u00b1 0.01  | 31.27 | \u00b1 0.70  |\\n| 28.38 \u00b1 1.69 | 0.74 \u00b1 0.02  |       | 31.03 \u00b1 1.23 |\\n| 30.62 \u00b1 1.15 | 0.84 \u00b1 0.01  |       | 31.73 \u00b1 0.85 |\\n| 32-Shots   | 31.56 \u00b1 0.86           | 0.84  | \u00b1 0.01  | 31.75 | \u00b1 0.72  |\\n| 30.18 \u00b1 1.73 | 0.78 \u00b1 0.01  |       | 33.06 \u00b1 1.28 |\\n| 32.08 \u00b1 1.62 | 0.86 \u00b1 0.01  |       | 32.79 \u00b1 0.87 |\\n\\nRow of Figure 4 for the learned and fixed trajectories. It is evident that the learned trajectory can better remove structural and aliasing artifacts and provided more realistic image features and accurate image contrast than that of the fixed trajectory at the same level of acceleration, indicating the efficacy of the learning-based trajectory optimization.\\n\\n4.2.2 Quantitative Comparison\\n\\nThe group-wise quantitative analyses further confirmed the qualitative comparisons as shown in the exemplary figures. Tables 1 and 2 summarized the quantitative comparison between images reconstructed using the fixed and learned Cartesian, radial, and spiral trajectories in all testing AXT1 and AXT1POST brain image datasets at different acceleration levels. In general, there is improved reconstruction quality with the decrease of acceleration level (i.e., more shots) for both fixed and learned trajectories, as indicated by the increased PSNR and SSIM values. At the same acceleration level, the learned trajectories show significantly better reconstruction quality (p < 0.05) than the fixed ones in terms of PSNR and SSIM metrics for all sampling patterns on AXT1 and AXT1POST brain images.\\n\\nMore comparison results can be found in the supplementary document of this paper.\\n\\n5. Discussion\\n\\nThis study demonstrated the feasibility of optimizing MRI k-space acquisition using a deep learning framework consisting of a neural ODE and an image reconstruction network. This framework has achieved a successful joint model for simultaneously learning optimal k-space acquisition and image reconstruction of the raw k-space data. The proposed method was evaluated under Cartesian and Non-Cartesian trajectories to demonstrate the generalization of the method. In several image datasets obtained with different MRI sequences at different anatomical structures, the proposed learning-based k-space optimization can correctly characterize the inherent k-space features. Our method provides an efficient acquisition strategy, meanwhile maintaining high-quality image reconstruction compared to the regular fully sampled Cartesian and Non-Cartesian trajectories, as shown in our results.\\n\\nA few previous studies have focused on k-space optimization under the compressed sensing framework [15, 21, 41, 43]. While most of these methods have shown success in optimizing k-space acquisition on an individual image, it becomes challenging to ensure consistent sampling efficiency and reconstruction performance for a wide variety of image slices. The optimization's performance is also dependent on the assumptions and priors imposed for interpreting the k-space features. Such optimization is primarily heuristic thus could lead to a suboptimal result in the presence of different image features. Recent deep learning-based approaches can be more robust and adaptive to complicated image features. Because the deep learning model can be trained against many image datasets, it learns comprehensive image contents and their corresponding k-space features, leading to more accurate k-space representation.\\n\\nIn our study, we applied a neural ODE to characterize the dynamics of k-space acquisition. Unlike several previous\"}"}
{"id": "CVPR-2022-1026", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Quantitative comparison on brain AXT1POST images at different acceleration levels (Acc. Level).\\n\\n| Acc. Level | Cartesian | Radial | Spiral |\\n|------------|-----------|--------|--------|\\n|            | PSNR      | SSIM   | PSNR   | SSIM   |\\n| 4-Shots    | 29.34 \u00b1 1.63 | 0.81 \u00b1 0.02 | 30.27 \u00b1 0.73 | 0.83 \u00b1 0.02 |\\n|            | 24.77 \u00b1 1.36 | 0.66 \u00b1 0.02 | 26.68 \u00b1 1.09 | 0.72 \u00b1 0.02 |\\n| 8-Shots    | 28.34 \u00b1 1.57 | 0.81 \u00b1 0.03 | 31.00 \u00b1 0.81 | 0.84 \u00b1 0.02 |\\n|            | 26.24 \u00b1 1.36 | 0.70 \u00b1 0.02 | 29.44 \u00b1 1.05 | 0.79 \u00b1 0.01 |\\n| 16-Shots   | 30.12 \u00b1 1.61 | 0.82 \u00b1 0.12 | 31.72 \u00b1 1.31 | 0.85 \u00b1 0.02 |\\n|            | 28.83 \u00b1 1.49 | 0.77 \u00b1 0.03 | 30.78 \u00b1 1.28 | 0.81 \u00b1 0.01 |\\n| 32-Shots   | 30.01 \u00b1 1.38 | 0.81 \u00b1 0.02 | 31.39 \u00b1 0.54 | 0.84 \u00b1 0.01 |\\n|            | 29.87 \u00b1 1.54 | 0.79 \u00b1 0.03 | 31.12 \u00b1 0.96 | 0.87 \u00b1 0.01 |\\n\\ndeep learning-based approaches, this new approach does not assume the k-space data distribution; instead, it relies solely on the learning process to characterize essential k-space features (Figure 3). In addition, neural ODE is a generic framework capable of optimizing any k-space trajectories. In the current study, we only demonstrated the optimization for Cartesian, radial, and spiral trajectories. The future extension could be applied for other trajectories such as echo planner imaging [46], propeller acquisition [34], and other hybrid sampling patterns [30]. Extension to multidimensional data acquisition such as 3D imaging [18] and dynamic imaging [47] is also possible given sufficient training datasets and computing power.\\n\\nIn addition, we demonstrated the influence of physical constraints on the k-space optimization. In contrast to the unconstrained optimization, the k-space optimization conditioned on MRI physics could lead to rapid k-space coverage with maximally allowable traverse patterns, resulting in homogeneous image acquisition as indicated by the PSFs in Figure 2. In the current study, we only considered the factors of maximum gradient amplitude and slew rate. In the future study, a more realistic physical model accounting for eddy current effects [1] could be imposed to estimate more robust trajectory patterns.\\n\\nOur framework not only estimates the optimal k-space acquisition but also simultaneously trains a deep learning-based image reconstruction network using the learned k-space data. Inspired by adversarial learning, the joint k-space optimization and image reconstruction can maximize the optimization performance in a competing manner. The mutual competing nature of training can improve both the k-space optimization network and the reconstruction network collaboratively, leading to a better performance than training each network separately. Undoubtedly, the learned k-space trajectory is subject to the selection of a reconstruction network. Unlike the optimized sampling patterns under compressed sensing, where the trajectories typically follow k-space distribution density, the learned trajectories using neural ODE provide unique k-space characterization that could be more suitable for the jointly learned reconstruction network. In the current study, we only applied U-Net for image reconstruction. Many recently proposed network architectures tailored for MRI reconstruction, such as unrolled networks [6,16], domain transfer learning networks [2], and recurrent neural networks [31], could be potential candidates to further improve the image reconstruction performance in combination with the k-space optimization neural ODE in our framework. In addition, the training loss applied in the current study is solely based on the difference between the fully sampled reference and the estimated image. Future work will explore a more comprehensive loss function to incorporate useful image priors and imaging parameters to create a more robust model.\\n\\nOur proof-of-concept study has several limitations. Due to the GPU memory limit, we must apply an RSS reconstruction to combine multi-coil images into a single image for image reconstruction. However, using multi-coil k-space data could further improve the reconstruction performance through advanced reconstruction networks. To demonstrate the feasibility of our framework, we only investigated retrospective acquisitions of the existing image datasets. The current study is ongoing to evaluate the learned trajectories on real MRI through implementing specific MRI sequences. Finally, the optimized imaging method has not been evaluated for pathology diagnosis in a large patient cohort to illustrate the clinical value.\\n\\n6. Conclusion\\n\\nOur study presented a novel deep learning framework to learn MRI k-space trajectory optimization. We have shown that the proposed method consistently outperforms the regular fixed k-space sampling strategy. The optimization is efficient and adaptable for various Cartesian and Non-Cartesian trajectories at different image sequences, contrast, and anatomies. The proposed method provides a new opportunity for improving rapid MRI, ensuring optimal acquisition while maintaining high-quality image reconstruction.\\n\\nAcknowledgement\\n\\nThis work was supported by the Academy of Finland for ICT 2023 project (grant 328115), Academy Professor project EmotionAI (grants 336116, 345122), by Ministry of Education and Culture of Finland for AI forum project, and Infotech Oulu. As well, the authors wish to acknowledge CSC-IT Center for Science, Finland, for computational resources.\"}"}
{"id": "CVPR-2022-1026", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning Optimal K-space Acquisition and Reconstruction using Physics-Informed Neural Networks\\n\\nWei Peng 1,3, Li Feng 2, Guoying Zhao 1,*, Fang Liu 3,\u2217\\n\\n1CMVS, University of Oulu\\n2Icahn School of Medicine at Mount Sinai\\n3Harvard Medical School\\n\\nAbstract\\n\\nThe inherent slow imaging speed of Magnetic Resonance Image (MRI) has spurred the development of various acceleration methods, typically through heuristically undersampling the MRI measurement domain known as k-space. Recently, deep neural networks have been applied to reconstruct undersampled k-space data and have shown improved reconstruction performance. While most of these methods focus on designing novel reconstruction networks or new training strategies for a given undersampling pattern, e.g., Cartesian undersampling or Non-Cartesian sampling, to date, there is limited research aiming to learn and optimize k-space sampling strategies using deep neural networks. This work proposes a novel optimization framework to learn k-space sampling trajectories by considering it as an Ordinary Differential Equation (ODE) problem that can be solved using neural ODE. In particular, the sampling of k-space data is framed as a dynamic system, in which neural ODE is formulated to approximate the system with additional constraints on MRI physics. In addition, we have also demonstrated that trajectory optimization and image reconstruction can be learned collaboratively for improved imaging efficiency and reconstruction performance. Experiments were conducted on different in-vivo datasets (e.g., brain and knee images) acquired with different sequences. Initial results have shown that our proposed method can generate better image quality in accelerated MRI than conventional undersampling schemes in Cartesian and Non-Cartesian acquisitions.\\n\\n1. Introduction\\n\\nMagnetic Resonance Imaging (MRI) is a powerful clinical tool for disease diagnosis [23]. MRI is non-invasive, radiation-free, and can provide excellent soft-tissue contrast, making it an ideal imaging modality for various neurological, oncological, and musculoskeletal applications [7]. Standard MRI acquisition sequentially collects the k-space data using segmented sampling patterns composed of multiple shots (e.g., spokes, or interleaves) [5, 7]. However, this acquisition scheme typically requires a long scan time, which is challenging for subjects who cannot tolerate long scans due to injuries, pain, discomfort, or claustrophobia. Notably, the long scan also makes data acquisition sensitive to different types of motion, which can cause image degradation reflected as motion artifact [51]. Therefore, accelerated MRI techniques have played an essential role in reducing MRI acquisition time.\\n\\nParallel imaging [14,36,44] and compressed sensing [26, 27, 32] have been extensively investigated for accelerating MRI over the past decades. Parallel imaging uses multi-coil information to estimate the missing k-space data; compressed sensing leverages image sparsity to reconstruct undersampled k-space data using a constrained nonlinear image reconstruction framework. Both techniques can increase imaging efficiency in MRI. Recently, deep learning methods using neural networks have been investigated to reconstruct undersampled k-space data through learning multilevel image representation to remove image artifacts and noises. Various techniques have been proposed using unroll network [6, 16, 42], end-to-end mapping [10, 17, 48], domain transfer learning [2, 55], adversarial learning [28, 37], and unsupervised learning [24]. However, while those methods focus on developing novel reconstruction networks or improving network training strategies, very few studies have investigated the optimization of k-space acquisition for learning-based reconstruction. The k-space undersampling patterns are usually kept the same as those previously used in parallel imaging and compressed sensing.\\n\\nThis study will investigate accelerating MRI by learning k-space sampling to maximize image acquisition efficiency for deep learning-based image reconstruction. The k-space acquisition is formulated as a dynamic optimization process and is solved using a neural Ordinary Differential Equation (ODE) [9]. This ODE system is first initialized with regular Cartesian and Non-Cartesian k-space trajectories, and the trajectories are then dynamically adjusted towards an optimal pattern that provides the best acquisition efficiency. Furthermore, a joint training strategy to optimize both the...\"}"}
{"id": "CVPR-2022-1026", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"neural ODE system and a deep learning reconstruction model is implemented, providing optimal data acquisition and image reconstruction. Experiments were conducted on a publicly available MRI database, fastMRI [52], to evaluate the proposed method. A comparative study showed that the proposed method could improve reconstruction performance for Cartesian and Non-Cartesian acquisition using different MRI image sequences. The contributions of this work are summarized as followings:\\n\\n\u2022 The paper introduces a novel k-space acquisition optimization strategy for accelerated MRI using neural ODE, which has never been presented previously to the best of our knowledge.\\n\u2022 We synergistically combined neural ODE and a deep learning-based reconstruction system to optimize k-space trajectories and a reconstruction model towards maximal acquisition efficiency and optimal reconstruction performance for Cartesian and Non-Cartesian acquisitions.\\n\u2022 The k-space trajectory optimization is conditioned on MRI hardware constraints considering the gradient amplitude and slew rate limits. This creates a physically feasible trajectory for practical MRI acquisition.\\n\u2022 Various experiments demonstrated that the proposed method could significantly improve acquisition efficiency and image quality for Cartesian and Non-Cartesian imaging in different anatomical structures and image contrasts.\\n\\n2. Related Works\\n\\nK-space undersampling is commonly used in accelerated MRI. Uniform undersampling has been the standard acquisition pattern in parallel imaging [14,36,44]. However, conventional parallel imaging methods typically allow for only an acceleration rate of 2-3, and excessive acceleration can lead to severe noise amplification and residual aliasing artifacts. A variable-density random undersampling can create noise-like imaging artifacts and is preferable in modern compressed sensing reconstruction [26,27]. This undersampling strategy typically uses higher sampling density in the low-frequency region of k-space, following the energy distribution in the frequency domain. Although this undersampling scheme achieves image acceleration at multidimensional image reconstruction, it remains heuristic and ignores the intrinsic k-space features, which might provide important information for improving reconstruction quality.\\n\\nA few compressed sensing studies have investigated optimizing k-space acquisition to improve reconstruction performance. Haldar and Kim [15] designed sampling patterns based on constrained Cramer-Rao lower bound with classical experiment design techniques. Sherry et al [43] framed the k-space acquisition as a bilevel optimization problem to simultaneously learn the optimal sampling pattern and regularization parameters in image reconstruction. Sanchez et al [41] learned optimal probability distribution to select a stochastic mask under a given acquisition constraint for dynamic MRI. All those methods are limited to optimize Cartesian sampling under a compressed sensing framework. Recently, Lazarus et al [21] proposed an optimization method for accelerated MRI targeting Cartesian and Non-Cartesian k-space acquisition. Their method uses a gradient descent optimizer to minimize the difference between the optimized k-space sampling distribution and a heuristically determined density.\\n\\nWith the rise of deep learning [22], a few studies have also proposed deep learning-based approaches for optimizing k-space acquisition. Zhang et al [54] proposed leveraging adversarial learning [12] to select k-space phase-encoding lines to reduce pixel-wise uncertainty on reconstructed MR images. Tim et al [4] formulated optimizing k-space as solving a decision-making problem through a policy search [53]. Bahadir et al [3] searched for the optimal sampling patterns by formulating a multivariate Bernoulli distribution on the Cartesian grid through Gumbel-Softmax gradient optimization. Those methods are limited to Cartesian trajectory optimization, and extending them to non-Cartesian imaging could be non-trivial. More recently, Weiss et al [50] proposed using a neural network to learn k-space sampling point locations of Non-Cartesian acquisition. This algorithm is sensitive to selecting initial trajectory points, resulting in a trajectory with a rapidly changing gradient waveform.\\n\\nOur study proposes a new framework to optimize k-space sampling trajectories by considering it as solving Ordinary Differential Equation (ODE) using neural ODE [9]. Neural ODE is a new family of neural networks, which has recently been applied in many applications, including time-series modeling [13], dynamic optimization [9], and image generation [45]. Our study is the first work to use neural ODE for MRI acquisition optimization to the best of our knowledge.\\n\\n3. Methodology\\n\\nAs illustrated in Figure 1, the proposed algorithm consists of two deep learning networks with their associated data processing pathways. First, a neural ODE is introduced to approximate the dynamic evolution of the k-space trajectory from its initial status. With the optimized trajectory, the multi-coil k-space data is undersampled using a non-uniform Fast Fourier Transform (nuFFT). Then, the undersampled data is transferred to image domain. On top of this, a second reconstruction network is trained to remove MR image artifacts and noise. In the following subsections, we provide more details for each component.\"}"}
{"id": "CVPR-2022-1026", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Schematic illustration of the proposed framework. Given a group of initial k-space control points (yellow points on each radial spoke), a neural ODE is used to approximate the dynamics of the trajectory. Then, a nuFFT is employed to transform the k-space data to the image domain, followed by a root-sum-of-squares reconstruction (RSS) to combine multi-channel images and a reconstruction model to remove artifacts, noises and improve the overall quality of the MR image.\\n\\n3.1. Problem Formulation\\n\\nODE mathematically describes variable changes using integrals and derivatives, which can model a dynamic system. Neural ODE \\\\[9, 29\\\\] uses a neural network to parameterize the dynamics of the variable. Neural ODE has recently shown impressive performance in sequential data optimization for regular and irregular time sampling \\\\[19\\\\], making it an ideal new approach for optimizing MRI k-space trajectory. In MRI, a k-space trajectory can be described as \\\\(k(t)\\\\). Because the k-space trajectory is typically differentiable, the description of the trajectory dynamics can be formulated as an ODE as\\n\\n\\\\[\\n\\\\frac{dk(t)}{dt} = f_{\\\\theta}(k(t), t),\\n\\\\]\\n\\nwhere \\\\(f_{\\\\theta}\\\\) is a neural network with parameters \\\\(\\\\theta\\\\). This linear ODE can be simply solved using integration as\\n\\n\\\\[\\nk(t) = k(0) + \\\\int_{t_0}^{t} f_{\\\\theta}(k(\\\\tau), \\\\tau) d\\\\tau,\\n\\\\]\\n\\nwhere \\\\(k(0)\\\\) is an initial trajectory point that can be provided using either Cartesian or Non-Cartesian trajectories (e.g., radial and spiral). We can use ODEsolver() to represent an ODE solver that takes the inputs of neural network mapping function \\\\(f_{\\\\theta}\\\\), initial state (initial k-space control points) \\\\(k(0)\\\\), and time sequence \\\\(t\\\\), and then outputs the predicted k-space trajectory \\\\(k(t) = \\\\text{ODEsolver}(f_{\\\\theta}, k(0), t)\\\\).\\n\\nSuppose \\\\(\\\\hat{k}\\\\) is an optimal k-space trajectory that provides the most efficient k-space acquisition. The optimization can then be framed as\\n\\n\\\\[\\n\\\\arg\\\\min_{\\\\theta} \\\\ell(k(t), \\\\hat{k}) = \\\\arg\\\\min_{\\\\theta} \\\\ell(\\\\text{ODEsolver}(f_{\\\\theta}, k(0), t), \\\\hat{k}),\\n\\\\]\\n\\nwhere the loss function \\\\(\\\\ell(k(t), \\\\hat{k})\\\\) minimizes the difference between predicted trajectory \\\\(k(t)\\\\) and \\\\(\\\\hat{k}\\\\) to update \\\\(f_{\\\\theta}\\\\). In addition, a physically applicable trajectory must obey the hardware constraints of an MRI system, including the maximum imaging gradient \\\\(G_{\\\\text{max}}\\\\), determined by the peak-current, and the maximum gradient slew rate \\\\(S_{\\\\text{max}}\\\\), describing the maximum change of gradient in a unit time. The hardware constraints on the maximum gradient and slew rate determine the trajectory speed and acceleration, which can be defined as\\n\\n\\\\[\\nv = \\\\frac{dk(t)}{dt}, \\\\quad a = \\\\frac{d^2k(t)}{dt^2},\\n\\\\]\\n\\nWith these definitions, the k-space trajectory optimization conditioned on MRI physics can be reframed as\\n\\n\\\\[\\n\\\\arg\\\\min_{\\\\theta} \\\\ell(\\\\text{ODEsolver}(f_{\\\\theta}, k(0), t), \\\\hat{k}), \\\\quad \\\\text{subject to } v \\\\leq \\\\gamma \\\\ast G_{\\\\text{max}}, \\\\quad a \\\\leq \\\\gamma \\\\ast S_{\\\\text{max}}.\\n\\\\]\\n\\nwhere the \\\\(\\\\gamma\\\\) is the gyromagnetic ratio for a specific nucleus (e.g., proton). To simplify the expression for \\\\(\\\\ell(k(t), \\\\hat{k})\\\\), we use \\\\(\\\\ell(k(t))\\\\) in the following manuscript.\\n\\n3.2. Neural ODE Solver\\n\\nThe integration operation in the ODE model makes it difficult to efficiently update \\\\(f_{\\\\theta}\\\\) using the standard backpropagation approach. Computing the gradients for all k-space points in the trajectory during the backpropagation will lead to a gigantic computational graph, which cannot be stored in the memory of typical computing hardware. For example, the loss for a k-space point at the end of the trajectory\"}"}
{"id": "CVPR-2022-1026", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"gation, it can be easily combined with other gradient-based\\nis a continuous-time analogy to the traditional backpropa-\\nprecise numerical error control. Besides, as this approach\\nthe adjoint method has several advantages: linear scalability\\nCompared with the traditional backpropagation approach,\\n\\\\( \\\\theta \\\\)\\ncompute\\n\\\\( a \\\\)\\nstate\\n\\\\( \\\\epsilon \\\\)\\nwhere\\n\\\\( t \\\\)\\nand\\n\\\\( a \\\\)\\ncompute the adjoint state\\npoint in the trajectory. For example, the adjoint method will\\nbe implemented using an integration operation for any time\\n\\\\[ \\\\text{dynamics of} \\\\quad \\\\text{ODE model in Eq. (6). Unlike the traditional back prop-} \\\\]\\n\\\\( \\\\eta \\\\)\\nTo back propagate the gradients from\\n\\\\( t \\\\)\\n\\\\[ \\\\text{using traditional back propagation approach [40], and} \\\\quad e \\\\]\\n\\\\[ \\\\text{the values of} \\\\quad t \\\\quad \\\\text{in the entire trajectory needs to be computed sep-} \\\\]\\n\\\\( \\\\text{the adjoint state} \\\\quad a \\\\quad \\\\text{is defined as,} \\\\quad a \\\\]\\n\\\\[ \\\\text{Likewise, similar to Eq. (10), the dynamics of the adjoint} \\\\quad t \\\\]\\n\\\\[ \\\\text{can be calculated as} \\\\quad a \\\\quad \\\\text{and} \\\\quad a \\\\quad \\\\text{can be computed as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{can be also denoted as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{the adjoint state denoted as} \\\\quad a \\\\quad \\\\text{of the augmented state} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad \\\\text{as} \\\\quad a \\\\quad"}
{"id": "CVPR-2022-1026", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] CB Ahn and ZH Cho. Analysis of eddy currents in nuclear magnetic resonance imaging. *Magnetic resonance in medicine*, 17(1):149\u2013163, 1991.\\n\\n[2] Mehmet Ak\u00e7akaya, Steen M\u00f8ller, Sebastian Weingartner, and Kamil Ugurbil. Scan-specific robust artificial-neural-networks for k-space interpolation (raki) reconstruction: Database-free deep learning for fast imaging. *Magnetic resonance in medicine*, 81(1):439\u2013453, 2019.\\n\\n[3] Cagla D Bahadir, Alan Q Wang, Adrian V Dalca, and Mert R Sabuncu. Deep-learning-based optimization of the undersampling pattern in MRI. *IEEE Transactions on Computational Imaging*, 6:1139\u20131152, 2020.\\n\\n[4] Tim Bakker, Herke van Hoof, and Max Welling. Experimental design for MRI by greedy policy search. *Advances in Neural Information Processing Systems*, 33, 2020.\\n\\n[5] Matt A Bernstein, Kevin F King, and Xiaohong Joe Zhou. *Handbook of MRI pulse sequences*. Elsevier, 2004.\\n\\n[6] Sampurna Biswas, Hemant K Aggarwal, and Mathews Jacob. Dynamic MRI using model-based deep learning and STORM priors: MODL-STORM. *Magnetic resonance in medicine*, 82(1):485\u2013494, 2019.\\n\\n[7] James R Brookeman. *MRI from picture to proton*, 2004.\\n\\n[8] M Calvo, JI Montijano, and L Randez. A fifth-order interpolant for the Dormand and Prince Runge-Kutta method. *Journal of computational and applied mathematics*, 29(1):91\u2013100, 1990.\\n\\n[9] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. *Advances in Neural Information Processing Systems*, 2018.\\n\\n[10] Taejoon Eo, Yohan Jun, Taeseong Kim, Jinseong Jang, Ho-Joon Lee, and Dosik Hwang. Kiki-net: cross-domain convolutional neural networks for reconstructing undersampled magnetic resonance images. *Magnetic resonance in medicine*, 80(5):2188\u20132201, 2018.\\n\\n[11] Jeffrey A Fessler. On nufft-based gridding for non-cartesian MRI. *Journal of magnetic resonance*, 188(2):191\u2013195, 2007.\\n\\n[12] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. *Advances in neural information processing systems*, 2014.\\n\\n[13] Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-form continuous dynamics for scalable reversible generative models. *ICLR*, 2019.\\n\\n[14] Mark A Griswold, Peter M Jakob, Robin M Heidemann, Mathias Nittka, Vladimir Jellus, Jianmin Wang, Berthold Kiefer, and Axel Haase. Generalized autocalibrating partially parallel acquisitions (GRAPPA). *Magnetic Resonance in Medicine*, 47(6):1202\u20131210, 2002.\\n\\n[15] Justin P Haldar and Daeun Kim. Oedipus: An experiment design framework for sparsity-constrained MRI. *IEEE transactions on medical imaging*, 38(7):1545\u20131558, 2019.\\n\\n[16] Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P Recht, Daniel K Sodickson, Thomas Pock, and Florian Knoll. Learning a variational network for reconstruction of accelerated MRI data. *Magnetic resonance in medicine*, 79(6):3055\u20133071, 2018.\\n\\n[17] Yoseob Han, Jaejun Yoo, Hak Hee Kim, Hee Jung Shin, Kyunghyun Sung, and Jong Chul Ye. Deep learning with domain adaptation for accelerated projection-reconstruction MR. *Magnetic resonance in medicine*, 80(3):1189\u20131205, 2018.\\n\\n[18] Kevin M Johnson. Hybrid radial-cones trajectory for accelerated MRI. *Magnetic resonance in medicine*, 77(3):1068\u20131081, 2017.\\n\\n[19] Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations for irregular time series. *Advances in Neural Information Processing Systems*, 2020.\\n\\n[20] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. *ICLR*, 2015.\\n\\n[21] Carole Lazarus, Pierre Weiss, Nicolas Chauffert, Franck Mauconduit, Loubna El Gueddari, Christophe Destrieux, Ilyess Zemmoura, Alexandre Vignaud, and Philippe Ciuciu. Sparkling: variable-density K-space filling curves for accelerated T2*-weighted MRI. *Magnetic resonance in medicine*, 81(6):3643\u20133661, 2019.\\n\\n[22] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. *Nature*, 521(7553):436\u2013444, 2015.\\n\\n[23] Debiao Li, Andrew C Larson, Oliver Speck, Axel Schreiber, Clemens Janz, J\u00fcrgen Hennig, Michael E Moseley, Roland Bammer, Joachim Rother, Petra Schmalbrock, et al. Modern applications of MRI in medical sciences. *Magnetism in Medicine: A Handbook*, pages 343\u2013476, 2006.\\n\\n[24] Fang Liu, Richard Kijowski, Georges El Fakhri, and Li Feng. Magnetic resonance parameter mapping using model-guided self-supervised deep learning. *Magnetic Resonance in Medicine*, 85(6):3211\u20133226, 2021.\\n\\n[25] Xiankai Lu, Chao Ma, Bingbing Ni, Xiaokang Yang, Ian Reid, and Ming-Hsuan Yang. Deep regression tracking with shrinkage loss. In *ECCV*, pages 353\u2013369, 2018.\\n\\n[26] Michael Lustig, David Donoho, and John M Pauly. Sparse MRI: The application of compressed sensing for rapid MR imaging. *Magnetic Resonance in Medicine*, 58(6):1182\u20131195, 2007.\\n\\n[27] Michael Lustig and John M Pauly. SPIRIT: Iterative self-consistent parallel imaging reconstruction from arbitrary K-space. *Magnetic resonance in medicine*, 64(2):457\u2013471, 2010.\\n\\n[28] Morteza Mardani, Enhao Gong, Joseph Y Cheng, Shreyas S Vasanawala, Greg Zaharchuk, Lei Xing, and John M Pauly. Deep generative adversarial neural networks for compressive sensing MRI. *IEEE transactions on medical imaging*, 38(1):167\u2013179, 2018.\\n\\n[29] Antoine McNamara, Adrien Treuille, Zoran Popovi\u0107, and Jos Stam. Fluid control using the adjoint method. *ACM Transactions on Graphics (TOG)*, 23(3):449\u2013456, 2004.\\n\\n[30] Douglas C Noll. Multishot rosette trajectories for spectrally selective MR imaging. *IEEE transactions on medical imaging*, 16(4):372\u2013377, 1997.\"}"}
{"id": "CVPR-2022-1026", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Changheun Oh, Dongchan Kim, Jun-Young Chung, Yeji Han, and HyunWook Park. A k-space-to-image reconstruction network for MRI using recurrent neural network. Medical Physics, 48(1):193\u2013203, 2021.\\n\\nRicardo Otazo, Daniel Kim, Leon Axel, and Daniel K Sodickson. Combination of compressed sensing and parallel imaging for highly accelerated first-pass cardiac perfusion MRI. Magnetic Resonance in Medicine, 64(3):767\u2013776, 2010.\\n\\nAdam Paszke, S. Gross, Francisco Massa, A. Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Z. Lin, N. Gimelshein, L. Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, B. Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019.\\n\\nJames G Pipe. Motion correction with propeller MRI: application to head motion and free-breathing cardiac imaging. Magnetic Resonance in Medicine, 42(5):963\u2013969, 1999.\\n\\nDaniel Potts, Gabriele Steidl, and Manfred Tasche. Fast Fourier transforms for nonequispaced data: A tutorial. In Modern Sampling Theory, pages 247\u2013270, 2001.\\n\\nKlaas P Pruessmann, Markus Weiger, Markus B Scheidegger, and Peter Boesiger. Sense: sensitivity encoding for fast MRI. Magnetic Resonance in Medicine, 42(5):952\u2013962, 1999.\\n\\nTran Minh Quan, Thanh Nguyen-Duc, and Won-Ki Jeong. Compressed sensing MRI reconstruction using a generative adversarial network with a cyclic loss. IEEE Transactions on Medical Imaging, 37(6):1488\u20131497, 2018.\\n\\nPeter B Roemer, William A Edelstein, Cecil E Hayes, Steven P Souza, and Otward M Mueller. The NMR phased array. Magnetic Resonance in Medicine, 16(2):192\u2013225, 1990.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In MICCAI, pages 234\u2013241. Springer, 2015.\\n\\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. Nature, 323(6088):533\u2013536, 1986.\\n\\nThomas Sanchez, Baran Gozcuz, Ruud B van Heeswijk, Armin Eftekhari, Efe Il\u0131cak, Tolga \u00c7ukur, and Volkan Cevher. Scalable learning-based sampling optimization for compressive dynamic MRI. In ICASSP, pages 8584\u20138588. IEEE, 2020.\\n\\nJo Schlemper, Jose Caballero, Joseph V Hajnal, Anthony N Price, and Daniel Rueckert. A deep cascade of convolutional neural networks for dynamic MR image reconstruction. IEEE Transactions on Medical Imaging, 37(2):491\u2013503, 2017.\\n\\nFerdia Sherry, Martin Benning, Juan Carlos De los Reyes, Martin J Graves, Georg Maierhofer, Guy Williams, Carola-Bibiane Sch\u00f6nlieb, and Matthias J Ehrhardt. Learning the sampling pattern for MRI. IEEE Transactions on Medical Imaging, 39(12):4310\u20134321, 2020.\\n\\nDaniel K Sodickson and Warren J Manning. Simultaneous acquisition of spatial harmonics (SMASH): fast imaging with radiofrequency coil arrays. Magnetic Resonance in Medicine, 38(4):591\u2013603, 1997.\\n\\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. ICLR, 2021.\\n\\nMichael K Stehling, Robert Turner, and Peter Mansfield. Echo-planar imaging: magnetic resonance imaging in a fraction of a second. Science, 254(5028):43\u201350, 1991.\\n\\nJonathan I Tamir, Martin Uecker, Weitian Chen, Peng Lai, Marcus T Alley, Shreyas S Vasanawala, and Michael Lustig. T2 shuffling: sharp, multicontrast, volumetric fast spin-echo imaging. Magnetic Resonance in Medicine, 77(1):180\u2013195, 2017.\\n\\nShanshan Wang, Zhenghang Su, Leslie Ying, Xi Peng, Shun Zhu, Feng Liang, Dagan Feng, and Dong Liang. Accelerating magnetic resonance imaging via deep learning. In 2016 IEEE 13th International Symposium on Biological Imaging (ISBI), pages 514\u2013517. IEEE, 2016.\\n\\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, 2004.\\n\\nTomer Weiss, Ortal Senouf, Sanketh Vedula, Oleg Michailovich, Michael Zibulevsky, and Alex Bronstein. Pilot: Physics-informed learned optimal trajectories for accelerated MRI. arXiv preprint arXiv:1909.05773, 2019.\\n\\nMaxim Zaitsev, Julian Maclaren, and Michael Herbst. Motion artifacts in MRI: A complex problem with many partial solutions. Journal of Magnetic Resonance Imaging, 42(4):887\u2013901, 2015.\\n\\nJure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J Muckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, et al. fastMRI: An open dataset and benchmarks for accelerated MRI. arXiv preprint arXiv:1811.08839, 2018.\\n\\nDavid Y Zeng, Christopher M Sandino, Dwight G Nishimura, Shreyas S Vasanawala, and Joseph Y Cheng. Reinforcement learning for online undersampling pattern optimization. In ISMRM 27th Annual Meeting and Exhibition, 2019.\\n\\nZizhao Zhang, Adriana Romero, Matthew J Muckley, Pascal Vincent, Lin Yang, and Michal Drozdzal. Reducing uncertainty in undersampled MRI reconstruction with active acquisition. In CVPR, pages 2049\u20132058, 2019.\\n\\nBo Zhu, Jeremiah Z Liu, Stephen F Cauley, Bruce R Rosen, and Matthew S Rosen. Image reconstruction by domain-transform manifold learning. Nature, 555(7697):487\u2013492, 2018.\"}"}
