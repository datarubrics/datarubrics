{"id": "CVPR-2023-1210", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"section over union (mIoU) as the metric to evaluate results. Since the annotations of the test set on ScanNetv2 [8] are not available, we conducted the runtime complexity experiments and ablation experiments on the validation set.\\n\\nS3DIS [1] is a large-scene semantic parsing benchmark, containing 6 areas with 271 rooms, annotated with 13 categories. Following [7, 16, 63, 71], we use Area 5 as the test set, other areas for training and mIoU for evaluation. We also report overall point-wise accuracy (OA) and mean class accuracy (mAcc).\\n\\nMatterport3D [4] is a large-scene RGB-D dataset, with 90 building-scale scenes annotated in 21 categories. Since the semantic labels are annotated for faces, we project them to the vertices using the same method as in [53], and then test on scene vertices directly. Following [29, 53], we use mAcc for evaluation.\\n\\n4.2. Implementation details\\n\\nOur experiments were all conducted on the full scenes without cropping. During training and inferencing, we just used vertex colors as input. Following [7, 29], we voxelized the input point cloud at a resolution of 2 cm for ScanNet v2. Following common practice, the input points of S3DIS and Matterport3D are voxelized at a resolution of 5 cm. During inferencing, we calculated metrics by projecting the predictions back to the raw point clouds using the nearest neighbor.\\n\\nDuring training, we performed scaling, z-axis rotation, translation and chromatic jitter for data augmentation [29]. For all datasets, we minimized the cross entropy loss using the SGD optimizer with a poly scheduler decaying from a learning rate of 0.1, following [5, 7, 29]. Following previous work [30, 53], we used class weights for cross entropy loss on Matterport3D. For ScanNet v2 and Matterport3D, we trained our model for 500 epochs with batch size set to 8, and for S3DIS, we trained for 1,000 epochs with batch size set to 4.\\n\\nOur experiments were conducted by using Jittor [26] and Pytorch [48]. All runtime complexity experiments are conducted on one RTX 3090 GPU.\\n\\n4.3. Comparison to other methods\\n\\nWe compared our model to other point-based and voxel-based state-of-the-art methods on ScanNet v2, S3DIS and Matterport3D, with results shown in Tables 1\u20133, respectively. For these datasets, our method gave the best overall results compared CNN-based methods. In particular, our method surpasses both the popular state-of-the-art MinkowskiNet [7], and the voxel and mesh fusion algorithm VMNet [29]. Our method even outperforms transformer-based methods, which learn the long range context at a cost of heavy computation, on ScanNet v2.\\n\\nIn addition, we also compared the number of network\\n\\n| Method       | Input | Val Test | OA  | mAcc | mIoU |\\n|--------------|-------|----------|-----|------|------|\\n| PointNet++   | point |          | 53.5| 55.7 |      |\\n| 3DMV         | point | -        | 48.4|      |      |\\n| PointCNN     | point | -        | 45.8|      |      |\\n| PointConv    | point |          | 61.0| 66.6 |      |\\n| JointPointBased | point |          | 69.2| 63.4 |      |\\n| PointASNL    | point |          | 63.5| 66.6 |      |\\n| RandLA-Net   | point | -        | 64.5|      |      |\\n| KPConv       | point |          | 69.2| 68.6 |      |\\n| PointTransformer | point | -        | 70.6|      |      |\\n| SparseConvNet| voxel |          | 69.3| 72.5 |      |\\n| MinkowskiNet | voxel | -        | 72.2| 73.6 |      |\\n| LargeKernel3D| voxel |          | 73.2| 73.9 |      |\\n| Fast Point Transformer | voxel | -        | 72.1|      |      |\\n| Stratified Transformer | point |          | 74.3| 73.7 |      |\\n| LRPNet (ours)| voxel |          | 75.0| 74.2 |      |\\n\\n| Method       | Input | OA  | mAcc | mIoU |\\n|--------------|-------|-----|------|------|\\n| PointNet     | point | -   | 49.0 | 41.1 |\\n| SegCloud     | point | -   | 57.4 | 48.9 |\\n| TangentConv  | point | -   | 62.2 | 52.6 |\\n| PointCNN     | point | 85.9| 63.9 | 57.3 |\\n| HPEIN        | point | 87.2| 68.3 | 61.9 |\\n| GACNet       | point |     | 87.8 | 62.9 |\\n| PAT          | point | 70.8| 60.1 |      |\\n| ParamConv    | point | 67.0| 58.3 |      |\\n| SPGraph      | point | 86.4| 66.5 | 58.0 |\\n| PCT          | point | 67.7| 61.3 |      |\\n| SegGCN       | point | 88.2| 70.4 | 63.6 |\\n| PAConv       | point |     | 66.6 |      |\\n| KPConv       | point | 72.8| 67.1 |      |\\n| MinkowskiNet | voxel | -   | 71.7 | 65.4 |\\n| Fast Point Transformer | voxel | -   | 77.3| 70.1 |\\n| PointTransformer | point | 90.8| 76.5 | 70.4 |\\n| Stratified Transformer | point | 91.5| 78.1 | 72.0 |\\n| LRPNet (ours)| voxel | 90.8| 74.9 | 69.1 |\\n\\nTable 1. mIoU (%) scores for various methods on the ScanNet v2 3D semantic benchmark, for validation and test sets. The best number is in boldface. \\\"-\\\" means the number is unavailable.\\n\\nTable 2. Several scores (%) for various methods on the S3DIS segmentation benchmark. The best number is in boldface. \\\"-\\\" means the number is unavailable.\"}"}
{"id": "CVPR-2023-1210", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Mean class accuracy (%) scores on the Matterport3D test set. The best number is in boldface.\\n\\n| Method            | Params (M) | Runtime (ms) | mAcc   |\\n|-------------------|------------|--------------|--------|\\n| SplatNet          | 26.7       | 90.8         | 95.7   |\\n| PointNet++        | 43.8       | 80.1         | 81.3   |\\n| ScanComplete      | 44.9       | 79.0         | 95.9   |\\n| TangentConv       | 46.8       | 56.0         | 87.7   |\\n| 3DMV              | 56.1       | 79.6         | 95.5   |\\n| TextureNet        | 63.0       | 63.6         | 91.3   |\\n| DCM-Net           | 66.2       | 78.4         | 93.6   |\\n| VMNet             | 67.2       | 85.9         | 94.4   |\\n| LRPNet (Ours)     | 70.7       | 83.7         | 95.0   |\\n\\nTable 4. Number of network parameters and speed for various models on the ScanNet v2 validation set.\\n\\n- We compared the qualitative results on the ScanNet v2 validation set, showing LRPNet's segmentations are closer to the ground truth than the baseline's segmentations.\\n- Our method still works well even in hard cases, such as the table and the cabinet in the corner in the first and four rows.\\n- LRPNet is more capable of segmenting out broken objects, as seen in the second and third rows of bookshelves and walls.\\n\\n4.4. Ablation study\\n\\n- In section 3.2, we used dilation max pooling to expand the effective receptive field and a selection module to allow each voxel to select receptive fields based on features.\\n- We conducted ablation experiments on the validation set of ScanNet v2 to verify the design of the LRP module.\\n\\n| Baseline | MaxPool | Dilation | Selection | Params (M) | Runtime (ms) | mIoU   |\\n|----------|---------|----------|-----------|------------|--------------|--------|\\n|          | \u2713       |          |           | 8.1        | 38.1         | 71.2   |\\n|          | \u2713 \u2713     |          |           | 8.1        | 74.2         | 72.6   |\\n|          | \u2713 \u2713 \u2713   |          |           | 8.1        | 65.0         | 73.7   |\\n|          | \u2713 \u2713 \u2713   |          |           | 8.5        | 76.4         | 73.1   |\\n|          | \u2713 \u2713 \u2713 \u2713  |          |           | 8.5        | 67.9         | 75.0   |\\n\\nSpecifically, we chose the sparse convolutional U-Net implemented by VMNet [29] as the baseline, and designed a new network using max pooling without any dilation. This new network was further enhanced with dilation or the receptive field selection module.\\n\\n- As shown in Table 5, max pooling can enlarge the receptive field to improve the results, and dilation max pooling can achieve a larger receptive field and a better result. The selection module can enhance the capability of the network with only introducing a few parameters.\\n\\n4.4.1 Position of LRP\\n\\n- In section 3.3, we added the LRP module after each stage of the baseline (VMNet [29]), which we name it (After).\\n- Actually, we may also put the LRP module before the stage (Before), in the middle of the stage (Middle), or as an additional path parallel to the stage (Parallel).\\n\\nTable 6 shows the ablation of different settings. It shows the LRP module works better when added after each stage than in other positions, also being more computationally friendly.\\n\\n4.4.2 Operations with non-linearity\\n\\n- As we mentioned in section 3.2, max pooling has more non-linearity than average pooling and convolution, and the operations with non-linearity enhance the results.\\n\\nSpecifically, we choose the sparse convolutional U-Net implemented by VMNet [29] as the baseline, and design a new network using max pooling without any dilation. This new network is further enhanced with dilation or the receptive field selection module.\\n\\n- As shown in Table 5, max pooling can enlarge the receptive field to improve the results, and dilation max pooling can achieve a larger receptive field and a better result. The selection module can enhance the capability of the network with only introducing a few parameters.\\n\\n4.4.1 Position of LRP\\n\\n- In section 3.3, we added the LRP module after each stage of the baseline (VMNet [29]), which we name it (After).\\n- Actually, we may also put the LRP module before the stage (Before), in the middle of the stage (Middle), or as an additional path parallel to the stage (Parallel).\\n\\nTable 6 shows the ablation of different settings. It shows the LRP module works better when added after each stage than in other positions, also being more computationally friendly.\\n\\n4.4.2 Operations with non-linearity\\n\\n- As we mentioned in section 3.2, max pooling has more non-linearity than average pooling and convolution, and the operations with non-linearity enhance the results.\"}"}
{"id": "CVPR-2023-1210", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Various ScanNet v2 validation set segmentation results. Red dotted boxes highlight differences between our results and the baseline results.\\n\\n| Position | Params (M) | Runtime (ms) | mIoU (%) |\\n|----------|------------|--------------|----------|\\n| Before   | 8.8        | 77.1         | 74.0     |\\n| Middle   | 8.5        | 69.6         | 73.4     |\\n| Parallel | 8.5        | 68.4         | 73.1     |\\n| After (Ours) | 8.5 | 67.9         | 75.0     |\\n\\nTable 6. LRP position study.\\n\\n- **Before**: LRP added before the stages.\\n- **After**: LRP added after the stages.\\n- **Middle**: LRP added at the middle of stages.\\n- **Parallel**: LRP as an additional branch parallel to the stage.\\n\\nErrors with non-linearity will enhance the capability of the network. To investigate the impacts of non-linearity for feature aggregation, we conducted non-linearity experiments by replacing the max pooling ($\\\\text{MaxPool}$) of LRP with convolution ($\\\\text{Conv}$) or average pooling ($\\\\text{AvgPool}$). In addition, we tested the effect of different receptive fields with these operations (MaxPool, AvgPool, Conv). In Table 7, $\\\\times N \\\\times N \\\\times N$ is the size of the receptive field of $N \\\\times N \\\\times N$ and $\\\\left[\\\\times 3, \\\\times 9 \\\\times 9, \\\\times 27\\\\right]$ means the outputs of the LRP module is selected from the features with the receptive field of $3 \\\\times 3 \\\\times 3$, $9 \\\\times 9 \\\\times 9$, and $27 \\\\times 27 \\\\times 27$.\\n\\nAs Table 7 shows, although Conv introduces a large number of parameters, MaxPool can still exceed AvgPool and Conv in most cases, which proves that the non-linearity of max pooling will enhance the networks for 3D segmentation. We have further explored the effect of non-linearity functions followed by a convolution or an average pooling in the supplementary materials.\\n\\n4.4.3 Range of LRP module\\n\\nAs described in section 3, while large receptive fields can improve network results, we used dilation max pooling to achieve large receptive fields and a selection module to give the network the ability to select the receptive field size according to the voxel feature.\"}"}
{"id": "CVPR-2023-1210", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7 shows that as we increase the levels of the receptive field of the LRP module, whether using convolution, max pooling or average pooling, the accuracy of results is gradually improved. We also used different ranges of the receptive fields with the same number of levels. See Table 7 lines 1\u20133: we fixed the number of receptive fields of LRP module to one level, and then compared results for \\\\([\\\\times 3]\\\\), \\\\([\\\\times 9]\\\\), and \\\\([\\\\times 27]\\\\) receptive fields. With the increasing receptive field for the LRP module, the model also clearly improved.\\n\\nIn addition, we visualized the Effective Receptive Field [45] of different methods at two positions. As shown in Figure 5, the features of interest are in the center of the table and the center of the chair. LRPNet has a larger receptive field than baseline, which helps to segment the table better. Columns 3,4 and 6 in Figure 5 show that the operations with linearity can not capture the long range context and their responses are mostly concentrated around the points of interest, while max pooling with non-linearity can make good use of the long range features. Furthermore, as shown in the last two columns of Figure 5, the selection module can give the network the ability to select the appropriate receptive field according to the voxel features.\\n\\n5. Conclusion\\nWe have proposed a simple yet efficient module, the long range pooling module, which can provide an adaptive large receptive field and improve the modeling capacity of the network. It is easy to add to any existing networks; we use it to construct LRPNet, which achieves state-of-the-art results for large-scene segmentation with almost no increase in the number of parameters. Since the LRP module is simple and easy to use, we hope to apply it to 3D object detection, 3D instance segmentation and 2D image processing, further exploring the principle of improving network performance by use of a large receptive field and operations with greater non-linearity in the future.\"}"}
{"id": "CVPR-2023-1210", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Long Range Pooling for 3D Large-Scale Scene Understanding\\n\\nXiang-Li Li\\nMeng-Hao Guo\\nTai-Jiang Mu\\nRalph R. Martin\\nShi-Min Hu\\n\\nBNRist, Tsinghua University\\nCardiff University\\n\\nlixl19@mails.tsinghua.edu.cn, gmh20@mails.tsinghua.edu.cn\\ntaijiang@tsinghua.edu.cn, martinrr@cardiff.ac.uk, shimin@tsinghua.edu.cn\\n\\nAbstract\\n\\nInspired by the success of recent vision transformers and large kernel design in convolutional neural networks (CNNs), in this paper, we analyze and explore essential reasons for their success. We claim two factors that are critical for 3D large-scale scene understanding: a larger receptive field and operations with greater non-linearity. The former is responsible for providing long range contexts and the latter can enhance the capacity of the network. To achieve the above properties, we propose a simple yet effective long range pooling (LRP) module using dilation max pooling, which provides a network with a large adaptive receptive field. LRP has few parameters, and can be readily added to current CNNs. Also, based on LRP, we present an entire network architecture, LRPNet, for 3D understanding. Ablation studies are presented to support our claims, and show that the LRP module achieves better results than large kernel convolution yet with reduced computation, due to its non-linearity. We also demonstrate the superiority of LRPNet on various benchmarks: LRPNet performs the best on ScanNet and surpasses other CNN-based methods on S3DIS and Matterport3D. Code will be available at https://github.com/li-xl/LRPNet.\\n\\n1. Introduction\\n\\nWith the rapid development of 3D sensors, more and more 3D data is becoming available from a variety of applications such as autonomous driving, robotics, and augmented/virtual reality. Analyzing and understanding this 3D data has become essential, which requires efficient and effective processing. Various data structures, including point clouds, meshes, multi-view images, voxels, etc, have been proposed for representing 3D data [73]. Unlike 2D image data, a point cloud or mesh, is irregular and unordered. These characteristics mean that typical CNNs cannot directly be applied to such 3D data. Thus, specially designed networks such as MLPs [49], CNNs [27, 40, 69], GNNs [53, 68] and transformers [16, 46, 79] are proposed to perform effective deep learning on them. However, processing large-scale point cloud or mesh is computationally expensive. Multi-view images contain multiple different views of a scene. Typically, CNNs are used to process each view independently and a fusion module is used to combine the results. Nevertheless, there is unavoidable 3D information loss due to the finite or insufficient number of views. Voxel data has the benefit of regularity like images, even if the on-surface voxels only contain sparse data. The simplicity of the structure helps to maintain high performance, so this paper focuses on processing voxel data.\\n\\nLearning on 3D voxels can be easily implemented by directly extending the well studied 2D CNN networks to 3D [52, 66, 72]. Considering that 3D voxel data is inherently sparse, some works usually adopt specially designed 3D sparse CNNs [7, 15, 56] for large-scale scene understanding. Since only the surface of the real scene data has values, the neighbors around each voxel do not necessarily belong to the same object, so a large receptive field is more...\"}"}
{"id": "CVPR-2023-1210", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"conducive to feature extraction. Sparse convolution has advantages in modeling local structures, but it ignores long range contexts, which are critical for 3D scene understanding tasks. Transformers [16, 35, 70, 71] have proved useful in processing 3D scenes, as they can capture the global relationship with their larger receptive field and a better way to interact. However, they usually have a quadratic computational complexity, which brings a heavy computing cost.\\n\\nA straightforward way to incorporate the long range contexts into the learning of 3D voxel data is to exploit a large kernel 3D convolution. However, the number of network parameters and the amount of computation would increase cubically due to the additional dimension compared to 2D images. Besides, current ways of feature interaction or aggregation, such as average pooling and convolution, usually adopt a linear combination of features of all the locations in the receptive field. This works for 2D images since all the locations in the receptive field have valid values, which, however, does not hold for 3D voxels due to the sparsity nature of 3D scenes. Directly applying such linear interaction or aggregation on the voxel that has few neighbors in the receptive field would make the feature of that voxel too small or over-smoothed and thus less informative.\\n\\nTaking the above into consideration, and to achieve a trade-off between the quality of results and computation, we propose long range pooling (LRP), a simple yet effective module for 3D scene segmentation. Compared with previous sparse convolution networks [7, 15], LRP is capable of increasing the effective receptive field with a negligible amount of computational overhead. Specifically, we achieve a large receptive field by proposing a novel dilation max pooling to enhance the non-linearity of the neural network. We further add a receptive field selection module, so that each voxel can choose a suitable receptive field, which is adaptive to the distribution of voxels. The above two components comprise the LRP module. Unlike dilation convolution, which is often used to enlarge the receptive field for 2D images [18, 19], our method can achieve a large receptive field with fewer parameters and computation by using dilation max pooling. Furthermore, LRP is a simple, and efficient module that can be readily incorporated into other networks. We construct a more capable neural network, LRPNet [108], by adding LRP at the end of each stage of the sparse convolution network, introduced by VMNet [29].\\n\\nExperimental results show that LRPNet achieves a significant improvement in 3D segmentation accuracy on large-scale scene datasets, including ScanNet [8], S3DIS [1] and Matterport3D [4]. Qualitative results are illustrated in Figure 1, which shows the improvement of a larger receptive field. We also experimentally compare the effects of the different receptive fields by reducing the number of dilation max pooling of LRP. Moreover, we explore the influence of non-linearity of LRP module by replacing max pooling with average pooling or convolution. Ablation results show that a larger receptive field and operations with greater non-linearity for feature aggregation will improve the segmentation accuracy.\\n\\nOur contributions are thus:\\n\\n- a simple and effective module, the long range pooling (LRP) module, which provides a network with a large adaptive receptive field without a large number of parameters,\\n- a demonstration that a larger receptive field and aggregation operations with greater non-linearity enhance the capacity of a sparse convolution network, and\\n- a simple sparse convolution network using the LRP module, which achieves superior 3D segmentation results on various large-scale 3D scene benchmarks.\"}"}
{"id": "CVPR-2023-1210", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"between features. However, the computational demands of these methods are higher, hindering their application to large scenes. In order to balance computational efficiency and accuracy, voxel-based networks use 3D sparse convolution to aggregate local features. Limited by a large number of 3D convolution parameters, it is difficult to improve results by directly increasing the receptive field. LargeKernel3D expands the receptive field of 3D convolution by depth-wise convolution and dilation convolution. However, compared to transformers, the receptive field of this approach is still small, and it introduces more parameters and computation. To solve these problems, we propose the long range pooling method, which can expand the receptive field with few additional parameters and introduce operations with greater non-linearity for feature aggregation to enhance the capability of the neural network.\\n\\n2.2. Vision Transformers\\n\\nThe transformer network architecture comes from natural language processing. Recently, due to its strong modeling capability, it has quickly provided leading methods for various vision tasks, including image classification, object detection, semantic segmentation, image generation, and self-supervised learning\u2014see the surveys in [20, 33]. The transformer architecture is also introduced into 3D vision by PCT and PT almost simultaneously, which propose a 3D transformer based on global attention and a 3D transformer based on local attention, respectively. The core module of a transformer is the self-attention block, which models relationships by calculating pairwise similarity between any two feature points. We believe the success of self-attention arises for two reasons: (i) self-attention captures long range dependencies, and (ii) the matrix multiplication of attention and value, and softmax function provide strong non-linearity.\\n\\n2.3. Large Kernel Design in CNNs\\n\\nInspired by the success of vision transformers, researchers have challenged the traditional small kernel design of CNNs and suggested the use of large convolution kernels for visual tasks. For example, ConvNeXt suggests directly adopting a 7 \u00d7 7 depth-wise convolution, while the Visual Attention Network (VAN) uses a kernel size of 21 \u00d7 21 and introduces an attention mechanism. RepLKNet introduces a 31 \u00d7 31 convolution by using a reparameterization technique. Recently, this design is also introduced into 3D field by LargeKernel3D. For the above works, we observe that VAN gives the best results, which we believe is because it introduces non-linearity via the Hadamard product, in addition to long range dependencies.\\n\\n3. Method\\n\\nOur analysis above suggests that long range interactions and greater non-linearity may be the key to success. Accordingly, we have designed a simple yet effective long range pooling (LRP) module, based on these principles. Next, we will introduce the LRP module and LRPNet in detail.\\n\\n3.1. Dilation Module for 3D Voxel\\n\\nTo our knowledge, we are the first to implement a large receptive field network for processing 3D voxel by using dilated operations. Here, we first revisit the decomposition of a large kernel by dilated operations in 2D CNNs to make the paper self-contained.\\n\\nIn fact, general convolutional neural networks can implicitly realize large receptive fields over the whole network. With increasing network depth, the receptive field of the last layer of features gradually increases\u2014this is one of the reasons why deep neural networks can be effective. Meanwhile, previous works have shown that increasing the local receptive field can give better results than increasing the depth. On the other hand, it is almost impossible to apply large kernels directly to the network because of their high computation load and large number of parameters. Currently, the commonly used large kernel size is 31 \u00d7 31 in 2D images, which improves accuracy while also introducing more parameters and computation. Therefore, most works have decomposed the large kernel module and used the computationally friendly small kernel module to approximate the large kernel convolution.\\n\\nFor 2D images, several dilation convolutions are usually used to achieve a decomposed large kernel convolution. However, 3D data is sparse, and the sparsity affects the response strength of the convolution, which makes large kernel decomposition more difficult. Besides, due to the sparsity, the weight convergence of convolution is too slow. Therefore, we use dilated pooling to achieve a large receptive field while keeping a low computational cost.\\n\\n3.2. Long Range Pooling Module\\n\\nTwo direct strategies can achieve a large receptive field and non-linearity: self-attention, and max pooling with a large window size. The large computational load placed by self-attention limits its application to 3D data. Max pooling also imposes huge computational demands as the window grows. We avoid the computational cost of max pooling by introducing sparse dilated pooling.\\n\\nAs shown in Figure 2, we approximate pooling with a large window size by stacking three 3 \u00d7 3 \u00d7 3 pooling modules with different dilation rates. This achieves a progressive increase in receptive field size while keeping the computational cost low. Besides, fixed receptive fields will limit\"}"}
{"id": "CVPR-2023-1210", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Long Range Pooling Module. Dilation max pooling is used to enlarge the receptive field and also outputs features of the different receptive field. \\n\\n$P_1$, $P_2$, $P_3$: max pooling with kernel size $3 \\\\times 3 \\\\times 3$ and dilation $1, 3, 9$. \\n\\nRF: the receptive field. \\n\\nSelection: a linear module producing the selection weight for each voxel. \\n\\nThe network's ability to model objects of different sizes. To choose a proper receptive field that can be adaptive to the distribution of voxels, a receptive field selection block is designed to choose a suitable window size for each voxel.\\n\\nFeatures of each location need to interact with each other or be aggregated to learn the local or global context. There are three typical ways to achieve this: 1) average pooling; 2) convolution; and 3) max pooling. The former two are linear and parameterized (fixed for average pooling and learnable for convolution); the last one is nonlinear and non-parametric. Considering the sparsity and non-uniformity of 3D voxels, the shared linear interactive ways would make the feature of a voxel too small (for voxels having few neighbors) or over-smoothed (for voxels having many neighbors). Max-pooling, in turn, will always draw the feature of a voxel to the most informative one. We illustrate the features learned by different interactive manners in Figure 5. We also compare these three different interactive manners in section 4.4.2 and the experiments show max pooling works best, which supports our viewpoint that more non-linearity for feature aggregation is critical for 3D scene understanding.\\n\\nThe overall LRP module can be formulated as \\n\\n$$S = (S_1, S_2, S_3) = \\\\text{linear}(x),$$ (1)\\n\\n$$P_1 = p(x, 1), P_2 = p(P_1, 3), P_3 = p(P_2, 9),$$ (2)\\n\\nOutput $= S_1 P_1 + S_2 P_2 + S_3 P_3,$ (3)\\n\\nwhere $x$ denotes the input features, $S$ is the selected weight, $Conv$, $LRP$, $ResBlock$, and $LRPBlock$ are convolution, linear resolution pooling, residual block, and long range pooling block, respectively.\\n\\nFigure 3. Network architecture for scene segmentation. We use the sparse convolutional U-Net [29] as our baseline, consisting of four stages. For the baseline, we use ResBlock as the basic block to extract features, and we replace the ResBlock with LRPBlock to build LRPNet.\\n\\nAs we have already noted, the LRP module can readily be incorporated into existing networks. Therefore, we build LRPNet by simply adding an LRP module after ResBlock in the baseline model. In addition to placing the LRP behind the ResBlock, we alternatively experimented with placing it front, middle, and as a parallel branch. These experiments and analysis are discussed in Section 4.4.1.\\n\\n4. Experiments\\n\\nWe performed various experiments on several semantic segmentation datasets, to verify our claims and compare our results to those of other methods.\\n\\n4.1. Datasets and metrics\\n\\nScanNet v2 [8] is a large-scene benchmark, with 1,613 scenes split 1201:312:100 for training, validation and testing. It has 20 semantic classes. We follow the same protocol as previous works [7, 15] and use mean class intersection-over-union (IoU) as the metric.\"}"}
{"id": "CVPR-2023-1210", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Iro Armeni, Ozan Sener, Amir R Zamir, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3D semantic parsing of large-scale indoor spaces. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1534\u20131543, 2016.\\n\\n[2] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.\\n\\n[3] Jun-Xiong Cai, Tai-Jiang Mu, Yu-Kun Lai, and Shi-Min Hu. Deep point-based scene labeling with depth mapping and geometric patch feature encoding. Graphical Models, 104:101033, 2019.\\n\\n[4] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017.\\n\\n[5] Yukang Chen, Jianhui Liu, Xiaojuan Qi, Xiangyu Zhang, Jian Sun, and Jiaya Jia. Scaling up kernels in 3D CNNs. arXiv preprint arXiv:2206.10555, 2022.\\n\\n[6] Hung-Yueh Chiang, Yen-Liang Lin, Yueh-Cheng Liu, and Winston H Hsu. A unified point-based framework for 3D segmentation. In 2019 International Conference on 3D Vision (3DV), pages 155\u2013163. IEEE, 2019.\\n\\n[7] Christopher Choy, Jun-Young Gwak, and Silvio Savarese. 4D spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3075\u20133084, 2019.\\n\\n[8] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3D reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828\u20135839, 2017.\\n\\n[9] Angela Dai and Matthias Nie\u00dfner. 3DMV: Joint 3D-multi-view prediction for 3D semantic scene segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 452\u2013468, 2018.\\n\\n[10] Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, J\u00fcrgen Sturm, and Matthias Nie\u00dfner. Scancomplete: Large-scale scene completion and semantic segmentation for 3D scans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4578\u20134587, 2018.\\n\\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\n[12] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in CNNs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11963\u201311975, 2022.\\n\\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\n[14] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the KITTI vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\\n\\n[15] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3D semantic segmentation with submanifold sparse convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9224\u20139232, 2018.\\n\\n[16] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu. PCT: Point cloud transformer. Computational Visual Media, 7(2):187\u2013199, 2021.\\n\\n[17] Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, and Shi-Min Hu. Beyond self-attention: External attention using two linear layers for visual tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence.\\n\\n[18] Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, and Shi-Min Hu. Segnext: Rethinking convolutional attention design for semantic segmentation. arXiv preprint arXiv:2209.08575, 2022.\\n\\n[19] Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, and Shi-Min Hu. Visual attention network. arXiv preprint arXiv:2202.09741, 2022.\\n\\n[20] Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-Hai Zhang, Ralph R Martin, Ming-Ming Cheng, and Shi-Min Hu. Attention mechanisms in computer vision: A survey. Computational Visual Media, pages 1\u201338, 2022.\\n\\n[21] Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel Cohen-Or. MeshCNN: A network with an edge. ACM Transactions on Graphics (TOG), 38(4):1\u201312, 2019.\\n\\n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022.\\n\\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[24] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. Randla-net: Efficient semantic segmentation of large-scale point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11108\u201311117, 2020.\\n\\n[25] Shi-Min Hu, Jun-Xiong Cai, and Yu-Kun Lai. Semantic labeling and instance segmentation of 3D point clouds using patch context analysis and multiscale processing. IEEE transactions on visualization and computer graphics, 26(7):2485\u20132498, 2018.\\n\\n[26] Shi-Min Hu, Dun Liang, Guo-Ye Yang, Guo-Wei Yang, and Wen-Yang Zhou. Jittor: A novel deep learning framework.\"}"}
{"id": "CVPR-2023-1210", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"with meta-operators and unified graph execution. Science China Information Sciences, 63(222103):1\u201321, 2020.\\n\\n[27] Shi-Min Hu, Zheng-Ning Liu, Meng-Hao Guo, Jun-Xiong Cai, Jiahui Huang, Tai-Jiang Mu, and Ralph R Martin. Subdivision-based mesh convolution networks. ACM Transactions on Graphics (TOG), 41(3):1\u201316, 2022.\\n\\n[28] Wenbo Hu, Hengshuang Zhao, Li Jiang, Jiaya Jia, and Tien-Tsin Wong. Bidirectional projection network for cross-dimension scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14373\u201314382, 2021.\\n\\n[29] Zeyu Hu, Xuyang Bai, Jiaxiang Shang, Runze Zhang, Jiayu Dong, Xin Wang, Guangyuan Sun, Hongbo Fu, and Chiew-Lan Tai. Vmnet: Voxel-mesh network for geodesic-aware 3D semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15488\u201315498, 2021.\\n\\n[30] Jingwei Huang, Haotian Zhang, Li Yi, Thomas Funkhouser, Matthias Nie\u00dfner, and Leonidas J Guibas. Texturenet: Consistent local parametrizations for learning from high-resolution signals on meshes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4440\u20134449, 2019.\\n\\n[31] Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi-Wing Fu, and Jiaya Jia. Hierarchical point-edge interaction network for point cloud semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10433\u201310441, 2019.\\n\\n[32] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two pure transformers can make one strong gan, and that can scale up. Advances in Neural Information Processing Systems, 34:14745\u201314758, 2021.\\n\\n[33] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s):1\u201341, 2022.\\n\\n[34] Alon Lahav and Ayellet Tal. Meshwalker: Deep mesh understanding by random walks. ACM Transactions on Graphics (TOG), 39(6):1\u201313, 2020.\\n\\n[35] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified transformer for 3D point cloud segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8500\u20138509, 2022.\\n\\n[36] Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic segmentation with superpoint graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4558\u20134567, 2018.\\n\\n[37] Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, and Ce Liu. Vitgan: Training gans with vision transformers. arXiv preprint arXiv:2107.04589, 2021.\\n\\n[38] Huan Lei, Naveed Akhtar, and Ajmal Mian. Seggcn: Efficient 3D point cloud segmentation with fuzzy spherical kernel. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11611\u201311620, 2020.\\n\\n[39] Xiang-Li Li, Zheng-Ning Liu, Tuo Chen, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu. Mesh neural networks based on dual graph pyramids. IEEE Transactions on Visualization and Computer Graphics, 2023.\\n\\n[40] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. Advances in neural information processing systems, 31, 2018.\\n\\n[41] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. arXiv preprint arXiv:2203.16527, 2022.\\n\\n[42] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Mykola Pechenizkiy, Decebal Mocanu, and Zhangyang Wang. More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620, 2022.\\n\\n[43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021.\\n\\n[44] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976\u201311986, 2022.\\n\\n[45] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolutional neural networks. Advances in neural information processing systems, 29, 2016.\\n\\n[46] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. Voxel transformer for 3D object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3164\u20133173, 2021.\\n\\n[47] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jaesik Park. Fast point transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16949\u201316958, 2022.\\n\\n[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\\n\\n[49] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3D classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652\u2013660, 2017.\\n\\n[50] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017.\"}"}
{"id": "CVPR-2023-1210", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Octnet: Learning deep 3d representations at high resolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3577\u20133586, 2017.\\n\\nJonas Schult, Francis Engelmann, Theodora Kontogianni, and Bastian Leibe. Dualconvmesh-net: Joint geodesic and euclidean convolutions on 3d meshes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8612\u20138622, 2020.\\n\\nNicholas Sharp, Souhaib Attaiki, Keenan Crane, and Maks Ovsjanikov. Diffusionnet: Discretization agnostic learning on surfaces. ACM Transactions on Graphics (TOG), 41(3):1\u201316, 2022.\\n\\nBaoguang Shi, Song Bai, Zhichao Zhou, and Xiang Bai. Deeppano: Deep panoramic representation for 3-d shape recognition. IEEE Signal Processing Letters, 22(12):2339\u20132343, 2015.\\n\\nShaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10529\u201310538, 2020.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\\n\\nAyan Sinha, Jing Bai, and Karthik Ramani. Deep learning 3d shape surfaces using geometry images. In European conference on computer vision, pages 223\u2013240. Springer, 2016.\\n\\nHang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang, and Jan Kautz. Splatnet: Sparse lattice networks for point cloud processing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2530\u20132539, 2018.\\n\\nHang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision, pages 945\u2013953, 2015.\\n\\nMaxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-Yi Zhou. Tangent convolutions for dense prediction in 3d. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3887\u20133896, 2018.\\n\\nLyne Tchapmi, Christopher Choy, Iro Armeni, JunYoung Gwak, and Silvio Savarese. Segcloud: Semantic segmentation of 3d point clouds. In 2017 international conference on 3D vision (3DV), pages 537\u2013547. IEEE, 2017.\\n\\nHugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Franc \u00b8ois Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6411\u20136420, 2019.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nLei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and Jie Shan. Graph attention convolution for point cloud semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10296\u201310305, 2019.\\n\\nPeng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. O-cnn: Octree-based convolutional neural networks for 3d shape analysis. ACM Transactions On Graphics (TOG), 36(4):1\u201311, 2017.\\n\\nShenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, and Raquel Urtasun. Deep parametric continuous convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2589\u20132597, 2018.\\n\\nYue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions On Graphics (tog), 38(5):1\u201312, 2019.\\n\\nWenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9621\u20139630, 2019.\\n\\nWenxuan Wu, Qi Shan, and Li Fuxin. Pointconvformer: Revenge of the point-based convolution. arXiv preprint arXiv:2208.02879, 2022.\\n\\nXiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point transformer v2: Grouped vector attention and partition-based pooling. arXiv preprint arXiv:2210.05666, 2022.\\n\\nZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lingguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1912\u20131920, 2015.\\n\\nYun-Peng Xiao, Yu-Kun Lai, Fang-Lue Zhang, Chunpeng Li, and Lin Gao. A survey on deep geometry learning: From a representation perspective. Computational Visual Media, 6(2):113\u2013133, 2020.\\n\\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077\u201312090, 2021.\\n\\nMutian Xu, Runyu Ding, Hengshuang Zhao, and Xiaojian Qi. Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3173\u20133182, 2021.\\n\\nXu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, and Shuguang Cui. Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5589\u20135598, 2020.\\n\\nJianwei Yang, Chunyuan Li, and Jianfeng Gao. Focal modulation networks. arXiv preprint arXiv:2203.11926, 2022.\\n\\nJiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, and Qi Tian. Modeling point...\"}"}
{"id": "CVPR-2023-1210", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"clouds with self-attention and gumbel subset sampling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3323\u20133332, 2019.\\n\\nHengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16259\u201316268, 2021.\\n\\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6881\u20136890, 2021.\"}"}
