{"id": "CVPR-2022-1144", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present Masked Feature Prediction (MaskFeat) for self-supervised pre-training of video models. Our approach first randomly masks out a portion of the input sequence and then predicts the feature of the masked regions. We study five different types of features and find Histograms of Oriented Gradients (HOG), a hand-crafted feature descriptor, works particularly well in terms of both performance and efficiency. We observe that the local contrast normalization in HOG is essential for good results, which is in line with earlier work using HOG for visual recognition. Our approach can learn abundant visual knowledge and drive large-scale Transformer-based models. Without using extra model weights or supervision, MaskFeat pre-trained on unlabeled videos achieves unprecedented results of 86.7% with MViTv2-L on Kinetics-400, 88.3% on Kinetics-600, 80.4% on Kinetics-700, 38.8 mAP on AVA, and 75.0% on SSv2. MaskFeat further generalizes to image input, which can be interpreted as a video with a single frame and obtains competitive results on ImageNet.\\n\\n1. Introduction\\n\\nSelf-supervised pre-training has been phenomenally successful in natural language processing powering large-scale Transformers [66] with billion-scale data [6, 21]. The underlying idea is an astonishingly simple mask-and-predict task, that is, first masking out some tokens within a text and then predicting the invisible content given the visible text. Humans have a remarkable ability to predict how the world appears and moves when observing it as a continuous stream of spatiotemporal information. Consider the examples in the 1st column of Fig. 1. Even without seeing the masked content, we are able to understand the object structure and draw a rough outline or silhouette of imagined information (up to some details), by using visual knowledge about the visible structures. In this work, we show that predicting certain masked features (e.g., gradient histograms in the 2nd column) can be a powerful objective for self-supervised visual pre-training, especially in the video domain which contains rich visual information.\\n\\n![Example HOG predictions on unseen validation input.](image)\\n\\nOur model is learned by predicting features (middle) given masked inputs (left). Original images (right) are not used for prediction. More qualitative examples are in the Appendix.\\n\\nOne essential difference between vision and language is that vision has no pre-existing vocabulary to shape the prediction task into a well-defined classification problem. In contrast, the raw spatiotemporal visual signal is continuous and dense posing a major challenge to masked visual prediction. One immediate solution is to imitate the language vocabulary by building a visual vocabulary that discretizes frame patches into tokens, as explored in BEiT [2, 58]. However, this requires an external tokenizer which can be limited in compute-intensive video understanding scenario.\\n\\nWe present Masked Feature Prediction (MaskFeat), a pre-training objective that directly regresses features of the masked content. Specifically, our approach ingests the masked space-time input with a vision Transformer backbone [23, 46] and predicts a certain feature representation of the masked content. In this way, the pre-trained model acquires an adequate understanding of the complex space-time structures within dense visual signals.\"}"}
{"id": "CVPR-2022-1144", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We study a broad spectrum of feature types, from pixel colors and hand-crafted feature descriptors, to discrete visual tokens, activations of deep networks, and pseudo-labels from network predictions. Our study reveals:\\n\\n(i) Simple histogram of oriented gradients (center column in Fig. 1), as in the popular HOG [18] and SIFT [49] descriptors which dominated visual recognition for over a decade, is a particularly effective target for MaskFeat in terms of both performance and efficiency.\\n\\n(ii) The discretization (tokenization) of visual signals is not necessary for masked visual prediction, and continuous feature regression (i.e., MaskFeat) can work well.\\n\\n(iii) Semantic knowledge from human annotations is not always helpful for MaskFeat, but characterizing local patterns seems important. For example, predicting supervised features from CNNs or ViTs trained on labeled data leads to degraded performance.\\n\\nOur approach is conceptually and practically simple. Compared to contrastive methods that require a siamese structure and two or more views of each training sample (e.g., [16, 29, 37]), MaskFeat uses a single network with a single view of each sample; and unlike contrastive methods that strongly rely on carefully designed data augmentation, MaskFeat works fairly well with minimal augmentation.\\n\\nCompared to previous masked visual prediction methods [2, 62], MaskFeat with HOG does not involve any external model, such as a dV AE tokenizer [58] that introduces not only an extra pre-training stage on 250M images, but also non-negligible training overhead in masked modeling.\\n\\nWe show that MaskFeat can pre-train large-scale video models that generalize well. Transformer-based video models, though powerful, are previously known to be prone to over-fitting and heavily rely on supervised pre-training [1, 46] on large-scale image datasets, e.g., ImageNet-21K (IN-21K) [20]. While MaskFeat opens the door for directly pre-training on unlabeled videos which shows enormous benefits for video understanding.\\n\\nOur results on standard video benchmarks are groundbreaking: MaskFeat pre-trained MViTv2-L [46] gets 86.7% top-1 accuracy on Kinetics-400 [43] without using any external data, greatly surpassing the best prior number of this kind by +5.2%, and also methods using large-scale image datasets, e.g., IN-21K and JFT-300M [61]. When transferring to downstream tasks, MaskFeat gets unprecedented results of 38.8 mAP on action detection (AVA [35]) and 75.0% top-1 accuracy on human-object interaction classification (SSv2 [33]). When generalized to the image domain, MaskFeat also obtains competitive 84.0% top-1 with ViT-B and 85.7% with ViT-L using only ImageNet-1K [20].\\n\\nOur code will be available in PyTorchVideo.\\n\\n1 https://github.com/facebookresearch/pytorchvideo\\n2 https://github.com/facebookresearch/mvit\\n\\nFigure 2. MaskFeat pre-training. We randomly replace the input space-time cubes of a video with a [MASK] token and directly regress features (e.g., HOG) of the masked regions. After pre-training, the Transformer is fine-tuned on end tasks.\\n\\n2. Method\\n\\nWe start by describing MaskFeat and its instantiations for video and image understanding in \u00a72.1. We then introduce and discuss five candidates for target features in \u00a72.2.\\n\\n2.1. Masked Feature Prediction\\n\\nOur method performs a masked visual prediction task, motivated by humans' ability to inpaint masked visual content up to some details. The task first randomly masks out a few space-time cubes of a video, and then predicts the masked ones given the remaining ones. By modeling masked samples, the model attains video understanding in the sense of recognizing parts and motion of objects. For instance, to solve the examples in Fig. 1, a model has to first recognize the objects based on the visible area, and also know what the objects typically appear and how they usually move to inpaint the missing area.\\n\\nOne key component of the task is the prediction target. Masked language modeling tokenizes the corpus with a vocabulary to serve as the target [21]. In contrast, the raw visual signal is continuous and high-dimensional and there is no natural vocabulary available. In MaskFeat, we propose to predict features of the masked area. And the supervsion is provided by features extracted from the original, intact sample. We use a wide interpretation of features [12], from hand-crafted feature descriptors, to activations of deep networks. The choice of the target feature largely defines the task and impacts the property of the pre-trained model, which we discuss in \u00a72.2.\\n\\nInstantiations. We first describe MaskFeat for video input. A video is first divided into space-time cubes as in typical video Vision Transformers [26, 46]. The cubes are then projected (i.e., convolved) to a sequence of tokens. To perform masking, some of the tokens in the sequence are randomly masked out by being replaced with a [MASK] token.\"}"}
{"id": "CVPR-2022-1144", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This is a learnable embedding indicating masked patches. A block of tokens is masked together which we detail in \u00a74.3. To make a prediction, the token sequence after [MASK] token replacement, with positional embedding added, is processed by the Transformer. Output tokens corresponding to the masked cubes are projected to the prediction by a linear layer. The prediction is simply the feature of the 2-D spatial patch temporally centered in each masked cube (see discussions in Appx. Tab. 11). The number of output channels is adjusted to the specific target feature (e.g., $3 \\\\times 16 \\\\times 16$ if predicting RGB colors of pixels in a $16 \\\\times 16$ patch). The loss is only operated on the masked cubes. Our instantiation is inspired by BERT [21] and BEiT [2], illustrated in Fig. 2.\\n\\nMaskFeat can be easily instantiated in the image domain, which can be interpreted as a video with one single frame. Most operations are shared, except that there is no temporal dimension and each token now represents only a spatial patch instead of a space-time cube.\\n\\n2.2. Target Features\\n\\nWe consider five different types of target features. The targets are categorized into two groups: 1) one-stage targets that can be directly obtained including pixel colors and HOG, and 2) other two-stage targets extracted by a trained deep network or teacher. As predicting two-stage targets is effectively learning from a trained deep network teacher, it resembles a form of model distillation [40]; thereby, an extra computational cost of pre-training and inference of the teacher model is inevitable. The five feature types are:\\n\\n- **Pixel colors.** The most straightforward target is arguably the colors of video pixels. Specifically, we use RGB values that are normalized by the mean and the standard deviation of the dataset. We minimize the $\\\\ell_2$ distance between the model's prediction and the ground-truth RGB values. A similar idea has been explored in [55] as a image inpainting task and in [2, 23] for masked image prediction. Though simple, pixels as target have a potential downside of overfitting to local statistics (e.g., illumination and contrast variations) and high-frequency details, which are presumably insignificant [60] for interpretation of visual content.\\n\\n- **HOG.** Histograms of Oriented Gradients (HOG) [18] is a feature descriptor that describes the distribution of gradient orientations or edge directions within a local subregion. A HOG descriptor is implemented by a simple gradient filtering (e.g., subtracting neighboring pixels) to compute magnitudes and orientations of gradients at each pixel. The gradients within each small local subregion or cell are then accumulated into orientation histogram vectors of several bins, voted by gradient magnitudes. The histogram is normalized to unit length. These features are also used in well-known SIFT [49] descriptors for detected keypoints or in a dense fashion for classification [12]. Similarly, we extract HOG on a dense grid for the whole image, which suits the prediction target for randomly masked patches. HOG is characteristic of capturing local shapes and appearances while being partially invariant to geometric changes as long as translations are within the spatial cell and rotations are smaller than orientation bin size. Further, it provides invariance to photometric changes as image gradients and local contrast normalization absorb brightness (e.g., illumination) and foreground-background contrast variation. These invariances are vital for good results when using HOG for pedestrian detection in both image [18] and video [19] domains. In accordance to this, our studies (\u00a75.2) reveal local-contrast normalization in HOG is also essential for MaskFeat pre-training. Finally, HOG computation is cheap and introduces negligible overhead. It can be implemented as a two-channel convolution to generate gradients in $x$ and $y$ axis (or by subtracting neighboring horizontal and vertical pixels), followed by histogramming and normalization. Our method then simply predicts the histograms summarizing masked patches. Instead of computing HOG only on masked patches, we first obtain a HOG feature map on the whole image and then split the map into patches. In this way, we reduce padding on boundaries of each masked patch. The histograms of masked patches are then flattened and concatenated into a 1-D vector as the target feature. Our loss minimizes the $\\\\ell_2$ distance between the predicted and original HOG feature. We collect HOG in each RGB channel to include color information which can slightly improve its performance (\u00a75.2).\\n\\n- **Discrete variational autoencoder (dVAE).** To address the continuous high-dimensional nature of visual signals, DALL-E [58] proposes to compress an image with a dVAE codebook. In particular, each patch is encoded into a token which can assume 8192 possible values using a pre-trained dVAE model. Now the task is to predict the categorical distribution of the masked token by optimizing a cross-entropy loss, as explored in BEiT [2]. However, there is an extra computational cost induced by pre-training the dVAE and tokenizing images alongside masked feature prediction.\\n\\n- **Deep features.** In comparison to discretized tokens, we consider directly using continuous deep network features as the prediction target. We use a pre-trained model to produce features as a teacher, either a CNN or ViT, and our loss minimizes the cosine distance (i.e., mean squared error of $\\\\ell_2$-normalized features). For CNN teachers, we use the last layers' features corresponding to the masked patches and for ViT we use the respective output patch tokens. We mainly compare features from self-supervised models, which are considered to contain more diverse scene layout [9] and preserve more visual details [74] than features from supervised models.\"}"}
{"id": "CVPR-2022-1144", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Training technically not self-supervised.) Supervised features are expected to be more semantic as they are trained through human annotations. Similar to dV AE, a non-trivial amount of extra computation is involved when using extra model weights for masked feature generation.\\n\\nPseudo-label.\\n\\nTo explore an even more high-level semantic prediction target, we consider predicting class labels of masked patches. We utilize labels provided by Token Labeling [42], where each patch is assigned an individual location-specific IN-1K pseudo-label. This class label map is generated by a pre-trained high-performance supervised deep network [5] teacher. The masked feature prediction stage is optimized by a cross-entropy loss.\\n\\nWe next study the features discussed in this section.\\n\\n### 3. Study: Target Features for MaskFeat\\n\\nSettings.\\n\\nWe use a pre-training and fine-tuning protocol, following BEiT [2]. We pre-train MViTv2-S, 16 \u00d7 4 [46] with MaskFeat on Kinetics-400 (K400) [43] training set for 300 epochs. We also apply MaskFeat on images, where we pre-train ViT-B [23] on the ImageNet-1K (IN-1K) [20] training set for 300 epochs. We report top-1 fine-tuning accuracy (%) on both datasets. We pre-train and fine-tune all targets with the same recipe which we find generally good in practice. For targets that involve a teacher model, we use official models released by the authors.\\n\\nMost features are compared on both video and image domains except pseudo-label for which the pseudo-label map is only available on IN-1K [42]. Results are summarized in Tables 1 (video) and 2 (image), analyzed next:\\n\\n#### One-stage methods.\\n\\nThe fine-tuning accuracy for pixel color prediction in Tables 1 & 2 shows, that compared to the from-scratch baselines, regressing RGB colors produces a slight drop of \u22120.4% for video classification and a relatively small gain of +0.7% for image. Even though our predicting pixel colors result on IN-1K (82.5%) is better than that reported in BEiT [2] (81.0%), we similarly observe that pixel values are not ideal direct targets, presumably because they are considered to be too explicit [58]. In comparison, HOG, by summarizing the local gradient distribution, contributes to large improvements of +1.1% on K400 and +1.8% on IN-1K over the from-scratch baselines without any extra model which is typical in two-stage methods.\\n\\n#### Two-stage methods.\\n\\nFirst, dV AE improves by +0.6% for K400 and +1.0% for IN-1K over their from-scratch baselines. This is better than pixel colors, but outperformed by HOG which does not use an external model.\\n\\nNext, compared to dV AE, we study MaskFeat to predict continuous, unsupervised features: We compare DINO [9] (with ViT-B) and MoCo [15, 17] (with ResNet50 [38] and ViT-B), all pre-trained on IN-1K, even for the video pre-training. Unsupervised features contribute a notable gain for both video and image classification: The DINO variant achieves a gain of +1.4% on K400 and +2.2% on IN-1K compared to their baselines. However, this approach has two main drawbacks, (i) the unsupervised feature extractor needs to be pre-trained e.g. worth over thousand epochs in the case of DINO, (ii) the unsupervised features need to be computed on the target data. Still, MaskFeat w/ DINO and MoCo v3 features boosts their original accuracy [9, 17].\\n\\nFinally, supervised features (from ResNet50 or ViT-B) as well as token labels, though utilizing human annotations, lag behind unsupervised features and HOG. In fact, we notice significant over-fitting during fine-tuning for supervised features and token labels, suggesting that predicting features learned from class labels is not suitable in MaskFeat.\\n\\n| Feature Type | One-stage Variant | Arch. Param. | Top-1 | Epoch |\\n|--------------|-------------------|-------------|------|-------|\\n| Pixel Colors | \u2713 | RGB | 82.5 | - |\\n| Image Descriptor | \u2713 | HOG [18] | 83.6 | - |\\n| dV AE Token | \u2717 | DALL-E [58] | 82.8 | 54 1199 |\\n| Unsupervised Feature | \u2717 | MoCo v2 [15] ResNet50 | 83.6 | 23 800 |\\n| Unsupervised Feature | \u2717 | MoCo v3 [17] ViT-B | 83.9 | 85 600 |\\n| Unsupervised Feature | \u2717 | DINO [9] ViT-B | 84.0 | 85 1535 |\\n| Supervised Feature | \u2717 | pytorch [53] ResNet50 | 82.6 | 23 90 |\\n| Supervised Feature | \u2717 | DeiT [63] ViT-B | 81.9 | 85 300 |\\n| Pseudo-label | \u2717 | Token Labeling [42] NFNet-F6 | 78.8 | 438 360 |\\n\\nTable 1. Comparing target features for MaskFeat (video).\\n\\nFor all targets, ViT-B is pre-trained with MaskFeat for 300 epochs on IN-1K. We report 100-epoch fine-tuning accuracy on IN-1K. For two-stage targets, we report the teacher architecture, number of parameters (M), and effective epoch \u2020 on IN-1K. The default entry is marked in gray. The plot on the left visualizes the acc/epoch trade-off of the table.\\n\\n\u2020 Different teachers use different training strategies. dV AE is pre-trained on an external 250M dataset, while self-supervised methods require multi-view training. To measure the cost in a unified way, we normalize the number of epochs by the cost of one epoch on IN-1K training set with one 224 \u00d7 2 view.\\n\\nTable 2. Comparing target features for MaskFeat (image).\"}"}
{"id": "CVPR-2022-1144", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our results suggest that a broad spectrum of Discussion.\\n\\nof MaskFeat to model object\u2019s internal structure. shapes and textures of the same object disables the ability\\nWe hypothesize that class label being invariant to local\\ntrade-off between performance and computational cost, pre-\\nsupervisedly entail a form of\\nmodel distillation\\nMViTv2\\n\\nlatter come without cost compared to the former which also\\nimage descriptors can be strong prediction targets, while the\\nvision. We find that continuous unsupervised features and\\n\\nadministration in \u00a74.3. More implementation details are in Appx. C.1.\\n\\nOur models are pre-trained and fine-tuned\\naugmentation includes random resized cropping and hori-\\nclips in the training set of K400 without labels. Our\\nMViTv2. The models are pre-trained\\nonly\\nWe evaluated with both base and large models of\\n4.1. Main Results on Kinetics\\nin horizontally\\n647\\n2063\\n234\\n4076\\n81.2\\n2063\\n\\nas default feature for MaskFeat in the following sections.\\ndictating HOG holds a good balance and therefore we use it\\nspecific information being present in features [3, 75] that is too\\nproduce poor results, which might relate to class-level spe-\\n\\n4. Experiments: Video Recognition\\n\\nTable 3 compares MaskFeat with prior work\\nKinetics-400.\\n\\n4.1. Main Results on Kinetics\\n\\non large-scale image datasets\\nof which are heavily dependent on supervised pre-training\\npresents representative Transformer-based methods, most\\ncommonly do not use any pre-training. The second section\\non K400 dataset. From top to bottom, it has three sections.\\n\\nTable 3. Comparison of MaskFeat with prior work on Kinetics-\\n400.\\n\\n| Model                  | Top-1 (%) | Top-5 (%) |\\n|------------------------|-----------|-----------|\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 300            | 84.3      | 95.9      |\\n| MaskFeat 30"}
{"id": "CVPR-2022-1144", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Transferring to AVA v2.2 [35]. We use single center crop inference (center) following MViT [26] and full resolution inference (full) to compare to the 2020 AVA Challenge winner ACAR [52]. Inference cost is with the center strategy.\\n\\nOur best 87.0% top-1 accuracy is achieved by fine-tuning the 85.1% MViTv2-L, 16 \u00d7 4 pre-trained with MaskFeat on 387K training videos in K600 using no labels.\\n\\nOur results with just K400 (86.7%) is already similar to recent 86.5% Florence [72] and 86.8% SwinV2-G [47]. Florence uses 900M curated text-image pairs. SwinV2-G utilizes a giant model with three billion parameters, and is first self-supervisedly then supervisedly pre-trained on a large dataset of IN-21K plus 70M in-house images. The efficiency of our approach in terms of parameter count, compute cost, data, and annotation suggests again the advantage of MaskFeat directly pre-training on unlabeled videos.\\n\\n4.2. Transfer Learning\\n\\nWe evaluate downstream transfer learning with the Kinetics MViTv2-L \u2191312, 40 \u00d7 3 in Table 3 and Appx. 9a.\\n\\nAction detection. AVA v2.2 [35] is a benchmark for spatiotemporal localization of human actions. We fine-tune the MViTv2-L \u2191312, 40 \u00d7 3 Kinetics models on AVA v2.2. Details are in Appx. C.2. Table 4 reports mean Average Precision (mAP) of our MaskFeat models compared with prior state-of-the-art. MaskFeat only using K400 contributes to a significant gain of +4.7 mAP over its IN-21K pre-trained counterpart using identical architectures. By utilizing a larger video dataset, K600, the model reaches an unprecedented accuracy of 38.8 mAP with full resolution testing, greatly surpassing all previous methods, including ActivityNet challenge winners. The strong performance of MaskFeat on AVA suggests a clear advantage of masked modeling on video over supervised classification on image pre-training for this localization-sensitive recognition task.\\n\\nHuman-object interaction classification. We fine-tune the MViTv2-L \u2191312, 40 \u00d7 3 Kinetics models in Table 3 and Appx. 9a to Something-Something v2 (SSv2) [33] which focuses on human-object interaction classification. Table 5 presents the results and details are in Appx C.3. In contrast to Kinetics, SSv2 requires fine-grained motion distinctions and temporal modeling to distinguish interactions like picking something up and putting something down.\\n\\nTable 5. Transferring to Something-Something v2 [33]. We report FLOPs with a single \u201cview\u201d. All entries use one temporal clip and three spatial crops (inference cost is FLOPs \u00d7 3 \u00d7 1).\\n\\nDespite the differences between the supervised tasks of Kinetics and SSv2, pre-training on Kinetics without supervised labels using MaskFeat still contributes to a large gain on fine-tuning accuracy of SSv2. Specifically, MaskFeat with only K400 data contributes to +1.1% top-1 over its IN-21K+K400 pre-trained counterpart. By utilizing the larger K600, the model reaches an unprecedented 75.0% top-1 accuracy, surpassing all previous methods. This suggests that MaskFeat can learn spatiotemporal representations from unlabeled Kinetics data which is known as appearance-biased, through self-supervised masked feature prediction.\\n\\n4.3. Ablations for Video Recognition\\n\\nThe ablations are with MViTv2-S, 16 \u00d7 4 pre-trained for 300 epochs and fine-tuned for 200 epochs on K400. More ablations (e.g., on masking ratio) are in Appx. A.\\n\\nMasking strategy. We study the masking strategy for spatiotemporal video data. In video, tokens sharing the same spatial position usually also share visual patterns. Therefore, we explore how to handle this redundancy brought by the addition of the temporal dimension. We consider three different ways of masking and present the results in Table 6. All entries share the same 40% masking ratio.\\n\\nTable 6. Masking strategy. Varying the strategy of masking in spatiotemporal data. The default entry is highlighted in gray.\\n\\nFirst, we consider \u201cframe\u201d masking, which independently masks out consecutive frames. This strategy mostly masks different spatial blocks in consecutive frames, but the model could temporally \u201cinterpolate\u201d between frames to solve the task. This strategy only obtains 81.0% top-1.\\n\\nSecond, we consider \u201ctube\u201d masking. Namely, we first sample a 2-D mask map by block-wise masking as for images, and then extend the 2-D map by repeating it in the temporal dimension. Thus, the masked area is a straight tube in a video clip, in which the spatially masked area is the same for every frame. Tube masking refrains from relying on the temporal repetition to predict the masked content in static video. It leads to 81.9% accuracy.\\n\\nThird, we consider \u201ccube\u201d masking, which includes both spatial and temporal blocks that are masked out together.\"}"}
{"id": "CVPR-2022-1144", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7. Comparison with previous work on IN-1K. All entries are pre-trained on IN-1K train split, except supervised using IN-21K. MoCo v3 and DINO use momentum encoder. BEiT uses 250M DALL-E data to pre-train dV AE. All entries are trained and evaluated at image size 224$^2$ except supervised at 384$^2$.\\n\\nThis is achieved by sampling random \\\"cubes\\\" of tokens until a certain masking ratio is reached. Cubes are sampled by first creating a 2-D block at a random time step, then extending in the temporal dimension with a random number of consecutive frames. Therefore, cube masking can be considered as a generalization of tube and frame masking. It produces 82.2% accuracy when used for pre-training.\\n\\nOverall, the results in Table 6 show that cube masking performs best, suggesting both spatial and temporal cues are helpful in masked spatiotemporal prediction.\\n\\n5. Experiments: Image Recognition\\n\\nSettings. The evaluation protocol is pre-training followed by end-to-end fine-tuning. We use vanilla base and large models in ViT [23] without modification. Our models are pre-trained at 224$^2$ resolution on IN-1K [20] training set without labels. We use minimal data augmentation: random resized cropping and horizontal flipping. We randomly mask out 40% of total image patches with block-wise masking following BEiT [2]. More details are in Appx. C.1.\\n\\n5.1. Main Results on ImageNet-1K\\n\\nIn Table 7 we compare MaskFeat to previous work including from-scratch, IN-21K supervised pre-training, and previous self-supervised methods. We pre-train MaskFeat for 1600 epochs here while for 300 epochs in Table 2. The fine-tuning schedule is the same everywhere and rather short, 100 epochs for ViT-B and 50 epochs for ViT-L.\\n\\nWe observe that MaskFeat pre-training significantly boosts the scratch baselines for both ViT-B and ViT-L. Our approach at image size 224$^2$ is on par with (ViT-B), or even outperforms (ViT-L) supervised pre-training on IN-21K that has 10$\\\\times$ more images and labels at image size 384$^2$. It has been shown [23] that ViT models are data-hungry and require large-scale supervised pre-training, possibly due to the lack of typical CNN inductive biases. Our results suggest that MaskFeat pre-training can overcome this without external labeled data by solving our feature inpainting task.\\n\\nInterestingly, more gains are observed on ViT-L compared with ViT-B, suggesting that it is scalable to larger models.\\n\\nCompared to self-supervised pre-training approaches, MaskFeat is more accurate and simpler. DINO [9] and norm. are contrastive methods that require multi-view training and carefully designed augmentation, while MaskFeat only uses single-views and minimal augmentation. See Tab. 15 in Appx. for ablation on data augmentation of MaskFeat. Compared with BEiT [2], MaskFeat gets rid of the dV AE tokenizer, which introduces both an extra pre-training stage on the 250M DALL-E dataset, and a non-negligible inference overhead during masked prediction. While MaskFeat simply calculates HOG features.\\n\\nMaskFeat in Table 7 is pre-trained for 1600 epochs with a single 224$^2$ view. DINO uses multiple global-local views and an extra momentum encoder, leading to 1535 effective epochs $\\\\dagger$ (Table 2). MoCo v3 saturates after 600 effective epochs [17]. BEiT is pre-trained for 800 epochs on IN-1K but requires another 1199 effective epochs for dV AE.\\n\\nWe also train the best model in Table 2, MaskFeat w/ DINO, for 1600 epochs and it reaches 84.2%; however, this uses a separate ViT-B model that is trained with another $\\\\dagger$1535 effective epochs using DINO. MaskFeat w/ HOG can reach 84.0% without extra model.\\n\\n5.2. Ablations for Image Recognition\\n\\nWe ablate the design choices of MaskFeat in the image domain first. We use ViT-B pre-trained for 300 epochs by default and report fine-tuning top-1 accuracy (%) on IN-1K. More ablations (e.g. on training epochs) are in Appx. B.\\n\\nHOG implementation. We ablate HOG implementation details in Table 8. We first investigate the local contrast normalization in HOG, which is key to its performance in image recognition [18]. It is applied by normalizing each histogrammed vector of local 8$\\\\times$8 pixel cells, which leads e.g. to local invariance in illumination change. We show in Table 8a that normalization is essential for MaskFeat. Compared with default $\\\\ell_2$ normalization, using $\\\\ell_1$ normalization results in a 0.8% drop and not using any normalization causes a large -1.4% drop. Similar results are reported in [18] for directly using HOG for image recognition.\\n\\nWe next investigate the effectiveness of color information in Table 8b. Gray refers to extracting HOG on grayscale images, which only contains intensity information.\"}"}
{"id": "CVPR-2022-1144", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Both two predictions make good sense given a small visible region at the bird's head. Pixel with color ambiguity: Though pixel prediction makes a sensible guess on the balloon, the loss penalty is large because of unmatched color (red vs. black). Pixel with texture ambiguity: Pixel prediction is blurry in texture-rich area because of ambiguity, while HOG successfully characterizes major edge directions. Figure 3. Pixel vs. HOG predictions on IN-1K validation images\u2020. Pixel targets can have large errors for ambiguous problems, and HOG is more robust to ambiguity by histogramming and normalizing local gradients. Best viewed in color and zoomed in.\u2020 The unmasked regions are not used for loss and thus qualitatively poor.\\n\\nTo include color information in HOG, rgb calculates separate gradients for each color channel and concatenates the three histograms. Opp. is an affine transformation of RGB to an opponent color space [64]. Results show that color information provides a small gain of around +0.4% compared with only using gray-scaled intensity information.\\n\\nWe vary the number of orientation and spatial bins in Table 8c and Table 8d, which provides geometric invariance in HOG descriptors. Following HOG [18], we use 9 orientation bins that are evenly spaced from 0\u00b0 to 180\u00b0 (unsigned), and use 8 \u00d7 8 pixel cells (SIFT [49] uses 8 bins in 8 \u00d7 8 cells). We observe that these default settings in [18] are good for MaskFeat and that it is robust to different numbers of orientation bins, but a specific size of 8 \u00d7 8 in a cell is the best.\\n\\nPixel vs. HOG. We qualitatively compare HOG to pixel colors as the feature target of MaskFeat in Fig. 3. Both pixel and HOG predictions look reasonable in close proximity to the unmasked input. However, compared to HOG, pixel color targets come with more ambiguity. In the balloon (second) example, the model makes a sensible guess predicting a red balloon, which is black in the original image, resulting in a high loss penalty. In the sea urchin (third) example, the model is just able to make a blurry color-wise guess on the object, which is a natural consequence of minimizing a pixel-wise MSE loss in texture-rich, high-frequency regions [45]. In both cases, HOG reduces the risk of ambiguity: normalizing gradients handles the color ambiguity and spatial binning of gradients texture ambiguity.\\n\\n6. Related Work\\n\\nMasked visual prediction was pioneered with autoencoders [67] and inpainting tasks [55] using ConvNets. Since ViT [23], masked prediction has re-attracted attention of the vision community, partially inspired by the success of BERT [21] in NLP. BERT performs masked language modeling where some input tokens are masked at random and the task it to predict those. BERT pre-trained models scale well and generalize to many different downstream tasks.\\n\\nFor vision, different masked prediction objectives have been proposed. iGPT [13] predicts the next pixels of a sequence. ViT [23] predicts mean colors of masked patches. BEiT [2] and VIMPAC [62] encode masked patches with discrete variational autoencoder (dVAE) [58, 65]. Compared to BEiT and VIMPAC, our method does not rely on dVAEs but directly regresses specific features of the input.\\n\\nSelf-supervised learning aims to learn from unlabeled visual data by a pre-text task that is constructed by image/patch operations (e.g., [7, 22, 31, 51, 69, 73]) and spatiotemporal operations (e.g., [30, 32, 50, 54, 68]). Recently, contrastive learning [24] capitalizes on augmentation invariance. The invariance is achieved by enforcing similarity over distorted views of one image while avoiding model collapse [8, 9, 14, 16, 29, 34, 37, 57, 71]. Contrastive methods learn linearly separable representations, evaluated by linear probing. While in this work we focus on optimizing for the end tasks with an end-to-end fine-tuning protocol [2, 62].\\n\\n7. Conclusion\\n\\nWe present Masked Feature Prediction (MaskFeat), a simple visual pre-training approach that regresses features of masked regions. In particular, HOG, a hand-designed feature that was driving visual recognition before the deep learning era, works surprisingly well as the prediction target. MaskFeat is efficient, generalizes well, and scales to large models for both video and image domains.\\n\\nOur results are especially groundbreaking for video understanding: There has been a large gap of over 5% accuracy between supervised pre-training on large-scale image datasets and training-from-scratch methods. MaskFeat has closed this gap by directly pre-training on unlabeled videos. Transfer learning performance is even more impressive where an MaskFeat model surpasses its IN-21K counterpart, which uses 60 \u00d7 more labels, by +4.7 mAP on action detection (AVA) and +1.1% top-1 on human-object interaction recognition (SSv2). These results suggest a clear benefit of masked prediction in the visually richer space-time domain to explore in future work.\\n\\nAcknowledgement\\n\\nThis project was partially influenced by initial signals of the MAE project [36]. We thank Kaiming He and Huiyu Wang for feedback on the manuscript. CW and AY is supported by ONR N00014-21-1-2812.\"}"}
{"id": "CVPR-2022-1144", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, and Cordelia Schmid. ViViT: A video vision transformer. In ICCV, 2021.\\n\\n[2] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.\\n\\n[3] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In CVPR, 2017.\\n\\n[4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, 2021.\\n\\n[5] Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. In ICML, 2021.\\n\\n[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.\\n\\n[7] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, 2018.\\n\\n[8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.\\n\\n[9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.\\n\\n[10] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600. arXiv preprint arXiv:1808.01340, 2018.\\n\\n[11] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR, 2017.\\n\\n[12] Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil in the details: Delving deep into convolutional nets. In BMVC, 2014.\\n\\n[13] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020.\\n\\n[14] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020.\\n\\n[15] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.\\n\\n[16] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021.\\n\\n[17] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In ICCV, 2021.\\n\\n[18] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.\\n\\n[19] Navneet Dalal, Bill Triggs, and Cordelia Schmid. Human detection using oriented histograms of flow and appearance. In ECCV, 2006.\\n\\n[20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.\\n\\n[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.\\n\\n[22] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015.\\n\\n[23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\\n\\n[24] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks. TPAMI, 2015.\\n\\n[25] Haoqi Fan, Tullie Murrell, Heng Wang, Kalyan Vasudev Alwala, Yanghao Li, Yilei Li, Bo Xiong, Nikhila Ravi, Meng Li, Haichuan Yang, Jitendra Malik, Ross Girshick, Matt Feiszli, Aaron Adcock, Wan-Yen Lo, and Christoph Feichtenhofer. PyTorchVideo: A deep learning library for video understanding. In ACM MM, 2021. https://pytorchvideo.org/.\\n\\n[26] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In ICCV, 2021.\\n\\n[27] Christoph Feichtenhofer. X3D: Expanding architectures for efficient video recognition. In CVPR, 2020.\\n\\n[28] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In ICCV, 2019.\\n\\n[29] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. In CVPR, 2021.\\n\\n[30] Basura Fernando, Hakan Bilen, Efstratios Gavves, and Stephen Gould. Self-supervised video representation learning with odd-one-out networks. In CVPR, 2017.\\n\\n[31] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018.\\n\\n[32] Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, and Yann LeCun. Unsupervised learning of spatiotemporally coherent metrics. In ICCV, 2015.\"}"}
{"id": "CVPR-2022-1144", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The \u201csomething something\u201d video database for learning and evaluating visual common sense. In ICCV, 2017.\\n\\nJean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl D\u00f6rsch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohamad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020.\\n\\nChunhui Gu, Chen Sun, Sudheendra Vijayanarasimhan, Caroline Pantofaru, David A. Ross, George Toderici, Yeqing Li, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, and Jitendra Malik. A V A: A video dataset of spatio-temporally localized atomic visual actions. In CVPR, 2018.\\n\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\\n\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\nRoei Herzig, Elad Ben-Avraham, Karttikeya Mangalam, Amir Bar, Gal Chechik, Anna Rohrbach, Trevor Darrell, and Amir Globerson. Object-region video transformers. arXiv preprint arXiv:2110.06915, 2021.\\n\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NeurIPS, 2015.\\n\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\\n\\nZihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, and Jiashi Feng. All tokens matter: Token labeling for training better vision transformers. In NeurIPS, 2021.\\n\\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.\\n\\nDan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew Brown, and Boqing Gong. MoviNets: Mobile video networks for efficient video recognition. In CVPR, 2021.\\n\\nChristian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017.\\n\\nYanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Improved multiscale vision transformers for classification and detection. arXiv preprint arXiv:2112.01526, 2021.\\n\\nZe Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, and Baining Guo. Swin transformer v2: Scaling up capacity and resolution. arXiv preprint arXiv:2111.09883, 2021.\\n\\nZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. arXiv preprint arXiv:2106.13230, 2021.\\n\\nDavid G Lowe. Object recognition from local scale-invariant features. In ICCV, 1999.\\n\\nIshan Misra, C Lawrence Zitnick, and Martial Hebert. Shuffl and learn: unsupervised learning using temporal order verification. In ECCV, 2016.\\n\\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016.\\n\\nJunting Pan, Siyu Chen, Mike Zheng Shou, Yu Liu, Jing Shao, and Hongsheng Li. Actor-context-actor relation network for spatio-temporal action localization. In CVPR, 2021.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019.\\n\\nDeepak Pathak, Ross Girshick, Piotr Doll\u00e1r, Trevor Darrell, and Bharath Hariharan. Learning features by watching objects move. In CVPR, 2017.\\n\\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016.\\n\\nMandela Patrick, Dylan Campbell, Yuki M. Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and Jo\u00e3o F. Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. In NeurIPS, 2021.\\n\\nRui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal contrastive video representation learning. In CVPR, 2021.\\n\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\\n\\nMichael S Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner: What can learned tokens do for images and videos? arXiv preprint arXiv:2106.11297, 2021.\\n\\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In NeurIPS, 2016.\\n\\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In ICCV, 2017.\"}"}
{"id": "CVPR-2022-1144", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hao Tan, Jie Lei, Thomas Wolf, and Mohit Bansal. VIM-PAC: Video pre-training via masked token prediction and contrastive learning. arXiv preprint arXiv:2106.11250, 2021.\\n\\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In ICML, 2021.\\n\\nKoen Van De Sande, Theo Gevers, and Cees Snoek. Evaluating color descriptors for object and scene recognition. TPAMI, 2009.\\n\\nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2017.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\\n\\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and L\u00e9on Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. JMLR, 2010.\\n\\nXiaolong Wang, Kaiming He, and Abhinav Gupta. Transitive invariance for self-supervised visual representation learning. In ICCV, 2017.\\n\\nChen Wei, Lingxi Xie, Xutong Ren, Yingda Xia, Chi Su, Jiaying Liu, Qi Tian, and Alan L Yuille. Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning. In CVPR, 2019.\\n\\nChao-Yuan Wu and Philipp Krahenbuhl. Towards long-form video understanding. In CVPR, 2021.\\n\\nZhirong Wu, Yuanjun Xiong, X Yu Stella, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In CVPR, 2018.\\n\\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan Zhang. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.\\n\\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\\n\\nNanxuan Zhao, Zhirong Wu, Rynson W. H. Lau, and Stephen Lin. What makes instance discrimination good for transfer learning? In ICLR, 2021.\\n\\nBolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors emerge in deep scene cnns. In ICLR, 2015.\"}"}
