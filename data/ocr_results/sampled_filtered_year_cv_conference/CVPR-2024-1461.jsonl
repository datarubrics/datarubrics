{"id": "CVPR-2024-1461", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. MMM can generate precise human motions given fine-grained textual descriptions while enabling motion editing applications. Blue frames represent conditioned motion input and red frames are the generated motion. Motion in-betweening can be performed by filling the gaps between keyframes or major motion points, conditioned on text or without conditions. Upper body part editing is done by fixing the lower body motion generated by one text prompt and altering the upper body motion according to another input prompt. An arbitrary long motion sequence can be generated according to a story (i.e., a sequence of text prompts), where MMM generates the motion for each prompt (red frames), while \u201challucinating\u201d the nature and smooth motion transitions (blue frames) between neighboring prompts (without being explicitly trained on motion transition datasets).\\n\\nAbstract\\n\\nRecent advances in text-to-motion generation using diffusion and autoregressive models have shown promising results. However, these models often suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this gap, we introduce MMM, a novel yet simple motion generation paradigm based on Masked Motion Model. MMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into a sequence of discrete tokens in latent space, and (2) a conditional masked motion transformer that learns to predict randomly masked motion tokens, conditioned on the pre-computed text tokens. By attending to motion and text tokens in all directions, MMM explicitly captures inherent dependency among motion tokens and semantic mapping between motion and text tokens. During inference, this allows parallel and iterative decoding of multiple motion tokens that are highly consistent with fine-grained text descriptions, therefore simultaneously achieving high-fidelity and high-speed motion generation. In addition, MMM has innate motion editability. By simply placing mask tokens in the place that needs editing, MMM automatically fills the gaps while guaranteeing smooth transitions between editing and non-editing parts. Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM surpasses current leading methods in generating high-quality motion (evidenced by superior FID scores of 0.08 and 0.429), while offering advanced editing features such as body-part modification, motion in-betweening, and the synthesis of long motion sequences. In addition, MMM is two orders of magnitude faster on a single mid-range GPU than editable motion diffusion models. Our project page is available at [https://exitudio.github.io/MMM-page/](https://exitudio.github.io/MMM-page/).\\n\\n1. Introduction\\n\\nText-driven human motion generation has recently become an emerging research focus due to the semantic richness and user-friendly nature of natural language descriptions, with its broad applications in animation, film, VR/AR, and robotics. However, generating high-fidelity motion that precisely aligns with text descriptors is challenging because of inherent differences between language and motion data distributions. To address this challenge, three predominant methods have been proposed, including (1) language-motion latent space alignment, (2) conditional diffusion model, and (3) conditional autoregressive model.\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2024-1461", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"quences are projected into separate latent spaces, forcibly aligned by imposing distance loss functions such as cosine similarity and KL losses \\\\[1, 12, 22, 23, 34, 41\\\\]. Due to unavoidable latent space misalignment, this method falls short of achieving high-fidelity motion generation, which requires the synthesized motion to accurately reflect the fine-grained textural descriptions. To this end, conditional diffusion and autoregressive models are proposed recently \\\\[15, 17, 19, 35, 38, 43, 44, 47\\\\]. Instead of brutally forcing latent space alignment, these models learn a probabilistic mapping from the textural descriptors to the motion sequences. However, the improvement in quality comes at the cost of motion generation speed and editability.\\n\\nMotion-space diffusion models learn text-to-motion mapping by applying diffusion processes to raw motion sequences conditioned on text inputs \\\\[17, 35, 38, 44\\\\]. The use of the raw motion data supports partial denoising on certain motion frames and body parts, naturally supporting semantic motion editing, such as motion inpainting and body part editing. However, the redundancy in raw data usually leads to high computational overhead and thus slow motion generation speed. A recent latent-space motion diffusion model \\\\[4\\\\] accelerates motion generation speed by compressing raw motion data into a single latent embedding. Nonetheless, this embedding hides rich temporal-spatial semantics present in the original motion data, hindering effective motion editing.\\n\\nFigure 2. The motion generation quality (FID score) and speed (AITS) comparisons between MMM and SOTA methods on HumanML3D dataset. The model closer to the origin is better. MMM achieves the best FID score (0.08) and the highest speed (0.081 AITS), while preserving motion editability. \u201c\u20dd\u201d represents editibility and \u201c\u00d7\u201d otherwise. All tests are performed on a single NVIDIA RTX A5000.\\n\\nCompared with motion diffusion models, motion autoregressive models further improve motion generation fidelity by modeling temporal correlations within motion sequences \\\\[15, 43, 47\\\\]. Following a training paradigm similar to large language models such as GPT \\\\[2\\\\], motion autoregressive models learn to predict and generate the next motion token conditioned on the text token and previously generated motion tokens. This process, known as autoregressive decoding, contributes to improved coherence and accuracy in motion generation. However, this sequential and unidirectional decoding approach not only results in significant delays in motion generation but also makes motion editing an extremely challenging, if not impossible, task.\\n\\nAs mentioned above, the existing text-driven motion generation models suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this critical issue, we introduce MMM, a novel motion generation paradigm based on conditional Motion Model. During the training phase, MMM follows a two-stage approach. In the first stage, a motion tokenizer is pretrained based on the vector quantized variational autoencoder (VQ-VAE) \\\\[37\\\\]. This tokenizer converts and quantizes raw motion data into a sequence of discrete motion tokens in latent space according to a motion codebook. A large-size codebook is learned to enable high-resolution quantization that preserves the fine-grained motion representations. In the second stage, a portion of the motion token sequence is randomly masked out, and a conditional masked transformer is trained to predict all the masked motion tokens concurrently, conditioned on both the unmasked ones and input text.\\n\\nBy attending to motion and text tokens in all directions, MMM explicitly captures inherent correlation among motion tokens and semantic mapping between motion and text tokens. This enables text-driven parallel decoding during inference, where in each iteration, the model concurrently and progressively predicts multiple high-quality motion tokens that are highly consistent with text descriptions and motion dynamics. This feature allows MMM to simultaneously achieve high-fidelity and high-speed motion generation. In addition, MMM has innate motion editability. By simply placing mask tokens in the place that needs editing, MMM automatically fills the gaps while ensuring smooth and natural transitions between editing and non-editing parts. Our contributions are summarized as follows.\\n\\n\u2022 We introduce the generative masked motion model, a novel yet simple text-to-motion generation paradigm. This model departs from diffusion and autoregressive models that currently dominate motion generation tasks.\\n\\n\u2022 We conduct extensive qualitative and quantitative evaluations on two standard text-to-motion generation datasets, HumanML3D \\\\[12\\\\] and KIT-ML \\\\[24\\\\]. It is shown in Figure 2 that our model outperforms current state-of-the-art methods in both motion generation quality and speed.\\n\\n\u2022 We demonstrate that our model enables fast and coherent motion editing via three tasks: motion in-between, upper body modification, and long sequence generation.\\n\\n2. Related Work\\n\\nText-driven Motion Generation. Early text-to-motion generation methods are mainly based on distribution alignment between motion and language latent spaces by applying certain loss functions, such as Kullback-Leibler (KL) divergences and contrastive losses. The representative methods...\"}"}
{"id": "CVPR-2024-1461", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3. Overall architecture of MMM.\\n\\n(a) Motion Tokenizer transforms the raw motion sequence into discrete motion tokens according to a learned codebook. (b) Conditional Masked Transformer learns to predict masked motion tokens, conditioned on word and sentence tokens obtained from CLIP text encoders. (c) Motion Generation starts from an empty canvas and the masked transformer concurrently and progressively predicts multiple high-confidence motion tokens.\\n\\nMethods include Language2Pose [1], TEMOS [22], T2M [12], MotionCLIP [34], TMR [23] and DropTriple [41]. Due to the significant variations between text and motion distributions, these latent space alignment approaches generally lead to unsatisfied motion generation quality.\\n\\nSince denoising diffusion models [33][13] have demonstrated notable success in vision generation tasks [14, 21, 29, 32], diffusion models have been adopted for motion generation, where MDM [35], MotionDiffuse [44] and FRAME [17] are recent attempts. However, these methods directly process diffusion models in raw and redundant motion sequences, thus resulting in extremely slow during inference. In our experiments, MDM [35] takes 28.11 seconds on average to generate a single motion on Nvidia RTX A5000 GPU. Inspired by pixel-based latent diffusion models [28], motion-space diffusion models MLD [4] mitigates this issue by applying diffusion process in low-dimensional motion latent space. Nonetheless, relying on highly compressed motion embedding for speed acceleration, MLD struggles to capture fine-grained details, greatly limiting its motion editability.\\n\\nInspired by the success of autoregressive models in language and image generations, such as GPT [2], DALL-E [27] and VQ-GAN [7, 39, 42], autoregressive motion models, T2M-GPT [43], AttT2M [47] and MotionGPT [15], have been recently developed to further improve motion generation quality. However, these autoregressive models utilize the causal attention for unidirectional and sequential motion token prediction, limiting its ability to model bidirectional dependency in motion data, increasing the training and inference time, and hindering the motion editability. To address these limitations, we aim to exploit masked motion modeling for real-time, editable and high-fidelity motion generation, drawing inspiration from the success of BERT-like masked language and image modeling [3, 5, 6, 8, 25, 45, 46].\\n\\nMotion Editing. MDM [35], FLAME [17], and Fg-T2M [38] introduce text-to-motion editing, demonstrating body part editing for specific body parts and motion in-between for temporal interval adjustments. They achieve this by adapting diffusion inpainting to motion data in both spatial and temporal domains. PriorMDM [30] uses a pretrained model from MDM [35] for three forms of motion composition: long sequence generation (DoubleTake), two-person generation (ComMDM), and fine-tuned motion control (DiffusionBlending). OmniControl [40] controls any joints at any time by combining spatial and temporal control together. EDGE [36] introduces editing for dance generation from music tasks with similar editing capabilities, including body part editing, motion in-betweening, and long sequence generation. GMD [16] employs the concept of editing spatial parts and temporal intervals to guide the position of the root joint (pelvic) in order to control the motion trajectory. Recently, COMODO [31] controls motion trajectory by integrating an RL-based control framework with the inpainting method. However, all current approaches utilize a diffusion process directly on motion space, which is slow and impractical for real-time applications.\\n\\n3. Method\\nOur goal is to design a text-to-motion synthesis paradigm that significantly improves synthesis quality, accelerates generation speed, and seamlessly preserves editability. Towards this goal, our paradigm, as depicted in Figure 3, consists of two modules: motion tokenizer (Section 3.1) and conditional masked motion transformer (Section 3.2). Motion tokenizer learns to transform 3D human motion into a sequence of discrete motion tokens without losing rich motion semantic information. Conditional masked motion transformer learns to predict masked motion tokens, conditioned on word and sentence tokens obtained from CLIP text encoders.\"}"}
{"id": "CVPR-2024-1461", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The transformer is trained to predict randomly masked motion tokens conditioned on the pre-computed text tokens. During inference, masked motion transformer allows parallel decoding of multiple motion tokens simultaneously, while considering the context from both preceding and succeeding tokens.\\n\\n3.1. Motion Tokenizer\\nThe objective of the first stage is to learn a discrete latent space by quantizing the embedding from encoder outputs \\\\( z \\\\) into the entries or codes of a learned codebook via vector quantization, as shown in Figure 3(a). The objective of vector-quantization is defined as\\n\\\\[\\nL_{VQ} = \\\\|sg(z) - e\\\\|_2^2 + \\\\beta \\\\|z - sg(e)\\\\|_2^2,\\n\\\\]\\nwhere \\\\( sg(\\\\cdot) \\\\) is the stop-gradient operator, \\\\( \\\\beta \\\\) is the hyperparameter for commitment loss, and \\\\( e \\\\) is a codebook vector from codebook \\\\( E \\\\) \\\\( (e \\\\in E) \\\\). The closest Euclidean distance of the embedding \\\\( z \\\\) and codebook vector index is calculated by\\n\\\\[\\ni = \\\\text{argmin}_j \\\\|z - E_j\\\\|_2^2.\\n\\\\]\\nTo preserve the fine-grained motion representations, we adopt a large codebook with a size of 8192 to reduce the information loss during embedding quantization. Our experiments show that large codebook size with a suitable code embedding dimension can lead to improved motion generation quality. However, using large codebook can aggravate codebook collapse, where the majority of tokens are assigned to just a few codes, rendering the remaining codebook inactive. To boost codebook usage, we adopt the factorized code, which decouples code lookup and code embedding to stabilize large-size codebook learning [42]. In addition, moving averages during codebook update and resetting dead codebooks are also adopted, which are often employed for enhancing codebook utilization in VQ-VAE and VQ-GAN [7, 39]. These schemes together serve to transform 3D human motion into a sequence of discrete motion tokes in a robust and efficient manner.\\n\\n3.2. Conditional Masked Motion Model\\nText-conditioned Masked Transformer. During training, the motion tokens are first obtained by passing the output of the encoder through the vector quantizer. The motion token sequence, text embeddings, and special-purpose tokens serve as the inputs of a standard multi-layer transformer. Specifically, we obtain sentence embeddings from the pre-trained CLIP model \\\\([26]\\\\) to capture the global relationships between the entire sentence and motion. The sentence embedding is then prepended to the motion tokens. Due to the nature of self-attention in transformers, all motion tokens are learned in relation to the sentence embedding. In addition, we obtain word embeddings from the same CLIP model \\\\([26]\\\\) to capture the local relationships between each word and motion through cross-attention\\n\\\\[\\n= \\\\text{softmax} \\\\frac{Q_{motion} K_{Tword}}{\\\\sqrt{D}} V_{word} [47].\\n\\\\]\\n[MASK], [PAD], and [END] are learnable special-purpose tokens. During training, the [MASK] token is used to represent input corruption, and the model learns to predict the actual tokens in place of [MASK]. During inference, [MASK] tokens not only serve as placeholders for motion token generation but also prove useful in various tasks by placing the [MASK] tokens where editing is required, as detailed in Section 4. [PAD] token is used to fill up shorter motion sequences, allowing computation of batches with multiple sequences of varying lengths. Lastly, the [END] token is appended to the input tokens after the last token, providing the model with an explicit signal of the motion's endpoint.\\n\\nTraining Strategy and Loss. The motion token sequence \\\\( Y \\\\) is represented as\\n\\\\[\\nY = [e_i]_{L_i=1}^L,\\n\\\\]\\nwhere \\\\( L \\\\) denotes the sequence length. We randomly mask out \\\\( r \\\\times L \\\\) tokens and replace them with learnable [MASK] tokens, where \\\\( r \\\\) is the masking ratio following a uniform distribution truncated between \\\\( \\\\alpha \\\\) and 1. Then, the original motion token sequence \\\\( Y \\\\) is updated with [MASK] tokens to form the corrupted motion sequence \\\\( Y_M \\\\). This corrupted sequence along with text embedding \\\\( W \\\\) are fed into a text-conditioned masked transformer to reconstruct input motion token sequence with reconstruction probability or confidence equal to \\\\( p_{y_i|Y_M, W} \\\\). The objective is to minimize the negative log-likelihood of the predicted masked tokens conditioned on text:\\n\\\\[\\nL_{mask} = -\\\\mathbb{E}_{Y \\\\in D} \\\\left[ \\\\sum_{i=1}^L \\\\log p(y_i|Y_M, W) \\\\right].\\n\\\\]\\n\\nInference via Parallel Decoding. To decode the motion tokens during inference, we initiate the process by inputting all [MASK] tokens, representing an empty canvas then progressively predict more tokens per iteration. Next, iterative parallel decoding is performed, where in each iteration, the transformer masks out the subset of motion tokens that the model is least confident about and then predicts these masked tokens in parallel in the next iteration. The number of masked tokens \\\\( n_M \\\\) is determined by a masked scheduling function, i.e., a decaying function of the iteration \\\\( t \\\\). The decaying function is chosen because at early iterations, there is high uncertainty in the predictions and thus the model begins with a large masking ratio and only keeps a small number of tokens with high prediction confidence. As the generation process proceeds, the masking ratio decreases due to the increase of context information from previous iterations. We experiment both linear\\n\\\\[\\nn_M = L (\\\\frac{T - t}{T}) \\\\quad [8]\\n\\\\]\\nand cosine function\\n\\\\[\\nn_M = L \\\\cos \\\\left( \\\\frac{1}{2} \\\\pi t / T \\\\right) \\\\quad [3],\\n\\\\]\\nwhere cosine function yields better performance. To obtain the final \\\\( n_M \\\\), the length of the generated motion sequence \\\\( L \\\\) should also be available. To address this issue, we adopt a pretrained predictor that estimates the motion sequence length based on the input text \\\\([12]\\\\).\"}"}
{"id": "CVPR-2024-1461", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Motion Editing\\n\\nAs illustrated in Figure 4, the advantage of our masked motion modeling lies in its innate ability to edit motion. \\n\\nMotion In-betweening. Thanks to its mask bidirectional decoding nature, we can easily place the [MASK] tokens wherever editing is needed regardless of past and future context. As an important editing task, the motion in-between involves interpolating or filling the gaps between keyframes or major motion points to create a smooth, continuous 3D animation. Since the model has already learned all possible random temporal masking combinations during training, motion in-between can be accomplished without any additional training.\\n\\nLong Sequence Generation. Due to the limited length of motion data in the available HumanML3D [12] and KIT [24] datasets, where no sample exceeds a duration of 10 seconds, generating arbitrarily long motions poses a challenge. To address this, we use the trained masked motion model as a prior for long motion sequence synthesis without additional training. Particularly, given a story that consists of multiple text prompts, our model first generates the motion token sequence for each prompt. Then, we generate transition motion tokens conditioned on the end of the previous motion sequence and the start of the next motion sequence. Diffusion methods such as DoubleTakes in PriorMDM [30] require multiple steps (up to 1,000 steps) to generate transition and average the spatial differences between the previous and next motion before generating the transition motion. Our approach only needs a single step to generate realistic and natural transition motions.\\n\\nUpper Body Editing. To enable body part editing, we pretrain the upper and lower body part tokenizers separately, each with its own encoders and decoders. The embedding size of each motion token is half of the regular full-body embedding size. In the second stage, the upper and lower tokens are concatenated back to form full body embeddings. Therefore, the embedding size and the conditional masked transformer remain unchanged. Ideally, we can train the transformer by predicting the masked upper body tokens, conditioned on the text for upper body motion along with the lower body tokens. However, the generated motions are inconsistent with the text. To address this problem, we introduce random [MASK] tokens into lower body part motion sequence via light corruptions so that the transformer can better learn the spatial and temporal dependency of the whole body motions. Thus, the training loss can be written as:\\n\\n$$L_{up} = -\\\\mathbb{E}_{Y \\\\in D} \\\\left[ \\\\sum_{i=1}^{L-1} \\\\log p(y_{up,i} | Y_{up,M}, Y_{down,M}, W) \\\\right] \\\\quad \\\\text{(3)}$$\\n\\nwhere $Y_{up,M}$ denotes the upper tokens with mask and $Y_{down,M}$ is the lower tokens with mask. It is important to note that the [MASK] tokens of the lower part remain unchanged throughout all iterations.\\n\\n5. Experiments\\n\\nIn this section, we present comparisons to evaluate our models on both quality and time efficiency. In Section 5.1, following the standard evaluation from [12] across multiple datasets, we observe that our method consistently outperforms the current state-of-the-art methods. Moreover, in Section 5.2, evaluating with the time efficiency metric from MLD [4], our method exhibits shorter inference times, both on average and with respect to motion lengths.\\n\\nDatasets. We conduct experiments on two standard datasets for text-to-motion generation: HumanML3D [12] and KIT Motion-Language (KIT-ML) [24] and follow the evaluation protocol proposed in [12]. KIT Motion-Language (KIT-ML) [24] contains 3911 motion sequences and 6,278 text descriptions, with an average of 9.5 words per annotation. One to four textual descriptions are provided for each motion clip, with an average description length of around 8 sentences. Motion sequences are selected from KIT [24] and CMU [41] datasets but have been downsampled to a rate of 12.5 frames per second (FPS). The dataset is split into training, validation, and test sets, with respective proportions of 80%, 5%, and 15%.\\n\\nHumanML3D [12] is currently the largest 3D human text-motion dataset, covers a diverse array of everyday human actions, including activities like exercising and dancing. The dataset comprises 14,616 motion sequences and 44,970 text descriptions. The entirety of the textual descriptions consists of 5,371 unique words. The motion sequences are originally from AMASS [20] and HumanAct12 [10]. Each sequence is adjusted to 20 frames per second (FPS) and trimmed to a 10-second duration, resulting in durations ranging from 2 to 10 seconds. Each motion clip is paired with at least three corresponding descriptions, with the average description length being approximately 12 words. Similar to KIT, the dataset is split into training, validation, and test sets, with respective proportions of 80%, 5%, and 15%.\\n\\nEvaluation Metrics. The embeddings of textual descriptions and motion are encoded by pre-trained models from [12] for evaluation metrics as proposed by [12] with\"}"}
{"id": "CVPR-2024-1461", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Qualitative comparison of state-of-the-art methods with textual description: \u201ca person walks forward then turns completely around and does a cartwheel.\u201d\\n\\nMDM is one of the most representative motion-space diffusion models. MLD is the first and SOTA latent-space motion diffusion model. T2M-GPT is the first and SOTA autoregressive motion model. Top-left: MDM [35] does not execute cartwheel motion. Top-right: MLD [4] generates unrealistic motion and lacks a complete cartwheel motion. Middle-left: the trajectory of T2M-GPT [43] is not \u201ccompletely around\u201d. Middle-right: our method generates realistic motion and trajectory compared to the ground truth on the bottom. Trajectories start from blue and end in red.\\n\\nWrong upper body position\\n\\nOurs\\n\\nFrame 147\\nFrame 146\\n\\nNot smooth transition\\n\\nMDM\\nOurs\\n\\nFrame 147\\nFrame 146\\n\\nQualitative comparison of upper body editing, generating upper body part based on the text \u201ca man throws a ball\u201d conditioned on lower body part of \u201ca man rises from the ground, walks in a circle and sits back down on the ground.\u201d\\n\\nFrame 147\\nFrame 146\\n\\nNot smooth transition\\n\\nMDM\\nOurs\\n\\nQualitative comparison of motion in-betweening, generating 50% motion in the middle (frame 50-146) based on the text \u201ca man throws a ball\u201d conditioned on first 25% and last 25% of motion of \u201ca person walks backward, turns around and walks backward the other way.\u201d Compared with MDM, MMM achieves smoother and more natural transitions between the conditioned and generated motions (at frames 146 and 147) five metrics.\\n\\nR-precision and Multimodal Distance (MM-Dist) measure how well the generated motions align with the input prompts. Top-1, Top-2, and Top-3 accuracy are reported for R-Precision. Frechet Inception Distance (FID) measures the distribution distance between the generated and ground truth motion features. Diversity is calculated by averaging Euclidean distances of random samples from 300 pairs of motion, and MultiModality (MModality) represents the average variance for a single text prompt by computing Euclidean distances of 10 generated pairs of motions.\\n\\n5.1. Comparison to State-of-the-art Approaches\\nWe evaluate our methods with state-of-the-art approaches [4, 9, 11, 12, 18, 22, 35, 38, 43, 44, 47] on HumanML3D [12] and KIT-ML [24]. We maintain the same architecture and hyperparameters for the evaluation on both datasets.\\n\\nQuantitative comparisons. Following [12], we report the average over 20 repeated generations with 95% confidence interval. Table 1 and Table 2 present evaluations on HumanML3D [12] and KIT-ML [24] dataset respectively, in comparison to state-of-the-art (SOTA) approaches. Our method consistently performs best in terms of FID and Multimodal Distance. For the R-Precision and Diversity metric, our method still shows competitive results when compared to SOTA methods. On HumanML3D [12], as shown in Table 1, our method outperforms all SOTA methods in most of the metrics. Specifically, our method excels in Top-1, Top-2, and Top-3 R-Precision, FID, Multimodal Distance, and Diversity metrics. It is worth noting that the HumanML3D [12] dataset is significantly larger than the KIT-ML [24] dataset, which suggests that our method tends to perform better with more data.\\n\\nQualitative comparison. Figure 5 shows visual result of the generated motion by textual description, \u201ca person walks forward then turns completely around and does a cartwheel.\u201d MDM [35] fails to generate a cartwheel motion. MLD [4] flips in an unrealistic motion and lacks a complete cartwheel motion.\"}"}
{"id": "CVPR-2024-1461", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison of text-conditional motion synthesis on KIT-ML [24] test set.\\n\\n| Method             | R-Precision Top-1 | R-Precision Top-2 | R-Precision Top-3 | R-Precision Top-4 |\\n|--------------------|------------------|------------------|------------------|------------------|\\n| Fg-T2M             | 0.85             | 0.64             | 0.57             | 0.43             |\\n| MMM (ours)         | 0.91             | 0.79             | 0.69             | 0.57             |\\n| T2M-GPT [43]       | 0.71             | 0.53             | 0.45             | 0.37             |\\n| TEMOS              | 0.68             | 0.51             | 0.42             | 0.34             |\\n| MDM                | 0.79             | 0.63             | 0.54             | 0.46             |\\n| MotionDiffuse      | 0.75             | 0.59             | 0.50             | 0.42             |\\n\\nThe gray arrow (\u2192) indicates that the closer the result is to real motion, the better. Red and Blue indicate the best.\\n\\nSuch high inference speed offers a significant advantage. The average duration of HumanML3D dataset is 9.6 seconds, AITS being two orders of magnitude faster than motion-space diffusion model (MLD), while faster than autoregressive motion models (T2M-GPT and Fg-T2M). All tests are performed on a single NVIDIA RTX 3080 Ti.\"}"}
{"id": "CVPR-2024-1461", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Ablation results on the masking ratio during training.\\n\\n| Masking Ratio | R-Precision | Top-1 \u2191 | FID \u2193 | MM-Dist \u2193 | Diversity \u2192 | MModality \u2191 |\\n|---------------|-------------|--------|-------|-----------|-------------|-------------|\\n| Uniform 0-1   | 0.513       | 0.097  | 2.923 | 9.640     | 1.097       |             |\\n| Uniform .3-1  | 0.520       | 0.089  | 2.901 | 9.639     | 1.149       |             |\\n| Uniform .5-1  | 0.515       | 0.089  | 2.926 | 9.577     | 1.226       |             |\\n| Uniform .7-1  | 0.505       | 0.098  | 2.975 | 9.523     | 1.245       |             |\\n\\nTable 5. Ablation results on speed and quality influenced by the mask scheduling during inference.\\n\\n| # of iterations | AITS (seconds) | R-Precision | Top-1 \u2191 | FID \u2193 | MM-Dist \u2193 | Diversity \u2192 | MModality \u2191 |\\n|-----------------|----------------|-------------|--------|-------|-----------|-------------|-------------|\\n| 5               | 0.043          | 0.505       | 0.169  | 3.003 | 9.370     |             |             |\\n| 10              | 0.081          | 0.515       | 0.089  | 2.926 | 9.577     | 1.226       |             |\\n| 15              | 0.118          | 0.516       | 0.091  | 2.919 | 9.576     | 1.134       |             |\\n| 20              | 0.149          | 0.518       | 0.096  | 2.912 | 9.590     | 1.058       |             |\\n| 25              | 0.176          | 0.517       | 0.102  | 2.911 | 9.682     | 1.008       |             |\\n| 30              | 0.205          | 0.515       | 0.100  | 2.908 | 9.698     | 0.937       |             |\\n| 49 (Linear)     | 0.345          | 0.517       | 0.109  | 2.911 | 9.716     | 0.952       |             |\\n\\nTable 6. Ablation results on quality influenced by the number of codes and the code dimension.\\n\\n| # of code x code dimension | R-Precision | Top-1 \u2191 | FID \u2193 | MM-Dist \u2193 | Diversity \u2192 | MModality \u2191 |\\n|----------------------------|-------------|--------|-------|-----------|-------------|-------------|\\n| 512 x 512                  | 0.499       | 0.108  | 3.077 | 9.683     |             |             |\\n| 1024 x 256                 | 0.496       | 0.085  | 3.001 | 9.641     |             |             |\\n| 2048 x 128                 | 0.504       | 0.093  | 3.033 | 9.795     |             |             |\\n| 4096 x 64                  | 0.505       | 0.080  | 3.035 | 9.697     |             |             |\\n| 8192 x 32                  | 0.503       | 0.075  | 3.027 | 9.697     |             |             |\\n\\n6. Ablation Study\\n\\nThe key success of our method lies in mask modeling. To understand how mask scheduling during training and inference impacts performance, we conduct ablation experiments using the same evaluation on HumanML3D [12].\\n\\nMasking Ratio during Training. During training, we apply a random masking ratio drawn from an uniform distribution bounded between $\\\\alpha$ and 1. The larger $\\\\alpha$ indicates that more aggressive masking is applied during training. As a result, the model has to predict a large number of masked tokens more based on the text prompt due to the reduced context information from unmasked motion tokens. It is shown in Table 4 that too aggressive ($\\\\alpha = 0.7$) or too mild ($\\\\alpha = 0.5$) masking strategy is not beneficial for generating high-fidelity motions with sufficient diversity.\\n\\nIteration Number. During inference, the number of iterations directly affects the speed and the quality of the generation. The number of iterations is the maximum number of iterations for the longest motion sequence in the dataset (196 frames). As shown in Table 5, increasing the number of iterations leads to higher generation latency with slightly improved R-precision and MM-dist. However, using a small number of iterations like 10, the model already achieves the lowest FID score along with high R-precision and low MM-dist. The lowest FID score means the best overall visual quality of the generated motion, ensuring that its realism and naturalness is very close to the genuine and ground-truth human movements. High R-precision and low MM-dist indicate the precise alignment between the generated motion and the text description.\\n\\nCodebook Size. By adopting codebook factorization, we can learn a large-size codebook for high-resolution motion embedding quantization to preserve fine-grained motion representations, which directly affect motion generation quality. In particular, codebook factorization decouples code lookup and code embedding, where codebook uses low-dimensional latent space for code lookup and the matched code is projected back to high-dimensional embedding space. Table 6 shows that motion reconstruction quality is improved by increasing the number of code entries in codebook from 512 to 8192, while reducing the dimension of the codebook latent space from 512-d to 32-d.\\n\\n7. Conclusion\\n\\nIn this work, we propose the generative masked motion model (MMM) to synthesize human motion based on textual descriptions. MMM consists of two key components: (1) a motion tokenizer converting 3D human motion into discrete latent space tokens, and (2) a conditional masked motion transformer predicting masked motion tokens based on text tokens. MMM enables parallel and iteratively-refined decoding for high-fidelity and fast motion generation. MMM has inherent motion editability. Extensive experiments demonstrate that MMM outperforms state-of-the-art methods both qualitatively and quantitatively. MMM is at least two times faster than autoregressive motion models and two orders of magnitude faster than diffusion models on raw motion sequences.\"}"}
{"id": "CVPR-2024-1461", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Chaitanya Ahuja and Louis-Philippe Morency. Language2pose: Natural language grounded pose forecasting. In 2019 International Conference on 3D Vision (3DV), pages 719\u2013728, 2019.\\n\\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.\\n\\n[3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11305\u201311315, 2022.\\n\\n[4] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, and Gang Yu. Executing your commands via motion diffusion in latent space. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18000\u201318010, 2022.\\n\\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019.\\n\\n[6] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. ArXiv, abs/2204.14217, 2022.\\n\\n[7] Patrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. Taming transformers for high-resolution image synthesis. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12868\u201312878, 2020.\\n\\n[8] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. In Conference on Empirical Methods in Natural Language Processing, 2019.\\n\\n[9] Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, and P. Slusallek. Synthesis of compositional animations from textual descriptions. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1376\u20131386, 2021.\\n\\n[10] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Action2motion: Conditioned generation of 3d human motions. Proceedings of the 28th ACM International Conference on Multimedia, 2020.\\n\\n[11] Chuan Guo, Xinxin Xuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. ArXiv, abs/2207.01696, 2022.\\n\\n[12] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5142\u20135151, 2022.\\n\\n[13] Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion probabilistic models. ArXiv, abs/2006.11239, 2020.\\n\\n[14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models. ArXiv, abs/2210.02303, 2022.\\n\\n[15] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as a foreign language. ArXiv, abs/2306.14795, 2023.\\n\\n[16] Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang. Gmd: Controllable human motion synthesis via guided diffusion models. ArXiv, abs/2305.12577, 2023.\\n\\n[17] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Freeform language-based motion synthesis & editing. In AAAI Conference on Artificial Intelligence, 2022.\\n\\n[18] Hanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi, and Xinchao Wang. Priority-centric human motion generation in discrete latent space. ArXiv, abs/2308.14480, 2023.\\n\\n[19] Yunhong Lou, Linchao Zhu, Yaxiong Wang, Xiaohan Wang, and Yezhou Yang. Diversemotion: Towards diverse human motion generation via discrete diffusion. ArXiv, abs/2309.01372, 2023.\\n\\n[20] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. Amass: Archive of motion capture as surface shapes. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 5441\u20135450, 2019.\\n\\n[21] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, 2021.\\n\\n[22] Mathis Petrovich, Michael J. Black, and G\u00fcll Varol. Temos: Generating diverse human motions from textual descriptions. ArXiv, abs/2204.14109, 2022.\\n\\n[23] Mathis Petrovich, Michael J. Black, and G\u00fcll Varol. Tmr: Text-to-motion retrieval using contrastive 3d human motion synthesis. ArXiv, abs/2305.00976, 2023.\\n\\n[24] Matthias Plappert, Christian Mandery, and Tamim Asfour. The KIT motion-language dataset. Big Data, 4(4):236\u2013252, 2016.\\n\\n[25] Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, and Lei Li. Glancing transformer for non-autoregressive neural machine translation. In Annual Meeting of the Association for Computational Linguistics, 2020.\\n\\n[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021.\"}"}
{"id": "CVPR-2024-1461", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021.\\n\\nRobin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10674\u201310685, 2021.\\n\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. ArXiv, abs/2205.11487, 2022.\\n\\nYonatan Shafir, Guy Tevet, Roy Kapon, and Amit H. Bermano. Human motion diffusion as a generative prior. ArXiv, abs/2303.01418, 2023.\\n\\nYi Shi, Jingbo Wang, Xuekun Jiang, and Bo Dai. Controlable motion diffusion model. ArXiv, abs/2306.00416, 2023.\\n\\nUriel Singer, Adam Polyak, Thomas Hayes, Xiaoyue Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. ArXiv, abs/2209.14792, 2022.\\n\\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. ArXiv, abs/2010.02502, 2020.\\n\\nGuy Tevet, Brian Gordon, Amir Hertz, Amit H. Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In European Conference on Computer Vision, 2022.\\n\\nGuy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H. Bermano. Human motion diffusion model. ArXiv, abs/2209.14916, 2022.\\n\\nJo-Han Tseng, Rodrigo Castellon, and C. Karen Liu. Edge: Editable dance generation from music. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 448\u2013458, 2022.\\n\\nAar\u00f3n van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. ArXiv, abs/1711.00937, 2017.\\n\\nYin Wang, Zhiying Leng, Frederick W. B. Li, Shun-Cheng Wu, and Xiaohui Liang. Fg-t2m: Fine-grained text-driven human motion generation via diffusion model. ArXiv, abs/2309.06284, 2023.\\n\\nWill Williams, Sam Ringer, Tom Ash, John Hughes, David Macleod, and Jamie Dougherty. Hierarchical quantized autoencoders. ArXiv, abs/2002.08111, 2020.\\n\\nYiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. Omnicontrol: Control any joint at any time for human motion generation. 2023.\\n\\nSheng Yan, Yang Liu, Haoqiang Wang, Xin Du, Mengyuan Liu, and Hong Liu. Cross-modal retrieval for motion and text via doptriple loss. 2023.\\n\\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. ArXiv, abs/2110.04627, 2021.\\n\\nJianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xiaodong Shen. Generating human motion from textual descriptions with discrete representations. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14730\u201314740, 2023.\\n\\nMingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. ArXiv, abs/2208.15001, 2022.\\n\\nZhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li, Ming Ding, Jianbo Tang, Jingren Zhou, and Hongxia Yang. M6-ufc: Unifying multi-modal controls for conditional image synthesis via non-autoregressive generative transformers. 2021.\\n\\nZhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li, Ming Ding, Jie Tang, Jingren Zhou, and Hongxia Yang. Ufc-bert: Unifying multi-modal controls for conditional image synthesis. In Neural Information Processing Systems, 2021.\\n\\nChongyang Zhong, Lei Hu, Zihao Zhang, and Shihong Xia. Attt2m: Text-driven human motion generation with multi-perspective attention mechanism. ArXiv, abs/2309.00796, 2023.\"}"}
