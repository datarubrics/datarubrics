{"id": "CVPR-2023-1530", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-image diffusion models with ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\\n\\n[2] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021.\\n\\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\\n\\n[4] Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Perez. Zero-shot semantic segmentation. Advances in Neural Information Processing Systems, 32, 2019.\\n\\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer, 2020.\\n\\n[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9650\u20139660, 2021.\\n\\n[7] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\\n\\n[8] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9640\u20139649, 2021.\\n\\n[9] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12475\u201312485, 2020.\\n\\n[10] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1290\u20131299, 2022.\\n\\n[11] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. Advances in Neural Information Processing Systems, 34:17864\u201317875, 2021.\\n\\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\\n\\n[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021.\\n\\n[14] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11583\u201311592, 2022.\\n\\n[15] Zheng Ding, Jieke Wang, and Zhuowen Tu. Open-vocabulary panoptic segmentation with maskclip. arXiv preprint arXiv:2208.08984, 2022.\\n\\n[16] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-based generative modeling with critically-damped langevin diffusion. arXiv preprint arXiv:2112.07068, 2021.\\n\\n[17] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li. Learning to prompt for open-vocabulary object detection with vision-language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14084\u201314093, 2022.\\n\\n[18] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021.\\n\\n[19] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303\u2013338, 2010.\\n\\n[20] Danil Galeev, Konstantin Sofiiuk, Danila Rukhovich, Mikhail Romanov, Olga Barinova, and Anton Konushin. Learning high-resolution domain-specific representations with a gan generator. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pages 108\u2013118. Springer, 2021.\\n\\n[21] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2918\u20132928, 2021.\\n\\n[22] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Open-vocabulary image segmentation. arXiv preprint arXiv:2112.12143, 2021.\\n\\n[23] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.\\n\\n[24] Tanmay Gupta, Arash Vahdat, Gal Chechik, Xiaodong Yang, Jan Kautz, and Derek Hoiem. Contrastive learning for weakly supervised phrase grounding. In European Conference on Computer Vision, pages 752\u2013768. Springer, 2020.\\n\\n[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022.\\n\\n[26] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.\"}"}
{"id": "CVPR-2023-1530", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.\\n\\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. If you use this software, please cite it as below.\\n\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.\\n\\nAlexia Jolicoeur-Martineau, R\u00e9mi Pich\u00e9-Taillefer, R\u00e9mi Tachet des Combes, and Ioannis Mitliagkas. Adversarial score matching and improved sampling for image generation. arXiv preprint arXiv:2009.05475, 2020.\\n\\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.\\n\\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8110\u20138119, 2020.\\n\\nAlexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Panoptic feature pyramid networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6399\u20136408, 2019.\\n\\nAlexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll\u00e1r. Panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9404\u20139413, 2019.\\n\\nBoyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Ren\u00e9 Ranftl. Language-driven semantic segmentation. arXiv preprint arXiv:2201.03546, 2022.\\n\\nDaiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Sanja Fidler, and Antonio Torralba. Bigdatasetgan: Synthesizing imagenet with pixel-wise annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21330\u201321340, 2022.\\n\\nFeng Li, Hao Zhang, Shilong Liu, Lei Zhang, Lionel M Ni, Heung-Yeung Shum, et al. Mask dino: Towards a unified transformer-based framework for object detection and segmentation. arXiv preprint arXiv:2206.02777, 2022.\\n\\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086, 2022.\\n\\nLiunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965\u201310975, 2022.\\n\\nYangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. arXiv preprint arXiv:2110.05208, 2021.\\n\\nYanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4804\u20134814, 2022.\\n\\nYanwei Li, Hengshuang Zhao, Xiaojuan Qi, Liwei Wang, Zeming Li, Jian Sun, and Jiaya Jia. Fully convolutional networks for panoptic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 214\u2013223, 2021.\\n\\nZhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Ping Luo, and Tong Lu. Panoptic segformer: Delving deeper into panoptic segmentation with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1280\u20131289, 2022.\\n\\nTsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2117\u20132125, 2017.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.\\n\\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021.\\n\\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976\u201311986, 2022.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\\n\\nChenlin Meng, Yutong He, Yang Song, Jiaming Song, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2021.\"}"}
{"id": "CVPR-2023-1530", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection with vision transformers. arXiv preprint arXiv:2205.06230, 2022.\\n\\nRoozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 891\u2013898, 2014.\\n\\nNorman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image pre-training. arXiv preprint arXiv:2112.12750, 2021.\\n\\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\\n\\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.\\n\\nZhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language super-vision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020.\\n\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\\n\\nJiawei Ren, Cunjun Yu, Zhongang Cai, Mingyuan Zhang, Chongsong Chen, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Refine: Prediction fusion network for panoptic segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2477\u20132485, 2021.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\\n\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\\n\\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\\n\\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.\\n\\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.\\n\\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nYang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438\u201312448, 2020.\\n\\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.\\n\\nSanjay Subramanian, Will Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, and Anna Rohrbach. Reclip: A strong zero-shot baseline for referring expression comprehension. arXiv preprint arXiv:2204.05991, 2022.\\n\\nHugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit iii: Revenge of the vit. arXiv preprint arXiv:2204.07118, 2022.\\n\\nNontawat Tritrong, Pitchaporn Rewatbowornwong, and Supasorn Suwajanakorn. Repurposing gans for one-shot semantic part segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4475\u20134485, 2021.\\n\\nArash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in Neural Information Processing Systems, 34:11287\u201311302, 2021.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nHuiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end panoptic segmentation with mask transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5463\u20135474, 2021.\\n\\nYongqin Xian, Subhabrata Choudhury, Yang He, Bernt Schiele, and Zeynep Akata. Semantic projection network for zero-and few-label semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8256\u20138265, 2019.\\n\\nJiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. GroupViT: Semantic Segmentation Emerges from Text Supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18134\u201318144, 2022.\"}"}
{"id": "CVPR-2023-1530", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, and Xiang Bai. A simple baseline for zero-shot semantic segmentation with pre-trained vision-language model. arXiv preprint arXiv:2112.14757, 2021.\\n\\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\\n\\nQihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. k-means mask transformer. In European Conference on Computer Vision, pages 288\u2013307. Springer, 2022.\\n\\nYuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Open-vocabulary detr with conditional matching. arXiv preprint arXiv:2203.11876, 2022.\\n\\nAlireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14393\u201314402, 2021.\\n\\nAndy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\\n\\nWenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change Loy. K-net: Towards unified image segmentation. Advances in Neural Information Processing Systems, 34:10326\u201310338, 2021.\\n\\nYuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, and Sanja Fidler. Datasetgan: Efficient labeled data factory with minimal human effort. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10145\u201310155, 2021.\\n\\nYiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16793\u201316803, 2022.\\n\\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127(3):302\u2013321, 2019.\\n\\nChong Zhou, Chen Change Loy, and Bo Dai. Denseclip: Extract free dense labels from clip. arXiv preprint arXiv:2112.01071, 2021.\\n\\nXingyi Zhou, Rohit Girdhar, Armand Joulin, Phillip Kr\u00a8ahenb\u00a8uhl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. arXiv preprint arXiv:2201.02605, 2022.\\n\\nYufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17907\u201317917, 2022.\\n\\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223\u20132232, 2017.\"}"}
{"id": "CVPR-2023-1530", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"categories' names as $C_{\\\\text{train}} \\\\equiv [T(c_1), T(c_2), \\\\ldots, T(c_{K_{\\\\text{train}}})]$, (4)\\n\\nwhere the category name $c_k \\\\in C_{\\\\text{train}}$. Then we compute the probability of the mask embedding feature $z_i$ belonging to one of the $K_{\\\\text{train}}$ classes via a classification loss as:\\n\\n$$L_C = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\text{CrossEntropy}(p(z_i, C_{\\\\text{train}}), y_i),$$\\n\\n$$p(z_i, C_{\\\\text{train}}) = \\\\text{Softmax}(z_i \\\\cdot T(C_{\\\\text{train}})/\\\\tau),$$\\n\\n(5)\\n\\n(6)\\n\\nwhere $\\\\tau$ is a learnable temperature parameter.\\n\\nImage Caption Supervision\\n\\nHere, we assume that we do not have any category labels associated with each annotated mask during training. Instead, we have access to a natural language caption for each image, and the model learns to classify the predicted mask embedding features using the image caption alone. To do so, we extract the nouns from each caption and treat them as the grounding category labels for their corresponding paired image. Following [22, 24, 82], we employ a grounding loss to supervise the prediction of the masks' category labels. Specifically, given the image-caption pair $\\\\langle x(m), s(m) \\\\rangle$, suppose that there are $K_{\\\\text{word}}$ nouns extracted from $s(m)$, denoted as $C_{\\\\text{word}} = \\\\{w_k\\\\}_{k=1}^{K_{\\\\text{word}}}$. Suppose further that we sample $B$ image-caption pairs $\\\\{\\\\langle x(m), s(m) \\\\rangle\\\\}_{m=1}^{B}$ to form a batch. To compute the grounding loss, we compute the similarity between each image-caption pair as:\\n\\n$$g(x(m), s(m)) = \\\\frac{1}{K_{\\\\text{word}}} \\\\sum_{k=1}^{K_{\\\\text{word}}} \\\\sum_{i=1}^{N} p(z_i, C_{\\\\text{word}}) \\\\cdot \\\\langle z_i, T(w_k) \\\\rangle,$$\\n\\n(7)\\n\\nwhere $z_i$ and $T(w_k)$ are vectors of the same dimension and $p(z_i, C_{\\\\text{word}})$ is the $k$-th element of the vector defined in Eq. 6 after Softmax. This similarity function encourages each noun to be grounded by one or a few masked regions of the image and avoids penalizing the regions that are not grounded by any word at all. Similar to the image-text contrastive loss in [30, 57], the grounding loss is defined by:\\n\\n$$L_G = -\\\\frac{1}{B} \\\\sum_{m=1}^{B} \\\\log \\\\exp\\\\left(\\\\frac{g(x(m), s(m))}{\\\\tau}\\\\right) \\\\prod_{n=1}^{B} \\\\exp\\\\left(\\\\frac{g(x(n), s(m))}{\\\\tau}\\\\right) - \\\\frac{1}{B} \\\\sum_{m=1}^{B} \\\\log \\\\exp\\\\left(\\\\frac{g(x(m), s(m))}{\\\\tau}\\\\right) \\\\prod_{n=1}^{B} \\\\exp\\\\left(\\\\frac{g(x(m), s(n))}{\\\\tau}\\\\right),$$\\n\\n(8)\\n\\nwhere $\\\\tau$ is a learnable temperature parameter. Finally, note that we train the entire ODISE model with either $L_C$ or $L_G$, together with the class-agnostic binary mask loss. In our experiments, we explicitly state which of these two supervision signals (label or caption) we use for training ODISE when comparing to the relevant prior works.\\n\\nFigure 3. Open-Vocabulary Inference Pipeline. To classify each mask embedding into the testing categories $C_{\\\\text{test}}$, we compute its similarity to the text encoder $T$'s embeddings of category names. Besides the mask embeddings from the text-to-image diffusion model $\\\\{z_i\\\\}_{i=1}^{N}$, we also perform masked pooling of the features of an image encoder $V$ from a text-image discriminative model to get $\\\\{z'_i\\\\}_{i=1}^{N}$. We fuse the predictions of the diffusion model (blue solid path) and the discriminative model (grey dash path) with a geometric mean.\\n\\n3.6. Open-Vocabulary Inference\\n\\nDuring inference (Fig. 3), the set of names of the test categories $C_{\\\\text{test}}$ is available. The test categories may be different from the training ones. Additionally, no caption/labels are available for a test image. Hence we pass it through the implicit captioner to obtain its implicit caption; input the two into the diffusion model to obtain the UNet's features; and use the mask generator to predict all possible binary masks of semantic categories in the image. To classify each predicted mask $m_i$ into one of the test categories, we compute $p(z_i, C_{\\\\text{test}})$ defined in Eq. 6 using ODISE and finally predict the category with the maximum probability.\\n\\nIn our experiments, we found that the internal representation of the diffusion model is spatially well-differentiated to produce many plausible masks for objects instances. However, its object classification ability can be further enhanced by combining it once again with a text-image discriminative model, e.g., CLIP [57], especially for open-vocabularies. To this end, here we leverage a text-image discriminative model's image encoder $V$ to further classify each predicted masked region of the original input image.\"}"}
{"id": "CVPR-2023-1530", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"into one of the test categories. Specifically, as Fig. 3 illustrates, given an input image $x$, we first encode it into a feature map with the image encoder $V$ of a text-image discriminative model. Then for a mask $m_i$, predicted by ODISE for image $x$, we pool all the features at the output of the image encoder $V(x)$ that fall inside the predicted mask $m_i$ to compute a mask pooled image feature for it $z_i' = \\\\text{MaskPooling}(V(x), m_i)$. (9)\\n\\nWe use $p(z_i', C_{\\\\text{test}})$ from Eq. 6 to compute the final classification probabilities from the text-image discriminative model. Finally, we take the geometric mean of the category predictions from the diffusion and discriminative models as the final classification prediction, $p_{\\\\text{final}}(z_i, C_{\\\\text{test}}) \\\\propto p(z_i, C_{\\\\text{test}})^{\\\\lambda} p(z_i', C_{\\\\text{test}})^{1-\\\\lambda}$, (10) where $\\\\lambda \\\\in [0, 1]$ is a fixed balancing factor. We find that pooling the masked features is more efficient and yet as effective as the alternative approach proposed in [14, 23], which crops each of the $N$ predicted masked region's bounding box from the original image and encodes it separately with the image encoder $V$ (see details in the supplement).\\n\\n4. Experiments\\n\\nWe first introduce our implementation details. Then we compare our results against the state of the art on open-vocabulary panoptic and semantic segmentation. Lastly, we present ablation studies to demonstrate the effectiveness of the components of our method.\\n\\n4.1. Implementation Details\\n\\nArchitecture\\n\\nWe use the stable diffusion [61] model pretrained on a subset of the LAION [63] dataset as our text-to-image diffusion model. We extract feature maps from every three of its UNet blocks and, like FPN [45], resize them to create a feature pyramid. We set the time step used for the diffusion process to $t = 0$, by default. We use CLIP [57] as our text-image discriminative model and its corresponding image $V$ and text $T$ encoders everywhere. We choose Mask2Former [10] as the architecture of our mask generator, and generate $N = 100$ binary mask predictions.\\n\\nTraining Details\\n\\nWe train ODISE for 90k iterations with images of size $1024 \\\\times 1024$ and use large scale jittering [21]. Our batch size is 64. For caption-supervised training, we set $K_{\\\\text{word}} = 8$. We use the AdamW [49] optimizer with a learning rate $0.0001$ and a weight decay of $0.05$. We use the COCO dataset [46] as our training set. We utilize its provided panoptic mask annotations as the supervision signal for the binary mask loss. For training with image captions, for each image we randomly select one caption from the COCO dataset's caption [7] annotations.\\n\\nInference and Evaluation\\n\\nWe evaluate ODISE on ADE20K [87] for open-vocabulary panoptic, instance and semantic segmentation; and the Pascal datasets [19, 52] for semantic segmentation. We also provide the results ODISE for open-vocabulary object detection and open-world instance segmentation in the supplement. We use only a single checkpoint of ODISE for mask prediction on all tasks on all datasets. For panoptic segmentation, we report the panoptic quality (PQ) [35], mean average precision (mAP) on the \u201cthing\u201d categories, and the mean intersection over union (mIoU) metrics (additional SQ and RQ metrics are in the supplement). In panoptic segmentation annotations [35], the \u201cthing\u201d classes are countable objects like people, animals, etc. and the \u201cstuff\u201d classes are amorphous regions like sky, grass, etc. Since we train ODISE with panoptic mask annotations, we can directly infer both instance and semantic segmentation labels with it. When evaluating for panoptic segmentation, we use the panoptic test categories as $C_{\\\\text{test}}$, and directly classify each predicted mask into the test category with the highest probability. For semantic segmentation, we merge all masks assigned to the same \u201cthing\u201d category into a single one and output it as the predicted mask.\\n\\nSpeed and Model Size\\n\\nODISE has 28.1M trainable parameters (only 1.8% of the full model) and 1,493.8M frozen parameters. It performs inference for an image ($1024 \\\\times 1024$) at 1.26 FPS on an NVIDIA V100 GPU and uses 11.9 GB memory.\\n\\n4.2. Comparison with State of the Art\\n\\nOpen-Vocabulary Panoptic Segmentation\\n\\nFor open-vocabulary panoptic segmentation, we train ODISE on COCO [46] and test on ADE20K [87]. We report results in Table 1 (and more in the supplement). ODISE outperforms the concurrent work MaskCLIP [15] by 8.3 PQ on ADE20K. Besides the PQ metric, our approach also surpasses MaskCLIP [15] at open-vocabulary instance segmentation on ADE20K, with 8.4 points gain in mAP.\\n\\nODISE's qualitative results can be found in Fig. 4 and more in the supplement.\\n\\nOpen-Vocabulary Semantic Segmentation\\n\\nWe show a comparison of ODISE to previous work on open-vocabulary semantic segmentation in Table 2. Following the experiment in [22], we evaluate mIoU on 5 semantic segmentation datasets: (a) A-150 with 150 common classes and (b) A-847 with all the 847 classes of ADE20K [87], (c) PC-59 with 59 common classes and (d) PC-459 with full 459 classes of Pascal Context [52] and (e) the classic Pascal VOC dataset [19] with 20 foreground classes and 1 background class (PAS-21). For a fair comparison to prior work, we train ODISE with either category or image caption labels. ODISE outperforms the existing state-of-the-art methods on open-vocabulary semantic segmentation [15, 22] by a large margin: 7.6 mIoU on A-150, 4.7 mIoU on A-847, 6.2 mIoU on A-847, 4.7 mIoU on A-847, and 6.2 mIoU on A-847.\"}"}
{"id": "CVPR-2023-1530", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Open-vocabulary panoptic segmentation performance.\\n\\n| Method          | Dataset | Supervision | PQ  | mAP | mIoU |\\n|-----------------|---------|-------------|-----|-----|------|\\n| MaskCLIP        | COCO    | \u2713 \u2713         | 15.1| 6.0 | 23.7 |\\n| ODISE (Ours)    | COCO    | \u2713 \u2713         | 22.6| 14.4| 29.9 |\\n| ODISE (Ours)    | COCO    | \u2713 \u2713         | 23.4| 13.9| 28.7 |\\n\\nTable 2. Open-vocabulary semantic segmentation performance.\\n\\n| Method          | Dataset | Supervision | PQ  | mAP | mIoU |\\n|-----------------|---------|-------------|-----|-----|------|\\n| SPNet           | Pascal VOC | \u2713 \u2713         | -   | -   | -    |\\n| ZS3Net          | Pascal VOC | \u2713 \u2713         | -   | -   | -    |\\n| LSeg            | Pascal VOC | \u2713 \u2713         | -   | -   | -    |\\n| SimBaseline     | COCO    | \u2713 \u2713         | -   | -   | -    |\\n| ZegFormer       | COCO    | \u2713 \u2713         | -   | -   | -    |\\n| LSeg+           | COCO    | \u2713 \u2713         | 3.8 | 7.8 | 18.0 |\\n| MaskCLIP        | COCO    | \u2713 \u2713         | 8.2 | 10.0| 23.7 |\\n| ODISE (Ours)    | COCO    | \u2713 \u2713         | 11.1| 14.5| 29.9 |\\n| GroupViT        | GCC+YFCC | \u2713            | 4.3 | 4.9 | 10.6 |\\n| OpenSeg         | COCO    | \u2713 \u2713         | 6.3 | 9.0 | 21.1 |\\n| ODISE (Ours)    | COCO    | \u2713 \u2713         | 11.0| 13.8| 28.7 |\\n\\n4.3. Ablation Study\\n\\nTo demonstrate the contribution of each component of our method, we conduct an extensive ablation study. For faster experimentation, we train ODISE with 512^2 resolution images and use image caption supervision everywhere. Specifically, we evaluate different visual representations; caption generators; open-vocabulary inference pipelines and diffusion time-step(s) used to extract features (the latter two ablations are in the supplement).\\n\\nVisual Representations\\n\\nWe compare the internal representation of text-to-image diffusion models to those of other state-of-the-art pre-trained discriminative and generative models. We evaluate various discriminative models trained with full label, text or self-supervision. In all experiments we freeze the weights of the pre-trained models and use exactly the same training hyperparameters and mask generator as in our method. For each supervision category we select the best-performing and largest publicly available discriminative models. We observe from Table 3 that ODISE outperforms all other models in terms of PQ on both datasets. To offset any potential bias arising from the larger size of the LAION dataset (2B image-caption pairs) with which the stable diffusion model is trained, versus the smaller datasets used to train the discriminative models, we also compare to CLIP(H) [29, 57], which is trained on an equal-sized LAION [63] dataset. Our diffusion-based method outperforms CLIP(H) on all metrics. In the supplement (Fig 3.4), we show k-means clustering of frozen diffusion and CLIP features and find that the diffusion features are much more semantically differentiated. This demonstrates that the diffusion model's internal representation is indeed superior for open-vocabulary segmentation that that of discriminative pre-trained models.\\n\\nThe recent DDPMSeg [2] model is somewhat related to our model. Besides us, it is the only prior work that uses diffusion models and obtains state-of-the-art performance on label-efficient segmentation learning. Since DDPMSeg relies on category specific diffusion models it is not directly comparable.\\n\\nTable 3. Comparison with the state-of-the-art visual representations.\\n\\n| Model                        | Data    | Supervision | PQ  | mAP | mIoU |\\n|------------------------------|---------|-------------|-----|-----|------|\\n| DeiT-v3(H)                   | IN-21k  |             | 21.4| 11.4| 28.0 |\\n| Swin(H)                      | IN-21k  |             | 20.9| 10.7| 27.7 |\\n| ConvNeXt(H)                  | IN-21k  |             | 21.0| 11.0| 27.8 |\\n| MViT(H)                      | IN-21k  |             | 21.1| 11.6| 28.1 |\\n| LDM                          | IN-1k   |             | 20.7| 10.9| 26.5 |\\n| DeiT-v3(H)                   | IN-21k  |             | 21.4| 11.4| 28.0 |\\n| Swin(H)                      | IN-21k  |             | 20.9| 10.7| 27.7 |\\n| ConvNeXt(H)                  | IN-21k  |             | 21.0| 11.0| 27.8 |\\n| MViT(H)                      | IN-21k  |             | 21.1| 11.6| 28.1 |\\n| LDM                          | IN-1k   |             | 20.7| 10.9| 26.5 |\\n| MoCo-v3(H)                   | IN-1k   |             | 19.3| 9.6 | 25.8 |\\n| DINO(B)                      | IN-1k   |             | 20.6| 10.5| 26.3 |\\n| MAE(H)                       | IN-1k   |             | 21.5| 10.9| 27.6 |\\n| BEiT-v2(H)                   | IN-21k  |             | 21.4| 11.4| 28.0 |\\n| CLIP(L)                      | WIT     |             | 20.4| 9.6 | 27.0 |\\n| CLIP(H)                      | LAION   |             | 21.2| 10.8| 28.1 |\\n| ODISE                        | LAION   |             | 23.3| 13.0| 29.2 |\\n\\nB, L, H in the parentheses denote the model's size.\"}"}
{"id": "CVPR-2023-1530", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative Visualization on COCO (first 2 rows) and ADE20K (last row) validation and test sets. To demonstrate open-vocabulary recognition capability, we merge category names of LVIS, COCO and ADE20K together and perform open-vocabulary inference with \\\\( \\\\sim 1.5 \\\\) \\\\( k \\\\) classes directly. \u201cBamboo\u201d, \u201cswimming pool\u201d, \u201cconveyer belt\u201d, \u201cchandelier\u201d, \u201cbooth\u201d, \u201cstool\u201d, \u201ccolumn\u201d, \u201cpool table\u201d, \u201cbannister\u201d, etc., are novel categories from LVIS/ADE20K that are not annotated in COCO. ODISE shows plausible open-vocabulary panoptic results. The supplement contains more visual results.\\n\\n| ADE20K Captioner | PQ mAP mIoU | PQ mAP mIoU |\\n|-------------------|-------------|-------------|\\n| (a) Empty         | 21.8 11.8   | 27.3        |\\n| (b) Heuristic [83] | 22.2 12.1   | 28.1        |\\n| (c) BLIP [39]     | 22.3 12.4   | 28.2        |\\n| (d) Implicit      | 23.3 13.0   | 29.2        |\\n\\nTable 4. Ablation results of different caption generators.\\n\\nAs discussed in Sec. 3.3, the internal features of a text-to-image diffusion model are dependent on the embedding of the input caption. To derive the optimal set of features for our downstream task, we introduce a novel implicit captioning module to directly generate an implicit text embedding from an image. This module also facilitates inference on images sans paired captions at test time. Here, we construct several baselines to show the effectiveness of our implicit captioning module (Table 4). The various alternatives that we compare are: providing an empty string to the text encoder for any given image, such that the text embedding for all images is fixed (row (a)); employing two different off-the-shelf image captioning networks to generate an explicit caption for each image on-the-fly (rows (b) and (c)), where (c) [39] is trained on the COCO caption dataset, while (b) [83] is not; and our proposed implicit captioning module (row (d)). Overall, we find that using an explicit/implicit caption is better than using empty text. Furthermore, (c) improves over (b) on COCO but has similar PQ on ADE20K. It may be because the pre-trained BLIP [39] model does not see ADE20K\u2019s image distribution during training. Lastly, since our implicit captioning module derives its caption from a text-image discriminative model trained on Internet-scale data, it is able to generalize best among all variants compared.\\n\\n5. Conclusion\\n\\nWe take the first step in leveraging the frozen internal representation of large-scale text-to-image diffusion models for downstream recognition tasks. ODISE shows the great potential of large generative models in open-vocabulary segmentation tasks and establishes a new state of the art. We demonstrate that diffusion models are not only capable of generating plausible images but also of learning rich semantic representations. Our work opens a new direction for how to effectively leverage the internal representation of large diffusion models for other tasks as well in the future.\\n\\nAcknowledgements. We thank Golnaz Ghiasi for providing the prompt engineering labels for evaluation. Prof. Xiaolong Wang\u2019s laboratory was supported, in part, by NSF CCF-2112665 (TILOS), NSF CAREER Award IIS-2240014, DARPA LwLL, Amazon Research Award, Adobe Data Science Research Award, and Qualcomm Innovation Fellowship.\"}"}
{"id": "CVPR-2023-1530", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWe present ODISE: Open-vocabulary DIffusion-based panoptic SEgmentation, which unifies pre-trained text-image diffusion and discriminative models to perform open-vocabulary panoptic segmentation. Text-to-image diffusion models have the remarkable ability to generate high-quality images with diverse open-vocabulary language descriptions. This demonstrates that their internal representation space is highly correlated with open concepts in the real world. Text-image discriminative models like CLIP, on the other hand, are good at classifying images into open-vocabulary labels. We leverage the frozen internal representations of both these models to perform panoptic segmentation of any category in the wild. Our approach outperforms the previous state of the art by significant margins on both open-vocabulary panoptic and semantic segmentation tasks. In particular, with COCO training only, our method achieves 23.4 PQ and 30.0 mIoU on the ADE20K dataset, with 8.3 PQ and 7.9 mIoU absolute improvement over the previous state of the art. We open-source our code and models at https://github.com/NVlabs/ODISE.\\n\\n1. Introduction\\n\\nHumans look at the world and can recognize limitless categories. Given the scene presented in Fig. 1, besides identifying every vehicle as a \u201ctruck\u201d, we immediately understand that one of them is a pickup truck requiring a trailer to move another truck. To reproduce an intelligence with such a fine-grained and unbounded understanding, the problem of open-vocabulary recognition [36, 57, 76, 89] has recently attracted a lot of attention in computer vision. However, very few works are able to provide a unified framework that parses all object instances and scene semantics at the same time, i.e., panoptic segmentation.\\n\\nMost current approaches for open-vocabulary recognition rely on the excellent generalization ability of text-image discriminative models [30, 57] trained with Internet-scale data. While such pre-trained models are good at classifying individual object proposals or pixels, they are not necessarily optimal for performing scene-level structural understanding. Indeed, it has been shown that CLIP [57] often confuses the spatial relations between objects [69]. We hypothesize that the lack of spatial and relational understanding in text-image discriminative models is a bottleneck for open-vocabulary panoptic segmentation.\\n\\nOn the other hand, text-to-image generation using diffusion models trained on Internet-scale data [1, 59, 61, 62, 63] is a promising avenue for open-vocabulary recognition. However, these models typically require large amounts of data and are trained in an unsupervised manner, which limits their applicability to real-world scenarios.\\n\\nOur approach, ODISE, leverages the strengths of both text-image discriminative models and text-to-image diffusion models to perform open-vocabulary panoptic segmentation. We first use K-Means clustering to learn semantic patches from pre-trained diffusion models. These patches are then used to guide the diffusion process, allowing us to generate high-quality images with diverse open-vocabulary descriptions. The resulting images are then used to perform panoptic segmentation, which is able to accurately identify and segment all objects in the scene.\\n\\nOur method outperforms the previous state of the art by significant margins on both open-vocabulary panoptic and semantic segmentation tasks. In particular, with COCO training only, our method achieves 23.4 PQ and 30.0 mIoU on the ADE20K dataset, with 8.3 PQ and 7.9 mIoU absolute improvement over the previous state of the art. We open-source our code and models at https://github.com/NVlabs/ODISE.\\n\\n*Jiarui Xu was an intern at NVIDIA during the project.\\n\u2020equal contribution.\"}"}
{"id": "CVPR-2023-1530", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"90 has recently revolutionized the field of image synthesis. It offers unprecedented image quality, generalizability, composition-ability and, semantic control via the input text. An interesting observation is that to condition the image generation process on the provided text, diffusion models compute cross-attention between the text's embedding and their internal visual representation. This design implies the plausibility of the internal representation of diffusion models being well-differentiated and correlated to high/mid-level semantic concepts that can be described by language. As a proof-of-concept, in Fig.1 (center), we visualize the results of clustering a diffusion model's internal features for the image on the left. While not perfect, the discovered groups are indeed semantically distinct and localized. Motivated by this finding, we ask the question of whether Internet-scale text-to-image diffusion models can be exploited to create universal open-vocabulary panoptic segmentation learner for any concept in the wild?\\n\\nTo this end, we propose ODISE: Open-vocabulary Diffusion-based panoptic SEgmentation (pronounced o-di-see), a model that leverages both large-scale text-image diffusion and discriminative models to perform state-of-the-art panoptic segmentation of any category in the wild. An overview of our approach is illustrated in Fig. 2. At a high-level it contains a pre-trained frozen text-to-image diffusion model into which we input an image and its caption and extract the diffusion model's internal features for them. With these features as input, our mask generator produces panoptic masks of all possible concepts in the image. We train the mask generator with annotated masks available from a training set. A mask classification module then categorizes each mask into one of many open-vocabulary categories by associating each predicted mask\u2019s diffusion features with text embeddings of several object category names. We train this classification module with either mask category labels or image-level captions from the training dataset. Once trained, we perform open-vocabulary panoptic inference with both the text-image diffusion and discriminative models to classify a predicted mask. On many different benchmark datasets and across several open-vocabulary recognition tasks, ODISE achieves state-of-the-art accuracy outperforming the existing baselines by large margins.\\n\\nOur contributions are the following:\\n\\n\u2022 To the best of our knowledge, ODISE is the first work to explore large-scale text-to-image diffusion models for open-vocabulary segmentation tasks.\\n\u2022 We propose a novel pipeline to effectively leverage both text-image diffusion and discriminative models to perform open-vocabulary panoptic segmentation.\\n\u2022 We significantly advance the field forward by outperforming all existing baselines on many open-vocabulary recognition tasks, and thus establish a new state of the art in this space.\\n\\n2. Related Work\\n\\nPanoptic Segmentation. Panoptic segmentation [35] is a fundamental vision task that encompasses both instance and semantic segmentation. However, previous works [5, 9\u201311, 35, 38, 43, 44, 60, 74, 79, 84] follow a closed-vocabulary assumption and only recognize categories present in the training set. They are hence limited in segmenting things/stuff present in finite-sized vocabularies, which are much smaller than the typical vocabularies that we use to describe the real world.\\n\\nOpen-Vocabulary Segmentation. Most prior works on open-vocabulary segmentation either perform object detection with instance segmentation alone [17, 22, 23, 40, 51, 80, 81, 86, 89] or open-vocabulary semantic segmentation alone [22, 36, 76, 88]. In contrast, we propose a novel unified framework for both open-vocabulary instance and semantic segmentation. Another distinction is that prior works only use large-scale models pre-trained for image discriminative tasks, e.g., image classification [27, 47] or image-text contrastive learning [30, 41, 53, 57]. The concurrent work MaskCLIP [15] also uses CLIP [57]. However, such discriminative models\u2019 internal representations are sub-optimal for performing segmentation tasks versus those derived from image-to-text diffusion models as shown in our experiments.\\n\\nGenerative Models for Segmentation. There exist prior works, which are similar in spirit to ours in their use of image generative models, including GANs [3, 18, 32, 33, 91] or diffusion models [13, 16, 28, 31, 55, 64\u201368, 72] to perform semantic segmentation [2, 20, 37, 50, 71, 85]. They first train generative models on small-vocabulary datasets, e.g., cats [78], human faces [32] or ImageNet [12] and then with the help of few-shot hand-annotated examples per category, learn to classify the internal representations of the generative models into semantic regions. They either synthesize many images and their mask labels to train a separate segmentation network [37, 85]; or directly use the generative model to perform segmentation [2]. Among them, DDPM-Seg [2] shows the state-of-the-art accuracy. These prior works introduce the key idea that the internal representations of generative models may be sufficiently differentiated and correlated to mid/high-level visual semantic concepts and could be used for semantic segmentation. Our work is inspired by them, but it is also different in many respects. While previous works primarily focus on label-efficient semantic segmentation of small closed vocabularies, we, on the other hand, tackle open-vocabulary panoptic segmentation of many more and unseen categories in the wild.\"}"}
{"id": "CVPR-2023-1530", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. ODISE Overview and Training Pipeline. We first encode the input image into an implicit text embedding with an implicit captioner (image encoder $V$ and MLP). With the image and its implicit text embedding as input, we extract their diffusion features from a frozen text-to-image diffusion UNet (Sec 3.3). With the UNet's features, a mask generator predicts class-agnostic binary masks and their associated mask embedding features (Sec 3.4). We perform a dot product between the mask embedding features and the text embeddings of training category names (red box) or the nouns of the image's caption (green box) to categorize them. The similarity matrix for mask classification is supervised by either a cross entropy loss with ground truth category labels (red solid path), or via a grounding loss with the paired image captions (green dash path) (Sec 3.5).\\n\\n3. Method\\n\\n3.1. Problem Definition\\n\\nFollowing [15, 35], we train a model with a set of base training categories $C_{\\\\text{train}}$, which may be different from the test categories, $C_{\\\\text{test}}$, i.e., $C_{\\\\text{train}} \\\\neq C_{\\\\text{test}}$. $C_{\\\\text{test}}$ may contain novel categories not seen during training. We assume that during training, the binary panoptic mask annotation for each category in an image is provided. Additionally, we also assume that either the category label of each mask or a text caption for the image is available. During testing, neither the category label nor the caption is available for any image, and only the names of the test categories $C_{\\\\text{test}}$ are provided.\\n\\n3.2. Method Overview\\n\\nAn overview of our method ODISE, for open-vocabulary panoptic segmentation of any category in the wild is shown in Fig. 2. At a high-level, it contains a text-to-image diffusion model into which we input an image and its caption and extract the diffusion model's internal features for them (Sec 3.3). With these extracted features as input, and the provided training mask annotations, we train a mask generator to generate panoptic masks of all possible categories in the image (Sec 3.4). Using the provided training images' category labels or text captions, we also train an open-vocabulary mask classification module. It uses each predicted mask's diffusion features along with a text encoder's embeddings of the training category names to classify a mask (Sec 3.5). Once trained, we perform open-vocabulary panoptic inference with both the text-image diffusion and discriminative models (Sec 3.6 and Fig. 3). In the following sections, we describe each of these components.\\n\\n3.3. Text-to-Image Diffusion Model\\n\\nWe first provide a brief overview of text-to-image diffusion models and then describe how we extract features from them for panoptic segmentation.\\n\\nBackground\\n\\nA text-to-image diffusion model can generate high-quality images from provided input text prompts. It is trained with millions of image-text pairs crawled from the Internet [54, 59, 62]. The text is encoded into a text embedding with a pre-trained text encoder, e.g., T5 [58] or CLIP [57]. Before being input into the diffusion network, an image is distorted by adding some level of Gaussian noise to it. The diffusion network is trained to undo the distortion given the noisy input and its paired text embedding. During inference, the model takes image-shaped pure Gaussian noise and the text embedding of a user-provided description as input, and progressively de-noises it to a realistic image via several iterations of inference.\\n\\nVisual Representation Extraction\\n\\nThe prevalent diffusion-based text-to-image generative models [54, 59, 61, 62] typically use a UNet architecture to learn the denoising process. As shown in the blue block in Fig. 2, the UNet consists of convolution blocks, upsampling and downsampling blocks, skip connections and attention...\"}"}
{"id": "CVPR-2023-1530", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"blocks, which perform cross-attention \\\\[73\\\\] between a text embedding and UNet features. At every step of the de-noising process, diffusion models use the text input to infer the de-noising direction of the noisy input image. Since the text is injected into the model via cross attention layers, it encourages visual features to be correlated to rich semantically meaningful text descriptions. Thus the feature maps output by the UNet blocks can be regarded as rich and dense features for panoptic segmentation.\\n\\nOur method only requires a single forward pass of an input image through the diffusion model to extract its visual representation, as opposed to going through the entire multi-step generative diffusion process. Formally, given an input image-text pair \\\\((x, s)\\\\), we first sample a noisy image \\\\(x_t\\\\) at time step \\\\(t\\\\) as:\\n\\n\\\\[\\nx_t = \\\\sqrt{\\\\bar{\\\\alpha}_t} x + \\\\sqrt{1 - \\\\bar{\\\\alpha}_t} \\\\epsilon, \\\\quad \\\\epsilon \\\\sim N(0, I),\\n\\\\]\\n\\nwhere \\\\(t\\\\) is the diffusion step we use, \\\\(\\\\alpha_1, \\\\ldots, \\\\alpha_T\\\\) represent a pre-defined noise schedule where \\\\(\\\\bar{\\\\alpha}_t = Q_t \\\\sum_{k=1}^t \\\\alpha_k\\\\), as defined in \\\\([28]\\\\). We encode the caption \\\\(s\\\\) with a pre-trained text encoder \\\\(T\\\\) and extract the text-to-image diffusion UNet's internal features \\\\(f\\\\) for the pair by feeding it into the UNet \\\\(f = \\\\text{UNet}(x_t, T(s))\\\\).\\n\\nIt is worth noting that the diffusion model's visual representation \\\\(f\\\\) for \\\\(x\\\\) is dependent on its paired caption \\\\(s\\\\). It can be extracted correctly when paired image-text data is available, e.g., during pre-training of the text-to-image diffusion model. However, it becomes problematic when we want to extract the visual representation of images without paired captions available, which is the common use case for our application. For an image without a caption, we could use an empty text as its caption input, but that is clearly suboptimal, which we also show in our experiments. In what follows, we introduce a novel Implicit Captioner that we design to overcome the need for explicitly captioned image data. It also yields optimal downstream task performance. Implicit Captioner\\n\\nInstead of using an off-the-shelf captioning network to generate captions, we train a network to generate an implicit text embedding from the input image itself. We then input this text embedding into the diffusion model directly. We name this module an implicit captioner. The red block in Fig. 2 shows the architecture of the implicit captioner. Specifically, to derive the implicit text embedding for an image, we leverage a pre-trained frozen image encoder \\\\(V\\\\), e.g., from CLIP \\\\([57]\\\\) to encode the input image \\\\(x\\\\) into its embedding space. We further use a learned MLP to project the image embedding into an implicit text embedding, which we input into text-to-image diffusion UNet. During open-vocabulary panoptic segmentation training, the parameters of the image encoder and of the UNet are unchanged and we only fine-tune the parameters of the MLP.\\n\\nFinally, the text-to-image diffusion model's UNet along with the implicit captioner, together form ODISE's feature extractor that computes the visual representation \\\\(f\\\\) for an input image \\\\(x\\\\). Formally, we compute the visual representation \\\\(f\\\\) as:\\n\\n\\\\[\\nf = \\\\text{UNet}(x_t, \\\\text{ImplicitCaptioner}(x)) = \\\\text{UNet}(x_t, \\\\text{MLP} \\\\circ V(x)).\\n\\\\]\\n\\n3.4. Mask Generator\\n\\nThe mask generator takes the visual representation \\\\(f\\\\) as input and outputs \\\\(N\\\\) class-agnostic binary masks \\\\(\\\\{m_i\\\\}_{i=1}^N\\\\) and their corresponding \\\\(N\\\\) mask embedding features \\\\(\\\\{z_i\\\\}_{i=1}^N\\\\). The architecture of the mask generator is not restricted to a specific one. It can be any panoptic segmentation network capable of generating mask predictions of the whole image. We can instantiate our method with both bounding box-based \\\\([5, 34]\\\\) and direct segmentation mask-based \\\\([9\u201311, 74]\\\\) methods. While using bounding box-based methods like \\\\([5,34]\\\\), we can pool the ROI-Aligned \\\\([26]\\\\) features of each predicted mask's region to compute its mask embedding features. For segmentation mask-based methods like \\\\([9\u201311, 74]\\\\), we can directly perform masked pooling on the final feature maps to compute the mask embedding features. Since our representation focuses on dense pixel-wise predictions, we use a direct segmentation-based architecture. Following \\\\([26]\\\\), we supervise the predicted class-agnostic binary masks via a pixel-wise binary cross entropy loss along with their corresponding ground truth masks (treated as class-agnostic ones as well). Next, we describe how we classify each mask, represented by its mask embedding feature, into an open vocabulary.\\n\\n3.5. Mask Classification\\n\\nTo assign each predicted binary mask a category label from an open vocabulary, we employ text-image discriminative models. These models \\\\([30, 53, 57]\\\\), trained on Internet-scale image-text pairs, have shown strong open-vocabulary classification capabilities. They consist of an image encoder \\\\(V\\\\) and a text encoder \\\\(T\\\\). Following prior work \\\\([22, 36]\\\\), while training, we employ two commonly used supervision signals to learn to predict the category label of each predicted mask. Next, we describe how we unify these two training approaches in ODISE.\\n\\nCategory Label Supervision\\n\\nHere, we assume that during training we have access to each mask's ground truth category label. Thus, the training procedure is similar to that of traditional closed-vocabulary training. Suppose that there are \\\\(K_{\\\\text{train}} = |C_{\\\\text{train}}|\\\\) categories in the training set. For each mask embedding feature \\\\(z_i\\\\), we dub its corresponding known ground truth category as \\\\(y_i \\\\in C_{\\\\text{train}}\\\\). We encode the names of all the categories in \\\\(C_{\\\\text{train}}\\\\) with the frozen text encoder \\\\(T\\\\), and define the set of embeddings of all the training\"}"}
