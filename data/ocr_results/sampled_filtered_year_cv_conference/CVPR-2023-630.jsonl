{"id": "CVPR-2023-630", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[2] Tadas Baltr\u016b\u0161aitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence, 41(2):423\u2013443, 2018.\\n\\n[3] Quentin Bammey, Rafael Grompone von Gioi, and Jean-Michel Morel. An adaptive neural network for unsupervised mosaic consistency analysis in image forensics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14194\u201314204, 2020.\\n\\n[4] Quentin Bammey, Rafael Grompone von Gioi, and Jean-Michel Morel. An adaptive neural network for unsupervised mosaic consistency analysis in image forensics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14194\u201314204, 2020.\\n\\n[5] Jawadul H Bappy, Amit K Roy-Chowdhury, Jason Bunk, Lakshmanan Nataraj, and BS Manjunath. Exploiting spatial structure for localizing manipulated image regions. In Proceedings of the IEEE international conference on computer vision, pages 4970\u20134979, 2017.\\n\\n[6] Tiziano Bianchi and Alessandro Piva. Image forgery localization via block-grained analysis of jpeg artifacts. IEEE Transactions on Information Forensics and Security, 7(3):1003\u20131017, 2012.\\n\\n[7] Luca Bondi, Luca Baroffio, David Guerra, Paolo Bestagini, Edward J Delp, and Stefano Tubaro. First steps toward camera model identification with convolutional neural networks. IEEE Signal Processing Letters, 24(3):259\u2013263, 2016.\\n\\n[8] Luca Bondi, Silvia Lameri, David Guerra, Paolo Bestagini, Edward J Delp, Stefano Tubaro, et al. Tampering detection and localization through clustering of camera-based cnn features. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2017.\\n\\n[9] Luca Bondi, Silvia Lameri, David Guerra, Paolo Bestagini, Edward J Delp, Stefano Tubaro, et al. Tampering detection and localization through clustering of camera-based cnn features. In CVPR Workshops, volume 2, 2017.\\n\\n[10] J-Y Bouguet. Camera calibration toolbox for matlab. http://www.vision.caltech.edu/bouguetj/calibdoc/index.html, 2004.\\n\\n[11] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n\\n[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020.\\n\\n[13] Davide Cozzolino and Luisa Verdoliva. Noiseprint: a cnn-based camera model fingerprint. IEEE Transactions on Information Forensics and Security, 15:144\u2013159, 2019.\\n\\n[14] Duc-Tien Dang-Nguyen, Cecilia Pasquini, Valentina Conotter, and Giulia Boato. Raise: A raw images dataset for digital image forensics. In Proceedings of the 6th ACM multimedia systems conference, pages 219\u2013224, 2015.\\n\\n[15] Tiago Jos\u00e9 De Carvalho, Christian Riess, Elli Angelopoulou, Helio Pedrini, and Anderson de Rezende Rocha. Exposing digital image forgeries by illumination color classification. IEEE Transactions on Information Forensics and Security, 8(7):1182\u20131194, 2013.\\n\\n[16] Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11162\u201311173, 2021.\\n\\n[17] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE international conference on computer vision, pages 1422\u20131430, 2015.\\n\\n[18] Jing Dong, Wei Wang, and Tieniu Tan. CASIA image tampering detection evaluation database. In 2013 IEEE China Summit and International Conference on Signal and Information Processing. IEEE, July 2013.\\n\\n[19] Jiayuan Fan, Hong Cao, and Alex C Kot. Estimating exif parameters based on noise features for image manipulation detection. IEEE Transactions on Information Forensics and Security, 8(4):608\u2013618, 2013.\\n\\n[20] Jiayuan Fan, Tao Chen, and Alex ChiChung Kot. Exif-white balance recognition for image forensic analysis. Multidimensional Systems and Signal Processing, 28(3):795\u2013815, 2017.\\n\\n[21] Hany Farid. Exposing digital forgeries from jpeg ghosts. IEEE transactions on information forensics and security, 4(1):154\u2013160, 2009.\\n\\n[22] Hany Farid. Photo forensics. MIT Press, 2016.\\n\\n[23] Pasquale Ferrara, Tiziano Bianchi, Alessia De Rosa, and Alessandro Piva. Image forgery localization via fine-grained analysis of cfa artifacts. IEEE Transactions on Information Forensics and Security, 7(5):1566\u20131577, 2012.\\n\\n[24] Pasquale Ferrara, Tiziano Bianchi, Alessia De Rosa, and Alessandro Piva. Image forgery localization via fine-grained analysis of cfa artifacts. IEEE Transactions on Information Forensics and Security, 7(5):1566\u20131577, 2012.\\n\\n[25] Thomas Gloe and Rainer B\u00f6ehme. The 'dresden image database' for benchmarking digital image forensics. In Proceedings of the 2010 ACM symposium on applied computing, pages 1584\u20131590, 2010.\\n\\n[26] Ashima Gupta, Nisheeth Saxena, and SK Vasistha. Detecting copy move forgery using dct. International Journal of Scientific and Research Publications, 3(5):1, 2013.\\n\\n[27] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.\\n\\n[28] James Hays and Alexei A Efros. Scene completion using millions of photographs. ACM Transactions on Graphics (ToG), 26(3):4\u2013es, 2007.\\n\\n[29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.\\n\\n[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6953\u20136953.\"}"}
{"id": "CVPR-2023-630", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[31] Yannick Hold-Geoffroy, Kalyan Sunkavalli, Jonathan Eisenmann, Matthew Fisher, Emiliano Gambaretto, Sunil Hadap, and Jean-Franc\u00b8ois Lalonde. A perceptual measure for deep single image camera calibration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2354\u20132363, 2018.\\n\\n[32] Wu-Chih Hu and Wei-Hao Chen. Effective forgery detection using dct+ svd-based watermarking for region of interest in key frames of vision-based surveillance. International Journal of Computational Science and Engineering, 8(4):297\u2013305, 2013.\\n\\n[33] Minyoung Huh, Andrew Liu, Andrew Owens, and Alexei A Efros. Fighting fake news: Image splice detection via learned self-consistency. In ECCV, 2018.\\n\\n[34] Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison, et al. Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera. In Proceedings of the 24th annual ACM symposium on User interface software and technology, pages 559\u2013568, 2011.\\n\\n[35] Michael R James and Stuart Robson. Straightforward reconstruction of 3d surfaces and topography with a camera: Accuracy and geoscience application. Journal of Geophysical Research: Earth Surface, 117(F3), 2012.\\n\\n[36] Armand Joulin, Laurens van der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from large weakly supervised data. In European Conference on Computer Vision, pages 67\u201384. Springer, 2016.\\n\\n[37] Thibaut Julliand, Vincent Nozick, and Hugues Talbot. Image noise and digital image forensics. In International Workshop on Digital Watermarking, pages 3\u201317. Springer, 2015.\\n\\n[38] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[39] Matthias Kirchner and Thomas Gloe. Forensic camera model identification. Handbook of Digital Forensics of Multimedia Data and Devices, pages 329\u2013374, 2015.\\n\\n[40] Vladimir V Kniaz, Vladimir Knyaz, and Fabio Remondino. The point where reality meets fantasy: Mixed adversarial generators for image splice detection. Advances in Neural Information Processing Systems, 32, 2019.\\n\\n[41] Vladimir V Kniaz, Vladimir Knyaz, and Fabio Remondino. The point where reality meets fantasy: Mixed adversarial generators for image splice detection. Advances in Neural Information Processing Systems, 32, 2019.\\n\\n[42] Pawe\u0142 Korus and Jiwu Huang. Multi-scale analysis strategies in prnu-based tampering localization. IEEE Transactions on Information Forensics and Security, 12(4):809\u2013824, 2016.\\n\\n[43] Akash Kumar and Arnav Bhavsar. Copy-move forgery classification via unsupervised domain adaptation. arXiv preprint arXiv:1911.07932, 2019.\\n\\n[44] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\\n\\n[45] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.\\n\\n[46] Ang Li, Allan Jabri, Armand Joulin, and Laurens Van Der Maaten. Learning visual n-grams from web data. In Proceedings of the IEEE International Conference on Computer Vision, pages 4183\u20134192, 2017.\\n\\n[47] Bo Liu and Chi-Man Pun. Splicing forgery exposure in digital image by detecting noise discrepancies. International Journal of Computer and Communication Engineering, 4(1):33, 2015.\\n\\n[48] Bo Liu, Chi-Man Pun, and Xiao-Chen Yuan. Digital image forgery detection using jpeg features and local noise discrepancies. The Scientific World Journal, 2014, 2014.\\n\\n[49] Yung-Cheng Liu, Wen-Hsin Chan, and Ye-Quang Chen. Automatic white balance for digital still camera. IEEE Transactions on Consumer Electronics, 41(3):460\u2013466, 1995.\\n\\n[50] Manuel Lopez, Roger Mari, Pau Gargallo, Yubin Kuang, Javier Gonzalez-Jimenez, and Gloria Haro. Deep single image camera calibration with radial distortion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11817\u201311825, 2019.\\n\\n[51] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.\\n\\n[52] Babak Mahdian and Stanislav Saic. Using noise inconsistencies for blind image forensics. Image and Vision Computing, 27(10):1497\u20131503, 2009.\\n\\n[53] Francesco Marra, Diego Gragnaniello, Luisa Verdoliva, and Giovanni Poggi. A full-image full-resolution end-to-end-trainable cnn framework for image forgery detection. IEEE Access, 8:133488\u2013133502, 2020.\\n\\n[54] Owen Mayer and Matthew C Stamm. Learned forensic source similarity for unknown camera models. In ICASSP, 2018.\\n\\n[55] Yasuhide Mori, Hironobu Takahashi, and Ryuichi Oka. Image-to-word transformation based on dividing and vector quantizing images with words. In First international workshop on multimedia intelligent storage and retrieval management, pages 1\u20139. Citeseer, 1999.\\n\\n[56] Tian-Tsong Ng, Shih-Fu Chang, and Q Sun. A data set of authentic and spliced image blocks. Columbia University, ADVENT Technical Report, pages 203\u20132004, 2004.\\n\\n[57] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\\n\\n[58] Andrew Owens and Alexei A Efros. Audio-visual scene analysis with self-supervised multisensory features. In Proceedings of the European Conference on Computer Vision (ECCV), pages 631\u2013648, 2018.\\n\\n[59] Guy Parsons. Dall-e 2 prompt book. http://dallery.gallery/wp-content/uploads/2022/07/The-DALLC2%B7E2-prompt-book-v1.02.pdf, 2022.\"}"}
{"id": "CVPR-2023-630", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Marc Pollefeys, Luc Van Gool, Maarten Vergauwen, Frank Verbiest, Kurt Cornelis, Jan Tops, and Reinhard Koch. Visual modeling with a hand-held camera. *International Journal of Computer Vision*, 59(3):207\u2013232, 2004.\\n\\nChi-Man Pun, Bo Liu, and Xiao-Chen Yuan. Multi-scale noise estimation for image splicing forgery detection. *Journal of visual communication and image representation*, 38:195\u2013206, 2016.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supersision. In *International Conference on Machine Learning*, pages 8748\u20138763. PMLR, 2021.\\n\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In *International Conference on Machine Learning*, pages 8821\u20138831. PMLR, 2021.\\n\\nAndreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie\u00dfner. FaceForensics++: Learning to detect manipulated facial images. In *ICCV*, 2019.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. *IJCV*, 2015.\\n\\nRonald Salloum, Yuzhuo Ren, and C-C Jay Kuo. Image splicing localization using a multi-task fully convolutional network (mfcn). *Journal of Visual Communication and Image Representation*, 51:201\u2013209, 2018.\\n\\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. *arXiv preprint arXiv:1910.01108*, 2019.\\n\\nMert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning visual representations with caption annotations. In *European Conference on Computer Vision*, pages 153\u2013170. Springer, 2020.\\n\\nJianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. *IEEE Transactions on pattern analysis and machine intelligence*, 22(8):888\u2013905, 2000.\\n\\nNoah Snavely, Steven M Seitz, and Richard Szeliski. Phototourism: exploring photo collections in 3d. In *ACM siggraph 2006 papers*, pages 835\u2013846. 2006.\\n\\nPeter Sturm. On focal length calibration from two views. In *Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001*, volume 2, pages II\u2013II. IEEE, 2001.\\n\\nXiaoting Sun, Yezhou Li, Shaozhang Niu, and Yanli Huang. The detecting system of image forgeries with noise features and exif information. *Journal of Systems Science and Complexity*, 28(5):1164\u20131176, 2015.\\n\\nBart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. *Communications of the ACM*, 59(2):64\u201373, 2016.\\n\\nAmel Tuama, Fr\u00e9d\u00e9ric Comby, and Marc Chaumont. Camera model identification with the use of deep convolutional neural networks. In *2016 IEEE International workshop on information forensics and security (WIFS)*, pages 1\u20136. IEEE, 2016.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. *Advances in neural information processing systems*, 30, 2017.\\n\\nSheng-Yu Wang, Oliver Wang, Andrew Owens, Richard Zhang, and Alexei A Efros. Detecting photoshopped faces by scripting photoshop. In *ICCV*, 2019.\\n\\nSheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images are surprisingly easy to spot... for now. *Computer Vision and Pattern Recognition (CVPR)*, 2020.\\n\\nXin Wang, Bo Xuan, and Si-long Peng. Digital image forgery detection based on the consistency of defocus blur. In *2008 International Conference on Intelligent Information Hiding and Multimedia Signal Processing*, pages 192\u2013195. IEEE, 2008.\\n\\nHaiwei Wu, Jiantao Zhou, Jinyu Tian, and Jun Liu. Robust image forgery detection over online social network shared images. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 13440\u201313449, 2022.\\n\\nYue Wu, Wael AbdAlmageed, and Premkumar Natarajan. Mantra-net: Manipulation tracing network for detection and localization of image forgeries with anomalous features. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 9543\u20139552, 2019.\\n\\nBin Xiao, Yang Wei, Xiuli Bi, Weisheng Li, and Jianfeng Ma. Image splicing forgery detection combining coarse to refined convolutional neural network and adaptive clustering. *Information Sciences*, 511:172\u2013191, 2020.\\n\\nShuiming Ye, Qibin Sun, and Ee-Chien Chang. Detecting digital image forgeries by measuring inconsistencies of blocking artifact. In *2007 IEEE International Conference on Multimedia and Expo*, pages 12\u201315. Ieee, 2007.\\n\\nShuiming Ye, Qibin Sun, and Ee-Chien Chang. Detecting digital image forgeries by measuring inconsistencies of blocking artifact. In *2007 IEEE International Conference on Multimedia and Expo*, pages 12\u201315. Ieee, 2007.\\n\\nYuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. Contrastive learning of medical visual representations from paired images and text. *arXiv preprint arXiv:2010.00747*, 2020.\\n\\nPeng Zhou, Xintong Han, Vlad I Morariu, and Larry S Davis. Learning rich features for image manipulation detection. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 1053\u20131061, 2018.\\n\\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In *Proceedings of the IEEE international conference on computer vision*, pages 19\u201327, 2015.\"}"}
{"id": "CVPR-2023-630", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Daniel Zoran, Phillip Isola, Dilip Krishnan, and William T. Freeman. Learning ordinal relationships for mid-level vision. In Proceedings of the IEEE International Conference on Computer Vision, pages 388\u2013396, 2015.\"}"}
{"id": "CVPR-2023-630", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We learn a visual representation that captures information about the camera that recorded a given photo. To do this, we train a multimodal embedding between image patches and the EXIF metadata that cameras automatically insert into image files. Our model represents this metadata by simply converting it to text and then processing it with a transformer. The features that we learn significantly outperform other self-supervised and supervised features on downstream image forensics and calibration tasks. In particular, we successfully localize spliced image regions \u201czero shot\u201d by clustering the visual embeddings for all of the patches within an image.\\n\\n1. Introduction\\n\\nA major goal of the computer vision community has been to use cross-modal associations to learn concepts that would be hard to glean from images alone [2]. A particular focus has been on learning high level semantics, such as objects, from other rich sensory signals, like language and sound [58, 62]. By design, the representations learned by these approaches typically discard imaging properties, such as the type of camera that shot the photo, its lens, and the exposure settings, which are not useful for their cross-modal prediction tasks [17].\\n\\nWe argue that obtaining a complete understanding of an image requires both capabilities \u2014 for our models to perceive not only the semantic content of a scene, but also the properties of the camera that captured it. This type of low level understanding has proven crucial for a variety of tasks, from image forensics [33, 52, 80] to 3D reconstruction [34, 35], yet it has not typically been a focus of representation learning. It is also widely used in image generation, such as when users of text-to-image tools specify camera properties with phrases like \u201cDSLR photo\u201d [59,63].\\n\\nWe propose to learn low level imaging properties from the abundantly available (but often neglected) camera metadata that is added to the image file at the moment of capture. This metadata is typically represented as dozens of Exchangeable Image File Format (EXIF) tags that describe the camera, its settings, and postprocessing operations that\"}"}
{"id": "CVPR-2023-630", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"white balancing and estimating camera models as for predicting focal length, performing Estimating camera properties.\\n\\n- Image manipulations can be identified \\\"zero shot\\\" by image-metadata embeddings are a useful representation.\\n- Image manipulations by flagging images whose patch embeddings are spliced regions by clustering the embeddings within an image.\\n\\nThus, the embeddings that our model assigns to each potentially captured with a different camera and images in camera fingerprints hidden within image patches. Drawing on recent work that detects inconsistencies of images: estimating an image's radial distortion parameters for various tasks that benefit from a low-level understanding of camera properties solely from images, and that it provides a format. We convert each tag to text, and then concatenating through contrastive learning that puts image patches together. Our model thus closely resembles contrastive learning that puts image patches or were applied to the image: e.g., supervised representation learning.\\n\\nWe show that our model can successfully estimate camera properties and that it provides a visual similarity metric that native them together. Our model thus closely resembles contrastive learning that puts image patches or were applied to the image: e.g., supervised representation learning.\\n\\n1. We train a joint embedding...\\n\\n3. Images\\n\\n5. I\\n\\n6. I\\n\\n4. I\\n\\n5. I\\n\\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022"}
{"id": "CVPR-2023-630", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"information through prompting, such as by adding text like \u201cDSLR photo of...\u201d or \u201cSigma 500mm f/5\u201d to prompts [59]. These methods, however, learn these camera associations through the (relatively rare) descriptions of cameras provided by humans, while ours learns them from an abundant and complementary learning signal, camera metadata.\\n\\n3. Associating Images with Camera Metadata\\n\\nWe desire a visual representation that captures low level imaging properties, such as the settings of the camera that were used to shoot the photo. We then apply this learned representation to downstream tasks that require an understanding of camera properties.\\n\\n3.1. Learning Cross-Modal Embeddings\\n\\nWe train a model to predict camera metadata from image content, thereby obtaining a representation that conveys camera properties. Following previous work in multimodal contrastive learning [62], we train a joint embedding between the two modalities, allowing our model to avoid the (error prone) task of directly predicting the attributes. Specifically, we want to jointly learn an image encoder and metadata encoder such that, given $N$ images and $N$ pieces of metadata information, the corresponding image\u2013metadata pairs can be recognized by the model by maximizing embedding similarity. We use full-resolution image patches rather than resized images, so that our model can analyze low-level details that may be lost during downsampling.\\n\\nGiven a dataset of image patches and their corresponding camera metadata $\\\\{(v_i, m_i)\\\\}_{i=1}^N$, we learn visual and EXIF representations $f_\\\\theta(v)$ and $g_\\\\phi(m)$ by jointly training $f_\\\\theta$ and $g_\\\\phi$ using a contrastive loss [57]:\\n\\n$$L_{V,M,i} = -\\\\log \\\\frac{\\\\exp (f_\\\\theta(v_i) \\\\cdot g_\\\\phi(m_i)/\\\\tau)}{\\\\sum_{j=1}^N \\\\exp (f_\\\\theta(v_i) \\\\cdot g_\\\\phi(m_j)/\\\\tau)},$$\\n\\nwhere $\\\\tau$ is a small constant. Following prior work [62], we define an analogous loss $L_{M,V}$ that sums over visual (rather than metadata) examples in the denominator, and minimize a combined loss $L = L_{V,M} + L_{M,V}$.\\n\\n3.2. Representing the Camera Metadata\\n\\nThis formulation raises a natural question: how should we represent the metadata? The metadata within photos is stored as a set of EXIF tags, each indicating a different image property as shown in Table 1. EXIF tags span a range of formats and data types, and the set of tags that are present in a given photo can be highly inconsistent. Previous works that predict camera properties from images typically extract attributes of interest from the EXIF tags, and cast them to an appropriate data format \u2014 e.g., extracting a scalar-valued focal length category. This tag-specific processing limits EXIF tag Example values\\n\\n| EXIF tag       | Example values                        | #values |\\n|----------------|---------------------------------------|---------|\\n| Camera Make    | Canon, NIKON Corporation, Apple       | 312     |\\n| Camera Model   | NIKON D90, Canon EOS 7                | 3071    |\\n| Software       | Picasa, Adobe Photoshop, QuickTime    | 1711    |\\n| Exposure Time  | 1/60 sec, 1/125 sec, 1/250 sec         | 2062    |\\n| Focal Length   | 18.0 mm, 50.0 mm, 6.3 mm              | 931     |\\n| Aperture Value | F2.8, F4, F5.6, F3.5                   | 137     |\\n| Scene Capture Type | Landscape, Portrait, Night Scene     | 5       |\\n| Exposure Program | Aperture priority, Manual control   | 9       |\\n| White Balance Mode | Auto, Manual                       | 3       |\\n| Thumbnail Compression | JPEG, Uncompressed         | 3       |\\n| Digital Zoom Ratio | 1, 1.5, 2, 1.2               | 49      |\\n| ISO speed Ratings | 100, 400, 300                   | 460     |\\n| Shutter Speed Value | 1/60 sec, 1/63 sec, 1/124 sec | 1161    |\\n| Date/Time Digitized | 2013:03:28 04:20:46  | 95932   |\\n\\nTable 1. What information is contained within photo EXIF metadata? We list several of the most common EXIF tags, along with the common values and number of values they contain in the YFCC100M dataset [73].\\n\\nThe amount of metadata information that can be used as part of learning, and requires special-purpose architectures. We exploit the fact that EXIF tags are typically stored in a human-readable format and can be straightforwardly converted to text (Fig. 2). This allows us to directly process camera metadata using models from natural language processing \u2014 an approach that has successfully been applied to processing various text-like inputs other than language, such as math [45] and code [11]. Specifically, we create a long piece of text from a photo's metadata by converting each tag's name and value to strings, and concatenating them together. We separate each tag name and value with a colon and space, and separate different tags with a space. We evaluate a number of design decisions for this model in Sec. 4.4, such as the text format, choice of tags, and network architecture.\\n\\n3.3. Application: Zero-shot Image Forensics\\n\\nAfter learning cross-modal representations from images and camera metadata, we can use them for downstream tasks that require an understanding of camera properties. One way to do this is by using the learned visual network features as a representation for classification tasks, following other work in self-supervised representation learning [12,29]. We can also use our learned visual embeddings to perform \u201czero shot\u201d image splice detection, by detecting inconsistencies in an input image's imputed camera properties.\\n\\nSpliced images are composed of regions from multiple real images. Since they are typically designed to fool humans, forensic models need to rely more on subtle (often non-semantic) cues to detect them. We got inspiration from Huh et al. [33], which predicts whether two image patches share the same camera properties. If two patches are predicted to have very different camera properties, then this...\"}"}
{"id": "CVPR-2023-630", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Zero shot splice localization. Given a spliced image (left), we compute our cross-modal embeddings for each image patch, which we visualize here using projections onto the top 3 principal components. We then compute the affinity matrix by taking dot product for every pair of patches. We localize the spliced region by clustering these embedding vectors.\\n\\nTo determine whether an image is likely to contain a splice, we first compute an affinity matrix $A_{ij} = f_\\\\theta(v_i) \\\\cdot f_\\\\theta(v_j)$ whose entries are the dot product between patches\u2019 normalized embedding vectors. We score an image using the sum of the exponentiated dot products between embeddings, $\\\\phi(v) = \\\\sum_{i,j} \\\\exp(A_{ij}/\\\\tau)$. This score indicates the likelihood that the image is unmodified, since high dot products indicate high similarity in imputed camera properties. To localize the spliced image regions within an image, we aggregate the similarity scores in $A_{ij}$ by clustering the rows using mean shift, following [33]. This results in a similarity map indicating the likelihood that each patch was extracted from the largest source photo that was used to create the composite. Alternatively, we can visualize the spliced region by performing spectral clustering via normalized cuts [33, 69], using $A_{ij}$ as an affinity matrix between patches. We visualize this approach in Fig. 3.\\n\\n4. Results\\n\\n4.1. Implementation\\n\\nArchitecture. We use ResNet-50 pretrained on ImageNet as our image encoder. We found that the text encoder in models trained on captioning, such as CLIP [62], were not well-suited to our task, since they place low limits on the number of tokens. For the EXIF text encoder, we use DistilBERT [67] pretrained on Wikipedia and the Toronto Book Corpus [86]. We compute the feature representation of the EXIF as the activations for the end-of-sentence token from the last layer which is layer normalized and then linearly projected into multi-modal embedding space.\\n\\nTraining. To train our model, we use 1.5M full-resolution images and EXIF pairs from a random subset of YFCC100M [73]. We discard images that have less than 10 of the EXIF tags. Because many images only have a small number of EXIF tags available, we only use tags that are present in more than half of these images. This results in 44 EXIF tags (see supplementary for the complete list). In contrast to other work [33], we do not rebalance the images to increase the rate of rare tags. During training, we randomly crop $124 \\\\times 124$ patches from high-resolution images. We use the AdamW optimizer [38] with a learning rate of $10^{-4}$, weight decay of $10^{-3}$, and mixed precision training. We use a cosine annealing learning rate schedule [51]. The batch size is set to 1024, and we train our model for 50 epochs.\\n\\nOther model variations. To study the importance of metadata supervision on the learned representation, we train a similar model that performs contrastive learning but does not use metadata. The model resembles image-image contrastive learning [12, 29, 33, 87], which has been shown to be highly effective for representation learning, and which may learn low-level camera information [17]. Different from typical contrastive learning approaches, we use strict cropping augmentation so that the views for our model (Eq. 1) come from different crops of the same image, to encourage it to learn low-level image features. We call this model CropCLR. Additionally, we evaluate a number of ablations of our model, including models that are trained with individual EXIF tags, that use different formats for the EXIF-derived text, and different network architectures (Table 5).\\n\\n4.2. Evaluating the learned features\\n\\nFirst, we want to measure how well the learned features convey camera properties. Since EXIF file is already embedded with a lot of camera properties such as camera model, focal length, shutter speed, etc., it should be unsurprising that the learned features capture these properties. We can also evaluate the learned features on a zero-shot splice localization task. Given a spliced image, we compute our cross-modal embeddings for each image patch, which we visualize here using projections onto the top 3 principal components. We then compute the affinity matrix by taking dot product for every pair of patches. We localize the spliced region by clustering these embedding vectors.\"}"}
{"id": "CVPR-2023-630", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We do linear probing on top of learned representation to predict two camera related properties that are not presented in EXIF files. The good performance indicates that our model learns general imaging properties. resize and crop denote the image preprocessing applied.\\n\\nWe evaluate our model\u2019s ability to distinguish real and manipulated images. This is a task that requires a broader understanding of low level imaging properties, such as spotting unusual image statistics. We use the CASIA I [18] and CASIA II [43] datasets. The former contains only spliced fakes, while the latter contains a wider variety of manipulations.\\n\\nIn both tasks, we found that our model\u2019s features significantly outperformed those of the other models (Table 2). Our method achieves much better performance than traditional representational learning methods [29, 30, 62], perhaps because these models are encouraged to discard low-level details, while for our training task they are crucially important. Interestingly, the variation of our model that does not use EXIF metadata, CropCLR, outperforms the supervised [30] and self-supervised baselines [29], but significantly lags behind our full method. This is perhaps because it often suffices to use high-level cues (e.g., color histograms and object co-occurrence) to solve CropCLR\u2019s pretext task.\\n\\nThis suggests metadata supervision is an important learning signal and can effectively guide our model to learn general imaging information.\\n\\n4.3. Zero Shot Splice Detection and Localization\\n\\nWe evaluate our model on the task of detecting spliced images without any labeled training data. This is in contrast to Sec. 4.2, which used labeled data. We perform both splice detection (distinguish an image being spliced or not) and splice localization (localize spliced region within an image).\\n\\n### Implementation\\n\\nFor fair evaluation, we closely follow the approach of Huh et al. [33]. Given an image, we sample patches in a grid, using a stride such that the number of patches sampled along the longest image dimension is 25. To increase the spatial resolution of each similarity map, we average the predictions of overlapping patches. We consider the smaller of the two detected regions to be the splice.\\n\\n### Evaluation\\n\\nIn splice localization task, we compare our model to a variety of forensics methods. These include traditional methods that use handcrafted features [24, 52, 83], supervised methods [41, 79, 80], and self-supervised approaches [13, 33]. The datasets we use include Columbia [56], DSO [15], Realistic Tampering (RT) [42], In-the-Wild [33] and Hays and Efros inpainting images [28]. Columbia and DSO are created purely...\"}"}
{"id": "CVPR-2023-630", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Zero shot splice localization.\\n\\nWe evaluate our model on several datasets using permutation-invariant mean average precision (p-mAP) over pixels and class-balanced IOU (cIoU) with optimal threshold selected per image. The result indicates that our model is comparable to state-of-the-art methods, although not specially optimized for this task.\\n\\n| Dataset     | CFA [24] | DCT [83] | NOI [52] |\\n|-------------|----------|----------|----------|\\n|             | 0.83     | 0.58     | 0.73     |\\n|             | 0.49     | 0.48     | 0.51     |\\n|             | 0.54     | 0.52     | 0.52     |\\n\\nTable 4. Zero-shot splice detection:\\n\\nWe compare our splice detection accuracy on 3 datasets. We measure the mean average precision (mAP) of detecting whether an image has been spliced.\\n\\nMethods that enable splice detection:\\n- EXIF-SC [33]\\n- Ours - CropCLR\\n- Ours - Full\\n\\nOur model ranks first or second place for metrics in most datasets, and obtains performance comparable to top self-supervised methods that are specially designed for this task. In particular, our model significantly outperforms the most related technique, EXIF-SC [33]. We note that both our method and EXIF-SC get relatively low performance on the Realistic Tampering dataset. This may be due to the fact that this dataset contains manipulations such as copy-move that we do not expect to detect (since both regions share the same camera properties). In contrast to methods based on segmentation [13, 79, 80], we do not aim to have spatially precise matches, and output relatively low-resolution localization maps based on large patches. Consequently, our model is not well-suited to detecting very small splices, which also commonly occur in the Realistic Tampering dataset.\\n\\nIn Fig. 4, we show qualitative results, including both similarity maps and spectral clustering results. In Fig. 6, we compare our model with those of several other techniques. Interestingly, EXIF-SC has false positives in overexposed regions (as pointed out by [33]), since its classifier cannot infer whether these regions are (or are not) part of the rest of the scene. In contrast, our model successfully handles these regions. CropCLR incorrectly flags regions that are semantically different from the background, because this is a strong indication that the patches come from different images. In contrast, we successfully handle these cases, since our model has no such \u201cshortcut\u201d in its learning task.\\n\\n4.4. Ablation Study\\n\\nTo help understand which aspects of our approach are responsible for its performance, we evaluated a variety of variations of our model, including different training supervision, representations for the camera metadata, and network architectures.\\n\\nWe evaluated each model's features quality using linear probing on the radial distortion estimation and splice detection task (same as Sec. 4.2). As an additional evaluation, we classify the values of common EXIF tags by applying linear classifiers to our visual representation. We convert the values of each EXIF tag into discrete categories, by quantizing common values and removing examples that do not fit into any category. We average prediction accuracies over 44 EXIF tags to obtain overall accuracy. We provide more details in the supplementary. All models were trained for 30 epochs on 800K images on a subset of YFCC100M dataset. The associated texts are obtained from the image descriptions and EXIF data provided by the dataset.\"}"}
{"id": "CVPR-2023-630", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Model ablations. Downstream accuracy for versions of the model trained with different text supervision, representations of camera metadata, and architectures. We use linear probing to evaluate the average prediction accuracy of EXIF tag values on our YFCC test set, radial distortion estimation on Dresden dataset, and real-or-fake classification on CASIA I dataset. Rows with gray background (replicated for ease of comparison) represent the same model which is our \u201cfull\u201d model.\\n\\nMetadata supervision. We evaluate a variation of our model that trains using the image descriptions provided by YFCC100M in lieu of camera metadata, as well as models supervised by individual EXIF tags (Table 5). For the variations supervised by a single EXIF tag, we chose 14 common tags for this experiment, training a separate network for each one. The results of the per-tag evaluation is shown in Fig. 5. These results suggest that having access to the full metadata provides significantly better performance than using individual tags. Moreover, there is a wide variation in the performance of models that use different tag. This may be because the high performing tags, such as Camera Model, convey significantly more information about the full range of camera properties than others, such as Color Space and Sensing Method. These results suggest that a model that simply uses the full range of tags can extract significantly more camera information from the metadata. We also found that the variation trained on image descriptions (rather than EXIF text) performed significantly worse than other models.\\n\\nTag format. Since EXIF does not have a natural order of tags, we ask what will happen if we randomize the EXIF tag order during training. Table 5 shows the performance drops.\"}"}
{"id": "CVPR-2023-630", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Qualitative comparison to other methods. Our method can correctly localize splices in many scenarios where other methods fail. For example, EXIF-SC [33] fails on overexposed image regions; OSN [79] and CropCLR often segment scenes based on semantics.\\n\\nFor all three evaluations in this case. This may be due to the fact that the Transformer model is forced to learn meaningful positional embeddings corresponding to each EXIF tag if their order keeps on changing. We also tried removing the tag names from the camera metadata and just provide the values for those keys, e.g., replacing Make: Apple with Apple. Interestingly, this model performs on par with the model that has tag names, suggesting that the network can discern information about the tags from the values alone.\\n\\nText encoder architecture. To test whether performance of our model tied to a specific transformer architecture, we experimented with two different transformer models, DistilBERT [67] and ALBERT [44]. We see that both architectures obtain similar performance on all three tasks with DistilBERT slightly outperforming ALBERT. We also test how much pretraining the text encoder helps with the performance. From Table 5, we can see pretraining improves performance on the radial distortion and forensics tasks.\\n\\n5. Discussion\\n\\nIn this paper, we proposed to learn camera properties by training models to find cross-modal correspondences between images and camera metadata. To achieve this, we created a model that exploits the fact that EXIF metadata can easily be represented and processed as text. Our model achieves strong performance amongst self-supervised methods on a variety of downstream tasks that require understanding camera properties, including zero shot image forensics and radial distortion estimation. We see our work opening several possible directions. First, it opens the possibility of creating multimodal learning systems that use camera metadata as another form of supervision, providing complementary information to high-level modalities like language and sound. Second, it opens applications that require an understanding of low level sensor information, which may benefit from our feature sets.\\n\\nLimitations and Broader Impacts. We have shown that our learned features are useful for image forensics, which has potential to reduce the spread of disinformation [22]. The model that we will release may not be fully representative of the cameras in the wild, since it was trained only on photos available in the YFCC100M datatset [73].\\n\\nReferences\\n\\n[1] Ali Azarbayejani and Alex P Pentland. Recursive estimation of motion, structure, and focal length. IEEE Transactions on...\"}"}
