{"id": "zhao23d", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The batch dependency issue during TTA and non-trivial model selection, for evaluating SHOT on CIFAR10-C (gaussian noise). Similar trends can be found in all corruption types. SHOT suffers a significant decline in performance in an online adaptation setting, particularly when improper hyperparameters are chosen. Despite efforts to improve adaptation performance through the implementation of multiple adaptation steps, the problem of batch dependency remains unresolved.\\n\\nOracle model selection may introduce a more serious dependency problem to TTA. Figure 3(c) shows that utilizing an oracle model selection strategy in TTA methods under an online adaptation setup with sufficient adaptation steps initially improves adaptation performance in the first several test batches, compared to Figure 3(a) and 3(b). However, such improvement is short-lived, as the adaptation performance quickly drops in subsequent test batches. It suggests that the oracle model selection strategy exacerbates the batch dependency problem when considering its use in isolation. This phenomenon is consistent across various choices of learning rates. Additionally, we find the same problem in TENT and NOTE as shown in Figure 10 of appendix B.2.\\n\\nAuxiliary Regularization. Given the suboptimality of the oracle-based model selection, we further investigate the effect of auxiliary regularization on mitigating batch dependency. Specifically, we consider Fisher regularizer (Niu et al., 2022b) and stochastically restoring (Wang et al., 2022), two regularizers originally proposed for non-stationary distribution shifts. Our results in Figure 8 of appendix B.1 indicate that while these strategies may alleviate the negative effects of batch dependence to some extent, there is currently no principle to trade-off the adaptation and regularization within a test batch, and leave the challenge of balancing adaptation across batches untouched.\"}"}
{"id": "zhao23d", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\nFigure 4: The impact of model quality on TTA performance, in terms of OOD v.s. OOD (TTA) on CIFAR10-C. We save the checkpoints from the pre-training phase of ResNet-26 with standard augmentation and evaluate TTA performance on these checkpoints using oracle model selection. The OOD generalization performance has a significant impact on the overall performance (i.e. averaged accuracy of all corruption types) of various TTA methods. Our analysis reveals a strong correlation between model quality and the effectiveness of TTA methods. Furthermore, certain TTA methods, specifically SHOT, may not provide an improvement in performance on OOD datasets and may even result in a decrease in performance when applied to models of low quality.\\n\\n5. Pre-trained Model Bottlenecks TTA Efficacy\\n\\nRecall that several recent TTA methods outlined in Table 1 necessitate modifications of pre-training, which naturally results in inconsistent model qualities across methods and may deteriorate the test performance even before the TTA. In this section, we conduct a comprehensive and large-scale evaluation to examine the impact of base model quality on TTA performance across various TTA methods.\\n\\nEvaluation setups. We thoroughly examine the pre-trained model quality from the aspects of (1) disentangled feature extractor and classifier, and (2) data augmentation. 1. We consider a model with decoupled feature extractor and classifier. We keep the checkpoints with varying performance levels, generated from the pre-training phase using the standard data augmentation technique (mentioned below). We then fine-tune a trainable linear classifier for each frozen feature extractor from the checkpoints, using data with a uniform label distribution, to study the effect of the feature extractors (equivalently full model). To study the effect of the linear classifiers, we freeze a well-trained feature extractor and fine-tune trainable linear classifiers on several non-i.i.d. datasets created from a Dirichlet distribution; we further use Dirichlet distribution to create non-i.i.d. test data streams.\\n\\nOn the influence of the feature extractor (equivalently full model). The results of our study, as depicted in Figure 4, reveal a strong correlation between the performance of test-time augmentation and out-of-distribution generalization on CIFAR10-C. Our analysis shows that across a wide range of TTA methods, the OOD generalization performance is highly indicative of TTA performance. A quadratic regression line was found to be an appropriate fit for the data, suggesting that TTA methods are more effective when applied to models of higher (OOD) quality.\\n\\nOn the influence of the linear classifier. Our study has revealed that the performance of TTA methods is significantly impacted by the quality of the feature extractor used. The question then arises, can TTA methods bridge the distribution shift gap when equipped with a high-quality feature extractor and a suboptimal linear classifier? Our analysis, as shown in Figure 6(a)-(d), indicates that most TTA methods on CIFAR10-C are only able to mitigate the distribution shift gap when the label distribution of the target domain is identical to that of the source domain, at which point the classifier is considered optimal. In this case, SHOT attains a 5.4% error rate, the best result observed in test domain #0. However, it is clear that all TTA methods either perform worse than the baseline in the remaining 3 test domains or yield only marginal improvements over the baseline. These findings suggest that the quality of the classifier plays a crucial role in determining the performance of TTA methods.\\n\\nOn the influence of the data augmentation strategies. We investigate the impact of various augmentation policies.\"}"}
{"id": "zhao23d", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\n(a) ID v.s. OOD\\n(b) OOD v.s. OOD (BN_Adapt)\\n(c) OOD v.s. OOD (TENT)\\n(d) OOD v.s. OOD (SHOT)\\n\\nFigure 5: The impact of data augmentation policy on the TTA performance of the target domain. We save various sequences of checkpoints from the pre-training phase of ResNet-26 with five data augmentation policies and fine-tune each sequence to study the impact of data augmentation. TENT and SHOT use episodic adaptation with oracle model selection. Different data augmentation strategies have different corruption robustness, which causes varying generalization performance on CIFAR10-C. However, good practice in data augmentations and architecture designs for out-of-distribution generalization can be bad for test-time adaptation.\\n\\n| Domain | Baseline | BN_Adapt | T3A | TENT | SHOT | TTT | MEMO |\\n|--------|----------|----------|-----|------|------|-----|------|\\n| #0     | 12.6     | 96.5     | 79.9 | 76.1 | 76.1 |\\n| #1     | 11.1     | 98.4     | 85.8 | 80.7 |\\n| #2     | 22.0     | 96.0     | 77.5 | 74.8 |\\n| #3     | 5.4      | 95.0     | 72.1 | 67.4 |\\n\\nFigure 6: Adaptation performance (error) of TTA methods over CIFAR10-C with different label shifts. (a) test domain #0: $\\\\alpha = 0.1$, same label distribution with training environment. (b) test domain #1: $\\\\alpha = 0.1$, different label distribution with training environment. (c) test domain #2: $\\\\alpha = 1$. (d) test domain #3: uniformly distributed test stream. We investigate the impact of the degree of non-i.i.d.-ness in the fine-tuning dataset on the performance of the linear classifier. Label smoothing (Liang et al., 2020) technique is used to learn higher quality features. Our findings reveal that the quality of the linear classifier plays a crucial role in determining the effectiveness of TTA methods, as they can only enhance performance on test data that shares similar i.i.d.-ness and label distribution characteristics. Despite utilizing a well-trained feature extractor, the quality of the linear classifier remains a significant determining factor in the overall performance of TTA methods.\\n\\nWe conduct experiments to assess the impact of data augmentation strategies on the performance of ResNet-26 models trained on the CIFAR10 dataset. Our experimental results, as depicted in Figure 5 (more results in Figure 11 of appendix E.1), reveal that models pre-trained with the augmentation techniques like AugMix and PixMix exhibit superior OOD generalization performance on CIFAR10-C compared to models that do not utilize augmentation or only employ standard augmentations. Interestingly, even though these robust augmentation strategies significantly improve the robustness of the base model in the target domain, they only result in a marginal performance increase when combined with TTA. This disparity is particularly pronounced when compared to the performance of models trained with no augmentation or standard augmentations. However, when all models are fully trained in the source domain, the use of techniques such as AugMix and PixMix still leads to the best adaptation performance on CIFAR10-C, owing to their exceptional OOD generalization capabilities. We reach the same conclusion across both evaluation protocols and different architectures (e.g., WideResNet40-2) as shown in appendix E.1. In order to prove the influence of data augmentation strategies on TTA performance, we also conduct experiments on CCT, a computationally efficient variant of ViT and present experimental results in Figure 7. We highlight that good practice in strengthening the generalization performance of the base model in the target domain will decline its ability to bridge the distribution gap.\"}"}
{"id": "zhao23d", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2 and Table 3 summarize the results of our experiments. The efficacy of TTA is contingent upon the nature of distributional shifts. Table 2: Adaptation performance (error) of TTA methods over OOD datasets with common distribution shifts.\\n\\nHere our evaluation of TTA common distribution shifts. Evaluation setups can be found in appendix C.1. Our evaluation protocols previously outlined in \u00a74.3 to re-evaluate non-stationary shifts. In this section, we employ two evaluation protocols: episodic and online. Table 3: TTA performance on more challenging and realistic distributional shift settings. Additionally, we found that a 0% TTA methods mitigate all shifts yet. Table 3: TTA performance on more challenging and realistic distributional shift settings. Additionally, we found that a 0% TTA methods mitigate all shifts yet.\"}"}
{"id": "zhao23d", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\n(a) BN_Adapt\\n(b) TENT\\n(c) SHOT\\n\\n(d) BN_Adapt\\n(e) TENT\\n(f) SHOT\\n\\n(g) BN_Adapt\\n(h) TENT\\n(i) SHOT\\n\\nFigure 11: The effect of data augmentation on TTA performance in the target domain. TENT and SHOT use episodic adaptation with oracle model selection and choose ResNet-26 as the base model.\"}"}
{"id": "zhao23d", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\n(a) BN_Adapt\\n(b) TENT\\n(c) SHOT\\n\\n(d) BN_Adapt\\n(e) TENT\\n(f) SHOT\\n\\n(g) BN_Adapt\\n(h) TENT\\n(i) SHOT\\n\\nFigure 12: The effect of data augmentation on TTA performance in the target domain. TENT and SHOT use online adaptation without oracle model selection and grid search the best performance. We use ResNet-26 as the base model here.\"}"}
{"id": "zhao23d", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\n(a) BN_Adapt  \\n(b) TENT  \\n(c) SHOT  \\n\\nd) BN_Adapt  \\ne) TENT  \\ni) SHOT  \\n\\nFigure 13: The effect of data augmentation on TTA performance in the target domain. TENT and SHOT use episodic adaptation with oracle model selection and choose WideResNet40-2 as the base model.\\n\\n23\"}"}
{"id": "zhao23d", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\nHao Zhao\\nYuejiang Liu\\nAlexandre Alahi\\nTao Lin\\n\\nAbstract\\nTest-Time Adaptation (TTA) has recently emerged as a promising approach for tackling the robustness challenge under distribution shifts. However, the lack of consistent settings and systematic studies in prior literature hinders thorough assessments of existing methods. To address this issue, we present TTAB, a test-time adaptation benchmark that encompasses ten state-of-the-art algorithms, a diverse array of distribution shifts, and two evaluation protocols. Through extensive experiments, our benchmark reveals three common pitfalls in prior efforts. First, selecting appropriate hyper-parameters, especially for model selection, is exceedingly difficult due to online batch dependency. Second, the effectiveness of TTA varies greatly depending on the quality and properties of the model being adapted. Third, even under optimal algorithmic conditions, none of the existing methods are capable of addressing all common types of distribution shifts. Our findings underscore the need for future research in the field to conduct rigorous evaluations on a broader set of models and shifts, and to re-examine the assumptions behind the empirical success of TTA. Our code is available at https://github.com/lins-lab/ttab.\\n\\n1. Introduction\\nTackling the robustness issue under distribution shifts is one of the most pressing challenges in machine learning (Koh et al., 2021). Among existing approaches, Test-Time Adaptation (TTA)\u2014in which neural network models are adapted to new distributions by making use of unlabeled examples at test time\u2014has emerged as a promising paradigm of growing popularity (Lee et al., 2022; Kundu et al., 2022; Gong et al., 2022; Chen et al., 2022; Goyal et al., 2022; Sinha et al., 2023). Compared to other approaches, TTA offers two key advantages: (i) generality: TTA does not rest on strong assumptions regarding the structures of distribution shifts, which is often the case with Domain Generalization (DG) methods (Gulrajani & Lopez-Paz, 2021); (ii) flexibility: TTA does not require the co-existence of training and test data, a prerequisite of the Domain Adaptation (DA) approach (Ganin & Lempitsky, 2015). At the core of TTA is to define a proxy objective used at test time to adapt the model in an unsupervised manner. Recent works have proposed a broad array of proxy objectives, ranging from entropy minimization (Wang et al., 2021) and self-supervised learning (Sun et al., 2020) to pseudo-labeling (Liang et al., 2020) and feature alignment (Liu et al., 2021). Nevertheless, the efficacy of TTA in practice is often called into question due to restricted and inconsistent experimental conditions in prior literature (Boudiaf et al., 2022; Su et al., 2022).\\n\\nThe goal of this work is to gain a thorough understanding of the current state of TTA methods while setting the stage for critical problems to be worked on. To this end, we present TTAB, an open-sourced Test-Time Adaptation Benchmark featuring rigorous evaluations, comprehensive analyses as well as extensive baselines. Our benchmark carefully examines ten state-of-the-art TTA algorithms on a wide range of distribution shifts using two evaluation protocols. Specifically, we place a strong emphasis on subtle yet crucial experimental settings that have been largely overlooked in previous works. Our analyses unveil three common pitfalls in prior TTA methods:\\n\\nPitfall 1: Hyperparameters have a strong influence on the effectiveness of TTA, and yet they are exceedingly difficult to choose in practice without prior knowledge of distribution shifts. Our results show that the common practice of hyperparameter choice for TTA methods does not necessarily improve test accuracy and may instead lead to detrimental effects. Moreover, we find that even given the labels of test examples, selecting TTA hyperparameters remains challenging, primarily due to the batch dependency that arises during online adaptation.\\n\\nPitfall 2: The effectiveness of TTA may vary greatly across different models. In particular, not only the model accuracy in the source domain but also its feature properties have a strong influence on the result post-adaptation. Crucially, we\\n\\n\"}"}
{"id": "zhao23d", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\nfind that good practice in data augmentations (Hendrycks et al., 2019; 2022) for out-of-distribution generalization leads to adverse effects for TTA.\\n\\nPitfall 3: Even under ideal conditions where optimal hyperparameters are used in conjunction with suitable pre-trained models, existing methods still perform poorly on certain families of distribution shifts, such as correlation shifts (Sagawa et al., 2019) and label shifts (Sun et al., 2022), which are infrequently considered in the realm of TTA but widely used in domain adaptation and domain generalization. This observation, together with the previously mentioned issues, raises questions about the potential of TTA in addressing unconstrained distribution shifts in nature that are beyond our control.\\n\\nAside from these empirical results, our TTAB benchmark is designed as an expandable package that standardizes experimental settings and eases the integration of new algorithmic implementations. We hope our benchmark library will not only facilitate rigorous evaluations of TTA algorithms across a broader range of base models and distribution shifts, but also stimulate further research into the assumptions that underpin the viability of TTA in challenging scenarios.\\n\\n2. Related Work\\n\\nEarly methods of test-time adaptation involve updating the statistics and/or parameters associated with the batch normalization layers (Schneider et al., 2020; Wang et al., 2021). This approach has shown promising results in mitigating image corruptions (Hendrycks & Dietterich, 2019), but its efficacy is often limited to a narrow set of distribution shifts due to the restricted adaptation capacity (Burns & Steinhardt, 2021). To effectively update more parameters, e.g., the whole feature extractor, using unlabeled test examples, prior works have explored a wide array of proxy objectives.\\n\\nOne line of works designs TTA objectives by exploiting common properties of classification problems, e.g., entropy minimization (Liang et al., 2020; Fleuret et al., 2021; Zhou & Levine, 2021), class prototypes (Li et al., 2020; Su et al., 2022; Yang et al., 2022), pseudo labels (Rusak et al., 2022; Li et al., 2021a), and invariance to augmentation (Zhang et al.; Kundu et al., 2022). These techniques are restricted to the cross-entropy loss of the main tasks, and hence inherently inapplicable to regression problems, e.g., pose estimation (Li et al., 2021b).\\n\\nAnother line of research seeks more general proxies through self-supervised learning, e.g., rotation prediction (Sun et al., 2020), contrastive learning (Liu et al., 2021; Chen et al., 2022), and masked auto-encoder (Gandelsman et al.). While these methods are task-generic, they typically require modifications of the training process to accommodate an auxiliary self-supervised task, which can be non-trivial.\\n\\nSome recent works draw inspiration from related areas for robust test-time adaptation, such as feature alignment (Liu et al., 2021; Eastwood et al., 2022; Jiang & Lin, 2023), style transfer (Gao et al., 2022), and meta-learning (Zhang et al., 2021). Unfortunately, the absence of standardized experimental settings in the previous literature has made it difficult to compare existing methods. Instead of introducing yet another new method, our work revisits the limitations of prior methods through a large-scale empirical benchmark.\\n\\nClosely related to ours, Boudiaf et al. (2022) has recently shown that hyperparameters of TTA methods often need to be adjusted depending on the specific test scenario. Our results corroborate their observations and go one step further by taking an in-depth analysis of the online TTA setting. Our findings not only shed light on the challenge of model selection arising from batch dependency but also identify other prevalent pitfalls associated with the quality of pre-train models and the variety of distribution shifts.\\n\\n3. TTA Settings and Benchmark\\n\\nDespite the growing number of TTA methods summarized in \u00a72, their strengths and limitations are not well understood yet due to the lack of systematic and consistent evaluations. In this section, we will first revisit the concrete settings of prior efforts, highlighting a few factors that vary greatly across different methods. We will then propose an open-source TTA benchmark, with a particular emphasis on three aspects: standardization of hyper-parameter tuning, quality of pre-trained models, and variety of distribution shifts.\\n\\n3.1. Preliminary\\n\\nLet $D_S = \\\\{X_S, Y_S\\\\}$ be the data from the source domain $S$ and $D_T = \\\\{X_T, Y_T\\\\}$ be the data from the target domain $T$ to adapt to. Each sample and the corresponding true label pair $(x_i, y_i) \\\\in X_S \\\\times Y_S$ in the source domain follows a probability distribution $P_S(x, y)$. Similarly, each test sample from the target domain and the corresponding label at test time $t$, $(x_t, y_t) \\\\in X_T \\\\times Y_T$, follows a probability distribution $P_T(x, y)$ where $y_t$ is unknown for the learner.\\n\\n$f_{\\\\theta_o}(\\\\cdot)$ is a base model trained on labeled training data $\\\\{(x_i, y_i)\\\\}_{i=1}^N$, where $\\\\theta_o$ denotes the base model parameters. During the inference time, the pre-trained base model may suffer from a substantial performance drop in the face of out-of-distribution test samples, namely $x_t \\\\sim P_T(x)$, where $P_T(x) \\\\neq P_S(x)$. Unlike traditional DA that uses $D_S$ and $X_T$ collected beforehand for adaptation, TTA adapts the pre-trained model $f_{\\\\theta_o}(\\\\cdot)$ from $D_S$ on the fly by utilizing unlabeled sample $x_t$ obtained at test time $t$.\\n\\n3.2. Inconsistent Settings in Prior Work\\n\\nTo gain a comprehensive understanding of the experimental settings used in previous studies, we outline in Table 1 some key factors that characterize the adaptation procedure. We observe that, despite a restricted selection of factors, existing TTA methods still exhibit substantial variation in the following three aspects:\\n\\n- **Hyperparameter Selection**: The range of hyperparameters explored in prior work varies widely, from a single value to a range of values.\\n- **Model Quality**: The quality of the pre-trained model is often not specified, making it difficult to compare the performance across different methods.\\n- **Distribution Shifts**: The variety of distribution shifts considered in prior studies is limited, often focusing on specific types of shifts such as corruptions or label shifts.\\n\\nThese inconsistencies highlight the need for a standardized benchmark to facilitate fair and comprehensive evaluations of TTA methods.\"}"}
{"id": "zhao23d", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\nTable 1: Comparison of experimental settings used in prior TTA methods. The inconsistent settings of hyperparameter tuning (\u00a74), pre-trained models (\u00a75), and distribution shifts (\u00a76) may yield different observations. More details are summarized in appendix A.\\n\\n| Methods         | Venue          | Nb. Hyperparameters | Reset Model | Batch-Norm Adjust | Pre-training | Distribution Shifts |\\n|-----------------|----------------|---------------------|-------------|-------------------|--------------|---------------------|\\n| TTT (Sun et al., 2020) | ICML 2020      | \u2717                   | \u2713           | \u2717                 | \u2713            | co-var. & non-stat. & natural shifts |\\n| SHOT (Liang et al., 2020) | ICML 2020      | \u2717                   | \u2717           | \u2717                 |             | domain gen. shifts   |\\n| BN_Adapt (Schneider et al., 2020) | NeurIPS 2020  | \u2717                   | \u2713           | \u2717                 |             | co-var. & natural shifts |\\n| TENT (Wang et al., 2021) | ICLR 2021      | \u2717                   | \u2713           | \u2717                 |             | co-var. & domain gen. shifts |\\n| TTT++ (Liu et al., 2021) | NeurIPS 2021  | \u2717                   | \u2713           | \u26ab                 | \u2713            | co-var. & domain gen. & natural shifts |\\n| T3A (Iwasawa & Matsuo, 2021) | NeurIPS 2021  | \u2717                   | \u2717           | \u2717                 |             | domain gen. shifts   |\\n| EATA (Niu et al., 2022a) | ICML 2022      | \u2717                   | \u2713           | \u2717                 |             | co-var. & non-stat. shifts |\\n| Conjugate PL (Goyal et al., 2022) | NeurIPS 2022  | \u2717                   | \u2713           | \u2717                 |             | co-var. & domain gen. shifts |\\n| MEMO (Zhang et al.) | NeurIPS 2022  | \u2713                   | \u2717           | \u2717                 |             | co-var. & natural shifts |\\n| NOTE (Gong et al., 2022a) | NeurIPS 2022  | \u2717                   | \u2713           | \u2713                 |             | co-var. & non-stat. shifts |\\n| SAR (Niu et al., 2023) | ICLR 2023      | \u2713                   | \u2717           | \u2717                 |             | co-var. & label shifts |\\n\\nHyperparameter. TTA methods typically require the specification of hyperparameters such as the learning rate, the number of adaptation steps, as well as other method-specific choices. However, prior research often lacks detailed discussions on how these hyperparameters were tuned. In fact, there is no consensus on even simple hyperparameters, such as whether to reset the model during adaptation. Some TTA methods are episodic, performing adaptations on the base model \\\\( \\\\theta_0 \\\\) for every adaptation step. Conversely, some other TTA methods adapt models \\\\( \\\\theta^* \\\\) in an online manner, leading to stronger dependency across batches and thereby further amplifying the importance of hyperparameter tuning, which we will elaborate in \u00a74.\\n\\nPre-trained Model. The choice of pre-trained models constitutes another prominent source of inconsistency in prior research. Earlier TTA methods often hinge on models with BatchNorm (BN) layers, while more recent ones start to incorporate modern architectures, such as GroupNorm (GN) layers and Vision Transformers. Besides model architectures, the pre-training procedure in the source domain also varies significantly due to the use of auxiliary training objectives and data augmentation techniques, among other factors. These variations not only affect the capacity and quality of the pre-trained model, but may also lead to different efficacies of TTA methods, as discussed in \u00a75.\\n\\nDistribution Shift. The most compelling property of TTA is, arguably, its potential to handle various distribution shifts depending on the encountered test examples. However, prior work often considers a narrow selection of distribution shifts biased toward the designed method. For instance, some methods (Iwasawa & Matsuo, 2021) undergo extensive evaluations on domain generalization benchmarks, while a few others (Sun et al., 2020; Wang et al., 2021) concentrate more on image corruption. As such, the efficacy of existing TTA methods under a wide spectrum of distribution shifts remains contentious, which we will further investigate in \u00a76.\\n\\n3.3. Our Proposed TTA Benchmark\\n\\nIn order to address the aforementioned inconsistencies and unify the evaluation of TTA methods, we present an open-source Test-Time Adaptation Benchmark, dubbed TTAB. Our TTAB features standardized experimental settings, extensive baseline methods as well as comprehensive evaluation protocols that enable rigorous comparisons of different methods.\\n\\nStandardized Settings. To streamline standardized evaluations of TTA methods, we first equip the benchmark library with shared data loaders for a set of common datasets, including CIFAR10-C (Hendrycks & Dietterich, 2019), CIFAR10.1 (Recht et al., 2018), ImageNet-C (Hendrycks & Dietterich, 2019), OfficeHome (Venkateswara et al., 2017), PACS (Li et al., 2017), ColoredMNIST (Arjovsky et al., 2019), and Waterbirds (Sagawa et al., 2019). These datasets allow us to examine each TTA method under various shifts, ranging from common image corruptions and natural style shifts that are widely used in prior literature to time-varying shifts and spurious correlation shifts that remain underexplored in the field, as detailed in appendix D.\\n\\nTo enable greater flexibility and extensibility that can go beyond existing settings, we further introduce a fine-grained formulation to capture a wide spectrum of empirical data distribution shifts. Specifically, we generalize the notations in \u00a73.1 and decompose data into an underlying set of factors of variations, i.e., we assume a joint distribution \\\\( P \\\\) of (i) inputs \\\\( x \\\\) and (ii) corresponding attributes \\\\( a_1:K = \\\\{a_1, \\\\ldots, a_k, \\\\ldots, a_K\\\\} \\\\), where the values of attribute \\\\( a_k \\\\) are sampled from a finite set. As shown in Figure 1(a), the empirical data distribution is characterized by the underlying distribution of attribute values \\\\( P(a_1:K) \\\\), sampling operators (e.g., \\\\# of sampling trials and sampling distribution), and the concatenation of sampled data.\\n\\nFigure 1: A generic formulation of distribution shifts, where \\\\( P(a_1:K) \\\\) is characterized by some attributes, for instance, two image styles and one target label.\"}"}
{"id": "zhao23d", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation over time-slots. Figure 1(b) exemplifies the distribution of data $P_{a_1:K}$ through three attributes. This formulation encompasses several kinds of distribution shifts, wherein the test data $P_T$ deviates from the training data $P_S$ across all time slots:\\n\\n1. **attribute-relationship shift** (a.k.a. spurious correlation): attributes are correlated differently between $P_S$ and $P_T$.\\n2. **attribute-values shift**: the distribution of attribute values under $P_S$ are different from that of $P_T$. Its extreme case generalizes to the shift that some attribute values are unseen under $P_S$ but are under $P_T$.\\n\\n**Extendable Baselines.** Given the rich set of distribution shifts described above, we benchmark 11 TTA methods: Batch Normalization Test-time Adaptation (BN_Adapt (Schneider et al., 2020)), Test-time Entropy Minimization (TENT (Wang et al., 2021)), Test-time Template Adjuster (T3A (Iwasawa & Matsuo, 2021)), Source Hypothesis Transfer (SHOT (Liang et al., 2020)), Test-time Training (TTT (Sun et al., 2020)), Marginal Entropy Minimization (MEMO (Zhang et al.)), Non-i.i.d. Test-time Adaptation (NOTE (Gong et al., 2022a)), Continual Test-time Adaptation (CoTTA (Wang et al., 2022)), Conjugate Pseudo-labels (Conjugate PL (Goyal et al., 2022)), Sharpness-aware Entropy Minimization (SAR (Niu et al., 2023)), and Fisher Regularizer (Niu et al., 2022a). These algorithms are implemented in a modular manner to support the seamless integration of other components, such as different model selection strategies. More implementation details of the TTAB can be found in appendix C.2.\\n\\n### 4. Batch Dependency Obstructs TTA Tuning\\n\\nAs summarized in Table 1, TTA methods often come with a number of hyper-parameters, ranging from at least one up to six. Yet, the influence of these hyper-parameters on adaptation outcomes, as well as the optimal strategies for tuning them, remains poorly understood. In this section, we will first shed light on these issues by examining the sensitivity of previous methods to hyperparameter tuning. We will further investigate the underlying challenge by looking into the online adaptation dynamics through the lens of batch dependency. We will finally propose two evaluation protocols that enable a more objective assessment of TTA methods through upper-bound performance estimates.\\n\\n#### 4.1. Sensitivity to Hyperparameter Tuning\\n\\n**Empirical Sensitivity.** To understand the importance of hyperparameter choices, we start by re-evaluating two renowned TTA methods, TENT and SHOT, with hyper-parameters deviated away from the default values. Figure 2 shows the test accuracy on the CIFAR10-C dataset resulting from different learning rates and adaptation steps. We observe that the effectiveness of TTA methods is highly sensitive to both two considered hyperparameters. Notably, the risk of over-adaptation raises a practical question: when should we terminate TTA given a stream of test examples? We next examine the challenge of model selection in the online TTA setting.\\n\\n#### 4.2. Difficulty of TTA Model Selection\\n\\nModel selection has recently gained great attention in the field of Domain Generalization (Gulrajani & Lopez-Paz, 2021) and Domain Adaptation (You et al., 2019). Yet, its importance and necessity in the context of TTA have been largely unexplored. We seek to shed light on this by exploring model selection in two paradigms: (i) with oracle information and (ii) with auxiliary regularization.\\n\\n**Oracle Information.** We first consider an oracle setting, where we assume access to true labels and select the optimal model (with early stopping) for each test batch with a sufficient number of adaptation steps. This approach is expected to achieve the highest possible adaptation performance per\"}"}
{"id": "zhao23d", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Contents of Appendix\\n\\nA Messages 13\\n\\nB The Limits of Evaluation for TTA Methods 14\\nB.1 Recent Regularization Techniques Proposed to Resist Batch Dependency Problem 14\\nB.2 Optimal Model Selection for TTA is Non-trivial 14\\n\\nC Implementation Details 14\\nC.1 Implementation Details of TTA Methods 14\\nC.2 Implementation Details of TTAB Methods 15\\n\\nD Datasets 15\\n\\nE Model Quality 15\\nE.1 On the Influence of Data Augmentation 15\\n\\nF Additional Results 16\\nF.1 TTA on Label Shifts 16\\nF.2 Empirical Studies of Normalization Layers Effects in TTA 16\\n\\nG Additional Related Work 17\\n\\nA. Messages\\nWe summarize some key messages of the manuscript here.\\n\\nLimit 1: unfair evaluation in TTA\\n\u2022 Methods are evaluated under distinct model statuses and experimental setups, e.g.,\\n  1. model quality used for the adaptation\\n  2. pretraining procedure\\n  3. optimizer used for the adaptation\\n  4. learning rate\\n  5. # of the adaptation steps per test mini-batch\\n  6. size of the test min-batch\\n  7. online v.s. offline adaptation\\n  8. w/ v.s. w/o resetting model (episodic v.s. online)\\n\u2022 Methodology designs are biased to some specific neural architectures, and TTA methods cannot be fairly evaluated over various neural architectures;\\n\\nLimit 2: pitfalls of model selection in TTA\\n\u2022 due to the lack of validation set and label information during test time.\\n\u2022 batch-dependency issue emerged in the streaming test mini-batches makes the oracle model selection method challenging.\\n\\nNote that the domain generalization field only starts to examine the time-varying scenarios very recently (Yao et al., 2022).\"}"}
{"id": "zhao23d", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Take-away messages\\n\\n\u2022 Improper evaluation in TTA methods. Hyperparameters have a strong influence on the effectiveness of TTA, and yet they are exceedingly difficult to choose in practice without prior knowledge of the properties and structures of distribution shifts. Even when the labels of test examples are available, selecting the TTA hyperparameters for model selection remains challenging, largely due to batch dependency during online adaptation.\\n\\n\u2022 Batch dependency is a significant issue restricting the performance of online TTA methods. Tackling the batch dependency issue of TTA methods or enabling effective model selection methods is beyond the scope of this manuscript and we leave it to the whole community for future work.\\n\\n\u2022 Pre-trained model quality matters for TTA methods. Even if hyperparameters are optimally selected given oracle information in the test domain, the effectiveness of TTA is not equal on different models. The degree of improvement strongly depends on the quality of the pre-trained model, not only on its accuracy in the source domain but also on its feature properties. Good practice in data augmentations (Hendrycks et al., 2019; 2022) for out-of-distribution generalization leads to reverse effects for TTA.\\n\\n\u2022 The community of TTA needs a comprehensive benchmark such as TTAB to guard effective progress. For example, even under ideal conditions where optimal hyperparameters are used in conjunction with suitable pre-trained models, existing methods still perform poorly on certain classes of distribution shifts, such as correlation shifts (Sagawa et al., 2019) and label shifts (Sun et al., 2022).\\n\\nB. The Limits of Evaluation for TTA Methods\\n\\nB.1. Recent Regularization Techniques Proposed to Resist Batch Dependency Problem\\n\\n\u2022 On the influence of batch dependency problem as shown in Figure 8\\n\\n\u2022 Stochastic restoring model parameters and Fisher regularizer still show large variance when considering multiple adaptation steps as shown in Figure 9.\\n\\nB.2. Optimal Model Selection for TTA is Non-trivial\\n\\nOracle model selection protocol also fails to solve the batch dependency issue in TENT and NOTE as shown in Figure 10\\n\\nC. Implementation Details\\n\\nC.1. Implementation Details of TTA Methods\\n\\nFollowing prior work (Gulrajani & Lopez-Paz, 2021; Sun et al., 2020; Wang et al., 2022), we use ResNet-18/ResNet-26/ResNet-50 as the base model on ColoredMNIST/CIFAR10-C/large-scale image datasets and always choose SGDm as\"}"}
{"id": "zhao23d", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\nWe choose method-specific hyperparameters following prior work. Following Iwasawa & Matsuo (2021), we assign the pseudo label in SHOT if the predictions are over a threshold which is 0.9 in our experiment and utilize $\\\\beta = 0.3$ for all experiments except $\\\\beta = 0.1$ for ColoredMNIST just as Liang et al. (2020). We set the number of augmentations $B = 32$ for small-scale images (e.g. CIFAR10-C, CIFAR100-C) and $B = 64$ for large-scale image sets like ImageNet-C, because this is the default option in Sun et al. (2020) and Zhang et al. We simply set $N = 0$ that controls the trade-off between source and estimated target statistics because it achieves performance comparable to the best performance when using a batch size of 64 according to Schneider et al. (2020). Training-domain validation data is used to determine the number of supports to store in T3A following Iwasawa & Matsuo (2021). We keep the average performance on the dataset if it has multiple test domains (e.g., CIFAR10-C, OfficeHome) and calculate the standard deviation over three different trials {2022, 2023, 2024}. We always examine the highest severity of corrupted data throughout our study.\\n\\nC.2. Implementation Details of TTAB Methods\\n\\nTo establish a consistent and realistic evaluation framework for TTA methods, we have implemented several key choices. In contrast to the inconsistent pre-training strategies employed in previous studies, we have adopted a self-supervised learning approach utilizing the rotation prediction task as an auxiliary head, in conjunction with standard data augmentation techniques. This allows us to include TTT variants and maintain a consistent level of model quality across different TTA methods.\\n\\nFor TTA methods that adapt a single image at a time (such as MEMO and TTT), we have modified the optimization procedure to accommodate larger batch sizes. Specifically, we have fixed the model parameters and accumulated gradients computed for each sample in a batch, only updating the model parameters once all samples have been adapted in a batch. Such a design excludes the unfairness caused by varied mini-batch sizes.\\n\\nWe have utilized Stochastic Gradient Descent with momentum for TTA throughout all experiments conducted in this work (see the discrepancy in Table 1).\\n\\nD. Datasets\\n\\nTTAB includes downloaders and loaders for all image classification tasks considered in our work:\\n\\n- ColoredMNIST (Arjovsky et al., 2019) is a variant of the MNIST handwritten digit classification dataset. Domain $d \\\\in \\\\{0.1, 0.3, 0.9\\\\}$ contains a disjoint set of digits colored either red or blue. The label is a noisy function of the digit and color, such that color bears correlation $d$ with the label and the digit bears correlation 0.75 with the label. This dataset contains 70000 examples of dimension $(28, 28, 28)$ and 2 classes.\\n\\n- OfficeHome (Venkateswara et al., 2017) comprises four domains $d \\\\in \\\\{art, clipart, product, real\\\\}$. This dataset contains 15,588 examples of dimension $(224, 224)$ and 65 classes.\\n\\n- PACS (Li et al., 2017) comprises four domains $d \\\\in \\\\{art, cartoons, photos, sketches\\\\}$. This dataset contains 9,991 examples of dimension $(224, 224)$ and 65 classes.\\n\\n- CIFAR10 (Krizhevsky et al., 2009) consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\\n\\n- CIFAR10-C (Hendrycks & Dietterich, 2019) is a dataset generated by adding 15 common corruptions + 4 extra corruptions to the test images in the Cifar10 dataset.\\n\\n- CIFAR10.1 (Recht et al., 2018) contains roughly 2,000 new test images that were sampled after multiple years of research on the original CIFAR-10 dataset. The data collection for CIFAR-10.1 was designed to minimize distribution shift relative to the original dataset.\\n\\n- Waterbirds (Sagawa et al., 2019) is constructed by cropping out birds from photos in the Caltech-UCSD Birds-200-2011 (CUB) dataset and transferring them onto backgrounds from the Places dataset.\\n\\nE. Model Quality\\n\\nE.1. On the Influence of Data Augmentation\\n\\nIn Figure 11, Figure 12, and Figure 13, we show more data augmentation results across different model architectures and different evaluation protocols.\"}"}
{"id": "zhao23d", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we report the additional results of TTA methods on the CIFAR10-100 dataset, which is another common benchmark for\\nTTA can perform more stably with batch-agnostic norm layers, i.e., group or layer norm. Here we revisit TTA performance\\nThe most recent work (Niu et al., 2023) dug into the effects of normalization layers on TTA performance and found that\\nThe efficacy of most TTA methods drops substantially when confronted with label shifts regardless of the data itself. Here\\nTable 6: Results of TTA performance on\\nTable 5:\\nF. Additional Results\"}"}
{"id": "zhao23d", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The experimental results indicate that, even with the appropriate model selection, we reproduce NOTE in TTAB, which is the current CIFAR10-C dataset introduced in Gong et al. (2022a). Additionally, TTA methods that heavily rely on the test batch for recalculating Batch Normalization statistics, such as TENT, experience over 76.0% error rate when the distribution is used to create non-i.i.d. test streams; the smaller value of $\\\\alpha$ results in test-time adaptation degeneration, with BN_Adapt incurring a 77.8% test error. We note that, in contrast, our work solely examines the effectiveness of various TTA implementations, namely the difficulty in selecting appropriate hyperparameter choices, and adheres to the evaluation through the use of commonly accepted practices in terms of the utilization of impractical model selection techniques. The datasets with two realistic distribution shifts.\\n\\nTable 3: Performance in the presence of non-stationary shifts. How-\"}"}
{"id": "zhao23d", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\nReferences\\n\\nArjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.\\n\\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n\\nBeery, S., Liu, Y., Morris, D., Piavis, J., Kapoor, A., Joshi, N., Meister, M., and Perona, P. Synthetic examples improve generalization for rare classes. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 863\u2013873, 2020.\\n\\nBlanchard, G., Lee, G., and Scott, C. Generalizing from several related classification tasks to a new unlabeled sample. Advances in neural information processing systems, 24, 2011.\\n\\nBoudiaf, M., Mueller, R., Ben Ayed, I., and Bertinetto, L. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8344\u20138353, 2022.\\n\\nBurns, C. and Steinhardt, J. Limitations of post-hoc feature alignment for robustness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2525\u20132533, 2021.\\n\\nChen, D., Wang, D., Darrell, T., and Ebrahimi, S. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 295\u2013305, 2022.\\n\\nEastwood, C., Mason, I., Williams, C., and Sch\u00f6lkopf, B. Source-free adaptation to measurement shift via bottom-up feature restoration. In International Conference on Learning Representations, 2022.\\n\\nFleuret, F. et al. Uncertainty reduction for model adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9613\u20139623, 2021.\\n\\nGandelsman, Y., Sun, Y., Chen, X., and Efros, A. A. Test-time training with masked autoencoders. In Advances in Neural Information Processing Systems.\\n\\nGanin, Y. and Lempitsky, V. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pp. 1180\u20131189. PMLR, 2015.\\n\\nGao, J., Zhang, J., Liu, X., Darrell, T., Shelhamer, E., and Wang, D. Back to the source: Diffusion-driven test-time adaptation. arXiv preprint arXiv:2207.03442, 2022.\\n\\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., and Brendel, W. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.\\n\\nGong, T., Jeong, J., Kim, T., Kim, Y., Shin, J., and Lee, S.-J. Note: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems (NeurIPS), 2022a.\\n\\nGong, T., Jeong, J., Kim, T., Kim, Y., Shin, J., and Lee, S.-J. Robust continual test-time adaptation: Instance-aware bn and prediction-balanced memory. arXiv preprint arXiv:2208.05117, 2022b.\\n\\nGoyal, S., Sun, M., Raghunathan, A., and Kolter, Z. Test-time adaptation via conjugate pseudo-labels. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\nGulrajani, I. and Lopez-Paz, D. In search of lost domain generalization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=lQdXeXDoWtI.\\n\\nHendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HJz6tiCqYm.\\n\\nHendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J., and Lakshminarayanan, B. Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019.\\n\\nHendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8340\u20138349, 2021a.\\n\\nHendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples. CVPR, 2021b.\\n\\nHendrycks, D., Zou, A., Mazeika, M., Tang, L., Li, B., Song, D., and Steinhardt, J. Pixmix: Dreamlike pictures comprehensively improve safety measures. CVPR, 2022.\\n\\nIwasawa, Y. and Matsuo, Y. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427\u20132440, 2021.\\n\\nJiang, L. and Lin, T. Test-time robust personalization for federated learning. In International Conference on Learning Representations, 2023.\"}"}
{"id": "zhao23d", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\nKoh, P. W., Sagawa, S., Marklund, H., Xie, S. M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R. L., Gao, I., et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637\u20135664. PMLR, 2021.\\n\\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.\\n\\nKundu, J. N., Kulkarni, A. R., Bhambri, S., Mehta, D., Kulkarni, S. A., Jampani, V., and Radhakrishnan, V. B. Balancing discriminability and transferability for source-free domain adaptation. In International Conference on Machine Learning, pp. 11710\u201311728. PMLR, 2022.\\n\\nLee, J., Jung, D., Yim, J., and Yoon, S. Confidence score for source-free unsupervised domain adaptation. In International Conference on Machine Learning, pp. 12365\u201312377. PMLR, 2022.\\n\\nLi, D., Yang, Y., Song, Y.-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pp. 5542\u20135550, 2017.\\n\\nLi, R., Jiao, Q., Cao, W., Wong, H.-S., and Wu, S. Model adaptation: Unsupervised domain adaptation without source data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9641\u20139650, 2020.\\n\\nLi, X., Chen, W., Xie, D., Yang, S., Yuan, P., Pu, S., and Zhuang, Y. A free lunch for unsupervised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 8474\u20138481, 2021a.\\n\\nLi, Y., Hao, M., Di, Z., Gundavarapu, N. B., and Wang, X. Test-time personalization with a transformer for human pose estimation. Advances in Neural Information Processing Systems, 34:2583\u20132597, 2021b.\\n\\nLiang, J., Hu, D., and Feng, J. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML), pp. 6028\u20136039, 2020.\\n\\nLiu, Y., Kothari, P., van Delft, B., Bellot-Gurlet, B., Mordan, T., and Alahi, A. TTT++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808\u201321820, 2021.\\n\\nLong, M., Cao, Y., Wang, J., and Jordan, M. Learning transferable features with deep adaptation networks. In International conference on machine learning, pp. 97\u2013105. PMLR, 2015.\\n\\nMehrabi, N., Morstatter, F., Saxena, N., Lerman, K., and Galstyan, A. A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6):1\u201335, 2021.\\n\\nMuandet, K., Balduzzi, D., and Sch\u00f6lkopf, B. Domain generalization via invariant feature representation. In International conference on machine learning, pp. 10\u201318. PMLR, 2013.\\n\\nNiu, S., Wu, J., Zhang, Y., Chen, Y., Zheng, S., Zhao, P., and Tan, M. Efficient test-time model adaptation without forgetting. In Proceedings of the 39th International Conference on Machine Learning, pp. 16888\u201316905, 2022b.\\n\\nNiu, S., Wu, J., Zhang, Y., Chen, Y., Zheng, S., Zhao, P., and Tan, M. Towards stable test-time adaptation in dynamic wild world. In International Conference on Learning Representations, 2023.\\n\\nPeng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., and Wang, B. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1406\u20131415, 2019.\\n\\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do cifar-10 classifiers generalize to cifar-10? 2018. https://arxiv.org/abs/1806.00451.\\n\\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pp. 5389\u20135400. PMLR, 2019.\\n\\nRusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P. V., Bringmann, O., Brendel, W., and Bethge, M. If your data distribution shifts, use self-learning. Transactions on Machine Learning Research, 2022. URL https://openreview.net/forum?id=vqRzLv6POg.\\n\\nSagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.\\n\\nSchneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., and Bethge, M. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539\u201311551, 2020.\\n\\nSinha, S., Gehler, P., Locatello, F., and Schiele, B. Test: Test-time self-training under distribution shift. In Proceedings of the 39th International Conference on Machine Learning, 2022.\"}"}
{"id": "zhao23d", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2759\u20132769, 2023.\\n\\nSu, Y., Xu, X., and Jia, K. Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering. In Advances in Neural Information Processing Systems, 2022.\\n\\nSun, Q., Murphy, K., Ebrahimi, S., and D'Amour, A. Beyond invariance: Test-time label-shift adaptation for distributions with \\\"spurious\\\" correlations. arXiv preprint arXiv:2211.15646, 2022.\\n\\nSun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pp. 9229\u20139248. PMLR, 2020.\\n\\nTsai, Y.-H., Hung, W.-C., Schulter, S., Sohn, K., Yang, M.-H., and Chandraker, M. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7472\u20137481, 2018.\\n\\nVapnik, V. N. Statistical Learning Theory. Wiley-Interscience, 1998.\\n\\nVenkateswara, H., Eusebio, J., Chakraborty, S., and Panchanathan, S. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5018\u20135027, 2017.\\n\\nWang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c.\\n\\nWang, Q., Fink, O., Van Gool, L., and Dai, D. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201\u20137211, 2022.\\n\\nWu, Y. and He, K. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3\u201319, 2018.\\n\\nYang, S., Wang, Y., Wang, K., Jui, S., et al. Attracting and dispersing: A simple approach for source-free domain adaptation. In Advances in Neural Information Processing Systems, 2022.\\n\\nYao, H., Choi, C., Cao, B., Lee, Y., Koh, P. W., and Finn, C. Wild-time: A benchmark of in-the-wild distribution shift over time. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.\\n\\nYou, K., Wang, X., Long, M., and Jordan, M. Towards accurate model selection in deep unsupervised domain adaptation. In International Conference on Machine Learning, pp. 7124\u20137133. PMLR, 2019.\\n\\nZhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\\n\\nZhang, M., Marklund, H., Dhawan, N., Gupta, A., Levine, S., and Finn, C. Adaptive risk minimization: Learning to adapt to domain shift. Advances in Neural Information Processing Systems, 34:23664\u201323678, 2021.\\n\\nZhang, M. M., Levine, S., and Finn, C. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems.\\n\\nZhou, A. and Levine, S. Bayesian adaptation for covariate shift. Advances in Neural Information Processing Systems, 34:914\u2013927, 2021.\"}"}
{"id": "zhao23d", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Results of TTA performance on ViTSmall (LN)\\n\\nDomain Generalization\\n\\nthe utilization of discrepancy losses (Long et al., 2015) or adversarial training (Ganin & Lempitsky, 2015; Tsai et al., 2018).\\n\\nUnsupervised Domain Adaptation (UDA) is a technique aimed at enhancing\\n\\nOn Pitfalls of Test-Time Adaptation\\n\\nerror in (%): optimal results in episodic & online are highlighted by bold.\\n\\nResNet50-GN\\n\\nViTSmall (LN)\"}"}
{"id": "zhao23d", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Distribution Shift Benchmarks. Distribution shift has been widely studied in the machine learning community. Prior works have covered a wide range of distribution shifts. The first line of such benchmarks applies different transformations to object recognition datasets to induce distribution shifts. These benchmarks include: (1) CIFAR10-C & ImageNet-C (Hendrycks & Dietterich, 2019), ImageNet-A (Hendrycks et al., 2021b), ImageNet-R (Hendrycks et al., 2021a), ImageNet-V2 (Recht et al., 2019), and many others; (2) ColoredMNIST (Arjovsky et al., 2019), which makes the color of digits a confounder. Most recent benchmarks collect sets of images with various styles and backgrounds, such as PACS (Li et al., 2017), OfficeHome (Venkateswara et al., 2017), DomainNet (Peng et al., 2019), and Waterbirds (Sagawa et al., 2019). Unlike most prior works that assume a specific stationary target domain, the study on continuous TTA that considers continually changing target data becomes more and more popular in the field. Recently, a few works have constructed datasets and benchmarks for scenarios under temporal shifts. Gong et al. (2022b) builds a temporally correlated test stream on CIFAR10-C sample by a Dirichlet distribution, where most existing TTA methods fail dramatically. Wild-Time (Yao et al., 2022) benchmark consists of 5 datasets that reflect temporal distribution shifts arising in a variety of real-world applications, including patient prognosis and news classification. Studies on fairness and bias (Mehrabi et al., 2021) have investigated the detrimental impact of spurious correlation in classification (Geirhos et al., 2018) and conservation (Beery et al., 2020). To our knowledge, there have been rare TTA work focused on tackling spurious correlation shifts.\\n\\n| Noise | Blur | Weather | Digital | Avg. |\\n|-------|------|---------|---------|------|\\n| Model | + Method | Gauss. | Shot Impul. | Defoc. | Glass | Motion | Zoom | Snow | Frost | Fog | Brit. | Contr. | Elastic | Pixel | JPEG |\\n| ViTBase (LN) | 74.1 | 78.2 | 75.4 | 70.1 | 78.6 | 67.5 | 73.1 | 84.2 | 75.3 | 52.8 | 46.4 | 66.9 |\\n| \u2022 SHOT-episodic | 56.7 | 56.7 | 56.0 | 52.6 | 59.7 | 50.1 | 52.4 | 43.3 | 45.3 | 39.4 | 26.3 | 41.6 |\\n| \u2022 SHOT-online | 73.2 | 60.5 | 59.5 | 63.9 | 57.5 | 49.2 | 42.2 | 42.9 | 46.6 | 34.0 | 24.8 | 60.8 |\\n| \u2022 TENT-episodic | 73.4 | 77.3 | 74.7 | 69.0 | 78.0 | 66.7 | 72.3 | 83.4 | 74.2 | 52.1 | 45.1 | 55.8 |\\n| \u2022 TENT-online | 50.5 | 50.1 | 51.9 | 44.8 | 45.5 | 39.4 | 42.8 | 41.6 | 46.9 | 34.4 | 29.1 | 34.7 |\\n| \u2022 T3A | 74.7 | 78.9 | 75.8 | 70.5 | 78.9 | 67.6 | 72.7 | 84.6 | 75.5 | 51.8 | 46.0 | 67.0 |\\n| \u2022 CoTTA-episodic | 98.6 | 98.6 | 99.1 | 95.5 | 97.8 | 92.8 | 88.1 | 86.9 | 97.3 | 92.6 | 55.1 | 95.6 |\\n| \u2022 CoTTA-online | 99.4 | 99.5 | 99.5 | 99.6 | 99.7 | 99.4 | 99.3 | 99.2 | 99.6 | 99.1 | 92.5 | 98.7 |\\n| \u2022 MEMO-episodic | 68.8 | 74.3 | 70.4 | 60.1 | 66.6 | 55.7 | 57.0 | 54.3 | 58.4 | 45.4 | 23.9 | 42.8 |\\n| \u2022 NOTE-episodic | 74.1 | 78.1 | 75.4 | 70.0 | 78.9 | 67.6 | 72.7 | 84.6 | 75.5 | 51.8 | 46.0 | 67.0 |\\n| \u2022 NOTE-online | 72.0 | 75.4 | 73.2 | 67.2 | 76.7 | 64.9 | 70.7 | 79.4 | 71.8 | 55.0 | 41.9 | 54.3 |\\n| \u2022 Conjugate PL-episodic | 69.1 | 72.8 | 70.2 | 65.1 | 74.3 | 63.1 | 68.7 | 80.7 | 71.7 | 48.7 | 41.4 | 62.4 |\\n| \u2022 Conjugate PL-online | 80.7 | 75.5 | 84.0 | 45.3 | 48.3 | 40.3 | 43.3 | 91.4 | 96.0 | 29.2 | 23.7 | 96.5 |\\n| \u2022 SAR-episodic | 73.6 | 77.6 | 75.0 | 69.4 | 78.3 | 67.1 | 72.6 | 83.6 | 75.4 | 52.0 | 45.1 | 56.3 |\\n| \u2022 SAR-online | 46.1 | 47.5 | 44.5 | 43.5 | 43.8 | 38.1 | 39.9 | 54.2 | 54.2 | 28.1 | 22.9 | 37.8 |\\n\\nTable 9: Results of TTA performance on ViTBase (LN). We report the error in (%) on ImageNet-C severity level 5 under uniformly distributed test streams. Optimal results in episodic & online are highlighted by bold and blue respectively.\"}"}
{"id": "zhao23d", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\n(a) Stochastic restoring: 1 steps\\n(b) Fisher regularizer: 1 steps\\n(c) Stochastic restoring: 2 steps\\n(d) Fisher regularizer: 2 steps\\n(e) Stochastic restoring: 3 steps\\n(f) Fisher regularizer: 3 steps\\n(g) Stochastic restoring: 4 steps\\n(h) Fisher regularizer: 4 steps\\n\\nFigure 9: The standard deviation of stochastic restoring and Fisher regularizer when considering multiple adaptation steps. Fisher regularizer (Niu et al., 2022b) aims to constrain important model parameters from drastic changes to alleviate the error accumulated due to batch dependency. Stochastically restoring (Wang et al., 2022) involves a small portion of model parameters to their pre-trained values after adaptation on each test batch to prevent catastrophic forgetting. The hyperparameter tuning for these two techniques is challenging due to the high degree of variability inherent in these methods, which might impede their practical utility, particularly when compounded by the issue of batch dependency.\"}"}
{"id": "zhao23d", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Pitfalls of Test-Time Adaptation\\n\\nFigure 10: Oracle model selection also fails in TENT and NOTE under the online setting. Here we use ResNet-26 as the base model and learning rate is equal to 0.005.\"}"}
