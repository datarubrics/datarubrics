{"id": "70jplnkLMe", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Therapeutic peptides have proven to have great pharmaceutical value and potential in recent decades. However, methods of AI-assisted peptide drug discovery are not fully explored. To fill the gap, we propose a target-aware peptide design method called PPF LOW, based on conditional flow matching on torus manifolds, to model the internal geometries of torsion angles for the peptide structure design. Besides, we establish a protein-peptide binding dataset named PPBench2024 to fill the void of massive data for the task of structure-based peptide drug design and to allow the training of deep learning methods. Extensive experiments show that PPF LOW reaches state-of-the-art performance in tasks of peptide drug generation and optimization in comparison with baseline models, and can be generalized to other tasks including docking and side-chain packing.\\n\\n1. Introduction\\n\\nTherapeutic peptides are a unique class of pharmaceutical agents composed of a series of well-ordered amino acids. The development of peptide drug design and discovery is accelerated by fast advances in structural biology, recombinant biologics, and synthetic and analytic technologies (Wang et al., 2022). Peptide drugs play an important role in pharmacology because they usually bind to cell surface receptors and trigger intracellular effects with high affinity and specificity, showing less immunogenicity and taking lower production costs (Muttenthaler et al., 2021). Therefore, they have raised great research interests as new peptide therapeutics are continuously developed with more than 150 peptides in clinical tests and another 400\u2013600 peptides undergoing preclinical studies.\\n\\nDeep learning has revolutionized fields like drug discovery and protein design (Makhatadze, 2021; Watson et al., 2023; Dauparas et al., 2022), which proves to be effective tools to assist the development of small and large molecule drugs. Peptide drugs occupy a unique chemical and pharmacological space between small and large molecules, but the AI-assisted peptide drug discovery methods remain limited compared with those established for small molecules and proteins. Unbound peptide chains are usually at high free energy and entropy values, thus showing unstable conformations, while they trigger pharmacological effects when binding to specific receptors, forming a complex with equilibrium structures that are composed of a pair of receptor and ligand. (Marullo et al., 2013; Seebach et al., 2006). Therefore, we focus on the designation of peptide drugs that can bind to specific receptors (Todaro et al., 2023). Recently, structure-based drug design (SBDD) methods are developed for target-aware small molecule generation (Peng et al., 2022a; Guan et al., 2023; Lin et al., 2022), while these methods cannot be simply transferred to peptide design tasks for the following reasons: (i) Peptide drugs are usually of larger molecular weights (500\u22125000 Da) compared with small molecule drugs (<1000 Da) (Francoeur et al., 2020); (ii) Topologies of atom connectivity in peptides are close to proteins rather than molecules; (iii) Internal geometries in peptides are of different patterns from small molecules. Hence, we identify four challenges for the target-aware peptide drug design.\\n\\nFirst, how to extract sufficient information from the conditional receptor contexts. This is indispensable for the model to generalize to new receptors. Second, how to generate valid peptides that satisfy physicochemical rules. If the generated peptides are not chemically valid, other drug properties are unnecessary for further consideration. Third, searching for natural peptides and replacing them with animal homologs, such as the discovery of insulin, GLP-1 and somatostatin, were the important strategies used for peptide drug discovery (Kelly et al., 2022). Therefore, instead of de novo design, the model should apply to another scenario: optimizing natural peptides to drugs with higher binding affinities. Besides, there are few available benchmark datasets large enough to support the training of deep learning models for structure-based peptide drug design tasks, and it is urged to collect and organize a high-quality dataset that satisfies the demand for massive data.\"}"}
{"id": "70jplnkLMe", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Low: Target-aware Peptide Design with Torsional Flow Matching\\n\\n(a) Histogram of peptide lengths\\n\\n(b) Empirical and estimated bond length distribution.\\n\\n(c) Empirical and estimated bond angle distribution.\\n\\n(d) Empirical and estimated torsion angle distribution.\\n\\nFigure 6: Distributions of flexible and inflexible geometries obtained by peptides in PPF datasets.\"}"}
{"id": "70jplnkLMe", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"hidden dimension as 64, and the MLP for single amino acid as 2 layers with hidden dimension as 128. Following, 6 layers of transformer are stacked behind, and the final layer is the LOCS which has been discussed before.\"}"}
{"id": "70jplnkLMe", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LOW: Target-aware Peptide Design with Torsional Flow Matching\\n\\nFrancoeur, P. G., Masuda, T., Sunseri, J., Jia, A., Iovanisci, R. B., Snyder, I., and Koes, D. R. 3d convolutional neural networks and a crossdocked dataset for structure-based drug design. *Journal of chemical information and modeling*, 2020.\\n\\nGuan, J., Qian, W. W., Peng, X., Su, Y., Peng, J., and Ma, J. 3d equivariant diffusion for target-aware molecule generation and affinity prediction. In *The Eleventh International Conference on Learning Representations*, 2023. URL https://openreview.net/forum?id=kJqXEPXMsE0.\\n\\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models, 2020.\\n\\nHoogeboom, E., Nielsen, D., Jaini, P., Forr\u00b4e, P., and Welling, M. Argmax flows and multinomial diffusion: Learning categorical distributions, 2021.\\n\\nHuang, Y., Li, S., Su, J., Wu, L., Zhang, O., Lin, H., Qi, J., Liu, Z., Gao, Z., Liu, Y., Zheng, J., and Li, S. Z. Protein 3d graph structure learning for robust structure-based protein property prediction. *ArXiv*, abs/2310.11466, 2023. URL https://api.semanticscholar.org/CorpusID:264288981.\\n\\nIngraham, J., Baranov, M., Costello, Z., Frappier, V., Ismail, A., Tie, S., Wang, W., Xue, V., Obermeyer, F., Beam, A., and Grigoryan, G. Illuminating protein space with a programmable generative model. *Nature*, 623:1070 \u2013 1078, 2022.\\n\\nJantzen, R. T. Geodesics on the torus and other surfaces of revolution clarified using undergraduate physics tricks with bonus: Nonrelativistic and relativistic kepler problems, 2012.\\n\\nJing, B., Corso, G., Chang, J., Barzilay, R., and Jaakkola, T. Torsional diffusion for molecular conformer generation, 2023.\\n\\nJoosten, R. P., Long, F., Murshudov, G. N., and Perrakis, A. The pdb redo server for macromolecular structure model optimization. *IUCrJ*, 1:213 \u2013 220, 2014.\\n\\nJumper, J. M., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., Z\u00b4\u0131dek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A., Cowie, A., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., Back, T., Petersen, S., Reiman, D. A., Clancy, E., Zielinski, M., Steinegger, M., Pacholska, M., Berghammer, T., Bodenstein, S., Silver, D., Vinyals, O., Senior, A. W., Kavukcuoglu, K., Kohli, P., and Hassabis, D. Highly accurate protein structure prediction with alphafold. *Nature*, 596:583 \u2013 589, 2021.\\n\\nKelly, M., Lewis, J., Rao, H., Carter, J., Portillo, I. B., and Beuttler, R. Effects of glp-1 receptor agonists on cardiovascular outcomes in patients with type 2 diabetes and chronic kidney disease: A systematic review and meta-analysis. *Pharmacotherapy*, 42:921 \u2013 928, 2022.\\n\\nKofinas, M., Nagaraja, N. S., and Gavves, E. Roto-translated local coordinate frames for interacting dynamical systems, 2022.\\n\\nK\u00a8ohler, J., Klein, L., and Noe, F. Equivariant flows: Extract likelihood generative learning for symmetric densities. In III, H. D. and Singh, A. (eds.), *Proceedings of the 37th International Conference on Machine Learning*, volume 119 of *Proceedings of Machine Learning Research*, pp. 5361\u20135370. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/kohler20a.html.\\n\\nLeach, A., Schmon, S. M., Degiacomi, M. T., and Willcocks, C. G. Denoising diffusion probabilistic models on SO(3) for rotational alignment. In *ICLR 2022 Workshop on Geometrical and Topological Representation Learning*, 2022. URL https://openreview.net/forum?id=BY88eBbkpe5.\\n\\nLeman, J. K., Weitzner, B. D., Lewis, S. M., Consortium, R., and Bonneau, R. Macromolecular modeling and design in rosetta: New methods and frameworks. 2019. URL https://api.semanticscholar.org/CorpusID:241255608.\\n\\nLin, H., Huang, Y., Liu, M., Li, X. C., Ji, S., and Li, S. Z. Diffbp: Generative diffusion of 3d molecules for target protein binding. *ArXiv*, abs/2211.11214, 2022.\\n\\nLin, H., Huang, Y., Zhang, O., Wu, L., Li, S., Chen, Z., and Li, S. Z. Functional-group-based diffusion for pocket-specific molecule generation and elaboration, 2023.\\n\\nLin, Y. and AlQuraishi, M. Generating novel, designable, and diverse protein structures by equivariantly diffusing oriented residue clouds, 2023.\\n\\nLipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. *ArXiv*, abs/2210.02747, 2022.\\n\\nLiu, M., Luo, Y., Wang, L., Xie, Y., Yuan, H., Gui, S., Xu, Z., Yu, H., Zhang, J., Liu, Y., Yan, K., Oztekin, B., Liu, H., Zhang, X., Fu, C., and Ji, S. Dig: A turnkey library for diving into graph deep learning research. *ArXiv*, abs/2103.12608, 2021. URL https://api.semanticscholar.org/CorpusID:232320529.\\n\\nLiu, M., Luo, Y., Uchino, K., Maruhashi, K., and Ji, S. Generating 3d molecules for target protein binding. In *International Conference on Machine Learning*, 2022.\"}"}
{"id": "70jplnkLMe", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LOW: Target-aware Peptide Design with Torsional Flow Matching\\n\\nLuo, S., Guan, J., Ma, J., and Peng, J. A 3D generative model for structure-based drug design. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.\\n\\nLuo, S., Su, Y., Peng, X., Wang, S., Peng, J., and Ma, J. Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=jSorGn2Tjg.\\n\\nLuo, S., Su, Y., Wu, Z., Su, C., Peng, J., and Ma, J. Rotamer density estimator is an unsupervised learner of the effect of mutations on protein-protein interaction. bioRxiv, 2023.\\n\\nMakhatadze, G. I. Faculty opinions recommendation of accurate prediction of protein structures and interactions using a three-track neural network. Faculty Opinions \u2013 Post-Publication Peer Review of the Biomedical Literature, 2021.\\n\\nMartins, P. M., Mariano, D. C. B., Carvalho, F. C., Bastos, L. L., Moraes, L., Paix\u00e3o, V. M., and de Melo Minardi, R. C. Propedia v2.3: A novel representation approach for the peptide-protein interaction database using graph-based structural signatures. Frontiers in Bioinformatics, 3, 2023. URL https://api.semanticscholar.org/CorpusID:256978014.\\n\\nMarullo, R., Kastantin, M., Drews, L. B., and Tirrell, M. Peptide contour length determines equilibrium secondary structure in protein-analogous micelles. Biopolymers: Original Research on Biomolecules, 99(9):573\u2013581, 2013.\\n\\nMasuda, T., Ragoza, M., and Koes, D. R. Generating 3d molecular structures conditional on a receptor binding site with deep generative models. arXiv preprint arXiv:2010.14442, 2020.\\n\\nMuttenthaler, M., King, G. F., Adams, D. J., and Alewood, P. F. Trends in peptide drug discovery. Nature Reviews Drug Discovery, 20:309 \u2013 325, 2021.\\n\\nPadmanabhan, S. Handbook of pharmacogenomics and stratified medicine. 2014.\\n\\nParsons, J., Holmes, J., Rojas, J., Tsai, J., and Strauss, C. Practical conversion from torsion space to cartesian space for in silico protein synthesis. Journal of computational chemistry, 26:1063\u20138, 07 2005. doi: 10.1002/jcc.20237.\\n\\nPeng, X., Luo, S., Guan, J., Xie, Q., Peng, J., and Ma, J. Pocket2mol: Efficient molecular sampling based on 3d protein pockets. In International Conference on Machine Learning, 2022a.\\n\\nPeng, X., Luo, S., Guan, J., Xie, Q., Peng, J., and Ma, J. Pocket2mol: Efficient molecular sampling based on 3d protein pockets. In International Conference on Machine Learning, 2022b.\\n\\nRezende, D. J., Papamakarios, G., Racani\u00e8re, S., Albergo, M. S., Kanwar, G., Shanahan, P. E., and Cranmer, K. Normalizing flows on tori and spheres, 2020.\\n\\nRudolph, M., Wandt, B., and Rosenhahn, B. Same same but different: Semi-supervised defect detection with normalizing flows. 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1906\u20131915, 2020.\\n\\nSatorras, V. G., Hoogeboom, E., Fuchs, F. B., Posner, I., and Welling, M. E(n) equivariant normalizing flows. In Neural Information Processing Systems, 2021.\\n\\nSchneuing, A., Du, Y., Harris, C., Jamasb, A., Igashov, I., Du, W., Blundell, T., Li\u00f2, P., Gomes, C., Welling, M., Bronstein, M., and Correia, B. Structure-based drug design with equivariant diffusion models, 2022.\\n\\nSchymkowitz, J., Ferkinghoff-Borg, J., Stricher, F., Nys, R., and Serrano, L. The foldx web server: an online force field. Nucleic Acids Research, 33:W382 \u2013 W388, 2005.\\n\\nSeebach, D., Hook, D. F., and Gl\u00e4ttli, A. Helices and other secondary structures of \u03b2- and \u03b3-peptides. Peptide Science: Original Research on Biomolecules, 84(1):23\u201337, 2006.\\n\\nSteinegger, M. and S\u00f6ding, J. Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature biotechnology, 35(11):1026\u20131028, 2017.\\n\\nTodaro, B., Ottalagana, E., Luin, S., and Santi, M. Targeting peptides: The new generation of targeted drug delivery systems. Pharmaceutics, 15, 2023.\\n\\nTong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Fatras, K., Wolf, G., and Bengio, Y. Improving and generalizing flow-based generative models with mini-batch optimal transport, 2023.\\n\\nWang, L., Wang, N., Zhang, W., Cheng, X., Yan, Z., Shao, G., Wang, X., Wang, R., and Fu, C. Therapeutic peptides: current applications and future directions. Signal Transduction and Targeted Therapy, 7, 2022.\\n\\nWatson, J. L., Juergens, D., Bennett, N. R., Trippe, B. L., Yim, J., Eisenach, H. E., Ahern, W., Borst, A. J., Ragotte, R. J., Milles, L. F., Wicky, B. I. M., Hanikel, N., Pellock, S. J., Courbet, A., Sheffler, W., Wang, J., Venkatesh, P., Sappington, I., Torres, S. V., Lauko, A., Bortoli, V. D., 11\"}"}
{"id": "70jplnkLMe", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LOW: Target-aware Peptide Design with Torsional Flow Matching\\n\\nMathieu, E., Ovchinnikov, S., Barzilay, R., Jaakkola, T., DiMaio, F., Baek, M., and Baker, D. De novo design of protein structure and function with rfdiffusion. *Nature*, 620:1089 \u2013 1100, 2023.\\n\\nWen, Z., He, J., Tao, H., and Huang, S. Pepbdb: a comprehensive structural database of biological peptide-protein interactions. *Bioinformatics*, 35 1:175\u2013177, 2018. URL https://api.semanticscholar.org/CorpusID:51600791.\\n\\nWeng, G., Gao, J., Wang, Z., Wang, E., Hu, X., Yao, X., Cao, D., and Hou, T. Comprehensive evaluation of fourteen docking programs on protein-peptide complexes and a new strategy for improving the performance of global docking. *Journal of chemical theory and computation*, 2020.\\n\\nWu, F. and Li, S. Z. A hierarchical training paradigm for antibody structure-sequence co-design. *ArXiv*, abs/2311.16126, 2023. URL https://api.semanticscholar.org/CorpusID:265466674.\\n\\nWu, K. E., Yang, K. K., van den Berg, R., Zou, J., Lu, A. X., and Amini, A. P. Protein structure generation via folding diffusion. *ArXiv*, abs/2209.15611, 2022a.\\n\\nWu, L., Lin, H., Gao, Z., Tan, C., and Stan.Z.Li. Self-supervised learning on graphs: Contrastive, generative, or predictive. *IEEE Transactions on Knowledge and Data Engineering*, 35:4216\u20134235, 2021. URL https://api.semanticscholar.org/CorpusID:238215156.\\n\\nWu, L., Huang, Y .-F., Lin, H. X., and Li, S. Z. A survey on protein representation learning: Retrospect and prospect. *ArXiv*, abs/2301.00813, 2022b. URL https://api.semanticscholar.org/CorpusID:255393752.\\n\\nWu, L., Lin, H., Huang, Y ., and Li, S. Z. Knowledge distillation improves graph structure augmentation for graph neural networks. In *Neural Information Processing Systems*, 2022c. URL https://api.semanticscholar.org/CorpusID:258509704.\\n\\nWu, L., Tian, Y ., Huang, Y ., Li, S., Lin, H., Chawla, N., and Li, S. Z. Mape-ppi: Towards effective and efficient protein-protein interaction prediction via microenvironment-aware protein embedding. *ArXiv*, abs/2402.14391, 2024. URL https://api.semanticscholar.org/CorpusID:267782631.\\n\\nYan, Y ., Tao, H., He, J., and Huang, S. The hdock server for integrated protein\u2013protein docking. *Nature Protocols*, 15:1829 \u2013 1852, 2020. URL https://api.semanticscholar.org/CorpusID:215411844.\\n\\nYim, J., Campbell, A., Foong, A. Y . K., Gastegger, M., Jim\u00b4enez-Luna, J., Lewis, S., Satorras, V . G., Veeling, B. S., Barzilay, R., Jaakkola, T., and No\u00b4e, F. Fast protein backbone generation with se(3) flow matching, 2023a.\\n\\nYim, J., Trippe, B. L., Bortoli, V . D., Mathieu, E., Doucet, A., Barzilay, R., and Jaakkola, T. Se(3) diffusion model with application to protein backbone generation, 2023b.\\n\\nZardecki, C., Dutta, S., Goodsell, D. S., V oigt, M., and Bur-ley, S. K. Rcsb protein data bank: A resource for chemical, biochemical, and structural explorations of large and small biomolecules. *Journal of Chemical Education*, 93:569\u2013575, 2016.\\n\\nZhang, Y . and Sanner, M. F. Autodock crankpep: combining folding and docking to predict protein-peptide complexes. *Bioinformatics*, 2019.\\n\\nZhang, Y ., Zhang, Z., Zhong, B., Misra, S., and Tang, J. Diffpack: A torsional diffusion model for autoregressive protein side-chain packing, 2023a.\\n\\nZhang, Z., Zheng, S., Min, Y ., and Liu, Q. Molecule generation for target protein binding with structural motifs. In *International Conference on Learning Representations*, 2023b. URL https://openreview.net/forum?id=Rq13idF0F73.\"}"}
{"id": "70jplnkLMe", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Method\\n\\nA.1. Proof of Proposition 3.1\\n\\nBy Equation 3, we can obtain\\n\\\\[ \\\\epsilon = \\\\tau - \\\\mu \\\\sigma. \\\\]\\nBesides, from the definition of \\\\( u_t(\\\\tau|\\\\tau_1) = \\\\dot{\\\\sigma}_t \\\\epsilon + \\\\dot{\\\\mu}_t \\\\]\\n\\\\[ = \\\\dot{\\\\sigma}_t (\\\\tau - \\\\mu_\\\\tau \\\\sigma) + \\\\dot{\\\\mu}_t. \\\\]\\n\\\\[ (15) \\\\]\\n\\nA.2. Proof of Proposition 3.2\\n\\nFirstly, we claim the disintegration of measures of \\\\( p_t(\\\\tau) \\\\), as \\\\( p_t(\\\\tau) = \\\\mathcal{Q}_i p(\\\\tau_i(\\\\tau)). \\\\) For \\\\( p_0(\\\\tau) \\\\), the disintegration satisfies. For \\\\( p_1(\\\\tau) \\\\), our parametrization assumes the torsion angles are orthogonal and the distribution is independent, so it also satisfies disintegration. In this way, it is easy to obtain an intermediate probability \\\\( p_t \\\\) satisfies disintegration, and similar for the conditional probability \\\\( p_t(\\\\tau|\\\\tau_1) \\\\).\\n\\nThen we can factorizes the metric on \\\\( \\\\mathbb{T}_N \\\\) into \\\\( S \\\\times \\\\cdots \\\\times S \\\\), and \\\\( p(\\\\tau) \\\\in \\\\mathcal{P}(S) \\\\).\\n\\nLet \\\\( u_t = \\\\mathbb{E}_{\\\\tau_0 \\\\sim p_0, \\\\tau_1 \\\\sim p_1} h_{p_t(\\\\tau|\\\\tau_0, \\\\tau_1)} p_t(\\\\tau) \\\\)\\ni, and\\n\\\\[ \\\\nabla \\\\theta (\\\\mathbb{E}_{\\\\tau_0 \\\\sim p_0, \\\\tau_1 \\\\sim p_1, \\\\tau \\\\sim p_t(\\\\tau|\\\\tau_0, \\\\tau_1)} [\\\\| v_t(\\\\tau) - u_t(\\\\tau|\\\\tau_0, \\\\tau_1) \\\\|^2] - \\\\mathbb{E}_{\\\\tau \\\\sim p_t} [\\\\| v_t(\\\\tau) - u_t(\\\\tau) \\\\|^2]) = -2 \\\\nabla \\\\theta (\\\\mathbb{E}_{\\\\tau_0 \\\\sim p_0, \\\\tau_1 \\\\sim p_1, \\\\tau \\\\sim p_t(\\\\tau|\\\\tau_0, \\\\tau_1)} \\\\langle v_t(\\\\tau), u_t(\\\\tau|\\\\tau_0, \\\\tau_1) \\\\rangle - \\\\mathbb{E}_{\\\\tau \\\\sim p_t} \\\\langle v_t(\\\\tau), u_t(\\\\tau) \\\\rangle) \\\\]\\n\\\\[ (16) \\\\]\\n\\nThen,\\n\\\\[ \\\\mathbb{E}_{\\\\tau \\\\sim p_t} \\\\langle v_t(\\\\tau), u_t(\\\\tau) \\\\rangle = \\\\mathbb{E} \\\\langle v_t(\\\\tau), u_t(\\\\tau) \\\\rangle_{p_t(\\\\tau|\\\\tau_0, \\\\tau_1)} \\\\mathbb{E} p_t(\\\\tau|\\\\tau_0, \\\\tau_1) \\\\mathbb{E} p_0(\\\\tau_0) p_1(\\\\tau_1) d\\\\tau_0 d\\\\tau_1 = \\\\mathbb{E} \\\\langle v_t(\\\\tau), u_t(\\\\tau) \\\\rangle_{p(\\\\tau|\\\\tau_0, \\\\tau_1)} \\\\mathbb{E} p_0(\\\\tau_0) p_1(\\\\tau_1) d\\\\tau d\\\\tau_1 = \\\\mathbb{E} \\\\langle v_t(\\\\tau), u_t(\\\\tau) \\\\rangle_{p(\\\\tau|\\\\tau_0, \\\\tau_1)} \\\\mathbb{E} p(\\\\tau) d\\\\tau. \\\\]\\n\\\\[ (17) \\\\]\\n\\nwhere in the last equality we change the order of integration. It proves Equation 16 equals 0.\\n\\nA.3. Proof of Proposition 3.3\\n\\nThe proof is inspired by (K\u00f6hler et al., 2020; Lin et al., 2023), as follows:\\n\\nLemma A1.\\nLet \\\\( T_g(\\\\cdot) \\\\) be the operation in \\\\( \\\\text{SE}(3) \\\\), if the following update function in the ODE sampler (Sec. 3.6) for the atom level's positions are defined as\\n\\\\[ v_t(\\\\{X^*(i)\\\\}|C^*t) = (\\\\{X^*(i)t + \\\\Delta t\\\\} - \\\\{X^*(i)t\\\\})/\\\\Delta t, \\\\]\\n\\\\[ (18) \\\\]\\nin which \\\\( \\\\{X^*(i)t\\\\} = \\\\text{Oterf} (\\\\tau_t) + x(C). \\\\) The invariance and equivariance of the following functions in the updating process are listed as\\n\\\\[ v(\\\\{X^*(i)\\\\}|T_g(C^*)) = T_g v(\\\\{X^*(i)\\\\}|C^*); \\\\]\\n\\\\[ v(\\\\{s(i)\\\\}|T_g(C^*)) = v(\\\\{s(i)\\\\}|C^*), \\\\]\\n\\\\[ (19) \\\\]\"}"}
{"id": "70jplnkLMe", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LOW: Target-aware Peptide Design with Torsional Flow Matching (RDE) (Luo et al., 2023) as the probabilistic model to model the side-chain rotamers \\\\( \\\\{\\\\chi^i_{N_{pp}}\\\\} \\\\). It uses a Conditional Flow on \\\\( T_{N_{rt}} \\\\) based on the rational quadratic spline flow (Durkan et al., 2019; Rezende et al., 2020), where \\\\( N_{rt} \\\\) is the total rotamer number. In practice, we use the pre-trained version of RDE capable of perceiving the side-chain conformations by training the model on large datasets of PDB-REDO (Joosten et al., 2014). Further, we conduct fine-tuning on our protein-peptide complex datasets. The effectiveness of the side-chain packing model named RDE-PP, transferred from protein-protein complexes to the protein-peptide ones, is empirically shown in Sec. 5.6.\\n\\nThe overall workflow of PPFLOW including the backbone generation and side-chain packing is shown in Figure. 2.\\n\\n4. Related Work\\n\\nStructure-based drug design. Success in 3D molecule generation and increasing available structural data raises scientific interest in structure-based drug design (SBDD). Grid-based methods regard the Euclidean space as discrete and predict the molecules' structures on the grids (Luo et al., 2021; Masuda et al., 2020). With great advances in Graph Neural Networks (Wu et al., 2021; Liu et al., 2021; Wu et al., 2022c), Equivariant neural networks have advanced the structure predictions and helped to improve the tasks greatly (Peng et al., 2022b; Liu et al., 2022). Then, diffusion methods (Guan et al., 2023; Schneuing et al., 2022; Lin et al., 2022) attempt to generate the ordered atoms' positions and types at full atom levels. Further, a series of works of fragment-based drug design are proposed, by mimicking classical drug design procedures to generate both atoms and functional groups that form molecule drugs (Zhang et al., 2023b; Lin et al., 2023).\\n\\nProtein generation. Machine learning methods for protein modeling have achieved great success in recent years (Huang et al., 2023; Wu et al., 2024; 2022b; Wu & Li, 2023), and peptides can be regarded as fragments making up a protein. Techniques on protein backbone generation assist realistic peptide structure design. For example, FoldingDiff (Wu et al., 2022a) is also based on redundant angle generation, which employs diffusion models on torus to achieve backbone design. RFDiffusion and Chroma (Watson et al., 2023; Ingraham et al., 2022) use different diffusion schemes, and achieve state-of-the-art generation performance on protein generations. Recently, protein backbone generation methods employ flow matching techniques, to explore the applicability and effectiveness (Yim et al., 2023a; Bose et al., 2023). For protein side chains, methods usually focus on protein-protein complexes, such as RED-PPI (Luo et al., 2023) and DiffPack (Zhang et al., 2023a). Our side-chain packing methods follow RDE-PPI, and achieve a good generalization performance on peptides.\\n\\n5. Experiment\\n\\n5.1. Dataset\\n\\nTraining set. To satisfy the need for massive data to train deep learning models, we construct PPBench2024, through a series of systematic steps: First, we source complexes from the RCSB database (Zardecki et al., 2016), specifically selecting those containing more than two chains and excluding any with nucleic acid structures, and defining interactions between a pair as a minimum intermolecular distance of 5.0 \u00c5 or less. Subsequently, only those complexes featuring peptide chains that do not exceed 30 amino acids in length are included, to better mimic existing peptide drug sizes. Then, the water molecules and heteroatoms are eliminated. Finally, we select only peptide molecules composed entirely of amino acids, remove the modified peptides with functional groups other than amino acids, and filter peptides with broken bonds according to the ideal bond length, i.e. the bond is unbroken if the observed bond length is between the ideal length plus or minus 0.5 \u00c5. The number of final screened complex instances of peptide-protein pairs is 9070. Further, we screen the existing datasets of PropediaV2.3 (Martins et al., 2023) and PepBDB (Wen et al., 2018) with the same criterion, leading to additional 6523 instances to expand it. Appendix. B.1 gives details. We split the PPBench2024 into training and validation sets according to the clustering of the proteins that are closest to the peptide ligand via MMS_EQS2 (Steinegger & S\u00f6ding, 2017) with the ratio of 9 : 1.\\n\\nTest set. To evaluate the model performance, we use an existing benchmark dataset called PPDBench (Agrawal et al., 2019) consisting of 133 pairs with peptides' lengths ranging from 9 to 15 as the test set. We eliminate all complexes from PPBench2024 whose PDB-ID are the same as those in PPDBench to avoid potential data leakage.\\n\\n5.2. Baseline Models\\n\\nOur method has been opened to the public in https://github.com/Edapinenut/ppflow. For comparison, we extend two models for the following tasks, including (i) DDIFF_PP, as a variant of DDIFF_AB, which is a diffusion model for generating CDRs in antibodies targeted at antigens. DDIFF_PP parametrizes the protein backbones in the same way as ALPHAFOLD2, in which the positions of atoms in a residue are determined by \\\\( \\\\alpha \\\\)'s translation vector \\\\( x^i \\\\), and the rotation matrix \\\\( O^i \\\\in SO(3) \\\\) of the frame constructed by positions of \\\\( \\\\{N, \\\\alpha, C\\\\} \\\\). The diffusion and reverse process on translation and orientation variables are modeled by DDPM (Ho et al., 2020) and SO(3)-DPM (Leach et al., 2022). For amino acid types, it uses a multinomial diffusion (Hoogeboom et al., 2021). (ii) DDIFF_BP-PP, as a variant of DDIFF_BP or TARGETDIFF, as\"}"}
{"id": "70jplnkLMe", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Low: Target-aware Peptide Design with Torsional Flow Matching\\n\\nTable 1: Comparison for target-aware peptide generation. Values in bold are the best.\\n\\n| Method       | \u2206G (\u2193) | IMP%-B (\u2191) | IMP%-S (\u2191) | Validity (\u2191) | Novelty (\u2191) | Diversity |\\n|--------------|--------|------------|------------|--------------|-------------|-----------|\\n| PP Flow      | -319.54 | 28.75%     | 7.56%      | 0.94         | 0.79        | 0.28      |\\n| BP-PP Flow   | -221.06 | 19.34%     | 4.09%      | 0.42         | 0.76        | 0.41      |\\n| PPF LOW-BB   | -349.59 | 36.02%     | 10.34%     | 1.00         | 0.84        | 0.76      |\\n| PPF LOW-FA   | -351.27 | 35.43%     | 12.88%     | 1.00         | 0.84        | 0.76      |\\n\\nFigure 3: Metrics for generated peptides of methods in different optimization steps.\\n\\nFigure 4: Peptides designed by different methods and reference.\\n\\n5.3. Peptide Generation\\n\\nMetrics. We choose 6 metrics for evaluating the quality of generated peptides. \u2206G is binding energies calculated by ADCP re-docking (Zhang & Sanner, 2019), since ADCP has shown the best performance in estimating the binding energies and poses of protein-peptide complexes (Weng et al., 2020). \u2206G reflects the potential of a peptide being pharmaceutically active towards the targets, and IMP%-B gives the percentages of the designed peptides with lower (better) \u2206G than the reference peptides. The mean of reference \u2206G is \u2212437.0 kJ/mol. Stability evaluates binding scores of the original pose of the generated peptides, which directly reflects the quality of generated peptides without re-docking. The stability scores are calculated by FOLDX (Schymkowitz et al., 2005) since it performs fast and accurate stability calculation in protein binding tasks compared with other energy-based methods (Luo et al., 2023). We give IMP%-S as the percentages of the designed ones with better stability than reference rather than average because some extremely unstable structures will make the comparison on average values meaningless. Validity is the ratio of the designed peptides that is chemically valid, through the criterion of whether the bonds of the atoms that should be bonded are broken. We follow our filtering process of datasets and define the bond is not broken if its length is within 0.5 \u00c5 above and below the ideal value. Novelty is measured considering two aspects of structure and sequence: (i) the fraction of peptides with TM-score < 0.55 as used in (Lin & AlQuraishi, 2023) and (ii) the sequence overlap (SeqOL) less than 0.5. The given score is the mean ratio of novel peptides v.s. references. Diversity is the product of pairwise (1 \u2212 TM-score) and (1 \u2212 SeqOL) of the generated samples averaged across their target proteins. The higher the score, the more diverse peptides a model generates. Note that all the metrics except Validity are measured with chemically valid peptides.\\n\\nSetup. For each model for comparison, we generate 20 peptides for each conditioned protein because the evaluation process requires excessive time. For example, \u2206G obtained by ADCP requires more than 10 minutes for one pair of protein and peptides on a server with 128 CPU threads. Besides, since the greater the number of atoms, the more likely it is that binding interactions will occur, we eliminate the effects of peptide sizes (lengths) on the binding affinity by setting the generated sequence lengths the same as references.\\n\\nResults. Table 1 gives the comparison of peptide generation task with generated examples shown in Figure 4. For PPF LOW, we test two variants of it. PPF LOW-BB (PPFlow-BackBone) uses the generated backbone structures with ROSETTA side-chain packing (Alford et al., 2017) before re-docking scoring and stability calculation, and side-chain atoms in PPF LOW-FA (PPFlow-FullAtom) is predicted by RDE. Note that the backbone structures before side-chain packing are the same for the two variants, leading to metrics except IMP%-S being of little difference. Besides, for the\"}"}
{"id": "70jplnkLMe", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparison for flexible peptide re-docking.\\n\\n| Methods | 10%  | 30%  | 50%  | 10%  | 30%  | 50%  | 2  | 4  | 8  |\\n|---------|------|------|------|------|------|------|----|----|----|\\n| D\u73f2FF  | 8.01 | 15.26| 28.47| 6.35 | 12.68| 30.25| 9.24| 20.21| 34.45|\\n| PPFLOW  | 12.44| 17.82| 31.24| 2.60 | 3.28 | 4.26 | 6.13| 11.24| 22.47|\\n| HDOCK  | 9.95 | 18.65| 23.07| 7.29 | 10.65| 18.84| 6.40| 10.72| 25.50|\\n| VINA   | 11.38| 19.03| 27.69| 6.27 | 9.56 | 16.59| 5.45| 9.96 | 21.12|\\n\\nTable 3: Comparison for side-chain packing.\\n\\n| Methods | \u03c71 | \u03c72 | \u03c73 | \u03c74 |\\n|---------|----|----|----|----|\\n| MAE     |    |    |    |    |\\n| ROSETTA | 38.58| 39.41| 68.75| 64.54|\\n| RDE-PP (w/o pt) | 44.28| 51.22| 70.21| 71.85|\\n| RDE-PP  | 37.24| 47.67| 66.88| 62.86|\\n| NLL     |    |    |    |    |\\n| RDE-PP (w/o pt) | 0.75| 1.26| 3.37| 4.10|\\n| RDE-PP  | 0.62| 0.78| 3.05| 3.85|\\n\\nIt can be concluded that (i) PPFLOW has the greatest potential to generate peptide drugs with high binding affinity towards the target protein according to \u2206G and IMP%-B metrics. (ii) The complexes of peptides of original poses generated by PPFLOW and the target proteins are the most stable, with the highest IMP%-S metrics. (iii) PPFLOW and D\u73f2FF generate a comparably high ratio of novel peptides, while peptides generated by PPFLOW are more diverse. (iv) The rotamers estimated by RDE-PP are more stable in the binding site than ROSETTA according to IMP%-S metrics of PPFLOW-BB and PPFLOW-FA. (v) With the bond lengths and angles fixed, the structures generated by PPFLOW are chemically valid, while D\u73f2FF shows the lowest validity due to the highest degree of freedom it models, i.e. 4\u00d73Npp, for atom-level backbone structure, which also empirically shows the infeasibility of SBDD methods to generalize to peptide drug design. Therefore, in the following parts, we will not compare D\u73f2FF as baselines.\\n\\n5.4. Peptide Optimization\\n\\nSetup. We apply our model to another common pharmaceutical application: optimization of the existing peptides. To optimize a peptide, we first choose a start time $t_s \\\\in [0, 1]$, and use the probability paths constructed in Sec. 3.2, 3.3 and 3.4, to sample a perturbed peptide $L_t = \\\\{X^*_i t_s, s_i t_s\\\\}_{N_{pp}=1}$. By using the ODE sampling methods in Sec. 3.6, the trained PPFLOW iteratively updates the sequences and structures to recover the peptides as a set of optimized ones. For D\u73f2FF, we use the same setting as (Luo et al., 2022). The 'num_step' is the number of how many optimization steps used in the generative diffusion process, where the start step of noised peptides is $(total_step - time_step)$. The conversion between them is $total_step - num_step = total_step \\\\times start_time$.\\n\\nWe sample 20 new peptides for each protein receptor in PPDBench and evaluate them in the same metrics as discussed in Sec. 5.3. We set total_step = 100 in this part.\\n\\nResults. Figure 3 gives the comparison on peptide optimization results. It can be concluded that (i) The larger number of optimization steps contributes little to the improvement in the binding affinity, but PPFLOW usually generates more peptides with higher binding affinities; (ii) Optimized peptides are usually similar to the original one in small optimization steps since the novelty are smaller, which is desired in many practical applications; (iii) The diversity is more controllable in PPFLOW by changing the number of optimization steps.\\n\\n5.5. Protein-Peptide Docking\\n\\nBesides the two tasks, in this part, we generalize our model to the peptide flexible re-docking task to figure out if the model can learn the binding poses of the ligands. The task can be regarded as establishing a probabilistic model of $p(\\\\{X^*_i\\\\}_{N_{pp}=0}, R \\\\mid \\\\{s_i\\\\}_{N_{pp}=0})$. For D\u73f2FF and PPFLOW, $\\\\{X^*_0\\\\}_{N_{pp}=0}$, as the initialized peptide structures, are obtained by first moving them to the pocket center, and then adding Gaussian noise to each atom. We retrain D\u73f2FF and PPFLOW with the sequences given as additional conditions. For classical docking methods, we choose HDOCK (Yan et al., 2020) which is a re-docking method for protein-protein interactions, and VINA Dock (Eberhardt et al., 2021) proposed for molecule docking. We choose the best 10%/30%/50% poses for comparison on ligand-RMSD (L-RMSD) between C\u03b1 atoms and centroid-RMSD (C-RMSD), and the success rate (Success%) of the docked ligands' RMSD smaller than 2/4/8 \u00c5. It can be concluded in Table 2 that (i) D\u73f2FF shows the best docking performance on L-RMSD and Success% because it directly optimized the RMSD between C\u03b1 atoms, and PPFLOW models the ligands' centroid most accurately. (ii) The deep-learning-based generative models can achieve competitive performance with the classical energy-based ones, while the latter requires enormous rounds of iteration, costing dozens of times longer for computation.\\n\\n5.6. Side-Chain Packing\\n\\nTo figure out the effectiveness of our fine-tuned model of RDE-PP, we compare it with two baseline methods Rosetta(fixbb) (Leman et al., 2019) and the unpretrained version RDE-PP (w/o pt). For each reference peptide, we initialize the rotamers randomly with uniform angle distribution, and sample 20 conformations on side chains. We evaluate the mean absolute error (MAE) of the predicted sidechain torsional angles as $MAE = |\\\\hat{\\\\chi} - \\\\chi|$ where $\\\\hat{\\\\chi}$ is the estimated ones and $\\\\chi$ is the ground truth, and negative likelihood (NLL). In Table 3, the results demonstrate that the RDE-PP with pretraining outperforms the baselines on three of the four torsional angles in terms of the evaluated metrics.\"}"}
{"id": "70jplnkLMe", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. Conclusion\\n\\nIn this paper, we focus on the target-specific peptide design tasks. To fulfill it, we first establish a dataset called PPBench2024, and then propose a flow-matching generative model on torus manifolds called PPF LOW, which attempts to learn the distribution of torsion angles of peptide backbones directly. Experiments are conducted to evaluate its performance on several tasks, indicating its superior performance on target-aware peptide design and optimization. However, since the classical docking methods for binding pose calculation are extremely slow, we extend our model to docking tasks. Still, it is not as competitive as the baseline models, which will be our future focus. Besides, the performance gaps between the extended side-chain packing model and the classical ones are still small, urging us to develop a side-chain packing model with high prediction accuracy.\\n\\nAcknowledgements\\n\\nThis work was supported by the Science & Technology Innovation 2030 Major Program Project No. 2021ZD0150100, National Natural Science Foundation of China Project No. U21A20427, Project No. WU2022A009 from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University, and Project No. WU2023C019 from the Westlake University Industries of the Future Research. Finally, we thank the Westlake University HPC Center for providing computational resources. Besides, we thank the help of Dr. Tailin Wu, and his great efforts in his deep insights into cutting-edge issues, the guidance provided in the rebuttal, and the funding that supplied us with the necessary equipment for our research.\\n\\nImpact Statement\\n\\nWith the long-term continuation of COVID-19, more and more AI scientists are beginning to turn their research interests to drug design. The focus of this paper is on one of them \u2013 peptide drugs. There is very limited work aimed at using AI algorithms to design and optimize peptide drugs. However, peptide drugs are now rising as effective therapeutics, with hundreds of them proving to be successful, such as semaglutide for obesity and diabetes. Here, this work presents a considerable contribution to establishing a large dataset and deep-learning model for protein-specific peptide design, in which the effectiveness is validated empirically. As far as we know, it is the first AI-assisted peptide drug design solution, so we here emphasize the social significance of the work and hope to get the attention of the reviewers and the review committee.\\n\\nReferences\\n\\nAgrawal, P., Singh, H., Srivastava, H. K., Singh, S., Kishore, G., and Raghava, G. P. Benchmarking of different molecular docking methods for protein-peptide docking. BMC Bioinformatics, 19, 2019.\\n\\nAlford, R. F., Leaver-Fay, A., Jeliazkov, J. R., O'Meara, M. J., DiMaio, F., Park, H., Shapovalov, M. V., Renfrew, P. D., Mulligan, V. K., Kappel, K., Labonte, J. W., Pacella, M. S., Bonneau, R., Bradley, P., Dunbrack, R. L., Das, R., Baker, D., Kuhlman, B., Kortemme, T., and Gray, J. J. The rosetta all-atom energy function for macromolecular modeling and design. bioRxiv, 2017.\\n\\nAustin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den Berg, R. Structured denoising diffusion models in discrete state-spaces. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021.\\n\\nBose, A. J., Akhound-Sadegh, T., Fatras, K., Huguet, G., Rector-Brooks, J., Liu, C.-H., Nica, A. C., Korablyov, M., Bronstein, M., and Tong, A. Se(3)-stochastic flow matching for protein backbone generation. ArXiv, abs/2310.02391, 2023.\\n\\nChen, R. T. Q. and Lipman, Y. Riemannian flow matching on general geometries. ArXiv, abs/2302.03660, 2023.\\n\\nCorso, G., Stark, H., Jing, B., Barzilay, R., and Jaakkola, T. S. Diffdock: Diffusion steps, twists, and turns for molecular docking. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=kKF8_K-mBbS.\\n\\nCreasey, P. E. and Lang, A. Fast generation of isotropic gaussian random fields on the sphere. Monte Carlo Methods and Applications, 24:1 \u2013 11, 2017.\\n\\nDauparas, J., Anishchenko, I. V., Bennett, N. R., Bai, H., Ragotte, R. J., Milles, L. F., Wicky, B. I. M., Courbet, A., de Haas, R. J., Bethel, N. P., Leung, P. J. Y., Huddy, T. F., Pellock, S. J., Tischer, D. K., Chan, F., Koepnick, B., Nguyen, H. A., Kang, A., Sankaran, B., Bera, A. K., King, N. P., and Baker, D. Robust deep learning based protein sequence design using proteinmpnn. Science (New York, N.Y.), 378:49 \u2013 56, 2022.\\n\\nDurkan, C., Bekasov, A., Murray, I., and Papamakarios, G. Neural spline flows, 2019.\\n\\nEberhardt, J., Santos-Martins, D., Tillack, A. F., and Forli, S. Autodock vina 1.2.0: New docking methods, expanded force field, and python bindings. Journal of chemical information and modeling, 2021. URL https://api.semanticscholar.org/CorpusID:236092162.\"}"}
{"id": "70jplnkLMe", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where \\\\( K \\\\) and it is easy to obtained that the transition kernel \\\\( p \\\\)\\n\\nThen the distribution which the final structures and sequences are sampled from as \\\\( p \\\\) kernels of \\\\( p \\\\) is equivalent to \\\\( p \\\\).\\n\\nThen we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the update process is equivariant, as\\n\\nProof:\\n\\nIn the following, we write \\\\( p \\\\) obtain that the"}
{"id": "70jplnkLMe", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma A.2. If $p(O(C)_{t+\\\\Delta t}|C_{t+\\\\Delta t})$, $p(x(C)_{t+\\\\Delta t}|C_{t+\\\\Delta t})$, $p(\\\\tau_{t+\\\\Delta t}|C_{t+\\\\Delta t})$ and $p(s_{t+\\\\Delta t}|C_{t+\\\\Delta t})$ are $SE(3)$-invariant, and $p(O(C)_{t}|C_{t})$ is $SO(3)$-equivariant and $T(3)$-invariant, then $p(T_g(L^1|T_g(R))) = p(O(C)_1 \\\\text{nerf}(\\\\tau_1) + x(C)_1, s_1)|T_g(R))$.\\n\\nProof: Here for $T_g$, we can decompose it as $T_g = T_r \\\\circ T_t$, meaning the rotation and translation operations as $SE(3) \\\\sim SO(3) + T(3)$. By this mean, $p(T_g(O(C)_1 \\\\text{nerf}(\\\\tau_1) + x(C)_1))|T_g(R))$.\\n\\nBecause $\\\\text{nerf}(\\\\tau_1)$ is always reconstructed with unit rotation $\\\\text{diag}(1, 1, 1)$ and zero-mass-centered, therefore, $p(T_g(O(C)_1 \\\\text{nerf}(\\\\tau_1)))|T_g(R))$.\\n\\nFrom both perspectives, the roto-translational equivariance can be proved. We demonstrate the first proof path, to shown that the conditions of Lemma A.2 holds.\"}"}
{"id": "70jplnkLMe", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For $\\\\tau$:\\n\\n$$\\\\mathcal{P}(T_g(\\\\tau t + \\\\Delta t)|T_g(C^* t)) = \\\\mathcal{P}(T_g(\\\\tau t)|T_g(C^* t)) \\\\Delta t$$\\n\\n$$Z \\\\mathcal{P}(T_g(\\\\tau t + \\\\Delta t)|T_g(C^* t)) \\\\mathcal{P}(T_g(\\\\tau t)|T_g(C^* t)) dT_g(\\\\tau t)$$\\n\\n$$Z \\\\mathcal{P}(v_t(\\\\tau t)|C^* t) \\\\Delta t dT_g(\\\\tau t)$$\\n\\n$$Z \\\\mathcal{P}(\\\\tau t + \\\\Delta t - \\\\tau t|C^* t) \\\\mathcal{P}(\\\\tau t|C^* t) d\\\\tau t$$\\n\\n$$\\\\mathcal{P}(\\\\tau t + \\\\Delta t|C^* t)$$\\n\\n(50)\\n\\nBeside, for the other three variables, the equations can be deduced similarly.\\n\\nA.4. LOSC Updating\\n\\nBecause the outputs should include the gradient vectors of rotation angle, global translational vector, rotation matrix, and type probability, we employ the LOSC to output the vector fields by\\n\\n$$v_t(c(i)|C^*) = MLP_s(h_i);$$\\n\\n$$v_t(\\\\{\\\\phi(i), \\\\psi(i), \\\\omega(i)\\\\}|C^*) = MLP_\\\\tau(h_i);$$\\n\\n$$v_t(x(C)|C^*) = MLP_x(h_G^O);$$\\n\\n$$v_t(O(C)|C^*) = tran_O^t(MLP_O(h_G^O)),$$\\n\\n(51)\\n\\nin which $h_G = P_i h_i$ is the global representation obtained by summation, $MLP_s: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}^{20}$, $MLP_\\\\tau: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{T}$, $MLP_x: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}^3$ and $MLP_O: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}^3$. $MLP_O$ predicts a vector in Lie group so (3), and translate it to $O_t$'s tangent space by $tran_O^t(\\\\cdot)$. The output vector fields satisfy the equivariance and invariance conditions in Proposition. 3.3.\\n\\nB. Dataset Statistics\\n\\nB.1. Data preprocess\\n\\nThe construction of raw PPBench2024 is given in the Sec. 5.1. For the additional complexes from PropediaV2.3 and PepBDB, we give the detailed screening process in the Figure. 5. Because in PropediaV2.3, one peptide can be paired with several chains in a protein complex, another re-matching step should be conducted first. Then, we give an empirical distribution on the peptide lengths of PPBench2024, in Figure. 6(a).\\n\\nB.2. Analysis on Geometry\\n\\nAs the PPF LOW models the internal redundant geometry of the peptides, here we give a statistical illustration to show the flexible geometries that need to be generated. NeRF can use the following geometries in Figure. 6(b), 6(c) and 6(d) to reconstruct the full backbone structures. For these internal geometries, it can be concluded that the torsion angles of 'N-C$_{\\\\alpha}$-C-N' and 'C-N-C$_{\\\\alpha}$-C' are the two most flexible geometries. Besides, 'C$_{\\\\alpha}$-C-N-C$_{\\\\alpha}' is theoretically inflexible since the constraints on peptide bond. However, in the observation, we find that it will deviate from the ideal value ($\\\\pi$) a lot (about plus and minus $7^\\\\circ$). In this way, we include it as another flexible geometries that the model needs to generate. These three torsion angles are named $\\\\phi$, $\\\\psi$ and $\\\\omega$, respectively in formal definition.\\n\\nB.3. Experiment\\n\\nHere we give the hyper-parameters and other training details. The learning rate $lr$ is $5 \\\\times 10^{-5}$. In all training, the max training iteration is 200000. LambdaLR schedule is used, with $lr$ is set as $95 \\\\times lr$. The batch size is set 16 or 32, because it affects the performance little. In the neural networks, we set the MLP for extracting pair relations as 2 layers with 16...\"}"}
{"id": "70jplnkLMe", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The process of screening the two datasets to expand the raw PPBench.\"}"}
{"id": "70jplnkLMe", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Residue\\n\\nTo address data shortage with high quality. Compared with previous protein-peptide drug discovery tasks, we establish a new model called PPF to fulfill the task. We establish a new model called PPF. We propose a deep-learning model for target-specific peptide optimization, flexible re-docking, and side-chain packing are also studied and evaluated. Our contributions include: (i) New Task Formulation, (ii) Novel Model Establishment, (iii) Dataset Establishment, and (iv) Evaluation of Performance.\\n\\nFor a binding system composed of a protein-ligand pair, we first construct a dataset consisting of 15593 protein-peptide pairs. For training the model, we collect a benchmark dataset called PPF generated from prior distribution to the peptide's sequence-structure distributions and uses the learned probability, PPF to the task through a series of systematic steps. Instead of directly learning the explicit equation, we employ neural networks as the approximation of torsion angles in peptides. We parametrize the backbone atoms, i.e., which contains atoms determining Residue.\\n\\nThere are several parameterizations for peptide structures. Here, we firstly focus on the four backbone atoms, i.e., C, N, C, N, O, C. The probability path on M is used to approximate the gradient fields of the variables' equation. Therefore, FM and CFM objectives have the same gradients as alternative is to construct conditional density path p(x) =\\\\text{CRFM}(u, x, p, v, \\\\theta, \\\\phi) where x\\\\in T, u\\\\in M, v\\\\in P, and \\\\theta are the parameters in E(\\\\phi). Once the gradient field \\\\phi is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian metric}} g(u, v, x, p, \\\\theta). A Flow Matching (FM) tangent vector u(i, M) is learned, one can use ordinary differential equations \\\\phi dt = g(u, v, x, p, \\\\theta) to push the random variable x at time t, whose gradient reads \\\\phi x = v x, \\\\parallel \\\\phi x\\\\parallel_{\\\\text{Riemannian"}
{"id": "70jplnkLMe", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C-N-C\\n\\nwhere \\\\( \\\\tau \\\\) is the logarithm map, projecting points on the patient space of \\\\( \\\\tau \\\\) using the geodesic connecting \\\\( \\\\tau \\\\) to \\\\( \\\\tau \\\\). For the torus, we parameterize the manifold as the quotient space of \\\\( \\\\tau \\\\). The structure defines a hypertorus \\\\( T \\\\). The parameterization, discussed in Sec. 3.8.\\n\\nOther atoms in side-chains, we use the rotamer \\\\( \\\\chi \\\\) type CFM, it will be discussed in Sec. 3.4. Besides, for the sequence, we use the redundant geometries as in Figure 1 as redundant geometries according to the physicochemical conclusions (Padmanabhan, 2005), the global translation and orientation set to be constants, avoiding broken or overlapped bonds as unrealistic structures and angles of 'C-N' and angles like 'N-C', the other inflexible inter-residues' geometries as shown in Figure 1. The advantages of the parameterization include the global frame constructed by \\\\( X \\\\). The local structures can be reconstructed with torsion angles and ideal bond lengths and angles with \\\\( N \\\\). While the inflexible bond lengths and angles are \\\\( \\\\alpha \\\\) and \\\\( \\\\omega \\\\), and the conditional \\\\( \\\\tau \\\\) to the tangent space, in which \\\\( \\\\tau \\\\) is the translation vector \\\\( \\\\tau = \\\\exp(\\\\tau \\\\cdot \\\\Delta \\\\tau) \\\\). Hence, the backbone structure is parameterized by variables \\\\( \\\\{x_i, \\\\ldots, x_N\\\\} \\\\). Note that \\\\( \\\\tau \\\\) is not constrained in the parameterization. However, while the intra-residues' internal geometries are inflexible, the hypertorus (Jantzen, 2012). Instead, we regard the conditional \\\\( \\\\tau \\\\) on \\\\( \\\\tau \\\\), leading to the equivalence relation \\\\( \\\\tau \\\\sim 1 \\\\). To build the gradient vector, we provide explicit equations for geodesics on a hypertorus (Partheniou et al., 2014) and our observations (See Appendix B.2). While the global translation and orientation \\\\( R^3 \\\\) are the exponential map, projecting points back to torus (Chen & Lipman, 2023). This is the conditional Torus Flow Matching to the parameterization, discussed in Sec. 3.2 and Sec. 3.3, and for the sequence, we use the redundant geometries as in Figure 1 as redundant geometries according to the physicochemical conclusions (Padmanabhan, 2005), the global translation and orientation set to be constants, avoiding broken or overlapped bonds as unrealistic structures and angles of 'C-N' and angles like 'N-C', the other inflexible inter-residues' geometries as shown in Figure 1. The advantages of the parameterization include the global frame constructed by \\\\( X \\\\). The local structures can be reconstructed with torsion angles and ideal bond lengths and angles with \\\\( N \\\\). While the inflexible bond lengths and angles are \\\\( \\\\alpha \\\\) and \\\\( \\\\omega \\\\), and the conditional \\\\( \\\\tau \\\\) to the tangent space, in which \\\\( \\\\tau \\\\) is the translation vector \\\\( \\\\tau = \\\\exp(\\\\tau \\\\cdot \\\\Delta \\\\tau) \\\\). Hence, the backbone structure is parameterized by variables \\\\( \\\\{x_i, \\\\ldots, x_N\\\\} \\\\). Note that \\\\( \\\\tau \\\\) is not constrained in the parameterization. However, while the intra-residues' internal geometries are inflexible, the hypertorus (Jantzen, 2012). Instead, we regard the conditional \\\\( \\\\tau \\\\) on \\\\( \\\\tau \\\\), leading to the equivalence relation \\\\( \\\\tau \\\\sim 1 \\\\). To build the gradient vector, we provide explicit equations for geodesics on a hypertorus (Partheniou et al., 2014) and our observations (See Appendix B.2). While the global translation and orientation \\\\( R^3 \\\\) are the exponential map, projecting points back to torus (Chen & Lipman, 2023). This is the conditional Torus Flow Matching to the parameterization, discussed in Sec. 3.2 and Sec. 3.3, and for the sequence, we use the redundant geometries as in Figure 1 as redundant geometries according to the physicochemical conclusions (Padmanabhan, 2005), the global translation and orientation set to be constants, avoiding broken or overlapped bonds as unrealistic structures and angles of 'C-N' and angles like 'N-C', the other inflexible inter-residues' geometries as shown in Figure 1. The advantages of the parameterization include the global frame constructed by \\\\( X \\\\). The local structures can be reconstructed with torsion angles and ideal bond lengths and angles with \\\\( N \\\\). While the inflexible bond lengths and angles are \\\\( \\\\alpha \\\\) and \\\\( \\\\omega \\\\), and the conditional \\\\( \\\\tau \\\\) to the tangent space, in which \\\\( \\\\tau \\\\) is the translation vector \\\\( \\\\tau = \\\\exp(\\\\tau \\\\cdot \\\\Delta \\\\tau) \\\\). Hence, the backbone structure is parameterized by variables \\\\( \\\\{x_i, \\\\ldots, x_N\\\\} \\\\). Note that \\\\( \\\\tau \\\\) is not constrained in the parameterization. However, while the intra-residues' internal geometries are inflexible, the hypertorus (Jantzen, 2012). Instead, we regard the conditional \\\\( \\\\tau \\\\) on \\\\( \\\\tau \\\\), leading to the equivalence relation \\\\( \\\\tau \\\\sim 1 \\\\). To build the gradient vector, we provide explicit equations for geodesics on a hypertorus (Partheniou et al., 2014) and our observations (See Appendix B.2). While the global translation and orientation \\\\( R^3 \\\\) are the exponential map, projecting points back to torus (Chen & Lipman, 2023). This is the conditional Torus Flow Matching to the parameterization, discussed in Sec. 3.2 and Sec. 3.3, and for the sequence, we use the redundant geometries as in Figure 1 as redundant geometries according to the physicochemical conclusions (Padmanabhan, 2005), the global translation and orientation set to be constants, avoiding broken or overlapped bonds as unrealistic structures and angles of 'C-N' and angles like 'N-C', the other inflexible inter-residues' geometries as shown in Figure 1. The advantages of the parameterization include the global frame constructed by \\\\( X \\\\). The local structures can be reconstructed with torsion angles and ideal bond lengths and angles with \\\\( N \\\\). While the inflexible bond lengths and angles are \\\\( \\\\alpha \\\\) and \\\\( \\\\omega \\\\), and the conditional \\\\( \\\\tau \\\\) to the tangent space, in which \\\\( \\\\tau \\\\) is the translation vector \\\\( \\\\tau = \\\\exp(\\\\tau \\\\cdot \\\\Delta \\\\tau) \\\\). Hence, the backbone structure is parameterized by variables \\\\( \\\\{x_i, \\\\ldots, x_N\\\\} \\\\). Note that \\\\( \\\\tau \\\\) is not constrained in the parameterization. However, while the intra-residues' internal geometries are inflexible, the hypertorus (Jantzen, 2012). Instead, we regard the conditional \\\\( \\\\tau \\\\) on \\\\( \\\\tau \\\\), leading to the equivalence relation \\\\( \\\\tau \\\\sim 1 \\\\). To build the gradient vector, we provide explicit equations for geodesics on a hypertorus (Partheniou et al., 2014) and our observations (See Appendix B.2). While the global translation and orientation \\\\( R^3 \\\\) are the exponential map, projecting points back to torus (Chen & Lipman, 2023). This is the conditional Torus Flow Matching to the parameterization, discussed in Sec. 3.2 and Sec. 3.3, and for the sequence, we use the redundant geometries as in Figure 1 as redundant geometries according to the physicochemical conclusions (Padmanabhan, 2005), the global translation and orientation set to be constants, avoiding broken or overlapped bonds as unrealistic structures and angles of 'C-N' and angles like 'N-C', the other inflexible inter-residues' geometries as shown in Figure 1. The advantages of the parameterization include the global frame constructed by \\\\( X \\\\). The local structures can be reconstructed with torsion angles and ideal bond lengths and angles with \\\\( N \\\\). While the inflexible bond lengths and angles are \\\\( \\\\alpha \\\\) and \\\\( \\\\omega \\\\), and the conditional \\\\( \\\\tau \\\\) to the tangent space, in which \\\\( \\\\tau \\\\) is the translation vector \\\\( \\\\tau = \\\\exp(\\\\tau \\\\cdot \\\\Delta \\\\tau) \\\\). Hence, the backbone structure is parameterized by variables \\\\( \\\\{x_i, \\\\ldots, x_N\\\\} \\\\). Note that \\\\( \\\\tau \\\\) is not constrained in the parameterization. However, while the intra-residues' internal geometries are inflexible, the hypertorus (Jantzen, 2012). Instead, we regard the conditional \\\\( \\\\tau \\\\) on \\\\( \\\\tau \\\\), leading to the equivalence relation \\\\( \\\\tau \\\\sim 1 \\\\). To build the gradient vector, we provide explicit equations for geodesics on a hypertorus (Partheniou et al., 2014) and our observations (See Appendix B.2). While the global translation and orientation \\\\( R^3 \\\\) are the exponential map, projecting points back to torus (Chen & Lipman, 2023). This is the conditional Torus Flow Matching to the parameterization, discussed in Sec. 3.2 and Sec. 3.3, and for the sequence, we use the redundant geometries as in Figure 1 as redundant geometries according to the physicochemical conclusions (Padmanabhan, 2005), the global translation and orientation set to be constants, avoiding broken or overlapped bonds as unrealistic structures and angles of 'C-N' and angles like 'N-C', the other inflexible inter-residues' geometries as shown in Figure 1. The advantages of the parameterization include the global frame constructed by \\\\( X \\\\). The local structures can be reconstructed with torsion angles and ideal bond lengths and angles with \\\\( N \\\\). While the inflexible bond lengths and angles are \\\\( \\\\alpha \\\\) and \\\\( \\\\omega \\\\), and the conditional \\\\( \\\\tau \\\\) to the tangent space, in which \\\\( \\\\tau \\\\) is the translation vector \\\\( \\\\tau = \\\\exp(\\\\tau \\\\cdot \\\\Delta \\\\tau) \\\\). Hence, the backbone structure is parameterized by variables \\\\( \\\\{x_i, \\\\ldots, x_N\\\\} \\\\). Note that \\\\( \\\\tau \\\\) is not constrained in the parameterization. However, while the intra-residues' internal geometries are inflexible, the hypertorus (Jantzen, 2012). Instead, we regard the conditional \\\\( \\\\tau \\\\) on \\\\( \\\\tau \\\\), leading to the equivalence relation \\\\( \\\\tau \\\\sim 1 \\\\). To build the gradient vector, we provide explicit equations for geodesics on a hypertorus (Partheniou et al., 2014) and our observations (See Appendix B.2). While the global translation and orientation \\\\( R^3 \\\\) are the exponential map, projecting points back to torus (Chen & Lipman, 2023). This is the conditional Torus Flow Matching to the parameterization, discussed in Sec. 3.2 and Sec. 3.3, and for the sequence, we use the redundant geometries as in Figure 1 as redundant geometries according to the physicochemical conclusions (Padmanabhan, 2005), the global translation and orientation set to be constants, avoiding broken or overlapped bonds as unrealistic structures and angles of 'C-N' and angles like 'N-C', the other inflexible inter-residues' geometries as shown in Figure 1. The advantages of the parameterization include the global frame constructed by \\\\( X \\\\). The local structures can be reconstructed with torsion angles and ideal bond lengths and angles with \\\\( N \\\\). While the inflexible bond lengths and angles are \\\\( \\\\alpha \\\\) and \\\\( \\\\omega \\\\), and the conditional \\\\( \\\\tau \\\\) to the tangent space, in which \\\\( \\\\tau \\\\) is the translation vector \\\\( \\\\tau = \\\\exp(\\\\tau \\\\cdot \\\\Delta \\\\tau) \\\\). Hence, the backbone structure is parameterized by variables \\\\( \\\\{x_i, \\\\ldots, x_N\\\\} \\\\). Note that \\\\( \\\\tau \\\\) is not constrained in the parameterization. However, while the intra-residues' internal geometries are inflexible, the hypertorus (Jantzen, 2012). Instead, we regard the conditional \\\\( \\\\tau \\\\) on \\\\( \\\\tau \\\\), leading to the equivalence relation \\\\( \\\\tau \\\\sim 1 \\\\). To build the gradient vector, we provide explicit equations for geodesics on a hypertorus (Partheniou et al., 2014) and our observations (See Appendix B.2). While the global translation and orientation \\\\( R^3 \\\\) are the exponential map, projecting points back to torus (Chen & Lipman, 2023). This is the conditional Torus Flow Matching to the parameterization, discussed in Sec. 3.2 and Sec. 3.3, and for the sequence, we use the redundant geometries as in Figure 1 as redundant geometries according to the physicochemical conclusions (Padmanabhan, 2005), the global translation and orientation set to be constants, avoiding broken or overlapped bonds as unrealistic structures and angles of 'C-N' and angles like 'N-C', the other inflexible inter-residues' geometries as shown in Figure 1. The advantages of the parameterization include the global frame constructed by \\\\( X \\\\). The local structures can be reconstructed with torsion angles and ideal bond lengths and angles with \\\\( N \\\\). While the inflexible bond lengths and angles are \\\\( \\\\alpha \\\\) and \\\\( \\\\omega \\\\), and the conditional \\\\( \\\\tau \\\\) to the tangent space, in which \\\\( \\\\tau \\\\) is the translation vector \\\\( \\\\tau = \\\\exp(\\\\tau \\\\cdot \\\\Delta \\\\tau) \\\\). Hence, the backbone structure is parameterized by variables \\\\( \\\\{x_i, \\\\ldots, x_N\\\\} \\\\). Note that \\\\( \\\\tau \\\\) is not constrained in the parameterization. However, while the intra-residues' internal geometries are inflexible, the hypertorus (Jantzen, 2012). Instead, we regard the conditional \\\\( \\\\tau \\\\) on \\\\( \\\\tau \\\\), leading to the equivalence relation \\\\( \\\\tau \\\\sim 1 \\\\). To build the gradient vector, we provide explicit equations for geodesics on a hypertorus (Partheniou et al., 2014) and our observations (See Appendix B.2). While the global translation and orientation \\\\( R^3 \\\\) are the exponential map, projecting points back to torus (Chen & Lipman, 2023). This is the conditional Torus Flow Matching to the parameterization, discussed in Sec. 3.2 and Sec. 3.3, and for the sequence, we use the redundant geometries as in Figure 1 as redundant geometries according to the physicochemical conclusions (Padmanabhan, 2005), the global translation and orientation set to be constants, avoiding broken or overlapped bonds as unrealistic structures and angles of 'C-N' and angles like 'N-C', the other inflexible inter-residues' geometries as shown in Figure 1. The advantages of the parameterization include the global frame constructed by \\\\( X \\\\). The local structures can be reconstructed with torsion angles and ideal bond lengths and angles with \\\\( N \\\\). While the inflexible bond lengths and angles are \\\\( \\\\alpha \\\\) and \\\\( \\\\omega \\\\), and the conditional \\\\( \\\\tau \\\\) to the tangent space, in which \\\\( \\\\tau \\\\) is the translation vector \\\\( \\\\tau = \\\\exp(\\\\tau \\\\cdot \\\\Delta \\\\tau) \\\\). Hence, the backbone structure is parameterized by variables \\\\( \\\\{x_i, \\\\ldots, x_N\\\\} \\\\). Note that \\\\( \\\\tau \\\\) is not constrained in the parameterization. However, while the intra-residues' internal geometries are inflexible, the hypertorus (Jantzen, 2012). Instead, we regard the conditional \\\\( \\\\tau \\\\) on \\\\( \\\\tau \\\\), leading to the equivalence relation \\\\( \\\\tau \\\\sim 1 \\\\). To build the gradient vector, we provide explicit equations for geodesics on a hypertorus (Partheniou et al., 2014) and our observations (See Appendix B.2). While the global translation and orientation \\\\( R^3 \\\\) are the exponential map, projecting points back to torus (Chen & Lipman, 2023). This is the conditional Torus Flow Matching to the parameterization, discussed in Sec. 3.2 and Sec. 3.3, and for the sequence, we use the redundant geometries as in Figure 1 as redundant geometries according to the physicochemical conclusions (Padmanabhan, 2005), the global translation and orientation set to be constants, avoiding broken or overlapped bonds as unrealistic structures and angles of 'C-N' and angles like 'N-C', the other inflexible inter-residues' geometries as shown in Figure 1. The advantages of the parameterization include the global frame constructed by \\\\( X \\\\). The local structures can be reconstructed with torsion angles and ideal bond lengths and angles with \\\\( N \\\\). While the inflexible bond lengths and angles are \\\\( \\\\alpha \\\\) and \\\\( \\\\omega \\\\), and the conditional \\\\( \\\\tau \\\\) to the tangent space, in which \\\\( \\\\tau \\\\) is the translation vector \\\\( \\\\tau = \\\\exp(\\\\tau \\\\cdot \\\\Delta \\\\tau) \\\\). Hence, the backbone structure is parameterized by variables \\\\( \\\\{x_i, \\\\ldots, x_N\\\\} \\\\). Note that \\\\( \\\\tau \\\\) is not constrained in the parameterization. However, while the"}
{"id": "70jplnkLMe", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LOW: Target-aware Peptide Design with Torsional Flow Matching\\n\\n\\\\[ \\\\nu_T(\\\\tau_t | \\\\tau_0, \\\\tau_1) \\\\]\\n\\n\\\\[ \\\\nu_T(x_t | x_0, x_1) \\\\]\\n\\n\\\\[ \\\\nu_T(O_t | O_0, O_1) \\\\]\\n\\n\\\\[ \\\\nu_0(\\\\psi) \\\\]\\n\\n\\\\[ \\\\nu_0(\\\\omega) \\\\]\\n\\nEncoder\\nEmbedding\\nRDE\\nEmbedding\\n\\\\[ \\\\chi^{-1} \\\\chi^{-2} \\\\cdots \\\\]\\n\\nType Flow\\n\\\\[ \\\\cdots \\\\]\\n\\nSO(3) Flow\\nEuclidean Flow\\nTorus Flow\\n\\nInitial State\\nDesigned Backbone\\nFull-atom Peptide\\nBackbone Generation\\nSide-chain Packing\\nBinding Site\\nAngle Distribution\\nN\\nC\\nCentroid\\nC\u03b1\\n\\nFigure 2: Workflows of PPF LOW in target-aware peptide drug generation task.\\n\\n3.3. PPF LOW on SE(3)\\n\\nThe torsion angles can reconstruct the local structures of the peptides, while in the global coordinate system, to represent the positions of residues, we need to determine their global translations and rotations.\\n\\nAs discussed in Sec. 3.1, the pose representation on SE(3) can be decomposed into global translation as \\\\( x(C) \\\\in \\\\mathbb{R}^3 \\\\) and rotation \\\\( O(C) \\\\in \\\\text{SO}(3) \\\\). For notation simplicity, we omit the superscript \\\\((C)\\\\) in this part. To model the probability path of \\\\( x_t \\\\), we employ vanilla Gaussian CFM on Euclidean manifolds, with Independent Coupling techniques:\\n\\n\\\\[ \\\\mu_t(x | x_0, x_1) = tx_1 + (1 - t)x_0; \\\\]\\n\\\\[ \\\\sigma_t = \\\\sigma, \\\\]\\n\\nwhere \\\\( x_0 \\\\sim N(x | 0, I) \\\\), thus leading to the Gaussian probability path of \\\\( p_t(x | x_0, x_1) = N(x | \\\\mu_t, \\\\sigma_t) \\\\), and the loss to train the conditional Euclidean Flow Matching as\\n\\n\\\\[ L_{EFM}(\\\\theta) = \\\\mathbb{E}_{t \\\\sim U(0, 1)}[p_1(x_1), p_0(x_0), p_t(x | x_0, x_1)] \\\\parallel \\\\nu_t(x) - x_1 + x_0 \\\\parallel_2^2. \\\\]\\n\\nTo model the probability path of rotation matrix \\\\( O_t \\\\in \\\\text{SO}(3) \\\\), we employ \\\\( \\\\text{SO}(3) \\\\)-CFM (Yim et al., 2023a), in which \\\\( O_t = \\\\exp_{O_0} t \\\\log_{O_0}(O_1) \\\\). In the implementation, since the \\\\( \\\\text{SO}(3) \\\\) is a simple manifold with closed-form geodesics, the exponential map can be computed using Rodrigues\u2019 formula and the logarithmic map is similarly easy to compute with its Lie algebra \\\\( so(3) \\\\) (Yim et al., 2023b). The prior distribution to sample \\\\( O_0 \\\\) is defined as isotropic Gaussian distribution, by first parameterizing \\\\( O_0 \\\\) in axis-angle, where the axis of rotation is sampled uniformly and the density function of rotation angle \\\\( \\\\vartheta \\\\) reads\\n\\n\\\\[ \\\\text{IG}(\\\\vartheta) = \\\\frac{1}{\\\\pi} - \\\\frac{\\\\cos \\\\vartheta}{\\\\pi} \\\\sum_{l=0}^{\\\\infty} (2l+1)e^{-l(l+1)\\\\epsilon} \\\\sin \\\\left( \\\\frac{(l+1)\\\\vartheta}{2} \\\\right) \\\\sin \\\\left( \\\\frac{\\\\vartheta}{2} \\\\right) \\\\]\\n\\n(Creasey & Lang, 2017). For the conditional gradient field, we employ fast numerical tricks (Bose et al., 2023) to calculate the \\\\( \\\\text{SO}(3) \\\\) component of the global Orientation conditional Flow Matching objectives:\\n\\n\\\\[ L_{OFM}(\\\\theta) = \\\\mathbb{E}_{t \\\\sim U(0, 1)}[p_1(O_1), p_0(O_0), p_t(O | O_0, O_1)] \\\\parallel v_t(O) - \\\\log_{O_0}(O_t) \\\\parallel_{\\\\text{SO}(3)}^2. \\\\]\\n\\nThe correctness of the learning objective of Equation. 9 and 10 is proposed and proven in (Lipman et al., 2022) and (Bose et al., 2023), respectively.\"}"}
{"id": "70jplnkLMe", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The overall loss function is the summation of the four loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10 and 11. To fully utilize the structure-sequence contexts as input, in this way, we infer one structure through the ODE sampling process SE(3)-equivariant. Further, in terms of CFM, we first adopt two multi-layer perceptrons Encoder and backbone structure in local frame, and translate and rotate field \\\\(v^0\\\\). Then, our overall training loss is written as the sum of the loss functions in Equation. 7, 9, 10"}
