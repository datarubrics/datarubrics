{"id": "V4qV08Vk6S", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Scene Question | Response | Ground Truth | Response Type |\\n|----------------|----------|--------------|---------------|\\n| What is the only orange object on my left side? | backpack | jacket | Wrong |\\n| Is there anything on the desk? | yes | laptop | Wrong but accurate |\\n| What type of bed is the one on the right? | loft bed | loft bed | Correct |\\n| What am I sitting at? | desk | desk | Correct |\\n| How many beds are in front of me? | one | one | Correct |\"}"}
{"id": "V4qV08Vk6S", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table A.25: Examples from our datasets\\n\\nWe provide the tasks, 2D images (optional), 3D point clouds, and instructions as the input of LEO, as well as the expected responses of LEO.\\n\\n| Dataset & Task      | 2D & 3D input | Instruction | Response |\\n|---------------------|---------------|-------------|----------|\\n| LEO-align object captioning |              | USER: Explain the object at the pinpointed location in the 3D setting. | ASSISTANT: a yellow metal cage with a fence around it. |\\n| LEO-align object referring |              | USER: Give me a snapshot of the object at the chosen 3D location. | ASSISTANT: find the office chair that is near the copier |\\n| LEO-align scene captioning |              | USER: Relay a brief summary of this scene's elements. | ASSISTANT: In this scene, there is a gray flat picture hanging on the wall, standing in front of a windowsill. There is also a rectangular white padded pillow lying on the bed, close to a big rectangular white padded pillow. The room has a white architectural wall and a flat overhead ceiling. The bed is tidy and white, located in front of a narrow tall commode. The curtains are hanging on the wall. The room has a calm and comfortable atmosphere, suitable for relaxation and sleep. |\\n\\n37\"}"}
{"id": "V4qV08Vk6S", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nTable A.26: Examples from our datasets. Continued.\\n\\nLEO-instruct 3D captioning\\n\\nUSER: What does the object at the specified position in the 3D visualization look like?\\nASSISTANT: There is a door in the middle of the northern wall next to a metal cabinet. The door is shaped like a brown rectangle and has a dark yellow colored door-knob.\\n\\nLEO-instruct 3D QA\\n\\nUSER: What is in the right corner of the room by the curtains?\\nASSISTANT: Brown cabinet with a TV sitting in it.\\n\\nLEO-instruct 3D dialogue\\n\\nUSER: I'm redecorating my living room. Can you give me some suggestions on how to arrange the furniture?\\nASSISTANT: Of course! I'd be happy to help. What furniture do you have in your living room?\\nUSER: I have a sofa, a table, a chair, and a couch table.\\nASSISTANT: Great! Here's a suggestion: you can place the sofa against the wall, with the table in front of it. The chair can be placed to the right of the table, and the couch table can be placed to the right of the chair. This arrangement creates a cozy seating area.\\n\\nLEO-instruct task planning\\n\\nUSER: Develop a plan for this: Arrange and embellish the family room.\\nASSISTANT: 1. Arrange and fluff the cushions and pillows on the armchair.\\n2. Place decorations, such as figurines or vases, on the cabinet or shelf.\\n3. Place hygiene products, such as wipes or lotions, on the commode.\\n4. Turn on the lights and adjust their brightness if needed.\\n5. Arrange diapers in a designated storage area.\\n6. Place a blanket or pillow on the armchair for added comfort.\\n7. Adjust the festoon or other decorative elements if desired.\"}"}
{"id": "V4qV08Vk6S", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table A.27: Examples from our datasets. Continued.\\n\\n**LEO-instruct**\\n\\n- **Embodied navigation**\\n\\n  USER: The task is navigation. Your goal is to find the counter by moving around in the scene. Past actions: `<31999> <31999> <31999> <31999>`.\\n\\n  ASSISTANT:\\n\\n- **Robotic manipulation**\\n\\n  USER: The task is manipulation. Your goal is to put the blue blocks in a green bowl. Past actions: `<31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511> <31991> <31671> <31511>`.\\n\\n  ASSISTANT:\"}"}
{"id": "V4qV08Vk6S", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nFigure A.13: Noun-verb pairs: 3RDialog instruction.\\n\\nFigure A.14: Noun-verb pairs: 3RPlan instruction.\\n\\nFigure A.15: Noun-verb pairs: 3RDialog response.\\n\\nFigure A.16: Noun-verb pairs: 3RPlan response.\\n\\nThe last portion of prompts is a task-specific instruction. For object-level caption and object-in-the-scene caption, we randomly chose one sentence from 151 sentences to be the instruction. Some examples can be found in Tab. A.8. For scene-level caption, we randomly choose one from 183 instructions. Examples can be found in Tab. A.9. For 3D question answering task, we simply use the question as the instruction. The dialog history is used as the instruction for 3D dialogue to provide continuity across multiple rounds of interactions. A planning instruction pool consisting of 202 instructions is introduced for scene-aware task planning and we randomly choose one from it as done in the caption tasks. Examples from the pool can be found in Tab. A.10. The chosen instruction is further followed by an instruction that specifies the task, e.g., set up a home office.\"}"}
{"id": "V4qV08Vk6S", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nTable A.8: Examples from our object-level caption instruction set.\\n\\n\u201cProduce a description for the object at the chosen spot in the 3D scene.\u201d\\n\u201cHow would you depict the object located at the selected point in the 3D environment?\u201d\\n\u201cFormulate a description of the item at the picked position within the 3D scene.\u201d\\n\u201cHow would you describe the entity at the designated location in the 3D backdrop?\u201d\\n\u201cCan you detail the object situated at the selected point in the 3D setting?\u201d\\n\u201cCompose a narrative for the object at the chosen locale within the 3D environment.\u201d\\n\u201cWhat does the object at the specified position in the 3D visualization look like?\u201d\\n\u201cProvide a description for the item located at the marked site in the 3D world.\u201d\\n\u201cHow would you illustrate the object placed at the selected spot in the 3D landscape?\u201d\\n\u201cCraft a depiction of the object at the pinpointed location within the 3D territory.\u201d\\n\u201cWhat kind of object is illustrated at the identified site in the 3D tableau?\u201d\\n\u201cDevelop a description of the object at the specified position in the 3D backdrop.\u201d\\n\u201cWhat is the entity\u2019s detail at the highlighted site in the 3D view?\u201d\\n\u201cWrite up a description of the entity at the selected spot in the 3D realm.\u201d\\n\u201cWhat does the object look like at the pinpointed location in the 3D space?\u201d\\n\u201cDetail the entity located at the chosen position within the 3D scene.\u201d\\n\u201cCan you explain the essence of the object at the selected spot in the 3D zone?\u201d\\n\\nWith past action tokens \\\\{PAST\\\\_ACTIONS\\\\} appended at the end, the instruction for embodied navigation is as follows, where \\\\{GOAL\\\\} stands for the goal specified by the target object name:\\n\\nThe task is navigation. Your goal is to find \\\\{GOAL\\\\} by moving around in the scene. Past actions: \\\\{PAST\\\\_ACTIONS\\\\}.\\n\\nThe instruction for robotic manipulation is similar to the one in embodied navigation. Here \\\\{GOAL\\\\} is the task description in CLIPort:\\n\\nThe task is manipulation. Your goal is to \\\\{GOAL\\\\}. Past actions: \\\\{PAST\\\\_ACTIONS\\\\}.\\n\\nD.2. Feature Encoding\\n\\nWe have several modules to encode the multi-modal features.\\n\\n\u2022 Object-centric 3D token embedding.\\n  The encoder for 3D object-centric point clouds is a PointNet++ (Qi et al., 2017) pre-trained on ScanNet (Dai et al., 2017) with object-classfication task. We sample 1024 points for every object as in (Chen et al., 2022). The architecture parameters all remain the same with (Chen et al., 2022). We freeze the PointNet++ for empirically better results.\\n\\n\u2022 Spatial Transformer (Chen et al., 2022).\\n  Spatial Transformer is a modified transformer architecture that explicitly encodes spatial relations between object pairs. Specifically, consider the vanilla self-attention (Vaswani et al., 2017) mechanism which takes as input a feature matrix $X \\\\in \\\\mathbb{R}^{N \\\\times d}$, where $N$ stands for the number of tokens and $d$ is the feature dimension. Vanilla self-attention first compute $Q = X W_Q$, $K = X W_K$, $V = X W_V$ from $X$ using learnable projection matrices $W_Q, W_K, W_V \\\\in \\\\mathbb{R}^{d \\\\times d_h}$ where $d_h$ stands for the output feature dimension. Then the attention weight matrix is computed by $(\\\\omega_o)_{N \\\\times N} = \\\\Omega_o = \\\\text{softmax}(QK^T \\\\sqrt{d_h})$ and finally used for re-weighting $\\\\Omega_o V$. The intuition of Spatial Transformer is that we can re-scale the elements $\\\\omega_o$ in the weight matrix $\\\\Omega_o$.\\n\\nIn the object-centric reasoning setting, the input feature matrix is $O \\\\in \\\\mathbb{R}^{N \\\\times d}$. Consider an object pair $(O_i, O_j)$ with their geometric centers $c_i, c_j$. Spatial Transformer (Chen et al., 2022) computes the Euclidean distance $d_{ij} = ||c_i - c_j||^2$ and the horizontal and vertical angles $\\\\theta_h, \\\\theta_v$ of the line connecting $c_i$ and $c_j$. The spatial feature between the two objects $(O_i, O_j)$ is a 5-dimensional vector $f_{ij} = [d_{ij}, \\\\sin(\\\\theta_h), \\\\cos(\\\\theta_h), \\\\sin(\\\\theta_v), \\\\cos(\\\\theta_v)]$. To combine this feature with objects, 25\"}"}
{"id": "V4qV08Vk6S", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nTable A.9: Examples from our scene-level caption instruction set.\\n\\n\\\"Describe this scene.\\\"\\n\\\"Generate a description of this scene.\\\"\\n\\\"Generate a caption of this scene.\\\"\\n\\\"Can you describe the scene?\\\"\\n\\\"Can you generate a description of the scene?\\\"\\n\\\"Can you generate a caption of the scene?\\\"\\n\\\"Summarize this scene.\\\"\\n\\\"Provide an outline of this 3D scene's characteristics.\\\"\\n\\\"How would you describe the 3D scene?\\\"\\n\\\"How would you summarize this scene?\\\"\\n\\\"Convey a summary of the 3D structure of this scene.\\\"\\n\\\"How would you interpret this 3D scene?\\\"\\n\\\"Offer a summary of the 3D scene.\\\"\\n\\\"Can you describe this scene in detail?\\\"\\n\\\"I'm interested in this scene, can you explain?\\\"\\n\\\"What is this scene made of?\\\"\\n\\\"Could you provide more info about this scene?\\\"\\n\\nTable A.10: Examples from our planning instruction pool.\\n\\n\\\"Plan for the task\\\"\\n\\\"Can you come up with a plan for this task\\\"\\n\\\"How can we do this task, provide a step-by-step plan\\\"\\n\\\"Draft a plan for completing this task\\\"\\n\\\"Detail a strategy for the task\\\"\\n\\\"What's the best plan for this task\\\"\\n\\\"Draw out a procedure for the task\\\"\\n\\\"Lay out the steps for this task\\\"\\n\\\"Could you devise a plan for the task\\\"\\n\\\"Show me a plan for this task\\\"\\n\\\"I need a plan for the task\\\"\\n\\\"Sketch a plan for the task at hand\\\"\\n\\\"Set up a plan for this\\\"\\n\\\"Recommend a plan for this task\\\"\\n\\\"Offer a strategy for this task\\\"\\n\\\"Design a blueprint for the task\\\"\\n\\\"Outline the approach for this task\\\"\\n\\nThe spatial attention computes \\\\( \\\\omega_{ij} = g_i f_{ij} \\\\) where \\\\( g_i = W^T S o_i \\\\) is a 5-dimensional vector. The spatial attention further reweights the original self-attention weight matrix as \\\\( \\\\omega_{ij} = \\\\sigma(\\\\omega_{sij}) \\\\exp(\\\\omega_{oij}) \\\\)\\n\\n\\\\( P_N l = 1 \\\\sigma(\\\\omega_{sill}) \\\\exp(\\\\omega_{oil}) \\\\).\\n\\nReaders are referred to (Chen et al., 2022) for more details. In summary, Spatial Transformer explicitly computes pairwise spatial relations and fuses them with vanilla self-attention to provide better spatial reasoning ability. We use a three-layer Spatial Transformer with 8 heads to process the object-centric features produced by PointNet++ and output object tokens for LLM. For other settings, we follow all the default hyperparameters in (Chen et al., 2022).\\n\\n\u2022 2D token embedding. We use OpenCLIP ConvNext-base model (Liu et al., 2022) pre-trained on LAION2B.\"}"}
{"id": "V4qV08Vk6S", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World (et al., 2022) to process the egocentric 2D image.\\n\\nTo inject more semantics into visual tokens, we use the text encoder from CLIP (Radford et al., 2021) to process the instruction tokens to obtain a global semantics feature. Next, we update the visual tokens via element-wise product between the CLIP semantics feature and each image & object token embedding.\\n\\nD.2.1. Embodiment Encoding\\n\\nIn addition to the egocentric 2D input, we introduce an embodiment token to help LEO reason in an embodiment-aware fashion. We find it useful to use it together with the situation prompt and 2D egocentric input. Specifically, an embodiment token \\\\( e \\\\) is introduced in embodied navigation, embodied reasoning, and object-in-the-scene caption tasks. Specifically, \\\\( e \\\\) is a learnable embedding that will be inserted into the 3D object list.\\n\\nSo what does embodiment information mean in these tasks? In embodied navigation, it means the agent's position and orientation in the scene, which can be derived from a GPS and a compass sensor. The orientation of the agent is further represented by a rotation which is Fourier-embedded and mapped to a feature vector \\\\( r \\\\) by a linear layer. It is the same in embodied reasoning task. In the object-in-the-scene caption task, we assume the agent is situated at the location of the object that is being referred to. Therefore, embodiment information also means the location of the referred object. We obtain this location by randomly choosing a spot inside the referred object bounding box. To sum up, we could simply treat the embodiment token as a special self object, where its object embedding is learnable, and its location/orientation corresponds to the actual or assumed \u201cagent.\u201d\\n\\nAfter inserting the embodiment token, we obtain a new 3D object token list:\\n\\n\\\\[\\n\\\\begin{align*}\\ne, s^{(1)}_3, & \\\\ldots, s^{(N)}_3, \\\\\\\\\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\( s^{(i)}_3 \\\\in \\\\{1, 2, \\\\ldots, N\\\\} \\\\) are 3D object token embeddings produced by PointNet++, along with location specified for each object (including the self-object). We can concatenate them together to get a feature matrix \\\\( O \\\\in \\\\mathbb{R}^{(N+1) \\\\times d} \\\\) and send them to the Spatial Transformer to explicitly fuse the spatial information of all the 3D objects and the self-object.\\n\\nD.3. Action Tokenization\\n\\nTo empower LEO to exert control over an embodiment or a robot, we encode all actions within the context of Object Navigation (Ramrakhya et al., 2022) and CLIPort (Shridhar et al., 2021) tasks using the least frequently employed language tokens. Specifically, for the Object Navigation task, we allocate 4 tokens to represent actions of move forward, turn right, turn left, and stop. For the CLIPort task, we use a total of 516 tokens to discretize action poses, with 320 tokens dedicated to the x-axis pose bins, 160 tokens for the y-axis pose bins, and 36 tokens for the z-rotation bins.\\n\\nD.4. LLM Hyperparameters\\n\\nWe set the maximum output length of our Vicuna-7B to be 256. The maximum context length is also set to 256 and if the length of the input is greater than 256, we truncate it to 256 by deleting tokens from the left (i.e., only the rightmost 256 tokens are preserved). We set rank and \\\\( \\\\alpha \\\\) in LoRA (Hu et al., 2022) to be 16 and the dropout rate to be 0. LoRA is implemented for all the projection matrices in the LLM, i.e., \\\\( (W_q, W_k, W_v, W_o) \\\\) in attention modules and \\\\( (W_{gate}, W_{up}, W_{down}) \\\\) in MLPs.\\n\\nThe hyperparameters for inference are listed in Tab. A.11.\\n\\nE. Alignment Setup\\n\\nThe hyperparameters for 3D VL alignment are presented in Tab. A.12.\\n\\nF. Instruction-tuning Setup\\n\\nThe hyperparameters for 3D VLA instruction tuning are presented in Tab. A.13.\"}"}
{"id": "V4qV08Vk6S", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nTable A.20: Quantitative comparison between 3D-LLM and LEO when both trained on 3D-LLM data. Metrics follow Tab. 4.\\n\\n|                | ScanQA (val) | SQA3D (test) |\\n|----------------|--------------|---------------|\\n|                | C            | B-4           | M          | R          | EM@1       | EM@5       |\\n| 3D-LLM         | 74.5         | 12.9          | 15.1       | 37.5       | 21.2       | 49.8       |\\n| LEO            | 97.4         | 14.6          | 19.1       | 46.8       | 23.2       | 50.6 (45.4)|\\n\\nTable A.21: Results on object navigation with OOD objects and human demonstrations.\\n\\nNote that the baseline Habitat-web is unable to generalize to MP3D-unseen as it uses categorical embedding rather than natural language to represent object goals.\\n\\n|                | MP3D-seen | MP3D-unseen |\\n|----------------|-----------|-------------|\\n| Success (\u2191)    |           |             |\\n| SPL (\u2191)        |           |             |\\n| Habitat-web (shortest) | 4.4 | 2.2 | - | - |\\n| Habitat-web (70k demo) | 35.4 | 10.2 | - | - |\\n| LEO (shortest, w/o 2D) | 7.8 | 4.6 | - | - |\\n| LEO (shortest, w/o 3D) | 8.6 | 6.8 | - | - |\\n| LEO (shortest) | 23.1 | 15.2 | 11.1 | 9.6 |\\n| LEO (70k demo) | 7.1 | 5.3 | 8.9 | 8.6 |\\n\\nTab. A.20 show that LEO consistently outperforms 3D-LLM when adopting the same training data. Notably, the magnitude of this subset is much smaller than their original training data, which further underscores the efficiency of our model.\\n\\nI.4. Embodied Acting\\n\\nQuantitative results of ObjNav.\\n\\nWe provide additional results of LEO 1) generalizing to unseen objects on MP3D (below is a list of the objects used during training (seen) and for OOD evaluation (unseen)), 2) learning with 70K human demonstrations provided by Habitat-web (Ramrakhya et al., 2022) instead of shortest path, and 3) learning without one modality (full vs. w/o 3D vs. w/o 2D). Evaluation results are shown in Tab. A.21. Note that the baseline Habitat-web is unable to generalize to novel objects as it uses categorical embedding rather than natural language to represent object goals.\\n\\nObjects (seen)\\n\\n| \"gym_equipment\", \"tv_monitor\", \"picture\", \"counter\", \"chair\", \"cabinet\", |\\n| \"table\", \"stool\", \"plant\", \"towel\", \"sofa\", \"cushion\", \"sink\", \"fireplace\", |\\n| \"toilet\", \"seating\", \"chest_of_drawers\", \"bed\", \"shower\", \"bathtub\", |\\n| \"clothes\" |\\n\\nObjects (unseen)\\n\\n| \"shelf\", \"pillow\", \"lamp\", \"box\", \"desk\", \"refrigerator\", \"vase\", \"armchair\" |\\n\\nThe results show that LEO can generalize to novel objects. On the other hand, human demonstrations include more explorations, compared with shortest-path data. Therefore, it will be much harder for agents without a recurrent module (e.g., LEO) to learn from human demonstrations (see Appendix H.2), leading to significantly weaker performances. Finally, more modalities (2D + 3D) are indeed crucial to the success of mastering object navigation skills.\\n\\nQualitative results\\n\\nWe provide more qualitative results of robotic manipulation and embodied navigation in the supplementary video.\\n\\nI.5. Scan2Cap\\n\\nWe provide additional qualitative results on Scan2Cap validation set in Tab. A.22. The results show that LEO can correctly refer to the queried object and provide accurate descriptions, including spatial relationships with other objects. However, LEO's responses are confined to simple formats that lack diversity. How to unlock more flexible responses while maintaining accuracy can be a direction for future research.\"}"}
{"id": "V4qV08Vk6S", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We provide additional qualitative results on ScanQA validation set in Tab. A.23 and categorize the responses into several types:\\n\\n- **Wrong**: The response is inaccurate and deemed wrong.\\n- **Wrong but reasonable**: The response is deemed wrong but is reasonable to some extent, probably due to ambiguities in the scene. Consider the second case in Tab. A.23. There are many objects such as a coat rack, a coat, and a mini fridge-shaped cabinet on the right side of the organizer. Though LEO\u2019s response \u201cmini fridge\u201d does not match the ground truth \u201ccoat rack\u201d, it is consistent with the 3D scene layout.\\n- **Wrong but accurate**: The response is accurate according to the scene but is deemed wrong due to imperfect ground truth annotations.\\n- **Correct**: The response is accurate and deemed correct.\\n- **Correct and more accurate**: The response is more accurate than the ground truth annotations.\\n\\nWe provide additional qualitative results on SQA3D test set in Tab. A.24 and follow the aforementioned response types. The embodied reasoning in SQA3D requires the understanding of not only the scene but also the situation of embodiment. In Tab. A.24, answering \u201cWhat am I sitting at?\u201d necessitates that LEO accurately identifies the objects at its current location. And the response to \u201cHow many beds are in front of me?\u201d indicates that LEO can reason based on the understanding of its orientation.\"}"}
{"id": "V4qV08Vk6S", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table A.22: Additional qualitative results on Scan2Cap validation set\\n\\n| Scene                                                                 | Instruction                                                                 | Response                                                                 | Ground Truth                                                                 |\\n|----------------------------------------------------------------------|----------------------------------------------------------------------------|--------------------------------------------------------------------------|----------------------------------------------------------------------------|\\n| **Develop a description of the object at the specified position in the 3D backdrop.** | This is a black keyboard. It is in front of two monitors.                  | The keyboard sits atop the workstation desk. It is located in front of the left-side monitor. |                                                                            |\\n| **Focus on the object at the given 3D location and describe it.**     | This is a white pillow. It is on the couch.                                 | It is a small pillow located on the couch. You can notice it directly on your left when walking through the door into the room. |                                                                            |\\n| **Provide a clearer description of the object at that specific 3D spot.** | This is a white cabinet. It is to the right of a couch.                    | This is a white ottoman. It is located to the right of the black couch.   |                                                                            |\\n| **What does the object at the specified position in the 3D visualization look like?** | There is a rectangular brown door. It is next to a couch.                 | There is a door in the middle of the northern wall next to a metal cabinet. | The door is shaped like a brown rectangle and has a dark yellow colored door-knob. |\"}"}
{"id": "V4qV08Vk6S", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nTable A.23: Additional qualitative results on ScanQA validation set.\\n\\n| Scene | Question | Response | Ground Truth | Response Type |\\n|-------|----------|----------|--------------|---------------|\\n|       | What is placed on the black chair out of the three chairs present in the room? | pillow | tan jacket, guitar box like object | Wrong |\\n|       | What is next to the organizer shelf? | mini fridge | coat rack | Wrong but reasonable |\\n|       | Where is the dresser located? | to right of door | underneath television next to black guitar case and green guitar case, under tv set next to guitar cases | Wrong but accurate |\\n|       | What kind of desk is the electric piano on? | wooden desk | wooden desk | Correct |\\n|       | How many monitor sits on top of the desk? | 2 | 2 | Correct |\\n|       | What is on the left side of the brown desk? | black computer monitor | monitor / computer monitor | Correct and more accurate |\"}"}
{"id": "V4qV08Vk6S", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nLeveraging massive knowledge from large language models (LLMs), recent machine learning models show notable successes in general-purpose task solving in diverse domains such as computer vision and robotics. However, several significant challenges remain: (i) most of these models rely on 2D images yet exhibit a limited capacity for 3D input; (ii) these models rarely explore the tasks inherently defined in 3D world, e.g., 3D grounding, embodied reasoning and acting. We argue these limitations significantly hinder current models from performing real-world tasks and approaching general intelligence. To this end, we introduce LEO, an embodied multimodal generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world. LEO is trained with a unified task interface, model architecture, and objective in two stages: (i) 3D vision-language (VL) alignment and (ii) 3D vision-language-action (VLA) instruction tuning. We collect large-scale datasets comprising diverse object-level and scene-level tasks, which require considerable understanding of and interaction with the 3D world. Moreover, we meticulously design an LLM-assisted pipeline to produce high-quality 3D VL data. Through extensive experiments, we demonstrate LEO's remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, navigation and manipulation. Our ablative studies and scaling analyses further provide valuable insights for developing future embodied generalist agents. Code and data are available on project page.\\n\\n1. Introduction\\nBuilding one generalist model that can handle comprehensive tasks like humans has been a long-existing pursuit in artificial intelligence and neuroscience (Lake et al., 2015; 2017; Zhu et al., 2020; Mountcastle, 1979; Schmidhuber, 2018; Huang et al., 2022a). Recent advances in LLMs (Brown et al., 2020) and \u201cfoundation models\u201d (Bommasani et al., 2021) emerge as a promising paradigm in building such generalist models in natural language processing (OpenAI, 2022; 2023), computer vision (Kirillov et al., 2023; Alayrac et al., 2022), and robotics (Brohan et al., 2022; 2023; Reed et al., 2022; Driess et al., 2023; Li et al., 2023c). The keys to the success of this paradigm lie in large-scale internet-level datasets from numerous tasks and domains, as well as scalable Transformer architectures (Vaswani et al., 2017) that can absorb generalizable and task-agnostic knowledge from the data. Nonetheless, existing generalist models primarily thrive within 2D domains, lacking comprehension of the 3D physical environment that envelops human-level intelligence. This limitation stands as an obstacle that prevents current models from solving real-world tasks and approaching general intelligence. Therefore, we ask a fundamental question: how to equip the generalist agent with a comprehensive understanding of and the ability to interact with the real 3D world? The development of such generalist agents encounters three primary challenges: the lack of suitable datasets, unified models, and effective learning strategies. Despite substantial progress in scaling up image-text models (Tsimpoukelli et al., 2021; Alayrac et al., 2022) and the curation of corresponding datasets (Radford et al., 2021; Schuhmann et al., 2022), advancement in 3D scene-level understanding has significantly lagged behind. This is largely attributed to the limited scale and manual labeling of 3D datasets (Dai et al., 2017; Wald et al., 2019; Chen et al., 2020), given the higher cost associated with collecting 3D data compared to 2D data. Furthermore, large-scale unified pretraining and efficient finetuning are under-explored by previous 3D VL models, which are often designed with strong priors (Zhao et al., 2021; Chen et al., 2022). Notably, recent works (Zhu et al., 2023c; Hong et al., 2023) utilize multi-modal Transformer together with synthetic data to enhance the model's capability in grounded 3D scene understanding. Nevertheless,\"}"}
{"id": "V4qV08Vk6S", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nEmbodied Navigation\\n\\nLEO:\\n1. Move forward\\n2. Turn right\\n3. Move forward\\n4. \u2026\\n\\nHuman: Find the pillow\\nRobotic Manipulation\\nHuman: pack the yoshi figure in the brown box\\nLEO:\\n\\\\[ P = [0, 1, -0, 2, 0] \\\\]\\n\\\\[ R = [0, 0, 0, 1] \\\\]\\n\\nTask Planning\\nHuman: Organize and tidy up the bedroom.\\nLEO:\\n1. Clean the floor by sweeping to remove \u2026\\n2. Make the bed \u2026\\n3. Fold and organize \u2026\\n\\n3D Question Answering\\nQuestion: Where is the Printer located?\\nLEO:\\nStanding on the desk next to the wardrobe.\\n\\n3D Dialogue\\nHuman: I need help organizing my bedroom. Can you assist me?\\nLEO:\\nOf course! What specifically do you need help with?\\nHuman: Can you check if there are any clean clothes hanging on the wall?\\nLEO:\\nI'm sorry, but the clothes hanging on the wall are messy.\\n\\nScene Captioning\\nLEO:\\nIn this room, there is a wooden floor that is clean and flat. A tall wardrobe \u2026\\n\\nEmbodied Reasoning\\nSituation:\\nSitting at the edge of the bed and facing the couch.\\nQuestion:\\nCan I go straight to the coffee table in front of me?\\nLEO:\\nNo.\\n\\n3D Object Captioning\\nLEO:\\nA 3D model of a Southern White Rhino, one of largest animal species in the world.\\n\\nLEO:\\nIt's a 3D model of a midnight green ceramic coffee cup with a plate.\\n\\nUser:\\nPlease describe the toy house over \u2026\\n\\n\u2744 \ud83d\udd25 \ud83d\udd25\\n\\nYou are an assistant situated in a 3D scene.\"}"}
{"id": "V4qV08Vk6S", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nIn summary, our main contributions are as follows: (i) we propose LEO, the first embodied generalist agent capable of following human instructions to perceive, ground, reason, plan, and act in the 3D world; (ii) we propose a simple yet effective framework that connects object-centric 3D representation and LLM to efficiently bridge the gap between vision, language, and embodied action; (iii) we collect large-scale comprehensive datasets for our two-stage generalist training scheme, and particularly propose an LLM-assisted pipeline for the generation of high-quality 3D VL data; (iv) we conduct extensive experiments to demonstrate LEO\u2019s proficiency across various tasks, and present in-depth analyses to reveal valuable insights; (v) we release the data, code, and model weights to endow the future research in embodied generalist agents.\\n\\n2. Model\\n\\nThe leading design principles of LEO are two-fold: 1) It should handle the multi-modal input of egocentric 2D, global 3D, and textual instruction, and the output of textual response as well as embodied action commands in a unified architecture; 2) It should leverage pre-trained large language models (LLMs) as a powerful prior for the downstream tasks.\\n\\nWe therefore convert all data of different modalities into a sequence of tokens, illustrated below:\\n\\n2D, ..., s_{2D}^{(M)}\\n\\n2D image tokens (optional)\\n\\nobject-centric 3D tokens\\n\\ninstruction s_{1}^{(1)} res, ..., s_{T}^{(T)} res\\n\\nresponse\\n\\nWith this representation, we formulate the learning of LEO as GPT-style autoregressive language modeling (Brown et al., 2020) given the prefix (from system message to instruction), i.e., prefix language modeling (Raffel et al., 2020). Therefore, a pretrained LLM can be used to process such sequences. Next, we will detail the tokenization of multi-modal data, model architecture, training loss, and inference settings. An overview of our model can be found in Fig. 1.\\n\\n2.1. Tokenization\\n\\nWe follow prior practices in 2D VLM (Liu et al., 2023b; Alayrac et al., 2022) and 3D VLM (Zhu et al., 2023c) to tokenize the multi-modal data in LEO. We use Sentence-Piece tokenizer (Kudo & Richardson, 2018) to encode text with 32k subwords; 2D image tokens for egocentric 2D images; and object-centric 3D tokens extracted over Mask3D-based (Schult et al., 2022) object proposals for 3D point cloud inputs. For embodied action commands, continuous actions (e.g., in manipulation) are discretized (details in Appendix D.3) to join the discrete actions (e.g., navigation) and form a unified discrete action space. We follow (Brohan et al., 2023) to map these discrete actions to the least used tokens in SentencePiece. After tokenization, all tokens are ordered into the format in (1).\\n\\n2.2. Token Embedding & LLM\\n\\nWe apply several token embedding functions to process the tokens in the sequence before sending them to the LLM. The LLM will then align these tokens of different modalities, and produce the response. Most of the responses are text and can be decoded directly. For responses that include embodied actions, we will map the reserved SentencePiece text tokens back to action commands.\\n\\nText & 2D token embedding. For text tokens (including embodied actions that have been mapped to the reserved text tokens), an embedding look-up table is used to map them into vectors. While the egocentric 2D image is encoded by a pretrained OpenCLIP ConvNext (Liu et al., 2022) for obtaining image token embeddings. We apply MLP adapters to match the dimensions of all token embeddings.\\n\\nObject-centric 3D token embedding. Each 3D object token (i.e., the point cloud of a 3D object) is first encoded by a pretrained point cloud encoder (e.g., PointNet++ (Qi et al., 2017)). We then adopt the Spatial Transformer introduced in (Chen et al., 2022) to further process the point cloud embedding of all objects into object-centric 3D token embeddings. In a nutshell, Spatial Transformer biases the standard attention score with relative position and size for capturing 3D relations between objects. Due to space limit, the readers are referred to (Chen et al., 2022) and Appendix D.2 for more details.\\n\\nPretrained LLM. We choose Vicuna-7B (Chiang et al., 2023) to process the token sequence. In order to tackle the challenging alignment and grounding problem of multi-modal tokens (2D, 3D, text, embodied action) while preserving the LLM pretrained knowledge, we employ LoRA (Hu et al., 2022) to introduce additional tunable parameters to the frozen pretrained LLM.\\n\\n2.3. Training & Inference\\n\\nWe formulate the learning objective of LEO following (Brown et al., 2020; Raffel et al., 2020) in a prefix language modeling fashion. For a batch $B$ of token sequence $s$, we optimize LEO via:\\n\\n$$L(\\\\theta, B) = -\\\\frac{1}{|B|} \\\\sum_{b=1}^{T} \\\\sum_{t=1}^{X} \\\\log p_{\\\\theta}(s_{b,t}^{\\\\text{res}}|s_{b,<t}^{\\\\text{res}}, s_{b}^{\\\\text{prefix}}),$$\\n\\n(2)\\n\\nwhere $s_{\\\\text{prefix}}$ denotes the prefix tokens (from system message to instruction) in (1). During training, we freeze the pretrained 3D point cloud encoder and the LLM and fine-tune the 2D image encoder, the Spatial Transformer, and the LoRA parameters. In total, LEO has ~7B parameters and ~142M of them will be tuned. During inference, we...\"}"}
{"id": "V4qV08Vk6S", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure A.7: The distribution of node numbers for 3DSSG scenes. The node number represents the number of objects in a scene.\\n\\nTable A.4: Sampling rates for scenes with different node numbers. The hyphen denotes a sweep of sampling rates, e.g., \\\"0.7-0.9\\\" means \\\"0.7, 0.8, 0.9\\\".\\n\\n| Node Number | Sampling Rate |\\n|-------------|---------------|\\n| 10-20       | 0.8, 0.9     |\\n| 20-30       | 0.7-0.9      |\\n| 30-40       | 0.6-0.9      |\\n| 40-50       | 0.6-0.9      |\\n| 50-60       | 0.5-0.9      |\\n| 60-70       | 0.5-0.9      |\\n| >70         | 0.4-0.9      |\\n\\nBox-based Content\\n\\n```\\nwall: [-0.66, 0.853, -0.329],\\nfloor: [0.291, 0.454, -1.533],\\nceiling: [0.3, 0.955, 0.9],\\nwall: [0.997, 0.577, -0.353],\\nlight: [0.213, 0.59, 0.905],\\nwall: [0.971, 3.168, -0.351],\\nwindow: [0.943, 3.385, 0.074],\\nboard: [-0.649, -0.117, 1.183],\\ndesk: [0.696, 2.259, -0.987],\\nbox: [-0.395, 0.64, 1.33],\\nbowl: [0.631, 3.071, -0.803],\\nbox: [0.797, 3.121, -0.91]\\n```\\n\\nScene Graph-based Content\\n\\n```\\n{'wall-1': {'relations': ['attached to floor-2'], 'attribute': {'shape': 'flat', 'lexical': 'architectural', 'color': 'white'}},\\n'floor-2': {'relations': [], 'attribute': {'material': 'plastic', 'shape': 'flat', 'lexical': 'inside', 'color': 'blue'}},\\nceiling-3': {'relations': ['attached to wall-1', 'attached to wall-4', 'attached to wall-7'], 'attribute': {'shape': 'flat', 'lexical': 'overhead', 'color': 'white'}},\\n'wall-4': {'relations': ['attached to floor-2'], 'attribute': {'shape': 'flat', 'lexical': 'architectural', 'color': 'white'}},\\n'light-6': {'relations': ['hanging on ceiling-3'], 'attribute': {'state': 'off'}},\\n'wall-7': {'relations': ['attached to floor-2'], 'attribute': {'shape': 'flat', 'lexical': 'architectural', 'color': 'white'}},\\n>window-8': {'relations': ['attached to wall-7', 'behind desk-10'], 'attribute': {'material': 'glass', 'color': 'dark', 'shape': 'rectangular', 'state': 'closed'}},\\n'board-9': {'relations': ['lying on floor-2', 'to the left of desk-10', 'close by box-11'], 'attribute': {'shape': 'flat', 'lexical': 'flat', 'color': 'brown'}},\\ndesk-10': {'relations': ['standing on floor-2', 'in front of window-8', 'to the right of board-9', 'to the right of box-11', 'close by box-11'], 'attribute': {'other': 'rigid', 'size': 'narrow'}},\\n'box-11': {'relations': ['standing on floor-2', 'close by board-9', 'close by desk-10', 'to the left of desk-10', 'in front of box-15', 'to the left of box-15'],\\n'attribute': {'state': 'written on', 'shape': 'rectangular', 'lexical': 'rectangular', 'other': 'rigid', 'size': 'tall'}},\\nbowl-14': {'relations': []},\\n'box-15': {'relations': ['standing on desk-10', 'to the right of box-11', 'behind box-11'], 'attribute': {'color': 'dark', 'shape': 'rectangular', 'lexical': 'rectangular', 'size': 'small'}}\\n```\\n\\nFigure A.8: Comparison of the content between box-based and scene-graph-based prompting. To verify whether the subgraph sampling strategy can maintain the consistency and diversity of scene captions, we generate scene captions for the same scene using both the full graph and subgraph. We then employ GPT-4 (OpenAI, 2023) to evaluate the similarities and differences between the two captions. The results in Tab. A.5 indicate that our subgraph sampling strategy can maintain both consistency and diversity.\\n\\nB.8. Scene-graph-based Prompting vs Box-based Prompting\\n\\nIn this section, we provide a comparative analysis of scene-graph-based prompting and box-based prompting (Hong et al., 2023). We refer the readers to Figure 6 in 3D-LLM (Hong et al., 2023) for details of the box-based prompting method. Fig. A.8 shows the contents of two methods. To present a fair comparison between the two methods, we prompt with 1) demonstrations that have similar content under the same scene (see Fig. A.9) and 2) identical new scene queries. Since\"}"}
{"id": "V4qV08Vk6S", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table A.5: The comparative analysis of full graph prompting and subgraph prompting.\\n\\nSettings\\n\\nGenerated Scene Captions\\n\\nFull Graph\\n\\nIn this scene, there is a colorful and flat white wall. A small TV stand is positioned in front of a low upholstered couch, to the left of a brown lamp, and to the right of a wooden table. The couch is covered with a pink padded square pillow and a patterned blanket. A light hangs on the wall and another light is supported by the table. There is a rectangular glass window attached to the wall, behind which is a yellow curtain. The room has a comfortable and cozy atmosphere, suitable for relaxation and entertainment.\\n\\nGPT-4 Evaluation\\n\\nSimilarities:\\n1. Both scenes describe a living room setting, emphasizing comfort and coziness suitable for relaxation and entertainment.\\n2. Both rooms have a white wall.\\n3. Each scene features a couch and a TV stand.\\n4. Both scenes describe a blanket and a padded square pillow on the couch.\\n\\nDifferences:\\n1. The first scene has a brown wooden floor, while the floor is not mentioned in the second scene.\\n2. The first scene has a red blanket on the couch; the second has a patterned blanket but doesn\u2019t specify the color.\\n3. The first scene describes the lamps as \u201cstanding on the floor\u201d, while the second mentions one light hanging on the wall and another supported by the table.\\n4. The second scene includes a yellow curtain behind the window, which the first scene does not mention.\\n\\nSummary:\\nOverall, both summaries provide a similar thematic view of a comfortable living room but differ in the specific arrangement and color details of the items within the room.\"}"}
{"id": "V4qV08Vk6S", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nBox-based Prompting\\n\\nResponse:\\n\\nHuman: Can you help me find a towel?\\n\\nRobot: Of course! I found three towels in the room. One is near the bathtub, one is near the sink, and another one is near the stool. Which one would you like to use?\\n\\nHuman: The one near the sink, please.\\n\\nRobot: Alright. I will fetch the towel near the sink for you. Is there anything else I can assist you with?\\n\\nHuman: No, thank you. That's all.\\n\\nRobot: You're welcome! If you need any further assistance, feel free to ask.\\n\\nScene-Graph-based Prompting\\n\\nResponse:\\n\\nDialogue Context:\\n\\nhigh level task: describe the objects in the bathroom.\\n\\nHuman: Can you describe the objects in the bathroom?\\n\\nThought:\\n\\nbathtub - 8\\nsink - 12\\ntoilet - 15\\nmirror - 4\\n\\nRobot: Sure! In the bathroom, there is a bathtub made of ceramic, with a shiny texture and a white color. There is also a sink with an oval shape and a shiny texture. The toilet is white and seat down. There is a mirror attached to the wall, which is polished. Is there anything specific you want to know about these objects?\\n\\nHuman: Is there a towel in the bathroom?\\n\\nThought:\\n\\ntowel - 10\\n\\nRobot: Yes, there is a folded together rectangular towel in the bathroom. It is located to the right of the door.\\n\\nBox-based Prompting\\n\\nResponse:\\n\\nHuman: Can you help me find a towel?\\n\\nRobot: Of course! I found three towels in the room. One is near the bathtub, one is near the sink, and another one is near the stool. Which one would you like to use?\\n\\nHuman: The one near the sink, please.\\n\\nRobot: Alright. I will fetch the towel near the sink for you. Is there anything else I can assist you with?\\n\\nHuman: No, thank you. That's all.\\n\\nRobot: You're welcome! If you need any further assistance, feel free to ask.\\n\\nFigure A.10: The responses of two prompting methods. Descriptions highlighted in red show our method leads to more flexible and reliable spatial relations.\\n\\nshown in Fig. A.10, we highlight some spatial relations in red. The comparison shows that our method provides more diverse and reliable spatial relations, which are important for 3D scene understanding.\\n\\nB.9. Additional Comparison Regarding Dataset Quality\\n\\nIn addition to assessing the factual accuracy of responses compared to 3D-LLM, we also compared the grammatical correctness of the responses with ScanScribe (Zhu et al., 2023c), a template-based synthetic dataset that focuses on 3D object caption. We observed that their dataset exhibited some grammar errors, whereas our dataset did not manifest such issues. We provide some data examples in Tab. A.6 and Tab. A.7. We highlighted the grammar errors present in ScanScribe dataset in red. Through comparison, it is evident that our sentences exhibit accurate and natural syntax, and also surpasses ScanScribe in the diversity and complexity of object descriptions.\\n\\nTable A.6: Object captions in the 3Rscan scene 8f0f144b-55de-28ce-8053-2828b87a0cc9.\\n\\n| object label-id | method     | response id | caption                                                                 |\\n|-----------------|------------|-------------|-------------------------------------------------------------------------|\\n| microwave-8     | ours       | 1           | The microwave is black in color and is standing on the table. It is located behind and to the left of the toaster. |\\n|                 |            | 2           | The microwave is black in color. It is standing on a table and is positioned behind and to the left of a toaster. |\\n| ScanScribe      | 1          |             | This is a black microwave. It is standing on the table. It is close by the toaster. A black microwave is in front of the this microwave. |\\n|                 | 2          |             | This is a black microwave. It is behind the toaster. It is standing on the table. A black microwave is in front of the this microwave. |\\n\\nB.10. Dataset Statistics\\n\\nWe provide statistics on the instruction-tuning datasets. We visualize the distribution of the question types in 3RQA (Fig. A.11) and 3RDialog (Fig. A.12). The pie chart's inner circle represents the first word of the questions, while the outer circle accounts for the second or third word in the corresponding questions. The results show that the questions cover the attributes and spatial relations of the objects, as well as high-level topics such as room types and functionalities.\\n\\nWe also provide statistics of the root noun-verb pairs for instructions and responses in 3RDialog and 3RPlan, as shown in Fig. A.13\u2013A.16.\\n\\nC. Data Examples\\n\\nPlease refer to Tabs. A.25\u2013A.27 for examples of our dataset.\"}"}
{"id": "V4qV08Vk6S", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table A.7: Object captions in the 3Rscan scene 0cac753c-8d6f-2d13-8e27-e0664fc33bb9.\\n\\n| object label-id | method | response id | caption |\\n|-----------------|--------|-------------|---------|\\n| counter-4       | ours   | 1           | The counter is standing on a wooden floor. It is close to 4 stools and an armchair. The stools are simple and wide in size, while the armchair is narrow in size. The floor is wooden and flat, and it is clean. |\\n| counter-4       | ours   | 2           | The counter is standing on the wooden floor. It is close to 4 stools and an armchair. The floor is made of wooden material and is flat in shape. It is also clean. The armchair is narrow in size while the stools are simple in design. Three of the stools are wide in size. |\\n| counter-1       | ScanScribe | 1         | This is a counter. It is close by the armchair. It is close by the stool. |\\n\\n**Figure A.11:** Question types: 3RQA.\\n\\n**Figure A.12:** Question types: 3RDialog.\\n\\n### D. Model Details\\n\\n#### D.1. Prompts\\n\\nThe first portion of prompts sent into the LLM is a **system message**. It consists of two parts: a role prompt and a situation prompt. The role prompt is the same for all tasks:\\n\\n> You are an AI visual assistant situated in a 3D scene. You can perceive (1) an ego-view image (accessible when necessary) and (2) the objects (including yourself) in the scene (always accessible). You should properly respond to the USER\u2019s instructions according to the given visual information.\\n\\nThe situation prompt begins with a common sentence:\\n\\n> You are at a selected location in the 3D scene.\\n\\nFor SQA3D (Ma et al., 2023), the situation prompt is further extended with the situation description in the dataset. The situation prompt is only used jointly with the embodiment token to support tasks that require information about the embodiment. Details can be found in Appendix D.2.1.\\n\\nNext are the **visual tokens**, including **2D image tokens** and **object-centric 3D tokens**. Each token sequence is interleaved...\"}"}
{"id": "V4qV08Vk6S", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nTable A.11: Hyperparameters for LEO inference.\\n\\n| Hyperparameter       | Value       |\\n|----------------------|-------------|\\n| Number of beams      | 5           |\\n| Maximum output length| 256         |\\n| Minimum output length| 1           |\\n| Top \\\\(p\\\\)            | 0.9         |\\n| Repetition penalty   | 3.0         |\\n| Length penalty       | 1.0         |\\n| Temperature          | 1.0         |\\n\\nTable A.12: Hyperparameters for the alignment stage.\\n\\n| Hyperparameter       | Value       |\\n|----------------------|-------------|\\n| Optimizer            | AdamW       |\\n| Weight decay         | 0.05        |\\n| Betas                | [0.9, 0.999]|\\n| Learning rate        | \\\\(3 \\\\times 10^{-4}\\\\) |\\n| Warmup steps         | 400         |\\n| Number of workers    | 4           |\\n| Parallel strategy    | DDP         |\\n| Type of GPUs         | NVIDIA A100 |\\n| Number of GPUs       | 4           |\\n| Accumulate gradient batches | 5        |\\n| Batch size per GPU (total) | 4 (80)   |\\n| Training precision   | bfloat16    |\\n| Gradient norm        | 5.0         |\\n| Epochs               | 5           |\\n\\nTable A.13: Hyperparameters for the instruction-tuning stage.\\n\\n| Hyperparameter       | Value       |\\n|----------------------|-------------|\\n| Optimizer            | AdamW       |\\n| Weight decay         | 0.05        |\\n| Betas                | [0.9, 0.999]|\\n| Learning rate        | \\\\(3 \\\\times 10^{-5}\\\\) |\\n| Warmup steps         | 400         |\\n| Number of workers    | 4           |\\n| Parallel strategy    | DDP         |\\n| Type of GPUs         | NVIDIA A100 |\\n| Number of GPUs       | 4           |\\n| Accumulate gradient batches | 5        |\\n| Batch size per GPU (total) | 4 (80)   |\\n| Training precision   | bfloat16    |\\n| Gradient norm        | 5.0         |\\n| Epochs               | 10          |\\n\\nG. Ablation Details\\n\\nG.1. Object-centric Mask\\n\\nGround truth vs. object proposals. As we adopt an object-centric 3D representation, the object-centric masks are necessary to segment the scene point cloud. For scenes that lack annotations of object-centric masks, we can utilize off-the-shelf detection or segmentation models to generate object proposals and thus obtain the masks. We compare the performances of LEO (w/o Act) between using ground-truth masks and Mask3D (Schult et al., 2022) proposals. The results in Tab. A.14 indicate that using Mask3D proposals leads to a moderate performance drop on Scan2Cap (mainly due to the IoU@0.5 metrics) and comparable performances on QA tasks.\"}"}
{"id": "V4qV08Vk6S", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nTable A.14: Quantitative comparison between LEO (w/o Act) using ground-truth masks and Mask3D proposals. Metrics follow Tab. 4.\\n\\n| Scan2Cap (val) | ScanQA (val) | SQA3D (test) |\\n|---------------|-------------|--------------|\\n|               | C | B-4 | M | R | Sim | C | B-4 | M | R | EM@1 | EM@1 |\\n| w/o Act (Mask3D) | 72.4 | 38.2 | 27.9 | 58.1 | 55.3 | 101.4 | 13.2 | 20.0 | 49.2 | 24.5 |\\n| w/o Act (GT) | 87.4 | 44.5 | 30.8 | 65.7 | 65.4 | 103.0 | 14.6 | 20.1 | 49.7 | 24.3 |\\n\\nTable A.15: Quantitative results of LEO equipped with LLMs at different scales. Metrics follow Tab. 7.\\n\\n| ScanNet 3RScan | Scan2Cap | ScanQA | SQA3D | 3RQA | 3RDialog | 3RPlan |\\n|---------------|----------|--------|-------|------|----------|--------|\\n| w/o Act (OPT-1.3B) | 64.6 | 20.3 | (44.2) | 45.5 | (47.6) | 50.0 | (54.5) | 71.1 | 78.3 |\\n| w/o Act (Vicuna-7B) | 65.4 | 24.3 | (48.5) | 50.0 | (52.5) | 51.9 | (57.4) | 73.3 | 81.1 |\\n| w/o Act (Vicuna-13B) | 65.2 | 23.4 | (48.9) | 49.7 | (52.3) | 56.2 | (60.4) | 72.5 | 80.5 |\\n\\nG.2. Model Ablation\\n\\nFollowing the setting of LEO (w/o Act), we ablate the default LLM (Vicuna-7B) with OPT-1.3B (Zhang et al., 2022) and Vicuna-13B (Chiang et al., 2023), respectively. We report the evaluation results on ScanNet and 3RScan tasks in Tab. A.15. The results show a significant gap between OPT-1.3B and Vicuna-7B and comparable performances between Vicuna-7B and Vicuna-13B. This indicates the notable improvements when scaling from smaller LLM to 7B scale and the potential saturation if we continue to scale up, resembling the finding in Sec. 4.5.\\n\\nG.3. Dialogue and Planning Data\\n\\nTo evaluate w/o Dialg, we design an evaluation set with three types of questions: 1) Answerable: general questions that can be answered based on the given 3D scenes; 2) Unanswerable: questions that cannot be answered given the 3D scenes due to a lack of information, e.g., \u201cTell me about the elephant in the room\u201d; 3) NLP: questions that solely examine the language functionality of LEO in term of factual knowledge, reasoning, and text coherence. We collect 30 representative questions for each subset and generate LEO\u2019s responses for each question. We then ask humans to choose their preferred responses between w/o Dialg and w/ Dialg. Based on the human preferences, we evaluate the two models with TrueSkill (Graepel et al., 2007), which is an algorithm that quantifies players\u2019 rating scores by Bayesian inference. The scores are estimated by Gaussian distribution and expressed as $\\\\mu \\\\pm \\\\sigma$.\\n\\nG.4. Data Balancing\\n\\nTo investigate the hallucination problem, we collect 150 questions querying object existence on 3RScan and ScanNet respectively. We split three subsets according to the category of queried object. The queried object can exist in the given scene (Yes), exist in other scenes instead of the given scene (No-1), or not exist in all the scenes (No-2). Each subset comprises 50 questions. We merge No-1 and No-2 when reporting the exact-match accuracy, as shown in Tab. 9.\\n\\nH. Evaluation Details\\n\\nH.1. 3D Question Answering\\n\\nRationality of QA evaluation protocol. We argue that exact match (EM), as a conventional metric for 3D QA, is unsuitable for evaluating the open-ended answer generated by LLMs. For example, given the question \u201cOn what side of the towel is a bathroom curtain?\u201d with ground-truth answer \u201cleft side of towel\u201d, it is never wrong to answer \u201cleft\u201d. However, this will be deemed incorrect if we adopt the strict exact match protocol. Such a misjudgment is quite likely to occur when evaluating the answers from LLMs. By contrast, the classifier heads for QA (e.g., MCAN) are less affected because they collect all possible answers in advance to formulate the QA as a close-set classification problem. Hence, we refine the strict\"}"}
{"id": "V4qV08Vk6S", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table A.16: Examples from ScanQA validation set manifest the rationality of our refined exact match protocol.\\n\\n| Question                                      | Ground-truth answer | Predicted answer | Strict EM | Refined EM |\\n|-----------------------------------------------|---------------------|------------------|-----------|------------|\\n| What color is the chair in the kitchen?       | dark brown          | brown            | \u2713         | \u2713          |\\n| What is under the long kitchen counter?       | kitchen cabinets    | brown rectangular kitchen cabinets | \u2713         | \u2713          |\\n| What type of refrigerator is on the right of a kitchen counter? | stainless steel refrigerator | stainless steel | \u2713         | \u2713          |\\n| Where is the beige wooden desk placed?        | up against wall     | against wall     | \u2713         | \u2713          |\\n| What color does the sofa look?                | it looks black      | black            | \u2713         | \u2713          |\\n| Where is the black office chair located?      | in front of desks   | in front of desk | \u2713         | \u2713          |\\n| What is in the corner by windows?             | book shelf          | bookshelf        | \u2713         | \u2713          |\\n| Where is the chair pulled into?               | under table         | under table      | \u2713         | \u2713          |\\n| How many chairs are to the left of the table? | 4                   | 4 chairs         | \u2713         | \u2713          |\\n| What objects are sitting on the black couch?  | pillow              | pillows          | \u2713         | \u2713          |\\n| Where are the two different size tables located in room? | in center | in center of room | \u2713         | \u2713          |\\n| Where is the laptop located?                  | on desk             | on desk          | \u2713         | \u2713          |\\n| Where is the soap dispenser mounted above sink on wall? | above sink | above sink | \u2713         | \u2713          |\\n\\nIn a nutshell, we squeeze the `pred` and `gt`, and then check whether one is a subset of the other. To justify our refined exact match protocol, in Tab. A.16 we provide some representative examples in the ScanQA validation set. Despite the improvements, we speculate such a simple refinement is still insufficient for a sound evaluation metric considering the flexibility of human language.\\n\\nH.2. Embodied Navigation\\n\\nTo construct our training set, we adopt all 57 scenes in the MP3D ObjNav training split (Savva et al., 2019; Ramrakhya et al., 2022) and generate ~60K shortest-path navigation episodes. The evaluation is conducted on the original validation split of the MP3D ObjNav task and the newly introduced HM3D ObjNav task (Ramakrishnan et al., 2021).\\n\\nIn contrast to most ObjNav agents that utilize recurrence through either RNN (Ramrakhya et al., 2022) or DT-style Transformer (Suglia et al., 2021), LEO only employs a simplistic feed-forward policy, i.e., the Transformer in LEO only takes in the instruction, current state (2D and 3D observation), and past 4 actions, and predicts the next action, similar to RT-2 (Brohan et al., 2023). Therefore, the only information relayed from the past is past actions. The absence of recurrence in LEO\u2019s acting policy is indeed the result of a trade-off between better performances and training efficiency. We will commit to exploring the possibility of looping in more sophisticated policy architectures (e.g., recurrence) in future work.\"}"}
{"id": "V4qV08Vk6S", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Additional Results\\n\\n### I. Impact of Data Refinement\\n\\nWe investigate the impact of data refinement by comparing the downstream performances between pretraining on the generated data before/after refinement. Specifically, since our generated data (where the refinement occurs) pertains to 3RScan scenes, we first pretrain the LEO after the alignment stage on a mix of 3RScan datasets, and then train on a mix of ScanNet datasets (Scan2Cap, ScanQA, and SQA3D), where we report the quantitative results as downstream performances.\\n\\nThe results in Tab. A.17 demonstrate that data refinement elicits consistent improvements. In particular, data refinement primarily benefits reasoning (QA) tasks, probably because the refinement operation mainly concerns QA and dialogue data.\\n\\n### I.2. Data Comparison\\n\\nWe collect the training data of LL3DA (Chen et al., 2024) to train LEO and compare the quantitative results with LEO trained with our original data to showcase the impact of training data. We report the performances on Scan2Cap and ScanQA, where their data overlaps ours.\\n\\nThe results in Tab. A.18 exhibit a consistent performance gap between training on LL3DA data and our original data, underscoring the advantage of our collected training data.\\n\\n### I.3. Model Comparison\\n\\nLEO adopts an object-centric 3D representation to encode 3D scenes, which is a novel approach compared with recent works. For example, 3D-LLM (Hong et al., 2023) leverages 2D foundation models to obtain dense semantic features and lift them to 3D space, and LL3DA (Chen et al., 2024) adopts scene-level encoding. They both use learnable queries to extract 3D features. Here we investigate the influence of model design with the same training data. For a fair comparison, we use Mask3D (Schult et al., 2022) object proposals instead of ground-truth masks for the evaluation results of LEO.\\n\\n#### LL3DA vs. LEO\\n\\nWe train LEO on the LL3DA training data and compare the performances with LL3DA generalist results (without task-specific fine-tuning). From the results in Tab. A.19, we highlight two takeaways: 1) with the same training data, LEO outperforms LL3DA on most metrics; 2) the gap between LL3DA and LEO is significant on ScanQA, which indicates a major advantage of object-centric 3D representation lies in handling the reasoning task.\\n\\n#### 3D-LLM vs. LEO\\n\\nAs LL3DA collects a subset (ScanNet part) of 3D-LLM training data, we leverage this subset to pretrain LEO and compare the downstream performances with 3D-LLM. In contrast to the task-specific fine-tuning results of 3D-LLM, we report LEO\u2019s evaluation results after instruction tuning without task-specific fine-tuning. The results in...\"}"}
{"id": "V4qV08Vk6S", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nPeng, S., Genova, K., Jiang, C., Tagliassacchi, A., Pollefeys, M., Funkhouser, T., et al. Openscene: 3D scene understanding with open vocabularies. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023b.\\n\\nQi, C. R., Yi, L., Su, H., and Guibas, L. J. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in Neural Information Processing Systems (NeurIPS), 2017.\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 2020.\\n\\nRamakrishnan, S. K., Gokaslan, A., Wijmans, E., Maksymets, O., Clegg, A., Turner, J., Undersander, E., Galuba, W., Westbury, A., Chang, A. X., et al. Habitat-matterport 3D dataset (hm3D): 1000 large-scale 3D environments for embodied AI. arXiv preprint arXiv:2109.08238, 2021.\\n\\nRamrakhya, R., Undersander, E., Batra, D., and Das, A. Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al. A generalist agent. Transactions on Machine Learning Research (TMLR), 2022.\\n\\nReimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. In Annual Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.\\n\\nSanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey, M., et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations (ICLR), 2022.\\n\\nSavva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub, J., Liu, J., Koltun, V., Malik, J., et al. Habitat: A platform for embodied AI research. In International Conference on Computer Vision (ICCV), 2019.\\n\\nSchmidhuber, J. One big net for everything. arXiv preprint arXiv:1802.08864, 2018.\\n\\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\nSchult, J., Engelmann, F., Hermans, A., Litany, O., Tang, S., and Leibe, B. Mask3D for 3D semantic instance segmentation. arXiv preprint arXiv:2210.03105, 2022.\\n\\nShridhar, M., Manuelli, L., and Fox, D. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning (CoRL), 2021.\\n\\nSuglia, A., Gao, Q., Thomason, J., Thattai, G., and Sukhatme, G. Embodied bert: A transformer model for embodied, language-guided visual task completion. arXiv preprint arXiv:2108.04927, 2021.\\n\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\\n\\nTsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S., Vinyals, O., and Hill, F. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 2017.\\n\\nWald, J., Avetisyan, A., Navab, N., Tombari, F., and Nie\u00dfner, M. Rio: 3D object instance re-localization in changing indoor environments. In International Conference on Computer Vision (ICCV), 2019.\\n\\nWang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.\\n\\nWang, X., Wang, W., Cao, Y., Shen, C., and Huang, T. Images speak in images: A generalist painter for in-context visual learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023b.\\n\\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions. In NeurIPS, 2023.\"}"}
{"id": "V4qV08Vk6S", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.\\n\\nWang, Z., Huang, H., Zhao, Y., Zhang, Z., and Zhao, Z. Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes. arXiv preprint arXiv:2308.08769, 2023.\\n\\nWei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In International Conference on Learning Representations (ICLR), 2022.\\n\\nWu, S.-C., Wald, J., Tateno, K., Navab, N., and Tombari, F. Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nXu, R., Wang, X., Wang, T., Chen, Y., Pang, J., and Lin, D. Pointllm: Empowering large language models to understand point clouds. arXiv preprint arXiv:2308.16911, 2023.\\n\\nYe, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\\n\\nYin, Z., Wang, J., Cao, J., Shi, Z., Liu, D., Li, M., Sheng, L., Bai, L., Huang, X., Wang, Z., et al. Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. arXiv preprint arXiv:2306.06687, 2023.\\n\\nYu, X., Tang, L., Rao, Y., Huang, T., Zhou, J., and Lu, J. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\\n\\nZhao, H., Cai, Z., Si, S., Ma, X., An, K., Chen, L., Liu, Z., Wang, S., Han, W., and Chang, B. Mmicl: Empowering vision-language model with multi-modal in-context learning. arXiv preprint arXiv:2309.07915, 2023.\\n\\nZhao, L., Cai, D., Sheng, L., and Xu, D. 3dvg-transformer: Relation modeling for visual grounding on point clouds. In International Conference on Computer Vision (ICCV), 2021.\\n\\nZhu, D., Chen, J., Haydarov, K., Shen, X., Zhang, W., and Elhoseiny, M. Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. arXiv preprint arXiv:2303.06594, 2023.\\n\\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\\n\\nZhu, Y., Gao, T., Fan, L., Huang, S., Edmonds, M., Liu, H., Gao, F., Zhang, C., Qi, S., Wu, Y., N., et al. Dark, beyond deep: A paradigm shift to cognitive ai with humanlike common sense. Engineering, 2020.\\n\\nZhu, Z., Ma, X., Chen, Y., Deng, Z., Huang, S., and Li, Q. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In International Conference on Computer Vision (ICCV), 2023.\"}"}
{"id": "V4qV08Vk6S", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nA. Qualitative Results\\n\\nTo right of toilet\\nWhere is the bathtub located in the bathroom?\\nThere is a rectangular brown table. It is in the center of the room.\\n\\nDescribe object.\\n\\nClosed\\nIs the door closed or open to my left side?\\n\\nOf course! I'd be happy to help. What do you need advice on?\\nI'm redecorating my bedroom and I need some advice. Can you help me?\\n\\n1. Wipe down the door, including the handle and any glass panels.\\n2. Vacuum or sweep the floor to remove any dust, dirt, or debris.\\n3. Dust and clean the cabinet, including any shelves or drawers.\\n4. Arrange the pillows on the couch neatly and fluff them up.\\n5. Dust and clean the ceiling using a long-handled duster or appropriate cleaning tool.\\n6. Check and adjust the temperature or settings of the radiator if necessary.\\n\\nNeatly organize and remove dirt from the living room.\\nI want to add some color to the room. Are there any colorful objects in the room?\\nYes, there is a colorful picture hanging on the wall. It is in front of a shelf and behind an armchair. It adds a nice pop of color to the room.\\nThat sounds great! What other decorations or furniture are in the room?\\nThere is a wardrobe to the right of a stuffed animal. It is in front of a doorframe.\\nAdditionally, there are shelves attached to the wall, with various items displayed on them.\\n\\nThank you for the information! It's really helpful for my redecorating plans.\\nYou're welcome! I'm glad I could assist you. If you have any questions, feel free to ask.\\n\\nPack the black shoe with green stripes in the brown box.\\n\\n(0.29, 0.07, 0.06), (0., 0., 0., 1.0), (0.53, -0.18, 0.01), (0., 0., 0.38, 0.92).\\n\\nPlease navigate to the toilet in the scene.\\nForward\\nLeft\\nStop\\n\u2026\"To right of toilet\\nWhere is the bathtub located in the bathroom?\\nThere is a rectangular brown table. It is in the center of the room.\"}"}
{"id": "V4qV08Vk6S", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nexpressions on 3RScan (Wald et al., 2019) scenes by prompting LLMs (details in Appendix B.4). During alignment, LEO needs to predict these referring expressions given the object-centric 3D input of the scene and the referred object.\\n\\nScene-level caption.\\n\\nFinally, we encourage LEO to capture scene-level descriptions of a 3D scene. These scene-level captions focus on global information depicting key objects in the scene as well as their attributes and functionalities, relations among multiple objects, and room types and styles. We leverage scene graph annotations (Wald et al., 2019) and prompt LLMs to produce a total of ~20K captions. To further increase caption diversity, we propose a subgraph sampling strategy to prevent LLMs from always attending to certain notable facets of the scene (details in Appendix B.7). Similar to previous settings, LEO needs to predict these captions given the corresponding 3D input.\\n\\nB.2. More Details on LEO-instruct\\n\\nBelow, we provide a comprehensive illustration of the data preparation process for these tasks and an overview of generated data in Fig. 2. We list the corresponding instructions in Appendix C.\\n\\n3D captioning.\\nThe task is to produce a generic caption given 3D input. We adopt the Scan2Cap dataset (Chen et al., 2021), which is based on the ScanNet (Dai et al., 2017) 3D scenes and covers various levels (object-level and scene-level) and aspects (attributes, relations, etc.) of scene details.\\n\\n3D question answering.\\nThe 3D-QA task is an extension of VQA (Antol et al., 2015) to 3D scenes with a focus on 3D knowledge, ranging from spatial relations to functionalities of objects. For this task, we first aggregate two existing 3D-QA datasets: ScanQA (Azuma et al., 2022) and SQA3D (Ma et al., 2023). To further generate questions concerning rich 3D knowledge, we prompt LLMs to generate ~35K QA pairs on 3RScanQA with our quality refinement techniques discussed in Sec. 3.3.\\n\\n3D dialogue.\\nThe goal of this task is to support natural conversations between LEO and users about a given 3D scene. This task necessitates coherence and continuity across multiple rounds of conversational interactions. We build such dialogues on 3RScan scenes by prompting LLMs with a variant of the Chain-of-Thought prompting method discussed in Sec. 3.3 to facilitate diverse dialogues about relevant and accurate details about the 3D scene. In total, ~11K dialogues are collected.\\n\\nScene-aware task planning.\\nIn this task, LEO is required to decompose high-level tasks into step-by-step low-level plans given 3D scenes. We expect LEO to generate feasible plans based on the current 3D scene and ground its inherent common sense knowledge about procedures to the scene configurations, including, objects, their attributes, relations, and functional characteristics, etc. By prompting LLMs, we end up collecting ~14K task-plan pairs on 3RScan scenes.\\n\\nEmbodied navigation.\\nWe follow imitation learning setting in Habitat-web (Ramrakhya et al., 2022) for the embodied navigation task. We choose ObjNav, where LEO needs to map navigation instructions (e.g., \u201cfind bed\u201d), object-centric 3D input, and an egocentric 2D input into discrete habitat motor commands. For simplicity, we use shortest path navigation trials rather than human demonstrations for learning as they are less noisy and therefore easier to learn when provided with the 3D scene. In total, we generate ~60K navigation episodes out of the MP3D ObjNav training scenes (Savva et al., 2019) for this task.\\n\\nRobotic manipulation.\\nWe employ a subset of the manipulation tasks introduced in CLIPort (Shridhar et al., 2021). The input of this task includes instructions, egocentric 2D observations, and object-centric 3D information. As discussed in Sec. 2.1, we discretize the continuous action space of CLIPort into bins to unify the action decoding of navigation and manipulation (more details in Appendix D.3). We generate 100K demonstrations for each selected manipulation task.\\n\\nB.3. Design of Seed Tasks for LLM-assisted 3D Data Generation\\n\\nObject Scene Caption & Scene Caption.\\nTo align the 3D scene/object with language, we prompt ChatGPT to curate these two types of caption data. Object Scene Caption includes the spatial relationships of the object with some adjacent objects in the scene. Scene Caption is the comprehensive description for the whole 3D scene, including some key objects and their spatial relationships.\\n\\nQA & Dialogue.\\nFor QA, we design several question-answer pairs given a scene graph. A diverse set of questions are asked about the 3D scene, including the object attributes, object counting, object existence, spatial relationships between the objects, object types, object affordance, room type and so on. For dialogue, we design a conversation between the assistant and a person asking questions about this scene. The answers are in a tone as if the assistant is understanding the scene and helping the person. Different from single-round QA, dialogue has some high-level tasks such as \u2018searching for specific...\u201d}"}
{"id": "V4qV08Vk6S", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are an AI visual assistant in a 3D scene. The scene contains some objects, which compose a scene graph in JSON format. Each entity in the scene graph denotes an object instance, with a class label and an object id. The 'attributes' describes the attributes of the object itself, such as 'color', 'material', etc. The 'relations' describes the spatial relations with other objects. For example, from the scene graph:\\n\\n```\\n{\\n    'sofa-1': {\\n        'attributes': {'color': 'red'},\\n        'relations': ['to the right of chair-2', 'in front of table-3']\\n    },\\n    'chair-2': {\\n        'attributes': {'color': 'brown'},\\n        'relations': ['to the left of sofa-1']\\n    },\\n    'table-3': {\\n        'attributes': {'material': 'wood'},\\n        'relations': []\\n    }\\n}\\n```\\n\\nwe can know that:\\n1) the sofa is red,\\n2) the chair is brown,\\n3) the football table is made of wood,\\n4) the chair is on the left of the sofa,\\n5) the chair is in front of the table.\\n\\nAll spatial positional relationships must be directly derivable from the 'relations', and any spatial relationship between objects with uncertainty cannot appear in the answer.\\n\\nYou need to generate meaningful conversations based on the scene information.\\n\\nThe conversations include questions from human and responses from an AI assistant. Ask questions about the object types, counting the objects, object attributes, relative positions between objects. Also ask questions concerning commonsense, e.g., how the objects can be used by human and human activity in the scene. You can ask questions about the affordance of the objects in the scene. The questions should conform to the given scene information. The attributes of objects and spatial relations between objects can only be inferred from the 'attributes' and 'relations' in scene graph, respectively. The questions should contain interrogative sentences and declarative sentences to cover diverse tones. You need to first provide the context of the dialogue. The context can be high level or low level tasks. The dialogue should be related to the context. Then you need to provide the clues about the question.\\n\\n**Dialogue**\\n\\n**Context:**\\n\\n**Human:**\\n\\n**Thought:**\\n\\n**Robot:**\\n\\nDo not use IDs of the objects ('<object>-<ID>') in <Question> and <Answer>. The IDs of the objects can appear in the <Thought>.\\n\\nFor sample in fewshot_samples:\\n\\n```python\\nmessages = [{\\n    'role': 'system',\\n    'content': 'You are an AI visual assistant in a 3D scene. The scene contains some objects, which compose a scene graph in JSON format. Each entity in the scene graph denotes an object instance, with a class label and an object id. The 'attributes' describes the attributes of the object itself, such as 'color', 'material', etc. The 'relations' describes the spatial relations with other objects. For example, from the scene graph:\\n\\n```\\n{\\n    'sofa-1': {\\n        'attributes': {'color': 'red'},\\n        'relations': ['to the right of chair-2', 'in front of table-3']\\n    },\\n    'chair-2': {\\n        'attributes': {'color': 'brown'},\\n        'relations': ['to the left of sofa-1']\\n    },\\n    'table-3': {\\n        'attributes': {'material': 'wood'},\\n        'relations': []\\n    }\\n}\\n```\\n\\nwe can know that:\\n1) the sofa is red,\\n2) the chair is brown,\\n3) the football table is made of wood,\\n4) the chair is on the left of the sofa,\\n5) the chair is in front of the table.\\n\\nAll spatial positional relationships must be directly derivable from the 'relations', and any spatial relationship between objects with uncertainty cannot appear in the answer.\\n\\nYou need to generate meaningful conversations based on the scene information.\\n\\nThe conversations include questions from human and responses from an AI assistant. Ask questions about the object types, counting the objects, object attributes, relative positions between objects. Also ask questions concerning commonsense, e.g., how the objects can be used by human and human activity in the scene. You can ask questions about the affordance of the objects in the scene. The questions should conform to the given scene information. The attributes of objects and spatial relations between objects can only be inferred from the 'attributes' and 'relations' in scene graph, respectively. The questions should contain interrogative sentences and declarative sentences to cover diverse tones. You need to first provide the context of the dialogue. The context can be high level or low level tasks. The dialogue should be related to the context. Then you need to provide the clues about the question.\\n\\n**Dialogue**\\n\\n**Context:**\\n\\n**Human:**\\n\\n**Thought:**\\n\\n**Robot:**\\n\\n---\\n\\n**Fig. A.2:** The prompt for generating 3D Dialogue.\\n\\n**Table A.1:** The effect of O-CoT on the answer accuracy for Object Counting questions.\\n\\n| Settings | Seed 1     | Seed 2     | Seed 3     | Seed 4     | Average | Avg. Gain |\\n|----------|------------|------------|------------|------------|---------|-----------|\\n| w/o O-CoT| 0.5838     | 0.5349     | 0.5962     | 0.5816     | 0.5741  | 0.2061    |\\n| O-CoT    | 0.7647     | 0.8117     | 0.7778     | 0.7667     | 0.7802  | 0.5515    |\\n\\nPlanning.\\n\\nTo include a deeper understanding of the global 3D scene information, we prompt ChatGPT to generate a high-level task and 5-10 action steps (interaction between the assistant and the objects in the scene) to finish the task.\\n\\n**B.4. Prompts for LLM-assisted 3D Data Generation**\\n\\nIn Fig. A.2\u2013A.6, we show the prompts for five types of LLM-assisted 3D-language data generation. We provide few-shot examples as the context. In each example, the \\\"content\\\" contains a scene graph, and the \\\"response\\\" refers to a human-labeled response. The query is a new scene graph, based on which ChatGPT (OpenAI, 2022) generates responses.\\n\\n**Fig. A.2** shows the prompt for generating 3D dialogue data. Red fonts outline our requirements of the dialogue content, including object attributes, spatial relations, and commonsense topics. Purple fonts formulate the template of the response.\\n\\nWe require the response generated by the ChatGPT should include the dialogue context as well; the \\\"thought\\\" contains the involved objects in the question, which is used to enhance the reliability of the answer. These two components will be removed after the refinement procedures.\"}"}
{"id": "V4qV08Vk6S", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are an AI visual assistant in a 3D scene. The scene contains some objects, which compose a scene graph in json format. Each entity in the scene graph denotes an object instance, with a class label and an object id. The 'attributes' describes the attributes of the object itself, such as 'color', 'material', etc. The 'relations' describes the spatial relations with other objects. For example, from the scene graph:\\n\\n```\\n{'sofa-1': {'attributes': {'color': 'red'}, 'relations': ['to the right of chair-2', 'in front of table-3']}, 'chair-2': {'attributes': {'color': 'brown'}, 'relations': ['to the left of sofa-1']}, 'table-3': {'attributes': {'material': 'wood'}, 'relations': []}}\\n```\\n\\nWe can know that 1) the sofa is red, 2) the chair is brown, 3) the football table is made of wood, 4) the chair is to the left of the sofa, 5) the chair is in front of the table.\\n\\nAll spatial positional relationships must be directly derivable from the 'relations', and any spatial relationship between objects with uncertainty cannot appear in the answer. Do not use the id of the object in the dialogue, use ordinal words and attributes to refer to different objects with the same label.\\n\\nUsing the provided scene graph, design a high-level task that can be performed in this 3D scene. Besides, decomposing this high-level task into a sequence of action steps that can be performed using the instances in this 3D scene. Remember, the high-level task and action steps must be able to be performed in the 3D scene using the given object instances. Do not use IDs of the objects (<object>-<ID> or <object> <ID>) in the planning.\"}"}
{"id": "V4qV08Vk6S", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nYou are an AI visual assistant in a 3D scene. The scene contains some objects, which compose a scene graph in json format. Each entity in the scene graph denotes an object instance, with a class label and an object id. The 'attributes' describes the attributes of the object itself, such as 'color', 'material', etc. The 'relations' describes the spatial relations with other objects. For example, from the scene graph:\\n\\n```\\n{\\n    'sofa-1': {'attributes': {'color': 'red'}, 'relations': ['to the right of chair-2', 'in front of table-3']},\\n    'chair-2': {'attributes': {'color': 'brown'}, 'relations': ['to the left of sofa-1']},\\n    'table-3': {'attributes': {'material': 'wood'}, 'relations': []}\\n}\\n```\\n\\nWe can know that 1) the sofa is red, 2) the chair is brown, 3) the football table is made of wood, 4) the chair is on the left of the sofa, 5) the chair is in front of the table.\\n\\nAll spatial positional relationships must be directly derivable from the 'relations', and any spatial relationship between objects with uncertainty cannot appear in the answer. Don't use IDs of the objects ('<object label> - <ID>') in the summary.\\n\\nYou need to provide a summary for a scene. The summary should be about the object types, object attributes, relative positions between objects. Also describe the scene concerning commonsense, e.g., how the objects can be used by human and human activity in the scene. The description should conform to the given scene information. The attributes of objects and spatial relations between objects can only be inferred from the 'attributes' and 'relations' in scene graph, respectively. You don't need to describe each object in the scene, pick some objects of the scene for summary. You can also summarize the room's function, style, and comfort level based on the arrangement and color of objects within the room. Your summary must not exceed 110 words.\\n\\n```\\nfor sample in few_shot_samples:\\n    messages.append({'role': 'user', 'content': sample['content']})\\n    messages.append({'role': 'assistant', 'content': sample['response']})\\n    messages.append({'role': 'user', 'content': '\\n'.join(sample['query'])})\\n```\\n\\nFigure A.5: The prompt for generating 3D scene caption.\\n\\nYou are a helpful assistant. You will receive a dictionary of an object. This dictionary provides information about a node in a scene graph, as well as its adjacent nodes. The value of the key 'object' is the object represented by the node. The value of 'relations' includes the spatial relationships with the adjacent nodes. The value of the key 'attribute' provides the attributes of the object. The value of 'edge attribute' provides a list of object attributes for the adjacent nodes. You need to describe the object according to the information of the target object node. The IDs of objects cannot appear in the summary.\\n\\n```\\nfor sample in few_shot_samples:\\n    messages.append({'role': 'user', 'content': sample['content']})\\n    messages.append({'role': 'assistant', 'content': sample['response']})\\n    messages.append({'role': 'user', 'content': '\\n'.join(sample['query'])})\\n```\\n\\nFigure A.6: The prompt for generating 3D object-in-the-scene caption.\\n\\nassess the answer accuracy for Object Counting questions. The results are presented in Tab. A.1. The results in Tab. A.1 indicate that O-CoT consistently improves the answer accuracy for Object Counting questions. Though there remain errors after applying O-CoT, we will conduct refinement to fix them. Examples of Object Counting questions are provided in Appendix B.6.\\n\\nB.6. Refinement Details\\n\\nWe conduct refinement by passing raw LLM-generated responses into several human-defined filtering procedures based on the 3D scene graph. The refinement considers five raw response categories:\\n\\n\u2022 Object Counting. The question concerns counting the target object.\\n\u2022 Object Existence. The response claims the existence of objects, which can be actually either existent or non-existent.\\n\u2022 Object Non-existence. The response claims the non-existence of objects, which can be actually either existent or non-existent.\\n\u2022 Negative Response. The scene graph cannot provide a solid response to the question, which means the question cannot be answered and will be discarded.\"}"}
{"id": "V4qV08Vk6S", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nTable A.2: Examples of dialogue refinement.\\n\\n| Types               | Raw Responses                                                                 | Refined Responses                                                                 |\\n|---------------------|-------------------------------------------------------------------------------|----------------------------------------------------------------------------------|\\n| Object Counting     | There are 3 chairs in the room.                                               | I see there are two washing machines in the bathroom.                             |\\n|                     | There are 4 chairs in the room.                                               | I see there are 4 washing machines in the bathroom.                               |\\n| Object Existence    | Yes, there is a cutting board in the kitchen.                                 | Yes, there is a computer and a monitor on the desk.                                |\\n|                     | Yes, there is a computer and a monitor on the desk. However, the monitor is   | No, there is no cutting board in the room.                                        |\\n|                     | currently off.                                                                | No, there is no computer in the room.                                             |\\n| Object Non-existence| No, there is no stereo equipment in the room.                                 | I'm sorry, but I couldn't find a hair dryer in the bathroom.                       |\\n|                     | I'm sorry, but I couldn't find a hair dryer in the bathroom.                  | No, there is a stereo equipment in the room.                                      |\\n|                     | I'm sorry, but I couldn't find a hair dryer in the bathroom.                  | Yes, I found a hair dryer in the room.                                             |\\n| Negative Response   | No, there is nothing else mentioned in the scene graph.                       | I'm sorry, but there is no mention of a mirror in the scene graph for the bathroom.|\\n|                     | I'm sorry, but there is no mention of a mirror in the scene graph for the    | No, there is nothing else mentioned in the scene graph.                           |\\n|                     | bathroom.                                                                    |                                                                                 |\\n| Response with ID    | You can place your backpack on the floor, to the left of the dining table-33. | You can place your backpack on the floor, to the left of the dining table.         |\\n|                     | As for your bag, you can place it on the floor, to the left of the bed-10.     | As for your bag, you can place it on the floor, to the left of the bed.            |\\n\\nTable A.3: Examples of QA refinement.\\n\\n| Types               | Raw Responses                                                                 | Refined Responses                                                                 |\\n|---------------------|-------------------------------------------------------------------------------|----------------------------------------------------------------------------------|\\n| Object Counting     | Q: How many chairs are in the room? A: 3                                    | Q: How many chairs are in the room? A: 4                                         |\\n| Object Existence    | Q: Is there a mirror in the room? A: yes                                     | Q: Is there a mirror in the room? A: no                                          |\\n| Object Non-existence| Q: Is there an ironing board in the room? A: no                               | Q: Is there an ironing board in the room? A: yes                                 |\\n| Negative Response   | Q: What is the material of the bathtub? A: unknown                           | Q: Where is the shampoo dispenser? A: unknown                                    |\\n|                     | Q: Where is the shampoo dispenser? A: unknown                               |                                                                                 |\\n|                     | Q: Where is the shampoo dispenser? A: unknown                               |                                                                                 |\\n\\nSpecifically, we employ regular expression matching to detect errors in these five categories. We also employ this method to correct the responses except for Response with ID, which will be rewritten by ChatGPT instead. The QA pair will be eliminated if multiple rounds of rewriting fail to remove the IDs. Tab. A.2 and Tab. A.3 show some examples of the responses subject to the above five categories as well as the effect of our refinement.\\n\\nB.7. Subgraph Sampling\\nTo enhance the diversity of the 3D scene graphs used for prompting, we perform subgraph sampling on the 3DSSG according to a sampling rate, which denotes the ratio of preserved nodes. The sampled subgraphs are used for generating scene captions and planning data. We analyze the distribution of node numbers across the 3DSSG dataset in Fig. A.7 and set different sampling rates for scenes with different numbers of nodes in Tab. A.4. For each sampling rate, we set 4 random...\"}"}
{"id": "V4qV08Vk6S", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nOur planning data can offer scene-grounded commonsense knowledge and also encourage detailed coherent text. Data balancing. We find imbalanced data could induce hallucination in LEO, e.g., it tends to respond with \\\"Yes\\\" when asked \\\"Is there something in this room?\\\". To address this, we augment the 3RScanQA data with more negative samples where non-existent objects are queried. We also design an evaluation set with different types (Yes and No) of object-existence questions (see details in Appendix G.4). Results in Tab. 9 demonstrate that we can effectively mitigate the hallucination problem by balancing the tuning data. Moreover, the benefit of augmenting 3RScan data can transfer to ScanNet scenes in a zero-shot manner.\\n\\n4.5. Scaling Law Analysis\\n\\nSettings. We study the scaling effect (Kaplan et al., 2020; Reed et al., 2022) of data and model in LEO by tracking the instruction-tuning loss on the test set with the growth of data scale. In addition to the default Vicuna-7B, we incorporate two LLMs at different scales: OPT-1.3B (Zhang et al., 2022) and Vicuna-13B (Chiang et al., 2023). For Vicuna-7B, we also probe the influence of alignment (Scratch vs. Aligned).\\n\\nResults & analysis. From the test loss curves in Fig. 3, we have the following findings: 1) The instruction tuning of LEO conforms to the scaling law (Kaplan et al., 2020; Reed et al., 2022). We observe that all curves decrease log-linearly with the data scale. 2) Scaling up LLM leads to consistent improvements. Aligned Vicuna-7B shows significantly lower losses than Aligned OPT-1.3B. In contrast, despite the consistent improvements, the gap between Aligned Vicuna-7B and Vicuna-13B appears less significant, suggesting potential saturation if we continue to scale up the LLM. This indicates the scalability of LEO and the necessity of scaling up data to match the model capacity. 3) Alignment leads to consistent improvements. Aligned Vicuna-7B shows consistently lower losses than Scratch Vicuna-7B, which corresponds to the inferior performances of w/o Align in Tab. 7.\\n\\n5. Related Work\\n\\nGeneralist agents. The AI community has witnessed the rising generalist models in both vision (Lu et al., 2023; Wang et al., 2023b; Kirillov et al., 2023) and language (OpenAI, 2022; 2023) domains. A generalist agent requires additional embodiment knowledge to interact with the environment and complete embodied acting tasks. Existing efforts towards generalist agents include: grounded reasoning and task planning in the real world (Ahn et al., 2022; Huang et al., 2022b), skill generalization in open-world environment (Fan et al., 2022; Cai et al., 2023a; Wang et al., 2023d; Cai et al., 2023b; Gong et al., 2023b), general robotic manipulation (Brohan et al., 2022; Jiang et al., 2023; Gong et al., 2023a), and unified vision-language-action (VLA) models such as Gato (Reed et al., 2022), PaLM-E (Driess et al., 2023), EmbodiedGPT (Mu et al., 2023), and RT-2 (Brohan et al., 2023). LEO belongs to the VLA model, however, its goal is to build a generalist agent that can understand the real 3D world beyond 2D images, which is absent in existing works.\\n\\nMulti-modal instruction tuning. Pre-trained LLMs demonstrated practical for solving vision-language tasks (Tsimpoukelli et al., 2021; Alayrac et al., 2022; Guo et al., 2023; Li et al., 2023d; Zhao et al., 2023). Meanwhile, the instruction-tuning paradigm exhibited strong zero-shot generalization in NLP tasks (Wei et al., 2022; Sanh et al., 2022; Ouyang et al., 2022; Chung et al., 2022). The two streams merged into instruction-tuned LVLMs (Liu et al., 2023b; Zhu et al., 2023b; Ye et al., 2023; Gao et al., 2023; Li et al., 2023b; Gong et al., 2023c; Dai et al., 2023). Despite the burst, these models are confined to 2D visual modalities, e.g., image or video. Concurrent works (Yin et al., 2023; Hong et al., 2023; Wang et al., 2023e; Xu et al., 2023) extend to 3D vision tasks, but these models either lack the acting capability or unified efficient architecture.\\n\\nGrounded 3D scene understanding. One key obstacle to building LEO is grounding the 3D world with natural languages. There exist diverse methods of grounded scene understanding, e.g., spatial relation modeling (Zhao et al., 2021; Chen et al., 2022; Zhu et al., 2023c) and fine-grained open-scene understanding (Peng et al., 2023b; Kerr et al., 2023). However, due to data scarcity, how to utilize LLMs to ground the 3D scene is rarely explored. Recently, 3D-LLM (Hong et al., 2023) leverages multi-view images and Chat3D (Wang et al., 2023e) uses object-centric point clouds to enable the LLMs with 3D grounding. In this work, we devise both 2D and 3D encoders for grounding various visual representations and employ LoRA (Hu et al., 2022) to efficiently fine-tune the LLMs.\\n\\n3D data prompting from LLMs. LLMs exhibit extraordinary capabilities of text generation and serve as a source for collecting diverse instruction-following data (Wang et al., 2023c; Taori et al., 2023; Peng et al., 2023a). However, the lack of access to visual modalities makes it troublesome to collect visual instruction-tuning data. To address this issue, existing methods provide bounding boxes (Liu et al., 2023b) and add dense captions (Li et al., 2023a; Liu et al., 2023a) as image descriptions or directly use off-the-shelf large vision-language models (LVLM) (Zhu et al., 2023a; Luo et al., 2023) to help collect such data. Unlike concurrent attempts (Yin et al., 2023; Hong et al., 2023; Wang et al., 2023e) in collecting 3D instruction-tuning data, our approach features a scene-graph-based prompting and refinement method to prompt and correct the data.\"}"}
{"id": "V4qV08Vk6S", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\n6. Conclusions\\n\\nThe proposed agent LEO extends the current generalist ability of LLMs from text towards the 3D world and embodied tasks. It is a crucial initial step towards building embodied generalist agents. Nonetheless, there are also limitations, e.g., generalization to novel scenes, and a notable gap between VL learning and embodied action control. In light of this work, we identify several promising directions that hold the potential for substantial advancement: (1) enhancing the 3D VL understanding capability by leveraging larger-scale VL data from richer 3D domains; (2) continually bridging the gap between 3D VL and embodied action, as our experiments reveal the efficacy of their joint learning; (3) investigating the issues of safety and alignment in the context of embodied generalist agents, particularly given that our scaling law analysis suggests significant enhancements through scaling on data and model.\\n\\nImpact Statement\\n\\nThis work introduces LEO, an embodied multi-modal generalist agent designed to extend machine learning capabilities into the 3D realm, marking a significant advance in the field. The potential societal implications of LEO are manifold, touching on robotics, AR/VR, assistive technologies, and environmental planning. Ethically, it underscores the importance of responsible AI development, emphasizing safety, privacy, and fairness in automated decision-making. As LEO ventures into new territories of human-machine interaction, it prompts a re-evaluation of ethical frameworks to ensure that advancement contributes positively to society. While the immediate societal consequences of our work align with the goals of advancing machine learning, we acknowledge the necessity of ongoing ethical consideration as applications of LEO evolve.\\n\\nAcknowledgements\\n\\nThis work is supported in part by the National Science and Technology Major Project (2022ZD0114900).\\n\\nReferences\\n\\nAchlioptas, P., Abdelreheem, A., Xia, F., Elhoseiny, M., and Guibas, L. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In European Conference on Computer Vision (ECCV), 2020. 4, 7, 14\\n\\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. 8\\n\\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems (NeurIPS), 2022. 1, 3, 8\\n\\nAntol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. Vqa: Visual question answering. In International Conference on Computer Vision (ICCV), 2015. 15\\n\\nAzuma, D., Miyanishi, T., Kurita, S., and Kawanabe, M. Scanqa: 3d question answering for spatial scene understanding. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 6, 15\\n\\nBang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T., Chung, W., et al. A multi-task, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023. 4\\n\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-lut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. 1\\n\\nBrohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. 1, 8\\n\\nBrohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 1, 3, 7, 8, 30\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 1, 3\\n\\nCai, D., Zhao, L., Zhang, J., Sheng, L., and Xu, D. 3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 6\\n\\nCai, S., Wang, Z., Ma, X., Liu, A., and Liang, Y. Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13734\u201313744, 2023a. 8\"}"}
{"id": "V4qV08Vk6S", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "V4qV08Vk6S", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nHuang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., et al. Inner monologue: Embodied reasoning through planning with language models. In Conference on Robot Learning (CoRL), 2022b. 8\\n\\nJiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A., Zhu, Y., and Fan, L. Vima: General robot manipulation with multimodal prompts. In International Conference on Machine Learning (ICML), 2023. 8\\n\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 2, 8\\n\\nKerr, J., Kim, C. M., Goldberg, K., Kanazawa, A., and Tancik, M. Lerf: Language embedded radiance fields. In International Conference on Computer Vision (ICCV), 2023. 8\\n\\nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 1, 8\\n\\nKudo, T. and Richardson, J. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018. 3\\n\\nLake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. Human-level concept learning through probabilistic program induction. Science, 2015. 1\\n\\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and think like people. Behavioral and Brain Sciences, 2017. 1\\n\\nLi, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., Li, C., and Liu, Z. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023a. 8\\n\\nLi, B., Zhang, Y., Chen, L., Wang, J., Yang, J., and Liu, Z. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023b. 8\\n\\nLi, C., Gan, Z., Yang, Z., Yang, J., Li, L., Wang, L., and Gao, J. Multimodal foundation models: From specialists to general-purpose assistants, 2023c. 1\\n\\nLi, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023d. 4, 8\\n\\nLiu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., and Wang, L. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023a. 8\\n\\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023b. 2, 3, 4, 8\\n\\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. A convnet for the 2020s. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3, 26\\n\\nLu, J., Clark, C., Zellers, R., Mottaghi, R., and Kembhavi, A. Unified-io: A unified model for vision, language, and multi-modal tasks. In International Conference on Learning Representations (ICLR), 2023. 8\\n\\nLuo, T., Rockwell, C., Lee, H., and Johnson, J. Scalable 3d captioning with pretrained models. arXiv preprint arXiv:2306.07279, 2023. 4, 8, 14\\n\\nMa, X., Yong, S., Zheng, Z., Li, Q., Liang, Y., Zhu, S.-C., and Huang, S. Sqa3d: Situated question answering in 3d scenes. In International Conference on Learning Representations (ICLR), 2023. 2, 4, 6, 15, 23\\n\\nMajumdar, A., Aggarwal, G., Devnani, B., Hoffman, J., and Batra, D. Zson: Zero-shot object-goal navigation using multimodal goal embeddings. Advances in Neural Information Processing Systems (NeurIPS), 2022. 7\\n\\nMountcastle, V. B. An organizing principle for cerebral function: the unit module and the distributed system. The neurosciences. Fourth study program, 1979. 1\\n\\nMu, Y., Zhang, Q., Hu, M., Wang, W., Ding, M., Jin, J., Wang, B., Dai, J., Qiao, Y., and Luo, P. Embodiedgpt: Vision-language pre-training via embodied chain of thought. arXiv preprint arXiv:2305.15021, 2023. 8\\n\\nOpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2022. 1, 2, 8, 16\\n\\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 2, 8, 20\\n\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems (NeurIPS), 2022. 8\\n\\nPeng, B., Li, C., He, P., Galley, M., and Gao, J. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023a. 8\\n\\n11\"}"}
{"id": "V4qV08Vk6S", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nTable 1: Datasets statistics. We illustrate key statistics of datasets for 3D VL alignment (LEO-align) and 3D VLA instruction tuning (LEO-instruct).\\n\\n| Dataset Type            | Task               | 2D Input | 3D Assets | #Data  | #Tokens (prefix + res.) |\\n|-------------------------|--------------------|----------|-----------|--------|------------------------|\\n| LEO-align               | Object captioning  | \u2713        | Objaverse | 660K   | 10M                    |\\n|                         | Object referring   | \u2713        | ScanNet + 3RScan | 354K   | 15M                    |\\n|                         | Scene captioning   | \u2717        | 3RScan    | 20K    | 3.3M                   |\\n| LEO-instruct            | 3D captioning      | \u2717        | ScanNet   | 37K    | 821K                   |\\n|                         | 3D QA              | \u2717        | ScanNet + 3RScan | 83K    | 177K                   |\\n|                         | 3D dialogue        | \u2717        | 3RScan    | 11K    | 1.1M                   |\\n|                         | Task planning      | \u2717        | 3RScan    | 14K    | 1.9M                   |\\n|                         | Navigation         | \u2713        | MP3D      | 60K    | 11.4M                  |\\n|                         | Manipulation       | \u2713        | CLIPort   | 300K   | 7.2M                   |\\n\\nTable 2: Answer accuracy of LLM-generated data on three types of questions.\\n\\n|                      | Counting | Existence | Non-existence |\\n|----------------------|----------|-----------|---------------|\\n| 3D-LLM               | 56.5     | 96.8      | 40.0          |\\n| Ours                 | 57.4     | 91.3      | 27.4          |\\n| + O-CoT              | 78.0     | 93.4      | 30.5          |\\n| + refinement         | 100.0    | 100.0     | 100.0         |\\n\\nTable 3: The amount of examined data in Tab. 2. 3D-LLM data (Hong et al., 2023) is much less since we can only access a subset.\\n\\n|                      | Counting | Existence | Non-existence |\\n|----------------------|----------|-----------|---------------|\\n| 3D-LLM               | 434      | 95        | 10            |\\n| Ours                 | 2666     | 6766      | 3314          |\\n\\nUse beam search to generate textual responses. For tasks that require action commands, we map the textual outputs to action commands as discussed in Sec. 2.1. More details on the model and training can be found in Appendix D.\\n\\n3. Datasets\\n\\nSince LEO is a generalist agent that receives multi-modal inputs and follows instructions, we adopt the two-stage training proposed by Liu et al. (2023b) and split the data into two sets: (i) LEO-align (Sec. 3.1) that focuses on 3D vision-language (VL) alignment to bridge the gap between 3D scene representation and natural language; and (ii) LEO-instruct (Sec. 3.2) that targets at 3D VLA instruction tuning to endow LEO with various capabilities. The statistics and examples of these datasets can be found in Tab. 1 and Appendix C, respectively. Due to the data scarcity, we adopt LLMs to facilitate the data generation process and outline the details in Sec. 3.3.\\n\\n3.1. LEO-align: 3D Vision-Language Alignment\\n\\nIn LEO-align, we focus on 3D VL alignment. Similar to BLIP-2 (Li et al., 2023d), we train LEO to generate captions given various 3D inputs. Specifically, we collect three types of 3D captioning data: 1) object-level captions, where we align 3D individual objects with their descriptions (Luo et al., 2023); 2) object-in-the-scene captions, where the goal is to generate the referring expressions of objects in a 3D scene context (Achlioptas et al., 2020; Zhu et al., 2023c); and 3) scene-level captions, which focuses on depicting global 3D scene using natural language. Due to the space limit, we defer details including data source and components to Appendix B.1.\\n\\n3.2. LEO-instruct: Instruction Following in 3D world\\n\\nIn LEO-instruct, LEO will be tuned to follow instructions and accomplish various 3D VLA tasks. We curate a comprehensive set of tasks that covers a broad spectrum from grounded scene understanding and reasoning (Chen et al., 2021; Ma et al., 2023), to dialogue, planning, and embodied acting (Savva et al., 2019; Shridhar et al., 2021). Specifically, we introduce 1) 3D captioning and question answering \u2013 given 3D scene input, the agent needs to generate a natural language response to describe the scene or answer questions; 2) 3D dialogue and task planning, where the agent is expected to generate flexible and coherent responses to complex instructions with respect to the given 3D scene, and 3) navigation and manipulation, which require the agent to accomplish a variety of embodied acting tasks in the 3D scene. We defer details to Appendix B.2.\\n\\n3.3. LLM-assisted 3D-language Data Generation\\n\\nAs mentioned above, at the core of producing a large proportion of LEO-align and LEO-instruct is the assistance of LLMs. We now detail the key techniques of prompting LLMs (i.e., ChatGPT) to generate 3D-text paired data. An overview can be found in Fig. 2.\\n\\nScene-graph-based prompting. Our data generation pipeline starts with 3D scene graphs from 3DSSG (Wu et al., 2021), which provide scene contexts for prompting. Compared to counterparts that utilize object boxes (Yin et al., 2023; Hong et al., 2023; Wang et al., 2023e), it offers both rich object attributes and accurate spatial relation information among objects, allowing LLMs to generate data with high-quality 3D details (comparisons in Appendix B.8). Next, we manually design some examples as seed tasks (Liu et al., 2023b), including scene and object captioning, QA, dialogue, and planning, and ask LLM to produce more tasks as well as the responses. Details for designing the seed tasks can be found in Appendix B.3.\\n\\nObject-centric CoT. To further combat the hallucination of LLMs (Bang et al., 2023) in open-ended generation as in our pipeline, we propose the object-centric chain of thought (O-CoT) prompting that requires the LLM to explicitly provide the label and ID of object candidates as thoughts during text generation. We also utilize subgraph sampling to further enhance the diversity of 3D scene graphs (see details in 4).\"}"}
{"id": "V4qV08Vk6S", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dialogue(O-CoT):\\nDialogue Context: high level task: organize the bedroom. low level task: check some objects.\\nHuman: I need help organizing my bedroom. Can you assist me?\\nThought: no specific objects\\nAssistant: Of course! What specifically do you need help with?\\nHuman: Can you check if there are any clean clothes hanging on the wall?\\nThought: clothes\\nAssistant: I'm sorry, but the clothes hanging on the wall are messy.\\nHuman: Thanks for your help!\\nThought: no specific object\\nAssistant: You're welcome! If you need any further assistance, feel free to ask.\\n\\nQA(O-CoT):\\nQuestion: Where is the printer located?\\nThought: printer\\nAnswer: standing on the desk\\nQuestion: How many blankets are on the bed?\\nThought: blanket\\nAnswer: 2\\nQuestion: What is the type of the room?\\nThought: wardrobe, desk, chair, bed\\nAnswer: bedroom\\n\\nObject Scene Caption:\\nThe showcase is supported by the wall and positioned behind, close to, and to the left of the shelf. The wall is flat, architectural, and dark in color. The shelf is made of wood and has a low size.\\n\\nScene Caption:\\nIn this room, there is a wooden floor that is clean and flat. A tall wardrobe stand on the right side of a desk, close to a basket. The wardrobe is in front of a chair, a bed, and behind a bedside table. Clothes are hanging on a white wall, to the right of a closed curtain. The bed is wide, clean, and covered with blue blanket. The room has a comfortable and organized setting with functional furniture.\\n\\nPlanning:\\nHigh-Level Task: Organize and tidy up the bedroom.\\nLow-Level Actions:\\n1. Clean the floor by sweeping to remove any dirt.\\n2. Make the bed by arranging the blanket and pillows.\\n3. Place any loose items or belongings into the basket.\\n4. Arrange items on the shelves and showcase in a tidy way.\\n\\nScene Graph Context:\\n\\nHuman-labeled Responses\\n\\nScene Graph Context\\n1) Object scene caption\\n2) Scene caption\\n3) Dialogue(O-CoT)\\n4) QA(O-CoT)\\n5) Planning\\n\\nRefinement Procedures\\n1. Wrong Answers\\nThere are two curtains in the room.\\nAnswer Refinement\\nThere are 3 curtains in the room.\\n3. Responses with ID\\nThe kitchen cabinet and kitchen counter are parts of the kitchen.\\nGPT-based rewriting\\nThe kitchen features a cabinet and a counter.\\n2. Negative Responses\\nQuestion: Where is the football table?\\nThought: football table Answer: unknown\\nRemoving Negative Responses\\nThe negative responses will be removed.\\n\\nOthers\\nRemove unnecessary content.\\n\\nMessages\\nPotential Issues:\\n1) Wrong Answers\\n2) Negative Responses\\n3) Responses with IDs\\n4) Others\"}"}
{"id": "V4qV08Vk6S", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\n4.1. 3D Vision-Language Understanding and Reasoning\\n\\nOverview.\\n\\nUnderstanding and reasoning about object attributes, object relations, and other facets of 3D scenes from an agent's egocentric perspective is a fundamental capability of an embodied generalist agent in the 3D world. We investigate how well LEO can perform 3D VL understanding and embodied reasoning tasks, especially when being compared against task-specific models and existing generalist agents.\\n\\nSpecifically, we consider three renowned 3D tasks: 3D captioning on Scan2Cap (Chen et al., 2021), 3D QA on ScanQA (Azuma et al., 2022), and 3D embodied reasoning on SQA3D (Ma et al., 2023). Our evaluation metrics include conventional scores (e.g., CIDEr, BLEU, METEOR, ROUGE) and other metrics adapted for open-ended generation, e.g., sentence similarity (Reimers & Gurevych, 2019) and refined exact-match accuracy (see details in Appendix H.1). Following 3D-VisTA (Zhu et al., 2023c), we use object proposals from Mask3D (Schult et al., 2022) instead of ground-truth object segments for evaluation.\\n\\nBaselines.\\n\\nFor quantitative comparisons, we include both task-specific approaches and generalist models: 1) state-of-the-art specialists in 3D dense captioning (Chen et al., 2021; Cai et al., 2022; Chen et al., 2023); 2) state-of-the-art specialists in 3D QA (Azuma et al., 2022; Ma et al., 2023); 3) task-specific fine-tuned generalist models like 3D-VisTA (Zhu et al., 2023c) and 3D-LLM (Hong et al., 2023). To the best of our knowledge, LEO is the first model that, in stark contrast to prior models, can directly handle the aforementioned 3D VL tasks in a unified architecture without task-specific fine-tuning. This lends greater credence to LEO's comparative superiority.\\n\\nResults & analysis.\\n\\nAs shown in Tab. 4, LEO surpasses both state-of-the-art single-task and task-specific fine-tuned models significantly on 3D dense captioning and 3D QA tasks. In contrast to the specialist models that utilize task-specific heads, our LLM-based approach not only affords the flexibility of generating open-ended responses but also exhibits excellent quantitative results. On the other hand, considering the complicated feature aggregation in 3D-LLM, we believe that object-centric 3D representation is a simple yet effective option to connect 3D scenes with LLM while harnessing the inherent knowledge of LLM.\\n\\n4.2. Scene-grounded Dialogue and Planning\\n\\nOverview.\\n\\nUpon the 3D VL understanding and reasoning, we anticipate LEO to support more sophisticated interaction with humans, e.g., responding to complex multi-round user instructions in the 3D world. To verify these capabilities, we conduct qualitative studies on 3D dialogue and planning tasks, with unseen scenarios from the held-out test sets of LEO-instruct. We defer the quantitative results of dialogue and planning to our ablation study in Sec. 4.4. Quantitative comparison with other approaches is infeasible given the absence of comparable benchmarks.\\n\\nResults & analysis.\\n\\nAs shown in Fig. A.1, LEO is capable of generating high-quality responses, which encompass two features: 1) Precisely grounded to the 3D scenes. The task plan proposed by LEO involves concrete objects related to the 3D scene, as well as plausible actions regarding these objects. 2) Rich informative spatial relations. The entities in LEO's responses often accompany detailed depictions. Such information helps identify specific objects in complex 3D scenes and affords considerable assistance to humans.\\n\\n4.3. Embodied Action in 3D World\\n\\nOverview.\\n\\nTo probe LEO's capacity of bridging vision-language-acting in the 3D world, we select two canonical embodied AI tasks: embodied navigation (ObjNav) on AI Habitat (Ramrakhya et al., 2022) and robotic manipulation on CLIPort (Shridhar et al., 2021). Specifically, for CLIPort robotic manipulation, we evaluate LEO on the three tasks listed in Tab. 5 including their unseen counterparts, and report the success scores. For ObjNav, we evaluate LEO on the original MP3D ObjNav validation split.\"}"}
{"id": "V4qV08Vk6S", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"An Embodied Generalist Agent in 3D World\\n\\nTable 7: Quantitative results of LEO trained with different data configurations.\\n\\n|                | w/o Align | ScanNet | w/o Act |\\n|----------------|-----------|---------|---------|\\n|                |           |         |         |\\n| ScanNet 3RScan  |           |         |         |\\n| Scan2Cap       | 62.8      | 64.0    |         |\\n| SQA            | 22.7 (45.0)| 24.4 (49.2)|         |\\n| 3RQA           | 50.9 (53.2)| 46.8 (49.5)|         |\\n| 3RDialog       | 49.7 (53.7)| 35.8 (50.0)|         |\\n| 3RPlan         | 73.0      | 25.5    |         |\\n\\nTable 8: TrueSkill scores with human preference.\\n\\n|                | w/o Dialg | w/ Dialg |\\n|----------------|-----------|----------|\\n|                |           |          |\\n| Answerable     | 24.4 \u00b1 1.3| 25.6 \u00b1 1.3|\\n| Unanswerable   | 23.1 \u00b1 1.4| 26.8 \u00b1 1.4|\\n| NLP            | 23.4 \u00b1 1.4| 26.6 \u00b1 1.4|\\n\\nTable 9: Answer accuracy (EM) on object-existence questions.\\n\\n|                | w/o Aug | w/ Aug |\\n|----------------|---------|--------|\\n|                |         |        |\\n| Yes            | 1.00    | 0.72   |\\n| No             | 0.01    | 0.91   |\\n| Overall        | 0.34    | 0.85   |\\n\\nFigure 3: LEO-instruct test loss with the growth of data and model scale, manifesting the scaling law.\\n\\nAdditional results.\\n\\nWe further investigate the perception modules, data regime, and generalization to unseen objects in the ObjNav task. See the results in Appendix I.4.\\n\\n4.4. More Insights into LEO\\n\\nOverview.\\n\\nIn this section, we aim to offer deeper insights into LEO's characteristics, mainly from the data perspective (model perspective is deferred to Appendix G.2). Specifically, we evaluate LEO when trained with different data configurations, including exact match, sentence similarity, and human rating. We regard LEO instruction-tuned without embodied acting tasks (w/o Act) as the default setting. Following Achlioptas et al. (2020), we use ground-truth object segments in these analyses. We present additional analyses on data in Appendix I.2 and model in Appendix I.3.\\n\\nAlignment stage.\\n\\nIn contrast to complete two-stage training (w/o Act), we direct instruction-tune a model without alignment stage (w/o Align). The results in Tab. 7 show the consistent impact of alignment. In particular, the benefit of alignment is significant on Scan2Cap since it concerns detailed scene understanding and captioning, which is a primary focus of alignment training.\\n\\nSpecialist vs. generalist.\\n\\nWe train a specialist on ScanNet scenes (ScanNet). As shown in Tab. 7, ScanNet performs slightly worse than w/o Act even on ScanNet tasks, and particularly struggles at generalization across scenes (3RQA) and tasks (3RDialog and 3RPlan). This demonstrates the advantage of generalist-style instruction tuning with broad coverage of scenes and tasks.\\n\\nVL vs. VLA.\\n\\nWe compare w/o Act and VLA, which differ in whether embodied acting tasks are included for training. The results in Tab. 7 show that incorporating embodied acting tasks could lead to performance drops on 3D VL tasks. This may stem from 1) the gap between language generation and embodied action prediction, and 2) the imbalanced data scale of embodied acting tasks. In contrast to the finding that VL data benefits embodied acting tasks in VLA co-training (Brohan et al., 2023), our observation implies that embodied acting tasks may harm VL capabilities in turn. How to continually bridge the gap between VL and embodied acting tasks is an important direction for further exploration.\\n\\nDialogue and planning data.\\n\\nIn contrast to the default model (w/ Dialg in Tab. 8), we train LEO without dialogue and planning data (w/o Dialg). We design an evaluation set with three types of questions (Answerable, Unanswerable, and NLP) and evaluate with TrueSkill (Graepel et al., 2007) according to human preference (see details in Appendix G.3). The results in Tab. 8 confirm more hallucinations (less preferred by users on \u201cUnanswerable\u201d) and worse NLP skills for w/o Dialg. This is probably because 1) the diverse conversations in our dialogue data can help cultivate flexible responses to complex instructions, and 2)\"}"}
