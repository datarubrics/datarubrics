{"id": "ortiz-jimenez23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nD.2. Clean validation set\\n\\nMost of the datasets we studied have a significant amount of label noise in their training set. In this regard, the small validation set we hold out from the training set is also very noisy, which can affect the performance of early stopping and hyperparameter tuning. For this reason, we also provide results in Table 5 in which we use the clean labels from the validation set for hyperparameter tuning and early stopping. As we can see, most methods perform better in this regime, although our main findings about how the PI properties affect performance are still valid.\\n\\nTable 5. Test accuracy of several methods trained using different features as PI using a clean validation set to select the best hyperparameters (baselines in gray and italics do not use PI). Here, Original denotes the standard PI of the dataset, Indicator a binary signal that separates clean from noisy examples, Labels the one-hot encoded labels, and Near-optimal a synthetic feature consisting on giving the annotator label to those examples that are miss-annotated and a zero-vector otherwise.\\n\\n|                  | CIFAR-10H Distillation (no-PI) | CIFAR-10N Distillation (no-PI) | (worst) TRAM | Approximate FM | Distillation (PI) | no-PI | Distillation (no-PI) |\\n|------------------|-------------------------------|--------------------------------|--------------|---------------|------------------|------|---------------------|\\n| Accuracy (%)     | 59.4 \u00b1 1.5                   | 82.9 \u00b1 0.3                     | 81.0 \u00b1 0.2   | 70.6 \u00b1 0.4    | 51.0 \u00b1 0.4       | 60.8 \u00b1 0.2 | 60.0 \u00b1 0.2          |\\n| (high-noise) TRAM| 53.3 \u00b1 0.7                   | 53.8 \u00b1 0.7                     | 40.7 \u00b1 0.7   | 55.6 \u00b1 0.4    | 51.0 \u00b1 0.4       | 48.3 \u00b1 0.7 | 48.0 \u00b1 0.7          |\\n\\nBold numbers represent significant maximum values across PI features where significance means p-value < 0.05.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nE. In this section, we provide complete results for the experiments in the main paper using other datasets and algorithms with the main findings.\\n\\nE.1. Training dynamics\\n\\nIn Figure 2 we provided a detailed analysis of the dynamics of TRAM on CIFAR-10H with different PI features. We now show results for TRAM on CIFAR-10N and CIFAR-100N (see Figure 6 and Figure 7, respectively). We also show results for AFM on CIFAR-10H, CIFAR-10N, and CIFAR-100N (see Figure 8, Figure 9 and Figure 10, respectively).\\n\\nFigure 6. Dynamics of TRAM on CIFAR-10N with different PI features. Top left: Test accuracy. Top center: Train accuracy on noisy examples evaluated at the no-PI head. Top right: Train accuracy on noisy examples evaluated at the PI-head. Bottom center: Train accuracy on clean examples evaluated at the no-PI head. Bottom right: Train accuracy of clean examples evaluated at the PI head.\\n\\nFigure 7. Dynamics of TRAM on CIFAR-100N with different PI features. Top left: Test accuracy. Top center: Train accuracy on noisy examples evaluated at the no-PI head. Top right: Train accuracy on noisy examples evaluated at the PI-head. Bottom center: Train accuracy on clean examples evaluated at the no-PI head. Bottom right: Train accuracy of clean examples evaluated at the PI head.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nFigure 8. Dynamics of AFM on CIFAR-10H with different PI features. Top left: Test accuracy. Top center: Train accuracy on noisy examples evaluated with marginalization. Top right: Train accuracy on noisy examples evaluated at the PI-head. Bottom center: Train accuracy on clean examples evaluated with marginalization. Bottom right: Train accuracy of clean examples evaluated at the PI head.\\n\\nFigure 9. Dynamics of AFM on CIFAR-10N with different PI features. Top left: Test accuracy. Top center: Train accuracy on noisy examples evaluated with marginalization. Top right: Train accuracy on noisy examples evaluated at the PI-head. Bottom center: Train accuracy on clean examples evaluated with marginalization. Bottom right: Train accuracy of clean examples evaluated at the PI head.\\n\\nE.2. Feature extractor size\\nWe replicate the results in Figure 3 for other settings with the same findings. In particular, we show results on CIFAR-10N and CIFAR-100N (see Figure 11 and Figure 12, respectively).\\n\\nE.3. PI head size\\nWe replicate the results in Figure 4 on CIFAR-10N and CIFAR-100N (see Figure 13 and Figure 14, respectively). In this case, however, we observe no clear trend in the results, probably due to the fact that the original PI on these datasets is not good enough for TRAM and AFM to shine (cf. Table 1). In this regard, increasing the PI head size does not lead to better...\"}"}
{"id": "ortiz-jimenez23a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nFigure 10. Dynamics of AFM on CIFAR-100N with different PI features. Top left: Test accuracy. Top center: Train accuracy on noisy examples evaluated with marginalization. Top right: Train accuracy on noisy examples evaluated at the PI-head. Bottom center: Train accuracy on clean examples evaluated with marginalization. Bottom right: Train accuracy of clean examples evaluated at the PI-head.\\n\\nFigure 11. Performance of different PI baselines on CIFAR-10N when increasing the parameter count of their feature extractor keeping the PI tower fixed. Larger models suffer from overfitting as they tend to use their larger capacity to overfit to noisy examples, discouraging the model from exploiting the PI.\\n\\nFigure 12. Performance of different PI baselines on CIFAR-100N when increasing the parameter count of their feature extractor keeping the PI tower fixed. Larger models suffer from overfitting as they tend to use their larger capacity to overfit to noisy examples, discouraging the model from exploiting the PI.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nperformance as there is nothing to extract from the PI.\\n\\n| PI Tower Width | Test accuracy |\\n|----------------|--------------|\\n| 80.00%         | 70.0%        |\\n| 81.00%         | 80.0%        |\\n| 82.00%         | 90.0%        |\\n\\n| PI Tower Width | Train acc (no-PI / mislabeled) |\\n|----------------|---------------------------------|\\n| 70.0%          | TRAM                            |\\n| 80.0%          | AFM                             |\\n\\nFigure 13. Performance of different PI baselines on CIFAR-10N when increasing the PI head size. A larger PI head size incentivizes the model to memorize the noisy examples using the PI making more use of the PI as a shortcut.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We now give the design details for TRAM++, the improved version of TRAM which appends a unique random PI vector to the original PI. In particular, we followed the same tuning strategy as in the rest of the TRAM experiments in the paper and we also tuned the parameter $\\\\lambda$ that weighs the losses of the two heads, i.e.,\\n\\n$$\\n\\\\min_{\\\\phi, \\\\pi, \\\\psi} \\\\mathbb{E}(x, a, \\\\tilde{y}) \\\\left[ \\\\ell(\\\\pi(\\\\phi(x), a), \\\\tilde{y}) + \\\\lambda \\\\ell(\\\\psi(\\\\phi(x)), \\\\tilde{y}) \\\\right].\\n$$\\n\\n(2)\\n\\nCollier et al. (2022) suggested that the gradients of the no-PI head do not affect the updates of the feature extraction, and thus $\\\\lambda$ could be folded directly into the tuning of the global learning rate of TRAM. However, in our experiments, we found that tuning $\\\\lambda$ given a fixed number of epochs can lead to significant gains in performance, as it can slow down training of the no-PI head. As seen in Figure 15, increasing $\\\\lambda$ has the same effect as increasing the learning rate of the no-PI head, and a sweet spot exists for values of $\\\\lambda < 1$ in which the no-PI head trains fast enough to fit the clean examples, but avoids learning all the noisy ones.\\n\\nIn general, $\\\\lambda$ was not tuned in any of the other experiments, in order to remain as close as possible to the original TRAM implementation. However, for the TRAM++ experiments, which aimed to achieve the best possible performance out of TRAM, $\\\\lambda$ was tuned.\\n\\nG. Design of AFM++\\n\\nIn Section 4.2, we have seen that appending random PI that uniquely identifies each example to the original PI can sometimes induce beneficial shortcuts in TRAM++. We now test the same strategy applied to AFM, and design AFM++, an augmented version of AFM with additional random PI. Table 6 shows the results of our experiments where we see that AFM++ also clearly improves over \u201cvanilla\u201d AFM. Again, the improvements are greater in those datasets where overfitting is a bigger issue in the first place.\\n\\n| Dataset       | no-PI | AFM    | AFM++   |\\n|---------------|-------|--------|---------|\\n| CIFAR-10H (worst) | 55.0 \u00b1 1.5 | 64.0 \u00b1 0.6 | 68.2 \u00b1 0.6 |\\n| CIFAR-10N (worst) | 80.6 \u00b1 0.2 | 82.0 \u00b1 0.3 | 84.6 \u00b1 0.2 |\\n| CIFAR-100N     | 60.4 \u00b1 0.5 | 60.0 \u00b1 0.2 | 61.9 \u00b1 0.2 |\\n| ImageNet-PI (high-noise) | 47.7 \u00b1 0.8 | 55.6 \u00b1 0.3 | 55.0 \u00b1 0.6 |\\n\\nTable 6. Performance comparison of no-PI, AFM and AFM++ on the different PI datasets.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We also evaluate the combination of TRAM with label smoothing (LS). In particular, we follow the standard label smoothing procedure and add the label smoothing hyperparameter to the hyperparameters swept over in Table 1. More specifically, we sweep over label smoothing of 0.2, 0.4, 0.6, and 0.8 and select the optimal hyperparameter setting following the same procedure as all experiments in the paper. The results are given in Table 7.\\n\\nWe observe that on all datasets, adding label smoothing to the TRAM method leads to performance improvements, demonstrating that TRAM can be successfully combined with label smoothing. More generally, this observation strengthens the point that TRAM and TRAM++ are compatible and yield additive performance gains when combined with widely used methods developed for noisy labels.\\n\\nTable 7. Performance comparison of no-PI, Label smoothing (LS), TRAM TRAM+LS on different PI datasets.\\n\\n| Dataset       | no-PI | TRAM | LS  | TRAM+LS |\\n|---------------|-------|------|-----|---------|\\n| CIFAR-10H     | 55.0  | 64.9 | 59.9| 65.4    |\\n| CIFAR-10N     | 80.6  | 80.5 | 80.5| 82.4    |\\n| CIFAR-100N    | 60.4  | 59.7 | 60.0| 61.9    |\\n\\nI. Experimental details for SOP and TRAM+SOP\\n\\nAs we have established in Section 5, the combination of TRAM and SOP has the potential to achieve cumulative gains in robustness to label noise. TRAM, with its original PI, has been shown to improve performance on datasets with dense noise, such as CIFAR-10H (worst), compared to a model with no PI. However, the PI may not always be explanatory of the noise and even if it is, it may not fully explain away all of the noise. Additionally, the feature extractor and subsequent layers of the model may still be susceptible to noise, even when the PI is able to explain away the noise.\\n\\nOn the other hand, SOP has been shown to work well for sparsely distributed noise and operates on the principle of modeling out the noise, which is distinct from the method used by TRAM. As these principles are complementary to one another, we propose to combine the advantages of both methods to achieve cumulative gains.\\n\\nAs highlighted in Section 5, the combination of TRAM+SOP consists of two main steps: pre-training with TRAM and fine-tuning with SOP. Our implementation of TRAM used regular TRAM with a few enhancements from TRAM++, such as random PI and a larger PI head size. It is important to note that our experiments were conducted using our own implementation of SOP and, although it incorporated the SOP method and was sanity-checked with the original authors of the paper, our experimental baseline environment and search space were different from theirs. As a result, the test accuracy on the CIFAR-N datasets may be lower than the results reported in the original SOP paper. However, the primary objective of these experiments was to explore whether TRAM+SOP can achieve cumulative gains over the respective implementations of TRAM and SOP alone and our results support this hypothesis.\\n\\nIn our experiments, both the SOP and TRAM+SOP models were trained for a total of 120 epochs, with a learning rate schedule that decayed at epochs 40, 80 and 110. We employed the SGD with Nesterov momentum for TRAM and regular momentum for SOP as in Liu et al. (2022). For a detailed description of the SOP parameters, we refer the reader to the original SOP paper. It is important to note that the results presented here for the TRAM+SOP method do not include all proposed enhancements in Liu et al. (2022). Further gains in performance may be achievable by incorporating these advancements and jointly optimizing the hyperparameter space for both the TRAM and SOP pretraining and fine-tuning stages.\\n\\nJ. Experimental details for TRAM+HET\\n\\nTRAM+HET consists of a simple two-headed TRAM model in which the last linear layer of each of the two heads has been substituted by a heteroscedastic linear layer (Collier et al., 2021). In these experiments, we thus also sweep over the temperature of the heteroscedastic layers. A similar method was already proposed in Collier et al. (2022), under the name Het-TRAM, but here we also make use of our insights and allow the model to make use of random PI on top of the original PI features. Interestingly, contrary to what happened with TRAM+SOP, the addition of random PI, i.e., TRAM++, did not always yield performance improvements using TRAM+HET. Instead, depending on the dataset (see Table 8) we observe that the use of random PI can sometimes hurt the final performance of the models (e.g., as in CIFAR-10H). We conjecture...\"}"}
{"id": "ortiz-jimenez23a", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise? This might be due to the TRAM+HET models using the random PI to memorize the clean labels as well. Understanding why this happens only when using heteroscedastic layers is an interesting avenue for future work.\\n\\nTable 8. Performance comparison of TRAM, TRAM++, HET, TRAM+HET (without additional random PI), and TRAM+HET (with additional random PI) on the different PI datasets.\\n\\n| Dataset            | TRAM | TRAM++ | HET  | TRAM+HET (w/o random) | TRAM+HET (+random) |\\n|--------------------|------|--------|------|-----------------------|--------------------|\\n| CIFAR-10H (worst)  | 64.9 | 66.8   | 50.8 | 67.7 \u00b1 0.7            | 56.5 \u00b1 0.7         |\\n| CIFAR-10N (worst)  | 80.5 | 83.9   | 81.9 | 82.0 \u00b1 0.3            | 83.5 \u00b1 0.5         |\\n| CIFAR-100N         | 59.7 | 61.1   | 60.8 | 62.1 \u00b1 0.1            | 61.2 \u00b1 0.3         |\\n| ImageNet-PI (high-noise) | 53.3 | 53.9  | 51.5 | 55.8 \u00b1 0.3            | 55.4 \u00b1 0.4         |\"}"}
{"id": "ortiz-jimenez23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nThis relabelling process can produce arbitrarily noisy labels whose distribution is very far from being symmetrical, i.e., not all misclassifications are equally likely. For example, it is more likely that similar dog breeds get confused among each other, but less likely that a \u2018dog\u2019 gets re-labelled as a \u2018chair\u2019.\\n\\nThe PI in this dataset comes from the confidences of the models on the sampled label, their parameter count, and their test accuracy on the clean test distribution. These PI features are a good proxy for the expected reliability of each of the models.\\n\\nIn our dataset release, we will provide the following files:\\n\\n- **labels-train.csv**, **labels-validation.csv**\\n  These files contain the new (noisy) labels for the training and validation set respectively. The new labels are provided by the pre-trained annotator models. Each file provides the labels in CSV format: `<image_id>,<label_1>,<label_2>,...,<label_16>\\n\\n- **confidences-train.csv**, **confidences-validation.csv**\\n  These files contain the confidence of each annotator model in its annotation; both for the training set and the validation set respectively. Each file provides the confidences in CSV format: `<image_id>,<confidence_1>,<confidence_2>,...,<confidence_16>\\n\\n- **annotator-features.csv**\\n  This file contains the annotator features (i.e., meta-data about the model annotators themselves) in CSV format (16 rows; one for each model annotator): `<model_accuracy>,<number_of_model_parameters`\\n\\nIn particular, we will provide two standardized sampled annotations obtained by applying the temperature sampling process discussed above: one with $\\\\beta = 0.1$ corresponding to high label noise and one with $\\\\beta = 0.5$ corresponding to low label noise.\\n\\n### B. Experimental details\\n\\nWe build upon the implementations and hyperparameters from the open source Uncertainty Baselines codebase (Nado et al., 2021). All results in the paper are reported based on 5 random seeds.\\n\\n#### B.1. Dataset-specific training settings\\n\\n##### B.1.1. CIFAR\\n\\nAll CIFAR models are trained using a SGD optimizer with $0.9$ Nestrov momentum for 90 epochs with a batch size of 256. We sweep over an initial learning rate of $\\\\{0.01, 0.1\\\\}$ with the learning rate decayed by a factor of 0.2 after 27, 54 and 72 epochs. We sweep over an L2 regularization parameter of $\\\\{0.00001, 0.0001, 0.001\\\\}$. Following Nado et al. (2021), we use a Wide ResNet model architecture with model-width multiplier of 10 and a model-depth of 28. Unless specified otherwise, for TRAM and AFM models, we set the PI tower width to be 1024 as this was the default parameter in Collier et al. (2022). We use the same architecture for the Distillation (PI) teacher. This controls the size of the subnetwork which integrates the PI which is parameterized as a concatenation of the pre-processed PI (and then passed through a Dense + ReLU layer), a residual connection and finally a concatenation of the joint feature space with the non-PI representation. The number of units in the Dense layers is controlled by the \u201cPI tower width\u201d.\\n\\nFor distillation models we uniformly sample over a temperature interval of $[0.5, 10]$. For CIFAR-10N and CIFAR-100N we split the original training set into a training and a validation set; 98% of the examples are used for training and the remaining 2% used as a validation set. Due to the smaller size of the CIFAR-10H training set (which following Collier et al. (2022) is actually the original CIFAR test set), 96% of the original training set is used as a training set with the remaining 4% used as a validation set.\\n\\nFor TRAM++ and where relevant for AFM, we search over a no-PI loss weight of $\\\\{0.1, 0.5\\\\}$, a PI tower width of $\\\\{512, 1024, 2048, 4096\\\\}$ and a random PI length of $\\\\{8, 14, 28\\\\}$. For heteroscedastic CIFAR models, we set the number of\"}"}
{"id": "ortiz-jimenez23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\n3 factors for the low-rank component of the heteroscedastic covariance matrix (Collier et al., 2021) to be\\n\\n3 for CIFAR-10H and\\n\\n3 for CIFAR-10N, and\\n\\n3 for CIFAR-100N and\\n\\nsearch over\\n\\n\\\\{0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 3, 5\\\\}\\n\\nfor the heteroscedastic temperature.\\n\\nB.1.2. I\\n\\nMAGE\\n\\nN\\n\\nET\\n\\nPI\\n\\nImageNet models are trained using a SGD optimizer with 0.9 Nestrov momentum for 90 epochs with a batch size of 128.\\n\\nWe set the initial learning rate of 0.05 with the learning rate decayed by a factor of 0.1 after 30, 60 and 80 epochs. We\\n\\nsweep over an L2 regularization parameter of\\n\\n\\\\{0.00001, 0.0001\\\\}.\\n\\nWe use a ResNet-50 model architecture.\\n\\nFor TRAM and AFM models by default we set the PI tower width to be 2048, with the same parameterization of the PI\\ntower as for the CIFAR models. For distillation models we set the distillation temperature to be 0.5. We use\\n\\n1% of the\\n\\noriginal ImageNet training set as a validation set.\\n\\nFor TRAM++ and where relevant for AFM, we set the no-PI loss weight of to 0.5 and use random PI length of 30.\\n\\nFor\\n\\nheteroscedastic models we set the number of factors for the low-rank component of the heteroscedastic covariance matrix to\\n\\nbe\\n\\n15\\n\\nand search over\\n\\n\\\\{0.75, 1, 1.5, 2, 3\\\\}\\n\\nfor the heteroscedastic temperature.\\n\\nB.2. Hyperparameter tuning strategy\\n\\nUnless otherwise indicated, we report the test-set accuracy at the hyperparameters determined by the\\n\\narg max\\n\\nof best\\n\\nvalidation set accuracy (where the number of epochs are considered to be part of the set to be maximized over). The\\n\\nvalidation set used has noisy labels generated by the same process as the training set. This implements a realistic and\\n\\nnoisy hyperparameter search with early stopping that we believe most closely replicates what is possible in real-world\\n\\nscenarios where a clean validation set may be unavailable. However, other papers report test-set metrics determined by a\\n\\nhyperparameter sweep assuming the availability of a clean validation set and/or without early stopping, which can have a\\n\\nlarge impact on the reported test-set metrics (see Appendix D for results computed in this way).\"}"}
{"id": "ortiz-jimenez23a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nC. Results on low-noise settings\\n\\nIn the main text, we always reported results for the high-noise settings of each of the datasets. However, we now show that all our findings from Table 1 also apply in the low-noise setting.\\n\\nTable 3. Test accuracy of several methods trained using different features as PI on the low-noise versions of the datasets (baselines in gray and italics do not use PI). Here, Original denotes the standard PI of the dataset, Indicator a binary signal that separates clean from noisy examples, Labels the one-hot encoded labels, and Near-optimal a synthetic feature consisting on giving the annotator label to those examples that are miss-annotated and a zero-vector otherwise.\\n\\n| Dataset          | Method         | Accuracy \u00b1 Error |\\n|------------------|----------------|-----------------|\\n| CIFAR10-H        | Distillation (no-PI) | 82.7 \u00b1 0.09 |\\n|                  | (uniform) TRAM  | 85.1 \u00b1 0.26 |\\n|                  | Approximate FM  | 85.9 \u00b1 0.12 |\\n|                  | Distillation (PI) | 81.0 \u00b1 0.02 |\\n| CIFAR10-N        | Distillation (no-PI) | 85.4 \u00b1 0.09 |\\n|                  | (uniform) TRAM  | 88.7 \u00b1 0.66 |\\n|                  | Approximate FM  | 88.6 \u00b1 0.62 |\\n|                  | Distillation (PI) | 88.1 \u00b1 0.07 |\\n| ImageNet-PI      | Distillation (no-PI) | 67.9 \u00b1 0.33 |\\n|                  | (low-noise) TRAM | 69.7 \u00b1 0.36 |\\n|                  | Approximate FM  | 70.6 \u00b1 0.20 |\\n|                  | Distillation (PI) | 67.2 \u00b1 0.21 |\\n\\nBold numbers represent significant maximum values across PI features where significance means p-value < 0.05.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Effect of early stopping\\n\\nAs early stopping is one of the strongest baselines against label noise, in all our experiments we held out a small portion of the noisy training set and reported clean test accuracy at the epoch with the best validation accuracy. However, to make sure that our findings do not depend on the use of early stopping, or the amount of label noise in the validation set, we now present a reproduction of the results in Table 1 when either disabling early stopping or using a clean validation set to perform early stopping and hyperparameter tuning.\\n\\nD.1. No early stopping\\n\\nTable 4 shows the results of our benchmark without using early stopping. In general, we observe that without early stopping most baselines perform significantly worse as they overfit more to the noisy labels. In this regard, since one of the main benefits of PI is that it prevents memorization of the noisy labels, we see that without early stopping the relative improvement of the PI techniques with respect to their no-PI baselines is much larger.\\n\\n| Test accuracy of several methods trained using different features as PI not using early stopping | baselines in gray and italics do not use PI. |\\n|---|---|\\n| | Original | Indicator | Labels | Near-optimal |\\n| CIFAR-10H | Distillation (no-PI) | 45.2 \u00b1 0.1 | 45.2 \u00b1 0.1 | 45.2 \u00b1 0.1 |\\n| | (worst) TRAM | 59.2 \u00b1 0.2 | 46.5 \u00b1 0.6 | 39.9 \u00b1 0.3 |\\n| | Approximate FM | 61.2 \u00b1 0.7 | 39.3 \u00b1 0.6 | 79.3 \u00b1 0.3 |\\n| | Distillation (PI) | 45.2 \u00b1 0.0 | 46.3 \u00b1 0.1 | 45.7 \u00b1 0.1 |\\n| | no-PI | 67.7 \u00b1 0.6 | 67.7 \u00b1 0.6 | 67.7 \u00b1 0.6 |\\n| | Distillation (no-PI) | 71.4 \u00b1 0.2 | 71.4 \u00b1 0.2 | 71.4 \u00b1 0.2 |\\n| | (worst) TRAM | 67.0 \u00b1 0.4 | 79.2 \u00b1 0.9 | 51.6 \u00b1 0.2 |\\n| | Approximate FM | 69.8 \u00b1 0.5 | 10.0 \u00b1 0.0 | 92.3 \u00b1 0.2 |\\n| | Distillation (PI) | 71.1 \u00b1 0.2 | 73.1 \u00b1 0.3 | 70.9 \u00b1 0.3 |\\n| | no-PI | 55.8 \u00b1 0.2 | 55.8 \u00b1 0.2 | 55.8 \u00b1 0.2 |\\n| | Distillation (no-PI) | 58.6 \u00b1 0.1 | 58.6 \u00b1 0.1 | 58.6 \u00b1 0.1 |\\n| CIFAR-10N | Distillation (no-PI) | 71.4 \u00b1 0.2 | 71.4 \u00b1 0.2 | 71.4 \u00b1 0.2 |\\n| | (worst) TRAM | 67.0 \u00b1 0.4 | 79.2 \u00b1 0.9 | 51.6 \u00b1 0.2 |\\n| | Approximate FM | 69.8 \u00b1 0.5 | 10.0 \u00b1 0.0 | 92.3 \u00b1 0.2 |\\n| | Distillation (PI) | 71.1 \u00b1 0.2 | 73.1 \u00b1 0.3 | 70.9 \u00b1 0.3 |\\n| | no-PI | 55.8 \u00b1 0.2 | 55.8 \u00b1 0.2 | 55.8 \u00b1 0.2 |\\n| | Distillation (no-PI) | 58.6 \u00b1 0.1 | 58.6 \u00b1 0.1 | 58.6 \u00b1 0.1 |\\n| | ImageNet-PI | Distillation (no-PI) | 50.4 \u00b1 0.8 | 50.4 \u00b1 0.8 | 50.4 \u00b1 0.8 |\\n| | (high-noise) TRAM | 53.3 \u00b1 0.4 | 53.5 \u00b1 0.4 | 41.0 \u00b1 0.8 |\\n| | Approximate FM | 55.0 \u00b1 0.4 | 0.4 \u00b1 0.1 | 58.3 \u00b1 0.4 |\\n| | Distillation (PI) | 50.9 \u00b1 0.4 | 50.6 \u00b1 0.2 | 39.1 \u00b1 5.1 |\\n| | no-PI | 47.7 \u00b1 0.8 | 47.7 \u00b1 0.8 | 47.7 \u00b1 0.8 |\\n| | Distillation (no-PI) | 50.4 \u00b1 0.8 | 50.4 \u00b1 0.8 | 50.4 \u00b1 0.8 |\\n\\nBold numbers represent significant maximum values across PI features where significance means p-value < 0.05.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: The original PI hurts the test performance when the clean and noisy labels agree and otherwise. Table 1 (Indicator) shows the test accuracy of several methods trained using different features as PI (baselines in gray and italics do not use PI). Here, Original Indicator Labels Near-optimal a synthetic feature that gives the annotator label to those examples that are miss-annotated and a zero-vector otherwise. Table 1 indicates that annotator features, e.g., confidences, could act as proxy to identify mislabeled samples. Intuitively, the indicator underperforms or does not significantly improve over the original PI (cf. AFM and TRAM on CIFAR10H). This suggests that the PI methods do not use PI. Here, Original Indicator Labels Near-optimal a synthetic feature that gives the annotator label to those examples that are miss-annotated and a zero-vector otherwise. Table 1 (Indicator) shows the test accuracy of several methods trained using different features as PI (baselines in gray and italics do not use PI). Here, Original Indicator Labels Near-optimal a synthetic feature that gives the annotator label to those examples that are mislabeled and a zero-vector otherwise. Table 1 indicates that annotator features, e.g., confidences, could act as proxy to identify mislabeled samples. Intuitively, the indicator underperforms or does not significantly improve over the original PI (cf. AFM and TRAM on CIFAR10H). This suggests that the PI methods do not use PI. Here, Original Indicator Labels Near-optimal a synthetic feature that gives the annotator label to those examples that are miss-annotated and a zero-vector otherwise.\\n\\n| Method                  | CIFAR-10N     | CIFAR-10H (high-noise) TRAM | CIFAR-10H (worst) TRAM |\\n|-------------------------|--------------|-----------------------------|------------------------|\\n| Distillation (no-PI)    | 50 \u00b1 0.6     | 45 \u00b1 0.4 \u00b1 0.7              | 60 \u00b1 0.5 \u00b1 0.4 \u00b1 0.2   |\\n| Distillation (PI)       | 50 \u00b1 0.6     | 45 \u00b1 0.4 \u00b1 0.7              | 60 \u00b1 0.5 \u00b1 0.4 \u00b1 0.2   |\\n| Approximate FM          | 50 \u00b1 0.6     | 45 \u00b1 0.4 \u00b1 0.7              | 60 \u00b1 0.5 \u00b1 0.4 \u00b1 0.2   |\\n| TRAM                    | 50 \u00b1 0.6     | 45 \u00b1 0.4 \u00b1 0.7              | 60 \u00b1 0.5 \u00b1 0.4 \u00b1 0.2   |\\n| Original Indicator Labels Near-optimal | 50 \u00b1 0.6 | 45 \u00b1 0.4 \u00b1 0.7 | 60 \u00b1 0.5 \u00b1 0.4 \u00b1 0.2 |\\n\\nFigure 2 (right column) explains why giving the labels as PI do not seem to learn anything useful. These differences are explained by the rates at which these models understand the previous results. For example, Figure 2 shows the evolution of test and training accuracies of a TRAM heads. The blue line sits behind the yellow line in Figure 2 (top right). Results for others settings can be found in Appendix E.1.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nFigure 2. Dynamics of TRAM on CIFAR-10H with different PIs. Top left: Test accuracy. Top center: Train accuracy on mislabeled examples evaluated at the no-PI head. Top right: Train accuracy on mislabeled examples evaluated at the PI-head. Bottom center: Train accuracy on clean examples evaluated at the no-PI head. Bottom right: Train accuracy on clean examples evaluated at the PI head.\\n\\nobtains 100% training accuracy on all examples (mislabeled and correct) using the PI head, which in turn slows the training speed of the no-PI head (central column). This happens because the feature extractor is only updated by gradients from the PI head, leading to a lack of meaningful representation of $p(\\\\tilde{y}|x)$ if the model is learning to fit all examples using PI features alone.\\n\\nFocusing on the training accuracies of the no-PI head in Figure 2 (central column), the best models are those that achieve the highest training accuracy on correct examples, while not overfitting to the mislabeled. The difference in test performance of indicator and original is explained by the original model having a harder time overfitting to the mislabeled examples. Interestingly, the original model memorizes mislabeled examples faster with the PI head than the indicator. It looks as though fitting the training examples fast with the PI head was discouraging the model from fitting the same examples with the no-PI head, i.e., the PI is enabling a learning shortcut to memorize the mislabeled examples with the original PI, without using $x$. This might be because the indicator signal only takes values in $\\\\{0, 1\\\\}$ for all examples, and these are not enough to separate the noisy training set. Indeed, as we will see, having access to PI that can be easily memorized on the mislabeled examples is fundamental to maximize performance.\\n\\n3.4. Near-optimal PI features\\n\\n**Hypothesis**: The optimal PI enables a learning shortcut to memorize only the mislabeled labels.\\n\\nThe experiments using the annotator labels as PI are a clear example of a PI-enabled learning shortcut which is very detrimental for the model performance. On the other hand, the dynamics of the original models hint that the same shortcut mechanism can also have a positive effect when it only applies to the mislabeled examples. To test this hypothesis, we design a new form of PI features, denoted as near-optimal in the tables and plots. As its name indicates, this PI should allow the models to get very close to their top performance. The near-optimal features are designed to exploit the PI shortcut only on the mislabeled examples, allowing the model to learn freely on the correct ones.\\n\\nTo that end, the near-optimal PI features consist of two concatenated values: (i) the indicator signal that says if a given example is mislabeled or not, and (ii) the annotator label only if that example is mislabeled. Otherwise an all-zero vector is concatenated with the same dimensionality as the one-hot encoded labels to the indicator signal.\\n\\nFinding: When a learning shortcut is provided only for mislabeled examples, PI methods achieve top performance. The results in Table 1 (Near-optimal) show that those PI features significantly outperform all other PI features by a large margin on all datasets when using TRAM or AFM.\\n\\nSimilarly, in Figure 2 we observe that the dynamics of the near-optimal models fully match our expectations. The near-optimal models train the fastest on the mislabeled examples 4. Near-optimal does not always outperform original when using Distillation (PI), but note that in general the gains of Distillation (PI) over (no-PI) are much smaller than for TRAM and AFM. In this regard, we leave the objective of finding a near-optimal policy for Distillation (PI) as an open question for future work.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise? \\n\\non the PI head, thus leading to a very slow training speed on mislabeled examples on the no-PI head. Moreover, since the mislabeled examples no longer influence (because their accuracies are already maximal on the PI head) the updates of the feature extraction, then we observe that the performance on the correct examples is much higher. \\n\\nThe same explanation applies to AFM whose dynamics are shown in Appendix E.1. In this case, the memorization of the mislabeled examples using PI alone also protects the no-PI features. This way, during inference, the PI sampled from the mislabeled examples simply adds a constant noise floor to the predicted probabilities of the incorrect labels. This averaged noise floor is usually much smaller than the probability predicted using the clean features of the no-PI, and thus does not lead to frequent misclassification errors.\\n\\n4. Improving PI algorithms \\n\\nIn this section, we use the insights of the previous analysis to improve the design of PI methods. We perform ablation studies on different design parameters of the main PI approaches, identifying simple modifications that significantly boost their performance. We primarily focus on TRAM and AFM as these methods outperform Distillation (PI) by a large margin when the PI is helpful (cf. Table 1). We provide illustrative results here, and full results in Appendix E.\\n\\n4.1. Model size \\n\\nWe explore how the model size affects performance. In particular, note that the parameter count of all PI algorithms can be split into two parts: the feature extractor \\\\( \\\\phi \\\\) of the standard features \\\\( x \\\\) and the tower \\\\( \\\\pi \\\\) that processes the PI; see Eq. (1) and Section 2.2. We therefore perform an ablation study in which we scale each of these parts of the models separately. \\n\\n**Feature extractor.** Figure 3 shows how test accuracy changes as we increase the size of the feature extractor of the PI approaches. The performance follows a U-shape, where scaling the model past a certain point harms final performance. Indeed, a larger capacity discourages the model from using PI features and causes overfitting to standard features, as shown by the simultaneous increase in training accuracy on mislabeled examples and decrease in test accuracy.\\n\\n**Finding:** Increasing the feature extractor size discourages using the PI as a shortcut.\\n\\n**PI head size.** Figure 4 shows the results of scaling the size of the PI processing tower while keeping the feature extractor size fixed. We observe how larger PI heads improve performance as they encourage more memorization using PI alone and protect the extraction of the no-PI features. This is illustrated by the decay of the training accuracy of the mislabeled examples on the no-PI head for larger PI heads.\\n\\n**Finding:** Increasing the capacity of the PI tower encourages using the PI as a shortcut.\\n\\n4.2. Random PI can enable positive shortcuts \\n\\n**Hypothesis:** Random PI that uniquely identifies each example can enable a PI shortcut that protects the model from memorizing incorrect labels with \\\\( x \\\\).\\n\\nThe near-optimal, labels, and indicator signals of Table 1 are all synthetic PI features that cannot be used in practice, as they rely on the knowledge of which examples are mislabeled and which examples are correct. However, they show that having access to a signal that can be more easily memorized than the standard features \\\\( x \\\\) on the mislabeled examples is a good recipe to improve performance. This being said, a key property of incorrect labels is that they are, by definition, a mistake. In this sense, fitting an incorrect training label simply amounts to memorizing a specific training example whose features are not predictive of the target label, i.e., the features serve just as an example ID. In fact, any set of features which are different enough for each\"}"}
{"id": "ortiz-jimenez23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nExample could act as such an ID.\\n\\nFinding: Random PI is effective at reducing overfitting to the incorrect labels using $x$.\\n\\nWe evaluate this hypothesis in Table 2 where we introduce TRAM++: a version of TRAM in which the original PI features are augmented with a unique random vector for each example (experimental details are provided in Appendix F and results for AFM++ in Appendix G). As we can see, TRAM++ generally achieves better performance than TRAM alone, with greater improvements in those datasets where overfitting is a bigger issue (i.e., CIFAR).\\n\\n5. Combination with other no-PI techniques\\n\\nIn this section, we show experimentally that the performance improvements obtained by PI methods on noisy datasets can work symbiotically with other state-of-the-art techniques from the noisy label literature. In particular, we show that TRAM++ can be easily combined with Sparse Over-parameterization (SOP) (Liu et al., 2022) and Heteroscedastic output layers (Collier et al., 2021) while providing cumulative gains with respect to those baselines.\\n\\n5.1. Sparse Over Parameterization (SOP)\\n\\nSparse over-parameterization (SOP) (Liu et al., 2022) is a state-of-the-art method which leverages the implicit bias of stochastic gradient descent (SGD) and overparameterization to estimate and correct the noisy label signal, a concept which has proven to work well (Zhao et al., 2022). It does so by adding two new sets of $K$-dimensional parameters $\\\\{u_i\\\\}_{i=1}^N$ and $\\\\{v_i\\\\}_{i=1}^N$, where $N$ denotes the number of training points, and solving\\n\\n$$\\\\min_{\\\\theta, \\\\{u_i, v_i\\\\}_{i=1}^N} 1/N \\\\sum_{i=1}^N \\\\ell(f_{\\\\theta}(x) + u_i \\\\odot u_i - v_i \\\\odot v_i, \\\\tilde{y}_i)$$\\n\\nusing SGD. This specific parameterization biases the solution of SGD towards the recovery of the noise signal $\\\\epsilon_i = u_i \\\\odot u_i - v_i \\\\odot v_i$ that corrupts $y$, i.e., $\\\\tilde{y}_i \\\\approx y_i + \\\\epsilon_i$, implicitly assuming that $\\\\epsilon_i$ is sparse across the dataset.\\n\\nIn this work, we explore whether the combination of TRAM++ with SOP can yield cumulative gains in performance against label noise. In particular, we propose a simple two-step training process to combine them: (i) We first pre-train a neural network using TRAM++ and (ii) we finetune the no-PI side of the network using the SOP loss without stop-gradients. Table 2 shows the results of this method where we see that, indeed, TRAM+SOP is able to significantly outperform TRAM++ or SOP alone in all datasets. More experimental details can be found in Appendix I.\\n\\n5.2. Heteroscedastic output layers\\n\\nFinally, we further analyze the combination of TRAM with HET, another state-of-the-art no-PI baseline from the noisy label literature that can be scaled up to ImageNet scale (Collier et al., 2021). HET here refers to the use of heteroscedastic output layers to model the aleatoric uncertainty of the predictions without PI. In particular, we apply HET layers to both heads of TRAM++ and follow the same training setup. We call the resulting approach TRAM+HET. Our experiments, presented in Table 2, show that the TRAM+HET model outperforms both TRAM++ and HET applied alone. More experimental details about that model combination can be found in Appendix J. All in all, these results corroborate our main findings:\\n\\nFinding: PI methods work symbiotically with other no-PI algorithms from the noisy label literature.\\n\\n6. Related work\\n\\nThe general framework of learning with privileged information (Vapnik & Vashist, 2009) has been widely studied in deep learning, with many works exploring different baselines, including loss manipulation (Yang et al., 2017), distillation (Lopez-Paz et al., 2016), or Gaussian dropout (Lambert et al., 2018). This line of work has mainly focused on the noiseless scenario, conceiving PI as a guiding signal that helps identify easy or hard instances (Vapnik & Izmailov, 2015). Similar to our work, Yang et al. (2022) also studied the role of PI in improving the performance of deep learning methods, but focusing on the task of learning-to-rank using distillation methods in the noiseless setting.\\n\\nMore recently, Collier et al. (2022) proposed a new perspective on PI, arguing that it can make models more robust to the presence of noise. Their proposed PI approach, referred to as TRAM, led to gains on various experimental settings, with both synthetic and real-world noise. However, their results lacked a detailed analysis of how different sources of PI affect performance.\\n\\nOur work takes inspiration from the rich deep-learning theory studying the memorization dynamics of neural networks (Zhang et al., 2017; Rolnick et al., 2017; Toneva et al., 2019; Maennel et al., 2020; Baldock et al., 2021). In the no-PI setting, the dynamics of neural networks wherein the incorrect labels tend to be later memorized during training has been heavily exploited by the noisy-label community through techniques such as early-stopping and regularization (Liu et al., 2020; Bai et al., 2021). Other works have...\"}"}
{"id": "ortiz-jimenez23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Leveraging privileged information (PI), or features available during training but not at test time, has recently been shown to be an effective method for addressing label noise. However, the reasons for its effectiveness are not well understood. In this study, we investigate the role played by different properties of the PI in explaining away label noise. Through experiments on multiple datasets with real PI (CIFAR-N/H) and a new large-scale benchmark ImageNet-PI, we find that PI is most helpful when it allows networks to easily distinguish clean from mislabeled data, while enabling a learning shortcut to memorize the mislabeled examples. Interestingly, when PI becomes too predictive of the target label, PI methods often perform worse than their no-PI baselines. Based on these findings, we propose several enhancements to the state-of-the-art PI methods and demonstrate the potential of PI as a means of tackling label noise. Finally, we show how we can easily combine the resulting PI approaches with existing no-PI techniques designed to deal with label noise.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nFigure 1. Conceptual illustration of ideal signal propagation while training a privileged information method such as TRAM (Collier et al., 2022) with noisy labels. Having access to PI allows a network to use a learning shortcut to memorize the mislabeled examples using only PI. This protects the extraction of features from the actual data, which are only refined using the correctly labeled examples. Shortcuts are key to understanding the role of PI in mitigating label noise and are directly linked to deep learning dynamics (Zhang et al., 2017). When focusing on the dynamics of the strongest PI methods, we show that using PI allows training larger models on datasets with a higher level of noise. PI can counteract the negative effect of memorizing incorrect associations between features and incorrect labels as it enables a shortcut that primarily affects the mislabeled examples. We use these new insights to improve current state-of-the-art PI algorithms.\\n\\nOverall, the main contributions of our work are:\\n\\n\u2022 We present the first large-scale study on the role of PI on supervised noisy datasets of various types.\\n\u2022 We release ImageNet-PI, the largest available testbed for experimenting with PI and label noise.\\n\u2022 We find that effective PI enables learning shortcuts only on mislabeled data greatly benefitting performance.\\n\u2022 We improve a wide range of PI methods using simple improvements, and demonstrate cumulative gains with other state-of-the-art noisy labels methods.\\n\\nWe believe our findings can have a significant impact on future research about both label noise and PI. They indeed not only inform us about the desired properties of the ideal PI (which can help design and collect PI features in practice) but also provide practical insights for improving existing methods. Formally capturing our empirical results is another promising direction for future research.\\n\\n2. Methodology\\n\\nOur large-scale experiments provide a comprehensive analysis of the usefulness of PI in the presence of label noise. Previous results have been provided in heterogeneous settings with testing performed on different datasets and various types of PI, making well-aligned comparisons difficult. We aim at standardizing these comparisons, making the unification of all the settings of our experiments part of our core contribution. Our code can be found at https://github.com/google/uncertainty-baselines. In what follows, we briefly describe the datasets and baselines used in our study.\\n\\n2.1. Datasets\\n\\nIn this work, we address the supervised learning setting with PI and label noise as described in Collier et al. (2022). Our training data consists of triplets \\\\((x, \\\\tilde{y}; a)\\\\), where \\\\(x \\\\in \\\\mathbb{R}^d\\\\) is a set of input features, \\\\(\\\\tilde{y} \\\\in \\\\{1, \\\\ldots, K\\\\}\\\\) is a noisy target label (assuming \\\\(K\\\\) classes), and \\\\(a \\\\in \\\\mathbb{R}^p\\\\) is a vector of PI features. In this work, we mainly focus in the case when these PI features are related to the annotation process, as this is a common source of label noise (Snow et al., 2008; Sheng et al., 2008). This PI may include information about the annotator, such as their ID or experience; or about the process itself, such as the annotation duration or confidence. At test time, we do not have access to any PI and evaluate our models based only on clean \\\\((x, y)\\\\) pairs from the data distribution.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nWe use relabelled versions of standard image recognition datasets which provide various forms of PI about the annotation process in our experiments. These datasets allow us to access both clean ($y$) and noisy labels ($\\\\tilde{y}$), but we only use the noisy labels for training and hyperparameter selection (see details in Appendix D for a discussion about the effect of noisy labels at this step). The clean labels are only used for evaluation. The datasets we use offer a range of training conditions, including differing numbers of samples and classes, levels of noise, types of PI, image sizes, and annotation processes, making our findings widely applicable.\\n\\nSome of these datasets provide multiple annotations per example. Nonetheless, to create a unified benchmark we only sample one label per example for datasets that provide multiple labels. So that we can control the noise level and examine its impact on the performance of PI methods, we create high and low noise versions of each dataset, when possible. We follow the terminology of Wei et al. (2022) by naming the low noise version the \\\"uniform\\\" version, which selects one of the available labels uniformly at random, and the high noise version, the \\\"worst\\\" version, which always selects an incorrect label if available. The \\\"worst\\\" version is by design more noisy than the \\\"uniform\\\" one.\\n\\nCIFAR-10/100N. A relabelled version of the CIFAR-10/100 datasets (Krizhevsky, 2009) that includes multiple annotations per image (Wei et al., 2022). The raw data includes information about the annotation process, such as annotation times and annotator IDs, but this information is not given at the example level. Instead, it is provided as averages over batches of examples, resulting in coarse-grained PI. We will show that the PI baselines perform poorly on this dataset. The \\\"uniform\\\" version of CIFAR-10N agrees 82.6% of the time with the clean labels, and the \\\"worst\\\" version 59.8%. CIFAR-100N agrees 59.8% with the clean labels. For reference, training our base architectures without label noise and without PI achieves test accuracies of 93.5% and 77.9% on CIFAR-10 and CIFAR-100, respectively.\\n\\nCIFAR-10H. An alternative human-relabelled version of CIFAR-10, where the new labels are provided only on the test set (Peterson et al., 2019). As in Collier et al. (2022), when we train on CIFAR-10H, we evaluate the performance of the models on the original CIFAR-10 training set (since CIFAR-10H relabels only the validation set). Contrary to the CIFAR-N datasets, CIFAR-10H contains rich PI at the example-level, with high-quality metadata about the annotation process. The \\\"uniform\\\" version agrees 95.1% of the time with the clean labels, and the \\\"worst\\\" version 35.4%. For reference, training our base architecture without label noise and without PI achieves a test accuracy of 88.4% on CIFAR-10H.\\n\\nImageNet-PI. Inspired by Collier et al. (2022), a relabeled version of ImageNet (Deng et al., 2009) in which the labels are provided by a set of pre-trained deep neural networks with different architectures. During the relabelling process, we sample a random label from a temperature-scaled predictive distribution of each model on each example. This leads to label noise that is asymmetrical and feature-dependent. Technical details of the relabelling process and temperature-scaling can be found in Appendix A. The PI of the dataset comes from the confidences of the models on the sampled labels, the parameter counts of the models, and the models' test accuracies on the clean test distribution. These PI features serve as a good proxy for the expected reliability of each model. The ImageNet-PI high-noise version that we use agrees 16.2% of the time with the clean labels and the low-noise version 51.9%. For reference, training our base architecture without label noise and without PI achieves a test accuracy of 76.2% on ImageNet. As a contribution of this work, we open-source ImageNet-PI (with different amounts of label noise) to encourage further research on PI and label noise at a scale larger than possible today with CIFAR-N/H. The data is publicly available at https://github.com/google-research-datasets/imagenet_pi.\\n\\n2.2. PI Algorithms\\n\\nWe study the performance of four representative approaches that exploit PI. They all have been shown to be effective at mitigating the effect of label noise (Collier et al., 2022): no-PI. A standard supervised learning baseline that minimizes the cross-entropy loss on the noisy labels to approximate $p(\\\\tilde{y}|x)$ without access to PI.\\n\\nDistillation (Lopez-Paz et al., 2016). A knowledge distillation method in which a teacher model is first trained using standard maximum likelihood estimation with access to PI to approximate $p(\\\\tilde{y}|x,a)$. A student model with the same architecture is later trained to match the output of the teacher without access to the PI. We also provide results for a standard self-distillation baseline in which the teacher model does not have access to PI (Hinton et al., 2015).\\n\\nTRAM (Collier et al., 2022). Method based on a two-headed model in which one head has access to PI and the other one not. At training time, a common feature representation $\\\\phi(x)$ is fed to two classification heads $\\\\pi(\\\\phi(x),a)$ (\\\"PI head\\\") and $\\\\psi(\\\\phi(x))$ (\\\"no-PI head\\\") to jointly solve\\n\\n$$\\\\min_{\\\\phi,\\\\pi,\\\\psi} E(x,a,\\\\tilde{y}) \\\\left[ \\\\ell(\\\\pi(\\\\phi(x),a),\\\\tilde{y}) + \\\\ell(\\\\psi(\\\\phi(x)),\\\\tilde{y}) \\\\right].$$\\n\\n(1)\\n\\nImportantly, during training, the no-PI feature extractor $\\\\phi$ is updated using only the gradients coming from the PI head. At test time, only the no-PI head is used for prediction.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nApproximate Full Marginalization (Collier et al., 2022).\\n\\nA neural network is first trained using maximum likelihood estimation with access to PI to approximate \\\\( p(\\\\tilde{y} | x, a) \\\\).\\n\\nDuring inference, a Monte-Carlo estimation is used to approximate the marginal \\\\( p(\\\\tilde{y} | x) = \\\\int p(\\\\tilde{y} | x, a) p(a | x) \\\\, da \\\\) typically further assuming the independence \\\\( p(a | x) \\\\approx p(a) \\\\). Note that this process increases the memory and computational costs during inference as it requires computing the output of the network for each of the different sampled values of \\\\( a \\\\) (in practice, in the order of \\\\( 1,000 \\\\) extra values).\\n\\nAll the methods use the same underlying network architecture with minimal changes to accommodate their specific requirements, like Collier et al. (2022). In particular, during inference, all methods use exactly the same network, except for the approximate full marginalization (AFM) baseline, which has additional parameters to deal with the sampled PI.\\n\\nIn all experiments, we use the same protocol for training with noisy labels and evaluating on a clean test set. As early stopping is a strong baseline against label noise (Bai et al., 2021), we always report values of test accuracy at the end of the epoch that achieves the best performance on a held-out validation percentage of the noisy training set. We reproduce our results without early stopping in Appendix D. Unless otherwise specified, we conduct a grid search to tune the most important hyperparameters of each method for each experiment, and report the mean test accuracy and standard deviation over five runs. Further details on the experimental setup can be found in Appendix B.\\n\\n3. When is PI helpful?\\n\\nTable 1 (Original) shows the performance of the different PI algorithms on our collection of noisy datasets, where we see that leveraging PI does not always yield big gains in performance. Indeed, while TRAM and AFM substantially improve upon the no-PI baseline on CIFAR-10H and ImageNet-PI, they do not perform much better on CIFAR-10N and CIFAR-100N. Moreover, we observe little gains of Distillation (PI) over the vanilla self-distillation baseline.\\n\\nThe performance disparities of the same algorithms on datasets where the main source of variation is the available PI, i.e., CIFAR-10N vs. CIFAR-10H, highlights that leveraging PI is not always helpful. In fact, depending on the predictive properties of the PI and the noise distribution, we report very different results. This begs the questions: i) \\\"what makes PI effective for these algorithms?\\\" and ii) \\\"how do they exploit PI to explain away label noise?\\\".\\n\\nWe present results for high-noise in the main text. A reproduction of Table 1 with low-noise can be found in Appendix C.\\n\\nTo answer these question, we perform a series of controlled experiments in which we train our PI methods using different PI features (including both real and synthetic ones). By doing so our objective is to identify the main mechanisms that lead to the top performance of these algorithms.\\n\\n3.1. Fully predictive PI\\n\\nHypothesis: The PI always complements the information about the labels \\\\( \\\\tilde{y} \\\\) contained in \\\\( x \\\\).\\n\\nIt is natural to assume that knowing \\\\( a \\\\) on top of \\\\( x \\\\) can help predict \\\\( \\\\tilde{y} \\\\) and thus improve over supervised learning. However, this reasoning is flawed as it forgets that during inference the models cannot exploit \\\\( a \\\\). On the contrary, as we will see, if \\\\( a \\\\) is very predictive of the target \\\\( \\\\tilde{y} \\\\), the test performance can severely degrade.\\n\\nWe test this hypothesis by retraining the algorithms on the noisy datasets but using \\\\( a = \\\\tilde{y} \\\\) instead of the original PI features. That is, having access to fully predictive PI.\\n\\nFinding: When \\\\( a \\\\) is fully predictive of \\\\( \\\\tilde{y} \\\\), most PI methods perform worse than the no-PI baselines.\\n\\nAs we can see in Table 1 (Labels), all the PI baselines greatly suffer in this regime. The reason for this is simple: when the PI is too informative of the target label, then the models are heavily relying on the PI to explain the label and they are discouraged from learning any associations between \\\\( x \\\\) and \\\\( \\\\tilde{y} \\\\) and do not learn any meaningful feature representations. In this regard, we see how Distillation (PI) achieves roughly the same performance as Distillation (no-PI), while TRAM and AFM achieve very low test accuracies.\\n\\nThe fact that very predictive PI can hurt the performance of these algorithms highlights a key element of their dynamics: PI can enable learning shortcuts (D'Amour et al., 2020; Geirhos et al., 2020) that prevent learning certain associations between \\\\( x \\\\) and \\\\( \\\\tilde{y} \\\\), possibly by starving the gradient signal that updates \\\\( \\\\phi(x) \\\\) (Pezeshki et al., 2021). This has practical implications as it discourages blindly appending arbitrarily complex metadata to \\\\( a \\\\) during training which could be very predictive of the target label.\\n\\n3.2. Noise indicator\\n\\nHypothesis: PI helps because it can separate mislabeled from correct examples.\\n\\nWe saw that when \\\\( a \\\\) is too predictive of \\\\( \\\\tilde{y} \\\\), the PI approaches perform poorly. We now turn to an alternative hypothesis of why PI can be beneficial to explain away label noise: The PI features can help the network separate the clean from the mislabeled examples. Indeed, the original motivation of using PI to fight label noise in Collier et al. (2022)\"}"}
{"id": "ortiz-jimenez23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nTable 2. Performance comparison of no-PI, TRAM, TRAM++, SOP, HET, TRAM+SOP and TRAM+HET on the different PI datasets.\\n\\n| Dataset                  | no-PI | TRAM  | TRAM++ | SOP   | TRAM+SOP | HET   | TRAM+HET |\\n|--------------------------|-------|-------|--------|-------|----------|-------|----------|\\n| CIFAR-10H (worst)        | 65.0  | 64.9  | 65.8   | 64.9  | 65.8     | 66.8  | 67.0     |\\n| CIFAR-10N (worst)        | 80.6  | 80.5  | 83.9   | 83.9  | 88.9     | 88.5  | 88.9     |\\n| CIFAR-100N               | 60.4  | 59.7  | 61.1   | 61.1  | 63.3     | 65.1  | 65.9     |\\n| ImageNet-PI (high-noise) | 47.7  | 53.3  | 53.9   | 53.9  | \u2013        | 58.8  | 55.8     |\\n\\nexploited the intrinsic difference between the learning of clean and mislabeled examples to detect and correct misclassification errors using self-supervision (Veit et al., 2017; Li et al., 2020), co-teaching (Han et al., 2018), or regularization (Cheng et al., 2021). Finally, many works have attempted to model the label corruption process by estimating the label transition matrix (Patrini et al., 2017) or the noisy signal directly in the prediction space (Liu et al., 2022). In general, we see this line of research about noisy labels (Song et al., 2020) as orthogonal to the use of PI and we have experimentally shown that our PI approach is in fact complementary and can be gracefully combined with such techniques.\\n\\nSome aspects of this work are suggestive of causal reasoning. In particular, explaining away is a well-known phenomenon when there are multiple explanations for the value that a particular variable has taken, e.g., whether it is the ground-truth label correctly annotated, or a mistake from an annotator (Pearl, 2009). We do not use causal formalism explicitly in this work, although we see similar learning dynamics at play in our results. PI (often called auxiliary labels) is also used in causally-motivated work on robust ML, although this is usually focused on the distinct problem of handling spurious correlations, rather than overcoming label noise (Kallus et al., 2018; Veitch et al., 2021; Makar et al., 2022). In self-supervised learning, the removal of shortcuts is also a topic of interest (Minderer et al., 2020).\\n\\n7. Conclusions\\n\\nIn this work, we have presented a systematic study in which we investigate which forms of PI are more effective at explaining away label noise. Doing so we have found that the most helpful PI is the one that allows the networks to separate correct from mislabeled examples in feature space, but also enable an easier learning shortcut to memorize the mislabeled examples. We have also shown that methods which use appropriate PI to explain away label noise, can be combined with other state-of-the-art methods to remove noise and achieve cumulative gains. Exploring this direction further is a promising avenue for future work. Our insights show that the use of PI is a promising avenue of research to fight against label noise. Our insights further highlight that collecting the right PI in datasets requires some care to enable the learning of effective shortcuts.\\n\\nAcknowledgements\\n\\nWe thank Jannik Kossen for helpful comments on this work. We also thank Josip Djolonga and Joan Puigcerver for helpful discussions related to infrastructure and data processing.\\n\\nReferences\\n\\nBai, Y., Yang, E., Han, B., Yang, Y., Li, J., Mao, Y., Niu, G., and Liu, T. Understanding and improving early stopping for learning with noisy labels. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\nBaldock, R. J. N., Maennel, H., and Neyshabur, B. Deep learning through the lens of example difficulty. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\nCheng, H., Zhu, Z., Li, X., Gong, Y., Sun, X., and Liu, Y. Learning with instance-dependent label noise: A sample sieve approach. In International Conference on Learning Representations (ICLR), 2021.\\n\\nCollier, M., Mustafa, B., Kokiopoulou, E., Jenatton, R., and Berent, J. Correlated input-dependent label noise in large-scale image classification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nCollier, M., Jenatton, R., Kokiopoulou, E., and Berent, J. Transfer and marginalize: Explaining away label noise with privileged information. In International Conference on Machine Learning (ICML), 2022.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nDeng, J., Dong, W., Socher, R., Li, L. J., Kai, L., and Li, F. F. ImageNet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248\u2013255, 2009.\\n\\nGeirhos, R., Jacobsen, J., Michaelis, C., Zemel, R. S., Brendel, W., Bethge, M., and Wichmann, F. A. Shortcut learning in deep neural networks. Nat. Mach. Intell., 2(11):665\u2013673, 2020.\\n\\nHan, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I. W., and Sugiyama, M. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\\n\\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015.\\n\\nKallus, N., Puli, A. M., and Shalit, U. Removing hidden confounding by experimental grounding. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\\n\\nKrizhevsky, A. Learning Multiple Layers of Features from Tiny Images. Technical report, University of Toronto, 2009.\\n\\nLambert, J., Sener, O., and Savarese, S. Deep learning under privileged information using heteroscedastic dropout. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\nLi, J., Socher, R., and Hoi, S. C. H. Dividemix: Learning with noisy labels as semi-supervised learning. In International Conference on Learning Representations (ICLR), 2020.\\n\\nLiu, S., Niles-Weed, J., Razavian, N., and Fernandez-Granda, C. Early-learning regularization prevents memorization of noisy labels. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nLiu, S., Zhu, Z., Qu, Q., and You, C. Robust training under label noise by over-parameterization. In International Conference on Machine Learning (ICML), 2022.\\n\\nLopez-Paz, D., Bottou, L., Sch\u00f6lkopf, B., and Vapnik, V. Unifying distillation and privileged information. In International Conference on Learning Representations (ICLR), 2016.\\n\\nMaennel, H., Alabdulmohsin, I. M., Tolstikhin, I. O., Balduck, R. J. N., Bousquet, O., Gelly, S., and Keysers, D. What do neural networks learn when trained with random labels? In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nMakar, M., Packer, B., Moldovan, D., Blalock, D., Halpern, Y., and D'Amour, A. Causally motivated shortcut removal using auxiliary labels. In International Conference on Artificial Intelligence and Statistics, (AISTATS), 2022.\\n\\nMinderer, M., Bachem, O., Houlsby, N., and Tschannen, M. Automatic shortcut removal for self-supervised representation learning. In International Conference on Machine Learning (ICML), 2020.\\n\\nNado, Z., Band, N., Collier, M., Djolonga, J., Dusenberry, M. W., Farquhar, S., Feng, Q., Filos, A., Havasi, M., Jenatton, R., et al. Uncertainty baselines: Benchmarks for uncertainty & robustness in deep learning. arXiv preprint arXiv:2106.04015, 2021.\\n\\nPatrini, G., Rozza, A., Menon, A. K., Nock, R., and Qu, L. Making deep neural networks robust to label noise: A loss correction approach. In IEEE Conference on Computer Vision and Pattern Recognition, (CVPR), 2017.\\n\\nPearl, J. Causality: Models, Reasoning and Inference. Cambridge University Press, 2009.\\n\\nPeterson, J. C., Battleday, R. M., Griffiths, T. L., and Rustakovsky, O. Human uncertainty makes classification more robust. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea, October 27 - November 2, 2019.\\n\\nPezeshki, M., Kaba, S., Bengio, Y., Courville, A. C., Precup, D., and Lajoie, G. Gradient starvation: A learning proclivity in neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\nPrabhu, V., Yenamandra, S., Singh, A., and Hoffman, J. Adapting self-supervised vision transformers by probing attention-conditioned masking consistency. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748\u20138763. PMLR, 2021.\\n\\nRolnick, D., Veit, A., Belongie, S., and Shavit, N. Deep learning is robust to massive label noise. arXiv preprint arXiv:1705.10694, 2017.\\n\\nSheng, V. S., Provost, F., and Ipeirotis, P. G. Get another label? improving data quality and data mining using multiple, noisy labelers. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2008.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nSnow, R., O'Connor, B., Jurafsky, D., and Ng, A. Cheap and fast \u2013 but is it good? evaluating non-expert annotations for natural language tasks. In Conference on Empirical Methods in Natural Language Processing (EMNLP), October 2008.\\n\\nSong, H., Kim, M., Park, D., and Lee, J. Learning from noisy labels with deep neural networks: A survey. CoRR, abs/2007.08199, 2020.\\n\\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\nToneva, M., Sordoni, A., des Combes, R. T., Trischler, A., Bengio, Y., and Gordon, G. J. An empirical study of example forgetting during deep neural network learning. In International Conference on Learning Representations (ICLR), 2019.\\n\\nVapnik, V. and Izmailov, R. Learning using privileged information: similarity control and knowledge transfer. J. Mach. Learn. Res., 16:2023\u20132049, 2015.\\n\\nVapnik, V. and Vashist, A. A new learning paradigm: Learning using privileged information. Neural Networks, 22(5-6), 2009.\\n\\nVeit, A., Alldrin, N., Chechik, G., Krasin, I., Gupta, A., and Belongie, S. J. Learning from noisy large-scale datasets with minimal supervision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\nVeitch, V., D'Amour, A., Yadlowsky, S., and Eisenstein, J. Counterfactual invariance to spurious correlations in text classification. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\nWei, J., Zhu, Z., Cheng, H., Liu, T., Niu, G., and Liu, Y. Learning with noisy labels revisited: A study using real-world human annotations. In International Conference on Learning Representations (ICLR), 2022.\\n\\nYang, H., Zhou, J. T., Cai, J., and Ong, Y. MIML-FCN+: multi-instance multi-label learning via fully convolutional networks with privileged information. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\nYang, S., Sanghavi, S., Rahmanian, H., Bakus, J., and Vishwanathan, S. Toward understanding privileged features distillation in learning-to-rank. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations (ICLR), 2017.\\n\\nZhao, P., Yang, Y., and He, Q.-C. High-dimensional linear regression via implicit regularization. Biometrika, 109(4):1033\u20131046, feb 2022. doi: 10.1093/biomet/asac010.\\n\\nURL https://doi.org/10.1093%2Fbiomet%2Fasac010.\"}"}
{"id": "ortiz-jimenez23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When does Privileged Information Explain Away Label Noise?\\n\\nAppendices\\n\\nThis appendix is organized as follows: In Appendix A we describe the relabelling process done to generate ImageNet-PI. In Appendix B we describe in depth the experimental details of our experiments and our hyperparameter tuning strategy. Appendix C replicates our main findings in the low-noise version of the PI datasets. Appendix D discusses and ablates the effect of early stopping in our experiments. Appendix E provides additional results of our ablation studies on other datasets. Appendix F and Appendix G give further details about TRAM++ and AFM++, respectively. And, finally, Appendix I and Appendix J describe in depth the experimental setup used to combine SOP and HET with TRAM, respectively.\\n\\nA. ImageNet-PI\\n\\nImageNet-PI is a re-labelled version of the standard ILSVRC2012 ImageNet dataset in which the labels are provided by a collection of 16 deep neural networks with different architectures pre-trained on the standard ILSVRC2012. Specifically, the pre-trained models are downloaded from `tf.keras.applications` and consist of: ResNet50V2, ResNet101V2, ResNet152V2, DenseNet121, DenseNet169, DenseNet201, InceptionResNetV2, InceptionV3, MobileNet, MobileNetV2, MobileNetV3Large, MobileNetV3Small, NASNetMobile, VGG16, VGG19, Xception.\\n\\nDuring the re-labelling process, we do not directly assign the maximum confidence prediction of each of the models, but instead, for each example, we sample a random label from the predictive distribution of each model on that example. Furthermore, to regulate the amount of label noise introduced when relabelling the dataset, ImageNet-PI allows the option to use stochastic temperature-scaling to increase the entropy of the predictive distribution. The stochasticity of this process is controlled by a parameter $\\\\beta$ which controls the inverse scale of a Gamma distribution (with shape parameter $\\\\alpha = 1.0$), from which the temperature values are sampled, with a code snippet looking as follows:\\n\\n```python\\n# Get the predictive distribution of the model annotator.\\npred_dist = model.predict(...)\\n# Sample the temperature.\\ntemperature = tf.random.gamma(tf.shape(pred_dist)[0], alpha=tf.constant([1.]), beta=tf.constant([beta_parameter]))\\n# Compute the new predictive distribution.\\nlog_probs = tf.math.log(pred_dist) / temperature\\nnew_pred_dist = tf.nn.softmax(log_probs)\\n# Sample from the new predictive distribution.\\nclass_predictions = tf.random.categorical(tf.math.log(new_pred_dist), 1)[:,0]\\n```\\n\\nIntuitively, smaller values of $\\\\beta$ translate to higher temperature values as shown in Figure 5, which leads to higher levels of label noise as softmax comes closer to uniform distribution for high temperatures.\\n\\n![Figure 5. The effect of parameter $\\\\beta$ in sampling temperatures.](https://www.tensorflow.org/api_docs/python/tf/keras/applications)\"}"}
