{"id": "ZtOXZCTgBa", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nWang, L., Zhang, W., He, X., and Zha, H. Supervised reinforcement learning with recurrent neural network for dynamic treatment recommendation. Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018.\\n\\nWang, R., Foster, D. P., and Kakade, S. M. What are the statistical limits of offline rl with linear function approximation. ArXiv, abs/2010.11895, 2020.\\n\\nWang, T., Du, S. S., Torralba, A., Isola, P., Zhang, A., and Tian, Y. Denoised mdps: Learning world models better than the world itself. ArXiv, abs/2206.15477, 2022.\\n\\nYang, Y., Ye, H.-J., Zhan, D.-C., and Jiang, Y. Auxiliary information regularized machine for multiple modality feature learning. In Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015.\\n\\nYarats, D., Fergus, R., Lazaric, A., and Pinto, L. Mastering visual continuous control: Improved data-augmented reinforcement learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\\n\\nYu, T., Thomas, G., Yu, L., Ermon, S., Zou, J. Y., Levine, S., Finn, C., and Ma, T. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129\u201314142, 2020.\\n\\nYuan, Z., Ma, G., Mu, Y., Xia, B., Yuan, B., Wang, X., Luo, P., and Xu, H. Don't touch what matters: Task-aware lipschitz data augmentation for visual reinforcement learning. ArXiv, abs/2202.09982, 2022.\\n\\nZanette, A. Exponential lower bounds for batch reinforcement learning: Batch rl can be exponentially harder than online rl. ArXiv, abs/2012.08005, 2020.\\n\\nZhang, A., McAllister, R. T., Calandra, R., Gal, Y., and Levine, S. Learning invariant representations for reinforcement learning without reconstruction. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\"}"}
{"id": "ZtOXZCTgBa", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1. Proof of Theorem 3.3\\n\\nLemma A.1. (Lemma 3.1) (Telescoping lemma in the endogenous state space). Let $M$ and $f$ be two MDPs with the same reward function $r$, but different dynamics $T$ and $e_T$ respectively. Let $G_\\\\pi f_M(s_t + a_t) := E_{s_t + 1 \\\\sim e_T(s_t + a_t)} h V_M(s_t + 1) - E_{s_t + 1 \\\\sim T(s_t + a_t)} h V_M(s_t + 1)$. Then,\\n\\n$$\\\\eta f_M(\\\\pi) - \\\\eta M(\\\\pi) = \\\\sum_{j=0}^{\\\\infty} (W_j + 1 - W_j).$$\\n\\nProof. We adopt the same proof as (Luo et al., 2019; Yu et al., 2020) to prove the telescoping lemma in the endogenous state space. Let $W_j$ be the expected return when executing $\\\\pi$ on $e_T$ for the first $j$ steps, then switching to $T$ for the remainder. That is,\\n\\n$$W_j = E_{t < j} s_{t+1} \\\\sim e_T(s_t + a_t) t \\\\geq j: s_{t+1} \\\\sim T(s_t+a_t) \\\\#$$\\n\\nNote that $W_0 = \\\\eta M(\\\\pi)$ and $W_{\\\\infty} = \\\\eta f_M(\\\\pi)$, so\\n\\n$$\\\\eta f_M(\\\\pi) - \\\\eta M(\\\\pi) = \\\\sum_{j=0}^{\\\\infty} (W_j + 1 - W_j).$$\\n\\nWrite\\n\\n$$W_j = R_j + E_{s_j, a_j \\\\sim \\\\pi, e_T} \\\\# E_{s_{j+1} \\\\sim T(s_j + a_j)} h V_M(s_{j+1}) - E_{s_{j+1} \\\\sim e_T(s_{j+1}, a_j)} h V_M(s_{j+1}) \\\\#$$\\n\\nThus\\n\\n$$\\\\eta f_M(\\\\pi) - \\\\eta M(\\\\pi) = \\\\sum_{j=0}^{\\\\infty} (W_j + 1 - W_j) = \\\\sum_{j=0}^{\\\\infty} \\\\gamma^{j+1} E_{s_j, a_j \\\\sim \\\\pi, e_T} h G_\\\\pi f_M(s_j, a_j).$$\\n\\nTheorem A.2. (Theorem 3.3) Under Assumption 3.2, we can define the uncertainty estimation $\\\\tilde{\\\\epsilon}_u(\\\\pi)$ under the EX-BMDP as $\\\\tilde{\\\\epsilon}_u(\\\\pi) := \\\\bar{E}_{s, a \\\\sim \\\\tilde{\\\\pi}} \\\\tilde{T}[\\\\tilde{u}(s, a)]$. Let $\\\\tilde{\\\\pi}$ denote the learned optimal policy under reward-penalized the endogenous MDP, then,\\n\\n$$\\\\eta M(\\\\tilde{\\\\pi}) \\\\geq \\\\sup_{\\\\pi} \\\\{ \\\\eta M(\\\\pi) - 2\\\\lambda \\\\tilde{\\\\epsilon}_u(\\\\pi) \\\\}.$$\"}"}
{"id": "ZtOXZCTgBa", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nProof. With Lemma 3.1, we can get the corollary that:\\n\\\\[ \\\\eta_M(\\\\pi) = \\\\bar{E}(s^+ , a) \\\\sim \\\\rho \\\\pi e_T r(s^+ , a) - \\\\gamma G_\\\\pi f_M(s^+ , a) \\\\geq \\\\bar{E}(s^+ , a) \\\\sim \\\\rho \\\\pi e_T r(s^+ , a) - \\\\gamma |G_\\\\pi f_M(s^+ , a)| \\\\]\\n\\nWith Assumption 3.2, we know that \\\\( |G_\\\\pi f_M(s^+ , a)| \\\\leq \\\\tilde{u}(s^+ , a) \\\\). Thus, we define the penalized reward function as \\\\( \\\\tilde{r}(s^+ , a) = r(s^+ , a) - \\\\lambda \\\\tilde{u}(s^+ , a) \\\\), where \\\\( \\\\lambda = \\\\gamma c \\\\) and \\\\( c \\\\) is the scalar that satisfies \\\\( |G f_M \\\\pi(s^+ , a)| \\\\leq cd F(e_T(s^+ , a), T(s^+ , a)) \\\\).\\n\\nWe can obtain the relationship between the policy performance under the true MDP and the reward-penalized endogenous MDP:\\n\\\\[ \\\\eta_M(\\\\tilde{\\\\pi}) \\\\geq \\\\bar{E}(s^+ , a) \\\\sim \\\\rho \\\\pi e_T \\\\tilde{r}(s^+ , a) = \\\\eta_M(\\\\pi) \\\\]\\n\\nFrom Assumption 3.2, we can easily get the two-sided bound that:\\n\\\\[ |\\\\eta_M(\\\\pi) - \\\\eta_M(\\\\tilde{\\\\pi})| \\\\leq \\\\bar{E}(s^+ , a) \\\\sim \\\\rho \\\\pi e_T |G_\\\\pi f_M(s^+ , a)| \\\\leq \\\\lambda \\\\bar{E}(s^+ , a) \\\\sim \\\\rho \\\\pi e_T [\\\\tilde{u}(\\\\pi)] = \\\\lambda \\\\epsilon \\\\tilde{u}(\\\\pi) \\\\]\\n\\nWith the help of Equation (3) and Equation (4), we can get the performance lower bound of the learned optimal policy \\\\( \\\\tilde{\\\\pi} \\\\) in the reward-penalized endogenous MDP:\\n\\\\[ \\\\eta_M(\\\\tilde{\\\\pi}) \\\\geq \\\\eta_M(\\\\pi) - 2 \\\\epsilon \\\\tilde{u}(\\\\pi) \\\\]\\n\\nA.2. Proof of Theorem 3.4\\n\\nAssumption A.3. There exists an underlying function \\\\( f : S^+ \\\\times S^- \\\\rightarrow Z \\\\) satisfying that for any \\\\( s^+ 1 , s^+ 2 \\\\in S^+ , s^- 1 , s^- 2 \\\\in S^- \\\\), if \\\\( f(s^+ 1 , s^- 1) = f(s^+ 2 , s^- 2) \\\\), then \\\\( p(o|s^+ 1 , s^- 1) = p(o|s^+ 2 , s^- 2) \\\\) (5)\\n\\nThis assumption is intuitive because the block structure indicates that each context \\\\( o \\\\) uniquely determines its generating state \\\\( z \\\\). However, \\\\( z \\\\) can have multiple different partitions of \\\\( s^+ \\\\) and \\\\( s^- \\\\) based on specific semantics, such as task-related information, viewpoint, etc.\\n\\nAssumption A.4. For each \\\\( i \\\\), the distribution of action for trajectory \\\\( \\\\tau_i \\\\) is the same, i.e., there exist \\\\( p(a) \\\\) such that \\\\( E_{s^+ \\\\pi i} (a|s^+) = p(a) \\\\), \\\\( i = 1, 2, \\\\ldots, n \\\\).\\n\\nTheorem A.5. (Theorem 3.4) Consider the likelihood optimization problem on the same offline dataset \\\\( B \\\\) but with two different sampling methods. Let \\\\( B_{\\\\pi i} \\\\) be the dataset collected by the behavior policy \\\\( \\\\pi_i \\\\), where \\\\( i = 1, 2, \\\\ldots, n \\\\). \\\\( B_{\\\\pi mix} = B_{\\\\pi 1} \\\\cup B_{\\\\pi 1} \\\\cup \\\\cdots \\\\cup B_{\\\\pi n} \\\\) is the mixture of the datasets of all policies. Then we have\\n\\\\[ E_{\\\\tau \\\\in B_{\\\\pi mix}} \\\\ln p(\\\\tau) \\\\leq \\\\frac{1}{n} \\\\sum_{i=1}^{n} E_{\\\\tau \\\\in B_{\\\\pi i}} \\\\ln p(\\\\tau) \\\\]\\n\\nwhere \\\\( p(\\\\tau) \\\\) is the true density of \\\\( \\\\tau \\\\).\"}"}
{"id": "ZtOXZCTgBa", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nrespectively for convenience, i.e.,\\n\\n\\\\[\\n\\\\hat{p}_{o}, \\\\hat{p}_{a}, \\\\hat{p}_{s} +, \\\\hat{p}_{s} - = \\\\arg \\\\max_{p_{o}, p_{a}, p_{s} +, p_{s} - \\\\in H} E_{\\\\tau \\\\in B} \\\\ln p(\\\\tau) \\\\in \\\\Theta\\n\\\\]\\n\\nHere \\\\( H \\\\Theta \\\\) is the approximate space and \\\\( \\\\Theta \\\\) is the class of parameters. In practice, \\\\( H \\\\Theta \\\\) is the variational posterior space with an encoder and decoder for each conditional distribution.\\n\\nWith Assumption A.3, we know that once the distribution of \\\\( f(s_{t+1}, s_{t-1}) \\\\) is fixed, the variant of \\\\( s_{t+1} \\\\) and \\\\( s_{t-1} \\\\) has no effect on the observation \\\\( o_t \\\\). Therefore, if we want to separate sub-optimal distributions for \\\\( s_{t+1} \\\\) and \\\\( s_{t-1} \\\\) from the optimal results, the key terms in the likelihood are those involving the action \\\\( a_t \\\\). Formally, we treat the likelihood sequentially as\\n\\n\\\\[\\nE_{\\\\tau \\\\in B} \\\\ln p(\\\\tau) := T_{X_{t=1}} \\\\pi(a_{t} | s_{t} + s_{t-1}) + E_{\\\\tau \\\\in B} \\\\ln p(o_{t} | s_{t+1}, s_{t-1})\\n\\\\]\\n\\nDefine the equivalent class for \\\\( s_{t+1}, s_{t-1} \\\\) as\\n\\n\\\\[\\nA_{p} = s_{t+1} \\\\sim p_{s_{t+1}}, s_{t-1} \\\\sim p_{s_{t-1}}:\\n\\\\]\\n\\n\\\\( f(s_{t+1}, s_{t-1}) \\\\sim p_{f} \\\\).\\n\\nDifferent elements in \\\\( A_{p} \\\\) means different distributions for \\\\( s_{t+1} \\\\) and \\\\( s_{t-1} \\\\) that induce the same distribution for \\\\( f(s_{t+1}, s_{t-1}) \\\\).\\n\\nFrom Assumption A.3, we know that for those \\\\( s_{t+1}, s_{t-1} \\\\) in a certain class \\\\( A_{p} \\\\),\\n\\n\\\\[\\nE_{\\\\tau \\\\in B} \\\\ln p(o_{t} | s_{t+1}, s_{t-1})\\n\\\\]\\n\\nis only related to the distribution \\\\( p_{f} \\\\) and we denote its value as \\\\( l_{o}(p_{f}) \\\\).\\n\\nRecall the conditional entropy and mutual information entropy for \\\\( a \\\\) and \\\\( s_{t+1} \\\\) such that\\n\\n\\\\[\\nH(a) = \\\\int - \\\\ln p(a) \\\\, d\\\\mu(a),\\nH(a | s_{t+1}) = \\\\int \\\\ln \\\\frac{p(a | s_{t+1})}{\\\\pi(a | s_{t+1})} \\\\, d\\\\mu(a),\\nI(a, s_{t+1}) = H(a) - H(a | s_{t+1})\\n\\\\]\\n\\nUnder Assumption A.4, consider two different type of policy \\\\( \\\\pi_i \\\\) as the policy of \\\\( i \\\\)-th curve and \\\\( \\\\pi_{mix} \\\\) as the mixture policy of all curves, i.e.,\\n\\n\\\\[\\n\\\\pi_{mix} = \\\\frac{1}{n} \\\\sum_{n} \\\\pi_i\\n\\\\]\\n\\nWe know that\\n\\n\\\\[\\nI(a \\\\sim \\\\pi_{mix}, s_{t+1} \\\\sim p_{s_{t+1}}) = H(a) + \\\\int \\\\ln \\\\frac{1}{n} \\\\sum_{n} \\\\pi_i(a | s_{t+1}) \\\\, da \\\\leq H(a) + \\\\int \\\\ln \\\\pi_i(a | s_{t+1}) \\\\, da = \\\\frac{1}{n} \\\\sum_{n} I(a \\\\sim \\\\pi_i, s_{t+1} \\\\sim p_{s_{t+1}}).\\n\\\\]\"}"}
{"id": "ZtOXZCTgBa", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nFigure 1. The SeMOPO framework encompasses two parts: model learning and policy optimization. In the model learning phase, SeMOPO employs conservative sampling to select trajectories, which are then used to train models for endogenous and exogenous dynamics, each implemented as an ensemble of Gaussian distributions. During policy optimization, SeMOPO trains a policy $\\\\pi_{\\\\theta}(a_t|s_t + t)$ and a value model $V_{\\\\theta}(s_t + t)$ based on the endogenous states generated by a sampled endogenous dynamics model $\\\\tilde{T}_j$. SeMOPO uses the reward penalized by the variance of the endogenous dynamics models' predictions to train the value model.\\n\\nTheorem 3.4. Consider the likelihood optimization problem on the same offline dataset $B$ but with two different sampling methods. Let $B_{\\\\pi_i}$ be the dataset collected by the behavior policy $\\\\pi_i$, where $i = 1, 2, \\\\ldots, n$. $B_{\\\\pi_{mix}} = B_{\\\\pi_1} \\\\cup B_{\\\\pi_2} \\\\cup \\\\cdots \\\\cup B_{\\\\pi_n}$ is the mixture of the datasets of all policies. Then we have\\n\\n$$E_{\\\\tau \\\\in B_{\\\\pi_{mix}}} \\\\ln p(\\\\tau) \\\\leq \\\\frac{1}{n} \\\\sum_{i=1}^{n} E_{\\\\tau \\\\in B_{\\\\pi_i}} \\\\ln p(\\\\tau),$$\\n\\nwhere $p(\\\\tau)$ is the true density of $\\\\tau$.\\n\\nTheorem 3.4 suggests that the total likelihood estimated on the sampled offline trajectory of a certain policy is larger than on the mixture dataset of all policies. The proof is in Appendix A.2. Maximizing the likelihood will make the estimated distribution toward the true distribution in Equation (1), which helps distinguish the endogenous and exogenous transitions. Notably, Theorem 3.4 does not guarantee that we can obtain the maximum likelihood via the conservative sampling method. Even so, we empirically show that the model can separate the task-relevant and irrelevant components well compared to the random sampling method.\\n\\n3.3. Practical Implementation of SeMOPO\\n\\nBased on the above analysis, we present a practical implementation of Separated Model-based Offline Policy Optimization. The overall method of SeMOPO is shown in Figure 1 and summarized in Algorithm 1.\\n\\nSeparated Model Learning. Theorem 3.3 allows us to achieve a significantly improved lower bound of the total return if we can learn a separate model and train the policy in the endogenous state space. By bifurcating the latent state $z$ into endogenous $s_t + t$ and exogenous $s_t - t$ parts, we obtain the following Evidence Lower Bound (ELBO):\\n\\n$$\\\\max_{\\\\theta} E_{T_{t=1}} \\\\ln U_\\\\theta(o_t|s_t + t, s_t - t) - D_{\\\\text{KL}} \\\\bar{q}_\\\\theta(s_t - t|o_\\\\leq t) || \\\\tilde{T}_\\\\theta(s_t - t|s_t - t - 1) - D_{\\\\text{KL}} \\\\tilde{q}_\\\\theta(s_t + t|o_\\\\leq t, a_{<t}) || e_{T_{\\\\theta}}(s_t + t|s_{t+1}, a_{t-1}).$$\\n\\nwhere $\\\\tilde{q}_\\\\theta$ and $\\\\bar{q}_\\\\theta$ represent the inference models for the endogenous and exogenous states, respectively. Likewise, $e_T$ and $T$ denote the corresponding transition dynamics models, and $U_\\\\theta$ is the observation model which reconstructs the observation jointly from the endogenous and exogenous states. The derivation of the ELBO is detailed in Appendix B, with the implementation specifics of each model described in Appendix D.\\n\\nFor the conservative sampling strategy outlined in Section 3.2, we design a simple but effective implementation. In the $m$-th training epoch, the SeMOPO's model is only trained on the sampled trajectory $\\\\tau_j$ generated by a certain policy, where $j \\\\leq \\\\min(m, n)$ and $n$ is the number of trajectories in the offline dataset. We provide an intuitive interpretation for this implementation: it forces the model to separate the task-relevant and irrelevant dynamics in the early training stages ($m \\\\leq n$); as training progresses ($m > n$), the model is trained on the trajectories sampled from the entire dataset to enhance the coverage of transitions.\"}"}
{"id": "ZtOXZCTgBa", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1.\\nNormalized test returns of SeMOPO and compared baselines on the LQV-D4RL benchmark. Mean scores (higher is better) with standard deviation are recorded across 4 seeds for each task. The original returns are shown in Table 5.\\n\\n| Dataset      | SeMOPO | Offline DV2 | LOMPO | DrQ+BC | DrQ+CQL | BC | InfoGating |\\n|--------------|--------|-------------|-------|--------|---------|----|------------|\\n| Walker Walk  |\\n| random       | 0.77 \u00b1 0.06 | 0.27 \u00b1 0.05 | 0.22 \u00b1 0.06 | 0.04 \u00b1 0.00 | 0.04 \u00b1 0.00 | 0.07 \u00b1 0.00 |\\n| mediarep     | 0.87 \u00b1 0.06 | 0.29 \u00b1 0.04 | 0.36 \u00b1 0.11 | 0.04 \u00b1 0.01 | 0.03 \u00b1 0.01 | 0.09 \u00b1 0.03 |\\n| medium       | 0.45 \u00b1 0.07 | 0.11 \u00b1 0.04 | 0.10 \u00b1 0.02 | 0.65 \u00b1 0.06 | 0.03 \u00b1 0.01 | 0.69 \u00b1 0.05 |\\n| Cheetah Run  |\\n| random       | 0.63 \u00b1 0.07 | 0.10 \u00b1 0.03 | 0.16 \u00b1 0.04 | 0.23 \u00b1 0.08 | 0.00 \u00b1 0.00 | 0.14 \u00b1 0.03 |\\n| mediarep     | 0.64 \u00b1 0.07 | 0.16 \u00b1 0.07 | 0.19 \u00b1 0.08 | 0.41 \u00b1 0.23 | 0.00 \u00b1 0.00 | 0.05 \u00b1 0.00 |\\n| medium       | 0.73 \u00b1 0.08 | 0.20 \u00b1 0.14 | 0.13 \u00b1 0.09 | 0.64 \u00b1 0.07 | 0.00 \u00b1 0.00 | 0.62 \u00b1 0.06 |\\n| Hopper Hop   |\\n| random       | 0.68 \u00b1 0.06 | 0.00 \u00b1 0.00 | 0.00 \u00b1 0.00 | 0.00 \u00b1 0.00 | 0.08 \u00b1 0.10 | 0.06 \u00b1 0.08 |\\n| mediarep     | 0.91 \u00b1 0.07 | 0.00 \u00b1 0.00 | 0.00 \u00b1 0.00 | 0.25 \u00b1 0.18 | 0.00 \u00b1 0.00 | 0.04 \u00b1 0.02 |\\n| medium       | 1.24 \u00b1 0.16 | 0.02 \u00b1 0.05 | 0.01 \u00b1 0.04 | 0.81 \u00b1 0.19 | 0.00 \u00b1 0.00 | 0.42 \u00b1 0.07 |\\n| Humanoid Walk|\\n| random       | 0.01 \u00b1 0.00 | 0.00 \u00b1 0.00 | 0.00 \u00b1 0.00 | 0.00 \u00b1 0.00 | 0.00 \u00b1 0.00 | 0.00 \u00b1 0.00 |\\n| mediarep     | 0.01 \u00b1 0.01 | 0.00 \u00b1 0.00 | 0.00 \u00b1 0.00 | 0.00 \u00b1 0.00 | 0.00 \u00b1 0.00 | 0.02 \u00b1 0.01 |\\n| medium       | 0.01 \u00b1 0.01 | 0.01 \u00b1 0.00 | 0.00 \u00b1 0.00 | 0.00 \u00b1 0.02 | 0.00 \u00b1 0.00 | 0.01 \u00b1 0.00 |\\n| Car Racing   |\\n| random       | 0.93 \u00b1 0.16 | 0.51 \u00b1 0.18 | 0.86 \u00b1 0.18 | -0.05 \u00b1 0.05 | -0.23 \u00b1 0.02 | -0.15 \u00b1 0.01 |\\n| mediarep     | 0.80 \u00b1 0.17 | 0.39 \u00b1 0.18 | 0.72 \u00b1 0.40 | -0.18 \u00b1 0.01 | -0.23 \u00b1 0.02 | -0.21 \u00b1 0.02 |\\n| medium       | 0.90 \u00b1 0.34 | 0.62 \u00b1 0.32 | 0.69 \u00b1 0.22 | -0.21 \u00b1 0.02 | -0.18 \u00b1 0.00 | -0.18 \u00b1 0.01 |\"}"}
{"id": "ZtOXZCTgBa", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nBoth of these two methods show great performance in offline visual RL tasks. In model-free approaches, Behavioral Cloning (BC) (Bain & Sammut, 1995; Bratko et al., 1995) learns by imitating the behavior of the policy that collected the data, and Conservative Q-Learning (CQL) (Kumar et al., 2020) samples actions from a broad distribution while penalizing those that fall outside the support region of the offline data. Remarkably, both BC and CQL are not originally tailored for scenarios involving high-dimensional image inputs. To address this, DrQ-v2's regularization techniques (Yarats et al., 2022) are applied to BC and CQL in (Lu et al., 2022), creating DrQ+BC and DrQ+CQL methods. These modified methods, alongside the original BC, are compared in our study to evaluate their effectiveness in offline visual RL tasks with low-quality datasets. Additionally, we compare the offline version of the InfoGating method (Tomar et al., 2024), a visual reinforcement learning approach that removes task-irrelevant noise by minimizing the information required for the task. We run all experiments on four seeds and report the normalized test return after training. The normalized return is obtained by normalizing the original return with the maximum and minimum values of the three levels of datasets for each task. The detailed calculation procedure is in Appendix D.2.\\n\\n4.1. Evaluation on the LQV-D4RL Benchmark\\n\\nWe evaluate SeMOPO against various baselines in nine scenarios within the LQV-D4RL benchmark. The results in Table 1 consistently demonstrate SeMOPO's superior performance across diverse datasets, confirming the effectiveness of uncertainty estimation in the endogenous state space, especially in low-quality visual datasets. Significantly, SeMOPO outperforms model-free approaches in nearly all environments. However, BC-based approaches also perform well when the behavior policy of the dataset is reasonably effective. Model-based methods, Offline DV2 and LOMPO, show improved results when trained on random and medium replay datasets compared to medium datasets. This indicates that trajectories with random behaviors may provide a wider range of transitions, which benefits mitigating the distribution shift problem. This phenomenon aligns with previous research (Jin et al., 2020; Rashidinejad et al., 2021; Kumar et al., 2022) and holds even in environments with complex noise in image inputs. Moreover, our findings highlight that the DrQ+BC and BC methods outperform Offline DV2 and LOMPO in several settings. This implies that inaccuracies in model uncertainty estimations can lead to worse performance of model-based methods than direct policy imitation. SeMOPO and all baseline methods have failed in the humanoid walk task. The visual reconstruction on our project website demonstrates that SeMOPO can effectively extract task information. Thus, task failure may be attributed to the inherently complex nature of the humanoid task, which involves controlling a high-dimensional action space of up to 21 dimensions. Particularly in our experiments, the challenge is further compounded by the high-dimensional image inputs and complex moving distractors in the background, significantly increasing learning difficulty. We anticipate that future researchers will focus on learning a high-performance policy for this task within noisy visual inputs. In addition, we assess these methods on the V-D4RL benchmark, as detailed in Appendix F.2, where observations do not include distractors.\\n\\n4.2. Can SeMOPO give a reasonable model uncertainty estimation?\\n\\nTo answer the question, we compare the model uncertainty of SeMOPO and Offline DV2 on randomly selected states, as shown in Figure 2. We find that SeMOPO exhibits lower model uncertainty than Offline DV2 across nine datasets, confirming the conclusion of Theorem 3.3. Given that SeMOPO and Offline DV2 employ the same method for uncertainty calculation, the reduced uncertainty observed in SeMOPO can be ascribed to the endogenous state transition it introduced. We also show the uncertainty of exogenous state transitions in Appendix F.1.\"}"}
{"id": "ZtOXZCTgBa", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nFigure 4. The performance comparison for ablated methods. Each row represents the comparative performance probabilities, complete with 95% bootstrap confidence intervals, suggesting that Algorithm X is superior to Algorithm Y (Agarwal et al., 2021). These probabilities are derived from 50 runs of 4 seeds for every task to ensure robustness in the evaluation. We show the aggregated results for all nine tasks.\\n\\n4.3. How does the sampling strategy for policy data affect the separated model training?\\n\\nTo validate the efficacy of our proposed sampling method for training the separated model, we conduct a comparison involving several ablated methods: 1) Random sampling of all policy trajectories (Se + RS); 2) Random sampling combined with additional dissociated prediction loss to hinder predicting the true reward from the exogenous state (Se + RS + DRP), as used in (Fu et al., 2021); 3) Our conservative sampling method (Se + CS (SeMOPO)). The \u201cSe\u201d indicates the application of these methods to the training of the separated model. We also compare these with the original Offline DV2 method trained under the random sampling (Offline DV2 + RS).\\n\\n1 More information about dissociated reward prediction loss is in Appendix D.3.\\n\\nAs illustrated in Figure 4, SeMOPO demonstrates superior performances over other ablated methods, with a high probability (> 0.8). There is no significant difference in performance between Se+RS and Se+RS+DRP (\u2248 0.4), suggesting that the model trained by dissociated reward prediction may not benefit offline policy learning. Both methods, however, outperform the original Offline DV2, indicating the effectiveness of the separated model in addressing offline visual RL challenges with noisy observations.\\n\\nTo further analyze these sampling strategies, we visualize the observation reconstruction of the endogenous state for each training method in Figure 5. The model trained with random sampling does not guarantee comprehensive task-related information in the endogenous state, especially in the medium replay dataset. While adding dissociated reward prediction loss to random sampling enriches task information in the endogenous state, it also introduces considerable distraction. Our conservative sampling method effectively reconstructs task-related information, ensuring the endogenous state contains only task-relevant details.\\n\\nTo corroborate the theoretical claims made in Section 3.2, we compute the action entropy for the first 30 epochs using both RS and CS methods across all nine datasets. As shown in Figure 6, the CS method consistently results in lower action entropy compared to RS across different environments and datasets. Lower action entropy implies more deterministic policy behavior, aiding in distinguishing endogenous states from exogenous noise. Due to little action variations among different policies in the medium dataset, the difference in action entropy between RS and CS is less than the random and medium replay datasets. It explains why SeMOPO shows a greater advantage in the latter datasets, as described in Table 1.\\n\\n7\"}"}
{"id": "ZtOXZCTgBa", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Then we also have\\n\\n$$s - \\\\text{condition on } C$$\\n\\nBesides, if we can select a subset $t > \\\\text{mix}$, similarly define that\\n\\n$$\\\\pi(s \\\\sim t) = \\\\pi(s \\\\sim a)$$\\n\\nFor $a \\\\sim \\\\text{mix}$, $\\\\pi(s \\\\sim t)$\\n\\n$X_i = 1$\\\\quad $I = \\\\#(C, \\\\pi(s))$\\n\\nis fixed. So we know that\\n\\n$$E(\\\\tau | \\\\pi(s)) = 1 \\\\quad \\\\text{and we know that } \\\\pi(s) \\\\sim \\\\text{mix}$$\\n\\nSo\\n\\n$$\\\\pi(s \\\\sim t, a) = \\\\pi(s \\\\sim a)$$\\n\\nis also fixed. So we know that\\n\\n$$\\\\pi(s \\\\sim t, a) = \\\\pi(s \\\\sim a)$$\\n\\n$X_i = 1$\\n\\n$\\\\pi(s \\\\sim t, a) = \\\\pi(s \\\\sim a)$\\n\\nLearning High-quality Model and Policy from Low-quality Offline Visual Datasets\"}"}
{"id": "ZtOXZCTgBa", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Derivation\\n\\nLikelihood. For the trajectory $\\\\tau_i = \\\\{o_1, a_1, \\\\ldots, o_T, a_T\\\\}$, the log-likelihood can be expressed as:\\n\\n$$\\\\ln p(\\\\tau_i) = \\\\ln p(o_0, a_0, \\\\ldots, o_T, a_T) = \\\\ln \\\\sum_{t=1}^T p(o_t|z_t) p(a_t|z_t) p(z_t|z_{t-1}, a_{t-1})$$\\n\\nThe third equation comes from the decomposition of endogenous and exogenous dynamics, as assumed in the EX-BMDP framework.\\n\\nOne-step predictive distribution. The variational bound for latent dynamics models $p(o_1:T, z_1:T|a_1:T)$ is $Q_t p(s_{t+1}|s_{t+1}^{t-1}, a_{t-1}) p(s_t|s_t^{t-1}) p(o_t|z_t)$ and a variational posterior $q(z_1:T|o_1:T, a_1:T)$ follows from importance weighting and Jensen's inequality as shown:\\n\\n$$\\\\ln p(o_1:T|a_1:T) \\\\equiv \\\\ln \\\\mathbb{E}_{p(z_1:T|a_1:T)} \\\\left[ \\\\sum_{t=1}^T p(o_t|z_t) \\\\right] \\\\geq \\\\mathbb{E}_{q(z_1:T|o_1:T, a_1:T)} \\\\left[ \\\\sum_{t=1}^T \\\\ln p(o_t|z_t) \\\\right] - \\\\ln q(s_{t+1}|o_{t}^{t-1}, a_{t}^{t-1}) - \\\\ln q(s_t|o_{t}^{t-1})$$\\n\\nC. The LQV-D4RL Benchmark\\n\\nTo evaluate the performance of visual RL methods with offline datasets containing noisy observations, we introduce a benchmark named Low-Quality Vision Datasets for Deep Data-Driven RL (LQV-D4RL). This benchmark comprises four typical environments from the DeepMind Control Suite and one environment from Gym:\\n\\n- Walker Walk: A bipedal agent is trained to first stand and then walk forward as efficiently as possible.\\n- Cheetah Run: A cheetah-like bipedal model aims to run at high speeds on a straight track.\\n- Hopper Hop: The agent, with a single-legged body, must balance and hop forward, focusing on agility and stability.\"}"}
{"id": "ZtOXZCTgBa", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\n- **Humanoid Walk**: A simplified humanoid with 21 joints, aims to walk stably, which is extremely difficult with many local minima.\\n- **Car Racing**: A highly challenging racing game where players must pass through checkpoints to score points. The faster they reach the finish line within a set time, the higher their score. The observations contain numerous distractors.\\n\\nEach task is represented across three different levels of policy performance:\\n\\n- **Random**: Trajectories are collected by randomly initialized policies.\\n- **Medium Replay (medrep)**: Trajectories are drawn from the replay buffer accumulated during training of a medium-performance policy.\\n- **Medium**: Trajectories are collected by a fixed policy of medium performance.\\n\\nFor each locomotion task's observations, the backgrounds are replaced with videos from the \u201cdriving car\u201d category of the Kinetics dataset (Kay et al., 2017), as utilized in DBC (Zhang et al., 2021). To simulate real data collection processes in natural settings, we train policies using the TIA approach (Fu et al., 2021) and then collect trajectories based on image observations with the aforementioned distractors. The \u201crandom\u201d and \u201cmedium\u201d datasets each contain 200 trajectories, while \u201cmedium replay\u201d comprises 400 trajectories, with each trajectory being 1000 steps long. Specific statistical details of the LQV-D4RL benchmark are reported in Table 3. We upload the dataset in the supplementary materials.\\n\\n### Table 3. Full summary statistics of per-episode return in the LQV-D4RL benchmark.\\n\\n| Dataset     | Episodes | Mean  | Std   | Min   | P25   | Median | P75   | Max   |\\n|-------------|----------|-------|-------|-------|-------|--------|-------|-------|\\n| **Walker Walk** |          |       |       |       |       |        |       |       |\\n| Random      | 200      | 86.6  | 48.1  | 5.9   | 51.9  | 70.8   | 128.3 | 199.1 |\\n| Medrep      | 400      | 106.6 | 79.7  | 5.0   | 45.4  | 81.3   | 148.8 | 398.2 |\\n| Medium      | 200      | 513.1 | 54.8  | 401.5 | 471.4 | 516.0  | 559.1 | 598.0 |\\n| **Cheetah Run** |          |       |       |       |       |        |       |       |\\n| Random      | 200      | 77.6  | 57.2  | 3.1   | 14.5  | 71.6   | 118.8 | 198.2 |\\n| Medrep      | 400      | 145.4 | 113.3 | 1.7   | 42.9  | 125.1  | 232.6 | 396.5 |\\n| Medium      | 200      | 350.3 | 30.8  | 300.5 | 322.2 | 352.5  | 376.0 | 403.2 |\\n| **Hopper Hop** |          |       |       |       |       |        |       |       |\\n| Random      | 200      | 2.1   | 4.9   | 0.0   | 0.0   | 0.0    | 0.1   | 19.5  |\\n| Medrep      | 400      | 4.7   | 9.7   | 0.0   | 0.0   | 0.0    | 3.7   | 39.9  |\\n| Medium      | 200      | 62.0  | 13.9  | 40.3  | 49.8  | 60.8   | 74.5  | 84.9  |\\n| **Humanoid Walk** |          |       |       |       |       |        |       |       |\\n| Random      | 200      | 1.1   | 0.8   | 0.0   | 0.5   | 1.0    | 1.5   | 5.7   |\\n| Medrep      | 400      | 95.6  | 114.3 | 0.0   | 1.4   | 5.4    | 202.8 | 359.0 |\\n| Medium      | 200      | 573.0 | 16.8  | 526.5 | 560.6 | 572.9  | 584.9 | 609.4 |\\n| **Car Racing** |          |       |       |       |       |        |       |       |\\n| Random      | 200      | 10.3  | 65.5  | -82.0 | -43.3 | -6.5   | 59.8  | 149.2 |\\n| Medrep      | 400      | 76.1  | 116.3 | -82.0 | -27.5 | 54.1   | 181.5 | 297.3 |\\n| Medium      | 200      | 372.3 | 42.7  | 302.3 | 335.5 | 370.3  | 408.3 | 449.9 |\\n\\nD. Implementation Details\\n\\n**D.1. Networks**\\nWe implement the proposed algorithm using TensorFlow 2 and conduct all experiments on an NVIDIA RTX 3090, totaling approximately 1000 GPU hours. The recurrent state-space model from DreamerV2 (Hafner et al., 2021) is employed for both forward dynamics and the posterior encoder. The hidden sizes of deterministic and stochastic parts of the model are 200 and 32, respectively. For learning an ensemble of forward dynamics, multiple MLP networks are utilized, each outputting the mean and standard deviation of the next state. The hidden size for each MLP is 1024. The reward predictor comprises 4 MLP layers, each of size 400. We use the convolutional encoder and decoder from TIA (Fu et al., 2021). All dense layers have a size of 400, and the activation function used is ELU. The ADAM optimizer is employed to train the network with batches of 64 sequences, each of length 50. The learning rate is 6e-5 for both the endogenous and exogenous models and 8e-5 for the action and value nets. We stabilize the training process by clipping gradient norms to 100 and set $\\\\lambda = 10$. \\n\\n...\"}"}
{"id": "ZtOXZCTgBa", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nfor the uncertainty penalty term. The imagine horizon of 5, as used in Offline DV2 (Lu et al., 2022), is adopted for policy optimization. We train a model comprising the dynamics and reward predictor for 25000 epochs using offline visual datasets, followed by policy training within the model for 100000 steps. The codes and datasets are contained in the supplementary materials.\\n\\nD.2. Evaluation Metric\\n\\nTo compare the performance of different methods, we define the normalized return based on the statistics of the offline dataset as follows:\\n\\n$$S_{\\\\text{normalize}} = \\\\frac{S_{\\\\text{score}} - S_{\\\\text{min}}}{S_{\\\\text{max}} - S_{\\\\text{min}}}$$\\n\\nHere, $S_{\\\\text{score}}$ represents the original return obtained by the test method, while $S_{\\\\text{min}}$ and $S_{\\\\text{max}}$ denote the minimum and maximum episodic returns of the dataset for each task across three levels (random, medium, large).\\n\\nD.3. Dissociated Reward Prediction\\n\\nThe Reward Dissociation used in TIA (Fu et al., 2021) for the exogenous model is achieved through the adversarial objective $J_{t \\\\text{Radv}}$.\\n\\n$$J_{t \\\\text{Radv}} = -\\\\lambda_{\\\\text{Radv}} \\\\max q \\\\ln q(r_t | s_{t-1})$$\\n\\nwhere $\\\\lambda_{\\\\text{Radv}}$ for Walker Walk, Cheetah Run, and Hopper Hop are 20000, 20000, and 30000, respectively. This setup involves a minimax strategy, wherein the training of the exogenous model's reward prediction head is interleaved with the exogenous model's training, occurring for multiple iterations per training step. The reward prediction head is trained to minimize the reward prediction loss, represented by $-\\\\ln q(r_t | s_{t-1})$. In contrast, the exogenous model aims to maximize this objective to prevent reward-correlated information from influencing its learned features, as outlined by Ganin & Lempitsky. At the same time, TIA optimizes the endogenous model to maximize the log-likelihood of predicting rewards from endogenous states via the objective $J_{t \\\\text{R}} = \\\\ln q(r_t | s_{t+1})$.\\n\\nThe reward prediction loss is calculated using $\\\\ln N(r_t; \\\\hat{r}_t, 1)$, where $N(\\\\cdot; \\\\mu, \\\\sigma^2)$ denotes the Gaussian likelihood and $\\\\hat{r}_t$ represents the predicted reward. Notably, neither loss is used to update the endogenous and exogenous models in SeMOPO. The reward prediction loss only updates the reward predictors by stopping the backward gradients to the endogenous states.\\n\\nE. Algorithm of SeMOPO\\n\\nThe pseudo-code of our proposed SeMOPO is provided in Algorithm 1.\\n\\n**Algorithm 1**\\n\\nTraining Procedure of SeMOPO\\n\\n**Input:** Offline datasets $B$\\n\\nInitialize forward dynamics model $e_T \\\\theta$, $\\\\bar{T} \\\\theta$, posterior encoder $\\\\tilde{q} \\\\theta$, $\\\\bar{q} \\\\theta$, observation decoder $e_U \\\\theta$, reward predictor $R \\\\theta$, policy $\\\\pi \\\\theta$, value model $V \\\\theta$.\\n\\n// Offline model training\\n\\nfor each training epoch $m = 1, \\\\ldots, M$\\ndo\\n\\nSample minibatch $(o_1:T, a_1:T-1, r_1:T)$ from the dataset $B$ via Conservative Sampling\\n\\nUpdate the forward dynamics model $e_T \\\\theta$, $\\\\bar{T} \\\\theta$ and the posterior encoder $\\\\tilde{q} \\\\theta$, $\\\\bar{q} \\\\theta$\\n\\nUpdate the observation decoder $U \\\\theta$ and the reward predictor $R \\\\theta$\\n\\nend for\\n\\n// Policy Optimization\\n\\nfor training iteration $i = 1, \\\\ldots, I_t$\\ndo\\n\\nImagine the endogenous latent states $s_{t+1: H}$ by policy $\\\\pi$ using the endogenous state model $e_T \\\\theta$\\n\\nEstimate the endogenous model uncertainty by the model disagreement\\n\\nObtain the penalized reward $\\\\tilde{r}$ with estimated uncertainty $\\\\tilde{u}$ via Equation (2)\\n\\nTrain the policy $\\\\pi \\\\theta$ and value model $V \\\\theta$ on the data $(s_{t+1: H-1}, a_{1: H-1}, \\\\tilde{r}_{1: H-1})$\\n\\nend for\"}"}
{"id": "ZtOXZCTgBa", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nFigure 6. Action entropy of sampled data by random sampling (RS) and conservative sampling (CS). We record values of the first 30 training epochs across nine datasets of LQV-D4RL. The conservative sampling yields data with significantly lower action entropy than random sampling.\\n\\nTable 2. Normalized test returns on tasks with different online distractions from offline datasets.\\n\\n| Dataset | Offline DV2 | DrQ+BC |\\n|---------|-------------|--------|\\n| Walker Walk random | 0.78 \u00b1 0.14 | 0.33 \u00b1 0.05 |\\n| medrep | 0.50 \u00b1 0.02 | 0.30 \u00b1 0.05 |\\n\\n| Cheetah Run random | 0.65 \u00b1 0.05 | 0.16 \u00b1 0.04 |\\n| medrep | 0.59 \u00b1 0.06 | 0.17 \u00b1 0.06 |\\n\\n4.4. Can SeMOPO generalize to online environments with different distractors?\\n\\nIn this section, we evaluate the effectiveness of SeMOPO in addressing the \u201coffline-to-online distraction gap\u201d challenge. We alter the online testing environment by introducing an RGB background, starkly contrasting with the grayscale background of the LQV-D4RL dataset. To assess the methods' adaptability to distinct online visual distractions, we compared SeMOPO with Offline DV2 and DrQ+BC in two distinct environments: Walker Walk and Cheetah Run, each with two datasets - \u201crandom\u201d and \u201cmedrep\u201d. As shown in Table 2, Offline DV2 performs better than DrQ+BC in the Walker Walk task, but the reverse is true for the Cheetah Run task. However, both are outperformed by SeMOPO, demonstrating SeMOPO's clear advantage in managing the distraction gap.\\n\\n5. Related Work\\n\\nOffline RL. Offline RL (Lange et al., 2012; Levine et al., 2020) aims to learn a policy from an offline dataset of trajectories for deployment in an online environment. It has been applied to various domains, such as robotic grasping (Kalashnikov et al., 2018; 2021), healthcare (Shortreed et al., 2010; Wang et al., 2018), and autonomous driving (Iroegbu & Madhavi, 2021; Diehl et al., 2023). The primary challenge in offline RL is the distribution shift (Fujimoto et al., 2018; Kumar et al., 2019), where the behavioral policy's data diverges from the data distribution in the actual online environment. Methods to address distribution shift broadly fall into two categories: (1) policy-constraints methods that restrict the learned policy to align closely with the behavior policy generating the dataset (Fujimoto et al., 2018; Kumar et al., 2019; Liu et al., 2019; Nachum et al., 2019; Peng et al., 2019; Siegel et al., 2020; Fujimoto & Gu, 2021), thus avoiding unexpected actions; (2) conservative methods (Kumar et al., 2020; Kidambi et al., 2020; Kostrikov et al., 2021; Yu et al., 2020) that construct a conservative return estimate, enhancing the learned policy's robustness against distribution shift. Previous studies have shown offline RL's efficacy even with random or sub-optimal datasets (Jin et al., 2020; Zanette, 2020; Wang et al., 2020; Rashidinejad et al., 2021). However, there is limited research on the performance of offline RL in scenarios where the observation data is noisy, particularly when learning from high-dimensional image inputs with complex distractors. Our work focuses on learning a policy from such low-quality offline visual RL datasets to effectively tackle related tasks.\\n\\nControl with Noisy Observations. Real-world control tasks often involve observations that contain irrelevant noise. Recent studies that focused on learning policies from noisy image observations, can be categorized into four types: (1) separating task-relevant and irrelevant information based on key factors (actions, rewards, etc.) (Fu et al., 2021; Wang et al., 2022; Pan et al., 2022; Liu et al., 2023b); (2) learning task-related representations through bisimulation metrics (Zhang et al., 2021; Liu et al., 2023a); (3) mitigating exogenous noises via data augmentation methods (Kostrikov et al., 2020; Hansen et al., 2021; Fan & Li, 2021; Yuan et al., 2022; Bertoin et al., 2022; Huang et al., 2022); (4) extracting task-related features using auxiliary prediction tasks (Yang et al., 2015; Badia et al., 2020; Baker et al., 2022; Efroni et al., 2022; Lamb et al., 2022). These methods primarily target training policies in online environments, where additional data can be gathered to differentiate between task-relevant and irrelevant information. However, this issue has been less explored in offline visual RL. The V-D4RL benchmark dataset (Lu et al., 2022) involves only two related subtasks and does not offer a specific approach for handling noisy observations. Our method distinctively trains a separated state-space model from offline noisy data using a conservative sampling method and learns a high-performance policy.\\n\\nSampling Strategy in Offline RL. Sampling strategies in offline RL aim to improve the performance of the learning agent by optimally selecting data from the dataset. One approach involves using uncertainty estimation of the Q-Value function to guide sampling (Kumar & Kuzovkin, 2022), which allows for a better exploration-exploitation balance. Another strategy is Offline Prioritized Experience Replay (OPER) (Hong et al., 2023), which assigns weights to transitions based on their importance, enhancing the learning process.\\n\\nTable 3. Normalized test returns on tasks with different online distractions from offline datasets.\\n\\n| Dataset | Offline DV2 | DrQ+BC |\\n|---------|-------------|--------|\\n| Walker Walk random | 0.78 \u00b1 0.14 | 0.33 \u00b1 0.05 |\\n| medrep | 0.50 \u00b1 0.02 | 0.30 \u00b1 0.05 |\\n\\n| Cheetah Run random | 0.65 \u00b1 0.05 | 0.16 \u00b1 0.04 |\\n| medrep | 0.59 \u00b1 0.06 | 0.17 \u00b1 0.06 |\"}"}
{"id": "ZtOXZCTgBa", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nRank-Based Sampling (RBS) (Shen et al., 2021) adopts a similar way, sampling transitions with high returns more frequently. Policy Regularization with Dataset Constraint (PRDC) (Ran et al., 2023) limits the policy towards the closest state-action pair in the dataset, avoiding out-of-distribution actions. To address the imbalance dataset issue, RB-CQL (Jiang et al., 2023) utilizes a retrieval process to use related experiences effectively. Unlike these methods, which directly target policy performance improvement, our proposed conservative sampling strategy focuses on helping the model distinguish between task-relevant and irrelevant information.\\n\\n6. Conclusion\\n\\nIn this paper, we consider the problem of learning a policy from offline visual datasets where the observations contain non-trivial distractors and the behavior policies that generate the dataset are either random or sub-optimal. To simulate this scenario, we construct the LQV-D4RL benchmark. We provide a theoretical analysis of the lower performance bound under the EX-BMDP assumption which is tighter than that under the POMDP in some specific cases. We propose a conservative sampling method to facilitate the decomposition of endogenous and exogenous states. Guided by this theoretical framework, we introduce the Separated Model-based Offline Policy Optimization (SeMOPO) method. Experimental results on the LQV-D4RL dataset indicate that SeMOPO outperforms other offline visual RL methods. Further experiments validate the theory's applicability and highlight the significance of each component within our method. Generalization experiments show SeMOPO's ability to handle variations between online and offline environmental disturbances.\\n\\nLimitations and Future Work.\\n\\nOur study adopts the independence assumption under the EX-BMDP for the transitions of endogenous and exogenous states. However, potential interactions between these states in real-world tasks may present a promising avenue for future research.\\n\\nImpact Statement\\n\\nThis paper presents work that aims to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.\\n\\nAcknowledgements\\n\\nWe thank Han-Jia Ye, Shaowei Zhang, Minghao Shao, Hai-Hang Sun, Kaichen Huang, Yucen Wang, and Jiayi Wu for their valuable discussions. This work was partially supported by the National Science and Technology Major Project under Grant No. 2022ZD0114805, Collaborative Innovation Center of Novel Software Technology and Industrialization, NSFC (62376118, 62006112, 61921006), the Postgraduate Research & Practice Innovation Program of Jiangsu Province, and the National Natural Science Foundation of China (No. 123B2009).\\n\\nReferences\\n\\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare, M. G. Deep reinforcement learning at the edge of the statistical precipice. In Neural Information Processing Systems, 2021.\\n\\nBadia, A. P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi, A., Guo, D., and Blundell, C. Agent57: Outperforming the atari human benchmark. In International Conference on Machine Learning, 2020.\\n\\nBain, M. and Sammut, C. A framework for behavioural cloning. In Machine Intelligence 15, Intelligent Agents [St. Catherine's College, Oxford, UK, July 1995], pp. 103\u2013129. Oxford University Press, 1995.\\n\\nBaker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., and Clune, J. Video pretraining (vpt): Learning to act by watching unlabeled online videos. ArXiv, abs/2206.11795, 2022.\\n\\nBertoin, D., Zouitine, A., Zouitine, M., and Rachelson, E. Look where you look! saliency-guided q-networks for visual rl tasks. ArXiv, abs/2209.09203, 2022.\\n\\nBratko, I., Urbancic, T., and Sammut, C. Behavioural cloning: Phenomena, results and problems. IFAC Proceedings Volumes, 28:143\u2013149, 1995.\\n\\nBrockman, G., Cheung, V ., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. OpenAI gym. ArXiv, abs/1606.01540, 2016.\\n\\nDiehl, C. E., Sievernich, T., Kr\u00fcger, M., Hoffmann, F., and Bertram, T. Uncertainty-aware model-based offline reinforcement learning for automated driving. IEEE robotics and automation letters, 2023. doi: 10.1109/LRA.2023.3236579.\\n\\nDu, S. S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dud\u0131k, M., and Langford, J. Provably efficient rl with rich observations via latent state decoding. ArXiv, abs/1901.09018, 2019.\\n\\nEfroni, Y ., Misra, D., Krishnamurthy, A., Agarwal, A., and Langford, J. Provable RL with exogenous distractors via multistep inverse dynamics. ArXiv, abs/2110.08847, 2021.\"}"}
{"id": "ZtOXZCTgBa", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "ZtOXZCTgBa", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nKumar, A., Zhou, A., Tucker, G., and Levine, S. Conservative q-learning for offline reinforcement learning. ArXiv, abs/2006.04779, 2020.\\n\\nKumar, A., Hong, J., Singh, A., and Levine, S. When should we prefer offline reinforcement learning over behavioral cloning? ArXiv, abs/2204.05618, 2022.\\n\\nLamb, A., Islam, R., Efroni, Y., Didolkar, A., Misra, D. K., Foster, D. J., Molu, L., Chari, R., Krishnamurthy, A., and Langford, J. Guaranteed discovery of control-endogenous latent states with multi-step inverse models. Trans. Mach. Learn. Res., 2023, 2022.\\n\\nLange, S., Gabel, T., and Riedmiller, M. Batch reinforcement learning. In Reinforcement learning: State-of-the-art, pp. 45\u201373. Springer, 2012.\\n\\nLevine, S., Kumar, A., Tucker, G., and Fu, J. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. ArXiv, abs/2005.01643, 2020.\\n\\nLiu, Q., Zhou, Q., Yang, R., and Wang, J. Robust representation learning by clustering with bisimulation metrics for visual reinforcement learning with distractions. In AAAI Conference on Artificial Intelligence, 2023a.\\n\\nLiu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. Off-policy policy gradient with stationary distribution correction. ArXiv, abs/1904.08473, 2019.\\n\\nLiu, Y.-R., Huang, B., Zhu, Z., Tian, H., Gong, M., Yu, Y., and Zhang, K. Learning world models with identifiable factorization. ArXiv, abs/2306.06561, 2023b.\\n\\nLu, C., Ball, P. J., Rudner, T. G. J., Parker-Holder, J., Osborne, M. A., and Teh, Y. W. Challenges and opportunities in offline reinforcement learning from visual observations. ArXiv, abs/2206.04779, 2022.\\n\\nLuo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T. Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\\n\\nNachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., and Schuurmans, D. Algaedice: Policy gradient from arbitrary experience. ArXiv, abs/1912.02074, 2019.\\n\\nPan, M., Zhu, X., Wang, Y., and Yang, X. Isolating and leveraging controllable and noncontrollable visual dynamics in world models. ArXiv, abs/2205.13817, 2022.\\n\\nPathak, D., Gandhi, D., and Gupta, A. K. Self-supervised exploration via disagreement. ArXiv, abs/1906.04161, 2019.\\n\\nPeng, X. B., Kumar, A., Zhang, G., and Levine, S. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. ArXiv, abs/1910.00177, 2019.\\n\\nPrudencio, R. F., M\u00e1ximo, M. R. O. A., and Colombini, E. L. A survey on offline reinforcement learning: Taxonomy, review, and open problems. ArXiv, abs/2203.01387, 2022.\\n\\nRafailov, R., Yu, T., Rajeswaran, A., and Finn, C. Offline reinforcement learning from images with latent space models. In Learning for Dynamics and Control, pp. 1154\u20131168. PMLR, 2021.\\n\\nRan, Y., Li, Y., Zhang, F., Zhang, Z., and Yu, Y. Policy regularization with dataset constraint for offline reinforcement learning. In International Conference on Machine Learning, 2023.\\n\\nRashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. J. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. IEEE Transactions on Information Theory, 68:8156\u20138196, 2021.\\n\\nShen, Y., Fang, Z., Xu, Y., Cao, Y., and Zhu, J. A rank-based sampling framework for offline reinforcement learning. 2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI), pp. 197\u2013202, 2021.\\n\\nShortreed, S. M., Laber, E. B., Lizotte, D. J., Stroup, T. S., Pineau, J., and Murphy, S. A. Informing sequential clinical decision-making through reinforcement learning: an empirical study. Machine Learning, 84:109\u2013136, 2010.\\n\\nSiegel, N., Springenberg, J. T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., Hafner, R., and Riedmiller, M. A. Keep doing what worked: Behavioral modelling priors for offline reinforcement learning. ArXiv, abs/2002.08396, 2020.\\n\\nTassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., de Las Casas, D., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T. P., and Riedmiller, M. A. DeepMind control suite. ArXiv, abs/1801.00690, 2018.\\n\\nTomar, M., Islam, R., Taylor, M., Levine, S., and Bachman, P. Ignorance is bliss: Robust control via information gating. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nTrabucco, B., Geng, X., Kumar, A., and Levine, S. Designbench: Benchmarks for data-driven offline model-based optimization. In Chaudhuri, K., Jegelka, S., Song, L., Szepesv\u00e1ri, C., Niu, G., and Sabato, S. (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 21658\u201321676. PMLR, 2022.\"}"}
{"id": "ZtOXZCTgBa", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nFigure 7. The model uncertainty estimation of SeMOPO, Offline DV2, and LOMPO on the LQV-D4RL dataset. We randomly select 1000 states and report the mean and standard deviation of uncertainty on them. \\\"md\\\" and \\\"vlp\\\" are the short names for the mean-disagreement and variance of logarithm prediction, denoting the calculation of uncertainty used in Offline DV2 and LOMPO, respectively.\\n\\nFigure 8. The model uncertainty estimation of SeMOPO on the LQV-D4RL dataset. We randomly select 1000 states and infer the endogenous states $s^+$ and exogenous states $s^-$ by SeMOPO. We report the mean and standard deviation of uncertainty on these inferred states. \\\"md\\\" and \\\"vlp\\\" follow the same definition as in Figure 7.\\n\\nF. Additional Results\\n\\nF.1. The Model Uncertainty Estimation\\n\\nTo validate that the accuracy of model uncertainty estimation is due to the separation of endogenous and exogenous states, rather than a specific computational approach, we employ two different uncertainty estimation methods: the mean-disagreement of the ensemble (md) from Offline DV2, and the variance of logarithm prediction (vlp) from LOMPO. The task-related model uncertainty across various environments and datasets is shown in Figure 7. The model uncertainty of SeMOPO is superior under both computational methods compared to other techniques, suggesting that learning the model in the endogenous state space can reduce the estimated uncertainty.\\n\\nFigure 8 presents the estimates of model uncertainty in both endogenous and exogenous state spaces. Notably, the model uncertainty in the exogenous state space is significantly higher than in the endogenous state space. This further implies that the overestimation of model uncertainty in task-related components in previous work is attributable to the unfiltered noise in the latent states.\\n\\nF.2. Evaluation on the V-D4RL Benchmark\\n\\nTo assess the performance of our method on datasets without distractors, we compared SeMOPO with other approaches on the V-D4RL dataset. The results in Table 4 show that SeMOPO outperforms other methods on the random datasets of two environments, and achieves comparable performance on several other datasets. This indicates that our method is more suitable for datasets collected using non-expert policies, and can also address certain offline visual reinforcement learning problems without distractors.\\n\\nF.3. Training Stability\\n\\nThe gradient norms during model training for Offline DV2 and SeMOPO are shown in Figure 9.\"}"}
{"id": "ZtOXZCTgBa", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4. The performance of different methods on the V-D4RL benchmark. We report the mean and standard deviation of test returns of SeMOPO over 4 seeds. The results for methods other than SeMOPO are sourced from Table 1 in (Lu et al., 2022).\\n\\n| Dataset | SeMOPO | Offline DV2 | LOMPO | DrQ+BC | DrQ+CQL | BC |\\n|---------|--------|-------------|-------|--------|--------|----|\\n| Walker Walk RANDOM | 305 \u00b1 8 | 287 \u00b1 130 | 219 \u00b1 81 | 55 \u00b1 9 | 144 \u00b1 124 | 20 \u00b1 2 |\\n| medrep | 218 \u00b1 21 | 565 \u00b1 181 | 347 \u00b1 197 | 287 \u00b1 69 | 114 \u00b1 124 | 165 \u00b1 43 |\\n| medium | 406 \u00b1 30 | 434 \u00b1 111 | 341 \u00b1 197 | 468 \u00b1 23 | 148 \u00b1 161 | 409 \u00b1 31 |\\n| Cheetah Run RANDOM | 330 \u00b1 2 | 329 \u00b1 2 | 114 \u00b1 51 | 58 \u00b1 6 | 59 \u00b1 84 | 0 \u00b1 0 |\\n| medrep | 410 \u00b1 13 | 616 \u00b1 10 | 363 \u00b1 136 | 448 \u00b1 36 | 107 \u00b1 128 | 250 \u00b1 36 |\\n| medium | 190 \u00b1 22 | 172 \u00b1 35 | 164 \u00b1 83 | 530 \u00b1 30 | 409 \u00b1 51 | 516 \u00b1 14 |\\n\\nFigure 9. Comparison of gradient norms during model training for Offline DV2 and SeMOPO across five tasks: the first three from LQV-D4RL and the latter two from V-D4RL. Each curve represents aggregated data from three levels of datasets (random, medium replay, medium), illustrating the mean (solid line) and standard deviation (shaded region) over four seeds. SeMOPO exhibits lower model gradient norms than Offline DV2, regardless of distractors in observations, indicating that the separation of task-relevant information from observations contributes to more stable model training.\\n\\nF.4. The results on the LQV-D4RL benchmark\\n\\nWe record the original unnormalized returns for each method in Table 5.\"}"}
{"id": "ZtOXZCTgBa", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"| Offline DV2+RS | Se+RS | Se+RS+DRP | Se+CS (SeMOPO) |\\n|---------------|-------|-----------|----------------|\\n| Offline DV2+RS | Se+RS | Se+RS+DRP | Se+CS (SeMOPO) |\\n| Offline DV2+RS | Se+RS | Se+RS+DRP | Se+CS (SeMOPO) |\\n| Offline DV2+RS | Se+RS | Se+RS+DRP | Se+CS (SeMOPO) |\\n| Offline DV2+RS | Se+RS | Se+RS+DRP | Se+CS (SeMOPO) |\\n| Offline DV2+RS | Se+RS | Se+RS+DRP | Se+CS (SeMOPO) |\\n| Offline DV2+RS | Se+RS | Se+RS+DRP | Se+CS (SeMOPO) |\\n\\nFigure 10. Performance evaluation results of ablated methods of SeMOPO on the LQV-D4RL benchmark for 200 test episodes. Shaded regions represent pointwise 95% confidence bands based on percentile bootstrap with stratified sampling (Agarwal et al., 2021). Removing any component of SeMOPO leads to a performance drop.\"}"}
{"id": "ZtOXZCTgBa", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 5.\\nThe unnormalized test returns of different methods on the LQV-D4RL benchmark. Mean scores (higher is better) with standard deviation are recorded across 4 seeds for each task.\\n\\n| Dataset | SeMOPO | Offline | DV2 | LOMPO | DrQ+BC | DrQ+CQL | BC InfoGating |\\n|---------|--------|---------|-----|-------|--------|---------|--------------|\\n| Walker  | random | 459 \u00b1 41 | 167 | 133 \u00b1 40 | 27 \u00b1 1 | 26 \u00b1 2 | 49 \u00b1 7 |\\n|         | medrep | 521 \u00b1 42 | 175 | 218 \u00b1 69 | 27 \u00b1 3 | 25 \u00b1 3 | 28 \u00b1 7 |\\n|         | medium | 273 \u00b1 47 | 68 | 63 \u00b1 26 | 390 \u00b1 36 | 25 \u00b1 3 | 413 \u00b1 30 | 98 \u00b1 41 |\\n| Cheetah | random | 254 \u00b1 30 | 40 | 66 \u00b1 15 | 94 \u00b1 30 | 0 \u00b1 0 | 22 \u00b1 20 | 58 \u00b1 16 |\\n|         | medrep | 258 \u00b1 28 | 66 | 78 \u00b1 31 | 166 \u00b1 93 | 1 \u00b1 0 | 20 \u00b1 18 | 267 \u00b1 52 |\\n|         | medium | 293 \u00b1 32 | 81 | 52 \u00b1 37 | 260 \u00b1 28 | 0 \u00b1 0 | 252 \u00b1 40 | 288 \u00b1 39 |\\n| Hopper  | random | 58 \u00b1 5  | 0 | 0 \u00b1 0 | 7 \u00b1 8 | 0 \u00b1 0 | 5 \u00b1 6 |\\n|         | medrep | 77 \u00b1 6 | 0 | 0 \u00b1 0 | 21 \u00b1 15 | 0 \u00b1 0 | 3 \u00b1 1 | 1 \u00b1 1 |\\n|         | medium | 105 \u00b1 14 | 2 | 4 \u00b1 4 | 1 | 0 \u00b1 0 | 3 \u00b1 5 | 14 \u00b1 1 |\\n| Humanoid| random | 6 \u00b1 3 | 3 \u00b1 1 | 2 \u00b1 2 | 1 \u00b1 1 | 1 \u00b1 1 | 2 \u00b1 2 |\\n|         | medrep | 7 \u00b1 4 | 2 \u00b1 1 | 2 \u00b1 2 | 2 \u00b1 2 | 2 \u00b1 1 | 7 \u00b1 2 |\\n|         | medium | 6 \u00b1 5 | 4 \u00b1 2 | 3 \u00b1 3 | 14 \u00b1 7 | 2 \u00b1 1 | 3 \u00b1 4 | 4 \u00b1 3 |\\n| Car     | random | 418 \u00b1 79 | 233 \u00b1 44 | 387 \u00b1 89 | -10 \u00b1 32 | -92 \u00b1 32 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79 \u00b1 13 | -59 \u00b1 13 | -79"}
{"id": "ZtOXZCTgBa", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SeMOPO: Learning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nShenghua Wan 1 2  \\nZiyuan Chen 3  \\nLe Gan 1 2  \\nShuai Feng 4  \\nDe-Chuan Zhan 1 2  \\n\\nAbstract\\n\\nModel-based offline reinforcement learning (RL) is a promising approach that leverages existing data effectively in many real-world applications, especially those involving high-dimensional inputs like images and videos. To alleviate the distribution shift issue in offline RL, existing model-based methods heavily rely on the uncertainty of learned dynamics. However, the model uncertainty estimation becomes significantly biased when observations contain complex distractors with non-trivial dynamics. To address this challenge, we propose a new approach - Separated Model-based Offline Policy Optimization (SeMOPO) - decomposing latent states into endogenous and exogenous parts via conservative sampling and estimating model uncertainty on the endogenous states only. We provide a theoretical guarantee of model uncertainty and performance bound of SeMOPO. To assess the efficacy, we construct the Low-Quality Vision Deep Data-Driven Datasets for RL (LQV-D4RL), where the data are collected by non-expert policy and the observations include moving distractors. Experimental results show that our method substantially outperforms all baseline methods, and further analytical experiments validate the critical designs in our method. The project website is https://sites.google.com/view/semopo.\\n\\n1 School of Artificial Intelligence, Nanjing University, China  \\n2 National Key Laboratory for Novel Software Technology, Nanjing University, China  \\n3 School of Mathematical Sciences, Center for Statistical Science, Peking University, Beijing, China  \\n4 School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China. Correspondence to: De-Chuan Zhan <zhandc@nju.edu.cn>.\\n\\nProceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\\n\\n1. Introduction\\n\\nOffline reinforcement learning (RL) (Lange et al., 2012; Levine et al., 2020), which learns policies from fixed datasets without the need for costly interactions with online environments, has been increasingly applied in real-world tasks such as drug discovery (Trabucco et al., 2022) and autonomous driving (Iroegbu & Madhavi, 2021; Diehl et al., 2023). Offline RL saves the cost of interacting with the environment and improves sample efficiency. Real-world RL datasets typically exhibit two main characteristics: (1) they are often collected by non-expert or random policies (Jin et al., 2020; Rashidinejad et al., 2021), and (2) they originate from real environments with high-dimensional observations, such as images or videos (Rafailov et al., 2021; Prudencio et al., 2022), containing complex noise like moving backgrounds (Lu et al., 2022). Learning high-quality policies from low-quality datasets poses a significant challenge. Concerning the first characteristic, previous research has demonstrated the efficacy of offline RL with highly suboptimal or random datasets (Jin et al., 2020; Rashidinejad et al., 2021; Kumar et al., 2022). Regarding the second characteristic, some studies have employed model-based RL (MBRL) methods to address the challenges of high-dimensional inputs. MBRL learns a low-dimensional surrogate model of the high-dimensional environment (Ha & Schmidhuber, 2018; Hafner et al., 2019), allowing the agent to interact with this model to gather additional trajectories in the low-dimensional state space. Offline MBRL methods improve sample efficiency and reduce storage costs. However, an unavoidable gap exists between the actual environment and the learned model from offline datasets. Offline MBRL methods (Yu et al., 2020) mitigate the distribution shift problem by incorporating the model prediction's uncertainty as a penalty in the reward function.\\n\\nIn real-world decision-making scenarios, uncertainty arises not only from task-relevant dynamics but also from irrelevant distractors in observations, such as moving backgrounds. However, previous offline visual RL works (Yu et al., 2020; Rafailov et al., 2021; Lu et al., 2022) do not differentiate between these two types of uncertainty. If both are treated as model uncertainty in offline policy training and used as a penalty in the reward function, the learned policies...\"}"}
{"id": "ZtOXZCTgBa", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning High-quality Model and Policy from Low-quality Offline Visual Datasets may become overly conservative. Therefore, it is crucial to consider task-irrelevant dynamics uncertainty during offline model training.\\n\\nTo address these challenges, we propose the Separated Model-based Offline Policy Optimization (SeMOPO) method. We first analyze the performance lower bound under the Exogenous Block MDP (EX-BMDP) assumption (Efroni et al., 2021) in the case of offline learning from noisy visual datasets. We find that such a lower bound is empirically tighter than that under the POMDP assumption (Kaelbling et al., 1998) commonly used in previous offline RL research (Rafailov et al., 2021; Lu et al., 2022). We replace the typical random sampling method in offline MBRL (Yu et al., 2020; Rafailov et al., 2021; Lu et al., 2022) with our proposed conservative sampling method, training the separated model on trajectories collected by relatively deterministic behavior policies. After obtaining the task-relevant model from the offline dataset, we train the policy on the endogenous states imagined by this model. To evaluate our method, we construct the Low-Quality Vision Datasets for Deep Data-Driven RL (LQV-D4RL), including 15 different settings from DMControl Suite (Tassa et al., 2018) and Gym (Brockman et al., 2016) environments. SeMOPO achieves significantly better performance than all baseline methods on the LQV-D4RL. Our analytical experiments confirm the effectiveness of the conservative sampling method in identifying task-relevant information and the superiority of estimating uncertainty estimation on it during offline policy training.\\n\\nThe main contributions of our work are: (i) We propose a new approach named Separated Model-based Offline Policy Optimization (SeMOPO), which aims to solve offline visual RL tasks from low-quality datasets collected by sub-optimal policies and with complex distractors in observations. (ii) To establish a benchmark for the practical setting, we construct Low-Quality Vision Datasets for Deep Data-Driven RL (LQV-D4RL), which offers new research opportunities for practitioners in the field. (iii) We provide a theoretical analysis of the lower performance bound of policies learned on the endogenous state space and the superiority of our proposed conservative sampling method in differentiating task-relevant and irrelevant information. (iv) We show excellent performance of SeMOPO on LQV-D4RL, with analytical experiments validating the efficacy of each component of the method.\\n\\n2. Preliminaries\\n\\nIn our work, we focus on learning skills from the offline dataset consisting of image observations, actions, and rewards, denoted as $B = \\\\{o_i^1:T, a_i^1:T, r_i^1:T\\\\}_{n_i=1}$. The dataset contains moving distractors within the visual observations and is collected by policies $\\\\pi_B$ with suboptimal or random behaviors. To better model the environment, we consider the Exogenous Block Markov Decision Process (EX-BMDP) setting (Efroni et al., 2021), an adaptation of the Block MDP (Du et al., 2019). A Block MDP consists of a set of observations $O$; a set of latent states, $Z$ with cardinality $Z$; a finite set of actions, $A$ with cardinality $A$; a transition function, $T: Z \\\\times A \\\\rightarrow \\\\Delta(Z)$; a reward function $R: O \\\\times A \\\\rightarrow [0, 1]$; an emission function $U: Z \\\\rightarrow \\\\Delta(O)$; and an initial state distribution $\\\\mu_0 \\\\in \\\\Delta(Z)$. The agent has no access to the latent states but can only receive the observations. The block structure holds if the support of the emission distributions of any two latent states are disjoint, $\\\\text{supp}(U(\\\\cdot|z_1)) \\\\cap \\\\text{supp}(U(\\\\cdot|z_2)) = \\\\emptyset$ when $z_1 \\\\neq z_2$, where $\\\\text{supp}(U(\\\\cdot|z)) = \\\\{o \\\\in O | U(o|z) > 0\\\\}$, distinguishing the BMDP from the Partially Observable MDP (Kaelbling et al., 1998). We now restate the definition of EX-BMDP:\\n\\nDefinition 2.1. (Exogenous Block Markov Decision Process). An EX-BMDP is a BMDP such that the latent state can be decoupled into two parts $z = (s^+, s^-)$ where $s^+ \\\\in S^+$ is the endogenous state and $s^- \\\\in S^-$ is the exogenous state. For $z \\\\in Z$ the initial distribution and transition functions are decoupled, that is: $\\\\mu(z) = \\\\mu(s^+)\\\\mu(s^-)$, and $T(z'|z,a) = T(s^'+|s^+,a)T(s'^-|s^-)$.\\n\\nEX-BMDP decomposes the dynamics and separates the exogenous noise from the endogenous state. This noise is not controlled by the agent, but it may have a non-trivial dynamic. EX-BMDP provides a natural way to characterize this type of noise.\\n\\nModel-based Offline Policy Optimization (Yu et al., 2020) (MOPO) is a typical offline RL method, showing the efficacy of model-based methods to learn policies from offline datasets. MOPO learns the dynamics model $b_T(\\\\cdot|s,a)$ and the reward model $\\\\hat{r}(s,a)$ from the fixed dataset $B$, and constructs an uncertainty-penalized MDP whose reward function is defined as $\\\\tilde{r}(s,a) = r(s,a) - u(s,a)$, where $u(s,a)$ is the model uncertainty estimation. MOPO proves that optimizing the policy in the uncertainty-penalized MDP with an admissible uncertainty estimation $u(s,a)$ is equivalent to optimizing it in the true MDP. We adopt a similar pipeline to MOPO to learn the model and policy from offline datasets.\\n\\n3. SeMOPO: Separated Model-based Offline Policy Optimization\\n\\nModel-based offline RL methods use model uncertainty to address the distribution shift problem between the online environment and the offline dataset. In Section 3.1, we first provide a theoretical justification of the performance bound under the Exogenous Block MDP assumption when learning from offline datasets with visual distractors and illustrate the inherent limitations of employing the POMDP framework in this scenario. In Section 3.2, we propose a sampling strategy\"}"}
{"id": "ZtOXZCTgBa", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Assumption 3.2. Let $d$.\\n\\nUnder Assumption 3.2, we can define the un-\\n\\nTheorem 3.3. Theorem 3.3 suggests that the performance under the true\\n\\nand the estimated dynamics\\n\\nThe telescoping lemma provides a way to measure\\n\\nNote that if\\n\\nLemma 3.1. (Telescoping lemma in the endogenous state\\n\\nIn this section, we theoretically analyze the lower bound of\\n\\n3.1. Uncertainty Estimation and Performance Bound\\n\\nIn Section 3.3, we detail a practical implementation of Se-\\n\\nto help the model differentiate task-relevant and irrelevant\\n\\ncomponents and give a corresponding theoretical analysis.\\n\\nIn prior works like LOMPO (Rafailov et al., 2021) and Offline\\n\\nRemarks.\\n\\nIn Appendix A.1.\\n\\ncorresponding model uncertainty, which provides a theoreti-\\n\\ncal guarantee on the performance of SeMOPO. The proof is\\n\\nDV2 (Lu et al., 2022) will give a biased model uncer-\\n\\ntainty estimation when learning from offline datasets con-\\n\\nin addition to task-relevant information, there are task-\\n\\nof the true state $s$ within the latent space $Z$.\\n\\nUnder the EX-BMDP assumption, the log-likelihood for the\\n\\nFrom the last three terms in Equation (1), we can find that\\n\\nrandom action distributions can weaken the influence of\\n\\nagent actions on the endogenous transition, causing it to col-\\n\\nlapse into\\n\\nExcessively\\n\\nintuitive insight is that the separated model should be up-\\n\\nfor convenience. The detailed derivation is in Appendix B.\\n\\nFrom the three terms in Equation (1), we can find that\\n\\nthe agent's action is the key factor to help the model dis-\\n\\nin image observations that are be-\\n\\nirrelevant components in image observations. These\\n\\nmethods assume that\\n\\nobtaining task-irrelevant distractors in observations. These\\n\\nmodel uncertainty is estimated as the variance of the pre-\\n\\npredicted probability of the next state given the current state\\n\\nmodel uncertainty is bounded by learning a policy in the en-\\n\\nbe two MDPs with the same re-\\n\\nbe the endogenous state space un-\\n\\nLearning High-quality Model and Policy from Low-quality Offline Visual Datasets\\n\\nWe omit the terms of the initial distribution of\\n\\n$p(\\\\tau)$.\\n\\n$p(\\\\tau)$ can be decomposed as:\\n\\n$log_\\\\theta(z, a | s, T)$, which is larger\\n\\n$log_\\\\theta(z, a | s, T) = ln p(\\\\tau) + ln p(z, a | \\\\theta, s, T)$.\\n\\n$log_\\\\theta(z, a | s, T) = ln p(\\\\tau) + ln p(z, a | \\\\theta, s, T) - ln p(z, a | s, T)$.\\n\\nThe model uncertainty $Var_{b}$ is estimated as the variance of the pre-\\n\\nmodel uncertainty $Var_{b}$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum_{t=1}^{T} \\\\log_\\\\theta(z, a | s, T)$.\\n\\n$L_{LOMPO}(\\\\theta, s, T, \\\\tau) = \\\\sum"}
