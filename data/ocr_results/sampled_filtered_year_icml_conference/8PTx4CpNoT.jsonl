{"id": "8PTx4CpNoT", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "8PTx4CpNoT", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "8PTx4CpNoT", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\n(a) Excess of original over adversarial semantic content using different probing classifiers.\\n\\n(b) Excess of original over adversarial semantic content for current and future abstract states (1-layer MLP).\\n\\nFigure 9: Excess of original over adversarial semantic content.\\n\\nA. Experimental details\\n\\nA.1. Karel grammar specification\\n\\nWe use the same grammar as Devlin et al. (2017), except with loops and conditionals removed.\\n\\n\\\\[\\n\\\\text{Prog} ::= \\\\text{def run():} \\\\\\\\\\n\\\\text{Stmts ::=} \\\\\\\\\\n\\\\text{s1; s2 | a} \\\\\\\\\\n\\\\text{Action a ::=} \\\\\\\\\\n\\\\text{move() | turnRight() | turnLeft() | pickMarker() | putMarker()}\\n\\\\]\\n\\nA.2. Language model and probe details\\n\\nWe train the 350M-parameter variant of the CodeGen architecture (Nijkamp et al., 2023) from the HuggingFace Transformers library (Wolf et al., 2020), implemented in PyTorch (Paszke et al., 2019). We use the Adam optimizer, a learning rate of 5e-5, a block size of 2048, and a batch size of 32768 tokens. We train for 2.5 billion tokens, which was close to 6 passes over our training corpus; we did not observe any instabilities with training (see the results in the main text). We use a warm up over roughly the first 3000 batches, then linearly decay the learning rate to 0 after 80000 batches (training runs over 76000 batches). On a single NVIDIA A100 GPU with 80GB of VRAM, training takes around 8 days.\\n\\nThe linear probe consists of a single linear layer. The MLP probes consist of stacked linear, batch norm, and ReLU layers, in that order. The first linear layer in both MLP probes projects to a hidden dimension of 256, and the second linear layer (in the 2-layer MLP) projects to a hidden dimension of 1024. Probes are trained on the first 100000 aligned traces in the training trace dataset. All probes are trained using the same recipe, which was tuned to saturate (or nearly saturate) their performance: we use the AdamW optimizer with weight decay of 1e-4 and a learning rate of 0.01 that decays by .1 at 75% and 90% of the way through training; and train for 10000000 steps using a batch size of 1024.\\n\\nB. Additional experimental results\\n\\nB.1. Plots for adversarial semantics\\n\\nFigures 9a and 9b provide plots for the adversarial semantics that are analogous to those provided in the main text for the flip semantics (specifically Figures 7 and 8, respectively). We observe that, in contrast to the flip semantics (which achieve close to zero excess semantic content in the babbling phase), the excess of the original over adversarial semantics exhibit significant noise during the babbling phase of the LM training, which is often negative; we attribute this to weaker relationship between the adversarial and original semantics conjectured in Section 4: certain distributions of programs may be easier for the adversarial vs. original semantics, and vice versa. We also note that the excess semantic content is largely constant across the current and future states in Figure 9b, particularly in the final phases of training, which is consistent with\"}"}
{"id": "8PTx4CpNoT", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\nFigure 10: The average length of generated programs over time generated from specifications sampled according to the training distribution and compared with the corresponding reference programs.\\n\\nB.2. Generated programs become shorter than reference programs over training\\n\\nFigure 10 plots the average length of programs generated by the LMs over specifications sampled from the training distribution (i.e., where the reference programs are of length 6 to 10, inclusive). The results indicate that the LM learns to generate programs which are, on average, shorter than the reference program length. While this can be partially explained by the fact that we use greedy decoding to generate the outputs, we emphasize that the generated programs are also growing increasingly correct over the second half of training, i.e., the quality of the LM's generation continues to improve despite diverging from the distribution of the training corpus.\\n\\nThese results complement our findings in the main text, which show that the LM tends to underfit the distribution of tokens in the training data. Although prior work has explored the differences between an LM's output and its training corpus based on surface statistics (Meister & Cotterell, 2021; LeBrun et al., 2022), we are, to the best of our knowledge, the first to present an account of how such syntactic divergence relates to the semantic properties of the LM generation and training data.\\n\\nB.3. Semantic content of reference programs\\n\\nThis section explores a possible implication of the finding in Section 3 that representations contain predictions of future program states. In particular, these future program states are defined by the tokens produced by greedy decoding, i.e., the program is generated by taking the token that the LM models as most likely at each step in the autoregressive loop of Equation (1). However, it has been observed in other settings that sampling from the tokens according to a distribution derived from the logits of the LM head can improve the quality of the LM's outputs, e.g., with multinomial or nucleus sampling (Holtzman et al., 2020). However, if the LM states do in fact encode the future program states according to greedy decoding, then any deviation could, in fact, force the LM states to visit \u201cunanticipated\u201d program states.\\n\\nWe perform a preliminary exploration of this hypothesis. Specifically, we measure the semantic content when decoding from the LM according to the reference programs used to generate the specifications in our train and test datasets: we create new reference trace datasets by replacing the greedy decoding step of Equation (2) with the tokens from the reference programs, i.e.,\\n\\n\\\\[\\n(LM_i) = (LM(input, output, \\\\{LM_j\\\\}_{j=1}^{i-1}))\\n\\\\]\\n\\n(8)\\n\\n\\\\[\\ntoken_i = prog_{ref}[i]\\n\\\\]\\n\\n(9)\\n\\n\\\\[\\n(prog_i) = exec(token_i, (prog_{i-1}))\\n\\\\]\\n\\n(10)\\n\\nwhere \\\\(prog_{ref}\\\\) is the reference program and \\\\(prog_{ref}[i]\\\\) is the \\\\(i\\\\)th program token in the reference program. The idea is that,\"}"}
{"id": "8PTx4CpNoT", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Emergent Representations of Program Semantics in Language Models Trained on Programs\\nCharles Jin\\nMartin Rinard\\n\\nAbstract\\nWe present evidence that language models (LMs) of code can learn to represent the formal semantics of programs, despite being trained only to perform next-token prediction. Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments. Each program in the corpus is preceded by a (partial) specification in the form of several input-output grid world states. Despite providing no further inductive biases, we find that a probing classifier is able to extract increasingly accurate representations of the unobserved, intermediate grid world states from the LM hidden states over the course of training, suggesting the LM acquires an emergent ability to interpret programs in the formal sense. We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe. We anticipate that this technique may be generally applicable to a broad range of semantic probing experiments. In summary, this paper does not propose any new techniques for training LMs of code, but develops an experimental framework for and provides insights into the acquisition and representation of formal semantics in statistical models of code.\\n\\n1. Introduction\\nAs LMs continue to improve on a range of downstream tasks, their capabilities in the domain of programming languages have drawn increasing attention (Bommasani et al., 2023; Zan et al., 2023). The advancement of recent models such as GPT-4 (OpenAI et al., 2023), Code Llama (Rozi\u00e8re et al., 2023), Gemini (Gemini Team et al., 2023), and Claude 3 (Anthropic, 2023) has also spurred the widespread adoption of LMs in developer workflows via mainstream commercial products, offering functionality such as code completion, debugging assistance, generating documentation and commit messages, and writing test cases (Fan et al., 2023).\\n\\nDespite these results, however, a major open question is whether current LMs capture any information about the semantics of the text that they consume and generate (Mitchell & Krakauer, 2023). Indeed, one hypothesis\u2014which takes a unified view of both natural and programming language domains\u2014is that LMs trained purely on form (e.g., to model the conditional distribution of tokens in a training corpus) produce text only according to surface statistical correlations gleaned from the training data (Bender & Koller, 2020), with any apparently sophisticated behavior attributable to the scale of the model and training data.\\n\\nThis work studies whether LMs of code learn aspects of semantics when trained using standard textual pretraining. We empirically evaluate the following hypothesis (MH):\\n\\nMain Hypothesis. LMs of code trained only to perform next token prediction on text do not model the formal semantics of the underlying programming language.\\n\\nTo investigate MH, we apply language modeling to the task of program synthesis, or generating a program that implements a given (partial) specification, which we take to be a set of input-output examples. Specifically, we explore whether an LM trained on text that encodes only the input-output behavior of programs also learns to model the intermediate program states specified by the small-step operational semantics of the synthesized program (Plotkin, 1981). We train an LM on example programs preceded by several input-output examples, then use small classifiers to probe the LM's hidden states for (abstractions of) the intermediate states in the program execution. Despite the text of the training corpus encoding only input-output behavior, we find the probe's ability to extract intermediate states undergoes a phase transition during training, with the phase transition strongly correlated with the LM's ability to generate a correct program in response to previously unseen specifications. We also present results from a novel interventional experiment, which indicate that the semantics are represented by the LM (rather than learned by the probe).\\n\\n1\"}"}
{"id": "8PTx4CpNoT", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: The excess semantic content at the end of training when comparing the original and alternative semantics using a linear probe on the input-only trace datasets. We separate the semantic content by the depth of the program state and the 3 features in the abstract state. The LM only observes program states at depth 6 or greater in the training corpus. We display depths consisting of at least 1% of the training set. A positive value indicates that the original semantic content is greater than the alternative semantic content.\\n\\n| Depth | Flip | Adversarial | Direction | Position | Obstacle | All |\\n|-------|------|-------------|-----------|----------|----------|-----|\\n|       | 1    | -0.51       | 0.15      | 0.17     | -0.11    |     |\\n|       | 2    | 0.36        | 0.31      | 0.50     | 0.40     |     |\\n|       | 3    | 1.53        | 0.39      | 0.65     | 0.96     |     |\\n|       | 4    | 1.92        | -0.14     | 0.17     | 0.83     |     |\\n|       | 5    | 3.12        | 0.32      | -0.00    | 1.49     |     |\\n|       | 6    | 2.33        | 0.55      | 0.00     | 1.25     |     |\\n|       | 7    | 2.01        | 0.39      | 0.71     | 1.22     |     |\\n|       | 8    | 1.46        | -0.17     | 0.15     | 0.64     |     |\\n|       | 9    | 1.04        | -0.04     | 0.50     | 0.57     |     |\\n\\nThe present and future program states are subject to an inherent degree of randomness (recall that the probe\u2019s task for the \u201ccurrent\u201d abstract state is to predict \\\\((\\\\text{state}_{\\\\text{prog}})_i\\\\) from \\\\((\\\\text{state}_{\\\\text{LM}})_i\\\\) in Equation (11)). Indeed, we note that the semantic content is maximized at 1 abstract state into the past across all settings. Nonetheless, our results suggest the LM still maintains the ability to interpret programs, even in this challenging setting; moreover, the interventional baseline offers strong support that the representations of the LM are aligned with the original semantics (rather than being learned by the probe). In particular, we see statistically significant correlations for all three probes and all 5 abstract states when comparing the original semantic content against the adversarial semantic content. Note that, even though predicting future states is difficult, the ability to predict future states is correlated with knowing the current state, so that it is not surprisingly that the adversarial semantic content for future states would be lower, despite the lack of information about future states.\\n\\nWe also observe a statistically significant positive correlation when regressing the excess of the original semantic content over the flip semantic content over the second half of training, when using the linear probe, though this effect disappears as we move to deeper probes. We emphasize that the flip semantics presents a very strong baseline as the semantics can often be inferred directly from the result of the original semantics (i.e., by reflecting the robot across an axis).\\n\\nCould these results could be due to retrieval? To test this hypothesis, we also reproduce the experiments from Appendix B.4 on the input-only trace dataset. We focus on 1 abstract state into the past, as it is where the semantic content is maximized across all settings; and the linear probe, as the original semantic content and excess over the alternative semantics are all correlated with the generative accuracy to a statistically significant degree over the course of training. Table 5 displays the excess semantic content of the original over the flip and adversarial semantics, respectively, using a linear probe on the input-only trace datasets when probing for 1 abstract state into the past. We see that almost every semantic content of unseen features (and overall abstract state) is greater when probing for the original semantics compared to the alternative semantics, confirming that the observed difference in semantic content cannot be entirely attributed to the LM performing retrieval.\\n\\nWe thus conclude that the LM is able to abstractly interpret programs even without seeing the final outputs of the program, which is emergent behavior on out-of-distribution text. For completeness, Figure 13 plots results over the entire LM training.\\n\\nB.6. Selected regression and residual plots\\n\\nFigures 14 and 15 display selected regression and residual plots from Tables 2 and 4, respectively. Specifically, we provide the regression and residual plots corresponding to the semantic content of the current state as measured by a linear classifier. In all cases (including those omitted for brevity and not explicitly shown in the figures), the residual plots confirm a linear relationship between the semantic content and generative accuracy.\"}"}
{"id": "8PTx4CpNoT", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\n(a) Probing with a linear classifier.\\n(b) Probing with a 1-layer MLP.\\n(c) Probing with a 2-layer MLP.\\n(d) Probing with a linear classifier, excess over flip semantics.\\n(e) Probing with a 1-layer MLP, excess over flip semantics.\\n(f) Probing with a 2-layer MLP, excess over flip semantics.\\n(g) Probing with linear classifier, excess over adversarial semantics.\\n(h) Probing with a 1-layer MLP, excess over adversarial semantics.\\n(i) Probing with a 2-layer MLP, excess over adversarial semantics.\\n\\nFigure 13: The semantic content on the input-only trace datasets, separated by depth over the full LM training run. Note that all plots show trends which, when aggregated over depth, exhibit a statistically significant linear correlation with the generate accuracy, except for Figures 13e and 13f.\"}"}
{"id": "8PTx4CpNoT", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\n(a) Regressing semantic content vs. generative accuracy.\\n\\n(b) Residuals for semantic content vs. generative accuracy.\\n\\n(c) Regressing excess of original over flip semantic content vs. generative accuracy.\\n\\n(d) Residuals for excess of original over flip semantic content vs. generative accuracy.\\n\\n(e) Regressing excess of original over adversarial semantic content vs. generative accuracy.\\n\\n(f) Residuals for excess of original over adversarial semantic content vs. generative accuracy.\\n\\nFigure 14: Regression and residual plots for semantic content (measured by a linear classifier) vs. generative accuracy over the second half of training.\"}"}
{"id": "8PTx4CpNoT", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\n(a) Regressing semantic content vs. generative accuracy.\\n\\n(b) Residuals for semantic content vs. generative accuracy.\\n\\n(c) Regressing excess of original over flip semantic content vs. generative accuracy.\\n\\n(d) Residuals for excess of original over flip semantic content vs. generative accuracy.\\n\\n(e) Regressing excess of original over adversarial semantic content vs. generative accuracy.\\n\\n(f) Residuals for excess of original over adversarial semantic content vs. generative accuracy.\\n\\nFigure 15: Regression and residual plots for semantic content (measured by a linear classifier on the input-only trace datasets) vs. generative accuracy over the second half of training.\"}"}
{"id": "8PTx4CpNoT", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\nFigure 5: The semantic content (1-layer MLP) for the current and next two abstract states over the second half of training.\\n\\n| Probe Type | Accuracy 1 | Accuracy 2 | Accuracy 3 |\\n|------------|-----------|-----------|-----------|\\n| Linear classifier | 63.2 | 61.1 | 60.1 |\\n| 1-layer MLP | 79.1 | 75.1 | 72.5 |\\n| 2-layer MLP | 82.3 | 79.2 | 77.3 |\\n| Baseline (current state) | 100.0 | 83.9 | 75.6 |\\n\\nTable 1: Probing accuracy for current and future abstract states at the end of training vs. a baseline of predicting the current abstract state for future abstract states.\\n\\nRepresentations are predictive of future program states. We next explore whether the trained LM encodes the semantics of text that has yet to be generated. Specifically, we train probes to predict future abstract states from model states. Figure 5 displays how well a 1-layer MLP is able to predict abstract states 1 and 2 steps into the future. As with the previous results, the probe's performance reaches a minimum during syntax acquisition, then increases for the remainder of training. We also find a strong correlation between the semantic content of future states and the generative accuracy in the second half of training; regressing the semantic content against the generative accuracy yields an $R^2$ of 0.878 and 0.874 ($p < .001$) for 1 and 2 abstract states into the future, respectively.\\n\\nTable 1 compares the probing results at the end of training against a baseline which simply predicts the current abstract state for all future abstract states (which is the Bayes-optimal predictor absent any information about future states). We observe that (1) the accuracy of using the baseline degrades more rapidly than the probe, which suggests that the probes are not simply using encodings of the current state to predict future states, and (2) the absolute accuracy at 2 states into the future is greater using the 2-layer MLP probe than the baseline. These results suggest that the LM encodes information about what it intends to say ahead of its generation.\\n\\n4. Semantic probing interventions\\n\\nWe next evaluate the possibility that semantics are learned by the probe instead of the LM. For example, because the probe is explicitly supervised on intermediate states, the model states may encode the inputs and a record of the generated program tokens, with the probe learning to interpret the tokens one-by-one. More generally, the semantic content could be attributed to (1) the LM encoding only lexical and syntactic structure while (2) the probe learns to derive the semantics from the lexical and syntactic structure encoded in the LM state (because it is explicitly supervised to predict the semantics from the LM state). We refer to this as the syntactic record hypothesis, which offers an explanation for the results in Section 3 consistent with MH.\\n\\nTo test this hypothesis, we design a novel interventional experiment that preserves the lexical and syntactic structure of the language and intervenes only on the semantics. Then, we re-execute the program with the alternative semantics to obtain a new trace with new abstract states, and train a new probe to predict the new abstract states using the original model states. Our key insight is that, if the model states encode only syntactic information, then the new probe should be capable of extracting the new semantics from the original syntactic record equally well, leaving the semantic content unchanged.\\n\\n4.1. Intervening on semantics\\n\\nConcretely, we define two alternative semantics by reassigning the semantics of individual operations as follows:\\n\\n| Original | Adversarial |\\n|----------|-------------|\\n| flip     | adversarial |\\n| pickMarker | pickMarker |\\n| turnRight | turnLeft    |\\n| putMarker | turnRight   |\\n| move     | turnLeft    |\\n| turnLeft | turnRight   |\\n| move     | move        |\\n| turnLeft | turnRight   |\\n| move     | turnLeft    |\\n\\nFor instance, exec(turnRight, \u00b7) in the original semantics would have the robot turn 90 degrees clockwise, while exec(adversarial(turnRight, \u00b7)) advances the robot by a space (i.e., according to the original semantics of move).\"}"}
{"id": "8PTx4CpNoT", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"The proposed semantic intervention baseline. We use green for the original semantics, red for the alternative semantics, and gray for non-semantic components (such as syntax). We aim to distinguish between two hypotheses: (left) the LM only maintains a syntactic record (e.g., a list of the inputs and program tokens generated thus far), while probe learns to infer semantics from the record; and (right) the LM learns to represent state, while probe just extracts the semantics. We mark the emergent connection between the original semantics and the LM representations in the latter case by a dashed green line. The top half depicts how, pre-intervention, both cases can lead to the high semantic content measured in Section 3. The bottom half displays why intervening on the semantics while preserving the form of programs separates the two hypotheses: if the LM representations contain only syntax (bottom left), then it should be possible to train probe to learn to interpret the record according to the alternative state (bold red outcome); however, if the LM representations encode the original abstract state (bottom right), then probe needs to extract the alternative state from the original state, yielding a lower semantic content (bold gray outcome).\\n\\nTable 2: The results of our semantic intervention experiments. For each of the original, flip, and adversarial semantics, we report the semantic content (SC) at the end of training for 2 abstract states into the past (-2, -1), the current state (0), and 2 abstract states into the future (+1, +2), using linear, 1-layer MLP, and 2-layer MLP probes. We also regress the SC against the generative accuracy over the second half of training ($R^2(p)$). For each of the alternative semantics, we additionally compute the difference with respect to the original semantics ($\\\\Delta$) and regress the difference against the generative accuracy over the second half of training as ($R^2(p)$) of $\\\\Delta$. Highlighted cells are statistically significant at a level of $p < 0.05$ with an $R^2$ of at least 50%; all such correlations are positive. We reject the syntactic record hypothesis due to the magnitude ($\\\\Delta$ and $R^2$ of $\\\\Delta$) and statistical significance ($p$ of $\\\\Delta$) of the drop in semantic content when probing for the alternative semantics.\"}"}
{"id": "8PTx4CpNoT", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Excess of original over flip semantic content using different probing classifiers.\\n\\n\\\\[(\\\\text{state}^\\\\prime_{\\\\text{prog}})_i = \\\\text{exec}_{\\\\text{alt}}(\\\\text{token}_i, (\\\\text{state}^\\\\prime_{\\\\text{prog}})_{i-1}), \\\\quad (5)\\\\]\\n\\nwhere we also start from the same initial program state:\\n\\n\\\\[(\\\\text{state}^\\\\prime_{\\\\text{prog}})_0 = (\\\\text{state}_{\\\\text{prog}})_0 \\\\text{ (i.e., the specification inputs).} \\\\quad (6)\\\\]\\n\\nFinally, with the new traces from Equation (5) and the original state LM from Equation (1), we train a new probe \\\\(\\\\text{probe}_{\\\\text{alt}}\\\\):\\n\\n\\\\[\\\\text{state}_{\\\\text{LM}} \\\\rightarrow \\\\text{state}^\\\\prime_{\\\\text{prog}}, \\\\quad (6)\\\\]\\n\\nand compare its accuracy against the original probe:\\n\\n\\\\[\\\\text{probe}_{\\\\text{orig}} : \\\\text{state}_{\\\\text{LM}} \\\\rightarrow \\\\text{state}_{\\\\text{prog}}. \\\\quad (7)\\\\]\\n\\nFigure 6 illustrates our setup.\\n\\nFor an intervention to be a proper control, we identify two critical properties: (1) the alternative semantics should be limited to reassigning the semantics of individual operations in the language (as opposed to inventing completely new semantics, e.g., \\\"jump three spaces diagonally\\\") and (2) the intervention must preserve the syntactic structure of programs (i.e., how the individual operations are composed when interpreting a full program). Then assuming the syntactic record hypothesis is true, probe \\\\(\\\\text{probe}_{\\\\text{alt}}\\\\) should be able to interpret the record according to an analogous procedure as probe \\\\(\\\\text{probe}_{\\\\text{orig}}\\\\), yielding comparable measurements of semantic content. Hence, rejecting the syntactic record hypothesis reduces to measuring a statistically significant degradation in the alternative semantic content (relative to the original semantic content).\\n\\n4.2. Results\\n\\nTable 2 displays the results of our semantic intervention baseline, where we trained probes to predict up to two abstract states into the past and future using the original and alternative semantics. In all 5 cases, the semantic content for the alternative semantics is significantly degraded when compared to the original semantics, which supports rejecting the hypothesis that the model states only encode a syntactic record (i.e., lexical and syntactic information only) while the probe learns to interpret the record (i.e., semantics).\\n\\nNote that the flip semantics are strongly related to the original semantics: absent any obstacles in the robot's path, they only require reflecting the original path of the robot across an axis; in contrast, the adversarial semantics completely changes the semantics of each operator. Hence, if the syntactic record hypothesis is false, then we would expect the semantic content to be lower for adversarial vs. flip, since it should be more challenging to map from the original to the adversarial semantics; our results support this interpretation.\\n\\nWe also plot the excess of the original over the flip semantic contents in Figures 7 and 8. Note that the apparently high semantic content of the babbling phase\u2014which was observed pre-intervention in Figures 4 and 5, respectively, and attributed to the probe being able to learn the semantics of a small number of unique generated programs (black)\u2014disappears post-intervention. This is consistent with the probe learning the semantics equally well for both the original and flip semantics during the babbling phase, and demonstrates the ability of the semantic intervention baseline to control for the ability of the probe to learn semantics. We conclude that a statistically significant portion of the observed semantic content from Section 3 cannot be explained as the probe learning semantics, refuting MH.\\n\\n5. Related work\\n\\nLMs, semantics, and interpretability\\n\\nWhile many works have evaluated the external behavior of LMs on a range of semantically meaningful tasks (Austin et al., 2021; Toshniwal et al., 2022), these tasks do not require the model to learn significant amounts of semantics. Other works have observed substantial excess semantic content in LMs (Mehri et al., 2021; Lee et al., 2021), but these results are often attributed to the evaluation methods used and are not consistent with the hypothesis that LMs learn meaning.\"}"}
{"id": "8PTx4CpNoT", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\nwal et al., 2022; Patel & Pavlick, 2022; Liu et al., 2023), our work explores the internal state of the LM, falling under the broad umbrella of efforts toward LM interpretability. For instance, Abdou et al. (2021) find that pretrained LMs' representations of color terms are geometrically aligned with CIELAB, a color space based on human perception. Li et al. (2021) fine-tune pretrained LMs on text that describes evolving situations, then probe whether the model states track entity states. Unlike this research, we study an LM trained from scratch, which (1) yields insights into how semantics emerge in the representations of LMs over time and (2) allows us to rigorously evaluate alternative explanations of the LM behavior. Li et al. (2023) train a Transformer on transcripts of Othello, then use probes to intervene on the LM's internal representations; they find that the LM's subsequent generations are consistent with the edited version of the extracted board state. In contrast, our semantic probing interventions introduce a novel method of distinguishing the contributions of the LM and probe. To the best of our knowledge, we are also the first to apply probing to find evidence that the LM encodes the meaning of text ahead of generation.\\n\\nGrounding programs from text\\n\\nThe specific question of whether LMs of code can ground programs from text has received prior attention in the literature. Merrill et al. (2021) show that there exist programs whose semantics provably cannot be learned from text, albeit under strong assumptions not satisfied by our setting. Bender & Koller (2020) concede that meaning could be learned from programs paired with unit tests, but assert this requires a \\\"learner which has been equipped by its human developer with the ability to identify and interpret unit tests.\\\" Our research, in contrast, provides empirical evidence that LMs trained only to predict the next token can, in fact, learn aspects of the program semantics.\\n\\nProgram synthesis with LMs\\n\\nThere is a growing body of work on training large-scale, Transformer-based LMs for program synthesis (Austin et al., 2021; Chen et al., 2021a; Li et al., 2022; Nijkamp et al., 2023; Fried et al., 2023; Rozi`ere et al., 2023), as well as program synthesis as a benchmark for LMs (Hendrycks et al., 2021; Liang et al., 2022). Several of these works have observed that the BLEU score with respect to a reference solution is not a good predictor of the LM's competency, which complements our results regarding the LM's perplexity on the training corpus.\\n\\nProbing\\n\\nProbing (Shi et al., 2016; Belinkov & Glass, 2019) is widely used as a technique to investigate the inner workings of LMs. A key challenge is controlling for what is learned by the probe rather than represented in the LM (Belinkov, 2022). The standard methodology is to establish a baseline measurement on a task for which the model states are assumed to be meaningless. Hewitt & Liang (2019) develop control tasks for word-level properties in the context of probing for parts of speech in LM representations. They compare against the performance of a probe that maps from the model states to a dataset with a random part of speech assigned to each word. In our case, the control task approach would assign random features to each program state; however, this also destroys the syntactic structure of the program, and hence cannot be used to test the syntactic record hypothesis. To address this, we introduce semantic probing interventions, a control framework for probing that intervenes on the semantics of individual operations while preserving the overall syntax of programs. As our techniques specifically advance the study of semantics in LMs, we believe our contributions can be broadly applicable to future interpretability research, particularly for investigations involving meaning (rather just than syntactic structure).\\n\\n6. Conclusion\\n\\nThis paper presents empirical evidence that LMs of code can acquire the formal semantics of programs from next token prediction. We find that, when training an LM to model text consisting of examples of input-output specifications followed by programs, the learning process of the LM appears to undergo 3 distinct phases, with the second half of training characterized by a strong, linear correlation between the emerging representations of the semantics and the ability of the LM to synthesize programs that correctly implement unseen specifications. We also find representations of future semantics, suggesting a notion of intent during generation. Further explorations of these dynamics could yield deeper insights into the behavior of LMs.\\n\\nWe also present semantic probing interventions, a framework for the application of probes\u2014a standard tool for interpreting the learned representations of, e.g., neural models\u2014to understanding whether representations capture information related to the underlying semantics of a domain. Specifically, we design experiments capable of distinguishing whether the probe's measurement is indicative of (1) the presence of semantic information intrinsic to the representations or (2) the ability of the probe to perform the task itself, with purely syntactic information encoded in the representations. This also allows us to justify the use of nonlinear probes that, absent our technique, are more likely yield false positives due to having more capacity to learn the task; we see moving beyond shallow probes as a way to progress toward understanding whether (and how) LMs represent more complex concepts.\\n\\nMore broadly, the question of what exactly LMs are learning from text has garnered considerable interest in recent years, driven by the increasingly impressive performance of frontier models. We believe the techniques and insights presented in this work can serve as a principled foundation for future studies of the capabilities and limitations of LMs.\"}"}
{"id": "8PTx4CpNoT", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\n(a) Probing 2 states into the past.\\n(b) Probing 1 state into the past.\\n(c) Probing the current state.\\n(d) Probing 1 state into the future.\\n(e) Probing 2 states into the future.\\n\\nFigure 11: Plotting the difference in the semantic content when decoding according to the reference programs vs. greedy decoding. A positive difference indicates that greedy decoding yields a higher semantic content.\\n\\nBecause the LM is specifically trained to minimize its perplexity on the reference programs, we might expect the LM representations to also be closely aligned with the semantics of the training corpus.\\n\\nFigure 11 plots the results of this experiment. For the linear probe, extracting both past and future semantic states does appear easier up until the beginning of the last phase of training. However, by the end of training, probing for the abstract states from greedy decoding all exhibit higher semantic content than the reference programs, which suggests that the LM representations are indeed better aligned with greedy decoding. We also observe that the difference is particularly large for the present and future semantic states. This can be explained by the fact that the future program states are essentially a random process (due to the program itself being generated according to a random sampling procedure), and hence, there is a fundamental limit to how informative the LM's representation can be. Note that even the \\\"current\\\" abstract state is a random process, as the current abstract state also results from a token that the LM has yet to see: the probe's task for the current abstract state is to predict \\\\((\\\\text{state}_{\\\\text{prog}})^i\\\\) from \\\\((\\\\text{state}_{\\\\text{LM}})^i\\\\) in Equation (8).\\n\\nFinally, we note that the difference (between the semantic contents with respect to the reference programs vs. generated programs) grows as the LM's generative accuracy improves; we conjecture that the LM's preference for greedy decoding may increase as it becomes better calibrated to the training data as a general principle. We leave a more detailed exploration of how different decoding strategies affect the coherence of the LM's internal state to future work.\\n\\nB.4. Semantics are inferred, not retrieved\\n\\nIn this section, we evaluate the retrieval hypothesis: the semantic content can be attributed entirely to the LM recalling previously seen training data. Similar to the syntactic record hypothesis, this hypothesis offers another potential explanation for the results in Section 3 which is consistent with MH. For instance, if we test the LM on (A) \\\"turnRight, move, turnLeft\\\" and the LM was trained on (B) \\\"turnRight, move\\\"., then the LM may have seen the second entry in the trace of (A) as the output of (B).\\n\\nWe design our experimental setting specifically to test this hypothesis: as the training corpus contains only programs of length 6 or greater, while the test set contains programs of length 1 to 10, it is impossible for the LM to \\\"retrieve\\\" program states corresponding to the first 5 entries in any trace. Hence, we argue that any representations of unseen program states...\"}"}
{"id": "8PTx4CpNoT", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| depth | direction | position | obstacle | direction | position | obstacle |\\n|-------|-----------|----------|----------|-----------|----------|----------|\\n|       | linear    | MLP-1    | MLP-2    | linear    | MLP-1    | MLP-2    |\\n|       |           |          |          |           |          |          |\\n|       | unseen    |          |          | seen      |          |          |\\n| 1     | 46.7      | 85.0     | 73.7     | 76.7      | 92.9     | 78.6     |\\n| 2     | 47.9      | 71.3     | 73.0     | 76.4      | 85.2     | 77.7     |\\n| 3     | 49.8      | 64.0     | 73.5     | 76.7      | 80.1     | 78.2     |\\n| 4     | 50.5      | 61.4     | 74.8     | 78.8      | 76.9     | 79.1     |\\n| 5     | 51.5      | 60.1     | 76.0     | 79.7      | 74.9     | 80.2     |\\n| 6     | 59.9      | 60.9     | 74.6     | 82.1      | 73.7     | 79.9     |\\n| 7     | 59.2      | 58.8     | 74.3     | 79.1      | 68.6     | 79.3     |\\n| 8     | 51.7      | 57.1     | 76.0     | 69.8      | 62.2     | 79.9     |\"}"}
{"id": "8PTx4CpNoT", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meaning in Language Models Trained on Programs must be inferred by the LM according to the semantics of the programming language. Table 3 displays the accuracy of the probes at the end of training; as the probe achieves non-trivial accuracy when tracing unseen programs (of length 5 or less), we argue that the observed semantic content cannot be fully attributed to a retrieval-like process, and instead requires the LM to perform some degree of generalization over the semantics. We also remark that Appendix B.3 offers another piece of evidence in support of this claim, as the semantic content is lower when using the reference programs (whereas we would expect the ability of the LM to perform retrieval to increase when following the training distribution more closely).\\n\\nWe make 2 additional observations. First, the effect of retrieval differs by feature, and appears more pronounced the \u201csimpler\u201d the feature is to compute. For instance, with the direction feature, there is a local optimum in the accuracy of the linear and 1-layer MLP probes at depth 6, which is indicative that some amount of retrieval is occurring in the representations of the LM: as the direction of the robot depends only on the initial direction and program, the LM has likely seen the answer previously in its training data. Conversely, the feature corresponding to the position of the robot decreases monotonically with depth for all 3 probes (with a very small exception at depth 6 for the linear probe), which is consistent with execution (deeper program states are harder to trace without a mistake due to compounding errors). Second, deeper probes exhibit lower sensitivity to retrieval across all 3 features. As deeper probes can extract more complex representations, this suggests that (1) the results of retrieval are represented more shallowly in the LM states, while (2) benefits of the shallow retrieval representations are less important once the probe is able to extract the deeper semantic representations.\\n\\nFigure 12 also plots, for all 3 probes and separated by depth across all training steps, (1) the semantic content for the original semantics and (2) the excess semantic content with respect to the flip and adversarial semantics. The results show that the excess over the flip and adversarial semantics begins around zero in the middle phase (for all depths), then becomes positive by the end of training. This behavior is consistent with the results in the main text that suggest that the LM learns semantics over the latter half of training. We also point out the excess semantic content for the flip semantics as shown in Figures 12d to 12f increase as the depth increases from 1 to 5 (starting from close to 0 at depth 1), which we attribute to the high degree of correlation between the flip and original semantics (which degrades as the program length increases); beyond depth 5, the difference actually decreases, whereas we would expect the difference to be greater if the probe is simply relying on representations of retrieved program states in the original semantics. To summarize, while we do find evidence that the LM is performing some retrieval process, our results indicate that this process coexists with semantics in the representations of the LM.\\n\\nB.5. Interpreting programs without outputs\\n\\nWe conduct another series of experiments to explore whether the LM is capable of interpreting programs when only inputs are provided in the specification, and the outputs in the specification are obscured. Specifically, for every reference program and specification in the original trace datasets, we generate an input-only trace dataset according to the following loop (cf. Equation (1)):\\n\\n\\\\[\\n(state_{LM})_i = LM(input, empty, \\\\{(state_{LM})_j\\\\}_{j=1}^{i-1})\\n\\\\]\\n\\n(11)\\n\\n\\\\[token_i = prog_{ref}[i]\\\\]\\n\\n(12)\\n\\n\\\\[(state_{prog})_i = exec(token_i, (state_{prog})_{i-1})\\\\]\\n\\n(13)\\n\\nwhere \\\\(prog_{ref}\\\\) is the reference program and \\\\(prog_{ref}[i]\\\\) is the \\\\(i^{th}\\\\) program token in the reference program; \\\\(input = (state_{prog})_0\\\\) is the input state from the specification; and we replace every output in the specification with an empty Karel grid. In other words, we force the LM to generate the reference program given only inputs and no outputs.\\n\\nWe emphasize that this task is firmly outside of the distribution of the training data, and there is no guarantee that the LM states will even be coherent. Hence, any ability to interpret programs in this setting could be considered emergent behavior on the part of the LM. Additionally, as the text still contains the necessary information for interpreting the program (as the output is not necessary for this task), we also expect to find that probing for the alternative semantics yields accuracies that are no higher than the original semantics.\\n\\nTable 4 displays the results. As expected, the semantic content for the original abstract states is significantly degraded (compared to Table 2), especially when probing into the future, which we attribute mainly to obscuring the outputs. We also see that there the performance of the 3 probes is highly compressed when probing into the future, i.e., deeper probes do not perform much better than shallow probes. This can be explained by the same reasoning as in Appendix B.3, i.e.,\"}"}
{"id": "8PTx4CpNoT", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4: The results of our input-only probing experiments. For each of the original, flip, and adversarial semantics, we report the semantic content (SC) at the end of training for 2 abstract states into the past (-2, -1), the current state (0), and 2 abstract states into the future (+1, +2), using linear, 1-layer MLP, and 2-layer MLP probes. We also regress the SC against the generative accuracy over the second half of training ($R^2(p)$). For each of the alternative semantics, we additionally compute the difference with respect to the original semantics ($\\\\Delta$) and regress the difference against the generative accuracy over the second half of training as ($R^2(p)$) of $\\\\Delta$. Highlighted cells are statistically significant at a level of $p < 0.05$ with an $R^2$ of at least 50%; all such correlations are positive.\\n\\n|        | original | flip | adversarial |\\n|--------|----------|------|-------------|\\n|        | SC       | R^2(p) | SC          | R^2(p) | \u2206       | R^2(p) | \u2206       | R^2(p) |\\n| -2     | 57.7     | < .001 | 57.5        | < .001 | 0.3     | 0.021  | 55.8    | 0.001  |\\n| -1     | 60.5     | < .001 | 59.8        | < .001 | 0.7     | < .001 | 84.1    | < .001 |\\n| 0      | 54.6     | < .001 | 54.2        | < .001 | 0.4     | < .001 | 92.9    | < .001 |\\n| 1      | 52.4     | < .001 | 52.2        | < .001 | 0.2     | < .001 | 75.7    | 0.002  |\\n| 2      | 51.7     | 0.005  | 51.6        | 0.006  | 0.1     | 0.012  | 62.1    | 0.012  |\\n|        | 77.3     | < .001 | 77.2        | < .001 | 0.0     | 0.824  | 62.6    | 0.023  |\\n|        | 79.8     | < .001 | 79.5        | < .001 | 0.3     | 0.076  | 64.7    | 0.020  |\\n|        | 62.3     | < .001 | 62.3        | < .001 | 0.1     | 0.558  | 50.2    | 0.005  |\\n|        | 55.3     | < .001 | 55.3        | < .001 | 0.0     | 0.194  | 49.1    | 0.071  |\\n|        | 52.9     | 0.005  | 52.7        | 0.003  | 0.1     | 0.974  | 48.5    | 0.021  |\\n|        | 81.6     | 0.006  | 81.7        | 0.005  | -0.1    | 0.710  | 76.1    | 0.055  |\\n|        | 82.5     | 0.005  | 82.4        | 0.005  | 0.1     | 0.245  | 77.7    | 0.028  |\\n|        | 63.8     | 0.006  | 63.7        | 0.005  | 0.2     | 0.140  | 51.3    | 0.015  |\\n|        | 56.7     | 0.002  | 56.8        | 0.005  | -0.1    | 0.872  | 49.3    | 0.039  |\\n|        | 53.9     | 0.015  | 53.7        | 0.015  | 0.2     | 0.580  | 48.8    | 0.035  |\"}"}
{"id": "8PTx4CpNoT", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\ninputs\\noutputs\\nreference program\\nLM prediction\\n\\n```\\ndef run() {\\n  putMarker()\\n  turnRight()\\n  move()\\n  putMarker()\\n  move()\\n}\\n```\\n\\nLM training corpus\\ninputs + outputs +\\nreference program\\n\\ntest prompt\\ninputs + outputs = ???\\n\\ntokens\\nprogram tokens\\nputMarker()\\npickMarker()\\nturnLeft()\\nturnRight()\\nmovest\\ngrid world tokens\\nrobot:\\nmarkers:\\nobstacles:\\nempty:\\n\\n```\\ndef run() {\\n  turnLeft()\\n  putMarker()\\n  turnRight()\\n  turnRight()\\n  move()\\n  putMarker()\\n  move()\\n}\\n```\\n\\nFigure 1: An overview of the experimental setting. We construct training examples by sampling a random reference program, then sampling 5 random inputs and executing the program to obtain the corresponding 5 outputs. The LM is trained for next-token prediction on a corpus of examples consisting of the interleaved inputs and outputs, then the reference program. At test time, we provide an unseen input-output specification to the LM, and use greedy decoding to predict a program.\\n\\nWe summarize our main contributions as follows:\\n\\nEmergence of meaning\\nWe present results that are consistent with the emergence of representations of formal semantics in LMs trained to perform next token prediction (Section 3). In particular, we use the trained LM to generate programs given input-output examples, then train small probing classifiers to extract information about the intermediate program states from the hidden states of the LM. We find that the LM states encode (1) an abstract semantics\u2014specifically, an abstract interpretation\u2014that tracks the intermediate states of the program through its execution and (2) predictions of future program states corresponding to program tokens that have yet to be generated. During training, these representations of semantics emerge in lockstep with the LM's ability to generate correct programs.\\n\\nSemantic probing interventions\\nWe present a novel interventional technique for disentangling the contributions of the LM and the probe when probing for semantics (Section 4). Specifically, one possible explanation for the results in Section 3 is that the LM states contain a (purely syntactic) record of the inputs and generated program, from which the probe learns to generate the abstract interpretation. Our key insight is that, if this were true, we should be able to supervise a new probe to interpret the (hypothetical) syntactic record according to an appropriately chosen set of alternative semantics and achieve accuracies similar to the original semantics. However, we find that probes trained on alternative semantics achieve lower accuracies, which is consistent with the proposition that LM states are aligned with the original semantics and inconsistent with the proposition that the LM states simply encode a syntactic record.\\n\\nTaken together, Sections 3 and 4 present evidence that rejects MH: we find that, contrary to MH, representations of formal semantics emerge via next token prediction in our setting. We therefore conclude that training LMs of code solely to predict the next token does not imply that they cannot develop accurate models of the underlying domain's semantics. More broadly, we see programs and their precise formal semantics as a promising direction for working toward a deeper understanding of the behavior of LMs, such as whether or how LMs acquire and use semantic representations of the underlying domain more generally.\\n\\n2. Background and setting\\nThis section provides brief background of the program trace as our chosen model of formal program semantics, introduces the language modeling task and setting, and presents qualitative results from our training run.\\n\\n2.1. Program tracing as meaning\\nA foundational topic in the theory of programming languages, formal semantics (Winskel, 1993) is the study of how to formally specify the meaning of programs. In this work, we use the small step semantics (Plotkin, 1981) to generate program traces (Cousot, 2002): given an input (i.e.,\"}"}
{"id": "8PTx4CpNoT", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\nan assignment of values to input variables), the trace is the sequence of intermediate program states traversed by the program as it executes over the input. A (syntactic) program can then be formally assigned a (semantic) meaning, given by the collection of all of its traces.\\n\\nBeyond its amenability to formal analysis, tracing is attractive as a model of program semantics for several reasons. In novice programmers, the ability to accurately trace a piece of code has been directly linked to the ability to explain the code (Lopez et al., 2008; Lister et al., 2009), and computer science education has emphasized tracing as a method of developing program understanding (Hertz & Jump, 2013) and localizing reasoning errors (Sorva, 2013). Expert programmers also rely on tracing, both as a mental process (Letovsky, 1987) and via trace-based debuggers.\\n\\nAbstract interpretation (Cousot & Cousot, 1977) is a technique for producing sound approximations of concrete program semantics. For example, given the multiplication operator \\\\( \\\\times \\\\) over the integers \\\\( \\\\mathbb{Z} \\\\), we could define an abstract interpretation \\\\( \\\\alpha \\\\) by abstracting each integer to its sign \\\\( \\\\alpha \\\\):\\n\\n\\\\[\\n\\\\mathbb{Z} \\\\rightarrow \\\\{ -\\\\mathbb{Z}, 0, +\\\\mathbb{Z} \\\\}\\n\\\\]\\n\\nIn this paper, we use abstract interpretation to establish a precise, formal connection between the concrete program states and the abstract program states measured in our experiments.\\n\\n2.2. Language modeling task and training\\n\\nKarel domain\\n\\nKarel is an educational programming language (Pattis, 1994) developed at Stanford in the 1970s, which is still in use in their introductory programming course today (Piech & Roberts, January 2019; CS106A, 2023). The domain features a robot (named Karel) navigating a 2D grid world with obstacles while leaving and picking up markers. Since being introduced by Devlin et al. (2017), Karel has been adopted by the program synthesis community as a standard benchmark (Bunel et al., 2018; Shin et al., 2018; Sun et al., 2018; Chen et al., 2019; 2021b), in which input-output examples are provided, and the task is to produce a program which maps each input grid to its corresponding output grid.\\n\\nFigure 1 gives an overview of our domain. Each 8x8 grid world contains 4 types of tokens: the robot controlled by the program, which we represent graphically with an arrow in the direction that the robot currently faces (red); markers (blue); obstacles (brown); or an empty space (gray). We use a subset of the language consisting of the following 5 operations:\\n\\n- move advances the robot by one space in the facing direction if there is not an obstacle ahead (otherwise, the robot does not move);\\n- turnRight and turnLeft turn the robot right and left, respectively;\\n- putMarker and pickMarker increment and decrement the number of markers on the space occupied by the robot (with no effect if there are 10 and 0 markers), respectively. The robot also obscures the number of markers on the space it currently occupies; the obscured markers have no effect on the correctness of the program. Note that there is no control flow and all programs consist of straight line programs, so that each operation produces exactly one program state when the program is traced.\\n\\nSynthetic dataset construction\\n\\nOur training set consists of 500,000 randomly sampled Karel programs of lengths between 6 and 10, inclusive. For each program, we randomly sample 5 grid worlds as input, then evaluate the program to obtain 5 output grids. We create textual representations for Karel grid worlds by scanning the grid in row order, with one token per grid space. Each training sample consists of the concatenation of the 5 input-output grid world states (the specification), followed by the reference program. The language modeling task thus consists of predicting a program from a (partial) specification in the form of input-output grid world states. Note that (1) the training set consists only of programs which correctly implement the preceding specification and (2) the intermediate states of the trace are not observed in the training data. We also generate a test set of 10,000 specifications in the same manner, except that we sample reference programs of length between 1 and 10.\\n\\nTraining an LM to synthesize programs\\n\\nWe train an off-the-shelf Transformer (Vaswani et al., 2017) to perform next token prediction on our dataset. Specifically, we train a 350M parameter variant of the CodeGen architecture (Nijkamp et al., 2023) in the HuggingFace Transformers library (Wolf et al., 2020) from initialization for approximately 2.5 billion tokens. Appendix A contains further details.\\n\\n2.3. Results\\n\\nFigure 2 plots the main results from our training run. To measure the ability of the LM to synthesize programs, we use the LM to generate text starting from a specification using greedy decoding constrained to program tokens, i.e., the generated text is guaranteed to be a syntactically well-formed program. The program is correct if it maps each input in the specification to its corresponding output; we define the generative accuracy as the percentage of correct programs over the test set. The LM reaches 92.4% generative accuracy at the end of training.\\n\\nWe also track two additional metrics related to the syntax (or form) of the LM outputs: the number of unique programs generated by the LM over the test set (black) and the perplexity of the LM over different subsets of tokens in the test set (orange). In particular, the solid orange line plots the perplexity on the reference programs, the dashed orange line plots the perplexity on the programs generated by the LM, and the dotted orange line plots the perplexity over all tokens. Note that overall perplexity improves over the entire run, indicating that the training dynamics remain stable.\"}"}
{"id": "8PTx4CpNoT", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\nFigure 2: Three distinct phases during training: babbling (gray), syntax acquisition (orange), and semantics acquisition (yellow), based on qualitative differences in the evolution of perplexity (orange), generative accuracy (blue), and diversity of output (black). The number of unique programs is measured over the test set, which contains 10,000 specifications and 6,473 unique reference programs.\\n\\nWe observe 3 distinct phases during training: in the babbling phase (up to 50% of training, gray background), the generated programs are often highly repetitive, with a plateau in perplexity on the reference programs starting around 20%. The generative accuracy stays flat at around 10%. The next phase (50-75% of training, orange background) exhibits a sharp increase in the diversity of the generated outputs with a corresponding decrease in perplexity on the reference programs\u2014i.e., the LM begins to model the program tokens\u2014with a modest increase in generative accuracy (from 10% to 25%). In the final phase (from 75% to the end of training, yellow background), the diversity of the generated programs stays roughly constant, and the perplexity on the reference programs continues to improve at the previous rate. Conversely, the generative accuracy of the LM increases rapidly, from 25% to over 90%. As such, the middle phase sees the most significant change in the syntactic properties of the LM's generation, while the final phase is characterized by a rapid improvement in the LM's ability to generate semantically correct output. We hence identify these two phases primarily with syntax acquisition and semantics acquisition, respectively.\\n\\nFinally, while it is natural for the perplexity of the generated programs to be lower than the reference programs (due to the use of greedy decoding), the constant margin between the two perplexities suggests that the LM consistently underfits the program tokens in the training data, despite producing increasingly correct programs; we refer to Appendix B for further analyses. These dynamics suggest that the increase in generative accuracy over the course of LM training cannot be attributed entirely to the LM's ability to model the surface distribution of program tokens in the training corpus.\\n\\n3. Emerging representations of semantics\\n\\nWe train small probing classifiers to extract information about the program state from the hidden states of the LM. The idea is to prompt the LM to generate a program given some inputs, and check whether the LM states contain a representation of the intermediate program states as it generates the program. A positive result is consistent with the LM having learned to model the underlying semantics of the programs it generates, constituting evidence against MH.\\n\\n3.1. Probing for representations of the program trace\\n\\nTrace dataset construction\\n\\nEvery 4000 steps (or roughly 5%) of training, we take a snapshot of (1) the hidden states of the LM as it generates programs using next-token prediction and (2) the corresponding program states after evaluating the partial program on each of the 5 specified inputs. Specifically, starting from an input-output specification, we generate according to the standard autoregressive loop:\\n\\n\\\\( \\\\text{state}_{LM}^i = \\\\text{LM}(\\\\text{input}, \\\\text{output}, \\\\{\\\\text{state}_{LM}^j\\\\}^i_{j=1}) \\\\)  \\n\\n\\\\( \\\\text{token}_i = \\\\text{greedy}(\\\\text{LM head}(\\\\text{state}_{LM}^i)) \\\\)  \\n\\n\\\\( \\\\text{state}_{prog}^i = \\\\text{exec}(\\\\text{token}_i, \\\\text{state}_{prog}^{i-1}) \\\\)\\n\\nwhere each \\\\( \\\\text{state}_{LM}^i \\\\) is a hidden state, \\\\( \\\\text{state}_{prog}^0 \\\\) is the inputs from the specification, \\\\text{greedy} performs greedy decoding (constrained to program tokens), and \\\\text{exec} executes a program token on the program state. We generate up to 14 tokens, or until the model outputs a special end-of-sequence token (\\\\text{<EOS>}).\\n\\nWe average the hidden state over the layer dimension, so that the snapshot is a 1-dimensional tensor of size 1024, and call this the model state. We repeat this process for each of the training and test sets, producing two trace datasets consisting of aligned pairs of \\\\( \\\\text{state}_{LM}^i, \\\\text{state}_{prog}^i \\\\) from all the programs generated from the specifications in the training and test sets, respectively.\\n\\nProbe training\\n\\nFor each training trace dataset, we train a set of probes (ranging from linear to 2-layer MLPs) to predict features of the program state given the model state, using standard supervised learning. The features consist of (1) the facing direction of the robot, (2) the position of the robot as an offset from its starting position, and (3) whether the robot is current facing an obstacle, i.e.:  \\n\\n\\\\[ \\\\alpha: \\\\text{state}_{prog}^7 \\\\rightarrow (\\\\text{position}, \\\\text{direction}, \\\\text{obstacle}) \\\\]\"}"}
{"id": "8PTx4CpNoT", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\nFigure 3: An overview of the trace dataset construction for the probe task. Given a specification consisting of input and output for some (unobserved) reference program, we use the trained LM to generate a program using next-token prediction (dotted blue arrows), yielding a sequence of \\\\((\\\\text{state}_{LM})_i\\\\). At the same time, each token is an operation that induces a transition in the program state to \\\\((\\\\text{state}_{prog})_i\\\\). The probe is trained to predict \\\\((\\\\text{state}_{prog})_i\\\\) given \\\\((\\\\text{state}_{LM})_i\\\\). Note that, while the depicted generation is correct as the final \\\\((\\\\text{state}_{prog})_k\\\\) is equal to the specified output state, this need not be the case in general (i.e., the LM may generate incorrect programs). For clarity, we depict the specification as a single input-output example (rather than 5); autoregressive edges are also hidden.\\n\\nAs the features are an abstraction of the full program state, we refer to them collectively as the abstract state. We then evaluate the accuracy of the probe over the corresponding test trace dataset, and define the semantic content as the geometric mean (over the 3 features). As tracing the abstract state is formally equivalent to performing an abstract interpretation of the program, the semantic content measures, in a precise sense, the extent to which the model states encode an abstract interpretation of the formal semantics.\\n\\n3.2. Results\\n\\nThis section presents a summary of the main results; Appendix B contains additional results, including results for individual features and all 3 probes. We also evaluate several additional hypotheses, including that the semantic content is due to a retrieval process (i.e., the LM is simply recalling the abstract states from previously seen data), which can be viewed as a variation on MH. The main idea is that the training corpus contains only programs of length 6 or greater, so the LM cannot use retrieval for shorter programs and must learn to infer (rather than retrieve) the semantics.\\n\\nEmergence of semantics is correlated with generative accuracy. Figure 4 plots the results of our probing experiments. Our first observation is that the semantic content during the babbling phase is extremely noisy, which can be attributed to a lack of diversity in the outputs of the LM, so that the probe only needs to fit a trivial set of semantics. For instance, about 20% of the way through training, the LM degenerates to generating a single program of 9 MARKER tokens, regardless of the specification. Conversely, all 3 probes reach a minimum during the syntax acquisition phase (where the LM outputs grow more diverse), and steadily increase over the semantics acquisition phase of training. This result is consistent with the proposition that the hidden states of the LM do in fact contain (relatively) shallow encodings of the abstract state, and crucially these representations emerge within an LM trained purely to perform next token prediction on text. Regressing generative accuracy against semantic content during the second half of training also yields strong, statistically significant results.\"}"}
{"id": "8PTx4CpNoT", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nWe would like to thank Jacob Andreas, Omar Costilla-Reyes, Kai Jia, Jason Kim, Armando Solar-Lezama, and Yichen Yang for their helpful comments and discussions on an earlier version of this paper. We also gratefully acknowledge support from DARPA Grants HR001120C0015, HR001120C0191, and N6600120C4025. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the United States Government.\\n\\nImpact statement\\n\\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.\\n\\nTo aid in reproducibility, we open source all our code, including the code we use to generate the training data, train the LM, and conduct the probing experiments, at https://github.com/charlesjin/emergent-semantics.\\n\\nReferences\\n\\nAbdou, M., Kulmizev, A., Hershcovich, D., Frank, S., Pavlick, E., and S\u00f8gaard, A. Can language models encode perceptual structure without grounding? a case study in color. In Proceedings of the 25th Conference on Computational Natural Language Learning, pp. 109\u2013132, 2021.\\n\\nAnthropic, A. The claudelook model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024.\\n\\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n\\nBelinkov, Y. and Glass, J. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49\u201372, 2019. doi: 10.1162/tacl_a_00254. URL https://aclanthology.org/Q19-1004.\\n\\nBender, E. M. and Koller, A. Climbing towards nlu: On meaning, form, and understanding in the age of data. In Proceedings of the 58th annual meeting of the association for computational linguistics, pp. 5185\u20135198, 2020.\\n\\nBommasani, R., Liang, P., and Lee, T. Holistic evaluation of language models. Annals of the New York Academy of Sciences, 2023.\\n\\nBunel, R., Hausknecht, M., Devlin, J., Singh, R., and Kohli, P. Leveraging grammar and reinforcement learning for neural program synthesis. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=H1Xw62kRZ.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021a.\\n\\nChen, X., Liu, C., and Song, D. Execution-guided neural program synthesis. In International Conference on Learning Representations, 2019.\\n\\nChen, X., Song, D., and Tian, Y. Latent execution for neural program synthesis beyond domain-specific languages. Advances in Neural Information Processing Systems, 34:22196\u201322208, 2021b.\\n\\nCousot, P. Constructive design of a hierarchy of semantics of a transition system by abstract interpretation. Theoretical Computer Science, 277(1-2):47\u2013103, 2002.\\n\\nCousot, P. and Cousot, R. Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints. In Proceedings of the 4th ACM SIGACT-SIGPLAN symposium on Principles of programming languages, pp. 238\u2013252, 1977.\\n\\nCS106A. CS106A: Programming Methodologies (spring 2023). https://web.archive.org/web/20230515003120/https://web.stanford.edu/class/cs106a/, 2023. URL https://web.stanford.edu/class/cs106a. Accessed: 2023-05-14.\\n\\nDevlin, J., Bunel, R. R., Singh, R., Hausknecht, M., and Kohli, P. Neural program meta-induction. Advances in Neural Information Processing Systems, 30, 2017.\\n\\nFan, A., Gokkaya, B., Harman, M., Lyubarskiy, M., Sen-gupta, S., Yoo, S., and Zhang, J. M. Large language models for software engineering: Survey and open problems. arXiv preprint arXiv:2310.03533, 2023.\\n\\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, S., Zettlemoyer, L., and Lewis, M. Incoder: A generative model for code infilling and synthesis. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=hQwb-lbM6EL.\"}"}
{"id": "8PTx4CpNoT", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\nGemini Team, Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lillicrap, T., Lazaridou, A., Firat, O., Molloy, J., Isard, M., Barham, P. R., Hennigan, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Doherty, R., Collins, E., Meyer, C., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Tucker, G., Piqueras, E., Krikun, M., Barr, I., Savinov, N., Danihelka, I., Roelofs, B., White, A., Andreassen, A., von Glehn, T., Yagati, L., Kazemi, M., Gonzalez, L., Khaliman, M., Sygnowski, J., Frechette, A., Smith, C., Culp, L., Proleev, L., Luan, Y., Chen, X., Lottes, J., Schucher, N., Lebron, F., Rrustemi, A., Clay, N., Crone, P., Kocisky, T., Zhao, J., Perz, B., Yu, D., Howard, H., Bloniarz, A., Rae, J. W., Lu, H., Sifre, L., Maggioni, M., Alcober, F., Garrette, D., Barnes, M., Thakoor, S., Austin, J., Barth-Maron, G., Wong, W., Joshi, R., Chaabouni, R., Fatiha, D., Ahuja, A., Liu, R., Li, Y., Cogan, S., Chen, J., Jia, C., Gu, C., Zhang, Q., Grimstad, J., Hartman, A. J., Chadwick, M., Tomar, G. S., Garcia, X., Senter, E., Taropa, E., Pillai, T. S., Devlin, J., Laskin, M., de Las Casas, D., Valter, D., Tao, C., Blanco, L., Badia, A. P., Reitter, D., Chen, M., Brennan, J., Rivera, C., Brin, S., Iqbal, S., Surita, G., Labanowski, J., Rao, A., Winkler, S., Parisotto, E., Gu, Y., Olszewska, K., Zhang, Y., Addanki, R., Miech, A., Louis, A., Shafey, L. E., Teplyashin, D., Brown, G., Catt, E., Attaluri, N., Balaguer, J., Xiang, J., Wang, P., Ashwood, Z., Briukhov, A., Webson, A., Ganapathy, S., Sanghavi, S., Kannan, A., Chang, M.-W., Stjerngren, A., Djolonga, J., Sun, Y., Bapna, A., Aitchison, M., Pejman, P., Michalewski, H., Yu, T., Wang, C., Love, J., Ahn, J., Bloxwich, D., Han, K., Humphreys, P., Sellam, T., Bradbury, J., Godbole, V., Samangooei, S., Damoc, B., Kaskasoli, A., Arnold, S. M. R., Vasudevan, V., Agrawal, S., Riesa, J., Lepikhin, D., Tanburn, R., Srinivasan, S., Lim, H., Hodkinson, S., Shyam, P., Ferret, J., Hand, S., Garg, A., Paine, T. L., Li, J., Li, Y., Giang, M., Neitz, A., Abbas, Z., York, S., Reid, M., Cole, E., Chowdhery, A., Das, D., Rogozi\u0144ska, D., Nikolaev, V., Sprechmann, P., Nado, Z., Zilka, L., Prost, F., He, L., Monteiro, M., Mishra, G., Welty, C., Newlan, J., Jia, D., Allamanis, M., Hu, C. H., de Liedekerke, R., Gilmer, J., Saroufim, C., Rijhwani, S., Hou, S., Shrivastava, D., Baddepudi, A., Goldin, A., Ozturel, A., Cassirer, A., Xu, Y., Sohn, D., Sachan, D., Amplayo, R. K., Swanson, C., Petrova, D., Narayan, S., Guez, A., Brahma, S., Landon, J., Patel, M., Zhao, R., Villela, K., Wang, L., Jia, W., Rahtz, M., Gim\u00e9nez, M., Yeung, L., Lin, H., Keeling, J., Georgiev, P., Mincu, D., Wu, B., Haykal, S., Saputro, R., Vodrahalli, K., Qin, J., Cankara, Z., Sharma, A., Fernando, N., Hawkins, W., Neyshabur, B., Kim, S., Hutter, A., Agrawal, P., Castro-Ros, A., van den Driessche, G., Wang, T., Yang, F., Yiin Chang, S., Komarek, P., McIlroy, R., Lu\u02c7ci\u00b4c, M., Zhang, G., Farhan, W., Sharman, M., Natsev, P., Michel, P., Cheng, Y., Bansal, Y., Qiao, S., Cao, K., Shakeri, S., Butterfield, C., Chung, J., Rubenstein, P. K., Agrawal, S., Mensch, A., Soparkar, K., Lenc, K., Chung, T., Pope, A., Maggiore, L., Kay, J., Jhakra, P., Wang, S., Maynez, J., Phuong, M., Tobin, T., Tacchetti, A., Trebacz, M., Robinson, K., Katariya, Y., Riedel, S., Bailey, P., Xiao, K., Ghelani, N., Aroyo, L., Slone, A., Houlsby, N., Xiong, X., Yang, Z., Gribovskaya, E., Adler, J., Wirth, M., Lee, L., Li, M., Kagohara, T., Pavagadhi, J., Bridgers, S., Bortsova, A., Ghemawat, S., Ahmed, Z., Liu, T., Powell, R., Bolina, V., Iinuma, M., Zablotskaia, P., Besley, J., Chung, D.-W., Dozat, T., Comanescu, R., Si, X., Greer, J., Su, G., Polacek, M., Kaufman, R. L., Tokumine, S., Hu, H., Buchatskaya, E., Miao, Y., Elhawaty, M., Sidhant, A., Tomasev, N., Xing, J., Greer, C., Miller, H., Ashraf, S., Roy, A., Zhang, Z., Ma, A., Filos, A., Besta, M., Blevins, R., Klimenko, T., Yeh, C.-K., Changpinyo, S., Mu, J., Chang, O., Pajarskas, M., Muir, C., Cohen, V., Lan, C. L., Haridasan, K., Marathe, A., Hansen, S., Douglas, S., Samuel, R., Wang, M., Austin, S., Lan, C., Jiang, J., Chiu, J., Lorenzo, J. A., Sj\u00f6sund, L. L., Cevey, S., Gleicher, Z., Avrahami, T., Boral, A., Srinivasan, H., Selo, V., May, R., Aisopos, K., Hussenot, L., Soares, L. B., Baumli, K., Chang, M. B., Recasens, A., Caine, B., Pritzel, A., Pavetic, F., Pardo, F., Gergely, A., Frye, J., Ramasesh, V., Horgan, D., Badola, K., Kassner, N., Roy, S., Dyer, E., Campos, V., Tomala, A., Tang, Y., Badawy, D. E., White, E., Mustafa, B., Lang, O., Jindal, A., Vikram, S., Gong, Z., Caelles, S., Hemsley, R., Thornton, G., Feng, F., Stokowiec, W., Zheng, C., Thacker, P., C\u00b8a\u02d8glar \u00a8Unl\u00a8u, Zhang, Z., Saleh, M., Svensson, J., Bileschi, M., Patil, P., Anand, A., Ring, R., Tsihlas, K., Vezer, A., Selvi, M., Shevlane, T., Rodriguez, M., Kwiatkowski, T., Daruki, S., Rong, K., Dafoe, A., FitzGerald, N., Gu-Lemberg, K., Khan, M., Hendricks, L. A., Pellat, M., Feinberg, V., Cobon-Kerr, J., Sainath, T., Rauh, M., Hashemi, S. H., Ives, R., Hasson, Y., Li, Y., Noland, E., Cao, Y., Byrd, N., Hou, L., Wang, Q., Sottiaux, T., Paganini, M., Lespiau, J.-B., Moufarek, A., Hassan, S., Shivakumar, K., van Amersfoort, J., Mandhane, A., Joshi, P., Goyal, A., Tung, M., Brock, A., Sheahan, H., Misra, V., Li, C., Raki\u00b4cevi\u00b4c, N., Dehghani, M., Liu, F., Mittal, S., Oh, J., Noury, S., Sezener, E., Huot, F., Lamm, M., Cao, N. D., Chen, C., Elsayed, G., Chi, E., Mahdieh, M., Tenney, I., Hua, N., Petrychenko, I., Kane, P., Scandinaro, D., Jain, R., Uesato, J., Datta, R., Sadovsky, A., Bunyan, O., Rabiej, D., Wu, S., Zhang, J., Vasudevan, G., Leurent, E., Alnahlawi, M., Georgescu, I., Wei, N., Zheng, I., Chan, B., Rabinovitch, P. G., Stanczyk, P., Zhang, Y., Steiner, D., Naskar, S., Azzam, M., Johnson, M., Paszke, A., Chiu, C.-C., Elias, J. S., Mohiuddin, A., Muhammad, F., Miao, J., Lee, A., Vieillard, N., Potluri, S., Park, J., Davoodi, E., Zhang, J., Stanway, J., Garmon, D., Karmarkar, A., Dong, Z., Lee, 11\"}"}
{"id": "8PTx4CpNoT", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meaning in Language Models Trained on Programs\\n\\nJ., Kumar, A., Zhou, L., Evens, J., Isaac, W., Chen, Z., Jia, J., Levskaya, A., Zhu, Z., Gorgolewski, C., Grabowski, P., Mao, Y., Magni, A., Yao, K., Snaider, J., Casagrande, N., Suganthan, P., Palmer, E., Irving, G., Loper, E., Faruqui, M., Arkatkar, I., Chen, N., Shafran, I., Fink, M., Casta\u00f1o, A., Giannoumis, I., Kim, W., Rybik\u00f3w, M., Sreevatsa, A., Prendki, J., Soergel, D., Goedeckemeyer, A., Gierke, W., Jafari, M., Gaba, M., Wiesner, J., Wright, D. G., Wei, Y., Vashisht, H., Kulizhskaya, Y., Hoover, J., Le, M., Li, L., Iwuanyanwu, C., Liu, L., Ramirez, K., Khorlin, A., Cui, A., LIN, T., Georgiev, M., Wu, M., Aguilar, R., Pallo, K., Chakladar, A., Repina, A., Wu, X., van der Weide, T., Ponnapalli, P., Kaplan, C., Simsa, J., Li, S., Dousse, O., Yang, F., Piper, J., Ie, N., Lui, M., Pasumarthi, R., Lintz, N., Vijayakumar, A., Thiet, L. N., Andor, D., Valenzuela, P., Paduraru, C., Peng, D., Lee, K., Zhang, S., Greene, S., Nguyen, D. D., Kurylowicz, P., Velury, S., Krause, S., Hardin, C., Dixon, L., Janzer, L., Choo, K., Feng, Z., Zhang, B., Singhal, A., Latkar, T., Zhang, M., Le, Q., Abellan, E. A., Du, D., McKinnon, D., Antropova, N., Bolukbasi, T., Keller, O., Reid, D., Finchelstein, D., Raad, M. A., Crocker, R., Hawkins, P., Dadashi, R., Gaffney, C., Lall, S., Franko, K., Filonov, E., Bulanova, A., Leblond, R., Yadav, V., Chung, S., Askham, H., Cobo, L. C., Xu, K., Fischer, F., Xu, J., Sorokin, C., Alberti, C., Lin, C.-C., Evans, C., Zhou, H., Dimitriev, A., Forbes, H., Banarse, D., Tung, Z., Liu, J., Omernick, M., Bishop, C., Kumar, C., Sterneck, R., Foley, R., Jain, R., Mishra, S., Xia, J., Bos, T., Cideron, G., Amid, E., Piccinno, F., Wang, X., Banzal, P., Gurita, P., Noga, H., Shah, P., Mankowitz, D. J., Polozov, A., Kushman, N., Krakow, V., Brown, S., Bateni, M., Duan, D., Firoiu, V., Thotakuri, M., Natan, T., Mohananey, A., Geist, M., Mudgal, S., Girgin, S., Li, H., Ye, J., Roval, O., Tojo, R., Kwong, M., Lee-Thorp, J., Yew, C., Yuan, Q., Bagri, S., Sinopalnikov, D., Ramos, S., Mellor, J., Sharma, A., Severyn, A., Lai, J., Wu, K., Cheng, H.-T., Miller, D., Sonnerat, N., Vnukov, D., Greig, R., Beattie, J., Caveness, E., Bai, L., Eisenschlos, J., Korrchema, A., Tsai, T., Jasarevic, M., Kong, W., Dao, P., Zheng, Z., Liu, F., Yang, F., Zhu, R., Geller, M., Teh, T. H., Sanmiya, J., Gladchenko, E., Trdin, N., Sozanschi, A., Toyama, D., Rosen, E., Tavakkol, S., Xue, L., Elkind, C., Woodman, O., Carpenter, J., Papamakarios, G., Kemp, R., Kafle, S., Grunina, T., Sinha, R., Talbert, A., Goyal, A., Wu, D., Owusu-Afriyie, D., Du, C., Thornton, C., Pont-Tuset, J., Narayana, P., Li, J., Fatehi, S., Wieting, J., Ajmeri, O., Uria, B., Zhu, T., Ko, Y., Knight, L., H\u00e9liou, A., Niu, N., Gu, S., Pang, C., Tran, D., Li, Y., Levine, N., Stolovich, A., Kalb, N., Santamaria-Fernandez, R., Goenka, S., Yustalim, W., Strudel, R., Elqursh, A., Lakshminarayanan, B., Deck, C., Upadhyay, S., Lee, H., Dusenberry, M., Li, Z., Wang, X., Levin, K., Hoffmann, R., Holtmann-Rice, D., Bachem, O., Yue, S., Arora, S., Malmi, E., Mirylenka, D., Tan, Q., Koh, C., Yeganeh, S. H., P\u00f6der, S., Zheng, S., Pongetti, F., Tariq, M., Sun, Y., Ionita, L., Seyedhosseini, M., Tafti, P., Kotikalapudi, R., Liu, Z., Gulati, A., Liu, J., Ye, X., Chrzaszcz, B., Wang, L., Sethi, N., Li, T., Brown, B., Singh, S., Fan, W., Parisi, A., Stanton, J., Kuang, C., Koverkathu, V., Choquette-Choo, C. A., Li, Y., Lu, T., Ittycheriah, A., Shroff, P., Sun, P., Varadarajan, M., Bahargam, S., Willoughby, R., Gaddy, D., Dasgupta, I., Desjardins, G., Cornero, M., Robenek, B., Mittal, B., Albrecht, B., Shenoy, A., Moiseev, F., Jacobsson, H., Ghaffarkhah, A., Rivi\u00e8re, M., Walton, A., Crepy, C., Parrish, A., Liu, Y., Zhou, Z., Farabet, C., Radebaugh, C., Srinivasan, P., van der Salm, C., Fidjeland, A., Scellato, S., Latorre-Chimoto, E., Klimczak-Pluci\u0144ska, H., Bridson, D., de Cesare, D., Hudson, T., Mendolicchio, P., Walker, L., Morris, A., Penchev, I., Maug\u0435\u0440, M., Guseynov, A., Reid, A., Odoom, S., Loher, L., Cotruta, V., Yenugula, M., Grewe, D., Petrushkina, A., Duerig, T., Sanchez, A., Yadlowsky, S., Shen, A., Globerson, A., Kurzrok, A., Webb, L., Dua, S., Li, D., Lahoti, P., Bhupatiraju, S., Hurt, D., Qureshi, H., Agarwal, A., Shani, T., Eyal, M., Khare, A., Belle, S. R., Wang, L., Tekur, C., Kale, M. S., Wei, J., Sang, R., Saeta, B., Liechty, T., Sun, Y., Zhao, Y., Lee, S., Nayak, P., Fritz, D., Vuyyuru, M. R., Aslanides, J., Vyas, N., Wicke, M., Ma, X., Bilal, T., Eltyshev, E., Balle, D., Martin, N., Cate, H., Manyika, J., Amiri, K., Kim, Y., Xiong, X., Kang, K., Luisier, F., Tripuraneni, N., Madras, D., Guo, M., Waters, A., Wang, O., Ainslie, J., Baldridge, J., Zhang, H., Pruthi, G., Bauer, J., Yang, F., Mansour, R., Gelman, J., Xu, Y., Polovets, G., Liu, J., Cai, H., Chen, W., Sheng, X., Xue, E., Ozair, S., Yu, A., Angermueller, C., Li, X., Wang, W., Wiesinger, J., Koukoumidis, E., Tian, Y., Iyer, A., Gurumurthy, M., Goldenson, M., Shah, P., Blake, M., Yu, H., Urbanowicz, A., Palomaki, J., Fernando, C., Brooks, K., Durden, K., Mehta, H., Momchev, N., Rahimtoroghi, E., Georgaki, M., Raul, A., Ruder, S., Redshaw, M., Lee, J., Jalan, K., Li, D., Perng, G., Hechtman, B., Schuh, P., Nasr, M., Chen, M., Milan, K., Mikulik, V., Strohman, T., Franco, J., Green, T., Hassabis, D., Kavukcuoglu, K., Dean, J., and Vinyals, O. Gemini: A family of highly capable multimodal models, 2023.\\n\\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.\\n\\nHertz, M. and Jump, M. Trace-based teaching in early programming courses. In Proceeding of the 44th ACM technical symposium on Computer science education, pp. 561\u2013566, 2013.\\n\\nHewitt, J. and Liang, P. Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.\"}"}
{"id": "8PTx4CpNoT", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
