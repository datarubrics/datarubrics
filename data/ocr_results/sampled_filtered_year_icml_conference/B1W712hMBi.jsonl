{"id": "B1W712hMBi", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A fundamental skill among human developers is the ability to understand and reason about program execution. As an example, a programmer can mentally simulate code execution in natural language to debug and repair code (aka rubber duck debugging). However, large language models (LLMs) of code are typically trained on the surface textual form of programs, thus may lack a semantic understanding of how programs execute at run-time. To address this issue, we propose \\\\textsc{NEXT}, a method to teach LLMs to inspect the execution traces of programs (variable states of executed lines) and reason about their run-time behavior through chain-of-thought (CoT) rationales. Specifically, \\\\textsc{NEXT} uses self-training to bootstrap a synthetic training set of execution-aware rationales that lead to correct task solutions (e.g., fixed programs) without laborious manual annotation. Experiments on program repair tasks based on \\\\textsc{MBP} and \\\\textsc{HUMANEVAL} demonstrate that \\\\textsc{NEXT} improves the fix rate of a PaLM 2 model, by 26.1\\\\% and 10.3\\\\% absolute, respectively, with significantly improved rationale quality as verified by automated metrics and human raters. Our model can also generalize to scenarios where program traces are absent at test-time.\"}"}
{"id": "B1W712hMBi", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def extract_values(string):\\n    str_list = string.replace('\"', '').split(',')\\n    return str_list\\n\\nThe problem spec asks to extract values between quotation marks of a string. Now let's look at the program. The program first uses replace() to remove all quotation marks, and then uses split() to split the string by commas. This seems to work fine for the second and third assertions. But for the first assertion, the return value is ['Python', 'PHP', 'Java'] while the expected output is ['Python', 'PHP', 'Java']. The reason is that the input string has spaces before and after each comma, and split() will split the string by commas and also keep the spaces. Here is a fixed version that uses re.findall() to extract the substrings between quotation marks:\\n\\n```python\\nimport re\\ndef extract_values(string):\\n    str_list = re.findall(r'\\\"(.*?)\\\"', string)\\n    return str_list\\n```\\n\\nFigure 1: NE \u00d7 T finetunes an LLM to naturalize execution traces into the chain-of-thought rationales for solving coding tasks. It performs iterative self-training from weak supervision, by learning from samples that lead to correct task solutions. A program (\\\\(\\\\epsilon\\\\)), an LLM solves the task (e.g., predict the fixed code \\\\(\\\\hat{y}\\\\)) using chain-of-thought (CoT) reasoning to generate a natural language rationale (\\\\(\\\\hat{r}\\\\)) leveraging the execution information. Intuitively, program traces encode useful debugging information such as line-by-line variable states (e.g., the value of str_list in \\\\(\\\\epsilon\\\\), Fig. 1) or any exceptions thrown, which could be useful for LLMs to identify and fix bugs by reasoning over the expected and the actual execution results (e.g., \\\"highlighted text\\\" in \\\\(\\\\hat{r}\\\\)). To help LLMs understand execution traces, NE \u00d7 T represent traces as compact inline code comments (e.g., # (1) str_list=... in \\\\(\\\\epsilon\\\\), more in \u00a73), without interrupting the original program structure.\\n\\nWhile execution traces capture informative runtime behavior, we find it challenging for LLMs to effectively leverage them out-of-box through CoT prompting (\u00a73). Therefore we opt to finetune LLMs on high-quality CoT rationales that reason about program execution (\u00a74). NE \u00d7 T uses weakly-supervised self-training (Zelikman et al., 2022) to bootstrap a synthetic training set by sampling rationales that lead to correct task solutions (e.g., fixed code \\\\(\\\\hat{y}\\\\) in Fig. 1) verified by unit tests (Ye et al., 2022). Using unit tests as weak supervision, NE \u00d7 T learns to discover task-specific, execution-aware NL rationales without relying on laborious manual annotation of rationales (Chung et al., 2022; Longpre et al., 2023; Lightman et al., 2023) or distilling such data from stronger teacher models (Gunasekar et al., 2023; Mukherjee et al., 2023; Mitra et al., 2023; Fu et al., 2023). NE \u00d7 T executes this self-training loop for multiple iterations (Anthony et al., 2017; Dasigi et al., 2019), solving more challenging tasks with improved success rate and rationale quality (\u00a75).\\n\\nWe evaluate NE \u00d7 T with the PaLM 2-L model (Anil et al., 2023) on two Python program repair tasks. Experiments (\u00a75) show that NE \u00d7 T significantly improves PaLM 2's ability to reason about program execution in natural language, improving the program fix rate on MBPP-R by 26.1% and HUMAN-EVAL-PIXUS by 10.3% absolute, respectively. When compared against a strong self-training program repair approach without predicting NL rationales (Ye et al., 2022), our model achieves comparable accuracy with significantly improved sample diversity. Interestingly, while our model learns to reason with pre-existing execution information in input program traces, it also generalizes to the out-of-distribution scenario where execution traces are unavailable at test-time. Finally, to measure the quality of model-generated rationales, we propose a proxy-based evaluation approach, which estimates rationale quality using the performance of smaller LLMs when prompted to solve the original task following those rationales from our models. Through both proxy-based evaluation and human annotation, we demonstrate that NE \u00d7 T produces helpful NL rationales which explain the causes of bugs while suggesting potential fixes. The generated rationales are of significantly higher quality compared to those from the base PaLM 2-L model.\"}"}
{"id": "B1W712hMBi", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NE: Teaching Large Language Models to Reason about Code Execution\\n\\n```python\\n1 def separate_odd_and_even(lst): # (0) lst=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10];\\n2 odd_list=[] # (1) odd_list=[];\\n3 even_list=[] # (2) even_list=[];\\n4 for n in lst: # (3) n=1; (5) n=2; (7) n=3; ...; (21) n=10;\\n5 if n % 2 == 1:\\n6 even_list.append(n) # (4) even_list=[1]; (8) even_list=[1, 3]; ...; (20) even_list=[1, 3, 5, 7, 9];\\n7 else:\\n8 odd_list.append(n) # (6) odd_list=[2]; (10) odd_list=[2, 4]; ...; (22) odd_list=[2, 4, 6, 8, 10];\\n9 return odd_list, even_list # (23) __return__=([2, 4, 6, 8, 10], [1, 3, 5, 7, 9])\\n```\\n\\nFigure 2: NEXT represents execution trace as inline comments. More details are discussed in \u00a72 and Appendix A.1.\\n\\n2. Task: Program Repair with Traces\\nHere we introduce our task of program repair with execution traces using chain-of-thought reasoning.\\n\\nProgram Repair with Execution Traces. As in Fig. 1, given an instruction $x$ and a buggy code solution $\\\\tilde{y}$, automated program repair (Le Goues et al., 2019) aims to generate a fixed program $\\\\hat{y}$ such that $\\\\hat{y}$ passes all test cases $t \\\\in T$ in an executor $E$, i.e., $E(\\\\hat{y}, T) = 1$ while $E(\\\\tilde{y}, T) = 0$.\\n\\nIn this paper we focus on the task of program repair using execution traces (Bouzenia et al., 2023). Specifically, a program trace $\\\\epsilon$ is a sequence of intermediate variable states after executing each statement in $\\\\tilde{y}$ against a test case $t$. Intuitively, traces record the computation of a program, and can provide useful debugging information (e.g., exceptions) to repair $\\\\tilde{y}$.\\n\\nTo use LLMs to repair programs with traces, we concatenate the task instruction, the buggy code, the test cases, and their execution traces as a prompt (Fig. 1). To help LLMs understand program traces, we design a prompt-friendly trace representation by formatting $\\\\epsilon$ as compact inline code comments (i.e., $\\\\epsilon$ in Fig. 1), as discussed later.\\n\\nCoT Reasoning with Execution. We focus on using chain-of-thought reasoning (Wei et al., 2022b) to solve program repair problems by reasoning with execution, where an LLM is prompted to generate an NL rationale $\\\\hat{r}$ together with a fixed program $\\\\hat{y}$ as in Fig. 1. Specifically, we consider rationales that contain reasoning steps to identify and explain bugs in the original code (e.g., the second paragraph in $\\\\hat{r}$, Fig. 1), as well as suggestions to fix the buggy code (e.g., \u201ca fixed version that uses re.findall()\u201d in $\\\\hat{r}$). Since rationales are generated using traces, they often include useful reasoning about program execution that helps localize the bug, such as identifying a counterfactual between the expected and the actual variable values of a statement (e.g., \u201chighlighted text\u201d in $\\\\hat{r}$). Such explanations can be helpful for developers to understand bugs in the original code and the model\u2019s fixed solutions (Kang et al., 2023). We therefore aim to improve the quality of NL rationales along with the fix rate by teaching LLMs to reason with execution information.\\n\\nAn LLM-friendly Trace Representation. The raw execution traces collected at runtime contain complete variable states for each executed statement. Encoding all such information in prompts is not feasible given the context limit and computation overhead of LLMs. To address this issue and make execution information more intelligible to LLMs, we propose an inline trace representation format, which encodes variable states as inline comments of the traced program. Fig. 2 shows an example. Specifically, each inline comment only encodes changed variables after executing that line. Because statements may be invoked multiple times in non-obvious orders (e.g., in loops like lines 4 to 8 in Fig. 2), we index the variable states based on the execution order (e.g., (3) n=1; and (4) even_list=[1]), and one may reconstruct the original execution footprint by following those variable states in order. We further compress the trace information for loops by omitting the variable states in intermediate iterations (e.g., \u201c...\u201d in lines 4, 6, and 8).\\n\\nIntuitively, by showing states as pseudo-comments within the original code without interrupting the program structure, our trace representation is significantly more compact than existing approaches that unroll executed lines of code and pair them with line-by-line variable states (c.f., Nye et al., 2021; Bouzenia et al., 2023), while allowing an LLM to leverage its learned code representation to understand the additional execution effect of each statement. Implementation details about handling complex control structures are discussed in Appendix A.1.\\n\\n3. Preliminary Study: Can LLMs reason with program traces in natural language? Before introducing NEXT, we first conduct a preliminary study to explore whether LLMs could reason with execution traces in natural language out-of-box without additional training. Answering this question will motivate our finetuning approach to improve such reasoning skills. Specifically,\"}"}
{"id": "B1W712hMBi", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Few(3)-shot prompting repair accuracy using greedy decoding. Results worse than the settings specified in the previous row above are underlined in red.\\n\\nWe follow the trace representation in \u00a72 and use LLMs to solve program repair tasks using CoT reasoning. We evaluate the following general-purpose models: PaLM 2 (Anil et al., 2023), GPT (OpenAI, 2023), Mixtral (Jiang et al., 2024). We also test two code-specific LLMs: StarCoder (Li et al., 2023) and DeepSeek Coder (Guo et al., 2024). Tab. 1 reports the results on two Python program repair datasets (see \u00a75 for details).\\n\\nLLMs struggle on CoT reasoning with traces. We observed mixed results when comparing vanilla prompting with traces without CoT (Vanilla w/ trace in Tab. 1) and CoT prompting with rationales (+CoT). Surprisingly, CoT prompting is even worse on HUMANEVAL-PPLUS, with an average drop of $-7.8\\\\%$ compared to vanilla prompting, especially for code-specific LLMs ($57.3 \\\\mapsto 30.5$ for DeepSeek Coder and $29.3 \\\\mapsto 16.5$ for StarCoder). After inspecting sampled rationales predicted by PaLM 2-L, we observe that the model is subject to strong hallucination issues, such as mentioning exceptions not reflected in the given traces. Indeed, as we later show in \u00a75.2, the overall correctness rate of explaining errors in input programs among these sampled rationales from PaLM 2-L is only around $30\\\\%$.\\n\\nMoreover, CoT reasoning is even more challenging for those models when we remove execution traces from the inputs (+CoT; \u2212trace), resulting in an average performance drop of $4.8\\\\%$ on MBPP-R and $4.3\\\\%$ on HUMANEVAL-PPLUS. These results suggest that while our trace representation is useful for LLMs to understand and leverage execution information for program repair (since $-trace$ leads to worse results), they could still fall short on CoT reasoning using natural language with those program traces. This finding therefore motivates us to improve LLMs in reasoning with execution through finetuning, which we elaborate in \u00a74.\\n\\n4. NEXT: Naturalized Execution Tuning\\n\\nWe present NEXT, a self-training method to finetune LLMs to reason with program execution using synthetic rationales.\\n\\nOverview of NEXT. Fig. 1 illustrates NEXT, with its algorithm detailed in Algo. 1. NEXT is based on existing self-trained reasoning approaches (Zelikman et al., 2022; Uesato et al., 2022), which employ expert iteration to improve a base LLM using synthetic rationales sampled from the model. Given a training set $D$ of repair tasks with execution traces, NEXT first samples candidate NL rationales and fixed code solutions from the LLM. Those candidate solutions are filtered using unit test execution diagnostics, and those that pass all test cases are then used to update the model via finetuning. This sample-filter-train loop is performed for multiple iterations, improving the model's rationales and repair success rate after each iteration.\\n\\nSampling rationales and code solutions. For each iteration $i$, we sample rationales $\\\\hat{r}$ and fixes $\\\\hat{y}$ in tandem from the current model $P_\\\\theta(i)$ (Line 5, Algo. 1). We use few-shot prompting (\u00a73) when $i=0$ and zero-shot prompting with trained models for later iterations. In contrast to existing self-training methods that leverage all training problems, NEXT only samples candidate solutions from the subset of problems in $D$ that are challenging for the base model $P_\\\\theta(0)$ to solve (Line 1). Specifically, given a metric $M(\\\\cdot)$, we only use problems $d \\\\in D$ if $P_\\\\theta(0)$'s metric on $d$ is below a threshold $m$. Refer to \u00a75 for more details about the $M(\\\\cdot)$ and $m$ of our program repair task. Focusing on sampling solutions from those hard problems not only significantly reduces sampling cost, it also improves program repair accuracy, as it helps the model towards learning to solve more challenging problems (Kommrusch et al., 2023). See Appendix C for a more detailed analysis.\\n\\nFiltering candidate solutions. Given a candidate set of sampled NL rationales and their code fixes, NEXT uses unit test execution results to identify plausible rationales that lead to correct fixes for learning (Line 6). Using test execution diagnostics as a binary reward function is natural for program repair tasks since each repair problem in our dataset comes with unit tests to test the functional correctness of its proposed fixes (Ye et al., 2022). While we remark that this filtering criteria does not directly consider rationale quality, we empirically demonstrate in \u00a75 that the quality of rationales improves as learning continues.\"}"}
{"id": "B1W712hMBi", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def search_literal(pattern, text):\\n    match = tuple(text.find(pattern), text.find(pattern) + len(pattern) - 1)  # (1)\\n    exception = TypeError('tuple expected at most 1 argument, got 2')\\n    return match\\n\\nassert search_literal('python', 'python programming language') == (0, 6)\\n\\n# Trace 2:\\ndef search_literal(pattern, text):\\n    match = tuple(text.find(pattern), text.find(pattern) + len(pattern) - 1)  # (1)\\n    exception = TypeError('tuple expected at most 1 argument, got 2')\\n    return match\\n\\nassert search_literal('programming', 'python programming language') == (7, 18)\\n\\n# Trace 3:\\ndef search_literal(pattern, text):\\n    match = tuple(text.find(pattern), text.find(pattern) + len(pattern) - 1)  # (1)\\n    exception = TypeError('tuple expected at most 1 argument, got 2')\\n    return match\\n\\nassert search_literal('language', 'python programming language') == (19, 27)\\n\\nRationale and Code Solution:\\n(code fix is wrong)\\nNow let's look at the program. The program looks good at the first glance as `text.find(pattern)` is the correct way to find the location of a literal string in a given string. But the problem is that it does not use the `tuple()` function correctly. The `tuple()` function takes a single iterable as input, and returns a tuple with the elements from the iterable. But in the program, it takes two arguments, which will throw a `TypeError` of 'tuple expected at most 1 argument, got 2'.\\n\\nHere is a simple fix:\\n\\n```python\\ndef search_literal(pattern, text):\\n    match = tuple(text.find(pattern),)\\n    match = match + (text.find(pattern) + len(pattern) - 1,)\\n    return match\\n```\"}"}
{"id": "B1W712hMBi", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We are doing pair programming, I will give you the problem definition and my implementation, which does not pass all test cases. And you need to help me to fix my code.\\n\\n1. We are doing pair programming, I will give you the problem definition and my implementation, which does not pass all test cases. And you need to help me to fix my code.\\n\\n2. Here is the specification in natural language and the program that I wrote:\\n\\n   ```python\\n   def similar_elements(t1, t2):\\n       s1 = set(t1)\\n       s2 = set(t2)\\n       return s1.intersection(s2)\\n   \\n   Here are the assertions I need to pass:\\n   ```\\n   ```python\\n   assert similar_elements((3, 4, 5, 6), (5, 7, 4, 10)) == (4, 5)\\n   assert similar_elements((1, 2, 3, 4), (5, 4, 3, 7)) == (3, 4)\\n   assert similar_elements((11, 12, 14, 13), (17, 15, 14, 13)) == (13, 14)\\n   ```\\n\\n   3. Here is a *cheatsheet* for the gold execution trace for each assertion, use the information here but pretend you did all the reasoning about execution yourself.\\n\\n   ```plaintext\\n   #################### cheatsheet starts ####################\\n   def similar_elements(t1, t2): # (0) t1=(3, 4, 5, 6), t2=(5, 7, 4, 10);\\n       s1 = set(t1) # (1) s1={3, 4, 5, 6};\\n       s2 = set(t2) # (2) s2={10, 4, 5, 7};\\n       return s1.intersection(s2) # (3) __return__={4, 5}\\n   \\n   assert similar_elements((3, 4, 5, 6), (5, 7, 4, 10)) == (4, 5) #\\n   \\n   def similar_elements(t1, t2): # (0) t1=(1, 2, 3, 4), t2=(5, 4, 3, 7);\\n       s1 = set(t1) # (1) s1={1, 2, 3, 4};\\n       s2 = set(t2) # (2) s2={3, 4, 5, 7};\\n       return s1.intersection(s2) # (3) __return__={3, 4}\\n   \\n   assert similar_elements((1, 2, 3, 4), (5, 4, 3, 7)) == (3, 4) #\\n   \\n   def similar_elements(t1, t2): # (0) t1=(11, 12, 14, 13), t2=(17, 15, 14, 13);\\n       s1 = set(t1) # (1) s1={11, 12, 13, 14};\\n       s2 = set(t2) # (2) s2={17, 13, 14, 15};\\n       return s1.intersection(s2) # (3) __return__={13, 14}\\n   \\n   assert similar_elements((11, 12, 14, 13), (17, 15, 14, 13)) == (13, 14) #\\n   \\n   #################### cheat sheet ends ####################\\n   ```\\n\\n   4. Given the above information, can you tell me why my code is wrong, and also provide a fixed version.\\n\\n   The problem spec asks to find the similar elements in the two tuples, from the assertion we can see that it essentially wants the common elements from the two tuples. It also seems that the common elements should be returned in a tuple with ascending order.\\n\\n   Now let's look at the program. While it is correct and efficient to use `intersection()` to find the common elements between two sets, it will also return a set. Take the first assertion as an example, `s1` will first be converted to a set like `{3, 4, 5, 6}` and similarly for `s2`, then the return value will be `{4, 5}`. Though the elements are right, the expected return value is a tuple and not a set. Thus we only need to further convert the sets to tuples using `tuple()`.\\n\\n   Here is the corrected code:\\n\\n   ```python\\n   def similar_elements(t1, t2):\\n       s1 = set(t1)\\n       s2 = set(t2)\\n       return tuple(s1.intersection(s2))\\n   ```\\n\\n   # Example 2\\n\\n   Here is the specification in natural language and the program that I wrote:\\n\\n   ```python\\n   # Write a python function to identify non-prime numbers.\\n   ```\"}"}
{"id": "B1W712hMBi", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def is_not_prime(n):\\n    if n==1:\\n        return False\\n    else:\\n        for i in range(2,n//2+1):\\n            if n%i==0:\\n                return False\\n            else:\\n                return True\\n\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True\\n\\n# cheatsheet\\n\\ndef is_not_prime(n):\\n    if n==1:\\n        return False\\n    else:\\n        for i in range(2,n//2+1):\\n            if n%i==0:\\n                return False\\n            else:\\n                return True\\n\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True\\n\\n# re-implementation\\n\\ndef is_prime(n):\\n    if n<2:\\n        return False\\n    for i in range(2,n):\\n        if n%i==0:\\n            return False\\n    return True\\n\\ndef is_not_prime(n):\\n    return not is_prime(n)\\n\\nassert is_not_prime(2) == False\\nassert is_not_prime(10) == True\\nassert is_not_prime(35) == True\"}"}
{"id": "B1W712hMBi", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def is_prime(x):\\n    if x <= 1:\\n        return False\\n    else:\\n        for i in range(2,x):\\n            if x%i == 0:\\n                return False\\n        return True\\n\\ndef is_not_prime(x):\\n    return not is_prime(x)\\n\\n# Example 3\\nHere is the specification in natural language and the program that I wrote:\\n```\\n# Write a function to find the largest integers from a given list of numbers using heap queue algorithm.\\n\\ndef heap_queue_largest(list1,n):\\n    largest=heapq.nlargest(n,list1)\\n    return largest\\n```\\nHere are the assertions I need to pass:\\n```\\nassert heap_queue_largest([25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65]\\nassert heap_queue_largest([25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75]\\nassert heap_queue_largest([25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]\\n```\\nHere is a *cheatsheet* for the gold execution trace for each assertions, use the information here but pretend you did all the reasoning about execution yourself.\\n\\n```\\n# Write a function to find the largest integers from a given list of numbers using heap queue algorithm.\\n\\ndef heap_queue_largest(list1,n):\\n    largest=heapq.nlargest(n,list1)  # (1) __exception__ = NameError(\\\"name 'heapq' is not defined\\\")\\n    return largest\\n```\\n\\n```\\nassert heap_queue_largest([25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65]\\n```\\n```\\nassert heap_queue_largest([25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75]\\n```\\n```\\nassert heap_queue_largest([25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]\\n```\\n```\\n# Write a function to find the largest integers from a given list of numbers using heap queue algorithm.\\n\\ndef heap_queue_largest(list1,n):\\n    largest=heapq.nlargest(n,list1)\\n    return largest\\n```\\n\\n```\\nimport heapq\\n\\ndef heap_queue_largest(list1,n):\\n    largest=heapq.nlargest(n,list1)\\n    return largest\\n```\\n\\nGiven the above information, can you tell me why my code is wrong, and also provide a fixed version? The problem spec asks to find `n` largest integers in a given list. It also suggests that the heap queue shall be used.\\n\\nNow let's look at the program. The program looks good at the first glance as `heapq.nlargest()` is the correct way to get the `n` largest integers from an iterable (e.g., a list). But the problem is that it does not import the `heapq` package first. Thus all three assertions will fail at the first line of the function, and throw a `NameError` of `name 'heapq' is not defined`.\\n\\nHere is a simple fix:\\n```\\nimport heapq\\n\\ndef heap_queue_largest(list1,n):\\n    largest=heapq.nlargest(n,list1)\\n    return largest\\n```\"}"}
{"id": "B1W712hMBi", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Xia, C. and Zhang, L. Less training, more repairing please: revisiting automated program repair via zero-shot learning. Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2022.\\n\\nXia, C. and Zhang, L. Keep the conversation going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT. ArXiv, abs/2304.00385, 2023.\\n\\nXia, C., Wei, Y., and Zhang, L. Automated program repair in the era of large pre-trained language models. IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 1482\u20131494, 2023.\\n\\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. ReAct: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022.\\n\\nYe, H., Martinez, M., Luo, X., Zhang, T., and Monperrus, M. SelfAPR: Self-supervised program repair with test execution diagnostics. Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2022.\\n\\nYin, P., Li, W.-D., Xiao, K., Rao, A., Wen, Y., Shi, K., Howland, J., Bailey, P., Catasta, M., Michalewski, H., Polozov, O., and Sutton, C. Natural language to code generation in interactive data science notebooks. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pp. 126\u2013173, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.9.\\n\\nZaremba, W. and Sutskever, I. Learning to execute. ArXiv, abs/1410.4615, 2014.\\n\\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. STaR: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems (NeurIPS), 35:15476\u201315488, 2022.\"}"}
{"id": "B1W712hMBi", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Additional Details of NE\\n\\nA.1. Details for Inline Trace Representation\\n\\nDefinitions.\\n\\nA program $y \\\\in Y$ consists of a sequence of statements $\\\\{u_1, \\\\ldots, u_m\\\\}$. And a program state $h$ is a mapping between identifiers (i.e., variable names) to values, i.e., $h \\\\in \\\\{k \\\\mapsto v | k \\\\in K, v \\\\in V\\\\}$. Given an input to the program, an execution trace is defined as a sequence of program states, i.e., $\\\\epsilon = \\\\{h_1, \\\\ldots, h_t\\\\}$, which are the results after executing the statements with the order of execution, i.e., $\\\\{u_e_1, u_e_2, \\\\ldots, u_e_t\\\\}$. In this way, the relation between program statements and execution states can be seen as a function that maps from states to statements, i.e., $h_i \\\\mapsto u_e_i$, because each statement could be executed multiple times due to loops or recursion.\\n\\nProgram state representation.\\n\\nFor typical programs, most of the variable values will stay the same between two adjacent states $h_i - 1$ and $h_i$. Thus to save tokens, we represent a state $h_i$ only by the variables that have changed the value compared with the previous state $h_i - 1$. And we use a reified variable state representation, i.e., using the grammar for an init function in Python (e.g., `lst=[1, 2, 3]`). Note that it is possible for a statement to have no effect on any traceable variables (e.g., \\\"pass\\\", or \\\"print\\\", or \\\"lst[i]=lst[i]\\\"). To distinguish this case with unreached statements (e.g., \\\"else\\\" branch that next got executed), we append a string \\\"NO CHANGE\\\" instead. In addition to the variable state, we number all the states by the order of execution and prepend the ordinal number to the beginning of the state, e.g., \\\"(1) odd list=[]\\\" in Fig. 2.\\n\\nInline trace representation.\\n\\nTo obtain the inline trace representation, we first group the program states in a trace $\\\\epsilon$ by the corresponding program statements to collect a sequence of states for the same statement $u_i$ as $H_i = \\\\{h_j | u_e_j = u_i\\\\}$, and we order the states in $H_i$ by the execution order. For statements inside a loop body, or a function that is called recursively, the number of corresponding states can be very large. In order to further save tokens, if $|H_i| > 3$, we will only incorporate the first two states and the last state, and skip the ones in the middle. After that, we simply concatenate all the state representations with the semicolon \\\";\\\" as the delimiter, and append it after the statement itself following a hash \\\"#\\\" to note it as an inline comment. An example of the resulting representation is \\\"even list.append(n) # (4) even list=[1]; (8) even list=[1, 3]; ...; (20) even list=[1, 3, 5, 7, 9];\\\", as shown in Fig. 2.\\n\\nLimitations.\\n\\nFirst of all, our tracing framework currently do not extend beyond native Python programs, thus it can not trace code that is not written in Python (e.g., C code in numpy). One other limitation of our tracing representation is that for \\\"if\\\" conditions, though it would be better to leave traces of \\\"(1) True; (2) True; (3); False;\\\", currently our tracing framework that based on the \\\"sys.settrace()\\\" hook of Python does not capture this. However, since we labeled all the states by the execution order, the LLMs can infer the conditions by the fact that certain branch is taken. Another limitation is the representation of Collections. Currently we still present all the elements in a collection, and empirically it works well with benchmarks as MBPP-R and HEXIST+. However, certain heuristics may be needed to skip certain elements (e.g., like the one we use to skip certain states in a loop) to be more token efficient. For more complex objects (e.g., Tensors, DataFrames), while we can define heuristics to represent key properties of those objects in traces (e.g., \\\"a float tensor of shape 128 x 64\\\", \\\"a Dataframe with columns Name, Math, ...\\\"), perhaps a more interesting idea would be to let the models decide which properties they would inspect and generate relevant code (e.g., \\\"tensor.shape\\\" or \\\"df.head(3)\\\") to inspect them in a debugger or interpreter (e.g., pdb). The same idea can be applied to longer programs, as the model can selectively decide which lines of code to inspect and create traces for, similar to how human developers debug programs. We will leave these as exciting future directions.\\n\\nA.2. Details for Iterative Self-Training\\n\\nBootstrapping rationales and fixes via temperature sampling.\\n\\nTo avoid potential \\\"cold start\\\" problem (Liang et al., 2018; Ni et al., 2020), for the first iteration, we use few-shot prompting with three exemplars (shown in Appendix E) and set the sample size to 96. For all later iterations, we use zero-shot prompting as the model is already adapted to the style of the rationales and fixes after the first round of finetuning, and we set the sample size to 32. We set the sampling temperature $T = 0.8$ for all iterations.\\n\\nFiltering rationales and fixes.\\n\\nGiven the inputs in the prompt, we sample the rationale and fixes in tandem. To separate the natural language rationale and the program fix, we use an regular expression in Python to extract the content between two\"}"}
{"id": "B1W712hMBi", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"After we filter out the rationales and fixes that are incorrect using the test cases, we create the training set by sub-sampling correct \\\\textit{(rationale, fix)} pairs to allow a maximum of 3 correct fixes with their rationales for each problem in \\\\( \\\\text{MBPP} \\\\)-R. This is to balance the number of rationales and fixes for each problem and avoid examples from certain examples (typically easier ones) being overly represented in the training set.\\n\\n### A.3. Discussion with Previous Work\\nHere we discuss \\\\textit{NEXT} in the context of two important previous work in the domain of reasoning about program execution, namely \\\\textit{Scratchpad} (Nye et al., 2021) and \\\\textit{Self-Debugging} (Chen et al., 2023). Such comparison is also characterized by Tab. 6.\\n\\n\\\\textit{Scratchpad} and \\\\textit{NEXT}. Similarly to \\\\textit{NEXT}, Nye et al. (2021) also proposed to use execution traces to help the LLMs to reason about program execution. However, Nye et al. (2021) aimed to generate these traces as intermediate reasoning steps at inference time, either via few-shot prompting or model fine-tuning. Yet in \\\\textit{NEXT}, we use execution traces as part of the input to the LLMs, so they can directly use the execution states to ground the generated natural language rationales. Moreover, we choose to use natural language as the primary format for reasoning, which is more flexible and easier to be understood by the human programmers. We also perform a length comparison of our proposed inline trace representation with the scratchpad representation proposed in Tab. 7, and results show that our proposed inline trace representation is much more compact than scratchpad.\\n\\n\\\\textit{Self-Debugging} and \\\\textit{NEXT}. Self-Debugging (Chen et al., 2023) is a seminal approach that also performs CoT reasoning over program execution to identify errors in code solutions. Different from \\\\textit{NEXT}, Self-Debugging can optionally leverage high-level execution error messages to bootstrap CoT reasoning, while our method trains LLMs to reason with concrete step-wise execution traces. In addition, Self-Debugging also introduced a particular form of CoT rationales that resemble step-by-step traces in natural language. Notably, such rationales are generated by LLMs to aid the model in locating bugs by simulating execution in a step-by-step fashion. They are not the ground-truth execution traces generated by actually running the program. As we discussed in \u00a76, in contrast, our model relies on existing traces from program execution. Since those traces already capture rich execution information, intuitively, the resulting CoT rationales in \\\\textit{NEXT} could be more succinct and \u201cto the point\u201d without redundant reasoning steps to \u201ctrace\u201d the program step-by-step by the model itself in order to recover useful execution information.\\n\\nFinally, we remark that our \u201cTest w/o Trace\u201d setting in\u00a75.1 shares similar spirits with the setup in Self-Debugging, as both methods perform CoT reasoning about execution without gold execution traces. From the results in Tab. 3, \\\\textit{NEXT} also greatly improves the model\u2019s ability to repair programs even without using gold execution traces at test time. This may suggest that \\\\textit{NEXT} can potentially improve the self-debugging skills of LLMs through iterative training, for which we leave...\"}"}
{"id": "B1W712hMBi", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Experiment Setup Details\\n\\nB.1. Creating MBPP-R\\n\\nThe original MBPP dataset (Austin et al., 2021) consists of three splits, i.e., train/dev/test sets of 374/90/500 Python programming problems. To increase the number of training example, we first perform a re-split of the original MBPP dataset, by moving half of the test data into the training split, resulting in 624/90/250 problems in the re-split dataset. Then for each MBPP problem in the re-split train and dev set, we collect a set of failed solutions from the released model outputs in Ni et al. (2023). More specifically, we take the 100 samples for each problem, filter out those correct solutions, and keep the ones that do not pass all the tests. As different problems have various number of buggy solutions, we balance this out by keeping at most 20 buggy solutions for each MBPP problem. This yields the MBPP-R dataset, with 10,047 repair tasks in the training set and 1,468 examples in the dev set.\\n\\nB.2. Use of test cases.\\n\\nFor each program repair task, there is typically a set of open test cases that are used for debugging purposes, as well as a set of hidden test cases that are only used for evaluation of correctness. When we generate traces using test cases, we use only the open test cases and only feed the open test cases to the model as part of the prompt. Then when we evaluate the generated fix, we resort to all test cases (i.e., open + hidden tests) and only regard a fix as correct when it passes all test cases. While the HUMANVAL dataset makes this distinction between open and test cases, the MBPP dataset does not make such distinction. Thus for MBPP-R, we use all test cases both as the inputs and during evaluation. While this may lead to false positives when the fixes are overfit to the test cases, and we did find such case during human annotations.\\n\\nB.3. Details of Human Annotation of Rationale Quality\\n\\nWe annotated model predictions on 104 sampled MBPP-R repair tasks from the DEV set. Those fix tasks are randomly sampled while ensuring that they cover all the 90 dev MBPP problems. All the tasks are pre-screened to be valid program repair problems. Annotation is performed in a three-way side-by-side setting. Models are anonymized and their order is randomized. Raters are asked to judge the quality of rationales from three models (PaLM 2-NEXT, PaLM 2-L and GPT-3.5) on the same MBPP-R problem. Each rationale is rated from two aspects: (1) its helpfulness in explaining bugs (Q1: Does the rationale correctly explain bugs in the original code? e.g., first two paragraphs in \\\\( \\\\hat{r} \\\\), Fig. 1), and (2) its helpfulness in suggesting code fixes (Q2: Does the rationale suggest a correct and helpful fix? e.g., \u201ca fixed version that uses ...\u201d in \\\\( \\\\hat{r} \\\\), Fig. 1).\\n\\nEach question has a three-scale answer (Completely correct and very helpful; Partially correct with minor errors but still helpful; Incorrect and not helpful). In a pilot study, we find that fix suggestions could often be redundant if the rationale already contains detailed explanation of bugs such that a developer could easily correct the code without an explicit fix suggestion (e.g., Example 2, Appendix D). Therefore, for Q2, we also consider such cases as correct (1) if a model didn't suggest a fix in its rationale but the fix is obvious after bug explanations. We list our annotation guideline in Fig. 5. Note that for Q2, both answers (1) and (3) are counted as correct (1) answers.\\n\\nC. Additional Experiment Results\\n\\nHere we show the learning curve of NEXT and all its ablations in Fig. 6. We also show the full results for MBPP-R and HEXIT+ in Tab. 8 and Tab. 9, respectively.\\n\\nLearning CoT rationales further improves \\\\( \\\\text{PASS}@25 \\\\). From \u00a75.1, we mention that learning to reason in natural language improves sample diversity, registering higher \\\\( \\\\text{PASS}@10 \\\\) than the baseline of finetuning for generating fixes only (NEXT w/o Rationale). From Tab. 8 and Tab. 9, we can observe that such performance advantage is even larger with \\\\( \\\\text{PASS}@25 \\\\), with 7.6% improvements on MBPP-R and 6.8% improvements on HEXIT+. This actually biased the dataset towards harder problems as easier problems may not have more than 20 buggy solutions from 100 samples, thus it might be one of the reasons for repairing solutions in MBPP-R to be more challenging than generating code for the original MBPP dataset.\\n\\nWe only rate the quality of rationales (not the fixed code), while we still show the predicted fixed code to raters for reference.\"}"}
{"id": "B1W712hMBi", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Training on hard-only examples.\\n\\nOne part of our data filtering pipeline is to only perform sampling and train on the samples from hard problems (\u00a74). Here we discuss more about the benefits and potential issues of doing so, by presenting results on a \\\"w/o hard-only\\\" ablation, where the model learns from rationales and fixes from both hard and easy examples.\\n\\nEfficiency-wise, by only sampling on the hard example, which is around half of the problems, we greatly can accelerate the sampling process. And from results in Fig. 6, only training with hard example also comes with performance benefits under the iterative self-training framework. More specifically, we notice a non-trivial gap between the training curve of this \\\"w/o hard-only\\\" baseline and the rest of the ablations, especially for \\\\( \\\\text{PASS}_{10} \\\\) and \\\\( \\\\text{PASS}_{25} \\\\) performance on the training set.\\n\\nThis means that the model trained on both easy and hard examples leads to more problems in the training set unsolved (i.e., none of the samples are correct), and no learning signal can come from such problems. This also reflects on the dev set.\"}"}
{"id": "B1W712hMBi", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6: PASS@k performance on the train and dev sets of MBPP-R for NEXT and all its ablations. While it is worth noticing that the end-to-end PASS@1 performance for \u201cw/o hard-only\u201d is slightly better than NEXT trained only trained on hard examples, it performs worse in all other evaluations, with the trend of larger gaps with higher k values for PASS@k, especially for the proxy-based evaluation. This suggests that training on hard examples not only improves sample efficiency, but also improves the general fix rate as well as the quality of the generated rationales.\\n\\nProxy-based evaluation results are consistent with different proxy models. In the previous proxy-based evaluation \u00a75.1, we report the proxy-based fix rates by averaging over the performance using PaLM 2-S and PaLM 2-S* as the proxy models. In Tab. 8 and Tab. 9, we show the separated results for different proxy models. From these results, we can observe that the relative rationale quality evaluated by different proxy models are largely consistent, with the stronger proxy model (PaLM 2-S*) having better proxy-based fix rates. In addition to the consistency we show with human annotations, this shows the robustness of our proposed proxy-based evaluation method for measuring CoT rationale quality.\\n\\nD. Case Study\\n\\nIn this section we present a set of examples to showcase how PaLM 2-L+NEXT reasons with program execution to solve MBPP-R problems. We discover several reasoning patterns the model exhibits that leverage trace information to identify and explain bugs in programs. First, as shown in Example 1, the model could refer to exceptions or error messages (eg in Trace 2) to explain bugs in the code. Next, Example 2 shows that the model could also leverage variable states (eg, in Trace 2) and compare them with the expected values to locate the cause of bugs. Besides, the NO CHANGE annotations for variables whose values are preserved after execution of a step could also help the model explain the execution process in the rationale (eg, (3)NO CHANGE \u2192 \u201cthe first sublist is already sorted\u201d). Perhaps a more interesting scenario is when the model reasons over multiple steps of computation to track down the cause of a bug. In Example 3, the model attempts to trace the computation of steps 2 - 4 in Trace 1 to explain why the sum is a float instead of an integer. Another example is Example 4, where the model summarizes the loop iterations in steps 2 - 9 of Trace 1 to explain the cause of 18.\"}"}
{"id": "B1W712hMBi", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Full results on MBPP-R. \\\"GD Acc.\\\" denotes PASS@1 evaluated with greedy decoding. All models in the top half are few-shot prompted while the bottom half shows the result of NEX T and its ablations.\\n\\nTable 9: Full results on HEXIT+. Same notations from Tab. 8 apply.\\n\\nInterestingly, while the model is able to reason over multiple steps of execution in its rationales, as the reasoning chain becomes longer, it is more likely to contain minor logical errors, as highlighted in the rationales of Examples 3 and 4.\\n\\nBesides minor logical errors in CoT reasoning, we also identify several more pronounced error modes. First, even if the NL rationale looks valid, the actual code fix might be incorrect (Example 4). Second, for cases where there are multiple bugs in the program, the model might only focus on the more obvious bugs during reasoning (e.g., explaining the reason of IndexError in Trace 2 of Example 5), while ignoring the other more nuanced bugs that are buried in the trace (e.g., Incorrect outputs in Trace 1 and Trace 3 caused by zero-initialized freq arr). This could be especially problematic when there are hidden bugs not captured by the trace, which could happen when a buggy program crashes in the middle of execution and the rest of the code is not covered in the trace. Since the model could be more reliant on trace information after fine-tuning, it might be more likely to miss such implicit bugs. As an example, in Example 6 the model only noticed the TypeError in the trace, without spotting that the code that computes the end index in the output tuple is off by one.\"}"}
{"id": "B1W712hMBi", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Example 1:\\n\\nAn example where our model explains exceptions in the trace\\n\\nTask:\\nWrite a function to remove leading zeroes from an IP address\\n\\nBuggy Code:\\n```python\\ndef removezero_ip(ip):\\nip_list = ip.split(.)\\nfor i in range(4):\\nip_list[i] = str(int(ip_list[i]))\\nreturn \\\".\\\".join(ip_list)\\n```\\n\\nTest Cases:\\n```python\\nassert removezero_ip(\\\"216.08.094.196\\\") == (\\\"216.8.94.196\\\")\\nassert removezero_ip(\\\"12.01.024\\\") == (\\\"12.1.24\\\")\\nassert removezero_ip(\\\"216.08.094.0196\\\") == (\\\"216.8.94.196\\\")\\n```\\n\\nTraces:\\n```\\n# Trace 1:\\ndef removezero_ip(ip): # (0) ip=\\n    ip_list = ip.split(.) # (1) ip_list=[216, 08, 094, 196];\\n    for i in range(4): # (2) i=0; (4) i=1; (6) i=2; ...; (10) NO CHANGE;\\n        ip_list[i] = str(int(ip_list[i])) # (3) NO CHANGE; (5) ip_list=[216, 8, 94, 196]; (7) ip_list=[216, 8, 094, 196]; (9) NO CHANGE;\\n    return \\\".\\\".join(ip_list) # (11) return = \\\"216.8.94.196\\\";\\n    assert removezero_ip(\\\"216.08.094.196\\\") == (\\\"216.8.94.196\\\")\\n# Trace 2:\\ndef removezero_ip(ip): # (0) ip=\\n    ip_list = ip.split(.) # (1) ip_list=[12, 01, 024];\\n    for i in range(4): # (2) i=0; (4) i=1; (6) i=2; (8) i=3;\\n        ip_list[i] = str(int(ip_list[i])) # (3) NO CHANGE; (5) ip_list=[12, 1, 024]; (7) ip_list=[12, 1, 24]; (9) exception = IndexError('list index out of range')\\n    return \\\".\\\".join(ip_list)\\n    assert removezero_ip(\\\"12.01.024\\\") == (\\\"12.1.24\\\")\\n# Trace 3:\\ndef removezero_ip(ip): # (0) ip=\\n    ip_list = ip.split(.) # (1) ip_list=[216, 08, 094, 0196];\\n    for i in range(4): # (2) i=0; (4) i=1; (6) i=2; ...; (10) NO CHANGE;\\n        ip_list[i] = str(int(ip_list[i])) # (3) NO CHANGE; (5) ip_list=[216, 8, 094, 0196]; (7) ip_list=[216, 8, 94, 0196]; (9) ip_list=[216, 8, 94, 196]; (11) return = \\\"216.8.94.196\\\";\\n    assert removezero_ip(\\\"216.08.094.0196\\\") == (\\\"216.8.94.196\\\")\\n```\\n\\nRationale and Code Solution:\\n\\nThe problem spec asks to write a function to remove leading zeroes from an IP address.\\n\\nNow let's look at the program. The program first splits the IP address into a list of strings, and then iterates through the list to convert each string to an integer, and then back to a string. This will remove leading zeroes. However, the problem is that it iterates through the list for 4 times, but the length of the list might not be 4. For example, when 'ip=\\\"12.01.024\\\"', the length of the list is only 3. Thus when 'i=3', it will throw an 'IndexError'.\\n\\nHere is a simple fix:\\n```python\\ndef removezero_ip(ip):\\nip_list = ip.split(.)\\nfor i in range(len(ip_list)):\\nip_list[i] = str(int(ip_list[i])).lstrip('0')\\nreturn \\\".\\\".join(ip_list)\\n```\\n\\nExample 2:\\n\\nAn example where our model uses variable states to solve the task\\n\\nTask:\\nWrite a function to sort each sublist of strings in a given list of lists\\n\\nBuggy Code:\\n```python\\ndef sort_sublists(words):\\nwords.sort(key = len)\\nfor w in words:\\n    w.sort()\\nreturn words\\n```\\n\\nTest Cases:\"}"}
{"id": "B1W712hMBi", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The third assertion passes as the first and the last sublists are already sorted, and by the problem spec.\\n\\n```\\nsublists(words):\\ndigits(number):\\ndigits(12)==3\\ndigits(345)==12\\ndigits(97)==16\\nsublists([[exception\\n```\\n\\nCHANGE;\\n\\nCHANGE;\\n\\n```\\n# (1) words=[[ 'black'], [ 'white'], [ 'green', 'orange'], [ 'green', 'orange']];\\nreturn\\nCHANGE;\\nCHANGE;\\n```\\n\\nThe second assertion fails as the list of lists is sorted by the length of each sublist, and thus the return value is not the same as the input.\\n\\n```\\nCHANGE; (7) NO\\nsublists(words): # (0) words=[[ ]];\\n```\\n\\nCHANGE;\\n\\n```\\nCHANGE; (5) NO\\nsublists(words): # (0) words=[[ ]];\\n```\\n\\n```\\nThe problem is that the program sorts the list of lists by the length of each sublist, which is not required the second and the third sublists are sorted in the for loop.\\n```\\n\\n```\\nreturn value is not the same as the input.\\n```\\n\\n```\\nassert sum\\n```\\n\\n```\\nassert sum\\n```\\n\\nTest Cases:\\n\\n```\\ndef sum\\n```\\n\\n```\\nWrite a function to get the sum of a non-negative integer\\n```\\n\\n```\\nTask:\\n```\\n\\n```\\ndef sort\\n```\\n\\n```\\nHere is the corrected code:\\n```\\n\\n```\\n# Trace 3:\\n'\\n```\\n\\n```\\n# Trace 2:\\n'\\n```\\n\\n```\\n# Trace 1:\\n'\\n```\\n\\n```\\nTraces:\\n```\\n\\n```\\nassert sort\\n```\\n\\n```\\nassert sort\\n```\\n\\n```\\nassert sort\\n```\\n\\n```\\nTraces:\\n```\\n\\n```\\n```\"}"}
{"id": "B1W712hMBi", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Trace 1:\\ndef sum_digits(number): # (0) number=345;\\n    sum = 0 # (1) sum=0;\\n    while(number):\\n        sum += number %\\n        number = number / 10 # (3) number=34.5; (5) number=3.45; (7) number=0.34500000000000003; ...; (655) number=0.0;\\n    return sum # (656)\\nreturn = 13.33333333333333;\\nassert sum_digits(345)==12 # (657)\\nexception = AssertionError()\\n# Trace 2:\\ndef sum_digits(number): # (0) number=12;\\n    sum = 0 # (1) sum=0;\\n    while(number):\\n        sum += number %\\n        number = number / 10 # (3) number=1.2; (5) number=0.12; (7) number=0.012; ...; (651) number=0.0;\\n    return sum # (652)\\nreturn = 3.3333333333333335;\\nassert sum_digits(12)==3 # (653)\\nexception = AssertionError()\\n# Trace 3:\\ndef sum_digits(number): # (0) number=97;\\n    sum = 0 # (1) sum=0;\\n    while(number):\\n        sum += number %\\n        number = number / 10 # (3) number=9.7; (5) number=0.97; (7) number=0.097; ...; (653) number=0.0;\\n    return sum # (654)\\nreturn = 17.77777777777777;\\nassert sum_digits(97)==16 # (655)\\nexception = AssertionError()\\nRationale and Code Solution:\\n(code fix is correct)\\nThe problem spec asks to write a function to get the sum of all digits in a non-negative integer.\\nNow let's look at the program. The program seems to be correct at the first glance. It first initializes the sum to 0, and then it uses a while loop to get each digit in the number and add it to the sum. However, if we look at the output of the program, we will find that the sum is not an integer. This is because the program does not convert the number to an integer before it gets the digits. For example, for the first assertion, when 'number=345', the first digit '5' is added to the sum and 'sum=5'. Then 'number' is updated to '34.5', and the second digit '4' is added to the sum and 'sum=9.5'. And so on.\\nHere is a fixed version:\\n\\ndef sum_digits(number):\\n    number = int(number)\\n    sum = 0\\n    while(number):\\n        sum += number %\\n        number = number // 10\\n    return sum\\n\\nExample 4:\\nAn example where PaLM 2-LXT reasons over multiple steps of execution\\nTask:\\nWrite a function to convert tuple into list by adding the given string after every element.\\nBuggy Code:\\ndef add_str(test_tuple, add_str):\\n    res = list(test_tuple)\\n    for i in range(1,len(test_tuple)):\\n        res.insert(i\u22172\u22121,add_str)\\n    return res\\nTest Cases:\\nassert add_str((5, 6, 7, 4, 9) , \\\"FDF\\\") == [5,\\\"FDF\\\", 6,\\\"FDF\\\", 7,\\\"FDF\\\", 4,\\\"FDF\\\", 9,\\\"FDF\\\"]\\nassert add_str((7, 8, 9, 10) , \\\"PF\\\") == [7,\\\"PF\\\", 8,\\\"PF\\\", 9,\\\"PF\\\", 10,\\\"PF\\\"]\\nassert add_str((11, 14, 12, 1, 4) , \\\"JH\\\") == [11,\\\"JH\\\", 14,\\\"JH\\\", 12,\\\"JH\\\", 1,\\\"JH\\\", 4,\\\"JH\\\"]\\nTraces:\"}"}
{"id": "B1W712hMBi", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Trace 1:\\ndef add_str(test_tuple, add_str):\\n    res = list(test_tuple)  # (1) res=[5, 6, 7, 4, 9];\\n    for i in range(1, len(test_tuple)):  # (2) i=1; (4) i=2; (6) i=3; ...\\n        res.insert(i * 2 - 1, add_str)  # (3) res=[5, 'FDF', 6, 7, 4, 9]; (5) res=[5, 'FDF', 6, 'FDF', 7, 4, 9];\\n        res=[5, 'FDF', 6, 'FDF', 7, 'FDF', 4, 9]; (9) res=[5, 'FDF', 6, 'FDF', 7, 'FDF', 4, 'FDF', 9];\\n    return res  # (11)\\nreturn res=[5, 'FDF', 6, 'FDF', 7, 'FDF', 4, 'FDF', 9];\\nassert add_str((5, 6, 7, 4, 9), 'FDF') == \\n    [5, 'FDF', 6, 'FDF', 7, 'FDF', 4, 'FDF', 9];  # (12)\\nexception=AssertionError()\\n# Trace 2:\\ndef add_str(test_tuple, add_str):\\n    res = list(test_tuple)  # (1) res=[7, 8, 9, 10];\\n    for i in range(1, len(test_tuple)):  # (2) i=1; (4) i=2; (6) i=3; (8) NO CHANGE;\\n        res.insert(i * 2 - 1, add_str)  # (3) res=[7, 'PF', 8, 9, 10]; (5) res=[7, 'PF', 8, 'PF', 9, 10]; (7) res=[7, 'PF', 8, 'PF', 9, 'PF', 10];\\n        res=[7, 'PF', 8, 'PF', 9, 'PF', 10];\\n    return res  # (9)\\nreturn res=[7, 'PF', 8, 'PF', 9, 'PF', 10];\\nassert add_str((7, 8, 9, 10), 'PF') == \\n    [7, 'PF', 8, 'PF', 9, 'PF', 10];  # (10)\\nexception=AssertionError()\\n# Trace 3:\\ndef add_str(test_tuple, add_str):\\n    res = list(test_tuple)  # (1) res=[11, 14, 12, 1, 4];\\n    for i in range(1, len(test_tuple)):  # (2) i=1; (4) i=2; (6) i=3; ...\\n        res.insert(i * 2 - 1, add_str)  # (3) res=[11, 'JH', 14, 12, 1, 4]; (5) res=[11, 'JH', 14, 'JH', 12, 1, 4];\\n        res=[11, 'JH', 14, 'JH', 12, 'JH', 1, 4]; (9) res=[11, 'JH', 14, 'JH', 12, 'JH', 1, 'JH', 4];\\n    return res  # (11)\\nreturn res=[11, 'JH', 14, 'JH', 12, 'JH', 1, 'JH', 4];\\nassert add_str((11, 14, 12, 1, 4), 'JH') == \\n    [11, 'JH', 14, 'JH', 12, 'JH', 1, 'JH', 4];  # (12)\\nexception=AssertionError()\\nRationale and Code Solution:\\n(code fix is wrong)\\nThe problem spec asks to convert a tuple to a list and add a given string after every element.\\nNow let's look at the program. The program first converts the tuple to a list, which is good. Then it iterates from 1 to the length of the list, and inserts the given string at index $i \\\\times 2 - 1$. This seems correct at the first glance, but the problem is that it will not add the string after the last element. For example, when $test_tuple=(5, 6, 7, 4, 9)$, the length of the list is 5, and the for loop will iterate from 1 to 4. Thus the string will be inserted at index 1, 3, 5, and 7, which corresponds to after the second, fourth, and sixth element. The string will not be inserted after the last element (i.e., the ninth element).\\nHere is a simple fix:\\n```python\\ndef add_str(test_tuple, add_str):\\n    res = list(test_tuple)\\n    for i in range(len(test_tuple)):\\n        res.insert(i * 2, add_str)\\n    return res\\n```\\nExample 5:\\nAn example where PaLM 2-L+NE failed to identify all the bugs in the program.\"}"}
{"id": "B1W712hMBi", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Trace 1:\\ndef find_diff(nums, n):\\n    freq_arr = [0 for i in range(n)]\\n    for i in range(len(nums)):\\n        freq_arr[nums[i]] += 1\\n    return max(freq_arr) - min(freq_arr)\\nassert find_diff([1, 1, 2, 2, 7, 8, 4, 5, 1, 4], 10) == 2\\n\\n# Trace 2:\\ndef find_diff(nums, n):\\n    freq_arr = [0 for i in range(n)]\\n    for i in range(len(nums)):\\n        freq_arr[nums[i]] += 1\\n    return max(freq_arr) - min(freq_arr)\\nassert find_diff([1, 7, 9, 2, 3, 3, 1, 3, 3], 9) == 3\\n\\n# Trace 3:\\ndef find_diff(nums, n):\\n    freq_arr = [0 for i in range(n)]\\n    for i in range(len(nums)):\\n        freq_arr[nums[i]] += 1\\n    return max(freq_arr) - min(freq_arr)\\nassert find_diff([1, 2, 1, 2], 4) == 0\\n\\nRationale and Code Solution:\\nThe problem spec asks to find the difference between the highest and the least frequencies in a given array.\\n\\nNow let's look at the program. The program first creates a frequency array of length \\\\( n \\\\), and then iterates through the input array to count the frequency of each number. It then returns the difference between the maximum and the minimum frequencies in the frequency array.\\n\\nThe program looks good at the first glance. However, the problem is that the length of the frequency array is \\\\( n \\\\), which is the length of the input array. However, the numbers in the input array can be larger than \\\\( n \\\\), and thus the frequency array will not be able to accommodate all the numbers in the input array.\\n\\nFor example, in the second assertion, the input array is \\\\([1, 7, 9, 2, 3, 3, 1, 3, 3]\\\\), and the length of the frequency array is also \\\\( 9 \\\\). When \\\\( nums[i] = 9 \\\\), it will try to access \\\\( freq_arr[9] \\\\), which is out of bound, and thus an \\\\( IndexError \\\\) is thrown.\\n\\nHere is a simple fix:\\n\\n```python\\ndef find_diff(nums, n):\\n    freq_arr = {}\\n    for i in range(len(nums)):\\n        if nums[i] in freq_arr:\\n            freq_arr[nums[i]] += 1\\n        else:\\n            freq_arr[nums[i]] = 1\\n    max_freq = 0\\n    min_freq = 0\\n    for num, freq in freq_arr.items():\\n        if freq > max_freq:\\n            max_freq = freq\\n        if freq < min_freq or min_freq == 0:\\n            min_freq = freq\\n    return max_freq - min_freq\\n```\\n\\nExample 6:\\nAn example where PaLM 2-L+NEX failed to identify a bug not revealed in the trace.\\n\\nTask:\\nWrite a function to search a literals string in a string and also find the location within the original string where the pattern occurs.\\n\\nBuggy Code:\\n\\n```python\\ndef search_literal(pattern, text):\\n    match = tuple(text.find(pattern), text.find(pattern) + len(pattern) - 1)\\n    return match\\n```\\n\\nTest Cases:\\n\\n```python\\nassert search_literal('python', 'python programming language')==(0, 6)\\nassert search_literal('programming', 'python programming language')==(7, 18)\\nassert search_literal('language', 'python programming language')==(19, 27)\\n```\"}"}
{"id": "B1W712hMBi", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Algorithm 1**\\n\\n**Naturalized Execution Tuning (NE\\\\textsuperscript{XT})**\\n\\n- **Input:**\\n  - Training set $D = \\\\{(x_{ij}, \\\\tilde{y}_{ij}, T_j, \\\\epsilon_j)\\\\}_{j=1}^{|D|}$\\n  - Development set $D_{dev}$\\n  - Base LLM $P_\\\\theta(0)$\\n  - Number of iterations $I$\\n  - Executor $E$\\n  - Evaluation metric $M$ and threshold $m$\\n\\n1. $D_H \\\\leftarrow \\\\{d \\\\mid d \\\\in D, M(P_\\\\theta(0), d) < m\\\\}$ // Identify hard problems\\n2. for $i = 0$ to $I$ do\\n3. $B(i) \\\\leftarrow \\\\{\\\\}$\\n4. for $(x_{ij}, \\\\tilde{y}_{ij}, T_j, \\\\epsilon_j)$ in $D_H$ do\\n5. $S(i)_j \\\\sim P_\\\\theta(i)(r, y | x_j, \\\\tilde{y}_j, T_j, \\\\epsilon_j)$ // Sample rationales $r$ and fixes $y$ using trace $\\\\epsilon_j$.\\n6. $B(i) \\\\leftarrow B(i) \\\\cup \\\\{(\\\\hat{r}, \\\\hat{y}) \\\\mid (\\\\hat{r}, \\\\hat{y}) \\\\in S(i)_j, E(\\\\hat{y}, T_j) = 1\\\\}$ // Filter with test cases $T_j$ and add to $B(i)$.\\n7. $\\\\theta(i+1) \\\\leftarrow \\\\arg \\\\max_{\\\\theta} \\\\mathbb{E}_{B(i)}[P_\\\\theta(\\\\hat{r}, \\\\hat{y} | x, \\\\tilde{y}, T, \\\\epsilon)]$ // Finetune model $P_\\\\theta(0)$ with data in $B(i)$.\\n8. $i^* \\\\leftarrow \\\\arg \\\\max_i \\\\sum_{d \\\\sim D_{dev}} M(P_{\\\\theta(i)}, d) / |D_{dev}|$ // Select the best checkpoint $i^*$\\n\\n**Output:** model $P_{\\\\theta(i^*)}$\\n\\n**Model training.**\\n\\nAfter collecting a set of training examples $B(i)$, we finetune the model to maximize the probability of generating the target rationales and code fixes given the task input (Line 7). Following Zelikman et al. (2022), we always finetune the model from its initial checkpoint $P_\\\\theta(0)$ to avoid over-fitting to instances sampled from early iterations that are potentially of lower-quality.\\n\\n**Discussion.**\\n\\nNE\\\\textsuperscript{XT} can be seen as an instantiation of the rationale bootstrapping method proposed in Zelikman et al. (2022) (\u00a7 3.1), which synthesizes latent rationales with correct answers for math and logical reasoning tasks. However, NE\\\\textsuperscript{XT} focuses on program comprehension by reasoning with execution traces, which is critical for solving challenging coding tasks that require understanding execution information, such as program repair (\u00a75). Besides, NE\\\\textsuperscript{XT} models both rationales and programs (code fixes) as latent variables. Using unit test execution results as weak supervision, NE\\\\textsuperscript{XT} is able to explore possible strategies to reason with execution and discover plausible rationales catered towards solving the specific downstream task. As we show in Appendix D, rationales generated by NE\\\\textsuperscript{XT} employ a variety of reasoning patterns to locate and explain bugs in our repair dataset. Finally, while we apply NE\\\\textsuperscript{XT} to program repair, our framework is general and can be extended to other programming tasks that require reasoning about execution, such as code generation with partial execution contexts (Yin et al., 2023) or inferring program execution results (Nye et al., 2021), which we leave as important future work.\\n\\n**5. Experiments**\\n\\n**Models.**\\n\\nWe evaluate NE\\\\textsuperscript{XT} using PaLM 2-L (Unicorn) as the base LLM (Anil et al., 2023). Its finetuning API is publicly accessible on Google Cloud Vertex AI platform.\\n\\n**Datasets.**\\n\\nWe use two Python program repair benchmarks, MBPP-R and HUMAN ValFix-Plus (HEF hereafter). MBPP-R is a new repair benchmark that we create from MBPP (Austin et al., 2021), a popular function-level iteration $i$. Python code generation dataset. We create MBPP-R by collecting LLM-generated incorrect code solutions to MBPP problems, with a total of $10,047$ repair tasks for training and $1,468$ tasks (from a disjoint set of MBPP problems) in the development for evaluation (Appendix B.1). In addition to MBPP-R, we also evaluate on HEF. HEF is derived from HUMAN EvalFix (Muennighoff et al., 2023) which consists of $164$ buggy programs for problems in the HUMAN dataset (Chen et al., 2021a). We further augment HUMAN EvalFix with the more rigorous test suites from EvalPlus (Liu et al., 2023) to obtain HEF+. While both original datasets MBPP and HUMAN EvalFix feature function-level algorithmic code generation problems, problems from the two datasets may still differ in their topics, algorithms or data structures used. Therefore, we use HEF+ to measure generalization ability without further finetuning.\\n\\n**Evaluating Code Fixes.**\\n\\nWe use PASS$_@^k$ (Kulal et al., 2019; Chen et al., 2021a), defined as the fraction of solved repair tasks using $k$ samples ($k \\\\leq 25$), to measure the end-to-end functional correctness of fixed programs with tests.\\n\\n**Evaluating Rationale Quality.**\\n\\nDecoupling the quality of intermediate CoT rationales and downstream task performance (program repair PASS$_@^k$) is a non-trivial research question in LLM reasoning (Prasad et al., 2023), with most works on improving CoT reasoning still hill-climbing towards downstream task performance without evaluating intermediate rational quality (e.g., Lightman et al. (2023)). To disentangle the evaluation of rationale quality from end-to-end repair accuracy, we propose an extrinsic proxy-based evaluation metric for rationales. Specifically, given a rationale $r$, we prompt a smaller LLM to solve the original repair task conditioning on $r$, and use the correctness of the predicted code fix (using greedy decoding) to approximate the quality of $r$. Intuitively, smaller LLMs would rely more on information from the rationale and could be more sensitive to its errors. Therefore, their performance could be a better indicator of rationale quality. We report averaged scores on two PaLM 2 variants for proxy-based evaluation: 1) a smaller general-purpose language model\"}"}
{"id": "B1W712hMBi", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**NEXT: Teaching Large Language Models to Reason about Code Execution**\\n\\n**End-to-end Fix Rate Proxy-based Evaluation**\\n\\n(\\\\textit{PASS}@k on smaller LMs)\\n\\n| Model       | \\\\text{PASS}@1 | \\\\text{PASS}@5 | \\\\text{PASS}@10 | \\\\text{PASS}@25 |\\n|-------------|---------------|---------------|----------------|----------------|\\n| GPT-4       | 63.2          | 75.1          | 78.5           | 82.7           |\\n| GPT-3.5     | 42.9          | 65.0          | 70.7           | 76.7           |\\n| PaLM 2-L    | 23.2          | 45.7          | 54.7           | 65.0           |\\n| PaLM 2-L + NExT | 49.3+26.1    | 68.1+22.4     | 73.5+18.8      | 79.4+14.4      |\\n\\n**Table 2:** Improvements by NExT on the PaLM 2-L model (in subscripts) on MBPP-R. GPT-3.5/4 results are for reference.\\n\\n**Figure 3:** Greedy-decoding results on MBPP-R on PaLM 2-L + NExT and existing LLMs.\\n\\nPaLM 2-S; and 2) PaLM 2-S\u2217 which is specialized in coding (Anil et al., 2023). Note that while we primarily use proxy-based metrics to evaluate rationales, we also perform human ratings of rationale quality (\u00a75.2), with results in line with our proxy-based evaluation.\\n\\n**Hyperparameters.**\\n\\nWe perform temperature sampling ($T = 0.8$) with a sample size of 32 for training ($|S_j| = 32$ in Algo. 1) and \\\\textit{PASS}@k evaluation. In the first iteration in Algo. 1, we use \\\\textit{PASS}@1 estimated with these 32 samples as the filtering metric $M(\u00b7)$ to find challenging problems whose $M(\u00b7) \\\\leq 10\\\\%$ for training. We perform 10 iterations of NExT training and pick the best model using \\\\textit{PASS}@1 on the development set.\\n\\n**5.1. Main Results**\\n\\nIn our experiments, we compare our model with strong LLMs (used in \u00a73), analyze the impact of rationales and program traces, and perform generalization experiments on HEFIX + and human evaluation of rationale quality.\\n\\n**NExT improves program fix rate.**\\nWe first compare the end-to-end program repair performance of PaLM 2-L before and after NExT training (PaLM 2-L + NExT) in Tab. 2 (Left). NExT leads to significant improvements on the end-to-end fix rates across the board, with a 26.1\\\\% absolute improvement on \\\\textit{PASS}@1. Interestingly, the gain on \\\\textit{PASS}@k is generally higher for smaller k. This might suggest that the model becomes more confident about program fixes after NExT training, while the sample diversity also improves, as indicated by improved \\\\textit{PASS}@25. For reference, we also include results from GPT models. Notably, PaLM 2-L + NExT outperforms GPT-3.5 on all \\\\textit{PASS}@k metrics.\\n\\n**NExT improves rationale quality.**\\nTab. 2 (Right) shows the improvements of PaLM 2-L + NExT on our proxy-based evaluation, where we approximate rationale quality using the performance of smaller LMs when conditioned on those rationales. Again, NExT yields consistent improvements across all \\\\textit{PASS}@k metrics. This suggests that NExT improves PaLM 2-L's skill in reasoning with execution to solve MBPP-R problems, leading to rationales that are more helpful for smaller LMs. In Appendix D, we present a case study to demonstrate different reasoning strategies PaLM 2-L + NExT adopts to repair programs using execution information. As we later show in \u00a75.2, our proxy-based metrics are also consistent with human ratings, and rationales from PaLM 2-L + NExT are strongly preferred by annotators compared to those from PaLM 2-L.\\n\\n**PaLM 2-L + NExT outperforms strong LLMs.**\\nWe compare PaLM 2-L + NExT with a series of strong LLMs from the preliminary study (\u00a73) in Fig. 3. PaLM 2-L + NExT outperforms strong open-source LLMs by a minimum of 29.4\\\\% and 11.1\\\\% on end-to-end and proxy-based \\\\textit{PASS}@1 results, respectively, while on par with GPT-3.5. These results show that PaLM 2-L + NExT is a competitive model on program repair by reasoning with execution.\\n\\n**Learning to reason in natural language improves generalization and sample diversity.**\\nTo further demonstrate the importance of using CoT reasoning in NExT self-training, we compare PaLM 2-L + NExT with a strong self-training-based program repair model implemented in NExT, which directly generates code fixes using runtime execution information without CoT reasoning. This ablation resembles SelfAPR (Ye et al., 2022), which also adopts self-training to iteratively synthesize data using unit test diagnostics, while...\"}"}
{"id": "B1W712hMBi", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NE: Teaching Large Language Models to Reason about Code Execution\\n\\nFigure 4: Ablations on removing rationales and/or traces during the iterative training of NE. Note that different min/max values are taken for the y-axis for clarity among different curves but consistent gridline intervals are used for easier comparison.\\n\\nOur ablation uses traces with richer execution information. Fig. 4 shows model performance w.r.t. NE training iterations. When trained without CoT reasoning (NE w/o rationale), PaLM 2-L converges much faster on the training set, which is not surprising since the model only learns to generate code fixes without additional reasoning tasks such as explaining bugs in NL. However, on the DEV set, PaLM 2-L + NE still outperforms this baseline in PASS@10 with comparable PASS@1 accuracy, and the gap on PASS@10 becomes larger with more iterations. This shows that by reasoning in natural language, PaLM 2-L + NE generalizes much better to unseen MBPP-R problems with greater sample diversity. In Fig. 6 of Appendix C, we also show that the gain from PaLM 2-L + NE against this ablation on PASS@k is even more pronounced for larger k>10, which suggests that learning to reason in CoT rationales improves sample diversity on program repair, similar to the findings on other code generation tasks (Yin et al., 2023).\\n\\nReasoning with execution traces is critical. To understand the importance of leveraging program traces to reason with execution, we compare with an ablation of NE without using program traces, which follows the same procedure in Algo. 1 except that traces are not used to generate rationales in Line 5 (NE w/o traces, Fig. 4). This variant can also be seen as a direct application of the rationale generation bootstrapping method in Zelikman et al. (2022), which trains a model on sampled rationales that lead to correct task solutions without relying on additional execution information. Without traces, PaLM 2-L is consistently worse than PaLM 2-L + NE on the DEV set across iterations, both in terms of end-to-end fix rate and proxy-based metrics. This suggests that reasoning with execution information is critical for PaLM 2-L on program repair tasks. Interestingly, while the gap on the development set is significant, the two models achieve similar scores on the training set, which suggests that reasoning with pre-existing execution traces also helps the model generalize better to unseen tasks at test-time.\\n\\nTable 3: PaLM 2-L + NE trained with traces outperforms PaLM 2-L when traces are absent at test time as shown in highlighted results. Results are on MBPP-R; Test w/ Trace: results from Tab. 2.\\n\\n| Methods          | Test w/ Trace | Test w/o Trace |\\n|------------------|---------------|----------------|\\n| PaLM 2-L         | 23.2          | 19.0           |\\n| + NE (w/ trace)  | 49.3          | 34.8           |\\n| + NE w/o trace   | -             | -              |\\n\\nOur model works without traces at test-time. While program traces are crucial for reasoning with execution, such execution information may not always be available at test time (e.g., when execution is prohibitively expensive). To stress-test PaLM 2-L + NE in scenarios where execution information is absent, we remove execution traces from its input at test time in Tab. 3. PaLM 2-L + NE still yields an end-to-end fix rate of 40.8%, which is an 21.8% improvement over the 3-shot PaLM 2-L baseline and is only 3.3% lower than NE trained without traces, for which is tested in-distribution. The results from the proxy-based evaluation of rationales are also consistent with the fix rate.\\n\\nOur model generalizes to HEF at test-time. To further evaluate the generalization ability of PaLM 2-L + NE, we test our model (trained on MBPP-R) on HEF. Tab. 4 summarizes the results. NE achieves reasonable generalization on HEF, outperforming the base PaLM 2-L model by a large margin (i.e., 14.3% on end-to-end fix rate and 6.0% on proxy evaluation). Aligned with our previous findings on MBPP-R in Fig. 4, reasoning with execution traces (c.f. w/o traces) improves fix rate and rationale quality. Moreover, we remark that with iterative learning, PaLM 2-L + NE is on par with the strong program repair method without CoT reasoning (w/o rationale), similar to the results on MBPP-R. This is in contrast with our preliminary study in \u00a73, where PaLM 2-L with CoT is on par with the strong program repair method without CoT reasoning (w/o rationale), similar to the results on MBPP-R. This is in contrast with our preliminary study in \u00a73, where PaLM 2-L with CoT is on par with the strong program repair method without CoT reasoning (w/o rationale), similar to the results on MBPP-R.\"}"}
{"id": "B1W712hMBi", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NE: Teaching Large Language Models to Reason about Code Execution\\n\\nModels | PASS | @ End-to-End Proxy-based Baselines w/ 3-shot prompting |\\n--- | --- | --- |\\nMistral-7B | 12.8 | 16.5 |\\nOctoCoder-15.5B | 15.5 | 17.7 |\\nStarCoder-15.5B | 14.6 | 13.1 |\\nDeepSeekCoder-33B | 28.0 | 18.3 |\\nMixtral-8x7B | 32.3 | 30.8 |\\nGPT-4 | 77.6 | 56.6 |\\nGPT-3.5 | 59.4 | 41.8 |\\nPaLM-2-L + NE X T | 42.5 | +10.3 | 38.0 | +6.1 |\\nPaLM-2-L w/o tracing | 38.1 | +7.8 | 30.6 | +0.2 |\\nw/o rationale | 44.5 | +12.3 |\\nw/o tracing + rationale | 31.4 | +1.1 |\\n\\nTable 4: Generalization results on HEFIX+. PaLM 2-L + NE X T models are only trained with MBPR. \u2217 obtained using greedy decoding; \u2020 no traces provided at test time.\\n\\nPrompting is much worse than vanilla prompting without using rationales. Overall, these results indicate that PaLM 2-L + NE X T could robustly generalize to out-of-distribution repair tasks without additional dataset-specific finetuning.\\n\\n5.2. Human Evaluation of Rationale Quality\\n\\nOur proxy-based evaluation suggests the extrinsic value of the CoT rationales from PaLM 2-L + NE X T. We further conduct an intrinsic evaluation by manually rating the quality of model-predicted rationales on 104 sampled MBPR repair tasks from the DEV set. Specifically, we ask raters to judge the quality of rationales generated by three models (PaLM 2-L + NE X T, PaLM 2-L and GPT-3.5) in a three-way side-by-side setting. Each rationale is rated in two aspects:\\n\\n1. Its helpfulness in explaining bugs ($Q_1$, e.g., first two paragraphs in $\\\\hat{r}$, Fig. 1),\\n2. Its helpfulness in suggesting code fixes ($Q_2$, e.g., \u201ca fixed version that uses ...\u201d in $\\\\hat{r}$).\\n\\nEach question has a three-scale answer (Completely correct and very helpful; Partially correct with minor errors but still helpful; Incorrect and not helpful). We also compute an overall score of rationale quality using numeric values of $\\\\{+1, 0.5, 0\\\\}$ for the three scales and averaged over $Q_1$ and $Q_2$. Finally, we ask raters to pick a single best choice if there is not a clear tie. More details about our human evaluation pipeline is described in Appendix B.3.\\n\\nTab. 5 summarizes the result. Compared to the base PaLM 2 model, PaLM 2-L + NE X T generates significantly more high-quality rationales with correct explanations of bugs and fix suggestions. Additionally, compared to GPT-3.5, PaLM 2-L + NE X T also has more rationales with correct bug explanations, while interestingly, GPT-3.5 generates more rationales with partially correct fix suggestions. We hypothesize that including more exemplars with detailed fix suggestions to our few-shot prompts during NE X T training (Appendix E) would help mitigate this issue. Nevertheless, the overall scores and rater-assigned best choice suggest that the rationales predicted by PaLM 2-L + NE X T are of significantly higher quality compared to those from PaLM 2-L, and are on par with the predictions from GPT-3.5. Overall, this finding is in line with the proxy evaluation results in Fig. 3 (GPT 3.5 \u2248 PaLM 2-L + NE X T \u226b PaLM 2-L), suggesting that the latter is a reasonable surrogate metric for rationale quality. In Appendix D, we present example generated rationales that show a variety of reasoning patterns.\\n\\n6. Related Work\\n\\nReasoning about Program Execution\\n\\nSeveral lines of research has explored learning methods to reason about program execution. Program synthesis systems often leverage the execution states of partially generated programs (Shin et al., 2018; Wang et al., 2018; Chen et al., 2021b; Shi et al., 2022) or the next execution subgoals (Shi et al., 2024) to guide search in sequence-to-sequence models. There has also been work on training neural networks to mimic program execution, like a learned interpreter (Zaremba & Sutskever, 2014; Bieber et al., 2020; Nye et al., 2021; Pi et al., 2022), often with specialized neural architectures to model the data flow of program execution (Graves et al., 2014; Gaunt et al., 2016; Bosnjak et al., 2016; Bieber et al., 2022). Instead of using domain-specific architectures to encode and reason about program execution, our work focuses on teaching LLMs to reason with execution in natural language. In particular, Scratchpad (Nye et al., 2021) and Self-Debugging (Chen et al., 2023) are two notable works that also models execution traces using LLMs. The core difference is that these methods focus on predicting reasoning chains that contain trace information, such as executed lines with variable states (Nye et al., 2021) or their natural language summaries (Chen et al., 2023). On the other hand, NE X T aims to leverage existing execution traces from a runtime to aid the reasoning process, which often leads to more compact rationales tailored for downstream tasks. We present a more detailed comparison and discussion on NE X T and these related works in Appendix A.3.\\n\\nProgram Repair\\n\\nSeveral works in program repair have...\"}"}
{"id": "B1W712hMBi", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NE: Teaching Large Language Models to Reason about Code Execution\\n\\nleveraged execution information such as traces (Gupta et al., 2020; Bouzenia et al., 2023) or test diagnostics (Xia & Zhang, 2023; Ye et al., 2022). Different from Bouzenia et al. (2023) which represents traces by directly pairing unrolled executed lines with their variable states, NE inlines indexed variable states as code comments, which is more token efficient while preserving the original code structure. Similar to NE, Ye et al. (2022) construct synthetic self-training data using test execution results, while our approach generates both NL rationales and fixed programs with better interpretability. Recently, LLMs have been applied to program repair (Fan et al., 2022; Xia & Zhang, 2022; Xia et al., 2023; Sobania et al., 2023; Paul et al., 2023; Jiang et al., 2023). Among them, Kang et al. (2023) uses a ReAct-style CoT reasoning loop (Yao et al., 2022) to predict repair actions based on interactive feedback from debuggers, while NE focuses on tuning LLMs to reason with pre-existing execution information without intermediate feedback. Finally, as a related stream of research, self-improvement methods iteratively refine a model's code solutions using CoT reasoning over self-provided (Madaan et al., 2023) or test-driven feedback (Chen et al., 2023; Olausson et al., 2023). Instead of relying on high-level execution signals like error messages, NE trains LLMs to reason with step-wise program traces. Our learnable rationales are also more flexible without following a predefined reasoning template. Besides, since traces already capture rich execution semantics, the resulting rationales could be more succinct and targeted to the downstream task (e.g., explain bugs), without redundant reasoning steps to trace the program by the model itself to recover useful execution information.\\n\\nSupervised CoT Reasoning\\n\\nLLMs can solve problems more accurately when instructed to work out the answer step by step in a chain of thought or a scratchpad (Wei et al., 2022a; Nye et al., 2021; Rajani et al., 2019; Shwartz et al., 2020). Improvements on this approach involve finetuning LLMs on chain-of-thought reasoning data. Such CoT data is either manually curated (Chung et al., 2022; Longpre et al., 2023; Lightman et al., 2023), or distilled from more capable teacher models (Gunasekar et al., 2023; Mukherjee et al., 2023; Mitra et al., 2023; Fu et al., 2023). Instead of relying on labeled or distilled data, NE uses self-training to iteratively bootstrap a synthetic dataset of high-quality rationales with minimal manual annotation. Our work differs from previous work using bootstrapping (Zelikman et al., 2022; Hoffman et al., 2023) in the type of rationales and the use of execution information; see \u00a74 for more discussion. While we use the correctness of the program fix for filtering the rationales, which is reminiscent of outcome supervision; it is also possible to use process supervision with human annotations (Uesato et al., 2022; Lightman et al., 2023), or obtain such supervision automatically by estimating the quality of each step using Monte Carlo Tree Search (Wang et al., 2024) and by identifying partially-correct program prefixes (Ni et al., 2022). Finally, existing research has investigated fine-tuning of LLMs to predict the execution information directly, such as predicting line-by-line execution traces (Nye et al., 2021), abstract runtime properties (Pei et al., 2023), or final output (Zaremba & Sutskever, 2014; Bieber et al., 2020). NE addresses a different problem; instead of predicting the execution information, NE takes it as given, and instead learns to discover flexible task-specific NL rationales that aid a downstream programming task.\\n\\n7. Conclusion\\n\\nIn this paper we present NE, a self-training method to finetune LLMs to reason with program execution given traces. We demonstrate that PaLM 2-L trained using NE yields high-quality natural language rationales and achieves stronger success rates on two program repair tasks. As future work, we plan to apply NE to a broader range of program understanding tasks while expanding the trace representation to support more programming languages.\\n\\nAcknowledgements\\n\\nWe would like to express our sincere gratitude to Mart\u00edn Abadi, Xinyun Chen, Hanjun Dai, Yixin Liu, Sirui Lu, Kexin Pei and members of the Learning for Code team at Google DeepMind for their invaluable feedback. We thank anonymous reviewers for their insightful comments. We are also grateful to Austin Tarango for his support to this work.\\n\\nImpact Statement\\n\\nOur work aims to improve the debugging ability of code LLMs. While this may help developers produce better code, the code itself might have any number of positive or negative societal and ethical implications none of which we feel must be specifically highlighted here.\\n\\nReferences\\n\\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. PaLM 2 technical report. arXiv preprint arXiv:2305.10403, 2023.\\n\\nAnthony, T. W., Tian, Z., and Barber, D. Thinking fast and slow with deep learning and tree search. In Neural Information Processing Systems (NeurIPS), 2017.\\n\\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\"}"}
{"id": "B1W712hMBi", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Program synthesis with large language models.\\n\\narXiv preprint arXiv:2108.07732, 2021.\\n\\nBieber, D., Sutton, C., Larochelle, H., and Tarlow, D. Learning to execute programs with instruction pointer attention graph neural networks. In Advances in Neural Information Processing Systems (NeurIPS), October 2020.\\n\\nBieber, D., Goel, R., Zheng, D., Larochelle, H., and Tarlow, D. Static prediction of runtime errors by learning to execute programs with external resource descriptions. ArXiv, abs/2203.03771, 2022.\\n\\nBosnjak, M., Rockt\u00e4schel, T., Naradowsky, J., and Riedel, S. Programming with a differentiable Forth interpreter. ArXiv, abs/1605.06640, 2016.\\n\\nBouzenia, I., Ding, Y., Pei, K., Ray, B., and Pradel, M. TraceFixer: Execution trace-driven program repair. ArXiv, abs/2304.12743, 2023.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021a.\\n\\nChen, X., Song, D. X., and Tian, Y. Latent execution for neural program synthesis beyond domain-specific languages. ArXiv, abs/2107.00101, 2021b.\\n\\nChen, X., Lin, M., Sch\u00e4rli, N., and Zhou, D. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.\\n\\nChen, Z., Kommrusch, S., Tufano, M., Pouchet, L.-N., Poshyvanyk, D., and Monperrus, M. SequenceR: Sequence-to-sequence learning for end-to-end program repair. IEEE Transactions on Software Engineering, 47:1943\u20131959, 2018.\\n\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Valter, D., Narang, S., Mishra, G., Yu, A. W., Zhao, V., Huang, Y., Dai, A. M., Yu, H., Petrov, S., hsin Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416, 2022.\\n\\nCito, J., Dillig, I., Murali, V., and Chandra, S. Counterfactual explanations for models of code. International Conference on Software Engineering (ICSE), 2022.\\n\\nDasigi, P., Gardner, M., Murty, S., Zettlemoyer, L., and Hovy, E. H. Iterative search for weakly supervised semantic parsing. In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\\n\\nFakhoury, S., Chakraborty, S., Musuvathi, M., and Lahiri, S. K. Towards generating functionally correct code edits from natural language issue descriptions. ArXiv, abs/2304.03816, 2023.\\n\\nFan, Z., Gao, X., Mirchev, M., Roychoudhury, A., and Tan, S. H. Automated repair of programs from large language models. 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 1469\u20131481, 2022.\\n\\nFu, Y., Peng, H.-C., Ou, L., Sabharwal, A., and Khot, T. Specializing smaller language models towards multi-step reasoning. In International Conference on Machine Learning (ICML), 2023.\\n\\nGaunt, A. L., Brockschmidt, M., Kushman, N., and Tarlow, D. Differentiable programs with neural libraries. In International Conference on Machine Learning (ICML), 2016.\\n\\nGraves, A., Wayne, G., and Danihelka, I. Neural turing machines. ArXiv, abs/1410.5401, 2014.\\n\\nGunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Giorno, A. D., Gopi, S., Javaheripi, M., Kauffmann, P. C., de Rosa, G., Saarikivi, O., Salim, A., Shah, S., Behl, H. S., Wang, X., Bubeck, S., Eldan, R., Kalai, A. T., Lee, Y. T., and Li, Y.-F. Textbooks are all you need. ArXiv, abs/2306.11644, 2023.\\n\\nGuo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y., Li, Y. K., Luo, F., Xiong, Y., and Liang, W. DeepSeek-Coder: When the large language model meets programming \u2013 the rise of code intelligence, 2024.\\n\\nGupta, K., Christensen, P. E., Chen, X., and Song, D. X. Synthesize, execute and debug: Learning to repair for neural program synthesis. ArXiv, abs/2007.08095, 2020.\\n\\nHeinonen, A., Lehtel\u00e4, B., Hellas, A., and Fagerholm, F. Synthesizing research on programmers' mental models of programs, tasks and concepts - a systematic literature review. Inf. Softw. Technol., 164:107300, 2022.\\n\\nHoffman, M. D., Phan, D., Dohan, D., Douglas, S., Le, T. A., Parisi, A. T., Sountsov, P., Sutton, C., Vikram, S., and Saurous, R. A. Training Chain-of-Thought via Latent-Variable inference. In Conference on Neural Information Processing Systems (NeurIPS), 2023.\\n\\nHu, X., Li, G., Xia, X., Lo, D., and Jin, Z. Deep code comment generation. 2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC), pp. 200\u201320010, 2018.\\n\\nHunt, A. and Thomas, D. The pragmatic programmer: From journeyman to master. 1999.\"}"}
{"id": "B1W712hMBi", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NE\\n\\nX: Teaching Large Language Models to Reason about Code Execution\\n\\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\\n\\nJiang, N., Liu, K., Lutellier, T., and Tan, L. Impact of code language models on automated program repair. 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 1430\u20131442, 2023.\\n\\nKang, S., Chen, B., Yoo, S., and Lou, J.-G. Explainable automated debugging via large language model-driven scientific debugging. ArXiv, abs/2304.02195, 2023.\\n\\nKommrusch, S., Monperrus, M., and Pouchet, L.-N. Self-supervised learning to prove equivalence between straight-line programs via rewrite rules. IEEE Transactions on Software Engineering, 2023.\\n\\nKulal, S., Pasupat, P., Chandra, K., Lee, M., Padon, O., Aiken, A., and Liang, P. S. SPoC: Search-based pseudocode to code. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.\\n\\nLe Goues, C., Pradel, M., and Roychoudhury, A. Automated program repair. Communications of the ACM, 62(12):56\u201365, 2019.\\n\\nLi, R., Allal, L. B., Zi, Y ., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. StarCoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.\\n\\nLi, Y ., Wang, S., and Nguyen, T. N. DLFix: Context-based code transformation learning for automated program repair. 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE), pp. 602\u2013614, 2020.\\n\\nLi, Z., Lu, S., Guo, D., Duan, N., Jannu, S., Jenks, G., Majumder, D., Green, J., Svyatkovskiy, A., Fu, S., and Sundaresan, N. Automating code review activities by large-scale pre-training. Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2022.\\n\\nLiang, C., Norouzi, M., Berant, J., Le, Q. V ., and Lao, N. Memory augmented policy optimization for program synthesis and semantic parsing. Advances in Neural Information Processing Systems (NeurIPS), 31, 2018.\\n\\nLightman, H., Kosaraju, V ., Burda, Y ., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\\n\\nLiu, J., Xia, C. S., Wang, Y ., and Zhang, L. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. arXiv preprint arXiv:2305.01210, 2023.\\n\\nLongpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y ., Zhou, D., Le, Q. V ., Zoph, B., Wei, J., and Roberts, A. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning (ICML), 2023.\\n\\nMa, W., Liu, S., Wang, W., Hu, Q., Liu, Y ., Zhang, C., Nie, L., and Liu, Y . ChatGPT: Understanding code syntax and semantics. 2023.\\n\\nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y ., Welleck, S., Majumder, B. P., Gupta, S., Yazdianbakhsh, A., and Clark, P. Self-refine: Iterative refinement with self-feedback. ArXiv, abs/2303.17651, 2023.\\n\\nMitra, A., Corro, L. D., Mahajan, S., Codas, A., Simoes, C., Agrawal, S., Chen, X., Razdaibiedina, A., Jones, E., Aggarwal, K., Palangi, H., Zheng, G., Rosset, C., Khanpour, H., and Awadallah, A. Orca 2: Teaching small language models how to reason. ArXiv, abs/2311.11045, 2023.\\n\\nMuennighoff, N., Liu, Q., Zebaze, A., Zheng, Q., Hui, B., Zhuo, T. Y ., Singh, S., Tang, X., V on Werra, L., and Longpre, S. OctoPack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023.\\n\\nMukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., and Awadallah, A. H. Orca: Progressive learning from complex explanation traces of GPT-4. ArXiv, abs/2306.02707, 2023.\\n\\nNi, A., Yin, P., and Neubig, G. Merging weak and active supervision for semantic parsing. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 34, pp. 8536\u20138543, 2020.\\n\\nNi, A., Inala, J. P., Wang, C., Polozov, A., Meek, C., Radev, D., and Gao, J. Learning math reasoning from self-sampled correct and partially-correct solutions. In The Eleventh International Conference on Learning Representations (ICLR), 2022.\\n\\nNi, A., Iyer, S., Radev, D., Stoyanov, V ., Yih, W.-t., Wang, S., and Lin, X. V . LEVER: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning (ICML), pp. 26106\u201326128. PMLR, 2023.\\n\\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.\"}"}
{"id": "B1W712hMBi", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NE\\n\\nX\\n\\nT: Teaching Large Language Models to Reason about Code Execution\\n\\nOlausson, T. X., Inala, J. P., Wang, C., Gao, J., and Solar-Lezama, A. Demystifying GPT self-repair for code generation. arXiv preprint arXiv:2306.09896, 2023.\\n\\nOpenAI. GPT-4 technical report, 2023.\\n\\nPaul, R., Hossain, M. M., Siddiq, M. L., Hasan, M., Iqbal, A., and Santos, J. C. S. Enhancing automated program repair through fine-tuning and prompt engineering. 2023.\\n\\nPei, K., Bieber, D., Shi, K., Sutton, C., and Yin, P. Can large language models reason about program invariants? In International Conference on Machine Learning (ICML), 2023.\\n\\nPi, X., Liu, Q., Chen, B., Ziyadi, M., Lin, Z., Fu, Q., Gao, Y., Lou, J.-G., and Chen, W. Reasoning like program executors. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 761\u2013779, 2022.\\n\\nPrasad, A., Saha, S., Zhou, X., and Bansal, M. Receval: Evaluating reasoning chains via correctness and informativeness. arXiv preprint arXiv:2304.10703, 2023.\\n\\nRajani, N. F., McCann, B., Xiong, C., and Socher, R. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 4932\u20134942, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1487.\\n\\nRoss, S. I., Martinez, F., Houde, S., Muller, M. J., and Weisz, J. D. The programmer's assistant: Conversational interaction with a large language model for software development. Proceedings of the 28th International Conference on Intelligent User Interfaces, 2023.\\n\\nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code Llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\\n\\nShi, K., Dai, H., Ellis, K., and Sutton, C. CrossBeam: Learning to search in bottom-up program synthesis. In International Conference on Learning Representations (ICLR), 2022.\\n\\nShi, K., Hong, J., Deng, Y., Yin, P., Zaheer, M., and Sutton, C. ExeDec: Execution decomposition for compositional generalization in neural program synthesis. In International Conference on Learning Representations (ICLR), 2024.\\n\\nShin, E. C., Polosukhin, I., and Song, D. Improving neural program synthesis with inferred execution traces. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems (NeurIPS), volume 31. Curran Associates, Inc., 2018.\\n\\nShwartz, V., West, P., Le Bras, R., Bhagavatula, C., and Choi, Y. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4615\u20134629, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.373.\\n\\nSiegmund, B., Perscheid, M., Taeumel, M., and Hirschfeld, R. Studying the advancement in debugging practice of professional software developers. In 2014 IEEE International Symposium on Software Reliability Engineering Workshops, pp. 269\u2013274, 2014. doi: 10.1109/ISSREW.2014.36.\\n\\nSobania, D., Briesch, M., Hanna, C., and Petke, J. An analysis of the automatic bug fixing performance of ChatGPT. 2023 IEEE/ACM International Workshop on Automated Program Repair (APR), pp. 23\u201330, 2023.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n\\nUesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solving math word problems with process- and outcome-based feedback. ArXiv, abs/2211.14275, 2022.\\n\\nWang, C., Tatwawadi, K., Brockschmidt, M., Huang, P.-S., Mao, Y., Polozov, O., and Singh, R. Robust text-to-SQL generation with execution-guided decoding. arXiv: Computation and Language, 2018.\\n\\nWang, P., Li, L., Shao, Z., Xu, R. X., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. ArXiv, abs/2312.08935, 2024.\\n\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, January 2022a.\\n\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems (NeurIPS), 35:24824\u201324837, 2022b.\"}"}
