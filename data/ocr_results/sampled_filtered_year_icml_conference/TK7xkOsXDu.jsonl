{"id": "TK7xkOsXDu", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HiSS: Hierarchical State Space Models\\n\\nA. ReSkin fabrication details\\n\\nReSkin measures the changes in magnetic flux in its X, Y and Z coordinate system, based on the change in relative distance between the embedded magnetic microparticles in an elastomer matrix and a nearby magnetometer. The use of magnetic microparticles enables freedom in regard to the shape and dimensions of the molded skin. In our use case here, we use a skin of thickness 2mm. This section further details the complete fabrication process involved in the sensorized gripper tips we use for the ReSkin setup described in Section 4.1. Figure 5 illustrates different components of the sensorized gripper.\\n\\nA.1. Circuitry\\n\\nData from the ReSkin sensors is streamed to a computer via USB. The two sensors are connected to an I2C MUX which in turn is connected to an Adafruit QT Py microcontroller as described in Bhirangi et al. (2021). See Figure 5.\\n\\nA.2. OnRobot Gripper Tips\\n\\nThe skins are affixed to the 3D-printed gripper tips using silicone adhesive, as shown in Figure 6. The dimensions of the tips are 32 mm \u00d7 30 mm \u00d7 2 mm. The same tips also house the flex-PCB boards, which measure the change in magnetic flux in all 3 axes.\\n\\nB. Model architectures and Training\\n\\nB.1. Flat Architectures\\n\\nFor each of the flat sequence models presented in this work, the input sequence is first embedded into a hidden state sequence by a linear layer. This hidden state is then passed to the respective sequence model. The outputs of the sequence model (the hidden states for LSTM, S4 and Mamba) are then mapped to the desired output space.\\n\\nB.2. Hierarchical architectures\\n\\nThe hierarchical models are obtained by simply stacking two flat models together. The input sequence is first divided into equal sized chunks as described in Section 5.2. Each chunk is passed through the low-level sequence model and the outputs corresponding to the last timestep of each chunk are concatenated to form the chunk feature sequence. This sequence is passed through a high-level sequence model to obtain the output sequence.\"}"}
{"id": "TK7xkOsXDu", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HiSS: Hierarchical State Space Models\\n\\nB.3. Hyperparameters\\n\\nAll models are trained for 600 epochs at a constant learning rate of 1e-3. Learning rate schedulers were not found to improve performance by noticeable amounts. Table 5 contains the ranges of hyperparameters used for training the flat models presented in the paper. We do not sweep over all of these hyperparameters for each task. A subset of these parameters is chosen for each task depending on the input and output dimensionality of the task and the best-performing models are reported. The exact hyperparameters for each experiment can be found on the Github repository. For any given task, we ensure that sweeps over all model classes consist of models that have the same order of magnitude of learnable parameters.\\n\\n| Parameter          | Values         |\\n|--------------------|----------------|\\n| Input size         | 16, 32, 64, 128, 256 |\\n| Model dim          | 32, 64, 128, 256, 512 |\\n| LSTM hidden size   | 256, 512, 1024 |\\n| No. of heads       | 2, 4           |\\n| No. of layers      | 2, 4, 6       |\\n| Dropout            | 0.0, 0.1       |\\n\\nTable 5. Hyperparameters for flat architectures\\n\\nFor the hierarchical models, we use a smaller subset of the parameters listed in Table 5 to sweep over the high level models. Parameter ranges swept over for low-level models are listed in Table 6. The exact hyperparameters for each experiment can be found on the Github repository.\\n\\n| Parameter          | Values         |\\n|--------------------|----------------|\\n| Input size         | 16, 32, 64     |\\n| Model dim          | 16, 32, 64, 128, 256 |\\n| LSTM hidden size   | 16, 32, 64, 128, 256 |\\n| No. of layers      | 1, 4, 6       |\\n\\nTable 6. Hyperparameters for low-level models used in hierarchical architectures\\n\\nThese hyperparameter sweeps result in a range of models with different numbers of parameters. Table 7 lists the range of parameters resulting from the sweeps, and Table 8 contains the number of parameters in the best-performing models.\"}"}
{"id": "TK7xkOsXDu", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model type | Model Architecture | MW IS JC R V TC |\\n|------------|-------------------|----------------|\\n| Flat       | Transformer       | 0.4 - 9.5      |\\n|            | LSTM              | 3.4 - 13.7     |\\n|            | S4                | 0.8 - 4.0      |\\n|            | Mamba             | 0.5 - 10.2     |\\n| Hierarchical | Transformer    | 1.2 - 4.8      |\\n|            | LSTM              | 0.9 - 2.8      |\\n|            | S4                | 1.1 - 3.6      |\\n|            | Mamba             | 1.2 - 4.2      |\\n| LSTM       | Transformer       | 0.7 - 3.3      |\\n|            | LSTM              | 0.3 - 1.3      |\\n|            | S4                | 0.5 - 2.2      |\\n|            | Mamba             | 0.6 - 2.8      |\\n| S4         | Transformer       | 0.7 - 3.6      |\\n|            | LSTM              | 0.4 - 1.6      |\\n|            | S4                | 0.6 - 2.5      |\\n|            | Mamba             | 0.6 - 3.0      |\\n| Mamba      | Transformer       | 0.9 - 5.1      |\\n|            | LSTM              | 0.6 - 3.0      |\\n|            | S4                | 0.8 - 3.9      |\\n|            | Mamba             | 0.8 - 4.5      |\"}"}
{"id": "TK7xkOsXDu", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 8.\\nParameter count for best-performing baseline and HiSS models on CSP-Bench. Reported numbers are in millions of parameters.\\n\\n| Model type | Model Architecture | MW | IS | JC | V | TC |\\n|------------|--------------------|----|----|----|---|----|\\n| Flat       | Transformer        | 6.3| 0.6| 2.9| - | 0.4|\\n|            | LSTM               | 13.7| 3.7| 14.2| 0.9| 0.2| 13.8|\\n|            | S4                 | 4.0| 4.0| 5.1| 0.8| 0.4| 0.9|\\n|            | Mamba              | 10.2| 2.6| 7.9| 0.7| 0.7| 0.5|\\n| High-level |                    |    |    |    |    |    |    |\\n| Hierarchical | Transformer  | 1.2| 3.6| 1.5| 4.0| 0.2| 2.5|\\n|            | LSTM               | 0.9| 3.6| 2.9| 0.6| 0.4| 2.5|\\n|            | S4                 | 3.6| 4.4| 2.2| 1.5| 0.5| 2.1|\\n|            | Mamba              | 2.9| 3.7| 3.1| 0.4| 0.7| 2.9|\\n| Low-level  |                    |    |    |    |    |    |    |\\n| Hierarchical | Transformer  | 0.7| 3.7| 1.7| 1.0| 0.2| 0.9|\\n|            | LSTM               | 1.3| 1.3| 0.5| 1.1| 0.1| 0.6|\\n|            | S4                 | 2.2| 2.1| 2.3| 1.3| 0.2| 1.6|\\n|            | Mamba              | 2.7| 4.0| 1.5| 1.0| 0.3| 2.2|\\n| Hierarchical | Transformer  | 0.9| 1.2| 1.9| 1.0| 0.3| 2.9|\\n|            | LSTM               | 1.3| 3.1| 1.3| 1.6| 0.3| 1.6|\\n|            | S4                 | 2.5| 4.0| 2.1| 0.8| 0.3| 2.5|\\n|            | Mamba              | 3.0| 5.9| 3.2| 0.5| 0.4| 0.8|\\n| Low-level  |                    |    |    |    |    |    |    |\\n| Hierarchical | Transformer  | 2.4| 2.2| 1.2| 2.7| 0.2| 2.2|\\n|            | LSTM               | 0.8| 2.2| 2.3| 1.9| 0.1| 3.0|\\n|            | S4                 | 3.0| 3.0| 3.2| 1.7| 0.2| 3.1|\\n|            | Mamba              | 4.5| 2.5| 3.3| 0.8| 0.6| 2.1|\"}"}
{"id": "TK7xkOsXDu", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 11. Comparison of MSE prediction losses for flat and HiSS models on CSP-Bench when passing the input sequences through a low-pass filter. Reported numbers are averaged over 5 seeds for the best performing models. MW: Marker Writing, IS: Intrinsic Slip, JC: Joystick Control, TC: TotalCapture\\n\\n| Model type | Model Architecture | MW (cm/s) | IS (m/s) | JC (m/s) | RoNIN (m/s) | VECtor (m/s) | TC (m/s) |\\n|------------|--------------------|-----------|----------|----------|-------------|-------------|----------|\\n| Flat | Transformer | 1.7940 | 0.3096 | 1.0080 | - | 0.0346 | 0.3845 |\\n| | LSTM | 1.1498 | 0.2596 | 1.0770 | 0.0382 | 0.0242 | 0.1234 |\\n| | S4 | 1.1885 | 0.2209 | 0.9449 | 0.0305 | 0.0228 | 0.2467 |\\n| | Mamba | 0.7823 | 0.1367 | 0.9459 | 0.0297 | 0.0188 | 0.1661 |\\n| High-level | Transformer | 1.0052 | 0.1883 | 0.9074 | 0.0532 | 0.0284 | 0.2314 |\\n| | LSTM | 0.8912 | 0.1251 | 1.0500 | 0.0435 | 0.0243 | 0.3118 |\\n| | S4 | 0.8063 | 0.2434 | 1.0500 | 0.0430 | 0.0272 | 0.1754 |\\n| | Mamba | 0.7515 | 0.1657 | 1.0080 | 0.0420 | 0.0234 | 0.1755 |\\n| Hierarchical | Transformer | 0.8525 | 0.1390 | 0.9269 | 0.0306 | 0.0272 | 0.1905 |\\n| | LSTM | 0.6462 | 0.1477 | 0.9885 | 0.0419 | 0.0288 | 0.1968 |\\n| | S4 | 0.5535 | 0.1074 | 0.8665 | 0.0362 | 0.0272 | 0.1301 |\\n| | Mamba | 0.7825 | 0.1180 | 0.8898 | 0.0396 | 0.0207 | 0.2527 |\\n\\nTable 12. Comparison of MSE prediction losses for flat and HiSS models on CSP-Bench when using a fraction of the training dataset. Reported numbers are averaged over 5 seeds for the best performing models. MW: Marker Writing, IS: Intrinsic Slip, JC: Joystick Control, TC: TotalCapture\\n\\n| Model type | Model Architecture | MW (cm/s) | IS (m/s) | JC (m/s) | RoNIN (m/s) | VECtor (m/s) |\\n|------------|--------------------|-----------|----------|----------|-------------|-------------|\\n| Flat | Transformer | 4.2975 | 0.8509 | 1.2370 | - | 0.0460 | 0.5430 |\\n| | LSTM | 1.8322 | 0.5376 | 1.3130 | 0.0533 | 0.0390 | 0.3855 |\\n| | S4 | 2.3070 | 0.4450 | 1.1970 | 0.0431 | 0.0379 | 0.4338 |\\n| | Mamba | 1.7443 | 0.3677 | 1.1950 | 0.0394 | 0.0358 | 0.4838 |\\n| High-level | S4 | 1.5417 | 0.3428 | 1.2350 | 0.0387 | 0.0331 | 0.3982 |\\n| | Mamba | 2.3302 | 0.3760 | 1.1060 | 0.0412 | 0.0326 | 0.4913 |\\n| Hierarchical | S4 | 1.2600 | 0.2883 | 1.1370 | 0.0378 | 0.0333 | 0.3675 |\\n| | Mamba | 1.7508 | 0.3688 | 1.1140 | 0.0383 | 0.0286 | 0.4320 |\"}"}
{"id": "TK7xkOsXDu", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HiSS: Hierarchical State Space Models\\n\\nE. TotalCapture Preprocessing\\n\\nThis dataset provides readings from 12 IMU sensors and the ground truth poses of 21 joints obtained from the Vicon motion capture system. To standardize the data within a consistent coordinate system, we transformed all IMU sensor readings from their native IMU frames to the Vicon frame. Our task is to predict the velocities of the 21 joints given the IMU acceleration data in the Vicon reference frame.\\n\\nTo convert IMU acceleration data into the Vicon frame, we utilize the calibration results provided in the files named `<subject id>` `<sequence name>` `calib imu ref.txt` and `<sequence name>` `Xsens AuxFields.sensors`. The acceleration of each IMU sensor in the Vicon frame is calculated as follows:\\n\\n\\\\[\\n a_{\\\\text{vicon}} = R_{\\\\text{vicon inertial}} R_{\\\\text{inertial imu}} a_{\\\\text{imu}},\\n\\\\]\\n\\nwhere \\\\( R_{\\\\text{inertial imu}} \\\\) is the rotation matrix converted from the IMU local orientation quaternion \\\\((w, x, y, z)\\\\) provided in the `<sequence name>` `Xsens AuxFields.sensors` files. This quaternion represents the IMU's orientation in the inertial reference frame.\\n\\nFurthermore, \\\\( R_{\\\\text{vicon inertial}} \\\\) is obtained by converting the quaternion information \\\\((<imu name> x y z w)\\\\) available in the `<subject id>` `<sequence name>` `calib imu ref.txt` files, which encapsulates the transformation from the inertial frame to the Vicon global frame.\"}"}
{"id": "TK7xkOsXDu", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HiSS: Hierarchical State Space Models\\n\\npitfall of handcrafted preprocessing techniques \u2013 they can often filter out information that could have been exploited by a sufficiently potent model. Consequently, the ability of HiSS models to require little to no preprocessing of the input sequence bolsters their credentials to serve as general purpose models for CSP data.\\n\\n6.6. How Does HiSS Perform on Smaller Datasets?\\n\\nThe lack of a comprehensive benchmark for continuous sequence prediction so far speaks to the difficulty of collecting large, labeled datasets of sensory data. Therefore, performance in low-data regimes could be critical to wider applicability of different sequence modeling architectures.\\n\\nTo benchmark this performance, we compare the performance of flat as well as HiSS models on subsets of the training data. While TotalCapture and VECtor are substantially smaller than the other datasets (see Table 1), we include them in this analysis while using a larger fraction of training data than other datasets. Results are presented in Table 3.\\n\\nWe only present the best performing HiSS model here for conciseness. The full table can be found in Appendix D.\\n\\nWe see that on smaller fractions of the training dataset, HiSS outperforms flat baselines on every task in CSP-Bench. This indicates an important property of HiSS models \u2013 data efficiency. Low-level models operate identically on all of the chunks in the data, allowing them to learn more effective representations from small datasets than flat models.\\n\\n6.7. Failure on TotalCapture\\n\\nThe most visible failure case for the performance of both flat SSMs as well as HiSS models is on the TotalCapture dataset, where the flat LSTM significantly outperforms all other models. We hypothesize that the high dimensionality of the input and output spaces prevents SSMs from learning sufficiently expressive representations that can filter out high frequency data. This is also evidenced by the higher performance of LSTM low-level models across hierarchical architectures for this dataset, which correlates with the correspondingly higher effectiveness of the flat LSTM over flat SSMs. Further evidence of the inability of SSMs to filter out noise can be found in Section 6.5, where the performance of HiSS models nearly matches the LSTM when the input sequence is passed through a lowpass filter. This indicates that the HiSS model struggles to learn the filtering behavior from data here, unlike other datasets where performance remains fairly consistent with and without the lowpass filter.\\n\\n7. Conclusion and Limitations\\n\\nWe present CSP-Bench, the first publicly available benchmark for Continuous Sequence Prediction, and show that SSMs do better than LSTMs and Transformers on CSP tasks. Then, we propose HiSS, a hierarchical sequence modeling architecture that is more performative, data efficient and minimizes preprocessing needs for CSP problems. However, sequence-to-sequence prediction from sensory data continues to be an open, relatively underexplored problem, and our work indicates significant room for improvement.\\n\\nMoreover, while SSMs show significant promise for CSP tasks, they are relatively new architectures whose strengths and weaknesses are far from being well-understood. Section 6.7 explains some of the challenges of SSMs, and as a result, HiSS, on high-dimensional prediction problems with small datasets of noisy sensor data. In terms of ease of training, current HiSS models introduce an additional hyperparameter of chunk size. While we provide a preliminary analysis of the effect of chunk size in Section 6.4, optimizing chunk size is an exciting future direction. Finally, CSP-Bench is large, but the number of sensors that can benefit from learned models is larger. We are committed to supporting CSP-Bench and adding more, larger datasets in the future.\\n\\nAcknowledgements\\n\\nNYU authors are supported by grants from Honda, and ONR award numbers N00014-21-1-2404 and N00014-21-1-2758. LP is supported by the Packard Fellowship. We also thank Aadhithya Iyer, Gaoyue Zhou, Irmak Guzey, Ulyana Piterbarg, Vani Sundaram and all other members of GRAIL, NYU for their valuable help and feedback throughout this project.\\n\\nImpact Statement\\n\\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.\\n\\nReferences\\n\\nAmini, N., Sarrafzadeh, M., Vahdatpour, A., and Xu, W. Accelerometer-based on-body sensor localization for health and medical monitoring applications. Pervasive and mobile computing, 7(6):746\u2013760, 2011.\\n\\nArunachalam, S. P., G\u00a8uzey, I., Chintala, S., and Pinto, L. Holo-dex: Teaching dexterity with immersive mixed reality. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 5962\u20135969. IEEE, 2023.\\n\\nBhirangi, R., Hellebrekers, T., Majidi, C., and Gupta, A. Reskin: versatile, replaceable, lasting tactile skins. arXiv preprint arXiv:2111.00071, 2021.\\n\\nBhirangi, R., DeFranco, A., Adkins, J., Majidi, C., Gupta, A., Hellebrekers, T., and Kumar, V. All the feels: A\"}"}
{"id": "TK7xkOsXDu", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "TK7xkOsXDu", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Herath, S., Yan, H., and Furukawa, Y. Ronin: Robust neural inertial navigation in the wild: Benchmark, evaluations, & new methods. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 3146\u20133152. IEEE, 2020.\\n\\nHolden, D., Saito, J., and Komura, T. A deep learning framework for character motion synthesis and editing. ACM Transactions on Graphics (TOG), 35(4):1\u201311, 2016.\\n\\nKoksal, N., Jalalmaab, M., and Fidan, B. Adaptive linear quadratic attitude tracking control of a quadrotor uav based on imu sensor data fusion. Sensors, 19(1):46, 2018.\\n\\nKong, B., Zhan, Y., Shin, M., Denny, T., and Zhang, S. Recognizing end-diastole and end-systole frames via deep temporal regression network. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part III 19, pp. 264\u2013272. Springer, 2016.\\n\\nKuchaiev, O. and Ginsburg, B. Factorization tricks for lstm networks. arXiv preprint arXiv:1703.10722, 2017.\\n\\nKulkarni, T. D., Narasimhan, K., Saeedi, A., and Tenenbaum, J. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. Advances in neural information processing systems, 29, 2016.\\n\\nLambeta, M., Chou, P.-W., Tian, S., Yang, B., Maloon, B., Most, V. R., Stroud, D., Santos, R., Byagowi, A., Kammerer, G., et al. Digit: A novel design for a low-cost compact high-resolution tactile sensor with application to in-hand manipulation. IEEE Robotics and Automation Letters, 5(3):3838\u20133845, 2020.\\n\\nLiu, W., Caruso, D., Ilg, E., Dong, J., Mourikis, A., Daniilidis, K., Kumar, V., and Engel, J. Tlio: Tight learned inertial odometry. IEEE Robotics and Automation Letters, PP:1\u20131, 07 2020a. doi: 10.1109/LRA.2020.3007421.\\n\\nLiu, W., Caruso, D., Ilg, E., Dong, J., Mourikis, A. I., Daniilidis, K., Kumar, V., and Engel, J. Tlio: Tight learned inertial odometry. IEEE Robotics and Automation Letters, 5(4):5653\u20135660, 2020b.\\n\\nMa, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., and Zettlemoyer, L. Mega: moving average equipped gated attention. arXiv preprint arXiv:2209.10655, 2022.\\n\\nMaddern, W., Pascoe, G., Linegar, C., and Newman, P. 1 year, 1000 km: The oxford robotcar dataset. The International Journal of Robotics Research, 36(1):3\u201315, 2017.\\n\\nMathieu, J. L., Koch, S., and Callaway, D. S. State estimation and control of electric loads to manage real-time energy imbalance. IEEE Transactions on power systems, 28(1):430\u2013440, 2012.\\n\\nMicucci, D., Mobilio, M., and Napoletano, P. Unimib shar: A dataset for human activity recognition using acceleration data from smartphones. Applied Sciences, 7(10):1101, 2017.\\n\\nMoody, G. B. and Mark, R. G. The impact of the mit-bih arrhythmia database. IEEE engineering in medicine and biology magazine, 20(3):45\u201350, 2001.\\n\\nMorrill, J., Salvi, C., Kidger, P., and Foster, J. Neural rough differential equations for long time series. In International Conference on Machine Learning, pp. 7829\u20137838. PMLR, 2021.\\n\\nOrvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., and De, S. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pp. 26670\u201326698. PMLR, 2023.\\n\\nPinto, L., Gandhi, D., Han, Y., Park, Y.-L., and Gupta, A. The curious robot: Learning visual representations via physical interactions. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 3\u201318. Springer, 2016.\\n\\nPoli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and R\u00b4e, C. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023.\\n\\nRusch, T. K., Mishra, S., Erichson, N. B., and Mahoney, M. W. Long expressive memory for sequence modeling. arXiv preprint arXiv:2110.04744, 2021.\\n\\nSch\u00a8utze, M., Campisano, A., Colas, H., Schilling, W., and Vanrolleghem, P. A. Real time control of urban wastewater systems\u2014where do we stand today? Journal of hydrology, 299(3-4):335\u2013348, 2004.\\n\\nSimon, D. Optimal state estimation: Kalman, H infinity, and nonlinear approaches. John Wiley & Sons, 2006.\\n\\nSmith, J. T., Warrington, A., and Linderman, S. W. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022.\\n\\nSonar, H. A., Yuen, M. C., Kramer-Bottiglio, R., and Paik, J. An any-resolution pressure localization scheme using a soft capacitive sensor skin. In 2018 IEEE International Conference on Soft Robotics (RoboSoft), pp. 170\u2013175. IEEE, 2018.\"}"}
{"id": "TK7xkOsXDu", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HiSS: Hierarchical State Space Models\\n\\nStetco, A., Dinmohammadi, F., Zhao, X., Robu, V., Flynn, D., Barnes, M., Keane, J., and Nenadic, G. Machine learning methods for wind turbine condition monitoring: A review. Renewable energy, 133:620\u2013635, 2019.\\n\\nSun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2446\u20132454, 2020.\\n\\nSutton, R. S., Precup, D., and Singh, S. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181\u2013211, 1999.\\n\\nThu, N. T. H. and Han, D. S. Hihar: A hierarchical hybrid deep learning architecture for wearable sensor-based human activity recognition. IEEE Access, 9:145271\u2013145281, 2021.\\n\\nTomo, T. P., Regoli, M., Schmitz, A., Natale, L., Kristanto, H., Somlor, S., Jamone, L., Metta, G., and Sugano, S. A new silicone structure for uskin\u2014a soft, distributed, digital 3-axis skin sensor and its integration on the humanoid robot icub. IEEE Robotics and Automation Letters, 3(3):2584\u20132591, 2018.\\n\\nTrumble, M., Gilbert, A., Malleson, C., Hilton, A., and Collomosse, J. Total capture: 3d human pose estimation fusing video and inertial sensors. In Proceedings of 28th British Machine Vision Conference, pp. 1\u201313, 2017.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nWagner, P., Strodthoff, N., Bousseljot, R.-D., Kreiseler, D., Lunze, F. I., Samek, W., and Schaeffter, T. Ptb-xl, a large publicly available electrocardiography dataset. Scientific data, 7(1):154, 2020.\\n\\nWarden, P. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209, 2018.\\n\\nWelch, G., Bishop, G., et al. An introduction to the kalman filter. 1995.\\n\\nYan, H., Shan, Q., and Furukawa, Y. Ridi: Robust imu double integration. In Proceedings of the European conference on computer vision (ECCV), pp. 621\u2013636, 2018.\\n\\nYou, J., Wang, Y., Pal, A., Eksombatchai, P., Rosenberg, C., and Leskovec, J. Hierarchical temporal convolutional networks for dynamic recommender systems. In The world wide web conference, pp. 2236\u20132246, 2019.\\n\\nYuan, W., Dong, S., and Adelson, E. H. Gelsight: High-resolution robot tactile sensors for estimating geometry and force. Sensors, 17(12):2762, 2017.\"}"}
{"id": "TK7xkOsXDu", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HiSS: Hierarchical State Space Models\\n\\nC. Experimental Setup and Data Collection details\\n\\nFigure 7. Marker Writing Frames (Top): The gripper tips hold the marker and bring it in contact with the paper before the sequence starts. The arm maneuvers the marker to execute eight strokes on the paper. Intrinsic Slip Frames (Middle): The gripper tips hold the box to start the sequence, and slip through the robot workspace with different orientations. Joystick Control Frames (Bottom): After the sequence begins, the hand holds the joystick, controlling its movement through various positions.\\n\\nC.1. ReSkin: Onrobot Gripper on a Kinova JACO Arm\\n\\nC.1.1. Marker Writing\\n\\nFor this experiment, we first grasp the marker with 300 N force in an arbitrary position and bring it in contact with the paper. We then start recording data and command the robot to move sequentially to 8-12 randomly sampled locations within a $10 \\\\times 10$ cm$^2$ plane workspace, making linear strikes on the paper. Figure 7 illustrates a sample sequence from this dataset. We note that during the strikes, the grasped marker undergoes orientation drifts at times, which adds to the complexity in signal. We record a total of 1000 trajectories of 15-30 seconds each, comprising of 2 different colored markers. The prediction task here is to predict the strike velocity ($\\\\delta x/\\\\delta t$, $\\\\delta y/\\\\delta t$), given the tactile signals thus reconstructing the overall trajectory.\\n\\nC.1.2. Intrinsic Slip\\n\\nIn Section 4.1.2, we outlined our methodology for collecting data through a total of 1000 trajectories. This involved using 10 distinct boxes and 4 sets of skins for 25 trajectories per combined pair. We first sample a random location and orientation within the task workspace. Next, we close the gripper with a random force sampled in the range of 50-75 N and then start recording data. With the gripper grasping the box, we uniformly sample 8-12 locations sequentially, thus slipping through the robot workspace. Figure 7 illustrates a sample sequence from this dataset. The workspace is the upper region of the box, which is a space of dimensions Box Length x Tip Size(3cm), shown in Figure 9. We clamp the wrist rotation limits at $[-\\\\pi/4, \\\\pi/4]$, making the overall local sampling bounds of the gripper tip position (center of tip), Y:[0, box length], Z:[0, tip size], $\\\\theta$:[-\\\\pi/4, \\\\pi/4].\"}"}
{"id": "TK7xkOsXDu", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bhirangi et al. (2021) characterize the ability of ReSkin sensor models to generalize to skins outside the training distribution, but these experiments are limited to single-frame, static data. Here, we collect an analogous dataset for the sequence-to-sequence prediction problem. To avoid confounding effects, the evaluations provided in this paper are based on a random partitioning of this dataset. However, we collect and publish an additional 100 trajectories on an unseen box and an unseen set of skins to test the generalizability of trained models.\\n\\nThe dimensions of all boxes used in this experiment are detailed below. See Table 9 and Figure 8.\\n\\nIn this experiment, in addition to predicting the linear velocities of the end-effector, we also predict the angular velocities at the wrist/the end-effector rotation ($\\\\delta x/\\\\delta t$, $\\\\delta y/\\\\delta t$, $\\\\delta \\\\theta/\\\\delta t$).\\n\\nC.2. Xela: Allegro Hand on a Franka Emika Panda Arm\\n\\nC.2.1. joystick control\\n\\nFor the final tactile dataset, we teleoperate an Allegro Hand with Xela sensors mounted on a Franka arm to interact with an Extreme3D Pro Joystick shown in Figure 10, which streams data comprising of 6 rotation axes (X, Y, Rz, Throttle, Hat0X, Hat0Y) and 12 buttons (Trigger, 2 Thumb Buttons, 2 Top Buttons, 1 Pinkie Button and 6 Base Buttons). Unlike the prior datasets, which originated out of random yet scripted policies, this dataset has an added complexity from the unstructured human interactive control. Figure 7 illustrates a sample sequence from this dataset. Due to the arm workspace and the finger size constraints, we focus on 3 axes - X, Y and Z-twist for our prediction task. Given the readings from the Xela sensors, we...\"}"}
{"id": "TK7xkOsXDu", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 9. Dimensions of Boxes in the Dataset\\n\\n| Box Number | Dimensions (L x H x W cm) |\\n|------------|---------------------------|\\n| 1          | 20 x 12 x 4               |\\n| 2          | 16.5 x 8.5 x 3            |\\n| 3          | 14 x 9 x 5                |\\n| 4          | 17 x 13 x 4.5             |\\n| 5          | 15 x 10 x 4.5             |\\n| 6          | 16.5 x 13 x 6             |\\n| 7          | 17 x 10 x 5.5             |\\n| 8          | 18 x 19.5 x 5.5           |\\n| 9          | 17 x 11 x 3.5             |\\n| 10         | 12 x 8 x 6.5              |\\n| 11 (unseen)| 23 x 16 x 5               |\\n\\nHiSS: Hierarchical State Space Models\\n\\nFigure 9. End-effector Workspace on the Box, & Local Co-ordinate System\\n\\nFigure 10. Extreme3D Pro Joystick & Co-ordinate System\"}"}
{"id": "TK7xkOsXDu", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D. Results and Ablations\\n\\nD.1. Standard deviations for reported results\\n\\nThe results presented in Table 2 are averaged over 5 random seeds each. Table 10 presents the standard deviations over seeds for each of the tasks and models.\\n\\nTable 10. Comparison of standard deviation in MSE over 5 seeds for baseline and HiSS models on CSP-Bench.\\n\\n| Model Type | Model Architecture | MW (cm/s) | IS (m/s) | JC (m/s) | RoNIN (m/s) | VECtor (m/s) | TotalCapture (m/s) |\\n|------------|--------------------|-----------|----------|----------|-------------|--------------|-------------------|\\n| Flat       | Transformer        | 0.0805    | 0.0161   | 0.0544   | -           | 0.0004       | -                 |\\n|            | LSTM               | 0.0540    | 0.0184   | 0.0006   | 0.0074      | 0.0014       | 0.0039            |\\n|            | S4                 | 0.0634    | 0.0159   | 0.0188   | 0.0049      | 0.0012       | 0.0172            |\\n|            | Mamba              | 0.0224    | 0.0111   | 0.1060   | 0.0040      | 0.0011       | 0.0064            |\\n| High-level | Transformer        | 0.0438    | 0.0164   | 0.0250   | 0.0057      | 0.0013       | 0.0159            |\\n|            | LSTM               | 0.0429    | 0.0250   | 0.0420   | 0.0039      | 0.0016       | 0.0114            |\\n|            | S4                 | 0.0215    | 0.0084   | 0.0188   | 0.0021      | 0.0028       | 0.0416            |\\n|            | Mamba              | 0.0617    | 0.0145   | 0.0180   | 0.0054      | 0.0015       | 0.0202            |\\n| Low-level  | Transformer        | 0.0359    | 0.0120   | 0.0721   | 0.1826      | 0.0017       | 0.0257            |\\n|            | LSTM               | 0.0310    | 0.0093   | 0.0244   | 0.0022      | 0.0012       | 0.0121            |\\n|            | S4                 | 0.0405    | 0.0069   | 0.0295   | 0.0022      | 0.0014       | 0.0038            |\\n|            | Mamba              | 0.1174    | 0.0179   | 0.0199   | 0.0049      | 0.0014       | 0.0143            |\\n| LSTM       | Transformer        | 0.0545    | 0.0273   | 0.0172   | 0.0031      | 0.0015       | 0.0030            |\\n|            | LSTM               | 0.0511    | 0.0099   | 0.0255   | 0.0012      | 0.0014       | 0.0069            |\\n|            | S4                 | 0.0274    | 0.0076   | 0.0238   | 0.0009      | 0.0008       | 0.0179            |\\n|            | Mamba              | 0.0357    | 0.0044   | 0.0136   | 0.0024      | 0.0012       | 0.0151            |\\n| Mamba      | Transformer        | 0.0499    | 0.0154   | 0.0500   | 0.0050      | 0.0007       | 0.0077            |\\n|            | LSTM               | -         | 0.0142   | 0.0131   | 0.0030      | 0.0013       | 0.0171            |\\n|            | S4                 | 0.0453    | 0.0066   | 0.0347   | 0.0019      | 0.0016       | 0.0088            |\\n|            | Mamba              | 0.0542    | 0.0042   | 0.0313   | 0.0022      | 0.0010       | 0.0156            |\\n\\nD.2. Sensor Data Preprocessing with Filtering\\n\\nIn this section, we provide more detailed tables for the experiments in Sections 6.5. Table 11 contains results from separately applying order 3 Butterworth filters to the input sequences with cutoff frequencies of 0.75Hz, 2.5Hz and 7.5Hz. For each setting, we pick the set of models corresponding to the cutoff frequency with the best performance, and report average performance over 3 seeds.\\n\\nD.3. Smaller Datasets\\n\\nIn this section, we provide more detailed tables for the experiments in Sections 6.6. Table 12 contains results from subsampling the training datasets \u2013 30% of the dataset for MW, IS, JC and RoNIN, and 50% of the dataset for VECtor and TotalCapture. We see that HiSS consistently outperforms flat models across tasks in CSP-Bench when training on fractions of the training dataset, indicating the sample efficiency of HiSS models.\"}"}
{"id": "TK7xkOsXDu", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hierarchical State Space Models\\nfor Continuous Sequence-to-Sequence Modeling\\n\\nRaunaq Bhirangi 1 2\\nChenyu Wang 3\\nVenkatesh Pattabiraman 3\\nCarmel Majidi 1\\nAbhinav Gupta 1\\nTess Hellebrekers 2\\nLerrel Pinto 3\\n\\nAbstract\\nReasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io\\n\\n1 Carnegie Mellon University, Pittsburgh, USA\\n2 FAIR, Meta\\n3 New York University, NYC, USA. Correspondence to: Raunaq Bhirangi <rbhirang@cs.cmu.edu>.\\n\\nProceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\\n\\nFigure 1. CSP-Bench is a publicly accessible benchmark for continuous sequence prediction on real-world sensory data. We show that Hierarchical State Space Models (HiSS) improve over conventional sequence models on sequential sensory prediction tasks.\\n\\n1. Introduction\\nSensors are ubiquitous. From air conditioners to smartphones, automated systems analyze sensory data sequences to control various parameters. This class of problems - continuous sequence-to-sequence prediction from streaming sensory data - is central to real-time decision-making and control (Sch\u00fctze et al., 2004; Stetco et al., 2019). Yet, it has received limited attention compared to discrete sequence problems in domains like language (Devlin et al., 2018) and computer vision (Deng et al., 2009).\\n\\nExisting approaches for prediction from sensory data have largely relied on model-based solutions (Welch et al., 1995; Daum, 2005). However, these approaches require domain expertise and accurate modeling of complex system dynamics, which is often intractable in real-world applications. Moreover, sensory data contains noise and sensor-specific drift that must be accounted for to achieve high predictive performance (Liu et al., 2020b). In this work, we investigate deep sequence-to-sequence models that can address these challenges by learning directly from raw sensory streams.\\n\\nHowever, to make progress on continuous sequence prediction (CSP), we first need a representative benchmark to measure performance. Most prior works in CSP focus on a single class of sensors (Herath et al., 2020; Liu et al., 2020b),\\n\\n| Sensor | Signal | Marker position | CSP-Bench (Normalized Mean Squared Error) |\\n|--------|--------|----------------|------------------------------------------|\\n| Magnetometer | signal | Marker position | x (cm) | Transformer | LSTM | Mamba | S4 | HiSS |\\n|        |        |                 |       | 1         | 2         | 3       | 4       | 5   |\\n| Time (in seconds) |       |                 |       |           |           |         |         |     |\\n| \u00a9 2022 MPL, ShanghaiTech Uni., China. All rights reserved.\"}"}
{"id": "TK7xkOsXDu", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HiSS: Hierarchical State Space Models making it difficult to develop general-purpose algorithms. To address this, we created CSP-Bench, a benchmark consisting of six real-world labeled datasets. This collection consists of three datasets created in-house and three curated from prior work \u2013 a cumulative 40 hours of real-world data. Given data from CSP-Bench, an obvious modeling choice is to use state-of-the-art sequence models like LSTMs or Transformers. However, sensory data is high-frequency, leading to long sequences of highly correlated data. For such data, Transformers quickly run out of memory, as they scale quadratically in complexity with sequence length (Vaswani et al., 2017), while LSTMs require significantly larger hidden states (Kuchaiev & Ginsburg, 2017). Deep State Space Models (SSMs) (Gu et al., 2021a; Gu & Dao, 2023) are a promising new class of sequence models. These models have been shown to effectively handle long context lengths while scaling linearly with sequence length in time and memory complexity, with strong results on audio (Goel et al., 2022) and language modeling. On CSP-Bench, we find that SSMs consistently outperform LSTMs and Transformers with an average of 10% improvement on MSE metrics (see Section 6). But can we do better?\\n\\nA key insight into continuous sensor data is that it has a significant amount of temporal structure and redundancies. While SSMs are powerful for modeling this type of data, they are still temporally flat in nature, i.e. every sample in the sequence is reasoned with every other sample. Therefore, inspired by work in hierarchical modeling (You et al., 2019; Thu & Han, 2021), we propose Hierarchical State-Space Models (HiSS). HiSS stacks two SSMs with different temporal resolutions on top of each other. The lower-level SSM temporally chunks the larger full-sequence data into smaller sequences and outputs local features, while the higher-level SSM operates on the smaller sequence of local features to output global sequence prediction. This leads to further improved performance on CSP-Bench, outperforming the best flat SSMs by 23% median MSE performance across tasks.\\n\\nWe summarize the contributions of this paper as follows:\\n\\n1. We release CSP-Bench, the largest publicly accessible benchmark for continuous sequence-to-sequence prediction for multiple sensor datasets. (Section 4)\\n2. We show that SSMs outperform prior SOTA models like LSTMs and Transformers on CSP-Bench. (Section 6.1)\\n3. We propose HiSS, a hierarchical sequence modeling architecture that further improves upon SSMs across tasks in CSP-Bench. (Section 5)\\n4. We show that HiSS increases sample efficiency with smaller datasets, and is compatible with standard sensor pre-processing techniques such as low-pass filtering. (Sections 6.5, 6.6)\\n\\n2. Related Work\\n2.1. Sequence-to-sequence prediction for sensory data\\nMost real world control systems, such as wind turbine condition monitoring (Stetco et al., 2019), MRI recognition (Kong et al., 2016) and inertial odometry (Amini et al., 2011; Liu et al., 2020a), often process noisy sensory data to deduce environmental states. Traditionally, these problems were solved as estimation and control problems using filtering techniques, like the Kalman Filter (Mathieu et al., 2012; Simon, 2006), that still require complex sensor models. Deep learning has shown promise in domains without analytical models, yet many solutions continue to be sensor-specific (Yan et al., 2018; Herath et al., 2020).\\n\\nMore recently, a number of works (Hasani et al., 2022; Ma et al., 2022; Rusch et al., 2021; Morrill et al., 2021; Orvieto et al., 2023) have been directed at developing neural architectures that improve over conventional sequence models in modeling long-range dependencies. This bodes well for learning sensory prediction models which must naturally reason over long sequences owing to the high frequency nature of sensory data. To the best of our knowledge, however, none of these models have been evaluated on continuous sensing data beyond audio (Goel et al., 2022). In this work, we focus on deep state space models (SSMs) (Gu et al., 2021a; Poli et al., 2023; Smith et al., 2022; Gu & Dao, 2023; Hasani et al., 2022), an emerging class of models in long range neural sequence modeling. We benchmark deep SSMs on six sequence-to-sequence prediction tasks on sensors like ReSkin, XELA, accelerometers, and gyroscopes.\\n\\n2.2. Hierarchical Modeling\\nIncorporating temporal hierarchies into sequence modeling architectures has been shown to improve performance across a number of tasks like recommender systems (You et al., 2019), human activity recognition (Thu & Han, 2021) and reinforcement learning (Sutton et al., 1999; Gardiol, 2000; Kulkarni et al., 2016). HiSS is inspired by this line of work and extends it to SSMs for continuous seq-to-seq tasks.\\n\\n2.3. Data for Continuous Sequence Prediction\\nA primary challenge with developing general models for continuous sequence prediction is the lack of a concrete evaluation benchmark. Odometry/SLAM datasets (Geiger et al., 2013; Maddern et al., 2017) are viable candidates (Chang et al., 2019; Sun et al., 2020) for CSP datasets. But most data across sensory modalities like audio (Warden, 2018; Gemmeke et al., 2017), ECG (Moody & Mark, 2001; Wagener et al., 2020), IMU (Chavarriaga et al., 2013; Micucci et al., 2017; Chen et al., 2021) and tactile sensing (Pinto et al., 2016; Funabashi et al., 2019; Bhirangi et al., 2023) is labeled sparsely only at the sequence level.\"}"}
{"id": "TK7xkOsXDu", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The recent proliferation of sensors in smartphones and other smart devices has resulted in renewed interest in creating labeled datasets for CSP (Chen et al., 2018; Herath et al., 2020). A common setting is to use a motion capture system to obtain dense, sequential labels for sensory data from inexpensive IMU sensors (Trumble et al., 2017; Gao et al., 2022). In this work, we curate three such datasets as part of CSP-Bench: a continuous sequence prediction benchmark.\\n\\nAnother category of sensors of significant interest for CSP are touch sensors. Touch sensors capture the dynamics of contact between the robot and its surroundings. Deep learning and rapid prototyping have driven a rapid surge across a range of tactile modalities from optical (Lambeta et al., 2020; Yuan et al., 2017) to capacitative (Sonar et al., 2018) and magnetic sensing (Tomo et al., 2018; Bhirangi et al., 2021). Most work on continuously reasoning over tactile data is directed towards policy learning (Guzey et al., 2023a; b; Calandra et al., 2018), where small datasets and confounding factors make it difficult to evaluate the efficacy of architectures for CSP. In this work, we set up supervised learning problems to investigate sequence-to-sequence models for two magnetic tactile sensors: ReSkin (Bhirangi et al., 2021) and XELA (Tomo et al., 2018).\\n\\n3. Background\\n\\n3.1. Sequence-to-sequence Prediction\\n\\nConsider a data-generating process described by the Hidden Markov Model in Figure 2. The observable processes \u2013 sensor, \\\\( S \\\\), and output, \\\\( Y \\\\), represent two measurement devices that capture the evolution of the unobserved latent process, \\\\( X \\\\). Generally, \\\\( S \\\\) is a noisy, low-cost device like an accelerometer, and \\\\( Y \\\\) is a precise, expensive labeling system like Motion Capture. The goal is to learn a model that allows us to estimate \\\\( Y \\\\) using data sequences from \\\\( S \\\\).\\n\\nThe CSP problem involves estimating the probability of the \\\\( t \\\\)-th output observation, \\\\( y_t \\\\), given the history of input observations, \\\\( s_1: t \\\\). For the experiments listed in this paper, we approximate this probability by a Gaussian with constant standard deviation, ie.\\\\( p(y_t | s_1, \\\\ldots s_t) = \\\\mathcal{N}(\\\\mu_{\\\\theta}(s_1:t), \\\\sigma^2 I) \\\\), where \\\\( \\\\sigma \\\\) is a constant, and parameterize \\\\( \\\\mu_{\\\\theta} \\\\) by a deep sequence model. Our goal is to find the maximum likelihood estimator for this distribution \u2013 arg min \\\\( \\\\theta P_t \\\\parallel y_t - \\\\mu_{\\\\theta}(s_1:t) \\\\parallel_2^2 \\\\). Therefore, our models are trained to minimize MSE loss over the length of the output sequence.\\n\\n3.2. Deep State Space Models\\n\\nDeep State Space Models (SSMs) build on simple state space models for sequence-to-sequence modeling. In its general form, a linear state space model may be written as,\\n\\n\\\\[\\n\\\\begin{align*}\\n    x'(t) &= A(t)x(t) + B(t)u(t) \\\\\\\\\\n    y(t) &= C(t)x(t) + D(t)u(t),\\n\\\\end{align*}\\n\\\\]\\n\\nmapping a 1-D input sequence \\\\( u(t) \\\\in \\\\mathbb{R} \\\\) to a 1-D output sequence \\\\( y(t) \\\\in \\\\mathbb{R} \\\\) through an implicit N-D latent state sequence \\\\( x(t) \\\\in \\\\mathbb{R}^n \\\\). Concretely, deep SSMs seek to use stacks of this simple model in a neural sequence modeling architecture, where the parameters, \\\\( A, B, C \\\\) and \\\\( D \\\\) for each layer can be learned via gradient descent.\\n\\nSSMs have been proven to handle long-range dependencies theoretically and empirically (Gu et al., 2021b) with linear scaling in sequence length, but were computationally prohibitive until Structured State Space Sequence Models (S4) (Gu et al., 2021a). S4 and related architectures by Fu et al. (2022); Smith et al. (2022); Poli et al. (2023) are based on a new parameterization that relies on time-invariance of the SSM parameters to enable efficient computation. Recently, Mamba (Gu & Dao, 2023) improved on S4-based architectures by relaxing the time-invariance constraint on SSM parameters, while maintaining computational efficiency. This allows Mamba to achieve high performance on a range of benchmarks from audio and genomics to language modeling, while maintaining linear scaling in sequence length. In this paper, we benchmark the performance of SSMs like S4 and Mamba on sensory CSP tasks, and show that they consistently outperform LSTMs and Transformers.\\n\\n4. CSP-Bench: A Continuous Sequence Prediction Benchmark\\n\\nWe address the scarcity of datasets with dense, continuous labels for sequence-to-sequence prediction by collecting three touch datasets with 1000 trajectories each and combining them with three IMU datasets from literature to create CSP-Bench. For each dataset, we design tasks to predict labeled sequences from single sensor data to avoid confounding factors. We also include data from varied sources like cameras and robot movements to facilitate future research in multi-sensor integration and multimodal learning. The detailed characteristics of these datasets are summarized in Table 1, aiming to support diverse sensory data analysis.\"}"}
{"id": "TK7xkOsXDu", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. CSP-Bench is comprised of six datasets. Three datasets \u2013 ReSkin Marker Writing, ReSkin Intrinsic Slip and XELA Joystick Control are tactile datasets collected in-house on two different robot setups as demonstrated above. Three other datasets \u2013 RoNIN (Herath et al., 2020), VECtor (Gao et al., 2022) and TotalCapture (Trumble et al., 2017) are curated open-source datasets.\\n\\n4.1. Touch Datasets\\nOur touch datasets are collected on two magnetic tactile sensor designs: ReSkin (Bhirangi et al., 2021) and Xela (Tomo et al., 2018). The ReSkin setup consists of a 6-DOF Kinova JACO Gen1 robot with a 1-DOF RG2 OnRobot gripper as shown in Figure 3. Both gripper surfaces are sensorized with a $32 \\\\times 30 \\\\times 2$ mm ReSkin sensor. Each sensor has five 3-axis magnetometers which measure changes in magnetic flux resulting from the deformation of the skin on the gripper surface. Appendix A contains more details on the fabrication and integration of ReSkin into the gripper.\\n\\nThe Xela setup consists of a 7-DOF Franka Emika robot fitted with a 16-DOF Allegro hand by Wonik Robotics. Each finger on the hand is sensorized with three 4x4 uSkin tactile sensors and one curved uSkin tactile sensor from XELA Robotics as shown in Figure 3. Sensor integration was provided by XELA robotics, which was designed specifically for the Allegro Hand. While the underlying sensory mode is the same for both ReSkin and Xela, they differ in spatial and temporal resolution, physical layout, and magnetic source.\\n\\n4.1.1. ReSkin: Marker Writing Dataset\\nWe collect 1000 Kinova robot trajectories of randomized linear strokes across a paper. Initially, the marker is arbitrarily placed between the gripper tips, and data collection begins when the marker touches the paper. The robot then moves linearly between 8-12 random points uniformly sampled within a 10cm x 10cm workspace, pausing for a randomly sampled delay of 1-4 seconds after each motion. Images of sample trajectories can be found in Appendix C.\\n\\nThe goal of this sequential prediction problem is to use tactile signal from the gripper to predict the velocity of the end-effector in the plane of the table. Velocity labels are easily obtained from robot kinematics, and serve as a proxy for the velocity of the marker strokes against the paper. What makes this problem challenging is that the sensor picks up contact information from both, the relative motion between the marker and the gripper, and the motion of the marker against the paper. The model must learn to disentangle these two motions to make accurate predictions.\\n\\n4.1.2. ReSkin: Intrinsic Slip Dataset\\nWe again use the Kinova setup to collect 1000 trajectories of intrinsic slip \u2013 the gripper grasping and slipping along different boxes clamped to a table. At the start of every episode, we close the gripper at a random location and orientation on the box and start recording data. We sample 8-12 random locations and orientations within the workspace of the robot along the length of the box, and then command the robot to move along the box while slipping against it. We use 10 boxes of different sizes to collect this dataset to improve data diversity in terms of contact dynamics. Example images and dimensions are available in Appendix C.1.2.\\n\\nThe goal of the sequential prediction problem is to use the sequence of tactile signals from the gripper tips to predict the translational and rotational velocity of the end-effector (again obtained from robot kinematics) in the plane of the robot's motion. In addition, the abrasive nature of the task causes the skin to wear out over time. To account for this wear, we change the gripper tips and skins after 25 trajectories on every box, improving data diversity as a result.\\n\\n4.1.3. Xela: Joystick Control Dataset\\nFor our final dataset, we record 1000 trajectories of data from the Allegro hand interacting with the joystick as shown in Figure 3. The hand/robot setup is teleoperated using a VR-based system derived from HoloDex (Arunachalam et al., 2023). Joystick interactions are logged synchronously.\"}"}
{"id": "TK7xkOsXDu", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset             | Modality | Model Inputs | Model Outputs | Size (dim) |\\n|---------------------|----------|--------------|---------------|------------|\\n| Marker Writing      | ReSkin   | End-effector velocity | 420          |\\n| Intrinsic Slip      | ReSkin   | End-effector velocity | 640          |\\n| Joystick Control    | Xela     | Joystick State | 580          |\\n| VECtor (Gao et al., 2022) | IMU      | User velocity | 22           |\\n| TotalCapture        | IMU      | Joint velocities | 45           |\\n| RoNIN (Herath et al., 2020) | IMU      | User velocity | 600          |\\n\\nSpecifically, this includes the full robot kinematics (7 DOF Arm at 50 Hz + 16 DOF Hand at 300 Hz), XELA tactile output (552 dim at 100 Hz), and 2 Realsense D435 cameras (1080p at 30 Hz). Each trajectory consists of 25-40 seconds of interaction with the joystick.\\n\\nThe goal of the sequential prediction problem is to use tactile signal from the Xela-sensorized robot hand to predict the state of the joystick, which is recorded synchronously with all the other modalities. The extra challenge with this dataset, in addition to the significantly higher dimensionality of the observation space, is the noisier trajectories resulting from human demos instead of a scripted policy.\\n\\n4.2. Curated Public Datasets\\n\\nIn addition to the tactile datasets we release with this paper, we also test our findings on data from other datasets, particularly ones using IMU sensor data (illustrated in Figure 3) \u2013 the RoNIN dataset (Herath et al., 2020) which contains smartphone IMU data from 100 human subjects with ground-truth 3D trajectories under natural human motions, the VECtor dataset (Gao et al., 2022) \u2013 a SLAM dataset collected across three different platforms, and the TotalCapture dataset \u2013 a 3D human pose estimation dataset.\\n\\n5. Hierarchical State-Space Models (HiSS)\\n\\nIn this work, we focus on continuous sequence-to-sequence prediction problems for sensors i.e. problems that involve mapping a sequence of sensory data to a sequence of outputs.\\n\\nIn the following sections, we describe our preprocessing pipeline and HiSS \u2013 our approach to sequence-to-sequence reasoning at different temporal scales.\\n\\n5.1. Data Preparation and Sampling\\n\\nEvery sensor in the real world operates at a different frequency, and data from different sensors is therefore collected at different nominal frequencies. Generally, our sensor sequences come from an inexpensive, noisy sensor operating at a higher frequency than an expensive, high precision device which gives us output sequences. To emulate this scenario and standardize our experiments, all sensor sequences are resampled at a frequency of 50Hz, and output sequences are resampled at 5Hz for all the datasets under consideration, unless specified otherwise. The specific choice of these frequencies is dictated by the sampling frequencies of sensors in the available data. Sequence length ranges (in number of sensor sequence timesteps) for each task are variable: 450-3000 for Marker Writing, 750-2150 for Intrinsic Slip, 750-4250 for Joystick Control, 12000 for the RoNIN, 1900-9100 for VectorEnv and 1700-5600 for TotalCapture.\\n\\nAll the sensors considered in CSP-Bench are prone to drift; therefore, in line with previous work (Bhirangi et al., 2021; Guzey et al., 2023b; Herath et al., 2020), we estimate a resting signal at the start of every sensor trajectory and deviations from this resting signal are passed to the model. Since sensor drift can be causally data-dependent, the entire sensory trajectory is passed to the model as input. Sensor and output sequences are normalized based on data statistics for their corresponding datasets, and details are listed.\"}"}
{"id": "TK7xkOsXDu", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"5.2. Model Architecture\\n\\nHere we describe Hierarchical State Space Models (HiSS) \u2013 a simple hierarchical architecture that uses SSMs to explicitly reason over sequential data at two temporal resolutions, as shown in Figure 4. The sensor sequence is first divided into a set of equally-sized chunks of size \\\\( k \\\\). Each chunk is passed through a shared SSM, say S4, which we refer to as the low-level SSM. The outputs of the low-level SSM corresponding to the \\\\( k \\\\)-th element of each chunk are then concatenated to form a rarified chunk feature sequence. Finally, this sequence is passed through a high-level sequence model to generate the output sequence.\\n\\nWhy should HiSS work? Sequential sensory data is subject to phenomena that occur at different natural frequencies. For instance, an IMU device mounted on a quadrotor is subject to high-frequency vibration noise and low-frequency drift characteristic of MEMS devices (Koksal et al., 2018). With HiSS, our goal is to create a neural architecture with explicit structure to operate at different temporal scales. This will allow the low-level model to learn effective, temporally local representations, while enabling the high-level model to focus on global predictions over a shorter sequence.\\n\\nComputational Complexity\\n\\nHiSS builds on top of models like S4 and Mamba which are linear in sequence length, \\\\( O(N) \\\\). For non-overlapping chunks of size \\\\( k \\\\) each, the low-level model operates on \\\\( N/k \\\\) chunks with each computation being \\\\( O(k) \\\\). The high-level model in turn operates on a sequence of length \\\\( N/k \\\\) resulting in a computation cost \\\\( O(N/k) \\\\). The net cost is therefore, \\\\( O(k \\\\times (N/k) + N/k) = O(N + N/k) = O(N) \\\\). For the case of overlapping chunks, in the most extreme case where we have an overlap of \\\\((k-1)\\\\) elements between chunks, we now have \\\\( N \\\\) chunks of size \\\\( k \\\\), each operated on by the low-level model. The high-level model operates on the resulting chunk feature sequence of length \\\\( N \\\\). Therefore the computational complexity in this case is \\\\( O(Nk + N) = O(Nk) \\\\), still significantly better than transformers which have a complexity of \\\\( O(N^2) \\\\).\\n\\n5.3. Training details\\n\\nWe focus on sequence-to-sequence prediction tasks. All our models are trained end-to-end to minimize MSE loss as explained in Section 3.1. For all tactile datasets and VEC-tor, we use an 80-20 train-validation split. For the RoNIN dataset, we use the first four minutes of every trajectory for our analysis, and use a validation set consisting of trajectories from unseen subjects. For TotalCapture, we use the train-validation split proposed by Trumble et al. (2017). Hyperparameter sweep ranges for each of our models and baselines, along with the resulting range of parameter counts are listed in Appendix B. We maintain similar ranges of parameter counts across models for the same task.\\n\\n6. Experiments and Results\\n\\nIn this section, we evaluate the performance of HiSS models on CSP tasks and understand their strengths and limitations. Unless otherwise specified, we use non-overlapping chunks of size 10, and aim to answer the following questions:\"}"}
{"id": "TK7xkOsXDu", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## HiSS: Hierarchical State Space Models\\n\\n### Table 2.\\n\\nComparison of MSE prediction losses for baseline and HiSS models on CSP-Bench. Reported numbers are averaged over 5 seeds for the best performing models. MW: Marker Writing, IS: Intrinsic Slip, R: RoNIN, V: VECtor, JC: Joystick Control, TC: TotalCapture\\n\\n| Model type       | Model Architecture | MW (cm/s) | IS (m/s) | JC (m/s) | R (m/s) | V (m/s) | TC (m/s) |\\n|------------------|--------------------|-----------|----------|----------|---------|---------|---------|\\n| Flat Transformer |                    | 2.3750    | 0.4600   | 1.0200   | -       | 0.0432  | -       |\\n| LSTM             |                    | 1.1685    | 0.3099   | 1.0740   | 0.0444  | 0.0353  | 0.1767  |\\n| S4               |                    | 1.3190    | 0.2617   | 0.9804   | 0.0382  | 0.0341  | 0.3483  |\\n| Mamba            |                    | 0.8830    | 0.1757   | 1.0640   | 0.0401  | 0.0319  | 0.3645  |\\n| MEGA             |                    | 0.8960    | 0.2105   | 0.9806   | 0.0370  | 0.0330  | 0.1944  |\\n| High-level       | Transformer        |          |          |          |         |         |         |\\n| Transformer      |                    | 0.6680    | 0.2192   | 0.9112   | 0.0620  | 0.0372  | 0.3048  |\\n| LSTM             |                    | 0.9958    | 0.2527   | 0.9350   | 0.0421  | 0.0377  | 0.3197  |\\n| S4               |                    | 0.6205    | 0.1574   | 0.8980   | 0.0363  | 0.0374  | 0.3583  |\\n| Mamba            |                    | 1.0268    | 0.2022   | 0.9060   | 0.0472  | 0.0372  | 0.4560  |\\n| MEGA-chunk       | S4                 |          |          |          |         |         |         |\\n| Transformer      |                    | 0.7620    | 0.9373   | 1.6090   | 0.3875  | 0.0302  | 0.2943  |\\n| LSTM             |                    | 0.8662    | 0.2837   | 1.0760   | 0.0436  | 0.0288  | 0.2522  |\\n| S4               |                    |          |          |          |         |         |         |\\n| Transformer      |                    | 0.7570    | 0.2898   | 0.9248   | 0.0439  | 0.0295  | 0.2452  |\\n| LSTM             |                    | 0.8590    | 0.1805   | 0.9520   | 0.0319  | 0.0293  | 0.2522  |\\n| S4               |                    |          |          |          |         |         |         |\\n| Transformer      |                    | 0.7020    | 0.3011   | 0.9553   | 0.0371  | 0.0293  | 0.2064  |\\n| LSTM             |                    | 0.7592    | 0.1746   | 0.9640   | 0.0346  | 0.0267  | 0.2428  |\\n| S4               |                    |          |          |          |         |         |         |\\n| Transformer      |                    | 0.5663    | 0.1316   | 0.9010   | 0.0302  | 0.0298  | 0.2527  |\\n| LSTM             |                    | 0.8257    | 0.1823   | 0.9200   | 0.0322  | 0.0294  | 0.4078  |\\n\\n**HiSS improvement over best Flat +35.87% +25.10% +8.10% +30.74% +21.30% -37.36%**\\n\\n\u2022 How do SSMs compare with LSTMs and Transformers on CSP-Bench?\\n\u2022 Can HiSS provide benefits over temporally flat models?\\n\u2022 How does chunk size affect the performance of HiSS?\\n\u2022 Is HiSS compatible with existing preprocessing techniques like filtering?\\n\u2022 How does HiSS perform in low-data regimes?\\n\\n### Baselines:\\n\\nWe use two categories of baselines: Flat and Hierarchical. Flat models consist of LSTMs, Causal Transformers, S4 and Mamba, in addition to MEGA (Ma et al., 2022). Hierarchical baselines include variations of HiSS models where the high-level and/or low-level SSMs are replaced by causal transformers and LSTMs, and MEGA-chunk (Ma et al., 2022), which is loosely classified as a high-level transformer with a low-level MEGA model. Table 2 presents a performance comparison on CSP-Bench for each of these baselines and proposed HiSS models.\\n\\n### 6.1. Performance of Flat models on CSP-Bench\\n\\nAt the outset, we see that SSMs \u2013 Mamba and S4, consistently outperform the best-performing Transformer and LSTM models by 10% and 14% median MSE respectively across CSP-Bench tasks. The only anomaly is the TotalCapture dataset where the LSTM outperforms all other models. We analyze this later in Section 6.7.\\n\\n### 6.2. Improving CSP Performance with HiSS\\n\\nHiSS models perform better than the best-performing flat models, SSM or otherwise, with a further improvement of $\\\\sim 23\\\\%$ median MSE across tasks. Among hierarchical models, HiSS models continue to do as well as or better than the others with a relative improvement of $\\\\sim 9.8\\\\%$ median MSE. Further, we make two key observations within models that...\"}"}
{"id": "TK7xkOsXDu", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3.\\n\\n| Method       | MW   | IS   | JC   | R    | V    | TC    |\\n|--------------|------|------|------|------|------|-------|\\n| Downsampled  | 2.41 | 0.33 | .957 | .116 | .039 | 0.34  |\\n| LSTM         | 1.92 | 0.27 | .975 | .094 | .034 | 0.20  |\\n| S4           | 2.22 | 0.29 | .974 | .081 | .036 | 0.31  |\\n| Mamba        | 1.96 | 0.26 | .980 | .077 | .033 | 0.25  |\\n| HiSS         | 0.57 | 0.13 | .901 | .027 | .025 | 0.26  |\\n\\nTable 4.\\n\\n| Chunk size | MW   | IS   | JC   | R    | V    | TC    |\\n|------------|------|------|------|------|------|-------|\\n| 5          | 1.18 | 0.20 | .933 | .046 | .033 | 0.32  |\\n| 10         | 0.57 | 0.13 | .901 | .027 | .025 | 0.25  |\\n| 15         | 0.54 | 0.12 | .899 | .035 | .026 | 0.24  |\\n\\nOur observations raised a natural question: What is happening under the hood? In the next four sections, we attempt to better understand the working of HiSS.\\n\\n6.3. Does HiSS Simply do Better Downsampling?\\nThe first question we seek to answer is whether simply down-sampling the sensor sequence to the same frequency as the output would do just as well as HiSS. As we see in Table 3, while some flat models with downsampled sensor sequences indeed improve performance over flat models in Table 2, they remain far behind HiSS models. This reinforces our hypothesis that HiSS models distill more information from the sensor sequence than naive downsampling.\\n\\nOne advantage of using hierarchical models is memory efficiency. They can significantly reduce computational load for models like transformers which scale quadratically in the length of the sequence. Using an SSM such as S4 or Mamba as the low-level model can significantly reduce the computational load $O(n^2) \\\\rightarrow O(n^2/k^2)$ for $k \\\\ll N$, where $n$ and $k$ are chunk size and sequence length respectively.\\n\\nTable 2 shows that such a model consistently improves performance over a flat causal Transformer across tasks.\\n\\n6.4. Effect of Chunk Size on Performance\\nHaving established the effectiveness of HiSS relative to conventional sequence modeling architectures, we seek to investigate the effect of a key parameter \u2013 the chunk size \u2013 on the performance of HiSS models. Downsampling the sensor sequences at the output frequency, as presented in Section 6.3 essentially corresponds to using a chunk size of 1. The rest of the analysis presented so far uses a chunk size of 10, corresponding to the largest non-overlapping chunks that cover the entire sensory sequence given sensor and output sequence frequencies of 50 Hz and 5 Hz respectively.\\n\\nIn this section, we conduct two additional experiments with chunk sizes of 5 and 15 and present the results in Table 4. We see that while performance improves drastically as the chunk size increases, it plateaus once the chunk size reaches the ratio of the sensor and output frequencies (10 in our case). This behavior can be explained by the fact that chunk sizes smaller than this ratio result in the model never seeing parts of the sensor sequence, while chunk sizes larger than this ratio result in an overlap between chunks.\\n\\n6.5. Effect of Sensory Preprocessing on Performance\\nA common approach to preprocessing noisy sensor data is to design low-pass filters to process the signal before it\u2019s passed through the model. To analyze the compatibility of HiSS models with such existing preprocessing techniques, we separately apply order 5 Butterworth filters with 3 different cut-off frequencies to the sensor sequence and report the model corresponding to the best cut-off frequency in Table 3. We make two key observations: (1) with the exception of the HiSS model for RoNIN, low pass filtering improves performance across the board; (2) HiSS models still perform comparably with or better than flat models.\\n\\nWith respect to (1), we see that the best-performing HiSS model from Table 2 continues to outperform the best flat model using filtered data, implying that the low-pass filter may have filtered useful information could have been used to improve task performance. This points to an important...\"}"}
