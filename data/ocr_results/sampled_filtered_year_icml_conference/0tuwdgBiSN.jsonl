{"id": "0tuwdgBiSN", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 11. Online Parity Learning: Upper: repeated experiments. Bottom: single experiment taken from the repeated experiments.\"}"}
{"id": "0tuwdgBiSN", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 12. Finite Parity Learning with 20000 Sampled Points. Upper: repeated experiments Bottom: single experiment\"}"}
{"id": "0tuwdgBiSN", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 13. Online Staircase: Upper: repeated experiments. Bottom: single experiment\\n\\nRefer to fig. 13 and fig. 14. In the case of the staircase task, the influence of simpler spurious features on convergence slowdown becomes more obvious. Different from the parity cases, the learning dynamics remain consistently stable across repeated runs in the staircase task.\\n\\nDomino\\n\\nSee fig. 15. We adhere to the convention of employing three image datasets as spurious features: MNIST-01, MNIST-79, and Fashion dress-coat, arranged in ascending order of difficulty with CIFAR-truck-automobile as the core feature (Izmailov et al., 2022; Kirichenko et al., 2023). It is noteworthy that the semi-real datasets including the domino datasets utilized in spurious correlation research are inherently noisy, meaning that the model cannot learn the core feature perfectly or achieve 0 generalization error, as highlighted in (Kirichenko et al., 2023). In fact, we see the core correlation of the model is well below 0.9. Furthermore, these datasets are limited in size, with only 10,000 images available for CIFAR-truck-automobile.\\n\\nPrevious studies have primarily focused on utilizing pretrained models to learn the spurious task. However, such an approach can obscure our understanding of feature learning dynamics, as pretrained models often achieve exceptionally high decoded core correlations from the outset, as noted in (Joshi et al., 2023). To better show this point, we present the learning dynamics for both pretrained and randomly initialized weights. We also found that pretrained models are more robust to spurious features at different complexities.\"}"}
{"id": "0tuwdgBiSN", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 14. Finite Staircase with 60,000 Sampled Points: $\\\\lambda = 0.90$.\\n\\nFigure 15. Learning dynamics of Domino dataset. The spurious dataset become harder from left to right. The subplot title shows the spurious task. mnist01: Classification task of handwritten digits images of 0 and 1 taken from the MNIST dataset. mnist79: Classification task of handwritten digits images of 7 and 9 taken from the MNIST dataset. fashion dress coat: Classification task of dress and coat images taken from the FashionMnist dataset. The core task is classification of Truck and AutoMobile images taken from the CiFar dataset. The plots shows similar to the staircase dataset, harder spurious feature has less influence of the end performance of the model. Note confounder strength $\\\\lambda = 0.95$ is fixed.\"}"}
{"id": "0tuwdgBiSN", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The impact of confounder strength on learning is more straightforward than the complexity. As confounder strength increases, the number of epochs needed for convergence also rises significantly. Notably, learning remains relatively insensitive to confounder strength until it reaches a threshold of 0.8, at which point we observe a notable increase in training epochs. The information of spurious feature, i.e., how well the spurious feature is learned and then memorized, depends heavily on the confounder strength.\\n\\nFor parity functions (fig. 16), we see when confounder strength surpasses 0.9, it converges much slower after the phase transition when compared to the experiment with lower $\\\\lambda$. The slower convergence reflects on learning under finite dataset where the end performance of the model is significantly impaired (fig. 17).\\n\\nAt higher confounder strength, the model has a higher correlation to the spurious, simpler staircase function at the early stage of learning. This implies the spurious staircase function is learned and memorized better by the model. We see higher $\\\\lambda$ causes harm to the end performance under finite dataset just as parity (fig. 18).\\n\\nThe pretrained model exhibits not only insensitivity to spurious features across a spectrum of complexities but also a remarkable resistance to higher $\\\\lambda$ values. Additionally, when compared to the initialization with random weights, models with pretrained weights consistently maintain low spurious correlations throughout the training process.\\n\\nRegarding the waterbirds dataset, it is noteworthy that initialization with random weights fails to learn the core feature entirely, as reported in (Kirichenko et al., 2023; Joshi et al., 2023).\\n\\nC.3. Core and Spurious subnetwork\\n\\nC.3.1. More neuron plots on a 2-layer NN\\n\\nRefer to fig. 21, fig. 22. We show the dynamics of a random batch of spurious neurons and core neurons for both the parity and staircase spurious learning task. It can be seen that spurious neurons have higher weights on spurious coordinates throughout training. And core neurons, which have significant weights on the core coordinates, are specifically the neurons which do not have a spurious weight spike at the start when the spurious feature is learned.\"}"}
{"id": "0tuwdgBiSN", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 17. Finite Parity with 40000 Sampled Points\\n\\nUpper: repeated experiments. Bottom: single experiment\\n\\nFigure 18. Staircase.\\n\\nUpper: Learning dynamics under sampling. Bottom: Learning dynamics under finite dataset.\"}"}
{"id": "0tuwdgBiSN", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\n(a) Domino-Image: Pretrained Weights\\n(b) Domino-Image: Random Weights\\n\\nFigure 19. Domino-Image. The plot shows pretrained model is more robust to the existence of a spurious feature across varying confounder strength.\\n\\nFigure 20. Learning Dynamics on Waterbirds\"}"}
{"id": "0tuwdgBiSN", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\n(a) Parity: Core Neurons at $\\\\lambda = 0.95$\\n\\n(b) Parity: Spurious Neurons at $\\\\lambda = 0.95$\\n\\n(c) Parity: All first layer neurons at $\\\\lambda = 0.95$\\n\\nFigure 21. Dynamics of neurons on Parity task.\"}"}
{"id": "0tuwdgBiSN", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 26. Training dynamics of different spurious datasets with default pretrained weights: Dataset specification is outlined in fig. 25.\\n\\nFigure 27. Training dynamics of different spurious datasets with random weights: Dataset specification is outlined in fig. 25.\"}"}
{"id": "0tuwdgBiSN", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 28. Core/spurious correlation and decoded correlation dynamics of the designed Hard Domino Dataset. Clean: $\\\\lambda = 0.5$; Spurious: $\\\\lambda = 0.95$. Even when the spurious feature in the domino task is challenging, it significantly influences the learning of the core feature. Thus justify its appropriateness as a benchmark that can be considered by future debiasing methods. The core correlation achieved by the model in the clean case represents the maximum possible correlation that any debiasing algorithm could achieve.\\n\\nFigure 29. A sample batch of images from the constructed Hard Domino dataset.\"}"}
{"id": "0tuwdgBiSN", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\n| Dataset/Metric | clean original CE KL JTT SPARE clean LLR |\\n|---------------|------------------------------------------|\\n| Hard Staircase| 0.76 0.56 -0.31 0.28 0.28 0.15 0.62       |\\n| Hard Domino   | 0.70 0.375 -0.22 0.0 0.15 0.02 0.55        |\\n\\nTable 6. End Performance of Different Debiasing methods on designed experiments:\\n- **clean**: Trained from scratch with $\\\\lambda = 0.5$, which can be seen as an optimal case.\\n- **original**: Before Retraining with $\\\\lambda = 0.95$ for Hard Domino task and $\\\\lambda = 0.9$ for staircase task.\\n- **clean LLR**: Last Layer Retraining with $\\\\lambda = 0.5$ dataset. The value here denotes core correlation after retraining. It is observed that previous debiasing algorithms either failed on the designed spurious task or cause failure to core feature learning.\\n\\nD.2. Discussion on spurious real dataset\\n\\nReal-world datasets typically used to study spurious correlations, such as MultiNLI (Williams et al., 2018) and CivilComment (Duchene et al., 2023), often do not meet the properties of the boolean spurious datasets where core feature and spurious feature are disentangled and realizable. In these datasets, spurious attributes are intricately intertwined at the word level, such that the removal of a negative word can significantly alter a sentence's semantic meaning. Furthermore, it has been reported that the labels in these datasets are not entirely clear-cut and may be inherently ambiguous, posing challenges for semantic labeling even for human annotators, thereby violating the realizability condition. We observe that the training dynamics in such real-world datasets are diversified and not fully understood, as indicated by (Izmailov et al., 2022; Joshi et al., 2023) (refer to fig. 26, fig. 27). It lacks justifications whether these datasets are suitable to be used in studying spurious correlation.\\n\\nAs previous studies (LaBonte et al., 2023; Idrissi et al., 2022) have repeatedly shown, class-balanced training achieves comparable performance to other, more sophisticated debiasing algorithms, or even group-balanced training such as DRO (Sagawa et al., 2020a) on real dataset. The analysis of Jaccard and containment scores reveals that the debiasing methods tested exhibit poor performance on all the tested real spurious dataset, casting doubt on their effectiveness for enhancing the second model. Therefore, the potential of these methods to improve core feature learning is questionable. The pervasive use of pretrained models further complicates the evaluation of debiasing algorithms. A deeper understanding of the complex nature of real-world data, an area that remains largely unexplored, is crucial for a comprehensive understanding of the training dynamics in these scenarios.\"}"}
{"id": "0tuwdgBiSN", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\n(a) Staircase: Core Neurons at $\\\\lambda = 0.90$\\n\\n(b) Staircase: Spurious Neurons at $\\\\lambda = 0.90$\\n\\n(c) Staircase: All neurons at $\\\\lambda = 0.90$\\n\\nFigure 22. Dynamics of neurons on Staircase task.\"}"}
{"id": "0tuwdgBiSN", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 23. Dynamics of Multi-layer neurons on Staircase task. The plot shows the neurons in third layers are also separated into core and spurious neurons. The color of each line corresponds to the legend shown in fig. 3 with core coordinates being the core neurons identified in the previous layer and the same for spurious neurons.\\n\\nC.3.2. Neurons Plots on a Multi-Layer NN\\nWe show a Multilayer Perceptron (MLP) may also separate into two subnetworks. We take a 4-layer MLP trained on a staircase boolean task with $\\\\deg(f_s) = 10$, $\\\\deg(f_c) = 14$, $\\\\lambda = 0.9$ as an example. At each layer, starting from the bottom layer to the top layer, we recursively categorized neurons into spurious and core neurons. We observed that core neurons in the next hidden layer primarily focus on core neurons in the current hidden layer, and the same applies to spurious neurons. See fig. 23 for a plot on a random batch of neurons in the third layer. This discovery implies that the theory we derived in the main paper can also be extended to MLP architectures.\\n\\nC.3.3. Spurious/Core Neurons on Vision Datasets\\nWe show the finding here that neural networks trained on vision datasets are also separated/distangled into a spurious sub-network and a core sub-network. We retrain the model either on the core or spurious feature and record the retrained correlation score. We further zero out the weights on intersected neurons which is higher than a threshold value. The result is shown in table 3, table 4.\\n\\n| Dataset | Core Neurons | Spurious Neurons | Intersected Neurons Before Retrain |\\n|---------|--------------|------------------|-----------------------------------|\\n| Waterbirds | 135          | 72               | 1622                              |\\n| Domino  | 86           | 81               | 74                                |\\n| CelebA  | 149          | 134              | 1449                              |\\n\\nTable 3. Number of different types of neurons. The table shows the number of each type of neurons before and after retraining on core, spurious feature. Before Retrain: Number of activated neurons before retrain. Threshold are 0.01, 0.05, 0.01 for waterbirds, Domino and CelebA respectively.\\n\\n| Dataset | Core | Spurious | Core w.o Spurious | Spurious w.o Core |\\n|---------|------|----------|-------------------|-------------------|\\n| Waterbirds | 0.834 | 0.826    | 0.779             | 0.816             |\\n| Domino  | 0.804 | 1        | 0.8                | 1                 |\\n| CelebA  | 0.814 | 0.578    | 0.792              | 0.542             |\\n\\nTable 4. Performance of the Retrained model using neurons in table 3. Core/Spurious: Retrained the model on a group balanced dataset to predict core/spurious feature. Core w.o Spurious/Spurious w.o Core: Performance of the model after removing intersected neuron weights.\"}"}
{"id": "0tuwdgBiSN", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 24. Hard Staircase: Core Correlation After Retrained On Dataset with Different $\\\\lambda$. We see the result follows the same trend as observed in (LaBonte et al., 2023) and is correctly predicted by our theory.\\n\\nC.3.4. LAST LAYER RETRAINING REDUCES SPURIOUS SUB-NETWORK WEIGHTS\\n\\nSee table 5. We compare the last layer weights ratio between core and spurious sub-network after Lat Layer Retraining either with spurious dataset and balanced dataset. We see after retraining, the ratio increase significantly in both cases, which has also been observed in (LaBonte et al., 2023).\\n\\n| Parity Staircase | Core | Spurious |\\n|------------------|------|----------|\\n| Core Spurious Ratio Before Retrain | 1.53 | 0.97 | 1.58 | 1.38 |\\n| Core Spurious Ratio Retrain Spurious | 0.57 | 0.16 | 3.56 | 0.17 |\\n| Core Spurious Ratio Retrain Clean | 0.57 | 0.01 | 57.00 | 0.17 |\\n\\nTable 5. The table shows the mean weights of core and spurious neurons before and after retraining with either the original spurious dataset $D_{\\\\lambda}$ or group balanced dataset $D_{\\\\lambda}=0$.\\n\\nD. Discussion\\n\\nD.1. Limitations of Existing Debiasing Algorithms\\n\\nIn instances where a spurious attribute is absent, numerous debiasing algorithms (Liu et al., 2021; 2023; Utama et al., 2020; Nam et al., 2020; Yaghoobzadeh et al., 2021) typically follow a two-stage methodology. The first stage involves training a conventional model using Early Stop Empirical Risk Minimization (ERM). These algorithms diverge in the second stage, where each implements a distinct heuristic to distinguish and separate data from minority groups. This separation is based on the initial model, which is then utilized to either upweight or upsample these data points in the next stage. We find these methods have several inherent limitations, evident even in our toy settings: (1) identifying the right time for early stopping, (2) assessing whether the first model sufficiently identifies minority group data points, (3) determining the quantity of data points to be selected, (4) establishing the appropriate degree of upweighting for the selected points. It is also unclear whether the first model provide enough information to separate data points in the first place. If the algorithm aims to accurately identify data points from a minority group, then we can use the Jaccard score and Containment score to evaluate their performance.\\n\\nThese methods implicitly assume a distinct separation in the learning phases of spurious and core features, often influenced...\"}"}
{"id": "0tuwdgBiSN", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 25. Performance of different debiasing methods on widely-used spurious datasets. Staircase: $\\\\lambda = 0.9$, $\\\\deg(f_s) = 10$, $\\\\deg(f_c) = 14$, Dataset Size 60000; Domino: $\\\\lambda = 0.95$, the core feature is Cat-Dog and the spurious feature is truck-automobile from CI-FAR(Krizhevsky) dataset (see fig. 29 for an sample batch of images from this dataset). We additional report that after upsampling the inferred group (performing the second training using the best group inference result among all early stopped models), the worst group accuracy for the hard staircase and hard domino dataset are 0.42 and 0.53 respectively; Parity: $\\\\lambda = 0.9$, $\\\\deg(f_s) = 10$, $\\\\deg(f_c) = 4$; CMNIST, CelebA, CivilComments, MultiNLI: $\\\\lambda = 0.9$; WaterBirds: $\\\\lambda = 0.95$.\\n\\nby the simplicity of the spurious feature and the strength of confounders. This is particularly apparent in JTT, which upweights all misclassified points in the second stage, implicitly assuming a specific temporal point where the model correlates more with spurious features than core features. This assumption holds true in cases where the spurious feature is trivial to learn compared to the core feature. such as with parity cases and popular spurious datasets like the image Domino dataset (Shah et al., 2020), Waterbirds (Sagawa et al., 2020a), and Color-MNIST (Arjovsky et al., 2020). However, our findings suggest that this demarcation can remain unclear throughout training, particularly with more challenging spurious features and limited datasets, as demonstrated by the limited hard staircase dataset and the hard domino dataset. As a result, these debiasing algorithms struggle to accurately distinguish minority groups from others, leading to unwanted bias in the model, as evidenced by low Jaccard scores and containment score (refer to the right two plots in Figure fig. 6).\\n\\nUnlike clustering methods such as JTT and SPARE, (LaBonte et al., 2023) proposes a more generalized approach. Instead of segregating points into groups for upweighting based on the inferred group's size, this method selects points using the initial model, focusing on those with the highest cross-entropy or KL divergence loss from a subsequently trained model. Assuming we can accurately determine the timing for early stopping, the challenge then becomes deciding on the number of points to select. We observe that minority group samples tend to rank higher in terms of loss, as indicated by a high containment score relative to the number of selected points. However, identifying the optimal number of points without explicit knowledge of the spurious attribute can be challenging, limiting the practicality of the algorithm. Detailed statistics for four methods\u2014JTT, Spare, Highest CE loss, Highest Disagreement Score\u2014over six popular spurious datasets are provided in the appendix, showcasing the accuracy of these methods in identifying minority groups. For the assessment of the ranking methods, we select the number of points to be equivalent to the total number of minority points present in the spurious dataset.\\n\\nNote despite not directly using a spurious attribute in training, previous algorithms often presume the availability of a validation dataset for hyperparameter tuning, which is impractical. Consequently, the reported performance typically reflects models tuned with optimal hyperparameters.\\n\\nOur experiments employed the SpuCo library (Joshi et al., 2023). At each epoch, we paused the initial model's training to perform group inference, following which we calculated the Jaccard and containment score to measure the accuracy of the inferred minority group against the actual minority group.\"}"}
{"id": "0tuwdgBiSN", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Background\\n\\nA.1. Boolean Function Analysis Background\\n\\nSee (O'Donnell, 2021) for a comprehensive review for boolean function analysis. We only include the most important tools here.\\n\\n**Theorem A.1.** Any boolean function $f: \\\\{\\\\pm 1\\\\}^n \\\\rightarrow \\\\mathbb{R}$ can be decomposed into an orthogonal basis $f(x) = \\\\sum_{S \\\\subseteq [n]} b_S f_S(x) \\\\chi_S(x)$ where $\\\\chi_S(x) = \\\\prod_{i \\\\in S} x_i$ and note we have $E[\\\\chi_S(x) \\\\chi_S'(x)] = 0$ for all $S' \\\\neq S$. Thus we have $b_S = E[f(x) \\\\chi_S(x)]$.\\n\\nA.2. Properties of Threshold Staircase Function\\n\\n**Lemma 5.** For every term $S$ inside $scd(x)$, with $d \\\\geq 2$, we have\\n\\n$$scd(S) = \\\\begin{cases} \\\\frac{(d-1) d - 1}{2 d - 1} & \\\\text{if } d \\\\text{ even} \\\\\\\\ \\\\frac{(d-2) d - 1}{2 d - 2} & \\\\text{if } d \\\\text{ odd} \\\\end{cases}$$\\n\\n**Proof.** We outline the proof of staircase function here. Note for each of the term in $x_1 + x_1 x_2 + x_1 x_2 x_3$, they are independently distributed. Thus for terms in the staircase the fourier coefficent of them for the threshold staircase is the same as the majority function for $\\\\text{Maj}_d(k)$ where $k = 1$. Further, we have $P_{s \\\\in S} scd(s) \\\\rightarrow 2\\\\pi$ as $d \\\\rightarrow \\\\infty$ (O'Donnell, 2021) (ch5.3).\\n\\nA.3. Properties of Boolean spurious distribution\\n\\n**Lemma A.1.** Given a spurious boolean distribution defined previously, if either of the feature is unbiased. Then the distribution is equivalent to a mixture of $2(1 - \\\\lambda) D_{unif} + (2\\\\lambda - 1) D_{same}$. \\n\\n**Proof.** This is equivalent to saying that given any boolean vector $x$, $P_{D}(x) = 2(1 - \\\\lambda) P_{unif}(x) + (1 - 2\\\\lambda) P_{same}(x)$. Note that $P_{D}(x) = \\\\lambda P_{same}(x) + (1 - \\\\lambda) P_{diff}(x) = (2\\\\lambda - 1) P_{same}(x) + (1 - \\\\lambda)(P_{diff}(x) + P_{same}(x))$. Thus we only need to argue that $P_{diff}(x) + P_{same}(x) = 2 P_{unif}(x)$ forms a uniform distribution. And we have $P_{diff}(x) + P_{same}(x) = P_{x \\\\sim U}(x, f_s(x) = f_c(x)) P(f_s(x) = f_c(x)) + P_{x \\\\sim U}(x, f_s(x) \\\\neq f_c(x)) P_{x \\\\sim U}(x, f_s(x) \\\\neq f_c(x)) = I[f_s(x) = f_c(x)] P_{x \\\\sim U}(x) + I[f_s(x) \\\\neq f_c(x)] P_{x \\\\sim U}(x) = I[f_s(x) = f_c(x)] P_{x \\\\sim U}(x) + I[f_s(x) \\\\neq f_c(x)] P_{x \\\\sim U}(x) = 2 P_{x \\\\sim U}(x)$. \\n\\n14\"}"}
{"id": "0tuwdgBiSN", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nLemma A.2. When $f_{c}(x_{c})$ is unbiased, $x_{s}$ is marginally uniformly distributed on $D_{\\\\text{same}}$, $D_{\\\\text{diff}}$, $D_{\\\\lambda}$. And when $f_{s}(x_{s})$ is unbiased $x_{c}$ is marginally uniformly distributed on $D_{\\\\text{same}}$, $D_{\\\\text{diff}}$, $D_{\\\\lambda}$.\\n\\nProof. Consider $P_{x_{s} \\\\sim U(X_{s} \\\\mid f_{s}(X_{s}) = f_{c}(X_{c}))} = P_{X_{s} = x_{s}} P_{f_{c}(X_{c}) = f_{s}(x_{s})} P_{f_{c}(X_{c}) = -1} P_{f_{s}(x_{s}) = -1} + P_{f_{c}(X_{c}) = 1} P_{f_{s}(x_{s}) = 1} = P_{U(X_{s} = x_{s})}$. The equality also holds if we take $f_{s}(x_{s}) = 1$ or $D_{\\\\text{diff}}$. Which would imply this is also true for $D_{\\\\lambda}$. Thus we complete the proof.\\n\\nLemma A.3. For spurious function $f_{s}$ and core function $f_{c}$ satisfying lemma A.1, we have $E_{D_{\\\\lambda}}[\\\\chi_{S}(x_{s})y] = b_{f_{c}(S)} + (2\\\\lambda - 1)b_{f_{s}(S)}$.\\n\\nProof. We have, $E_{D_{\\\\lambda}}[\\\\chi_{S}(x_{s})y] = 2(1 - \\\\lambda) E_{D_{\\\\text{unif}}}[\\\\chi_{S}(x_{s})f_{c}(x_{c})] + (2\\\\lambda - 1) E_{D_{\\\\text{unif}}}[\\\\chi_{S}(x_{s})f_{c}(x_{c}) \\\\mid f_{c}(x_{c}) = f_{s}(x_{s})] = 2(1 - \\\\lambda) b_{f_{c}(S)} + (2\\\\lambda - 1) E_{D_{\\\\text{unif}}}[\\\\chi_{S}(x_{s})f_{c}(x_{c})] \\\\cdot \\\\frac{1}{2} = 2(1 - \\\\lambda) b_{f_{c}(S)} + (2\\\\lambda - 1) b_{f_{c}(S)} + b_{f_{s}(S)}$.\\n\\nFrom the above, we have $E_{D_{\\\\lambda}}[\\\\chi_{S}(x_{s})y] = \\\\begin{cases} b_{f_{c}(S)} & \\\\text{if } S \\\\subseteq [c] \\\\\\\\ (2\\\\lambda - 1)b_{f_{s}(S)} & \\\\text{if } S \\\\subseteq [s] \\\\\\\\ 0 & \\\\text{otherwise.} \\\\end{cases}$\\n\\nB. Theory\\n\\nB.1. Calculation of Gradient Gaps and Revised Proof for Layer Wise Training for Parity Function\\n\\nB.1.1. Setting\\n\\nRecall the definition of the boolean task we defined in the draft. We define 1. $x$ the concatenation of three vectors $x_{s}$, $x_{c}$, $x_{u}$. The length of $x_{s}$ is $s$ and the length of $x_{c}$ is $c$ with $s \\\\ll c$. $x_{u}$ here denote a length $u$ random boolean vector which does not have any correlation with the label. Thus the length of $x$ is $n = s + c + u$. Without loss of generality, we additionally require $c$, $s$ to be even length and $u$ to be odd length. 2. $f_{s}$ and $f_{c}$ are parity functions defined as $\\\\chi_{S}(x_{s}) = \\\\sum_{i \\\\in [s]} x_{i}$ and $\\\\chi_{C}(x_{c}) = \\\\sum_{i \\\\in [c]}$. So $\\\\deg(f_{s}) = s$, $\\\\deg(f_{c}) = c$. 3. We then form a spurious distribution as defined in the main paper with confounder strength $\\\\lambda$.\"}"}
{"id": "0tuwdgBiSN", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.1.3. INITIALIZATION\\n\\nLet us consider the following initialization scheme as in (Barak et al., 2023)\\n\\n1. For all \\\\(1 \\\\leq i \\\\leq r/2\\\\), randomly initialize \\\\(w(0)_i \\\\sim \\\\text{Unif}(+1, -1)\\\\), \\\\(a(0)_i \\\\sim \\\\text{Unif}(+1, -1)\\\\), \\\\(b(0)_i \\\\sim \\\\text{Unif}(-1 + 1/c, -1 + 2/c, \\\\ldots, 1 - 1/c)\\\\).\\n\\n2. For all \\\\(r/2 < i \\\\leq r\\\\), initialize \\\\(w(0)_i = w(0)_{i - r/2}\\\\), \\\\(a(0)_i = -a(0)_{i - r/2}\\\\), \\\\(b(0)_i = b(0)_{i - r/2}\\\\).\\n\\nKey properties of this initialization scheme are: (1) It is unbiased, since the model output is 0 on all inputs at initialization, and (2) Biases \\\\(b\\\\) are set such that they enable computing parity linearly once we have the correct coordinates identified. For the informal lemma in the main paper, we assume \\\\(w_i\\\\) is the all 1s vector.\\n\\nWe will first analyze the gradients at initialization, then after spurious feature is learned.\\n\\nB.1.4. POPULATION GRADIENT AT INITIALIZATION\\n\\nNotice that our initialization makes the model output 0 on all \\\\(x\\\\). Thus \\\\(l(y, b_y) = 0\\\\) and then we have \\\\(\\\\nabla b_y l'(y, b_y) = -y\\\\).\\n\\nWe can now formulate the population gradient at initialization. Without loss of generality, we will assume \\\\(\\\\lambda > 0\\\\), then \\\\(D_\\\\lambda\\\\) is a mixture such that w.p \\\\(2(1 - \\\\lambda)\\\\) we draw a sample \\\\(x\\\\) from the uniform distribution \\\\(\\\\text{Unif}\\\\left\\\\{\\\\{+1, -1\\\\}\\\\right\\\\}_n\\\\). And with \\\\(2\\\\lambda - 1\\\\), we draw a sample from \\\\(D\\\\) where we first draw \\\\(x_c \\\\sim \\\\text{Unif}\\\\left\\\\{\\\\{+1, -1\\\\}\\\\right\\\\}_c\\\\) and then draw a \\\\(x_s \\\\sim \\\\text{Unif}\\\\{x_s|\\\\chi_S(x_s) = \\\\chi_C(x_c)\\\\}\\\\).\\n\\nThen the population gradient for weight \\\\(w_{i,j}\\\\) is \\\\(E_{D_\\\\lambda}[\\\\nabla w_{i,j} l(f(x; \\\\theta_0), y)] = E_{D_\\\\lambda}[-y \\\\nabla w_{i,j} f(x; \\\\theta_0)] = E_{D_\\\\lambda}[-ya_{i,1}\\\\{w^\\\\top x + b_i > 0\\\\}x_j] = 2(1 - \\\\lambda)E_{\\\\text{Unif}}[-ya_{i,1}\\\\{w^\\\\top x + b_i > 0\\\\}x_j] + (2\\\\lambda - 1)E_{D}\\\\left[-ya_{i,1}\\\\{w^\\\\top x + b_i > 0\\\\}x_j\\\\right]\\) (1)\\n\\nWe will study the two terms separately.\\n\\nPopulation Gradient on uniform distribution.\\n\\nSet \\\\(g_{i,j} = E_{\\\\text{Unif}}[-ya_{i,1}\\\\{w^\\\\top x + b_i > 0\\\\}x_j]\\\\). As long as \\\\(w_i \\\\in \\\\{-1, 1\\\\}_n\\\\), from (Barak et al., 2023), we have\\n\\n\\\\[\\n\\\\begin{align*}\\n1. & \\\\quad j \\\\in [c]: g_{i,j} = -\\\\frac{1}{2}a_i \\\\xi_c - 1 \\\\cdot \\\\chi_c\\\\{j\\\\}(w_i) \\\\\\\\\\n2. & \\\\quad j \\\\in [s] \\\\cup [u]: g_{i,j} = -\\\\frac{1}{2}a_i \\\\xi_c + 1 \\\\cdot \\\\chi_c\\\\{j\\\\}(w_i)\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\(\\\\xi_k = d\\\\text{Maj}(S)\\\\) with \\\\(|S| = k\\\\). Thus we have for the first term \\\\(g_{u,i,j} = 2(1 - \\\\lambda)E_{\\\\text{Unif}}[-ya_{i,1}\\\\{w^\\\\top x + b_i > 0\\\\}x_j]\\\\).\"}"}
{"id": "0tuwdgBiSN", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We can study the Fourier spectrum of each of the terms in 3 to construct the Fourier spectrum of $q$. We will first ignore the $yx$. This gives us $|\\\\{0\\\\}$. By the orthogonality and linearity of the Fourier basis, we thus have $|\\\\{1\\\\}$.\\n\\nBy the initialization scheme, we have $w$. Thus we have $|\\\\{0\\\\}$. Observe that $\\\\chi_i > 0$. Now let us put it back in 2.}\\n\\n$|\\\\{1\\\\}$.\\n\\nNote for the second term of 1, $\\\\chi_i > 0$. Thus $\\\\chi_i = 1$.\\n\\n$|\\\\{0\\\\}$. Complex.\\n\\n$|\\\\{0\\\\}$.\"}"}
{"id": "0tuwdgBiSN", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nAbstract\\nExisting research often posits spurious features as easier to learn than core features in neural network optimization, but the impact of their relative simplicity remains underexplored. Moreover, studies mainly focus on end performance rather than the learning dynamics of feature learning. In this paper, we propose a theoretical framework and an associated synthetic dataset grounded in boolean function analysis. This setup allows for fine-grained control over the relative complexity (compared to core features) and correlation strength (with respect to the label) of spurious features to study the dynamics of feature learning under spurious correlations. Our findings uncover several interesting phenomena: (1) stronger spurious correlations or simpler spurious features slow down the learning rate of the core features, (2) two distinct subnetworks are formed to learn core and spurious features separately, (3) learning phases of spurious and core features are not always separable, (4) spurious features are not forgotten even after core features are fully learned. We demonstrate that our findings justify the success of retraining the last layer to remove spurious correlation and also identifies limitations of popular debiasing algorithms that exploit early learning of spurious features. We support our empirical findings with theoretical analyses for the case of learning XOR features with a one-hidden-layer ReLU network.\\n\\n1. Introduction\\nThere is increasing evidence (Geirhos et al., 2020; Zhou et al., 2021; Geirhos et al., 2022; Xiao et al., 2020; McCoy et al., 2019) indicating that neural networks inherently tend to learn spurious features in classification tasks. These features, while correlated with the data label, are non-causal and lead to enhanced training and in-distribution performance. However, this inherent tendency overlooks core or invariant features that are crucial for robustness against distribution shifts. This phenomenon is attributed to the relative simplicity of spurious features compared to core features, reflecting a simplicity bias in neural network training (Geirhos et al., 2020; Shah et al., 2020; Rahaman et al., 2019; Nakkiran et al., 2019; Xue et al., 2023), where networks inherently prefer simpler features over more complex, yet essential ones. Interestingly, recent empirical work (Kirichenko et al., 2023; Izmailov et al., 2022) has shown that despite this bias and the compromised predictive performance, standard neural network training does in fact learn the harder core features in its representation, as long as the spurious correlation is not perfect. However, a fine-grained understanding of the impact of \u201csimplicity\u201d of the spurious features on the learning of the robust features has remained unexplored. Moreover, a precise definition of simplicity that accounts for computational aspects of learning is lacking. In our work, we characterize the impact of the relative complexity of spurious features and their correlation strength with the true label on the dynamics of core feature learning in neural networks trained with (stochastic) gradient descent. To ground our exploration, we introduce a versatile framework and corresponding synthetic datasets based on the rich theory of boolean functions (see appendix A.1 for a quick review). We quantify simplicity/complexity using the computational time/pattern of learning the different features (represented as boolean functions) by gradient-based training, and subsequently study the dynamics of gradient-based learning on these datasets. We focus on two types of boolean functions: parity and staircase functions (Abbe et al., 2022). Our key findings are summarized below:\\n\\n- Easier spurious features lead to slower core feature emergence. We find that the presence of spurious features notably harm the convergence rate of core feature learning when infinite data is available. This is particularly evident in scenarios where two parity functions of differing degrees are being learned; we give a concrete formula that quantify the initial gradient gap between them. Notably, we found even spurious features are not forgotten even after core features are fully learned.\"}"}
{"id": "0tuwdgBiSN", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nfeatures that are of similar or slightly lower complexity than the core feature can substantially slow down the convergence rate. This delay in convergence manifests as poor robustness of the model when data availability is limited.\\n\\nThe common assumption that the learning phase is separated into spurious feature learning and then core feature learning can lead to complete failure of various debiasing algorithms (Liu et al., 2021; 2023; Utama et al., 2020; Nam et al., 2020; Yaghoobzadeh et al., 2021) heavily depend on early learning of shortcut features and make an implicit assumption of a clear-cut separation between the learning phases of core and spurious features. We show that this assumption is generally incorrect and provide an insightful counterexample to demonstrate how such algorithms can fail completely. Staircase functions\u2014a category of functions characterized by their hierarchical structure and learning curves similar to those in real datasets\u2014illustrate that both core and spurious features are learned concurrently. The degree to which core and spurious features are learned is influenced by their relative complexity and correlation strength. This observation challenges the effectiveness of widely adopted machine learning algorithms.\\n\\nSpurious features are retained. We observe that networks retain spurious features in their representations, even after the core feature has been learned sufficiently well. This retention is particularly notable for spurious features with lower complexity compared to the core features. Not only do these spurious features persist in the network's representation, but their corresponding weights in the last layer also remain stable, especially under high confounder strength. We provide theoretical explanation for such phenomenon and show how well the spurious feature is being memorized is closely related to the correlation strength.\\n\\nThe network is separated into two distinct subnetworks in learning different features, and Last Layer Retraining (LLR) decreases reliance on the spurious subnetwork: (Kirichenko et al., 2023; Izmailov et al., 2022) show LLR with balanced dataset is able to improve robustness of the model. While it is clear from the previous works that the core feature can be linearly decoded from the last layer, the mechanism behind this remains elusive. We demonstrate across numerous datasets that this improvement primarily stems from a reduction in the weights of the last layer that are connected to the spurious subnetwork. This observation is based on the finding that spurious and core representations are disentangled in the last layer.\\n\\nWe use semi-synthetic and real world datasets to validate the above findings and also provide theoretical justifications for these observations using our boolean spurious feature setting.\\n\\n1.1. Related Work\\n\\nDatasets for Studying Spurious Correlations. Numerous datasets have been employed to study learning under spurious correlation. These include synthetic datasets such as WaterBirds (Sagawa et al., 2020a), Domino Image dataset (Shah et al., 2020), Color-MNIST (Zhang et al., 2022), and a series of datasets proposed in (Hermann & Lampinen, 2020). It's important to note that these datasets are constructed in an ad-hoc manner, making it challenging to justify the complexity of the spurious features. Real datasets known to contain spurious correlations, such as CivilComments (Duchene et al., 2023), MultiNLI (Williams et al., 2018), CelebA (Liu et al., 2015), and CXR (Kermany et al., 2018), are also used to evaluate algorithms designed to mitigate shortcut features. A recent work (Joshi et al., 2023) points out several problems of existing datasets that has been used to study spurious correlation and evaluating algorithm performances. Our observation provide further support for their claims (see appendix C.2.2).\\n\\nMitigating Spurious Correlations. Learning under spurious correlation can be interpreted as an Out-Of-Distribution (OOD) or group imbalance task, as spurious features divide the dataset into imbalanced groups. Two cases arise: (1) when the spurious attribute is given, popular methods like (Sagawa et al., 2020a; Idrissi et al., 2022) can be applied, (2) when the spurious label is unknown during training, various algorithms have been proposed to exploit the phenomenon of simplicity bias (Valle-P\u00e9rez et al., 2019; Shah et al., 2020; Nakkiran et al., 2019), which posits that spurious features are learned by the model in the early stages of learning, to upweight underrepresented groups. A representative method of this type is the \u201cJust Train Twice Algorithm\u201d (Liu et al., 2021), where a model is first trained to upweight \u201ceasy\u201d samples. It is worth noting that almost all algorithms assume a balanced validation dataset for extensive hyperparameter tuning, as observed in (Izmailov et al., 2022). Another line of work focuses on underspecified tasks where the spurious features are fully correlated with the label (Teney et al., 2022; Lee et al., 2023).\\n\\nLast Layer Retraining. A key line of work related to our research is (Kirichenko et al., 2023; Izmailov et al., 2022), where it is demonstrated that last layer retraining on a biased model with balanced data is enough for achieving state-of-art result on many benchmark datasets. (LaBonte et al., 2023) further shows this is even true for some benchmark dataset when the spurious data is used. The method essentially runs by first finetune the model then apply logistic regression on...\"}"}
{"id": "0tuwdgBiSN", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use $n$ with smaller complexity. Formally, consider two boolean functions $f$ and $g$. To rigorously examine the learning mechanisms of neural networks in the presence of spurious correlations, we propose a dataset that encapsulates features via Boolean functions.\\n\\n2. Boolean Spurious Features Dataset\\n\\nWe show that threshold staircase functions have the form of leap 1 degree curves that look like a staircase where the features are learned one by one. The second criterion is identical invariant distribution. These are referred to as staircase functions since the training curves look like a staircase where the features are learned one by one.\\n\\nRepresentations for both the spurious and core features by a fundamental challenge in computational learning theory. Learning Boolean functions with Neural Network (NN).\\n\\n- Learning the parity function with neural networks in great detail (Merrill et al., 2023; Daniely & Malach, 2020; Edelmann et al., 2022). In our study, we assess the quality of the learned representations for both the spurious and core features by evaluating the model's performance after reweighting. Retraining the last layer has also been explored widely and shown to be highly efficient in a group balanced dataset with heavy regularization term to ensure that the conditional probabilities are well-defined.\\n\\nWe focus our study on two choices of the spurious and core features: one as easy-to-learn \\\"OOD task (Hermann & Lampinen, 2020). Out-of-distribution learning (Rosenfeld et al., 2018), and other settings, such as long-tail learning (Kang et al., 2020), have been explored widely and shown to be highly efficient.\\n\\nWe assume that for both the core and spurious functions, the parameter $\\\\lambda$ is an odd number. With $\\\\lambda > 0$, the core function satisfies the five constraints proposed in (Nagarajan et al., 2021) to be considered as a markov chain. We show some important properties of this curve in lemma A.1. It is noteworthy that our proposed framework/dataset also satisfies the five constraints.\\n\\nFor a boolean vector $x$, we take $u = \\\\{0, 1\\\\}$ to denote the total dimensions of the vector where the remaining $c$ coordinates/features and spurious coordinates/features respectively, while $s$ coordinates/features and spurious coordinates/features are denoted by $\\\\{0, 1\\\\}$ and we call them core and spurious label agree: $\\\\{0, 1\\\\}$ respectively, while $\\\\{0, 1\\\\}$ represent the confounder $\\\\chi$ that is unbiased when $\\\\lambda$ is an odd number. With $\\\\lambda > 0$, the core function satisfies the five constraints.\\n\\nWith $\\\\lambda > 0$, the core function satisfies the five constraints. Without loss of generality, we assume $\\\\lambda$ is unbiased when $\\\\lambda$ is an odd number. With $\\\\lambda > 0$, the core function satisfies the five constraints.\"}"}
{"id": "0tuwdgBiSN", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 1. A comparison of our dataset with the domino image dataset. Here $\\\\lambda = 0.75$. We take both $f_s$ and $f_c$ to be parity function. Dark grey square on a boolean vector denote 1 and light grey square denote $-1$.\\n\\nreflected in the convergence rate of the model. Our choice for these particular cases is strategic: both functions offer a solid ground for theoretical analysis, having been extensively examined in the context of deep learning (Barak et al., 2023; Daniely & Malach, 2020; Edelman et al., 2023; Abbe et al., 2022; 2023), and despite the same degree $d$, parity features are computationally much harder (exponential in $d$) to learn than staircase functions (polynomial in $d$) (Abbe et al., 2023). Furthermore, staircase functions are arguably more representative of the intricacies of feature learning on real dataset due to their hierarchical structure, where learning lower degrees aids in advancing to higher ones, and their learning loss curves more closely resemble those encountered in real dataset.\\n\\nWhy this spurious dataset? Numerous works have proposed different theoretical and experimental setups to study spurious correlation. It is noteworthy that spurious features are often used interchangeably with 'shortcut' or 'easier' features. However, different works have drastically different notions to encapsulate the easiness of a feature. For example, (Shah et al., 2020) examines features along different dimensions, quantifying simplicity by the number of linear segments needed for perfectly separating the data. (Wen & Li, 2021; Yang et al., 2023; Sagawa et al., 2020b; Chen et al., 2023) encapsulate both spurious and core features as 1-bit vectors, gauging simplicity by the amount or variance of noise applied to each feature. Despite our framework bearing resemblance to previously proposed notions of simplicity, we distinguish ourselves by: (1) employing non-linear features for both spurious and core attributes, (2) providing a more general notion of feature complexity and allowing us to explore functions with different properties (3) providing a modular, lightweight implementation of our dataset. Additionally, our dataset allows us to provide theoretical explanation for numerous observed behaviours in learning dynamics under spurious correlation.\\n\\nWe observe that popular semi-synthetic spurious datasets such as Waterbirds (Sagawa et al., 2020a), Colorful-MNIST (Zhang et al., 2022), and Domino-Image (Shah et al., 2020) share characteristic learning dynamics shown on the boolean feature datasets (see fig. 4). Therefore, our dataset serves as a good proxy to evaluate algorithms developed to deal with spurious features. Beyond capturing behaviors of existing dataset, our dataset additionally provides precise control over the complexity and structure of the spurious and core functions, which is under-explored in prior datasets.\\n\\n3. Empirical Findings\\nHere, we provide a comprehensive evaluation of a two-layer 4 neural network (width 100) optimized using Batch Stochastic Gradient Descent with the cross entropy loss on the boolean features dataset under the online setting. The exact experimental setup can be found in the Appendix C.1. We emphasize that our main focus here is on the online setting, and we provide more experimental findings regarding limited dataset size in the appendix. We mainly focus on two metrics to measure feature learning: Core and spurious correlation: correlation between the 4 Neural Networks with more layers share the same behavior. See Figure 9.\"}"}
{"id": "0tuwdgBiSN", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 2. Influence of confounder strength and complexity of spurious correlation on learning of core features. The y-axis shows the number of epochs required to reach 0.95 core correlation. The 0 degree bar indicates the epochs required to learn core feature when spurious correlation is not present i.e $\\\\lambda = 0.5$. Each bar of the left two plots is based on 30 repetitions of experiments.\\n\\nDecoded core and spurious correlation (Kirichenko et al., 2023; Hermann & Lampinen, 2020; Rosenfeld et al., 2022; Alain & Bengio, 2018): we first retrain the last layer of a model with logistic regression to fit either the spurious or core function using a group balanced dataset. Then measure the corresponding correlation as above. The decoded correlation metric is used to capture the extent to which a feature's representation has been effectively learned by the model.\\n\\nAlthough our primary focus is on the spurious Boolean dataset, we emphasize that our findings closely align with observations from other semi-synthetic datasets such as Waterbirds (Sagawa et al., 2020a), CMNIST(Zhang et al., 2022), and Domino Image(Shah et al., 2020). For more detailed exploration of these datasets, see appendix C.\\n\\n(R1) Simpler spurious features and higher correlation strength slow down the convergence rate of core feature learning (Figure 2). We observe a concave U-shaped phenomenon in the relationship between the complexity of spurious features and convergence time, where lower complexity features slow down convergence. Remarkably, even when the spurious feature approaches the complexity of the core feature, the model's performance is still adversely affected by its presence. Additionally, we find that slower convergence in learning the core feature leads to poorer overall performance on limited-size datasets (see Table C.2.1). This suggests that the existence of a spurious feature impacts the sample complexity required for learning the core feature.\\n\\nOur investigations indicate that the learning process remains relatively insensitive to the confounder strength until a certain threshold is reached. Beyond this point, there is a sudden and substantial increase in the computational time required to learn the core feature. We hypothesize that this threshold phenomenon can be attributed to two factors. Firstly, different features possess varying learning signal strengths. In the simplest case, exemplified by the parity function, differences in gradient signals for features with different complexities are noticeable from initialization. The gradient of a spurious feature can only surpass that of the core feature if the spurious correlation exceeds a certain threshold value. Secondly, as we will explore later, when $\\\\lambda$ is high, it becomes significantly more challenging for gradient descent to \u201cunlearn\u201d spurious neurons.\\n\\n(R2) Spurious and core features are learned by two separate sub-networks (Figure 3). There exists a classification of neurons into two groups, \u201cspurious neurons\u201d which have larger weights on the spurious index and \u201ccore neurons\u201d which have larger weights on the core index in the late stage of learning. For both parity and staircase tasks, almost all spurious neurons remain focused on spurious coordinates, while core neurons, at the start, do not focus on spurious coordinates and gradually develop an emphasis on core coordinates. See Appendix C.3.1 for more detail.\\n\\nIn vision tasks, it becomes more challenging to identify spurious or core neurons. To address this, we retrain the last layer of the neural network to learn either the spurious or core function separately. We observe that the set of neurons with significant weights in both trials is indeed very small, suggesting that neurons are separated into two distinct networks, similar to the spurious Boolean case. Our studies indicate that non-causally related feature representations are perhaps disentangled (at least in the last layer) from the...\"}"}
{"id": "0tuwdgBiSN", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 3. Each plot here shows the weight dynamic throughout training within a single selected neuron and each curve here corresponding to the weight dynamic on a single coordinate. The left two plots are for staircase with $\\\\text{deg}(f_s) = 7$, $\\\\text{deg}(f_c) = 14$ and the right two plots are for parity with $\\\\text{deg}(f_s) = 4$, $\\\\text{deg}(f_c) = 10$. For both experiments, $\\\\lambda = 0.9$. We see the neurons are separated into core and spurious neurons. Spurious neurons remain focused on learning spurious features and core neurons eventually emerge and learn the core feature.\\n\\nOutset, which aligns with a common goal in the fairness and model explainability literature (Locatello et al., 2019; Higgins et al., 2018). Therefore, it is of future interest to understand what conditions are sufficient for a model to learn disentangled representations under common training procedures.\\n\\n(R3) Spurious correlation strength determines how well the spurious feature is memorized. (fig. 4, fig. 5). When $\\\\lambda$ is high, the decoded spurious correlation and the total weight within the spurious subnetwork remains high even after extended training, both in the hidden and last layers. This phenomenon persists even when regularization is applied to the loss function. Notably, when $\\\\lambda$ is low, around 0.6 in our cases, the decoded spurious correlation and total weights on the spurious subnetwork decrease over time as the core feature is learned. In both cases, the learning process for spurious features plateaus when the core correlation starts to exceed the spurious correlation. Our observation thus illustrates another kind of in-distribution forgetting that occurs during training, contrasting with the established catastrophic forgetting, which happens when training on out-of-distribution (OOD) data. Thus to learn diverse features, it could be beneficial to identify and freeze such spurious neurons adaptively as have been done in (Kirkpatrick et al., 2017; Ye et al., 2023).\\n\\n(R4) Last Layer Retraining works by decreasing reliance on spurious subnetwork. We observed that last layer retraining consistently improves the worst group accuracy or core function correlation, with the most significant performance boost occurring during the early stages of training (Figure 4). This improvement is attributed to a substantial decrease in the ratio of second-layer weights between spurious neurons and core neurons (Table 5) which is a consequence of R2 and R3.\\n\\nNotably, our findings align with those in (LaBonte et al., 2023), where we observed that even retraining the last layer on the training dataset (with heavy $l_1$ regularization) significantly enhances robustness. Furthermore, we found the performance boost is most significant when just a small amount of group-balanced data is used for LLR (fig. 24).\\n\\n(R5) Popular debiasing algorithms fail in more general settings. (Figure 6) In scenarios where a spurious attribute is absent, debiasing algorithms typically adopt a two-stage approach (Liu et al., 2021; Yang et al., 2023; Nam et al., 2020; Kim et al., 2021; 2022). They first train an initial model using Early Stop Empirical Risk Minimization (ERM). These algorithms diverge in the second stage, where different heuristics are applied to distinguish and separate minority group data based on insights from the initial model. Implicit in their approach is the assumption of an extreme bias toward simplicity in the spurious feature, expecting the early model to prioritize learning the spurious feature and providing valuable information for segregating minority group samples.\\n\\nHowever, our investigation reveals that the benchmark datasets commonly used by these algorithms exhibit a crucial dataset bias towards having a much simpler spurious feature compared to the core feature. This bias creates a distinct separation between the learning phases of spurious and core features, as demonstrated in our parity case, allowing these methods to effectively separate minority groups (see Figure 6). We demonstrate that this separation may not hold true in many cases, particularly with limited datasets and spurious features of similar complexity to the core feature, as illustrated in the staircase case. To emphasize the practicability of our dataset, we introduce a domino-vision dataset.\"}"}
{"id": "0tuwdgBiSN", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 4. Core/spurious correlation and decoded correlation dynamics of different datasets. Leftmost figure shows the Fourier coefficients of both the spurious and core function are fitted from low (light color) to high (deep color) for the staircase function. All of the experiments have $\\\\lambda = 0$.\\n\\n| Staircase: $\\\\deg(f_s) = 7$, $\\\\deg(f_c) = 14$ |\\n| Parity: $\\\\deg(f_s) = 4$, $\\\\deg(f_c) = 10$ |\\n| CIFAR-MINST: (c) Truck-car (s) 01 |\\n\\nFigure 5. The two plots are produced by running experiments on parity cases under different $\\\\lambda$, the focus here is on the decoded spurious correlation. The experiments are run under $\\\\deg(f_s) = 4$, $\\\\deg(f_c) = 10$. Left: we see when $\\\\lambda$ is low, the spurious feature is being forgotten in the later stage of training. Right: when $\\\\lambda$ is relatively high, the spurious feature is memorized once it is learned.\\n\\nWith more challenging spurious and core features and shows the model is not able to improve further using early stopped model. To further assess the effectiveness of these debiasing algorithms, we employ Jaccard scores defined as $|A \\\\cap B|/|A \\\\cup B|$ and containment scores $|A \\\\cap B|/|A|$, where $A$ represents the predicted minority group by the algorithm using an early stop model, and $B$ represents the ground truth minority group. These metrics allow us to evaluate the extent to which minority group data is included in the predictions. For a more detailed discussion of the weaknesses of previous debiasing algorithms and their performance on real datasets, please refer to Appendix D.1.\\n\\n4. Theoretical Explanation\\n\\nWe will focus on the case of parities for our theoretical analysis. We do not endeavor to present a comprehensive end-to-end analysis of the feature learning dynamics in the general spurious parity case i.e $f_c, f_s$ are different degree of parity functions. We stress that our understanding of end-to-end dynamics of feature learning in the boolean case is still very limited with only recent work (Glasgow, 2023) providing an analysis on the end-to-end learning dynamics of 2-parity case. Furthermore, adding spurious correlations, introduces an additional phase of learning which is not tackled by these prior works. However, we hope to provide justification for each observation under certain assumptions at the beginning of each phase. We leave the full end-to-end case analysis to future work.\\n\\nSetup and Notations.\\n\\nWe consider a two layer neural network with $p$ neurons as $h(x) := \\\\sum_{i=1}^{p} a_i \\\\sigma(w^\\\\top_i x)$ where $\\\\sigma$ is the ReLU activation function $\\\\sigma(x) = \\\\max(0, x)$. $L_D(h) := \\\\mathbb{E}_{(x,y) \\\\sim D}[\\\\ell(h(x), y)]$ is defined as the population loss of the model on distribution $D$. We will use cross-entropy loss $\\\\ell(b_y, y) := -\\\\log(\\\\phi(yb_y))$ where $\\\\phi(x) := 1/(1+e^{-x})$ is the sigmoid function.\\n\\nOutline. We begin by quantifying the \u201cFourier gap\u201d which represents the difference in population gradient at initialization between the core and spurious features relative to the irrelevant coordinates. The gap immediately implies that, with layer-wise training as proposed in (Barak et al., 2023), the spurious feature can be learned sufficiently well. After this phase, we estimate the influence of the learned spurious feature on core feature learning by analyzing the change in magnitude of the gradient. Lastly we show that even when the network has learned the core features, learned spurious features must still persist. For detailed calculations and proof, we refer the reader to appendix B.1.\\n\\nSpurious Feature is Learned First. Following (Barak et al., 2023), at initialization, the Fourier gap on the spurious and core coordinates relative to the irrelevant coordinates is as follows.\\n\\nLemma 1. Let $\\\\xi_k = d_{\\\\text{Maj}}([k])$ be the $k$-th Fourier coordinate.\"}"}
{"id": "0tuwdgBiSN", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 6. The two plots on the right demonstrate that debasing methods JTT (Liu et al., 2021) and Spare (Yang et al., 2023) is able to infer points from the minority group successfully on datasets that share the characteristic of the parity case where there is an early spike in spurious correlation and the spurious feature is much easier than the core feature. While for the spurious staircase experiment and more challenging Domino dataset shown in the left the highest Jaccard score remains below 0.5. We report the worst group accuracy is not improving after upsampling based on the inferred group. See appendix D.1 for more detail. The black dash line remark the minority group proportion in the training dataset.\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{coefficient of the variable } & n = c + s + u \\\\\\\\\\n\\\\text{Majority function. At initialization, there is a set of neurons such that the population gradient gap on the variables compared to the irrelevant variables } & 6 \\\\\\\\\\n\\\\text{are:} & 1. \\\\text{Spurious Variable:} & -\\\\frac{1}{2}(\\\\xi - 1 - \\\\xi + 1), \\\\\\\\\\n\\\\text{2. Core Variable:} & -\\\\frac{1}{2}(\\\\xi - 1 - \\\\xi + 1). \\\\\\\\\\n\\\\text{We know that } |\\\\xi_k| & \\\\approx \\\\Theta(n - (k - 1)/2) \\\\text{ is monotonically decreasing with } k, \\\\text{ and thus we see the population gradient gap is exponentially higher for the spurious feature than the core feature with respect to the difference in complexity } c - s \\\\text{ when } \\\\lambda \\\\text{ is large, which would imply the following:} \\\\\\\\\\n\\\\text{Theorem 4.1 (informal, (Barak et al., 2023))} & \\\\text{Layer-wise training of the two-layer neural network with SGD is able to learn the spurious parity function up to error } \\\\epsilon \\\\text{ with } n = O(s) \\\\lambda \\\\text{ samples and time when } c \\\\gg s. \\\\\\\\\\n\\\\text{After the above layer-wise training, the model would become a Bayes-optimal predictor that depends only on the spurious coordinates. This corresponds to our empirical observation in the parity case when the model is fully correlated with the spurious feature and has not learned the core feature. We have:} \\\\\\\\\\n\\\\text{Lemma 2.} & \\\\text{The Bayes-optimal classifier, with respect to the logistic loss, among the classifiers that depend only on spurious coordinates is } h_s(x_s) = \\\\log \\\\frac{1}{1 - \\\\lambda} f_s(x_s). \\\\\\\\\\n\\\\text{Slow down of Core Feature Learning.} & \\\\text{Suppose the network can be divided into a part that has learned this Bayes optimal and the remaining part, we show that having learned the spurious feature leads to a reduction in the gradient in the remaining network of the core feature compared to the gradient if there was no spurious feature.} \\\\\\\\\\n\\\\text{Lemma 3 (informal).} & \\\\text{Assume the model can be decomposed into a sub-network } h_s(x_s) \\\\text{ that is at the Bayes optimal from lemma 2 and the remaining network } h(x) \\\\approx 0. \\\\text{ Then the gradient with respect to core weights in } h \\\\text{ is } 4\\\\lambda(1 - \\\\lambda) \\\\text{ smaller relative to the gradient if there was no spurious correlation.} \\\\\\\\\\n\\\\text{This implies that the core feature will continue to increase but at a slower convergence rate which depends on the correlation strength } \\\\lambda. \\\\text{ Therefore, if the spurious feature is simpler, then the core feature gradient reduces to the lower value earlier, leading to even slower convergence rate. This is consistent with our empirical observations.} \\\\\\\\\\n\\\\text{Persistence of Learned Spurious Features.} & \\\\text{Another empirical observation is the persistence of spurious features despite the core feature being learned. Here we present a justification for this in an idealized setup. We consider a neural network that can be decomposed into two sub-networks, } h_s(x_s) \\\\text{ and } h_c(x_c), \\\\text{ such that } h(x) = h_s(x_s) + h_c(x_c) \\\\text{ where } h_s(x_s) = \\\\sum_{i \\\\in S} \\\\sigma(w^\\\\top_i x_s), h_c(x_c) = \\\\sum_{i \\\\in C} \\\\sigma(v^\\\\top_i x_c). \\\\\\\\\\n\\\\text{The } S, C \\\\text{ represent the index set of spurious and core neurons respectively. Note that we base this assumption on the empirical finding we have in (R3).} \\\\\\\\\\n\\\\text{We further make the assumption that the spurious feature is learned and being Bayes optimal throughout the later stage of training (as observed in our experiments) while the core feature is being learned and the model gives homogeneous response to } x_c. \\\\text{ In particular we assume the following:} \\\\\\\\\\n\\\\text{Assumption 4.2 (Complete correlation to spurious and core features).} & \\\\text{For all } x, \\\\text{ we have } h_s(x) = \\\\gamma_s f_s(x) \\\\text{ and } h_c(x) = \\\\gamma_c f_c(x).\"}"}
{"id": "0tuwdgBiSN", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The reason we require that the model gives homogeneous response to $x_c$ is due to the property of cross entropy loss which has caused the slowdown to be non-linear for different samples if response to $x_c$ is not the same.\\n\\nWe first show that spurious neurons are \\\"dead\\\" as they do not learn core feature in the later stage.\\n\\n**Lemma 4 (informal)**\\n\\nIf $P_{i \\\\in C} |w_i| < |w_j|$ for all $j \\\\in S$.\\n\\nThen the gradient on core coordinates of the spurious neuron will be 0.\\n\\nThe lemma would also suggest that learned spurious neurons occupy part of the neural network capacity and if the network is of small size, the core feature may not be learned at all.\\n\\nOn the other hand, the population loss of the model on $D_\\\\lambda$ is given by the equation:\\n\\n$$L_{D_\\\\lambda}(h) = -\\\\lambda \\\\log(\\\\phi(\\\\gamma_c + \\\\gamma_s)) - (1 - \\\\lambda) \\\\log(\\\\phi(\\\\gamma_c - \\\\gamma_s))$$\\n\\nThe loss function is convex when $\\\\lambda$ is within the range $[0, 1]$ with respect to $\\\\gamma_s$. The optimal point occurs when $\\\\gamma_c \\\\to \\\\infty$, at which point $\\\\gamma_s \\\\to 0$. However, in practice, due to bounded iterations of optimization, this ideal scenario is unattainable. Instead, $\\\\gamma_c$ will inevitably be less than some constant. In such cases, a Bayes optimal model that considers both features will have a positive value for $\\\\gamma_s$ given that the spurious feature has been learned in the early stage. We conduct numerical experiment (see fig. 7) to illustrate the optimal value of $\\\\gamma_s$ for varying $\\\\lambda$ and $\\\\gamma_c$ and the slow down ratio of core feature gradient. The slow down ratio can be formulated as\\n\\n$$2\\\\lambda(1 - \\\\phi(\\\\gamma^*_s + \\\\gamma_c)) \\\\frac{1}{1 - \\\\phi(\\\\gamma_c)}$$\\n\\nwhere the numerator is gradient toward core feature under spurious distribution and the denominator is the same when spurious and core feature is uncorrelated or $\\\\lambda = 0$.\\n\\n5. We use $\\\\gamma^*_s$ to denote the optimal value of spurious margin under a fixed $\\\\gamma_c$ and $\\\\lambda$.\\n\\nThe left plot in fig. 7 offers insights into the efficacy of LLR (or data balancing retraining in general) when both the spurious and core features can be adequately learned. Incorporating minority group data points effectively diminishes the convergence point of the spurious margin by reducing $\\\\lambda$, subsequently decreasing the weights of the spurious subnetwork, as evidenced by our findings. Additionally, the plot illustrates that LLR yields more pronounced improvements when the confidence in the core feature is low, as observed in (R4) during the early stages of learning. This observation aligns with the findings in (LaBonte et al., 2023), where the addition of a small number of minority examples results in the most significant improvement. The right plot demonstrates that the slow down is less significant in the later stage of training although $\\\\lambda$ plays a key role in determining the empirical convergence point. It is noteworthy that the values of $\\\\gamma_s$ at $\\\\gamma_c = 0$ are equal to the value calculated in lemma 2.\\n\\nFigure 7. Optimal values of spurious margin under different $\\\\lambda$ with varying confidence core margin. The left plot shows how spurious weights changes with varying $\\\\lambda$ and confidence on core feature. The plot implies retraining a model with more balanced data points would yield most significant improvement when $\\\\lambda$ is high and core feature is learned poorly. The right plot shows how core feature learning is slowed down by the learned spurious feature. Note that under the cross-entropy loss, the margin represents the confidence level of a model, hence $\\\\text{confidence} = \\\\phi(\\\\gamma) \\\\leq 1$. For $\\\\gamma_c = 15$, we have $\\\\text{confidence} > 0.99999$, which is likely to be an empirical upper bound for most iterative optimization methods. The plots are generated by fixing $\\\\lambda$ and $\\\\gamma_c$ at various values and optimizing $\\\\gamma_s$ with respect to $L_{D_\\\\lambda}$ thereby offering an estimate of the weight dynamics within the spurious network after lemma 2.\\n\\n5. Conclusion\\n\\nOur study uses a detailed set of experiments on the boolean feature dataset to better understand how spurious features affect the learning of core features. We show that the dataset is a valuable tool in highlighting the shortcomings of previous algorithms that only perform well with simpler, less challenging spurious features. Our dataset assumes that spurious and core features are completely separate, which might not always be the case in real world datasets, thus improving our dataset to more closely mirror real-world complexities is an exciting opportunity for future research.\\n\\nOn the theoretical side, studying the end-to-end dynamics with spurious features is a technically interesting problem, and may require new tools. Our analysis currently assumes certain well-motivated conditions to bypass some parts of the unknown underlying feature learning process, and we believe that rigorously proving them would be challenging. Notably, showing that the spurious and core networks remain disjoint seems particularly non-trivial to show. A key finding of our analysis is the persistence of spurious feature weights, which tend to converge to significant values when $\\\\lambda$ is high even if the model has learned the core feature confidently. Our investigation reveals that retraining the model with more balanced dataset effectively reduces the weights on the spurious network. While we refer to the observed correlation as \\\"spurious\\\" in this work, it's important to acknowledge that there are scenarios where we seek to learn a diverse array of features.\"}"}
{"id": "0tuwdgBiSN", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\n1. For random index $j \\\\in \\\\mathcal{U}$, we have\\n\\n$$g_{s,i,j} = -2a_i (2\\\\lambda - 1) \\\\cdot \\\\mathbb{E}_{x_c, x_s \\\\sim \\\\mathcal{U}[yx_j] \\\\{ w^\\\\top x + b > 0 \\\\}} \\\\{ \\\\chi(x_c) = \\\\chi(x_s) \\\\}$$\\n\\n$$= -2a_i (2\\\\lambda - 1) \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\mathcal{U}[\\\\chi[c] \\\\cup j] (x)} q(x)$$\\n\\n$$= -12a_i (2\\\\lambda - 1) \\\\cdot (\\\\chi[c] \\\\{ j \\\\} (w^i)) - \\\\chi[s] \\\\cup j (w^i)$$\\n\\n2. For core index $j \\\\in \\\\mathcal{C}$, in a similar manner, we have\\n\\n$$g_{s,i,j} = -2a_i (2\\\\lambda - 1) \\\\cdot \\\\mathbb{E}_{x_c, x_s \\\\sim \\\\mathcal{U}[yx_j] \\\\{ w^\\\\top x + b > 0 \\\\}} \\\\{ \\\\chi(x_c) = \\\\chi(x_s) \\\\}$$\\n\\n$$= -2a_i (2\\\\lambda - 1) \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\mathcal{U}[\\\\chi[c] \\\\setminus j] (x)} q(x)$$\\n\\n$$= -12a_i (2\\\\lambda - 1) \\\\cdot (\\\\chi[c] \\\\{ j \\\\} (w^i)) - \\\\chi[s] \\\\cup j (w^i)$$\\n\\n3. For spurious index $j \\\\in \\\\mathcal{S}$, in a similar manner, we have\\n\\n$$g_{s,i,j} = -2a_i (2\\\\lambda - 1) \\\\cdot \\\\mathbb{E}_{x_c, x_s \\\\sim \\\\mathcal{U}[yx_j] \\\\{ w^\\\\top x + b > 0 \\\\}} \\\\{ \\\\chi(x_c) = \\\\chi(x_s) \\\\}$$\\n\\n$$= -2a_i (2\\\\lambda - 1) \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\mathcal{U}[\\\\chi[s] \\\\setminus j] (x)} q(x)$$\\n\\n$$= -12a_i (2\\\\lambda - 1) \\\\cdot (\\\\chi[c] \\\\{ j \\\\} (w^i)) - \\\\chi[s] \\\\cup j (w^i)$$\\n\\nPutting it together.\\n\\nWe summarize the final population gradient on each type of index.\\n\\n**Lemma 1 (formal).** Under the proposed setup, we have the population gradient at initialization for different type of coordinates as below:\\n\\n1. For random index $j \\\\in \\\\mathcal{U}$, we have\\n\\n$$g_{u,i,j} + g_{s,i,j} = -2a_i (2\\\\lambda - 1) \\\\cdot \\\\mathbb{E}_{x_c, x_s \\\\sim \\\\mathcal{U}[yx_j] \\\\{ w^\\\\top x + b > 0 \\\\}} \\\\{ \\\\chi(x_c) = \\\\chi(x_s) \\\\}$$\\n\\n$$= -2a_i (2\\\\lambda - 1) \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\mathcal{U}[\\\\chi[c] \\\\cup j] (x)} q(x)$$\\n\\n$$= -12a_i (2\\\\lambda - 1) \\\\cdot (\\\\chi[c] \\\\{ j \\\\} (w^i)) - \\\\chi[s] \\\\cup j (w^i)$$\\n\\n2. For core index $j \\\\in \\\\mathcal{C}$, in a similar manner, we have\\n\\n$$g_{u,i,j} + g_{s,i,j} = -2a_i (2\\\\lambda - 1) \\\\cdot \\\\mathbb{E}_{x_c, x_s \\\\sim \\\\mathcal{U}[yx_j] \\\\{ w^\\\\top x + b > 0 \\\\}} \\\\{ \\\\chi(x_c) = \\\\chi(x_s) \\\\}$$\\n\\n$$= -2a_i (2\\\\lambda - 1) \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\mathcal{U}[\\\\chi[c] \\\\setminus j] (x)} q(x)$$\\n\\n$$= -12a_i (2\\\\lambda - 1) \\\\cdot (\\\\chi[c] \\\\{ j \\\\} (w^i)) - \\\\chi[s] \\\\cup j (w^i)$$\\n\\n3. For spurious index $j \\\\in \\\\mathcal{S}$, in a similar manner, we have\\n\\n$$g_{u,i,j} + g_{s,i,j} = -2a_i (2\\\\lambda - 1) \\\\cdot \\\\mathbb{E}_{x_c, x_s \\\\sim \\\\mathcal{U}[yx_j] \\\\{ w^\\\\top x + b > 0 \\\\}} \\\\{ \\\\chi(x_c) = \\\\chi(x_s) \\\\}$$\\n\\n$$= -2a_i (2\\\\lambda - 1) \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\mathcal{U}[\\\\chi[s] \\\\setminus j] (x)} q(x)$$\\n\\n$$= -12a_i (2\\\\lambda - 1) \\\\cdot (\\\\chi[c] \\\\{ j \\\\} (w^i)) - \\\\chi[s] \\\\cup j (w^i)$$\"}"}
{"id": "0tuwdgBiSN", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Finding Hidden Functions That Makes Gradient Descent on the Last Layer Weights Recover Parity Function\\n\\nThis part is correspond to the lemma 4 claim 5 in the hidden progress paper. What we are going to argue is that there is a set of functions represented by the hidden layer $\\\\sigma(P_i x - b_i)$ such that there is a set of weight $a_i$ makes the composition function equivalent to a parity function. Formally\\n\\nLemma B.1. Fix $k$, there is a set of $k + 1$ ReLU functions in the form $f_j(x) = \\\\sigma(P_n x_i + b_j)$ where $b_j = \\\\{k + 1, k - 1, k - 3, \\\\ldots, -k + 3, -k + 1\\\\}$ and a set of weights $u_j$ with $\\\\|u\\\\|_2 \\\\leq \\\\sqrt{k}$ modeling the $k$-degree parity function by $f(x) = \\\\chi_k(x) = \\\\sum_{j=1}^{k+1} u_j f_j(x)$.\\n\\nProof. This is best illustrated by an example. Consider $k = 5$, if there is one $-1$ in $x$, then $f_1(x) = 9$, $f_2(x) = 7$, $f_3(x) = 5$, $f_4(x) = 3$, $f_5(x) = 1$, $f_6(x) = 0$. Let $g(x)$ denote the number of $-1$ a sample $x$ has. We have for all $\\\\{x \\\\in X | g(x) = c\\\\}$, they will have the same value of $P_x$ and thus their value on $f_1, \\\\ldots, f_5$ will be the same. In this way, we can categorize samples into 6 cases where $g(x) = 0$, $g(x) = 1$, ..., $g(x) = 5$. And we can form a matrix where each row represent the type of sample and column represent its value on $f_j$. Thus the matrix we form can be represented as $M_{i,j} = f_j(x | g(x) = i)$. In the case of $k = 5$, the matrix is\\n\\n$$\\n\\\\begin{pmatrix}\\n11 & 9 & 7 & 5 & 3 & 1 \\\\\\\\\\n9 & 7 & 5 & 3 & 1 & 0 \\\\\\\\\\n7 & 5 & 3 & 1 & 0 & 0 \\\\\\\\\\n5 & 3 & 1 & 0 & 0 & 0 \\\\\\\\\\n3 & 1 & 0 & 0 & 0 & 0 \\\\\\\\\\n1 & 0 & 0 & 0 & 0 & 0\\n\\\\end{pmatrix}\\n$$\\n\\nAnd we want to find a weight $u$ on the matrix such that $Mu = y$, where $y_i$ represent the corresponding parity value of $x$ represented by a row. In our example, $y = [1, -1, 1, -1, 1, -1]$. Notice that the matrix is triangular and full rank, thus there is a unique $u$ that solve the system. Also, notice that the eigenvalues of the matrix are all 1. Thus the norm of $u$ can be bounded by $\\\\|u\\\\|_2 \\\\leq \\\\sqrt{k}$.\\n\\nWe are going to relax the condition showing that as long as the weight $w_j$ on a set of neuron is not too far from 1. Then there still exist a $u_\\\\ast$ with small norm that solve the system.\\n\\nLemma B.2. Fix $k$, there is a set of $k + 1$ ReLU functions in the form $f_j(x) = \\\\sigma(w_j^\\\\top x_i + b_j)$ with $b_j = \\\\{k + 1, k - 1, k - 3, \\\\ldots, -k + 3, -k + 1\\\\}$ where if $w_{j,i}$ satisfy $|w_{j,i} - 1| \\\\leq \\\\frac{1}{2}k$, then there is a set of weights $u_j$ with $\\\\|u\\\\|_2 \\\\leq 2\\\\sqrt{k}$ modeling the $k$-degree parity function by $f(x) = \\\\chi_k(x) = \\\\sum_{j=1}^{k+1} u_j f_j(x)$.\\n\\nProof. From the proof of lemma B.1, we see for the solution $u_\\\\ast$ to exist, all we required is the function outputs on different cases of $x$ form a triangular matrix and the dependency of the upper bound of $\\\\|u\\\\|_2$ will be on the smallest entry of the diagonal of the matrix. We will show that given $|w_{j,i} - 1| \\\\leq \\\\frac{1}{2}k$, for all $x$, if $P_x + b_j \\\\geq 1$, then $w_{j,i}^\\\\top x + b_j \\\\geq 1/2$. If $P_x + b_j \\\\leq -1$, then $w_{j,i}^\\\\top x + b_k \\\\leq -1/2$. Using this result and by the fact that our construction have for all $x$ and a fixed $j$ either $P_x + b_j \\\\geq 1$ or $P_x + b_j \\\\leq -1$, we can replace the function $f_j = \\\\sigma(P_x + b_j)$ with $f'_j = \\\\sigma(w_{j,i}^\\\\top x + b_j)$ such that the matrix is still triangular and all diagonal entries $M'_{j,j} \\\\geq 1/2$, $\\\\forall j \\\\in [k]$. This implies the smallest eigenvalue of the matrix $\\\\lambda_{\\\\text{min}}(M)$ $\\\\geq 1/2$. And we have $\\\\|y\\\\|_2 \\\\geq \\\\lambda_{\\\\text{min}}(M)\\\\|u\\\\|_2$, which implies $\\\\|u\\\\|_2 \\\\leq \\\\|y\\\\|_2\\\\lambda_{\\\\text{min}}(M) = 2\\\\sqrt{k}$. \\\\end{proof}\"}"}
{"id": "0tuwdgBiSN", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We now prove the earlier claim. Note\\n\\\\[ w_j^\\\\top x + b_j - (X x + b_j) \\\\]\\n\\\\[ = \\\\| X_i (w_j,i - 1) x \\\\| \\\\]\\n\\\\[ = \\\\| < w_j - 1, x > \\\\| \\\\]\\n\\\\[ \\\\leq \\\\| w_j - 1 \\\\|_2^2 \\\\| x \\\\|_2^2 \\\\]\\n\\\\[ \\\\leq k_2^2 (| w_j - 1 |) \\\\]\\n\\\\[ \\\\leq \\\\frac{1}{4} \\\\]\\nThus if \\\\( P_x + b_j \\\\geq 1 \\\\), then we have\\n\\\\[ w_j^\\\\top x + b_j \\\\geq \\\\frac{3}{4} \\\\geq \\\\frac{1}{2} \\\\]\\nand if \\\\( P_x + b_j \\\\leq -1 \\\\), we have\\n\\\\[ w_j^\\\\top x + b_j \\\\leq -\\\\frac{3}{4} \\\\leq -\\\\frac{1}{2} \\\\].\\n\\nB.1.6. ON A SET OF NEURONS\\nWe first redo the proof of the hidden progress paper with \\\\( B.3 \\\\), given the population gradient gap between two indexes, we can learn the sparse parity function that is defined on the index (which are with higher absolute value of gradient) without any error with high probability. The difference is that we do thresholding and only requiring the population gradient to be accurate enough.\\n\\nTheorem B.1. Suppose the population gradient gap between the spurious coordinates \\\\([ s]\\\\) and core coordinates \\\\([ c]\\\\) is \\\\( \\\\Delta s - c \\\\). On a set of neurons in the configuration of appendix B.1.3, if \\\\( -a_i \\\\chi_s \\\\{ j \\\\} (w_j) = \\\\text{sgn}(\\\\xi_s - 1) \\\\), \\\\( \\\\forall j \\\\in [s] \\\\) and \\\\( -a_i \\\\chi_{s \\\\cup j} (w_j) = \\\\text{sgn}(\\\\xi_s + 1) \\\\), \\\\( \\\\forall j \\\\in [c] \\\\). Suppose we take \\\\( m \\\\) sample points, and let the first step learning rate to be \\\\( \\\\mu = g_s \\\\) where \\\\( g_s \\\\) is the spurious gradient we found in appendix B.1.4. We then do thresholding by comparing the gradient of each coordinate to the empirical mean of the absolute value of gradient applied on the whole hidden weight vector. We zero out weights on coordinates that have gradient small then the mean. Then with \\\\( m \\\\geq \\\\max \\\\{ \\\\frac{2 s^2 \\\\log(2 s/\\\\delta)}{\\\\xi_s^2 - 1}, \\\\frac{\\\\log(2 s/\\\\delta)}{n^2 u^2 \\\\Delta^2 s - c}, \\\\frac{\\\\log(2 s/\\\\delta)}{n^2 c^2 \\\\Delta^2 s - c} \\\\} \\\\), the neuron satisfy the condition in lemma B.2 w.p \\\\( \\\\geq 1 - \\\\delta \\\\).\\n\\nProof. We denote a random sample point by \\\\( x_i \\\\) and its corresponding gradient imposed on the a neuron weights by \\\\( g_{i,j} = \\\\left[ -y_a j I \\\\{ w^\\\\top x + b > 0 \\\\} x_j \\\\right] \\\\).\\n\\nWe will use \\\\( g_s, g_c, g_u \\\\) refer to the population gradient calculated in appendix B.1.4. We want bound the probability of the event 1). Only spurious weights are kept after thresholding. 2). on all spurious indexes \\\\( | \\\\widetilde{\\\\mu} P_m \\\\leq 1 \\\\) by finding the required number of \\\\( m \\\\). That is for all weights, if the weight \\\\( j \\\\) is on a spurious variable, it must satisfy at the same time \\\\( | \\\\mu b g_j - 1 \\\\| \\\\leq \\\\frac{1}{2} s \\\\) and \\\\( b g_j - \\\\text{sgn}(\\\\text{sgn}(g_k) > 0) \\\\) and if the weight \\\\( j \\\\) is on a core/independent variable, it must satisfy \\\\( b g_j - \\\\text{sgn}(\\\\text{sgn}(g_k) < 0) \\\\) where \\\\( b g_j = \\\\sum_{m=1}^{P} g_{i,j} \\\\). The first inequality can be written as, for \\\\( j \\\\in [s] \\\\)\\n\\\\[ | \\\\mu g_j - 1 \\\\| \\\\leq \\\\frac{1}{2} s \\\\iff | g_j - E[g_j]| \\\\leq g_s \\\\]\\nTake the expectation for the second formula we have for \\\\( j \\\\in [c] \\\\cup [u] \\\\)\\n\\\\[ E[g_j - \\\\sum_{n=1}^{P} g_{k,n}] \\\\leq -(c + u) \\\\Delta s - c n \\\\]\"}"}
{"id": "0tuwdgBiSN", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We want to bound the probability of a bad event by $\\\\delta$. We use the Hoeffding bound and require, for $j \\\\in [s]$, \\n\\n$$P\\\\left[g_j - E[g_j] \\\\geq g_j \\\\right] \\\\leq \\\\delta n$$\\n\\nand for $j \\\\in [u]$, \\n\\n$$P\\\\left[|g_j - P_{n_k=1} g_k n - E[g_j - P_{n_k=1} g_k n]| \\\\geq (c + u)\\\\Delta_s - cn\\\\right] \\\\leq \\\\delta n$$\\n\\nIf all three are satisfied, then by union bound, we would have for a given set of neurons, the condition lemma B.2 is not satisfied with probability less than $\\\\delta$. In terms of $m$, by Hoeffding bounds, we have when $m \\\\geq \\\\max\\\\{2s^2 \\\\log(2s/\\\\delta), \\\\log(2s/\\\\delta) n^2 (c + u)^2 \\\\Delta_s^2 - c, \\\\log(2s/\\\\delta) n^2 \\\\Delta_s^2 - c\\\\}$, the condition will be satisfied with error probability less than $\\\\delta$.\\n\\n**B.1.7. NUMBER OF NEURONS REQUIRED**\\n\\nFollowing the same argument in (Barak et al., 2023), we want to have a set of neurons that satisfy B.3.\\n\\n**Theorem B.2.**\\n\\nTake $r \\\\geq 2s \\\\log(2s/\\\\delta)$ and $m \\\\geq \\\\max\\\\{2s^2 \\\\log(2s/\\\\delta), \\\\log(2s/\\\\delta) n^2 (c + u)^2 \\\\Delta_s^2 - c, \\\\log(2s/\\\\delta) n^2 \\\\Delta_s^2 - c\\\\}$, with probability $\\\\geq 1 - \\\\delta$, after the first gradient step, we will get a set of neurons that satisfy lemma B.2.\\n\\n**Proof.**\\n\\nFor some $w_i \\\\sim \\\\{\\\\pm 1\\\\}^n$, the probability that $w_i$ satisfies the condition is $\\\\frac{1}{2} - s^{-2}$. Additionally, for some fixed $i' \\\\in [s]$, the probability that $b_i = -s + i'$ is $\\\\frac{1}{s}$. Therefore, for some fixed $i \\\\in [r/2]$ and $i' \\\\in [s]$, with probability $\\\\frac{1}{s^2} - \\\\frac{1}{s}$, $b_i = b_i + r/2 = -k + i'$ and the weight satisfies the condition. Taking $r \\\\geq 2s \\\\log(2s/\\\\delta)$, we get the probability that there is no $i \\\\in [r/2]$ that satisfies the above condition for any fixed $i'$ is:\\n\\n$$1 - \\\\frac{1}{n^2} \\\\frac{1}{2} \\\\leq \\\\exp(-\\\\frac{r^2}{n^2}) \\\\leq \\\\delta$$\\n\\nBy union bound, with probability $\\\\geq 1 - \\\\delta$, there exists a set of $s$ neurons satisfying the conditions of Theorem 3.\\n\\n**B.1.8. STOCHASTIC GRADIENT DESCENT**\\n\\nWe use the following result on convergence of SGD (see (Shalev-Shwartz & Ben-David, 2014)).\\n\\n**Theorem B.3.**\\n\\nLet $M, \\\\rho > 0$. Fix $T$ and let $\\\\mu = M\\\\rho \\\\sqrt{T}$. Let $F$ be a convex function and $u^* \\\\in \\\\arg \\\\min \\\\|u\\\\|_2 \\\\leq M f(u)$. Let $u(0) = 0$ and for every $t$, let $v_t$ be some random variable such that $E[v_t | u(t)] = \\\\nabla u(t) F(u(t))$ and let $u(t+1) = u(t) - \\\\mu v_t$. Assume that $\\\\|v_t\\\\|_2 \\\\leq \\\\rho$ w.p. 1. Then,\\n\\n$$E[F(u(t))] \\\\leq F(u^*) + M\\\\rho \\\\sqrt{T}$$\\n\\n**Theorem B.4.**\\n\\n(SGD on MLPs learns sparse parities) Take $r \\\\geq 2s \\\\log(2s/\\\\delta)$, $B \\\\geq m \\\\geq \\\\max\\\\{2s^2 \\\\log(2s/\\\\delta), \\\\log(2s/\\\\delta) n^2 (c + u)^2 \\\\Delta_s^2 - c, \\\\log(2s/\\\\delta) n^2 \\\\Delta_s^2 - c\\\\}$, $T \\\\geq 37srn^2 \\\\epsilon^2$, take the first gradient step with size $\\\\mu = \\\\frac{1}{g_s}$, then w.p. $\\\\geq 1 - \\\\delta$, by fixing the hidden layer weights and running SGD on the second layer weights with step size $\\\\mu = \\\\frac{2}{\\\\sqrt{s} \\\\sqrt{rTn}}$, we can solve the parity task with error less than $\\\\epsilon$.\\n\\n**Proof.**\\n\\nWe take the first gradient step and then fix the hidden layer. Let $F(u) = E_x[l(u^\\\\top \\\\sigma(W(1)x + b(1)), y)]$. Thus, $F$ is a convex function. For every $t$, denote $v_t = \\\\frac{1}{B} \\\\sum_{i=1}^{B} \\\\nabla u(t) l(f(x_t, \\\\theta_t), y) = \\\\frac{1}{B} \\\\nabla u(t) l((u(t))^\\\\top \\\\sigma(W(1)x_i + b(1)), y_l,t)$.\"}"}
{"id": "0tuwdgBiSN", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nAcknowledgements\\n\\nWe thank Ben L. Edelman, Vaishnavh Nagarajan, and the anonymous reviewers of the M3L workshop at NeurIPS and ICML for their helpful suggestions to improve the paper.\\n\\nImpact Statement\\n\\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.\\n\\nReferences\\n\\nAbbe, E., Boix-Adsera, E., and Misiakiewicz, T. The merged-staircase property: A necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks, February 2022. URL http://arxiv.org/abs/2202.08658. arXiv:2202.08658 [cs, stat].\\n\\nAbbe, E., Boix-Adsera, E., and Misiakiewicz, T. SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics, August 2023. URL http://arxiv.org/abs/2302.11055. arXiv:2302.11055 [cs, stat].\\n\\nAlain, G. and Bengio, Y. Understanding intermediate layers using linear classifier probes, November 2018. URL http://arxiv.org/abs/1610.01644. arXiv:1610.01644 [cs, stat].\\n\\nArjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. Invariant Risk Minimization, March 2020. URL http://arxiv.org/abs/1907.02893. arXiv:1907.02893 [cs, stat].\\n\\nBarak, B., Edelman, B. L., Goel, S., Kakade, S., Malach, E., and Zhang, C. Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit, January 2023. URL http://arxiv.org/abs/2207.08799. arXiv:2207.08799 [cs, math, stat].\\n\\nChen, Y., Huang, W., Zhou, K., Bian, Y., Han, B., and Cheng, J. Understanding and Improving Feature Learning for Out-of-Distribution Generalization, October 2023. URL http://arxiv.org/abs/2304.11327. arXiv:2304.11327 [cs, stat].\\n\\nDaniely, A. and Malach, E. Learning Parities with Neural Networks, July 2020. URL http://arxiv.org/abs/2002.07400. arXiv:2002.07400 [cs, stat].\\n\\nDuchene, C., Jamet, H., Guillaume, P., and Dehak, R. A benchmark for toxic comment classification on Civil Comments dataset, January 2023. URL http://arxiv.org/abs/2301.11125. arXiv:2301.11125 [cs, eess].\\n\\nEdelman, B. L., Goel, S., Kakade, S., Malach, E., and Zhang, C. Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck, September 2023. URL http://arxiv.org/abs/2309.03800. arXiv:2309.03800 [cs, stat] version: 1.\\n\\nGeirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. Shortcut Learning in Deep Neural Networks. Nature Machine Intelligence, 2(11):665\u2013673, November 2020. ISSN 2522-5839. doi: 10.1038/s42256-2020-00257-z. URL http://arxiv.org/abs/2004.07780. arXiv:2004.07780 [cs, q-bio].\\n\\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., and Brendel, W. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness, November 2022. URL http://arxiv.org/abs/1811.12231. arXiv:1811.12231 [cs, q-bio, stat].\\n\\nGlasgow, M. SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem, October 2023. URL http://arxiv.org/abs/2309.15111. arXiv:2309.15111 [cs, stat].\\n\\nHermann, K. L. and Lampinen, A. K. What shapes feature representations? Exploring datasets, architectures, and training, October 2020. URL http://arxiv.org/abs/2006.12433. arXiv:2006.12433 [cs, stat].\\n\\nHiggins, I., Amos, D., Pfau, D., Racaniere, S., Matthey, L., Rezende, D., and Lerchner, A. Towards a Definition of Disentangled Representations, December 2018. URL http://arxiv.org/abs/1812.02230. arXiv:1812.02230 [cs, stat].\\n\\nIdrissi, B. Y., Arjovsky, M., Pezeshki, M., and Lopez-Paz, D. Simple data balancing achieves competitive worst-group-accuracy, February 2022. URL http://arxiv.org/abs/2110.14503. arXiv:2110.14503 [cs].\\n\\nIzmailov, P., Kirichenko, P., Gruver, N., and Wilson, A. G. On Feature Learning in the Presence of Spurious Correlations, October 2022. URL http://arxiv.org/abs/2210.11369. arXiv:2210.11369 [cs, stat].\\n\\nJoshi, S., Yang, Y., Xue, Y., Yang, W., and Mirzasoleiman, B. Towards Mitigating Spurious Correlations in the Wild: A Benchmark and a more Realistic Dataset, September 2023. URL http://arxiv.org/abs/2306.11957. arXiv:2306.11957 [cs].\"}"}
{"id": "0tuwdgBiSN", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nKang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., and Kalantidis, Y. Decoupling Representation and Classifier for Long-Tailed Recognition, February 2020. URL http://arxiv.org/abs/1910.09217.\\n\\nKermany, D. S., Goldbaum, M., Cai, W., Valentim, C. C. S., Liang, H., Baxter, S. L., McKeown, A., Yang, G., Wu, X., Yan, F., Dong, J., Prasadha, M. K., Pei, J., Ting, M. Y. L., Zhu, J., Li, C., Hewett, S., Dong, J., Ziyar, I., Shi, A., Zhang, R., Zheng, L., Hou, R., Shi, W., Fu, X., Duan, Y., Huu, V. A. N., Wen, C., Zhang, E. D., Zhang, C. L., Li, O., Wang, X., Singer, M. A., Sun, X., Xu, J., Tafreshi, A., Lewis, M. A., Xia, H., and Zhang, K. Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell, 172(5):1122\u20131131.e9, February 2018. ISSN 0092-8674, 1097-4172. doi: 10.1016/j.cell.2018.02.010. URL https://www.cell.com/cell/abstract/S0092-8674(18)30154-5. Publisher: Elsevier.\\n\\nKim, E., Lee, J., and Choo, J. BiaSwap: Removing Dataset Bias with Bias-Tailored Swapping Augmentation. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 14972\u201314981, Montreal, QC, Canada, October 2021. IEEE. ISBN 978-1-66542-812-5. doi: 10.1109/ICCV48922.2021.01472. URL https://ieeexplore.ieee.org/document/9710845/.\\n\\nKim, N., Hwang, S., Ahn, S., Park, J., and Kwak, S. Learning Debiased Classifier with Biased Committee, June 2022. URL https://arxiv.org/abs/2206.10843v5.\\n\\nKirichenko, P., Izmailov, P., and Wilson, A. G. Last Layer Re-training is Sufficient for Robustness to Spurious Correlations, June 2023. URL http://arxiv.org/abs/2204.02937. arXiv:2204.02937 [cs, stat].\\n\\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., and Hadsell, R. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, March 2017. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.1611835114. URL http://arxiv.org/abs/1612.00796. arXiv:1612.00796 [cs, stat].\\n\\nKrizhevsky, A. Learning Multiple Layers of Features from Tiny Images.\\n\\nLaBonte, T., Muthukumar, V., and Kumar, A. Towards Last-layer Retraining for Group Robustness with Fewer Annotations, November 2023. URL http://arxiv.org/abs/2309.08534. arXiv:2309.08534 [cs].\\n\\nLee, Y., Yao, H., and Finn, C. Diversify and Disambiguate: Learning From Underspecified Data, February 2023. URL http://arxiv.org/abs/2202.03418. arXiv:2202.03418 [cs, stat].\\n\\nLiu, E. Z., Haghgoo, B., Chen, A. S., Raghunathan, A., Koh, P. W., Sagawa, S., Liang, P., and Finn, C. Just Train Twice: Improving Group Robustness without Training Group Information, September 2021. URL http://arxiv.org/abs/2107.09044. arXiv:2107.09044 [cs, stat].\\n\\nLiu, S., Zhang, X., Sekhar, N., Wu, Y., Singhal, P., and Fernandez-Granda, C. Avoiding spurious correlations via logit correction, February 2023. URL http://arxiv.org/abs/2212.01433. arXiv:2212.01433 [cs, eess, stat].\\n\\nLiu, Z., Luo, P., Wang, X., and Tang, X. Deep Learning Face Attributes in the Wild, September 2015. URL http://arxiv.org/abs/1411.7766. arXiv:1411.7766 [cs] version: 3.\\n\\nLocatello, F., Abbati, G., Rainforth, T., Bauer, S., Sch\u00f6lkopf, B., and Bachem, O. On the Fairness of Disentangled Representations. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/hash/1b486d7a5189ebe8d8c46afc64b0d1b4-Abstract.html.\\n\\nMcCoy, T., Pavlick, E., and Linzen, T. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3428\u20133448, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1334. URL https://aclanthology.org/P19-1334.\\n\\nMerrill, W., Tsilivis, N., and Shukla, A. A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks, March 2023. URL http://arxiv.org/abs/2303.11873. arXiv:2303.11873 [cs].\\n\\nNagarajan, V., Andreassen, A., and Neyshabur, B. Understanding the Failure Modes of Out-of-Distribution Generalization, April 2021. URL http://arxiv.org/abs/2010.15775. arXiv:2010.15775 [cs, stat].\\n\\nNakkiran, P., Kaplun, G., Kalimeris, D., Yang, T., Edelman, B. L., Zhang, F., and Barak, B. SGD on Neural Networks Learns Functions of Increasing Complexity, May 2019. URL http://arxiv.org/abs/1905.11604. arXiv:1905.11604 [cs, stat].\\n\\nNam, J., Cha, H., Ahn, S., Lee, J., and Shin, J. Learning from Failure: De-biasing Classifier from Biased Classifier.\"}"}
{"id": "0tuwdgBiSN", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "0tuwdgBiSN", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nYang, Y., Gan, E., Dziugaite, G. K., and Mirzasoleiman, B. Identifying Spurious Biases Early in Training through the Lens of Simplicity Bias, May 2023. URL arxiv.org/abs/2305.18761. arXiv:2305.18761 [cs].\\n\\nYe, H., Zou, J., and Zhang, L. Freeze then Train: Towards Provable Representation Learning under Spurious Correlations and Feature Noise, April 2023. URL arxiv.org/abs/2210.11075. arXiv:2210.11075 [cs].\\n\\nZhang, M., Sohoni, N. S., Zhang, H. R., Finn, C., and R\u00e9, C. Correct-N-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations, March 2022. URL arxiv.org/abs/2203.01517. arXiv:2203.01517 [cs].\\n\\nZhou, C., Ma, X., Michel, P., and Neubig, G. Examining and Combating Spurious Features under Distribution Shift, June 2021. URL arxiv.org/abs/2106.07171. arXiv:2106.07171 [cs].\"}"}
{"id": "0tuwdgBiSN", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nNote that by the condition, we have a set of good neurons after first gradient step with probability greater than $1 - \\\\delta$. And there exists $\\\\|u^*\\\\|_2^2 \\\\leq 2\\\\sqrt{k}$ such that $F(u^*) = 0$ and for all $i$, $x_i$ it holds that $\\\\|\\\\sigma(w_i x_i + b_i)\\\\|_\\\\infty \\\\leq 2(n + 1)$. Using this, we get $\\\\|v^2\\\\|_2 \\\\leq B B \\\\sum_{i=1}^X \\\\|\\\\sigma(W(1) x_i, t + b(1))\\\\|_\\\\infty \\\\leq 2\\\\sqrt{r(n + 1)}$.\\n\\nNow we can apply the theorem of SGD with $M = 2\\\\sqrt{k}$ and $\\\\rho = 3\\\\sqrt{r n}$ and get that with probability greater than $1 - \\\\delta$ over the initialization and the first step, it holds that $E_{t \\\\in \\\\{2, \\\\ldots, T\\\\}} \\\\min l(f(x; \\\\theta_t), y) \\\\leq E_{t \\\\in \\\\{1, \\\\ldots, T-1\\\\}} X_{t=2} l(f(x; \\\\theta_t), y) \\\\leq 0 + 6\\\\sqrt{r n} T^{-1} \\\\leq \\\\epsilon$.\\n\\nAnd finally because $m \\\\leq O((\\\\log(2s/\\\\delta) n^2/2(c + u)^2 \\\\Delta^2 s - c)$, we have $m \\\\leq O(1/\\\\lambda n^2 - s - n - c)$.\\n\\nB.2. Dynamics after spurious feature has been learned\\n\\nLemma B.3. Suppose a model only has access to the spurious coordinates $x_s$, then under cross entropy loss with distribution $D_\\\\lambda$, the Bayes optimal model output is $\\\\log(1 - \\\\lambda) f_s(x_s)$.\\n\\nProof. Let the bayes classifier be $g(x)$. The bayes classifier is defined as $\\\\inf g(x) E_{x,y} [l(g(x), y)]$. Here $\\\\inf g(x) L(g(x)) = E_{x,y} [l(g(x), y)] = E_{x} E_{y|x} [l(g(x_s), y)] = E_{x} P_{f_s(x_s)} = f_c(x_c) | x_s l(g(x_s)) + P_{f_s(x_s)} = \\\\neq f_c(x_c) | x_s l(g(x_s)) = E_{x_s} [\\\\lambda(-\\\\log(\\\\phi(f_c(x_c) g(x_s)))) + (1 - \\\\lambda)(-\\\\log(\\\\phi(-f_s(x_s) g(x_s)))) = E_{x_s} [\\\\lambda(-\\\\log(\\\\phi(f_s(x_s) g(x_s)))) + (1 - \\\\lambda)(-\\\\log(\\\\phi(-f_s(x_s) g(x_s)))) = E_{x_s} [\\\\phi(f_s(x_s) g(x_s)) - \\\\lambda = 0].$\\n\\nSolving the equation, we get $g(x_s) = \\\\log(1 - \\\\lambda) f_s(x_s)$.\\n\\nLemma B.4. Suppose a neural network can be decomposed to the sum of two models $h_s = \\\\sum_{i \\\\in S} a_i \\\\sigma(w_i x)$, $h_c = \\\\sum_{i \\\\in C} a_i \\\\sigma(w_i x)$ which learns different features of the data. The learning process of $h_c$ then will be slowed down by $4\\\\lambda(1 - \\\\lambda)$ when compared to the gradient where there is no spurious correlation.\\n\\nProof. Let the model be $m(x) = h_s(x_s) + h(x)$. From Lemma 2, we have $\\\\phi(f_s(x_s) h_s(x_s)) = \\\\lambda$. Suppose $h(x) = 22$. \\n\\n\"}"}
{"id": "0tuwdgBiSN", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While the majority of networks evaluated in our primary empirical findings are relatively compact, we trained a substantial\\n\\nWe compare the gradient toward $h_i$ optimization parameters is listed in table 2. We adopt the parameters based on (LaBonte et al., 2023; Sagawa et al., 2020a; either from scratch with randomly initialized weights or finetuned with IMAGENET1K_V1).\\n\\nAdopt the ResNet-50 and add an additional linear layer on top of it based on the number of class label. The model is trained experiments, without further specification, we use a 2-layer, 100 neurons NN. For the Domino and Waterbird datasets, we\\n\\nNeural networks were initialized using a uniform distribution (as by the default setting of pytorch). For all the boolean\\n\\nQuadro RTX 8000 GPUs, cumulatively consuming around 2,500 GPU hours.\\n\\nnumber of models to validate the breadth of the \\\"robust space\\\" outcomes. These experiments utilized NVIDIA T4 and\\n\\nModel and Default Hyper-Parameters\\n\\nAll training experiments were conducted using PyTorch (Paszke et al., 2019). We first show the training procedure we adopt.\\n\\nIn this section, we conduct a more detailed experiment report and discussion of the results and claims presented in the main\\n\\nC.1. Detailed Experiments configuration\\n\\nC. Additional Experiments\\n\\nProof.\\n\\nWe have for any weight on a core coordinate $h_i$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\frac{\\\\partial E}{\\\\partial w_i} = \\\\lambda E_i w_i - \\\\lambda E_i w_i \\\\sigma h_i\\n\\n\\\\lambda \\\\in \\\\text{same}\\\\[log(\\\\text{same}_f(x_s)) + 1 - \\\\text{unif}_f(x_s)]$\\n\\n$s_i \\\\in \\\\{\\\\text{same}_f(x_s)\\\\}$\\n\\n$\\\\"}
{"id": "0tuwdgBiSN", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nIdrissi et al., 2022; Kirichenko et al., 2023; Izmailov et al., 2022) with minor modifications to ensure consistency across different experiments. Notably, for different boolean datasets and domino datasets we control the number of gradient updates of one epoch to be the same. This ensure us to compare them in a consistent manner. We provide further experiments to check how different hyper-parameters choices affect learning dynamics on different datasets fig. 9, fig. 10 and fig. 8.\\n\\nTable 1. Default Training Hyper-Parameters Across Datasets\\n\\n| Dataset   | \\\\(\\\\eta\\\\) | \\\\(\\\\beta\\\\) | Optimizer | Weight Decay | \\\\(B\\\\) | Method     | Sample per Epoch |\\n|-----------|----------|-----------|-----------|--------------|------|-----------|-----------------|\\n| boolean   | 0.0001   | 0.5       | SGD       | 0            | 64   | N/A       | 10000           |\\n| domino   | 0.001    | 0.9       | Adam      |              |      | N/A       | 5000            |\\n| waterbirds| 0.003    | 0.5       | SGD       | 0.0001       | 32   | reweight  | 500             |\\n| celebA   | 0.001    | 0.5       | SGD       | 0.0001       | 100  | resample  | dataset size    |\\n| cmnist   | 0.003    | 0.5       | SGD       | 0.0001       | 32   | N/A       | dataset size    |\\n| multinli | 0.00001  | N/A       | AdamW     | 0.0001       | 16   | resample  | dataset size    |\\n| civilcomment | 0.00001 | N/A       | AdamW     | 0.0001       | 16   | resample  | dataset size    |\\n\\nTable 2. The optimization hyperparameters for various datasets are outlined in the table with \\\\(\\\\eta\\\\): learning rate, \\\\(\\\\beta\\\\): momentum, \\\\(B\\\\): batch size. There are three possible methods for \u201cLabel Class Reweight.\u201d For class balanced dataset, such as the Domino, CMnist, and boolean datasets, no class balancing technique is necessary. In cases of class imbalance, we employ either resampling, which ensures each batch contains an equal number of samples from each class, or reweighting, which assigns greater importance or weight to samples from the minority class, as discussed in (Idrissi et al., 2022). Should there be any deviations from these default parameters in specific experiments, such changes will be explicitly noted.\\n\\nTraining and Evaluation Procedure\\n\\nAs specified in the main paper, we mainly use correlation and decoded correlation to assess the state of the model. For datasets that has more than two labels or spurious classes (MultiNli, CivilComments, CMnist), we use a generalized metrics of correlation \\\\(\\\\text{corr}(f_c, h) = P_x[f_c(x) = h(x)] - P_x[f_c(x) \\\\neq h(x)]\\\\) where \\\\(f_c\\\\) is the core/ground truth label and \\\\(h(x)\\\\) represent the prediction made by the trained model.\\n\\nThe decoded correlation is calculated in the following procedure:\\n\\n1. At the end of each epoch, we select \\\\(\\\\min(2000, \\\\lceil n/2 \\\\rceil)\\\\) samples from the group balanced validation dataset and use the model to output their embeddings, where \\\\(n\\\\) is the size of the group balanced dataset.\\n2. We then fit a logistic regression model on the output embeddings with default hyperparameters provided by the Scikit-learn package (Pedregosa et al., 2011).\\n3. Finally, we take another \\\\(\\\\min(2000, \\\\lceil n/2 \\\\rceil)\\\\) samples from the validation dataset and output their embeddings. We then evaluate the correlation of the trained logistic regression model on the output embeddings. We note that the default hyperparameters in most cases could provide the near optimal correlation score, and adjusting the hyperparameters like the regularization term or strength only yields marginal improvement.\\n\\nC.2. Additional Experiments on the interplay between Confounder Strength and Complexity\\n\\nWe divide this section into two sections to provide a comprehensive review of the influence of the two factors, complexity and confounder strength on learning either on a online setting learning or a finite dataset.\\n\\nC.2.1. COMPLEXITY\\n\\nParity\\n\\nfig. 11, fig. 12 show the learning dynamics of parity functions under different \\\\(\\\\text{deg}(f_s)\\\\). Note that the variance between repeated experiments is significant when \\\\(\\\\lambda\\\\) and the complexity of the spurious function \\\\(\\\\text{deg}(f_s)\\\\) are both high. In the context of learning parity with finite datasets, numerous runs converging to a low core correlation value. For learning under finite dataset, it is worth highlighting that the end performance of the network is heavily influenced by the randomness of initialization. Note here the total length of the feature vector is fixed to 20 so the computational complexity in learning core parity function stay fixed if \\\\(\\\\lambda = 0\\\\) for each case.\"}"}
{"id": "0tuwdgBiSN", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Complexity Matters: Feature Learning in the Presence of Spurious Correlations\\n\\nFigure 8. Learning dynamics on Spurious Boolean Dataset under different weight decay/l2 regularization values.\\n\\nFigure 9. Learning dynamics on Spurious Boolean Dataset under different model depths.\\n\\nFigure 10. Learning dynamics on Waterbirds under different hyperparameters. \\\\( \\\\lambda = 0.95 \\\\). Model is pretrained ResNet. \\\\( \\\\text{lr} = 0.001 \\\\). Weight Decay is 0.0001 or 0.0, \\\\( \\\\beta \\\\) is Momentum.\"}"}
