{"id": "K9NTPRvVRI", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neighboring Perturbations of Knowledge Editing on Large Language Models\\n\\nA. Datasets\\n\\nA.1. Construction of False Answers\\n\\nFor the additivity proposed in this paper, given an edit \\\\( E = \\\\{s, r, O, o^*, p\\\\} \\\\) where \\\\( t(r(s)) = p \\\\), both the original correct answers and sampled false answers are utilized. The false answers were sampled in two settings: Hard and Random.\\n\\nFor the Hard setting, some objects establish direct relations with the new object \\\\( o^* \\\\) are selected. Specifically, denote the triples of Wikidata as \\\\( W(r) = \\\\{s, r, o\\\\} \\\\), we firstly seek entities in \\\\( W(r) \\\\) which have a link \\\\( r' \\\\) with \\\\( o^* \\\\) (could be the form of \\\\( (o^*, r', o^*) \\\\), \\\\( (*, r', o^*) \\\\)), then we collect them and choose the entities has same entity type with \\\\( o^* \\\\). For example, if \\\\( o^* \\\\) is a person, then the selected entities are persons, too. Finally, we sample some selected entities as the false answers. For the Random setting, we seek entities do not have links with \\\\( o^* \\\\), while other parts are the same. In this way, the false answers in the Hard setting are more semantically close to the new appended object, while false answers in the Random setting are semantically distant from the new object.\\n\\nA.2. Relations and Templates of Datasets\\n\\nTable 6 and 7 show some examples of relations and their templates.\\n\\nTable 6.\\n\\n| Relation (r)          | \\\\( T(r) \\\\)                              |\\n|----------------------|-----------------------------------------|\\n| producer             | \\n| &quot;{} is the producer of what products?&quot;, \\n| &quot;{} has produced many products such as&quot; |\\n| official language    | \\n| &quot;{} is the official language of&quot;   |\\n| illustration         | \\n| &quot;The illustrator {} has created illustrations&quot; |\\n| written              | \\n| &quot;{} who has written some books like&quot; |\\n| &quot;Which books have been written by {}?&quot; |\\n\\nTable 7.\\n\\n| Relation (r)          | \\\\( T(r) \\\\)                              |\\n|----------------------|-----------------------------------------|\\n| plays for            | \\n| &quot;For so long, {} has been at which clubs?&quot; |\\n| &quot;What clubs have hired {} as a player?&quot; |\\n| created              | \\n| &quot;{} was the individual responsible for creating&quot; |\\n| has won prize        | \\n| &quot;{} was the recipient of the prize&quot; |\\n| &quot;What prizes have been gained by {}?&quot; |\\n\\nB. Experimental Setup\\n\\nB.1. Baseline Methods\\n\\nFive popular model editing methods were selected as baselines including:\\n\\n- **FT** (Zhu et al., 2020): this method simply performed gradient descent on the edits to update model parameters. It fine-tuned one layer in the model with a norm constraint on weight changes to prevent overfitting. Since the original authors did not publish their code, we followed Meng et al. (2022) re-implementation in their study.\\n\\n- **KN** (Dai et al., 2022): it first selected neurons that were associated with knowledge expression via gradient-based attributions, and then modified MLP layer at the rows corresponding to those neurons by adding scaled embedding vectors.\\n\\n- **MEND** (Mitchell et al., 2022a): it learned a hypernetwork to produce weight updates by decomposing the fine-tuning gradients into rank-1 form.\\n\\n---\\n\\n3 https://github.com/EleutherAI/knowledge-neurons.\\n\\n4 https://github.com/eric-mitchell/mend\"}"}
{"id": "K9NTPRvVRI", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neighboring Perturbations of Knowledge Editing on Large Language Models\\n\\n\u2022 **ROME** (Meng et al., 2022): It first localized the factual knowledge at a specific layer in the transformer MLP modules, and then updated the knowledge by directly writing new key-value pairs in the MLP module.\\n\\n\u2022 **MEMIT** (Meng et al., 2023): It extended ROME to edit a large set of facts and updated a sequence of MLP layers to update knowledge.\\n\\nThe ability of these methods were assessed based on EasyEdit (Wang et al., 2023), an easy-to-use knowledge editing framework which integrates the released codes and hyperparameters from previous methods.\\n\\n### B.2. Hyperparameters for APP\\nTo set the hyperparameters, we additionally created a small validation set. Table 8 shows the details of hyperparameters set for different LLMs. Besides, the margin $M$ is set to 2.\\n\\n| Editor | GPT-2 XL (1.5B) | GPT-J (6B) | LLaMA-2 (7B) |\\n|--------|-----------------|-------------|--------------|\\n| $\\\\alpha$ | 0.2             | 0.2         | 0.2          |\\n| $\\\\beta$  | 0.5             | 0.3         | 0.6          |\\n| $\\\\gamma$ | 0.2             | 0.2         | 0.1          |\\n\\n### B.3. Examples of previous metrics\\nHere we provide an example to show what each previous metric means.\\n\\n**Efficacy**\\nGiven an editing fact ($s = \\\\text{Apple}$, $r = \\\\text{products}$, $O = \\\\{\\\\text{AirPods 3}, \\\\text{MacBook Air}, ..., \\\\text{iPhone 14}\\\\}$, $o^* = \\\\text{iPhone 15}$), if the edited model $F^*$ assigns a higher probability to the new object $o^*$ than the minimum probability in $O$ under the corresponding question $t(r)(s)$ \u201cWhat are the products of Apple?\u201d, then this new knowledge is appended effectively.\\n\\n**Generalization**\\nThe edited model is considered to have generalized successfully if it can recall the new knowledge with paraphrased prompt like \u201cWhat items does Apple produce?\u201d. That is, to determine whether the probability assigned by the edited model $F^*$ to the new object $o^*$ is higher than the minimum probability in $O$ under the paraphrased prompt.\\n\\n**Locality**\\nThe edited model should remain unchanged in response to prompts that are irrelevant or outside the scope of editing. For example, the answer to the question \u201cWhich company developed Windows?\u201d should still be \u201cMicrosoft\u201d.\\n\\n### B.4. Results of LLaMA-2 (13B)\\nThe results of LLaMA-2 (13B) are shown in Table 9. Compared with previous results, we can conclude that previous important conclusions are still valid, such as \u201cThe performance of memorizing the new target knowledge is good\u201d, \u201clarger LLMs suffer more serious perturbation\u201d and \u201cAPP significantly mitigates the perturbation\u201d.\\n\\n### B.5. Ablation study\\nFigure 5 shows other results of the ablation study in Section 7.6. As MEND could only use $L_1$ in APP, we only list the results of FT.\\n\\n### C. Examples of how APP is coupled with existing methods\\nHere we take the editing method ROME as an example.\"}"}
{"id": "K9NTPRvVRI", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 9. Evaluation results (%) on the PEAK-CF dataset with LLaMA-2 (13B).\\n\\n| Editor          | Additivity (hard) | Additivity (ran) |\\n|-----------------|-------------------|-----------------|\\n| Previous Additivity (hard) | Previous Additivity (ran) |\\n| ES              | \u2191                 | \u2191               |\\n| GS              | \u2191                 | GS             |\\n| LS              | \u2191                 | LS             |\\n| AFF             | \u2193                 | ANF            |\\n| ANF             | \u2193                 | ANF            |\\n| FT              | 99.66             | 84.46           |\\n| MEMIT           | 99.66             | 98.47           |\\n| ROME            | 100.00            | 93.50           |\\n| FT+APP          | 97.61             | 80.75           |\\n| MEMIT+APP       | 97.95             | 96.13           |\\n| ROME+APP        | 96.59             | 94.97           |\\n\\nC.1. Rank-One Model Editing (ROME)\\n\\nROME (Meng et al., 2022) applies causal mediation analysis (Pearl, 2001; Vig et al., 2020) to locate the MLP modules that store facts. An MLP module consists of two layers, where the first and second layers are denoted as $W_l^{\\\\text{fc}}$ and $W_l^{\\\\text{proj}}$, respectively. $W_l^{\\\\text{proj}}$ is considered as a linear associative memory and an editing area. A new fact is represented as a key-value pair $(k^*, v^*)$ (Geva et al., 2021), which can be inserted into the MLP module of the model by solving a constrained least-squares problem (Bau et al., 2020). All that remains is to choose the appropriate $k^*$ and $v^*$.\\n\\nDenote $L$ as an edited MLP layer, $F$ as the unedited model, and $e = (s, r, o^*)$ as the editing fact. The key $k^*$ is obtained by calculating the average representation of the last token of the subject $s$ outputted by $W_l^{\\\\text{fc}}$ as follows:\\n\\n$$k^* = \\\\frac{1}{N} \\\\sum_{j=1}^{N} k(x_j + s),$$\\n\\nwhere $k(x) = \\\\sigma(W_l^{\\\\text{fc}}\\\\gamma a(L-1)[x,i] + h(L-1)[x,i]).$ (11)\\n\\nHere, $x_j$ is the random sampled sequence and $i$ is the location of the last token of the subject in sentence $x$. $h$ is the hidden states, $a$ is the attention, $\\\\gamma$ is the layernorm, and $\\\\sigma$ is the activation function.\\n\\nThe value $v^*$ encodes the new knowledge $(r, o^*)$ as a property of $s$. To calculate it, ROME sets $v^* = \\\\arg\\\\min_z L_z$, where the objective is denoted as:\\n\\n$$L_e(o^*, z) = -\\\\log P_{F_m}(L_i := z|p) + D_{KL}P_{F_m}(L_i := z|x|p').$$ (12)\\n\\nThe first term in Eq. (12) seeks a vector $z$ that, when substituted as the output of the MLP at the token $i$ (notated $F_m(L_i := z)$ and here $i$ is the end of the subject in prompt $p$), will cause the network to predict the target object $o^*$ in response to the factual prompt (e.g., $p$ = Eiffel Tower is located in, $o^*$ = London). The second term minimizes the KL divergence (Ma et al., 2022) of predictions for the prompt $p'$ (of the form \\\"{} is a\\\") to the unchanged model, which helps preserve the model's understanding of the subject's essence ($i'$ is the location of the last token of the subject in $p'$).\"}"}
{"id": "K9NTPRvVRI", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Finally, the MLP weight $W_L$ is updated with a rank-one update to insert the new fact:\\n\\n$$\\\\hat{W} = W + \\\\Lambda C - 1 k^T k^*,$$  (13)\\n\\nwhere $C$ is a constant by estimating the covariance of many existing keys. $\\\\Lambda = (v^* - W k^*)/C - 1 k^* T k^*$, representing the residual error of the new key\u2013value pair on the original memory matrix. Readers can refer to Meng et al. (2022) for more details of ROME.\\n\\nC.2. Apply APP to ROME\\n\\nIn ROME, the editing objective is $L_e(o^*, z)$, and the parameter to be optimized is $z$ for $v^* = \\\\arg\\\\min z L_e(z)$. Therefore, the proposed APP could be coupled with $L_e(o^*, z)$ to optimize $z$.\\n\\nOn the one hand, the editing objective $L_1(O, O_h, z)$ designed to maintain a certain margin between the probabilities of original correct answers $O$ and those of false answers $O_h$ could be written as follows:\\n\\n$$L_1(O, O_h, z) = \\\\frac{1}{NM} \\\\sum_{i=1}^{N} \\\\max \\\\{0, M - \\\\log P_F(m_i := z(o_i|p)) + \\\\log P_F(m_i := z(o_i|p))\\\\},$$  (14)\\n\\nwhere $N$ and $M$ represent the number of elements in $O$ and $O_h$ respectively. This equation seeks a $z$ that the log probabilities of correct answers are encouraged to be larger than those of false answers by at least a certain margin $M$.\\n\\nOn the other hand, it involves ensuring that the absolute probabilities of correct answers do not decrease while simultaneously controlling that the probabilities of false answers do not increase during editing, which can be represented as two objectives $L_2(O, z)$ and $L_3(O_h, z)$ respectively as:\\n\\n$$L_2(O, z) = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\max \\\\{0, \\\\log P_F(o_i|p) - \\\\log P_F(m_i := z(o_i|p))\\\\},$$  (15)\\n\\n$$L_3(O_h, z) = \\\\frac{1}{M} \\\\sum_{i=1}^{M} \\\\max \\\\{0, \\\\log P_F(m_i := z(o_i|p)) - \\\\log P_F(o_{hi}|p)\\\\}.$$\\n\\n$P_F(o_i|p)$ refers the probability output by the unedited model. $P_F(m_i := z(o_i|p))$ refers the probability output by the model operated with the same operation in Eq. (12). These two equations respectively seek a $z$ that the log probabilities of correct answers are encouraged not to be decreased and the log probabilities of false answers not to be increased. Finally, these proposed objectives are jointly optimized with the editing objective $L_e(o^*, z)$ to obtain $v^*$:\\n\\n$$v^* = \\\\arg\\\\min z L_e(o^*, z) + \\\\alpha L_1(O, O_h, z) + \\\\beta L_2(O, z) + \\\\gamma L_3(O_h, z).$$  (17)\\n\\nFinally, the Eq. (13) is utilized to update parameters in model.\"}"}
{"id": "K9NTPRvVRI", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. An example in the PEAK-T dataset.\\n\\n\\\\[ a \\\\] and \\\\[ a^* \\\\] in different rows represent the original answer and desired answer after editing respectively. \\\\( \\\\cup \\\\) refers to the set union operation.\\n\\n\\\\[\\nE(s, r) = \\\\text{(Olympic Winter Games, host country)}\\n\\\\]\\n\\n\\\\[\\nO = \\\\{ \\\\text{France, United States, ..., South Korea} \\\\}\\n\\\\]\\n\\n\\\\[\\no^* = \\\\text{China}\\n\\\\]\\n\\n\\\\[\\np: \\\\text{What are the host countries of Olympic Winter Games? (} a : O, a^* : O \\\\cup \\\\{ o^* \\\\} \\\\)\\n\\\\]\\n\\n\\\\[\\n\\\\text{The host countries of Olympic Winter Games are Which nations have hosted the Winter Olympics? (} a : O, a^* : O \\\\cup \\\\{ o^* \\\\} \\\\)\\n\\\\]\\n\\n\\\\[\\nP_G \\\\text{ Big Ben is located in (} a \\\\& a^* : \\\\text{London})\\n\\\\]\\n\\n\\\\[\\nP_L \\\\text{ The headquarters of Apple is in (} a \\\\& a^* : \\\\text{California})\\n\\\\]\\n\\n\\\\[\\no_h = \\\\{ \\\\text{Japan, Shanghai, Hong Kong, ..., Russia} \\\\}\\n\\\\]\\n\\n\\\\[\\no_r = \\\\{ \\\\text{Uruguay, Mozambique, Fiji, ..., Tripoli} \\\\}\\n\\\\]\\n\\n\\\\[\\n\\\\text{ity, a set of triples outside the scope of editing is selected:} \\\\quad S = \\\\{(s', r', o')\\\\}\\n\\\\]\\n\\n\\\\[\\n\\\\text{For example, select (} s = \\\\text{Olympic Games, r = host city, o = Paris), \\\\quad S \\\\text{ might contain triple like (} s' = \\\\text{France, r' = shares border, o' = Germany})\\\\]\\n\\n\\\\[\\n\\\\text{A set of prompts} \\\\{P_G(s', r') | (s', r', o') \\\\in S\\\\} \\\\text{ is constructed to sample the locality prompts} P_L.\\n\\\\]\\n\\n\\\\[\\n\\\\text{Sampling false answers}\\n\\\\]\\n\\n\\\\[\\n\\\\text{For the additivity proposed in this paper, given a factual question} \\\\ t(r(s)) (\\\\text{e.g., the editing prompt} p), \\\\text{both the original correct answers and sampled false answers are utilized.} O \\\\text{ is the list of original correct answers, while the false answers were sampled in two settings Hard and Random (sampling details are in Appendix A.1). For the Hard setting, some objects that establish direct relations with the new object} o^* \\\\text{ are selected, represented as} \\\\quad O_h = \\\\{ o_{h1}, o_{h2}, ..., o_{hM} \\\\} \\\\quad \\\\text{where} \\\\quad O_h \\\\cap O = \\\\emptyset \\\\]\\n\\n\\\\[\\n\\\\text{Similarly, for the Random setting, some objects} \\\\quad O_r = \\\\{ o_{r1}, o_{r2}, ..., o_{rM} \\\\} \\\\text{ that are semantically distant from the new object are sampled. Intuitively, false answers in the Hard setting are more easily introduced into the model compared to answers in the Random setting.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Filtering correct and false answers}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Subsequently, according to additivity defined in Section 4, for} \\\\ t(r(s)) \\\\text{, the minimum probability of correct answers} O \\\\text{ should be larger than the maximum probability of false answers} O_h \\\\text{ and} \\\\ O_r \\\\text{ before and after editing. Therefore, before editing, we filtered out correct answers with probabilities below a certain threshold in the original model. Then false answers with probabilities greater than the minimum probability of the selected correct answers were also filtered out.}\\n\\\\]\\n\\n5.2. Data Construction of PEAK-T\\n\\nThis dataset focuses on temporal-based, real-world edits, where each edit is factually correct and occurs after the model is trained. Following Yin et al. (2023), it is built upon YAGO (Mahdisoltani et al., 2015), a knowledge base containing fact triples extracted from Wikipedia, enriched with WordNet, GeoNames, and other data sources, and contains the time when the facts occurred. We sampled facts in YAGO that occurred after 2021, which took place following the release of GPT-2 XL and GPT-J. Due to the limited number of relation types, 9 relations have been selected. Then the construction is similar to PEAK-CF.\\n\\n5.3. Dataset Summary\\n\\nDataset Format\\n\\nAs shown in Table 1, each instance in the PEAK benchmark is represented as a tuple \\\\(E, P_G, P_L, O_h, O_r\\\\), where \\\\(E\\\\) comprises a piece of new knowledge and original correct knowledge. \\\\(P_G\\\\) and \\\\(P_L\\\\) are the prompts utilized to validate generalization and locality respectively. \\\\(O_h\\\\) and \\\\(O_r\\\\) represent the sampled false answer lists for the factual questions \\\\(p\\\\) and \\\\(P_G\\\\) under the Hard and Random settings respectively. \\\\(a\\\\) and \\\\(a^*\\\\) denote the original answer and desired answer after editing respectively.\\n\\nDataset Statistics\\n\\nTable 2 summarizes the statistics of the PEAK-CF and PEAK-T datasets. To verify generalization and locality, there is at least one prompt for each instance. Besides, each fact question has about an average of 10 correct and 20 false answers. Generally speaking, these two datasets contain counterfactual edits and temporal edits respectively, and are used to study neighborhood perturbations of edited models. Readers can refer to Appendix A.2 for the details of relations and prompts.\\n\\n6. Appending via Preservation and Prevention\\n\\nThe neighboring perturbations in the process of appending knowledge may lead to the forgetting of original correct knowledge, as well as the unintentional inclusion of noise. A plug-and-play framework \\\\(\\\\text{APP (Appending via Preservation and Prevention)}\\\\) is proposed to improve existing editing methods to mitigate this detriment in editing.\\n\\nGiven the new knowledge to append \\\\((s, r, o^*)\\\\) and an editing prompt \\\\(p\\\\), current editing methods usually define an editing objective \\\\(L(e(o^*, \\\\theta_F))\\\\) to incorporate the new knowledge, where \\\\(\\\\theta_F\\\\) denotes the parameters to be updated in the unedited model \\\\(F\\\\). \\\\(\\\\text{APP}\\\\) designs a set of editing objectives which can be coupled with \\\\(L(e(o^*, \\\\theta_F))\\\\), to minimize the probability perturbations of both neighboring correct and\"}"}
{"id": "K9NTPRvVRI", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Evaluation results (%) of the PEAK-CF dataset. \u201chard\u201d refers to the additivity calculated using false answers in the Hard setting, while \u201cran\u201d refers to that in the Random setting. \u2191 indicates that a higher value corresponds to better performance, while \u2193 is the opposite. Numbers marked with \u2020 indicate statistically significant improvements in the method coupled with APP over the original method (t-test with p-value < 0.05). Since KN has no loss to optimize, APP is not applied to it. Due to space limitation, the results of LLaMA-2 (13B) were put in Appendix B.4.\\n\\n| Editor      | Previous Additivity (hard) | Additivity (ran) | Previous Additivity (hard) | Additivity (ran) |\\n|-------------|----------------------------|------------------|----------------------------|------------------|\\n| GPT-2 XL (1.5B) | FT 90.99                  | 64.68            | FT+APP 87.89               | 59.30            |\\n| LLaMA-2 (7B)   | KN 26.48                   | 26.08            | KN+APP 46.67               | 46.45            |\\n| LLaMA-2 (13B)  | MEND 97.46                 | 84.87            | MEND+APP 95.22             | 86.02            |\\n|               | MEMIT 72.90                | 63.08            | MEMIT+APP 99.95            | 99.11            |\\n|               | ROME 99.27                 | 94.05            | ROME+APP 99.69             | 97.74            |\\n\\nEditor GPT-2 XL (1.5B) LLaMA-2 (7B) LLaMA-2 (13B) FT 90.99 64.68 87.71 78.86 59.04 69.29 37.12 98.27 87.72 68.48 74.51 60.62 64.22 38.72 KN 26.48 26.08 89.08 44.71 29.47 41.23 22.96 46.67 46.45 66.16 45.63 40.51 43.72 35.64 MEND 97.46 84.87 42.46 30.15 33.97 12.86 11.62 95.22 86.02 47.47 35.39 33.51 21.16 14.61 MEMIT 72.90 63.08 98.44 51.49 52.11 31.71 17.22 99.95 99.11 92.12 94.26 84.14 86.03 53.61 ROME 99.27 94.05 90.57 87.44 69.80 75.16 35.58 99.69 97.74 94.40 93.05 82.47 83.80 52.25 FT+APP 87.89 59.30 89.21 70.48 49.73 59.85 29.76 98.01 85.24 73.55 68.44 49.62 57.93 33.34 MEND+APP 94.25 81.02 45.51 26.27 28.81 12.21 11.07 92.86 82.46 51.43 32.55 29.73 20.44 13.99 MEMIT+APP 69.75 59.26 98.66 39.89 43.41 22.14 13.58 99.42 96.51 94.81 38.83 17.41 32.65 11.69 ROME+APP 96.15 87.17 93.27 40.45 31.60 25.92 12.61 100.00 94.24 96.22 43.11 20.14 36.86 13.74\"}"}
{"id": "K9NTPRvVRI", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4. Evaluation results (%) of the PEAK-T dataset.\\n\\n| Editor      | GPT-2 XL (1.5B) | GPT-J (6B) |\\n|-------------|-----------------|------------|\\n|             | Previous Additivity (hard) | Additivity (ran) |\\n| ES          | \u2191 GS            | \u2191 LS       |\\n| AFF         | \u2193 ANF           | \u2193 AFF      |\\n\\nFT 83.31 59.89 61.67 80.44 38.10 72.24 18.76 95.21 80.68 42.74 87.25 40.14 77.09 14.97\\n\\nKN 28.39 27.38 83.81 50.85 24.41 47.29 18.73 45.49 43.11 67.05 93.84 46.47 93.29 40.23\\n\\nMEND 99.00 89.90 8.78 24.02 19.78 12.91 8.18 99.72 87.55 10.05 24.73 16.82 13.66 5.12\\n\\nMEMIT 57.92 46.46 97.71 25.92 31.45 7.31 4.19 100.00 85.42 94.00 50.18 36.35 22.85 2.95\\n\\nROME 100.00 86.77 92.95 57.65 33.84 36.05 5.35 100.00 92.57 89.15 60.48 38.66 33.87 4.02\\n\\nFT+APP 79.03 53.19 67.29 75.24 31.64 67.47 17.75 90.85 74.50 49.53 82.75 31.06 75.02 13.41\\n\\nMEND+APP 99.29 90.68 8.37 26.27 28.81 12.21 11.07 99.72 87.49 9.93 26.38 17.05 14.85 5.12\\n\\nMEMIT+APP 59.06 47.68 97.92 23.30 29.00 6.86 4.20 100.00 83.23 94.36 21.40 16.78 17.15 1.93\\n\\nROME+APP 99.86 86.70 93.65 28.81 14.84 16.79 3.16 100.00 90.83 89.90 31.56 17.15 17.17 2.17\\n\\n7.2. Evaluation Metrics\\n\\nThree basic metrics of Efficacy (ES), Generalization (GS) and Locality (LS) (Meng et al., 2022; 2023) defined in Section 3.2 were still adopted to measure each editing method. Besides, the proposed Additivity was to evaluate the degree of perturbation to neighboring knowledge. AFF and ANF designed in Section 4 were employed to quantify the extent of forgetting original correct answers and the inclusion of noise and incorrect answers respectively. When assessing the additivity, both the editing and paraphrase prompts \\\\( \\\\{ p \\\\} \\\\cup P \\\\) were utilized without loss of generality. For all the above metrics, the average results across all edits in each dataset were reported.\\n\\n7.3. Results of Existing Editing Methods\\n\\nThe top five rows of Table 3 and Table 4 reported the knowledge editing results of current editing methods on PEAK-CF and PEAK-T respectively. These results were analyzed from the following perspectives.\\n\\nThe performance of editing the new target knowledge. Except for KN, editing methods performed well in efficacy (ES) and generalization (GS), showing that most of existing editing methods are capable of effectively appending new target knowledge. For locality, locate-then-edit methods (KN, ROME, MEMIT) significantly outperformed other methods, demonstrating that they have little interference with irrelevant knowledge. Moreover, as the model size increased, the performance of a particular editing method exhibited continuous improvement in appending new facts.\\n\\nThe perturbations of editing on neighboring knowledge. We were surprised to observe that existing editing methods significantly perturbed the knowledge neighboring to the target knowledge in LLMs after editing, compromising the integrity of original correct knowledge and introducing unintentional noise. Taking LLaMA-2 edited by ROME on PEAK-CF as an example, despite its superior performance evaluated by previous metrics, it achieved remarkably poor performance of 93.05% AFF and 82.47% ANF respectively in the Hard negative setting. Even in the Random setting, they still show poor performance, indicating severe damage to neighboring knowledge. Meanwhile, edited models generally showed worse performance under the Hard setting than those under the Random setting in terms of AFF and ANF, demonstrating that the closer the semantic relationship between the false answer and the newly appended answer, the more susceptible it is to perturbation. Remarkably, while MEND performed the best in terms of additivity, its relatively low LS indicates a significant disruption to irrelevant knowledge. Our findings suggest that current editing methods primarily care about the editing performance of new target knowledge, while neglecting the negative perturbations on its neighboring knowledge.\\n\\nComparison between datasets. Comparing the results of each method on the two datasets, two conclusions were drawn. (1) The ES and GS of each method on PEAK-T were lower than those on PEAK-CF, showing that the PEAK-T is more challenging for existing methods to append knowledge without neighboring perturbations. This could be attributed to the fact that, for PEAK-T, new knowledge originates from the external world and has never been seen by the model. In contrast, PEAK-CF already contains some weak counterfactual associations, making it easier for the model to integrate counterfacts. (2) Both AFF and ANF on PEAK-T were lower than those on PEAK-CF, suggesting that PEAK-T suffers fewer neighboring perturbations during editing than PEAK-CF. This is possibly due to the weaker correlation between the sampled false answers and the new external knowledge compared to the original counterfactual knowledge in the model. Consequently, the perturbations when appending new knowledge in PEAK-T are smaller.\\n\\n7.4. Results of APP\\n\\nAs shown in the bottom four rows in Table 3 and Table 4, APP was coupled with four editing methods. The false answers utilized in APP were from the Hard setting.\"}"}
{"id": "K9NTPRvVRI", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2.\\nAPP effectively mitigates the probability perturbations.\\n\\nTo further analyze the neighboring perturbations in edited text, indicating that the false answers more closely tied to the new appended answers warrant greater attention. Compared to the false answers (random), the probabilities of correct answers dropped significantly, while the probability of both correct and false answers in LLaMA-2 models and the mechanism of our proposed APP, the average perturbations in terms of AFF and ANF under both the hard and random settings. Particularly, ROME+APP and MEND have shown minimal disturbances of false answers (random) of LLaMA-2 after editing. LLaMA-2 refers to the extended to utilize the entire set of correct answers and those of false answers.\\n\\nAblation analysis of probability and additivity for APP. Figure 3.\\nAblation tests of removing each editing objective L, indicating that APP performs better at maintaining the integrity of the answer list. The proposed method has access to all the correct answers during editing, whereas the baseline methods can only have access to the new correct answer (hard) or (random) answers were illustrated when editing LLMs to ensure a fair comparison. The results are shown in Table 5.\\n\\nTable 3, we observe that, Despite similar performance in appending new knowledge, the additivity of the new baselines improved in APP were conducted to validate their effectiveness. In APP, which is designed to explain why APP can effectively mitigate perturbations, the inclusion of false knowledge thereby preserving the correct knowledge and preventing the most importance of L, their effectiveness. Second, removing the editing objective of the APP led to performance degradation in terms of additivity and probability perturbations, showing the most significant performance degradation. It demonstrated the importance of the proposed neighboring perturbations. Addressing this challenge will necessitate collaborative efforts from the community.\\n\\n7.5. Probability Change of Answers\\n\\nNeighboring Perturbations of Knowledge Editing on Large Language Models\\n\\nFigure 4 illustrates how the performance of APP changed with respect to different numbers of neighboring answers when editing. Specifically, all baseline methods have been limited to a page limit, results for other methods are put in Appendix B.5. Results were conducted with LLaMA-2 on PEAK-CF dataset. Due to the page limit, results for other methods are put in Appendix B.5. Existing editing methods severely perturb probabilities.\\n\\n7.6. Ablation study\\n\\nAblation study of probability and additivity for APP. Table 3, we observe that, Despite similar performance in appending new knowledge, the additivity of the new baselines improved in APP were conducted to validate their effectiveness. In APP, which is designed to explain why APP can effectively mitigate perturbations, the inclusion of false knowledge thereby preserving the correct knowledge and preventing the most importance of L, their effectiveness. Second, removing the editing objective of the APP led to performance degradation in terms of additivity and probability perturbations, showing the most significant performance degradation. It demonstrated the importance of the proposed neighboring perturbations. Addressing this challenge will necessitate collaborative efforts from the community.\"}"}
{"id": "K9NTPRvVRI", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nDespite their exceptional capabilities, large language models (LLMs) are prone to generating unintended text due to false or outdated knowledge. Given the resource-intensive nature of retraining LLMs, there has been a notable increase in the development of knowledge editing. However, current approaches and evaluations rarely explore the perturbation of editing on neighboring knowledge. This paper studies whether updating new knowledge to LLMs perturbs the neighboring knowledge encapsulated within them. Specifically, we seek to figure out whether appending a new answer into an answer list to a factual question leads to catastrophic forgetting of original correct answers in this list, as well as unintentional inclusion of incorrect answers. A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge. Besides, a plug-and-play framework termed Appending via Preservation and Prevention (APP) is proposed to mitigate the neighboring perturbation by maintaining the integrity of the answer list. Experiments demonstrate the effectiveness of APP coupling with four editing methods on four LLMs. The code and data are available at https://github.com/mjy1111/PEAK.\\n\\n1. Introduction\\n\\nLarge language models (LLMs) such as GPT-4 (OpenAI, 2023) have demonstrated remarkable capabilities in Natural Language Processing (NLP) (Ouyang et al., 2022; Zhang et al., 2023a; Touvron et al., 2023). Nonetheless, current LLMs often inevitably exhibit hallucinations stemming from outdated or erroneous knowledge within their parameters (Zhang et al., 2023b; Peng et al., 2023; Ji et al., 2023; Huang et al., 2023). Given that retraining LLMs is both costly and time-consuming, there has been a surge in research focused on knowledge editing (a.k.a., model editing) (Sinitsin et al., 2020; Zhu et al., 2020; Cao et al., 2021; Meng et al., 2022; 2023; Dai et al., 2022; Mitchell et al., 2022b; Yao et al., 2023; Zhong et al., 2023; Cohen et al., 2023; Ma et al., 2023; Gu et al., 2024), which aims at efficiently altering LLMs' behaviors within specific domains while preserving overall performance across various inputs.\\n\\nMany researchers endeavor to develop editing methods to modify the parameters of models, which can be generally classified into two categories of Meta-Learning and Locate-then-Edit (Wang et al., 2023; Yao et al., 2023; Zhang et al., 2024). Numerous benchmarks have been designed to assess these methods across various dimensions, the most fundamental of which are Efficacy, Generalization, and Locality (Meng et al., 2022; Zhong et al., 2023; Cohen et al., 2023; Ma et al., 2023). These approaches and benchmarks primarily focus on determining if the new target knowledge has been successfully memorized. However, the perturbations of editing on knowledge neighboring to the new target...\"}"}
{"id": "K9NTPRvVRI", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neighboring Perturbations of Knowledge Editing on Large Language Models\\n\\nIn this study, we particularly investigate whether the editing operation of appending a new answer into an answer list to a factual question perturbs the neighboring knowledge encapsulated within them, as illustrated in Figure 1. Specifically, we seek to figure out whether the knowledge appending leads to catastrophic forgetting of original correct answers in this answer list, as well as the unintentional inclusion of incorrect answers. In addition to the metrics that are commonly used in previous works, a new metric of additivity is introduced to assess the degree of perturbation to neighboring knowledge when appending. To evaluate the additivity of edited models, a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed, with each example comprising a factual question, a list of original correct answers, a list of incorrect distractors, and a piece of knowledge to append. After an editing operation of knowledge appending, the list of original correct answers is utilized to calculate the proportion of them that are accidentally removed. Similarly, the list of incorrect distractors is utilized to calculate the proportion of them that are unintentionally included. PEAK includes two datasets of PEAK-CF based on counterfactual edits, and of PEAK-T based on temporal knowledge edits of changes in real-world.\\n\\nTo mitigate the neighboring perturbations after appending new knowledge, a plug-and-play framework named Appending via Preservation and Prevention (APP) is proposed. A set of editing objectives is designed to minimize the probability perturbations of both correct and incorrect knowledge. On the one hand, a certain margin is maintained between the probability of correct knowledge and that of incorrect knowledge. On the other hand, it involves ensuring that the probability of correct knowledge does not decrease while controlling that the probability of incorrect knowledge does not increase. In this way, this framework helps preserve the integrity of original correct knowledge and prevent the inclusion of false knowledge while appending new knowledge. Furthermore, the proposed editing framework eliminates the need for a training step, and is adaptable to be coupled with multiple existing editing methods.\\n\\nTo investigate the performance of current knowledge editing methods for appending knowledge, this study conducts comprehensive experiments that encompass a wide range of methods, including FT (Zhu et al., 2020), MEND (Mitchell et al., 2022a), KN (Dai et al., 2022), ROME (Meng et al., 2022) and MEMIT (Meng et al., 2023). Four representative LLMs of varying sizes are utilized as our foundational models, including GPT-2 XL (1.5B) (Radford et al., 2019), GPT-J (6B) (Wang & Komatsuzaki, 2021), LLaMA-2 (7B) and LLaMA-2 (13B) (Touvron et al., 2023). We surprisingly observe that although current editing methods can effectively incorporate new facts, they significantly undermine the probability distribution of knowledge neighboring to the new facts, disrupting the integrity of original correct knowledge and introducing unintentional noise. Furthermore, experimental results extensively demonstrate the effectiveness of the proposed APP in mitigating the neighboring perturbations of editing, as well as its compatibility with four editing methods on four representative LLMs of different sizes.\\n\\nIn essence, our research offers three significant contributions: (1) This study pioneers the exploration of neighboring perturbations via appending new knowledge to LLMs. A metric of additivity is introduced and a new benchmark PEAK is constructed to gauge the degree of perturbation to neighboring knowledge. (2) Through comprehensive experiments, we observe that although established methods and LLMs absorb new knowledge effectively, they seriously disrupt the integrity of original correct knowledge and introduce unintentional false knowledge. (3) The plug-and-play APP is proposed which is effective in mitigating neighboring perturbations when appending knowledge. We aspire that both our benchmark and method can shed light on the neighboring perturbations of knowledge editing on LLMs.\\n\\n2. Related Work\\n\\nMany knowledge editing methods have been proposed to modify knowledge encoded in model, such as meta-learning, and locate-then-edit (Yao et al., 2023). On the one hand, meta-learning methods train a hypernetwork to get gradient changes to update model parameters (Cao et al., 2021; Mitchell et al., 2022a). MEND (Mitchell et al., 2022a) learns to transform the fine-tuning gradient into a low-rank decomposition of the gradient. On the other hand, locate-then-edit methods first locate knowledge neurons in LLMs that exhibit a positive correlation with a knowledge expression, and then modify them accordingly (Dai et al., 2022; Meng et al., 2022; 2023). Dai et al. (2022) computed the contribution of each neurons to a certain knowledge, then updated or erased knowledge by modifying these neurons with the embedding vectors of facts. Meng et al. (2022) located multi-layer perceptron (MLP) storing factual knowledge, and then edited such knowledge by injecting new key-value pair in the MLP module, which follows recent observations that these layers can be cast as key-value memories that store factual knowledge (Geva et al., 2021; 2022). In addition, Zhu et al. (2020) also propose the constrained fine-tuning approach on modified facts. DeepEdit (Wang et al., 2024) is neuro-symbolic that decodes with constraints and can be flexibly applied to black-box LLMs. Besides, some benchmarks are proposed for assessing knowledge editing (Meng et al., 2022; Cohen et al., 2023; Ma et al., 2023).\"}"}
{"id": "K9NTPRvVRI", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neighboring Perturbations of Knowledge Editing on Large Language Models\\n\\nFor example, MQUAKE (Zhong et al., 2023) uses the multi-hop questions to assess knowledge editing, suggesting that editing a particular fact implies that many other facts need to be updated. BAKE (Ma et al., 2023) assesses knowledge editing bidirectionally. Li et al. (2023) have explored two significant areas of concern: Knowledge Conflict and Knowledge Distortion, which aims to unveil more weakness in knowledge editing. Additionally, editing has been applied in various domains, such as changing model personality (Mao et al., 2023), editing multimodal models (Cheng et al., 2023), protecting users privacy (Wu et al., 2023), etc.\\n\\nCompared with previous studies (Meng et al., 2022; 2023; Mitchell et al., 2022a; Zhong et al., 2023; Cohen et al., 2023; Ma et al., 2023; Yao et al., 2023) that are the most relevant to our work, a main difference should be highlighted. These approaches generally focus on determining if the new target knowledge has been successfully memorized, while this study explores the perturbation and impact of editing on neighboring knowledge. To the best of our knowledge, this paper makes the first attempt to introduce a new metric of additivity, build a benchmark for assessing the degree of perturbation to neighboring knowledge when appending knowledge, and propose a general framework to mitigate the neighboring perturbations.\\n\\n3. Preliminary\\n\\n3.1. Querying Factual Knowledge in LLMs\\n\\nFollowing previous works (Meng et al., 2022; Zhong et al., 2023; Yin et al., 2023; Yao et al., 2023; Ma et al., 2023), we study factual knowledge of the form $(s, r, o)$, consisting of a subject $s$, a relation $r$, and an object $o$ (e.g., $s$ = Eiffel Tower, $r$ = location, $o$ = Paris). Besides, we also follow them to employ discrete prompts to test whether the knowledge is in a language model. Specifically, a natural language template $t_r(\\\\cdot)$ is constructed for each relation $r$, which is then combined with a subject $s$ as input to generate a prompt (question or cloze-style statement) $t_r(s)$. For instance, given the subject \u201cEiffel Tower\u201d and the relation \u201clocation\u201d, we can form a cloze sentence \u201cThe Eiffel Tower is located in\u201d.\\n\\n3.2. Knowledge Editing\\n\\nThe goal of knowledge editing is to incorporate new facts into model parameters without retraining. Previous works focus on altering the original knowledge stored in LLMs as $(s, r, o) \\\\rightarrow (s, r, o^*)$, which only cares about the new target knowledge. However, the potential impact of the editing operation of appending new knowledge on its neighboring knowledge remains unclear. Therefore, this paper explores the scenarios where a subject has multiple corresponding objects under a given relation. In detail, appending knowledge aims at incorporating a new object to the set of original objects, retaining original objects, and not introducing false objects:\\n\\n$$(s, r, \\\\{o_1, o_2, ..., o_N\\\\}) \\\\rightarrow (s, r, \\\\{o_1, o_2, ..., o_N, o^*\\\\})$$\\n\\nIn this paper, for a new piece of knowledge to append $(s, r, o^*)$, its neighboring knowledge is defined as $(s, r, O)$ where $O = \\\\{o_1, o_2, ..., o_N\\\\}$. Define an editing fact as $e = (s, r, O, o^*)$ and given an unedited model $F$, the edited language model is gained by using a model editor $K$, $F^* = K(F, e)$.\\n\\nTo evaluate the performance of editing methods in appending knowledge, the previous three metrics are still utilized. Efficacy validates whether the edited models could recall the appended fact under the sole editing prompt $p$. The assessment is based on Efficacy Score (ES) representing as:\\n\\n$$1 \\\\left[ P_{F^*}(o^* | p) > \\\\min \\\\{ P_{F^*}(o | p) | o \\\\in O \\\\} \\\\right]$$\\n\\nGeneralization verifies whether the edited models could recall the appending fact under the paraphrase prompts $p_G$ via Generalization Score (GS):\\n\\n$$E_{p \\\\in P_G} \\\\left[ 1 \\\\left[ P_{F^*}(o^* | p) > \\\\min \\\\{ P_{F^*}(o | p) | o \\\\in O \\\\} \\\\right] \\\\right]$$\\n\\nLocality verifies whether the output of the edited models for inputs out of editing scope remains unchanged under the locality prompts $p_L$ via Locality Score (LS):\\n\\n$$E_{p_l \\\\in P_L} \\\\left[ 1 \\\\left[ P_{F}(o_l | p_l) > P_{F^*}(o^* | p_l) \\\\right] \\\\right],$$\\n\\nwhere $o_l$ was the original answer of $p_l$. $1$ is the indicator function.\\n\\nFor example, given an editing fact $(s = Apple, r = products, O = \\\\{AirPods 3, MacBook Air, ..., iPhone 14\\\\}, o^* = iPhone 15)$, the editing prompt $p$ could be \u201cWhat are the products of Apple?\u201d and a paraphrase prompt could be \u201cWhat items does Apple produce?\u201d A locality prompt and its original answer could be \u201cWhich company developed Windows?\u201d and \u201cMicrosoft\u201d respectively.\\n\\n4. Definition of Additivity\\n\\nGiven a subject $s$, a relation $r$, a composed question $t_r(s)$, original objects $O$, and an object to append $o^*$, it is unclear whether the original correct knowledge $K_c = (s, r, \\\\{o_1, o_2, ..., o_N\\\\})$ is still retained, and part of the false knowledge $K_f = (s, r, \\\\{o_{f1}, o_{f2}, ..., o_{fM}\\\\})$ is unintentionally included after editing, where $\\\\{o_{f1}, o_{f2}, ..., o_{fM}\\\\} \\\\cap \\\\{o_1, o_2, ..., o_N, o^*\\\\} = \\\\emptyset$. Therefore, a new metric termed additivity is designed to evaluate the degree of perturbation to neighboring knowledge, which is calculated in terms of relative ranking and absolute probability change of objects.\\n\\nRelative ranking of objects\\n\\nWe assume that the minimum probability of correct knowledge $K_c$ should be larger than the maximum probability of false knowledge $K_f$ before and after editing. Otherwise, there is a risk of forgetting original correct knowledge or introducing false knowledge.\\n\\nPrevious works care about whether the probability surpasses the original $o$. Here $O$ is a set, so we make a slight adaptation.\"}"}
{"id": "K9NTPRvVRI", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neighboring Perturbations of Knowledge Editing on Large Language Models\\n\\nIntuitively, the maximum probability of false knowledge\\n\\\\[ P_{\\\\text{max}} f = \\\\max_{M_i=1} P_{F^\\\\ast}(o_i|t_r(s)) \\\\]\\nis used as a threshold to calculate the proportion of the list of original correct objects of \\\\( K_c \\\\) whose probabilities are below this threshold.\\n\\nMathematically, we have\\n\\n\\\\[\\n\\\\text{Ranking Forgetting Factor (RFF)}:\\n\\\\]\\n\\n\\\\[\\nRF F = \\\\sum_{i=1}^{N} \\\\left\\\\{ \\\\begin{array}{ll}\\nP_{F^\\\\ast}(o_i|t_r(s)) < P_{\\\\text{max}} f & \\\\times \\\\sigma(P_{F^\\\\ast}(o_i|t_r(s))) \\\\\\\\\\n0 & \\\\end{array} \\\\right. \\n\\\\]\\n\\n\\\\[\\nP_{N_i=1} \\\\sigma(P_{F^\\\\ast}(o_i|t_r(s))),\\n\\\\]\\n\\n(1)\\n\\n\\\\( \\\\sigma \\\\) is the sigmoid function. Here, \\\\( \\\\sigma(P) \\\\) of each original correct knowledge is regarded as its weight, since knowledge with larger probability should weigh more and the sigmoid function can also be used for smoothing.\\n\\nSimilarly, the minimum probability of original correct knowledge\\n\\\\[ P_{\\\\text{min}} c = \\\\min_{N_i=1} P_{F^\\\\ast}(o_i|t_r(s)) \\\\]\\nis used as a threshold to calculate the proportion of the list of sampled false objects of \\\\( K_f \\\\) whose probabilities are above this threshold. Then,\\n\\n\\\\[\\n\\\\text{Ranking Noise Factor (RNF)}\\n\\\\]\\n\\n\\\\[\\nRNF = \\\\sum_{i=1}^{M} \\\\left\\\\{ \\\\begin{array}{ll}\\nP_{F^\\\\ast}(o_i|t_r(s)) > P_{\\\\text{min}} c & \\\\times \\\\sigma(P_{F^\\\\ast}(o_i|t_r(s))) \\\\\\\\\\n0 & \\\\end{array} \\\\right. \\n\\\\]\\n\\n\\\\[\\nP_{M_i=1} \\\\sigma(P_{F^\\\\ast}(o_i|t_r(s))),\\n\\\\]\\n\\n(2)\\n\\nAbsolute probability change of objects\\n\\nIn addition to satisfying the assumption of relative ranking of objects, it is also necessary to characterize their absolute probability changes. Even if the relative ranking remains unchanged, substantial harm is inflicted upon the model if the absolute probability changes unexpectedly. Therefore, \\\\text{Correct Probability Change (CPC)} is introduced to characterize this issue, which is defined as the ratio of the mean probability of correct knowledge after and before editing:\\n\\n\\\\[\\n\\\\text{CPC} = \\\\frac{\\\\sum_{i=1}^{N} P_{F^\\\\ast}(o_i|t_r(s))}{\\\\sum_{i=1}^{N} P_{F}(o_i|t_r(s))}\\n\\\\]\\n\\n(3)\\n\\nSimilarly, \\\\text{False Probability Change (FPC)} is defined as the ratio of the mean probability of false knowledge after and before editing:\\n\\n\\\\[\\n\\\\text{FPC} = \\\\frac{\\\\sum_{i=1}^{M} P_{F^\\\\ast}(o_i|t_r(s))}{\\\\sum_{i=1}^{M} P_{F}(o_i|t_r(s))}\\n\\\\]\\n\\n(4)\\n\\nAggregation\\n\\nFinally, we aim to aggregate these two dimensions of relative ranking and absolute probability change into a unified metric, providing a more comprehensive representation of the detrimental impact induced by appending knowledge. The \\\\text{Additive Forgetting Factor (AFF)} is defined as the degree to which the original correct knowledge is forgotten:\\n\\n\\\\[\\n\\\\text{AFF} = 1 - (1 - RF F) \\\\times \\\\min \\\\{ 1, \\\\text{CPC} \\\\}\\n\\\\]\\n\\n(5)\\n\\nThis definition means that if the probability of correct knowledge does not decrease after editing (\\\\( \\\\text{CPC} = 1 \\\\)), \\\\( \\\\text{AFF} \\\\) equals \\\\( \\\\text{RFF} \\\\). Otherwise, the adverse effects of \\\\( \\\\text{CPC} \\\\) and \\\\( \\\\text{RFF} \\\\) would combine, resulting in \\\\( \\\\text{AFF} \\\\) surpassing \\\\( \\\\text{RFF} \\\\). The \\\\( \\\\text{AFF} \\\\) spans from 0 to 1, reflecting the extent to which the original correct knowledge is forgotten amidst neighboring perturbations. A higher \\\\( \\\\text{AFF} \\\\) value corresponds to a more substantial negative impact. Similarly, the other aggregated metric \\\\text{Additive Noising Factor (ANF)} is defined as the degree to which false knowledge is introduced:\\n\\n\\\\[\\n\\\\text{ANF} = 1 - (1 - RNF) \\\\times \\\\min \\\\{ 1, \\\\text{FPC} \\\\}\\n\\\\]\\n\\n(6)\\n\\nThe ANF spans from 0 to 1, reflecting the extent to which the false knowledge is introduced in neighboring perturbations. A higher ANF value corresponds to a more substantial negative impact.\\n\\n5. PEAK: Perturbation Evaluation of Appending Knowledge\\n\\nThis paper designs the PEAK benchmark to assess the extent of perturbation to neighboring knowledge during editing. It comprises two datasets of PEAK-CF and PEAK-T. The former is designed as a counterfact dataset for the evaluation of knowledge editing methods on counterfactual appending following Meng et al. (2022) and Ma et al. (2023). The latter is based on temporal knowledge edits of changes in the real-world following Zhong et al. (2023).\\n\\n5.1. Data Construction of PEAK-CF\\n\\nAggregating facts\\n\\nThis dataset is constructed based on Wikidata (Vrandecic & Kr\u00f6tzsch, 2014), a knowledge base containing millions of fact triples. First, we manually chose a total of 32 relations, where a subject has multiple corresponding objects under each relation. Then we collected all triples for each relation and classified triples of the same subject \\\\( s \\\\) and relation \\\\( r \\\\) together, denoted as \\\\( F_m(r) = \\\\{ (s, r, O) | O = \\\\{ o_1, o_2, ..., o_N \\\\} \\\\} \\\\). For each relation \\\\( r \\\\), ChatGPT (gpt-3.5-turbo) was used to generate some templates \\\\( T(r) \\\\) expressing the same semantics. The subject \\\\( s \\\\) can be replaced to form the full prompts:\\n\\n\\\\[\\nP(s, r) = \\\\{ tr(s) | tr(\\\\cdot) \\\\in T(r) \\\\}\\n\\\\]\\n\\nFor example, a template for \\\\( r = \\\\text{product} \\\\) might be \\\"{} has products like\\\", where \\\"Apple\\\" substitutes \\\"{}\\\".\\n\\nConstructing counterfactual edits\\n\\nFollowing previous works (Meng et al., 2022; Yao et al., 2023; Ma et al., 2023), counterfactual edits were built in this dataset. The previous three metrics are used in this dataset. Given a set of triples \\\\( (s, r, O) \\\\in F_m(r) \\\\) and an object \\\\( o^* \\\\), where \\\\( o^* \\\\in O \\\\) and \\\\( (s, r, o^*) \\\\) is a counterfact, an edit is represented as \\\\( E = \\\\{ s, r, O, o^*, p \\\\} \\\\) to test efficacy, where the editing prompt \\\\( p \\\\in P(s, r) \\\\). Besides, to test generalization, a set of two semantically-equivalent paraphrase prompts \\\\( P_G \\\\) are sampled from \\\\( P(s, r) \\\\{ p \\\\} \\\\). Moreover, to test local-\"}"}
{"id": "K9NTPRvVRI", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Evaluation results (%) of the baselines using LLaMA-2 (7B) to access all correct answers on the PEAK-CF dataset. \\\"+\\\" refers to the baseline coupled with all correct answers $O$. Here only false answers in the Hard setting were used.\\n\\n| Editor     | Previous Additivity (hard) | ES  | GS  | LS  | AFF | ANF |\\n|------------|----------------------------|-----|-----|-----|-----|-----|\\n| FT+        | 97.54                      | 85.93| 71.68| 74.35| 61.83|     |\\n| MEND+      | 93.26                      | 79.90| 50.91| 33.92| 32.03|     |\\n| MEMIT+     | 98.29                      | 93.07| 94.20| 48.10| 66.72|     |\\n| ROME+      | 100.00                     | 93.13| 95.48| 50.27| 64.91|     |\\n\\nTo this end, we extended our evaluation and considered utilizing $k$ correct and false answers in APP, where $k \\\\in [0, 1, 3, 5, \\\\text{all}]$. It can be seen from these results that the performance of all editing methods coupled with APP was significantly improved as the number of neighboring answers increased. This trend shows the effectiveness of considering more neighboring answers to comprehensively characterize the neighboring perturbations. It also shows that both AFF and ANF can be significantly improved even with fewer answers, showcasing the practicality of the proposed APP method.\\n\\n8. Conclusion & Limitation\\n\\nThis study investigates the neighboring perturbations of knowledge editing on LLMs. A metric of additivity is introduced and a benchmark dubbed as PEAK is constructed for assessing the degree of perturbations in neighboring knowledge. A plug-and-play framework APP is proposed to mitigate perturbations by minimizing the probability disruptions during knowledge appending. Comprehensive experiments on various knowledge editing methods and LLMs reveal that they inevitably perturb neighboring knowledge during editing, and the proposed APP method demonstrates its effectiveness in mitigating this perturbations to a certain extent. In the future, we will explore expanding the scope of neighbor knowledge to more comprehensively characterize the neighboring perturbations of knowledge editing.\\n\\nBesides, there are several limitations to our work. First, regarding the experimental settings, the editing methods employed in this paper involve modifying the model parameters. But some editing methods that preserve model parameters remain to be explored. Second, this paper focuses on factual knowledge, and it would be beneficial to extend this approach to different types of knowledge, such as commonsense, logical, or spatial knowledge. Third, this paper designs the proposed additivity metric based on probability. However, the results generated by the actual model do not entirely depend on probability. Therefore, while this evaluation roughly reflects the degree of perturbation, it may deviate from real-world scenarios to some extent.\\n\\nAcknowledgements\\n\\nWe would like to express gratitude to the anonymous reviewers for their kind comments. This work is funded by the National Science and Technology Major Project (No. 2023ZD0121103). Ningyu Zhang is supported by the National Natural Science Foundation of China (No. 62206246), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Yongjiang Talent Introduction Programme (2021A-156-G), CCF-Tencent Rhino-Bird Open Research Fund.\\n\\nImpact Statement\\n\\nThe research presented in this paper focuses on a critical aspect of LLMs \u2013 their susceptibility to unintended text generation due to false or outdated knowledge. In light of the resource-intensive nature of retraining LLMs, the study focuses on the emerging field of knowledge editing. It develops technologies to reduce perturbations to preserve factuality. In terms of future societal consequences, this work contributes to mitigating the need for full retraining, reducing computational resources and energy consumption. This aligns with sustainable AI and makes it more feasible to deploy LLMs in resource-constrained settings. Besides, APP's ability to perform targeted edits and mitigate neighboring perturbations can enhance LLMs' effectiveness in specialized domains, such as medical diagnostics, and legal analysis, where precise and updated knowledge is essential. Additionally, editing methods for appending knowledge to LLMs could be misused to propagate misinformation. Furthermore, it could pose privacy risks if sensitive information is appended without proper safeguards. In the future, some strategies should be implemented to reduce these risks.\"}"}
{"id": "K9NTPRvVRI", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bau, D., Liu, S., Wang, T., Zhu, J., and Torralba, A. \\n\\\"Rewriting a deep generative model.\\\" \\nIn Vedaldi, A., Bischof, H., Brox, T., and Frahm, J. (eds.), \\nComputer Vision - ECCV 2020 - 16th European Conference, \\nGlasgow, UK, August 23-28, 2020, Proceedings, Part I, \\nvolume 12346 of Lecture Notes in Computer Science, \\npp. 351\u2013369. Springer, 2020. doi: 10.1007/978-3-030-58452-8_21. \\nURL https://doi.org/10.1007/978-3-030-58452-8_21.\\n\\nCao, N. D., Aziz, W., and Titov, I. \\n\\\"Editing factual knowledge in language models.\\\" \\nIn Moens, M., Huang, X., Specia, L., and Yih, S. W. (eds.), \\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, \\nEMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, \\n7-11 November, 2021, pp. 6491\u20136506. Association for Computational Linguistics, 2021. \\ndoi: 10.18653/v1/2021.emnlp-main.522. URL https://doi.org/10.18653/v1/2021.emnlp-main.522.\\n\\nCheng, S., Tian, B., Liu, Q., Chen, X., Wang, Y., Chen, H., and Zhang, N. \\n\\\"Can we edit multimodal large language models?\\\" \\nIn Bouamor, H., Pino, J., and Bali, K. (eds.), \\nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, \\nEMNLP 2023, Singapore, December 6-10, 2023, pp. 13877\u201313888. Association for Computational Linguistics, 2023. \\nURL https://aclanthology.org/2023.emnlp-main.856.\\n\\nCohen, R., Biran, E., Yoran, O., Globerson, A., and Geva, M. \\n\\\"Evaluating the ripple effects of knowledge editing in language models.\\\" \\nCoRR, abs/2307.12976, 2023. doi: 10.48550/ARXIV.2307.12976. URL https://doi.org/10.48550/arXiv.2307.12976.\\n\\nDai, D., Dong, L., Hao, Y., Sui, Z., Chang, B., and Wei, F. \\n\\\"Knowledge neurons in pretrained transformers.\\\" \\nIn Muresan, S., Nakov, P., and Villavicencio, A. (eds.), \\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), \\nACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 8493\u20138502. Association for Computational Linguistics, 2022. \\ndoi: 10.18653/v1/2022.acl-long.581. URL https://doi.org/10.18653/v1/2022.acl-long.581.\\n\\nGandikota, R., Materzynska, J., Fiotto-Kaufman, J., and Bau, D. \\n\\\"Erasing concepts from diffusion models.\\\" \\nCoRR, abs/2303.07345, 2023. doi: 10.48550/ARXIV.2303.07345. URL https://doi.org/10.48550/arXiv.2303.07345.\\n\\nGentile, C. and Warmuth, M. K. \\n\\\"Linear hinge loss and average margin.\\\" \\nIn Kearns, M. J., Solla, S. A., and Cohn, D. A. (eds.), \\nAdvances in Neural Information Processing Systems 11, [NIPS Conference, Denver, Colorado, USA, November 30 - December 5, 1998], pp. 225\u2013231. The MIT Press, 1998.\\n\\nGeva, M., Schuster, R., Berant, J., and Levy, O. \\n\\\"Transformer feed-forward layers are key-value memories.\\\" \\nIn Moens, M., Huang, X., Specia, L., and Yih, S. W. (eds.), \\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, \\nEMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 5484\u20135495. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.446. URL https://doi.org/10.18653/v1/2021.emnlp-main.446.\\n\\nGeva, M., Caciularu, A., Wang, K. R., and Goldberg, Y. \\n\\\"Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.\\\" \\nIn Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), \\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, \\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 30\u201345. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.emnlp-main.3. URL https://doi.org/10.18653/v1/2022.emnlp-main.3.\\n\\nGu, J., Xu, H., Ma, J., Lu, P., Ling, Z., Chang, K., and Peng, N. \\n\\\"Model editing can hurt general abilities of large language models.\\\" \\nCoRR, abs/2401.04700, 2024. doi: 10.48550/ARXIV.2401.04700. URL https://doi.org/10.48550/arXiv.2401.04700.\\n\\nHuang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., and Liu, T. \\n\\\"A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.\\\" \\nCoRR, abs/2311.05232, 2023. doi: 10.48550/ARXIV.2311.05232. URL https://doi.org/10.48550/arXiv.2311.05232.\\n\\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y., Madotto, A., and Fung, P. \\n\\\"Survey of hallucination in natural language generation.\\\" \\nACM Comput. Surv., 55(12):248:1\u2013248:38, 2023. doi: 10.1145/3571730. URL https://doi.org/10.1145/3571730.\\n\\nLi, Z., Zhang, N., Yao, Y., Wang, M., Chen, X., and Chen, H. \\n\\\"Unveiling the pitfalls of knowledge editing for large language models.\\\" \\nCoRR, abs/2310.02129, 2023. doi: 10.48550/ARXIV.2310.02129. URL https://doi.org/10.48550/arXiv.2310.02129.\\n\\nMa, J., Chen, B., Gu, J., Ling, Z., Guo, W., Liu, Q., Chen, Z., and Liu, C. \\n\\\"Wider & closer: Mixture of short-channel distillers for zero-shot cross-lingual named entity recognition.\\\" \\nIn Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), \\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, \\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 30\u201345. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.emnlp-main.3. URL https://doi.org/10.18653/v1/2022.emnlp-main.3.\"}"}
{"id": "K9NTPRvVRI", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "K9NTPRvVRI", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neighboring Perturbations of Knowledge Editing on Large Language Models\\n\\nVrandecic, D. and Kr\u00f6tzsch, M. Wikidata: a free collaborative knowledgebase. Commun. ACM, 57(10):78\u201385, 2014. doi: 10.1145/2629489.\\n\\nWang, B. and Komatsuzaki, A. Gpt-j-6b: A 6 billion parameter autoregressive language model, 2021.\\n\\nWang, P., Zhang, N., Xie, X., Yao, Y., Tian, B., Wang, M., Xi, Z., Cheng, S., Liu, K., Zheng, G., and Chen, H. Easyedit: An easy-to-use knowledge editing framework for large language models. CoRR, abs/2308.07269, 2023. doi: 10.48550/arXiv.2308.07269.\\n\\nWang, Y., Chen, M., Peng, N., and Chang, K.-W. Deepedit: Knowledge editing as decoding with constraints. arXiv preprint arXiv:2401.10471, 2024.\\n\\nWu, X., Li, J., Xu, M., Dong, W., Wu, S., Bian, C., and Xiong, D. DEPN: detecting and editing privacy neurons in pretrained language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 2875\u20132886. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.emnlp-main.174.\\n\\nWu, Y. and Liu, Y. Robust truncated hinge loss support vector machines. Journal of the American Statistical Association, 102(479):974\u2013983, 2007.\\n\\nYao, Y., Wang, P., Tian, B., Cheng, S., Li, Z., Deng, S., Chen, H., and Zhang, N. Editing large language models: Problems, methods, and opportunities. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 10222\u201310240. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.emnlp-main.632.\\n\\nYin, X., Jiang, J., Yang, L., and Wan, X. History matters: Temporal knowledge editing in large language model. CoRR, abs/2312.05497, 2023. doi: 10.48550/ARXIV.2312.05497.\\n\\nZhang, B., Haddow, B., and Birch, A. Prompting large language model for machine translation: A case study. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 41092\u201341110. PMLR, 2023a. URL https://proceedings.mlr.press/v202/zhang23m.html.\\n\\nZhang, N., Yao, Y., Tian, B., Wang, P., Deng, S., Wang, M., Xi, Z., Mao, S., Zhang, J., Ni, Y., et al. A comprehensive study of knowledge editing for large language models. arXiv preprint arXiv:2401.01286, 2024.\\n\\nZhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y., Chen, Y., Wang, L., Luu, A. T., Bi, W., Shi, F., and Shi, S. Siren\u2019s song in the AI ocean: A survey on hallucination in large language models. CoRR, abs/2309.01219, 2023b. doi: 10.48550/arXiv.2309.01219.\\n\\nZhong, Z., Wu, Z., Manning, C. D., Potts, C., and Chen, D. Mquake: Assessing knowledge editing in language models via multi-hop questions. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 15686\u201315702. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.emnlp-main.971.\\n\\nZhu, C., Rawat, A. S., Zaheer, M., Bhojanapalli, S., Li, D., Yu, F. X., and Kumar, S. Modifying memories in transformer models. CoRR, abs/2012.00363, 2020. URL https://arxiv.org/abs/2012.00363.\"}"}
