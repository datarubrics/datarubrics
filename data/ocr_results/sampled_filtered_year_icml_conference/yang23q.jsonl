{"id": "yang23q", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Average IID Tasks \\\\( \\\\tau \\\\) | Task Group | Task GOAT(\u03c4) |\\n|-----------------------------|------------|-------------|\\n|                             | HandReach  | Near-Far    |\\n|                             | Slide      | Near-Far    |\\n|                             | Slide      | Left-Right  |\\n|                             | Pick       | Low-High    |\\n|                             | Pick       | Left-Right  |\\n|                             | Push       | Near-Far    |\\n|                             | Push       | Left-Right  |\\n|                             | Reach      | Near-Far    |\\n|                             | Reach      | Left-Right  |\\n\\nAverage cumulative return with standard deviation over 5 random seeds. Blue lines refer to IID tasks and purple lines indicate OOD tasks.\"}"}
{"id": "yang23q", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nAppendix\\n\\nA. Algorithm Pseudo Code\\n\\nAlgorithm 1 GOAT Algorithm\\n\\nInitialize policy $\\\\pi^\\\\theta$ and $N$ value functions $Q_1, \\\\ldots, Q_N$, and two FIFO queues $B_{ba} = \\\\emptyset$ and $B_{std} = \\\\emptyset$;\\n\\nfor training step $t = 1, 2, \\\\ldots$ do\\n\\nSample a mini-batch from the offline dataset: $\\\\{(s_t, a_t, g, r_t, s_{t+1})\\\\} \\\\sim D$;\\n\\nRelabel the mini-batch with a probability of $p_{relabel}$: $\\\\{(s_t, a_t, g', r', s_{t+1})\\\\} \\\\sim D_{relabel}$;\\n\\nUpdate value functions $Q_i, i \\\\in [1, N]$ to minimize Eq (3) with the mini-batch;\\n\\nEstimate advantage values $A(s_t, a_t, g')$ using $Q_i, i \\\\in [1, N]$ and store them into the queue $B_{ba}$;\\n\\nGet the $\\\\alpha$ percentile advantage value from $B_{ba}$ to calculate the DSW;\\n\\nEstimate the bootstrapped uncertainty $\\\\text{Std}(s_t, g)$ and store them into $B_{std}$;\\n\\nCompute the UW according to Eq (4);\\n\\nUpdate policy $\\\\pi^\\\\theta$ to maximize the objective in Eq (2) with the mini-batch:\\n\\nend for\\n\\nB. Theoretical Proofs\\n\\nB.1. Useful Lemmas\\n\\nLemma B.1. Assume the maximum reward is $R_{\\\\text{max}}$. For any two goal-conditioned policies $\\\\pi$ and $\\\\pi_E$, we have that\\n\\n$$V_{\\\\pi_E}(s_0, g) - V_{\\\\pi}(s_0, g) \\\\leq 2R_{\\\\text{max}}(1 - \\\\gamma)\\\\frac{1}{2}E_{s \\\\sim \\\\pi_E(s|s_0, g)}^D \\\\text{DTW} \\\\pi(\\\\cdot|s, g), \\\\pi_E(\\\\cdot|s, g)}.$$  \\n\\nProof. For any policy $\\\\pi$, its value function can be formulated as $V_{\\\\pi} = 1 - \\\\gamma \\\\cdot E_{(s, a) \\\\sim \\\\rho_{\\\\pi}}[r]$. In the goal-conditioned setting, we also need to include goals into consideration. Then, we can derive\\n\\n$$|V_{\\\\pi_E}(s_0, g) - V_{\\\\pi}(s_0, g)| = 1 - \\\\gamma \\\\cdot E_{(s, a) \\\\sim \\\\rho_{\\\\pi_E}(s|s_0, g)}[r] - 1 - \\\\gamma \\\\cdot E_{(s, a) \\\\sim \\\\rho_{\\\\pi}(s|s_0, g)}[r] \\\\leq 2R_{\\\\text{max}}(1 - \\\\gamma)\\\\frac{1}{2}E_{s \\\\sim \\\\pi_E(s|s_0, g)}^D \\\\text{DTW} \\\\pi(\\\\cdot|s, g), \\\\pi_E(\\\\cdot|s, g)}.$$  \\n\\nWith Lemma 5 in (Xu et al., 2020), we complete the proof:\\n\\n$$|V_{\\\\pi_E}(s_0, g) - V_{\\\\pi}(s_0, g)| \\\\leq 2R_{\\\\text{max}}(1 - \\\\gamma)\\\\frac{1}{2}E_{s \\\\sim \\\\pi_E(s|s_0, g)}^D \\\\text{DTW} \\\\pi(\\\\cdot|s, g), \\\\pi_E(\\\\cdot|s, g)}.$$  \\n\\nLemma B.2. Assume the maximum reward is $R_{\\\\text{max}}$. For any two goal-conditioned policies $\\\\pi$ and $\\\\pi_E$, we have that\\n\\n$$\\\\text{SubOpt}(\\\\pi_E, \\\\pi) = E_{(s_0, g) \\\\sim P_T \\\\mathcal{X}}[V_{\\\\pi_E}(s_0, g) - V_{\\\\pi}(s_0, g)] \\\\leq 2R_{\\\\text{max}}(1 - \\\\gamma)\\\\frac{1}{2}E_{s \\\\sim \\\\pi_E(s|s_0, g)}^D \\\\text{DTW} \\\\pi(\\\\cdot|s, g), \\\\pi_E(\\\\cdot|s, g)}.$$  \\n\\nLemma B.2 is a direct result of combining Lemma B.1 and the definition of the suboptimality in Eq (1).\\n\\nDefinition B.3. For any state-goal distribution $\\\\rho(s, g)$, we define\\n\\n$$\\\\varepsilon_{\\\\rho}(\\\\pi_E, \\\\pi) = E_{(s, g) \\\\sim \\\\rho(s, g)}^D \\\\text{DTW} \\\\pi(\\\\cdot|s, g), \\\\pi_E(\\\\cdot|s, g)}.$$  \\n\\n13\"}"}
{"id": "yang23q", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma B.4. For any three policy \\\\( \\\\pi_1, \\\\pi_2, \\\\pi_3 \\\\) and any state-goal distribution \\\\( \\\\rho \\\\), we have:\\n\\n\\\\[\\n\\\\varepsilon_\\\\rho(\\\\pi_1, \\\\pi_2) \\\\leq \\\\varepsilon_\\\\rho(\\\\pi_1, \\\\pi_3) + \\\\varepsilon_\\\\rho(\\\\pi_3, \\\\pi_2)\\n\\\\]\\n\\nProof. This can be proved by noticing that \\\\( \\\\text{D} \\\\text{TV} \\\\) is a distance metric.\\n\\n\\\\[\\n\\\\varepsilon_\\\\rho(\\\\pi_1, \\\\pi_2) = \\\\mathbb{E}_{(s,g) \\\\sim \\\\rho} \\\\text{D} \\\\text{TV}(\\\\pi_1(\\\\cdot|s,g), \\\\pi_2(\\\\cdot|s,g)) \\\\leq \\\\mathbb{E}_{(s,g) \\\\sim \\\\rho} \\\\text{D} \\\\text{TV}(\\\\pi_1(\\\\cdot|s,g), \\\\pi_3(\\\\cdot|s,g)) + \\\\text{D} \\\\text{TV}(\\\\pi_3(\\\\cdot|s,g), \\\\pi_2(\\\\cdot|s,g))\\n\\\\]\\n\\nLemma B.5. Assume the expert policy \\\\( \\\\pi_E \\\\) is invariant across training and testing domains. For a policy \\\\( \\\\pi \\\\), we have\\n\\n\\\\[\\n\\\\varepsilon_T(\\\\pi_E, \\\\pi) \\\\leq \\\\varepsilon_S(\\\\pi_E, \\\\pi) + d_1(T, S)\\n\\\\]\\n\\nwhere the variation divergence \\\\( d_1 \\\\) between two distribution \\\\( S_1 \\\\) and \\\\( S_2 \\\\) is defined as follows:\\n\\n\\\\[\\nd_1(S_1, S_2) = 2 \\\\sup J \\\\subseteq X \\\\mathbb{E}_x \\\\in J (P_{S_1}(x) - P_{S_2}(x))\\n\\\\]\\n\\nProof. With the definition of variation divergence, we have\\n\\n\\\\[\\n\\\\varepsilon_T(\\\\pi_E, \\\\pi) = \\\\varepsilon_T(\\\\pi_E, \\\\pi) + \\\\varepsilon_S(\\\\pi_E, \\\\pi) - \\\\varepsilon_S(\\\\pi_E, \\\\pi) \\\\leq \\\\varepsilon_S(\\\\pi_E, \\\\pi) + |\\\\varepsilon_T(\\\\pi_E, \\\\pi) - \\\\varepsilon_S(\\\\pi_E, \\\\pi)|\\n\\\\]\\n\\n\\\\[\\n\\\\leq \\\\varepsilon_S(\\\\pi_E, \\\\pi) + \\\\mathbb{E}_{(s_0,g) \\\\sim P_T X} \\\\mathbb{E}_{s \\\\sim d\\\\pi_E} ||P_{T X}(s_0,g) - P_{S X}(s_0,g)||_X d_{\\\\pi_E} s|s_0,g| - \\\\pi(s|s_0,g)\\n\\\\]\\n\\nLemma B.6 (Generalization Bound for Finite ERM). Consider finite hypothesis space \\\\( F \\\\) and bounded loss function in \\\\([a, b]\\\\).\\n\\nWhen optimizing empirical loss function \\\\( \\\\hat{L}(f) = \\\\frac{1}{m} \\\\sum_{i} L(f(x_i), y_i) \\\\) instead of the expected one \\\\( L(f) = \\\\mathbb{E}_{x,y} L(f(x), y) \\\\), with probability as least \\\\( 1 - \\\\delta \\\\), the true loss can be bounded as:\\n\\n\\\\[\\nL(f) \\\\leq \\\\hat{L}(f) + \\\\frac{s(b - a)}{2m \\\\log(2|F| + \\\\frac{1}{\\\\delta})}\\n\\\\]\\n\\nLemma B.6 is a well-known result from (Mohri et al., 2018).\\n\\nB.2. Proof of Theorem 3.1\\n\\nProof. With Lemma B.2 and the definition of policy discrepancy on training and testing distributions:\\n\\n\\\\[\\n\\\\varepsilon_T(\\\\pi_E, \\\\pi) = \\\\mathbb{E}_{(s_0,g) \\\\sim P_T X} s \\\\sim d\\\\pi_E s|s_0,g| \\\\text{D} \\\\text{TV}(\\\\pi(\\\\cdot|s,g), \\\\pi_E(\\\\cdot|s,g))\\n\\\\]\\n\\n\\\\[\\n\\\\varepsilon_S(\\\\pi_E, \\\\pi) = \\\\mathbb{E}_{(s_0,g) \\\\sim P_S X} s \\\\sim d\\\\pi_E s|s_0,g| \\\\text{D} \\\\text{TV}(\\\\pi(\\\\cdot|s,g), \\\\pi_E(\\\\cdot|s,g))\\n\\\\]\\n\\nwe have\\n\\n\\\\[\\n\\\\text{SubOpt}(\\\\pi_E, \\\\pi) \\\\leq 2R_{\\\\max}(1 - \\\\gamma)^2 \\\\mathbb{E}_{(s_0,g) \\\\sim P_T X} s \\\\sim d\\\\pi_E s|s_0,g| \\\\text{D} \\\\text{TV}(\\\\pi(\\\\cdot|s,g), \\\\pi_E(\\\\cdot|s,g)) = 2R_{\\\\max}(1 - \\\\gamma)^2 \\\\varepsilon_T(\\\\pi_E, \\\\pi)\\n\\\\]\"}"}
{"id": "yang23q", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nRegarding $\\\\varepsilon_T(\\\\pi_E, \\\\pi)$, we use Lemma B.4 and Lemma B.5 to obtain an upper bound:\\n\\n$$\\n\\\\varepsilon_T(\\\\pi_E, \\\\pi) \\\\leq \\\\varepsilon_S(\\\\hat{\\\\pi}_E, \\\\pi) + d_1(T, S) \\\\leq \\\\varepsilon_S(\\\\hat{\\\\pi}_E, \\\\pi) + \\\\varepsilon_S(\\\\pi_E, \\\\hat{\\\\pi}_E) + d_1(T, S)\\n$$\\n\\nWhen we use finite sample to estimate $\\\\varepsilon_S(\\\\hat{\\\\pi}_E, \\\\pi)$, the true loss can be bounded with Lemma B.6. Note that the $\\\\text{D}_\\\\text{TV}$ can be bounded in $[0, 1]$. Therefore, we can complete the proof. With probability at least $1 - \\\\delta$, $\\\\text{SubOpt}(\\\\pi_E, \\\\pi) \\\\leq 2 R_{\\\\text{max}} \\\\frac{1}{1 - \\\\gamma} + 2 \\\\hat{\\\\varepsilon}_S(\\\\hat{\\\\pi}_E, \\\\pi) + \\\\varepsilon_S(\\\\pi_E, \\\\hat{\\\\pi}_E) + d_1(T, S) + s \\\\log 2 |\\\\Pi| + \\\\log \\\\frac{1}{\\\\delta} m$.\\n\\nB.3. Proof of Theorem 3.2\\n\\nProof.\\n\\nStep 1. First we explicitly show $\\\\sup_{Z \\\\in Z} d_1(Z, \\\\bar{S}) = 2 \\\\left(1 - \\\\frac{1}{C} \\\\frac{1}{|X|}\\\\right)$. (5)\\n\\nOn one side, simple algebra shows that the following distribution $S' \\\\in S$ will induce the distance shown in Eq (5):\\n\\n$$\\nP_{S'}(x) = \\\\begin{cases} \\nC, & \\\\text{if } x \\\\in J, \\\\\\\\\\n0, & \\\\text{otherwise}\\n\\\\end{cases}\\n$$\\n\\nwhere $|J| = \\\\frac{1}{C} < |X|$. On the other hand, we are going to show there is no distribution that can elicit a distance larger than that in Eq (5). We prove it by contradiction by assuming a distribution $S''$ which has $d_1(S'', \\\\bar{S}) > 2 \\\\left(1 - \\\\frac{1}{C} \\\\frac{1}{|X|}\\\\right)$. (6)\\n\\nThen there exists $J'' \\\\subset X$ such that\\n\\n$$\\nP_{S''}(x) - P_{\\\\bar{S}}(x) > \\\\left(1 - \\\\frac{1}{C} \\\\frac{1}{|X|}\\\\right).$$\\n\\n(7)\\n\\nWith out loss of generality, we assume $P_{S''}(x) - P_{\\\\bar{S}}(x) > \\\\left(1 - \\\\frac{1}{C} \\\\frac{1}{|X|}\\\\right)$ (8)\\n\\nDenote $\\\\bar{x}_{J''} = \\\\frac{1}{|J''|} \\\\int_{J''} P_{S''}(x) dx$. It is clear that $\\\\bar{x}_{J''} \\\\leq C$. Then the RHS of Eq (8) is\\n\\n$$\\n|J''| (\\\\bar{x}_{J''} - \\\\frac{1}{C} \\\\frac{1}{|X|}) = \\\\frac{1}{|J''|} \\\\int_{J''} P_{S''}(x) dx - \\\\frac{1}{C} \\\\frac{1}{|X|} \\\\int_{J''} 1 dx \\\\\\\\\\n\\\\leq \\\\left(1 - \\\\frac{1}{C} \\\\frac{1}{|X|}\\\\right) |J''| \\\\\\\\\\n\\\\leq \\\\left(1 - \\\\frac{1}{C} \\\\frac{1}{|X|}\\\\right),\\n$$\\n\\n(10)\\n\\nwhere the first inequality is due to $|J''| \\\\bar{x}_{J''} \\\\leq 1$ and the second inequality is due to $\\\\bar{x}_{J''} \\\\leq C$. Thus we arrive at a contradiction. So Eq (6) does not hold. Putting these together, we show that Eq (5) holds.\\n\\nStep 2. We now proceed to show $\\\\forall S \\\\in S - \\\\bar{S}$, $\\\\sup_{Z \\\\in Z} d_1(Z, S) > 2 \\\\left(1 - \\\\frac{1}{C} \\\\frac{1}{|X|}\\\\right)$. (12)\"}"}
{"id": "yang23q", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nWe know that for any $S \\\\in S^-$, there exists a subset $J \\\\subset X$ such that\\n\\n$$Z_x \\\\in J \\\\quad P_S(x) \\\\ dx < \\\\frac{|J|}{|X|} \\\\quad (13)$$\\n\\nLet $J_M$ be the subset of $X \\\\setminus J$ which contains the smallest $1/C - |J|$ points:\\n\\n$$J_M = \\\\min_{M \\\\subset X \\\\setminus J} |M| = 1/C - |J|$$\\n\\n$$Z_M \\\\quad P_S(x) \\\\ dx. \\\\quad (14)$$\\n\\nBy the definition of $J_M$, it is easy to see that the mean density ratio $X / (J \\\\cup J_M)$ is larger than that of $J_M$,\\n\\n$$\\\\frac{1}{|X|} - \\\\frac{1}{C} \\\\leq \\\\frac{1}{|X|} - \\\\frac{1}{C} \\\\quad Z_x \\\\in X \\\\setminus (J \\\\cup J_M) \\\\quad P_S(x) \\\\ dx \\\\geq \\\\frac{1}{1/C - |J|} \\\\quad Z_x \\\\in J_M \\\\quad P_S(x) \\\\ dx, \\\\quad (15)$$\\n\\nWe now proceed to prove (by contradiction) that\\n\\n$$\\\\frac{1}{1/C - |J|} \\\\quad Z_x \\\\in J_M \\\\cup J \\\\quad P_S(x) \\\\ dx < \\\\frac{1}{|X|}. \\\\quad (16)$$\\n\\nWe first assume\\n\\n$$\\\\frac{1}{1/C - |J|} \\\\quad Z_x \\\\in J_M \\\\cup J \\\\quad P_S(x) \\\\ dx \\\\geq \\\\frac{1}{|X|} \\\\quad (17)$$\\n\\nBy Eq (13) and (17), we have\\n\\n$$Z_x \\\\in J_M \\\\quad P_S(x) \\\\ dx > \\\\frac{1}{C} - |J| \\\\quad (18)$$\\n\\nand further with Eq (15) we have\\n\\n$$Z_x \\\\in X \\\\setminus (J \\\\cup J_M) \\\\quad P_S(x) \\\\ dx > |X| - \\\\frac{1}{C} \\\\quad (19)$$\\n\\nPutting Eq (19) and Eq (17) together, we have\\n\\n$$Z_x \\\\in X \\\\quad P_S(x) \\\\ dx > 1 \\\\quad (20)$$\\n\\nwhich arrives at a contradiction. So Eq (16) holds. We then construct $Z$ as\\n\\n$$P_Z(x) = \\\\begin{cases} C, & \\\\text{if } x \\\\in J \\\\cup J_M, \\\\\\\\ 0, & \\\\text{otherwise}. \\\\end{cases} \\\\quad (21)$$\\n\\nWith Eq (21) and Eq (16), we have\\n\\n$$Z_x \\\\in J \\\\quad (P_Z(x) - P_S(x)) \\\\ dx > 1 - \\\\frac{1}{C} \\\\quad (22)$$\\n\\nSo we prove Eq (12).\\n\\nPutting Step 1 and 2 together, we finish the proof.\"}"}
{"id": "yang23q", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nDomain Generalization (DG)\\n\\nDG aims to learn a model from training domains that can generalize on unseen testing domains (Zhou et al., 2021a; Wang et al., 2022). Solutions to DG include data augmentation (Zhou et al., 2020; 2021b), meta learning (Li et al., 2018; Balaji et al., 2018; Yong et al., 2023), invariant representation learning (Arjovsky et al., 2019; Lin et al., 2022a; Zhou et al., 2022; 2023; Lin et al., 2022b) and distributionally robust optimization (Sagawa et al., 2019). In reinforcement learning, DG is handled with data augmentation (Wang et al., 2020), environment generation (Jiang et al., 2021), and representation learning (Mazoure et al., 2021; Sonar et al., 2021; Han et al., 2021).\\n\\nUnlike these works, we mainly consider the covariate shift and handle pessimism and generalization simultaneously for OOD generalization of offline GCRL.\\n\\n7. Conclusion\\n\\nLearning from purely offline datasets and generalizing to unseen goals is one of the pursuits of the RL community. In this paper, we investigate the problem of out-of-distribution (OOD) generalization of offline GCRL. Through theoretical analysis and empirical evaluation, we demonstrate that (1) the choice of offline RL methods, particularly weighted imitation learning, and (2) the techniques to minimize the generalization bound, are crucial for this problem. With these insights, we propose GOAT, a new weighted imitation learning method that achieves strong OOD generalization performance across a variety of tasks. In the future, we believe our work will inspire more scalable and generalizable reinforcement learning research.\\n\\n8. Limitations\\n\\nThe major limitation of this work is that we mainly consider algorithmic designs motivated by the OOD generalization theory. There are many interesting future directions not included in this paper, e.g., studying representation learning (Mazoure et al., 2021), goal embeddings (Islam et al., 2022), world models (Anand et al., 2021; Ding et al., 2022), and network designs (Lee et al., 2022; Xu et al., 2022; Hong et al., 2022) to improve OOD generalization for offline RL and offline GCRL.\\n\\nAcknowledgements\\n\\nThis work is supported by GRF 16310222 and GRF 16201320, in part by Science and Technology Innovation 2030 - \u201cNew Generation Artificial Intelligence\u201d Major Project (No. 2018AAA0100904) and the National Natural Science Foundation of China (62176135). The authors would like to thank the anonymous reviewers for their comments to improve the paper.\\n\\nReferences\\n\\nAn, G., Moon, S., Kim, J.-H., and Song, H. O. Uncertainty-based offline reinforcement learning with diversified q-ensemble. Advances in neural information processing systems, 34:7436\u20137447, 2021.\\n\\nAnand, A., Walker, J., Li, Y., V\u00b4ertes, E., Schrittwieser, J., Ozair, S., Weber, T., and Hamrick, J. B. Procedural generalization by planning with self-supervised world models. arXiv preprint arXiv:2111.01587, 2021.\\n\\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O., and Zaremba, W. Hindsight experience replay. Advances in neural information processing systems, 30, 2017.\\n\\nArjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. Invariant risk minimization. ArXiv, abs/1907.02893, 2019.\\n\\nBai, C., Wang, L., Yang, Z., Deng, Z.-H., Garg, A., Liu, P., and Wang, Z. Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning. In International Conference on Learning Representations, 2022.\\n\\nBalaji, Y., Sankaranarayanan, S., and Chellappa, R. Metareg: Towards domain generalization using meta-regularization. Advances in neural information processing systems, 31, 2018.\\n\\nBen-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J. W. A theory of learning from different domains. Machine learning, 79(1):151\u2013175, 2010.\\n\\nBlanchard, G., Lee, G., and Scott, C. Generalizing from several related classification tasks to a new unlabeled sample. Advances in neural information processing systems, 24, 2011.\\n\\nBorsa, D., Barreto, A., Quan, J., Mankowitz, D., Munos, R., Van Hasselt, H., Silver, D., and Schaul, T. Universal successor features approximators. arXiv preprint arXiv:1812.07626, 2018.\\n\\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.\\n\\nChebotar, Y., Hausman, K., Lu, Y., Xiao, T., Kalashnikov, D., Varley, J., Irpan, A., Eysenbach, B., Julian, R., Finn, C., et al. Actionable models: Unsupervised offline reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021.\\n\\nCobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman, J. Quantifying generalization in reinforcement learning. In International Conference on Machine Learning, pp. 1282\u20131289. PMLR, 2019.\"}"}
{"id": "yang23q", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nCsisz\u00e1r, I. and Korner, J.\\n\\nInformation theory: coding theorems for discrete memoryless systems. Cambridge University Press, 2011.\\n\\nDing, W., Lin, H., Li, B., and Zhao, D. Generalizing goal-conditioned reinforcement learning with variational causal reasoning. arXiv preprint arXiv:2207.09081, 2022.\\n\\nEysenbach, B., Geng, X., Levine, S., and Salakhutdinov, R. R. Rewriting history with inverse rl: Hindsight inference for policy improvement. Advances in neural information processing systems, 33:14783\u201314795, 2020.\\n\\nFujimoto, S. and Gu, S. S. A minimalist approach to offline reinforcement learning. Advances in neural information processing systems, 34:20132\u201320145, 2021.\\n\\nFujimoto, S., Meger, D., and Precup, D. Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pp. 2052\u20132062. PMLR, 2019.\\n\\nGhasemipour, S. K. S., Gu, S. S., and Nachum, O. Why so pessimistic? estimating uncertainties for offline rl through ensembles, and why their independence matters. arXiv preprint arXiv:2205.13703, 2022.\\n\\nGhosh, D., Gupta, A., Reddy, A., Fu, J., Devin, C., Eysenbach, B., and Levine, S. Learning to reach goals via iterated supervised learning. arXiv preprint arXiv:1912.06088, 2019.\\n\\nGoh, J. and Sim, M. Distributionally robust optimization and its tractable approximations. Operations research, 58(4-part-1):902\u2013917, 2010.\\n\\nHan, B., Zheng, C., Chan, H., Paster, K., Zhang, M., and Ba, J. Learning domain invariant representations in goal-conditioned block mdps. Advances in Neural Information Processing Systems, 34:764\u2013776, 2021.\\n\\nHansen-Estruch, P., Zhang, A., Nair, A., Yin, P., and Levine, S. Bisimulation makes analogies in goal-conditioned reinforcement learning. In International Conference on Machine Learning, pp. 8407\u20138426. PMLR, 2022.\\n\\nHong, Z.-W., Yang, G., and Agrawal, P. Bi-linear value networks for multi-goal reinforcement learning. In International Conference on Learning Representations, 2022.\\n\\nIslam, R., Zang, H., Goyal, A., Lamb, A., Kawaguchi, K., Li, X., Laroche, R., Bengio, Y., and des Combes, R. T. Discrete compositional representations as an abstraction for goal conditioned reinforcement learning. In Advances in Neural Information Processing Systems, 2022.\\n\\nJiang, M., Grefenstette, E., and Rockt\u00e4schel, T. Prioritized level replay. In International Conference on Machine Learning, pp. 4940\u20134950. PMLR, 2021.\\n\\nJin, Y., Yang, Z., and Wang, Z. Is pessimism provably efficient for offline rl? In International Conference on Machine Learning, pp. 5084\u20135096. PMLR, 2021.\\n\\nKirk, R., Zhang, A., Grefenstette, E., and Rockt\u00e4schel, T. A survey of zero-shot generalisation in deep reinforcement learning. Journal of Artificial Intelligence Research, 76:201\u2013264, 2023.\\n\\nKostrikov, I., Nair, A., and Levine, S. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.\\n\\nKumar, A., Zhou, A., Tucker, G., and Levine, S. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191, 2020.\\n\\nKumar, A., Hong, J., Singh, A., and Levine, S. Should i run offline reinforcement learning or behavioral cloning? In International Conference on Learning Representations, 2021.\\n\\nLee, K.-H., Nachum, O., Yang, M., Lee, L., Freeman, D., Xu, W., Guadarrama, S., Fischer, I., Jang, E., Michalewski, H., et al. Multi-game decision transformers. arXiv preprint arXiv:2205.15241, 2022.\\n\\nLevine, S., Kumar, A., Tucker, G., and Fu, J. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\\n\\nLi, A., Pinto, L., and Abbeel, P. Generalized hindsight for reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020a.\\n\\nLi, D., Yang, Y., Song, Y.-Z., and Hospedales, T. Learning to generalize: Meta-learning for domain generalization. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\\n\\nLi, J., Koyamada, S., Ye, Q., Liu, G., Wang, C., Yang, R., Zhao, L., Qin, T., Liu, T.-Y., and Hon, H.-W. Suphx: Mastering mahjong with deep reinforcement learning. arXiv preprint arXiv:2003.13590, 2020b.\\n\\nLin, Y., Dong, H., Wang, H., and Zhang, T. Bayesian invariant risk minimization. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16000\u201316009, 2022a.\\n\\nLin, Y., Zhu, S., Tan, L., and Cui, P. Zin: When and how to learn invariance without environment partition? In Neural Information Processing Systems, 2022b.\\n\\nMa, C., Wen, J., and Bengio, Y. Universal successor representations for transfer reinforcement learning. arXiv preprint arXiv:1804.03758, 2018.\"}"}
{"id": "yang23q", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "yang23q", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nWang, K., Kang, B., Shao, J., and Feng, J. Improving generalization in reinforcement learning with mixture regularization. Advances in Neural Information Processing Systems, 33:7968\u20137978, 2020.\\n\\nWang, Q., Xiong, J., Han, L., Liu, H., Zhang, T., et al. Exponentially weighted imitation learning for batched historical data. Advances in Neural Information Processing Systems, 31, 2018.\\n\\nXu, M., Shen, Y., Zhang, S., Lu, Y., Zhao, D., Tenenbaum, J., and Gan, C. Prompting decision transformer for few-shot policy generalization. In International Conference on Machine Learning, pp. 2463\u201324645. PMLR, 2022.\\n\\nXu, T., Li, Z., and Yu, Y. Error bounds of imitating policies and environments. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\\n\\nYang, R., Fang, M., Han, L., Du, Y., Luo, F., and Li, X. Mher: Model-based hindsight experience replay. In Deep RL Workshop NeurIPS 2021, 2021a.\\n\\nYang, R., Bai, C., Ma, X., Wang, Z., Zhang, C., and Han, L. Rorl: Robust offline reinforcement learning via conservative smoothing. In Advances in Neural Information Processing Systems, 2022a.\\n\\nYang, R., Lu, Y., Li, W., Sun, H., Fang, M., Du, Y., Li, X., Han, L., and Zhang, C. Rethinking goal-conditioned supervised learning and its connection to offline rl. In International Conference on Learning Representations, 2022b.\\n\\nYang, Y., Ma, X., Li, C., Zheng, Z., Zhang, Q., Huang, G., Yang, J., and Zhao, Q. Believe what you see: Implicit constraint approach for offline multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 34:10299\u201310312, 2021b.\\n\\nYong, L., Pi, R., Zhang, W., Xia, X., Gao, J., Zhou, X., Liu, T., and Han, B. A holistic view of label noise transition matrix in deep learning and beyond. In The Eleventh International Conference on Learning Representations, 2023.\\n\\nYu, T., Kumar, A., Rafailov, R., Rajeswaran, A., Levine, S., and Finn, C. Combo: Conservative offline model-based policy optimization. Advances in neural information processing systems, 34:28954\u201328967, 2021.\\n\\nZhang, C., Zhang, L., and Ye, J. Generalization bounds for domain adaptation. Advances in neural information processing systems, 25, 2012.\\n\\nZhao, R., Sun, X., and Tresp, V. Maximum entropy-regularized multi-goal reinforcement learning. In International Conference on Machine Learning, pp. 7553\u20137562. PMLR, 2019.\\n\\nZhou, K., Yang, Y., Hospedales, T., and Xiang, T. Learning to generate novel domains for domain generalization. In European conference on computer vision, pp. 561\u2013578. Springer, 2020.\\n\\nZhou, K., Liu, Z., Qiao, Y., Xiang, T., and Loy, C. C. Domain generalization: A survey. 2021a.\\n\\nZhou, K., Yang, Y., Qiao, Y., and Xiang, T. Domain generalization with mixstyle. arXiv preprint arXiv:2104.02008, 2021b.\\n\\nZhou, X., Lin, Y., Zhang, W., and Zhang, T. Sparse invariant risk minimization. In International Conference on Machine Learning, 2022.\\n\\nZhou, X., Lin, Y., Pi, R., Zhang, W., Xu, R., Cui, P., and Zhang, T. Model agnostic sample reweighting for out-of-distribution learning. ArXiv, abs/2301.09819, 2023.\"}"}
{"id": "yang23q", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nFigure 8. (a) Visualization of three 2D goal-reaching datasets, and two groups of evaluation goals, \u201cR10\u201d and \u201cR20\u201d, with a radius of 10 and 20, respectively. (b) Average success rates of different agents over 5 random seeds.\\n\\nFigure 9. The reciprocal of uncertainty is an estimation of density. The uncertainty is measured by the variance of ensemble value functions for initial state \\\\((0,0)\\\\) and goals on \\\\([-10,10] \\\\times [-10,10]\\\\).\\n\\nD.2. Uncertainty and Density\\nTo verify if the estimated uncertainty can approximate density, we visualize the value of \\\\(\\\\frac{1}{\\\\text{Std}}(s_0, g)\\\\) in Figure 9, where \\\\text{Std} is the standard deviation of a group of 5 value networks. To calculate values on the figure, we set the state \\\\(s_0\\\\) as \\\\((0,0)\\\\) and use the value functions of GOAT to calculate each \\\\(\\\\frac{1}{\\\\text{Std}}(s_0, g)\\\\), where \\\\(g\\\\) is set on a \\\\([-10,10] \\\\times [-10,10]\\\\) grid with equal intervals of 1. We can observe that positions with more achieved goals (i.e., near the red stars) have larger values \\\\(\\\\frac{1}{\\\\text{Std}}(s_0, g)\\\\), which validates the relationship between uncertainty and density.\\n\\nTable 8. Ablation on ensemble size for GOAT over all 26 tasks.\\n\\n| Success Rate (%) | \\\\(N = 2\\\\) | \\\\(N = 3\\\\) | \\\\(N = 5\\\\) (default) | \\\\(N = 7\\\\) |\\n|------------------|-----------|-----------|---------------------|-----------|\\n| Average IID      | 90.4      | 90.4      | 90.3                | 90.6      |\\n| Average OOD      | 64.0      | 66.7      | 67.9                | 65.3      |\\n\\nD.3. Ablation on the Ensemble Size\\nThe ensemble size \\\\(N\\\\) of GOAT can affect the value estimation and the uncertainty estimation. As shown in Table 8, though the IID performance is similar, \\\\(N = 5\\\\) works better than \\\\(N \\\\in \\\\{2, 3, 7\\\\}\\\\) on the average OOD success rate. Note that with different \\\\(N\\\\), the average OOD success rate is still better than that of WGCSL (i.e., 62.1 for OOD tasks).\\n\\nD.4. Additional Ablations of GOAT\\nTaking into account additional design considerations, we also revisit two design choices, namely the value function and the exponential weight. Specifically, in GOAT, we adopt the approach proposed by WGCSL (Yang et al., 2022b) to learn Q functions for estimating the advantage value, which is given by\\n\\n\\\\[\\nA(s_t, a_t, g) = r(s_t, a_t, g) + \\\\gamma Q(s_{t+1}, \\\\pi(s_{t+1}, g), g) - Q(s_t, \\\\pi(s_t, g), g).\\n\\\\]\\n\\nThe difference is that Q values are averaged by an ensemble of Q functions. Alternatively, we can estimate the advantage values by learning V functions \\\\(V(s, g)\\\\), which can be expressed as\\n\\n\\\\[\\nA(s_t, a_t, g) = r(s_t, a_t, g) + \\\\gamma V(s_{t+1}, g) - V(s_t, g).\\n\\\\]\\n\\nNote that the learned action \\\\(\\\\pi\\\\) is not needed in the value function learning for \\\\(V(s, g)\\\\). We use the...\"}"}
{"id": "yang23q", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 9. Additional ablations of GOAT. Average success rates (\\\\%) with standard deviation over 5 random seeds.\\n\\n| Task Group | Task | GOAT | GOAT(V) | GOAT(V+ \\\\(\\\\chi^2\\\\)) |\\n|------------|------|------|---------|----------------------|\\n| Right      | 100.0 \u00b1 0.0 | 99.9 \u00b1 0.2 | 100.0 \u00b1 0.0 |\\n| Reach Left-Right | 99.0 \u00b1 2.0 | 94.5 \u00b1 7.5 | 96.6 \u00b1 2.5 |\\n| Near      | 100.0 \u00b1 0.0 | 99.6 \u00b1 0.8 | 100.0 \u00b1 0.0 |\\n| Reach Near-Far | 97.6 \u00b1 1.1 | 89.8 \u00b1 4.5 | 90.6 \u00b1 1.0 |\\n| Right2Right | 95.9 \u00b1 1.2 | 92.6 \u00b1 3.4 | 93.9 \u00b1 1.6 |\\n| Right2Left | 69.3 \u00b1 6.6 | 48.9 \u00b1 5.8 | 52.0 \u00b1 5.8 |\\n| Push Left-Right | 76.0 \u00b1 7.4 | 56.2 \u00b1 5.1 | 56.6 \u00b1 11.2 |\\n| Left2Right | 61.1 \u00b1 7.6 | 39.5 \u00b1 4.0 | 34.1 \u00b1 5.1 |\\n| Left2Left | 92.0 \u00b1 2.6 | 92.2 \u00b1 2.2 | 87.5 \u00b1 1.7 |\\n| Push Near-Far | 69.5 \u00b1 3.6 | 65.0 \u00b1 3.9 | 63.4 \u00b1 3.7 |\\n| Far2Near | 69.5 \u00b1 3.6 | 65.0 \u00b1 3.9 | 63.4 \u00b1 3.7 |\\n| Far2Far | 50.8 \u00b1 1.8 | 41.3 \u00b1 2.2 | 43.5 \u00b1 4.3 |\\n| Average | 99.5 97.2 98.3 |\\n| Near2Near | 92.0 \u00b1 2.6 | 92.2 \u00b1 2.2 | 87.5 \u00b1 1.7 |\\n| Near2Far | 70.3 \u00b1 5.7 | 59.4 \u00b1 5.1 | 63.8 \u00b1 3.9 |\\n| Push Near-Far | 69.5 \u00b1 3.6 | 65.0 \u00b1 3.9 | 63.4 \u00b1 3.7 |\\n| Far2Far | 50.8 \u00b1 1.8 | 41.3 \u00b1 2.2 | 43.5 \u00b1 4.3 |\\n| Average | 99.5 97.2 98.3 |\\n\\nThe notation \\\"GOAT(V)\\\" to describe this variant of GOAT with V functions. Furthermore, the exponential advantage weight is a special case of maximizing expected value and minimizing the \\\\(f\\\\)-divergence. Following (Ma et al., 2022b), we also consider replacing the exponential weight with \\\\(\\\\max(A(s, a, g) + c \\\\chi^2, 0)\\\\), which is derived by considering the \\\\(\\\\chi^2\\\\)-divergence. We denote GOAT with both V function and \\\\(\\\\chi^2\\\\)-divergence as \\\"GOAT(V+\\\\(\\\\chi^2\\\\))\\\". After the parameter search, we find \\\\(c_{\\\\chi^2} = 0\\\\) performs well for GOAT(V+\\\\(\\\\chi^2\\\\)).\\n\\nThe final results are reported in Table 9. Our results indicate that, on average across the 17 out-of-distribution tasks, the use of a V function in GOAT significantly reduces the OOD generalization performance, with negligible impact from the \\\\(\\\\chi^2\\\\)-divergence. It is worth noting, however, that for the high-dimensional HandReach task, the best performance is achieved by incorporating both a V function and \\\\(\\\\chi^2\\\\)-divergence into GOAT. The rationale behind this finding lies in the high-dimensional nature of the state-goal and action spaces in the HandReach task. As a result of this high dimensionality, the multimodal problem is more pronounced, rendering the learning of a V function useful for achieving better stability than the Q function. Moreover, the integration of a weighting function induced by the \\\\(\\\\chi^2\\\\)-divergence serves to eliminate inferior data and also alleviate the multimodal problems arising in such high-dimensional spaces. Therefore, the choice of value function and weighting function also depends on the task characteristics.\\n\\nD.5. The Effectiveness of Expectile Regression\\n\\nKostrikov et al. (2021) proposed IQL to combine expectile regression with weighted imitation learning in offline RL. The difference is that we do not learn additional V function to avoid OOD actions when learning value functions and we validate its effectiveness for improving advantage value estimation of offline GCRL. As shown in Figure 10, expectile regression (ER) improves both WGCSL and GOAT by around 3 points on average OOD success rates. Besides, in Figure 11 we show...\"}"}
{"id": "yang23q", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nFigure 10.\\n\\nAblation of Expectile Regression\\n\\nSuccess Rate (%)\\n\\n|       | WGCSL | WGCSL+ER | GOAT | GOAT+ER |\\n|-------|-------|----------|------|---------|\\n| Average | 71.1  | 73.6     | 75.7 | 77.9    |\\n| Average OOD | 62.1  | 65.1     | 67.9 | 70.9    |\\n\\nFigures 11-12.\\n\\nComparing different $\\\\tau$ in Expectile Regression for Weighted Imitation Learning.\\n\\nthat the performance of WGCSL+ER improves with the decrease of $\\\\tau$. We conjecture that the smaller $\\\\tau$ is, the more accurate the relative relationship of the advantage values are, and thus the better the estimation of the expert policy for imitation.\\n\\nDDPG+HER\\n\\nCQL+HER\\n\\nEnsemble\\n\\nMARVIL+HER\\n\\n Ensemble\\n\\nFigure 12.\\n\\n(a) Comparison between methods with and without value function ensemble. (b) Comparison of GoFAR and MARVIL with and without HER. (c) Comparison between WGCSL and WGCSL with value underestimation.\\n\\nD.6. Ablations of Ensemble and HER for Other GCRL Algorithms\\n\\nIn Figure 12(a) and Figure 12(b), we demonstrate that ensemble value functions and HER can improve the performance of different RL algorithms, indicating that they are generally useful techniques for OOD generalization of offline GCRL.\\n\\nTable 10.\\n\\nAblations of GoFAR on the HandReach task. Average success rates (%) with standard deviation over 5 random seeds.\\n\\n| Task | Group | Task | GoFAR | GoFAR(binary) | GoFAR(binary+Q) | GoFAR(binary+exp) | GoFAR(binary+relabel) | GoFAR(binary+exp+relabel) |\\n|------|-------|------|-------|---------------|----------------|--------------------|------------------------|---------------------------|\\n| Near |       |      | 77.4  | \u00b1 1.7         | 78.9 \u00b1 3.3     | 4.9 \u00b1 5.0          | 10.8 \u00b1 2.7             | 62.0 \u00b1 7.9                |\\n| Near-Far |      |      | 36.9  | \u00b1 3.1         | 39.2 \u00b1 4.3     | 3.1 \u00b1 1.8          | 2.6 \u00b1 1.4              | 29.3 \u00b1 8.2                |\\n| Average |      |      | 57.1  |               | 59.0 \u00b1 4.0     | 4.0 \u00b1 1.7          | 6.7 \u00b1 2.3              | 45.7 \u00b1 7.9                |\\n\\nD.7. Ablations of GoFAR\\n\\nGoFAR (Ma et al., 2022b) is a recent offline GCRL method which is also based on weighted imitation learning, but we observe its performance drops significantly on OOD tasks. We conjecture the primary reason is that GoFAR does not use goal relabeling. To validate our conjecture, we compare GoFAR and GoFAR+HER in Figure 12(b). The results show that HER does not improve the overall performance of GoFAR but increases the average OOD success rates by a large margin. In Figure 12(b), the performance increment of GoFAR+HER over GoFAR also matches that of MARVIL+HER over MARVIL.\\n\\nIn our benchmark experiments, we observe that GoFAR has an advantage on the high-dimensional HandReach task over\"}"}
{"id": "yang23q", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nWGCSL and GOAT. Given this observation, we investigate and analyze the roles played by the key techniques of GoFAR\u2019s design, namely the discriminator-based rewards, the advantage estimation, and the weighting function. We subsequently compare the following variants:\\n\\n- GoFAR: it employs discriminator-based rewards, learns $V(s, g)$ function to estimate the advantage value $A(s_t, a_t, g) = r(s_t, a_t, g) + \\\\gamma V(s_{t+1}, g) - V(s_t, g)$, weights the imitation loss with $\\\\max(A(s_t, a_t, g) + 1, 0)$, and does not use goal relabeling;\\n- GoFAR(binary): it replaces the discriminator-based rewards with binary rewards $r(s_t, a_t, g) = 1[\\\\|\\\\phi(s_t) - g\\\\|^2_2 \\\\leq \\\\epsilon]$;\\n- GoFAR(binary+Q): it uses binary rewards to learn $Q(s, a, g)$ functions instead of $V$ functions, then the advantage value are estimated by $A(s_t, a_t, g) = r(s_t, a_t, g) + \\\\gamma Q(s_{t+1}, \\\\pi(s_{t+1}, g), g) - Q(s_t, \\\\pi(s_t, g), g)$;\\n- GoFAR(binary+exp): it also employs binary rewards to learn $V$ functions, but includes a weighting term of $\\\\exp(A(s, a, g))$ based on the KL divergence;\\n- GoFAR(binary+relabel): it uses binary rewards and goal relabeling for GoFAR;\\n- GoFAR(binary+exp+relabel): it utilizes binary rewards and goal relabeling to learn $V$ functions, and the exponential weighting function $\\\\exp(A(s, a, g))$ for weighted imitation learning.\\n\\nTable 10 presents the findings of this study, which suggest that the value function and weighting function are the most crucial components of GoFAR for the high-dimensional HandReach task. \u201cGoFAR(binary+Q)\u201d and \u201cGoFAR(binary+exp)\u201d both fail on this task. We posit that this maybe due to the following reasons: (1) with high-dimensional state-goal and action spaces, the Q value function has more dimensions as input than the V function and is therefore less stable and harder to train. Additionally, the learned policy $\\\\pi$ via weighted imitation learning is prone to interpolation into out-of-distribution actions, which causes imprecise advantage value estimation using $Q(s, \\\\pi(s, g), g)$. (2) The weighting function $\\\\max(A(s, a, g) + 1, 0)$ also serves the purpose of clearing poor quality data from our weighted imitation learning, whereas the exponential weighting function is more sensitive to the multimodal problem. Furthermore, our results indicate that the discriminator-based rewards play no significant role and may even decrease performance when compared to binary rewards. It is also worth noting that the effectiveness of goal relabeling varies with the type of weighting function. While it diminishes performance for $\\\\max(A(s, a, g) + 1, 0)$, it enhances the performance of weighting with $\\\\exp(A(s, a, g))$.\\n\\nD.8. Combining Weighted Imitation Learning with Value Underestimation\\n\\nThe value function learning has an impact on the estimation of the expert policy for imitation. It is interesting to see whether simply underestimating values for weighted imitation learning is also helpful for OOD generalization. In Figure 12(c), we consider two methods to be on top of WGCSL, CQL (Kumar et al., 2020), and IQL (Kostrikov et al., 2021) (specifically, the expectile regression technique). We demonstrate that while CQL is not helpful for WGCSL, while the expectile regression in IQL is a good choice for better value function estimation of offline GCRL.\\n\\nD.9. Discounted Relabeling Weight (DRW)\\n\\nYang et al. (2022b) introduced the Discounted Relabeling Weight (DRW) for offline GCRL. For a relabeled transition $(s_t, a_t, \\\\phi(s_i))$, $i \\\\geq t$, DRW is defined as $\\\\gamma^{i-t}$, which has an effect of optimizing a tighter lower bound for offline GCRL. Intuitively, DRW assigns relatively larger weights on closer relabeling goals with smaller $i$. In Table 11, we find DRW can slightly improve the IID performance but reduce the average OOD performance of WGCSL. It is reasonable because DRW assigns larger weights for closer goals, which contradicts the Uncertainty Weight as discussed in Section 5.2 and has the risk of overfitting simpler goals.\\n\\nTable 11. Comparison of WGCSL and WGCSL+DRW on 26 tasks.\\n\\n|               | Success Rate (%) |\\n|---------------|------------------|\\n| Average IID   | WGCSL | 88.1   |\\n|               | WGCSL+DRW | 88.4   |\\n| Average OOD   | WGCSL | 62.1   |\\n|               | WGCSL+DRW | 59.5   |\"}"}
{"id": "yang23q", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nD.10. Adaptive Data Selection Weight\\n\\nYang et al. (2022b) introduced the Data Selection Weight (DSW) to tackle the multi-modal problem in multi-goal datasets, which also improves the OOD generalization performance through narrowing the expert estimation gap. However, the introduced approach utilizes a global threshold for all \\\\((s, g)\\\\) pairs.\\n\\nIs it helpful to include an adaptive threshold function for different \\\\((s, g)\\\\) pairs? This can be done using expectile regression for advantage values, i.e., learning a function \\\\(f(s, g)\\\\) to estimate the \\\\(\\\\beta\\\\) expectile value of the distribution of \\\\(A(s, g, a)\\\\).\\n\\n\\\\[\\n\\\\begin{align*}\\nL_f &= \\\\mathbb{E}_{(s, a, g) \\\\sim D_{relabel}} \\\\left[ L_{\\\\beta}^2 \\\\left( A(s, a, g) - f(s, g) \\\\right) \\\\right] \\\\\\\\\\n\\\\beta &\\\\text{ is the hyper-parameter similar to } \\\\alpha \\\\text{ in the original DSW controlling the quality of data used for weighted imitation learning.}\\n\\\\end{align*}\\n\\\\]\\n\\nFinally, the adaptive data selection weight is \\\\(\\\\varepsilon(A(s, a, g)) = 1[A(s, a, g) \\\\geq f(s, g)]\\\\). We compare WGCSL and WGCSL with adaptive data selection weight (ADSW) on Push and Pick task groups. As shown in Table 12, WGCSL with ADSW only brings slight improvement over global threshold. For most tasks, WGCSL also obtains top two scores with a simpler data selection method. Considering the extra computation of learning the threshold function \\\\(f\\\\), we do not include it in GOAT. But we believe it is a good start for future research on how to achieve more efficient adaptive data selection for offline goal-conditioned RL.\\n\\nTable 12. Comparison with Adaptive Data Selection Weight (ADSW). Average success rates (%) with standard deviation over 5 random seeds. Top two scores for each task are highlighted.\\n\\n| Task Group | Task | WGCSL, \\\\(\\\\beta = 0.8\\\\) | ADSW, \\\\(\\\\beta = 0.8\\\\) | ADSW, \\\\(\\\\beta = 0.9\\\\) | ADSW, \\\\(\\\\beta = 0.95\\\\) |\\n|------------|------|------------------------|------------------------|------------------------|------------------------|\\n| Right2Right | 93.2 \u00b1 0.9 | 95.1 \u00b1 1.4 | 95.5 \u00b1 0.8 | 92.8 \u00b1 1.9 |\\n| Right2Left | 63.3 \u00b1 8.9 | 65.3 \u00b1 4.7 | 69.5 \u00b1 6.4 | 62.5 \u00b1 7.4 |\\n| Push Left-Right | 67.6 \u00b1 7.1 | 74.9 \u00b1 8.6 | 74.6 \u00b1 6.0 | 64.3 \u00b1 13.1 |\\n| Left2Right | 47.7 \u00b1 7.4 | 59.8 \u00b1 3.7 | 66.4 \u00b1 5.9 | 52.1 \u00b1 6.9 |\\n| Average | 68.0 | 73.8 | 76.5 | 67.9 |\\n| Near2Near | 93.5 \u00b1 1.0 | 90.7 \u00b1 2.8 | 93.4 \u00b1 1.0 | 91.9 \u00b1 1.2 |\\n| Near2Far | 67.0 \u00b1 5.4 | 68.1 \u00b1 4.2 | 69.4 \u00b1 3.8 | 63.4 \u00b1 5.0 |\\n| Push Near-Far | 68.0 \u00b1 2.4 | 66.1 \u00b1 2.6 | 67.6 \u00b1 2.3 | 62.1 \u00b1 4.0 |\\n| Far2Near | 51.1 \u00b1 4.7 | 47.4 \u00b1 3.4 | 53.1 \u00b1 1.9 | 40.2 \u00b1 6.1 |\\n| Far2Far | 47.7 \u00b1 7.4 | 59.8 \u00b1 3.7 | 66.4 \u00b1 5.9 | 52.1 \u00b1 6.9 |\\n| Average | 69.9 | 68.1 | 70.9 | 64.4 |\\n| Right2Right | 93.8 \u00b1 5.3 | 96.6 \u00b1 2.6 | 94.4 \u00b1 4.1 | 96.0 \u00b1 2.0 |\\n| Right2Left | 89.4 \u00b1 3.9 | 83.5 \u00b1 6.4 | 71.1 \u00b1 9.4 | 78.9 \u00b1 11.2 |\\n| Pick Left-Right | 90.0 \u00b1 4.1 | 89.8 \u00b1 4.8 | 91.6 \u00b1 4.7 | 89.3 \u00b1 2.8 |\\n| Left2Right | 87.0 \u00b1 5.1 | 83.0 \u00b1 5.7 | 73.3 \u00b1 8.6 | 78.6 \u00b1 8.4 |\\n| Average | 90.0 | 88.2 | 82.6 | 85.7 |\\n| Low | 98.6 \u00b1 1.3 | 99.8 \u00b1 0.2 | 99.1 \u00b1 0.6 | 99.4 \u00b1 0.6 |\\n| Pick Low-High | 66.6 \u00b1 6.6 | 63.4 \u00b1 5.1 | 66.8 \u00b1 13.5 | 63.5 \u00b1 8.6 |\\n| High | 66.6 \u00b1 6.6 | 63.4 \u00b1 5.1 | 66.8 \u00b1 13.5 | 63.5 \u00b1 8.6 |\\n| Average | 82.6 | 81.6 | 83.0 | 81.4 |\\n\\nTable 13. Varying hyper-parameter of MSG over all 26 tasks. Success Rate (%)\\n\\n| Hyper-parameter | Average IID | Average OOD |\\n|-----------------|-------------|-------------|\\n| 1               | 60.1        | 41.0        |\\n| 3               | 65.4        | 42.4        |\\n| 5               | 68.5        | 43.1        |\\n| 7               | 69.9        | 39.4        |\\n\\nD.11. MSG+HER with Varying Hyper-parameter\\n\\nMSG (Ghasemipour et al., 2022) is a recent SOTA ensemble-based offline RL method, which learns a group of independent Q networks and estimates a Lower Confidence Bound (LCB) objective with the standard deviation (std) of Q networks.\\n\\nOne important hyper-parameter for MSG+HER is the weight parameter \\\\(c\\\\) for the std (see Appendix C.2). We compare the performance of MSG+HER with varying \\\\(c \\\\in \\\\{1, 3, 5, 7\\\\}\\\\) in Table 13. The results demonstrate that \\\\(c = 5\\\\) achieves the best OOD generalization results. When \\\\(c\\\\) is larger than \\\\(5\\\\) (i.e., \\\\(c = 7\\\\)), the agent is too conservative to generalize, leading to improvement on IID tasks but decrease on OOD tasks. GOAT still outperforms MSG+HER by a large margin, which also supports that pessimism-based offline RL method can inhibit generalization.\"}"}
{"id": "yang23q", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nFigure 13. Online fine-tuning of different supervised learning methods in FetchPush Left-Right and FetchPick Left-Right tasks.\\n\\nD.12. Online Fine-tuning\\nTo understand the effect of the generalization ability of pre-trained agents for online learning, we design an experiment to fine-tune pre-trained agents with online samples. The pre-trained agents are trained on offline datasets with partial coverage (e.g., Right2Right) and evaluated with full coverage goals (Right2Right, Right2Left, Left2Right, Left2Left). In the fine-tuning period, agents explore with additional Gaussian noise (zero mean and 0.2 standard deviation) and random actions (with a probability of 0.3). For all pre-trained agents, we fine-tune the policy and value function for 10 (FetchPick) or 20 (FetchPush) batches after every a trajectory collected. The training batch size is 512, the learning rate is $5 \\\\times 10^{-4}$, and the optimizer is Adam. Note that we do not use offline datasets during online fine-tuning, and we fine-tune agents to goals not seen in the pre-training phase, which is different from prior offline-to-online setting (Nair et al., 2020b).\\n\\nFor the first experiment in Figure 6, all pre-trained agents are fine-tuned with DDPG+HER (Andrychowicz et al., 2017), which is a general baseline in the online setting. In addition to DDPG+HER, we apply different supervised learning methods, i.e, GOAT, WGCSL, MARVIL+HER, GCSL for online fine-tuning in Figure 13. The fine-tuning algorithms are the same as their pre-training algorithms. Other settings are kept the same as the above fine-tuning experiments. Comparing Figure 13 with Figure 6, we can also conclude that (1) off-policy method (i.e., DDPG+HER) is more efficient than supervised methods for online fine-tuning, (2) GOAT substantially outperforms other supervised methods such as WGCSL and MARVIL+HER when fine-tuned using their respective pre-training algorithms.\\n\\nD.13. Training Time\\nWe consider the training time as a measure of computational cost in Figure 14. For our experiments, we use one single GPU (NVIDIA GeForce RTX 2080 Ti 11 GB) and one cpu core (Intel Xeon W-2245 CPU @ 3.90GHz). Among all the algorithms, BC and GCSL require the least training time due to their simplicity, but they suffer to generalize given non-expert datasets. WGCSL, MARVIL+HER and DDPG+HER need more training time because they are equipped with additional Q networks. Besides, MSG leverages an ensemble of Q networks and averages their gradients to the policy, leading to the longest training time. CQL+HER requires the second longest training time because of the OOD action sampling and the logsumexp approximation procedures. Though GoFAR is also a weighted imitation method similar to WGCSL, it is the second slowest method because it uses additional discriminator for reward estimation and it is implemented based on Torch. GOAT introduces ensemble networks, expectile regression and uncertainty estimation on top of WGCSL, thus increasing the computational cost. Thanks to the efficient implementation based on tensorflow, GOAT is still more efficient than CQL+HER, and GoFAR, and requires affordable computational cost.\\n\\nD.14. Random Network Distillation as the Uncertainty Measurement\\nThe uncertainty weight is an empirical instance to estimate the density under our framework. Random Network Distillation (RND) (Burda et al., 2018) is a potential alternative for density estimation in the continuous state space. We implement\"}"}
{"id": "yang23q", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Table 14.** Average success rates (%) with standard deviation over 5 random seeds.\\n\\n| Tasks            | GOAT    | GOAT(RND) | WGCSL   |\\n|------------------|---------|-----------|---------|\\n| Reach Left-Right | 99.5    | 99.7      | 98.9    |\\n| Reach Near-Far   | 98.8    | 95.1      | 94.5    |\\n| Push Left-Right  | 75.6    | 76.3      | 68.0    |\\n| Push Near-Far    | 70.6    | 69.2      | 69.9    |\\n| Pick Left-Right  | 92.0    | 91.6      | 90.0    |\\n| Pick Low-High    | 85.8    | 85.1      | 82.6    |\\n| Slide Left-Right | 57.4    | 53.1      | 48.3    |\\n| Slide Near-Far   | 53.0    | 49.3      | 45.2    |\\n| HandReach Near-Far | 55.2 | 53.9      | 50.9    |\\n| Average IID Tasks | 90.3   | 89.4      | 88.1    |\\n| Average OOD Tasks | 67.9   | 66.0      | 62.1    |\\n\\n**D.15. Full Benchmark Experiments**\\n\\nAs demonstrated in Table 15 and Table 16, we include more baselines (i.e., IQL+HER and MARVIL+HER) and additional measures (i.e., average cumulative return) for the benchmark experiments. GOAT achieves the highest average success rate and average return on the benchmark. Other conclusions are consistent with Section 5.3.\\n\\n**E. Additional Related Works**\\n\\nIn ML community, there are different types of OOD studied by prior works (Nair et al., 2020a; Ma et al., 2022a; Han et al., 2021; Hansen-Estruch et al., 2022; Pitis et al., 2022), e.g., handling spurious feature, assuming Factored MDPs, or learning generalizable representations for different objects or scenes. Different from these works, our work focuses on the OOD goal generalization problem, which is essentially a type of covariate shift. In practical applications, more than one type of OOD is generally involved. The work (Hong et al., 2022) studies a similar goal generalization setting of our work, but it is in the online setting with exploration. Instead, we consider the offline setting, where online interaction is prohibited and commonly used pessimism-based method can inhibit OOD generalization.\"}"}
{"id": "yang23q", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 15. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n| Task                  | Average | OOD Tasks |\\n|-----------------------|---------|-----------|\\n| HandReach Near-Far    | 55.2    | 88.1      |\\n| Slide Near-Far        | 64.8    | 57.4      |\\n| Slide Left-Right      | 79.6    | 70.6      |\\n| Pick Low-High         | 88.8    | 82.6      |\\n| Pick Left-Right       | 89.3    | 78.0      |\\n| Push Near-Far         | 99.9    | 99.3      |\\n| Push Left-Right       | 99.9    | 99.3      |\\n| Reach Near-Far        | 98.9    | 98.9      |\\n| Far2Near              | 97.3    | 97.3      |\\n| Near2Far              | 97.3    | 97.3      |\\n| Near2Near             | 97.3    | 97.3      |\\n| Left2Left             | 97.3    | 97.3      |\\n| Left2Right            | 97.3    | 97.3      |\\n| Right2Left            | 97.3    | 97.3      |\\n| Right2Right           | 97.3    | 97.3      |\\n\\nThe top two scores for each task are highlighted.\"}"}
{"id": "yang23q", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nRui Yang, Yong Lin, Xiaoteng Ma, Hao Hu, Chongjie Zhang, Tong Zhang\\n\\nAbstract\\nOffline goal-conditioned RL (GCRL) offers a way to train general-purpose agents from fully offline datasets. In addition to being conservative within the dataset, the generalization ability to achieve unseen goals is another fundamental challenge for offline GCRL. However, to the best of our knowledge, this problem has not been well studied yet. In this paper, we study out-of-distribution (OOD) generalization of offline GCRL both theoretically and empirically to identify factors that are important. In a number of experiments, we observe that weighted imitation learning enjoys better generalization than pessimism-based offline RL method. Based on this insight, we derive a theory for OOD generalization, which characterizes several important design choices. We then propose a new offline GCRL method, Generalizable Offline goal-conditioned RL (GOAT), by combining the findings from our theoretical and empirical studies. On a new benchmark containing 9 independent identically distributed (IID) tasks and 17 OOD tasks, GOAT outperforms current state-of-the-art methods by a large margin.\\n\\n1. Introduction\\nDeep reinforcement learning (DRL) makes it possible for a learning agent to achieve superhuman performance on a range of challenging tasks (Silver et al., 2016; 2018; Vinyals et al., 2019; Li et al., 2020b). However, recent studies have found that DRL is prone to overfitting the training tasks and is sensitive to environmental changes (Cobbe et al., 2019; Wang et al., 2020; Han et al., 2021; Kirk et al., 2023). Goal-conditioned reinforcement learning (GCRL) is gaining increasing attention because it enables learning general-purpose decision-making rather than overfitting to a single task (Andrychowicz et al., 2017; Ghosh et al., 2019; Li et al., 2020a). Particularly, offline GCRL (Chebotar et al., 2021; Yang et al., 2022b), which learns as many skills as possible from previously collected datasets without any exploration in the environment, is promising for large-scale and general-purpose pre-training. Nevertheless, prior works (Chebotar et al., 2021; Yang et al., 2022b; Ma et al., 2022b) have largely focused on reaching goals in the dataset, without systematically studying the problem of out-of-distribution (OOD) goal generalization. There are a number of questions: what is the OOD generalization performance of current offline GCRL algorithms? And more importantly, what is essential for OOD generalization of offline GCRL?\\n\\nTo answer these questions, we first design a 2D goal-reaching task with different types of offline data. We find that (1) pessimism-based offline RL is restrained from generalizing to OOD goals and (2) imitation learning overfits the data noise and fails to generalize when given non-expert data. On the contrary, (3) weighted imitation learning is a strong baseline for OOD generalization across different types of training data. The observation motivates us to derive a generalization theory from the perspective of domain generalization (Muandet et al., 2013; Zhang et al., 2012; Zhou et al., 2021a). Through analyzing our theory, we find several techniques that are essential to minimize the generalization bound, including advantage re-weighting, data selection, density re-weighting, and goal-relabeling. Particularly, we find re-weighting the training state-goal distribution with the reciprocal of its density can minimize the worst-case distribution shift. Based on these results, we propose, Generalizable Offline goal-conditioned RL (GOAT), by integrating these techniques into a general weighted imitation learning framework, which encourages optimistic goal sampling while still maintaining pessimism on action selection.\\n\\nDue to the lack of benchmarks for evaluating the OOD generalization performance of offline GCRL, we develop a challenging robot manipulation benchmark based on a robotic arm or an anthropomorphic hand. The benchmark comprises nine offline datasets and 26 evaluation tasks, 9 of which contain independent and identically distributed (IID) tasks.\"}"}
{"id": "yang23q", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nFigure 1. Training datasets and trajectories generated by different agents trained on \u201cExpert 10\u201d and \u201cNon-Expert 10\u201d datasets.\\n\\nIn our experiments, we demonstrate that GOAT considerably improves the OOD generalization performance of existing offline GCRL methods, as well as enhances efficiency in online fine-tuning for unseen goals. Furthermore, we conduct in-depth ablation studies to validate the effectiveness of each component used in GOAT, which may benefit future research on OOD generalization for offline RL.\\n\\n2. Preliminaries\\n\\n2.1. Goal-conditioned RL\\n\\nGoal-conditioned RL (GCRL) considers a goal-augmented Markov Decision Process (GMDP), denoted by a tuple $(S, A, G, P, r, \\\\gamma)$. $S, G, A$ refer to state, goal, and action spaces, respectively. $\\\\gamma$ is the discount factor, and $r: S \\\\times G \\\\times A \\\\rightarrow \\\\mathbb{R}$ is the goal-conditioned reward function. Generally, we consider a sparse and binary reward function $r(s, a, g) = 1[\\\\|\\\\phi(s) - g\\\\|_2^2 \\\\leq \\\\delta]$, where $\\\\delta$ is a threshold and $\\\\phi$ is a known state-to-goal mapping (Andrychowicz et al., 2017). A policy $\\\\pi: S \\\\times G \\\\rightarrow A$ aims to maximize the expected return:\\n\\n$$J(\\\\pi) = \\\\mathbb{E}_{g \\\\sim p(g), s_0 \\\\sim \\\\mu(s_0), s_t + 1 \\\\sim P(s|s_t, a_t)} \\\\sum_{t=0}^{\\\\infty} \\\\gamma^t r(s_t, a_t, g_t),$$\\n\\nwhere $\\\\mu(s_0)$ is the distribution of initial states. The value function is defined as $V_\\\\pi(s, g) = \\\\mathbb{E}_{a_t \\\\sim \\\\pi(\\\\cdot|s_t, g), s_{t+1} \\\\sim P(\\\\cdot|s_t, a_t)} \\\\sum_{t=0}^{\\\\infty} \\\\gamma^t r(s_t, a_t, g_t)$.\\n\\nFor offline GCRL, the agent cannot interact with the environment during training, and the training data is sampled from a static dataset $D = \\\\{(s_t, a_t, g, r_t, s_{t+1})\\\\}$.\\n\\n2.2. Domain Generalization\\n\\nDomain Generalization (DG) was first studied in the supervised learning setting (Blanchard et al., 2011). A domain is defined as a joint distribution $P_{XY}$ on $X \\\\times Y$, where $X$ is the input space and $Y$ is the label space. DG learns a model from $K$ different training domains $S = \\\\{(x_k, y_k)\\\\}_{k=1}^{K}$ that aims to generalize on unseen testing domains $T = \\\\{x_T\\\\}$, $P_{TXY} \\\\neq P_kX$, $k \\\\in \\\\{1, \\\\ldots, K\\\\}$. DG mainly handles covariate shift (Zhou et al., 2021a), assuming that the labeling function $P_{Y|X}$ is stable across domains (Muandet et al., 2013) and only the marginal distribution changes $P_{TX} \\\\neq P_kX$, $k \\\\in \\\\{1, \\\\ldots, K\\\\}$.\\n\\n3. OOD Generalization for Offline GCRL\\n\\nIn this section, we first compare different GCRL algorithms in a 2D goal-reaching environment, showing that weighted imitation learning method is preferable to other methods across different data settings. Based on the observations, we formulate the OOD generalization problem as domain generalization, and then derive a theoretical framework to analyze the essential techniques for OOD generalization.\\n\\n3.1. Didactic Example\\n\\nWe design a 2D point environment as shown in Figure 2(a) to characterize the generalization ability of different offline GCRL algorithms, including BC, GCSL (Ghosh et al., 2019), WGCSL (Yang et al., 2022b), DDPG+HER (Andrychowicz et al., 2017), and CQL+HER (Chebotar et al., 2021). There are three types of training data, namely \u201cExpert N\u201d and \u201cNon-Expert N\u201d, where $N$ refers to the number of trajectories in the dataset. In the training datasets, trajectories and goals are mainly distributed on the top semi-circle with a radius of 10. Unlike the training data, the\"}"}
{"id": "yang23q", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Evaluation Goals\\n\\n| Training Data | Expert 10 | Non-Expert 10 | Non-Expert 50 |\\n|---------------|-----------|---------------|---------------|\\n\\n### Figure 2.\\n\\n- **(a)** Visualization of three 2D goal-reaching datasets and two groups of evaluation goals. \u201cR10\u201d and \u201cR20\u201d refer to the radius (10 or 20) of the desired goals for evaluation.\\n- **(b)** Average success rates of different agents over 5 random seeds. Evaluation goals are on the full circles of radius 10 and 20. Both states and goals in this environment are represented as 2D coordinates indicating their positions, while actions are 2D vectors of the displacement. In this example, the optimal policy is $\\\\pi(s, g) = \\\\text{clip}(g - s, 0, 1)$, where the maximum movement in one dimension is 1. If the agent learns the optimal policy, it can successfully generalize to any unseen goal.\\n\\nFrom the results in Figure 1 and Figure 2(b), we can draw the following conclusions:\\n\\n- **Given a clean expert dataset, BC generalizes well for OOD goals. However, in the case of training with non-expert and noisy data, it can overfit the noise and thus fail to generalize.**\\n- **DDPG+HER (short for \\\"HER\\\") suffers from overestimating values of OOD actions. As a result, it avoids in-dataset actions and produces odd trajectories.**\\n- **For the pessimism-based approach CQL+HER, its trajectories are restricted to the upper semicircle and fail to generalize to the lower part when given clean expert data. It can only generalize relatively well when the data size and coverage are sufficiently large.**\\n- **WGCSL significantly improves the OOD generalization ability over GCSL by re-weighting samples and performs consistently well across different datasets.**\\n\\nThe designed task is simple but representative for characterizing the characteristics of different algorithms. More results can be found in Appendix D.1. As suggested by the empirical results, the weighted imitation-based method enjoys better OOD generalization than pessimism-based method. Moreover, pessimism-based offline RL methods are inhibited from reaching OOD area in theory (Jin et al., 2021; Kumar et al., 2021). In contrast, weighted imitation learning method has theoretical guarantees for OOD generalization, which we will show in Section 3.3.\\n\\n### 3.2. Problem Formulation\\n\\nWe define $X = S \\\\times G$ as the input space, $Y = A$ as the action space. The offline data $D = \\\\{(s_t, a_t, g, r_t, s_{t+1})\\\\}$ is collected by any behavior policy $\\\\pi_b$, where $(s, g) \\\\sim P_{S \\\\times X}$. In the testing phase, initial states and desired goals can be sampled from any unknown distribution $P_{T_X}$, $P_{T_X} \\\\neq P_{S \\\\times X}$, which is named \\\"OOD distribution\\\" in this paper. We assume the expert policy $\\\\pi_E(a|s, g)$ (or $P_Y|X$) is stable with $P_X$ and generalizes well across different state-goal pairs, which is reasonable because OOD generalization is meaningless when $\\\\pi_E$ cannot generalize. The objective is to minimize the suboptimality on the testing domain $P_{T_X}$:\\n\\n$$\\\\text{SubOpt} (\\\\pi_E, \\\\pi) = E_{(s_0, g) \\\\sim P_{T_X}} [V_{\\\\pi_E}(s_0, g) - V_{\\\\pi}(s_0, g)] \\\\quad (1)$$\\n\\n### 3.3. A Domain Generalization View\\n\\nBy establishing a link between weighted imitation learning and supervised learning, we can analyze the OOD generalization performance according to the domain generalization bound (Ben-David et al., 2010; Zhang et al., 2012; Mansour et al., 2009).\\n\\nOur following analysis is based on the Total Variation Distance $D_{TV}$ between any two policies $\\\\pi_1$ and $\\\\pi_2$:\\n\\n$$D_{TV}(\\\\pi_1(\\\\cdot|s, g), \\\\pi_2(\\\\cdot|s, g)) = \\\\sup_{B \\\\subset A} \\\\left| \\\\int_a B (\\\\pi_1(a|s, g) - \\\\pi_2(a|s, g)) \\\\right|,$$\\n\\nwhere $B$ is any measurable subset of the action space $A$.\\n\\nDenote the discounted occupancy of state as $d_{\\\\pi}(s|s_0, g) = (1 - \\\\gamma) \\\\sum_{t=0}^{\\\\infty} \\\\gamma^t \\\\Pr(s_t = s|\\\\pi, s_0, g)$. We define the policy discrepancy on any state-goal distribution $\\\\rho$ as:\\n\\n$$\\\\varepsilon_\\\\rho(\\\\pi_1, \\\\pi_2) = E_{(s_0, g) \\\\sim \\\\rho_{X}} [D_{TV}(\\\\pi_1(\\\\cdot|s, g), \\\\pi_2(\\\\cdot|s, g))]$$\\n\\nGenerally, we do not have access to the true expert policy $\\\\pi_E$, but we can imitate a surrogate policy $\\\\hat{\\\\pi}_E$ instead. Then, we provide the following OOD generalization theorem.\\n\\n**Theorem 3.1.** Consider finite hypothesis space $\\\\Pi$ and we minimize the empirical loss function $\\\\hat{\\\\varepsilon}_S$ with $m$ samples. For a policy $\\\\pi$ and a surrogate expert policy $\\\\hat{\\\\pi}_E$, with probability $\\\\geq 1 - \\\\delta$, the OOD generalization error is bounded by:\\n\\n$$\\\\hat{\\\\varepsilon}_S - \\\\varepsilon_{\\\\rho}(\\\\hat{\\\\pi}_E, \\\\pi) \\\\leq \\\\sqrt{\\\\frac{2 \\\\log \\\\frac{1}{\\\\delta}}{m}}$$\\n\\nAs suggested by the empirical results, the weighted imitation-based method enjoys better OOD generalization than pessimism-based method. Moreover, pessimism-based offline RL methods are inhibited from reaching OOD area in theory (Jin et al., 2021; Kumar et al., 2021). In contrast, weighted imitation learning method has theoretical guarantees for OOD generalization, which we will show in Section 3.3.\"}"}
{"id": "yang23q", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nIdeally, we can eliminate all data from other modalities to best advantage weight introduced by (Yang et al., 2022b). However, when the behavior policy is deterministic, as often encountered in multi-goal RL, there is a risk of inferior modalities achieving goals. A viable solution to this issue is to interpolate between modalities, leading to a widened expert estimation gap. A more practical method for estimating the advantage function is to re-weight the training distribution. This can lead to a more tighter upper bound. This gives justifications that our analysis considers an oracle advantage function, but note that the last term in the above bound suggests that we can re-weight the training distribution to a uniform distribution to obtain a smaller worst-case distribution shift, i.e.,\\n\\n\\\\[ \\\\sup_{S, \\\\pi} \\\\mathbb{E}_{x \\\\sim \\\\bar{Z}} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot \\\\mathbb{E}_{x \\\\sim \\\\pi} \\\\cdot"}
{"id": "yang23q", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We denote a trajectory of horizon $T$ we refer the readers to Appendix D.10. In the subsequent\\nwhere\\n\\n$u$ contains three parts, i.e., the uncertainty\\n\\n$w$ is the uncertainty weight to replace the density,\\n\\n$\\\\beta_A$ is the quantile of advantage values, in\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$A$ is the uncertainty weight to replace the density,\\n\\n$\\\\gamma$ is the DSW. In DSW, the constant\\n\\n$\\\\alpha$ is more\\n\\n$\\\\alpha_{DSC}$ is the proportion of ranked samples to down-\\n\\n$\\\\alpha_{DSW}$ is set to\\n\\n$\\\\alpha_{GQ}$ is the hyperparam-\\n\\n$\\\\alpha_{\\text{std}}$ is the advantage value and the uncertainty weight. Specifically,\\n\\n$\\\\epsilon(a)$ is the DSW. In DSW, the constant\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage value and the uncertainty weight. Specifically,\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$\\\\epsilon$ is the advantage function, and\\n\\n$"}
{"id": "yang23q", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n(a) Near | Far\\n(b) Near | Far\\n(c) Low | High\\n\\nFigure 3. Examples of designed benchmark tasks. (a) Push Left-Right, (b) Slide Near-Far, (c) Reach Near-Far, and (d) Pick Low-High.\\n\\n5. Experiments\\n\\nIn this section, we introduce a new benchmark consisting of 9 task groups and 26 tasks to evaluate the OOD generalization performance of offline GCRL algorithms.\\n\\n5.1. Environments and Experimental Setup\\n\\nEnvironments\\n\\nThe introduced benchmark is modified from MuJoCo robotic manipulation environments (Plappert et al., 2018). Agents aim to move a box, a robot arm, or a bionic hand to reach desired positions. The reward for each environment is sparse and binary, i.e., 1 for reaching the desired goal and 0 otherwise. As listed in Table 1, there are 9 task groups with a total of 26 tasks, 17 of which are OOD tasks whose goals are not in the training data. For example, as shown in Figure 3(a), the dataset of Push Left-Right contains trajectories where both the initial object and achieved goals are on the right side of the table. Then the IID task is evaluating agents with object and goals on the right side (i.e., Right2Right). The OOD tasks can be generated by changing the side of the initial object or desired goals. Following (Yang et al., 2022b), we collect datasets with the online DDPG+HER agent. More information about the task design and offline datasets can be found in Appendix C.\\n\\nExperimental Setup\\n\\nWe compare GOAT with current SOTA offline GCRL methods, including WGCSL (Yang et al., 2022b), GoFAR (Ma et al., 2022b), CQL+HER (Chebotar et al., 2021), GCSL (Ghosh et al., 2019), and DDPG+HER (Andrychowicz et al., 2017). Besides, we also include a SOTA ensemble-based offline RL methods, MSG (Ghasemipour et al., 2022), namely \u201cMSG+HER\u201d. To evaluate performance, we assess agents across 200 randomly generated goals for each task and benchmark their average success rates. More details and additional experiments are provided in Appendix C and Appendix D.\\n\\n5.2. Understanding the Uncertainty Weight\\n\\nIn our theoretical analysis, the uncertainty weight (UW) has the effect of reducing the worst-case distance between the training and unknown testing distributions. To make it more clear, we collect 10000 relabeled samples \\\\((s, a, g')\\\\) and rank these samples according to the UW in Eq(4). For a sample \\\\((s, a, g')\\\\), we record two values, the supervised loss (i.e., \\\\(\\\\|a - \\\\pi_\\\\theta(s, g')\\\\|^2\\\\)), and the distance between the desired goal and the achieved goal (i.e., \\\\(\\\\|g' - \\\\phi(s)\\\\|^2\\\\), short for \u201cstate-goal distance\u201d). Then, we average their values for every 1000 ranked samples. The results are shown in Figure 4.\\n\\nInterestingly, UW assigns more weights to samples with larger supervised loss, which may also be related to Distributionally Robust Optimization (Rahimian & Mehrotra, 2019; Goh & Sim, 2010), thereby improving performance on worst-case scenarios. Moreover, UW prefers samples with larger state-goal distance. Since every state-goal pair \\\\((s, g')\\\\) defines a task from \\\\(s\\\\) to \\\\(g'\\\\), UW enhances harder tasks with larger state-goal distance. In general, OOD goals are relatively further away than IID goals, which also interprets why UW works for OOD generalization.\\n\\n5.3. Generalizing to OOD Goals\\n\\nTable 1 reports the average success rates of GOAT and other baselines on the introduced benchmark. We denote GOAT with expectile regression as GOAT(\\\\(\\\\tau\\\\)), where \\\\(\\\\tau < 0\\\\).\\n\\nFrom the results, we can conclude that OOD generalization is more challenging than IID tasks. For example, the performance of GoFAR, GCSL, and BC drops by more than half on OOD tasks. On the contrary, GOAT and GOAT(\\\\(\\\\tau\\\\)) achieve the highest OOD success rates over 16 out of 17 tasks. Compared with WGCSL, GOAT improves the IID performance slightly but considerably enhances the OOD performance.\"}"}
{"id": "yang23q", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1.\\n\\n| Task Group | Task          | GOAT (\u03c4) | WGCSL | GCSL | BC | GoFAR | DDPG+HER | CQL+HER | MSG+HER | Push Near-Far | Reach Near-Far | Reach Left-Right | Slide Near-Far | Pick Low-High | Reach Left-Right | Right2Right | Right2Left | Near2Near | Left2Left | Right2Right | Left2Left | Right2Left | Near2Near | Left2Left | Right2Right | Left2Left | Right2Left | Near2Near | Left2Left | Right2Right | Left2Left | Right2Left | Near2Near | Left2Left | Right2Right | Left2Left | Right2Left | Near2Near | Left2Left | Right2Right | Left2Left | Right2Left | Near2Near |\\n|------------|---------------|----------|-------|------|----|-------|----------|---------|---------|---------------|---------------|-----------------|---------------|--------------|-----------------|-------------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------|------------|-----------|-----------|-------------|-----------"}
{"id": "yang23q", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"**Table 2. Ablations of each component of GOAT.**\\n\\n| Component          | OOD Tasks | All Tasks |\\n|--------------------|-----------|-----------|\\n| BC +HER +EAW +DSW +Ens + UW + ER | 21.7 28.8 53.1 62.1 63.4 67.9 70.9 | 34.8 50.7 65.4 71.1 72.2 75.7 77.9 |\\n| **Increment**      | +0 +7 .1 +24 .3 +9 .0 +1 .3 +4 .5 +3 .0 | +0 +15 .9 +14 .7 +5 .7 +1 .7 +1 .1 +3 .5 +2 .2 |\\n\\n5.4. **Ablations**\\n\\nTo measure the contribution of each component of GOAT, we gradually add one component from BC to GOAT and record the performance increment caused by each component. As shown in Table 2, the recorded results are average success rates of 17 OOD tasks and all 26 tasks. On average, each component brings improvement for OOD generalization of offline GCRL. For OOD tasks, EAW and DSW contribute the most by improving the surrogate expert policy for imitating. Besides, HER and UW also bring considerable improvement through data augmentation and uncertainty re-weighting. In addition, ensemble technique (Ens) improves the estimation of value functions but has the least effect on the overall performance. Expectile regression (ER) improves the average performance, but slightly reduces OOD performance on hard tasks such as Slide Near-Far and HandReach as shown in Table 1. Furthermore, we also compare variants of GOAT with V functions and $\\\\chi^2$-divergence in Appendix D.4.\\n\\n5.5. **Online Fine-tuning to Unseen Goals**\\n\\nWe design an experiment to fine-tune pre-trained agents with online samples to verify whether the generalization ability of pre-trained agents is beneficial for online learning. The pre-trained agents are trained on offline datasets with partial coverage (Right2Right) and fine-tuned to full coverage (Right2Right, Right2Left, Left2Right, Left2Left). We apply DDPG+HER to fine-tune the policies and value functions after each episode collection. Additional Gaussian noise and random actions are applied for exploration. More detailed description can be found in Appendix D.12.\\n\\nThe experimental results are show in Figure 6, which demonstrate that (1) most pre-trained agents learn faster than the randomly initialized agent (namely \u201crandom\u201d) and (2) different initializations for goal-conditioned agents perform significantly different during fine-tuning. Specifically, GOAT outperforms other methods on the efficiency of online fine-tuning, while CQL, MARVIL (Wang et al., 2018) and GCSL result in slow-growing curves. We observe that the performance of GCSL initialization is similar to that of random initialization. It is likely that value networks contain valuable information for DDPG+HER agents to transfer from offline to online. This also explains why GOAT brings improvement, as it enhances value function learning via ensemble and expectile regression.\\n\\n8. **Related Work**\\n\\n**Goal-conditioned RL**\\n\\nGCRL is a branch of reinforcement learning where agents need to achieve multiple goals sharing the same environmental dynamics (Schaul et al., 2015; Andrychowicz et al., 2017). Goal relabeling (Andrychowicz et al., 2017; Li et al., 2020a; Eysenbach et al., 2020; Yang et al., 2021a) is an effective technique that handles the sparse reward problem in GCRL and augments the data for policy learning. To improve the generalization ability, several prior works mainly focus on learning generalizable representations, e.g., combining Successor Feature with UVFA (Ma et al., 2018; Borsa et al., 2018), decomposing $Q$ value via Bilinear Value Networks (Hong et al., 2022), and learning discretization bottleneck representation for goals (Islam et al., 2022). Han et al. (2021) propose to learn invariant representation via aligned sampling to tackle the spurious feature problem. Our work differs from previous works in that we consider the offline GCRL setting, where pessimism can inhibit OOD generalization.\\n\\n**Offline RL and Offline GCRL**\\n\\nOffline RL handles the distribution shift challenge and learns policies from static datasets (Levine et al., 2020). Generally, offline RL methods can be divided into two main directions, i.e., policy regularization and value underestimation. The first direction includes methods that constrain the learned policy to be close to the behavior policy under certain distance measure (Wang et al., 2018; Fujimoto et al., 2019; Nair et al., 2020b; Yang et al., 2021b; Fujimoto & Gu, 2021). Another direction is to underestimate values for OOD actions (Kumar et al., 2020; Yu et al., 2021; An et al., 2021; Bai et al., 2022; Yang et al., 2022a; Ghasemipour et al., 2022). As for offline GCRL, current methods can also be grouped into policy regularization (Yang et al., 2022b; Ma et al., 2022b) and value underestimation (Chebotar et al., 2021) methods. Different from prior works, our work focuses on learning policies from offline data and improving the ability to generalize to out-of-distribution goals.\"}"}
{"id": "yang23q", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"C. Offline Datasets and Implementation Details\\n\\nC.1. Offline Datasets\\n\\nFor the benchmark tasks, offline datasets are collected by the final online policy trained with HER (Andrychowicz et al., 2017). Additional Gaussian noise with zero mean and \\\\( \\\\sigma^2 \\\\) standard deviation and random actions with probability 0.3 is used for data collection to increase the diversity, following previous work (Yang et al., 2022b). For the FetchSlide task, we only use noise with a standard deviation of 0.1 because the behavior policy is already suboptimal. After data collection, different from (Yang et al., 2022b), we need additional data processing to select trajectories whose achieved goals are all in the IID region. The IID region is defined by each task group, which is shown in Table 3. A special case is the HandReach task, where we do not divide the dataset due to its high dimensional space and we use different scales of evaluation goals instead. Compared with prior offline GCRL works (Yang et al., 2022b; Ma et al., 2022b), we use relatively smaller datasets to study the OOD generalization problem. Our datasets encompass trajectories of different length, ranging from 200 to 20000, with each trajectory comprising 50 transitions. A comprehensive summary of this information is presented in Table 3. The dataset division standard refers to the location requirements of initial states and desired goals for IID tasks (e.g., Right2Right). For OOD tasks, the initial state or the desired goal are designed to deviate from the IID requirement (e.g., Right2Left, Left2Right, Left2Left).\\n\\n| Task Group          | OOD task | Trajectory number | Size (M) | Dataset Division Standard |\\n|---------------------|----------|-------------------|----------|--------------------------|\\n| Reach Left-Right    | Right    | 200               | 1.6      | the gripper's y coordinate value > the initial position |\\n| Reach Near-Far      | Near     | 200               | 1.6      | the l\u00b2 distance between gripper and the initial position \u2264 0.15 |\\n| Push Left-Right     | Right2Right, Right2Left, Left2Right, Left2Left | 5000     | 67       | the object's y coordinate value > the initial position |\\n| Push Near-Far       | Near2Near, Near2Far, Far2Near, Far2Far | 5000     | 67       | the l\u00b2 distance between the object and the initial position \u2264 0.15 |\\n| Pick Left-Right     | Right2Right, Right2Left, Left2Right, Left2Left | 5000     | 67       | the object's y coordinate value > the initial position |\\n| Pick Low-High       | Low      | 5000              | 67       | the object's z coordinate value < 0.6 |\\n| Slide Left-Right    | Right2Right, Right2Left, Left2Right, Left2Left | 20000    | 266      | the object's y coordinate value > the initial position |\\n| Slide Near-Far      | Near     | 20000             | 266      | the object's x coordinate value \u2264 0.14 |\\n| HandReach Near-Far  | Near     | 10000             | 429      | the range of meeting position for two fingers |\\n\\nTable 3. Information about 9 Task Groups and Datasets.\\n\\nTable 4. w and \u03c4 used for GOAT and GOAT(\u03c4).\\n\\n| Task Group          | GOAT | GOAT(\u03c4) |\\n|---------------------|------|---------|\\n| Reach Left-Right    | w = 1.5 | w = 2.5, \u03c4 = 0.3 |\\n| Reach Near-Far      | w = 2.0 | w = 1.5, \u03c4 = 0.1 |\\n| Push Left-Right     | w = 2.5 | w = 1.5, \u03c4 = 0.1 |\\n| Push Near-Far       | w = 1.5 | w = 2.5, \u03c4 = 0.1 |\\n| Pick Left-Right     | w = 1.0 | w = 2.5, \u03c4 = 0.3 |\\n| Pick Low-High       | w = 2.0 | w = 1.5, \u03c4 = 0.3 |\\n| Slide Left-Right    | w = 1.5 | w = 2.5, \u03c4 = 0.1 |\\n| Slide Near-Far      | w = 2.0 | w = 1.5, \u03c4 = 0.3 |\\n| HandReach Near-Far  | w = 2.5 | w = 2.0, \u03c4 = 0.3 |\\n\\nC.2. Implementation Details\\n\\nImplementations Following (Yang et al., 2022b; Ma et al., 2022b), value functions and policy networks (along with their target networks) are all 3-layer MLPs with 256-unit layers and relu activations. We use a batch size of 512, a discount factor of \\\\( \\\\gamma = 0.98 \\\\), and an Adam optimizer with learning rate \\\\( 5 \\\\times 10^{-4} \\\\) for all algorithms. We also normalize the observations and goals with estimated mean and standard deviation. The relabel probability \\\\( p_{relabel} = 1 \\\\) for most environments except for Slide Left-Right and Slide Near-Far, where \\\\( p_{relabel} = 0.2 \\\\) and 0.5, respectively. In EA W, the ratio \\\\( \\\\beta \\\\) is set to 2 and EAW is clipped into range \\\\((0, M]\\\\) for numerical stability, where \\\\( M \\\\) is set to 10 in our experiments. For DSW, we utilize a First-In-First-Out (FIFO) queue \\\\( B_a \\\\) of size \\\\( 5 \\\\times 10^4 \\\\) to store recent calculated advantage values, and the percentile threshold \\\\( \\\\alpha \\\\) gradually increases from 0 to \\\\( \\\\alpha_{max} \\\\). We use \\\\( \\\\alpha_{max} = 80 \\\\) for all tasks except HandReach and Slide Left-Right, and \\\\( \\\\alpha_{max} = 50 \\\\) for HandReach, \\\\( \\\\alpha_{max} = 0 \\\\) for Slide Left-Right. When \\\\( A(s, a, g') < c \\\\) and \\\\( c \\\\) is the \\\\( \\\\alpha \\\\) quantile value of \\\\( B_a \\\\), we set \\\\( \\\\epsilon(A(s, a, g')) = 0.05 \\\\) instead of 0 following (Yang et al., 2022b). For the uncertainty weight (UW), we use \\\\( N = 5 \\\\) ensemble Q networks to calculate the standard deviation \\\\( \\\\text{Std}(s, g) \\\\) and maintain another FIFO queue \\\\( B_{std} \\\\) to store recent.\"}"}
{"id": "yang23q", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"several baselines used in our paper. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as the future. Regarding the expectile regression ($\\\\tau$), it is therefore necessary to develop a more stable uncertainty weight estimation method with less hyperparameter tuning in the future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$. The future. Regarding the expectile regression ($\\\\tau$), we search for numerical stability. What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n**Baseline Descriptions**\\n\\nIn our experiments, all the baselines share the same policy and value network structures, as well as hyperparameters. As regards to WGCSL (Yang et al., 2022b) and GoFAR (Ma et al., 2022b), we use their official implementations. Denote the original dataset as $D$."}
{"id": "yang23q", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\nTable 5. Final average success rates of different agents trained on three types of PointReach datasets. The results are averaged over 5 random seeds.\\n\\n|            | GOAT (ours) | WGCSL (tuned) | WGCSL | GCSL | Goal | BC | HER | CQL+HER |\\n|------------|-------------|---------------|-------|------|------|----|-----|---------|\\n| Expert 10  | R10         | 0.86 \u00b1 0.08   | 0.94 \u00b1 0.06 | 0.82 \u00b1 0.07 | 0.72 \u00b1 0.09 | 0.67 \u00b1 0.02 | 0.0 \u00b1 0.00 | 0.39 \u00b1 0.05 |\\n|            | R20         | 0.48 \u00b1 0.21   | 0.30 \u00b1 0.14 | 0.40 \u00b1 0.19 | 0.03 \u00b1 0.06 | 0.45 \u00b1 0.10 | 0.0 \u00b1 0.00 | 0.01 \u00b1 0.02 |\\n| Non-Expert 10 | R10       | 0.94 \u00b1 0.08   | 0.89 \u00b1 0.12 | 0.94 \u00b1 0.10 | 0.66 \u00b1 0.18 | 0.29 \u00b1 0.06 | 0.0 \u00b1 0.00 | 0.52 \u00b1 0.05 |\\n|            | R20         | 0.69 \u00b1 0.18   | 0.63 \u00b1 0.19 | 0.53 \u00b1 0.10 | 0.12 \u00b1 0.09 | 0.06 \u00b1 0.04 | 0.0 \u00b1 0.00 | 0.16 \u00b1 0.06 |\\n| Non-Expert 50 | R10        | 1.00 \u00b1 0.00   | 0.98 \u00b1 0.04 | 0.96 \u00b1 0.08 | 0.98 \u00b1 0.04 | 0.57 \u00b1 0.02 | 0.60 \u00b1 0.30 | 0.84 \u00b1 0.12 |\\n|            | R20         | 0.92 \u00b1 0.04   | 0.91 \u00b1 0.14 | 0.81 \u00b1 0.14 | 0.33 \u00b1 0.12 | 0.10 \u00b1 0.04 | 0.16 \u00b1 0.16 | 0.75 \u00b1 0.08 |\\n\\nD. Additional Experiments\\n\\nIn this section, we include the following experiments:\\n1. 2D PointReach Task;\\n2. Uncertainty and Density;\\n3. Ablation on the Ensemble Size;\\n4. Additional Ablations of GOAT;\\n5. The Effectiveness of Expectile Regression;\\n6. Ablations of Ensemble and HER for Other GCRL Algorithms;\\n7. Ablations of GoFAR;\\n8. Combining Weighted Imitation Learning with Value Underestimation;\\n9. Discounted Relabeling Weight (DRW);\"}"}
{"id": "yang23q", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?\\n\\n10. Adaptive Data Selection Weight;\\n11. MSG+HER with Varying Hyper-parameter;\\n12. Online Fine-tuning;\\n13. Training Time;\\n14. Random Network Distillation as the Uncertainty Measurement;\\n15. Full Benchmark Experiments.\\n\\nD.1. 2D PointReach Task\\n\\nAverage Success Rates and Cumulative Returns\\n\\nIn Table 5 and Table 6, we provide average success rates and average cumulative returns of different algorithms on \u201cExpert 10\u201d, \u201cNon-Expert 10\u201d, and \u201cNon-Expert 50\u201d datasets. We also include the performance of our method GOAT for comparison. As demonstrated in the two tables, GOAT achieves the highest average success rates and average returns on all three types of training data, surpassing the strong baseline WGCSL. In addition, we visualize trajectories collected by these agents in Figure 7. The results also show that GOAT performs better than WGCSL with a smaller trajectory variance on the lower semicircle. Other conclusions keep the same as the didactic example in Section 3.1.\\n\\nTable 6. Final average returns of different agents trained on three types of PointReach datasets. The results are averaged over 5 random seeds.\\n\\n|                  | GOAT (ours) | WGCSL (tuned) | WGCSL | GCSL | Goal BC | HER | CQL+HER |\\n|------------------|-------------|---------------|-------|------|---------|-----|---------|\\n| Expert 10 R10    | 34.69 \u00b1 2.81| 38.37 \u00b1 2.36 | 33.76 \u00b1 3.07 | 30.23 \u00b1 3.22 | 28.48 \u00b1 0.75 | 0.05 \u00b1 0.00 | 15.22 \u00b1 1.16 |\\n| Expert 10 R20    | 15.34 \u00b1 5.35| 10.35 \u00b1 3.96 | 12.99 \u00b1 5.49 | 1.75 \u00b1 2.24 | 14.62 \u00b1 2.77 | 0.00 \u00b1 0.00 | 0.14 \u00b1 0.28 |\\n| Non-Expert 10 R10| 37.57 \u00b1 3.21| 35.50 \u00b1 4.88 | 37.34 \u00b1 3.56 | 22.42 \u00b1 4.75 | 12.62 \u00b1 1.89 | 0.21 \u00b1 0.32 | 20.90 \u00b1 1.06 |\\n| Non-Expert 10 R20| 20.88 \u00b1 5.84| 19.27 \u00b1 5.52 | 16.19 \u00b1 2.89 | 2.42 \u00b1 2.09 | 1.58 \u00b1 0.75 | 0.00 \u00b1 0.00 | 3.93 \u00b1 1.20 |\\n| Non-Expert 50 R10| 39.99 \u00b1 0.10 | 39.22 \u00b1 1.46 | 38.23 \u00b1 3.29 | 32.84 \u00b1 1.75 | 18.72 \u00b1 0.90 | 24.70 \u00b1 12.41| 32.50 \u00b1 4.33 |\\n| Non-Expert 50 R20| 27.89 \u00b1 1.10 | 27.34 \u00b1 4.19 | 23.93 \u00b1 4.37 | 6.53 \u00b1 2.41 | 3.65 \u00b1 0.78 | 5.21 \u00b1 4.96 | 21.09 \u00b1 2.53 |\\n\\nTable 7. Final average success rates of CQL+HER agents with different hyperparameter $\\\\alpha$. The results are averaged over 5 random seeds.\\n\\n| CQL+HER | $\\\\alpha = 5$ | $\\\\alpha = 2$ | $\\\\alpha = 1$ | $\\\\alpha = 0$ | $\\\\alpha = 0.1$ |\\n|---------|--------------|--------------|--------------|--------------|---------------|\\n| Expert 10 R10 | 0.34 \u00b1 0.07  | 0.45 \u00b1 0.03  | 0.39 \u00b1 0.05  | 0.36 \u00b1 0.07  | 0.21 \u00b1 0.08  |\\n| Expert 10 R20 | 0.09 \u00b1 0.08  | 0.08 \u00b1 0.04  | 0.01 \u00b1 0.02  | 0.06 \u00b1 0.06  | 0.06 \u00b1 0.07  |\\n| Non-Expert 10 R10 | 0.28 \u00b1 0.17  | 0.53 \u00b1 0.07  | 0.52 \u00b1 0.05  | 0.60 \u00b1 0.10  | 0.10 \u00b1 0.15  |\\n| Non-Expert 10 R20 | 0.07 \u00b1 0.04  | 0.19 \u00b1 0.07  | 0.16 \u00b1 0.06  | 0.32 \u00b1 0.04  | 0.02 \u00b1 0.04  |\\n| Non-Expert 50 R10 | 0.57 \u00b1 0.04  | 0.76 \u00b1 0.05  | 0.84 \u00b1 0.12  | 0.95 \u00b1 0.10  | 0.77 \u00b1 0.21  |\\n| Non-Expert 50 R20 | 0.21 \u00b1 0.12  | 0.52 \u00b1 0.12  | 0.75 \u00b1 0.08  | 0.66 \u00b1 0.16  | 0.46 \u00b1 0.24  |\\n\\nHyper-parameter Tuning for CQL+HER\\n\\nIt is also interesting to check whether tuning the ratio $\\\\alpha$ of the CQL loss can enable better OOD generalization. Specifically, we tune $\\\\alpha$ in $\\\\{0.01, 0.1, 1, 2, 5\\\\}$. The results are shown in Table 7. The generalization performance of CQL+HER drops as $\\\\alpha$ becomes large, e.g., $\\\\alpha = 5$, and as $\\\\alpha$ becomes small, e.g., $\\\\alpha = 0.01$.\\n\\nWhen the data coverage is insufficient (i.e., the \u201cExpert 10\u201d setting), CQL+HER cannot generalize on the full circle of radius 20 (\u201cR20\u201d), no matter how $\\\\alpha$ is adjusted. The tuned results of CQL+HER are still incomparable to weighted imitation learning methods such as GOAT and WGCSL.\\n\\nAdditional Tasks\\n\\nIn addition to tasks introduced before, we include another three datasets in Figure 8 (a), where we have few trajectories on the lower semicircle. As shown in Figure 8 (b), these tasks are relatively easy compared with those used in our didactic example, achieving higher average success rates. The conclusions are also consistent with Section 3.1.\"}"}
