{"id": "chen23i", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Subequivariant Graph Reinforcement Learning in 3D Environments\\n\\nTable 7. Fixed initial orientation (about 0\u00b0) training, arbitrary initial orientation (any given angle) test on 3d cheetah full.\\n\\n| Methods | 500k training steps | 1M training steps |\\n|---------|--------------------|------------------|\\n| 0\u00b0      | 1886 \u00b1 148.9       | 1005 \u00b1 615.3     |\\n| 90\u00b0     | 120.5 \u00b1 178.5      | 791 \u00b1 493.4      |\\n| 180\u00b0    | 1232 \u00b1 72.9        | 2592 \u00b1 155.6     |\\n| 270\u00b0    | 0 \u00b1 5.6            | 1340 \u00b1 668.0     |\\n| random  | 1193 \u00b1 345.2       | 1178 \u00b1 674.9     |\\n\\n**D. More Discussion about Invariant Methods**\\n\\nSpecifically, by choosing the \u201cforward\u201d direction, we can achieve heading-equivariance with heading normalization. In essence, the lack of a predetermined \u201cforward\u201d direction that is consistent across all agents prevents us from transferring experiences between different agents. For example, if we create a duplicate of one agent and redefine the \u201cforward\u201d direction, heading normalization will no longer be applicable. In particular, let\u2019s consider two agents that have very similar morphology, with the only difference being that their torso orientations are opposite and both encourage movement along the torso orientation. If the torso orientation is selected as the \u201cforward\u201d direction, the normalization applied to these two agents will vary significantly. As a result, the policy learned by one agent will not generalize to the other agent, unless the other agent\u2019s movement mode is to move in the opposite orientation of the torso. Therefore, generalization performance is affected by the choice of the \u201cforward\u201d direction and the agent\u2019s movement mode.\\n\\nBesides, there is extensive experimental evidence (Hsu et al., 2022; J\u00f8rgensen & Bhowmik, 2022; Sch\u00fctt et al., 2021; Joshi et al., 2022) indicating that equivariant methods that preserve equivariance at each layer outperform those invariant methods that solely apply transformations at the input layer to obtain invariant features and then use an invariant network. Our framework, falling into the equivariant family, enables the propagation of directional information through message passing steps, allowing the extraction of rich geometric information such as angular messages. In contrast, the invariant methods may result in the loss of higher-order correlations between nodes, which are crucial for modeling the geometric relationships between them.\\n\\n**E. More Ablation on Equivariance**\\n\\nIn addition, we conduct another experiment by fixing the initial orientation as 0\u00b0 when training, but allowing arbitrary angles when testing. As shown in Table 7, SET generalizes well to all cases. On the contrary, SW AT only obtains desirable performance when the testing angle is fixed to 0\u00b0 which is the same as that during the training process, and its performance drops rapidly in other cases, especially at 180\u00b0. The experiments here justify the efficacy of involving orthogonality equivariance.\\n\\n**F. The Evaluation on v2-variants**\\n\\nThe v2-variants ($R = 10 \\\\sim 20$ m) are more challenging. We train the policy in the multi-task setting where $R = 10$ km, then we do the test in v2-variants. The results and related demos are shown in Figure 10, Figure 11, Figure 12 and Figure 13. While SW AT fails to perform well, SET has obvious advantages. With more episode timesteps, SET locomotes closer to the destination (a shorter distance) and gets more episode rewards.\"}"}
{"id": "chen23i", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"The last frame illustrating SET\u2013produced demos on morphologies\\n\\nFigure 10. The evaluation on v2-variants on 3D Hopper++. \"}"}
{"id": "chen23i", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11. The evaluation on v2-variants on 3D Walker++.\"}"}
{"id": "chen23i", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The last frame illustrating SWAT\u2013produced demos on morphologies.\\n\\n(b) The last frame illustrating SET\u2013produced demos on morphologies.\\n\\nFigure 12. The evaluation on v2-variants on 3D Humanoid++.\"}"}
{"id": "chen23i", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With multiple layers of message fusion on the morphological\\nwhere\\n\u20d7v\\n\u20d7u\\nL\\nVisualized in Figure 2.\\n\\nThe operations are stacked over\\nL\\nvisualized in Figure 2.\\n\\nO\\nW\\nlast dimension,\\n\u03c3\\n[34\\nThe operations are stacked over\\nL\\nvisualized in Figure 2.\\n\\n\u03c5\\n\u03c7\\nj\\n\u20d7v\\nj\\n\u20d7u\\n\u03b1\\nl\\n\u03c5\\nb\\nR\\nj\\n, \u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nwhere vec\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n(3)\\nl\\n\u20d7v\\nj\\n("}
{"id": "chen23i", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Graph Reinforcement Learning in 3D Environments\\n\\nFigure 3. Multi-task performance of our method SET compared to the morphology-agnostic RL baselines: SWAT and SMP. Training curves on 6 collections of environments. The shaded area represents the standard error.\\n\\nHowever, in 3D-SGRL, the half-cheetah is highly vulnerable to falling over in its locomotion, adding more difficulties to policy optimization. On account of this limitation, we extend the model to a full-cheetah with one torso, four legs, and one tail made of 14 limbs, enabling it stronger locomotion ability to explore in our 3D-SGRL environments. More design details are shown in Appendix C.1.\\n\\nState Space\\nWe take the initial position of the agent\u2019s torso as the center, and randomly select its initial orientation and the destination within a radius of $R$. When the agent reaches the assigned target position, we set another destination for it. To relieve the agent from falling down when turning at a high speed, we set the radius $R = 10$ km by default so that the agent will turn less frequently in an episode. We also set $R \\\\in [10\\\\text{ m}, 20\\\\text{ m}]$ as \u201cv2-variants\u201d, which is more difficult since the agent will change the direction more frequently.\\n\\nAction Space\\nThe action space is enlarged by changing the type of the joint of torso from \u201cslide-slide-hinge\u201d to \u201cfree\u201d and adding two more actuators that rotate around different axes of the joint. This allows the agent to have full DoFs to turn and move in arbitrary directions starting from arbitrary initial spatial configurations.\\n\\nTermination and Reward\\nThe goal in 3D-SGRL environments is learning to turn and move towards the assigned destination as fast as possible without falling over. Episode Termination follows that of the morphology-agnostic RL benchmark, but we modify the cheetah\u2019s termination to be the time it falls over or squats still. The reward consists of four parts.\\n\\n1. Alive bonus: Every timestep the agent is alive, it gets a reward of a fixed value 1 (3D Cheetah\u2019s is 0 due to the stability of its morphological structure);\\n2. Locomotion reward: It is a reward for moving towards the assigned target which is measured as $(\\\\text{distance before action} - \\\\text{distance after action})/dt$, where $dt$ is the time between consecutive actions. This reward will be positive if the agent is close to the target position;\\n3. Control cost: It is a cost for penalizing the agent if it takes actions that are too large. It is measured as $0.001 \\\\cdot \\\\sum_{k=1}^{K} a_k^2$;\\n4. Forward reward (not available for 3D Hopper): It is a reward of moving forward measured as $\\\\text{(coordinate after action - coordinate before action)}$.\"}"}
{"id": "chen23i", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"5. Evaluations and Ablations\\n\\nThis section first introduces the baselines and implementations, then compares the performance of different methods on our 3D benchmarks and reports the ablation studies for the design of our method.\\n\\n5.1. Baseline, Metric and Implementation\\n\\nBaselines\\n\\nWe compare our method SET against state-of-the-art methods SMP (Huang et al., 2020) and SWAT (Hong et al., 2021). We also compare SET with standard TD3-based non-morphology-agnostic RL: Monolithic in single-tasks. Please refer to Appendix C.2 for more details about baselines.\\n\\nMetrics\\n1. Multi-task with different morphologies: For each multi-task environment discussed in Section 4, a single policy is simultaneously trained on multiple variants. The policy in each plot is trained jointly on the training set (80% of variants from that environment) and evaluated on these seen variants.\\n2. Zero-Shot Generalization: We take the trained policies from multi-task and test on the unseen zero-shot testing variants.\\n3. Evaluation on v2-variants: We evaluate SET in a transfer learning setting where the trained policies from multi-task are tested and fine-tuned on the v2-variants environments.\\n4. Single-task Learning: The policy in each plot is trained on one morphology variant and evaluated on this variant.\\n\\nImplementations\\n\\nWe adopt the same input information and TD3 (Fujimoto et al., 2018) as the underlying reinforcement learning algorithm for training the policy over all baselines, ablations, and SET for fairness. We implement SET in the SWAT codebase. There is no weight sharing between actor $\\\\pi_\\\\theta$ and critic $Q_\\\\pi_\\\\theta$. Each experiment is run with three seeds to report the mean and the standard error. The reward for each environment is calculated as the sum of instant rewards across an episode. The value of the maximum timesteps of an episode is $1,000$.\\n\\n5.2. Main Results\\n\\nMulti-task with different morphologies\\n\\nAs shown in Figure 3, our SET outperforms all baselines by a large margin in all cases, indicating the remarkable superiority of taking into account the subequivariance upon Transformer. The baselines fail to achieve meaningful returns in most cases, which is possibly due to the large exploration space in our 3D-SGRL environments and they are prone to get trapped in local extreme points.\\n\\nTable 2. Comparison in zero-shot evaluation on the test set. Note that we omit the lacking part in the name of morphologies.\\n\\n| Environment      | SET       | SWAT      | SMP       |\\n|------------------|-----------|-----------|-----------|\\n| 3d walker        | 3276.2 \u00b1 17.4 | 207.0 \u00b1 52.7 | 56.8 \u00b1 15.1 |\\n| 3d humanoid      | 3214.3 \u00b1 7.9 | 343.0 \u00b1 16.6 | 143.4 \u00b1 50.7 |\\n| 3d cheetah       | 14643.9 \u00b1 292.6 | 1785.3 \u00b1 999.3 | 2 | 0.0 \u00b1 2.9 |\\n\\nZero-Shot Generalization\\n\\nDuring test time, we assess the trained policy on a set of held-out agent morphologies. Table 2 records the results of both in-domain and cross-domain settings. The training and zero-shot testing variants are listed on Table 5. For example, SET is trained on 3D Humanoid++ without 3d humanoid left leg and 3d humanoid right knee, while these two excluded environments are used for testing. Table 2 reports the average performance and the standard error over 3 seeds, where the return of each seed is calculated over 100 rollouts. Once again, we observe that SET yields better performance.\"}"}
{"id": "chen23i", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Graph Reinforcement Learning in 3D Environments\\n\\nFigure 6. Training curves of multi-task on 3D CWHH++. The comparison with invariant methods.\\n\\nEvaluation on v2-variants (R = 10 \u223c 20 m) are more challenging. We conduct two-stage training in this scenario. In the first stage, we train the policy under the multi-task setting where R = 10 km. The results and related demos are in Appendix F. In the second stage, we transfer the currently-trained policy to the R = 10 \u223c 20 m setting on 3D Cheetah++ and 3D Humanoid++.\\n\\nIt is seen from Figure 4 that SET is able to further improve the performance upon the first stage, while SWAT hardly receives meaningful performance gain especially on 3D Humanoid++.\\n\\nSingle-task Learning\\n\\nApart from SMP and SWAT, we implement another baseline Monolithic for reference. Figure 5 displays the performance on 3d humanoid full and 3d cheetah full. In line with the observations in (Dong et al., 2022), the GNN-based method SMP is worse than the MLP-based model Monolithic; but different from the results in (Dong et al., 2022), SWAT still surpasses Monolithic on 3d cheetah full. We conjecture SWAT benefits from the application of Transformer that is expressive enough to characterize the variation of our 3d cheetah full environments. Our model SET takes advantage of both the expressive power of the Transformer-akin model and the rational constraint by subequivariance, hence it delivers much better performance than all other methods.\\n\\n5.3. Comparison with Invariant Methods\\n\\nInvariant methods have been widely utilized in the 3D RL literature. For instance, in humanoid control, the presence of gravity allows for the normalization of state and action spaces in the heading (yaw) direction (e.g., a recent work (Won et al., 2022)). This heading normalization (HN) technique transforms the global coordinate frame into a local coordinate frame, enabling the input geometric information to be mapped to a rotation- and translation-invariant representation. We compare SET with the following invariant variants:\\n\\n1. SWAT+HN: a state-of-the-art morphology-agnostic baseline that uses the heading normalization, and\\n2. Monolithic+HN: a standard TD3-based non-morphology-agnostic baseline.\\n\\nTable 3. Single-task performance with added bias in the heading normalization. The table header (the first row of the table) represents the environment and the bias.\\n\\n| Methods      | 3d humanoid 9 full | 3d cheetah 14 full |\\n|--------------|-------------------|-------------------|\\n| Monolithic+HN| 13142 \u00b1 2840 2     | 57 \u00b1 12 4         |\\n| SWAT+HN      | 8517 \u00b1 1796 92    | 15924 \u00b1 543 9 1    |\\n| SET          | 9931 \u00b1 632 9      | 106 \u00b1 2023 4     |\\n\\nTable 4. Compared with Heading Normalization in zero-shot evaluation on the test set. Note that we omit the lacking part in the name of morphologies.\\n\\n| Environment | SET | SWAT+HN |\\n|-------------|-----|---------|\\n| 3d walker   | 206.8 \u00b1 37 4 26 | 243.7 \u00b1 32 4 156 8 \u00b1 11 8 1 |\\n| 3d humanoid | 161.9 \u00b1 3 4 130 2 \u00b1 2 1 | 180.0 \u00b1 6 5 152 9 \u00b1 36 8 1 |\\n| 3d cheetah  | 1078.1 \u00b1 722 8 786 5 \u00b1 779 3 | 3038.3 \u00b1 2803 3 2517 3 \u00b1 2113 9 |\\n\\nTable 5 demonstrates heading-equivariance by construction. Indeed, there is a limitation of heading normalization in that it assumes a consistent definition of the \\\"forward\\\" direction across all agents. Without a consistent \\\"forward\\\" direction, the normalization scheme would need to be redefined for each individual agent, which could limit its transfer ability to different types of agents or environments. On the contrary, equivariant methods, such as the one proposed in our work, can be more generalizable as they do not rely on a specific normalization scheme and can adapt to different transformations in the environment. We design a simple experiment to verify the above statement by translating the \\\"forward\\\" direction of the agent via a certain bias angle during testing. Table 3 demonstrates the significant performance degradation caused by adding bias in the heading normalization. Moreover, we can support this point through zero-shot generalization experiments, where we evaluate the trained policies from multi-task on unseen zero-shot testing variants. Table 4 demonstrates that SET has stronger generalization ability compared to SWAT+HN. For more detailed discussions, please refer to Appendix D.\\n\\n5.4. Ablation\\n\\nWe ablate the following variants in Figure 7:\\n\\n1. SET_g: an O(3) equivariant model, where gravity \\\\( \\\\vec{g} \\\\) is removed from the external force and concatenated into the scalar input, \\\\( h(i) = [h(i), \\\\vec{g}] \\\\);\\n2. SET_gd: an O(3) equivariant model, where both \\\\( \\\\vec{g} \\\\) and \\\\( \\\\vec{d} \\\\) are considered as scalars: \\\\( h(i) = [h(i), \\\\vec{g}, \\\\vec{d}] \\\\);\\n3. SET_z: an O(\\\\( \\\\vec{g}(3) \\\\)) equivariant model without Equation (5), by omitting the height \\\\( \\\\vec{p}_z \\\\).\"}"}
{"id": "chen23i", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof. By Theorem A.1, we have $\\\\vec{Z}^*(L) i = \\\\vec{Z}(L) i$ and $M^*(L) i = M(L) i$. Therefore, $\\\\vec{u}(L) i = \\\\vec{Z}^*(L) i W(L) \\\\vec{u} = \\\\vec{Z}(L) i W(L) \\\\vec{u}$. Hence, $\\\\vec{T} i = [\\\\vec{u}(L) i, \\\\vec{g}, \\\\vec{d}] \\\\sigma(M(L) i W(L) \\\\vec{T} i)$, $\\\\vec{g} = \\\\vec{g}$, given the definition of $\\\\vec{g}$. Finally, $a^* i = [\\\\vec{T} i \\\\top \\\\vec{x} i, \\\\vec{T} i \\\\top \\\\vec{y} i, \\\\vec{T} i \\\\top \\\\vec{z} i]$, $\\\\vec{g} = \\\\vec{g}$, $\\\\vec{T} i \\\\cdot \\\\vec{x} i, \\\\vec{T} i \\\\cdot \\\\vec{y} i, \\\\vec{T} i \\\\cdot \\\\vec{z} i$.\\n\\nB. Related Works\\n\\nMorphology-Agnostic RL\\n\\nIn recent years, we have seen the emergence and development of multi-task RL with the inhomogeneous morphology setting, where the state and action spaces are different across tasks (Devin et al., 2017; Chen et al., 2018; D'Eramo et al., 2020). The morphology-agnostic approach, which learns policies for each joint using multiple message passing schemes, decentralizes the control of multi-joint robots. In order to deal with the inhomogeneous setting, NerveNet (Wang et al., 2018), DGN (Pathak et al., 2019) and SMP (Huang et al., 2020) represent the morphology of the agent as a graph and deploy GNNs as the policy network. MORPHEUS (Kurin et al., 2020), SWAT (Hong et al., 2021) and SOLAR (Dong et al., 2022) utilize the self-attention mechanism instead of GNNs for direct communication. In morphology-agnostic RL, both of their investigations demonstrate that the graph-based policy has significant advantages over a monolithic policy. Our work is based on SWAT and introduces a set of new benchmarks that relax the over-simplified state and action space of existing works to a much more challenging scenario with immersive search space.\\n\\nGeometrically Equivariant Models\\n\\nProminently, there are certain symmetries in the physical world and there have been a number of studies about group equivariant models (Cohen & Welling, 2016; Cohen & Welling, 2017; Worrall et al., 2017). In recent years, a field of research known as geometrically equivariant graph neural networks (Han et al., 2022b), leverages symmetry as an inductive bias in learning. These models are designed such that their outputs will rotate/translate/reflect in the same way as the inputs, hence retaining the symmetry. Several methods are used to achieve this goal, such as using irreducible representation to solve group convolution (Thomas et al., 2018; Fuchs et al., 2020) or utilizing invariant scalarization (Villar et al., 2021) like taking the inner product (Satorras et al., 2021; Huang et al., 2022; Han et al., 2022a). Along with GMN's (Huang et al., 2022) and SGNN's (Han et al., 2022a) approaches to scalarization, our method is a member of this family. In a Markov decision process (MDP) with symmetries (van der Pol et al., 2020), there are symmetries in the state-action space where policies can thus be optimized in the simpler abstract MDP. van der Pol et al. (2020) attempts to learn equivariant policy and invariant value networks in 2D toy environments. Our work focuses on the realization of this motivation in more complex 3D physics simulation environments.\\n\\nC. More Experimental Details\\n\\nC.1. Environments and Agents\\n\\nWe choose the following environments from morphology-agnostic RL benchmark (Huang et al., 2020) to evaluate our methods: Hopper++, Walker++, Humanoid++, Cheetah++. To facilitate the study of subequivariant graph reinforcement learning across these agents, we modify the 2D-Planar agents and extend them into 3D agents. Specifically, we modify...\"}"}
{"id": "chen23i", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the joint of torso from the combination of \u201cslide-slide-hinge\u201d type to \u201cfree\u201d type. Normally, each joint of the agent in the 2D-Planar environment has only one hinge-type actuator to make it rotate around \\\\( y \\\\)-axis. In order to make the agent more flexible to explore and optimize the learning process, we expand its action space including increasing the number of hinge-type actuators from 1 to 3, thus the DoF of each joint is also enlarged to 3. The two newly-added actuators enable the joint to basically rotate around \\\\( x \\\\) and \\\\( z \\\\)-axis, respectively.\\n\\n**3D Hopper:** The rotation range of the joint\u2019s two newly-added actuators is limited to \\\\((-10\\\\pi, 10\\\\pi)\\\\).\\n\\n**3D Walker:** The legs of 3D Walker is designed with reference to the legs of standard 3D Humanoid (Tassa et al., 2012). The rotation range of each joint is limited to new intervals. The rotation range of the joints in left and right leg are the same, we only show the intervals of a joint of the left leg:\\n\\n- the joint of thigh: \\\\((-25\\\\pi, 5\\\\pi), (-20\\\\pi, 110\\\\pi), (-60\\\\pi, 35\\\\pi)\\\\).\\n- the joint of shin: \\\\((-1\\\\pi, 1\\\\pi), (-160\\\\pi, -2\\\\pi)\\\\).\\n- the joint of foot: \\\\((-1\\\\pi, 1\\\\pi), (-45\\\\pi, 45\\\\pi), (-30\\\\pi, 5\\\\pi)\\\\).\\n\\n**3D Humanoid:** We refer to the standard 3D Humanoid (Tassa et al., 2012) and expand the number of actuators. The rotation range of newly-added joint actuators are limited to \\\\((-\\\\pi, \\\\pi)\\\\).\\n\\n**3D Cheetah:** The standard half-cheetah (Wawrzynski, 2007; Wawrzy\u0144ski, 2009) is specially designed as a planar model of a walking animal, which would not fall over in 2D-Planar environments, so there is no interruption in each episode. But in 3D-SGRL environments, the half-cheetah very easy to falls over and this will interrupt its learning process, making it more difficult for effective locomotion. So we modify the model of a half-cheetah into a full-cheetah, and its torso, four legs and tail are made of 14 limbs. 3D Cheetah is about 1.1 meters long, 0.6 meters high and weighs 55kg. We limit the \u201cstrengths\u201d of its joints within the range from 30 to 120Nm. So it is designed as a 3D model of a large and agile cat with many joints yet smaller strength, making it more stable and less easy to fall over in 3D-SGRL environments while retaining a strong locomotion ability. As a result, the full-cheetah is more adaptable to 3D-SGRL environments. The rotation range of joints is limited to new intervals. The rotation range of the tail is \\\\((-20\\\\pi, 20\\\\pi), (-80\\\\pi, 80\\\\pi), (-\\\\pi, \\\\pi)\\\\).\\n\\nThe rotation range of the left limb and the right limb are the same, we only show the intervals of those left:\\n\\n- the joint of back thigh: \\\\((-10\\\\pi, 0\\\\pi), (-60\\\\pi, 30\\\\pi), (-15\\\\pi, 5\\\\pi)\\\\).\\n- the joint of back shin: \\\\((-\\\\pi, \\\\pi), (-45\\\\pi, 45\\\\pi), (-\\\\pi, \\\\pi)\\\\).\\n- the joint of back foot: \\\\((-1\\\\pi, 1\\\\pi), (-45\\\\pi, 25\\\\pi), (-15\\\\pi, 5\\\\pi)\\\\).\\n- the joint of front thigh: \\\\((-15\\\\pi, 5\\\\pi), (-40\\\\pi, 60\\\\pi), (-20\\\\pi, 10\\\\pi)\\\\).\\n- the joint of front shin: \\\\((-\\\\pi, \\\\pi), (-50\\\\pi, 70\\\\pi), (-\\\\pi, \\\\pi)\\\\).\\n- the joint of front foot: \\\\((-\\\\pi, \\\\pi), (-30\\\\pi, 30\\\\pi), (-20\\\\pi, 5\\\\pi)\\\\).\\n\\nTo systematically investigate the proposed method applied to multi-task training, we construct several variants from the agents we mentioned above, as shown in Table 5. The morphologies of ten variants of 3D Cheetah are different from that of the 2D-Planar, as is shown in Figure 9.\\n\\n**C.2. Baselines**\\n\\nThis part illustrates the implementations of these baselines. **SMP** Huang et al. (2020) employs GNNs as policy networks and uses both bottom-up and top-down message passing schemes through the links between joints for coordinating. We use the implementation of SMP in the SWAT codebase, which is the same as the original implementation of SMP provided by Huang et al. (2020).\"}"}
{"id": "chen23i", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 5\\n\\n| Environment          | Training | Zero-shot testing |\\n|----------------------|----------|-------------------|\\n| 3D Hopper++          | 3d       | 3d shin           |\\n| 3D Hopper++          | 3d       | 3d full           |\\n| 3D Walker++          | 3d right | 3d left leg       |\\n| 3D Walker++          | 3d left  | 3d left knee      |\\n| 3D Walker++          | 3d left  | 3d right foot     |\\n| 3D Humanoid++        | 3d left  | 3d left arm       |\\n| 3D Humanoid++        | 3d lower | 3d left leg       |\\n| 3D Humanoid++        | 3d right | 3d right arm      |\\n| 3D Humanoid++        | 3d right | 3d right leg      |\\n| 3D Humanoid++        | 3d left  | 3d left knee      |\\n| 3D Cheetah++         | 3d left  | 3d left arm       |\\n| 3D Cheetah++         | 3d left  | 3d left leg       |\\n| 3D Cheetah++         | 3d tail  | 3d right leg      |\\n| 3D Cheetah++         | 3d tail  | 3d left foot      |\\n| 3D Cheetah++         | 3d tail  | 3d right foot     |\\n| 3D Cheetah++         | 3d tail  | 3d tail           |\\n| 3D Cheetah-3D Walker-3D Humanoid-3D Hopper++ (3D CHWHH++) | Union of 3D Walker++, 3D Humanoid++, and 3D Hopper++ |\\n| 3D Cheetah-3D Walker-3D Humanoid-3D Hopper++ (3D CWHH++) | Union of 3D Cheetah++, 3D Walker++, 3D Humanoid++, and 3D Hopper++ |\"}"}
{"id": "chen23i", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Graph Reinforcement Learning in 3D Environments\\n\\nFigure 9. The morphologies of 10 variants of cheetah.\\n\\nSWAT\\n\\nAll of the GNN-like works show that morphology-agnostic policies are more advantageous than the monolithic policy in tasks aiming at tackling different morphologies. However, Kurin et al. (2020) validate a hypothesis that the benefit extracted from morphological structures by GNNs can be offset by their negative effect on message passing. They further propose a transformer-based method, A\\\\textsubscript{MORPHEUS}, which relies on mechanisms for self-attention as a way of message transmission. Hong et al. (2021) make use of morphological traits via structural embeddings, enabling direct communication and capitalizing on the structural bias. We use the original implementation of SWAT released by Hong et al. (2021). For a fair comparison, S\\\\textsubscript{ET} uses the same hyperparameters as SWAT (Table 6).\\n\\nMonolithic\\n\\nWe choose TD3 as the standard monolithic RL baseline. The actor and critic of TD3 are implemented by fully-connected neural networks.\\n\\nC.3. Implementation details\\n\\nFor the scalar features $h_i \\\\in \\\\mathbb{R}^{13}$, in addition to retaining the original rotation angle of joint, we also undergo the following processing: the rotation angle and range of joint are represented as three scalar numbers $(\\\\text{angle}_t, \\\\text{low}, \\\\text{high})$ normalized to $[0, 1]$, where $\\\\text{angle}_t$ is the joint position at time $t$, and $[\\\\text{low}, \\\\text{high}]$ is the allowed joint range. The type of limb is a 4-dimensional one-hot vector representing \\\"torso\\\", \\\"thigh\\\", \\\"shin\\\", \\\"foot\\\" and \\\"other\\\" respectively. Besides, note that the torso limb has no joint actuator in any of these environments, so we ignore its predicted torque values. We implement S\\\\textsubscript{ET} based on SWAT codebase (Hong et al., 2021), which is built on Official PyTorch Tutorial. SWAT also shares the codebase with SMP (Huang et al., 2020) and A\\\\textsubscript{MORPHEUS} (Kurin et al., 2020). Table 6 provides the hyperparameters needed to replicate our experiments. Our codes are available on https://github.com/alpc91/SGRL.\\n\\n| Hyperparameter | Value |\\n|----------------|-------|\\n| Learning rate  | 0.0001|\\n| Gradient clipping | 0.1 |\\n| Normalization LayerNorm | |\\n| Total attention layers | 3 |\\n| Attention heads | 2 |\\n| Attention embedding size | 128 |\\n| Attention hidden size | 256 |\\n| Matrix embedding size | 32 $\\\\times$ 32 |\\n| Matrix hidden size | 512 |\\n| Encoder output size | 128 |\\n| Mini-batch size | 100 |\\n| Maximum Replay buffer size | 10M |\"}"}
{"id": "chen23i", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13. The evaluation on v2-variants on 3D Cheetah++.\"}"}
{"id": "chen23i", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Graph Reinforcement Learning in 3D Environments\\n\\nFigure 7. Training curves of ablations of \\\\textit{S\\\\textsubscript{ET}} on 3d humanoid 9 full and 3d cheetah 14 full.\\n\\nFigure 8. Average height of all limbs.\\n\\n4. \\\\textit{S\\\\textsubscript{ET}} in invar: a non-equivariant model without all geometric vectors, instead taking them as the scalar input, \\\\( h(0)_i = [h(0)_i, \\\\vec{Z}_i, \\\\vec{g}, \\\\vec{d}] \\\\);\\n\\n5. \\\\textit{S\\\\textsubscript{ET}} out invar: an \\\\( \\\\vec{g}(3) \\\\)-equivariant model by replacing the action output by the projection strategy in Equation (14) with an \\\\( \\\\vec{g}(3) \\\\)-invariant mapping \\\\( a_i = W_{\\\\pi} \\\\theta \\\\text{vec}(M(L)_i) + b_{\\\\pi} \\\\).\\n\\n1. \\\\textit{S\\\\textsubscript{ET}} \\\\_g and \\\\textit{S\\\\textsubscript{ET}} \\\\_z, compared with \\\\textit{S\\\\textsubscript{ET}}, gain close performance on 3d cheetah 14 full, but are much worse on 3d humanoid 9 full. This is reasonable, as the agent 3d cheetah 14 full has four legs and can locomote stably (see Figure 8). It is thus NOT so essential to consider the effect of gravity and the height to the ground on 3d cheetah 14 full. As for 3d humanoid 9 full with 2 legs, however, it is important to sense the direction of gravity and detect the height to avoid potential falling down, hence the correct modeling of gravity and the height are necessary for locomotion policy learning.\\n\\n2. The performance of \\\\textit{S\\\\textsubscript{ET}} \\\\_gd is poor in both cases, indicating that maintaining the direction information of the task guidance is indispensable.\\n\\n3. \\\\textit{S\\\\textsubscript{ET}} in invar behaves much worse than \\\\textit{S\\\\textsubscript{ET}}, which verifies the importance to incorporate subequivariance into our model design.\\n\\n4. \\\\textit{S\\\\textsubscript{ET}} out invar is worse than \\\\textit{S\\\\textsubscript{ET}} but already exceeds other variants. The equivariant output \\\\( \\\\vec{T}_i \\\\) in \\\\textit{S\\\\textsubscript{ET}} contains rich orientation information, and it is more direct to obtain the output torque by projecting \\\\( \\\\vec{T}_i \\\\), than \\\\textit{S\\\\textsubscript{ET}} out invar which uses the invariant matrix \\\\( M(L)_i \\\\) to predict the action.\\n\\n6. Discussion\\nIn current machine learning research, equivariance and attention are both powerful ideas. To learn a shared graph-based policy in 3D-SGRL, we design \\\\textit{S\\\\textsubscript{ET}}, a novel transformer model that preserves geometric symmetry by construction. Experimental results strongly support the necessity of encoding symmetry into the policy network, which demonstrates its wide applicability in various 3D environments. We also compare the Monolithic MLP-based model using heading normalization for single-task training in Figure 5. It can be found that a simple MLP with heading normalization may outperform the benefits brought by equivariance and attention. Therefore, in comparison to traditional methods in single-task settings, we cannot guarantee that all humanoids and legged robots will experience considerable enhancement when using our equivariant methods. In this work, our main contribution is extending the 2D benchmark to 3D for morphology-agnostic RL, which mainly addresses challenges in multi-task learning with agents of heterogeneous morphology where MLP may not be applicable. Although these are just initial steps, we believe that further exploration of this research direction will lead to valuable contributions to the research community.\\n\\nAcknowledgements\\nThis work is jointly funded by \\\"New Generation Artificial Intelligence\\\" Key Field Research and Development Plan of Guangdong Province (2021B0101410002), the National Science and Technology Major Project of the Ministry of Science and Technology of China (No.2018AAA0102900), the Sino-German Collaborative Research Project Cross-modal Learning (NSFC 62061136001/DFG TRR169), THU-Bosch JCML Center, the National Natural Science Foundation of China under Grant U22A2057, the National Natural Science Foundation of China (No.62006137), Beijing Outstanding Young Scientist Program (No.BJJWZYJH012019100020098), and Scientific Research Fund Project of Renmin University of China (Start-up Fund Project for New Teachers). We sincerely thank the reviewers for their comments that significantly improved our paper's quality. Our heartfelt thanks go to Yu Luo, Tianying Ji, Chengliang Zhong, and Chao Yang for fruitful discussions. Finally, Runfa Chen expresses gratitude to his fianc\u00e9e, Xia Zhong, for her unwavering love and support.\"}"}
{"id": "chen23i", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Graph Reinforcement Learning in 3D Environments\\n\\nReferences\\n\\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n\\nBattaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R., et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\\n\\nChen, T., Murali, A., and Gupta, A. Hardware conditioned policies for multi-robot transfer learning. In Advances in Neural Information Processing Systems, volume 31, 2018.\\n\\nCohen, T. S. and Welling, M. Group equivariant convolutional networks. In International Conference on Machine Learning, 2016.\\n\\nCohen, T. S. and Welling, M. Steerable CNNs. In International Conference on Learning Representations, 2017.\\n\\nD'Eramo, C., Tateo, D., Bonarini, A., Restelli, M., Peters, J., et al. Sharing knowledge in multi-task deep reinforcement learning. In International Conference on Learning Representations, 2020.\\n\\nDevin, C., Gupta, A., Darrell, T., Abbeel, P., and Levine, S. Learning modular neural network policies for multi-task and multi-robot transfer. In 2017 IEEE International Conference on Robotics and Automation, pp. 2169\u20132176. IEEE, 2017.\\n\\nDong, H., Wang, T., Liu, J., and Zhang, C. Low-rank modular reinforcement learning via muscle synergy. In Advances in Neural Information Processing Systems, 2022.\\n\\nFuchs, F., Worrall, D., Fischer, V., and Welling, M. SE(3)-transformers: 3D roto-translation equivariant attention networks. In Advances in Neural Information Processing Systems, volume 33, pp. 1970\u20131981. Curran Associates, Inc., 2020.\\n\\nFujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning, pp. 1587\u20131596. PMLR, 2018.\\n\\nFuruta, H., Iwasawa, Y., Matsuo, Y., and Gu, S. S. A system for morphology-task generalization via unified representation and behavior distillation. In International Conference on Learning Representations, 2023.\\n\\nGupta, A., Fan, L., Ganguli, S., and Fei-Fei, L. Metamorph: Learning universal controllers with transformers. In International Conference on Learning Representations, 2022.\\n\\nHan, J., Huang, W., Ma, H., Li, J., Tenenbaum, J. B., and Gan, C. Learning physical dynamics with subequivariant graph neural networks. In Advances in Neural Information Processing Systems, volume 35, pp. 26256\u201326268, 2022a.\\n\\nHan, J., Rong, Y., Xu, T., and Huang, W. Geometrically equivariant graph neural networks: A survey. arXiv preprint arXiv:2202.07230, 2022b.\\n\\nHong, S., Yoon, D., and Kim, K.-E. Structure-aware transformer policy for inhomogeneous multi-task reinforcement learning. In International Conference on Learning Representations, 2021.\\n\\nHsu, C., Verkuil, R., Liu, J., Lin, Z., Hie, B., Sercu, T., Lerer, A., and Rives, A. Learning inverse folding from millions of predicted structures. In International Conference on Machine Learning, pp. 8946\u20138970. PMLR, 2022.\\n\\nHuang, W., Mordatch, I., and Pathak, D. One policy to control them all: Shared modular policies for agent-agnostic control. In International Conference on Machine Learning, pp. 4455\u20134464. PMLR, 2020.\\n\\nHuang, W., Han, J., Rong, Y., Xu, T., Sun, F., and Huang, J. Equivariant graph mechanics networks with constraints. In International Conference on Learning Representations, 2022.\\n\\nJ\u00f8rgensen, P. B. and Bhowmik, A. Equivariant graph neural networks for fast electron density estimation of molecules, liquids, and solids. npj Computational Materials, 8(1):183, 2022.\\n\\nJoshi, C. K., Bodnar, C., Mathis, S. V., Cohen, T., and Li`o, P. On the expressive power of geometric graph neural networks. In NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations, 2022.\\n\\nKurin, V., Igl, M., Rockt\u00e4schel, T., Boehmer, W., and White-son, S. My body is a cage: the role of morphology in graph-based incompatible control. In International Conference on Learning Representations, 2020.\\n\\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. In International Conference on Learning Representations, 2016.\\n\\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.\"}"}
{"id": "chen23i", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Graph Reinforcement Learning in 3D Environments\\n\\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928\u20131937. PMLR, 2016.\\n\\nPathak, D., Lu, C., Darrell, T., Isola, P., and Efros, A. A. Learning to control self-assembling morphologies: a study of generalization via modularity. In Advances in Neural Information Processing Systems, volume 32, 2019.\\n\\nSatorras, V. G., Hoogeboom, E., and Welling, M. E (n) equivariant graph neural networks. In International Conference on Machine Learning, pp. 9323\u20139332. PMLR, 2021.\\n\\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n\\nSch\u00fctz, K., Unke, O., and Gastegger, M. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In International Conference on Machine Learning, pp. 9377\u20139388. PMLR, 2021.\\n\\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., PANNEERSHELVAM, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.\\n\\nTassa, Y., Erez, T., and Todorov, E. Synthesis and stabilization of complex behaviors through online trajectory optimization. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 4906\u20134913. IEEE, 2012.\\n\\nThomas, N., Smidt, T., Kearnes, S., Yang, L., Li, L., Kohlhoff, K., and Riley, P. Tensor field networks: Rotation-and translation-equivariant neural networks for 3D point clouds. arXiv preprint arXiv:1802.08219, 2018.\\n\\nTodorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033. IEEE, 2012.\\n\\nTrabucco, B., Phielipp, M., and Berseth, G. Anymorph: Learning transferable policies by inferring agent morphology. In International Conference on Machine Learning, pp. 21677\u201321691. PMLR, 2022.\\n\\nvan der Pol, E., Worrall, D., van Hoof, H., Oliehoek, F., and Welling, M. Mdp homomorphic networks: Group symmetries in reinforcement learning. In Advances in Neural Information Processing Systems, volume 33, pp. 4199\u20134210, 2020.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017.\\n\\nVillar, S., Hogg, D. W., Storey-Fisher, K., Yao, W., and Blum-Smith, B. Scalars are universal: Equivariant machine learning, structured like classical physics. In Advances in Neural Information Processing Systems, volume 34, pp. 28848\u201328863, 2021.\\n\\nWang, T., Liao, R., Ba, J., and Fidler, S. Nervenet: Learning structured policy with graph neural networks. In International conference on learning representations, 2018.\\n\\nWawrzynski, P. Learning to control a 6-degree-of-freedom walking robot. In EUROCON 2007-The International Conference on\u201d Computer as a Tool\u201d, pp. 698\u2013705. IEEE, 2007.\\n\\nWawrzy\u0144ski, P. A cat-like robot real-time learning to run. In International Conference on Adaptive and Natural Computing Algorithms, pp. 380\u2013390. Springer, 2009.\\n\\nWon, J., Gopinath, D., and Hodgins, J. Physics-based character controllers using conditional vaes. ACM Transactions on Graphics (TOG), 41(4):1\u201312, 2022.\\n\\nWorrall, D. E., Garbin, S. J., Turmukhambetov, D., and Brostow, G. J. Harmonic networks: Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5028\u20135037, 2017.\"}"}
{"id": "chen23i", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Graph Reinforcement Learning in 3D Environments\\n\\nA. Proofs\\n\\nIn this section, we theoretically prove that our proposed SubEquivariant Transformer (SET), and the final output action and critic Q-function value preserve the symmetry as desired. We start by verifying our design in SET.\\n\\nTheorem A.1. Let \\\\((\\\\vec{Z}', h') = \\\\phi(\\\\vec{Z}, \\\\vec{g}, \\\\vec{d}, h)\\\\), where \\\\(\\\\phi\\\\) is one layer of our SET specified from Equation (7) to Equation (12).\\n\\nLet \\\\((\\\\vec{Z}'^*, h'^*) = \\\\phi(O\\\\vec{Z}, \\\\vec{g}, O\\\\vec{d}, h)\\\\), \\\\(\\\\forall O \\\\in O\\\\vec{g}(3)\\\\). Then, we have \\\\((\\\\vec{Z}'^*, h'^*) = (O\\\\vec{Z}', h')\\\\), indicating \\\\(\\\\phi\\\\) is \\\\(O\\\\vec{g}(3)\\\\)-equivariant.\\n\\nProof. In the first place, we have \\\\(\\\\vec{m}^* i = O\\\\vec{Z}_i W \\\\vec{m} = O\\\\vec{m}_i\\\\). For the message \\\\(M_i\\\\), we have,\\\\n\\n\\\\[\\nM^* i = \\\\sigma M \\\\sigma \\\\vec{m} [\\\\vec{m}^* i, \\\\vec{g}, O\\\\vec{d}] \\\\top [\\\\vec{m}^* i, \\\\vec{g}, O\\\\vec{d}] h, \\\\tag{16}\\n\\\\]\\n\\n\\\\[\\n= \\\\sigma M \\\\sigma [\\\\vec{m} \\\\top O \\\\top O \\\\vec{m}_i, \\\\vec{g}, O\\\\vec{d}] \\\\top [\\\\vec{m} \\\\top O \\\\top O \\\\vec{m}_i, \\\\vec{g}, O\\\\vec{d}] h, \\\\tag{17}\\n\\\\]\\n\\n\\\\[\\n= \\\\sigma M \\\\sigma [\\\\vec{m} \\\\top O \\\\top O \\\\vec{m}_i, \\\\vec{g}, O\\\\vec{d}] \\\\top [\\\\vec{m} \\\\top O \\\\top O \\\\vec{m}_i, \\\\vec{g}, O\\\\vec{d}] h, \\\\tag{18}\\n\\\\]\\n\\nFrom Equation (18) to Equation (19) we use the fact \\\\(O \\\\top O = I\\\\) and \\\\(O \\\\top \\\\vec{g} = \\\\vec{g}\\\\), by the definition of the group \\\\(O\\\\vec{g}(3)\\\\). With the \\\\(O\\\\vec{g}(3)\\\\)-invariant message \\\\(M_i\\\\), it is then immediately illustrated that the query \\\\(q_i\\\\), key \\\\(k_i\\\\), value message \\\\(v_j\\\\), and the attention coefficient \\\\(\\\\alpha_{ij}\\\\) are all \\\\(O\\\\vec{g}(3)\\\\)-invariant, and value message \\\\(\\\\vec{u}^* j = \\\\vec{Z}^* j W \\\\vec{u} = O\\\\vec{Z}_j W \\\\vec{u} = O\\\\vec{u}_j\\\\) is \\\\(O\\\\vec{g}(3)\\\\)-equivariant.\\n\\nFinally, we have,\\\\n\\n\\\\[\\n\\\\vec{Z}'^* i = O\\\\vec{Z}_i + X_j \\\\alpha_{ij} [\\\\vec{u}^* j, \\\\vec{g}, O\\\\vec{d}] W \\\\vec{Z}, \\\\tag{21}\\n\\\\]\\n\\n\\\\[\\n= O\\\\vec{Z}_i + X_j \\\\alpha_{ij} O[\\\\vec{u}^* j, \\\\vec{g}, \\\\vec{d}] W \\\\vec{Z}, \\\\tag{22}\\n\\\\]\\n\\n\\\\[\\n= O [\\\\vec{Z}_i + X_j \\\\alpha_{ij} O[\\\\vec{u}^* j, \\\\vec{g}, \\\\vec{d}] W \\\\vec{Z}] , \\\\tag{23}\\n\\\\]\\n\\n\\\\[\\n= O\\\\vec{Z}'^* i, \\\\tag{24}\\n\\\\]\\n\\nand similarly,\\\\n\\n\\\\[\\nh'^* i = LN [h_i + W h X_j (\\\\alpha_{ij} v_j) + b h] = h'^* i, \\\\tag{25}\\n\\\\]\\n\\nBy going through all nodes \\\\(i \\\\in \\\\{1, \\\\ldots, |V|\\\\}\\\\) the proof is completed.\\n\\nBy iteratively applying Theorem A.1 for \\\\(l \\\\in \\\\{1, \\\\ldots, L\\\\}\\\\) layers, we readily obtain the \\\\(O\\\\vec{g}(3)\\\\)-equivariance of the entire SET.\\n\\nAs for the actor and critic, we additionally have the following corollary.\\n\\nCorollary A.2. Let \\\\(a, Q_\\\\pi \\\\theta\\\\) be the output action and the critic of 3D-SGRL with \\\\(\\\\vec{Z}, \\\\vec{g}, \\\\vec{d}, h\\\\) as input. Let \\\\(a^*, Q^*_\\\\pi \\\\theta\\\\) be the action and critic with \\\\(O\\\\vec{Z}, \\\\vec{g}, O\\\\vec{d}, h\\\\) as input, \\\\(O \\\\in O\\\\vec{g}(3)\\\\). Then, \\\\((a^*, Q^*) = (a, Q)\\\\), indicating the output action and critic preserve \\\\(O\\\\vec{g}(3)\\\\)-invariance.\"}"}
{"id": "chen23i", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nLearning a shared policy that guides the locomotion of different agents is of core interest in Reinforcement Learning (RL), which leads to the study of morphology-agnostic RL. However, existing benchmarks are highly restrictive in the choice of starting point and target point, constraining the movement of the agents within 2D space. In this work, we propose a novel setup for morphology-agnostic RL, dubbed Subequivariant Graph RL in 3D environments (3D-SGRL). Specifically, we first introduce a new set of more practical yet challenging benchmarks in 3D space that allows the agent to have full Degree-of-Freedoms to explore in arbitrary directions starting from arbitrary configurations. Moreover, to optimize the policy over the enlarged state-action space, we propose to inject geometric symmetry, i.e., subequivariance, into the modeling of the policy and Q-function such that the policy can generalize to all directions, improving exploration efficiency. This goal is achieved by a novel SubEquivariant Transformer (SET) that permits expressive message exchange. Finally, we evaluate the proposed method on the proposed benchmarks, where our method consistently and significantly outperforms existing approaches on single-task, multi-task, and zero-shot generalization scenarios. Extensive ablations are also conducted to verify our design.\\n\\n1. Introduction\\n\\nLearning to locomote, navigate, and explore in the 3D world is a fundamental task in the pathway of building intelligence. Impressive breakthrough has been made towards realizing such intelligence thanks to the emergence of deep reinforcement learning (RL) (Mnih et al., 2015; Silver et al., 2016; Mnih et al., 2016; Schulman et al., 2017; Fujimoto et al., 2018), where the policy of the agent is acquired through interactions with the environment. More recently, by getting insight into the morphology of the agent, morphology-agnostic RL (Wang et al., 2018; Pathak et al., 2019; Huang et al., 2020; Kurin et al., 2020; Hong et al., 2021; Dong et al., 2022; Trabucco et al., 2022; Gupta et al., 2022; Furuta et al., 2023) has been proposed with the paradigm of learning a local and shared policy for all agents and the tasks involved, offering enhanced performance and transferability, especially in the multi-task scenario. It is usually fulfilled by leveraging Graph Neural Networks (GNNs) (Battaglia et al., 2018) or even Transformers (Vaswani et al., 2017) to derive the policy through passing and fusing the state information on the morphological graphs of the agents.\\n\\nIn spite of the fruitful progress by morphology-agnostic RL, in this work, we identify several critical setups that have been over-simplified in existing benchmarks, giving rise to a limited state/action space such that the obtained policy is unable to explore the entire 3D space. In particular, the agents are assigned a fixed starting point and restricted to moving towards a single direction along the \\\\( x \\\\)-axis, leading...\"}"}
{"id": "chen23i", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Graph Reinforcement Learning in 3D Environments\\n\\nto 2D motions only. Nevertheless, in a more realistic setup as depicted in Figure 1, the agents would be expected to have full Degree-of-Freedoms (DoFs) to turn and move in arbitrary directions starting from arbitrary configurations. To address the concern, we extend the existing environments to a set of new benchmarks in 3D space, which meanwhile introduces significant challenges to morphology-agnostic RL due to the massive enlargement of the state-action space for policy optimization.\\n\\nOptimizing the policy in our new setup is prohibitively difficult, and existing morphology-agnostic RL frameworks like (Huang et al., 2020; Hong et al., 2021) are observed to be susceptible to getting stuck in the local minima and exhibited poor generalization in our experiments. To this end, we propose to inject geometric symmetry (Cohen & Welling, 2016; Cohen & Welling, 2017; Worrall et al., 2017; van der Pol et al., 2020) into the design of the policy network to compact the space redundancy in a lossless way (van der Pol et al., 2020). In particular, we restrict the policy network to be subequivariant in two senses (Han et al., 2022a): 1. the output action will rotate in the same way as the input state of the agent; 2. the equivariance is partially relaxed to take into account the effect of gravity in the environment. We design SubEquivariant Transformer (SET) with a novel architecture that satisfies the above constraints while also permitting expressive message propagation through self-attention. Upon SET, the action and Q-function could be obtained with desirable symmetries guaranteed. We term our entire task setup and methodology as Subequivariant Graph Reinforcement Learning in 3D Environments (3D-SGRL).\\n\\nOur contributions are summarized as follows:\\n\\n\u2022 We introduce a set of more practical yet highly challenging benchmarks for morphology-agnostic RL, where the agents are permitted to turn and move in the 3D environments with arbitrary starting configurations and arbitrary target directions. For this purpose, we redesign the agents in current benchmarks by equipping them with more DoFs in a considerate way.\\n\\n\u2022 To effectively optimize the policy on such challenging benchmarks, we propose to enforce the policy network with geometric symmetry. We introduce a novel architecture dubbed SET that captures the rotation/translation equivariance particularly when external force fields like gravity exist in the environment.\\n\\n\u2022 We verify the performance of the proposed method on the proposed 3D benchmarks, where it outperforms existing morphology-agnostic RL approaches by a significant margin in various scenarios, including single-task, multi-task, and zero-shot generalization. Extensive ablations also reveal the efficacy of the proposed ideas.\\n\\n2. Background\\n\\nMorphology-Agnostic RL\\n\\nIn the context of morphology-agnostic RL (Huang et al., 2020), we are interested in an environment with $N$ agents (a.k.a tasks), where the $n$-th agent comprises $K_n$ limbs that control its motion. At time $t$, each limb $k \\\\in \\\\{1, \\\\ldots, K_n\\\\}$ of agent $n$ receives a state $s_{n,k}(t) \\\\in \\\\mathbb{R}^d$ and outputs a torque $a_{n,k}(t) \\\\in [-1, 1]$ to its actuator. As a whole, agent $n$ executes the joint action $a_n(t) = \\\\{a_{n,k}(t)\\\\}_{k=1}^{K_n}$ to interact with the environment which will return the next state of all limbs $s_n(t+1) = \\\\{s_{n,k}(t+1)\\\\}_{k=1}^{K_n}$ and a reward $r_n(s_n(t), a_n(t))$ for agent $n$. The goal of morphology-agnostic RL is to learn a shared policy $\\\\pi_\\\\theta$ among different agents to maximize the expected return:\\n\\n$$J(\\\\theta) = \\\\mathbb{E}_{\\\\pi_\\\\theta} \\\\sum_{n=1}^N \\\\sum_{t=0}^\\\\infty \\\\gamma^t r_n(s_n(t), a_n(t)),$$\\n\\nwhere $a_n(t) = \\\\pi_\\\\theta(s_n(t))$, $\\\\gamma$ is a discount factor, and $\\\\theta$ consists of trainable parameters.\\n\\nThe objective in Equation (1) is usually optimized via the actor-critic setup of the deterministic policy gradient algorithm for continuous control (Lillicrap et al., 2016), which estimates the Q-function for agent $n$:\\n\\n$$Q_{\\\\pi_\\\\theta}(s_n, a_n) = \\\\mathbb{E}_{\\\\pi_\\\\theta} \\\\sum_{t=0}^\\\\infty \\\\gamma^t r_n(s_n(t+1), a_n(t)) | s_n(0) = s_n, a_n(0) = a_n.$$\\n\\nTo uniformly learn a shared policy across all agents and tasks, previous methods (Wang et al., 2018; Pathak et al., 2019; Huang et al., 2020; Kurin et al., 2020; Hong et al., 2021; Dong et al., 2022), take into account the interaction of connected limbs and joints, and view the morphological structure of the agent as an undirected graph $G = (V, E)$, where each $v_i \\\\in V$ represents a limb and the edge $(v_i, v_j) \\\\in E$ stands for the joint connecting limb $i$ and $j$. A graph neural network $\\\\phi_\\\\theta$ is then employed to instantiate the policy $\\\\pi_\\\\theta$, which predicts the action $a$ given the state of all limbs $s$ and the graph topology $E$ as input, i.e., $a = \\\\phi_\\\\theta(s, E)$.\\n\\nEquivariance and Subequivariance\\n\\nTo further relieve the difficulty of learning a desirable policy within the massive search space formed by the states and actions of the agent in 3D space, we propose to encode the physical geometric symmetry of the policy learner $\\\\phi_\\\\theta$, so that the learned policy can generalize to operations in 3D, including rotations, translations, and reflections, altogether forming the $1$. For simplicity, we omit the index $n$ and $t$ henceforth in the above notations of agent $n$ at time $t$, since all agents share the same model for all time, e.g., $s_n(t) \\\\rightarrow s$ and $a_n(t) \\\\rightarrow a$.\"}"}
{"id": "chen23i", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"group of $E(3)$. Such constraint enforced on the model is formally described by the concept of equivariance (Thomas et al., 2018; Fuchs et al., 2020; Villar et al., 2021; Satorras et al., 2021; Huang et al., 2022; Han et al., 2022a,b).\\n\\n**Definition 2.1** ($E(3)$-equivariance). Suppose $\\\\vec{Z}$ to be 3D geometric vectors (positions, velocities, etc) that are steerable by $E(3)$ transformations, and $h$ non-steerable features. The function $f$ is $E(3)$-equivariant, if for any transformation $g \\\\in E(3)$, $f(g \\\\cdot \\\\vec{Z}, h) = g \\\\cdot f(\\\\vec{Z}, h)$, $\\\\forall \\\\vec{Z} \\\\in \\\\mathbb{R}^3 \\\\times m$, $h \\\\in \\\\mathbb{R}^d$.\\n\\nSimilarly, $f$ is invariant if $f(g \\\\cdot \\\\vec{Z}, h) = f(\\\\vec{Z}, h)$.\\n\\nBuilt on this notion, Han et al. (2022a) additionally considers equivariance on the subgroup of $O(3)$, induced by the external force $\\\\vec{g} \\\\in \\\\mathbb{R}^3$ like gravity, defined as $O(\\\\vec{g}) := \\\\{O \\\\in \\\\mathbb{R}^3 \\\\times 3 \\\\mid O^T O = I, O \\\\vec{g} = \\\\vec{g}\\\\}$. By this means, the symmetry is only restrained to the rotations/reflections along the direction of $\\\\vec{g}$. Such relaxation of group constraint is crucial in environments with gravity, as it offers extra flexibility to the model so that the effect of gravity could be captured. Han et al. (2022a) also presented a universally expressive construction of the $O(\\\\vec{g})$-equivariant functions:\\n\\n$$f(\\\\vec{g})(\\\\vec{Z}, h) = [\\\\vec{Z}, \\\\vec{g}] M(\\\\vec{g}),$$\\n\\ns.t. $M(\\\\vec{g}) = \\\\sigma([\\\\vec{Z}, \\\\vec{g}]^T [\\\\vec{Z}, \\\\vec{g}], h)$, (4)\\n\\nwhere $\\\\sigma(\\\\cdot)$ is an Multi-Layer Perceptron (MLP) and $[\\\\vec{Z}, \\\\vec{g}] \\\\in \\\\mathbb{R}^3 \\\\times (m+1)$ is a stack of $\\\\vec{Z}$ and $\\\\vec{g}$ along the last dimension. In particular, $f$ will reduce to be $O(3)$-equivariant if $\\\\vec{g}$ is omitted in the computation. In this way, $f(\\\\vec{g})$ can then be leveraged in the message passing process of the graph neural network $\\\\phi_\\\\theta$ in Equation (3) to obtain desirable geometric symmetry.\\n\\n### 3. Our task and method: 3D-SGRL\\n\\nIn this section, we present our novel formulation for morphology-agnostic RL, dubbed Subequivariant Graph Reinforcement Learning in 3D Environments (3D-SGRL). We first elaborate on the extensions made to the environment in Section 3.1, then introduce our entire framework, consisting of an input processing module (Section 3.2), a novel SubEquivariant Transformer (SET) for expressive information passing and fusion (Section 3.3), and output modules of actor and critic to obtain the final policy and Q-function (Section 3.4).\\n\\n#### 3.1. From 2D-Planar to 3D-SGRL\\n\\nA core mission of developing RL algorithms is enabling the agent (e.g., a robot) to learn to move in the environment with a designated goal. Ideally, the exploration should happen in the open space where the agent is able to move from the arbitrary starting point, via arbitrary direction, towards an arbitrary destination, offering much flexibility which highly corresponds to how the robot walks/runs in the real world. However, in the widely acknowledged setup in existing morphology-agnostic RL literature (Huang et al., 2020; Kurin et al., 2020; Hong et al., 2021; Dong et al., 2022), the agents are unanimously restricted in the fixed choice of starting position, target direction, and even the Degree-of-Freedom (DoF) of each joint in the action space. We summarize the limitations of the existing setup, which we dub 2D-Planar, and compare it with our introduced 3D-SGRL in Table 1 in three aspects, including state space, action space, and the consideration of geometric symmetry.\\n\\n| State Space | 2D-Planar | Our 3D-SGRL |\\n|-------------|------------|-------------|\\n| Range | xoz-plane | 3D space |\\n| Initial | $x^+$-axis | Arbitrary direction |\\n| Target | $x^+$-axis | Arbitrary direction |\\n\\n| Action Space | # Actuators | DoF |\\n|-------------|-------------|-----|\\n| 2D-Planar  | 1 per joint | 1 per joint |\\n| Our 3D-SGRL | 3 per joint | 3 per joint |\\n\\n| Symmetry | External Force | NULL | Gravity |\\n|-----------|---------------|------|---------|\\n| Group     | $\\\\emptyset$  | $O(\\\\vec{g})$ |\\n\\nWe summarize the limitations of the existing setup, which we dub 2D-Planar, and compare it with our introduced 3D-SGRL in Table 1 in three aspects, including state space, action space, and the consideration of geometric symmetry.\"}"}
{"id": "chen23i", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SubEquivariant Transformer (SET)\\nshin\\nfoot\\nother\\nthigh\\n\\nEquation (4);\\n\\nEquation (3)\\n\\nSubequivariant function, which ensures that\\n\\nself-attention\\n\\nmessage fusion between the limbs and joints, where the\\n\\ntorso\\n\\nother\\n\\nthigh\\n\\nshin\\n\\nfoot\\n\\n\ud835\udc56\\n\\nLimbJoint\\n\\n\ud835\udc47!=\ud835\udf0b\\\"(\ud835\udc94#)\\n\\nTorques\\n\\n\ud835\udc82! ...\\n\\n\ud835\udc88\ud835\udc90\ud835\udc9a\ud835\udc99\ud835\udc9b\\n\\n\ud835\udc88\ud835\udc90\ud835\udc9a\ud835\udc99\ud835\udc9b\\n\\n\ud835\udc56\\n\\nState\\n\\nState\\n\\n3.2. Input Processing\\n\\nindispensable clue in our 3D-SGRL tasks.\\n\\nspecified in the previous 2D-Planar setting but comes as an\\n\\nexpected destinations. The task guidance is not explicitly\\n\\nand acts like an attracted force guiding the agent towards\\n\\ngeometric symmetry, leading to poor performance in a real\\n\\nmorphology-agnostic RL works lack the consideration of\\n\\nin the policy can be well captured. By contrast, existing\\n\\npower the model such that the effect of gravity reflecting\\n\\nlessly to arbitrary direction rotated along the gravity axis.\\n\\nchallenge, we propose to take advantage of the geometric\\n\\n\u03c0\\n\\nlayers of our proposed SubEquivariant Transformer. The actor and critic are finally obtained, which are guaranteed to preserve the\\n\\n4-dimensional one-hot vector representing \\\"torso\\\", \\\"thigh\\\",\\n\\nand more challenging setup like 3D-SGRL. In addition to\\n\\ngeometric symmetry, the constraint in the design of\\n\\n\u03c6\\n\\nin the morphological graph\\n\\nsubdivide the state\\n\\nZ\\n\\nR\\n\\nConsole state\\n\\nis the\\n\\nM\\n\\nWith the value matrix\\n\\ncoefficients\\n\\n\u03b1\\n\\n\u2080\\n\\n\u03b9\\n\\n\u03c3\\n\\n handgun\\n\\noutside\\n\\n3D\\n\\nEnvironments generated by MuJoCo (Todorov et al.,\\n\\n6)\\n\\napart from\\n\\nx\\n\\ny\\n\\nz\\n\\n1\\n\\nxy\\n\\nR\\n\\nxyz\\n\\n13\\n\\n\u2212\\n\\n1\\n\\n\u2225\\n\\np\\n\\nxy\\n\\nz\\n\\nl\\n\\np\\n\\nxy\\n\\nR\\n\\nxyz\\n\\nl\\n\\nl\\n\\n1\\n\\n\u2225\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\na\\n\\n0\\n\\n\u03b9\\n\\n\u03c3\\n\\n handgun\\n\\noutside\\n\\n3D\\n\\nEnvironments generated by MuJoCo (Todorov et al.,\\n\\n6)\\n\\napart from\\n\\nx\\n\\ny\\n\\nz\\n\\n1\\n\\nxy\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n\\nl\\n\\np\\n\\nxy\\n\\nz\\n\\nR\\n\\nxyz\\n"}
