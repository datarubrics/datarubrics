{"id": "lai22a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FedScale: Benchmarking Model and System Performance of Federated Learning at Scale\\n\\nFan Lai\\nYinwei Dai\\nSanjay S. Singapuram\\nJiachen Liu\\nXiangfeng Zhu\\nHarsha V. Madhyastha\\nMosharaf Chowdhury\\n\\nAbstract\\nWe present FedScale, a federated learning (FL) benchmarking suite with realistic datasets and a scalable runtime to enable reproducible FL research. FedScale datasets encompass a wide range of critical FL tasks, ranging from image classification and object detection to language modeling and speech recognition. Each dataset comes with a unified evaluation protocol using real-world data splits and evaluation metrics. To reproduce realistic FL behavior, FedScale contains a scalable and extensible runtime. It provides high-level APIs to implement FL algorithms, deploy them at scale across diverse hardware and software backends, and evaluate them at scale, all with minimal developer efforts. We combine the two to perform systematic benchmarking experiments and highlight potential opportunities for heterogeneity-aware co-optimizations in FL. FedScale is open-source and actively maintained by contributors from different institutions at http://fedscale.ai. We welcome feedback and contributions from the community.\\n\\n1. Introduction\\nFederated learning (FL) is an emerging machine learning (ML) setting where a logically centralized coordinator orchestrates many distributed clients (e.g., smartphones or laptops) to collaboratively train or evaluate a model (Bonawitz et al., 2019; Kairouz et al., 2021b) (Figure 1). It enables model training and evaluation on end-user data, while circumventing high cost and privacy risks in gathering the raw data from clients, with applications across diverse ML tasks. In the presence of heterogeneous execution speeds of client devices as well as non-IID data distributions, existing efforts have focused on optimizing different aspects of FL: (1) System efficiency: reducing computation load (e.g., using smaller models (Sandler et al., 2018)) or communication traffic (e.g., local SGD (McMahan et al., 2017)) to achieve shorter round duration; (2) Statistical efficiency: designing data heterogeneity-aware algorithms (e.g., client clustering (Ghosh et al., 2020b)) to obtain better training accuracy with fewer training rounds; (3) Privacy and security: developing reliable strategies (e.g., differentially private training (Kairouz et al., 2021a)) to make FL more privacy-preserving and robust to potential attacks.\\n\\nA comprehensive benchmark to evaluate an FL solution must investigate its behavior under the practical FL setting with (1) data heterogeneity and (2) device heterogeneity under (3) heterogeneous connectivity and (4) availability conditions at (5) multiple scales on a (6) broad variety of ML tasks. While the first two aspects are oft-mentioned in the literature (Li et al., 2020), realistic network connectivity and the availability of client devices can affect both types of heterogeneity (e.g., distribution drift (Eichner et al., 2019)), impairing model convergence. Similarly, evaluation at a large scale can expose an algorithm's robustness, as practical FL deployment often runs across thousands of concurrent participants out of millions of clients (Yang et al., 2018). Overlooking any one aspect can mislead FL evaluation (\u00a72). Unfortunately, existing FL benchmarks often fall short across multiple dimensions (Table 1). First, they are limited in the versatility of data for various real-world FL appli-\"}"}
{"id": "lai22a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FedScale: Benchmarking Model and System Performance of Federated Learning at Scale\\n\\nFeatures\\n\\n- LEAF\\n- TFF\\n- FedML\\n- Flower\\n\\nFedScale\\n\\n- Heter. Client Dataset\\n- \u20dd \u20dd \u20dd \u20dd\\n- Heter. System Speed\\n- \u20dd \u20dd \u20dd \u20dd\\n- Client Availability\\n- \u20dd\\n- Scalable Platform\\n- \u20dd\\n- Real FL Runtime\\n- \u20dd\\n- Flexible APIs\\n- \u20dd\\n\\nTable 1. Comparing FedScale with existing FL benchmarks and libraries.\\n\\n- \u20dd implies limited support.\\n\\n- Indeed, even though they may have quite a few datasets and FL training tasks (e.g., LEAF (Caldas et al., 2019)), their datasets often contain synthetically generated partitions derived from conventional datasets (e.g., CIFAR) and do not represent realistic characteristics. This is because these benchmarks are mostly borrowed from traditional ML benchmarks (e.g., MLPerf (Mattson et al., 2020)) or designed for simulated FL environments like TensorFlow Federated (TFF) (tff) or PySyft (pys).\\n\\n- Second, existing benchmarks often overlook system speed, connectivity, and availability of the clients (e.g., FedML (He et al., 2020) and Flower (Beutel et al., 2021)). This discourages FL efforts from considering system efficiency and leads to overly optimistic statistical performance (\u00a72).\\n\\n- Third, their datasets are primarily small-scale, because their experimental environments are unable to emulate large-scale FL deployments. While real FL often involves thousands of participants in each training round (Kairouz et al., 2021b; Yang et al., 2018), most existing benchmarking platforms can merely support the training of tens of participants per round.\\n\\n- Finally, most of them lack user-friendly APIs for automated integration, resulting in great engineering efforts for benchmarking at scale. We attach a detailed comparison of existing benchmarks against FedScale in Appendix C.\\n\\nContributions\\n\\n- We introduce an FL benchmark and accompanying runtime, FedScale, to enable comprehensive and standardized FL evaluations:\\n\\n  - To the best of our knowledge, FedScale presents the most comprehensive collection of FL datasets for evaluating different aspects of real FL deployments. It currently has 20 realistic FL datasets with small, medium, and large scales for a wide variety of task categories, such as image classification, object detection, word prediction, speech recognition, and reinforcement learning.\\n\\n  - To account for practical client behaviors, we include real-world measurements of mobile devices and associate each client with their computation and communication speeds, as well as the availability status over time.\\n\\n- We present an automated evaluation platform, FedScale Runtime, to simplify and standardize FL evaluation in\\n\\n- 0\\n\\n- 500\\n\\n- 1000\\n\\n- 1500\\n\\n- Training Rounds\\n\\n- 30\\n\\n- 45\\n\\n- 60\\n\\n- 75\\n\\n- Accuracy (%) W/ Sys. Trace\\n\\n- W/o Sys. Trace\\n\\n(a) Impact of system trace.\\n\\n- Figure 2. Existing benchmarks can be misleading. We train Shufflene on OpenImage classification (Detailed setup in Section 5).\\n\\n- More realistic settings. FedScale Runtime provides a mobile backend to enable on-device FL evaluation and a cluster backend to benchmark various practical FL metrics (e.g., real client round duration) on GPUs/CPUs using real FL statistical and system datasets. The cluster backend can efficiently train thousands of clients in each round on a handful of GPUs. FedScale Runtime is also extensible, allowing easy deployment of new algorithms and ideas with flexible APIs.\\n\\n- We perform systematic experiments to show how FedScale facilitates comprehensive FL benchmarking and highlight the pressing need for co-optimizing system and statistical efficiency, especially in tackling system stragglers, accuracy bias, and device energy trade-offs.\\n\\n2. Background\\n\\n- Existing efforts toward practical FL\\n\\n- To tackle heterogeneous client data, FedProx (Li et al., 2020), FedYoGi (Reddi et al., 2020) and Scaffold (Karimireddy et al., 2020) introduce adaptive client/server optimizations that use control variates to account for the 'drift' in model updates. Instead of training a single global model, some efforts enforce guided client selection (Lai et al., 2021), train a mixture of models (Shi et al., 2021; Fallah et al., 2020), or cluster clients over training (Ghosh et al., 2020a); To tackle the scarce and heterogeneous device resource (Lai et al., 2020), FedAvg (McMahan et al., 2017) reduces communication cost by performing multiple local SGD steps, while some works compress the model update by filtering out or quantizing unimportant parameters (Rothchild et al., 2020; Karimireddy et al., 2019); After realizing the privacy risk in FL (Geiping et al., 2020; Wang et al., 2020), DP-SGD (Geyer et al., 2017) enhances the privacy by employing differential privacy, and DP-FTRL (Kairouz et al., 2021a) applies the tree aggregation to add noise to the sum of mini-batch gradients. These FL efforts often navigate accuracy-computation-privacy trade-offs. As such, a realistic FL setting is crucial for comprehensive evaluations.\"}"}
{"id": "lai22a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"**FedScale: Benchmarking Model and System Performance of Federated Learning at Scale**\\n\\n| Category     | Name          | Data Type | #Clients | #Instances | Example Task       |\\n|--------------|---------------|-----------|----------|------------|-------------------|\\n| CV           | OpenImage     | Image     | 13,771   | 1.3M       | Classification, Object detection |\\n|              | Google Landmark| Image     | 43,484   | 3.6M       | Classification    |\\n|              | Charades      | Video     | 266      | 10K        | Action recognition |\\n| VLOG         | VLOG          | Video     | 4,900    | 9.6K       | Classification, Object detection |\\n|              | Waymo Motion  | Video     | 496,358  | 32.5M      | Motion prediction |\\n| NLP          | Europarl      | Text      | 27,835   | 1.2M       | Text translation  |\\n|              | Reddit        | Text      | 1,660,820| 351M       | Word prediction   |\\n|              | LibriTTS      | Text      | 2,456    | 37K        | Text to speech    |\\n|              | Google Speech | Audio     | 2,618    | 105K       | Speech recognition |\\n|              | Common Voice  | Audio     | 12,976   | 1.1M       | Speech recognition |\\n| Misc ML      | Taobao        | Text      | 182,806  | 20.9M      | Recommendation    |\\n|              | Puffer Streaming| Text     | 121,551  | 15.4M      | Sequence prediction |\\n|              | Fox Go        | Text      | 150,333  | 4.9M       | Reinforcement learning |\\n\\nTable 2. Statistics of partial FedScale datasets (the full list with more details is available in Appendix B). Currently, FedScale has 20 real-world federated datasets; each dataset is partitioned by its real client-data mapping, and we have removed sensitive information in these datasets.\\n\\nExisting FL benchmarks can be misleading. Existing benchmarks often lack realistic client statistical and system behavior datasets and/or fail to reproduce large-scale FL deployments. As a result, they are not only insufficient for benchmarking diverse FL optimizations but can even mislead performance evaluations. For example, (1) As shown in Figure 2(a), statistical performance becomes worse when encountering realistic client behavior (e.g., training failures and availability dynamics), which indicates existing benchmarks that do not have systems traces can produce overly optimistic statistical performance; (2) FL training with hundreds of participants each round performs better than that with tens of participants (Figure 2(b)). As such, existing benchmark platforms can under-report FL optimizations as they cannot support the practical FL scale with a large number of participants.\\n\\n### 3. FedScale Dataset: Realistic FL Workloads\\n\\nWe next introduce how we curate realistic datasets in FedScale to fulfill the desired properties of FL datasets.\\n\\n#### 3.1. Client Statistical Dataset\\n\\nFedScale currently has 20 realistic FL datasets (Table 2) across diverse practical FL scenarios and disciplines. For example, Puffer dataset (Yan et al., 2020) is from FL video streaming deployed to edge users over the Internet. The raw data of FedScale datasets are collected from different sources and stored in various formats. We clean up the raw data, partition them into new FL datasets, streamline new datasets into consistent formats, and categorize them into different FL use cases. Moreover, FedScale provides standardized APIs, a Python package, for the user to easily leverage these datasets (e.g., using different distributions of the same data or new datasets) in other frameworks.\\n\\nRealistic data and partitions\\n\\nWe target realistic datasets with client information, and partition the raw dataset using the unique client identification. For example, OpenImage is a vision dataset collected by Flickr, wherein different mobile users upload their images to the cloud for public use. We use the **AuthorProfileUrl** attribute of the OpenImage data to map data instances to each client, whereby we extract the realistic distribution of the raw data. Following the practical FL deployments (Yang et al., 2018), we assign the clients of each dataset into the training, validation and testing groups, to get its training, validation and testing set. Here, we pick four real-world datasets \u2013 video (Charades), audio (Google Speech), image (OpenImage), and text (Reddit) \u2013 to illustrate practical FL characteristics. Each dataset consists of hundreds or up to millions of clients and millions of data points. Figure 3 reports the **Probability Density Function** (PDF) of the data distribution, wherein we see a high statistical deviation (e.g., wide distribution of the density) across clients not only in the quantity of samples (Figure 3(a)) but also in the data distribution (Figure 3(b)).\\n\\nWe notice that realistic datasets mostly have unique Non-IID patterns, implying the impracticality of existing artificial FL partitions.\\n\\n1 We report the pairwise Jensen\u2013Shannon distance of the label distribution between two clients.\"}"}
{"id": "lai22a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FedScale: Benchmarking Model and System Performance of Federated Learning at Scale\\n\\nFigure 4. Heterogeneous client system speed.\\n\\n(a) Compute capacity.\\n\\n(b) Network capacity.\\n\\nFigure 5. Client availability is dynamic.\\n\\nDifferent scales across diverse task categories\\nTo accommodate diverse scenarios in practical FL, FedScale includes small-, medium-, and large-scale datasets across a wide range of tasks, from hundreds to millions of clients. Some datasets can be applied in different tasks, as we enrich their use case by deriving different metadata from the same raw data. For example, the raw OpenImage dataset can be used for object detection, and we extract each object therein and generate a new dataset for image classification. Moreover, we provide APIs for the developer to customize their dataset (e.g., enforcing new data distribution or taking a subset of clients for evaluations with a smaller scale).\\n\\n3.2. Client System Behavior Dataset\\n\\nClient device system speed is heterogeneous\\nWe formulate the system trace of different clients using AI Benchmark (Ignatov et al.) and MobiPerf Measurements (mob) on mobiles. AI Benchmark provides the training and inference speed of diverse models (e.g., MobileNet) across a wide range of device models (e.g., Huawei P40 and Samsung Galaxy S20), while MobiPerf has collected the available cloud-to-edge network throughput of over 100k world-wide mobile clients. As specified in real FL deployments (Bonawitz et al., 2019; Yang et al., 2018), we focus on mobile devices that have larger than 2GB RAM and connect with WiFi; Figure 4 reports that their compute and network capacity can exhibit order-of-magnitude difference. As such, how to orchestrate scarce resources and mitigate stragglers are paramount.\\n\\nClient device availability is dynamic\\nWe incorporate a large-scale user behavior dataset spanning across 136k users (Yang et al., 2021) to emulate the behaviors of clients. It includes 180 million trace items of client devices (e.g., battery charge or screen lock) over a week. We follow the real FL setting, which considers the device in charging to be available (Bonawitz et al., 2019) and observe great dynamics in their availability: (i) the number of available clients reports diurnal variation (Figure 5(a)). This confirms the cyclic patterns in the client data, which can deteriorate the statistical performance of FL (Eichner et al., 2019). (ii) the duration of each available slot is not long-lasting (Figure 5(b)). This highlights the need of handling failures (e.g., clients become offline) during training, as the round duration (also a few minutes) is comparable to that of each available slot. This, however, is largely overlooked in the literature.\\n\\n4. FedScale Runtime: Evaluation Platform\\n\\nExisting FL evaluation platforms are poor at reproducing practical, large-scale FL deployment scenarios. Worse, they often lack user-friendly APIs and require significant developer efforts to introduce new plugins. We introduce, FedScale Runtime, an automated, extensible, and easily-deployable evaluation platform equipped with mobile and cluster backends, to simplify and standardize FL evaluation under realistic settings.\\n\\n4.1. FedScale Runtime: Mobile Backend\\n\\nFedScale Runtime deploys a mobile backend to enable on-device FL evaluation on smartphones. The first principle in building our mobile backend is to minimize any engineering effort for the developer (e.g., without reinventing their Python code) to benchmark FL on mobiles. To this end, FedScale mobile backend (Singapuram et al., 2022) is built atop the Termux app (ter), an Android terminal that supports Linux environment.\\n\\n```python\\nfrom fedscale.core.client import Client\\nclass Mobile_Client(Client):\\n    def train(self, client_data, model, conf):\\n        for local_step in range(conf.local_steps):\\n            optimizer.zero_grad()\\n            ...\\n            loss.backward()\\n            optimizer.step()\\n        # Results will be sent to cloud aggregator via gRPC\\n        return gradient_update\\n```\\n\\nFigure 6. Training on mobile client.\"}"}
{"id": "lai22a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FedScale: Benchmarking Model and System Performance of Federated Learning at Scale\\n\\n| Category | Name          | Data Type | #Clients  | #Instances  | Example Task                   |\\n|----------|---------------|-----------|-----------|-------------|--------------------------------|\\n| CV       | iNature       | Image     | 2,295     | 193K        | Classification                  |\\n|          | FEMNIST       | Image     | 3,400     | 640K        | Classification                  |\\n|          | OpenImage     | Image     | 13,771    | 1.3M        | Classification, Object detection|\\n|          | Google Landmark| Image     | 43,484    | 3.6M        | Classification                  |\\n|          | Charades      | Video     | 266       | 10K         | Action recognition              |\\n|          | VLOG          | Video     | 4,900     | 9.6K        | Classification, Object detection|\\n|          | Waymo Motion  | Video     | 496,358   | 32.5M       | Motion prediction               |\\n| NLP      | Europarl      | Text      | 27,835    | 1.2M        | Text translation                |\\n|          | Blog Corpus   | Text      | 19,320    | 137M        | Word prediction                 |\\n|          | Stackoverflow | Text      | 342,477   | 135M        | Word prediction, Classification  |\\n|          | Reddit        | Text      | 1,660,820 | 351M        | Word prediction                 |\\n|          | Amazon Review | Text      | 1,822,925 | 166M        | Classification, Word prediction  |\\n|          | CoQA          | Text      | 7,685     | 116K        | Question Answering              |\\n|          | LibriTTS      | Text      | 2,456     | 37K         | Text to speech                  |\\n|          | Google Speech | Audio     | 2,618     | 105K        | Speech recognition              |\\n|          | Common Voice  | Audio     | 12,976    | 1.1M        | Speech recognition              |\\n| Misc ML  | Taxi Trajectory| Text      | 442       | 1.7M        | Sequence prediction             |\\n|          | Taobao        | Text      | 182,806   | 20.9M       | Recommendation                  |\\n|          | Puffer Streaming| Text      | 121,551   | 15.4M       | Sequence prediction             |\\n|          | Fox Go        | Text      | 150,333   | 4.9M        | Reinforcement learning          |\\n\\nTable 5. Statistics of FedScale datasets. FedScale has 20 realistic client datasets, which are from the real-world collection, and we partitioned each dataset using its real client-data mapping. System behaviors in practical FL, FedScale incorporates real-world traces of mobile devices, associates each client with his system speeds, as well as the availability. Moreover, we develop a more efficient evaluation platform, FedScale Runtime, to automate FL benchmarking.\\n\\nScalability\\n\\nExisting frameworks, perhaps due to the heavy burden of building complicated system support, largely rely on the traditional ML architectures (e.g., the primitive parameter-server architecture of Pytorch). These architectures are primarily designed for the traditional large-batch training on a number of workers, and each worker often trains a single batch at a time. However, this is ill-suited to the simulation of thousands of clients concurrently: (1) they lack tailored system implementations to orchestrate the synchronization and resource scheduling, for which they can easily run into synchronization/memory issues and crash down; (2) their resource can be under-utilized, as FL evaluations often use a much smaller batch size.\\n\\nTackling all these inefficiencies requires domain-specific system designs. Specifically, we built a cluster resource scheduler that monitors the fine-grained resource utilization of machines, queues the overcommitted simulation requests, adaptively dispatches simulation requests of the client across machines to achieve load balance, and then orchestrates the simulation based on the client virtual clock. Moreover, given a much smaller batch size in FL, we maximize the resource utilization by overlapping the communication and computation phrases of different client simulations. The former and the latter make FedScale more scalable across machines and on single machines, respectively.\\n\\nEmpirically, we have run the 20-GPU set up on different datasets and models in Figure 10 and Table 6, and are aware of at least one group who ran FedScale Runtime with more than 60 GPUs (Lai et al., 2021).\\n\\nTable 6. FedScale is more scalable and faster. Image classification on iNature dataset using MobileNet-V2 on 20-GPU setting.\\n\\n| Eval. Duration/Round vs. # of Clients/Round | FedScale | FedML | Flower |\\n|--------------------------------------------|----------|-------|--------|\\n| 10K                                        | 0.03 min | 0.58 min | fail to run |\\n| 1K                                         | 0.16 min | 4.4 min | fail to run |\\n| 10K                                        | 1.14 min | fail to run | fail to run |\\n| 10K                                        | 10.9 min | fail to run | fail to run |\\n\\nModularity\\n\\nAs shown in Table 1, some existing frameworks (e.g., LEAF and FedEval) do not provide user-friendly modularity, which requires great engineering efforts to benchmark different components, and we recognize that FedML and Flower provide the API modularity too. On the other hand, FedScale Runtime's modularity for easy deployments and broader use cases is not limited to APIs (Figure 8): (1) FedScale Runtime Data Loader: it simplifies...\"}"}
{"id": "lai22a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FedScale: Benchmarking Model and System Performance of Federated Learning at Scale\\n\\n```python\\nfrom fedscale.core.client_manager import ClientManager\\nimport Oort\\nclass Customized_ClientManager(ClientManager):\\n    def __init__(self, *args):\\n        super().__init__(*args)\\n        self.oort_selector = Oort.create_training_selector(*args)\\n\\n    # Replace default client selection algorithm with Oort\\n    def select_participants(self, numOfClients, cur_time, feedbacks):\\n        # Feed Oort with execution feedbacks\\n        self.oort_selector.update_client_info(feedbacks)\\n        selected_clients = self.oort_selector.select_participants(numOfClients, cur_time)\\n        return selected_clients\\n```\\n\\nFigure 17. Evaluate client selection algorithm (Lai et al., 2021).\\n\\n```python\\nfrom fedscale.core.client import Client\\nclass Customized_Client(Client):\\n    # Customize the training on each client\\n    def train(self, client_data, model, conf):\\n        training_result = super().train(client_data, model, conf)\\n        # Implementation of compression\\n        compressed_result = compress_impl(training_result)\\n        return compressed_result\\n```\\n\\nFigure 18. Evaluate model compression (Rothchild et al., 2020).\\n\\n```python\\nfrom fedscale.core.client import Client\\nclass Customized_Client(Client):\\n    # Customize the training on each client\\n    def train(self, client_data, model, conf):\\n        training_result = super().train(client_data, model, conf)\\n        # Clip updates and add noise\\n        secure_result = secure_impl(training_result)\\n        return secure_result\\n```\\n\\nFigure 19. Evaluate security enhancement (Sun et al., 2019).\\n\\nand expands the use of realistic datasets. e.g., developers can load and analyze the realistic FL data to motivate new algorithm designs, or import new datasets/customize data distributions in FedScale evaluations; (2) Client simulator: it emulates the system behaviors of FL clients, and developers can customize their system traces in evaluating the FL system efficiency too; (3) Resource manager: it hides the system complexity in training large-scale participants.\\n\\nD. Examples of New Plugins\\n\\nIn this section, we demonstrate examples to show the ease of integrating today's FL efforts in FedScale evaluations.\\n\\nAt its core, FedScale Runtime provides flexible APIs on each module so that the developer can access and customize\\n\\n```python\\nimport flwr as fl\\ndef get_config_fn():\\n    # Implementation of randomly selecting client ids\\n    client_ids = random_selection()\\n    config = {\"ids\": client_ids}\\n    return config\\n\\n# Customized Strategy\\nstrategy = CustomizedStrategy(on_fit_config_fn=get_config_fn())\\nfl.server.start_server(config={\"num_rounds\": args.round}, strategy=strategy)\\n```\\n\\n```python\\nimport flwr as fl\\nclass Customized_Client():\\ndef fit(self, config, net):\\n    # Customization of client data\\n    trainloader = select_dataset(config['ids'][args.partition])\\n    train(net, trainloader)\\n    compressed_result = self.get_parameters()\\n    # Implementation of compression\\n    compressed_result = compress_impl(training_result)\\n    return compressed_result\\nfl.client.start_numpy_client(args.address, client=CustomizedClient())\\n```\\n\\nFigure 20. Evaluate model compression with Flower (Beutel et al., 2021). The developer needs to implement the functions in grey by his own. Note that each function can take tens of lines of code.\\n\\nmethods of the base class. Table 3 illustrates some example APIs that can facilitate diverse FL efforts. Note that FedScale Runtime will automatically integrate new plugins into evaluations, and then produce practical FL metrics. Figure 17 demonstrates that we can easily evaluate new client selection algorithms, Oort (Lai et al., 2021), by modifying a few lines of the clientManager module.\\n\\nSimilarly, Figure 18 and Figure 19 show that we can extend the basic Client module to apply new gradient compression (Rothchild et al., 2020) and enhancement for malicious attack (Sun et al., 2019), respectively.\\n\\nComparison with other work\\n\\nFigure 20 shows the same evaluation of gradient compression (Rothchild et al., 2020) as that in Figure 18 using flower (Beutel et al., 2021), which requires much more human effort than Figure 18. The gray components in figure 20 require implementation. Flower falls short in providing APIs for passing metadata between client and server, for example client id, which makes server and running workers client-agnostic. To customize anything for clients during FL training, developers have to go through the source code and override many components to share client metadata between server and workers.\"}"}
{"id": "lai22a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FedScale: Benchmarking Model and System Performance of Federated Learning at Scale\\n\\nFigure 13. FedScale can benchmark privacy efforts in more realistic FL settings.\\n\\nFigure 14. FedScale can benchmark security optimizations with realistic FL data.\\n\\nFigure 15. System stragglers slow down practical FL greatly.\\n\\nFigure 16. Biased accuracy distributions of the trained model across clients (ShuffleNet on OpenImage).\\n\\nCo-optimizations of statistical and system efficiency\\n\\nMost of today's FL efforts focus on either optimizing the statistical or the system efficiency, whereas we observe a great opportunity to jointly optimize both efficiencies: (1) As the system behavior determines the availability of client data, predictable system performance can benefit statistical efficiency. For example, in alleviating the biased model accuracy (Figure 16), we may prioritize the use of upcoming offline clients to curb the upcoming distribution drift of client data; (2) Statistical optimizations should be aware of the heterogeneous client system speed. For example, instead of applying one-fit-all strategies (e.g., local steps or gradient compression) for all clients, faster workers can trade more system latency for more statistical benefits (e.g., transferring more traffics with less intensive compression).\\n\\nFL design-decisions considering mobile environment\\n\\nExisting efforts have largely overlooked the interplay of client devices and training speed (e.g., using a large local steps to save communication (McMahan et al., 2017)), however, as shown in Figure 7, running intensive on-device computation for a long time can quickly drain the battery, or even burn the device, leading to the unavailability of clients. Therefore, we believe that a power and temperature-aware training algorithm (e.g., different local steps across clients or device-aware NAS) can be an important open problem.\\n\\n6. Conclusion\\n\\nTo enable scalable and reproducible FL research, we introduce FedScale, a diverse set of realistic FL datasets in terms of scales, task categories, and client system behaviors, along with a scalable and extensible evaluation platform, FedScale Runtime. FedScale Runtime performs fast-forward evaluation of the practical FL runtime metrics needed in today's works. More importantly, FedScale Runtime provides ready-to-use realistic datasets and flexible APIs to allow more FL applications, such as benchmarking NAS, model inference, and a broader view of federated computation (e.g., multi-party computation). FedScale is open-source at http://fedscale.ai, and we hereby invite the community to develop and contribute state-of-the-art FL efforts.\\n\\nAcknowledgments\\n\\nWe would like to thank the anonymous reviewers and SymbioticLab members for their insightful feedback. We also thank FedScale contributors and users from many different academic institutions and industry for their valuable inputs. This work was supported in part by NSF grants CNS-1900665, CNS-1909067, and CNS-2106184, and a grant from Cisco.\\n\\nReferences\\n\\nCommon Voice Data. https://commonvoice.mozilla.org/en/datasets.\\nFox go dataset. https://github.com/featurecat/go-dataset.\\niNaturalist 2019. https://sites.google.com/view/fgvc6/competitions/inaturalist-2019.\\nMobiPerf. https://www.measurementlab.net/tests/mobiperf/.\\nGoogle Open Images Dataset. https://storage.googleapis.com/openimages/web/index.html.\\nPySyft. https://github.com/OpenMined/PySyft.\\nReddit Comment Data. https://files.pushshift.io/reddit/comments/.\"}"}
{"id": "lai22a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FedScale: Benchmarking Model and System Performance of Federated Learning at Scale\\n\\nStack Overflow Data. https://cloud.google.com/bigquery/public-data/stackoverflow.\\n\\nTaobao Dataset. https://tianchi.aliyun.com/dataset/dataDetail?dataId=56&lang=en-us.\\n\\nTermux. https://termux.com/.\\n\\nTensorFlow Federated. https://www.tensorflow.org/federated.\\n\\nBeutel, D. J., Topal, T., Mathur, A., Qiu, X., Parcollet, T., de Gusmao, P. P. B., and Lane, N. D. Flower: A friendly federated learning framework. arXiv preprint arXiv:2007.14390, 2021.\\n\\nBonawitz, K., Eichner, H., and et al. Towards federated learning at scale: System design. In MLSys, 2019.\\n\\nCaldas, S., Meher, S., Duddu, K., and et al. Leaf: A benchmark for federated settings. NeurIPS\u2019 Workshop, 2019.\\n\\nChai, D., Wang, L., Chen, K., and Yang, Q. FedEval: A benchmark system with a comprehensive evaluation model for federated learning. In arxiv.org/abs/2011.09655, 2020.\\n\\nCohen, G., Afshar, S., Tapson, J., and van Schaik, A. EMNIST: an extension of MNIST to handwritten letters. In arxiv.org/abs/1702.05373, 2017.\\n\\nEichner, H., Koren, T., McMahan, H. B., Srebro, N., and Talwar, K. Semi-cyclic stochastic gradient descent. In ICML, 2019.\\n\\nEttinger, S., Cheng, S., Caine, B., Liu, C., Zhao, H., Pradhan, S., Chai, Y., Sapp, B., Qi, C., Zhou, Y., Yang, Z., Chouard, A., Sun, P., Ngiam, J., Vasudevan, V., McCauley, A., Shlens, J., and Anguelov, D. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. CoRR, abs/2104.10133, 2021.\\n\\nFallah, A., Mokhtari, A., and Ozdaglar, A. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. In NeurIPS, 2020.\\n\\nFouhey, D. F., Kuo, W., Efros, A. A., and Malik, J. From lifestyle vlogs to everyday interactions. In CVPR, 2018.\\n\\nGeiping, J., Bauermeister, H., Dr\u00f6ge, H., and Moeller, M. Inverting gradients - how easy is it to break privacy in federated learning? In NeurIPS, 2020.\\n\\nGeyer, R. C., Klein, T., and Nabi, M. Differentially private federated learning: A client level perspective. In NeurIPS, 2017.\\n\\nGhosh, A., Chung, J., Yin, D., and Ramchandran, K. An efficient framework for clustered federated learning. In NeurIPS, 2020a.\\n\\nGhosh, A., Chung, J., Yin, D., and Ramchandran, K. An efficient framework for clustered federated learning. In NeurIPS, 2020b.\\n\\nHe, C., Li, S., So, J., and Zeng, X. FedML: A research library and benchmark for federated machine learning. In arxiv.org/abs/2007.13518, 2020.\\n\\nHe, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S. Amc: Automl for model compression and acceleration on mobile devices. In ECCV, 2018.\\n\\nIgnatov, A., Timofte, R., Kulik, A., Yang, S., Wang, K., Baum, F., Wu, M., Xu, L., and Gool, L. V. AI benchmark: All about deep learning on smartphones in 2019. arXiv preprint arXiv:1910.06663.\\n\\nKairouz, P., McMahan, B., Song, S., Thakkar, O., Thakurta, A., and Xu, Z. Practical and private (deep) learning without sampling or shuffling. In arxiv.org/abs/2103.00039, 2021a.\\n\\nKairouz, P., McMahan, H. B., and et al. Advances and open problems in federated learning. In Foundations and Trends\u00ae in Machine Learning, 2021b.\\n\\nKarimireddy, S. P., Rebjock, Q., Stich, S. U., and Jaggi, M. Error feedback fixes signsgd and other gradient compression schemes. In arXiv preprint arXiv:1901.09847, 2019.\\n\\nKarimireddy, S. P., Kale, S., Mohri, M., Reddi, S. J., Stich, S. U., and Suresh, A. T. SCAFFOLD: Stochastic controlled averaging for federated learning. In ICML, 2020.\\n\\nKoehn, P. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the tenth Machine Translation Summit, 2005.\\n\\nLai, F., Chowdhury, M., and Madhyastha, H. To relay or not to relay for inter-cloud transfers? In HotCloud, 2018.\\n\\nLai, F., You, J., Zhu, X., Madhyastha, H. V., and Chowdhury, M. Sol: A federated execution engine for fast distributed computation over slow networks. In NSDI, 2020.\\n\\nLai, F., Zhu, X., Madhyastha, H. V., and Chowdhury, M. Oort: Efficient federated learning via guided participant selection. In OSDI, 2021.\\n\\nLi, C., Zeng, X., Zhang, M., and Cao, Z. PyramidFL: Fine-grained data and system heterogeneity-aware client selection for efficient federated learning. In MobiCom, 2022.\"}"}
{"id": "lai22a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FedScale: Benchmarking Model and System Performance of Federated Learning at Scale\\n\\nLi, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V. Federated optimization in heterogeneous networks. In MLSys, 2020.\\n\\nMao, H., Netravali, R., and Alizadeh, M. Neural adaptive video streaming with Pensieve. In SIGCOMM, 2017.\\n\\nMattson, P., Cheng, C., Coleman, C., and et al. Mlperf training benchmark. In MLSys, 2020.\\n\\nMcAuley, J., Targett, C., Shi, Q., and van den Hengel, A. Image-based recommendations on styles and substitutes. In SIGIR, 2015.\\n\\nMcMahan, H. B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. Communication-efficient learning of deep networks from decentralized data. In AISTATS, 2017.\\n\\nReddi, S., Charles, Z., and et al. Adaptive federated optimization. In arxiv.org/abs/2003.00295, 2020.\\n\\nReddy, S., Chen, D., and Manning, C. D. Coqa: A conversational question answering challenge. arXiv preprint arXiv:1808.07042, 2019.\\n\\nRo, J. H., Suresh, A. T., and Wu, K. FedJAX: Federated learning simulation with jax. arXiv preprint arXiv:2108.02117, 2021.\\n\\nRothchild, D., Panda, A., Ullah, E., Ivkin, N., Stoica, I., Braverman, V., Gonzalez, J., and Arora, R. Fetchsgd: Communication-efficient federated learning with sketching. In ICML, 2020.\\n\\nSandler, M., Howard, A. G., Zhu, M., Zhmoginov, A., and Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.\\n\\nSchler, J., Koppel, M., Argamon, S., and Pennebaker, J. Effects of age and gender on blogging. In Proceedings of AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs, 2006.\\n\\nShi, N., Lai, F., Kontar, R. A., and Chowdhury, M. Fed-ensemble: Improving generalization through model ensembling in federated learning. arXiv preprint arXiv:2107.10663, 2021.\\n\\nSigurdsson, G. A., Varol, G., Wang, X., Farhadi, A., Laptev, I., and Gupta, A. Hollywood in homes: Crowdsourcing data collection for activity understanding. In ECCV, 2016.\\n\\nSingapuram, S. S. V., Lai, F., Hu, C., and Chowdhury, M. Swan: A neural engine for efficient dnn training on smartphone socs. arXiv preprint arXiv:2206.04687, 2022.\\n\\nSun, Z., Kairouz, P., Suresh, A. T., and McMahan, H. B. Can you really backdoor federated learning? In arxiv.org/abs/1911.07963, 2019.\\n\\nWang, H., Sreenivasan, K., Rajput, S., Vishwakarma, H., Agarwal, S., yong Sohn, J., Lee, K., and Papailiopoulos, D. Attack of the tails: Yes, you really can backdoor federated learning. In NeurIPS, 2020.\\n\\nWarden, P. Speech commands: A dataset for limited-vocabulary speech recognition. In arxiv.org/abs/1804.03209, 2018.\\n\\nWeyand, T., Araujo, A., Cao, B., and Sim, J. Google landmarks dataset v2 a large-scale benchmark for instance-level recognition and retrieval. In arxiv.org/abs/2004.01804, 2020.\\n\\nYan, F. Y., Ayers, H., and et al. Learning in situ: a randomized experiment in video streaming. In NSDI, 2020.\\n\\nYang, C., Wang, Q., and et al. Characterizing impacts of heterogeneity in federated learning upon large-scale smartphone data. In WWW, 2021.\\n\\nYang, T., Andrew, G., Eichner, H., Sun, H., Li, W., Kong, N., Ramage, D., and Beaufays, F. Applied federated learning: Improving Google keyboard query suggestions. In arxiv.org/abs/1812.02903, 2018.\\n\\nYu, P. and Chowdhury, M. Fine-grained gpu sharing primitives for deep learning applications. In MLSys, 2020.\\n\\nZen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. Libritts: A corpus derived from librispeech for text-to-speech. arXiv preprint arXiv:1904.02882, 2019.\"}"}
{"id": "lai22a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Experiment Setup\\nScalability Evaluations\\nWe evaluate the scalability of FedScale Runtime, FedML (GitHub commit@2ee0517) and Flower (v0.17.0 atop Ray v1.9.2) using a cluster with 10 GPU nodes. Each GPU node has a P100 GPU with 12GB GPU memory and 192GB RAM. We train the ShuffleNet-V2 model on the OpenImage dataset. We set the minibatch size of each participant to 32, and the number of local steps $K$ to 20, which takes around 2800MB GPU memory for each model training. As such, we allow each GPU node to run 4 processes in benchmarking these three frameworks.\\n\\nEvaluation Setup\\nApplications and models used in our evaluations are widely used on mobile devices. We set the minibatch size of each participant to 32, and the number of local steps $K$ to 20. We cherry-pick the hyper-parameters with grid search, ending up with an initial learning rate 0.04 for CV tasks and 5e-5 for NLP tasks. The learning rate decays by 0.98 every 10 training rounds. These settings are consistent with the literature. More details about the input dataset are available in Appendix B.\\n\\nB. Introduction of FedScale Datasets\\nFedScale currently has 20 realistic federated datasets across a wide range of scales and task categories (Table 5). Here, we provide the description of some representative datasets.\\n\\nGoogle Speech Commands. A speech recognition dataset (Warden, 2018) with over ten thousand clips of one-second-long duration. Each clip contains one of the 35 common words (e.g., digits zero to nine, \\\"Yes\\\", \\\"No\\\", \\\"Up\\\", \\\"Down\\\") spoken by thousands of different people.\\n\\nOpenImage. OpenImage (ope) is a vision dataset collected from Flickr, an image and video hosting service. It contains a total of 16M bounding boxes for 600 object classes (e.g., Microwave oven). We clean up the dataset according to the provided indices of clients.\\n\\nReddit and StackOverflow. Reddit (red) (StackOverflow (sta)) consists of comments from the Reddit (StackOverflow) website. It has been widely used for language modeling tasks, and we consider each user as a client. In this dataset, we restrict to the 30k most frequently used words, and represent each sentence as a sequence of indices corresponding to these 30k frequently used words.\\n\\nVLOG. VLOG (Fouhey et al., 2018) is a video dataset collected from YouTube. It contains more than 10k Lifestyle Vlogs, videos that people purportedly record to show their lives, from more than 4k actors. This dataset is aimed at understanding everyday human interaction and contains labels for scene classification, hand-state prediction, and hand detection tasks.\\n\\nLibriTTS. LibriTTS (Zen et al., 2019) is a large-scale text-to-speech dataset. It is derived from audiobooks in LibriVox project. There are 585 hours of read English speech from 2456 speakers at a 24kHz sampling rate.\\n\\nTaobao. Taobao Dataset (tao) is a dataset of click rate prediction about display Ad, which is displayed on the website of Taobao. It is composed of 1,140,000 users ad display/click logs for 8 days, which are randomly sampled from the website of Taobao.\\n\\nWaymo Motion. Waymo Motion (Ettinger et al., 2021) is composed of 103,354 segments each containing 20 seconds of object tracks at 10Hz and map data for the area covered by the segment. These segments are further broken into 9 second scenarios, and we consider each scenario as a client.\\n\\nPuffer Streaming. Puffer is a Stanford University research study about using machine learning (e.g., reinforcement learning (Mao et al., 2017)) to improve video-streaming algorithms: the kind of algorithms used by services such as YouTube, Netflix, and Twitch. Puffer dataset (Yan et al., 2020) consists of 15.4M sequences of network throughput on edge clients over time.\\n\\nC. Comparison with Existing FL Benchmarks\\nIn this section, we compare FedScale with existing FL benchmarks in more details.\\n\\nData Heterogeneity\\nExisting benchmarks for FL are mostly limited in the variety of realistic datasets for real-world FL applications. Even they have various datasets (e.g., LEAF (Caldas et al., 2019)) and FedEval (Chai et al., 2020)), their datasets are mostly synthetically derived from conventional datasets (e.g., CIFAR) and limited to quite a few FL tasks. These statistical client datasets can not represent realistic characteristics and are inefficient to benchmark various real FL applications. Instead, FedScale provides 20 comprehensive realistic datasets for a wide variety of tasks and across small, medium, and large scales, which can also be used in data analysis to motivate more FL designs.\\n\\nSystem Heterogeneity\\nThe practical FL statistical performance also depends on the system heterogeneity (e.g., internet bandwidth (Lai et al., 2018) and client availability), which has inspired lots of optimizations for FL system efficiency. However, existing FL benchmarks have largely overlooked the system behaviors of FL clients, which can produce misleading evaluations, and discourage the benchmarking of system efforts. To emulate the heterogeneous...\"}"}
{"id": "lai22a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FedScale: Benchmarking Model and System Performance of Federated Learning at Scale\\n\\nFigure 7. FedScale Runtime can benchmark the mobile runtime of power, energy and latency. We train Resnet34 and Shufflenet on ImageNet and CIFAR-10 on Xiaomi Mi10 and Samsung S10e. Of Python script (e.g., PyTorch) built from source on the mobile device; the full-operator set (e.g., PyTorch modules) is available too. This speeds up the deployment cycle: FL models and algorithms that were prototyped on server GPUs/CPUs can also be deployed using FedScale Runtime.\\n\\nWe are currently implementing the Google Remote Procedure Call (gRPC) for distributed mobile devices to interface with FedScale Runtime cloud server.\\n\\nBenchmarking the mobile backend\\nFedScale mobile backend enables developers to benchmark realistic FL training/testing performance on mobile phones. For example, Figure 7 reports the performance metrics of training Shufflenet and ResNet34 on one mini-batch (batch size 32), drawn from the ImageNet and CIFAR-10 datasets, on Xiaomi Mi10 and Samsung S10e Android devices. We benchmark the average training time. We notice that ResNet34 runs at higher instantaneous power than ShuffleNet on both devices, but it requires less total energy to train since it takes shorter latency. ImageNet takes longer than CIFAR-10 per mini-batch, as the larger training image sizes lead to longer execution. The heterogeneity in computational capacity is evident as the Xiaomi Mi10 device outperforms the Samsung S10e device due to a more capable processor. As such, we believe that FedScale mobile backend can facilitate future on-device FL optimizations (e.g., hardware-aware neural architecture search (He et al., 2018)).\\n\\n4.2. FedScale Runtime: Cluster Backend\\nFedScale Runtime provides an automated cluster backend that can support FL evaluations in real deployments and in-cluster simulations. In the deployment mode, FedScale Runtime acts as the cloud aggregator and orchestrates FL executions across real devices (e.g., laptops, mobiles, or even cloud servers). To enable cost-efficient FL benchmarking, FedScale Runtime also includes a simulation mode that per-\"}"}
{"id": "lai22a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FedScale: Benchmarking Model and System Performance of Federated Learning at Scale\\n\\n| Module | API Name | Example Use Case |\\n|--------|----------|------------------|\\n| Aggregator | round_completion_handler | Adaptive/secure aggregation |\\n| Simulator | client_completion_handler | Straggler mitigation |\\n| Client | select_participants | Client selection |\\n| Manager | select_model_for_client | Adaptive model selection |\\n\\nClient | train | Local SGD/malicious attack |\\n\\nSimulator | serialize_results | Model compression |\\n\\nTable 3. Some example APIs. FedScale provides APIs to deploy new plugins for various designs. We omit input arguments for brevity here.\\n\\nfrom fedscale.core.client import Client\\n\\nclass Customized_Client(Client):\\n    # Redefine training (e.g., for local SGD/gradient compression)\\n    def train(self, client_data, model, conf):\\n        # Code of plugin\\n        ...\\n        # Results will be serialized, and then sent to aggregator\\n        return training_result\\n\\nFigure 9. Add plugins by inheritance.\\n\\nclient drops out), indicated by the availability trace.\\n\\n\u2022 Resource Manager: It orchestrates the available physical resource for evaluation to maximize the resource utilization. For example, when the number of participants/round exceeds the resource capacity (e.g., simulating thousands of clients on a few GPUs), the resource manager queues the overcommitted tasks of clients and schedules new client simulation from this queue once resource becomes available. Note that this queuing will not affect the simulated FL runtime, as this runtime is controlled by a global virtual clock, and the event monitor will manage events in the correct runtime order.\\n\\nNote that capturing runtime performance (e.g., wall clock time) is rather slow and expensive in practical FL \u2013 each mobile device takes several minutes to train a round \u2013 but our simulator enables fast-forward simulation, as training on CPUs/GPUs takes only a few seconds per round, while providing simulated runtime using realistic traces.\\n\\nFedScale Runtime enables automated FL simulation\\nFedScale Runtime incorporates realistic FL traces, using the aforementioned trace by default or the developer-specified profile from the mobile backend, to automatically emulate the practical FL workflow:\\n\\n1. Task submission: FL developers specify their configurations (e.g., model and dataset), which can be federated training or testing, and the resource manager will initiate the aggregator and client simulator on available resource (GPU, CPU, other accelerators, or even smartphones);\\n2. FL simulation: Following the standardized FL lifecycle (Figure 1), in each training round, the aggregator inquires the client manager to select participants, whereby the resource manager distributes the client configuration to the available client simulators. After the completion of each client, the client simulator pushes the model update to the aggregator, which then performs the model aggregation;\\n3. Metrics output: During training, the developer can query the practical evaluation metrics on the fly. Figure 8 lists some popular metrics in FedScale.\\n\\nTable 1: Performance comparison among different frameworks.\\n\\n| Framework | Evaluation Duration/Round (min) | # of Clients/Round | Ease of Use |\\n|-----------|--------------------------------|-------------------|-------------|\\n| FedML     | 1.28                           | 10                | Easy to use |\\n| Flower    | 1.18                           | 100               | Easy to use |\\n| FedScale  | 1.12                           | 1000              | Easy to use |\\n|           | 0.6                            | 10000             | Easy to use |\\n\\nLower is better\\n\\nFigure 10. FedScale Runtime can run thousands of clients/round on 10 GPUs where others fail. More results are in Appendix C.\\n\\nFedScale Runtime is easily deployable and extensible with plugins. FedScale Runtime provides flexible APIs, which can accommodate with different execution backends (e.g., PyTorch) by design, for the developer to quickly benchmark new plugins. Table 3 illustrates some example APIs that can facilitate diverse FL efforts, and Figure 9 dictates an example showing how these APIs help to benchmark a new design of local client training with a few lines of code by inheriting the base Client module. Moreover, FedScale Runtime can embrace new realistic (statistical client or system behavior) datasets with the built-in APIs. For example, the developer can import his own dataset of the client availability with the API (load_client_availability), and FedScale Runtime will automatically enforce this trace during evaluations. We provide more examples and a comparison with other frameworks, in Appendix D to show the ease of evaluating various today's FL work in FedScale\u2013 a few lines are all we need (Lai et al., 2021; Li et al., 2022)!\\n\\nFedScale Runtime is scalable and efficient\\nIn the simulation mode, FedScale Runtime can perform large-scale simulations (thousands of clients per round) in both standalone (single CPU/GPU) and distributed (multiple machines) settings. This is because: (1) FedScale Runtime uses GPU sharing techniques (Yu & Chowdhury, 2020) to divide GPU among tasks so that multiple client simulators can co-locate...\"}"}
{"id": "lai22a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FedScale: Benchmarking Model and System Performance of Federated Learning at Scale\\n\\n### Task Dataset Model IID FedAvg FedProx FedYoGi\\n\\n**FEMNIST**\\n- ResNet-18: 86.40% 78.50% 78.40% 76.30%\\n\\n**ShuffleNet-V2**\\n- MobileNet-V2: 81.37% 70.27% 69.54% 74.04%\\n\\n**Image Classification**\\n- OpenImage\\n\\n**Text Classification**\\n- Amazon Review\\n- Logistic Regression: 66.10% 65.80% 65.10% 65.30%\\n\\n**Language Modeling**\\n- Reddit\\n- Albert\\n\\n**Speech Recognition**\\n- Google Speech\\n- ResNet-34: 72.58% 63.37% 63.25% 62.67%\\n\\n---\\n\\nTable 4. Benchmarking of different FL algorithms across realistic FL datasets. We report the mean test accuracy over 5 runs.\\n\\n---\\n\\n(a) Convergence on Google speech.\\n\\n---\\n\\n(b) Convergence on OpenImage.\\n\\n---\\n\\n(c) Final model performance.\\n\\n---\\n\\nFigure 11. FedScale can benchmark the statistical FL performance. (c) shows existing benchmarks can under-report the FedYoGi performance as they cannot support a large number of participants.\\n\\n---\\n\\n5. Experiments\\n\\nIn this section, we show how FedScale can facilitate better benchmarking of FL efforts over its counterparts.\\n\\n**Experimental setup**\\n\\nWe use 10 NVIDIA Tesla P100 GPUs in our evaluations. Following the real FL deployments (Bonawitz et al., 2019; Yang et al., 2018), the aggregator collects updates from the first $N$ completed participants out of $1$. The value of $N$ is chosen to mitigate system stragglers in each round, and we set $N = 100$ by default. We experiment with representative FedScale datasets in different scales and tasks (detailed experiment setup in Appendix A).\\n\\n5.1. How Does FedScale Help FL Benchmarking?\\n\\nExisting benchmarks are insufficient to evaluate the various metrics needed in today's FL. We note that the performance of existing benchmarks and FedScale are quite close in the same settings if we turn off the optional system traces in FedScale. Because the underlying training and FL protocols in evaluations are the same. However, the limited scalability can mislead the practical FL performance. Next, we show the effectiveness of FedScale in benchmarking different FL aspects over its counterparts.\\n\\n**Benchmarking FL statistical efficiency.**\\n\\nFedScale provides various realistic client datasets to benchmark the FL statistical efficiency. Here, we experiment with state-of-the-art optimizations (FedAvg, FedProx, and FedYoGi) \u2013 each aims to mitigate the data heterogeneity \u2013 and the traditional IID data setting. Figure 11 and Table 4 report that: (1) the round-to-accuracy performance and final model accuracy of the non-IID setting is worse than that of the IID setting, which is consistent with existing findings (Kairouz et al., 2021b); (2) different tasks can have different preferences...\"}"}
{"id": "lai22a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FedScale: Benchmarking Model and System Performance of Federated Learning at Scale\\n\\nFigure 12. FedScale can benchmark realistic FL runtime. (a) and (b) report FedYoGi results on OpenImage with different number of local steps (K); (b) reports the FL runtime to reach convergence on the optimizations. For example, FedYoGi performs the best on OpenImage, but it is inferior to FedAvg on Google Speech. With much more FL datasets, FedScale enables extensive studies of the sweet spot of different optimizations; and (3) existing benchmarks can under-report the FL performance due to their inability to reproduce the FL setting. Figure 11(c) reports the final model accuracy using FedML and FedScale, where we attempt to reproduce the scale of practical FL with 100 participants per round in both frameworks, but FedML can only support 30 participants because of its suboptimal scalability, which under-reports the FL performance that the algorithm can indeed achieve.\\n\\nBenchmarking FL system efficiency.\\n\\nExisting system optimizations for FL focus on the practical runtime (e.g., wall-clock time in real FL training) and the FL execution cost. Unfortunately, existing benchmarks can hardly evaluate the FL runtime due to the lack of realistic system traces, but we now show how FedScale can help such benchmarking: (1) FedScale Runtime enables fast-forward evaluations of the practical FL wall-clock time with fewer evaluation hours. Taking different number of local steps $K$ in local SGD as an example (McMahan et al., 2017), Figure 12(a) and Table 12(b) illustrate that FedScale can evaluate this impact of $K$ on practical FL runtime in a few hours. This allows the developer to evaluate large-scale system optimizations efficiently; and (2) FedScale Runtime can dictate the FL execution cost by using realistic system traces. For example, Figure 12(c) reports the practical FL communication cost in achieving the performance of Figure 11, while Figure 15 reports the system duration of individual clients. These system metrics can facilitate developers to navigate the accuracy-cost trade-off.\\n\\nBenchmarking FL privacy and security.\\n\\nFedScale can evaluate the statistical and system efficiency for privacy and security optimizations more realistically. Here, we give an example of benchmarking DP-SGD (Geyer et al., 2017; Kairouz et al., 2021a), which applies differential privacy to improve the client privacy. We experiment with different privacy targets $\\\\sigma$ ($\\\\sigma=0$ indicates no privacy enhancement) and different number of participants per round $N$. Figure 13 shows that the scale of participants (e.g., $N=30$) that today's benchmarks can support can mislead the privacy evaluations too: for $\\\\sigma=0.01$, while we notice great performance degradation (12.8%) in the final model accuracy when $N=30$, this enhancement is viable in practical FL ($N=100$) with decent accuracy drop (4.6%). Moreover, FedScale is able to benchmark more practical FL metrics, such as wall-clock time, communication cost added in privacy optimizations, and the number of rounds needed to leak the client privacy under realistic individual client data and Non-IID distributions.\\n\\nAs for benchmarking FL security, we follow the example setting of recent backdoor attacks (Sun et al., 2019; Wang et al., 2020) on the OpenImage, where corrupted clients flip their ground-truth labels to poison the training. We benchmarked two settings: one without security enhancement, while the other clips the model updates as (Sun et al., 2019). As shown in Figure 14, while state-of-the-art optimizations report this can mitigate the attacks without hurting the overall performance on their synthesized datasets, large accuracy drops can occur in more practical FL settings.\\n\\n5.2. Opportunities for Future FL Optimizations\\n\\nNext, we show FedScale can shine light on the need for yet unexplored optimizations owing to its realistic FL settings. Heterogeneity-aware co-optimizations of communication and computation\\n\\nExisting optimizations for the system efficiency often apply the same strategy on all clients (e.g., using the same number of local steps (McMahan et al., 2017) or compression threshold (Rothchild et al., 2020)), while ignoring the heterogeneous client system speed. When we outline the timeline of 5 randomly picked participants in training of the ShuffleNet (Figure 15), we find: (1) system\"}"}
