{"id": "jiang23b", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nPrompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to $2.9 \\\\times$ task success rate given the same training data. With $10 \\\\times$ less training data, VIMA still performs $2.7 \\\\times$ better than the best competing variant. Code and video demos are available at vimalabs.github.io.\\n\\n1. Introduction\\nTransformer models (Vaswani et al., 2017) have given rise to remarkable multi-task consolidation across many AI domains. For example, users can describe a task using natural language prompt to GPT-3 (Brown et al., 2020), allowing the same model to perform question answering, machine translation, text summarization, etc. Prompt-based learning provides an accessible and flexible interface to communicate a natural language understanding task to a general-purpose model.\\n\\nWe envision that a generalist robot should have a similarly intuitive and expressive interface for task specification. What does such an interface for robot learning look like? As a motivating example, consider a personal robot tasked with household activities. We can ask the robot to bring us a cup of water by a simple natural language instruction. If we require more specificity, we can instead instruct the robot to \u201cbring me <image of the cup>.\u201d For tasks requiring new skills, the robot should be able to adapt, preferably from a few video demonstrations (Duan et al., 2017). Tasks that need interaction with unfamiliar objects can be easily explained via a few image examples for novel concept grounding (Hermann et al., 2017). Finally, to ensure safe deployment, we can further specify visual constraints like \u201cdo not enter <image> room.\u201d\\n\\nTo enable a single agent with all these capabilities, we make three key contributions in this work: 1) a novel multimodal prompting formulation that converts a wide spectrum of robot manipulation tasks into one sequence modeling problem; 2) a large-scale benchmark with diverse tasks to systematically evaluate an agent\u2019s scalability and generalization; and 3) a multimodal-prompted robot agent capable of multi-task and zero-shot generalization.\\n\\nWe start with the observation that many robot manipulation tasks can be formulated by multimodal prompts that interleave language and images or video frames (Fig. 1). For example, Rearrangement (Batra et al., 2020), a type of Visual Goal, can be formulated as \u201cPlease rearrange objects to match this {scene image}\u201d; Few-shot Imitation can embed video snippet in the prompt \u201cFollow this motion trajectory for the wooden cube: {frame 1}, {frame 2}, {frame 3}, {frame 4}.\u201d Multimodal prompts not only have more expressive power than individual modalities but also enable a uniform sequence IO interface for training generalist robots. Previously, different robot manipulation tasks required distinct policy architectures.\"}"}
{"id": "jiang23b", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VIMA: Robot Manipulation with Multimodal Prompts\\n\\nWe observe that many robot manipulation tasks can be expressed as multimodal prompts that interleave language and image/video frames. We introduce VIMA, an embodied agent capable of processing multimodal prompts (left) and controlling a robot arm to solve the task (right).\\n\\nTo systematically evaluate agents with multimodal prompts, we develop a new benchmark, named VIMA-Bench, built on the Ravens simulator (Zeng et al., 2020; Shridhar et al., 2021). We provide 17 representative tasks with multimodal prompt templates. Each task can be procedurally instantiated into thousands of instances by various combinations of textures and tabletop objects. VIMA-Bench establishes a four-level protocol to evaluate progressively stronger generalization capabilities, from randomized object placement to novel tasks (Fig. 2).\\n\\nTo this end, we introduce the ViMotor Attention agent (VIMA) to learn robot manipulation from multimodal prompts. The model architecture follows the encoder-decoder transformer design proven to be effective and scalable in NLP (Raffel et al., 2020). VIMA encodes an input sequence of interleaving textual and visual prompt tokens with a pre-trained language model (Tsimpoukelli et al., 2021) and decodes robot control actions autoregressively for each environment interaction step. The transformer decoder is conditioned on the prompt via cross-attention layers that alternate with the usual causal self-attention. Instead of operating on raw images, VIMA adopts an object-centric approach. We parse all images in the prompt or observation into objects by off-the-shelf then domain fine-tuned detectors (He et al., 2017) and flatten them into sequences of object tokens. To demonstrate the scalability of VIMA, we train a spectrum of 7 models ranging from 2M to 200M parameters. Our approach outperforms other design alternatives, such as image patch tokens (Reed et al., 2022), image Perceiver (Jaegle et al., 2021b; Alayrac et al., 2022), and decoder-only conditioning (Radford et al., 2018). VIMA obtains consistent performance gains across all four levels of zero-shot generalization and all model capacities, in some cases by a large margin (up to $2.9 \\\\times$ task success rate given the same amount of training data, and $2.7 \\\\times$ better even with $10 \\\\times$ less data). We open-source the simulation environment, training dataset, algorithm code, and pre-trained model checkpoints to ensure reproducibility and facilitate future work from the community. These materials along with video demos are available at vimalabs.github.io.\"}"}
{"id": "jiang23b", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Evaluation Protocol in VIMA-B\\n\\nENCH.\\n\\nWe design 4 levels of evaluation settings to systematically measure the zero-shot generalization capability of an agent. Each level deviates more from the training distribution, and thus is strictly more challenging than the previous level. Resulting in siloed robot systems that do not generalize well across tasks. Our key insight is that various task specifications (such as goal conditioning, video demonstration, natural language instruction) can all be instantiated as multimodal prompts (Fig. 1). Concretely, a multimodal prompt $P$ of length $l$ is defined as an ordered sequence of arbitrarily interleaved texts and images $P := x_1, x_2, \\\\ldots, x_l$, where each element $x_i \\\\in \\\\{\\\\text{text}, \\\\text{image}\\\\}$.\\n\\nTask Suite.\\nThe flexibility afforded by multimodal prompts allows us to specify and build models for a variety of task specification formats. Here we consider the following six categories.\\n\\n1. Simple object manipulation.\\n   Simple tasks like \\\"put <object> into <container>\\\", where each image in the prompt corresponds to a single object;\\n2. Visual goal reaching.\\n   Manipulating objects to reach a goal configuration, e.g., Rearrangement (Batra et al., 2020);\\n3. Novel concept grounding.\\n   The prompt contains unfamiliar words like \\\"dax\\\" and \\\"blicket\\\", which are explained by in-prompt images and then immediately used in an instruction. This tests the agent's ability to rapidly internalize new concepts;\\n4. One-shot video imitation.\\n   Watching a video demonstration and learning to reproduce the same motion trajectory for a particular object;\\n5. Visual constraint satisfaction.\\n   The robot must manipulate the objects carefully and avoid violating the (safety) constraints;\\n6. Visual reasoning.\\n   Tasks that require reasoning skills, such as appearance matching \\\"move all objects with same textures as <object> into <container>\\\", and visual memory \\\"put <object> in <container> and then restore to their original position\\\".\\n\\nNote that these six categories are not mutually exclusive. For example, a task may introduce a previously unseen verb (Novel Concept) by showing a video demonstration, or combine goal reaching with visual reasoning. More details about the task suite are discussed in Appendix, Sec. B.\\n\\n3. VIMA-B\\nENCH: Benchmark for Multimodal Robot Learning\\n\\nSimulation Environment.\\nExisting benchmarks are generally geared towards a particular task specification. To our knowledge, there is no benchmark that provides a rich suite of multimodal tasks and a comprehensive testbed for targeted probing of agent capabilities. To this end, we introduce a new benchmark suite for multimodal robot learning called VIMA-B\\nENCH. We build our benchmark by extending the Ravens robot simulator (Zeng et al., 2020). VIMA-B\\nENCH supports extensible collections of objects and textures to compose multimodal prompts and to procedurally generate a large number of tasks. Specifically, we provide 17 tasks with multimodal prompt templates, which can be instantiated into thousands of task instances. Each task belongs to one or more of the 6 task categories mentioned above. VIMA-B\\nENCH can generate large quantities of imitation learning data via scripted oracle agents. More details are elaborated in Appendix, Sec. A.\\n\\nObservation and Actions.\\nThe observation space of our simulator includes RGB images rendered from both frontal view and top-down view. Ground-truth object segmentation and bounding boxes are also provided for training object-centric models (Sec. 4). We inherit the high-level action space from Zeng et al. (2020), which consists of primitive motor skills like \\\"pick and place\\\" and \\\"wipe\\\". These are parameterized by poses of the end effector. Our simulator also features scripted oracle programs that can generate expert demonstrations by using privileged simulator state information, such as the precise location of all objects, and the ground-truth interpretation of the multimodal instruction.\"}"}
{"id": "jiang23b", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VIMA: Robot Manipulation with Multimodal Prompts\\n\\nFigure 3: VIMA Architecture.\\n\\nWe encode the multimodal prompts with a pre-trained T5 model, and condition the robot controller on the prompt through cross-attention layers. The controller is a causal transformer decoder consisting of alternating self and cross attention layers that predicts motor commands conditioned on prompts and interaction history.\\n\\nTraining Dataset.\\n\\nWe leverage oracles to generate a large offline dataset of expert trajectories for imitation learning. Our dataset includes 50K trajectories per task, and 650K successful trajectories in total. We hold out a subset of objects and textures for evaluation and designate 4 out of 17 tasks as a testbed for zero-shot generalization.\\n\\nEvaluating Zero-Shot Generalization.\\n\\nEach task in VIMA-B has a binary success criterion and does not provide partial reward. During test time, we execute agent policies in the simulator for multiple episodes to compute a percentage success rate. The average success rate over all evaluated tasks will be the final reported metric.\\n\\nWe design a four-level evaluation protocol (Fig. 2) to systematically probe the generalization capabilities of learned agents. Each level deviates more from the training distribution, and is thus strictly harder than the previous one.\\n\\n1. Placement generalization. All prompts are seen verbatim during training, but only the placement of objects on the tabletop is randomized at testing;\\n2. Combinatorial generalization. All textures and objects are seen during training, but new combinations of them appear in testing;\\n3. Novel object generalization. Test prompts and the simulated workspace include novel textures and objects;\\n4. Novel task generalization. New tasks with novel prompt templates at test time.\\n\\nVIMA: Visuomotor Attention Agent\\n\\nOur goal is to build a robot agent capable of performing any task specified by multimodal prompts. There is no prior method that works out of the box with multimodal prompts. To learn an effective multi-task robot policy, we propose VIMA, a robot agent with a multi-task encoder-decoder architecture and object-centric design (Fig. 3).\\n\\nConcretely, we learn a robot policy $\\\\pi(a_t | P, H)$, where $H := o_1, a_1, o_2, a_2, \\\\ldots, o_t$ denotes the past interaction history, and $o_t \\\\in O, a_t \\\\in A$ are observations and actions at each interaction steps. We encode multimodal prompts via a frozen pre-trained language model and decode robot waypoint commands conditioned on the encoded prompts via cross-attention layers. Unlike prior work (Florence et al., 2019; Sieb et al., 2019; Zhu et al., 2022), VIMA adopts an object-centric representation that computes tokens from bounding box coordinates and cropped RGB patches.\"}"}
{"id": "jiang23b", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VIMA: Robot Manipulation with Multimodal Prompts\\nShridhar, M., Manuelli, L., and Fox, D. Perceiver-actor: A multi-task transformer for robotic manipulation. arXiv preprint arXiv: Arxiv-2209.05451, 2022.\\n\\nSieb, M., Xian, Z., Huang, A., Kroemer, O., and Fragkiadaki, K. Graph-structured visual imitation. arXiv preprint arXiv: Arxiv-1907.05518, 2019.\\n\\nSong, S., Zeng, A., Lee, J., and Funkhouser, T. Grasping in the wild: learning 6dof closed-loop grasping from low-cost demonstrations. arXiv preprint arXiv: Arxiv-1912.04344, 2019.\\n\\nSrinivasan, K., Eysenbach, B., Ha, S., Tan, J., and Finn, C. Learning to be safe: Deep rl with a safety critic. arXiv preprint arXiv: Arxiv-2010.14603, 2020.\\n\\nSrivastava, S., Li, C., Lingelbach, M., Mart\u00edn-Mart\u00edn, R., Xia, F., Vainio, K. E., Lian, Z., Gokmen, C., Buch, S., Liu, C. K., Savarese, S., Gweon, H., Wu, J., and Fei-Fei, L. BEHAVIOR: benchmark for everyday household activities in virtual, interactive, and ecological environments. In Faust, A., Hsu, D., and Neumann, G. (eds.), Conference on Robot Learning, 8-11 November 2021, London, UK, volume 164 of Proceedings of Machine Learning Research, pp. 477\u2013490. PMLR, 2021. URL https://proceedings.mlr.press/v164/srivastava22a.html.\\n\\nStengel-Eskin, E., Hundt, A., He, Z., Murali, A., Gopalan, N., Gombolay, M., and Hager, G. Guiding multi-step rearrangement tasks with natural language instructions. In Faust, A., Hsu, D., and Neumann, G. (eds.), Proceedings of the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning Research, pp. 1486\u20131501. PMLR, 08\u201311 November 2022. URL https://proceedings.mlr.press/v164/stengel-eskin22a.html.\\n\\nStepputtis, S., Campbell, J., Phielipp, M., Lee, S., Baral, C., and Amor, H. B. Language-Conditioned Imitation Learning for Robot Manipulation Tasks, October 2020. URL http://arxiv.org/abs/2010.12083. arXiv:2010.12083 [cs].\\n\\nStone, A., Xiao, T., Lu, Y., Gopalakrishnan, K., Lee, K.-H., Vuong, Q., Wohlhart, P., Zitkovich, B., Xia, F., Finn, C., and Hausman, K. Open-world object manipulation using pre-trained vision-language models. arXiv preprint arXiv: Arxiv-2303.00905, 2023.\\n\\nSzot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J., Maestre, N., Mukadam, M., Chaplot, D. S., Maksymets, O., Gokaslan, A., von Drusas, V., Dharur, S., Meier, F., Galuba, W., Chang, A. X., Kira, Z., Koltun, V., Malik, J., Savva, M., and Batra, D. Habitat 2.0: Training home assistants to rearrange their habitat. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 251\u2013266, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/021bbc7ee20b71134d53e20206bd6feb-Abstract.html.\\n\\nTay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A survey. arXiv preprint arXiv: Arxiv-2009.06732, 2020.\\n\\nTeam, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck, C., Bauer, J., Sygnowski, J., Trebacz, M., Jaderberg, M., Mathieu, M., McAleese, N., Bradley-Schmieg, N., Wong, N., Porcel, N., Raileanu, R., Hughes-Fitt, S., Dalibard, V., and Czarnecki, W. M. Open-ended learning leads to generally capable agents. arXiv preprint arXiv: Arxiv-2107.12808, 2021.\\n\\nThananjeyan, B., Balakrishna, A., Nair, S., Luo, M., Srinivasan, K., Hwang, M., Gonzalez, J. E., Ibarz, J., Finn, C., and Goldberg, K. Recovery RL: safe reinforcement learning with learned recovery zones. IEEE Robotics Autom. Lett., 6(3):4915\u20134922, 2021. doi: 10.1109/LRA.2021.3070252. URL https://doi.org/10.1109/LRA.2021.3070252.\\n\\nToyama, D., Hamel, P., Gergely, A., Comanici, G., Glaese, A., Ahmed, Z., Jackson, T., Mourad, S., and Precup, D. Androidenv: A reinforcement learning platform for android. arXiv preprint arXiv: Arxiv-2105.13231, 2021.\\n\\nToyer, S., Shah, R., Critch, A., and Russell, S. The MAGICAL benchmark for robust imitation. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/d464b5ac99e74462f321c06ccacc4bff-Abstract.html.\\n\\nTsimpoukelli, M., Menick, J., Cabi, S., Eslami, S. M. A., Vinyals, O., and Hill, F. Multimodal few-shot learning with frozen language models. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 200\u2013212, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/17.\"}"}
{"id": "jiang23b", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "jiang23b", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VIMA: Robot Manipulation with Multimodal Prompts\\n\\nXiao, Z., Yang, J., Zeng, M., Zhou, L., and Zhang, P.\\n\\nFlorence: A new foundation model for computer vision.\\narXiv preprint arXiv: Arxiv-2111.11432, 2021.\\n\\nZellers, R., Lu, X., Hessel, J., Yu, Y., Park, J. S., Cao, J., Farhadi, A., and Choi, Y.\\nMerlot: Multimodal neural script knowledge models.\\narXiv preprint arXiv: Arxiv-2106.02636, 2021.\\n\\nZellers, R., Lu, J., Lu, X., Yu, Y., Zhao, Y., Salehi, M., Kusupati, A., Hessel, J., Farhadi, A., and Choi, Y.\\nMerlot reserve: Neural script knowledge through vision and language and sound.\\nCVPR, 2022.\\n\\nZeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M., Armstrong, T., Krasin, I., Duong, D., Wahid, A., Sindhwani, V., and Lee, J.\\nTransporter networks: Rearranging the visual world for robotic manipulation.\\narXiv preprint arXiv: Arxiv-2010.14406, 2020.\\n\\nZeng, A., Wong, A., Welker, S., Choromanski, K., Tombari, F., Purohit, A., Ryoo, M., Sindhwani, V., Lee, J., Vanhoucke, V., and Florence, P.\\nSocratic models: Composing zero-shot multimodal reasoning with language.\\narXiv preprint arXiv: Arxiv-2204.00598, 2022.\\n\\nZhang, Z. and Weihs, L.\\nWhen learning is out of reach, reset: Generalization in autonomous visuomotor reinforcement learning.\\narXiv preprint arXiv: Arxiv-2303.17600, 2023.\\n\\nZhao, M., Liu, F., Lee, K., and Abbeel, P.\\nTowards more generalizable one-shot visual imitation learning.\\nIn 2022 International Conference on Robotics and Automation, ICRA 2022, Philadelphia, PA, USA, May 23-27, 2022, pp. 2434\u20132444. IEEE, 2022. doi: 10.1109/ICRA46639.2022.9812450. URL https://doi.org/10.1109/ICRA46639.2022.9812450.\\n\\nZheng, Q., Zhang, A., and Grover, A.\\nOnline decision transformer.\\narXiv preprint arXiv: Arxiv-2202.05607, 2022.\\n\\nZhu, Y., Wong, J., Mandlekar, A., and Mart\u00edn-Mart\u00edn, R.\\nrobosuite: A modular simulation framework and benchmark for robot learning.\\narXiv preprint arXiv: Arxiv-2009.12293, 2020.\\n\\nZhu, Y., Joshi, A., Stone, P., and Zhu, Y.\\nViola: Imitation learning for vision-based manipulation with object proposal priors.\\narXiv preprint arXiv: Arxiv-2210.11339, 2022.\"}"}
{"id": "jiang23b", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We build our VIMA-B benchmark simulation suite upon the Ravens physics simulator (Zeng et al., 2020; Shridhar et al., 2021). Specifically, it is supported by PyBullet (Coumans & Bai, 2016\u20132021) with a Universal Robot UR5 arm. The size of the tabletop workspace is 0.5 \u00d7 1 m. Our benchmark contains extensible sets of 3D objects and textures. Instantiated from an object-texture combination, all object instances can be rendered as RGB images appeared in multimodal prompts. Figure A.1 displays all 3D objects. Figure A.2 displays all textures.\\n\\nOur benchmark contains extensible sets of 3D objects and textures. Instantiated from an object-texture combination, all object instances can be rendered as RGB images appeared in multimodal prompts. Figure A.1 displays all 3D objects. Figure A.2 displays all textures.\\n\\nThe observation space of VIMA-B includes RGB images from both frontal and top-down views. It also includes a one-hot vector \\\\( \\\\in \\\\{0, 1\\\\}^2 \\\\) to indicate type of the end-effector \\\\( \\\\in \\\\{\\\\text{suction cup, spatula}\\\\} \\\\). While a suction cup is equipped in most manipulation tasks, a spatula is used in particular for visual constraint tasks, where an agent is asked to \u201cwipe\u201d objects. VIMA-B inherits the same action space from Zeng et al. (2020) and Shridhar et al. (2021), which consists of primitive actions of \u201cpick and place\u201d for tasks with a suction cup as the end effector, or \u201cpush\u201d for tasks with a spatula. Both primitive actions contain two poses \\\\( \\\\in \\\\text{SE}(2) \\\\) specifying target poses of the end effector. For the \u201cpick and place\u201d primitive, they represent the pick pose and the place pose. For the \u201cpush\u201d primitive, they represent the push starting pose and push.\\n\\n| Object Gallery in VIMA-B | textured with random textures. Bowl and pan are from Google Scanned Objects (Downs et al., 2022), while others are from Ravens (Zeng et al., 2020). |\\n|--------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\\n| L-shaped block           |                                                                                                                                          |\\n| block                   |                                                                                                                                          |\\n| bowl                    |                                                                                                                                          |\\n| container               |                                                                                                                                          |\\n| cross                   |                                                                                                                                          |\\n| diamond                 |                                                                                                                                          |\\n| flower                  |                                                                                                                                          |\\n| frame                   |                                                                                                                                          |\\n| heart                   |                                                                                                                                          |\\n| hexagon                 |                                                                                                                                          |\\n| letter A                |                                                                                                                                          |\\n| letter E                |                                                                                                                                          |\\n| letter G                |                                                                                                                                          |\\n| letter M                |                                                                                                                                          |\\n| letter R                |                                                                                                                                          |\\n| letter T                |                                                                                                                                          |\\n| letter V                |                                                                                                                                          |\\n| line                    |                                                                                                                                          |\\n| pallet                  |                                                                                                                                          |\\n| pan                     |                                                                                                                                          |\\n| pentagon                |                                                                                                                                          |\\n| ring                    |                                                                                                                                          |\\n| round                   |                                                                                                                                          |\\n| shorter block           |                                                                                                                                          |\\n| small block             |                                                                                                                                          |\\n| square                  |                                                                                                                                          |\\n| star                    |                                                                                                                                          |\\n| three-sided             |                                                                                                                                          |\\n| rectangle               |                                                                                                                                          |\\n| triangle                |                                                                                                                                          |\"}"}
{"id": "jiang23b", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 12: Data scaling when baseline variants\u2019 ViT is trained from scratch, indicated inside parentheses. \u2191 and \u2193 denote performance increase and decrease. Numbers in the first row represent the size of training dataset.\\n\\n| Level | Method         | L1   | L2   | L3   | L4   |\\n|-------|----------------|------|------|------|------|\\n|       | Ours           | 0.0  | 0.0  | 0.0  | 0.0  |\\n| 0.1%  | VIMA-Gato      | 36.3 | 2.0  | 11.5 | 57.5 |\\n|       | VIMA-Flamingo  | 32.6 | 2.0  | 37.7 | 52.3 |\\n|       | VIMA-GPT       | 39.6 | 6.0  | 30.9 | 52.8 |\\n| 1%    | VIMA-Gato      | 76.3 | 2.0  | 11.9 | 57.0 |\\n|       | VIMA-Flamingo  | 73.8 | 2.0  | 33.9 | 47.2 |\\n|       | VIMA-GPT       | 79.3 | 6.0  | 26.5 | 47.6 |\\n| 10%   | VIMA-Gato      | 79.3 | 2.0  | 26.5 | 47.6 |\\n|       | VIMA-Flamingo  | 80.8 | 2.0  | 33.9 | 47.5 |\\n|       | VIMA-GPT       | 80.8 | 6.0  | 26.5 | 47.6 |\\n| Full  | Ours           | 81.5 | 15.4 | 80.8 | 49.3 |\\n|       | VIMA-Gato      | 79.0 | 1.1  | 24.6 | 53.9 |\\n|       | VIMA-Flamingo  | 73.2 | 1.0  | 12.6 | 42.1 |\\n|       | VIMA-GPT       | 73.2 | 5.5  | 22.6 | 47.3 |\\n\\nL1 fine-tune the last two layers and freeze all other layers. We fix the parameter count of the decision-making part to be 200M.\\n\\nAs shown in Table 13, we find no significant difference among the variants. Thus we set the standard t5-base as default for all our models.\\n\\nTable 13: Performances of our method with differently sized pre-trained T5 prompt encoder. We fix the parameter count of the decision-making part to be 200M.\\n\\n| Model Size  | L1   | L2   | L3   | L4   |\\n|-------------|------|------|------|------|\\n| t5-small    | 78.8 | 79.0 | 80.3 | 49.1 |\\n| t5-base     | 81.5 | 81.5 | 78.7 | 48.6 |\\n| t5-large    | 80.8 | 81.0 | 81.0 | 49.3 |\\n\\nE.5. Policy Robustness\\n\\nIncreasing Amounts of Distractors. We study the policy robustness against increasing amounts of distractors in scenes. For all tasks being evaluated, we add one more distractor object. We run our largest VIMA model with 200M parameters. The result is presented in Table 14.\\n\\nIt turns out that the performance of VIMA degrades minimally with more distractors than the training distribution. This indicates that our agent has learned a reasonably robust policy against objects that are irrelevant to the task.\\n\\nTable 14: Evaluation results on tasks with increased amounts of distractors. We fix the parameter count of the decision-making part to be 200M.\\n\\n| L1         | L2         | L3         | L4         |\\n|------------|------------|------------|------------|\\n| Original   | 81.5       | 81.5       | 78.7       | 48.6       |\\n| More Distractors | 78.5       | 78.6       | 72.9       | 47.8       |\\n| Relevant Performance Decrease (%) | 3.6       | 3.5       | 7.3       | 1.6       |\"}"}
{"id": "jiang23b", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We then study the policy robustness against imperfect prompts, including incomplete prompts (randomly masking out words with `<UNK>` token) and corrupted prompts (randomly swapping words, which could have changed the task meaning altogether). We run our largest VIMA model with 200M parameters, results are shown in Table 15. Our well-trained model exhibits minimal performance decrease when evaluated on masked prompts and minor decrease on corrupted prompts. We attribute this robustness to the high-quality pre-trained T5 language backbone.\\n\\nTable 15: Evaluation results with incomplete and corrupted prompts. We fix the parameter count of the decision-making part to be 200M.\\n\\n|       | L1   | L2   | L3   | L4   |\\n|-------|------|------|------|------|\\n| Original | 81.5 | 81.5 | 78.7 | 48.6 |\\n| Incomplete Prompts | 80.8 | 81.1 | 77.0 | 48.0 |\\n| Corrupted Prompts   | 78.2 | 78.1 | 73.8 | 45.3 |\\n\\nRelevant Performance Decrease w/ Incomplete Prompts (%) 0.8 0.4 2.1 1.2\\nRelevant Performance Decrease w/ Corrupted Prompts (%) 4.2 4.3 6.6 7.2\"}"}
{"id": "jiang23b", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide an extended review of related work as complementary to Section 6.\\n\\nMulti-Task Learning by Sequence Modeling. In computer vision, Mask R-CNN (He et al., 2017), UberNet (Kokkinos, 2016), and 12-in-1 (Lu et al., 2020) leverage a single backbone model with multiple independent heads for different tasks. UVim (Kolesnikov et al., 2022) is another unified approach for vision that uses a language model to generate the guiding code for a second model to predict raw vision outputs. In multimodal learning, numerous works (Lu et al., 2022; Wang et al., 2022a; Zellers et al., 2021; 2022; Buch et al., 2022; Fu et al., 2021; Yang et al., 2022) investigate the unification of image, video, audio, and/or language modalities to deliver multi-purpose foundation models, although most of which are not equipped with decision-making capabilities. BEiT-3 (Wang et al., 2022c) performs masked data modeling on images, texts and image-text pairs to pre-train a backbone for various downstream tasks.\\n\\nFoundation Models for Embodied Agents. Embodied agent research (Duan et al., 2022; Batra et al., 2020; Ravichandar et al., 2020; Collins et al., 2021) is adopting the large-scale pre-training paradigm (Yang et al., 2023), powered by a collection of learning environments (Abramson et al., 2020; Shridhar et al., 2020; Savva et al., 2019; Puig et al., 2018; Team et al., 2021; Toyama et al., 2021; Shi et al., 2017). From the aspect of pre-training for better representations, Reid et al. (2022) fine-tunes from LLM checkpoints to accelerate policy learning. LaTTe (Bucker et al., 2022) and Embodied-CLIP (Khandelwal et al., 2021) leverage the frozen visual and textual representations of CLIP (Radford et al., 2021) for robotic manipulation. MaskDP (Liu et al., 2022a) pre-trains bidirectional transformers for various downstream embodied tasks. From the perspective of leveraging transformer as agent architecture, methods such as Dasari & Gupta (2020) and MOSAIC (Zhao et al., 2022) achieve superior performance in one-shot video imitation tasks. They both use the self-attention mechanism with auxiliary losses such as inverse dynamics loss (Dasari & Gupta, 2020) and contrastive loss (Zhao et al., 2022) to learn robot controllers.\\n\\nInstruct RL (Liu et al., 2022b) leverages jointly pre-trained vision-language models as robot agents to perform manipulation tasks. From the perspective of large language models for robot learning, Socratic Models (Zeng et al., 2022) composes multiple vision and language foundation models for multimodal reasoning in videos. ROSIE (Yu et al., 2023) leverages text-to-image diffusion models to augment existing robotic dataset (Brohan et al., 2022) via inpainting. MOO (Minderer et al., 2022) adopts a similar object-centric representation as ours for open-world object manipulation. Furthermore, Voyager (Wang et al., 2023) develops a LLM-powered agent operating in an open-ended virtual world (Fan et al., 2022).\\n\\nRobot Manipulation and Benchmarks. There are many prior works that are not mentioned in the main paper that study different robotic manipulation tasks, such as instruction following (Shridhar et al., 2021; Lynch & Sermanet, 2021), constraint satisfaction (Bharadhwaj et al., 2021; Srinivasan et al., 2020; Thananjeyan et al., 2021), one-shot imitation (Paine et al., 2018; Huang et al., 2019; Dasari & Gupta, 2020; Aceituno et al., 2021; Zhao et al., 2022), rearrangement (Weihs et al., 2021; Szot et al., 2021; Liu et al., 2021; Ehsani et al., 2021; Gan et al., 2021; Stengel-Eskin et al., 2022), and reasoning (Gupta et al., 2019; Ahmed et al., 2021; Toyer et al., 2020; Lim et al., 2021). Multiple simulation benchmarks are introduced to study the above tasks: 1) Indoor simulation environments: Habitat (Savva et al., 2019; Szot et al., 2021) is equipped with a high-performance 3D simulator for fast rendering and proposes a suite of common tasks for assistive robots. 2) Tabletop environments: RLBench (James et al., 2019) and SURREAL (Fan et al., 2018; 2019) are other widely used simulator benchmarks studying robotics manipulation with tabletop settings. STRETCH-P&P (Zhang & Weihs, 2023) studies generalization across goals for reset-free reinforcement learning. All these aforementioned simulators and benchmarks do not natively support task specification and prompting with multiple modalities.\"}"}
{"id": "jiang23b", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"G. Limitations and Further Discussions\\n\\nReliance on a separate object detector. VIMA inherits the errors from the standalone object detector, which may struggle in the cases of occlusion or out-of-distribution object forms. However, using object detectors is not entirely without merits. First, it allows us to seamlessly switch to stronger detection models when they become available. For example, we can switch to object detectors that are more robust and open-vocabulary, such as OWL-ViT (Minderer et al., 2022). This would enable VIMA to transfer to real-world scenarios with minimal modifications. Second, by leveraging pre-trained vision pipelines, several concurrent works have demonstrated the superiority of object-centric representation in robot manipulation. For example, VIOLA (Zhu et al., 2022) achieves better performance with a pre-trained Region Proposal Network (Ren et al., 2015). MOO (Stone et al., 2023) also shows that a robot agent with OWL-ViT (Minderer et al., 2022) as the object detector significantly outperforms RT-1 (Brohan et al., 2022), which directly learns from raw pixels, on various real-world manipulation tasks. In fact, MOO (Stone et al., 2023) includes a baseline called \u201cVIMA-like\u201d that already demonstrates strong performance on real robots under real-world scenarios. As we witness image segmentation is becoming more robust and general-purpose (Kirillov et al., 2023), we envision such design choice will become more effective and further gain more popularity.\\n\\nLimited simulator realism and task complexity. Our goal with VIMA-BENCH is to explore the multi-task ability, generalization, and understanding of multi-modality. Therefore, these aspects are not the primary focus of this work. However, we envision future works can combine this formulation with more physically realistic simulators such as Zhu et al. (2020), Srivastava et al. (2021), and Mittal et al. (2023).\\n\\nLimited action primitives. We inherit the same high-level action space from well-established prior works, such as Transporter (Zeng et al., 2020). While \u201cpick-and-place\u201d and \u201cwipe\u201d seem simple, they do cover a wide range of tabletop manipulation tasks and are crucial to industrial use cases like warehouse robots (Yoon et al., 2003; Berscheid et al., 2020; Devin et al., 2020; Song et al., 2019). While VIMA is currently using these two actions, the algorithm design is general-purpose and does not make assumptions about the particular action choices. For example, VIMA would require only minimal modifications to support more low-level action spaces like joint-torque control.\"}"}
{"id": "jiang23b", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We train it end-to-end with causal behavior cloning loss. VIMA-Flamingo differs from ours since it processes image observations into a fixed number of visual tokens through a learned Perceiver Resampler. Model hyperparameters for our reimplemention of the Perceiver Resampler is listed in Table 6.\\n\\n### Table 6: Model hyperparameters for Perceiver Resampler used in VIMA-Flamingo method.\\n\\n| Hyperparameter          | Value |\\n|-------------------------|-------|\\n| Number of Latent Queries | 4     |\\n| Number of Blocks         | 4     |\\n| Self-Attn per Block     | 4     |\\n| Self-Attn Heads         | 24    |\\n| Cross-Attn Heads        | 24    |\\n\\n**C.3.3. VIMA-GPT**\\n\\nVIMA-GPT is a GPT-based behavior cloning agent conditioned on tokenized multimodal prompts with the GPT architecture. It autoregressively decodes next actions given multimodal prompts and interaction histories. We optimize this method end-to-end with causal behavior cloning loss. Similar to prior works of casting RL problems as sequence modeling (Chen et al., 2021; Janner et al., 2021; Zheng et al., 2022), it encodes an image into a single \\\"state\\\" token through a learned ViT encoder. It also directly models entire trajectory sequences prepended with prompt tokens. Therefore, it differs from our method in the representation of observation tokens and prompt conditioning. For visual tokenizer, we employ a learned ViT with hyperparameters listed in Table 5.\\n\\n**C.4. Mask R-CNN Detection Model**\\n\\nFinally, we elaborate on the mask R-CNN model (He et al., 2017) for scene parsing and object extraction. We fine-tune a pre-trained lightweight mask R-CNN (mask rcnn R50 FPN 3x) from Wu et al. (2019) to adapt to scenes and images in our tabletop environment. We fine-tune it on a subset of agent training dataset. It contains 100 trajectories for each task, resulting in 22,741 images and 61,822 annotations in total. We use learning rate $5 \\\\times 10^{-4}$ and train for 10 epochs. During model selection, we particularly favor models with high recall to reduce the number of missed objects. To compensate for resulting false-positives, we adopt object augmentation during agent training (Appendix, Sec. D).\\n\\nA visualization of its output is provided in Figure A.20. We do not use the predicted object names in our models.\\n\\nFigure A.20: Visualization of fine-tuned mask R-CNN. Left: Prediction from the detection model. Right: Ground-truth scene parsing. The detection model agrees well with ground-truth objects.\"}"}
{"id": "jiang23b", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D. VIMA Training Details\\n\\nWe follow the best practice to train Transformer models using the AdamW optimizer (Loshchilov & Hutter, 2019), learning rate warm-up, cosine annealing (Loshchilov & Hutter, 2017), etc. Training hyperparameters are provided in Table 7. We use GEGLU activation (Shazeer, 2020) inside Transformer models across all methods.\\n\\n| Hyperparameter      | Value         |\\n|---------------------|---------------|\\n| Learning Rate       | 0.0001        |\\n| Warmup Steps        | 7K            |\\n| LR Cosine Annealing Steps | 17K       |\\n| Weight Decay        | 0             |\\n| Dropout             | 0.1           |\\n| Gradient Clip Threshold | 1.0        |\\n\\nTo make trained models robust to detection inaccuracies and failures, we apply object augmentation by randomly injecting false-positive detection outputs. Concretely, for observation at each time step, we sample number of augmented objects $i.i.d. n_{\\\\text{augmented objects}} \\\\sim \\\\text{Cat}(K, p)$, where $\\\\text{Cat}(\\\\cdot)$ denotes a categorical distribution with $K$ supports parameterized by $p$. For each augmented object, we then randomly sample a bounding box and corresponding cropped image to add to object tokens. In our experiments, we set $p = \\\\{0 : 0.95, 1 : 0.05\\\\}$ with $K = 2$.\\n\\nD.1. Vary Model Capacity\\n\\nWe train a spectrum of 7 models ranging from 2M to 200M parameters. To vary the model capacity, we follow prior work (Chowdhery et al., 2022) to change embedding dimension and number of layers. We list configurations for methods with cross-attention prompt conditioning (i.e., ours and VIMA-Flamingo) in Table 8, and configurations for methods only with causal self-attention (i.e., VIMA-Gato and VIMA-GPT) in Table 9.\\n\\n| Model Size (M) | Embedding Dimension | Num Blocks | X-Attn Heads | Self-Attn Heads |\\n|----------------|---------------------|------------|--------------|-----------------|\\n| 2              | 256                 | 1          | 8            | 8               |\\n| 4              | 256                 | 2          | 8            | 8               |\\n| 9              | 320                 | 3          | 10           | 10              |\\n| 20             | 384                 | 4          | 12           | 12              |\\n| 43             | 512                 | 5          | 16           | 16              |\\n| 92             | 640                 | 7          | 20           | 20              |\\n| 200            | 768                 | 11         | 24           | 24              |\\n\\n| Model Size (M) | Embedding Dimension | Num Blocks | Self-Attn Heads |\\n|----------------|---------------------|------------|-----------------|\\n| 2              | 64                  | 1          | 2               |\\n| 4              | 96                  | 2          | 3               |\\n| 9              | 192                 | 3          | 6               |\\n| 20             | 320                 | 4          | 10              |\\n| 43             | 512                 | 5          | 16              |\\n| 92             | 768                 | 7          | 24              |\\n| 200            | 768                 | 18         | 24              |\"}"}
{"id": "jiang23b", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"E. Extended Experiment Results\\n\\nE.1. Training Time and Compute\\n\\nAll experiments are conducted on cluster nodes, each with 8 NVIDIA V100 GPUs. The largest experiment takes approximately one day. We utilize DDP (distributed data parallel) to accelerate the training.\\n\\nE.2. Model Scaling\\n\\nE.2.1. Numerical Results\\n\\nWe present numerical results that constitute Fig. 4 in Table 10. The claim of \u201cup to $2.9 \\\\times$ improvement\u201d made in Abstract and Sec. 1 is calculated as follows. The best competing variant is VIMA-Gato. On the hardest L4, our method shows the most significant relative improvement with a model size of 20M. We compute the performance gap, divide by VIMA-Gato\u2019s performance, and only keep the first digit after decimal to obtain the result.\\n\\n| Level | Method       | 2M  | 4M  | 9M  | 20M | 43M | 92M | 200M |\\n|-------|--------------|-----|-----|-----|-----|-----|-----|------|\\n|       | Ours         | 76.5 | 79.2 | 77.4 | 77.1 | 78.2 | 79.3 | 81.5 |\\n|       | VIMA-Gato    | 37.6 | 42.6 | 44.2 | 46.1 | 49.5 | 57.0 | 58.0 |\\n|       | VIMA-Flamingo| 42.4 | 48.9 | 45.6 | 46.6 | 47.0 | 47.2 | 47.4 |\\n|       | VIMA-GPT     | 30.0 | 37.0 | 44.9 | 48.5 | 48.0 | 47.9 | 46.9 |\\n|       |              |     |     |     |     |     |     |      |\\n|       | Ours         | 77.1 | 79.2 | 78.2 | 77.6 | 77.6 | 80.1 | 81.5 |\\n|       | VIMA-Gato    | 35.9 | 39.3 | 41.3 | 44.1 | 46.6 | 53.9 | 53.1 |\\n|       | VIMA-Flamingo| 41.0 | 46.5 | 44.6 | 44.6 | 45.4 | 47.1 | 46.0 |\\n|       | VIMA-GPT     | 29.8 | 35.0 | 43.3 | 45.8 | 45.9 | 47.4 | 46.9 |\\n|       |              |     |     |     |     |     |     |      |\\n|       | Ours         | 77.3 | 77.8 | 78.5 | 77.3 | 81.8 | 81.9 | 78.7 |\\n|       | VIMA-Gato    | 29.0 | 33.2 | 37.5 | 40.2 | 42.5 | 45.6 | 46.0 |\\n|       | VIMA-Flamingo| 35.0 | 41.9 | 39.2 | 40.5 | 40.3 | 42.1 | 40.7 |\\n|       | VIMA-GPT     | 25.3 | 29.3 | 39.0 | 43.5 | 43.0 | 42.6 | 42.2 |\\n|       |              |     |     |     |     |     |     |      |\\n|       | Ours         | 25.7 | 49.0 | 47.1 | 48.8 | 49.0 | 49.6 | 48.6 |\\n|       | VIMA-Gato    | 13.3 | 13.2 | 12.2 | 12.3 | 12.8 | 13.5 | 16.8 |\\n|       | VIMA-Flamingo| 12.3 | 11.6 | 10.7 | 12.1 | 10.7 | 11.1 | 12.1 |\\n|       | VIMA-GPT     | 11.1 | 10.3 | 12.7 | 14.2 | 11.8 | 12.1 | 12.1 |\\n\\nE.3. Data Scaling\\n\\nE.3.1. Detailed Setup\\n\\nTo ensure all methods are fairly pre-trained on the same amount of data (i.e., they have roughly the same amount of built-in information, thus the x-axis in Fig. 4 faithfully corresponds to the extra bits of information seen during further training), we initialize variants that directly learn from raw pixels with MVP pre-trained ViT (Xiao et al., 2022; Radosavovic et al., 2022). It is further MAE fine-tuned (He et al., 2021), using the same in-domain data as for the Mask R-CNN object detector. Note that the MVP pre-trained then domain fine-tuned ViT also updates weights jointly with robot controllers later on. We use the ViT-B backbone from MVP. The in-domain data for fine-tuning include 100 trajectories for each task.\\n\\nE.3.2. Numerical Results\\n\\nWe present numerical results that constitute Fig. 4 in Table 11. The claim of \u201c2.7\u00d7 improvement\u201d made in Abstract and Sec. 1 is calculated as follows. The best competing variant is VIMA-Gato that achieves 12.2% average success rate trained with full data on L4. Our method trained with 10% data achieves 46% average success rate on the same level. We compute the performance gap, divide by VIMA-Gato\u2019s performance, and only keep the first digit after decimal to obtain the result.\"}"}
{"id": "jiang23b", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 11: Data scaling numerical results that constitute Fig. 4. Numbers in the first row indicate the size of training dataset.\\n\\n| Method       | 0.1% | 1%  | 10% | Full (100%) |\\n|--------------|------|-----|-----|-------------|\\n| L1           | 0.0  | 36.3| 76.3| 79.3        |\\n| Ours         |      |     |     |             |\\n| VIMA-Gato    | 0.0  | 11.5| 41.5| 57.5        |\\n| VIMA-Flamingo| 0.0  | 2.0 | 37.7| 52.3        |\\n| VIMA-GPT     | 0.0  | 6.0 | 30.9| 52.8        |\\n| L2           | 0.0  | 34.3| 75.8| 80.1        |\\n| Ours         |      |     |     |             |\\n| VIMA-Gato    | 0.0  | 10.1| 37.9| 41.2        |\\n| VIMA-Flamingo| 0.0  | 2.0 | 33.8| 32.6        |\\n| VIMA-GPT     | 0.0  | 6.0 | 29.7| 40.3        |\\n| L3           | 0.0  | 15.4| 73.2| 81.9        |\\n| Ours         |      |     |     |             |\\n| VIMA-Gato    | 0.0  | 10.2| 34.8| 40.9        |\\n| VIMA-Flamingo| 0.0  | 1.0 | 33.1| 33.6        |\\n| VIMA-GPT     | 0.0  | 5.5 | 28.6| 39.2        |\\n\\n### E.3.3. What if baseline variants\u2019 ViT is trained from scratch?\\n\\nWe further investigate what if baseline variants\u2019 ViT is trained from scratch and end-to-end with the robot controllers. We visualize the results in Fig. A.21 and numerically present them in Table 12. We annotate with arrows to indicate performance increase (\u2191) and decrease (\u2193). We highlight two findings.\\n\\nFirst, MVP pre-trained ViT is most beneficial in the setting with sufficient in-domain training data (i.e., the 10% data scenario). It boosts the performance for the most competing baseline variant VIMA-Gato. However, in other settings with abundant in-domain data (i.e., the full data scenario) or insufficient in-domain data (i.e., 1% and 0.1% scenarios), the advantage of MVP pre-trained ViT diminishes and it even becomes detrimental. This aligns with the finding in previous empirical studies (Hansen et al., 2022). Second, in settings with reasonable amounts of in-domain data (i.e., the 1%, 10%, and 100% scenarios), our recommended recipe always outperforms other variants. We notice that such a data demand generally can be satisfied by both simulated robotics data (Mandlekar et al., 2021) and real robotics data (Dasari et al., 2019; Brohan et al., 2022). Therefore, it demonstrates that our recommended recipe is highly sample-efficient compared to alternative designs, especially under practical settings.\\n\\n### Figure A.21: Data scaling when baseline variants\u2019 ViT is trained from scratch.\\n\\nIn settings with reasonable amounts of in-domain data (i.e., the 1%, 10%, and 100% scenarios), our recommended recipe always outperforms other variants.\\n\\n### E.4. Vary T5 Encoder Sizes\\n\\nWe vary the size of the pre-trained T5 encoder (Raffel et al., 2020) to study the effect of prompt encoding. We experiment with three T5 model capacities: t5-small (30M), t5-base (111M), and t5-large (368M). For all T5 variants, we...\"}"}
{"id": "jiang23b", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Task 12: Sweep the designated number of objects into a specified region without exceeding the boundary.\\n\\n- **Prompt:** Sweep {quantifier} {object} into {bounds} without exceeding {constraint}.\\n- **Description:** {object} is the image placeholder of the target object to be swept spawned with a random amount in the workspace. Distractors have the same amount, same shape, but different color from target objects. {quantifier} is the text placeholder to determine the target quantity of objects to be wiped, sampled from any, one, two, three, and all. {bounds} is the image placeholder for a three-sided rectangle as the goal region. {constraint} is the constraint line.\\n- **Success Criteria:** The exact number of target objects to be swept are all inside the specified region. Potential failure cases include 1) any distractor being wiped into the region, 2) target object exceeding the constraint, or 3) incorrect number of target objects being swept into the goal region.\\n- **Oracle Trajectory:** Shown in Fig. A.14 with its multimodal prompt.\\n\\nTask 13: Sweep the designated number of objects into a specified region without touching the constraint.\\n\\n- **Prompt:** Sweep {quantifier} {object} into {bounds} without touching {constraint}.\\n- **Description:** Similar as task 12 but requiring a different way to satisfy the constraint. The agent has to learn to avoid contacting the constraint line in this case.\\n- **Success Criteria:** Similar as task 12 except that the constraint is to not touch the red line.\\n- **Oracle Trajectory:** Shown in Fig. A.15 with its multimodal prompt.\"}"}
{"id": "jiang23b", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This task category requires agents to make decisions by reasoning over or memorizing information conveyed through multimodal prompts.\\n\\nTask 14:\\nBy reasoning the \\\"same texture\\\", the agent is required to pick all objects in the workspace with the same texture as the container objects specified in the prompt and place them into it.\\n\\n- **Prompt:** Put all objects with the same texture as \\\\{object\\\\} into it.\\n- **Description:** \\\\{object\\\\} is the sampled goal container object. In the workspace, there are objects with the same texture as the container but potentially different shapes. Distractors with different textures are spawned.\\n- **Success Criteria:** All objects with the same texture as the goal container are within the bounds of the container.\\n- **Oracle Trajectory:** Shown in Fig. A.16 with its multimodal prompt.\\n\\nTask 15:\\nBy reasoning the \\\"same shape\\\", the agent is required to pick all objects in the workspace with the same top-down profile as the goal container specified in the prompt and place them into it. For example, blocks and boxes have the same rectangular profile.\\n\\n- **Prompt:** Put all objects with the same profile as \\\\{object\\\\} into it.\\n- **Description:** Similar to the task 14 except the objects to be picked and placed have the same shape. There are three different shapes: rectangular-like (e.g. block and pallet), circle-like (e.g. ring and bowl), and undetermined for the rest.\"}"}
{"id": "jiang23b", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Task 15:\\n\\n- **Prompt:** Put all objects with the same profile into it.\\n- **Description:** Objects in image placeholders are the target objects to be picked and the container, respectively. We then ask the agent to put one of the objects into the same container. The old neighboring object is specified through cardinal directions.\\n- **Success Criteria:** The target object and the correct neighboring object are inside the container.\\n- **Oracle Trajectory:** Shown in Fig. A.17 with its multimodal prompt.\\n\\nTask 16:\\n\\n- **Prompt:** First put \\\\( \\\\{ \\\\text{object} \\\\} \\\\) into \\\\( \\\\{ \\\\text{object} \\\\} \\\\) then put the object that was previously at its \\\\( \\\\{ \\\\text{direction} \\\\} \\\\) into the same \\\\( \\\\{ \\\\text{object} \\\\} \\\\).\\n- **Description:** Objects in image placeholders \\\\( \\\\{ \\\\text{object} \\\\} \\\\) and \\\\( \\\\{ \\\\text{object} \\\\} \\\\) are the target object to be picked and the container, respectively. We then ask the agent to put one of the old neighbors of the previous target object into the same container. The old neighboring object is specified through the cardinal directions.\\n- **Success Criteria:** The target object and the correct neighboring object are inside the container.\\n- **Oracle Trajectory:** Shown in Fig. A.18 with its multimodal prompt.\\n\\nTask 17:\\n\\n- **Prompt:** Pick and place the target object specified in the prompt into different containers in order then restore to the initial container.\\n- **Description:** The object in the image placeholder \\\\( \\\\{ \\\\text{object} \\\\} \\\\) is the target object to be manipulated across the task. There are more than one target containers (e.g. \\\"Put \\\\( \\\\{ \\\\text{object} \\\\} \\\\) into \\\\( \\\\{ \\\\text{object} \\\\} \\\\) then \\\\( \\\\{ \\\\text{object} \\\\} \\\\). Finally restore it into its original container\\\") for two target containers to be placed in order). The rest of spawned containers naturally becomes distractors.\"}"}
{"id": "jiang23b", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Success Criteria:\\nThe target object is first put into multiple containers following the specific order. Finally it should be restored into its original container.\\n\\nOracle Trajectory:\\nShown in Fig.A.19 with its multimodal prompt.\"}"}
{"id": "jiang23b", "page_num": 45, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"This section contains more detailed tables that correspond to the results in Figure 4. Specifically, we show breakdown results on each task that constitute the model scaling results in Tables 16, 17, 18, and 19.\\n\\nTable 16: L1 level generalization results. Model indicates robot controller parameter count. Integers in the first row refer to indices of tasks described in Appendix, Sec. B.\\n\\n| Method       | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 09 | 11 | 12 | 15 | 16 | 17 |\\n|--------------|----|----|----|----|----|----|----|----|----|----|----|----|----|\\n| 2M Ours      | 100| 0  | 100| 0  | 100| 0  | 96 | 0  | 37 | 0  | 93 | 5  | 64 |\\n| VIMA-Gato    | 62 | 5  | 22 | 46 | 13 | 7  | 44 | 43 | 0  | 85 | 44 | 43 | 0  |\\n| VIMA-Flamingo| 56 | 5  | 53 | 45 | 37 | 0  | 54 | 55 | 3  | 83 | 40 | 43 | 2  |\\n| VIMA-GPT     | 59 | 5  | 7  | 7  | 0  | 43 | 49 | 2  | 62 | 76 | 27 | 5  | 0  |\\n| 4M Ours      | 100| 0  | 100| 0  | 100| 0  | 99 | 0  | 45 | 5  | 90 | 44 | 43 |\\n| VIMA-Gato    | 61 | 0  | 22 | 46 | 13 | 7  | 64 | 85 | 4  | 86 | 46 | 42 | 2  |\\n| VIMA-Flamingo| 61 | 0  | 57 | 47 | 45 | 0  | 59 | 60 | 5  | 81 | 40 | 43 | 2  |\\n| VIMA-GPT     | 58 | 0  | 17 | 25 | 12 | 47 | 54 | 3  | 59 | 81 | 27 | 41 | 0  |\\n| 9M Ours      | 100| 0  | 100| 0  | 100| 0  | 99 | 5  | 51 | 0  | 58 | 96 | 42 |\\n| VIMA-Gato    | 59 | 0  | 41 | 50 | 38 | 47 | 59 | 9  | 58 | 80 | 44 | 24 | 2  |\\n| VIMA-Flamingo| 58 | 0  | 46 | 49 | 42 | 45 | 60 | 4  | 67 | 82 | 41 | 41 | 1  |\\n| VIMA-GPT     | 58 | 0  | 40 | 47 | 37 | 47 | 58 | 9  | 72 | 85 | 27 | 41 | 0  |\\n| 43M Ours     | 100| 0  | 100| 0  | 100| 0  | 99 | 5  | 51 | 0  | 69 | 99 | 40 |\\n| VIMA-Gato    | 57 | 0  | 59 | 57 | 43 | 50 | 56 | 5  | 68 | 83 | 47 | 24 | 2  |\\n| VIMA-Flamingo| 56 | 0  | 55 | 50 | 42 | 41 | 58 | 6  | 62 | 83 | 44 | 38 | 1  |\\n| VIMA-GPT     | 58 | 0  | 50 | 44 | 41 | 48 | 61 | 7  | 72 | 85 | 27 | 41 | 0  |\\n| 92M Ours     | 100| 0  | 100| 0  | 99 | 0  | 100| 0  | 15 | 0  | 86 | 98 | 40 |\\n| VIMA-Gato    | 76 | 0  | 90 | 56 | 44 | 48 | 68 | 14 | 64 | 89 | 64 | 85 | 43 |\\n| VIMA-Flamingo| 56 | 0  | 66 | 50 | 41 | 48 | 56 | 6  | 70 | 87 | 41 | 38 | 2  |\\n| VIMA-GPT     | 57 | 0  | 69 | 53 | 42 | 47 | 55 | 4  | 67 | 81 | 45 | 33 | 1  |\\n| 200M Ours    | 100| 0  | 100| 0  | 99 | 0  | 100| 0  | 18 | 0  | 77 | 97 | 76 |\\n| VIMA-Gato    | 79 | 0  | 92 | 56 | 45 | 54 | 74 | 18 | 61 | 88 | 64 | 83 | 33 |\\n| VIMA-Flamingo| 56 | 0  | 65 | 50 | 41 | 48 | 56 | 3  | 70 | 87 | 41 | 38 | 2  |\\n| VIMA-GPT     | 62 | 0  | 58 | 53 | 45 | 51 | 61 | 8  | 65 | 87 | 46 | 33 | 1  |\"}"}
{"id": "jiang23b", "page_num": 46, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 17: L2 level generalization results. Model indicates robot controller parameter count. Integers in the first row refer to indices of tasks described in Appendix, Sec. B.\\n\\n| Model     | Method | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 09 | 11 | 12 | 15 | 16 | 17 |\\n|-----------|--------|----|----|----|----|----|----|----|----|----|----|----|----|----|\\n| 2M        | Ours   | 100 | 0  | 100 | 0  | 100 | 0  | 95 | 5  | 37 | 5  | 100 | 0  | 17 | 5  | 87 | 5  | 67 | 0  | 97 | 5  | 46 | 0  | 54 | 5  |\\n| VIMA-Gato |        | 49.5| 49.0| 23.0| 17.5| 5.0| 47.5| 46.5| 5.5| 50.0|     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| VIMA-Flamingo |      | 45.5| 46.0| 56.0| 39.5| 35.5| 49.0| 47.0| 9.0| 53.0| 80.0| 43.0|     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| VIMA-GPT  |        | 51.0| 45.5| 9.5 | 7.0 | 0.5 | 45.5| 45.0| 0.0| 65.0| 81.5| 32.0|     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| 9M        | Ours   | 100 | 0  | 100 | 0  | 100 | 0  | 99 | 5  | 44 | 5  | 99 | 5  | 100 | 0  | 14 | 5  | 89 | 5  | 91 | 5  | 89 | 5  | 67 | 0  | 95 | 5  | 43 | 0  | 52 | 5  |\\n| VIMA-Gato |        | 44.5| 52.0| 9.0 | 39.0| 28.0| 49.5| 48.5| 2.0| 64.0| 86.5| 44.5| 2.0 |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| VIMA-Flamingo |      | 49.5| 50.5| 51.0| 48.0| 43.0| 50.5| 53.5| 5.5| 81.5| 82.5| 48.5| 1.0 |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| VIMA-GPT  |        | 50.5| 49.5| 16.5| 25.5| 12.0| 41.0| 47.0| 4.0| 63.0| 79.0| 28.5| 39.0| 0.0 |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| 43M       | Ours   | 100 | 0  | 100 | 0  | 100 | 0  | 49 | 5  | 100 | 0  | 100 | 0  | 100 | 0  | 19 | 5  | 80 | 5  | 19 | 5  | 80 | 5  | 19 | 5  | 80 | 5  | 19 | 5  | 80 | 5  |\\n| VIMA-Gato |        | 47.0| 44.5| 39.5| 46.5| 37.5| 48.5| 52.5| 5.5| 59.0| 83.0| 51.5| 1.0 |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| VIMA-Flamingo |      | 48.0| 47.5| 49.0| 52.5| 42.0| 47.5| 48.5| 8.5| 66.0| 81.5| 45.5| 42 | 0.5 |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| VIMA-GPT  |        | 48.5| 47.0| 43.5| 47.0| 37.0| 47.5| 45.5| 10.5| 74.5| 85 | 80.0| 33.0| 1.0 |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| 92M       | Ours   | 100 | 0  | 100 | 0  | 99 | 0  | 54 | 5  | 100 | 0  | 100 | 0  | 100 | 0  | 19 | 5  | 81 | 5  | 19 | 5  | 81 | 5  | 19 | 5  | 81 | 5  | 19 | 5  | 81 | 5  |\\n| VIMA-Gato |        | 64.5| 51.5| 53.0| 57.5| 42.5| 47.0| 51.0| 8.5| 67.0| 83 | 63.5| 32.0| 0.5 |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| VIMA-Flamingo |      | 48.0| 46.5| 52.0| 51.5| 43.5| 45.0| 51.5| 5.0| 68.0| 81.5| 47.5| 37.0| 0.5 |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| VIMA-GPT  |        | 50.5| 49.0| 64.5| 53.5| 40.0| 46.5| 48.5| 8.5| 68.0| 82.0| 50.0| 40.0| 1.5 |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| 200M      | Ours   | 100 | 0  | 100 | 0  | 99 | 5  | 57 | 5  | 99 | 5  | 100 | 0  | 100 | 0  | 17 | 5  | 77 | 5  | 17 | 5  | 77 | 5  | 17 | 5  | 77 | 5  | 17 | 5  | 77 | 5  |\\n| VIMA-Gato |        | 56.5| 53.5| 88.0| 55.5| 43.5| 55.5| 53.0| 14.0| 63.0| 90.5| 81.5| 33.0| 4.0 |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| VIMA-Flamingo |      | 51.0| 52.5| 61.5| 49.5| 38.5| 47.5| 55.5| 4.5| 67.0| 84.0| 52.5| 44 | 1.5 |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| VIMA-GPT  |        | 52.0| 52.0| 49.5| 54.5| 45.5| 52.5| 51.0| 11.0| 76.5| 84.0| 43.0| 38.0| 3.0 |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\"}"}
{"id": "jiang23b", "page_num": 47, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 18: L3 level generalization results. Model indicates robot controller parameter count. Integers in the first row refer to indices of tasks described in Appendix, Sec. B.\\n\\n| Model     | Method | 01  | 02  | 03  | 04  | 05  | 06  | 07  | 09  | 11  | 15  | 16  | 17  |\\n|-----------|--------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\\n| 2M Ours   |        | 100 | 100 | 100 | 98  | 34  | 100 | 99  | 99  | 17  | 97  | 94  | 48  |\\n| VIMA-Gato |        | 45.5| 48.0| 28.0| 23.0| 3.0 | 45.5| 45.0| 2.5 | 40.5| 29.5| 37.0| 1.0 |\\n| VIMA-Flamingo |       | 41.5| 54.5| 50.5| 39.5| 29.0| 45.0| 49.5| 5.5 | 57.5| 22.5| 25.0| 0.0 |\\n| VIMA-GPT  |        | 48.5| 50.0| 5.0 | 7.0 | 2.5 | 47.0| 45.5| 2.0 | 69.5| 22.5| 5.0 | 0.0 |\\n| 4M Ours   |        | 99.5| 100 | 100 | 98  | 55  | 99  | 99  | 12  | 88  | 88  | 98  | 47  |\\n| VIMA-Gato |        | 44.5| 55.0| 9.5 | 37.5| 24.5| 47.0| 50.0| 3.5 | 60.0| 30.5| 23.0| 0.5 |\\n| VIMA-Flamingo |      | 46.0| 53.5| 59.0| 49.5| 35.5| 47.5| 48.0| 7.0 | 87.5| 30.5| 39.5| 0.0 |\\n| VIMA-GPT  |        | 44.0| 47.0| 14.5| 22.0| 9.0 | 39.5| 40.0| 2.0 | 62.0| 28.5| 43.0| 1.0 |\\n| 9M Ours   |        | 99.0| 100 | 100 | 98  | 44  | 99  | 99  | 18  | 88  | 88  | 98  | 48  |\\n| VIMA-Gato |        | 44.5| 53.5| 42.5| 52.0| 28.0| 46.5| 51.5| 6.0 | 67.0| 35.0| 23.0| 0.5 |\\n| VIMA-Flamingo |      | 44.5| 53.0| 53.0| 48.5| 33.0| 41.0| 45.5| 8.0 | 72.5| 27.0| 44.5| 0.5 |\\n| VIMA-GPT  |        | 49.0| 50.5| 39.0| 46.5| 30.5| 43.0| 52.0| 6.5 | 84.0| 31.5| 35.0| 0.5 |\\n| 43M Ours  |        | 99.0| 99.5| 97.5| 99  | 43  | 13.0| 100 | 99  | 13.0| 82  | 99  | 44  |\\n| VIMA-Gato |        | 46.5| 55.0| 44.5| 57.0| 31.5| 47.5| 51.5| 2.5 | 72.5| 44  | 44  | 0.0 |\\n| VIMA-Flamingo |      | 47.0| 54.5| 53.0| 55.0| 36.0| 42.5| 48.0| 6.5 | 70.0| 33.0| 41.5| 0.0 |\\n| VIMA-GPT  |        | 47.5| 57.0| 61.0| 50.0| 34.5| 48.0| 53.5| 8.0 | 92  | 32.5| 43.5| 1.5 |\\n| 92M Ours  |        | 99.0| 99.5| 97.5| 99  | 58  | 13.0| 100 | 99  | 13.0| 82  | 99  | 44  |\\n| VIMA-Gato |        | 61.5| 54.0| 73.0| 56.0| 36.0| 50.0| 48.0| 17.0| 66.5| 44.0| 41.5| 0.0 |\\n| VIMA-Flamingo |      | 51.0| 51.5| 68.0| 51.5| 36.5| 50.5| 47.0| 6.0 | 69.5| 28.0| 45  | 0.5 |\\n| VIMA-GPT  |        | 50.0| 56.5| 63.0| 52.5| 32.0| 49.5| 53.0| 5.0 | 78.0| 34.5| 37.5| 0.0 |\\n| 200M Ours |        | 99.0| 100 | 100 | 97  | 54  | 17  | 100 | 99  | 17  | 90  | 97  | 46  |\\n| VIMA-Gato |        | 51.0| 58.0| 84.5| 56.5| 35.5| 53.5| 49.0| 15.0| 65.0| 52.0| 33.0| 0.0 |\\n| VIMA-Flamingo |      | 49.0| 50.0| 66.5| 47.0| 35.0| 47.5| 50.0| 4.0 | 66.0| 28.0| 45  | 0.5 |\\n| VIMA-GPT  |        | 52.0| 51.0| 55.0| 49.5| 40.0| 46.0| 50.5| 5.0 | 82.0| 37.0| 38.0| 1.5 |\"}"}
{"id": "jiang23b", "page_num": 48, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | Method  | 08  | 10  | 13  | 14  |\\n|---------------|---------|-----|-----|-----|-----|\\n| 2M Ours       | 6.5     | 0   | 0   | 96.5|     |\\n| VIMA-Gato     | 21.0    | 0   | 2   | 34.0|     |\\n| VIMA-Flamingo | 22.0    | 0   | 2   | 27.5|     |\\n| VIMA-GPT      | 22.5    | 0   | 2   | 22.0|     |\\n| 4M Ours       | 97.0    | 0   | 0   | 99.0|     |\\n| VIMA-Gato     | 17.0    | 2   | 0   | 34.0|     |\\n| VIMA-Flamingo | 17.0    | 0.5 | 0   | 29.0|     |\\n| VIMA-GPT      | 19.0    | 0   | 2   | 22.5|     |\\n| 9M Ours       | 92.0    | 0   | 0   | 96.0|     |\\n| VIMA-Gato     | 18.0    | 0   | 0   | 31.0|     |\\n| VIMA-Flamingo | 21.5    | 0   | 0   | 21.5|     |\\n| VIMA-GPT      | 20.5    | 0   | 0   | 30.5|     |\\n| 20M Ours      | 100.0   | 0   | 0   | 95.5|     |\\n| VIMA-Gato     | 20.5    | 0   | 0   | 29.0|     |\\n| VIMA-Flamingo | 21.0    | 0   | 0   | 27.5|     |\\n| VIMA-GPT      | 20.5    | 0   | 0   | 36.0|     |\\n| 43M Ours      | 99.0    | 0   | 0   | 97.0|     |\\n| VIMA-Gato     | 21.0    | 0   | 0   | 30.5|     |\\n| VIMA-Flamingo | 18.5    | 0   | 0   | 24.5|     |\\n| VIMA-GPT      | 17.5    | 0   | 0   | 30.0|     |\\n| 92M Ours      | 100.0   | 0   | 0   | 98.5|     |\\n| VIMA-Gato     | 22.0    | 0   | 0   | 32.0|     |\\n| VIMA-Flamingo | 19.5    | 0   | 0   | 25.0|     |\\n| VIMA-GPT      | 18.5    | 0   | 0   | 29.5|     |\\n| 200M Ours     | 100.0   | 0   | 0   | 94.5|     |\\n| VIMA-Gato     | 30.5    | 0   | 0   | 37.0|     |\\n| VIMA-Flamingo | 24.5    | 0   | 0   | 24.0|     |\\n| VIMA-GPT      | 20.0    | 0   | 0   | 28.5|     |\"}"}
{"id": "jiang23b", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VIMA: Robot Manipulation with Multimodal Prompts\\n\\nSimilar to prior work (Zeng et al., 2020; Shridhar et al., 2021), VIMA-BENCH provides scripted oracles to generate successful demonstrations for all tasks. We leverage them to construct an offline imitation dataset for behavioral cloning. Given a prompt, these programmed bots can access privileged information, such as the correct object to pick and target location to place.\\n\\nFigure A.2: Texture Gallery in VIMA-BENCH. The first row of image-based textures is from Blender Cloud Libraries (Weikert et al., 2022), while others are hard-coded.\"}"}
{"id": "jiang23b", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Task Suite\\n\\nWe develop 17 task templates that belong to 6 diverse categories. Thousands of individual task instances and their corresponding multimodal prompts can be procedurally generated from these task templates. We use PyBullet (Coumans & Bai, 2016\u20132021) as our backend and the default renderer to produce the RGB frames for training data and interactive test environments. For demonstration purpose, we apply the NVISII (Morrical et al., 2020) ray tracing to enhance the visual quality. We elaborate on each task in the following subsections.\\n\\nB.1. Simple Object Manipulation\\n\\nThis task category asks agents to follow basic instructions specified by multimodal prompts.\\n\\nTask 01: Pick the specified object(s) and place it (them) into the specified container.\\n\\n- Prompt: Put the \\\\{object\\\\}_1 into the \\\\{object\\\\}_2.\\n- Description: The image placeholder \\\\{object\\\\}_1 is the object to be picked and the \\\\{object\\\\}_2 is the container object. The agent requires to recognize the objects with the correct color-shape combinations. To extend the difficulties, it supports more than one object to be picked or placed. For example, the prompt \\\"Put the \\\\{object\\\\}_1 and \\\\{object\\\\}_2 into the \\\\{object\\\\}_3\\\" asks to pick two different objects and place into a target container. We uniformly sample different color-shape combos for objects to be picked and containers.\\n- Success Criteria: All specified object(s) to pick are within the bounds of the container object(s), with specified shapes and textures provided in the prompt.\\n- Oracle Trajectory: Shown in Fig. A.3 with its multimodal prompt.\\n\\nTask 02: In the workspace, put the objects with a specified texture shown in the scene image in the prompt into container object(s) with a specified color. This task requires the agent to find the correct object to manipulate by grounding the textural attributes from both natural language descriptions and the visual scene images.\\n\\n- Prompt: Put the \\\\{texture\\\\}_1 object in \\\\{scene\\\\} into the \\\\{texture\\\\}_2 object.\\n- Description: The text placeholder \\\\{texture\\\\}_1 and \\\\{texture\\\\}_2 are sampled textures for objects to be picked and the container objects, respectively. The number of dragged objects with the same texture can be varied. \\\\{scene\\\\} is the workspace-like image placeholder. There is a designated number of distractors with different textures (and potentially different shapes) in the scene. For each distractor in the workspace, it has 50% chance to be either dragged or container distractor object with different textures from those specified in the prompt.\\n- Success Criteria: All objects in the workspace with \\\\{texture\\\\}_1 are within the bounds of the container object with \\\\{texture\\\\}_2.\"}"}
{"id": "jiang23b", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VIMA: Robot Manipulation with Multimodal Prompts\\n\\nTask 02:\\nPut the green and blue stripe object into the yellow paisley object.\\n\\nFigure A.4: Simple Object Manipulation: Task 02\\n\\nTask 03:\\nRotate objects clockwise by certain degrees along $z$-axis. Only rotationally asymmetric objects are considered in this task.\\n\\n\u2022 Prompt: Rotate the $\\\\{\\\\text{object}\\\\}$ $1\\\\{\\\\text{angles}\\\\}$ degrees.\\n\\n\u2022 Description: The agent is required to rotate all objects in the workspace specified by the image placeholder $\\\\{\\\\text{object}\\\\}$. There are also objects with different color-shape combinations in the workspace as distractors. $\\\\{\\\\text{angles}\\\\}$ is the sampled degree that needs to be rotated. A target angle is sampled from $30^\\\\circ$, $60^\\\\circ$, $90^\\\\circ$, $120^\\\\circ$, and $150^\\\\circ$.\\n\\n\u2022 Success Criteria: The position of the specified object matches its original position, and the orientation matches the orientation after rotating specific angles.\\n\\n\u2022 Oracle Trajectory: Shown in Fig. A.5 with its multimodal prompt.\\n\\nFigure A.5: Simple Object Manipulation: Task 03\"}"}
{"id": "jiang23b", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.2. Visual Goal Reaching\\n\\nThis task category requires agents to manipulate objects in the workspace to reach goal states represented as images shown in prompts.\\n\\nTask 04: Rearrange target objects in the workspace to match goal configuration shown in prompts. Note that to achieve the goal configuration, distractors may need to be moved away first.\\n\\n- **Prompt:** Rearrange to this {scene}.\\n- **Description:** Objects in the scene placeholder {scene} are target objects to be manipulated and rearranged. In the workspace, the same target objects are spawned randomly, potentially with distractors randomly spawned as well. With a pre-defined distractor conflict rate, the position of each distractor has this probability to occupy the position of any target object such that the rearrangement can only succeed if moving away that distractor first.\\n- **Success Criteria:** The configuration of target objects in the workspace matches that specified in the prompt.\\n- **Oracle Trajectory:** Shown in Fig. A.6 with its multimodal prompt.\\n\\nTask 05: Extend the task 04 by requiring the agent to restore rearranged objects to the initial setup after the \\\"rearranging\\\" phase.\\n\\n- **Prompt:** Rearrange objects to this setup {scene} and then restore.\\n- **Description:** Same as the task 04, except introducing the instruction \\\"restore\\\".\\n- **Success Criteria:** Meet the success criteria of the task 04, and then within the allowed max steps restore all target objects to their initial configurations.\\n- **Oracle Trajectory:** Shown in Fig. A.7 with its multimodal prompt.\"}"}
{"id": "jiang23b", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C. Model Architecture\\nIn this section, we provide comprehensive details about VIMA model architecture as well as other adapted baseline methods. We implement all models in PyTorch (Paszke et al., 2019) and adapt Transformer-related implementation from Wolf et al. (2019).\\n\\nC.1. Summary of Different Methods\\nWe summarize differences between VIMA and other baseline variants in Table 1. In the column \u201cPrompt Conditioning\u201d, an alternative to cross-attention is to first concatenate prompt and interaction into a big sequence, then repetitively apply transformer decoders to predict actions. It is referred to as \u201cDirect modeling\u201d. The relative computation cost is quadratically proportional to the number of observation tokens.\\n\\n| Method               | Visual Tokenization          | Prompt Conditioning | Number of Observation Tokens per Step |\\n|----------------------|------------------------------|--------------------|---------------------------------------|\\n| Ours                 | Object tokens consisting of cropped images and bounding boxes | Cross-attention    | Equal to number of objects, typically 3 to 8 |\\n| VIMA-Gato (Reed et al., 2022) | Image patch tokens encoded by a ViT | Direct modeling    | Equal to number of image patches, 16  |\\n| VIMA-Flamingo (Alayrac et al., 2022) | Image patch tokens encoded by a ViT, further downsampled by a Perceiver module | Cross-attention    | Equal to number of learned query vectors, 4  |\\n| VIMA-GPT (Brown et al., 2020) | Single image token encoded by a ViT | Direct modeling    | Single visual feature, 1               |\\n\\nC.2. VIMA Architecture\\nC.2.1. MULTIMODAL PROMPT TOKENIZATION\\nAs introduced in Section 4, there are 3 types of input formats in multimodal prompts, namely (1) text inputs, (2) images of full scenes, and (3) images of single objects.\\n\\nFor text inputs, we follow the standard pipeline in NLP to first tokenize raw language to discrete indices through pre-trained t5-base tokenizer. We then obtain corresponding word tokens from the embedding look-up of the pre-trained t5-base model. For images of full scenes, we first parse the scene through a fine-tuned Mask R-CNN detection model (He et al., 2017; Wu et al., 2019) to extract individual objects. Each object representation contains a bounding box and a cropped image. The bounding box is in the format of $x_{center}$, $y_{center}$, $height$, $width$. We normalize it to be within $[0, 1]$ by dividing each dimension with corresponding upper-bound value. We then pass it through a bounding box encoder MLP and obtain a feature vector. To process the cropped image, we first pad non-square image to a square by padding along the shorter dimension. We then resize it to a pre-configured size and pass it through a ViT (trained from scratch) to obtain the image feature. Finally, an object token is obtained by concatenating the bounding box feature and the image feature and mapping to the embedding dimension. For images of single objects, we obtain tokens in the same way except with a dummy bounding box. Detailed model hyperparameters about tokenization are listed in Table 2.\\n\\nAfter obtaining a sequence of prompt tokens, we follow Tsimpoukelli et al. (2021) to pass it through a pre-trained t5-base encoder to obtain encoded prompt. Note that we add adapter MLP between object tokens and the T5 encoder. To prevent catastrophic forgetting, VIMA only fine-tunes the last two layers of the language encoder with layer-wise learning rate decay (He et al., 2021) but freezes all other layers. We adopt learned absolute positional embedding. Model hyperparameters are listed in Table 2 as well.\\n\\nC.2.2. OBSERVATION ENCODING\\nSince all RGB observations are images of full scenes, we follow the same procedure discussed above to obtain flattened object tokens. Because we provide RGBs from two views (frontal and top-down), we order object tokens by following the\"}"}
{"id": "jiang23b", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Model hyperparameters for multimodal prompt tokenization.\\n\\n| Hyperparameter          | Value               |\\n|-------------------------|---------------------|\\n| Tokenization            |                     |\\n| Tokenizer               | t5-base tokenizer   |\\n| Embedding Dimension     | 768                 |\\n| Image Tokenization      |                     |\\n| ViT Input Image Size    | 32 x 32             |\\n| ViT Patch Size          | 16                  |\\n| ViT Width               | 768                 |\\n| ViT Layer               | 4                   |\\n| ViT Number of Heads     | 24                  |\\n| Bounding Box MLP        |                     |\\n| Hidden Dimension        | 768                 |\\n| Hidden Depth            | 2                   |\\n| Prompt Encoding         |                     |\\n| Pre-Trained LM          | t5-base             |\\n| Unfrozen Last Layers    | N                   |\\n| Positional Embedding    | Absolute            |\\n| Token Adapter MLP Depth | 2                   |\\n\\nOrder of frontal, top-down. We one-hot encode the state of the end effector. We then concatenate object tokens with the end-effector state and transform to observation tokens. We adopt learned absolute positional embedding. Detailed model hyperparameters about observation encoding is provided in Table 3.\\n\\nTable 3: Model hyperparameters for observation encoding.\\n\\n| Hyperparameter          | Value               |\\n|-------------------------|---------------------|\\n| Observation Token Dimension | 768                  |\\n| End Effector Embedding Dimension | 2                   |\\n| Positional Embedding    | Absolute            |\\n\\nC.2.3. ACTION ENCODING\\n\\nSince our model is conditioned on observation-action interleaved history, we also tokenize past actions. We follow common practice in Chen et al. (2021); Zheng et al. (2022) to encode past actions with a two-layer MLP. It has a hidden dimension of 256. We then map outputs to token dimension and obtain action tokens.\\n\\nC.2.4. SEQUENCE MODELING\\n\\nThe robot controller in VIMA is a causal decoder that autoregressively predicts actions. To condition the decoder on prompt tokens, we perform cross-attention between history tokens and prompt tokens (Figure 3). Concretely, we pass history tokens as the query sequence and prompt tokens as the key-value sequence into cross-attention blocks. The output prompt-aware trajectory tokens then go through causal self-attention blocks. We alternate cross-attention and self-attention L times. This procedure is technically described in Pseudocode 1.\"}"}
{"id": "jiang23b", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def xattn_sequence_modeling(\\nprompt_tokens, # the [L, d] prompt tokens (L=prompt length)\\nobs_tokens, # the [T, d] obs tokens (T=time step)\\nact_tokens, # the [T-1, d] action tokens\\ntraj_pos_embd, # learned positional embedding for trajectory\\nprompt_pos_embd, # learned positional embedding for prompt\\n):\\n# interleave obs and action tokens\\ntraj_tokens = interleave(obs_tokens, act_tokens) # [2T-1, d]\\n# add positional embedding to trajectory tokens\\nx = traj_tokens + traj_pos_embd\\n# add positional embedding to prompt tokens\\nprompt_tokens = prompt_tokens + prompt_pos_embd\\n# apply xattn and causal self-attn\\nfor i in range(num_layers):\\n    # cross-attention\\n    x = x + attn_i(q=x, kv=prompt_tokens)\\n    # feed forward\\n    x = x + ffw_xattn_i(x)\\n    # self-attention\\n    x = x + causal_attn_i(q=x, kv=x)\\n    # feed forward\\n    x = x + ffw_i(x)\\n# the last token is the predicted action token\\npredicted_act_token = x[-1]\\nreturn predicted_act_token\\n\\nPseudocode 1: Cross-attention operation that conditions the trajectory history on prompt. We repetitively alternate cross-attention and self-attention to model the trajectory given a specific task.\\n\\nC.2.5. ACTION DECODING\\n\\nAfter obtaining the predicted action token, we map it to the action space $A$ and obtain the predicted action. This is achieved though a group of action heads. Since the action space consists of two poses, for each pose we use six independent heads to decode discrete actions (two for xy coordinate and four for rotation represented in quaternion). These discrete actions are then integrated and mapped to continuous actions through affine transformation. The two poses are modeled independently. Early ablations show that this independent modeling is equally good as alternative techniques, such as autoregressive decoding (Vinyals et al., 2019; OpenAI et al., 2019). Detailed model hyperparameters are listed in Table 4.\\n\\n| Hyperparameter       | Value |\\n|----------------------|-------|\\n| Hidden Dimension     | 512   |\\n| Hidden Depth         | 2     |\\n| Activation           | ReLU  |\\n| X-Axis Discrete Bins | 50    |\\n| Y-Axis Discrete Bins | 100   |\\n| Rotation Discrete Bins | 50   |\\n\\nC.3. Baselines Architectures\\n\\nIn this section, we elaborate model architectures for adapted baseline methods. Some components such as the action decoder are same across all models. Therefore, we only discuss unique model components.\"}"}
{"id": "jiang23b", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.3.1. VIMA-G\\n\\nGato (Reed et al., 2022) introduces a decoder-only model that solves tasks from multiple domains including robotics, video game, image captioning, language modeling, etc. Different tasks are specified by supplying the model with an initial sequence of corresponding tokens. For example, in tasks involving decision making, these tokens include observation and action tokens. For fair comparison, we provide the same conditioning as VIMA, i.e., our multimodal tokenized prompts. This adapted baseline variant is referred to as \\\"VIMA-Gato\\\". Similar to our method, VIMA-Gato also predicts actions in an autoregressive manner. VIMA-Gato and our method share the same training philosophy to only optimize the causal behavior cloning objective. However, unlike our method that adopts an object-centric representation to treat individual objects as observation tokens, VIMA-Gato divides input images into patches and encodes them by a ViT (Dosovitskiy et al., 2020) to produce observation tokens. Furthermore, VIMA-Gato relies on causal self-attention to model entire trajectory sequences starting with prompt tokens. Hyperparameters of VIMA-Gato's ViT is listed in Table 5. The transformer-decoder style sequence modeling is technically illustrated in Pseudocode 2.\\n\\nTable 5: Model hyperparameters for ViT used in baseline methods.\\n\\n| Hyperparameter | Value |\\n|----------------|-------|\\n| Image Size     | 64 \\\\times 128 |\\n| Patch Size     | 32    |\\n| ViT Width      | 768   |\\n| ViT Layers     | 4     |\\n| ViT Heads      | 24    |\\n\\n```python\\ndef causal_sequence_modeling(prompt_tokens, # the \\\\([L, d]\\\\) prompt tokens (L=prompt length)\\nsep_token, # the \\\\([1, d]\\\\) learned token to separate prompt and trajectory history\\nobs_tokens, # the \\\\([T, d]\\\\) obs tokens (T=time step)\\nact_tokens, # the \\\\([T-1, d]\\\\) action tokens\\npos_embd, # learned positional embedding):\\n    # interleave obs and action tokens\\n    traj_tokens = interleave(obs_tokens, act_tokens) # \\\\([2T-1, d]\\\\)\\n    # assemble input tokens\\n    x = concat([prompt_tokens, sep_token, traj_tokens])\\n    x = x + pos_embd\\n    # apply GPT layers with causal mask\\n    for i in range(num_layers):\\n        # self-attention\\n        x = x + causal_attn_i(q=x, kv=x)\\n        # feed forward\\n        x = x + ffw_i(x)\\n    # the last token is the predicted action token\\n    predicted_act_token = x[-1]\\n    return predicted_act_token\\n```\\n\\nPseudocode 2: Plain sequence modeling that directly concatenates prompt and trajectory history and repetitively perform causal self-attention operation.\\n\\nC.3.2. VIMA-F\\n\\nLAMINGO Flamingo (Alayrac et al., 2022) is a vision-language model that learns to generate textual completion in response to multimodal prompts. It embeds a variable number of prompt images into a fixed number of tokens via the Perceiver Resampler module (Jaegle et al., 2021b), and conditions the language decoder on encoded prompts by cross-attention. Flamingo does not work with embodied agents out of the box. We adapt it by replacing the output layer with robot action heads (hyperparameters listed in Table 4) and using tokenized rollout histories as inputs. We thus call it \\\"VIMA-Flamingo\\\".\"}"}
{"id": "jiang23b", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VIMA: Robot Manipulation with Multimodal Prompts\\n\\nBiT (Kolesnikov et al., 2020), and MuST (Ghiasi et al., 2021) pre-train shared backbone models at scale for general visual representations and transfer them to downstream tasks. In multimodal learning, Perceiver (Jaegle et al., 2021b;a) proposes an efficient architecture to handle structured inputs and outputs. Flamingo (Alayrac et al., 2022) and Frozen (Tsimpoukelli et al., 2021) design a universal API that ingests interleaving sequences of images and text and generates free-form text. Gato (Reed et al., 2022) is a massively multi-task model across NLP, vision, and embodied agents. Our work is most similar in spirit to Gato, but we focus primarily on enabling an intuitive multimodal prompting interface for a generalist robot agent.\\n\\nFoundation Models for Embodied Agents.\\n\\nFoundation models (Bommasani et al., 2021) have demonstrated strong emergent properties. There are many ongoing efforts to replicate this success for embodied agents (Yang et al., 2023), focusing on 3 aspects. 1) Transformer agent architecture: Decision Transformer and Trajectory Transformer (Chen et al., 2021; Janner et al., 2021; Zheng et al., 2022; Xu et al., 2022; 2023) leverage the powerful self-attention models for sequential decision making. CLI-Port (Shridhar et al., 2021), Perceiver-Actor (Shridhar et al., 2022), and RT-1 (Brohan et al., 2022) apply large transformers to robot manipulation tasks. BeT (Shafiullah et al., 2022) and C-BeT (Cui et al., 2022) design novel techniques to learn from demonstrations with multiple modes with transformers. 2) Pre-training for better representations: MaskViT (Gupta et al., 2022b), R3M (Nair et al., 2022), VIP (Ma et al., 2022), and VC-1 (Majumdar et al., 2023) pre-train general visual representations for robotic perception. Li et al. (2022b) fine-tunes from LLM checkpoints to accelerate policy learning. MineDojo (Fan et al., 2022) and Ego4D (Grauman et al., 2021) provide large-scale multimodal databases to facilitate scalable policy training. 3) LLMs for robot learning: SayCan (Ahn et al., 2022) leverages PaLM (Chowdhery et al., 2022) for zero-shot concept grounding. Huang et al. (2022a), Inner Monologue (Huang et al., 2022b) and LM-Nav (Shah et al., 2022) apply LLMs to long-horizon robot planning. PaLM-E (Driess et al., 2023) is instead a multimodal language model that can be repurposed for sequential robotic manipulation planning. Ours differs from these works in our novel multimodal prompting formulation, which existing LLMs do not easily support.\\n\\nRobot Manipulation and Benchmarks.\\n\\nA wide range of robot manipulation tasks require different skills and task specification formats, such as instruction following (Stepputtis et al., 2020), one-shot imitation (Finn et al., 2017; Duan et al., 2017), rearrangement (Batra et al., 2020), constraint satisfaction (Brunke et al., 2021a), and reasoning (Shridhar et al., 2020). Multiple physics simulation benchmarks are introduced to study the above tasks. For example, iGibson (Shen et al., 2020; Li et al., 2021; Srivastava et al., 2021; Li et al., 2022a) simulates interactive household scenarios. Ravens (Zeng et al., 2020) and Robosuite (Zhu et al., 2020; Fan et al., 2021) design various tabletop manipulation tasks with realistic robot arms. CALVIN (Mees et al., 2021) develops long-horizon language-conditioned tasks. Meta-World (Yu et al., 2019) is a widely used simulator benchmark studying robotics manipulation with tabletop settings. CausalWorld (Ahmed et al., 2021) is a benchmark for causal structure and transfer learning in manipulation, requiring long-horizon planning and precise low-level motor control. AI2-THOR (Ehsani et al., 2021; Deitke et al., 2022) is a framework that supports visual object manipulation and procedural generation of environments. Our VIMA-B Bench is the first robot learning benchmark to support multimodal-prompted tasks. We also standardize the evaluation protocol to systematically measure an agent\u2019s generalization capabilities.\\n\\nAn extended review can be found in Appendix, Sec. F.\\n\\n7. Conclusion\\n\\nIn this work, we introduce a novel multimodal prompting formulation that converts diverse robot manipulation tasks into a uniform sequence modeling problem. We instantiate this formulation in VIMA-B Bench, a diverse benchmark with multimodal tasks and systematic evaluation protocols for generalization. We propose VIMA, a conceptually simple transformer-based agent capable of solving tasks such as visual goal reaching, one-shot video imitation, and novel concept grounding with a single model. Through comprehensive experiments, we show that VIMA exhibits strong model scalability and zero-shot generalization. Therefore, we recommend our agent design as a solid starting point for future work.\\n\\nAcknowledgement\\n\\nWe are extremely grateful to Shyamal Buch, Jonathan Tremblay, Ajay Mandlekar, Chris Choy, De-An Huang, Silvio Savarese, Fei Xia, Josiah Wong, Abhishek Joshi, Soroush Nasiriany, and many other colleagues and friends for their helpful feedback and insightful discussions. We also thank the anonymous reviewers for offering us highly constructive advice and kind encouragement during the review period. NVIDIA provides the necessary computing resource and infrastructure for this project. This work is done during Yunfan Jiang and Guanzhi Wang\u2019s internships at NVIDIA. Guanzhi Wang is supported by the Kortschak fellowship in Computing and Mathematical Sciences at Caltech.\"}"}
{"id": "jiang23b", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abramson, J., Ahuja, A., Barr, I., Brussee, A., Carnevale, F., Cassin, M., Chhaparia, R., Clark, S., Damoc, B., Dudzik, A., Georgiev, P., Guy, A., Harley, T., Hill, F., Hung, A., Kenton, Z., Landon, J., Lillicrap, T., Mathewson, K., Mokr\u0101, S., Muldal, A., Santoro, A., Savinov, N., Varma, V., Wayne, G., Williams, D., Wong, N., Yan, C., and Zhu, R. Imitating interactive intelligence. arXiv preprint arXiv:Arxiv-2012.05672, 2020.\\n\\nAceituno, B., Rodriguez, A., Tulsiani, S., Gupta, A., and Mukadam, M. A differentiable recipe for learning visual non-prehensile planar manipulation. In 5th Annual Conference on Robot Learning, 2021. URL https://openreview.net/forum?id=f7KaqYLO3iE.\\n\\nAgrawal, P. The task specification problem. In Faust, A., Hsu, D., and Neumann, G. (eds.), Proceedings of the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning Research, pp. 1745\u20131751. PMLR, 08-11 Nov 2022. URL https://proceedings.mlr.press/v164/agrawal22a.html.\\n\\nAhmed, O., Tr\u00e4uble, F., Goyal, A., Neitz, A., Wuthrich, M., Bengio, Y., Sch\u00f6lkopf, B., and Bauer, S. Causal-world: A robotic manipulation benchmark for causal structure and transfer learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=SK7A5pdrgov.\\n\\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., and Yan, M. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:Arxiv-2204.01691, 2022.\\n\\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., and Simonyan, K. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:Arxiv-2204.14198, 2022.\\n\\nBaker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., and Clune, J. Video pretraining (vpt): Learning to act by watching unlabeled online videos. arXiv preprint arXiv:Arxiv-2206.11795, 2022.\\n\\nBatra, D., Chang, A. X., Chernova, S., Davison, A. J., Deng, J., Koltun, V., Levine, S., Malik, J., Mordatch, I., Motlaghi, R., Savva, M., and Su, H. Rearrangement: A challenge for embodied ai. arXiv preprint arXiv:Arxiv-2011.01975, 2020.\\n\\nBerscheid, L., Mei\u00dfner, P., and Kr\u00f6ger, T. Self-supervised learning for precise pick-and-place without object model. arXiv preprint arXiv:Arxiv-2006.08373, 2020.\\n\\nBharadhwaj, H., Kumar, A., Rhinehart, N., Levine, S., Shkurti, F., and Garg, A. Conservative safety critics for exploration. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=iaO86DUuKi.\\n\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-lut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X. L., Li, X., Ma, T., Malik, A., Manning, C. D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park, J. S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., R\u00e9e, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A. W., Tram\u00e8r, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, Y., Zheng, L., Zhou, K., and Liang, P. On the opportunities and risks of foundation models. arXiv preprint arXiv:Arxiv-2108.07258, 2021.\\n\\nBrohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jackson, T., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Leal, I., Lee, K.-H., Levine, S., Lu, Y., Malla, R. 10\"}"}
{"id": "jiang23b", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VIMA: Robot Manipulation with Multimodal Prompts\\n\\nU., Manjunath, D., Mordatch, I., Nachum, O., Parada, C., Peralta, J., Perez, E., Pertsch, K., Quiambao, J., Rao, K., Ryoo, M., Salazar, G., Sanketi, P., Sayed, K., Singh, J., Sontakke, S., Stone, A., Tan, C., Tran, H., Vanhoucke, V., Vega, S., Vuong, Q., Xia, F., Xiao, T., Xu, P., Xu, S., Yu, T., and Zitkovich, B.\\n\\nRt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv: Arxiv-2212.06817, 2022.\\n\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.\\n\\nLanguage models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, volume 33, pp. 1877\u20131901, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\\n\\nBrunke, L., Greeff, M., Hall, A. W., Yuan, Z., Zhou, S., Panerati, J., and Schoellig, A. P.\\n\\nSafe learning in robotics: From learning-based control to safe reinforcement learning. arXiv preprint arXiv: Arxiv-2108.06266, 2021a.\\n\\nBrunke, L., Greeff, M., Hall, A. W., Yuan, Z., Zhou, S., Panerati, J., and Schoellig, A. P.\\n\\nSafe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning, December 2021b. URL http://arxiv.org/abs/2108.06266. arXiv:2108.06266 [cs, eess].\\n\\nBuch, S., Eyzaguirre, C., Gaidon, A., Wu, J., Fei-Fei, L., and Niebles, J. C.\\n\\nRevisiting the \u201cvideo\u201d in video-language understanding. CVPR, 2022.\\n\\nBucker, A., Figueredo, L., Haddadin, S., Kapoor, A., Ma, S., Vemprala, S., and Bonatti, R.\\n\\nLatte: Language trajectory transformer. arXiv preprint arXiv: Arxiv-2208.02918, 2022.\\n\\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I.\\n\\nDecision transformer: Reinforcement learning via sequence modeling. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 15084\u201315097, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/7f489f642a0ddb10272b5c31057f0663-Abstract.html.\\n\\nChen, T., Saxena, S., Li, L., Fleet, D. J., and Hinton, G. E.\\n\\nPix2seq: A language modeling framework for object detection. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022a. URL https://openreview.net/forum?id=e42KbIw6Wb.\\n\\nChen, T., Saxena, S., Li, L., Lin, T.-Y., Fleet, D. J., and Hinton, G. A unified sequence interface for vision tasks. arXiv preprint arXiv: Arxiv-2206.07669, 2022b.\\n\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellet, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N.\\n\\nPalm: Scaling language modeling with pathways. arXiv preprint arXiv: Arxiv-2204.02311, 2022.\\n\\nCollins, J., Chand, S., Vanderkop, A., and Howard, D.\\n\\nA review of physics simulators for robotic applications. IEEE Access, 9:51416\u201351431, 2021.\\n\\nCoumans, E. and Bai, Y.\\n\\nPybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016\u20132021.\\n\\nCui, Z. J., Wang, Y., Shafiullah, N. M. M., and Pinto, L.\\n\\nFrom play to policy: Conditional behavior generation from uncurated robot data. arXiv preprint arXiv: Arxiv-2210.10047, 2022.\\n\\nDasari, S. and Gupta, A.\\n\\nTransformers for one-shot visual imitation. In Kober, J., Ramos, F., and Tomlin, C. J. (eds.), 4th Conference on Robot Learning, CoRL 2020, 16-18 November 2020, Virtual Event / Cambridge, MA, USA, volume 155 of Proceedings of Machine Learning Research, pp. 2071\u20132084. PMLR, 2020. URL https://proceedings.mlr.press/v155/dasari21a.html.\"}"}
{"id": "jiang23b", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VIMA: Robot Manipulation with Multimodal Prompts\\n\\nLarge-scale multi-robot learning. In Kaelbling, L. P., Kragic, D., and Sugiura, K. (eds.), 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pp. 885\u2013897. PMLR, 2019. URL http://proceedings.mlr.press/v100/dasari20a.html.\\n\\nDeitke, M., VanderBilt, E., Herrasti, A., Weihs, L., Salvador, J., Ehsani, K., Han, W., Kolve, E., Farhadi, A., Kembhavi, A., and Mottaghi, R. Procthor: Large-scale embodied AI using procedural generation. arXiv preprint arXiv:2206.06994, 2022.\\n\\nDevin, C., Rowghanian, P., Vigorito, C., Richards, W., and Rohanimanesh, K. Self-supervised goal-conditioned pick and place. arXiv preprint arXiv:2008.11466, 2020.\\n\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\nDowns, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K., McHugh, T. B., and Vanhoucke, V. Google scanned objects: A high-quality dataset of 3D scanned household items. arXiv preprint arXiv:2204.11918, 2022.\\n\\nDriess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.\\n\\nDuan, J., Yu, S., Tan, H. L., Zhu, H., and Tan, C. A survey of embodied AI: from simulators to research tasks. IEEE Trans. Emerg. Top. Comput. Intell., 6(2):230\u2013244, 2022. doi: 10.1109/TETCI.2022.3141105. URL https://doi.org/10.1109/TETCI.2022.3141105.\\n\\nDuan, Y., Andrychowicz, M., Stadie, B. C., Ho, J., Schneider, J., Sutskever, I., Abbeel, P., and Zaremba, W. One-shot imitation learning. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1087\u20131098, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ba3866600c3540f67c1e9575e213be0a-Abstract.html.\\n\\nEhsani, K., Han, W., Herrasti, A., VanderBilt, E., Weihs, L., Kolve, E., Kembhavi, A., and Mottaghi, R. Manipulathor: A framework for visual object manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4497\u20134506, June 2021.\\n\\nFan, L., Zhu, Y., Zhu, J., Liu, Z., Zeng, O., Gupta, A., Creusco, J., Savarese, S., and Fei-Fei, L. Surreal: Open-source reinforcement learning framework and robot manipulation benchmark. In Billard, A., Dragan, A., Peters, J., and Morimoto, J. (eds.), Proceedings of The 2nd Conference on Robot Learning, volume 87 of Proceedings of Machine Learning Research, pp. 767\u2013782. PMLR, 29-31 Oct 2018. URL https://proceedings.mlr.press/v87/fan18a.html.\\n\\nFan, L., Zhu, Y., Zhu, J., Liu, Z., Zeng, O., Gupta, A., Creuscot, J., Savarese, S., and Fei-Fei, L. Surreal-system: Fully-integrated stack for distributed deep reinforcement learning. arXiv preprint arXiv:1909.12989, 2019.\\n\\nFan, L., Wang, G., Huang, D., Yu, Z., Fei-Fei, L., Zhu, Y., and Anandkumar, A. SECANT: self-expert cloning for zero-shot generalization of visual policies. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 3088\u20133099. PMLR, 2021. URL http://proceedings.mlr.press/v139/fan21c.html.\\n\\nFan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y., and Anandkumar, A. Minedojo: Building open-ended embodied agents with internet-scale knowledge. arXiv preprint arXiv:2206.08853, 2022.\\n\\nFinn, C., Yu, T., Zhang, T., Abbeel, P., and Levine, S. One-shot visual imitation learning via meta-learning. arXiv preprint arXiv:1709.04905, 2017.\\n\\nFlorence, P., Manuelli, L., and Tedrake, R. Self-supervised correspondence in visuomotor policy learning. arXiv preprint arXiv:1909.06933, 2019.\\n\\nFu, T.-J., Li, L., Gan, Z., Lin, K., Wang, W. Y., Wang, L., and Liu, Z. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681, 2021.\\n\\nGan, C., Zhou, S., Schwartz, J., Alter, S., Bhandwaldar, A., Gutfreund, D., Yamins, D. L. K., DiCarlo, J. J., McDermott, J., Torralba, A., and Tenenbaum, J. B. The threedworld transport challenge: A visually guided task-and-motion planning benchmark for physically realistic embodied AI. arXiv preprint arXiv:2103.14025, 2021.\"}"}
{"id": "jiang23b", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VIMA: Robot Manipulation with Multimodal Prompts\\nGhiasi, G., Zoph, B., Cubuk, E. D., Le, Q. V., and Lin, T.\\nMulti-task self-training for learning general representations. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pp. 8836\u20138845. IEEE, 2021. doi: 10.1109/ICCV48922.2021.00873. URL https://doi.org/10.1109/ICCV48922.2021.00873.\\n\\nGrauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., Martin, M., Nagarajan, T., Radosavovic, I., Ramakrishnan, S. K., Ryan, F., Sharma, J., Wray, M., Xu, M., Xu, E. Z., Zhao, C., Bansal, S., Batra, D., Cartillier, V., Crane, S., Do, T., Doulaty, M., Erapalli, A., Feichtenhofer, C., Fragomeni, A., Fu, Q., Gebreselasie, A., Gonzalez, C., Hillis, J., Huang, X., Huang, Y., Jia, W., Khoo, W., Kolar, J., Kottur, S., Kumar, A., Landini, F., Li, C., Li, Y., Li, Z., Mangalam, K., Modhugu, R., Munro, J., Murrell, T., Nishiyasu, T., Price, W., Puentes, P. R., Ramazanova, M., Sari, L., Somasundaram, K., Southerland, A., Sugano, Y., Tao, R., Vo, M., Wang, Y., Wu, X., Yagi, T., Zhao, Z., Zhu, Y., Arbelaez, P., Crandall, D., Damen, D., Farinella, G. M., Fuegen, C., Ghanem, B., Ithapu, V. K., Jawahar, C. V., Joo, H., Kitani, K., Li, H., Newcombe, R., Oliva, A., Park, H. S., Rehg, J. M., Sato, Y., Shi, J., Shou, M. Z., Torralba, A., Torresani, L., Yan, M., and Malik, J. Ego4d: Around the world in 3,000 hours of egocentric video. arXiv preprint arXiv: Arxiv-2110.07058, 2021.\\n\\nGupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. In Kaelbling, L. P., Kragic, D., and Sugiura, K. (eds.), 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pp. 1025\u20131037. PMLR, 2019. URL http://proceedings.mlr.press/v100/gupta20a.html.\\n\\nGupta, A., Fan, L., Ganguli, S., and Fei-Fei, L. Meta-morph: Learning universal controllers with transformers. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum?id=Opmqtk_GvYL.\\n\\nGupta, A., Tian, S., Zhang, Y., Wu, J., Martin-Mart\u00edn, R., and Fei-Fei, L. Maskvit: Masked visual pre-training for video prediction. arXiv preprint arXiv: Arxiv-2206.11894, 2022b.\\n\\nHansen, N., Yuan, Z., Ze, Y., Mu, T., Rajeswaran, A., Su, H., Xu, H., and Wang, X. On pre-training for visuo-motor control: Revisiting a learning-from-scratch baseline. arXiv preprint arXiv: Arxiv-2212.05749, 2022.\\n\\nHe, K., Gkioxari, G., Doll\u00e1r, P., and Girshick, R. Mask r-cnn. arXiv preprint arXiv: Arxiv-1703.06870, 2017.\\n\\nHe, K., Chen, X., Xie, S., Li, Y., Doll\u00e1r, P., and Girshick, R. Masked autoencoders are scalable vision learners. arXiv preprint arXiv: Arxiv-2111.06377, 2021.\\n\\nHeibeck, T. H. and Markman, E. M. Word learning in children: An examination of fast mapping. Child development, pp. 1021\u20131034, 1987.\\n\\nHermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R., Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg, M., Teplyashin, D., Wainwright, M., Apps, C., Hassabis, D., and Blunsom, P. Grounded language learning in a simulated 3d world. arXiv preprint arXiv: Arxiv-1706.06551, 2017.\\n\\nHill, F., Tieleman, O., von Glehn, T., Wong, N., Merzic, H., and Clark, S. Grounded language learning fast and slow. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=wpSWuz_hyqA.\\n\\nHuang, D., Xu, D., Zhu, Y., Garg, A., Savarese, S., Fei-Fei, L., and Niebles, J. C. Continuous relaxation of symbolic planner for one-shot imitation learning. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2019, Macau, SAR, China, November 3-8, 2019, pp. 2635\u20132642. IEEE, 2019. doi: 10.1109/IROS40897.2019.8967761. URL https://doi.org/10.1109/IROS40897.2019.8967761.\\n\\nHuang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In Chaudhuri, K., Jegelka, S., Song, L., Szepesv\u00e1ri, C., Niu, G., and Sabato, S. (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 9118\u20139147. PMLR, 2022a. URL https://proceedings.mlr.press/v162/huang22a.html.\\n\\nHuang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Brown, N., Jackson, T., Luu, L., Levine, S., Hausman, K., and Ichter, B. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv: Arxiv-2207.05608, 2022b.\\n\\nJaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ding, D., Koppula, S., Zoran, D., Brock, A., Shelhamer, E., H\u00e9naff, O., Botvinick, M. M., Zisserman, A., Vinyals, O., and Carreira, J. Perceiver io: A general architecture for structured inputs & outputs. arXiv preprint arXiv: Arxiv-2107.14795, 2021a.\"}"}
{"id": "jiang23b", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "jiang23b", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VIMA: Robot Manipulation with Multimodal Prompts\\n\\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\\n\\nLu, J., Goswami, V., Rohrbach, M., Parikh, D., and Lee, S. 12-in-1: Multi-task vision and language representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 10434\u201310443. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01045. URL https://openaccess.thecvf.com/content_CVPR_2020/html/Lu_12-in-1_Multi-Task_Vision_and_Language_Representation_Learning_CVPR_2020_paper.html.\\n\\nLu, J., Clark, C., Zellers, R., Mottaghi, R., and Kembhavi, A. Unified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv: Arxiv-2206.08916, 2022.\\n\\nLynch, C. and Sermanet, P. Language conditioned imitation learning over unstructured data. In Shell, D. A., Toussaint, M., and Hsieh, M. A. (eds.), Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021, 2021. doi: 10.15607/RSS.2021.XVII.047. URL https://doi.org/10.15607/RSS.2021.XVII.047.\\n\\nMa, Y. J., Sodhani, S., Jayaraman, D., Bastani, O., Kumar, V., and Zhang, A. Vip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint arXiv: Arxiv-2210.00030, 2022.\\n\\nMajumdar, A., Yadav, K., Arnaud, S., Ma, Y. J., Chen, C., Silwal, S., Jain, A., Berges, V.-P., Abbeel, P., Malik, J., Batra, D., Lin, Y., Maksymets, O., Rajeswaran, A., and Meier, F. Where are we in the search for an artificial visual cortex for embodied intelligence? arXiv preprint arXiv: Arxiv-2303.18240, 2023.\\n\\nMandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R., Fei-Fei, L., Savarese, S., Zhu, Y., and Mart\u00edn-Mart\u00edn, R. What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv: Arxiv-2108.03298, 2021.\\n\\nMcCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv: Arxiv-1806.08730, 2018.\\n\\nMees, O., Hermann, L., Rosete-Beas, E., and Burgard, W. Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. arXiv preprint arXiv: Arxiv-2112.03227, 2021.\\n\\nMinderer, M., Gritsenko, A., Stone, A., Neumann, M., Weisensenborn, D., Dosovitskiy, A., Mahendran, A., Arnab, A., Dehghani, M., Shen, Z., Wang, X., Zhai, X., Kipf, T., and Houlsby, N. Simple open-vocabulary object detection with vision transformers. arXiv preprint arXiv: Arxiv-2205.06230, 2022.\\n\\nMittal, M., Yu, C., Yu, Q., Liu, J., Rudin, N., Hoeller, D., Yuan, J. L., Tehrani, P. P., Singh, R., Guo, Y., Mazhar, H., Mandlekar, A., Babich, B., State, G., Hutter, M., and Garg, A. Orbit: A unified simulation framework for interactive robot learning environments. arXiv preprint arXiv: Arxiv-2301.04195, 2023.\\n\\nMorrical, N., Tremblay, J., Birchfield, S., and Wald, I. NVISII: Nvidia scene imaging interface, 2020. https://github.com/owl-project/NVISII/.\\n\\nNair, S., Rajeswaran, A., Kumar, V., Finn, C., and Gupta, A. R3m: A universal visual representation for robot manipulation. arXiv preprint arXiv: Arxiv-2203.12601, 2022.\\n\\nOpenAI, Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., J\u00f3zefowicz, R., Gray, S., Olsson, C., Pachocki, J., Petrov, M., d. O., Pinto, H. P., Raiman, J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J., Wolski, F., and Zhang, S. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv: Arxiv-1912.06680, 2019.\\n\\nPaine, T. L., Colmenarejo, S. G., Wang, Z., Reed, S., Aytar, Y., Pfaff, T., Hoffman, M. W., Barth-Maron, G., Cabi, S., Budden, D., and de Freitas, N. One-shot high-fidelity imitation: Training large-scale deep nets with rl. arXiv preprint arXiv: Arxiv-1810.05017, 2018.\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Wallach, H., Larochelle, H., Beygelzimer, A., d\u2019Alch\u00b4e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 8024\u20138035. Curran Associates, Inc., 2019.\\n\\nPuig, X., Ra, K., Boben, M., Li, J., Wang, T., Fidler, S., and Torralba, A. Virtualhome: Simulating household activities via programs. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 8494\u20138502. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.00886. URL http://openaccess.thecvf.com/content_15\"}"}
{"id": "jiang23b", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "jiang23b", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.3. Novel Concept Grounding\\n\\nThis task category requires agents to ground new concepts of adjectives, nouns, or verbs via visual perception and language understanding. Similar task design can be found in prior work (Hill et al., 2021). Completing these tasks are challenging, because the model should a) first understand prompts with interleaved texts, images, and even video frames; b) quickly internalize new concepts that are different across task instances, which even tests the ability to meta-learn; and c) do complicated reasoning such as comparing between \u201ctaller\u201d vs \u201cless taller\u201d vs \u201cshorter\u201d and then ground this reasoning into the robot action space.\\n\\nPrompts consist of two parts: a definition part followed by an instruction part. In the definition part, novel concepts are defined by multimodal illustrations with multiple support examples. In the instruction part, agents are asked to achieve the goal by properly applying concepts from the definition part. The assignment of dummy object names is varied and independent for each task instance such that tasks can only be solved if the agent applies the reasoning correctly. This ability is also referred to as fast-mapping (Heibeck & Markman, 1987).\\n\\nTask 06: Ground comparative adjectives by comparing the size or the textural saturation of objects and manipulating the correct object(s) instructed in the prompt.\\n\\n\u2022 Prompt:\\n  \\n  \\\\{\\n  \\\\text{demo object}\\\\}\\n  1\\n  is\\n  \\\\{\\n  \\\\text{novel adj}\\\\}\\n  than\\n  \\\\{\\n  \\\\text{demo object}\\\\}\\n  2.\\n  Put the\\n  \\\\{\\n  \\\\text{adv}\\\\}\\n  \\\\{\\n  \\\\text{novel adj}\\\\} \\\\{object\\\\}\\n  1\\n  into the\\n  \\\\{object\\\\}\\n  2.\\n\\n\u2022 Description:\\n  The sampled adjective \\\\{novel adj\\\\} is a dummy adjective placeholder for agent to ground. By default, the novel adjective set is \\\\{daxer, blicker, modier, kobar\\\\}. The real meaning can be related to size (smaller/larger) or textural saturation (lighter/darker texture). The image placeholders \\\\{demo object\\\\} 1 and \\\\{demo object\\\\} 2 illustrate how the novel adjective is defined. For example, if the real comparison is \u201ctaller\u201d, then the sampled object in \\\\{demo object\\\\} 1 is taller than \\\\{demo object\\\\} 2. The choices of the novel adjective and the real meaning are independently sampled for different task instances. For the instruction part, this task is similar to task 01, where the agent is required to pick the specified object(s) with the novel adjective attribute and then place it into the specified container object. To avoid revealing the correct object to manipulate, we use a neutral texture for objects appeared in the instruction part.\\n\\n\u2022 Success Criteria:\\n  All target objects with the specified adjective attribute are within the bounds of the specified container object.\\n\\n\u2022 Oracle Trajectory:\\n  Shown in Fig. A.8 with its multimodal prompt.\"}"}
{"id": "jiang23b", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Task 07:\\n\\n- **Prompt:** This is a \\\\{novel name\\\\} \\\\{object\\\\}. This is a \\\\{novel name\\\\} \\\\{object\\\\}. Put \\\\{novel name\\\\} \\\\{object\\\\}.\\n\\n- **Description:** Novel noun words are defined with the text placeholders \\\\{novel name\\\\} and \\\\{novel name\\\\}, following their image placeholders \\\\{object\\\\} and \\\\{object\\\\}, for the target object and container object, respectively. Novel nouns are sampled from \\\\{dax, blicket, wug, zup\\\\}. In the instruction part, objects are expressed as novel nouns defined in the previous definition part. Distractors are defined the same as task 01.\\n\\n- **Success Criteria:** All target object(s) are within the bounds of the container object(s).\\n\\n- **Oracle Trajectory:** Shown in Fig. A.9 with its multimodal prompt.\\n\\nTask 08:\\n\\n- **Prompt:** This is a \\\\{novel name\\\\} \\\\{object\\\\}. This is a \\\\{novel name\\\\} \\\\{object\\\\}. \\\\{demo object\\\\} is \\\\{adj\\\\} than \\\\{demo object\\\\}. Put the \\\\{adv\\\\} \\\\{novel adj\\\\} \\\\{novel name\\\\} into the \\\\{novel name\\\\}.\\n\\n- **Description:** See task description for task 06 and task 07.\\n\\n- **Success Criteria:** Similar as tasks 06 and 07.\\n\\n- **Oracle Trajectory:** Shown in Fig. A.10 with its multimodal prompt.\"}"}
{"id": "jiang23b", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure A.10: Novel Concept Grounding: Task 08\\n\\nTask 09:\\nA novel verb \\\"twist\\\" is defined as rotating a specific angle illustrated by several examples. This task is similar to task 03, but it requires the agent to infer what is the exact angle to rotate from the prompt and to ground novel verbs that are semantically similar but different in exact definitions.\\n\\n\u2022 Prompt:\\n\\\"Twist\\\" is defined as rotating object a specific angle. For examples: From \\\\{before twist\\\\} to \\\\{after twist\\\\}.\\nNow twist all \\\\{texture\\\\} objects.\\n\\n\u2022 Description:\\nBoth \\\\{before twist\\\\} and \\\\{after twist\\\\} are scene placeholders where \\\\{before twist\\\\} shows a randomly sampled object before \\\"twisting\\\" and \\\\{after twist\\\\} shows the same object pose after \\\"twisting\\\".\\nAll examples illustrate the same sampled angle to rotate. In the workspace, the target objects have the texture specified by \\\\{texture\\\\} and randomly sampled shapes.\\n\\n\u2022 Success Criteria:\\nSame as the task 03.\\n\\n\u2022 Oracle Trajectory:\\nShown in Fig. A.11 with its multimodal prompt.\"}"}
{"id": "jiang23b", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.4. One-Shot Video Imitation\\n\\nThis task category requires agents to imitate motions demonstrated through videos shown in prompts. We follow prior works (Finn et al., 2017; Dasari & Gupta, 2020; Duan et al., 2017) to formulate the problem by giving one video demonstration (represented as key frames in prompts), then test the learned imitator\u2019s ability to produce target trajectories.\\n\\nThis setup is challenging because:\\n1. only one demonstration is available to the agent;\\n2. the model needs to understand video frames interleaved with textual instructions;\\n3. missing correspondences between demonstrations and target trajectories since demonstrations only show partial key frames.\\n\\nTask 10:\\nFollow motions for specific objects.\\n\\n\u2022 Prompt: Follow this motion for \\\\{object\\\\}: \\\\{frame\\\\}_1...\\\\{frame\\\\}_i...\\\\{frame\\\\}_n.\\n\\n\u2022 Description: Image placeholder \\\\{object\\\\} is the target object to be manipulated and \\\\{\\\\{frame\\\\}_i\\\\} is set of workspace-like scene placeholders to represent a video trajectory, where \\\\(n\\\\) is the trajectory length. There is an object spawned at the center in both the workspace and the prompt video but with different textures as a distractor. The initial position of the target object matches that in \\\\{frame\\\\}_1.\\n\\n\u2022 Success Criteria: In each step, the pose of the target object matches the pose in the corresponding video frame. Incorrect manipulation sequences are considered as failures.\\n\\n\u2022 Oracle Trajectory: Shown in Fig. A.12 with its multimodal prompt.\\n\\nTask 11:\\nStack objects with the order illustrated in the prompt video.\\n\\n\u2022 Prompt: Stack objects in this order \\\\{frame\\\\}_1...\\\\{frame\\\\}_i...\\\\{frame\\\\}_n.\\n\\n\u2022 Description: There are multiple objects with the same shape but different textures spawned in the workspace without any stacking initially. Distractor objects with different shapes are spawned in the workspace but not in the prompt video. At each step of the prompt video, one object is stacked over another or put at an empty position.\\n\\n\u2022 Success Criteria: Similar as task 10.\\n\\n\u2022 Oracle Trajectory: Shown in Fig. A.13 with its multimodal prompt.\\n\\nB.5. Visual Constraint Satisfaction\\n\\nThis task category requires agents to wipe a specific number of objects in the workspace to a goal region while also satisfy the given visual constraint.\"}"}
{"id": "jiang23b", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Scaling model and data.\\n\\nTop: We compare performance of different methods with model sizes ranging from 2M to 200M parameters. Across all model sizes and generalization levels, VIMA outperforms baseline variants.\\n\\nBottom: For a fixed model size of 92M parameters we compare the effect of imitation learning dataset size with 0%, 1%, 10%, and full data. VIMA is extremely sample efficient and can achieve performance comparable to other methods with 10 \u00d7 less data.\\n\\nTokenization.\\n\\nThere are 3 formats of raw input in the prompt \u2014 text, image of a single object, and image of a full tabletop scene (e.g., for Rearrangement or imitation from video frames). For text inputs, we use pre-trained T5 tokenizer and word embedding to obtain word tokens. For images of full scenes, we first extract individual objects using domain fine-tuned Mask R-CNN (He et al., 2017) (Appendix, Sec. C.4). Each object is represented as a bounding box and a cropped image. We then compute object tokens by encoding them with a bounding box encoder and a ViT (Dosovitskiy et al., 2020), respectively. Since Mask R-CNN is imperfect, the bounding boxes can be noisy and the cropped images may have irrelevant pixels. For images of single objects, we obtain tokens in the same way except with a dummy bounding box. Prompt tokenization produces a sequence of interleaved textual and visual tokens. We then follow the practice in Tsimpoukelli et al. (2021) and encode the prompt via a pre-trained T5 encoder (Raffel et al., 2020). Since T5 has been pre-trained on large text corpora, VIMA inherits the semantic understanding capability and robustness properties. To accommodate tokens from new modalities, we insert MLPs between non-textual tokens and T5.\\n\\nRobot Controller.\\n\\nA challenging aspect of designing a multi-task policy is to select a suitable conditioning mechanism. In our schema (Fig. 3), the robot controller (decoder) is conditioned on the prompt sequence $P$ by a series of cross-attention layers between $P$ and the trajectory history sequence $H$. We compute key $K_P$ and value $V_P$ sequences from the prompt and query $Q_H$ from the trajectory history, following the encoder-decoder convention in Raffel et al. (2020). Each cross-attention layer then generates an output sequence $H' = \\\\text{softmax}(Q_H K_P \\\\sqrt{d}) V_P$, where $d$ is the embedding dimension. Residual connections are added to connect higher layers with the input rollout trajectory sequence. The cross-attention design enjoys three advantages: 1) strengthened connection to prompt; 2) intact and deep flow of the original prompt tokens; and 3) better computational efficiency. VIMA decoder consists of $L$ alternating cross-attention and self-attention layers. Finally, we follow common practice (Baker et al., 2022) to map predicted action tokens to discretized poses of the robot arm. See Appendix, Sec. C.2 for more details.\\n\\nTraining.\\n\\nWe follow behavioral cloning to train our models by minimizing the negative log-likelihood of predicted actions. Concretely, for a trajectory with $T$ steps, we optimize $\\\\min_{\\\\theta} \\\\sum_{t=1}^{T} - \\\\log \\\\pi_{\\\\theta}(a_t | P, H)$. The entire training is conducted on an offline dataset with no simulator access. To make VIMA robust to detection inaccuracies and failures, we apply object augmentation by randomly injecting false-positive detection outputs. After training, we select model checkpoints for evaluation based on the aggregated accuracy on a held-out validation set. The evaluation involves interacting with the physics simulator. We follow the\"}"}
{"id": "jiang23b", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"5. Experiments\\n\\nIn this section, we aim to answer three main questions:\\n\\n1. What is the best recipe for building multi-task transformer-based robot agents with multimodal prompts?\\n2. What are the scaling properties of our approach in model capacity and data size?\\n3. How do different components, such as visual tokenizers, prompt conditioning, and prompt encoding, affect robot performance?\\n\\n5.1. Baselines\\n\\nBecause there is no prior method that works out of the box with our multimodal prompting setup, we make our best effort to select a number of representative transformer-based agent architectures as baselines, and re-interpret them to be compatible with VIMA-Bench:\\n\\n**Gato** (Reed et al., 2022) introduces a decoder-only model that solves tasks from multiple domains where tasks are specified by prompting the model with the observation and action subsequence. For a fair comparison, we provide the same conditioning as VIMA, i.e., our multimodal encoded prompts. Input images are divided into patches and encoded by a ViT model to produce observation tokens. This variant is referred to as \\\"VIMA-Gato\\\".\\n\\n**Flamingo** (Alayrac et al., 2022) is a vision-language model that learns to generate textual completion in response to multimodal prompts. It embeds a variable number of prompt images into a fixed number of tokens via Perceiver (Jaegle et al., 2021b), and conditions the language decoder on the encoded prompt by cross-attention. Flamingo does not work with embodied agents out of the box. We adapt it to support decision-making by replacing the output layer with robot action heads. We denote the method as \\\"VIMA-Flamingo\\\".\\n\\n**VIMA-GPT** is a decoder-only architecture conditioned on tokenized multimodal prompts. It autoregressively decodes the next actions given instructions and interaction histories. Similar to prior work (Chen et al., 2021; Janner et al., 2021), it encodes an image into a single state token by a ViT encoder and prepends the rollout trajectory with prompt tokens. This baseline does not use cross-attention.\\n\\nA more detailed comparison between these variants can be found in Appendix, Sec. C.1.\\n\\n![Performance Drop](image)\\n\\nFigure 5: VIMA incurs much less performance drop than baselines as we evaluate on progressively harder settings.\\n\\n5.2. Evaluation Results\\n\\nWe compare VIMA against the baseline variants on four levels of generalization provided in our benchmark for different model and training dataset sizes. Our empirical results demonstrate that VIMA's choice of object tokens combined with cross-attention conditioning is the most effective recipe among the model designs we consider.\\n\\n**Model Scaling.** We train all methods for a spectrum of model capacities from 2M to 200M parameters, evenly spaced on the log scale (Fig. 4). The encoder size is kept constant (T5-Base, 111M) for all methods and excluded from the parameter count. Across all levels of zero-shot generalization, we find that VIMA strongly outperforms other alternatives. Although models like VIMA-Gato and VIMA-Flamingo show improved performance with bigger model sizes, VIMA consistently achieves superior performance over all model sizes. We note that this can only be achieved with both cross-attention and object token sequence representations \u2014 altering any component will significantly degrade the performance, especially in the low model capacity regime (ablations in Sec. 5.3).\\n\\n**Data Scaling.** Next we investigate how different methods scale with varying dataset sizes. We compare model performance at 0.1%, 1%, and 10% and full imitation learning dataset provided in VIMA-Bench (Fig. 4). Note that to ensure all methods are fairly pre-trained on the same amount of data, we initialize baseline variants that directly learn from raw pixels with MVP pre-trained ViT (Xiao et al., 2022; Radosavovic et al., 2022). It is further MAE fine-tuned (He et al., 2021), using the same in-domain data as for the Mask R-CNN object detector. See Appendix, Sec. E.3 for detailed setup. VIMA is extremely sample efficient and, with just 1% of the data, can achieve performance similar to baseline methods trained with 10\u00d7 more data on L1 and L2 levels of generalization. In fact, for L4 we find that with 6\u00d7 more data on L4.\"}"}
{"id": "jiang23b", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VIMA: Robot Manipulation with Multimodal Prompts\\n\\nFigure 6: Ablation on visual tokenizers. We compare the performance of VIMA-200M model across different visual tokenizers. Our proposed object tokens outperform all methods that learn directly from raw pixels, and Object Perceiver that downsamples the object sequence to a fixed number of tokens. Just 1% of training data, VIMA already surpasses other variants trained with entire dataset. Finally, across all levels with just 10% of the data, VIMA can outperform other architectures trained with the full dataset by a significant margin. We hypothesize that the data efficiency can be attributed to the object-centric representation employed in the VIMA recipe, which is less prone to overfitting than learning directly from pixels in the low-data regime. This is consistent with findings from Sax et al. (2018), which demonstrates that embodied agents conditioned on mid-level visual representations tend to be significantly more sample-efficient than end-to-end control from raw pixels.\\n\\nProgressive Generalization. Finally, we compare the relative performance degradation as we test the models on progressively challenging zero-shot evaluation levels without further fine-tuning (Fig. 5). Our method exhibits a minimal performance regression, especially between $L_1 \\\\rightarrow L_2$ and $L_1 \\\\rightarrow L_3$. In contrast, the baselines can degrade as much as 20%, particularly in more difficult generalization scenarios. Although all methods degrade significantly when evaluated on $L_4$ (Novel Tasks), the performance drop for VIMA is only half as severe as all other baselines. These results suggest that VIMA has developed a more generalizable policy and robust representations than the alternative approaches.\\n\\n5.3. Ablation Studies\\n\\nThrough extensive experiments, we ablate different design choices in VIMA and study their impact on robot decision making. We focus on four aspects: visual tokenization, prompt conditioning, prompt-encoding language models, and policy robustness against distractions and corruptions.\\n\\nVisual Tokenization. As explained in Sec. 4, VIMA processes the prompt and observation images into a variable number of object tokens with a domain fine-tuned Mask R-CNN implementation. How important is this particular choice of visual tokenizer? We study 5 different variants and empirically evaluate their 4 levels of generalization performance on VIMA-BENCH. 1) Ours (Oracle): instead of using Mask R-CNN, we directly read out the ground-truth bounding box from the simulator. In other words, we use a perfect object detector to estimate the upper bound on the performance of this study; 2) Object Perceiver: we apply a Perceiver module to convert the variable number of objects detected in each frame to a fixed number of tokens. Perceiver is more computationally efficient because it reduces the average sequence length; 3) Image Perceiver: the same architecture as the Perceiver Resampler in VIMA-Flamingo, which converts an image to a small, fixed number of tokens; 4) Image patches: following VIMA-Gato, we divide an RGB frame into square patches, and extract ViT embedding.\"}"}
{"id": "jiang23b", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7: Ablation on prompt conditioning. We compare our method (xattn: cross-attention prompt conditioning) with a vanilla transformer decoder (gpt-decoder) across different model sizes. Cross-attention is especially helpful in low-parameter regime and for harder generalization tasks.\\n\\nPrompt Conditioning.\\n\\nVIMA conditions the robot controller (decoder) on the encoded prompt by cross-attention. A simple alternative is to concatenate the prompt $P$ and interaction history $H$ into one big sequence, and then apply a decoder-only transformer like GPT (Radford et al., 2018) to predict actions. In this ablation, we keep the object tokenizer constant and only switch the conditioning mechanism to causal sequence modeling. Note that this variant is conceptually \\\"VIMA-Gato with object tokens\\\". Fig. 7 shows the comparison of VIMA (xattn) and the gpt-decoder variant across 4 generalization levels. While the variant achieves comparable performance in larger models, cross-attention still dominates in the small-capacity range and generalizes better in the most challenging L4 (Novel Task) setting. Our hypothesis is that cross-attention helps the controller stay better focused on the prompt instruction at each interaction step. This bears a resemblance to the empirical results in Sanh et al. (2021); Wang et al. (2022b), which show that well-tuned encoder-decoder architectures can outperform GPT-3 in zero-shot generalization.\\n\\nPrompt Encoding.\\n\\nWe vary the size of the pre-trained T5 encoder to study the effect of prompt encoding. We experiment with three T5 capacities: small (30M), base (111M), and large (368M). We further fix the parameter count of the decision-making part to be 200M. For all T5 variants, we fine-tune the last two layers and freeze all other layers. We find no significant difference among the variants (Appendix, Sec. E.4), thus we set base as default for all our models.\\n\\nPolicy Robustness.\\n\\nWe study the policy robustness against increasing number of distractors and corrupted task specifications, including incomplete prompts (randomly masking out words with <UNK> token) and corrupted prompts (randomly swapping words, which could have changed the task meaning altogether). See Appendix, Sec. E.5 for exact setup and results. VIMA exhibits minimal performance degradation with increased distractors and minor decrease with corrupted prompts. We attribute this robustness to the high-quality pre-trained T5 backbone.\\n\\n6. Related Work\\n\\nMulti-Task Learning by Sequence Modeling.\\n\\nTransformers (Vaswani et al., 2017) have enabled task unification across many AI domains (Brown et al., 2020; Chen et al., 2022a;b; Lu et al., 2022; Wang et al., 2022c). For example, in NLP, the Natural Language Decathlon (McCann et al., 2018) adopts a consistent question-answering format for a suite of 10 NLP tasks. T5 (Raffel et al., 2020) unifies all language problems into the same text-to-text format. GPT-3 (Brown et al., 2020) and Megatron (Shoeybi et al., 2019) demonstrate emergent behaviours of intuitive task specifications by zero-shot prompting. In computer vision, Pix2Seq (Chen et al., 2022b) casts many vision problems into a unified sequence format. Florence (Yuan et al., 2021),\"}"}
