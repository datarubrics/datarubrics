{"id": "jJ9BoXAfFa", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nLarge Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.\\n\\n1. Introduction\\n\\nLarge Language Models (LLMs) have emerged as a pivotal breakthrough in natural language processing (NLP). When augmented with action modules that allow access to APIs, their action space expands beyond conventional text processing, allowing LLMs to acquire capabilities such as tool invocation and memory management (Mialon et al., 2023; Schick et al., 2023) and venture into real-world tasks such as controlling robots (Ahn et al., 2022; Huang et al., 2023; Ma et al., 2023) and performing scientific experiments (Bran et al., 2023).\\n\\nWe inquire: how to effectively expand LLM agents' action space for solving complex real-world problems? Much existing research has examined using text (Yao et al., 2022b; Park et al., 2023, inter alia) or JSON (Qin et al., 2023b; Chase, 2022, inter alia) to produce actions (e.g., tool uses in Fig. 1 top left). However, both methods typically suffer from constrained scope of action spaces (actions are usually tailored for specific tasks) and restricted flexibility (e.g., inability to compose multiple tools in a single action). As an alternative approach, several work (Liang et al., 2022; Singh et al., 2023; Wang et al., 2023a) demonstrate the potential of using LLMs to generate code to control robots or game characters. However, they typically rely on pre-specified control primitives and hand-engineered prompts and, more importantly, struggle to dynamically adjust or emit actions based on new environmental observation and feedback.\\n\\nThis work proposes CodeAct, a general-purpose framework that allows LLMs to generate executable Python code as actions (Fig. 1 top right). CodeAct is designed to handle a variety of applications and comes with unique advantages: (1) Integrated with a Python interpreter, CodeAct can execute code actions and dynamically adjust prior actions or emit new actions based on observations (e.g., code execution results) it receives through multiple turns of interactions. (2) Code actions allow LLM to leverage existing software packages. CodeAct can use readily available Python packages for an expanded action space instead of hand-crafted task-specific tools (Yuan et al., 2023; Shen et al., 2023). It also allows LLM to use automated feedback (e.g., error messages) implemented in most software to improve task-solving by self-debugging its generated code.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Executable Code Actions Elicit Better LLM Agents\\n\\nFigure 1: Comparison between CodeAct and Text/JSON as action.\\n\\nIllustrative example comparing different actions.\\n\\nQuantitative results on M3 ToolEval (\u00a72.3).\\n\\n(3) Code data is widely used in pre-training today's LLMs (Yang et al., 2024b). These models are already familiar with structured programming languages, allowing cost-effective adoption of CodeAct.\\n\\n(4) Compared to JSON and text with a pre-defined format, code inherently supports control and data flow, allowing for the storage of intermediate results as variables for reuse and the composition of multiple tools to perform complex logical operations (e.g., if-statements, for-loops) with one piece of code, thereby unlocking LLMs' potential to tackle complex tasks by leveraging its pre-trained knowledge of programming. In Fig. 1, an LLM using with CodeAct (top right) can apply the same sequence of tools (e.g., passing one tool's output as input to another tool using the data flow feature) to all inputs through for-loops (i.e., control flow feature) with one action; while text or JSON have to take action for every input (top left).\\n\\nOur extensive experiments with 17 LLMs (including both open-source and proprietary ones) confirm the above benefits (3 & 4) of CodeAct. To demonstrate benefit (3), our first experiment (\u00a72.2) compares CodeAct to baselines on basic tasks involving atomic tool use (i.e., only one tool is used per action), ablating the control and data flow advantage offered by CodeAct. The results show that, for most LLMs, CodeAct achieves comparable or better performance than the baselines. CodeAct's performance gains are more prominent on complex tasks, as demonstrated in our second experiment (benefit 4). We curate a new benchmark consisting of 82 human-curated tasks that typically require multiple calls to multiple tools in multi-turn interactions (M3 ToolEval; \u00a72.3). Problems in this benchmark often require intricate coordination and composition of multiple tools. With its strengths in control and data flow, CodeAct achieves up to a 20% absolute improvement over baselines on the success rate of solving the problems while requiring up to 30% fewer actions. These performance gains widen as the capabilities of the LLMs increase (Fig. 1 bottom).\\n\\nThe promising performance of CodeAct motivates an open-source LLM agent that can effectively act through CodeAct, and collaborate with humans through natural language.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Executable Code Actions Elicit Better LLM Agents\\n\\nOn the other hand, CodeAct is a multi-turn interaction agent framework that allows dynamic adjustment of prior actions or emitting new actions by design (\u00a72.1, Fig. 2) and is compatible with any form of textual observation (e.g., tool execution output, automated feedback) from the environment. Beyond being compatible with environmental observation, our instruction tuning dataset CodeActInstruct specifically collects data for multi-turn self-improving, offering a practical solution to enhance LLM's multi-turn self-improving process.\\n\\nIn addition, previous approaches require heavy prompt engineering and crafting of few-shot demonstrations to tailor LLMs to a particular domain or task (e.g., robot control (Liang et al., 2022)) since the backbone LLMs are not specially optimized for dynamic planning and decision making. In contrast, in this work, we propose the CodeAct framework that uses executable Python code to consolidate LLM agents' actions into unified action space and collect CodeActInstruct on a diverse array of tasks (e.g., information seeking, tabular reasoning, robot planning, etc) to make the trained model, CodeActAgent, easily scale to diverse tasks and domains with minimal human efforts as shown in \u00a73.2.\\n\\nOne notable exception among prior work is Voyager (Wang et al., 2023a), which performs iterative prompting in a constrained action space of function definitions to fix code errors. Different from CodeAct, such setting disallows dynamic re-adjustment of atomic actions on the fly: In CodeAct, for a particular task (e.g., craft stone sword in Minecraft), the agent can first execute one line of code (any atomic action or composed functions, e.g., move forward, locate stone), and dynamically produce different actions based on the observation of the first action. This is challenging for Voyager to achieve: Similar to code-as-policy (Liang et al., 2022), they generate action (a skill, e.g., craft stone sword) as a Python function definition that outlines the entire plan for a task (e.g., multi-step code outlining how you should craft a stone sword and handles for different potential cases, which requires strong domain knowledge). This imposes significant constraints on the agent's action space and disallows dynamic re-adjustment of atomic actions on the fly: That is, the agent can only generate one complete function first (e.g., by imaging all possible cases that might happen when you try to locate stones), execute the entire function, observe the feedback, and update the entire function as action in the subsequent move. Besides the constrained ability to re-adjust action from environmental observation, they also rely on heavy prompting engineering (a typical drawback discussed above) to provide relevant information (e.g., current state, additional self-critics via prompting) to generate revised code, whereas CodeAct is situated in a setting that requires no prompt engineering efforts: the context window of LLM only contains its past actions and observations and does not require human efforts to filter for relevant information.\\n\\nSimilar to CodeAct, concurrent work OpenCodeInterpreter (Zheng et al., 2024), with a specific focus on competitive code generation questions, collects code-debugging trajectories to improve an LLM's iterative code debugging performance. However, its applicability to general LLM agent tasks remains unknown.\\n\\nB. Comparison with TaskWeaver\\n\\nIn the landscape of unifying the action space of LLM agents, our work represents a leap over the previous initiative, TaskWeaver (Qiao et al., 2023). While TaskWeaver deserves acknowledgment for initially integrating code into the action space of LLM agents, its exploration remains limited. This work, primarily characterized by its reliance on a limited set of qualitative examples with close-sourced models as the backbones, fails to harness the full potential of this integration, remaining merely conceptual demonstrations. Our work transcends mere conceptualization by conducting an extensive and rigorous analysis, clearly quantifying the benefits of code action within LLM agents. Beyond this, we introduce a unique instruction-tuning dataset CodeActInstruct specifically designed to amplify the agent's capabilities in executing code-based actions and an open-source LLM agent CodeActAgent. These contributions not only extend the work of TaskWeaver but also pave the way for future explorations, offering valuable resources to the open-source community and redefining the potential of LLM agents in practical applications.\\n\\nC. General Data Down-sample\\n\\n- ShareGPT (Anonymous, 2023): We remove all single-turn conversations, then perform random sub-sample to a desired final number.\\n- ShareGPT (GPT-4) (OpenChat, 2023): We do not perform sub-sampling on this dataset.\\n- OpenOrca (Lian et al., 2023): We select the CoT subset of OpenOrca, then perform a random sub-sample to a desired final number.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D. CodeActAgent Training Details\\n\\nAll SFT experiments are performed on one 4xA100 40GB SXM node using a fork of Megatron-LLM (Cano et al., 2023) with a training throughput of around 9k tokens per second. We use chatML format for all multi-turn data, and we only calculate and optimize for loss on the assistant response. We pack short instances into longer ones and apply flash attention for training efficiency.\\n\\nWe train both LLaMA-2 and Mistral LLMs with Tensor Parallel of 4, the learning rate of $1e^{-5}$ with 50 warmup steps and cosine decay (end learning rate of $1e^{-6}$). We train for five epochs with a batch size of 32. We use the 3rd epoch checkpoint for all our experiments.\\n\\nE. Example Prompt for CodeAct\\n\\nThis is an example (zero-shot) system prompt used in a deploy instance of CodeAct where we used chatML format. The users may optionally include tools descriptions similar to \u00a7F or including extra in-context examples similar to \u00a7G.3.\\n\\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. The assistant can interact with an interactive Python (Jupyter Notebook) environment and receive the corresponding output when needed. The code should be enclosed using `<execute>` tag, for example: `<execute> print(\"Hello World!\") </execute>.\\n\\nThe assistant should attempt fewer things at a time instead of putting too much code in one `<execute>` block. The assistant can install packages through PIP by `<execute> !pip install [package needed] </execute>` and should always import packages and define variables before starting to use them.\\n\\nThe assistant should stop `<execute>` and provide an answer when they have already obtained the answer from the execution result. Whenever possible, execute the code for the user using `<execute>` instead of providing it.\\n\\nThe assistant's response should be concise, but do express their thoughts.\\n\\nF. ToolEval Prompt\\n\\nYou have access to the following tools:\\n\\n{{Tool Definition}}\\n\\n{{Formatting Instruction}}\\n\\nNow, let's get started!\\n\\nInstruction: {{Example: Find the current price of Legendary Wand.}}\\n\\nAnswer in the format of 'xx.xx' (e.g., 12.34).\\n\\nYou can optionally express your thoughts using natural language before your action. For example, 'Thought: I want to use tool_name to do something. Action: <your action to call tool_name> End Action'.\\n\\nNote that your output should always contain either 'Action:' or 'Answer:', but not both. When you are done, output the result using 'Answer: your answer'\\n\\nPlease ONLY output the answer (e.g., single number), without any other text.\\n\\nEach {{...}} component above will be substituted with corresponding information.\\n\\nF.1. Example of {{...}}\\n\\nThe following is an example tool definition for web-browsing.\\n\\nhttps://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md\"}"}
{"id": "jJ9BoXAfFa", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Executable Code Actions Elicit Better LLM Agents\\n\\n1. click_url: Clicks on a URL. A clickable URL looks like \\\\[Clickable '<url_argument>'\\\\] in the webpage.\\n   Arguments: url (str).\\n   Returns the rendered content of the webpage after clicking the URL showing on the current rendered page.\\n   Signature: click_url(url: str) -> str\\n\\n2. go_to_previous_page: Goes back to the previous page. It has no arguments.\\n   After going back to the previous page, return the rendered content of the webpage.\\n   Signature: go_to_previous_page() -> str\\n\\n3. scroll_down: Scrolls down the view. It has no arguments.\\n   Returns the rendered content of the webpage after scrolling down.\\n   Signature: scroll_down() -> str\\n\\n4. scroll_up: Scrolls up the view. It has no arguments.\\n   Returns the rendered content of the webpage after scrolling up.\\n   Signature: scroll_up() -> str\\n\\n5. view: Return the current view in string format of the rendered webpage. It has no arguments.\\n   Returns the rendered content of the webpage.\\n   You should call this when you want to see the rendered content of the current webpage.\\n   Signature: view() -> str\\n\\n6. calculator: Evaluates the given expression and returns the result. Accepts a calculation expression as input. For example, \\\"2 + (3 * 4)\\\" will return 14.\\n   Signature: calculator(expression: str) -> float\\n\\nF.2. Example of {{ Formatting Instruction }}\\n\\nDifferent action format has different formatting instructions.\\n\\nF.3. Formatting Instruction for Code as Action\\n\\nYou can use the tools by outputing a block of Python code that invoke the tools.\\nYou may use for-loops, if-statements, and other Python constructs when necessary.\\nBe sure to print the final answer at the end of your code.\\nYou should begin your tool invocation with 'Action:' and end it with 'End Action'.\\nExample: 'Action: tool_name(argument_1) End Action'\\n\\nF.4. Formatting Instruction for Json as Action\\n\\nYou can use the tools by outputing a JSON object with the following fields:\\n   - 'tool': the name of the tool\\n   - 'args': a list of arguments to the tool\\nYou should begin your tool invocation with 'Action:' and end it with 'End Action'.\\nExample: 'Action: {\"tool\": \"tool_name\", \"args\": \\\\[\\\"argument_1\\\"]} End Action'\\n\\nYou can only invoke one tool at a time.\\n\\nF.5. Formatting Instruction for Text as Action\\n\\nYou can use the tools by outputing the tool name followed by its arguments, delimited by commas.\\nYou should begin your tool invocation with 'Action:' and end it with 'End Action'.\\nExample: 'Action: tool_name, argument_1 End Action'\\n\\nYou can only invoke one tool at a time.\\n\\nG. CodeAct Interaction Data\\n\\nG.1. Dataset Downsample\\n\\n\u2022 Code generation tasks in APPS (Hendrycks et al., 2021a): We remove instances without any test case available.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table A.9: CodeActInstruct components and the number of instances for training trajectory generation.\\n\\n| Domain Capability                                           | Dataset                | # of Instances |\\n|-------------------------------------------------------------|------------------------|----------------|\\n| Web Search Information                                      | HotpotQA (Yang et al., 2018) | 3,000          |\\n| Math Reasoning Math problem-solving using math Libraries in Python (e.g., `sympy`) | MATH (Hendrycks et al., 2021a) | 5,586          |\\n| Code Generation Self-debug from Python error messages and traceback | APPS (Hendrycks et al., 2021b) | 4,439          |\\n| Tabular Reasoning using `pandas` and `sqlite3` (for SQL) library | WikiTableQuestion (Pasupat & Liang, 2015) | 3,000          |\\n| Embodied Planning Interact with embodied environments through APIs | ALFWorld (Shridhar et al., 2020) | 3,553          |\\n\\n- Tabular reasoning tasks in WikiTableQuestion (Pasupat & Liang, 2015): We select a subset of 3000 instances with the largest table size (i.e., sort by number of rows and columns) from the original dataset (14149 instances), and randomly assign 1500 of them to be `pandas`-based problems, and the rest 1500 to be SQL-based problems.\\n- Web search tasks in HotpotQA (Yang et al., 2018): We select the 15661 problems labeled as \u201chard\u201d in the original dataset (with 90447 instances), then randomly down-sample them to 3000 problems.\\n- Math reasoning in MATH (Hendrycks et al., 2021b): We remove problems with the annotated difficulty lower than 3, which results in 5586 instances as shown in Tab. A.9.\\n- Embodied Planning in ALFWorld (Shridhar et al., 2020): We did not perform down-sampling for ALFWorld.\\n\\nG.2. Data Selection Heuristic\\n\\nGiven successful task-solving trajectories that have more than 2 turns, we apply the following heuristic to select instances that can promote the code-as-actions, self-improvement, and instruction-following capabilities of LLM agents:\\n\\n- **Code-as-Actions:** We exclude trajectories wherein LLM agents do not adhere to the code-as-actions framework, either due to incorrect API invocation or the generation of actions in formats unsuitable for parsing and execution.\\n- **Self-Improving:** We selectively preserve those trajectories wherein the model initially encounters errors but subsequently rectifies these inaccuracies in later interactions. In addition, we eliminate successful trajectories that exclusively yield errors in all code executions. These are deemed ineffective demonstrations, as our objective is to prevent the model from learning to consistently execute erroneous code while still managing to provide correct answers.\\n- **Instruction-Following:** We remove rare cases where the LLM agents fail to follow the instruction and respond to the user, identified by an odd number of interaction turns.\\n\\nAfter applying all these heuristics, we obtain 6728 trajectories (out of 6985) from gpt-3.5 and clade, and 411 trajectories (out of 413) from gpt-4-0613.\\n\\nG.3. Example of Trajectory Generation Prompt\\n\\nThe format of the data generation prompt closely follow MINT (Wang et al., 2023e).\\n\\nG.3.1. TABLE REASONING (WIKI TABLE QUESTION)\\n\\nWe only provide one-shot example for SQL-based tabular reasoning. This is a prompt with one-shot example for SQL-based tabular reasoning problem:\\n\\nYou are a helpful assistant assigned with the task of problem-solving. To achieve this, you will be using an interactive coding environment equipped with a variety of tool functions to assist you throughout the process.\\n\\nAt each turn, you should first provide your step-by-step thinking for solving the task. Your thought process should be enclosed using `<thought>` tag, for example: `<thought>I need to print \\\"Hello World!\\\" </thought>.\\n\\nAfter that, you have two options:\"}"}
{"id": "jJ9BoXAfFa", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Task:\\nGiven the following table (only the first 5 rows are shown):\\n\\n| Region | Country | Location | Size (m) x | Payload (metric tonnes) | Degrees of Freedom | Z Vert vel (mm/s) | X Horiz accel (m/s\u00b2) | Y Horiz accel (m/s\u00b2) | Z Vert accel (m/s\u00b2) | Max Freq (Hz) | Details checked |\\n|--------|---------|----------|------------|------------------------|-------------------|------------------|-------------------|-------------------|------------------|-------------|----------------|\\n|        |         |          |            |                        |                   |                  |                   |                   |                  |             |                |\\n| 0      | Africa  | Algeria  | CGS Laboratory (in construction) | 6.1 x 6.1 | 60 | ... | 1000 | 19 |\"}"}
{"id": "jJ9BoXAfFa", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Executable Code Actions Elicit Better LLM Agents\\n\\n|    |     |     |      |      |      |     |     |     |\\n|----|-----|-----|------|------|------|-----|-----|-----|\\n| 1. | Africa | South Africa | University of Witwatersrand | 4 x 4 | 10 | 17/7/2009 |\\n| 2. | Asia | China | China Academy of Building Research, Beijing | 6.1 x 6.1 | 60 | 6 |\\n| 3. | Asia | China | Guangzhou University | 3 x 3 | 20 | 6 |\\n| 4. | Asia | China | Nanjing University of Technology | 3 x 5 | 15 | 3 |\\n\\nWrite a SQL query to find the answer to the question: which is the other besides Asia the most region charted.\\n\\nThe SQLite3 database is preloaded for you and can be accessed within <execute> block via the variable 'conn' (SQLite3 connection object).\\n\\nThis is an example instruction for Pandas-package-based tabular reasoning problem:\\n\\nTask:\\nGiven the following table (only the first 5 rows are shown):\\n\\n| Pos | No | Rider | Bike  | Laps | Time     | Grid | Points |\\n|-----|----|-------|-------|------|----------|------|--------|\\n| 0   | 1  | 93    | Marc Marquez | Derbi | 22.0     | 40:46.315 | 1     | 25.0   |\\n| 1   | 2  | 38    | Bradley Smith | Aprilia | 22.0     | +4.638 | 3     | 20.0   |\\n| 2   | 3  | 44    | Pol Espargaro | Derbi | 22.0     | +4.996 | 2     | 16.0   |\\n| 3   | 4  | 11    | Sandro Cortese | Derbi | 22.0     | +45.366 | 5     | 13.0   |\\n| 4   | 5  | 7     | Efren Vazquez | Derbi | 22.0     | +45.433 | 8     | 11.0   |\\n\\nWrite a Pandas query to find the answer to the question: Bradley Smith lost the 2010 Catalan Motorcycle Grand Prix 125cc by more/less than 4 seconds?\\n\\nThe dataframe is preloaded for you and can be accessed within <execute> block via the variable 'df'.\\n\\nG.3.2. CODE GENERATION\\n\\nHere is an example of the prompt with one in-context example for code generation on the APPS dataset (Hendrycks et al., 2021a) that encourages the LLM to self-debug its solution:\\n\\nYou are a helpful assistant assigned with the task of problem-solving. To achieve this, you will be using an interactive coding environment equipped with a variety of tool functions to assist you throughout the process.\\n\\nAt each turn, you should first provide your step-by-step thinking for solving the task. Your thought process should be enclosed using \"<thought>\" tag, for example: <thought> I need to print \\\"Hello World!\\\" </thought>.\\n\\nAfter that, you have two options:\\n1) Interact with a Python programming environment and receive the corresponding output. Your code should be enclosed using \"<execute>\" tag, for example: <execute> print(\\\"Hello World!\\\") </execute>.\\n2) Directly provide a solution that adheres to the required format for the given task. Your solution should be enclosed using \"<solution>\" tag, for example: The answer is <solution> A </solution>.\\n\\nYou have 5 chances to interact with the environment or propose a solution. You can only propose a solution 2 times.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mikhail walks on a Cartesian plane. He starts at the point $(0, 0)$, and in one move he can go to any of eight adjacent points. For example, if Mikhail is currently at the point $(0, 0)$, he can go to any of the following points in one move: $(1, 0)$; $(1, 1)$; $(0, 1)$; $(-1, 1)$; $(-1, 0)$; $(-1, -1)$; $(0, -1)$; $(1, -1)$.\\n\\nIf Mikhail goes from the point $(x_1, y_1)$ to the point $(x_2, y_2)$ in one move, and $x_1 \\\\neq x_2$ and $y_1 \\\\neq y_2$, then such a move is called a diagonal move.\\n\\nMikhail has $q$ queries. For the $i$-th query Mikhail's target is to go to the point $(n_i, m_i)$ from the point $(0, 0)$ in exactly $k_i$ moves. Among all possible movements he want to choose one with the maximum number of diagonal moves. Your task is to find the maximum number of diagonal moves or find that it is impossible to go from the point $(0, 0)$ to the point $(n_i, m_i)$ in $k_i$ moves.\\n\\nNote that Mikhail can visit any point any number of times (even the destination point!).\\n\\n-----Input-----\\nThe first line of the input contains one integer $q$ ($1 \\\\le q \\\\le 10^{4}$) - the number of queries.\\n\\nThen $q$ lines follow. The $i$-th of these $q$ lines contains three integers $n_i$, $m_i$ and $k_i$ ($1 \\\\le n_i, m_i, k_i \\\\le 10^{18}$) - $x$-coordinate of the destination point of the query, $y$-coordinate of the destination point of the query and the number of moves in the query, correspondingly.\\n\\n-----Output-----\\nPrint $q$ integers. The $i$-th integer should be equal to -1 if Mikhail cannot go from the point $(0, 0)$ to the point $(n_i, m_i)$ in exactly $k_i$ moves described above. Otherwise the $i$-th integer should be equal to the maximum number of diagonal moves among all possible movements.\\n\\n-----Example-----\\nInput\\n3\\n2 2 3\\n4 3 7\\n10 1 9\\nOutput\\n1\\n6\\n-1\\n-----Note-----\\nOne of the possible answers to the first test case: $(0, 0) \\\\to (1, 0) \\\\to (1, 1) \\\\to (2, 2)$.\\n\\nOne of the possible answers to the second test case: $(0, 0) \\\\to (0, 1) \\\\to (1, 2) \\\\to (0, 3) \\\\to (1, 4) \\\\to (2, 3) \\\\to (3, 2) \\\\to (4, 3)$.\\n\\nIn the third test case Mikhail cannot reach the point $(10, 1)$ in 9 moves.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You should write code that expects inputs from stdin and print outputs (to stdout).\\n\\nAssistant:\\n\\nI should use the provided example as a test case to develop and test my solution.\\n\\nTo solve this question:\\n\\n1. To get to any point \\\\((n_i, m_i)\\\\), at most, we would need \\\\(\\\\max(n_i, m_i)\\\\) moves if he only uses diagonal moves. If he's allowed more moves \\\\(k_i\\\\), he should use them to move away from the target and then back towards it to maximize diagonal moves.\\n\\n2. To simplify our logic, always choose the larger coordinate as the primary direction of movement. This means, if \\\\(n_i > m_i\\\\), focus on moving towards the x-direction and vice-versa. This avoids having to check both directions every time.\\n\\n3. If both coordinates have the same parity (both odd or both even), and the number of allowed moves is of a different parity, we know Mikhail can't reach the destination directly using only diagonal moves. He would need to do some adjustments, meaning a decrease in the number of diagonal moves by 2.\\n\\nLet me try to implement this solution.\\n\\n```\\n# I should first mock the input data to test my solution\\ninput_data = [\"3\", \"2 2 3\", \"4 3 7\", \"10 1 9\"]\\nmock_input = iter(input_data).__next__\\n# Overwrite the built-in input function with our mock function\\nold_input = input\\ninput = mock_input\\n\\n# Now I can implement my solution\\nq = int(input())\\nfor i in range(q):\\n    a, b, k = list(map(int, input().split()))\\n    if a < b:\\n        a, b = b, a\\n    if a % 2 == b % 2 != k % 2:\\n        print(k - 2)\\n    elif (a + b) % 2 != 0:\\n        print(k - 1)\\n    else:\\n        print(k)\\n\\n# Restore the built-in input function\\ninput = old_input\\n```\\n\\nObservation:\\n1\\n6\\n8\\n\\nAssistant:\\n\\nI made a mistake. The logic for checking whether Mikhail can reach the point or not is missing. Let me correct this.\\n\\nIf Mikhail needs more moves than allowed to even reach the primary coordinate direction (either x or y, whichever is greater), then he cannot reach the destination. In this case, the output should be -1.\\n\\nThis is the missing check if \\\\(a > k\\\\): which results in the output -1.\\n\\nLet's add this logic and re-run the solution.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Executable Code Actions Elicit Better LLM Agents\\n\\n```python\\n# I should first mock the input data to test my solution\\ninput_data = ['3', '2 2 3', '4 3 7', '10 1 9']\\nmock_input = iter(input_data).__next__\\n# Overwrite the built-in input function with our mock function\\nold_input = input\\ninput = mock_input\\n\\n# Now I can implement my solution\\nq = int(input())\\nfor i in range(q):\\n    a, b, k = list(map(int, input().split()))\\n    if a < b:\\n        a, b = b, a\\n\\n    if a > k:\\n        print(-1)\\n    elif a % 2 == b % 2 != k % 2:\\n        print(k - 2)\\n    elif (a + b) % 2 != 0:\\n        print(k - 1)\\n    else:\\n        print(k)\\n\\n# Restore the built-in input function\\ninput = old_input\\n```\\n\\n---\\n\\n**Task:**\\n\\nGiven a permutation $p$ of length $n$, find its subsequence $s_1$, $s_2$, $\\\\ldots$, $s_k$ of length at least $2$ such that:\\n\\n$$|s_1-s_2|+|s_2-s_3|+\\\\ldots+|s_{k-1}-s_k|$$\\n\\nis as big as possible over all subsequences of $p$ with length at least $2$. Among all such subsequences, choose the one whose length, $k$, is as small as possible.\\n\\nIf multiple subsequences satisfy these conditions, you are allowed to find any of them.\\n\\n23\"}"}
{"id": "jJ9BoXAfFa", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A sequence $a$ is a subsequence of an array $b$ if $a$ can be obtained from $b$ by deleting some (possibly, zero or all) elements.\\n\\nA permutation of length $n$ is an array of length $n$ in which every element from 1 to $n$ occurs exactly once.\\n\\n-----Input-----\\nThe first line contains an integer $t$ ($1 \\\\leq t \\\\leq 2 \\\\cdot 10^4$) - the number of test cases. The description of the test cases follows.\\nThe first line of each test case contains an integer $n$ ($2 \\\\leq n \\\\leq 10^5$) - the length of the permutation $p$.\\nThe second line of each test case contains $n$ integers $p_1$, $p_2$, \u2026, $p_n$ ($1 \\\\leq p_i \\\\leq n$, $p_i$ are distinct) - the elements of the permutation $p$.\\nThe sum of $n$ across the test cases doesn't exceed $10^5$.\\n\\n-----Output-----\\nFor each test case, the first line should contain the length of the found subsequence, $k$. The second line should contain $s_1$, $s_2$, \u2026, $s_k$ - its elements.\\nIf multiple subsequences satisfy these conditions, you are allowed to find any of them.\\n\\n-----Example-----\\nInput\\n2\\n3\\n3 2 1\\n4\\n1 3 4 2\\nOutput\\n2\\n3 1\\n3\\n1 4 2\\n\\n-----Note-----\\nIn the first test case, there are 4 subsequences of length at least 2: $[3,2]$ which gives us $|3-2|=1$. $[3,1]$ which gives us $|3-1|=2$. $[2,1]$ which gives us $|2-1|=1$. $[3,2,1]$ which gives us $|3-2|+|2-1|=2$.\\nSo the answer is either $[3,1]$ or $[3,2,1]$. Since we want the subsequence to be as short as possible, the answer is $[3,1]$.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"found no match for the generated artifacts, suggesting that these artifacts might have been introduced in the pre-training corpus (Touvron et al., 2023), which we don\u2019t have access to. Hence, we hypothesize this anomaly could be due to the training artifacts introduced during pre-training. Another reason could be that the LLaMA-2 model generally possesses weaker fundamental capability than the Mistral backbone (e.g., lack of essential knowledge for task completion).\"}"}
{"id": "jJ9BoXAfFa", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Executable Code Actions Elicit Better LLM Agents\\n\\nLessWrong-Amplify-Instruct, 2023. A dataset focusing on reasoning in multi-turn conversations.\\n\\nLee, M., Liang, P., and Yang, Q. Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities. In Proceedings of the 2022 CHI conference on human factors in computing systems, pp. 1\u201319, 2022.\\n\\nLi, M., Song, F., Yu, B., Yu, H., Li, Z., Huang, F., and Li, Y. Api-bank: A benchmark for tool-augmented llms, 2023.\\n\\nLian, W., Goodson, B., Pentland, E., Cook, A., Vong, C., and Teknium. Openorca: An open dataset of gpt augmented flan reasoning traces. https://huggingface.co/Open-Orca/OpenOrca, 2023.\\n\\nLiang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., Florence, P., and Zeng, A. Code as policies: Language model programs for embodied control. In arXiv preprint arXiv:2209.07753, 2022.\\n\\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023a.\\n\\nLiu, R., Yang, R., Jia, C., Zhang, G., Zhou, D., Dai, A. M., Yang, D., and Vosoughi, S. Training socially aligned language models in simulated human society. arXiv preprint arXiv:2305.16960, 2023b.\\n\\nMa, Y. J., Liang, W., Wang, G., Huang, D.-A., Bastani, O., Jayaraman, D., Zhu, Y., Fan, L., and Anandkumar, A. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931, 2023.\\n\\nMialon, G., Dessi, R., Lomeli, M., Nalmpantis, C., Pa sunuru, R., Raileanu, R., Roziere, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.\\n\\nNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\\n\\nOpenChat. Sharegpt dataset. https://hf.co/datasets/openchat/openchat_sharegpt_v3/blob/main/sharegpt_gpt4.json, 2023. A dataset containing multi-turn conversations between human and LLM assistants. It is filtered to contain data only from GPT-4.\\n\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\\n\\nPark, J. S., O'Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface and Technology, pp. 1\u201322, 2023.\\n\\nPasupat, P. and Liang, P. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1470\u20131480, 2015.\\n\\nPatil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected with massive apis. ArXiv, abs/2305.15334, 2023. URL https://api.semanticscholar.org/CorpusID:258865184.\\n\\nQian, C., Cong, X., Yang, C., Chen, W., Su, Y., Xu, J., Liu, Z., and Sun, M. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.\\n\\nQiao, B., Li, L., Zhang, X., He, S., Kang, Y., Zhang, C., Yang, F., Dong, H., Zhang, J., Wang, L., et al. Taskweaver: A code-first agent framework. arXiv preprint arXiv:2311.17541, 2023.\\n\\nQin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng, Z., Huang, Y., Xiao, C., Han, C., et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023a.\\n\\nQin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y.-T., Lin, Y., Cong, X., Tang, X., Qian, B., Zhao, S., Tian, R., Xie, R., Zhou, J., Gerstein, M. H., Li, D., Liu, Z., and Sun, M. Toolllm: Facilitating large language models to master 16000+ real-world apis. ArXiv, abs/2307.16789, 2023b. URL https://api.semanticscholar.org/CorpusID:260334759.\\n\\nSchick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\\n\\nShen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.\\n\\nShridhar, M., Yuan, X., Cote, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2020.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Executable Code Actions Elicit Better LLM Agents\\n\\nSingh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J., and Garg, A.\\n\\nProgprompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 11523\u201311530, 2023. doi: 10.1109/ICRA48891.2023.10161317.\\n\\nSuris, D., Menon, S., and Vondrick, C. Vipergpt: Visual inference via python execution for reasoning. Proceedings of IEEE International Conference on Computer Vision (ICCV), 2023.\\n\\nTang, X., Jin, Q., Zhu, K., Yuan, T., Zhang, Y., Zhou, W., Qu, M., Zhao, Y., Tang, J., Zhang, Z., et al. Prioritizing safeguarding over autonomy: Risks of LLM agents for science. arXiv preprint arXiv:2402.04247, 2024.\\n\\nTIOBE Index. Tiobe index. https://www.tiobe.com/tiobe-index/, Accessed at Jan 23rd, 2024. The TIOBE Programming Community index is an indicator of the popularity of programming languages. The index is updated once a month. The ratings are based on the number of skilled engineers worldwide, courses and third party vendors.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n\\nWang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.\\n\\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023b.\\n\\nWang, R., Jansen, P. A., Cote, M.-A., and Ammanabrolu, P. Scienceworld: Is your agent smarter than a 5th grader? In Conference on Empirical Methods in Natural Language Processing, 2022a. URL https://api.semanticscholar.org/CorpusID:247451124.\\n\\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.\\n\\nWang, X., Li, S., and Ji, H. Code4Struct: Code generation for few-shot event structure prediction. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3640\u20133663, Toronto, Canada, July 2023c. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.202. URL https://aclanthology.org/2023.acl-long.202.\\n\\nWang, X., Peng, H., Jabbarvand, R., and Ji, H. Leti: Learning to generate from textual interactions. ArXiv, abs/2305.10314, 2023d.\\n\\nWang, X., Wang, Z., Liu, J., Chen, Y., Yuan, L., Peng, H., and Ji, H. Mint: Evaluating LLMs in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691, 2023e.\\n\\nWang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023f.\\n\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.\\n\\nXu, Q., Hong, F., Li, B., Hu, C., Chen, Z., and Zhang, J. On the tool manipulation capability of open-source large language models, 2023.\\n\\nYang, J., Prabhakar, A., Narasimhan, K., and Yao, S. Intercode: Standardizing and benchmarking interactive coding with execution feedback. Advances in Neural Information Processing Systems, 36, 2024a.\\n\\nYang, K., Liu, J., Wu, J., Yang, C., Fung, Y.R., Li, S., Huang, Z., Cao, X., Wang, X., Wang, Y., Ji, H., and Zhai, C. If LLM is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents, 2024b.\\n\\nYang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369\u20132380, 2018.\\n\\nYang, Z., Liu, A., Liu, Z., Liu, K., Xiong, F., Wang, Y., Yang, Z., Hu, Q., Chen, X., Zhang, Z., Luo, F., Guo, Z., Li, P., and Liu, Y. Towards unified alignment between agents, humans, and environment, 2024c.\\n\\nYao, S., Chen, H., Yang, J., and Narasimhan, K. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744\u201320757, 2022a.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2022b.\\n\\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023a.\\n\\nYao, W., Heinecke, S., Niebles, J. C., Liu, Z., Feng, Y., Xue, L., Murthy, R., Chen, Z., Zhang, J., Arpit, D., et al. Retroformer: Retrospective large language agents with policy gradient optimization. arXiv preprint arXiv:2308.02151, 2023b.\\n\\nYuan, L., Chen, Y., Wang, X., Fung, Y. R., Peng, H., and Ji, H. Craft: Customizing llms by creating and retrieving from specialized toolsets. ArXiv, abs/2309.17428, 2023. URL https://api.semanticscholar.org/CorpusID:263310662.\\n\\nZeng, A., Liu, M., Lu, R., Wang, B., Liu, X., Dong, Y., and Tang, J. Agenttuning: Enabling generalized agent abilities for llms, 2023.\\n\\nZhang, C., Liu, L., Wang, J., Wang, C., Sun, X., Wang, H., and Cai, M. Prefer: Prompt ensemble learning via feedback-reflect-refine. arXiv preprint arXiv:2308.12033, 2023.\\n\\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.\\n\\nZheng, T., Zhang, G., Shen, T., Liu, X., Lin, B. Y., Fu, J., Chen, W., and Yue, X. Opencodeinterpreter: Integrating code generation with execution and refinement. https://arxiv.org/abs/2402.14658, 2024.\\n\\nZhu, X., Chen, Y., Tian, H., Tao, C., Su, W., Yang, C., Huang, G., Li, B., Lu, L., Wang, X., et al. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Executable Code Actions Elicit Better LLM Agents\\n\\nTable A.6: Example of actions for re-purposed API-Bank (Li et al., 2023) and M3ToolEval.\\n\\n| Format | Action | content | time       |\\n|--------|--------|---------|------------|\\n| CodeAct | AddAgenda | content=\\\"Meeting with John\\\" | time=\\\"2023-10-26 09:00:00\\\" |\\n| JSON   | {\"action\": \\\"AddAgenda\\\", \\\"content\\\": \\\"Meeting with John\\\", \\\"time\\\": \\\"2023-10-26 09:00:00\\\"} |\\n| Text   | Action: AddAgenda, content: Meeting with John, time: 2023-10-26 09:00:00 |\\n\\nTable A.7: Comparison between M3ToolEval and existing tool-use evaluation benchmark.\\n\\n| Benchmark | M3ToolEval | ToolBench | APIBench | ToolBench | API-Bank | ToolBench | API-Bench | API-Bank |\\n|-----------|------------|-----------|----------|-----------|----------|-----------|-----------|----------|\\n| Requiring multi-turn interaction | \u201c\u201d | \u201c\u201d | \u201c\u201d | \u201c\u201d | \u2217 | \u201c\u201d | \u201c\u201d | \u201c\u201d |\\n| Multiple tools | \u201c\u201d | \u201c\u201d | \u201c\u201d | \u201c\u201d | \u201c\u201d | \u201c\u201d | \u201c\u201d | \u201c\u201d |\\n| Evaluation | Answer Match | LLM Evaluator | AST Tree | Match API-Call | Match Test Case | | | |\\n| No dependency on external API | \u2217 | | | | | | | |\\n| Supported API Action Format | CodeAct | & JSON & Text | JSON | CodeAct | JSON |\\n\\nAblation study results. The best results are bolded, and the second-best results are underlined.\\n\\n| Agent Tasks | Overall averaged performance | Code as Action | Text as Action |\\n|-------------|----------------------------|----------------|----------------|\\n| Generic LLM Tasks | Overall | CodeActAgent (Llama2-based) | 7B | 51.3 | 20.4 |\\n| | | w/o CodeAct | 7B | 17.0 | 15.5 |\\n| | | w/o general conversations | 7B | 29.2 | 15.9 |\\n| | OD | CodeActAgent (Mistral-based) | 7B | 57.4 | 32.4 |\\n| | | w/o CodeAct | 7B | 32.9 | 23.0 |\\n| | | w/o general conversations | 7B | 50.5 | 13.9 |\\n\\nIn this section, we discuss the fundamental differences between CodeAct and prior work that prompt LLM to generate code for problem-solving. Existing work have explored using code generation for task-solving in different domains, for example, Code4Struct (Wang et al., 2023c) for structured prediction, PaL (Gao et al., 2023) for math reasoning, Meta-GPT (Hong et al., 2023) for multi-agent collaboration, code-as-policy (Liang et al., 2022) for robot control, ViperGPT (Sur\u00ed et al., 2023) for visual question answering, Voyager (Wang et al., 2023a) for playing games, Data Interpreter (Hong et al., 2024) for data science tasks, etc.\\n\\nMost prior work generates code (i.e., a static sequence of actions) in a single-turn setting and cannot dynamically readjust action on new observation: It is considered a failure when the model-generated code fails to solve a task on the first attempt. This setting overlooks the potential of environmental observation (e.g., code execution results) that might benefit future action and overall decision (e.g., dynamically adjusting subsequent code after observing intermediate code execution results, fixing erroneous code after seeing an error message). That is, the generated code is a static sequence of actions that cannot be dynamically re-adjusted on the fly by incorporating new observations. Such a single-turn setting makes it challenging to scale to more challenging problems since even expert human programmers usually cannot write functionally correct code in\"}"}
{"id": "jJ9BoXAfFa", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Executable Code Actions Elicit Better LLM Agents\\n\\nTable 4: Statistics of our training mixture and comparison with prior work. Please refer to \u00a73.1 for details about CodeActInstruct and general conversation data. Token statistics are computed using Llama-2 tokenizer.\\n\\n| Data Mixture          | Data Type         | Data Name                          | # of Data Instances | # of Total Tokens | Avg. Tokens Per Instance |\\n|-----------------------|-------------------|-----------------------------------|---------------------|-------------------|--------------------------|\\n| Prior Work            | - FireAct (Chen et al., 2023a) | 2,063,063                          | 542,176             | 2,620,812         | 262                      |\\n|                       | - AgentInstruct (Zeng et al., 2023) | 1,866                             | 2,517               | 4,475,785         | 1,349                    |\\n| CodeActInstruct (Ours)| Information Seeking | HotpotQA (Yang et al., 2018)      | 1,664               | 2,472            | 1,485                    |\\n|                       | Software Packages | MATH (Math, (Hendrycks et al., 2021b)) | 732                   | 1,719            | 2,467                    |\\n|                       |                    | APPS (Code, (Hendrycks et al., 2021a)) | 647                   | 1,235            | 1,909                    |\\n|                       | External Memory   | WikiTableQuestion (Pasupat & Liang, 2015) | 1065                  | 1,316            | 1,235                    |\\n|                       | Robot Planning    | ALFWorld (Shridhar et al., 2020)   | 2,031               | 3,838            | 1,889                    |\\n|                       | Total             |                                   | 7,139               | 10,581           | 1,482                    |\\n\\nGeneral Conversation\\n\\n| Trajectory Generation | Single-Turn Reasoning | OpenOrca (Sub-sampled, (Lian et al., 2023)) | 50,000               | 14,034           | 280                      |\\n|                      | Multi-Turn Conversations | ShareGPT (Sub-sampled, (Anonymous, 2023)) | 10,000               | 17,933           | 1,793                    |\\n|                      |                      | ShareGPT (GPT-4, (OpenChat, 2023))       | 4,583                | 18,195           | 3,970                    |\\n|                      | Multi-turn Reasoning | CapyBara (LDJnr, 2023)                 | 4,647                | 4,982            | 1,072                    |\\n|                      | Total               |                                   | 69,230               | 55,146           | 7,967                    |\\n\\nFollowing MINT (Wang et al., 2023e), we repurpose single-turn problems into multi-turn ones by allowing LLM to interact with the environment for multiple turns before it decides to submit one solution for evaluation. Specifically for code generation problems, we provide an in-context example to guide LLMs to test their solution on provided test cases before they submit the solution. Metrics from the original data will evaluate the submitted solution to determine its correctness. We include examples in \u00a7G.3.\\n\\nTrajectory Generation. We use MINT\u2019s evaluation framework (Wang et al., 2023e) to generate interaction trajectories for the aforementioned datasets and determine the correctness of each trajectory. We run gpt-3.5-turbo-0613 from OpenAI, claude-1-instant and claude-2 from Anthropic on down-sampled data, except code generation, which we use a longer-context version of GPT-3.5 (gpt-3.5-turbo-0613-16k) due to the long-context requirement of the self-debugging process. On a subset of problems that none of these models can solve, we use gpt-4-0613 to generate trajectories.\\n\\nEnhancing Agent\u2019s Capabilities of Improving from Interaction. We select a high-quality subset of all the generated trajectories from CodeActInstruct to promote the agent\u2019s ability to improve the next action based on prior observations (e.g., self-debugging from code execution error message, a planning capability in Fig. 2). To achieve this, we selectively preserve those trajectories wherein the model initially encounters errors but rectifies these inaccuracies in later interactions. For these instances, the LLM typically engages in self-reflection following the initial error, thereby proactively enhancing its future actions. Other filtering details are discussed in \u00a7G.2. On all trajectories generated, we keep 4,111 trajectories from gpt-4-0613 and 6,728 trajectories from gpt-3.5 and claude. The statistics of the resulting dataset CodeActInstruct are shown in Tab. 4.\\n\\nComparing CodeActInstruct with Prior Work. Compared with prior work AgentInstruct (Zeng et al., 2023) and FireAct (Chen et al., 2023a) that mainly focus using text as action, CodeActInstruct results in models that are more practical in real-world implementation, as such models using CodeAct can directly interact with Python interpreters and open-source toolkits (Fig. 3), reducing the development effort for action parsing and tool creations. CodeActInstruct is systematically constructed following the general agent framework (Fig. 2). It covers diverse domains (e.g., compared to FireAct that only considers QA-task and search API), contains quality data (e.g., promotes agent\u2019s capability of self-debug) and of larger size (3.8x / 3.5x more data trajectories and 5x / 19x more tokens compared to AgentInstruct / FireAct respectively in Tab. 4). As we empirically show in Tab. 5, the resulting model (same backbone) of CodeActInstruct achieves 24% and 119% relative improvement compared to AgentInstruct and FireAct.\\n\\nCodeActInstruct Can Be Used With Existing Agent-User Conversation Data. We use a sub-sampled set of OpenOrca (Lian et al., 2023) that focuses on single-turn chain-of-thought (CoT) reasoning, ShareGPT (Anonymous, 2023; OpenChat, 2023) from two sources that contain multi-turn conversations between human and LLM, and CapyBara (LDJnr, 2023) that focuses on reasoning in multi-turn conversations. Statistics and down-sampling details can be found in Tab. 4 and \u00a7C.\\n\\n3.2. CodeActAgent\\n\\nWe fine-tune Llama-2 7B (Touvron et al., 2023) and Mistral 7B (Jiang et al., 2023) on a mixture of CodeActInstruct and general conversations (Tab. 4) to obtain CodeActAgent.\\n\\nTraining Setup. We perform full-parameter supervised fine-tuning with a sequence length of 4,096 tokens for Llama-2 and 16,384 for Mistral. Please refer to \u00a7D for more details.\\n\\nEvaluation Setup. We use MINT (Wang et al., 2023e) to evaluate LLMs with CodeAct on a diverse range of agent tasks. CodeActAgent has some training domains...\"}"}
{"id": "jJ9BoXAfFa", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Evaluation results for CodeActAgent. The best results among all open-source LLMs are bolded, and the second-best results are underlined. ID and OD stand for in-domain and out-of-domain evaluation correspondingly. Overall averaged performance normalizes the MT-Bench score to be consistent with other tasks and excludes in-domain tasks for fair comparison.\\n\\n| Agent Tasks | Generic Tasks | Overall Average |\\n|-------------|---------------|----------------|\\n| Model Size  | MINT (ID)     | MINT (OD) M3   |\\n|             | ToolEval (OD)| Miniwob++ SciWorld MMLU HumanEval GSM8K MTBench |\\n| Open-source LLMs (LLaMA-2-based) | | |\\n| Llama2 Base 7B | - | - | - | - | - | 45.3 | 12.8 | 14.6 | - | - | 45.9 |\\n| Llama2 Chat 7B | 3.2 | 11.0 | 0.0 | 0.0 | 0.0 | 6.0 | 5.9 | 4.9 | - | - | 21.1 |\\n| FireAct (Chen et al., 2023a) | 0.0 | 0.3 | 0.3 | 0.0 | 0.0 | 6.8 | 4.4 | 3.5 | - | - | 14.0 |\\n| AgentLM (Zeng et al., 2023) | 8.7 | 6.1 | 0.0 | 28.9 | 13.7 | 48.7 | 15.4 | 24.6 | 6.5 | 24.8 |\\n| CodeActAgent (LLaMA-2) | 51.3 | 20.4 | 0.0 | 25.5 | 17.6 | 50.6 | 18.1 | 38.3 | 7.5 | 30.7 |\\n| Open-source LLMs (Mistral-based) | | |\\n| Mistral Base 7B | - | - | - | - | - | 60.2 | 30.5 | 52.1 | - | - | 60.7 |\\n| Mistral Instruct 7B | 18.8 | 9.7 | 0.0 | 0.0 | 4.5 | 53.8 | 29.3 | 43.3 | 6.4 | 25.6 |\\n| CodeActAgent (Mistral) | 57.4 | 32.4 | 12.2 | 46.2 | 15.9 | 59.7 | 34.7 | 58.1 | 8.2 | 42.5 |\\n| Closed-source LLMs | | |\\n| gpt-3.5-turbo-0613 | 33.9 | 38.2 | 51.2 | 66.7 | 21.2 | 70.0 | 48.1 | 57.1 | 7.9 | 54.0 |\\n| gpt-4-0613 | 68.6 | 70.2 | 67.1 | 69.4 | 36.4 | 86.4 | 67.0 | 87.0 | 9.0 | 71.7 |\\n\\nSome results are only available with instruction-tuned models.\\n\\nOverlapping with MINT's evaluation (i.e., MINT includes ALFWorld and MATH), hence we report separate numbers for MINT's in- and out-of-domain performance. Unless otherwise specified, we measure MINT tasks' success rates with interaction turn \\\\( k = 5 \\\\). We also evaluate out-of-domain agent tasks using text actions from MiniWob++ (computer tasks, (Kim et al., 2023)) and ScienceWorld (text-based simulator for elementary science curriculum, (Wang et al., 2022a)) to test whether CodeActAgent can generalize to different action formats. Finally, we include a suite of general LLM evaluation tasks to assess general capability: MMLU (Hendrycks et al., 2020) for knowledge-based QA, HumanEval (Chen et al., 2021) for single-turn code-generation, GSM8K (Cobbe et al., 2021) for single-turn tool-free math reasoning, and MTBench (Zheng et al., 2023) for instruction-following.\\n\\n**CodeActAgent Excels in CodeAct Task.** As shown in Tab. 5, CodeActAgent (both variants) perform better than all evaluated open-source LLMs on both the in- and out-of-domain subsets of MINT. On M3 ToolEval, we find CodeActAgent (Mistral) outperforms open-source LLMs of similar size (7B and 13B) and even reaches similar performance to those 70B models (Tab. 3). Surprisingly, no improvement is observed for the Llama-2 variant. We discuss potential reasons in \u00a7H.\\n\\n**CodeActAgent Generalizes to Text Action.** When evaluated on out-of-domain text actions, CodeActAgent (LLaMA2, 7B), which has never been optimized for text action, achieves comparable performance to AgentLM-7B (Zeng et al., 2023) which has explicit tuning for text actions.\\n\\n**CodeActAgent Maintains or Improves the Performance on General LLM Tasks.** In Tab. 5, we find that CodeActAgent (both variants) performs better on generic LLM tasks we tested, except for a slight degradation on MMLU for CodeActAgent (Mistral, 7B).\\n\\n**Ablation Study.** Tab. A.8 presents ablation experiments to determine the importance of CodeActInstruct and general conversations. Both CodeActInstruct and general conversations contribute to agent tasks, while general conversations are essential to maintain performance on general tasks.\\n\\n**4. Related Work**\\n\\n**4.1. Action Module in LLM Agents**\\n\\nAs detailed in (Wang et al., 2023b), LLM-based autonomous agents are typically structured around four components: customized profiles (Park et al., 2023; Qian et al., 2023), long-term memory capabilities (Zhu et al., 2023; Fischer, 2023), reasoning and planning algorithms (Wei et al., 2022; Chen et al., 2023d), and, most crucially, action modules. The action modules are key to facilitating LLM agents to effectively interact with external entities, including humans (Lee et al., 2022) and tools (Qin et al., 2023a) in the environment (Wang et al., 2023e; Yang et al., 2024a). In this study, we address the critical problem of standardizing the action space for LLM agents. We further discuss the difference between CodeAct and the line of work that uses code generation for problem-solving in \u00a7A. We notice a concurrent study TaskWeaver (Qiao et al., 2023) similarly endorses the use of code. We discuss the principal distinctions in \u00a7B.\\n\\n**4.2. Improving LLM Agents**\\n\\nTwo primary methods for enhancing LLM agents are prompt engineering and instruction tuning, as surveyed by (Wang et al., 2023b). For prompt engineering (Liu et al., 2023a), numerous strategies have been introduced to improve the chain-of-thought reasoning (Wei et al., 2022), including self-consistency-based reasoning (Wang et al., 2022b; Chen et al., 2023d) and tree-based approaches (Yao et al., 2023a). Moreover, LLMs can be strategically prompted to reflect on\"}"}
{"id": "jJ9BoXAfFa", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Executable Code Actions Elicit Better LLM Agents\\n\\nprevious plans (Yao et al., 2023b; Wang et al., 2023f; Zhang et al., 2023), enabling them to refine initial actions through trial and error. Contrast to prompt engineering, instruction tuning intrinsically enhances LLMs (Chung et al., 2022), particularly in their agent capabilities (Zeng et al., 2023; Chen et al., 2023a). For effective training, human annotators can curate expert demonstrations for specific agent tasks, such as web browsing (Yao et al., 2022a; Nakano et al., 2021). To minimize human annotation efforts, prior work creates synthetic datasets using stronger LLMs to distill agent capabilities into local models, focusing on tool usage (Qin et al., 2023b), interaction (Chen et al., 2023c), and social skills (Liu et al., 2023b). CodeActInstruct aligns with the latter approach and creates datasets using stronger LLMs.\\n\\n5. Conclusions\\n\\nThis work introduces CodeAct that employs executable Python code for the LLM agent's action, which is advantageous over using text or JSON action, especially in complex scenarios. We collect CodeAct-focused multi-turn interaction trajectories CodeActInstruct for instruction tuning, and train CodeActAgent that is specially designed for seamless integration with Python and can execute sophisticated tasks (e.g., model training) leveraging existing Python packages and autonomously rectifying errors through self-debugging.\\n\\nAcknowledgement\\n\\nWe thank the anonymous reviewers for their suggestions and comments. This research is based upon work supported by U.S. DARPA ECOLE Program No. HR00112390060 and U.S. DARPA ITM Program No. FA8650-23-C-7316 and KAIROS Program No. FA8750-19-2-1004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. This work used the Delta system at the National Center for Supercomputing Applications through allocation CIS230256 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS, Boerner et al. 2023) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.\\n\\nImpact Statement\\n\\nThis paper presents work whose goal is to advance LLM-based autonomous agents that can communicate with humans through natural language and assist human users by performing tasks in environments on behalf of humans. In this section, we discuss potential societal consequences, limitations, and future work related to our work and its goal.\\n\\nCodeActAgent is an initial prototype of an autonomous agent and still has several practical limitations. For example, it may suffer from hallucination commonly seen in LLMs (e.g., imagine the content of a variable without actually printing it out), suggesting the need for subsequent alignment (Ouyang et al., 2022) for further improvements.\\n\\nDespite being a prototype, CodeActAgent has already demonstrated limited self-improving capability (e.g., self-debug error messages to improve its action) and the ability to interact with environments. Future work may build upon CodeActAgent to develop better agents by having them perform extensive interactions within a given environment and iteratively bootstrap their self-improving capability to learn to improve from past mistakes. More powerful agents, as results of such algorithms, are potentially beneficial for solving a wide range of real-world problems (e.g., theorem proving, drug discovery). As extensively discussed in (Eloundou et al., 2023), a fully autonomous agent may transform the current landscape of the labor market and impact the jobs of existing workers.\\n\\nFurthermore, since CodeAct directly grants access for the agent to freely execute code in a sandbox environment, in the worst scenario (e.g., in Sci-Fi movies), such an agent may potentially break free of the sandbox restriction and cause harm to the world through cyber-attack, highlighting the need for future work to design better safety mechanisms to safeguard autonomous agents (Tang et al., 2024).\\n\\nReferences\\n\\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi, N., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022.\\n\\nAnonymous. Sharegpt dataset. https://hf.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json, 2023. A dataset containing multi-turn conversations between human and LLM assistant.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Executable Code Actions Elicit Better LLM Agents\\n\\nBoerner, T. J., Deems, S., Furlani, T. R., Knuth, S. L., and Towns, J. Access: Advancing innovation: Nsf\u2019s advanced cyberinfrastructure coordination ecosystem: Services & support. In Practice and Experience in Advanced Research Computing, pp. 173\u2013176. 2023.\\n\\nBran, A. M., Cox, S., White, A. D., and Schwaller, P. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.\\n\\nCano, A. H., Pagliardini, M., K\u00f6pf, A., Matoba, K., Mohtashami, A., Wang, X., Fan, O. S., Marmet, A., Bayazit, D., Krawczuk, I., Chen, Z., Salvi, F., Bosselut, A., and Jaggi, M. epfllm megatron-llm, 2023. URL https://github.com/epfLLM/Megatron-LLM.\\n\\nChase, H. LangChain, October 2022. URL https://github.com/langchain-ai/langchain.\\n\\nChen, B., Shu, C., Shareghi, E., Collier, N., Narasimhan, K., and Yao, S. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023a.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n\\nChen, X., Lin, M., Sch\u00e4rli, N., and Zhou, D. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023b.\\n\\nChen, Y., Sikka, K., Cogswell, M., Ji, H., and Divakaran, A. Dress: Instructing large vision-language models to align and interact with humans via natural language feedback. arXiv preprint arXiv:2311.10081, 2023c.\\n\\nChen, Y., Sikka, K., Cogswell, M., Ji, H., and Divakaran, A. Measuring and improving chain-of-thought reasoning in vision-language models. arXiv preprint arXiv:2309.04461, 2023d.\\n\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\\n\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n\\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. Gpts are gpts: An early look at the labor market impact potential of large language models. arXiv preprint arXiv:2303.10130, 2023.\\n\\nFischer, K. A. Reflective linguistic programming (rlp): A stepping stone in socially-aware agi (socialagi). arXiv preprint arXiv:2305.12647, 2023.\\n\\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764\u201310799. PMLR, 2023.\\n\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.\\n\\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., et al. Measuring coding challenge competence with apps. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021a.\\n\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b.\\n\\nHong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.\\n\\nHong, S., Lin, Y., Liu, B., Liu, B., Wu, B., Li, D., Chen, J., Zhang, J., Wang, J., Zhang, L., Zhang, L., Yang, M., Zhuge, M., Guo, T., Zhou, T., Tao, W., Wang, W., Tang, X., Lu, X., Zheng, X., Liang, X., Fei, Y., Cheng, Y., Xu, Z., and Wu, C. Data interpreter: An llm agent for data science, 2024.\\n\\nHuang, W., Wang, C., Zhang, R., Li, Y., Wu, J., and Fei-Fei, L. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023.\\n\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\\n\\nKim, G., Baldi, P., and McAleer, S. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023.\\n\\nLDJnr. Capybara dataset. https://hf.co/datasets/LDJnr/Verified-Camel, https://hf.co/datasets/LDJnr/Pure-Dove, https://hf.co/datasets/LDJnr/10.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: The benefit of CodeAct compared to using Text/JSON for LLM action.\\n\\n| Benefit                                      | CodeAct for LLM action | JSON or Text for LLM action |\\n|----------------------------------------------|------------------------|----------------------------|\\n| Availability of Data                         | Large quantity of code available for pre-training | Data curation required for particular format |\\n| Complex Operation                           | Natively supported via control and data flow | Requires careful engineering if feasible (e.g., define new tools to mimic if-statement) |\\n| Availability of Tools                        | Can directly use existing software packages | Requires human effort to curate tools from scratch or existing software |\\n| Automated Feedback                          | Feedback mechanism is already implemented as an infrastructure for most programming languages | Requires human effort to provide feedback or re-route feedback from the underlying programming language used to implement the tools |\\n\\n1 Including code demonstrating useful behaviors for LLM agents (e.g., task decomposition, coordination of multiple function calls to different tools).\\n2 Human-written Python packages covering a wide range of applications are available on https://pypi.org/.\\n3 For example, in Python, errors and exceptions (https://docs.python.org/3/tutorial/errors.html) are available. Most software provides error messages in natural language to help human programmers debug their code. CodeAct enables LLM to use them directly.\\n\\nIn this section, we first describe CodeAct framework (\u00a72.1) and provide empirical evidence that supports the choice of CodeAct. We focus on Python as the programming language for CodeAct due to its popularity (ranked top-1 at TIOBE Index, 2024) and numerous open-source packages.\\n\\nWe aim to answer several research questions (RQs) using 17 off-the-shelf LLMs. In \u00a72.2, we examine RQ1: Does LLMs' familiarity with code due to a large amount of code pre-training data bring CodeAct advantages over text and JSON? We discuss RQ2 in \u00a72.3: Does CodeAct benefit from Python's innate control and data flow feature in complex problems? Finally, as an additional benefit, we discuss how using CodeAct further enhances LLM agents by enabling multi-turn interactions and allowing them to access existing software in \u00a72.4 and Fig. 3.\\n\\n2.1. What is CodeAct?\\nIn Fig. 2, we first introduce a general multi-turn interaction framework for LLM agents' real-world usage that considers three roles (Yang et al., 2024c): agent, user, and environment. We define interaction as the information exchange between the agent and an external entity (user or environment). For each turn of interaction, the agent receives an observation (input) either from the user (e.g., natural language instruction) or the environment (e.g., code execution result), optionally planning for its action through chain-of-thought (Wei et al., 2022), and emits an action (output) to either user in natural language or the environment. CodeAct employs Python code to consolidate all actions for agent-environment interaction. In CodeAct, each emitted action to the environment is a piece of Python code, and the agent will receive outputs of code execution (e.g., results, errors) as observation. We include an example prompt of CodeAct in \u00a7E.\\n\\n2.2. CodeAct Shows the Promise as a Strong Tool Use Framework\\nIn this section, we perform a controlled experiment to understand which format (text, JSON, CodeAct) is more likely to lead an LLM to generate correct atomic tool calls. The performance in this experiment reflects LLM's familiarity with the corresponding format. We hypothesize that using CodeAct to call tools is a more natural way to use tools for the models, which typically have extensive exposure to\\n\\n3\\n\\nError messages from the environment further enable it to rectify errors autonomously through self-debugging in multi-turn interaction. Thanks to LLM's extensive programming knowledge acquired during pre-training, these are achieved without needing in-context demonstrations, reducing the human efforts for adapting CodeActAgent to different tasks.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: General agent multi-turn interaction framework that describes the role of CodeAct and motivates the construction of our data mixture. CodeActInstruct focuses on the agent-environment interactions and specifically filters for the self-improved planning behavior, while general conversation data we include focuses on agent-user interaction (\u00a73.1).\\n\\nSetup. We re-purpose API-Bank (Li et al., 2023) and test LLMs' API-calling performance, comparing CodeAct, JSON, and text actions. For each evaluation instance, we instruct LLM to generate one atomic tool call in the format of a Python function call, JSON object, or text expression in a pre-defined format. A concrete example is shown in Tab. A.6. We use API-Bank's level-1 instructions and the provided toolset. To evaluate API-calling, we follow their correctness metric, matching the ground-truth API outputs with the actual model-generated API's execution outputs.\\n\\nResults. We present results in Tab. 2. For most LLMs, CodeAct achieves comparable or better performance even in atomic actions (the simplistic tool use scenario) where its control and data flow strengths are ablated. Compared to closed-source LLMs, CodeAct's improvements are more prominent in open-source models. Furthermore, code data is usually more accessible for fine-tuning open-source LLMs than the specialized JSON or text tool-calling format. Although JSON is consistently weaker than other approaches for open-source models, it achieves decent performance with closed-source LLMs, indicating that these closed-source models may have gone through targeted fine-tuning toward their JSON capabilities. These results suggest optimizing for CodeAct is a better route for open-source LLMs than alternatives to improve their tool-use capabilities, as they already show good initial CodeAct capability due to extensive exposure to code data during pre-training.\\n\\n2.3. CodeAct Gets More Done with Fewer Interactions\\n\\nIn this section, we investigate whether LLM agents can benefit from the control and data flow of code on problems that require complex patterns of tool use.\\n\\nM\u2083ToolEval. As shown in Tab. A.7, to the best of our knowledge, no existing tool-use benchmarks contain complex tasks requiring the composition of multiple tools while supporting evaluating different action formats. Hence, we curate a benchmark M\u2083ToolEval to fill this gap, which evaluates LLMs' capabilities in solving complex tasks that typically require multiple calls to multiple tools in multi-turn interactions. It contains 82 human-curated instances, spanning tasks including web browsing, finance, travel itinerary planning, science, and information processing. Each domain is accompanied by a unique set of manually crafted tools. We intentionally keep the prompt simple (examples in \u00a7F) and avoid providing any demonstration to test the LLM's zero-shot ability to use tools, similar to how a novice user without knowledge of few-shot prompting would use the model.\\n\\nSetup. We allow the model to generate fully functional Python code that enables control and data flow (e.g., if-statement, for-loop). We follow the action format for JSON and text described in Tab. A.6. Within each turn, the model can either emit an action or propose an answer to be verified by an exact match with the ground-truth solution. The interaction will terminate when a maximum of 10 interaction turns are reached or a correct solution has been submitted, similar to (Wang et al., 2023e).\\n\\nMetric. We measure the success rate by calculating the percentage of the model proposed answers that match the ground-truth solutions. We also include the avg. turns metric: the average number of turns on all evaluated instances.\\n\\nQuantitative Results on M\u2083ToolEval. We include full results in Tab. 3 and a subset of results for visualization in 4.\"}"}
{"id": "jJ9BoXAfFa", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"without requiring user-provided tools or demonstrations. As (12 out of 17 evaluated LLMs), similar to the trend in \u00a72.2. This is potentially due to open-source models' weak text-davinci-003 text-davinci-002 gpt-4-1106-preview gpt-4-0613 gpt-3.5-turbo-1106 M-Mistral-7B-Instruct-v0.1 CodeLlama-34b-Instruct-hf CodeLlama-13b-Instruct-hf Open-source CodeAct Format of Action learning train-test data split and regression model training, load and process tabular data, use Scikit-Learn for machine learning purposes illustrated in Fig. 3, CodeActAgent can use Pandas to download the correct Python libraries learned during pre-training, the LLM agent can automate interactions. Thanks to its extensive knowledge of Python existing software to perform complex tasks in multi-turn interactions. In Fig. 3, we show how an LLM agent can integrate with CodeAct 2.4. under the zero-shot setting.\\n\\nto improve open-source LLMs for practical, real-world tasks instructions without demonstration, suggesting an urgent need task-solving capability and inability to follow complex in-\\n\\nFig. 1. Overall\\n\\n4 Closed-source\\n\\n12\\n\\nClosed-source\\n\\n$\\\\text{Closed-source}$ LLMs as the best open-source model achieves 13.4% while the best closed-source model\\n\\nachieves a $\\\\text{Closed-source}$ performance between open- and closed-source\\n\\nHowever, there is still a significant gap in terms of absolute improvement compared to the next best action format.\\n\\nMoreover, using\\n\\nFig. 1. Overall\\n\\nClosed-source\\n\\n$\\\\text{Closed-source}$ generally has a higher task success rate and Existing Software Packages\\n\\nBenefits from Multi-turn Interactions\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n12\\n\\n13\\n\\n14\\n\\n15\\n\\n16\\n\\n17\\n\\n18\\n\\n19\\n\\n20\\n\\n21\\n\\n22\\n\\n23\\n\\n24\\n\\n25\\n\\n26\\n\\n27\\n\\n28\\n\\n29\\n\\n30\\n\\n31\\n\\n32\\n\\n33\\n\\n34\\n\\n35\\n\\n36\\n\\n37\\n\\n38\\n\\n39\\n\\n40\\n\\n41\\n\\n42\\n\\n43\\n\\n44\\n\\n45\\n\\n46\\n\\n47\\n\\n48\\n\\n49\\n\\n50\\n\\n51\\n\\n52\\n\\n53\\n\\n54\\n\\n55\\n\\n56\\n\\n57\\n\\n58\\n\\n59\\n\\n60\\n\\n61\\n\\n62\\n\\n63\\n\\n64\\n\\n65\\n\\n66\\n\\n67\\n\\n68\\n\\n69\\n\\n70\\n\\n71\\n\\n72\\n\\n73\\n\\n74\\n\\n75\\n\\n76\\n\\n77\\n\\n78\\n\\n79\\n\\n80\\n\\n81\\n\\n82\\n\\n83\\n\\n84\\n\\n85\\n\\n86\\n\\n87\\n\\n88\\n\\n89\\n\\n90\\n\\n91\\n\\n92\\n\\n93\\n\\n94\\n\\n95\\n\\n96\\n\\n97\\n\\n98\\n\\n99\\n\\n100\\n\\n101\\n\\n102\\n\\n103\\n\\n104\\n\\n105\\n\\n106\\n\\n107\\n\\n108\\n\\n109\\n\\n110\\n\\n111\\n\\n112\\n\\n113\\n\\n114\\n\\n115\\n\\n116\\n\\n117\\n\\n118\\n\\n119\\n\\n120\\n\\n121\\n\\n122\\n\\n123\\n\\n124\\n\\n125\\n\\n126\\n\\n127\\n\\n128\\n\\n129\\n\\n130\\n\\n131\\n\\n132\\n\\n133\\n\\n134\\n\\n135\\n\\n136\\n\\n137\\n\\n138\\n\\n139\\n\\n140\\n\\n141\\n\\n142\\n\\n143\\n\\n144\\n\\n145\\n\\n146\\n\\n147\\n\\n148\\n\\n149\\n\\n150\\n\\n151\\n\\n152\\n\\n153\\n\\n154\\n\\n155\\n\\n156\\n\\n157\\n\\n158\\n\\n159\\n\\n160\\n\\n161\\n\\n162\\n\\n163\\n\\n164\\n\\n165\\n\\n166\\n\\n167\\n\\n168\\n\\n169\\n\\n170\\n\\n171\\n\\n172\\n\\n173\\n\\n174\\n\\n175\\n\\n176\\n\\n177\\n\\n178\\n\\n179\\n\\n180\\n\\n181\\n\\n182\\n\\n183\\n\\n184\\n\\n185\\n\\n186\\n\\n187\\n\\n188\\n\\n189\\n\\n190\\n\\n191\\n\\n192\\n\\n193\\n\\n194\\n\\n195\\n\\n196\\n\\n197\\n\\n198\\n\\n199\\n\\n200\\n\\n201\\n\\n202\\n\\n203\\n\\n204\\n\\n205\\n\\n206\\n\\n207\\n\\n208\\n\\n209\\n\\n210\\n\\n211\\n\\n212\\n\\n213\\n\\n214\\n\\n215\\n\\n216\\n\\n217\\n\\n218\\n\\n219\\n\\n220\\n\\n221\\n\\n222\\n\\n223\\n\\n224\\n\\n225\\n\\n226\\n\\n227\\n\\n228\\n\\n229\\n\\n230\\n\\n231\\n\\n232\\n\\n233\\n\\n234\\n\\n235\\n\\n236\\n\\n237\\n\\n238\\n\\n239\\n\\n240\\n\\n241\\n\\n242\\n\\n243\\n\\n244\\n\\n245\\n\\n246\\n\\n247\\n\\n248\\n\\n249\\n\\n250\\n\\n251\\n\\n252\\n\\n253\\n\\n254\\n\\n255\\n\\n256\\n\\n257\\n\\n258\\n\\n259\\n\\n260\\n\\n261\\n\\n262\\n\\n263\\n\\n264\\n\\n265\\n\\n266\\n\\n267\\n\\n268\\n\\n269\\n\\n270\\n\\n271\\n\\n272\\n\\n273\\n\\n274\\n\\n275\\n\\n276\\n\\n277\\n\\n278\\n\\n279\\n\\n280\\n\\n281\\n\\n282\\n\\n283\\n\\n284\\n\\n285\\n\\n286\\n\\n287\\n\\n288\\n\\n289\\n\\n290\\n\\n291\\n\\n292\\n\\n293\\n\\n294\\n\\n295\\n\\n296\\n\\n297\\n\\n298\\n\\n299\\n\\n300\\n\\n301\\n\\n302\\n\\n303\\n\\n304\\n\\n305\\n\\n306\\n\\n307\\n\\n308\\n\\n309\\n\\n310\\n\\n311\\n\\n312\\n\\n313\\n\\n314\\n\\n315\\n\\n316\\n\\n317\\n\\n318\\n\\n319\\n\\n320\\n\\n321\\n\\n322\\n\\n323\\n\\n324\\n\\n325\\n\\n326\\n\\n327\\n\\n328\\n\\n329\\n\\n330\\n\\n331\\n\\n332\\n\\n333\\n\\n334\\n\\n335\\n\\n336\\n\\n337\\n\\n338\\n\\n339\\n\\n340\\n\\n341\\n\\n342\\n\\n343\\n\\n344\\n\\n345\\n\\n346\\n\\n347\\n\\n348\\n\\n349\\n\\n350\\n\\n351\\n\\n352\\n\\n353\\n\\n354\\n\\n355\\n\\n356\\n\\n357\\n\\n358\\n\\n359\\n\\n360\\n\\n361\\n\\n362\\n\\n363\\n\\n364\\n\\n365\\n\\n366\\n\\n367\\n\\n368\\n\\n369\\n\\n370\\n\\n371\\n\\n372\\n\\n373\\n\\n374\\n\\n375\\n\\n376\\n\\n377\\n\\n378\\n\\n379\\n\\n380\\n\\n381\\n\\n382\\n\\n383\\n\\n384\\n\\n385\\n\\n386\\n\\n387\\n\\n388\\n\\n389\\n\\n390\\n\\n391\\n\\n392\\n\\n393\\n\\n394\\n\\n395\\n\\n396\\n\\n397\\n\\n398\\n\\n399\\n\\n400\\n\\n401\\n\\n402\\n\\n403\\n\\n404\\n\\n405\\n\\n406\\n\\n407\\n\\n408\\n\\n409\\n\\n410\\n\\n411\\n\\n412\\n\\n413\\n\\n414\\n\\n415\\n\\n416\\n\\n417\\n\\n418\\n\\n419\\n\\n420\\n\\n421\\n\\n422\\n\\n423\\n\\n424\\n\\n425\\n\\n426\\n\\n427\\n\\n428\\n\\n429\\n\\n430\\n\\n431\\n\\n432\\n\\n433\\n\\n434\\n\\n435\\n\\n436\\n\\n437\\n\\n438\\n\\n439\\n\\n440\\n\\n441\\n\\n442\\n\\n443\\n\\n444\\n\\n445\\n\\n446\\n\\n447\\n\\n448\\n\\n449\\n\\n450\\n\\n451\\n\\n452\\n\\n453\\n\\n454\\n\\n455\\n\\n456\\n\\n457\\n\\n458\\n\\n459\\n\\n460\\n\\n461\\n\\n462\\n\\n463\\n\\n464\\n\\n465\\n\\n466\\n\\n467\\n\\n468\\n\\n469\\n\\n470\\n\\n471\\n\\n472\\n\\n473\\n\\n474\\n\\n475\\n\\n476\\n\\n477\\n\\n478\\n\\n479\\n\\n480\\n\\n481\\n\\n482\\n\\n483\\n\\n484\\n\\n485\\n\\n486\\n\\n487\\n\\n488\\n\\n489\\n\\n490\\n\\n491\\n\\n492\\n\\n493\\n\\n494\\n\\n495\\n\\n496\\n\\n497\\n\\n498\\n\\n499\\n\\n500\\n\\n501\\n\\n502\\n\\n503\\n\\n504\\n\\n505\\n\\n506\\n\\n507\\n\\n508\\n\\n509\\n\\n510\\n\\n511\\n\\n512\\n\\n513\\n\\n514\\n\\n515\\n\\n516\\n\\n517\\n\\n518\\n\\n519\\n\\n520\\n\\n521\\n\\n522\\n\\n523\\n\\n524\\n\\n525\\n\\n526\\n\\n527\\n\\n528\\n\\n529\\n\\n530\\n\\n531\\n\\n532\\n\\n533\\n\\n534\\n\\n535\\n\\n536\\n\\n537\\n\\n538\\n\\n539\\n\\n540\\n\\n541\\n\\n542\\n\\n543\\n\\n544\\n\\n545\\n\\n546\\n\\n547\\n\\n548\\n\\n549\\n\\n550\\n\\n551\\n\\n552\\n\\n553\\n\\n554\\n\\n555\\n\\n556\\n\\n557\\n\\n558\\n\\n559\\n\\n560\\n\\n561\\n\\n562\\n\\n563\\n\\n564\\n\\n565\\n\\n566\\n\\n567\\n\\n568\\n\\n569\\n\\n570\\n\\n571\\n\\n572\\n\\n573\\n\\n574\\n\\n575\\n\\n576\\n\\n577\\n\\n578\\n\\n579\\n\\n580\\n\\n581\\n\\n582\\n\\n583\\n\\n584\\n\\n585\\n\\n586\\n\\n587\\n\\n588\\n\\n589\\n\\n590\\n\\n591\\n\\n592\\n\\n593\\n\\n594\\n\\n595\\n\\n596\\n\\n597\\n\\n598\\n\\n599\\n\\n600\\n\\n601\\n\\n602\\n\\n603\\n\\n604\\n\\n605\\n\\n606\\n\\n607\\n\\n608\\n\\n609\\n\\n610\\n\\n611\\n\\n612\\n\\n613\\n\\n614\\n\\n615\\n\\n616\\n\\n617\\n\\n618\\n\\n619\\n\\n620\\n\\n621\\n\\n622\\n\\n623\\n\\n624\\n\\n625\\n\\n626\\n\\n627\\n\\n628\\n\\n629\\n\\n630\\n\\n631\\n\\n632\\n\\n633\\n\\n634\\n\\n635\\n\\n636\\n\\n637\\n\\n638\\n\\n639\\n\\n640\\n\\n641\\n\\n642\\n\\n643\\n\\n644\\n\\n645\\n\\n646\\n\\n647\\n\\n648\\n\\n649\\n\\n650\\n\\n651\\n\\n652\\n\\n653\\n\\n654\\n\\n655\\n\\n656\\n\\n657\\n\\n658\\n\\n659\\n\\n660\\n\\n661\\n\\n662\\n\\n663\\n\\n664\\n\\n665\\n\\n666\\n\\n667\\n\\n668\\n\\n669\\n\\n670\\n\\n671\\n\\n672\\n\\n673\\n\\n674\\n\\n675\\n\\n676\\n\\n677\\n\\n678\\n\\n679\\n\\n680\\n\\n681\\n\\n682\\n\\n683\\n\\n684\\n\\n685\\n\\n686\\n\\n687\\n\\n688\\n\\n689\\n\\n690\\n\\n691\\n\\n692\\n\\n693\\n\\n694\\n\\n695\\n\\n696\\n\\n697\\n\\n698\\n\\n699\\n\\n700\\n\\n701\\n\\n702\\n\\n703\\n\\n704\\n\\n705\\n\\n706\\n\\n707\\n\\n708\\n\\n709\\n\\n710\\n\\n711\\n\\n712\\n\\n713\\n\\n714\\n\\n715\\n\\n716\\n\\n717\\n\\n718\\n\\n719\\n\\n720\\n\\n721\\n\\n722\\n\\n723\\n\\n724\\n\\n725\\n\\n726\\n\\n727\\n\\n728\\n\\n729\\n\\n730\\n\\n731\\n\\n732\\n\\n733\\n\\n734\\n\\n735\\n\\n736\\n\\n737\\n\\n738\\n\\n739\\n\\n740\\n\\n741\\n\\n742\\n\\n743\\n\\n744\\n\\n745\\n\\n746\\n\\n747\\n\\n748\\n\\n749\\n\\n750\\n\\n751\\n\\n752\\n"}
{"id": "jJ9BoXAfFa", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Software Package (Tool) Usage:\\n\\ndomains to generate trajectories: https://chat.xwang.dev/r/Vqn108G\\n\\nFigure 3: Example multi-turn interaction with Python packages using CodeActAgent (Mistral-7b).\\n\\nWikiTableQuestion (Pasupat & Liang, 2015) and tweak it into two variants of tabular reasoning tasks that require symbolic math for problem-solving.\\n\\nWe repurpose the training subset of sympy for symbolic math in APPS (Hendrycks et al., 2021b). The code generation tasks already involve set of code generation problems in APPS (Hendrycks et al., 2021b).\\n\\nWe use the training subset of potQA (Yang et al., 2018) to generate information-seeking context demonstration of importing Python packages (e.g., sklearn) in the data, which is causing the error when trying to convert the data to a certain type.\\n\\n```python\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\nimport pandas as pd\\n\\ndf = pd.read_csv(\\\"auto-mpg.csv\\\")\\n\\n# Remove rows containing '?'\\ndf = df.replace('?', np.nan)\\n\\n# Check for missing values\\nX = df.drop(columns=['mpg', 'car name'])\\ny = df['mpg']\\n\\n# Remove rows with missing values\\nX = X.dropna()\\n\\n# Train a regression model\\nmodel = LinearRegression()\\nmodel.fit(X_train, y_train)\\n\\n# Evaluate the model\\ny_pred = model.predict(X_test)\\n\\n# Calculate R^2 score for training set\\nmse_train = mean_squared_error(y_train, model.predict(X_train))\\nr2_train = r2_score(y_train, model.predict(X_train))\\n\\n# Calculate MSE (training set)\\nmean_squared_error = mean_squared_error(y_train, y_pred)\\n\\ncertainly this model has a good fit with the data [..]\\n```\\n\\n```\\nA user can ask every item except \\\"mpg\\\" and \\\"car name\\\" as the input features. spli train and test a regression model predicting its MPG. You should download and load the dataset from the URL, take a look at it, check for missing values, do a train-test split, and use the scikit-learn library to train a regression model.\\n```\\n\\nThe code generation tasks already involve the down-sample process. The statistics of the filtered dataset can be found in Tab. A.9. Please refer to \u00a7G.1 for details about the down-sample process.\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\nFollowing MINT (Wang et al., 2023e), we provide an embodied environment simulator, to perform robot planning (e.g., searching for items by visiting different locations and picking up items).\\n\\n```\\n2020), a text-only embodied environment simulator, to perform robot planning (e.g., searching for items by visiting different locations and picking up items).\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nWe use ALFWorld (Shridhar et al., 2020), a text-only embodied environment simulator, to perform robot planning (e.g., searching for items by visiting different locations and picking up items).\\n```\\n\\n```\\nData Down-sampling.\\n```\\n\\n```\\nWe down-sample each dataset by 24/561 and 18/137.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\n Examples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n\\n```\\nExamples of instructions can be found in \u00a7G.3.1.\\n```\\n"}
