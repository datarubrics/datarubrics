{"id": "FYQIgQWH3d", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Additional metrics for multi-part assembly.\\n\\nAlso, we employ two additional metrics to evaluate multi-part assembly performance. Consider a set of input point clouds $P_i = \\\\{P_i\\\\}_{i=1}^P$ with $P$ parts. The ground truth SE(3) relative poses between the point clouds are represented by $\\\\{T_{GTi}\\\\}_{i=1}^P$, while the prediction is denoted as $\\\\{Ti\\\\}_{i=1}^P$. Similar to pairwise assembly, the assembled object can be represented with $\\\\bigcap_{i=1}^P T_i(P_i)$. Note that in our context, the direction of pose is defined as the transformation that aligns each part $P_i$ with the coordinate frame of largest fracture as anchor.\\n\\nPart Accuracy (Chamfer Distance-based).\\n\\nPart accuracy (PA) (Li et al., 2020b) is defined as the percentage of fractures with Chamfer Distance (CD) less than the predefined threshold $\\\\tau_{CD} = 0.01$:\\n\\n$$PA_{CD} = \\\\frac{1}{P} \\\\sum_{i=1}^P \\\\frac{1}{|P_i|} \\\\sum_{j=1}^{|P_i|} \\\\frac{d_{CD}(T_i(P_i)_j, T_{GTi}(P_i)_j)}{\\\\tau_{CD}}.$$  \\n\\nPart Accuracy (Correspondence Distance-based).\\n\\nOur proposed CoRrespondence Distance (CRD) can be seamlessly adapted for Part Accuracy (PA) evaluation. This adaptation involves substituting the Chamfer Distance (CD) with the CoRrespondence Distance (CRD), and setting the threshold $\\\\tau_{CRD} = 0.1$:\\n\\n$$PA_{CRD} = \\\\frac{1}{P} \\\\sum_{i=1}^P \\\\frac{1}{|P_i|} \\\\sum_{j=1}^{|P_i|} \\\\frac{\\\\|T_i(P_i)_j - T_{GTi}(P_i)_j\\\\|_F}{\\\\tau_{CRD}}.$$  \\n\\n(29)\\n\\n16\"}"}
{"id": "FYQIgQWH3d", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Additional qualitative results of pairwise shape assembly on Breaking Bad dataset.\"}"}
{"id": "FYQIgQWH3d", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Additional qualitative results of multipart shape assembly on Breaking Bad dataset.\"}"}
{"id": "FYQIgQWH3d", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nLearning to assemble geometric shapes into a larger target structure is a pivotal task in various practical applications. In this work, we tackle this problem by establishing local correspondences between point clouds of part shapes in both coarse- and fine-levels. To this end, we introduce Proxy Match Transform (PMT), an approximate high-order feature transform layer that enables reliable matching between mating surfaces of parts while incurring low costs in memory and compute. Building upon PMT, we introduce a new framework, dubbed Proxy Match TransformeR (PMTR), for the geometric assembly task. We evaluate the proposed PMTR on the large-scale 3D geometric shape assembly benchmark dataset of Breaking Bad and demonstrate its superior performance and efficiency compared to state-of-the-art methods. Project page: https://nahyuklee.github.io/pmtr.\\n\\n1. Introduction\\n\\nShape assembly aims to determine a precise placement for each constituent part and construct a larger target shape as a whole. This task holds paramount significance, especially in the context of various applications encompassing robotics (Wang & Hauser, 2019; Zakka et al., 2020; Zeng et al., 2021), manufacturing (Tian et al., 2022), computer graphics (Li et al., 2012), and computer-aided design (Chen et al., 2015; Jacobson, 2017). Despite its pivotal role in industrial productivity and the plethora of applications, the field of shape assembly remains relatively underexplored in the literature due to the intricate challenge it presents: demands for comprehensive understanding of geometric structures and analyses of pairwise relationships between local surfaces of given input parts for accurate assembly.\\n\\nThere have been several recent attempts (Schor et al., 2019; Li et al., 2020a; Wu et al., 2020; Li et al., 2020b; Huang et al., 2020; Narayan et al., 2022; Chen et al., 2022; Wu et al., 2023b) to address the task of shape assembly, but these methods fall short of achieving accurate assembly. They typically represent each part as a global embedding and perform regression to predict a placement for each part. The global encoding strategy for each part, while simplifying the process, greatly limits local information by collapsing spatial resolutions, which is necessary to localize mating surface. Indeed, accurate shape assembly requires a detailed analysis of both fine- and coarse-level spatial information of the parts in recognizing mating surfaces and establishing correspondences between the surfaces. Therefore, a promising approach would be to retain the spatially-rich part representations during the encoding phase and analyze pairwise local correspondence relationships between them for reliable localization and matching of mating surfaces.\\n\\nIn the realm of correspondence analysis within image matching, prior methods (Rocco et al., 2018; Min & Cho, 2021; Kim et al., 2022; Min et al., 2021; Rocco et al., 2020) typically utilize a high-order feature transform, i.e., high-dimensional convolution or attention, to achieve objectives of localizing relevant instances and establishing correspondences between them. The high-order feature transforms, which assess structural patterns of correlations in high-dimensional spaces, have been empirically validated for their efficacy in identifying accurate visual matches. However, the quadratic complexity with respect to input spatial resolution still remains as a significant drawback, limiting their application to only low-resolution (coarse-grained) inputs. Such a limitation becomes particularly problematic in the context of geometric assembly since meticulous alignment between parts requires to analyze high-resolution (fine-grained) to precisely identify \u2018geometric compatibility\u2019 between mating surfaces to match.\\n\\nIn this paper, we address this issue by introducing a new form of low-complexity high-order feature transform layer, dubbed Proxy Match Transform (PMT), to tackle the challenges of geometric shape assembly. The layer is designed\"}"}
{"id": "FYQIgQWH3d", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3D Geometric Shape Assembly via Efficient Point Cloud Matching\\nto align analogous local embeddings in feature space, e.g., points on mating surfaces, with sub-quadratic complexity, thus offering low-complexity yet high-order approach as illustrated in Fig. 1. We theoretically prove that the proposed PMT layer can effectively approximate the conventional high-order feature transforms (Rocco et al., 2018; Choy et al., 2020; Min & Cho, 2021) under particular conditions. To demonstrate its efficacy, we incorporate the PMT layer into a coarse-to-fine matching framework Proxy Match Transformer (PMTR), which uses PMTs for both coarse- and fine-level matching steps for establishing reliable correspondences on mating surfaces. We compare our results with recent state of the arts and provide thorough performance analysis on the standard geometric shape assembly benchmark dataset of Breaking Bad (Sell\u00e1n et al., 2022). The experiments demonstrate that our method outperforms existing approaches by a significant margin while being computationally efficient compared to the baselines.\\n\\nOur main contributions can be summarized as follows:\\n\\n\u2022 We introduce Proxy Match Transform (PMT), a low-complexity high-order feature transform layer that effectively refines the matching of the feature pair.\\n\\n\u2022 Our theoretical analysis shows that PMT effectively approximates high-order feature transform while incurring sub-quadratic memory and time complexity.\\n\\n\u2022 The performance improvements in geometric shape assembly over the state-of-the-art baselines demonstrate the effectiveness and efficiency of our approach.\\n\\n2. Related Work\\n\\n3D shape assembly & registration. Previous research in generative models for 3D objects has primarily focused on building objects through the combination of basic 3D primitives. One prevalent approach trains specialized models tailored to individual object classes, enabling the assembly of objects from volumetric primitives such as cuboids (Tulsiani et al., 2017). Conversely, Khan et al. (2019) propose a unified model that can generate cuboid primitives across various classes. Additionally, variational autoencoders (VAEs) have been employed to model objects as compositions of cuboids, offering robust abstractions that distill local geometric details and elucidate object correspondences (Kingma & Welling, 2014; Jones et al., 2020).\\n\\nParallel to these developments, research in part assembly has aimed to construct complete objects from predefined semantic parts. The method of Li et al. (2020b) predicts translations and rotations for part point clouds to assemble a target object from an image reference. Extending this, Narayan et al. (2022); Huang et al. (2020) have conceptualized part assembly as a graph learning challenge, utilizing iterative message passing techniques to integrate parts into cohesive objects. These approaches heavily rely on the PartNet dataset (Mo et al., 2019) to ensure semantic correspondence between the assembled parts and the target models, demonstrating that while geometric shapes are foundational, semantic cues can significantly guide and streamline the assembly process. Our research diverges from these methods by focusing on the assembly of parts without predefined semantics. A closely related methodology is that of Chen et al. (2022), which also tackles the problem of 3D shape assembly by integrating implicit shape reconstruction, providing a relevant benchmark.\\n\\nAdditionally, the concept of 3D shape assembly overlaps with the domain of 3D registration, especially in scenarios characterized by low overlap between a pair of point clouds. Techniques such as those proposed by Huang et al. (2021) and Yu et al. (2021) leverage self-attention and cross-attention mechanisms within and across point cloud features to transform 3D features, facilitating enhanced matching accuracy. Qin et al. (2022) further advances this by embedding transformation-invariant data into the positional embeddings of transformer layers, optimizing the matching process in low-overlap conditions. Despite their efficacy, the practical application of these methods in fine-grained matching scenarios is often constrained by the quadratic complexity associated with their matching layers, highlighting a critical area for improvement in computational efficiency and scalability. Our work addresses these challenges by proposing a novel approach that optimizes the computational demands of feature matching while maintaining high robustness.\\n\\nHigh-order feature transform for matching. High-order feature transforms are essential in (both image and point cloud) matching tasks, helping to establish consensus among correspondences within a high-dimensional space. Initially introduced by Rocco et al. (2018), the concept of a learning-based neighborhood consensus supports the identification of accurate matches by leveraging neighboring ambiguous matches between 2D images. This approach has also been adapted for 3D registration tasks, notably by Choy et al. (2020), who utilized a 6D sparse convolutional layer to filter out outlier correspondences. Given high computational complexity associated with high-order feature transforms, several studies have proposed methods to reduce this burden. Techniques such as decomposing high-dimensional convolutional kernels (Min et al., 2021) and sparsifying the correlation map with top-k scores (Rocco et al., 2020) have been effective. Further, Shi et al. (2023) enhanced matching efficiency by creating a sparse correlation matrix through the clustering of input tokens, significantly reducing the number of tokens involved. More recent advancements have integrated the self-attention mechanism to utilize global feature consensus effectively, although these methodologies, proposed by Cho et al. (2021) and Kim et al. (2022), come at a higher computational cost.\"}"}
{"id": "FYQIgQWH3d", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"High-order feature transform with quadratic complexity\\n\\nOur work introduces the Proxy Match Transform (PMT), which simplifies existing high-order feature transforms to significantly reduce computational demands. We apply PMT in a coarse-to-fine approach, identifying reliable correspondences between the mating surfaces of input parts and subsequently refining them for precise assembly.\\n\\n3. Proposed Approach\\n\\nIn the task of geometric shape assembly, analyzing geometric compatibility between fractured shapes is of utmost importance; the geometric properties of the mating surfaces should exhibit consistency, where vertices, edges, and surfaces seamlessly fit together to form a coherent structure.\\n\\nTo achieve reliable localization of mating surfaces between shapes, a model needs to analyze the compatibility of all possible feature correspondences and accurately identify spatially consistent matches.\\n\\nIn the field of visual matching and its applications (Rocco et al., 2018; Choy et al., 2020; Min & Cho, 2021; Cho et al., 2021; Min et al., 2021), a trending approach for assessing match reliability is the utilization of high-order feature transform, e.g., convolution or self-attention. This technique effectively assesses patterns within neighborhood matches in a differentiable manner.\\n\\nBuilding upon these principles, we will now explore the theoretical formulation of high-order transform, with a specific emphasis on its application for enhancing pairwise feature correlation.\\n\\nPreliminary. High-order convolution (Rocco et al., 2018; Choy et al., 2020; Min & Cho, 2021) generalizes the standard convolution by taking as input more functions, feature maps, or sets. In the context of our problem, we consider two point clouds $X = \\\\{x_i \\\\in \\\\mathbb{R}^3\\\\}_{N_{x}=1}$ and $Y = \\\\{y_i \\\\in \\\\mathbb{R}^3\\\\}_{M_{y}=1}$, and focus on the 2nd-order convolution with two sets of features $F_X$ and $F_Y$, associated with the two point clouds, respectively. For ease of notation, we represent these features in matrix form, i.e., $F_X \\\\in \\\\mathbb{R}^{\\\\mid X \\\\mid \\\\times D_{emb}}$, where $D_{emb}$ is the feature embedding dimension, and indexes each feature embedding using its associated point $x \\\\in X$ such that $(F_X)_x \\\\in \\\\mathbb{R}^{D_{emb}}$, and same goes for $F_Y$. We also express the feature correlation of two points from each point cloud, $x$ and $y$, as $C(x, y) := (F_X)_x \\\\cdot (F_Y)_y \\\\top \\\\in \\\\mathbb{R}$. The 2nd-order convolution on $(F_X, F_Y)$ with kernel $K$ is then defined as:\\n\\n$$\\\\text{Conv}(F_X, F_Y)(x, y) := \\\\sum_{n, m}^{\\\\mathbb{N}(x) \\\\times \\\\mathbb{N}(y)} C(n, m) K([n-x, m-y])$$\\n\\n(1)\\n\\nwhere $\\\\mathbb{N}(\\\\cdot)$ represents a set of neighbor points and $K : \\\\mathbb{R}^6 \\\\rightarrow \\\\mathbb{R}$ is a convolutional kernel, represented as a mapping function that takes a displacement vector onto learnable weight scalar.\\n\\nBuilding upon insights from the work of Cordonnier et al. (2020), we consider Lemma 1 which states that the conv layer in Eq. 1 can be re-formulated as a form of multi-head self-attention under sufficient conditions:\\n\\nLemma 1. Consider a bijective mapping of natural numbers, i.e., heads, onto 6-dimensional local displacements: $t(h) : [N_h] \\\\rightarrow \\\\Delta(x, y)$. Let $A(h) \\\\in \\\\mathbb{R}^{|X||Y|\\\\times|X||Y|}$ be an\"}"}
{"id": "FYQIgQWH3d", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3D Geometric Shape Assembly via Efficient Point Cloud Matching\\n\\nWu, R., Tie, C., Du, Y., Zhao, Y., and Dong, H. Leveraging SE(3) equivariance for learning 3D geometric shape assembly. In *Proc. IEEE International Conference on Computer Vision (ICCV)*, 2023.\\n\\nWu, Y. and He, K. Group normalization. In *Proc. European Conference on Computer Vision (ECCV)*, 2018.\\n\\nYu, H., Li, F., Saleh, M., Busam, B., and Ilic, S. Cofinet: Reliable coarse-to-fine correspondences for robust point-cloud registration. *Advances in Neural Information Processing Systems (NeurIPS)*, 2021.\\n\\nYu, J., Ren, L., Zhang, Y., Zhou, W., Lin, L., and Dai, G. Peal: Prior-embedded explicit attention learning for low-overlap point cloud registration. In *Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2023.\\n\\nZaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. Big bird: Transformers for longer sequences. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2021.\\n\\nZakka, K., Zeng, A., Lee, J., and Song, S. Form2fit: Learning shape priors for generalizable assembly from disassembly. In *2020 IEEE International Conference on Robotics and Automation (ICRA)*, pp. 9404\u20139410. IEEE, 2020.\\n\\nZeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M., Armstrong, T., Krasin, I., Duong, D., Sindhwani, V., et al. Transporter networks: Rearranging the visual world for robotic manipulation. In *Conference on Robot Learning*, pp. 726\u2013747. PMLR, 2021.\\n\\nZhao, H., Wei, S., Shi, D., Tan, W., Li, Z., Ren, Y., Wei, X., Yang, Y., and Pu, S. Learning symmetry-aware geometry correspondences for 6D object pose estimation. In *Proc. IEEE International Conference on Computer Vision (ICCV)*, 2023.\\n\\nZhou, Q. and Jacobson, A. Thingi10k: A dataset of 10,000 3D-printing models. *arXiv preprint arXiv:1605.04797*, 2016.\"}"}
{"id": "FYQIgQWH3d", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Theoretical Analysis of Proxy Match Transform\\n\\nWe now derive sufficient conditions such that Proxy Match Transform can express high-dimensional convolution. Our main theoretical result is given below.\\n\\n**Theorem 1.**\\nIf we assume \\n\\\\[ P(i)\\\\top P(j) = I_{\\\\text{emb}} \\\\text{ if } i = j \\\\]\\n\\\\[ P(i)\\\\top P(j) = 0 \\\\text{ otherwise} \\\\]\\nfor all \\\\( i, j \\\\in \\\\{1, \\\\ldots, N_h\\\\} \\\\), and define\\n\\\\[ A(h)(x, y), (n, m) := A(h)X(x, n) \\\\cdot A(h)Y(y, m) \\\\]\\n\\\\[ w(h) := w(h)Xw(h)Y, \\\\]\\nthen, the dot-product of Proxy Match Transform outputs with a sufficient number of heads \\\\( N_h \\\\) can express high-dimensional convolutional layer with kernel \\\\( K : \\\\mathbb{R}^6 \\\\rightarrow \\\\mathbb{R} \\\\):\\n\\\\[ \\\\text{PMT}(F_X) \\\\cdot \\\\text{PMT}(F_Y)\\\\top = \\\\text{Conv}(F_X, F_Y). \\\\]\\n\\n**Proof.**\\nWe first take the dot-product of Proxy Match Transform outputs and simplify:\\n\\\\[ \\\\text{PMT}(F_X) \\\\cdot \\\\text{PMT}(F_Y)\\\\top = \\\\begin{bmatrix} X_h \\\\in \\\\{1, \\\\ldots, N_h\\\\} A(h)X(F_X)P(h)\\\\top w(h)X \\\\end{bmatrix} \\\\begin{bmatrix} X_h \\\\in \\\\{1, \\\\ldots, N_h\\\\} A(h)Y(F_Y)P(h)\\\\top w(h)Y \\\\end{bmatrix}^\\\\top \\\\]\\n\\\\[ = \\\\sum_{(i,j) \\\\in \\\\{1, \\\\ldots, N_h\\\\}^2} \\\\delta(i, j) w(h)X A(h)X(F_X)P(h)\\\\top P(h)F_Y \\\\cdot A(h)Y(F_Y)P(h)\\\\top w(h)Y. \\\\]\\n\\nWhere \\\\( \\\\delta(i, j) \\\\) provides \\\\( 1 \\\\) if \\\\( i = j \\\\) and \\\\( 0 \\\\) otherwise.\\n\\nUsing definitions of \\\\( A(h) \\\\in \\\\mathbb{R}^{|X||Y| \\\\times |X||Y|} \\\\) and \\\\( w(h) \\\\in \\\\mathbb{R} \\\\), the output at a specific position \\\\((x, y) \\\\in \\\\mathbb{R}^6\\\\) is as follows:\\n\\\\[ \\\\text{PMT}(F_X) \\\\cdot \\\\text{PMT}(F_Y)\\\\top (x, y) = \\\\sum_{h \\\\in \\\\{1, \\\\ldots, N_h\\\\}} A(h)((x, y), (:)) C(n, m) w(h). \\\\]\\n\\nNow consider the following Lemma:\\n\\n**Lemma 1.**\\nConsider a bijective mapping of natural numbers, i.e., heads, onto 6-dimensional local displacements:\\n\\\\[ t(h) : \\\\{1, \\\\ldots, N_h\\\\} \\\\rightarrow \\\\Delta(x, y). \\\\]\\nLet \\\\( A(h) \\\\in \\\\mathbb{R}^{|X||Y| \\\\times |X||Y|} \\\\) be an attention matrix that holds the following:\\n\\\\[ A(h)(x, y), (n, m) = \\\\begin{cases} 1, \\\\text{ if } t(h) = (n, m) - (x, y) \\\\\\\\ 0, \\\\text{ otherwise} \\\\end{cases} \\\\]\\nThen, for any high-dimensional convolution with a kernel \\\\( K : \\\\mathbb{R}^6 \\\\rightarrow \\\\mathbb{R} \\\\), there exists \\\\( \\\\{w(h) \\\\in \\\\mathbb{R}\\\\}_{h \\\\in \\\\{1, \\\\ldots, N_h\\\\}} \\\\) such that following equality holds:\\n\\\\[ \\\\text{Conv}(F_X, F_Y)(x, y) = \\\\sum_{h \\\\in \\\\{1, \\\\ldots, N_h\\\\}} A(h)((x, y), (:)) C(n, m) w(h). \\\\]\"}"}
{"id": "FYQIgQWH3d", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof. Consider high-dimensional convolution at position \\\\((x, y)\\\\):\\n\\n\\\\[\\n\\\\text{Conv}(F_X, F_Y)(x, y) = X_{(n, m) \\\\in \\\\mathbb{N}(x)} \\\\times \\\\mathbb{N}(y) C_{(n, m)} K_{[n-x, m-y]} = X_{(\\\\nu, \\\\mu) \\\\in \\\\Delta(x, y)} + (\\\\nu, \\\\mu) K_{(\\\\nu, \\\\mu)} = X_{h \\\\in [N_h]} C(x, y) + t(h) K(t(h)) \\\\quad (24)\\n\\\\]\\n\\nBy applying Lemma 1, we conclude that the dot-product of Proxy Match Transform outputs is equivalent to the high-order convolution. \u25a0\\n\\nB. Efficiency of Proxy Match Transform\\n\\nTo demonstrate the superiority of the proposed PMT, we provide the efficiency comparison between different matchers, e.g., Geometric Transformer (GeoTr) by Qin et al. (2022) and Proxy Match Transform (PMT), both during training and inference phases in Tab. 7. \u201cCoarse-only\u201d and \u201cCoarse + Fine\u201d refer to two different Proxy Match Transform (PMTR) models with PMT integrated only at the coarse-level and both levels, respectively. Specifically, we measure the computational efficiency by employing Floating Point Operations Per Second (FLOPS), and to assess the memory overhead and footprint, we record the peak memory usage for each method during both the training and inference phases, as well as the number of parameters. We also provide the training/inference times required for each matcher. For clarity in our comparison, when measuring the FLOPS, number of parameters, and train/inference times, we exclude those associated with the backbone and focus solely on the matchers: the coarse- or fine-level matcher.\\n\\n| Method               | Coarse-level | Fine-level |\\n|----------------------|--------------|------------|\\n|                      | (G)          | (K)        |\\n|                      | FLOPS        | # Param.   |\\n|                      | Mem. train   | Mem. test  |\\n|                      | Train time   | Inference time |\\n| GeoTransformer (2022)| GeoTr None   | 9.67       |\\n|                      |              | 926.85     |\\n|                      |              | 6.96       |\\n|                      |              | 3.10       |\\n|                      |              | 8.93       |\\n|                      |              | 8.04       |\\n| PMTR (Coarse-only)   | PMT None     | 0.45       |\\n|                      |              | 273.85     |\\n|                      |              | 2.12       |\\n|                      |              | 0.28       |\\n|                      |              | 4.06       |\\n|                      |              | 3.23       |\\n| PMTR (Coarse + Fine) | PMT PMT      | 0.78       |\\n|                      |              | 296.15     |\\n|                      |              | 3.78       |\\n|                      |              | 0.88       |\\n|                      |              | 5.35       |\\n|                      |              | 3.75       |\\n\\nThe results clearly indicate that PMT delivers substantial reductions not only in training/inference time but also in memory requirements. Notably, PMT is approximately \\\\(\\\\times 21.5\\\\) more efficient in FLOPS, needs \\\\(\\\\times 3.4\\\\) more compact number of parameters and \\\\(\\\\times 3.28/\\\\times 11.07\\\\) less required memory for training/inference phases compared to GeoTr. Such efficiency is crucial, as it facilitates the practical deployment of our fine-level matcher for intricate matching tasks.\"}"}
{"id": "FYQIgQWH3d", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3D Geometric Shape Assembly via Efficient Point Cloud Matching\\n\\nC. Additional implementation Details\\n\\nAttention Calculation. We adopt the relative-position encoding strategy of PerViT (Min et al., 2022) to compute the attention \\\\( A(h)X \\\\). Specifically, we compute pairwise Euclidean distances \\\\( RX \\\\in \\\\mathbb{R}^{|X| \\\\times |X|} \\\\) each of which entry at position \\\\( q, k \\\\in \\\\mathbb{R}^3 \\\\) is defined as\\n\\\\[\\n(R_X)_{q,k} = ||q - k||^2.\\n\\\\]\\nAn MLP processes this to provide an attention score \\\\( A(h)X \\\\). \\\\( A(h)Y \\\\) is similarly defined. We refer the readers to the work of Min et al. (2022) for additional details.\\n\\nModel hyperparameters. For the backbone network, we utilize KPConv-FPN (Thomas et al., 2019) with subsampling radius of 0.01. We leverage global attention matrix for coarse-level matcher, and local attention matrix (See Sec. 4.2) for fine-level matchers. The number of attention heads \\\\( N_h \\\\) is set to 4. Refer to Tab. 8 the rest of hyperparameters. Each matcher takes a specific input and output feature pair, applies a type of attention mechanism, and uses various hyperparameters crucial for its operation.\\n\\nTable 8. Detail configurations and hyperparameters of different type of matchers.\\n\\n| Matcher Type   | Input Feature Pair | Output Feature Pair | Attention Type |\\n|---------------|--------------------|---------------------|----------------|\\n| Coarse-level matcher | {FX_1, FY_1} | {FX_c, FY_c} | global attention |\\n| Fine-level matcher  | {FX_2, FY_2} | {FX_3, FY_3} | local attention |\\n| Fine-level matcher  | {FX_3, FY_3} | {FX_f, FY_f} | local attention |\\n\\nD. Evaluation Metrics\\n\\nWe employ four different metrics to assess the results. Consider a pair of input point clouds \\\\( \\\\{X, Y\\\\} \\\\). The ground truth SE(3) relative pose between the point clouds is represented by \\\\( \\\\{R_{GT}, t_{GT}\\\\} \\\\), while the prediction is denoted as \\\\( \\\\{R, t\\\\} \\\\). We define \\\\( T(\\\\cdot) \\\\) as a function that transform input pose with corresponding rotation \\\\( R \\\\) and translation \\\\( t \\\\).\\n\\nChamfer Distance (CD). The chamfer distance between two point clouds \\\\( S_1, S_2 \\\\) is defined as\\n\\\\[\\nd_{CD}(S_1, S_2) = \\\\frac{1}{S_1} \\\\sum_{x \\\\in S_1} \\\\min_{y \\\\in S_2} \\\\|x - y\\\\|^2 + \\\\frac{1}{S_2} \\\\sum_{y \\\\in S_2} \\\\min_{x \\\\in S_1} \\\\|x - y\\\\|^2,\\n\\\\]\\nwhich measures the sum of the distance between nearest neighbor correspondences between point clouds. To assess the quality of shape assembly, we measure the chamfer distance between ground truth assembly and the prediction as:\\n\\\\[\\nCD = d_{CD}(T(X) \\\\cup Y, T_{GT}(X) \\\\cup Y).\\n\\\\]\\n\\nCoRrespondence Distance (CRD). While the Chamfer distance calculates the distance between two point clouds, its ability to capture more complex features of the object's geometry, such as symmetry and rotation, is limited. To overcome this limitation, we define a new metric, CoRrespondence Distance (CRD). CRD is simply defined as the Frobenius norm between two point clouds:\\n\\\\[\\nCRD = \\\\frac{1}{L} \\\\sum_{i=1}^{L} \\\\|T(X) \\\\cup Y)_i - (T_{GT}(X) \\\\cup Y)_i\\\\|_F,\\n\\\\]\\nwhere \\\\( L = |X| + |Y| \\\\) is the size of assembled object. By considering all pairwise distances between point clouds, it offers a more comprehensive measure of similarity, capturing both proximity and structural alignment:\\n\\nRotational-, Translational-RMSE (RMSE(R), RMSE(T)). Finally, to directly measure the prediction accuracy of transformation parameters, we compute the root mean square error (RMSE) between predicted and ground-truth rotation and translation, respectively. Following the protocols of Sell\u00e1n et al. (2022), we use Euler angle representation for rotation:\\n\\\\[\\nRMSE(R) = \\\\frac{1}{\\\\sqrt{3}} \\\\|R - R_{GT}\\\\|_F,\\n\\\\]\\n\\\\[\\nRMSE(T) = \\\\frac{1}{\\\\sqrt{3}} \\\\|t - t_{GT}\\\\|_F.\\n\\\\]\"}"}
{"id": "FYQIgQWH3d", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Then, for any high-dimensional convolution with a kernel $w$ and inputs $x$, the following equality holds:\\n\\n$$K_i \\\\cdot x = \\\\sum_{j} w_{ij} x_j.$$ \\n\\nAs illustrated in the left of Fig. 1, the 2nd-order convolution (Eq. 1 and 3) is designed to disambiguate spatially consistent geometric-level input processing for geometric compatibility analysis to ensure precise correspondence alignments.\\n\\nTo overcome the limitation, we introduce an efficient feature matching layer, dubbed Proxy Match Transform (PMT), that satisfies the following constraints:\\n\\n(i) orthonormality constraint: $D_{ij} \\\\in \\\\mathbb{R}^{N \\\\times |Y|}$ is a learnable weight scalar, corresponding to the spatial resolution of the proxy tensor satifying $D_{ij} \\\\preceq |X| \\\\times |Y|$.\\n\\n(ii) zero-matrix constraint: each head performs distinct attentions and feature transform, allowing the layer to attend different aspects of inputs.\\n\\nWe refer to the Appendix A for the complete proof.\\n\\n**Proof.**\\n\\nIn order for the Proxy Match Transforms to express the high-order convolution, we assume the following constraints, (i) orthogonal proxy tensors $D_{ij} \\\\in \\\\mathbb{R}^{N \\\\times |Y|}$ and (ii) zero-matrix constraint $D_{ij} \\\\preceq |X| \\\\times |Y|$: \\n\\n$$D_{ij} = \\\\begin{cases} 1, & \\\\text{if } i = j \\\\in N, \\\\\\\\ 0, & \\\\text{otherwise.} \\\\end{cases}$$ \\n\\nTheorem 1.\\n\\nTheorem 2.\\n\\nIn order for the Proxy Match Transforms to effectively approximate high-order convolution. Our main result is that the following holds:\\n\\n$$F_{PMT}^h(x,y) = \\\\sum_{i,j \\\\in N} w_{ij} x_i y_j,$$ \\n\\nwhere $w_{ij}$ is a learnable weight scalar.\\n\\nIt is important to note that the PMT layers perform two effectivements of information between the features, eliminating the need to construct and convolve memory-intensive pairwise feature correlations, which often contain sparse and limited information.\\n\\nWe demonstrate that how the PMT transforms for $X$, $Y$ effectively approximate existing high-order convolution of information between the feature pair is effectively facilitated by a shared proxy tensor.\\n\\nIn order for the Proxy Match Transforms to express the high-order convolution, we assume the following constraints, (i) orthogonal proxy tensors $D_{ij} \\\\in \\\\mathbb{R}^{N \\\\times |Y|}$ and (ii) zero-matrix constraint $D_{ij} \\\\preceq |X| \\\\times |Y|$: \\n\\n$$D_{ij} = \\\\begin{cases} 1, & \\\\text{if } i = j \\\\in N, \\\\\\\\ 0, & \\\\text{otherwise.} \\\\end{cases}$$ \\n\\nTheorem 1.\\n\\nIn order for the Proxy Match Transforms to effectively approximate high-order convolution. Our main result is that the following holds:\\n\\n$$F_{PMT}^h(x,y) = \\\\sum_{i,j \\\\in N} w_{ij} x_i y_j,$$ \\n\\nwhere $w_{ij}$ is a learnable weight scalar.\\n\\nIt is important to note that the PMT layers perform two effectivements of information between the features, eliminating the need to construct and convolve memory-intensive pairwise feature correlations, which often contain sparse and limited information.\\n\\nWe demonstrate that how the PMT transforms for $X$, $Y$ effectively approximate existing high-order convolution of information between the feature pair is effectively facilitated by a shared proxy tensor.\\n\\nIn order for the Proxy Match Transforms to express the high-order convolution, we assume the following constraints, (i) orthogonal proxy tensors $D_{ij} \\\\in \\\\mathbb{R}^{N \\\\times |Y|}$ and (ii) zero-matrix constraint $D_{ij} \\\\preceq |X| \\\\times |Y|$: \\n\\n$$D_{ij} = \\\\begin{cases} 1, & \\\\text{if } i = j \\\\in N, \\\\\\\\ 0, & \\\\text{otherwise.} \\\\end{cases}$$ \\n\\nTheorem 1.\\n\\nIn order for the Proxy Match Transforms to effectively approximate high-order convolution. Our main result is that the following holds:\\n\\n$$F_{PMT}^h(x,y) = \\\\sum_{i,j \\\\in N} w_{ij} x_i y_j,$$ \\n\\nwhere $w_{ij}$ is a learnable weight scalar.\\n\\nIt is important to note that the PMT layers perform two effectivements of information between the features, eliminating the need to construct and convolve memory-intensive pairwise feature correlations, which often contain sparse and limited information.\\n\\nWe demonstrate that how the PMT transforms for $X$, $Y$ effectively approximate existing high-order convolution of information between the feature pair is effectively facilitated by a shared proxy tensor.\\n\\nIn order for the Proxy Match Transforms to express the high-order convolution, we assume the following constraints, (i) orthogonal proxy tensors $D_{ij} \\\\in \\\\mathbb{R}^{N \\\\times |Y|}$ and (ii) zero-matrix constraint $D_{ij} \\\\preceq |X| \\\\times |Y|$: \\n\\n$$D_{ij} = \\\\begin{cases} 1, & \\\\text{if } i = j \\\\in N, \\\\\\\\ 0, & \\\\text{otherwise.} \\\\end{cases}$$ \\n\\nTheorem 1.\\n\\nIn order for the Proxy Match Transforms to effectively approximate high-order convolution. Our main result is that the following holds:\\n\\n$$F_{PMT}^h(x,y) = \\\\sum_{i,j \\\\in N} w_{ij} x_i y_j,$$ \\n\\nwhere $w_{ij}$ is a learnable weight scalar.\\n\\nIt is important to note that the PMT layers perform two effectivements of information between the features, eliminating the need to construct and convolve memory-intensive pairwise feature correlations, which often contain sparse and limited information.\\n\\nWe demonstrate that how the PMT transforms for $X$, $Y$ effectively approximate existing high-order convolution of information between the feature pair is effectively facilitated by a shared proxy tensor.\\n\\nIn order for the Proxy Match Transforms to express the high-order convolution, we assume the following constraints, (i) orthogonal proxy tensors $D_{ij} \\\\in \\\\mathbb{R}^{N \\\\times |Y|}$ and (ii) zero-matrix constraint $D_{ij} \\\\preceq |X| \\\\times |Y|$: \\n\\n$$D_{ij} = \\\\begin{cases} 1, & \\\\text{if } i = j \\\\in N, \\\\\\\\ 0, & \\\\text{otherwise.} \\\\end{cases}$$ \\n\\nTheorem 1.\"}"}
{"id": "FYQIgQWH3d", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overall pipeline of the Proxy Match Trans-formeR (PMTR) for pairwise shape assembly. The proposed architecture consists of coarse-level matching and fine-level matching. Each part of matching uses coarse-level features and fine-level features, respectively, acquired from the KPConv-FPN backbone as their input. Each matcher consists of $N_t$ PMT layers in series. See Sec. 3.3 for details.\\n\\n$w(h)X_{w(h)}Y$, then, the dot-product of Proxy Match Transform outputs with a sufficient number of heads $N_h$ can express high-dimensional convolutional layer with kernel $K$:\\n\\n$$\\\\text{PMT}(F_X) \\\\cdot \\\\text{PMT}(F_Y)^\\\\top = \\\\text{Conv}(F_X, F_Y).$$\\n\\nWe refer to the Appendix A for the complete proof of the theorem. For the proxy tensors to satisfy the conditions, we design two auxiliary training objectives on proxy tensors, orthonormal loss $L_{orth}$ and zero loss $L_{zero}$, as follows:\\n\\n$$L_{orth} = \\\\sum_{(i,j) \\\\in [N_h]^2} \\\\delta(i,j) (P(i)^\\\\top P(j) - I_{D_{emb}}),$$\\n\\n$$L_{zero} = \\\\sum_{(i,j) \\\\in [N_h]^2} (1 - \\\\delta(i,j)) P(i)^\\\\top P(j),$$\\n\\nwhere $\\\\delta(i,j)$ provides 1 if $i = j$ and 0 otherwise.\\n\\n3.3. Overall architecture\\n\\nThe proposed architecture, dubbed Proxy Match Trans-formeR (PMTR) comprises four main parts: (1) feature extraction, (2) coarse-level matching, (3) fine-level matching, and (4) transformation prediction & training objectives. As illustrated in Fig. 2, our pipeline begins with the point cloud pair embedding. The feature extraction network generates three pairs of features, each at distinct spatial resolutions. These feature pairs are subsequently fed to a corresponding PMT layer, which facilitates both coarse-level matching (for mating surface localization) and fine-level matching (for geometric matching). The outputs from the coarse matching phase are utilized to establish a preliminary correspondence between the mating surfaces of the input parts, which is crucial for identifying potential areas of alignment. Subsequently, the fine matching phase are designed to refine these correspondences, focusing exclusively on reliable matches identified during the coarse matching stage. This allows for precise correspondence establishment, ensuring accurate assembly as demonstrated by our experiments in Sec. 4.4.\"}"}
{"id": "FYQIgQWH3d", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3D Geometric Shape Assembly via Efficient Point Cloud Matching\\n\\nthe output of a high-order feature transformation. The approximation is conceptualized as if the features had undergone a high-order transform, according to Theorem 1:\\n\\n$$\\\\text{PMT}(F_X^1) \\\\cdot \\\\text{PMT}(F_Y^1)^\\\\top = F_X^c \\\\cdot F_Y^c\\\\top \\\\quad (11)$$\\n\\nThis approach allows for the independent refinement of the features while still capturing the essence of their interaction, akin to high-order convolution, without the direct computation of their pairwise correlation, thereby effectively avoiding the burden of quadratic complexity.\\n\\nFine-level matching.\\n\\nIn fine-level matching, we leverage the high-resolution feature pairs $\\\\{(F_X^n, F_Y^n)\\\\}_{n=2}^3$ to achieve more precise alignment. This stage mirrors the coarse-level matching in its use of PMT layers for transforming features but in a serial configuration. This setup ensures that the output of one PMT layer serves as the input to the next, such that $\\\\text{PMT}(F_X^2) = F_X^3$ and subsequently, $\\\\text{PMT}(F_X^3) = F_X^f$, with an analogous sequence for providing fine-level feature $F_Y^f$. Note that PMT effectively addresses the infeasibility of employing vanilla high-order convolution for high-resolution matching, especially under conditions where $|X|, |Y| > 1500$, as observed in our experiments. Compared to vanilla high-order convolution with complexity of $O(|X| \\\\cdot |Y|)$, rendering it infeasible for large-scale applications, the proposed PMT having $O(\\\\max(|X|, |Y|) \\\\cdot D_{\\\\text{proxy}})$ complexity where $D_{\\\\text{proxy}} \\\\ll |X|, |Y|$ provides a more efficient means of analyzing feature correlations. In Sec. 4.4, we present an apples-to-apples comparison, illustrating the practical advantages of PMT over traditional matching methods, e.g., high-order convolution (Rocco et al., 2018).\\n\\nTransformation prediction.\\n\\nAfter the coarse-level matching, the refined feature pair $(F_X^c, F_Y^c)$ is utilized to compute correlation in size of $|X^c| \\\\times |Y^c|$ where each score at position $(x, y)$ is defined as $\\\\exp(-||F_X^c x - F_Y^c y||^2_2)$ similarly to the work of Qin et al. (2022). From $|X^c| \\\\times |Y^c|$ number of scores, we collect top-$k$ matches as reliable coarse matches, laying the foundation for more granular alignment at the subsequent fine-level matching. Building on coarse-level matches and fine-level features, we employ the point-to-node grouping method (Yu et al., 2021), which clusters fine-level features that are spatially proximate to the coarse matches, effectively sharpening the broad coarse-level correspondence into more precise fine-level ones. In essence, the computation of fine-level matches is directly influenced by the groundwork laid at the coarse level, establishing a hierarchical refinement process. We then incorporate an optimal transport layer (Sarlin et al., 2020) to the fine-level matches to obtain final correspondences for the subsequent transformation prediction. Finally, similarly to Qin et al. (2022), we use the final correspondences to predict the relative transformation $\\\\{R|t\\\\}$ between the point cloud pair $(X, Y)$.\\n\\nTraining objectives.\\n\\nFollowing the previous 3D matching literatures (Zhao et al., 2023; Wu et al., 2023a; Chen et al., 2023; Yu et al., 2023), we adopt overlap-aware circle loss $L_{\\\\text{oc}}$ (Qin et al., 2022), and point matching loss $L_p$ (Sarlin et al., 2020), as our main training objectives for coarse- and fine-level correspondence matching respectively. We direct readers to the work of Qin et al. (2022) for further details of $L_{\\\\text{oc}}$ and $L_p$. With two auxiliary losses in Eq. 9, our main training objective is defined as follows:\\n\\n$$L = L_{\\\\text{oc}} + L_p + \\\\lambda_{\\\\text{orth}} L_{\\\\text{orth}} + \\\\lambda_{\\\\text{zero}} L_{\\\\text{zero}}, \\\\quad (13)$$\\n\\nwhere $\\\\lambda_{\\\\text{orth}}$ and $\\\\lambda_{\\\\text{zero}}$ are hyperparameters which are set to 1.0 in our experiments.\\n\\n4. Experiments\\n\\nIn this section, we discuss the dataset and evaluation metrics used (Sec. 4.1), implementation details (Sec. 4.2), the results of pairwise shape assembly with comprehensive analysis (Sec. 4.3), an in-depth ablation study to inspect the efficacy of the proposed techniques (Sec. 4.4), and extension of our evaluation to the task of multi-part assembly (Sec. 4.5).\\n\\n4.1. Dataset and Evaluation Metrics\\n\\nDataset.\\n\\nIn our experiments, we utilize the Breaking Bad dataset (Sell\u00e1n et al., 2022) which is a large-scale dataset of fractured objects for the task of geometric shape assembly, which consists of over 1 million fractured objects simulated from 10K meshes of PartNet (Mo et al., 2019) and Thingi10k (Zhou & Jacobson, 2016). For pairwise assembly training and evaluation, we exclusively select a subset of the Breaking Bad dataset containing two-part objects (Sec. 4.3). For multi-part assembly, we expand our evaluation to include all samples in the dataset, encompassing objects with 2 to 20 parts (Sec. 4.5).\\n\\nEvaluation metrics.\\n\\nFollowing the evaluation protocol of Sell\u00e1n et al. (2022), we measure the root mean square error (RMSE) between the ground-truth and predicted rotation (R) and translation (T) parameters, and the Chamfer distance (CD) between the assembly results and ground-truth. In addition, we introduce and report a new metric, called CoRrespondence Distance (CRD), which is defined as Frobenius norm between the input pair of the assembled point cloud; unlike CD, CRD offers a more comprehensive measure of correspondence, capturing both proximity and structural alignment between the assembled objects. We compute the evaluation metrics of RMSE (R) and RMSE (T) based on relative transformation, e.g., rotation and translation, between the input fracture pair, instead of absolute pose as in previous literature (Chen et al., 2022; Wu et al., 2023b) by setting the largest fracture as an anchor and compute...\"}"}
{"id": "FYQIgQWH3d", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1.\\nPairwise shape assembly results. Numbers in bold indicate the best performance and underlined ones are the second best.\\n\\n| Method          | Estimator Type | Target       | RMSE (R) | RMSE (T) |\\n|-----------------|----------------|--------------|-----------|-----------|\\n| CRD             | {R | t}          | \u2193            | CD        | \u2193         |\\n| Global (2019; 2020a) | MLP absolute pose | \u2193 | 27.77 | 15.26 |\\n| LSTM (2020)     | \u2193              | \u2193            | 20.04     | 7.77      |\\n| DGL (2020)      | \u2193              | \u2193            | 20.32     | 6.40      |\\n| NSM (2022)      | \u2193              | \u2193            | 21.71     | 11.09     |\\n| Wu et al. (2023b)| \u2193              | \u2193            | 20.65     | 11.66     |\\n| GeoTransformer (2022) | correspondence | alignment | relative transformation | RMSE (R) | RMSE (T) |\\n| Jigsaw (2023)   | \u2193              | \u2193            | 5.48      | 1.34      |\\n\\n4.2. Implementation details\\nWe implement our PMTR using PyTorch Lightning (Falcon & team, 2019). Experiments were conducted on a machine with Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz and NVIDIA GeForce RTX 3090 GPU. For all experiments, except the ones include GeoTransformer, we use ADAM (Kingma & Ba, 2015) optimizer with a learning rate of $1 \\\\times 10^{-3}$ for 150 epochs. For GeoTransformer, we use the identical settings but only reduce the learning rate to $1 \\\\times 10^{-4}$ to prevent model divergence. To ensure uniform point density among fractures, we uniform-sample approximately 5,000 points on the surface of holistic objects and allocate the number of sample points for each fracture proportional to the surface area of each fracture. Each of both coarse-level and fine-level matchers consists of 2 PMT(\u00b7) layers ($N_t = 2$) with nonlinearity and group norm (Wu & He, 2018). See Appendix C for further details.\\n\\nAvoiding quadratic complexity of attention in PMT. In our actual implementation, we use local, i.e., sparse, attention for $A(h) X$ by collecting attention scores of \u2018neighborhood\u2019 of each position, thus reducing attention size to $|X| \\\\times \\\\epsilon$ instead of $|X| \\\\times |X|$ where $\\\\epsilon \\\\in \\\\mathbb{N}$+ is the number of neighbors: $\\\\epsilon \\\\ll |X|$. Specifically, attention at position $x \\\\in \\\\mathbb{R}^3$ denoted as $A(h) X(x, :) \\\\in \\\\mathbb{R}^{1 \\\\times \\\\epsilon}$ are limited to the neighborhood of $x$, represented by $N(x)$. Then, the output of PMT at $x$ is formulated as $PMT(F_X)(x, :) = \\\\sum_{h \\\\in [N_h]} A(h) X(x, :) F_X(N(x), :) P(h)^T w(h)$ where $F_X(N(x), :) \\\\in \\\\mathbb{R}^{\\\\epsilon \\\\times d_{emb}}$ is neighborhood features of position $x$. This method significantly reduces the computational complexity typically associated with full pairwise attention, which would otherwise be quadratic, i.e., $|X| \\\\times |X|$. This reduction in complexity mirrors strategies found in existing literature, such as those described by Thomas et al. (2019). For simplicity in presentation, however, this paper narrates with a conventional square attention matrix $A(h) X \\\\in \\\\mathbb{R}^{|X| \\\\times |X|}$ to illustrate our methodology. A similar approach applies to the other matrix $A(h) Y \\\\in \\\\mathbb{R}^{|Y| \\\\times |Y|}$.\\n\\nAssessment with relative transformations. Note that the previous methods for pairwise geometric assembly (Li et al., 2020a; Wu et al., 2020; Huang et al., 2020; Chen et al., 2022) predict two different transformation parameters for the input pair of parts to assemble them in 3D space. However, this approach has a limitation in accurate evaluation: even if a model perfectly assembles the pair of parts, the assessment may be inaccurate if the assembled object does not match the specific absolute pose of the ground truth. To address this issue, we suggest to predict the relative transformation between input parts, allowing us to focus solely on the assembly rather than the predefined absolute poses.\\n\\n4.3. Pairwise Shape Assembly\\nTo evaluate our method, we categorize previous baseline methods into two groups based on their approach to transformation parameters $\\\\{R | t\\\\}$ prediction. The first group includes \u2018regression methods\u2019 that encode each part into a global embedding and directly regress their absolute transformations using MLP: Global (Li et al., 2020a), LSTM (Wu et al., 2020), DGL (Huang et al., 2020), NSM (Chen et al., 2022), and Wu et al. (2023b). The second group consists of \u2018matching-based methods\u2019 that estimate relative transformations by aligning their predicted correspondences between each pair of parts: GeoTransformer (Qin et al., 2022) and Jigsaw (Lu et al., 2023).\\n\\nExperimental results and analysis. We evaluate our method and compare it against baseline methods on the everyday and artifact subsets of the Breaking Bad dataset. Tab. 1 presents the results, demonstrating that our method consistently outperforms all baseline methods on both subsets. In Fig. 4, we provide qualitative comparisons between ours and the baselines, using mesh representation for better visualization.\\n\\nTo provide deeper insights to the learned shared proxy $P(h)$, we visualize how the proxy and the refined coarse-level features ($F_X c$ and $F_Y c$) are distributed in the feature space via t-SNE. As shown in Fig. 3, The visualization reveals that\"}"}
{"id": "FYQIgQWH3d", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3D Geometric Shape Assembly via Efficient Point Cloud Matching\\n\\nTable 2. Ablation study on the proxy sharing. By sharing proxy tensor in each Proxy Match Transform layer, two independent feature transforms share information, yielding the highest score.\\n\\n| Ref. proxy shared | CRD | CD | RMSE (R) | RMSE (T) |\\n|-------------------|-----|----|----------|----------|\\n| proxy (10\u22122)      | \u2717  | \u2717  | 0.53     | 0.47     |\\n| (10\u22123)            |    |    | 21.04    | 6.93     |\\n| \u25e6 (10\u22122)          |    |    | 0.44     | 0.31     |\\n| Ours              | \u2713  | \u2713  | 0.39     | 0.25     |\\n\\nTable 3. Ablation study on the contribution of $L_{orth}$ and $L_{zero}$. They constrains Proxy Match Transform in approximating the high-dimensional convolution layers, yielding the highest score.\\n\\n| Ref. $L_{orth}$  | $L_{zero}$ | CRD | CD | RMSE (R) | RMSE (T) |\\n|------------------|------------|-----|----|----------|----------|\\n| $L_{orth}$       | $L_{zero}$ | \u2717  | \u2717  | 0.43     | 0.31     |\\n| $L_{zero}$       | $L_{orth}$ |    |    | 0.43     | 0.32     |\\n| \u2717                | \u2713          | 0.43 | 0.27   | 18.77    | 5.73     |\\n| Ours             | \u2713          | 0.39 | 0.25   | 17.14    | 5.53     |\\n\\n4.4. Ablation studies\\n\\nEffect of proxy tensor in assembly. To verify the effect of proxy tensor in PMT, we conducted a series of ablation studies on the everyday subset of the Breaking Bad dataset. Specifically, we examine the impact of the shared proxy tensor by either removing it or using two different proxies instead of a shared one. The results, summarized in Tab. 2, clearly indicate that both removing the proxy and not sharing it lead to a significant decline in assembly performance. This underscores the efficacy of the shared proxy in facilitating information exchange in PMT.\\n\\nNext, we explore the impact of $L_{orth}$ and $L_{zero}$, which serve as the sufficient conditions to constrain the PMT layer to represent the high-dimensional convolutional layers, detailed in Sec. 3.2. The results are presented in Tab. 3; as evident from the table, the best performance is achieved when both losses are incorporated. This highlights that the significance of these constraining conditions for PMT, as they are crucial in enabling PMT to effectively approximate the high-dimensional convolution.\\n\\nComparison between different matchers. To demonstrate the efficacy and efficiency of the proposed matching layer, PMT, we conduct ablations by either removing it (None) or replacing it with different layers: a single linear transformation, multi-layer perceptron (MLP), high-dimensional convolution (HDC by Min et al. (2021)), and GeoTransformer (GeoTr by Qin et al. (2022)). In Tab. 4, we compare ours with other layers at fine-level; Undoubtedly, the layers without any information exchange between source and target features, e.g., None, Linear, and MLP, show dramatic drops in performance. While the matching layers of HDC and GeoTr cause out-of-memory-error due to their quadratic complexity, being unable to be incorporated at fine-level with large input spatial resolutions, the proposed PMT not only efficiently processes source and target features without memory burden but also effectively source (non-mating) source (mating) target (mating) target (non-mating) proxy source (non-mating)source (mating)target (mating)target (non-mating)\\n\\nFigure 3. (a) t-SNE visualization of proxy tensor (colored in purple), source features $F_X$ and target features $F_Y$. The source and target features are colored in warm (red) and cool (blue) tones, respectively, and those on mating surfaces are colored in orange and light blue. (b) Feature visualization in 3D space. Source $X_1$ and target features $Y_1$ with closer proximity to the proxy tensor are highlighted in red and blue, respectively, and features on mating surfaces are highlighted in orange and light blue. For this visualization, we use proxy tensor at a head index of $h = 0$: $P(0)$. \\n\\nTable 4. Ablation study on the choice of fine-level matcher. Proxy Match Transform layer at fine-level yields the best assembly accuracy while incurring low-compute complexity than baselines.\\n\\n| Ref. Coarse-level | Fine-level | CRD | CD | RMSE (R) | RMSE (T) |\\n|------------------|------------|-----|----|----------|----------|\\n| None             | PMT        | 0.53| 0.43| 20.70    | 6.63     |\\n| Linear           | PMT        | 0.47| 0.37| 17.55    | 5.68     |\\n| MLP              | PMT        | 0.49| 0.38| 17.35    | 5.69     |\\n| HDC              | Out of memory error |    |     |          |          |\\n| GeoTr            | Out of memory error |    |     |          |          |\\n| Ours             | PMT        | 0.39| 0.25| 17.14    | 5.53     |\\n\\nTable 5. Ablation study on the impact of Proxy Match Transform as a fine-level matcher. Proxy Match Transform layer consistently boosts performance with various coarse-level matchers.\\n\\n| Ref. Coarse-level | Fine-level | CRD | CD | RMSE (R) | RMSE (T) |\\n|------------------|------------|-----|----|----------|----------|\\n| None             | None       | 0.69| 0.57| 27.71    | 8.78     |\\n| PMT              | None       | 0.60| 0.52| 24.66    | 7.42     |\\n| Linear           | PMT        | 0.64| 0.53| 26.14    | 7.42     |\\n| MLP              | PMT        | 0.66| 0.50| 26.93    | 7.35     |\\n| HDC              | PMT        | 0.76| 0.63| 27.75    | 8.68     |\\n| GeoTr            | PMT        | 0.61| 0.51| 22.81    | 7.28     |\\n| Ours             | PMT        | 0.48| 0.33| 23.91    | 7.32     |\"}"}
{"id": "FYQIgQWH3d", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6. Multi-part assembly results on the Breaking Bad dataset.\\n\\n| Method       | CRD RMSE (R) | CD RMSE (T) | PA CRD | PA CD |\\n|--------------|--------------|-------------|--------|-------|\\n| Global (2019; 2020a) | 27.79 15.30 55.42 | 15.31 36.42 | 37.90 26.42 | 14.92 54.41 |\\n| LSTM (2020) | 27.69 15.23 54.78 | 15.24 36.74 | 38.97 28.15 | 14.61 53.59 |\\n| DGL (2020) | 27.90 13.23 55.76 | 15.33 36.99 | 39.70 27.48 | 13.91 54.66 |\\n| Wu et al. (2023b) | 28.18 19.70 54.98 | 15.59 35.66 | 36.28 26.02 | 15.81 54.35 |\\n| Jigsaw (2023) | 14.13 11.82 41.12 | 11.74 52.48 | 60.26 16.10 | 9.53 42.01 |\\n| PMTR (Ours) | 6.51 5.56 31.57 | 9.95 66.95 | 70.56 5.67 | 4.33 31.58 |\\n\\n**Figure 4.** Qualitative results of pairwise shape assembly (Upper row) and multipart shape assembly (Bottom row) on Breaking Bad dataset.\\n\\nExchanges information between them via proxy tensor. In Tab. 5, similar experiments are conducted at coarse-level. As evident from the tables, incorporating the PMT layer as both fine and coarse matcher consistently leads to superior performance, affirming its superiority over the state-of-the-art matching layers (Min et al., 2021; Qin et al., 2022).\\n\\n### 4.5. Multi-part Assembly\\n\\nTo assess the generalizability of our method, we extend our evaluation to include multiple input parts, i.e., multi-part assembly, which requires the model to understand the pairwise correspondence relationships among all input parts. Utilizing the two-part assembly framework (Fig. 2), it begins with computing relative transformations between each pair of the parts. We then construct a pose graph wherein each node and factor respectively represent an individual part and the predicted relative transformation, i.e., pose, between two parts. To optimize this pose graph for assembly, we employ a recent transformation averaging method detailed in the work of Dellaert et al. (2020). After the optimization, we evaluate our method using the metrics from pairwise assembly, supplemented by Part Accuracy (PA) (Huang et al., 2020) \u2013 the percentage of parts with Chamfer Distance less than the predefined threshold of 0.01 \u2013 as well as CRD-based Part Accuracy (PA) with 0.1 threshold. As seen from Tab. 6 and Fig. 4, our method significantly surpasses all baselines on all metrics on the multi-part assembly, demonstrating robust generalization to multiple input scenarios.\\n\\nFor details on the evaluation metrics, refer to Appendix D.\\n\\n### 5. Scope and Limitations\\n\\nDespite the advances in efficient point cloud matching and shape assembly, our method still faces several limitations. First, the accuracy of our method can be compromised in scenarios with extremely low overlap between point clouds, which can hinder the identification of reliable correspondences. Second, our method, like many others in the field, requires extensive training on domain-specific datasets to achieve optimal performance. Third, while our experiments demonstrate the efficacy of PMT in shape assembly tasks, it have not been extensively tested across other potential applications such as robotics, manufacturing, digital artistry, or even restoration of ancient artifacts via more accurate and detailed part assembly. Thus, the applicability of our approach beyond geometric shape assembly remains to be fully validated. We leave this to future work.\\n\\n### 6. Conclusion\\n\\nWe have introduced a low-complexity, high-order feature transform layer, Proxy Match Transform, designed for efficient approximation of traditional compute-intensive high-order feature transforms. The significant performance improvements over the recent state of the arts with lower computational load indicate that its effective real-world applicability from artifact reconstruction to manufacturing. Although the proposed method has been applied exclusively to geometric shape assembly, its remarkable improvements across various evaluation metrics indicate its profound potential for broad applications.\"}"}
{"id": "FYQIgQWH3d", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nThis work was supported by IITP grants (RS-2022-II220290: Visual Intelligence for Space-Time Understanding and Generation (30%), RS-2021-II212068: AI Innovation Hub (60%), RS-2019-II191906: AI Graduate School Program at POSTECH (5%), RS-2021-II211343: AI Graduate School Program at SNU: 2021-0-01343 (5%)) funded by the Korea government.\\n\\nImpact Statement\\n\\nThe advancements in geometric shape assembly hold paramount potentials across numerous fields, from archaeological artifact reconstruction to industrial manufactures. This research can also advance the field manufacturing, robotics, digital artistry, and even restoration of ancient artifacts via more accurate and robust shape assembly.\\n\\nReferences\\n\\nChen, S., Xu, H., Li, R., Liu, G., Fu, C.-W., and Liu, S. Sirapcrr: Sim-to-real adaptation for 3d point cloud registration. In Proc. IEEE International Conference on Computer Vision (ICCV), 2023.\\n\\nChen, X., Zhang, H., Lin, J., Hu, R., Lu, L., Huang, Q.-X., Benes, B., Cohen-Or, D., and Chen, B. Dapper: decompose-and-pack for 3d printing. ACM Trans. Graph., 34(6):213\u20131, 2015.\\n\\nChen, Y., Zeng, Q., Ji, H., and Yang, Y. Skyformer: remodel self-attention with gaussian kernel and nystr\u00f6m method. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\nChen, Y.-C., Li, H., Turpin, D., Jacobson, A., and Garg, A. Neural shape mating: Self-supervised object assembly with adversarial shape priors. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nChen, Z., Gong, M., Ge, L., and Du, B. Compressed self-attention for deep metric learning with low-rank approximation. In Proc. International Joint Conference on Artificial Intelligence (IJCAI), 2020.\\n\\nCho, S., Hong, S., Jeon, S., Lee, Y., Sohn, K., and Kim, S. Cats: Cost aggregation transformers for visual correspondence. Advances in Neural Information Processing Systems, 34:9011\u20139023, 2021.\\n\\nChoy, C., Dong, W., and Koltun, V. Deep global registration. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nCordonnier, J.-B., Loukas, A., and Jaggi, M. On the relationship between self-attention and convolutional layers. In International Conference on Learning Representations (ICLR), 2020.\\n\\nDellaert, F., Rosen, D. M., Wu, J., Mahony, R., and Carlone, L. Shonan rotation averaging: Global optimality by surfing so(p)\u0302 so(p) n. In Proc. European Conference on Computer Vision (ECCV), 2020.\\n\\nDenton, R., Zaremba, W., Bruna, J., LeCun, Y., and Fergus, R. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in Neural Information Processing Systems (NeurIPS), 2014.\\n\\nFalcon, W. and team, T. P. L. Pytorch lightning, 2019. URL https://github.com/PyTorchLightning/pytorch-lightning.\\n\\nHuang, J., Zhan, G., Fan, Q., Mo, K., Shao, L., Chen, B., Guibas, L., and Dong, H. Generative 3d part assembly via dynamic graph learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nHuang, S., Gojcic, Z., Usvyatsov, M., Wieser, A., and Schindler, K. Predator: Registration of 3d point clouds with low overlap. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nJacobson, A. Generalized matryoshka: Computational design of nesting objects. In Computer Graphics Forum, volume 36, pp. 27\u201335. Wiley Online Library, 2017.\\n\\nJones, R. K., Barton, T., Xu, X., Wang, K., Jiang, E., Guerrero, P., Mitra, N. J., and Ritchie, D. Shapeassembly: Learning to generate programs for 3d shape structure synthesis. ACM Transactions on Graphics (TOG), 39(6):1\u201320, 2020.\"}"}
{"id": "FYQIgQWH3d", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3D Geometric Shape Assembly via Efficient Point Cloud Matching\\n\\nLi, H., Alhashim, I., Zhang, H., Shamir, A., and Cohen-Or, D. Stackabilization. ACM Transactions on Graphics, (Proc. of SIGGRAPH Asia 2012), 31(6), 2012.\\n\\nLi, J., Niu, C., and Xu, K. Learning part generation and assembly for structure-aware shape synthesis. In Proc. AAAI Conference on Artificial Intelligence (AAAI), 2020a.\\n\\nLi, Y., Mo, K., Shao, L., Sung, M., and Guibas, L. Learning 3d part assembly from a single image. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VI 16, pp. 664\u2013682. Springer, 2020b.\\n\\nLu, J., Sun, Y., and Huang, Q. Jigsaw: Learning to assemble multiple fractured objects. arXiv preprint arXiv:2305.17975, 2023.\\n\\nMin, J. and Cho, M. Convolutional hough matching networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2940\u20132950, June 2021.\\n\\nMin, J., Kang, D., and Cho, M. Hypercorrelation squeeze for few-shot segmentation. In Proc. IEEE International Conference on Computer Vision (ICCV), 2021.\\n\\nMin, J., Zhao, Y., Luo, C., and Cho, M. Peripheral vision transformer. Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\nMo, K., Zhu, S., Chang, A. X., Yi, L., Tripathi, S., Guibas, L. J., and Su, H. Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 909\u2013918, 2019.\\n\\nNarayan, A., Nagar, R., and Raman, S. Rgl-net: A recurrent graph learning framework for progressive part assembly. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 78\u201387, 2022.\\n\\nQin, Z., Yu, H., Wang, C., Guo, Y., Peng, Y., and Xu, K. Geometric transformer for fast and robust point cloud registration. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nRocco, I., Cimpoi, M., Arandjelovi\u0107, R., Torii, A., Pajdla, T., and Sivic, J. Neighbourhood consensus networks. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\\n\\nRocco, I., Arandjelovi\u0107, R., and Sivic, J. Efficient neighbourhood consensus networks via submanifold sparse convolutions. In Proc. European Conference on Computer Vision (ECCV), 2020.\\n\\nSarlin, P.-E., DeTone, D., Malisiewicz, T., and Rabinovich, A. Superglue: Learning feature matching with graph neural networks. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nSchor, N., Katzir, O., Zhang, H., and Cohen-Or, D. Componet: Learning to generate the unseen by part synthesis and composition. In Proc. IEEE International Conference on Computer Vision (ICCV), 2019.\\n\\nSell\u00e1n, S., Chen, Y.-C., Wu, Z., Garg, A., and Jacobson, A. Breaking bad: A dataset for geometric fracture and reassembly. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS), 2022.\\n\\nShi, Y., Cai, J.-X., Shavit, Y., Mu, T.-J., Feng, W., and Zhang, K. Clustergnn: Cluster-based coarse-to-fine graph neural network for efficient feature matching. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nThomas, H., Qi, C. R., Deschaud, J.-E., Marcotegui, B., Goulette, F., and Guibas, L. J. Kpconv: Flexible and deformable convolution for point clouds. In Proc. IEEE International Conference on Computer Vision (ICCV), 2019.\\n\\nTian, Y., Xu, J., Li, Y., Luo, J., Sueda, S., Li, H., Willis, K. D., and Matusik, W. Assemble them all: Physics-based planning for generalizable assembly by disassembly. ACM Trans. Graph., 41(6), 2022.\\n\\nTulsiani, S., Su, H., Guibas, L. J., Efros, A. A., and Malik, J. Learning shape abstractions by assembling volumetric primitives. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2635\u20132643, 2017.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 2017.\\n\\nWang, F. and Hauser, K. Stable bin packing of non-convex 3d objects with a robot manipulator. In 2019 International Conference on Robotics and Automation (ICRA), pp. 8698\u20138704. IEEE, 2019.\\n\\nWu, Q., Shen, Y., Jiang, H., Mei, G., Ding, Y., Luo, L., Xie, J., and Yang, J. Graph matching optimization network for point cloud registration. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023a.\\n\\nWu, R., Zhuang, Y., Xu, K., Zhang, H., and Chen, B. Pq-net: A generative part seq2seq network for 3d shapes. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\"}"}
