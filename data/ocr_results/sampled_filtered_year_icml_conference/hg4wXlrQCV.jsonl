{"id": "hg4wXlrQCV", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
{"id": "hg4wXlrQCV", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\n\\nAll ACCEL-methods use exploratory gradient updates.\\n\\n### Table 14. UED Hyperparameters\\n\\n| Parameter                   | DR | PLR/PLR\u22a5 | ACCEL |\\n|-----------------------------|----|----------|-------|\\n| Network Width               | 512|          |       |\\n| # Environments              | 1024|         |       |\\n| \u03b3                           | 0.99|          |       |\\n| \u03bb                           | 0.9 |          |       |\\n| GAE                         |    |          |       |\\n| PPO inner rollout           | 64 |          |       |\\n| PPO outer rollout           | 64 |          |       |\\n| PPO epochs                  | 5  |          |       |\\n| PPO minibatches per epoch   | 2  |          |       |\\n| PPO clip range              | 0.2|          |       |\\n| Adam learning rate          | 0.0002/0.0003|   |\\n| Anneal LR                   | yes|          |       |\\n| Adam \u03f5                      | 1e-5|         |       |\\n| PPO max gradient norm       | 1.0|          |       |\\n| PPO value clipping          | yes|          |       |\\n| return normalization        | no |          |       |\\n| value loss coefficient      |    | 0.5      |       |\\n| student entropy coefficient |    | 0.01     |       |\\n\\n- **PLR**: Replay rate, \\\\( p \\\\) - 0.8 (PLR\u22a5), 0.5 (PLR, ACCEL)\\n- **Buffer size**, \\\\( K \\\\) - 4000\\n- **Scoring function** - MaxMC\\n- **Prioritization** - Rank\\n- **Temperature**, \\\\( \u03b2 \\\\) - 1.0\\n- **Staleness coefficient** - 0.3\\n\\n- **ACCEL** -\\n  - Number of Mutations (Swap) - 10\\n  - Number of Mutations (RSwap) - 200\\n  - Number of Mutations (Noise) - 100\"}"}
{"id": "hg4wXlrQCV", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| ID  | Name          | Texture     |\\n|-----|---------------|-------------|\\n| 2   | Grass         |             |\\n| 3   | Water         |             |\\n| 4   | Stone         |             |\\n| 5   | Tree          |             |\\n| 6   | Wood          |             |\\n| 7   | Path          |             |\\n| 8   | Coal          |             |\\n| 9   | Iron          |             |\\n| 10  | Diamond       |             |\\n| 11  | Crafting Table|             |\\n| 12  | Furnace       |             |\\n| 13  | Sand          |             |\\n| 14  | Lava          |             |\\n| 15  | Plant         |             |\\n| 16  | Ripe Plant    |             |\\n| 17  | Wall          |             |\\n| 18  | Darkness      |             |\\n| 19  | Mossy Wall    |             |\\n| 20  | Stalagmite    |             |\\n| 21  | Sapphire      |             |\\n| 22  | Ruby          |             |\\n| 23  | Chest         |             |\\n| 24  | Fountain      |             |\\n| 25  | Fire Grass    |             |\\n| 26  | Ice Grass     |             |\\n| 27  | Gravel        |             |\\n| 28  | Fire Tree     |             |\\n| 29  | Ice Shrub     |             |\\n| 30  | Enchantment Table (Fire) | |\\n| 31  | Enchantment Table (Ice) | |\\n| 32  | Necromancer   |             |\\n| 33  | Grave 1       |             |\\n| 34  | Grave 2       |             |\\n| 35  | Grave 3       |             |\\n| 36  | Necromancer (Vulnerable) | |\\n\\nTable 2. Listing of block types. Most textures were either taken directly or adapted from those in Crafter.\"}"}
{"id": "hg4wXlrQCV", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 3. Listing of creatures.\\n\\n| Name        | Texture Type | Health | Damage | Defense (%) | Collision Type | Floor       |\\n|-------------|--------------|--------|--------|-------------|----------------|-------------|\\n| Zombie      | Melee        | 5      | 2      | 0           | Ground        | 0           |\\n| Skeleton    | Ranged       | 3      | 2      | 0           | Ground        | 0           |\\n| Cow         | Passive      | 3      |        | -           | Ground        | 0           |\\n| Orc Soldier | Melee        | 7      | 3      | 0           | Ground        | 1           |\\n| Orc Mage    | Ranged       | 5      | 3      | 0           | Ground        | 1           |\\n| Snail       | Passive      | 6      |        | -           | Ground        | 1, 3, 4     |\\n| Gnome Warrior | Melee     | 9      | 4      | 0           | Ground        | 2           |\\n| Gnome Archer | Ranged      | 6      | 2      | 0           | Ground        | 2           |\\n| Bat         | Passive      | 4      |        | -           | Flying        | 2, 5, 6     |\\n| Lizard      | Melee        | 11     | 5      | 0           | Amphibian     | 3           |\\n| Kobold      | Ranged       | 8      | 4      | 0           | Ground        | 3           |\\n| Knight      | Melee        | 12     | 6      | 50          | Ground        | 4           |\\n| Archer      | Ranged       | 12     | 4      | 50          | Ground        | 4           |\\n| Troll       | Melee        | 20     | 6      | + 1 + 1     | Ground        | 5           |\\n| Deep Thing  | Ranged       | 6      | 4      | + 3 + 3     | Aquatic       | 5           |\\n| Pig Man     | Melee        | 20     | 3      | + 5         | Ground        | 6           |\\n| Fire Elemental | Ranged     | 14     | 3      | + 5         | Flying        | 6           |\\n| Frost Troll | Melee        | 24     | 4      | + 5         | Ground        | 7           |\\n| Ice Elemental | Ranged   | 16     | 4      | + 4         | Flying        | 7           |\\n\\nText in red and blue denote fire and ice attack/defence respectively.\"}"}
{"id": "hg4wXlrQCV", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Best case speed comparison for each environment.\\n\\n| Environment  | Steps Per Second (Best case) | Environment Workers (Best Case) |\\n|--------------|------------------------------|---------------------------------|\\n| Craftax-Classic | 405618                      | 4096                            |\\n| Craftax      | 266961                      | 4096                            |\\n| Procgen      | 7638                        | 1024                            |\\n| NetHack      | 5628                        | 64                              |\\n| Crafter      | 1580                        | 1024                            |\\n| MineRL       | 133                         | 8                               |\\n\\nC. Optimistic Environment Resets\\n\\nOne of the features of compilation in JAX is that branching cannot occur inside a function that is parallelised with the \\\\texttt{vmap} operator. In order to achieve branching-like behaviour, both branches must always be evaluated and then the appropriate output can be selected with the branching condition. The most significant place that this occurs within JAX RL environments is with resets, where applying this constraint involves both resetting the environment and stepping it every timestep, and then selecting the desired next state based on the outputted done flag. Indeed, this is the standard approach taken in Gymnax (Lange, 2022). While this approach is fine when dealing with simple environments, resetting in \\\\textit{Craftax} is significantly more expensive as it involves generating 9 floors with multiple passes of Perlin Noise and room generation. Furthermore, episode lengths tend to be in the hundreds of timesteps, so over 99% of generated worlds are simply discarded.\\n\\nTo overcome this bottleneck, we instead perform what we term \u2018optimistic environment resets\u2019. When running \\\\(N\\\\) parallel environment workers, we only generate \\\\(M\\\\) new worlds every timestep, where \\\\(M \\\\ll N\\\\). Then, for the environments that do need resetting, we sample without replacement from our set of \\\\(M\\\\) generated new states. Typically this will make no tangible difference to the RL training process, except in the exceedingly rare case where more than \\\\(M\\\\) environments reset simultaneously. In this case, some environment workers will be allocated the same new state. It should be noted that, while this will reduce the diversity of experiences seen by the agent, the stochasticity in the agent and the environment will mean the trajectories will still be different.\\n\\nIn practice, we use a ratio of 1 new state generated for each 16 environment workers. In the case of 1024 environment workers, this means we generate 64 new states every timestep. Assuming an average episode length of around 200 (in practice we usually see much longer), we can model the number of done flags as a binomial distribution \\\\(X \\\\sim B(1024, \\\\frac{1}{200})\\\\). The probability that we end up with more resets than generated levels is therefore \\\\(P(X > 64)\\\\) which is less than \\\\(10^{-10}\\\\). This shows that, when running for 1 billion timesteps, it is extremely unlikely we would even see a single duplicated world.\\n\\nWe have shown that this optimisation has essentially no effect on the RL process, while providing a significant speedup (around 2x in \\\\textit{Craftax}).\\n\\nD. Further Environment Details\\n\\nThe maximum episode length is 100,000 at which the episode is truncated.\\n\\nD.1. Block Types\\n\\nThe list of block types in \\\\textit{Craftax} is shown in Table 2.\\n\\nD.2. Creatures\\n\\nThe list of creatures in \\\\textit{Craftax} is shown in Table 3.\\n\\nD.3. World Generation\\n\\nExamples of procedurally generated floors are shown in Figures 11, 12, 13, 14, 15, 16, 17 and 18. Floors 2, 5, 7 and 8 have been artificially lit up to make them visible, whereas in gameplay torches need to be placed to reveal them. Floor 0 is the overworld and is essentially equivalent to the world from \\\\textit{Craftax-Classic}. \\n\\n14\"}"}
{"id": "hg4wXlrQCV", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\n\\nFigure 11. Craftax: Floor 1 - Dungeons\\n\\nFigure 12. Craftax: Floor 2 - Gnomish Mines\\n\\nFigure 13. Craftax: Floor 3 - Sewers\\n\\nFigure 14. Craftax: Floor 4 - Vaults\"}"}
{"id": "hg4wXlrQCV", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\n\\nFigure 15. Craftax: Floor 5 - Troll Mines\\n\\nFigure 16. Craftax: Floor 6 - Fire Realm\\n\\nFigure 17. Craftax: Floor 7 - Ice Realm\\n\\nFigure 18. Craftax: Floor 8 - Graveyard\"}"}
{"id": "hg4wXlrQCV", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.4. Observation Space\\n\\nBoth Craftax and Craftax-Classic have options for pixel-based and symbolic observations. Each consist of a view of the map and the players stats and inventory. The map view in Craftax-Classic is 7x9 squares, while Craftax uses 9x11.\\n\\n**Pixels**\\nThe pixel-based observations for Craftax-Classic take the same form as those in Crafter, with each 16x16 square downscaled to 7x7. For Craftax, we downscale only to 10x10. This is because we deal with numbers greater than 9, meaning that the digits had to be rendered in a smaller font size. At 7x7 downscaling many of these digits were indistinguishable. Pixel observations from Crafter, Craftax-Classic and Craftax are shown in Figure 25.\\n\\n**Symbolic**\\nThe primary observation space we experiment on is symbolic, as it allows for significantly faster runtime. Each square on the map encodes three one-hot vectors and a light level. The first one-hot vector is of size 37 and encodes the block type, the second is of size 5 and encodes the item type, while the third is of size 36 and encodes the creature type. The light level is a float between 0 and 1. If the light level for a block is $< 0.05$, then the block, item and creature are masked out.\\n\\nThe inventory/stats bar is represented as an array with each element representing the amount corresponding to a particular item or attribute. All values are scaled to be roughly in the range 0-1. All inventory items (which the player can carry up to 99 of) are represented as $\\\\sqrt{10}^n$. Table 4 lists the symbolic inventory observation for Craftax. The observation for Craftax-Classic is the subset of these elements that exist in the classic version.\\n\\n**Textual**\\nAlthough we don't make use of it for our experiments, we also provide a textual 'renderer'. This encodes the grid in the agents view as \\\"<CREATURE> on <ITEM> on <BLOCK>\\\" for each viewable block, so a troll standing on a path block with a torch on it would be encoded as \\\"troll on torch on path\\\". We encode the players inventory and intrinsics as \\\"<INVENTORY_ITEM>: X\\\", so an agent with 2 diamonds in its inventory would see \\\"DIAMOND: 2\\\". Use in conjunction with the text tutorial found in the codebase, this could be use for language-conditioned learning.\\n\\nD.5. Action Space\\n\\nThe action space for Craftax is shown in Table 5.\\n\\nD.6. Achievements\\n\\nThe achievements for Craftax are listed in Table 6. Achievements for Craftax-Classic are those with an ID of up to and including 21.\\n\\nE. Hyperparameter Tuning\\n\\nE.1. Craftax-1B\\nThe hyperparameters and considered values for PPO on Craftax-1B are shown in Table 7. Each run was performed for 1 billion timesteps with 1 seed. We started from a set of values picked informally and then tuned each hyperparameter individually. These hyperparameters were then taken as the baseline for the other methods, which all build off of the PPO implementation. For PPO-RNN, we kept the same hyperparameters as PPO, with the hidden size being equal to the MLP layer size. The hyperparameters for ICM, E3B and RND are shown in Tables 8, 9 and 10 respectively.\\n\\nE.2. Craftax-1M\\nThe hyperparameters for Craftax-1M were tuned more thoroughly, as the experiments were very fast to run. We performed a random search over the PPO hyperparameters to obtain a baseline PPO implementation, before then running a random search for ICM and E3B. The hyperparameters for PPO, ICM and E3B are shown in Tables 11, 12 and 13 respectively.\\n\\nF. Further Results\\n\\nF.1. Individual Achievements\\nThe success rates through training for Craftax-1B and Craftax-1M are shown in Figures 19 and 20 respectively.\"}"}
{"id": "hg4wXlrQCV", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nBenchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long-term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.\\n\\n1. Introduction\\n\\nProgress in reinforcement learning (RL) algorithms is driven in large part by the development and adoption of suitable benchmarks. Examples include the Arcade Learning Environment (Bellemare et al., 2013) for value based deep RL algorithms, Mujoco (Todorov et al., 2012) for continuous control and the StarCraft Multi-Agent Challenge (Samvelyan et al., 2019) for multi-agent RL. In the effort towards increasingly general agents, there has arisen a community focused on benchmarks that exhibit more open-ended dynamics (Stanley et al., 2017), in the form of procedural world generation, skill acquisition and reuse, long term dependencies and continual learning. This has motivated the development of environments like MALMO (Johnson et al., 2016) (Minecraft), The NetHack Learning Environment (K\u00fcttler et al., 2020), MiniHack (Samvelyan et al., 2021) and Crafter (Hafner, 2021). However, slow runtime has rendered them inaccessible to current methods without large-scale computational resources, limiting their practicality to the research community.\\n\\nSimultaneously, as the speed of running an end-to-end compiled RL pipeline has become fully appreciated (Lu et al., 2022), there has been an explosion of RL environments implemented in JAX (Freeman et al., 2021; Lange, 2022; Bonnet et al., 2023; Rutherford et al., 2023; Nikulin et al., 2022).\"}"}
{"id": "hg4wXlrQCV", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\n\\nThe elimination of CPU-GPU transfer combined with efficient parallelisation and compilation means that experiments that would previously have taken a large compute cluster days to run can finish in minutes on a single GPU (Lu et al., 2022).\\n\\nTo bring these two paradigms together, we present the Craftax benchmark: a JAX-based environment exhibiting complex, open-ended dynamics and running orders of magnitude faster than comparable environments (Figure 2). Concretely, we first propose Craftax-Classic, a reimplementation of Crafter (Hafner, 2021) in JAX that runs 250 times faster than the Python-native original. We then go on to show that, with easy access to drastically more timesteps, Craftax-Classic is solved (to within 90% of maximum return) by a simple PPO (Schulman et al., 2017) agent in 51 minutes (see Appendix F.3 for further details).\\n\\nWe therefore also present Craftax, a significantly more challenging environment that incorporates dynamics inspired from NetHack (K\u00fcttler et al., 2020) and more broadly the Roguelike genre as a whole. Our experiments show that existing methods make little progress on Craftax. We therefore hope that it presents a meaningful challenge for future RL research, while allowing experimentation with limited computational resources.\\n\\n$\\\\text{Environment Workers}$\\n\\n$\\\\begin{array}{cccccc}\\n10^2 & 10^3 & 10^4 & 10^5 \\\\\\\\\\n\\\\text{Steps per Second} & \\\\text{Craftax-Classic} & \\\\text{Craftax} & \\\\text{Procgen} & \\\\text{NetHack} & \\\\text{Crafter} & \\\\text{MineRL}\\n\\\\end{array}$\\n\\nFigure 2. Speed comparison with popular benchmarks for open-ended learning. Craftax-Classic and Craftax are 257x and 169x faster than Crafter respectively. Details of the speed test are in Appendix B and best case results are in Table 1.\\n\\n2. Background\\n\\n2.1. JAX-Based Environments\\n\\nJAX (Bradbury et al., 2018) is a library that allows for code written in Python to be compiled down to Accelerated Linear Algebra and run on hardware accelerators like GPUs and TPUs. While deep RL training has traditionally been split between collecting trajectories on CPU-based environments and then training policy and value networks on the GPU, the relatively new phenomenon of JAX-based environments allows for the whole RL pipeline to be run on the GPU. This allows for massive parallelisation of trajectory gathering (we use up to 4096 parallel environment workers), the elimination of the GPU-CPU transfer bottleneck and just-in-time (JIT) compilation of the whole training process.\\n\\n2.2. Crafter\\n\\nCrafter (Hafner, 2021) is a top-down, procedurally generated, open-world survival game that works like a simplified, 2D Minecraft. The player finds themselves in a world consisting of grassland, forests, lakes and mountains and must survive by gathering materials to craft tools with, maintaining hunger, thirst and energy levels, and fighting enemies. The Crafter RL environment uses pixel-based observations and has a discrete action space made up of 4 movement directions and 13 interactions (e.g. MAKE_STONE_PICKAXE, SLEEP, PLACE_FURNACE). It has a set of 22 unique achievements (e.g. DEFEAT_ZOMBIE, EAT_PLANT, COLLECT_DIAMOND), each giving the agent a +1 reward the first time it is achieved in an episode. Crafter was proposed to evaluate \\\"strong generalization, deep exploration, and long-term reasoning\\\" (Hafner, 2021).\\n\\nWhile Crafter has become a popular benchmark, the evaluation protocol proposed allocates algorithms only 1 million environment interactions, a very limiting constraint when compared to other RL benchmarks. For instance, the standard protocol for the Atari benchmark (a set of environments that require limited generalisation (Cobbe et al., 2020)) is to use 200M environment interactions per game (Machado et al., 2018), while The NetHack Learning Environment suggests using 1 billion (K\u00fcttler et al., 2020). In practice, this has made Crafter a benchmark largely focused on testing sample efficiency (similar to other benchmarks like Atari 100k (Bellemare et al., 2013)).\\n\\nWhile we reuse many of the Crafter dynamics, our aim is to provide a benchmark for investigations into open-endedness rather than sample efficiency. Open-endedness, by its very definition, should not be constrained by a fixed number of...\"}"}
{"id": "hg4wXlrQCV", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"samples. In practice we have to impose some limit, but this should be suitably high as to not impact the emergence of interesting phenomena.\\n\\n2.3. The NetHack Learning Environment\\n\\nNetHack is an infamously difficult video game, adapted into an RL benchmark through The NetHack Learning Environment (NLE) (K\u00fcttler et al., 2020) and MiniHack (Samvelyan et al., 2021). The game involves the player descending through around fifty floors of dungeons, collecting items, fighting enemies and gaining experience before retrieving an amulet and finally beating the game by \u2018ascending\u2019. The game is known to be incredibly hard, with it typically taking years of practice to achieve an ascension (K\u00fcttler et al., 2020). Current algorithmic approaches (both rule-based and deep RL) fail to complete the game (Hambro et al., 2022).\\n\\nWhile NLE executes NetHack natively in C++ and is faster than Python-based Crafter, it is still significantly slower than GPU based environments (Figure 2).\\n\\n2.4. Open-Ended Learning\\n\\nOpen-Ended Learning (Stanley et al., 2017) refers to a broad category of problem domains and algorithms that focus on learning in a perpetual and unguided manner. We investigate two subfields that are particularly relevant to Craftax.\\n\\nExploration through Intrinsic Rewards\\n\\nExploration is a key part of RL performance, especially in environments with sparse or deceptive rewards, for which simple methods like max-entropy RL (Haarnoja et al., 2018) or $\\\\epsilon$-greedy policies are insufficient. One method to overcome these difficulties is through the idea of an intrinsic reward (Schmidhuber, 1991; Oudeyer et al., 2007; Barto, 2013; Bellemare et al., 2016), in addition to the extrinsic reward received from the environment. This signal should intuitively reward the agent for visiting states or performing actions that satisfy some novelty-like heuristic (Lehman & Stanley, 2008), causing the agent to actively explore the environment by pushing back the frontiers of its knowledge.\\n\\nUnsupervised Environment Design\\n\\nUnsupervised environment design (UED) is a paradigm in RL where an adversary proposes environments configurations (referred to as levels) for an agent to train on (Dennis et al., 2020; Jiang et al., 2021a; Parker-Holder et al., 2022). The adversary is rewarded for choosing levels that maximise the agent\u2019s regret, defined as the difference in return between the current and optimal agent. This has been empirically shown to automatically induce a curriculum of progressively harder levels that aid the performance and generalisation properties of the learned agent. Different UED algorithms require different levels of access to the underlying environment state, ranging from simply being able to repeat seeds (Jiang et al., 2021a) to directly editing the levels (Parker-Holder et al., 2022).\\n\\nDue to the functional nature of Craftax necessitated by JAX, the entire environment state is exposed as a single object, making UED methods easy to apply.\\n\\n3. Craftax\\n\\n3.1. Craftax-Classic: A Reimplementation of Crafter in JAX\\n\\nWe first present Craftax-Classic, a JAX-based reimplementaion of the Crafter benchmark. The aim of Craftax-Classic is to mimic the Crafter dynamics as closely as possible, while allowing orders of magnitude faster RL training (Figure 2). While most of the dynamics of Crafter were remade exactly in line with the original, some subtle changes were made for performance reasons (See Appendix A for a discussion of all the differences). Our hope is that Craftax-Classic provides an easy introduction to Craftax for those already familiar with the Crafter benchmark.\\n\\nWhile Crafter uses pixel-based observations, many of the aspects that Crafter tests (exploration, memory) are indifferent to the exact nature of the observation and the use of pixels simply adds an extra layer of representation learning to the problem. For this reason, we provide versions of Craftax with both symbolic and pixel-based observations, with the former running around 10x faster than the latter.\\n\\n3.2. Craftax: An Extension of Crafter with NetHack-Like Mechanics in JAX\\n\\nAs shown in Appendix F.3, Craftax-Classic is convincingly solved by a PPO agent running for under an hour on a single GPU. To provide a more compelling challenge we present the main Craftax environment, designed to be significantly harder, while retaining a fast runtime. Craftax adds a large and diverse range of new game mechanics. This section provides a brief overview, with Appendix D containing more details.\\n\\nMultiple Floors\\n\\nWhile in Crafter the player is confined to a single 64x64 grid, Craftax contains 9 unique procedurally generated floors, including caves, dungeons, fire and ice floors and a final boss floor. The player can descend and ascend through the world by finding the ladders that connect adjacent floors. Each floor contains distinct challenges in the forms of different terrain generation, enemies and required skills, necessitating deep exploration and generalisation. While each floor is unique, many game mechanics are shared between them and, on a meta-level, exploration strategies that worked on earlier floors (for instance moving adjacent to a block and trying different actions to figure out its characteristics) will also work on later floors. In this way we hope to not only facilitate generalisation across different procedurally generated worlds but also generalisation of the\"}"}
{"id": "hg4wXlrQCV", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\n\\nCombat\\n\\nWe overhaul the combat system with damage and defence split into physical, fire and ice categories. As well as being able to craft higher-tier weapons, the player can use a bow (found in a random chest on the first dungeon floor) for ranged attacks, and read books (also found in dungeon chests) to learn offensive spells to cast. The player can also craft armour to protect themselves from damage (essential when descending to the lower floors). As well as increasing the general complexity of the environment, the diversity in combat furthers the in-context learning element provided by the procedural level generation \u2014 an agent that stumbles upon a strong weapon or armour piece should suitably change its strategy. This further extends the exploration problem as, by design, there should not be one fixed strategy (for instance, always putting experience points into strength and defeating enemies with melee attacks) that works on every level, meaning that an agent will have to explore a diverse range of strategies to achieve consistently high return.\\n\\nNew Creatures\\n\\nThe 3 creatures present in Crafter (cow, zombie and skeleton) have been increased to 19 different creatures, each with unique effects, behaviours and techniques required to defeat them. For instance, the Fire Elementals present on floor 7 either require an appropriate enchantment on the player\u2019s sword or magic to defeat, while beating the aquatic enemies on floor 6 typically requires ranged weaponry. This diversity of creatures further enhances the exploration problem, as no two are the same and strategies for dealing with one may entirely fail for others.\\n\\nPotions and Enchantments\\n\\nThe player can find potions of varying colours spread over the 9 floors; however, the effects of these potions are randomly permuted every episode. This means that an agent will need to discover which potions correspond to which effects through trial and error each episode, further testing in-context learning and memory. Also, by spending mana (magical energy) and sacrificing one of the new gemstones, the player can enchant weapons and armour. This provides them with increased damage or protection against a certain element.\\n\\nAttributes\\n\\nThe player is rewarded with an experience point every time they descend to a new floor. These points can be spent to increase the player\u2019s dexterity, strength or intelligence. Dexterity makes the player more capable with a bow, as well as allowing them to go longer between eating, drinking and sleeping. Strength increases the player\u2019s health and melee damage, while intelligence increases the player\u2019s magical powers. The assignment of experience points can drastically alter the player\u2019s abilities and have a large influence on the strategy an agent should use, testing the agent\u2019s long-term reasoning capabilities, as successful runs typically take tens of thousands of frames.\\n\\nBoss Floor\\n\\nThe final floor contains a very challenging boss fight, with the player having to defeat waves of every existing enemy in the game. As well as serving as a final challenge, the boss fight tests how the agent can transfer its prior knowledge of fighting these enemies in different contexts to the unique structure of the boss floor.\\n\\nDifficulty\\n\\nCraftax strikes a balance of difficulty, being significantly more challenging than Crafter, but not as hard as games like NetHack. For perspective, it took one of the authors (with extensive knowledge of the game mechanics) roughly 5 hours of gameplay to first achieve a \u2018perfect\u2019 run where every achievement was completed. This was playing in a GUI that allowed for unlimited time to pause and think before taking each action. We think that Craftax is hard enough to present a significant challenge beyond the capabilities of existing RL methods, while being ultimately achievable without domain knowledge.\\n\\n3.3. RL Environment Interface\\n\\nCraftax conforms to the Gymnax (Lange, 2022) wrapper for easy integration with existing frameworks.\\n\\nObservation Space\\n\\nCraftax provides options for both pixel-based and symbolic observations. The pixel-based observation is a downscaled $63 \\\\times 63 \\\\times 3$ image for Craftax-Classic (in line with the original Crafter) and $110 \\\\times 130 \\\\times 3$ for Craftax. The symbolic observations use a one-hot encoding to represent both block types and creatures in the player\u2019s visual area, which is then appended to an array containing the rest of the player\u2019s information (inventory, health, hunger, attributes, etc.), resulting in a flat observation space of 1345 for Craftax-Classic and 8268 for Craftax.\\n\\nAction Space\\n\\nCraftax uses a discrete action space of 4.\"}"}
{"id": "hg4wXlrQCV", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\n\\nFigure 3. Rewards on Craftax-1B for PPO, PPO-RNN, ICM, E3B and RND. Each algorithm is run for 1 billion timesteps with 10 seeds. The shaded area denotes 1 standard error.\\n\\nsize 17 and 43 for Craftax-Classic and Craftax respectively. As in Crafter, every action can be taken at any timestep, so attempting an action without its specific prerequisites will effectively cause the agent to execute a no-op action, stepping the environment forward one timestep. For an exhaustive list of actions see Appendix D.5.\\n\\nReward\\nWe follow a similar reward structure to Crafter, defining a set of achievements and giving the agent a reward the first time each achievement is completed each episode. There are 22 achievements in Craftax-Classic and 65 in Craftax. We also penalise the agent by 0.1 for every point of damage taken and reward it by 0.1 for every point recovered. Early experiments on Craftax showed that providing a flat +1 reward for each achievement, did not provide enough incentive for the agent to start exploring the dungeons, as the agent would usually be killed quickly upon descending and learn to avoid them. To encourage progress towards harder tasks we grouped achievements into four categories: 'Basic', 'Intermediate', 'Advanced' and 'Very Advanced', each worth 1, 3, 5 and 8 reward respectively. An exhaustive listing and description of all the achievements is located in Appendix D.6.\\n\\n3.4. Evaluation Framework\\nWe propose two separate benchmarks to evaluate performance on Craftax (we leave Craftax-Classic mostly as a tool to help researchers iterate quickly on an environment very similar to the original Crafter). We also limit the benchmarks to considering symbolic observations, as these environments are significantly faster to run, and the pixel-based environments do not contribute anything significant to the core research directions we believe Craftax is useful for investigating.\\n\\nFirst, we propose the Craftax-1B Challenge, where a budget of 1 billion environment interactions is permitted on the Craftax-Symbolic environment. This challenge is meant to provide a chance for algorithms focusing on exploration, continual learning, and long-term planning and reasoning to differentiate themselves on a hard benchmark, without being overly constrained by sample efficiency concerns. We set the benchmark at 1 billion steps to keep it manageable for researchers with limited computational resources (a run on a single RTX 4090 can finish in an hour), while providing enough timesteps for a substantial amount of meaningful exploration of the environment. Averaging around 5 steps per second (roughly the rate the authors moved at when playing), 1 billion timesteps corresponds to over 6 years of continual human gameplay. We use reward as the metric to compare agents.\\n\\nSecondly, we propose the Craftax-1M Challenge, where a budget of 1 million environment interactions is permitted on the Craftax-Symbolic environment. This benchmark is meant as a test of sample efficiency, with the benefit that experiments can take only seconds to finish. We believe the main value of this benchmark is in the almost instantaneous results researchers can obtain, allowing for iteration of methods at an unprecedented speed. It should be noted that even with such a tight environment interaction budget, we found superior performance with massive parallelism of workers (see Table 11 in Appendix E).\\n\\n4. Experiments\\n4.1. Exploration Baselines\\nWe use the PPO implementations from PureJaxRL (Lu et al., 2022) as the foundation for our baselines, allowing us to run fully compiled end-to-end RL pipelines. We flatten the symbolic observations and use a 4 layer MLP of width 512 for both policy and value networks. We experimented initially with using convolutions for the map view part of the observation, but consistently found that fully connected networks performed better.\\n\\nWe run PPO with an MLP as well as PPO with memory (using a Gated Recurrent Unit (Chung et al., 2014)), which we refer to as PPO-RNN. We also experiment using two state of the art intrinsic rewards for additional exploration:\\n\\nRandom Network Distillation We use the popular exploration baseline Random Network Distillation (RND) (Burda et al., 2018) as our first method for global intrinsic reward. RND works by training a network that is given observations to match a network of the same architecture that was randomly initialised. Intuitively, the error of this network will be high on unseen observations, as the network will not yet have learned to generalise consistently. We train two RND networks, one for each network head, so that the error is averaged over both heads.\"}"}
{"id": "hg4wXlrQCV", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"have been trained to match the random output. This error is then used as an additional intrinsic reward that incentivises the agent to explore states with novel observations.\\n\\nIntrinsic Curiosity Module\\n\\nOur second baseline is the Intrinsic Curiosity Module (ICM) (Sukhbaatar et al., 2018). ICM learns a world model and provides intrinsic reward proportional to the error of this world model, incentivising the agent to visit areas of the environment which are badly modelled and therefore probably less visited. The latent space of this world model is constructed in such a way as to ignore aleatoric uncertainty in the environment, in order to focus exploration and avoid the so called 'Noisy TV problem' (Burda et al., 2018).\\n\\nExploration via Elliptical Episodic Bonuses\\n\\nOur third and final exploration baseline is the E3B algorithm (Henaff et al., 2022). Rather than considering exploration over the entire training process, E3B formulates its intrinsic reward to encourage exploration within each episode. An ellipse is fitted around the agents current trajectory in latent space and it is rewarded for moving far away from this ellipse. This approach has been shown to outperform global exploration methods, especially in non-singleton (i.e. procedurally generated) environments (Henaff et al., 2022; 2023).\\n\\n4.2. UED Baselines\\n\\nWe assess PLR and ACCEL and compare them to Domain Randomisation (Jakobi, 1997; Tobin et al., 2017, DR), which corresponds to uniformly sampling randomly generated levels, as per usual. We use JaxUED (Coward et al., 2024) as the base implementation for all our UED experiments.\\n\\nPLR\\n\\nPrioritised level replay (PLR) (Jiang et al., 2021a;b) is a curation-based UED method which maintains a buffer of training levels. At every step, one of the two options is randomly chosen: (a) the agent is evaluated on randomly generated levels, and those with high regret are added to the buffer; (b) the agent is trained on a sample of high-regret levels from the buffer. PLR performs gradient updates on the random levels, whereas robust PLR (denoted as $\\\\text{PLR}^\\\\perp$) only updates the agent on the levels sampled from the buffer. This is theoretically justified and (in some domains) empirically results in higher performance (Jiang et al., 2021a).\\n\\nACCEL\\n\\nACCEL (Parker-Holder et al., 2022) is similar to PLR, but also considers randomly mutated levels from the buffer as candidate levels. ACCEL therefore increases the complexity of levels over time via iterative editing instead of having to rely solely on random generation and curation.\\n\\nIn this section, we provide results for the two proposed benchmarks using the previously discussed algorithms. We use JaxUED (Coward et al., 2024) for all UED experiments, and hyperparameter tuning is detailed in Appendix E.\\n\\n4.3. Craftax-1B\\n\\nThe returns for the evaluated algorithms on Craftax-1B are summarised in Figure 3 with the final achievement yields split by difficulty shown in Figure 4. We report reward as a percentage of the maximum achievable reward (obtained by completing every achievement) of 226 for readability. Fine-grained achievement results are shown in Appendix F.1 with a selected set of achievements highlighted in Figure 5. We see that PPO and PPO-RNN both quickly master most of the basic achievements, with memory helping to push progress further into the intermediate category. Interestingly, none of the tested exploration methods improved performance and E3B in fact significantly reduced the reward. We hypothesise this is because the reward structure is dense enough for RL methods without intrinsic reward to learn on, with the intrinsic reward potentially serving to distract the agent. None of the agents make any progress in the harder achievement categories.\\n\\nTaking a closer look at the achievements highlighted in Figure 5 we first see that all methods quickly and robustly learn basic achievements such as $\\\\text{MAKE_WOOD_PICKAXE}$. While all methods enter the dungeon (the first floor beneath the overworld) an appreciable amount, we see that PPO-RNN does so significantly more than the others, giving it more access to the high-reward achievements and driving its strong performance. Interestingly, the success rate on some simple achievements like $\\\\text{DEFEAT_ZOMBIE}$ decrease over time, with PPO-RNN doing the worst of all. This is explained by the fact that zombies only appear in the overworld, so the stronger agents are trading off low-reward achievements in the overworld for high-reward ones underground. We see that no appreciable progress is made into the second floor (the gnomish mines), with only very rare\"}"}
{"id": "hg4wXlrQCV", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\n\\nFigure 5. Achievement success rate on Craftax-1B for selected achievements over 10 seeds, with error bars denoting 1 standard error.\\n\\nApproaches being made into it. Interestingly, we also see that the EAT_PLANT (notable for being perhaps the hardest achievement in the original Crafter) is entirely ignored by PPO-RNN and is actually best achieved by E3B, indicating that while the intrinsic reward may not be helping with overall return, it does incentivise the agent to explore different parts of the environment.\\n\\nAlthough it looks like PPO-RNN in particular is still learning, we find that running for 10 billion timesteps barely improves performance (Appendix F.2), indicating that this is likely an artefact of learning rate decay (You et al., 2019).\\n\\n4.4. Craftax-1M\\n\\nThe results for Craftax-1M are shown in Figure 6. Compared to Craftax-1B there is much less separation between the algorithms with all of them performing relatively similarly. Fine grained achievements are shown in Appendix F.1, showing that progress is made only on the very basic achievements (mostly corresponding to those carried over from Crafter), with agents entering the dungeons only on individual occurrences.\\n\\n4.5. Craftax-1B: UED\\n\\nWe now investigate applying DR, PLR, Robust PLR and ACCEL to Craftax-1B. For ACCEL, we evaluate three different mutation operators applied to the overworld:\\n\\n- Noise: Each level $\\\\theta$ corresponds to the angle vectors that the perlin noise algorithm uses to generate worlds. Mutation adds a uniform random number to each element.\\n- Swap: We randomly swap two different tiles. To increase relevance, one of these is within the $16 \\\\times 16$ central area of the level\u2014close to where the agent starts.\\n- Restricted Swap (RSwap): To obtain more coherent levels, we restrict the tiles that can be swapped. For instance, stone can be swapped with any ore tile, and grass can be swapped with trees, leading to more realistic and coherent levels.\\n\\nResults\\n\\nAfter training for 1B timesteps, we evaluate the saved checkpoints on a fixed set of 20 evaluation levels, which were generated normally. We plot the mean and standard error over 10 independent runs.\\n\\nThe primary results are presented in Figure 7, and success rates on all achievements are provided in Figure 34. We find that PLR performs best, followed by DR, PLR $\\\\perp$, restricted swap and noise ACCEL, which all perform roughly the same. Unrestricted swap ACCEL performs the worst.\\n\\nFigure 7. Evaluation reward for various UED methods on Craftax-1B on 10 seeds, with the shaded area denoting 1 standard error.\\n\\nPLR vs Robust PLR\\n\\nWe find that Robust PLR performs worse than vanilla PLR, and only slightly outperforms...\"}"}
{"id": "hg4wXlrQCV", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"One reason could be that, unlike the gridworld mazes it has previously been tested on, Craftax consistently generates high-quality and solvable levels. Therefore, the agent is not penalised by training on these DR levels. Since we compared each algorithm when using the same number of environment interactions, robust PLR performs fewer gradient updates than DR or PLR. This result shows how important it is to expand the pool of environments UED is tested on for thorough analysis of the methods.\\n\\nDistribution Shift\\nSince UED alters the training distribution (a phenomenon named curriculum-induced covariate shift (CICS) by Jiang et al. (2022)), we make the distinction between DR levels sampled from the normal generator and replay levels sampled from the shifted UED distribution. We find that ACCEL-based methods perform differently in replay levels compared to DR. For instance, the unrestricted swap method collects diamonds more than 30% of the time during replay, compared to 7% on DR levels (see Figures 32 and 33). In addition, restricted swap ACCEL reaches the gnomish mines 1% of the time in replay levels, compared to almost never in DR levels.\\n\\nThe average return (shown in Figure 8) is also different between replay and DR levels. The two swapping-based ACCEL methods have significantly higher returns during replay. This may indicate that the generated levels are easier (e.g., with more resources near the start), and the agent is able to perform more reward-generating activities. The other methods have similar episode lengths during both phases.\\n\\nFigure 8. Average episode length for replay and DR levels on Craftax-1B over 10 seeds, with the shaded area denoting 1 standard error.\\n\\nUED Results Analysis\\nWe find that DR performs competitively to the other UED methods, with only normal PLR significantly outperforming it. We believe this is partly due to Craftax evaluating on in distribution levels, rather than a set of hold-out levels, where UED generally performs better (Jiang et al., 2021a; Parker-Holder et al., 2022).\\n\\n5. Related Work\\n5.1. Environments for Open-Endedness\\nExploration\\nThe most widely known hard-exploration environments are those from the Atari benchmark with sparse rewards, most famously Montezuma's Revenge and Pitfall (Bellemare et al., 2013). While these environments once posed a significant challenge to the community, they have been solved by methods like Go-Explore (Ecoffet et al., 2021), which exhaustively explore the entire game tree using state resets or goal-conditioned policies. This technique is only possible due to these being singleton environments with mostly deterministic mechanics. As was similarly argued in K\u00fcttler et al. (2020), we believe that these environments are therefore no longer suitable for furthering research into exploration and that focus should instead be applied to more complex environments with randomised levels and stochastic mechanics.\\n\\nMany recent environments have subscribed to this view and introduced elements like procedural generation and more complex world mechanics. Procgen (Cobbe et al., 2020) addresses the singleton and determinism problems seen in the Arcade Learning Environment. It randomly samples an initial 'context' at the start of each episode, corresponding to concepts like obstacle placement or maze layouts. This idea is taken even further with environments like Minecraft (Johnson et al., 2016) which generates entirely unique (and effectively infinite) worlds each episode. As discussed in Section 2, both Crafter (Hafner, 2021) and NetHack (K\u00fcttler et al., 2020) make use of procedural generation to produce finite worlds each episode. MiniHack (Samvelyan et al., 2021) provides a set of small levels that make use of the NetHack mechanics, but are less complicated than attempting to solve the entire NetHack game. XLand (Open Ended Learning Team et al., 2021) can represent a huge array of 3D games with different goals and emergent strategies.\\n\\nUED\\nThe most commonly used environments for UED are MiniHack (Samvelyan et al., 2021) and Mini-Grid (Chevalier-Boisvert et al., 2023), BipedalWalker (Wang et al., 2019) and Car Racing (Jiang et al., 2021a). All of these environments are relatively simple and while they were vital for getting the field off the ground, for UED to be used in more advanced settings it needs to be shown to generalise to more complex environments, for which we believe Craftax is a suitable candidate.\\n\\n5.2. JAX-Based Environments\\nBrax (Freeman et al., 2021) arguably kickstarted the development of JAX-based environments by reimplementing MuJoCo-like mechanics (Todorov et al., 2012) and showing that training with large numbers of environment workers...\"}"}
{"id": "hg4wXlrQCV", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\n\\ncould drastically reduce wall clock time without unduly affecting performance. Gymnax (Lange, 2022) implemented classic control (Brockman et al., 2016), BSuite (Osband et al., 2020) and MinAtar environments (Young & Tian, 2019) among others. JaxMARL (Rutherford et al., 2023) remade many popular environments for multi-agent RL like StarCraft (Samvelyan et al., 2019; Ellis et al., 2024) and Hanabi (Bard et al., 2020). Pgx (Koyamada et al., 2023) provides implementations of classic board games like Chess and Go. Jumanji (Bonnet et al., 2023) recreated many combinatorial problems like Sudoku and graph colouring.\\nThe most similar to our own work is that of XLand-Minigrid (Nikulin et al., 2023), which can represent a huge diversity of tasks in a fast gridworld-based environment. While this provides an incredible tool for open-ended research, we see Craftax as filling a different niche. Specifically, Craftax can be seen as providing a very difficult exploration task in the form of a distribution of meaningfully similar levels. Whereas, XLand-Minigrid encompasses a very wide range of semantically unique levels of which each one is significantly simpler than an instance of Craftax. In other words, Craftax assesses deep exploration whereas XLand-Minigrid assesses wide generalisation.\\n\\n6. Conclusion\\n\\nWe present Craftax, a blazing fast environment filled with complex mechanics that cannot be solved by existing RL algorithms. We hope that Craftax will facilitate research into areas including exploration, continual learning, generalisation, skill acquisition and long term reasoning. We believe an agent that could solve Craftax would represent a substantial step forward for the field and look forward to seeing what the community uses the benchmark to develop.\\n\\nAcknowledgements\\n\\nWe would like to thank Alex Goldie and Sebastian Towers for their invaluable contributions to game design, our anonymous reviewers for their useful feedback and to the open-source software community who have already contributed numerous improvements to the codebase.\\n\\nImpact Statement\\n\\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.\\n\\nReferences\\n\\nBard, N., Foerster, J. N., Chandar, S., Burch, N., Lanctot, M., Song, H. F., Parisotto, E., Dumoulin, V., Moitra, S., Hughes, E., et al. The hanabi challenge: A new frontier for ai research. Artificial Intelligence, 280:103216, 2020.\\nBarto, A. G. Intrinsic motivation and reinforcement learning. In Intrinsically motivated learning in natural and artificial systems, pp. 17\u201347, 2013.\\nBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016.\\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013.\\nBonnet, C., Luo, D., Byrne, D., Surana, S., Coyette, V., Duckworth, P., Midgley, L. I., Kalloniatis, T., Abramowitz, S., Waters, C. N., Smit, A. P., Grinsztajn, N., Sob, U. A. M., Mahjoub, O., Tegegn, E., Mimouni, M. A., Boige, R., de Kock, R., Furelos-Blanco, D., Le, V., Pretorius, A., and Laterre, A. Jumanji: a diverse suite of scalable reinforcement learning environments in jax, 2023. URL https://arxiv.org/abs/2306.09884.\\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\\nBurda, Y., Edwards, H., Storkey, A. J., and Klimov, O. Exploration by random network distillation. CoRR, abs/1810.12894, 2018. URL http://arxiv.org/abs/1810.12894.\\nChevalier-Boisvert, M., Dai, B., Towers, M., de Lazcano, R., Willems, L., Lahlou, S., Pal, S., Castro, P. S., and Terry, J. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023.\\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\"}"}
{"id": "hg4wXlrQCV", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Item Representation | Min | Max |\\n|---------------------|-----|-----|\\n| Wood                | \u221a   | 10  |\\n| Stone               | \u221a   | 10  |\\n| Coal                | \u221a   | 10  |\\n| Iron                | \u221a   | 10  |\\n| Diamond             | \u221a   | 10  |\\n| Sapphire            | \u221a   | 10  |\\n| Ruby                | \u221a   | 10  |\\n| Sapling             | \u221a   | 10  |\\n| Torch               | \u221a   | 10  |\\n| Arrow               | \u221a   | 10  |\\n| Potion (x6)         |     |     |\\n| Book                |     | 2   |\\n| Pickaxe (level 4)   |     |     |\\n| Sword (level 4)     |     |     |\\n| Sword Enchantment   | I   | + 2 |\\n| Bow                 | bow | 1   |\\n| Armour (x4)         |     | 2   |\\n| Armour Enchantment  | I   | + 2 |\\n| Health (x10)        | 0   | 4   |\\n| Food (x10)          | 0   | 7   |\\n| Drink (x10)         |     | 7   |\\n| Energy (x10)        | 0   | 7   |\\n| Mana (x10)          | 0   | 2   |\\n| XP (x10)            | 0   | 8   |\\n| Dexterity (x10)     | 0   | 1   |\\n| Strength (x10)      | 0   | 1.5 |\\n| Intelligence (x10)  | 0   | 1.5 |\\n| Direction onehot (n)|   - |     |\\n| Day/Night           |     | 1   |\\n| Sleeping            | sleep| 0   |\\n| Resting             | rest| 0   |\\n| Learned Fireball    | fireball| 0   |\\n| Learned Iceball     | iceball| 0   |\\n| Floor               |     | 9   |\\n| Floor Cleared       | cleared floor| 0   |\\n| Boss Vulnerable     | boss vulnerable| 0   |\"}"}
{"id": "hg4wXlrQCV", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Action ID | Action Name          | Key |\\n|----------|----------------------|-----|\\n| 0        | NOOP                 | Q   |\\n| 1        | LEFT                 | A   |\\n| 2        | RIGHT                | D   |\\n| 3        | UP                   | W   |\\n| 4        | DOWN                 | S   |\\n| 5        | DO                   | Space |\\n| 6        | SLEEP                | Tab |\\n| 7        | PLACE_STONE          | R   |\\n| 8        | PLACE_TABLE          | T   |\\n| 9        | PLACE_FURNACE        | F   |\\n| 10       | PLACE_PLANT          | P   |\\n| 11       | MAKE_WOOD_PICKAXE    | 1   |\\n| 12       | MAKE_STONE_PICKAXE   | 2   |\\n| 13       | MAKE_IRON_PICKAXE    | 3   |\\n| 14       | MAKE_WOOD_SWORD      | 5   |\\n| 15       | MAKE_STONE_SWORD     | 6   |\\n| 16       | MAKE_IRON_SWORD      | 7   |\\n| 17       | REST                 | e   |\\n| 18       | DESCEND              |  .  |\\n| 19       | ASCEND               | ,   |\\n| 20       | MAKE_DIAMOND_PICKAXE | 4   |\\n| 21       | MAKE_DIAMOND_SWORD   | 8   |\\n| 22       | MAKE_IRON_ARMOUR     | Y   |\\n| 23       | MAKE_DIAMOND_ARMOUR  | U   |\\n| 24       | SHOOT_ARROW          | I   |\\n| 25       | MAKE_ARROW           | O   |\\n| 26       | CAST_FIREBALL        | G   |\\n| 27       | CAST_ICEBALL         | H   |\\n| 28       | PLACE_TORCH          | J   |\\n| 29       | DRINK_POTION_RED     | Z   |\\n| 30       | DRINK_POTION_GREEN   | X   |\\n| 31       | DRINK_POTION_BLUE    | C   |\\n| 32       | DRINK_POTION_PINK    | V   |\\n| 33       | DRINK_POTION_CYAN    | B   |\\n| 34       | DRINK_POTION_YELLOW  | N   |\\n| 35       | READ_BOOK            | M   |\\n| 36       | ENCHANT_SWORD        | K   |\\n| 37       | ENCHANT_ARMOUR       | L   |\\n| 38       | MAKE_TORCH           | [   |\\n| 39       | LEVEL_UP_DEXTERITY   | ]   |\\n| 40       | LEVEL_UP_STRENGTH    | -   |\\n| 41       | LEVEL_UP_INTELLIGENCE| =   |\\n| 42       | ENCHANT_BOW          | ;   |\\n\\nTable 5. Actions for Craftax.\"}"}
{"id": "hg4wXlrQCV", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| ID | Name               | Category       |\\n|----|--------------------|----------------|\\n| 0  | COLLECT_WOOD       | Basic (1)      |\\n| 1  | PLACE_TABLE        | Basic (1)      |\\n| 2  | EAT_COW            | Basic (1)      |\\n| 3  | COLLECT_SAPLING    | Basic (1)      |\\n| 4  | COLLECT_DRINK      | Basic (1)      |\\n| 5  | MAKE_WOOD_PICKAXE  | Basic (1)      |\\n| 6  | MAKE_WOOD_SWORD    | Basic (1)      |\\n| 7  | PLACE_PLANT        | Basic (1)      |\\n| 8  | DEFEAT_ZOMBIE      | Basic (1)      |\\n| 9  | COLLECT_STONE      | Basic (1)      |\\n| 10 | PLACE_STONE        | Basic (1)      |\\n| 11 | EAT_PLANT          | Basic (1)      |\\n| 12 | DEFEAT_SKELETON    | Basic (1)      |\\n| 13 | MAKE_STONE_PICKAXE | Basic (1)      |\\n| 14 | MAKE_STONE_SWORD   | Basic (1)      |\\n| 15 | WAKE_UP            | Basic (1)      |\\n| 16 | PLACE_FURNACE      | Basic (1)      |\\n| 17 | COLLECT_COAL       | Basic (1)      |\\n| 18 | COLLECT_IRON       | Basic (1)      |\\n| 19 | COLLECT_DIAMOND    | Basic (1)      |\\n| 20 | MAKE_IRON_PICKAXE  | Basic (1)      |\\n| 21 | MAKE_IRON_SWORD    | Basic (1)      |\\n| 22 | MAKE_ARROW         | Basic (1)      |\\n| 23 | MAKE_TORCH         | Basic (1)      |\\n| 24 | PLACE_TORCH        | Basic (1)      |\\n| 25 | MAKE_DIAMOND_SWORD | Intermediate (3) |\\n| 26 | MAKE_IRON_ARMOUR   | Intermediate (3) |\\n| 27 | MAKE_DIAMOND_ARMOUR| Intermediate (3) |\\n| 28 | ENTER_GNOMISH_MINES| Intermediate (3) |\\n| 29 | ENTER_DUNGEON      | Intermediate (3) |\\n| 30 | ENTER_SEWERS       | Advanced (5)   |\\n| 31 | ENTER_VAULT        | Advanced (5)   |\\n| 32 | ENTER_TROLL_MINES  | Advanced (5)   |\\n| 33 | ENTER_FIRE_REALM   | Very Advanced (8) |\\n| 34 | ENTER_ICE_REALM    | Very Advanced (8) |\\n| 35 | ENTER_GRAVEYARD    | Very Advanced (8) |\\n| 36 | DEFEAT_GNOME_WARRIOR| Intermediate (3) |\\n| 37 | DEFEAT_GNOME_ARCHER| Intermediate (3) |\\n| 38 | DEFEAT_ORC_SOLIDER | Intermediate (3) |\\n| 39 | DEFEAT_ORC_MAGE    | Intermediate (3) |\\n| 40 | DEFEAT_LIZARD      | Advanced (5)   |\\n| 41 | DEFEAT_KOBOLD      | Advanced (5)   |\\n| 42 | DEFEAT_TROLL       | Advanced (5)   |\\n| 43 | DEFEAT_DEEP_THING  | Advanced (5)   |\\n| 44 | DEFEAT_PIGMAN      | Very Advanced (8) |\\n| 45 | DEFEAT_FIRE_ELEMENTAL| Very Advanced (8) |\\n| 46 | DEFEAT_FROST_TROLL | Very Advanced (8) |\\n| 47 | DEFEAT_ICE_ELEMENTAL| Very Advanced (8) |\\n| 48 | DAMAGE_NECROMANCER | Very Advanced (8) |\\n| 49 | DEFEAT_NECROMANCER | Very Advanced (8) |\\n| 50 | EAT_BAT            | Intermediate (3) |\\n| 51 | EAT_SNAIL          | Intermediate (3) |\\n| 52 | FIND_BOW           | Intermediate (3) |\\n| 53 | FIRE_BOW           | Intermediate (3) |\\n| 54 | COLLECT_SAPPHIRE  | Intermediate (3) |\\n| 55 | LEARN_FIREBALL     | Advanced (5)   |\\n| 56 | CAST_FIREBALL      | Advanced (5)   |\\n| 57 | LEARN_ICEBALL      | Advanced (5)   |\\n| 58 | CAST_ICEBALL       | Advanced (5)   |\\n| 59 | COLLECT_RUBY       | Intermediate (3) |\\n| 60 | MAKE_DIAMOND_PICKAXE| Intermediate (3) |\\n| 61 | OPEN_CHEST         | Intermediate (3) |\\n| 62 | DRINK_POTION       | Intermediate (3) |\\n| 63 | ENCHANT_SWORD      | Advanced (5)   |\\n| 64 | ENCHANT_ARMOUR     | Advanced (5)   |\\n| 65 | DEFEAT_KNIGHT      | Advanced (5)   |\\n| 66 | DEFEAT_ARCHER      | Advanced (5)   |\\n\\nTable 6. Achievements listing for Craftax. The category column indicates the difficulty classification of the achievement and the associated reward.\"}"}
{"id": "hg4wXlrQCV", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Hyperparameter                  | Considered Values | Value  |\\n|--------------------------------|-------------------|--------|\\n| Entropy Coefficient            | {0.0, 0.01}       | 0.01   |\\n| GAE $\\\\lambda$                 | {0.8, 0.9, 0.95}  | 0.8    |\\n| $\\\\gamma$                      | {0.98, 0.99, 0.995, 0.999, 0.9995} | 0.9995 |\\n| MLP Layer Size                 | {256, 512, 1024}  | 512    |\\n| Learning Rate                  | {0.0002, 0.0003, 0.0004} | 0.0002 |\\n| Environment Workers            | {256, 512, 1024}  | 1024   |\\n| # Minibatches                  | {4, 8, 16}        | 8      |\\n| # Steps                        | {64}              | 64     |\\n| # Update Epochs                | {4}               | 4      |\\n| Clip $\\\\epsilon$               | {0.2}             | 0.2    |\\n| Value Function Coefficient     | {0.5}             | 0.5    |\\n| Activation Function            | {tanh}            | tanh   |\\n| Anneal Learning Rate           | {True}            | True   |\\n\\nTable 7: Hyperparameters for PPO for Craftax-1B.\\n\\n| Hyperparameter                  | Considered Values | Value  |\\n|--------------------------------|-------------------|--------|\\n| Intrinsic Reward Coefficient $\\\\beta$ | {0.1, 0.5, 1.0} | 1.0    |\\n| World Models Learning Rate      | {0.0003}          | 0.0003 |\\n| Forward Loss Coefficient        | {1.0}             | 1.0    |\\n| Inverse Loss Coefficient        | {1.0}             | 1.0    |\\n| World Models Layer Size         | {256}             | 256    |\\n| Latent Size                     | {32, 64, 128}     | 32     |\\n\\nTable 8: Hyperparameters for ICM for Craftax-1B.\\n\\n| Hyperparameter                  | Considered Values | Value  |\\n|--------------------------------|-------------------|--------|\\n| Intrinsic Reward Coefficient $\\\\beta$ | {0.00005, 0.001, 0.005} | 0.001  |\\n| Ridge Regulariser $\\\\lambda$     | {0.005, 0.1, 0.5} | 0.1    |\\n\\nTable 9: Hyperparameters for E3B for Craftax-1B.\\n\\n| Hyperparameter                  | Considered Values | Value  |\\n|--------------------------------|-------------------|--------|\\n| Intrinsic Reward Coefficient $\\\\beta$ | {0.01, 0.1, 1.0} | 1.0    |\\n| RND Output Dimension            | {4, 32, 256, 2048} | 256    |\\n| Distillation Network Learning Rate | {0.00003, 0.0003, 0.003} | 0.0003 |\\n| RND Loss Coefficient            | {0.0001, 0.001, 0.01} | 0.01   |\\n| RND GAE Coefficient             | {0.01}            | 0.01   |\\n\\nTable 10: Hyperparameters for RND for Craftax-1B and Craftax-1M.\"}"}
{"id": "hg4wXlrQCV", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\nCoward, S., Beukman, M., and Foerster, J.\\n\\nJaxued: A simple and useable library in Jax.\\narXiv preprint, 2024.\\n\\nDennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S.\\nEmergent complexity and zero-shot transfer via unsupervised environment design.\\nIn Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems.\\n\\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and Clune, J.\\nFirst return, then explore.\\nNature, 590(7847):580\u2013586, 2021.\\n\\nEllis, B., Cook, J., Moalla, S., Samvelyan, M., Sun, M., Mahajan, A., Foerster, J., and Whiteson, S.\\nSmacv2: An improved benchmark for cooperative multi-agent reinforcement learning.\\nAdvances in Neural Information Processing Systems, 36, 2024.\\n\\nFreeman, C. D., Frey, E., Raichuk, A., Girgin, S., Mordatch, I., and Bachem, O.\\nBrax \u2013 a differentiable physics engine for large scale rigid body simulation, 2021.\\nURL https://arxiv.org/abs/2106.13281.\\n\\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S.\\nSoft actor-critic: Off-policy maximum entropy deep reinforcement learning.\\nIn International conference on machine learning, pp. 1861\u20131870. PMLR, 2018.\\n\\nHafner, D.\\nBenchmarking the spectrum of agent capabilities.\\nIn International Conference on Learning Representations, 2021.\\n\\nHambro, E. et al.\\nInsights from the NeurIPS 2021 Nethack challenge.\\nIn NeurIPS 2021 Competitions and Demonstrations Track, pp. 41\u201352. PMLR, 2022.\\n\\nHenaff, M., Raileanu, R., Jiang, M., and Rockt\u00e4schel, T.\\nExploration via elliptical episodic bonuses.\\nAdvances in Neural Information Processing Systems, 35:37631\u201337646, 2022.\\n\\nHenaff, M., Jiang, M., and Raileanu, R.\\nA study of global and episodic bonuses for exploration in contextual mdps.\\narXiv preprint arXiv:2306.03236, 2023.\\n\\nJakobi, N.\\nEvolutionary robotics and the radical envelope-of-noise hypothesis.\\nAdaptive behavior, 6(2):325\u2013368, 1997.\\n\\nJiang, M., Dennis, M., Parker-Holder, J., Foerster, J. N., Grefenstette, E., and Rockt\u00e4schel, T.\\nReplay-guided adversarial environment design.\\nIn Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, pp. 1884\u20131897, 2021a.\\nURL https://proceedings.neurips.cc/paper/2021/hash/0e915db6326b6fb6a3c56546980a8c93-Abstract.html.\\n\\nJiang, M., Grefenstette, E., and Rockt\u00e4schel, T.\\nPrioritized level replay.\\nIn Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4940\u20134950. PMLR, 2021b.\\nURL http://proceedings.mlr.press/v139/jiang21b.html.\\n\\nJiang, M., Dennis, M., Parker-Holder, J., Lupu, A., K\u00fcttler, H., Grefenstette, E., Rockt\u00e4schel, T., and Foerster, J.\\nGrounding aleatoric uncertainty for unsupervised environment design.\\nAdvances in Neural Information Processing Systems, 35:32868\u201332881, 2022.\\n\\nJiang, M., Dennis, M., Grefenstette, E., and Rockt\u00e4schel, T.\\nMinimax: Efficient baselines for autocurricula in Jax.\\nIn Agent Learning in Open-Endedness Workshop at NeurIPS, 2023.\\n\\nJohnson, M., Hofmann, K., Hutton, T., and Bignell, D.\\nThe malmo platform for artificial intelligence experimentation.\\nIn Ijcai, pp. 4246\u20134247, 2016.\\n\\nKoyamada, S., Okano, S., Nishimori, S., Murata, Y., Habara, K., Kita, H., and Ishii, S.\\nPgx: Hardware-accelerated parallel game simulators for reinforcement learning.\\nIn Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.\\n\\nK\u00fcttler, H., Nardelli, N., Miller, A., Raileanu, R., Selvatici, M., Grefenstette, E., and Rockt\u00e4schel, T.\\nThe Nethack learning environment.\\nAdvances in Neural Information Processing Systems, 33:7671\u20137684, 2020.\\n\\nLange, R. T.\\nGymnax: A JAX-based reinforcement learning environment library, 2022.\\nURL http://github.com/RobertTLange/gymnax.\\n\\nLehman, J. and Stanley, K.\\nExploiting open-endedness to solve problems through the search for novelty.\\nArtificial Life - ALIFE, 01 2008.\\n\\nLu, C., Kuba, J., Letcher, A., Metz, L., Schroeder de Witt, C., and Foerster, J.\\nDiscovered policy optimisation.\\nAdvances in Neural Information Processing Systems, 35:16455\u201316468, 2022.\"}"}
{"id": "hg4wXlrQCV", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\n\\nMachado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. *Journal of Artificial Intelligence Research*, 61:523\u2013562, 2018.\\n\\nNikulin, A., Kurenkov, V., Zisman, I., Sinii, V., Agarkov, A., and Kolesnikov, S. XLand-minigrid: Scalable meta-reinforcement learning environments in JAX. In Intrinsically-Motivated and Open-Ended Learning Workshop, NeurIPS2023. URL https://openreview.net/forum?id=xALDC4aHGz.\\n\\nOpen Ended Learning Team et al. Open-ended learning leads to generally capable agents, 2021. URL https://arxiv.org/abs/2107.12808.\\n\\nOsband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E., Saraiva, A., McKinney, K., Lattimore, T., Szepesv\u00e1ri, C., Singh, S., Van Roy, B., Sutton, R., Silver, D., and van Hasselt, H. Behaviour suite for reinforcement learning. In *International Conference on Learning Representations*, 2020. URL https://openreview.net/forum?id=rygf-kSYwH.\\n\\nOudeyer, P.-Y., Kaplan, F., and Hafner, V. V. Intrinsic motivation systems for autonomous mental development. *IEEE transactions on evolutionary computation*, 11(2):265\u2013286, 2007.\\n\\nParker-Holder, J., Jiang, M., Dennis, M., Samvelyan, M., Foerster, J., Grefenstette, E., and Rockt\u00e4schel, T. Evolving curricula with regret-based environment design. In Proceedings of the International Conference on Machine Learning, volume 162 of *Proceedings of Machine Learning Research*, pp. 17473\u201317498. PMLR, 2022. URL https://proceedings.mlr.press/v162/parker-holder22a.html.\\n\\nRaffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N. Stable-baselines3: Reliable reinforcement learning implementations. *Journal of Machine Learning Research*, 22(268):1\u20138, 2021. URL http://jmlr.org/papers/v22/20-1364.html.\\n\\nRutherford, A., Ellis, B., Gallici, M., Cook, J., Lupu, A., Ingvarsson, G., Willi, T., Khan, A., de Witt, C. S., Souly, A., Bandyopadhyay, S., Samvelyan, M., Jiang, M., Lange, R. T., Whiteson, S., Lacerda, B., Hawes, N., Rocktaschel, T., Lu, C., and Foerster, J. N. Jaxmarl: Multi-agent rl environments in jax. *arXiv preprint arXiv:2311.10090*, 2023.\\n\\nSamvelyan, M., Rashid, T., De Witt, C. S., Farquhar, G., Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H., Foerster, J., and Whiteson, S. The starcraft multi-agent challenge. *arXiv preprint arXiv:1902.04043*, 2019.\\n\\nSamvelyan, M., Kirk, R., Kurin, V., Parker-Holder, J., Jiang, M., Hambro, E., Petroni, F., K\u00fcttler, H., Grefenstette, E., and Rockt\u00e4schel, T. Minihack the planet: A sandbox for open-ended reinforcement learning research, 2021. URL https://arxiv.org/abs/2109.13202.\\n\\nSchmidhuber, J. A possibility for implementing curiosity and boredom in model-building neural controllers. In Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pp. 222\u2013227, 1991.\\n\\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*, 2017.\\n\\nStanley, K. O., Lehman, J., and Soros, L. Open-endedness: The last grand challenge you've never heard of. While open-endedness could be a force for discovering intelligence, it could also be a component of AI itself, 2017.\\n\\nSukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via asymmetric self-play. In 6th International Conference on Learning Representations, ICLR 2018, 2018. URL https://openreview.net/forum?id=SkT5Yg-RZ.\\n\\nTobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P. Domain randomization for transferring deep neural networks from simulation to the real world. In *International Conference on Intelligent Robots and Systems*, pp. 23\u201330. IEEE, 2017. doi: 10.1109/IROS.2017.8202133. URL https://doi.org/10.1109/IROS.2017.8202133.\\n\\nTodorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026\u20135033. IEEE, 2012.\\n\\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. *arXiv preprint arXiv:1901.01753*, 2019.\\n\\nYou, K., Long, M., Wang, J., and Jordan, M. I. How does learning rate decay help modern neural networks? *arXiv preprint arXiv:1908.01878*, 2019.\\n\\nYoung, K. and Tian, T. Minatar: An atari-inspired testbed for thorough and reproducible reinforcement learning experiments. *arXiv preprint arXiv:1903.03176*, 2019.\"}"}
{"id": "hg4wXlrQCV", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\n\\nA. Differences between Craftax-Classic and Crafter\\n\\nWhile Craftax-Classic was designed to be as similar to Crafter as possible, the realities of coding in JAX necessitated some technical deviations.\\n\\nA.1. Creatures\\n\\nIn Crafter, the creatures (zombies, skeletons and cows) are continually spawned around the player and despawned when the player moves far away from them. This allows for the appearance of a world full of creatures, without having to simulate far-away entities that will have no effect on the player.\\n\\nWe broadly follow this system with Craftax-Classic, with the additional constraint that we have to pre-specify the maximum number of each creature that can exist simultaneously. This is because JAX requires that when we compile a function, the shapes of every array in the compute graph must be known a priori, preventing the use of arbitrarily sized arrays. This means that every timestep we have to simulate the steps for a fixed number of creatures. It should be noted that this has the extra side effect that we actually always have to simulate the maximum number of each creature every timestep, even if less are currently spawned. In order to allow for a dynamic number of creatures (up to the maximum) we mask out the influences of unspawned creatures.\\n\\nThe maximum number of creatures are 3, 3, 2 and 3 for zombies, cows, skeletons and arrows (which we treat as creatures for purpose of code reuse), respectively. In practice it is quite rare to observe more than these amounts of each creature simultaneously in Crafter.\\n\\nA similar reasoning applies to cultivating plants, where only a finite number of growing plants can be tracked. This was set to 10 in Craftax-Classic.\\n\\nA.2. World Generation\\n\\nWhile Crafter uses Simplex Noise as the base of its world generation, we used the very similar Perlin Noise, adapted from the Perlin-Numpy repository. This produces qualitatively similar results to Crafter (Figures 9 and 10).\\n\\nB. Speed Comparison Details\\n\\nAll experiments were run on a single machine with a GeForce RTX 4090 (24GB of VRAM), i9-13900K (24 cores with 32 threads) and 32GB of RAM. For each environment we started with a single worker and proceeded to double the number of environment workers until the machine crashed from an out-of-memory error. We measure the average steps per second over the entire RL learning process, unlike other works which report times from running the environment with a static/random agent. We believe this provides a more useful indication of the times a researcher could actually expect to see when experimenting on these benchmarks.\\n\\nFor Craftax we tested on the Craftax-Classic-Symbolic and Craftax-Symbolic environments, using an end-to-end compiled RL pipeline based off PureJaxRL (Lu et al., 2022). For NetHack we used the built-in IMPALA baseline.\\n\\nFor Crafter and Procgen we used the stable-baselines3 (Raffin et al., 2021) PPO implementation, with the SubprocVecEnv for parallelised workers on multiple threads. We were unable to get any baselines working on MineRL so, in order to be as generous as possible, we simply ran the environment on multiple threads with random actions. For this reason the MineRL numbers are likely overestimates of the true expected end-to-end steps-per-second. Since this was by far the slowest environment anyway, we didn't feel the need to explore this further.\\n\\nFor Craftax, Crafter, Procgen and NetHack we tried to match the magnitude of the value/policy network learning as much as possible by matching number of network parameters (different observation structures made it impossible to completely match network architectures), epochs and minibatch size. There is no way to perfectly compare all these environments, but we felt that our decisions represent the speeds that a researcher using these environments is likely to see in practice and therefore is a fair and useful metric.\"}"}
{"id": "hg4wXlrQCV", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\n\\nFigure 9. Levels from Craftax-Classic\\n\\nFigure 10. Levels from Crafter. Figure reprinted from Hafner (2021).\"}"}
{"id": "hg4wXlrQCV", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 11. Hyperparameters for PPO for Craftax-1M.\\n\\n| Hyperparameter               | Considered Values                                                                 | Value |\\n|-----------------------------|-----------------------------------------------------------------------------------|-------|\\n| Entropy Coefficient         | {0.01, 0.05, 0.1, 0.5, 1.0}                                                       | 0.01  |\\n| GAE \u03bb                       | {0.7, 0.8, 0.9, 0.95}                                                            | 0.8   |\\n| \u03b3                           | {0.9, 0.99}                                                                      | 0.99  |\\n| MLP Layer Size              | {128, 256, 512, 1024}                                                            | 512   |\\n| Learning Rate               | {0.0002, 0.0003, 0.0004, 0.001}                                                   | 0.0003|\\n| Environment Workers         | {16, 64, 256}                                                                    | 256   |\\n| # Minibatches               | {2, 4, 8, 16}                                                                    | 8     |\\n| # Steps                     | {16, 64, 256}                                                                    | 16    |\\n| # Update Epochs             | {1, 2, 4, 8, 16}                                                                 | 4     |\\n| Clip \u03f5                     | {0.2, 1.0}                                                                       | 0.2   |\\n| Value Function Coefficient  | {0.5, 1.0}                                                                       | 0.5   |\\n| Activation Function         | {tanh, relu}                                                                     | tanh |\\n| Anneal Learning Rate        | {True, False}                                                                    | True  |\\n\\n## Table 12. Hyperparameters for ICM for Craftax-1M.\\n\\n| Hyperparameter               | Considered Values                                                                 | Value |\\n|-----------------------------|-----------------------------------------------------------------------------------|-------|\\n| Intrinsic Reward Coefficient | \u03b2                                                                                 | 0.01  |\\n| World Models Learning Rate  |                                                                                   | 0.0003|\\n| Forward Loss Coefficient    | {0.1, 0.5, 1.0}                                                                   | 0.1   |\\n| Inverse Loss Coefficient    | {0.1, 0.5, 1.0}                                                                   | 0.5   |\\n| World Models Layer Size     |                                                                                   | 256   |\\n| Latent Size                 | {16, 32, 64, 128}                                                                 | 16    |\\n\\n## Table 13. Hyperparameters for E3B for Craftax-1M.\\n\\n| Hyperparameter               | Considered Values                                                                 | Value |\\n|-----------------------------|-----------------------------------------------------------------------------------|-------|\\n| Intrinsic Reward Coefficient | \u03b2                                                                                 | 0.0001|\\n| Ridge Regulariser \u03bb         | {0.0001, 0.001, 0.01, 0.1, 1.0}                                                   | 0.5   |\"}"}
{"id": "hg4wXlrQCV", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"| Task                        |Algorithm| Timestep (M) | Success Rate (%) |\\n|-----------------------------|---------|--------------|------------------|\\n| collect_wood                | PPO-RNN | 0.05         | 0.00             |\\n| place_table                 | PPO-RNN | 0.05         | 0.00             |\\n| eat_cow                     | PPO-RNN | 0.05         | 0.00             |\\n| collect_sapling             | PPO-RNN | 0.05         | 0.00             |\\n| collect_drink               | PPO-RNN | 0.05         | 0.00             |\\n| make_wood_pickaxe           | PPO-RNN | 0.05         | 0.00             |\\n| make_wood_sword             | PPO-RNN | 0.05         | 0.00             |\\n| place_plant                 | PPO-RNN | 0.05         | 0.00             |\\n| defeat_zombie               | PPO-RNN | 0.05         | 0.00             |\\n| collect_stone               | PPO-RNN | 0.05         | 0.00             |\\n| place_stone                 | PPO-RNN | 0.05         | 0.00             |\\n| eat_plant                   | PPO-RNN | 0.05         | 0.00             |\\n| defeat_skeleton             | PPO-RNN | 0.05         | 0.00             |\\n| make_stone_pickaxe          | PPO-RNN | 0.05         | 0.00             |\\n| make_stone_sword            | PPO-RNN | 0.05         | 0.00             |\\n| place_furnace               | PPO-RNN | 0.05         | 0.00             |\\n| collect_coal                | PPO-RNN | 0.05         | 0.00             |\\n| collect_iron                | PPO-RNN | 0.05         | 0.00             |\\n| collect_diamond             | PPO-RNN | 0.05         | 0.00             |\\n| make_diamond_pickaxe        | PPO-RNN | 0.05         | 0.00             |\\n| make_diamond_sword          | PPO-RNN | 0.05         | 0.00             |\\n| enter_gnomish_mines         | PPO-RNN | 0.05         | 0.00             |\\n| enter_dungeon               | PPO-RNN | 0.05         | 0.00             |\\n| enter_sewers                | PPO-RNN | 0.05         | 0.00             |\\n| enter_vault                 | PPO-RNN | 0.05         | 0.00             |\\n| enter_troll_mines           | PPO-RNN | 0.05         | 0.00             |\\n| enter_fire_realm            | PPO-RNN | 0.05         | 0.00             |\\n| enter_ice_realm             | PPO-RNN | 0.05         | 0.00             |\\n| enter_graveyard             | PPO-RNN | 0.05         | 0.00             |\\n| defeat_gnome_warrior        | PPO-RNN | 0.05         | 0.00             |\\n| defeat_gnome_archer         | PPO-RNN | 0.05         | 0.00             |\\n| defeat_orc_soldier          | PPO-RNN | 0.05         | 0.00             |\\n| defeat_orc_mage             | PPO-RNN | 0.05         | 0.00             |\\n| defeat_lizard               | PPO-RNN | 0.05         | 0.00             |\\n| defeat_kobold               | PPO-RNN | 0.05         | 0.00             |\\n| defeat_knight               | PPO-RNN | 0.05         | 0.00             |\\n| defeat_archer               | PPO-RNN | 0.05         | 0.00             |\\n| defeat_troll                | PPO-RNN | 0.05         | 0.00             |\\n| defeat_deep_thing           | PPO-RNN | 0.05         | 0.00             |\\n| defeat_pigman               | PPO-RNN | 0.05         | 0.00             |\\n| defeat_fire_elemental       | PPO-RNN | 0.05         | 0.00             |\\n| defeat_frost_troll          | PPO-RNN | 0.05         | 0.00             |\\n| defeat_ice_elemental        | PPO-RNN | 0.05         | 0.00             |\\n| damage_necromancer          | PPO-RNN | 0.05         | 0.00             |\\n| defeat_necromancer          | PPO-RNN | 0.05         | 0.00             |\\n| eat_bat                     | PPO-RNN | 0.05         | 0.00             |\\n| eat_snail                   | PPO-RNN | 0.05         | 0.00             |\\n| find_bow                    | PPO-RNN | 0.05         | 0.00             |\\n| fire_bow                    | PPO-RNN | 0.05         | 0.00             |\\n| learn_fireball              | PPO-RNN | 0.05         | 0.00             |\\n| cast_fireball               | PPO-RNN | 0.05         | 0.00             |\\n| learn_iceball               | PPO-RNN | 0.05         | 0.00             |\\n| cast_iceball                | PPO-RNN | 0.05         | 0.00             |\\n| open_chest                  | PPO-RNN | 0.05         | 0.00             |\\n| drink_potion                | PPO-RNN | 0.05         | 0.00             |\\n| enchant_sword               | PPO-RNN | 0.05         | 0.00             |\\n| enchant_armour              | PPO-RNN | 0.05         | 0.00             |\\n\\nFigure 19. Success rate curves for all achievements on Craftax-1B. Each algorithm was run on 10 seeds for 1 billion timesteps. The shaded area denotes 1 standard error.\"}"}
{"id": "hg4wXlrQCV", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 20. Success rate curves for all achievements on Craftax-1M. Each algorithm was run on 10 seeds for 1 million timesteps. The shaded area denotes 1 standard error.\\n\\nF.2. Craftax-Extended with 10B Environment Interactions\\n\\nTo really push the limits of the environment, we ran PPO-RNN on 10B environment interactions, with the results shown in Figures 22 and 21. The results show that learning for 10B iterations barely affects performance, with the agent still not making it into the gnomish mines. Interestingly, as in the main results on Craftax-1B, the reward increases more quickly towards the end of the experiment. The fact that this occurs at roughly the same value for both 1B and 10B implies that this is an artifact of the learning rate schedule.\"}"}
{"id": "hg4wXlrQCV", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 21. Achievement yields for PPO-RNN on Craftax-Extended given 10 billion environment interactions. The experiment was repeated with 4 seeds and the shaded areas shows 1 standard error.\"}"}
{"id": "hg4wXlrQCV", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning\\n\\nFigure 22. Reward for PPO-RNN on Craftax-Extended given 10 billion environment interactions. The experiment was repeated with 4 seeds and the shaded areas shows 1 standard error.\\n\\nF.3. Craftax-Classic\\n\\nFigures 23 and 24 show the results of running PPO-RNN on Craftax-Classic for 1 billion timesteps. As can be seen, all achievements excepting COLLECT_DIAMOND and EAT_PLANT are reliably fulfilled. Collecting diamonds is achieved somewhat reliably while eating plants is only achieved very rarely. While the agent hasn't 'solved' the benchmark by reliably completing every achievement, it is so close to doing so that releasing only the Craftax-Classic would be of little use, as it would likely be solved very quickly.\\n\\nFigure 23. Reward for PPO-RNN on Craftax-Classic given 1 billion environment interactions. The experiment was repeated with 10 seeds and the shaded areas shows 1 standard error.\"}"}
{"id": "hg4wXlrQCV", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm\\nPPO-RNN\\n\\nFigure 24. Achievement yields for PPO-RNN on Craftax-Classic given 1 billion environment interactions. The experiment was repeated with 10 seeds and the shaded areas shows 1 standard error.\\n\\nG. UED\\nG.1. Implementation Changes\\nIn standard implementations of UED (Jiang et al., 2023), the PPO rollout length is determined by the episode's length. This is because, whenever new levels are sampled, one rollout is performed. This is used to both (a) learn and (b) to approximate a regret value for this level. This does not pose a problem if the maximum episode length is short (such as in Minigrid); however, with Craftax and its long episodes, this approach is limited, as there must either be a long time between subsequent agent updates, or the score would only take into account part of an episode.\\n\\nWe modify the implementation to have multiple short inner rollouts, which the agent learns on as normal. After a certain number of these, we use the entire trajectory information to approximate regret, which then updates the adversary. This allows us to completely decouple these two aspects. Although this induces some non-stationarity\u2014as the agent is learning on the trajectories that the adversary uses for scores\u2014we found this does not significantly hamper performance. Finally, since the agent rarely runs out of time, we restrict the episode length to be 4096 instead of the full 10000.\"}"}
{"id": "hg4wXlrQCV", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Qualitative Results\\n\\nFigures 26 to 31 show several levels curated by each method. The curation-based methods generally cause resources to be more centrally-located, near where the player spawns. The unrestricted ACCEL swapping method generates levels that are visually very different to the normally generated ones, as there is no restriction to where it can place tiles. The other mutations and PLR generate more plausible levels.\\n\\nFigure 26. ACCEL Noise replay levels at the start, middle and end of training\\n\\nFigure 27. ACCEL Swap replay levels at the start, middle and end of training\\n\\nFigure 28. ACCEL Swap (Restrict) replay levels at the start, middle and end of training\\n\\nFigure 29. Normal PLR replay levels at the start, middle and end of training\\n\\nFigure 30. Robust PLR replay levels at the start, middle and end of training\\n\\nFigure 31. DR levels at the start, middle and end of training\\n\\nG.3. Hyperparameters\\n\\nTable 14 contains the UED hyperparameters used. We tuned these as follows.\\n\\n1. We used PPO hyperparameters found in an earlier hyperparameter sweep and did a small search over learning rate \\\\{0.0002, 0.0003\\\\}.\\n\\n2. We then performed a grid search over the following parameters for both PLR variations: temperature \\\\{0.3, 1.0\\\\}, replay probability \\\\{0.5, 0.8\\\\}, score function \\\\{MaxMC, PVL\\\\}, prioritisation \\\\{rank, topk\\\\} and topk's k \\\\{32, 256\\\\}.\\n\\n3. We then used the best parameters for the ACCEL-based methods, and further tuned over the number of mutations in \\\\{10, 100, 200\\\\}. In this case, PLR outperformed robust PLR, so all of our ACCEL methods also perform gradient updates on DR levels, but not on mutated levels.\"}"}
{"id": "hg4wXlrQCV", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Breed                  | DR | PLR | ACCEL (Swap) | ACCEL (RSwap) | ACCEL (Noise) |\\n|-----------------------|----|-----|--------------|---------------|---------------|\\n| Collect Wood          | 95 | 100 |              |               |               |\\n| Place Table           | 80 | 90  |              |               |               |\\n| Eat Cow               | 100|     |              |               |               |\\n| Collect Sapling       | 50 | 75  |              |               |               |\\n| Collect Drink         | 100|     |              |               |               |\\n| Make Wood Pickaxe     | 50 |     |              |               |               |\\n| Make Wood Sword       | 50 |     |              |               |               |\\n| Place Plant           | 25 | 50  |              |               |               |\\n| Eat Plant             | 100|     |              |               |               |\\n| Collect Stone         | 0  | 50  |              |               |               |\\n| Place Stone           | 0  | 25  |              |               |               |\\n| Eat Plant             | 0  | 50  |              |               |               |\\n| Collect Stone         | 0  | 50  |              |               |               |\\n| Place Stone           | 0  | 25  |              |               |               |\\n| Collect Coal          | 0  | 50  |              |               |               |\\n| Collect Iron          | 0  | 40  |              |               |               |\\n| Collect Diamond       | 0  | 20  |              |               |               |\\n| Make Iron Pickaxe     | 0  | 20  |              |               |               |\\n| Make Iron Sword       | 0  |     |              |               |               |\\n| Make Arrow            | 0  | 20  |              |               |               |\\n| Make Torch            | 0  |     |              |               |               |\\n| Enter Gnomish Mines   | 0  |     |              |               |               |\\n| Enter Dungeon         | 0.05 | 0.00 |              |               |               |\\n| Enter Sewers          | 0.05 | 0.00 |              |               |               |\\n| Enter Vault           | 0.05 | 0.00 |              |               |               |\\n| Enter Troll Mines     | 0.05 | 0.00 |              |               |               |\\n| Enter Fire Realm      | 0.05 | 0.00 |              |               |               |\\n| Enter Ice Realm       | 0.05 | 0.00 |              |               |               |\\n| Enter Graveyard       | 0  | 0.0 | 0.2 | 0.4 | 0.00 |\\n| Defeat Gnome Warrior  | 0  | 0.02 | 0.05 | 0.00 | 0.50 |\\n| Defeat Gnome Archer   | 0  | 50  | 0.05 | 0.00 | 0.05 |\\n| Defeat Orc Soldier    | 0  | 25  | 0.05 | 0.00 | 0.05 |\\n| Defeat Orc Mage       | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Defeat Lizard         | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Defeat Kobold         | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Defeat Knight         | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Defeat Archer         | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Defeat Troll          | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Defeat Deep Thing     | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Defeat Pigman         | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Defeat Fire Elemental | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Defeat Frost Troll    | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Defeat Ice Elemental  | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Damage Necromancer    | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Defeat Necromancer    | 0  | 0.1 | 0.00 | 0.05 | 0.00 |\\n| Eat Bat               | 0  | 50  | 0.05 | 0.00 | 0.05 |\\n| Eat Snail             | 0  | 50  | 0.05 | 0.00 | 0.05 |\\n| Find Bow              | 0  | 50  | 0.05 | 0.00 | 0.05 |\\n| Fire Bow              | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Learn Fireball        | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Cast Fireball         | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Learn Iceball         | 0.05 | 0.00 | 0.05 | 0.00 | 0.05 |\\n| Cast Iceball          | 0  | 50  | 0.05 | 0.00 | 0.05 |\\n| Open Chest            | 0  | 1   | 1e9          |               |               |\\n| Drink Potion          | 0  | 1   | 1e9          |               |               |\\n| Enchant Sword         | 0  | 1   | 1e9          |               |               |\\n| Enchant Armour        | 0  | 1   | 1e9          |               |               |\\n\\nFigure 32. Replay achievement success rates for UED.\"}"}
{"id": "hg4wXlrQCV", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Action          | 0   | 50  | 100 |\\n|-----------------|-----|-----|-----|\\n| collect_wood    |     |     |     |\\n| place_table     | 0   | 50  | 100 |\\n| eat_cow         |     |     |     |\\n| collect_sapling | 25  | 50  | 75  |\\n| collect_drink   |     |     |     |\\n| make_wood_pickaxe | 0 | 50 | 100 |\\n| make_wood_sword | 50  | 100 |\\n| place_plant     |     |     |     |\\n| defeat_zombie   |     | 50  | 100 |\\n| collect_stone   |     |     |     |\\n| place_stone     |     | 0.01|\\n| eat_plant       |     | 25  | 100 |\\n| defeat_skeleton |     | 0   | 50  |\\n| make_stone_pickaxe | 0 | 50 | 100 |\\n| make_stone_sword | 50  | 100 |\\n| wake_up         |     |     |     |\\n| place_furnace   |     | 0   | 50  |\\n| collect_coal    |     | 0   | 50  |\\n| collect_iron    |     | 0   | 50  |\\n| collect_diamond |     | 0   | 50  |\\n| make_iron_pickaxe | 0 | 50 | 100 |\\n| make_iron_sword | 50  | 100 |\\n| make_arrow      |     |     |     |\\n| make_torch      |     | 0.01|\\n| place_torch     |     |     |     |\\n| collect_sapphire | 0 | 50 | 100 |\\n| collect_ruby    | 0.0 | 0.5 | 1.0 |\\n| make_diamond_pickaxe | 0 | 0.5 | 1.0 |\\n| make_diamond_sword | 0.0 | 2.5 | 5.0 |\\n| make_iron_armour | 0.0 | 0.002 | 0.004 |\\n| make_diamond_armour | 0.0 | 0.02 | 0.04 |\\n| enter_gnomish_mines | 0 | 0.05 | 0.05 |\\n| enter_dungeon   |     | 0.05|\\n| enter_sewers     |     | 0.05|\\n| enter_vault      |     | 0.05|\\n| enter_troll_mines| 0.05|\\n| enter_fire_realm |     | 0.05|\\n| enter_ice_realm  |     | 0.05|\\n| enter_graveyard  |     | 0.05|\\n| defeat_gnome_warrior | 0.05|\\n| defeat_gnome_archer | 0 | 25 | 50 |\\n| defeat_orc_soldier | 0 | 20 | 40 |\\n| defeat_orc_mage  |     | 0.05|\\n| defeat_lizard    |     | 0.05|\\n| defeat_kobold    |     | 0.05|\\n| defeat_knight    |     | 0.05|\\n| defeat_archer    |     | 0.05|\\n| defeat_troll     |     | 0.05|\\n| defeat_deep_thing |     | 0.05|\\n| defeat_pigman    |     | 0.05|\\n| defeat_fire_elemental | 0.05|\\n| defeat_frost_troll | 0.05|\\n| defeat_ice_elemental | 0.05|\\n| damage_necromancer | 0.05|\\n| defeat_necromancer | 0.05|\\n| eat_bat          |     |     |     |\\n| eat_snail        |     |     |     |\\n| find_bow         |     |     |     |\\n| fire_bow         |      |     |     |\\n| learn_fireball   |     |     |     |\\n| cast_fireball    |     |     |     |\\n| learn_iceball    |     |     |     |\\n| cast_iceball     |     |     |     |\\n| open_chest       | 0   | 1   | 1e9 |\\n| drink_potion     |     |     |     |\\n| enchant_sword    |     |     |     |\\n| enchant_armour   |     |     |     |\\n\\nFigure 33. DR achievement success rates for UED.\"}"}
