{"id": "WPt9HRmMrG", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Text Prompts for Warm-start\\n\\nIn Section 3.1, for the warm-start process, we generate initial pseudo labels through a sequence of text prompts described. For example, we employ \\\"Road. Sidewalk. Building. . . . Bicycle.\\\" prompts for Cityscapes and \\\"Aeroplane. Bicycle. Bird. . . . Tvmonitor.\\\" for PASCAL, where each word aligns with the respective target class. However, each prompt, such as \\\"Diningtable\\\", can be segmented into multiple tokens, such as \\\"Dining\\\" and \\\"table\\\". Therefore, we assign each token to its corresponding class to derive the initial labels for the warm-start process.\\n\\nB. User Study with Different Queries\\n\\nTo verify the efficiency of the proposed correction query \\\\textit{C} in Active Label Correction (ALC) compared to classification query \\\\textit{C}_{cls} in conventional Active Learning (AL), we conduct a user study focusing on actual labeling costs, specifically annotation time. The example of the correction query questionnaire is illustrated in Figure 2, and the results are summarized in Table 1. Each question presents the user with instructions, an image with an object highlighted, and options for classifying the object. For the correction query scenario, the instructions include the pseudo label of the foundation model, and users only need to correct if the pseudo label is incorrect. The detailed instruction for correction query is given as follows:\\n\\n\\\\textit{Is this pixel a TV?}\\n\\nGive the correct label only if the pseudo label is incorrect.\\n\\nOn the other hand, the example instruction for classification query is given as follows:\\n\\n\\\\textit{Give the correct label of the pixel.}\\n\\nBased on the ground-truth, we collect 20 images consisting of 10 images with correct pseudo labels, and 10 images with incorrect pseudo labels counterparts, i.e., \\\\( p = 0.5 \\\\). We ask for labels of these images in both correction queries and classification queries. A total of 20 volunteers participate in the survey. To prevent the user from memorizing images, we only ask one type of query per user, which means we ask the correction queries to 10 users and the classification queries to the others. The responses from annotators are evaluated by calculating the accuracy of the classification prediction. As shown in Table 1, the correction query only requires 75\\\\% labeling time of that of the classification query. In terms of accuracy, both queries show the same 95\\\\%.\\n\\nC. Absolute Performance of ALC vs. AL\\n\\nWhile conventional AL for semantic segmentation methods use the same DeepLab-v3+ (Chen et al., 2018) segmentation decoder combined with backbone pre-trained with the ImageNet (Deng et al., 2009) dataset, the architecture of their backbones are slightly different. ALC (ours) utilize plain ResNet101, MulSpx (Hwang et al., 2023) use ResNet101 combined with deepstem tricks (He et al., 2019), and MerSpx (Kim et al., 2023a) and Spx (Cai et al., 2021) employ Xception-65 (Chollet, 2017). Figure 3 presents the performance in terms of recovery rate relative to a fully supervised model, calculated as the ratio of our model's performance to that of the fully supervised model.\\n\\nHere, we additionally report the comparison with absolute mIoU in Figure 8 over various budget levels, represented by the number of clicks, for both PASCAL and Cityscapes datasets. The 95\\\\% performance of each baseline's fully supervised model is illustrated with a dashed line labeled as 95\\\\% (\\\\( \\\\cdot \\\\)). Our proposed ALC method consistently demonstrate the most efficient performance.\\n\\nD. Ablation Studies\\n\\nD.1. Initial Pseudo Labels\\n\\nTable 7: Performance of initial pseudo labels from Grounded-SAM.\\n\\n| Box-threshold | # of objects | Data mIoU (%) | Model mIoU (%) |\\n|--------------|-------------|--------------|----------------|\\n| 0.2          | 11,257      | 55.32        | 59.04          |\\n| 0.3          | 5,995       | 65.14        | 66.15          |\\n| 0.4          | 3,890       | 66.71        | 65.30          |\\n| 0.5          | 2,798       | 60.87        | 59.50          |\\n\\nTo evaluate the quality of pseudo labels generated by Grounded-SAM. Noisy pseudo labels cause the data and model mIoU to worsen.\"}"}
{"id": "WPt9HRmMrG", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We correct PASCAL into PASCAL+ utilizing the superpixels of Grounded-SAM. Grounded-SAM (Liu et al., 2023) on the PASCAL dataset, we measure the data and model mIoU while adjusting a hyperparameter. Grounded-SAM operates with two hyperparameters: box-threshold and text-threshold. The text-threshold aims to identify all potential classes with a potential value exceeding the threshold. As we only focus on a specific class per an object, we employ the argmax function on the potential classes. The box-threshold determines the confidence level in the bounding box of the identified object. With a lower box-threshold, the foundation model can detect more objects, as demonstrated in Table 7. However, this often leads to numerous incorrectly labeled objects, resulting in decreased mIoU for both data and model. Yet, the benefit of detecting lots of objects lies in the potential for enhanced performance when correcting the pseudo labels of all detected objects, resulting in model mIoU of 72.59%, 70.90%, and 66.97% for box-thresholds of 0.2, 0.3, and 0.4, respectively.\\n\\nD.2. Similarity Threshold for Label Expansion\\n\\nDuring the label expansion phase detailed in Section 3.4, a challenge can emerge when superpixels contain pixels belonging to various classes, potentially diminishing the dataset's overall quality. To this end, we propose expanding the clean label of a pixel \\\\( x_i \\\\) only to similar pixels within its corresponding superpixel \\\\( s_i \\\\) as follows:\\n\\n\\\\[\\ns_i(x_i, \\\\epsilon) := \\\\{ x \\\\in s_i : \\\\cos f_\\\\theta(x_i), f_\\\\theta(x) \\\\geq \\\\epsilon \\\\},\\n\\\\]\\n\\nwhere the degree of expansion is determined by hyperparameter \\\\( \\\\epsilon \\\\). The more incomplete the superpixel, the larger \\\\( \\\\epsilon \\\\) is required. For our main experiments in Section 4, we set \\\\( \\\\epsilon \\\\) as 0, indicating complete expansion, where \\\\( s_i(x_i, \\\\epsilon) = s_i \\\\).\\n\\nHere, we investigate how the value of \\\\( \\\\epsilon \\\\) in (12) affects results. Since foundation models accurately generate superpixel boundaries in PASCAL, we observe that setting \\\\( \\\\epsilon \\\\) to 0, thereby allowing the corrected pixel label to cover the entire superpixel, yields the best performance, as demonstrated in Table 8.\"}"}
{"id": "WPt9HRmMrG", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Label Correction for Semantic Segmentation with Foundation Models\\n\\nFigure 10: Uncorrectable examples of noisy and corrected labels in PASCAL.\\n\\nWe correct PASCAL into PASCAL+ utilizing the superpixels of Grounded-SAM, however, due to the inherent limitations of superpixels, some failure cases can be observed.\\n\\nD.3. Comparison with Other Diversified Pixel Pool\\n\\nTable 9: Experiments for diversified pixel pools.\\n\\n| Methods   | Data mIoU (%) | Model mIoU (%) |\\n|-----------|---------------|----------------|\\n| PixelPick | 66.88         | 62.59          |\\n| ALC       | 83.60         | 68.71          |\\n\\nTo solve the issue of picking similar pixels, as described in Section 3.3, PixelPick employs an acquisition function to rank all pixels, subsequently uniformly selecting them from the top 5% ranked pixels in each image (Shin et al., 2021). Thus, we contrast our diversified pixel pool based on superpixels with the PixelPick method. For a fair comparison, we incorporate all other techniques, including SIM acquisition equipped with the concept of look-ahead and label expansion. As shown in Table 9, our ALC performs better than PixelPick in terms of both data and model mIoU.\\n\\nD.4. Comparison with Other Acquisitions\\n\\nIn Table 10, our SIM acquisition outperforms other various acquisitions including Entropy, Best-versus-Second-Best (BvSB), and Class-Balanced (ClassBal), employed in active learning, due to the incorporation of the look-ahead concept. We concentrate on adjusting the acquisition function, while simultaneously applying other techniques such as diversified pixel pool and expansion techniques. We correct the labels of 3K pixels selected using various acquisition functions, and expand the labels to their corresponding superpixels.\\n\\nD.5. Class IoU on PASCAL\\n\\nWe provide the rationale of IoU gain in Figure 7a. For a detail, thanks to the corrected PASCAL+, we observe that the IoU values of pottedplant, sofa, chair, and diningtable classes increase. This is related to the number of corrections in Figure 7b, as those class are corrected lots than other classes. However, in case of background and person classes, \\n\\n15\"}"}
{"id": "WPt9HRmMrG", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Label Correction for Semantic Segmentation with Foundation Models\\n\\nFigure 11: Correction that appears to cause negative IoU gains. Here, the colors black, red, purple, green, and pink represent the background, chair, bottle, bicycle, and person classes, respectively.\\n\\nFigure 12: Class IoU on PASCAL. The IoU values of diningtable, pottedplant, sofa, and chair classes are relatively low when trained with PASCAL. We cannot obtain IoU gain as those classes already attain high IoU with PASCAL as depicted in Figure 12.\\n\\nE. Additional Results of PASCAL+\\n\\nE.1. Qualitative Results\\n\\nAdditional qualitative results of corrected labels using our proposed method are depicted in Figure 9. These results demonstrate that our proposed correction method effectively identifies objects overlooked in the original labels.\\n\\nE.2. Uncorrectable Cases\\n\\nFigure 10 presents examples where corrections made by our proposed method are not entirely successful. Specifically, the examples in the first and second rows of Figure 10 illustrate situations where annotators mistakenly assign pixel clicks to the wrong classes. Such errors can occur under limited budgets. In the last row of Figure 10, an area mislabeled as person class is effectively corrected to car class. However, due to the insufficient granularity of the superpixels, small areas remain uncorrected. This limitation can be mitigated by employing more refined superpixels or utilizing improved foundational models.\\n\\nE.3. Negative IoU Gains of PASCAL+\\n\\nFigure 6a represents negative IoU gains for certain classes such as person, bottle, and cow. Here, we provide the rationale for these negative gains. The final IoU gain is determined by the positive and negative impacts of corrections. Although corrections generally aim to reduce noisy labels, yielding positive effects, they can also have negative effects, especially on challenging objects, as shown in Figure 11.\"}"}
{"id": "WPt9HRmMrG", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 13: Segmentation changes through active label correction.\\n\\n(b) The initial pseudo labels obtained from Grounded-SAM contain numerous noisy labels, exemplified by instances like tvmonitor inside the cyan box. (c) In the first round, the object labeled as tvmonitor is corrected to background. Nonetheless, many noisy labels exist within the yellow boxes. (c) In the second round, we rectify all remaining noisy labels. With the help of the proposed look-ahead acquisition function, we prioritize correcting large objects before addressing small ones. Here, the colors black, blue, red, dark red, purple, and pink represent the background, tvmonitor, chair, airplane, bottle, and person classes, respectively.\"}"}
{"id": "WPt9HRmMrG", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Label Correction for Semantic Segmentation with Foundation Models\\n\\n$s'$, consisting of pixels that align with the pseudo dominant label $D_\\\\theta(s)$, as follows:\\n\\n$$s' := \\\\{ x \\\\in s : y_\\\\theta(x) = D_\\\\theta(s) \\\\}.$$ (6)\\n\\nAfter that, we select the pixel that best represents $s'$ for each superpixel based on (4), contributing to the formation of a diverse pixel pool in (3). We highlight that the proposed diversified pixel pool reduces time and memory usage and lessens the redundancy issue in the chosen pixels.\\n\\nRemarks. While various superpixel generation algorithms (Achanta et al., 2012; Van den Bergh et al., 2012) can be used for $S$ in (3), these standard algorithms typically group neighboring pixels based on similar inherent properties like color and maintain nearly uniform sizes. Recent research indicates that semantically considered superpixels from a model are effective for AL in segmentation (Kim et al., 2023a). Therefore, we opt to organize superpixels based on the objects identified by Grounded-SAM.\\n\\n3.4. Look-Ahead Acquisition Function\\n\\nOnce the set of pixels $X_{dt}$ for examination through an acquisition function is established, we select a pixel batch $B_t \\\\subset X_{dt}$ of size $B_t$ to be corrected. In each round $t$, we iteratively select the most informative pixel, guided by the acquisition function $a(x; \\\\theta_{t-1})$:\\n\\n$$x^* := \\\\arg \\\\max_{x \\\\in X_{dt}} a(x; \\\\theta_{t-1}).$$ (7)\\n\\nFor simplicity, we refer to $\\\\theta_{t-1}$ as $\\\\theta$. Recently, Lad & Mueller (2023) propose a confidence in label (CIL), which evaluates the confidence of a given label $y$ for a pixel $x$, using the predictions of the model $\\\\theta$ as follows:\\n\\n$$a_{CIL}(x; \\\\theta) := 1 - f_\\\\theta(y; x).$$ (8)\\n\\nThe underlying assumption is that a pixel is likely mislabeled if the model demonstrates insufficient learning about that pixel's label. However, correcting only a single pixel with each query is not only inefficient but also has minimal impact on the learning process. To enhance the efficiency of pixel-wise query, we introduce a label expansion technique, which involves extending the corrected label of a pixel $x$ into pixels in the same superpixel $s$.\\n\\nAccordingly, we suggest a look-ahead acquisition function that not only assesses the unreliability of a pixel $x$ as described in (8), but also takes into account the effect of label expansion into the superpixel $s$. Here, we rename $x$ to $x_r$ as it serves as a representative pixel for $s$. For a representative pixel $x_r$ of $s$, our acquisition function is defined as follows:\\n\\n$$a_{SIM}(x_r; s, \\\\theta) := \\\\sum_{x \\\\in s} f_\\\\theta(x_r) \\\\cdot f_\\\\theta(x) \\\\left\\\\| f_\\\\theta(x_r) \\\\right\\\\| \\\\left\\\\| f_\\\\theta(x) \\\\right\\\\| a_{CIL}(x; \\\\theta),$$ (9)\\n\\nwhere the cosine similarity between two feature vectors is related to the likelihood of correctly expanding the correct label of pixel $x_r$ to another pixel $x$.\\n\\nWe note that previous acquisitions including CIL in (8) can be transformed easily to its look-ahead counterparts. For instance, the look-ahead CIL (LCIL) acquisition can be defined by adjusting the weight of each pixel from the cosine similarity to the inverse of the superpixel size as:\\n\\n$$a_{LCIL}(x_r; s, \\\\theta) := \\\\frac{1}{|s|} a_{CIL}(x; \\\\theta).$$ (10)\\n\\nFinally, in round $t$, we select the $B_t$ most informative pixels from the diversified pixel pool $X_{dt}$ in order of SIM acquisition to form query batch $B_t$.\\n\\nAfter obtaining the clean labels of selected $B_t$ pixels, we expand them to the associated superpixels. We finally construct the dataset $D_t$ for round $t$ by combining the previous dataset $D_{t-1}$ with the updated annotations. Analogously to the warm-start, we initialize $\\\\theta_t$ to a model pre-trained on ImageNet, minimizing the following CE loss:\\n\\n$$\\\\hat{E}(x,y) \\\\sim D_t \\\\left[ CE(y, f_\\\\theta(x)) \\\\right].$$ (11)\\n\\n4. Experiments\\n\\n4.1. Experimental Setup\\n\\nDatasets. We use three semantic segmentation datasets: Cityscapes (Cordts et al., 2016), PASCAL VOC 2012 (PASCAL) (Everingham et al., 2012), and Kvasir-SEG (Jha et al., 2020). Cityscapes comprises 2,975 training and 500 validation images with 19 classes, while PASCAL consists of 1,464 training and 1,449 validation images with 20 classes. Kvasir-SEG is a medical dataset for polyp segmentation consists of 880 training and 120 validation images with 2 classes.\\n\\nImplementation Details. We adopt DeepLab-v3+ architecture (Chen et al., 2018) with Resnet101 pre-trained on ImageNet (Deng et al., 2009) as our segmentation model. During training, we use the SGD optimizer with a momentum of 0.9 and set a base learning rate of 0.1. We decay the learning rate by polynomial decay with a power of 0.9. For Cityscapes, we resize training images to 768 x 768 and train a model for 30K iterations with a mini-batch size 16. For PASCAL, we resize training images to 513 x 513 and train a model for 30K iterations with a mini-batch size 16. For Kvasir-SEG, we resize training images to 352 x 352 and train a model for 6.3K iterations with a mini-batch size 32. For the initial dataset generated with Grounded-SAM, we use the box threshold of 0.2 for Cityscapes and PASCAL, and 0.05 for Kvasir-SEG.\"}"}
{"id": "WPt9HRmMrG", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Label Correction for Semantic Segmentation with Foundation Models\\n\\nThe number of clicks % of fully supervised mIoU (%)\\n\\n(a) PASCAL\\n\\n(b) Cityscapes\\n\\nFigure 3: Effect of active label correction. ALC shows comparable results on both datasets with much fewer clicks.\\n\\nALC (normalized) reflects the reduced budget of correction queries with normalization by Theorem 3.1.\\n\\nTable 1: User study for different queries.\\n\\n| Query      | Total time (s) | Time per query (s) | Accuracy (%) |\\n|------------|----------------|--------------------|--------------|\\n| C_{cls}   | 126.1 \u00b1 19.8   | 6.31 \u00b1 0.9         | 95.0 \u00b1 3.3   |\\n| C_{cor}   | 95.1 \u00b1 9.0    | 4.76 \u00b1 0.45        | 95.0 \u00b1 4.0   |\\n\\n4.2. Main Experiments\\n\\nBaselines. Our Active Label Correction (ALC) method is compared with the state-of-the-art (SOTA) superpixel-based active learning (AL) methods: Spx (Cai et al., 2021), MerSpx (Kim et al., 2023a), and MulSpx (Hwang et al., 2023). They are chosen for two reasons: (1) Their measure of labeling cost is the same as ours, i.e., the number of label clicks. (2) They are SOTA methods in AL for segmentation.\\n\\nFollowing conventional AL methods (Cai et al., 2021), we highlight the amount of annotation used to achieve 95% performance of the fully supervised baseline, where 95% denotes performance.\\n\\nEvaluation Protocol. Given a limited budget, we identify and fix noisy pixel labels, and expand them to the related superpixels to construct the corrected dataset. Then, we develop a model using the dataset and evaluate its effectiveness with mean Intersection over Union (mIoU). In all experiments, we report the average results from three trials, with graph shading indicating the standard deviation. We access the model not only on the test dataset but also on the training dataset to calculate the quality of the dataset itself.\\n\\nActive Label Correction vs. Active Learning. In Figure 3, we show the effectiveness of our framework, named ALC, compared with current AL methods over various budget levels.\\n\\nFigure 4: Precision and recall comparisons. Our SIM acquisition shows a high recall, indicating it corrects many noisy pixels with limited budgets.\\n\\nTable 2: Quality of corrected datasets.\\n\\n| Acquisition function | Data mIoU (%) | Model mIoU (%) |\\n|----------------------|--------------|---------------|\\n| LCIL                 | 56.59 \u00b1 0.07 | 56.82 \u00b1 0.05  |\\n| SoftMin              | 59.28 \u00b1 0.59 | 58.66 \u00b1 0.89  |\\n| AIoU                 | 59.95 \u00b1 0.57 | 59.04 \u00b1 0.27  |\\n| SIM (ours)           | 83.04 \u00b1 0.62 | 68.72 \u00b1 0.10  |\\n\\nels, represented by the number of clicks, for both PASCAL and Cityscapes datasets. Due to variations in models and hyperparameters used in previous methods, we ensure a fair comparison by evaluating the percentage of fully supervised mIoU, where additional comparisons with absolute mIoU is reported in Appendix C. The results illustrate that our ALC substantially reduces the necessary budgets to achieve 95% target performance. Specifically, ALC achieves 95% of the fully supervised baseline performance with just 6K clicks for PASCAL and 150K clicks for Cityscapes. This is only 30% and 75% of the budget required by the previous SOTA methods, respectively. Even when considering the efficient labeling cost of correction queries in Theorem 3.1, the cost of our proposed method reduces to 68% of its original version, where $p$ in (2) is 0.27 and 0.5 in PASCAL and Cityscapes, respectively. This result is denoted as ALC (normalized) in Figure 3.\\n\\nVerification of Labeling Costs with User Study. In Theorem 3.1, we prove that the labeling cost of the correction query $C_{cor}$ is lower than the classification query $C_{cls}$. In Table 1, we empirically show its effectiveness with a user study conducted by 20 annotators, where they are given 20 queries with $p = 0.5$ scenarios. Theoretically, as $L = 20$ in PASCAL, the cost ratio between the two queries is about 0.62. In Table 1, we observe that $C_{cor}$ requires 0.75 times...\"}"}
{"id": "WPt9HRmMrG", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Label Correction for Semantic Segmentation with Foundation Models\\n\\nThe number of clicks\\n\\nRecall (%)\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK\\n\\nK"}
{"id": "WPt9HRmMrG", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Synergy of proposed components. We conduct an ablation study, when correcting the initial dataset using 5K budgets in PASCAL.\\n\\n| Acquisition Expansion | Data mIoU | Model mIoU | Diversity | Look-ahead |\\n|-----------------------|-----------|------------|-----------|------------|\\n| \u2717 \u2717 \u2717                 | 55.03 \u00b1 0.25 | 56.30 \u00b1 0.56 |           |            |\\n| \u2713 \u2713 \u2713                 | 55.38 \u00b1 0.08 | 56.01 \u00b1 0.58 |           |            |\\n| \u2713 \u2717 \u2713                 | 56.59 \u00b1 0.07 | 56.82 \u00b1 0.05 |           |            |\\n| \u2713 \u2713 \u2713                 | 83.04 \u00b1 0.62 | 68.72 \u00b1 0.10 |           |            |\\n\\nIn subsequent rounds, namely for budgets of 6K and 9K, both R-SAM and G-SAM adhere to the same experimental settings, including the same SIM acquisition function. Another baseline is to use superpixels from SEEDS (Van den Bergh et al., 2012) instead of the ones from SAM, which is denoted as G-SEEDS. We denote R-SEEDS as a baseline combining both random sampling in the initial round and superpixels from SEEDS. As shown in Figure 6, both aspects improve both Data mIoU and Model mIoU. In particular, utilizing the superpixels from SAM shows significant performance improvement.\\n\\nSynergy of Proposed Components. Table 3 quantifies the contribution of each component in our method: (1) the diversified pixel pool (Diversity) in Section 3.3, (2) the look-ahead acquisition (Look-ahead), and (3) the label expansion technique (Expansion) in Section 3.4. The ablation study is conducted by correcting the initial dataset using 5K budgets in PASCAL, and evaluated with both the accuracy of corrected labels (Data mIoU) and the performance of a model trained with them (Model mIoU). The results show that all components improve both Data mIoU and Model mIoU. In particular, the synergy of proposed components is pronounced. Since correcting numerous pixels across various regions simultaneously is significant, omitting even one component results in significant performance degradation.\\n\\nFair Comparison with Baselines. We provide additional experiments and discussions to clarify the advantages of our method called ALC, compared to adopting Grounded-SAM (G-SAM) to Spx baseline. In turn, only our method fully leverages G-SAM mainly thanks to our acquisition function, SIM. Table 4 presents an ablation study on the advantages of G-SAM, which are two-fold: warm-start with initial pseudo-labels and SAM superpixels. The gap between the first and second rows quantifies the advantage of warm-start with G-SAM when using Spx. This is not substantial since the pseudo labels from G-SAM contain considerable noises, as shown in Figure 1b, i.e., Data mIoU 55.32% in PASCAL. In addition, comparing the second and third rows, the advantage of using SAM superpixels for Spx is negligible. The gain of our method in the fourth row is clear. This is mainly thanks to the proposed acquisition function, SIM, with the look-ahead ability. We note that MerSpx (Kim et al., 2023a) based on ClassBal of Spx has no such look-ahead. MulSpx (Hwang et al., 2023) proposes a multi-class query, which requests labeling all classes within a superpixel, making it difficult to conduct a fair comparison.\\n\\n5. PASCAL+ corrected from PASCAL\\n\\nTo demonstrate the practicality of the proposed framework, we apply corrections to the widely-used PASCAL dataset (Everingham et al., 2012), resulting in an enhanced version named PASCAL+ dataset (Section 5.1). Figures 1c and 1d illustrate the change in labels between PASCAL and PASCAL+ datasets, respectively. We demonstrate the enhanced model performance when using PASCAL+ compared to PASCAL and verify the cost-effectiveness of our SIM acquisition function (Section 5.2).\\n\\n5.1. Construction Process\\n\\nWe apply our active label correction to construct the refined version of the PASCAL dataset. We first generate 81K superpixels using Grounded-SAM, where we use 0.1 as the box threshold. Considering that PASCAL has 1,464 images for training and 1,449 for validation, the average number of superpixels per image is around 29. Then we correct the pseudo label of each superpixel by annotating the true label to the corresponding representative pixel and expanding the label to the superpixel. The relabeling tasks are conducted by two annotators, each spending around 60 hours over two weeks. When labels from two annotators are different, the final annotation is determined by discussion. The qualitative result of PASCAL+ compared to PASCAL is illustrated in Figures 1 and 9. Additionally, in Figure 10, we report few failure cases for label correction due to the imperfection of superpixels, budget constraints, and human error.\\n\\n5.2. Analysis of PASCAL+\\n\\nEffect of PASCAL+. In PASCAL+, we make 743 super-pixel label corrections in total, with 375 in the training set and 368 in the validation set. Approximately 0.5% of the 81K superpixels are corrected.\"}"}
{"id": "WPt9HRmMrG", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Label Correction for Semantic Segmentation with Foundation Models\\n\\nHoyoung Kim 1\\nSehyun Hwang 2\\nSuha Kwak 1, 2\\nJungseul Ok 1, 2\\n\\nAbstract\\nTraining and validating models for semantic segmentation require datasets with pixel-wise annotations, which are notoriously labor-intensive. Although useful priors such as foundation models or crowdsourced datasets are available, they are error-prone. We hence propose an effective framework of active label correction (ALC) based on a design of correction query to rectify pseudo labels of pixels, which in turn is more annotator-friendly than the standard one inquiring to classify a pixel directly according to our theoretical analysis and user study. Specifically, leveraging foundation models providing useful zero-shot predictions on pseudo labels and superpixels, our method comprises two key techniques: (i) an annotator-friendly design of correction query with the pseudo labels, and (ii) an acquisition function looking ahead label expansions based on the superpixels. Experimental results on PASCAL, Cityscapes, and Kvasir-SEG datasets demonstrate the effectiveness of our ALC framework, outperforming prior methods for active semantic segmentation and label correction. Notably, utilizing our method, we obtained a revised dataset of PASCAL by rectifying errors in 2.6 million pixels in PASCAL dataset 1.\\n\\n1. Introduction\\nSemantic segmentation has seen remarkable advancements powered by deep neural networks capable of learning from huge datasets with dense annotations for all pixels. However, such pixel-wise annotations are labor-intensive and error-prone. To address or bypass these challenges, various approaches have been studied, including crowdsourcing systems to collect large-scale human annotations (Crowston, 2012), weakly supervised learning methods to train models with image-wise annotations (Ru et al., 2023), and foundation models capable of useful zero-shot prediction on superpixels (Kirillov et al., 2023) or even semantic segmentation (Liu et al., 2023). However, those are unreliable to train and more importantly validate models for exquisite or domain-specific prediction. For instance, despite recent advances, the zero-shot prediction with foundation models (Kirillov et al., 2023; Liu et al., 2023) is considerably erroneous as demonstrated in Table 7. This can be more problematic when the semantic segmentation requiring expertise such as medical knowledge (Ma et al., 2024).\\n\\nHence, we consider the problem of active label correction (ALC) to construct a reliable pixel-wise dataset from an unreliable or unlabeled dataset with a minimum cost of user intervention. To this end, we propose an ALC framework which leverages foundation models and correction queries. Our correction query is designed to rectify the pseudo labels of pixels, only if these pseudo labels are incorrect. Unlike the standard classification query that directly requests a specific class (Cai et al., 2021; Kim et al., 2023a), our correction query allows annotators to skip labeling if the pseudo labels are correct, making it more annotator-friendly. Borrowing the information-theoretic annotation cost (Hu et al., 2020), we prove that our correction query is less costly than the classification query. Moreover, our user study in Section 4.2 reveals that the correction query is faster to complete than the classification query in practice.\\n\\nSpecifically, we leverage useful zero-shot predictions on pseudo labels and superpixels from foundation models. These pseudo labels are employed in our correction query to designate pixel labels. They also allow us to warm-start, avoiding the typical cold-start problem that comes from the absence of a reliable way to evaluate data at the beginning of active learning (Mahmood et al., 2021; Chen et al., 2023). Furthermore, we fully enjoy the decent superpixels to solve the challenges of pixel-wise queries. Although pixel-wise queries can generate a flawless dataset, they require substantial time and memory to examine each pixel and lead to redundancy in the pixels chosen (Shin et al., 2021). To address the problems, we devise superpixel-aware strategies across our entire framework. Initially, we build a diversified pixel pool consisting of partial key pixels representing...\"}"}
{"id": "WPt9HRmMrG", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Label Correction for Semantic Segmentation with Foundation Models\\n\\nFigure 1: Examples of noisy and corrected labels in PASCAL.\\n\\n(a, b) Initial pseudo labels are generated by applying Grounded-SAM (G-SAM) to unlabeled images. As depicted by the yellow boxes, noisy pseudo labels result in a decline in performance, as shown in Table 7. (c) PASCAL also contains noisy labels in cyan boxes. (d) By employing the superpixels from G-SAM, we construct a corrected version of PASCAL, called PASCAL+. For instance, in the first row, we correct the object labeled as person to tvmonitor, and in the second row, the object labeled as background to tvmonitor. Here, the colors black, blue, red, green, and pink represent the background, tvmonitor, chair, sofa, and person classes, respectively.\\n\\nEach image. As superpixels cluster pixels with similar features (Van den Bergh et al., 2012), we choose one representative pixel per superpixel and add it to our pixel pool. To solve the inefficiency of correcting each pixel individually per query, we extend the corrections from individual pixels to the entire superpixels they belong to. Accordingly, we propose a look-ahead acquisition function, which anticipates the benefits of label expansion beforehand.\\n\\nThe proposed framework is notably cost-efficient in constructing clean segmentation datasets. We evaluate it by constructing new segmentation datasets from the initial pseudo labels given by foundation models in different fields, including the medical domain. Our ALC framework outperforms prior methods for active semantic segmentation and label correction over a range of budgets. In particular, we highlight its practical application by enhancing the popular PASCAL dataset (Everingham et al., 2012). We call our corrected dataset PASCAL+, which can be widely used in the literature of semantic segmentation.\\n\\nOur main contributions are summarized as follows:\\n\\n\u2022 We provide theoretical and empirical justifications on the efficacy of the correction query, compared to the classification query (Section 3.2 and 4.2).\\n\\n\u2022 We propose an active label correction framework, leveraging the correction query and foundation models, where the look-ahead acquisition function enables selecting informative and diverse pixels to be corrected (Section 3.3 and 3.4).\\n\\n\u2022 To achieve comparable performance with SOTA active semantic segmentation methods, we only use 33% to 50% of budgets on various datasets (Section 4.2).\\n\\n\u2022 Using the proposed framework, we correct 2.6 million pixel labels in PASCAL and provide a revised version, called PASCAL+ (Section 5.2).\\n\\n2. Related Work\\n\\nActive Learning for Segmentation.\\n\\nActive Learning (AL) (Kim et al., 2023b; Saran et al., 2023; Yang et al., 2023) aims at increasing labeling efficiency by selectively annotating informative subsets of data. In semantic segmentation, previous work focuses on two aspects: the design of labeling units and acquisition functions. In terms of labeling unit design, classical approaches explore image-based (Yang et al., 2017; Sinha et al., 2019) and patch-based (Mackowiak et al., 2018; Casanova et al., 2019) selection. Recently, superpixel-based approaches (Siddiqui et al., 2020; Cai et al., 2021; Hwang et al., 2023; Kim et al., 2023a), are gaining attention as they only require one click for labeling each region. In terms of acquisition functions, they generally focus on selecting uncertain regions, measured with entropy (Mackowiak et al., 2018; Kasarla et al., 2019), the gap between the top-1 and the top-2 predictions (Joshi et al., 2009; Wang et al., 2016; Cai et al., 2021; Hwang et al., 2023; Kim et al., 2023a). While conventional AL methods collect labels from scratch, the proposed method starts from the initial pseudo labels from foundation models, correcting erroneous labels.\"}"}
{"id": "WPt9HRmMrG", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Label Correction for Semantic Segmentation with Foundation Models\\n\\nNoisy Label Detection. The studies in noisy label detection (NLD) aim to identify incorrect labels efficiently by selecting error-like samples. In computer vision, methods for robust training toward label noise often include NLD components (Natarajan et al., 2013; Xiao et al., 2015; Patrini et al., 2017; Han et al., 2018; Ren et al., 2018; Song et al., 2022), and recently, there is an increase in studies focusing solely on NLD (M\u00fcller & Markert, 2019; Northcutt et al., 2021b). NLD methods for semantic segmentation aggregate pixel-wise error scores into labeling units, like an image or superpixel. Lad & Mueller (2023) aggregate per-pixel error scores from Confident Learning (Northcutt et al., 2021a) into per-image scores, while Rottmann & Reese (2023) average error scores from locally connected components sharing the same pseudo label. Recently, the Active Label Correction (ALC) (Bernhardt et al., 2022; Kim, 2022) methods identify noisy labels and correct them in a classification task. Our work is the first ALC method for semantic segmentation, correcting pixel labels and expanding them to their corresponding superpixels.\\n\\nEfficient Query Design. Designing a practical and cost-effective annotation query is crucial, as it directly impacts annotation budgets. In semantic segmentation, various approaches have been explored, including classification queries asking for a specific class (Cai et al., 2021; Kim et al., 2023a), one-bit queries requesting yes or no responses (Hu et al., 2020), and multi-class queries obtaining all classes in a superpixel (Hwang et al., 2023). Recently, there have been studies on efficiently constructing datasets using foundation models. For instance, Wang et al. (2023) leverages these models for automated labeling in remote sensing imagery, and Qu et al. (2023) focuses on building large medical datasets with them. However, its query form is stagnant in previous query types. By employing the initial pseudo labels from foundation models, we suggest correction queries that only request the correct label when the given pseudo label is incorrect.\\n\\n3. Active Label Correction Framework\\n\\nGiven an initial noisy dataset \\\\( D_0 \\\\), we consider an active label correction (ALC) scenario operating with pixel-wise labeling. Each query to an oracle annotator requests the accurate label \\\\( y \\\\in C := \\\\{1, 2, \\\\ldots, C\\\\} \\\\) for an associated pixel \\\\( x \\\\). In contrast to active learning (AL), which commences with an unlabeled image set, ALC focuses on progressively refining a labeled dataset \\\\( D_0 \\\\) which may include noisy labels.\\n\\nFor each round \\\\( t \\\\), we issue a batch \\\\( B_t \\\\) of queries from a pixel pool \\\\( X_t \\\\) and train a model \\\\( \\\\theta_t \\\\) with the corrected annotations obtained so far.\\n\\nIn the following, we first prepare an initial dataset for correction (Section 3.1). After that, we present a correction query that requests for rectifying pseudo labels of pixels (Section 3.2). To fully enjoy the corrections, we introduce a look-ahead acquisition function, which selects from a diversified pixel pool (Section 3.3), considering the effect of label expansion (Section 3.4). The overall procedure is summarized in Algorithm 1.\\n\\nAlgorithm 1\\n\\n```\\nRequire: Batch size \\\\( B \\\\), and final round \\\\( T \\\\).\\n1: Prepare initial dataset \\\\( D_0 \\\\) requiring label correction\\n2: Obtain model \\\\( \\\\theta_0 \\\\) training with \\\\( D_0 \\\\) via (1)\\n3: for \\\\( t = 1, 2, \\\\ldots, T \\\\) do\\n4: Construct diversified pixel pool \\\\( X_d^t \\\\) via (4)\\n5: Correct labels of selected \\\\( B \\\\) pixels \\\\( B_t \\\\subset X_d^t \\\\) via (9)\\n6: Expand corrected labels to corresponding superpixels\\n7: Obtain model \\\\( \\\\theta_t \\\\) training with corrected \\\\( D_t \\\\) via (11)\\n8: end for\\n9: return \\\\( D_T \\\\) and \\\\( \\\\theta_T \\\\)\\n```\\n\\n3.1. Initial Dataset Preparation\\n\\nFor ALC, an initial segmentation dataset is essential, and we can start with well-known datasets like Cityscapes (Cordts et al., 2016) or PASCAL VOC (PASCAL) (Everingham et al., 2012). However, the presence of labeled datasets may be impractical in many domains. Employing AL is one method for preparing labeled datasets. However, AL typically builds datasets through random pixel (Shin et al., 2021) or superpixel labeling (Cai et al., 2021) leading to lots of budgets and rounds, as it starts from unlabeled images, commonly known as the cold-start problem (Mahmood et al., 2021). Away from conventional AL methods, we utilize recent foundation models to construct segmentation datasets. Recently, foundation models for zero-shot segmentation have been emerged. For example, Grounded-SAM, a fusion of Grounding DINO (Liu et al., 2023) and Segment Anything Model (Kirillov et al., 2023) is capable of detecting and segmenting objects based on text prompts. Each class is identified with its own text prompt, and we can obtain the initial pseudo labels by using a series of \\\\( |C| \\\\) text prompts, one for each class. We solve the problem of multi-classes in object detection by giving each object the most likely class as a pseudo-label. Figures 1a and 1b display examples of the unlabeled images in PASCAL and corresponding initial pseudo labels generated by Grounded-SAM.\\n\\nWarm-start. In contrast to the cold-start problem in AL, our ALC benefits from warm-start thanks to the initial labels provided by foundation models. In Appendix A, detailed descriptions of text prompts for warm-start are provided. To obtain \\\\( \\\\theta_0 \\\\), we initialize \\\\( \\\\theta \\\\) to a model pre-trained on ImageNet (Deng et al., 2009). We then train it to reduce the following cross-entropy (CE) loss:\\n\\n\\\\[\\n\\\\hat{E}(x, y) \\\\sim D_0 \\\\left[ CE(y, f_{\\\\theta}(x)) \\\\right],\\n\\\\]\"}"}
{"id": "WPt9HRmMrG", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Label Correction for Semantic Segmentation with Foundation Models\\n\\nIs this pixel a boat? Give the correct label only if the pseudo label is incorrect.\\n\\nFigure 2: An example of correction query. Correction query presents an instruction requesting a label for a representative pixel (green star), an image displaying an object within a bounding box (green rectangle), and possible class options.\\n\\nwhere $f_\\\\theta(x) \\\\in \\\\mathbb{R}^{\\\\left|\\\\mathcal{C}\\\\right|}$ represents the estimated class probability for pixel $x$ by the model $\\\\theta$. Here, the difference lies in $D_0$: AL uses only partial $y$, while ALC can access all $y$ for each pixel $x$. However, compared to ground-truth in Figure 1c, the initial pseudo-labels in Figure 1b contain noisy labels. This results in negative impacts on the model\u2019s performance, as shown in Table 7. Therefore, active label correction is essential for rectifying these noisy labels.\\n\\n3.2. Correction Query\\n\\nOnce we prepare the initial dataset for correction, we use our correction query to rectify the pseudo labels of pixels. As the number of classes increases, the classification query asking for the precise label of a pixel can become more time-consuming (Zhang et al., 2022). In contrast, our correction query lowers the overall cost by reducing the number of classification queries needed, allowing annotators to bypass labeling when the pseudo label is already correct. Specifically, we use the instruction with a pseudo label on a pixel, written as follows:\\n\\nGive the correct label only if the pseudo label is incorrect.\\n\\nFigure 2 and Appendix B provide detailed descriptions of our correction query. In the following, we information-theoretically compare the expected costs of classification and correction queries, denoted by $C_{cls}$ and $C_{cor}$, respectively.\\n\\nTheorem 3.1. Assume the information-theoretic annotation cost (Hu et al., 2020) of selecting one out of $L$ possible options to be $\\\\log_2 L$. Let $L \\\\geq 2$ be the number of classes, and $p$ be the probability that the pseudo label is correct.\\n\\nThen, $C_{cls}(L) = \\\\log_2 L$ and $C_{cor}(L, p) = p + (1 - p) \\\\log_2 L$.\\n\\nThus, for any $p \\\\in [0, 1]$ and $L \\\\geq 2$,\\n\\n$$1 - \\\\frac{C_{cor}(L, p)}{C_{cls}(L)} = \\\\frac{p}{1 - \\\\log_2 L} \\\\geq 0.$$ (2)\\n\\nProof. The correction query can be interpreted as a binary question if the pseudo label is correct, and a $L$-ary one otherwise. Recalling the definition of $p$ and $C_{cls}(L) = \\\\log_2 L$, we have $C_{cor}(L, p) = p \\\\log_2 2 + (1 - p) \\\\log_2 L$.\\n\\nThe costs of both correction and classification queries are the same if $L = 2$. Indeed, those are logically identical when $L = 2$. In (2), the cost-saving rate using the correction query instead on the classification one is computed as $1 - \\\\frac{C_{cor}(L, p)}{C_{cls}(L)} = \\\\frac{p}{1 - \\\\log_2 L}$, which is increasing in $p$ and $L$.\\n\\nHence, using the correction query is particularly beneficial when the number of classes is large or the pseudo labels can be obtained accurately. In addition, a user study on correction queries experimentally confirms their practical effectiveness in Section 4.2.\\n\\n3.3. Diversified Pixel Pool\\n\\nEmploying pixel-wise queries is instrumental in constructing error-free segmentation datasets. However, examining each pixel with an acquisition function requires substantial time and memory. Furthermore, as adjacent pixels often share similar acquisition values, there exists a risk of lacking diversity in the selected pixels, i.e., pixels in a certain area of the image with high acquisition values may be picked simultaneously. To tackle these challenges at once, we propose a diversified pixel pool $X_d$, which is a subset of the total pixel set $X$, as follows:\\n\\n$$X_d := \\\\{x_1, x_2, \\\\ldots, x_\\\\left|\\\\mathcal{S}\\\\right|\\\\},$$ (3)\\n\\nwhere each $x_i$ represents a key pixel from the superpixel $s_i$ within the set of superpixels $\\\\mathcal{S}$.\\n\\nSpecifically, starting with a model $\\\\theta_{t-1}$ trained on the dataset $D_{t-1}$ from the previous round, we construct a diversified pixel pool $X_{d t} := \\\\{x_{t1}, x_{t2}, \\\\ldots, x_{t\\\\left|\\\\mathcal{S}\\\\right|}\\\\}$ for the current round $t$. For ease of explanation, we refer to $\\\\theta_{t-1}$ simply as $\\\\theta$, $x_{ti}$ as $x_i$ and $X_{d t}$ as $X_d$. We select a representative pixel $x_i$ from each superpixel $s_i$ based on the highest cosine similarity as:\\n\\n$$x_i := \\\\arg \\\\max_{x \\\\in s_i} \\\\frac{f_\\\\theta(x) \\\\cdot f_\\\\theta(s_i')}{\\\\|f_\\\\theta(x)\\\\| \\\\|f_\\\\theta(s_i')\\\\|},$$ (4)\\n\\nwhere $f_\\\\theta(s_i) := \\\\mathbb{P}_{x \\\\in s_i} f_\\\\theta(x) \\\\left|\\\\{x: x \\\\in s_i\\\\}\\\\right.$ represents the averaged class prediction for superpixel $s_i$. To address the flaws in superpixels and ensure more uniformity of pixel labels within them, we employ a subset $s_i'$ rather than the complete set $s_i$. We start by defining the pseudo dominant label $D_\\\\theta(s_i)$, which serves as the representative label for superpixel $s_i$ according to model $\\\\theta$, as follows:\\n\\n$$D_\\\\theta(s_i) := \\\\arg \\\\max_{c \\\\in \\\\mathcal{C}} \\\\left|\\\\{x \\\\in s_i: y_\\\\theta(x) = c\\\\}\\\\right|,$$ (5)\\n\\nwhere $y_\\\\theta(x) := \\\\arg \\\\max_{c \\\\in \\\\mathcal{C}} f_\\\\theta(c; x)$ is the estimated label for pixel $x$ using model $\\\\theta$. Subsequently, we form the subset $s_i'$.\"}"}
{"id": "WPt9HRmMrG", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Label Correction for Semantic Segmentation with Foundation Models\\n\\nFigure 7: PASCAL+ statistics.\\n\\n(a) The IoU gain is calculated by averaging the improvements in the train and valid datasets in Table 5. The orange line (---) denotes the average gain. (b) Certain classes are corrected a lot. Notably, the pottedplant, sofa, chair, and diningtable classes get many corrections, leading to a noticeable increase in IoU gain.\\n\\nTable 5: Effect of PASCAL+.\\n\\n|       | Train | Valid | Model |       |\\n|-------|-------|-------|-------|-------|\\n| P     | 99.1  | 75.36 |       | 100.0 |\\n| P+    | 100.0 | 75.78 |       | 100.0 |\\n| P     | 99.1  | 76.18 |       | 100.0 |\\n| P+    | 100.0 | 76.42 |       | 100.0 |\\n\\nPixel labels, equivalent to 2.6 million pixels, are altered, resulting in a 0.9% improvement in the mean Intersection over Union (mIoU) for the training set, as shown in Table 5. Regardless of whether the valid set is PASCAL or PASCAL+, corrections to the training data enhance the mIoU by around 0.3%. In particular, Figure 7a represents that IoU scores for the pottedplant and sofa classes are increased by more than 2%. This trend is related to the distribution of the corrected classes in Figure 7b. Excepting the background and person classes, which already achieve high IoU scores with PASCAL in Figure 12, the IoU scores tend to improve in line with the number of corrections applied to classes that initially have more errors. PASCAL+ not only enhances the reliability of segmentation model evaluations but also has the potential to reduce both false negatives and false positives in the literature of detecting noisy labels for segmentation tasks, thereby contributing to more reliable and precise outcomes in this field.\\n\\nVarious Acquisitions for PASCAL+.\\n\\nSince it is possible to access both the noisy PASCAL and clean PASCAL+ datasets at the same time, we analyze which acquisition function is effective in real-world. Table 6 indicates that our SIM acquisition achieves nearly 100% Data mIoU, i.e., almost similar to PASCAL+, with selecting 10K pixels for correction. As the training dataset's quality improves, there is a corresponding slight increase in model performance.\\n\\nTable 6: Performance of corrected dataset.\\n\\n| Acquisition function | Data mIoU (%) | Model mIoU (%) |\\n|----------------------|--------------|----------------|\\n| LCIL                 | 99.16 \u00b1 0.00 | 75.68 \u00b1 0.25   |\\n| SoftMin              | 99.38 \u00b1 0.01 | 75.76 \u00b1 0.23   |\\n| AIoU                 | 99.28 \u00b1 0.02 | 75.61 \u00b1 0.22   |\\n| SIM (ours)           | 99.78 \u00b1 0.11 | 75.87 \u00b1 0.22   |\\n\\n6. Conclusion\\n\\nIn this work, we propose a framework for active label correction in semantic segmentation operating with foundation models. Our framework includes cost-efficient correction queries, which are verified theoretically and empirically, that ask for a pixel label to be corrected if needed. We fully enjoy the benefits of foundation models, namely initial pseudo-labels and decent superpixels, resulting in significant budget reduction across various datasets in different domains. In addition, we demonstrate the practicality of our framework by constructing PASCAL+, a corrected version of the PASCAL dataset.\\n\\nLimitations.\\n\\nOur framework depends on foundation models, particularly Grounded-SAM (Liu et al., 2023), and shares the same inherent limitations as these models, like generating incomplete superpixels for minor domains. However, we demonstrate the effectiveness of our framework in the medical field, and we expect these issues to be resolved as foundation models continue to improve over time.\\n\\nAcknowledgements.\\n\\nThis work was partly supported by the IITP grants and the NRF grants funded by Ministry of Science and ICT, Korea (No.RS-2019-II191906, Artificial Intelligence Graduate School Program (POSTECH); No.RS-2021-II212068, Artificial Intelligence Innovation Hub; No.RS-2023-00217286; No.RS-2022-II220926).\"}"}
{"id": "WPt9HRmMrG", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.\"}"}
{"id": "WPt9HRmMrG", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Label Correction for Semantic Segmentation with Foundation Models\\n\\nKim, Y.-Y., Cho, Y., Jang, J., Na, B., Kim, Y., Song, K., Kang, W., and Moon, I.-C.\\n\\nSaal: sharpness-aware active learning. In Proc. International Conference on Machine Learning (ICML), pp. 16424\u201316440. PMLR, 2023b.\\n\\nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollar, P., and Girshick, R.\\n\\nSegment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 4015\u20134026, October 2023.\\n\\nLad, V. and Mueller, J.\\n\\nEstimating label quality and errors in semantic segmentation data via any model. In ICML Workshop on Data-centric Machine Learning Research, 2023.\\n\\nLiu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al.\\n\\nGrounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.\\n\\nMa, J., He, Y., Li, F., Han, L., You, C., and Wang, B.\\n\\nSegment anything in medical images. Nature Communications, 15(1):654, 2024.\\n\\nMackowiak, R., Lenz, P., Ghori, O., Diego, F., Lange, O., and Rother, C.\\n\\nCereals-cost-effective region-based active learning for semantic segmentation. In BMVC, 2018.\\n\\nMahmood, R., Fidler, S., and Law, M. T.\\n\\nLow-budget active learning via wasserstein distance: An integer programming approach. In International Conference on Learning Representations, 2021.\\n\\nM\u00fcller, N. M. and Markert, K.\\n\\nIdentifying mislabeled instances in classification datasets. In 2019 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138. IEEE, 2019.\\n\\nNatarajan, N., Dhillon, I. S., Ravikumar, P. K., and Tewari, A.\\n\\nLearning with noisy labels. Advances in neural information processing systems, 26, 2013.\\n\\nNorthcutt, C., Jiang, L., and Chuang, I.\\n\\nConfident learning: Estimating uncertainty in dataset labels. Journal of Artificial Intelligence Research, 70:1373\u20131411, 2021a.\\n\\nNorthcutt, C. G., Athalye, A., and Mueller, J. P.\\n\\nPervasive label errors in test sets destabilize machine learning benchmarks. In Proceedings of the 35th Conference on Neural Information Processing Systems Track on Datasets and Benchmarks, December 2021b.\\n\\nPatrini, G., Rozza, A., Krishna Menon, A., Nock, R., and Qu, L.\\n\\nMaking deep neural networks robust to label noise: A loss correction approach. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1944\u20131952, 2017.\\n\\nQu, C., Zhang, T., Qiao, H., Liu, J., Tang, Y., Yuille, A., and Zhou, Z.\\n\\nAbdomenatlas-8k: Annotating 8,000 ct volumes for multi-organ segmentation in three weeks. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.\\n\\nRen, M., Zeng, W., Yang, B., and Urtasun, R.\\n\\nLearning to reweight examples for robust deep learning. In International conference on machine learning, pp. 4334\u20134343. PMLR, 2018.\\n\\nRottmann, M. and Reese, M.\\n\\nAutomated detection of label errors in semantic segmentation datasets via deep learning and uncertainty quantification. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3214\u20133223, 2023.\\n\\nRu, L., Zheng, H., Zhan, Y., and Du, B.\\n\\nToken contrast for weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3093\u20133102, June 2023.\\n\\nSaran, A., Yousefi, S., Krishnamurthy, A., Langford, J., and Ash, J. T.\\n\\nStreaming active learning with deep neural networks. In Proc. International Conference on Machine Learning (ICML), pp. 30005\u201330021. PMLR, 2023.\\n\\nShin, G., Xie, W., and Albanie, S.\\n\\nAll you need are a few pixels: semantic segmentation with pixelpick. In Proc. IEEE/CVF International Conference on Computer Vision (ICCV), 2021.\\n\\nSiddiqui, Y., Valentin, J., and Nie\u00dfner, M.\\n\\nViewal: Active learning with viewpoint entropy for semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9433\u20139443, 2020.\\n\\nSinha, S., Ebrahimi, S., and Darrell, T.\\n\\nVariational adversarial active learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5972\u20135981, 2019.\\n\\nSmailagic, A., Costa, P., Noh, H. Y., Walawalkar, D., Khan-delwal, K., Galdran, A., Mirshekari, M., Fagert, J., Xu, S., Zhang, P., et al.\\n\\nMedal: Accurate and robust deep active learning for medical image analysis. In 2018 17th IEEE international conference on machine learning and applications (ICMLA), pp. 481\u2013488. IEEE, 2018.\\n\\nSong, H., Kim, M., Park, D., Shin, Y., and Lee, J.-G.\\n\\nLearning from noisy labels with deep neural networks: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.\"}"}
{"id": "WPt9HRmMrG", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Label Correction for Semantic Segmentation with Foundation Models\\n\\nVan den Bergh, M., Boix, X., Roig, G., de Capitani, B., and Van Gool, L. Seeds: Superpixels extracted via energy-driven sampling. In Computer Vision\u2013ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VII, pp. 13\u201326. Springer, 2012.\\n\\nWang, D., Zhang, J., Du, B., Xu, M., Liu, L., Tao, D., and Zhang, L. Samrs: Scaling-up remote sensing segmentation dataset with segment anything model. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.\\n\\nWang, K., Zhang, D., Li, Y., Zhang, R., and Lin, L. Cost-effective active learning for deep image classification. IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2016.\\n\\nWu, X., Chen, C., Zhong, M., and Wang, J. Hal: Hybrid active learning for efficient labeling in medical domain. Neurocomputing, 456:563\u2013572, 2021.\\n\\nXiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X. Learning from massive noisy labeled data for image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2691\u20132699, 2015.\\n\\nYang, J., Wang, H., Wu, S., Chen, G., and Zhao, J. Towards controlled data augmentations for active learning. In Proc. International Conference on Machine Learning (ICML), pp. 39524\u201339542. PMLR, 2023.\\n\\nYang, L., Zhang, Y., Chen, J., Zhang, S., and Chen, D. Z. Suggestive annotation: A deep active learning framework for biomedical image segmentation. In International conference on medical image computing and computer-assisted intervention, pp. 399\u2013407. Springer, 2017.\\n\\nZhang, Y., Zhang, X., Xie, L., Li, J., Qiu, R. C., Hu, H., and Tian, Q. One-bit active query with contrastive pairs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9697\u20139705, 2022.\"}"}
