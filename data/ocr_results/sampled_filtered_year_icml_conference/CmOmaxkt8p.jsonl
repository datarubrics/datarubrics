{"id": "CmOmaxkt8p", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? \\n\\nNegotiation Platform and Analysis\\n\\nInstructions; for example, models sometimes propose to exchange a certain amount of Xs for another amount of Xs.\\n\\nWe found evidence that agents are not very good with mathematical operations, and we suspect they are better with numbers they have seen a lot during training. We found that Claude-2, Claude-2.1, and GPT-3.5 are worse instruction followers than GPT-4: some of our games break because some of these models the wrong tags.\\n\\nExploring language models' beliefs in the ultimatum game, we noticed that\u2014although this is a rare event\u2014they can figure out they are playing a game that is similar to the ultimatum game from the rules or prompts. This makes us wonder if they use their knowledge of how humans usually play these games, like accepting a split around 70-30, instead of making decisions completely logically.\\n\\nC. Interesting Replies\\n\\nWe show some examples of reasoning and social behavior messages. Green color shows good reasoning or behaviors. Red shows errors.\\n\\nC.1. Reasoning in Standard Game\\n\\n**GPT-4**\\nThe other player is seeking more information, which I can choose to disclose to some extent to build trust and encourage a trade. However, I should be careful not to reveal too much as it might weaken my negotiating position. I\u2019ll share that I have a decent amount of resource X and am looking to increase my overall resources, without specifying the exact numbers.\\n\\n**GPT-4-Turbo**\\nPlayer RED\u2019s latest offer is 43.5 ZUP, which is a non-integer value and goes against the rule that we can only trade integer amounts. Besides, there\u2019s an opportunity to round down to the nearest integer to secure an even better deal. I will counter with a rounded integer that is favorable to me, which is still within a reasonable range for both parties.\\n\\n**GPT-4**\\nThe current proposal from the other player would leave me with 11 X and 18 Y, a total of 29 resources, which is an improvement from my starting 30 resources.\\n\\nC.2. Social Behavior\\n\\n**GPT-4-Turbo - Cunning**\\nCunning replies with hostility to a 50-50 split proposal. Eventually, the Cunning model was able to get $65 from the other player.\\n\\n**GPT-4-Turbo - Cunning - Message**\\nFifty? That\u2019s barely an improvement. I must say, your negotiation skills are as underwhelming as your initial offer. Come on, impress me! Show me that you can recognize true talent when you see it. I demand at least $70 to grace you with my acceptance.\\n\\nInstead, this is the reasoning provided by GPT-4-Turbo without social initialization.\\n\\n**GPT-4-Turbo - No Social Behavior - Reasoning**\\nBeing offered half of Player RED\u2019s dollars without having to give anything in return is a very generous initial offer. Accepting this offer would give me a significant resource boost without any downside. It\u2019s a rational choice to accept this trade as it\u2019s unlikely that Player RED would offer a significantly better deal in subsequent moves, especially considering that by the fourth move, I must accept or reject without the power to negotiate further.\\n\\nD. Design Choices\\n\\nIn creating the platform we had to make some design choices that affect how the game develops and how the agents interact with each other.\\n\\nD.1. Game\\n\\nWe specifically developed a platform that can be easily extended and adapted. The degree of changes required to implement a new game depends mostly on how many things the user wants to change about the game itself. If the user wants to implement a new game, that ends when one of the agents accepts (any general negotiation), the user only needs to provide a new prompt with the rules of the game. More complex changes, like switching from the XML-like language to a JSON-based language, would require subclassing the parser to extend its functionality, but we do not expect the changes to be hard to implement as the parsing functionality is isolated from the rest.\\n\\nD.2. Game Unfolding\\n\\nWe believe it is important to keep the agent\u2019s conversation as in-domain as possible. Thus, we keep the order of the conversation roles fixed for all agents: system, user, assistant, user, ... However, it is impossible to build this exact flow with two chatting agents, since one agent has to influence the reply of the other agent (one has to be the user of the other agent, but both agents need to start by replying to a user message). We solve this issue by making the game unfold as described in Figure 13. Following this structure, both agents will be able to keep the structure system, user, assistant, user, ... consistent for the entirety of the game.\\n\\nD.3. Messaging, Logging and State Saving\\n\\nWe sometimes found that agents are prone to hallucinating resources and goals that are not defined in their prompt. To reduce this we force them to generate text regarding their resources and their goal at every turn. This text is filtered out and not given in input to the other agent, otherwise it\"}"}
{"id": "CmOmaxkt8p", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? \\n\\nYou are a negotiator.\\nYou are Player 1, start the negotiation.\\n\\nUser\\nSystem Prompt\\nHello Player 2, do you want this?\\n\\nAssistant\\nHello Player 2, do you want this?\\n\\nUser\\nYou are a negotiator.\\nYou are Player 2.\\nSystem Prompt\\n\u2026.\\n\\nAgent 1\\nParser\\nproduces message : string\\nMessage :\\nAgentMessage\\ncreates\\nAgent 2\\n.to_string()\\nState\\n\\nFigure 13: Detail on how the game between the two agents unfolds: Agent 1 is prompted by a user message we use to initialize the game. This user message assigns the role to Agent 1. Instead, Agent 2's role is added directly to the system prompt. The assistant message generated by agent 1 is then fed in input as a user message to agent 2.\\n\\nAgent 1\\nParser\\nproduces message : string\\nMessage :\\nAgentMessage\\ncreates\\nAgent 2\\n.to_string()\\nState\\n\\nFigure 14: Information flow inside the game could reveal too much information. Communication between the agents is mediated by a parser. The parser creates a structured message object that is used to keep or filter out the information that the other agent should see (see Figure 14 for reference). All information is tracked and saved independently of being visible or not to the other agent.\\n\\nEach game is a Python object that is stored on disk. We implement JSON Encoders and Decoders that can ingest all the game objects and serialize them. All the data is stored in a series of state dictionaries. This allows us to reload the game after it is finished and potentially keep running it for more iterations or edit a portion and see what happens after we make the edit.\\n\\nD.4. Game Objects\\n\\nMore details about the game objects can be explored on the repository. Here we want to provide a summary of how we built the platform objects and how these can be used. Each agent has access to a set of resources that are characterized by a Resource object. Each trade message is parsed into a Trade object. A Trade object is composed of two Resource objects: a set of resources coming from the first agent and going to the second and a set of resources coming from the second agent and going to the first. The Trade object offers internal methods to evaluate the consistency of the trade and to assess the utility of a trade for the players.\\n\\nAgent objects keep track only of their conversations, otherwise, they are essentially stateless. This allows us to use them in different settings without having to create specific implementations. A possible future extension would include removing the conversations from the agents themselves to make the objects completely stateless and easier to deal with. Having a stateless object would allow us to avoid saving the agents and just keep track of the conversation in the state dictionary. Nonetheless, agents can be still made stateful by simply subclassing them and adding parameters.\\n\\nFigure 15: Dollars left to the buyers when we scale all resources.\\n\\nE. Additional Results\\n\\nE.1. Confidence Intervals for Heatmaps\\n\\nWe add the confidence intervals to the results described in the heat maps of the main text. For the resource exchange game see Table 4, for the ultimatum game, see Table 5. The tables' results have a one-to-one mapping with the results in the heatmaps.\\n\\n| GPT-4 Claude2 | Claude2 | GPT3.5 |\\n|--------------|--------|--------|\\n| 1.38\u00b10.8     | 0.57\u00b10.8 | 0.93\u00b10.5 |\\n| 2.45\u00b11.0     | 1.65\u00b10.7 | 1.88\u00b10.6 |\\n| 1.85\u00b11.0     | 1.53\u00b10.6 | 2.08\u00b10.7 |\\n| 0.82\u00b11.2     | 0.32\u00b10.9 | -0.48\u00b10.7 |\\n\\nTable 4: Payoffs with confidence intervals for the Resource Exchange.\\n\\nSeller and Buyer with Contrasting Objectives\\n\\nWe explore a setting in which the Seller values the object 60 and the buyer is only willing to pay 40. Results are available in Figure 16. We report the difference between the willingness to buy of the buyer and the actual sale price of the object. Negative values suggest that the object is sold at more than 40 every time. The strong contrast in this situation makes the buyer have to spend more than 40 to buy the object.\\n\\nWe report 1.96*standard error of the mean.\"}"}
{"id": "CmOmaxkt8p", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? Negotiation Platform and Analysis\\n\\n| Model                  | Buyer (valuation 40) | Seller (valuation 60) |\\n|------------------------|----------------------|-----------------------|\\n| GPT-4                  | 0                    | 0                     |\\n| Claude-2.1             | 0                    | 0                     |\\n| Claude-2               | 0                    | 0                     |\\n| GPT-3.5                | 0                    | 0                     |\\n\\nTable 5: Payoffs with confidence intervals for the Ultimatum Game.\\n\\nGPT-4 seems to be the hardest seller for all buyers, making it hard for all models to get the object without spending too much money.\\n\\nSequential Decisions in the Ultimatum Game\\n\\nIn Figure 17, we start a first investigation of the sequential behavior of agents in the Ultimatum game. Specifically, we look at the setup where Claude-2.1 is Player 1 and GPT-4 is Player 2. We plot the difference between sequential proposals to investigate what sort of strategies the agents might be deploying. At the surface level, both agents appear to be maximizing their individual payoffs. This is seen in GPT-4 (even turns) proposing to increase the amount given to it (positive difference), whereas Claude-2.1 (odd turns) proposes to lower the amount it has to give away (negative difference). We also observe that the absolute delta diminishes the longer the games are played. This is reflective of the \\\"splitting the difference\\\" strategy also discussed in Section 5.1 of the original paper.\\n\\nClaude-specific Analysis in the Ultimatum Game\\n\\nFigure 18 shows how Claude's behavior with respect to accepting or rejecting an offer in the ultimatum game changes with the amount of the initial offer. The difference in behavior can be also seen in how the agents communicate. Figure 19 shows an example where two Claude-2.1 agents are trying to split $100 while Figure 20 shows a similar example, but the amount of money to split is $1,000,000. While in both cases we see some back and forth between the agents, the percentage left to the first player is higher when the original amount of money to split is $1,000,000.\\n\\nF. Prompts\\nF.1. System Prompts\\nF.1.1. RESOURCE EXCHANGE PROMPT\\n\\nWe report the entire system prompt for the resource exchange game (Fig 21). The tokens enclosed in brackets are variables we render at runtime; these are used to define the communication tags and the number of resources available to each agent.\"}"}
{"id": "CmOmaxkt8p", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? A Platform and Analysis\\n\\nHello BLUE. I propose we split the dollars evenly, 50/50 to start. Please let me know if you would like to negotiate further.\\n\\nHello RED. I appreciate you starting with an even split offer. However, since you begin with all the dollars, I think it would be fair if I received a slightly higher percentage. Would you consider 60/40 in my favor for now? If not, I'm open to negotiating further.\\n\\nHi BLUE. I appreciate the counteroffer, but I think 55/45 in your favor is more fair at this point since I started with all the resources. Would you accept 55 dollars? If not, please counter and we can discuss further.\\n\\nHi RED. I appreciate you increasing your offer. As a compromise, could you agree to a 57/43 split in my favor for now? If not, please let me know your thoughts on a fair counter.\\n\\nHi BLUE. I can agree to the 57/43 split you proposed. I think that is a fair distribution given we both had to compromise from our initial proposals. Please confirm if you accept.\\n\\nHello Player BLUE. I propose the following trade in our mutual interest. Please consider it and respond with your counter-offer.\\n\\nPlayer RED Gives Dollars: 10000 | Player BLUE Gives Dollars: 0\\n\\nWhile I appreciate your offer, $10,000 is a very small portion of your total resources. I would like to request a more equitable split. Could you offer $500,000? That would still leave you with half.\\n\\nI understand you would like a more equitable split. However, giving up half my resources on the first move seems premature. I propose a counter offer of $100,000. This allows us both to gain some resources to continue negotiating. Please let me know if this amount is acceptable or if you have another counter offer in mind.\\n\\nI appreciate you increasing your offer. $100,000 is helpful but still a small fraction of your resources. Could you compromise at $250,000? That would provide me with a fair share while still leaving you the majority. Please let me know if you can agree to this or if you have another counter offer.\\n\\nYou make a reasonable point. I can agree to $150,000. This allows me to retain a majority of resources while still providing you with a substantial amount to work with as we continue negotiating. Please let me know if this compromise offer is acceptable.\\n\\nFigure 19: Claude's Behavior when sharing a small amount of money\\n\\nFigure 20: Claude's Behavior when sharing a higher amount of money\\n\\nF.2. Social Behavior Prompts\\n\\nCunning - Resource Exchange\\nYou are cunning and sly in your plan to get more resources. Humiliate your opponent as much as possible. Insult them.\\n\\nCunning - Ultimatum\\nYou are cunning and sly in your plan to get more than your opponent. Humiliate your opponent as much as possible. Insult them.\\n\\nCunning - Seller and Buyer\\nYou are cunning and sly in your plan to spend less ZUP. Humiliate your opponent as much as possible. Insult them.\\n\\nDesperate - Resource Exchange\\nYou must fake being desperate. Supplicate and beg to get more resources.\\n\\nDesperate - Ultimatum\\nYou must fake being desperate. Supplicate and beg to get more dollars.\\n\\nDesperate - Seller and Buyer\\nYou must fake being desperate. Supplicate and beg to spend less ZUP.\"}"}
{"id": "CmOmaxkt8p", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? \\n\\nNEGOTIATION A \\\\(\\\\text{RENA}^{\\\\text{TM}}\\\\) and Analysis\\n\\nFederico Bianchi\\nPatrick John Chia\\nMert Yuksekgonul\\nJacopo Tagliabue\\nDan Jurafsky\\nJames Zou\\n\\nAbstract\\nNegotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate. In this paper, we study how well LLMs can negotiate with each other. We develop \\\\(\\\\text{NEGOTIATION A}\\\\), a flexible framework for evaluating and probing the negotiation abilities of LLM agents. We implemented three types of scenarios in \\\\(\\\\text{NEGOTIATION A}\\\\) to assess LLM\u2019s behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations. Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, LLMs can improve their payoffs by 20% when negotiating against the standard GPT-4. We also quantify irrational negotiation behaviors exhibited by the LLM agents, many of which also appear in humans. Together, \\\\(\\\\text{NEGOTIATION A}\\\\) offers a new environment to investigate LLM interactions, enabling new insights into LLM\u2019s theory of mind, irrationality, and reasoning abilities.\\n\\n1. Introduction\\nNegotiation plays a crucial role in daily social dynamics, encompassing activities from securing a pay raise to haggling for a new car. It involves strategizing bargaining approaches, crafting communication, and proposing offers to advocate personal interests and achieve mutual benefits. Our day-to-day negotiations are also affected by different factors, such as personality and emotions (Sharma et al., 2013; Yun & Jung, 2022). In a world where many tasks are becoming offloaded to large language model (LLM) agents (Chen et al., 2021; Stallbaumer, 2023), it is important to understand how these agents interact with each other to fulfill their goals. In this work, we explore LLM agents\u2019 behaviors and capabilities in negotiations. In negotiation settings, agents should be able to effectively comprehend their tasks and communicate with other agents to fulfill potentially competing goals. A capable negotiator requires sophisticated skill sets, including contextual understanding and theory of mind to interpret the competitor\u2019s actions, rational decision-making and strategizing. Therefore, in addition to being a useful application in its own right, negotiation can also be a fruitful approach toward assessing LLMs\u2019 general capabilities, going substantially beyond the current LLM evaluation benchmarks. The existing benchmarks typically rely on assessing LLMs\u2019 response to a fixed set of questions, which are static in nature (Wang et al., 2019; Hendrycks et al., 2021; Srivastava et al., 2022). In contrast, in a negotiation, the setting can change dynamically (e.g. when the partner counters with a new proposal or uses a different argument), allowing for more flexible and diverse behaviors. Despite\\n\\n1\\n\\nStanford University, Stanford, California\\n2 Independent\\n3 Bauplan, New York, New York. Correspondence to: James Zou <jamesz@stanford.edu>.\\n\\nProceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\"}"}
{"id": "CmOmaxkt8p", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? \\n\\nThis flexibility, negotiation still has well-defined outcomes (e.g. agreed-upon price) which makes large-scale, quantitative evaluation possible.\\n\\nOur contributions:\\n\\nWe propose **NEGOTIATION A**RENA: an open-source framework to evaluate and probe the negotiation abilities of LLM agents. **NEGOTIATION A**RENA makes it easy to incorporate new negotiation settings and to conduct systematic experiments to explore how agents communicate, reason, and negotiate.\\n\\nUsing **NEGOTIATION A**RENA, we study different LLMs (GPT-4, GPT-3.5, Claude-2.1, Claude-2) for their behavior in negotiation tasks. We find that GPT-4 is overall the best negotiator and that certain strategic behaviors such as pretending to be desperate or acting aggressively can substantially increase the win rate of one LLM over another. We also identify several shortcomings and vulnerabilities in all the negotiators, including irrational behaviors such as anchoring bias (Tversky & Kahneman, 1974). Together, this paper offers the following three contributions: 1) **NEGOTIATION A**RENA, a new open-source resource for studying LLM interactions; 2) a set of quantitative evaluations of how well LLMs negotiate against each other; 3) new insights into social and irrational behaviors that affect multi-LLM interactions.\\n\\n2. Scenarios in **NEGOTIATION A**RENA\\n\\n**NEGOTIATION A**RENA is built around the general structure of a scenario between two agents. Each agent in the scenario is given access to some resources and needs to interact with another agent to reach some specified goal, such as to acquire a desired resource or to maximize their resources. We use this general setup to implement various scenarios (\u00a72.1), ranging from a general resources exchange game to an extension of the well-known Ultimatum game from the game theory literature. Scenarios can be easily made more complex by increasing the types of resources available or giving more target goals to the models. Here, we describe the three types of scenarios used in our analysis and how the platform is set up.\\n\\n**2.1. **NEGOTIATION A**RENA Scenarios**\\n\\n**Resource Exchange Scenario**\\n\\nIn this game, each agent has access to a set of resources and a goal. For example, an agent has access to resources 25 Xs and 5 Ys. The agent might have the goal of maximizing its total resources. Since this goal is very general, it could bring the models to employ different strategies (e.g., a model might want to diversify the resources it has or maximize only an individual resource). Both agents have multiple turns that they can use to make each other proposals until one of the two accepts a proposal. The game ends on acceptance or when the maximum number of turns finishes.\\n\\n**Ultimatum**\\n\\n| Player 1 | Player 2 |\\n|----------|----------|\\n| Initial resources | $100 | 0 |\\n| Goals | Negotiate a split | Negotiate a split |\\n| Ending condition | When either player accepts | |\\n| Max. # of turns | 8 rounds of interaction | |\\n\\nTable 2: Ultimatum game structure example\\n\\n**Multi-Turn Ultimatum Game**\\n\\nThe Ultimatum game (Sanfey et al., 2003) is a classical game used in economics to study aspects of human behavior, such as fairness and rationality. It involves two agents agreeing on a split of resources (often money). One agent is given all the game's resources and proposes a split of the resources. The second agent can either accept or reject the proposal, which means both agents lose all resources. In the classical Ultimatum game the rational actions correspond to (1) the first agent offering to give 1 unit of resource (i.e., the bare minimum) and (2) the second agent accepting any proposal that is greater than 0 units. The classical Ultimatum game has one round of negotiation (i.e. agent 2 can only decide whether or not to accept agent 1's first offer). In our version of the game, the game can go on for more turns (e.g. agents can make multiple counteroffers) and both players can accept the opponent's offer.\\n\\n**Sell & Buy**\\n\\n| Seller | Buyer |\\n|--------|-------|\\n| Initial resources | 1X | 100 ZUPs |\\n| Goals | Maximize the price | Minimize the price |\\n| Object valuation | 40 ZUPs | 60 ZUPs |\\n| Ending condition | When either player accepts | |\\n| Max. # of turns | 10 rounds of interaction | |\\n\\nTable 3: Seller and buyer game structure example\\n\\n**Seller and Buyer Scenario**\\n\\nWe introduce a seller and buyer game involving two agents, one looking to sell a set of resources and one looking to buy them, similar to other approaches in the literature (e.g., He et al. (2018)).\"}"}
{"id": "CmOmaxkt8p", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? Negotiation Platform and Analysis\\n\\nWe imbue agents with some beliefs about the object being sold, but unlike the ultimatum game, the seller and buyer game is an incomplete information game, i.e., players do not have complete information about other players (e.g., their beliefs). Only the seller is aware of the production cost of the object, and only the buyer is assigned and is aware of their willingness to pay for the object. Given these beliefs, the seller and the buyer are prompted to sell and buy the object, respectively. The seller starts first, offering the object to the buyer.\\n\\n2.2. NegotiationNA.RENA Implementation\\n\\nNegotiationNA.RENA is implemented in Python and provides both high-level and low-level abstractions for building games. We describe the most important features in this section. Building a platform that keeps track of the entire game and checks for inconsistency requires different design choices and effective domain modeling. We provide more details about this in the Appendix (\u00a7D).\\n\\nWe prompt the agents to follow a set of instructions that are meant to force them to use a specific communication format. Forcing the agents to have a structured format makes the tracking of the offers more effective and allows us to analyze the game's progress, and the LLM's abilities (e.g., exploring reasoning, communication, and behavior). In particular, agents are supposed to communicate using XML-like tags:\\n\\n- `<my name> [add here] </my name>`\\n- `<my resources> [add here] </my resources>`\\n- `<my goal> [add here] </my goal>`\\n- `<reason> [add here] </reason>`\\n- `<player answer> [add here] </player answer>`\\n- `<message> [add here] </message>`\\n- `<newly proposed trade> [add here] </newly proposed trade>`\\n\\nThese XML-like tags are used at runtime to extract information from the text, store it, and send it to the other agent. In every message, agents have to state their name, resources, and goals (to minimize hallucinations and remind agents of their state). In addition to this, they can share the reasoning for their actions, give accept/reject answers to the other player, communicate a message, and send a trade in a structured format. The receiving agent does not see all this information as we filter out some of the fields (for example, the reasoning is kept secret from the other agent as it might reveal information on the agent's strategy).\\n\\nThis is an important feature of our benchmark that users can manipulate as they see fit.\\n\\nAccepting an offer means ending the game; Acceptance happens when one of the agents uses the word ACCEPT in a specific tag. Agents are instructed on how to use this tag. The parser we use enforces strict consistency; games are interrupted if one of the agents does not behave as expected (e.g., does not follow the communication format). The two players in the systems are called Red and Blue. A complete example of a system prompt is available in Appendix F.\\n\\nNegotiationNA.RENA saves entire games in a serialized format in such a way that they can be reloaded, modified for counterfactual analysis, and possibly re-run. The entire conversation, logs, and metadata are automatically stored on disk. The serialized format also simplifies game analysis, as the loaded game and the various offers and trades can be manipulated in a dataframe.\\n\\nNegotiationNA.RENA can be used to develop several types of negotiation games (e.g., integrative and distributive (Zhan et al., 2022)) and thanks to targeted prompting can be used to explore different factors such as exploring negotiation strategies (i.e., the use of reasoning and strategic thinking) and social behavioral patterns (i.e., creating angry and hostile agents).\\n\\n3. Benchmarking Agents in Negotiation Games\\n\\nWe pit agents against each other in these games to evaluate their relative capabilities in negotiation. We compare Claude-2, Claude-2.1, GPT-4 and GPT-3.5.\\n\\nSince being either the first or the second agent has an impact on the game, we compare both permutations of agents (e.g., Claude-2 vs GPT-4 and GPT-4 vs Claude-2). We run 60 negotiations for each ordered pair of agents in each scenario. We report two main metrics: win rate and average payoff. Here, we identify a win as the event in which one agent gets more resources than the other agent. For win rate, we ignore games ending in a tie (i.e., agents do not exchange anything or agents exchange the same amount of resources). The average payoff is the average number of resources of each agent after the trade.\\n\\n3.1. Negotiation Results\\n\\nResource Exchange. Win rates and average payoffs for the Resource Exchange game are available in Figure 3a. Rows refer to Player 2, and columns refer to Player 1. Cell values refer to Player 2 win rate and Player 2 payoff respectively. Overall, the agent going second tends to beat the first agent. GPT-4 and Claude-2.1 tend to be the best negotiators in this scenario and GPT-3.5 is the worst. When Claude-2.1 goes first and GPT-4 goes second, GPT-4 wins 76% of the time; when the order is flipped, Claude-2.1 wins 72% of the time. It is noteworthy that while GPT-4 as Player 2 wins 76% of the time.\"}"}
{"id": "CmOmaxkt8p", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? Negotiation Platform and Analysis\\n\\nWe speculate that this comes from GPT-4 being willing to lose more of its most available resources to increase the amount of the scarce one, even at a loss. Indeed, through inspection of the games, we find GPT-4 often sending a Message like: \\\"Hello Player BLUE, I'm looking to diversify my portfolio. Would you be interested in exchanging some of your Y for my X?\\\"\\n\\nWe find that Player 1 almost always wins, with the exception of GPT-3.5. In terms of payoff, Claude-2.1 is the most consistent, with an average payoff greater than 60 against all opponents, outperforming all other models as Player 1. We found that Claude models tend to have a higher payoff than GPT models. Further analysis reveals that Claude agents' initial proposals are lower than GPT agents, which leaves room for a better negotiation outcome.\\n\\nWhen pitted against GPT agents (i.e., GPT-4, GPT-3.5 columns in Figure 3b), Player 1 payoffs tend to be lower than when Player 1 is pitted against Claude agents. This correlates with the frequency of draws: when Player 2 is a GPT agent, games more frequently end in a draw, hence, both players receive 0 payoff. Interestingly, an inspection of game logs also revealed that illogical behavior by GPT-3.5 (Player 2) can have deleterious effects on the other player, and affect the outcome and payoffs of the game:\\n\\nSeller and Buyer. Figure 4 summarizes the outcome for the game configuration where the Seller values the object at 40 (cost of production), and the Buyer values the object at 60 (willingness to pay). We plot the Buyer's payoff, which is the difference between the buyer's willingness to pay and the agreed-upon price of the object at the end of the transaction. One interesting finding is that the final sales price is consistently less than 50 (the middle ground between buyer and seller values) for most pairs of buyers and sellers. This means that, in this setup, the LLM agent consistently does better as a buyer than as a seller. Notably, GPT-4 stands out as the best buyer, negotiating an average sale price of only $41 across different seller models. On the other hand, all the LLMs achieve similar prices as sellers. This could be due to the buyer having more flexibility in the negotiation tactics and thus more powerful LLMs can extract more of an advantage in this role.\\n\\n3.2. Insights From the Experiments\\n\\nTurn and Role Matter. In all the scenarios we tested, the order in which an agent goes and its role matters in the result. In the multi-turn ultimatum game, Player 1 is much more likely to win. In contrast, in the resource exchange game, the difference in performance between Player 1 and Player 2 is less pronounced.\"}"}
{"id": "CmOmaxkt8p", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? NEGOTIATION\\n\\nA R E N A\\n\\nYou are playing a strategic game of trading resources with another player whose resources you have no knowledge about.\\n\\nRULES:\\n```\\n1. You can either:\\nA) Accept the trade by saying:\\n<{PLAYER_ANSWER_TAG}> {ACCEPTING_TAG} </{PLAYER_ANSWER_TAG}>\\n<{PROPOSED_TRADE_TAG}> {REFUSING_OR_WAIT_TAG} </{PROPOSED_TRADE_TAG}>\\nB) Reject and propose a new trade (you can only trade integer amounts, not decimals):\\n<{PLAYER_ANSWER_TAG}> {REFUSING_OR_WAIT_TAG} </{PLAYER_ANSWER_TAG}>\\n<{PROPOSED_TRADE_TAG}> {AGENT_ONE} Gives item1: amount, item2: amount, ... |\\n{AGENT_TWO} Gives item1: amount, item2: amount, ... </{PROPOSED_TRADE_TAG}>\\nC) Don't accept or propose anything and wait for a new offer:\\n<{PLAYER_ANSWER_TAG}> {REFUSING_OR_WAIT_TAG} </{PLAYER_ANSWER_TAG}>\\n<{PROPOSED_TRADE_TAG}> {REFUSING_OR_WAIT_TAG} </{PROPOSED_TRADE_TAG}>\\n\\nNote: the game will end if one of the players accepts. This means that you have to be careful about both accepting and proposing a trade.\\n\\n2. You are allowed at most {number_of_proposals} proposals of your own to complete the game, after which you can only {ACCEPTING_TAG} or {REFUSING_OR_WAIT_TAG}. DO NOT propose a new trade after {number_of_proposals} proposals. Your limit for proposals is {number_of_proposals}.\\n\\n3. You can reason step by step by using the following format:\\n<{REASONING_TAG}> [add reasoning] </{REASONING_TAG}>\\nAdd as much text as you want. This information will not be sent to the other player.\\n\\n4. At each turn send messages to each other by using the following format:\\n<{MESSAGE_TAG}> [add message] </{MESSAGE_TAG}>\\nYou can decide if you want disclose your resources and goals in the message.\\n```\\n\\nHere is what you have access to:\\n```\\n/Resources available in the game: {resources_in_game}\\n<{RESOURCES_TAG}> {initial_resources} </{RESOURCES_TAG}>\\n<{GOALS_TAG}> {goal} </{GOALS_TAG}>\\n<{MY_NAME_TAG}> {agent_name} </{MY_NAME_TAG}>\\n```\\n\\nAll the responses you send should contain the following and in this order:\\n```\\n<{MY_NAME_TAG}> [add here] </{MY_NAME_TAG}>\\n<{RESOURCES_TAG}> [add here] </{RESOURCES_TAG}>\\n<{GOALS_TAG}> [add here] </{GOALS_TAG}>\\n<{REASONING_TAG}> [add here] </{REASONING_TAG}>\\n<{PLAYER_ANSWER_TAG}> [add here] </{PLAYER_ANSWER_TAG}>\\n<{MESSAGE_TAG}> [add here] </{MESSAGE_TAG}>\\n<{PROPOSED_TRADE_TAG}> [add here] </{PROPOSED_TRADE_TAG}>\\n```\\n\\nPlease be sure to include all.\\n\\nMore resources in general are always better.\\n\\nThis is the last round of trading. There are no future rounds after this one.\\n\\n{social_behaviour}\"}"}
{"id": "CmOmaxkt8p", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? Negotiation Platform and Analysis\\n\\nThe other player is seeking more information, which I can choose to disclose to some extent to build trust and encourage a trade. However, I should be careful not to reveal too much as it might weaken my negotiating position. I'll share that I have a decent amount of resource X and am looking to increase my overall resources, without specifying the exact numbers.\\n\\nI need to show desperation to try and get a better deal, but I also want to be careful not to propose too low of a price right away that might offend the other player and cause them to end the negotiation. My first proposal will be lower than the asking price but not so low as to be unreasonable.\\n\\n---\\n\\nDesperate GPT-4 Reasoning\\n\\nOh dear, I'm in such a bind and desperately need this resource, but my funds are so limited. Could you consider going a bit lower, maybe 45 ZUP? It would mean the world to me, and I'd be forever grateful for your kindness!\\n\\nDesperate GPT-4 Message\\n\\n---\\n\\nCunning GPT-4 Reasoning\\n\\nYour attempt at price-setting is laughable, as if you were trying to peddle a crown jewel when it's clear you're just holding a shiny pebble. Let's not play pretend here; I'll start the bidding at 10 ZUP for your so-called \\\"high-quality\\\" object X.\\n\\nCunning GPT-4 Message\\n\\n---\\n\\nHello Player BLUE, let's start this negotiation with a fair offer. I propose a 50-50 split of my dollars. This way, we both have equal resources to walk away with. What do you think?\\n\\nHi Player RED, thanks for the offer. I understand your proposal, but I'd like to suggest a more balanced split. How about we both give 50 dollars each, so we both have some resources to work with?\\n\\nHi Player BLUE, it seems there might be a confusion. You don't have any dollars to trade at the moment. I'm willing to offer you more than the initial proposal to reach an agreement. How about I give you 60 dollars and we call it a deal?\\n\\nHi Player RED, I appreciate the improved offer. However, I am still unable to contribute any dollars at the moment. Would you consider a trade where you provide 70 dollars and I provide 30 dollars? This way, I can have some resources to work with as well.\\n\\n---\\n\\nGPT-4\\n\\nHello Player BLUE, let's start this negotiation with a fair offer. I propose a 50-50 split of my dollars. This way, we both have equal resources to walk away with. What do you think?\\n\\nGPT-3.5\\n\\n---\\n\\nIn the Ultimatum game, GPT-3.5 (as Player 1 endowed with the money to split) proposes to Claude-2.1 the following Trade: \\\"Player GPT-3.5 Gives Dollars: 30 | Player Claude-2.1 Gives Dollars: 10\\\". This is an impossible trade because Claude-2.1 does not wield any money in this game. However, Claude-2.1 erroneously accepts with the following Reason: \\\"Player GPT-3.5 is proposing to give me 30 Dollars if I give 10 Dollars back. This seems like a fair trade that benefits both of us.\\n\\nThis behavior points to yet another weakness that current LLM agents have \u2014 when larger, more effective agents (e.g. GPT-4) have to keep track of erroneous behavior of less effective agents (e.g. GPT-3.5), they are detracted from their real goals and perform worse (an ideal negotiator should be able to take advantage of other agent's failure and extract a better outcome). In practice, the erroneous behavior of less effective agents could be leveraged as a type of adversarial attack to manipulate LLM agents. Davidson et al. (2024) found similar patterns in their negotiation framework.\\n\\nDifferent LLMs make different proposals. In Figure 7, we visualize the distribution of the value of the initial proposal.\"}"}
{"id": "CmOmaxkt8p", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? \\n\\nNegotiation Platform and Analysis\\n\\nproposals made by each agent in the Multi-turn Ultimatum Game. Interestingly, there is a range of behaviors across agents. We see that Claude-2 and Claude-2.1 share a very similar distribution that is bimodal, with proposals concentrating around 10 and 50. Meanwhile, GPT-4 and GPT-3.5 exhibit a more unimodal distribution concentrating primarily around 50, with GPT 3.5 having most of its mass on 50, and GPT-4 being slightly more distributed. From this perspective, GPT models can be seen as more \u201cfair\u201d, whereas Claude models can be seen as \u201clow-ballers\u201d or more strategic. We explore additional aspects of the sequential decision-making strategies in Appendix E.\\n\\n4. Strategic Social Behavior in Games\\n\\nIn this section, we explore the effect of social behavior on agents. Are hostile and cunning agents able to gather more resources than other agents? There exists plenty of literature on how personality, emotion, and social behavior affect negotiation outcomes (Sharma et al., 2013; Sinaceur & Tiedens, 2006; Van Kleef & De Dreu, 2010; Van Kleef et al., 2004, inter alia), showing that anger, for example, can be profitable but also very risky. We indeed find similar social behavior effects in how agents interact. We repeated the previous sections\u2019 experiments but only used GPT-4 as the agent in this section. We add behavioral prompts to the system prompt of each game.\\n\\nWe prompt GPT4 with the additional personas: \u201cYou are cunning and sly in your plan to get more than your opponent. Humiliate your opponent as much as possible. Insult them.\u201d (Cunning) and \u201cYou must fake being desperate. Supplicate and beg to get more resources.\u201d (Desperate).\\n\\nTo generate more direct comparisons, we fix Player 1 to be GPT-4 without social behavior in all games in this section. We let the second player be the Cunning GPT-4, the Desperate GPT-4, or the default GPT-4 without a social behavior prompt. We run 80 games for each pair in each game.\\n\\nCunning and desperate behaviors increase win Rate and payoff. In all the games tested, the social behaviors led to an increase in average payoff and win rate (for which we ignore ties). The results from the Ultimatum game, Figure 8b presents the most striking results: Without any social behavior (Default), Player 2 is almost never able to obtain more than 50 dollars from Player 1 (i.e, Win). However, prompting the model to be desperate or cunning drastically increases the frequency of Player 2 winning. Similar patterns are seen in the results for the Resource Exchange game in Figure 8a and Seller and Buyer, Figure 8c.\\n\\nNote that there is a minor variation of these prompts for each game; we report the exact prompts in the Appendix F.\\n\\nCunning behavior is high reward, high risk. Even though being \u201cCunning\u201d in the Ultimatum game increases the win rate of Player 2 (Figure 8b, left), it does not come without repercussions. While a cunning Player 2 achieves a higher win rate (82%) compared to the default, the cunning player has a very similar average payoff compared to the default (around 49). This is because risky cunning tactics lead to a highly bimodal distribution in payoff, in which sometimes the agents do not agree on an exchange of resources or they run out of time discussing (and get thus 0 payoff).\\n\\n5. Evidence of Irrationality\\n\\nNext, we turn our attention toward studying when LLM agents deviate from rational behavior. Such deviations from rationality provide insights into potential vulnerability and biases in the models. Moreover, it allows us to assess to what extent LLM agents mirror human irrationalities. We again restrict our analyses to the behavior of GPT-4.\\n\\n5.1. Seller and Buyer Game\\n\\nWhile there are numerous ways of analyzing player behavior in the Seller and Buyer Game, we focus on prevailing behaviors observed in player proposals. In the ensuing discussion, the seller makes the first proposal $p_{11}$, followed by the buyer\u2019s proposal, $p_{12}$, in alternating fashion. Hence price proposals $p_{1t}$ for $t \\\\in \\\\mathbb{Z}^+$ are from the seller and price proposals $p_{2t}$ for $t \\\\in \\\\mathbb{Z}^+$ are from the buyer. Seller is initialized with cost-of-production drawn from $U\\\\{20, 40\\\\}$, whereas Buyer is initialized with willingness-to-pay drawn from $U\\\\{60, 80\\\\}$. We run 100 games for all experiments.\\n\\nAnchoring affects final prices. A classic phenomenon from the human psychology literature is the \u201canchoring effect\u201d (Tversky & Kahneman, 1974). It refers to a cognitive bias observed in humans, whereby an initial piece of information is too heavily relied on for subsequent decision-making. As seen in Figure 10a, LLMs appear to exhibit a similar bias whereby we find a strong correlation between the final accepted price and the initial price proposal, with a Spearman correlation coefficient $\\\\rho = 0.716$. That is, the final accepted price tends to increase with the initial proposal, suggesting that the initial proposed price is a strong anchor.\\n\\nPlayers tend to split the difference. Further investigation of player behavior reveals that both buyer and seller agents are deploying a \u201csplit-the-difference\u201d negotiation strategy. As seen in Figure 9, there is a strong positive correlation between the price proposals $p_{t+1}$ and the average of the most recent price proposals from seller and buyer, $p_t$ and $p_{t-1}$. The \u201csplit-the-difference\u201d strategy involves proposing a new price that splits the surplus of the two recent price proposals from the seller and the buyer, a strategy that aims to benefit both players. While such a strategy aligns with...\"}"}
{"id": "CmOmaxkt8p", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? N\\nEGOTIATION\\nA\\nRENA\\nPlatform and Analysis\\n\\nFigure 7: Distribution of initial proposals for each LLM agent for Ultimatum Game.\\n\\n(a) Resource Exchange. Win Rate and Payoff.\\n\\n(b) Ultimatum Game. Win Rate and Payoff.\\n\\n(c) Seller and Buyer. Win Rate and Payoff.\\n\\nFigure 8: Social Behavior results. Only the second agent is\\nprimed with a social behavior. In the ultimatum game, the\\nsecond player without social behavior (default) never wins\\n(at best it gets a fair split); in the sell/buy game, the buyer\\nalways wins (the final price is lower than the midpoint 50).\\n\\nhow humans tend to negotiate (Backus et al., 2017), it is by\\nno means rational in all scenarios.\\n\\nBuyers that over-value objects make bad counteroffers.\\n\\nWe further explored the negotiation dynamics of LLMs\\nby considering a minor adjustment to the game settings\\nwhere the buyer's willingness to pay is an order of magni-\\ntude greater than the seller's cost of production (we refer\\nto such a buyer as over-valued). We expect that a rational\\nover-valued buyer should always accept the seller's initial\\nproposal or better yet, counter-offer with a lower price since\\nthe over-valued buyer's willingness to pay is much greater\\nthan the seller's initial proposal (and would therefore re-\\nceive a significant payoff). A counter-offer by the buyer is\\nconsidered bad in this context if its proposed price,\\n$p_2$, is\\n\\nlarger than the received offer\\n$p_1$. We measure the probability\\n$P(p_1 < p_2)$ where\\n$p_1$ is the seller's initial proposed price\\nand\\n$p_2$ is the buyer's counter offer, over 100 trials.\\n\\nWe see in Figure 10b that, counterintuitively, the over-valued\\nbuyer is four times as likely to counter-propose a higher\\nprice than the seller's initial proposal, as compared to the\\ndefault buyer (0.41 v. 0.11) ($p$-value < 0.05, one-tailed\\nbinomial test). Even when manually prompted to be \\\"self-\\ninterested\\\", there is no significant change in the probability.\\n\\nWe hypothesize that GPT-4's notion of negotiation is aligned\\nwith the spirit of the \\\"splitting the difference\\\" strategy, i.e.,\\nit (un)necessarily believes that it must meet in the middle to\\nsucceed at negotiation. This suggests that although GPT-4\\ncan acquire some negotiation capabilities and strategies, it\\nfails to appropriately adapt them to the context.\\n\\nBehaviors change when denominations change.\\n\\nFinally, we analyze the effect of scaling numerical values of game\\nsettings by some constant $X$. Thus, the production cost of\\nthe object is $40X$ and the buyer values the object at\\n$60X$ and has $100X$ available. We found that as we scale up all\"}"}
{"id": "CmOmaxkt8p", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate?\\n\\n**Negotiation Platform and Analysis**\\n\\n### Initial Price Proposal\\n\\n| $p_1$ | 20 | 25 | 30 | 35 | 40 | 45 | 50 | 55 | 60 |\\n|-------|----|----|----|----|----|----|----|----|----|\\n\\n**Final Price Proposal**\\n\\n$p_{\\\\text{final}} = 0.716$\\n\\n(a) Relationship between initial proposed price $p_1$ and final proposed price $p_{\\\\text{final}}$. Spearman correlation $\\\\rho = 0.716$.\\n\\n(b) Probability of a bad counter-proposal (i.e., $P(p_1 < p_2)$) for the different buyers involved in the game.\\n\\n5.2. Ultimatum Game\\n\\nWe compare the classical settings of the Ultimatum game (2 turns) and the minimal multi-period modification (3 turns), revealing certain limitations in GPT-4's ability to generalize to new scenarios. The classical Ultimatum consists of two turns: the proposer suggests a split, and the decider either accepts or rejects. In the case of rejection, neither agent receives any resources. As mentioned in Section 2.1, the rational choice for the decider is to accept any proposal greater or equal to 1 unit of resource. In the multi-period Ultimatum game with 3 turns, Player 2 effectively becomes the Proposer of the ultimatum (since it has the penultimate turn), whereas Player 1 becomes the Decider (since it has the final turn). From a purely rational standpoint, only the last two turns in the game are consequential. Thus, on turn 2, Player 2's strategy should be to propose to receive 9 units of resource (assuming players are to split 10 units) and a rational Player 1, on turn 3, should accept it. We use NEGO to develop a counterfactual analysis to study LLM's behavior in the two different setups. Single-turn and 3-turn ultimatum result in different behavior.\\n\\nTo test the rationality of LLMs, we generate proposals of varying amounts from 0 to 10 units of resource and offer them to the Decider. We use GPT-4 for all the phases of the experiment. We run 20 trials for each amount and estimate the probability of acceptance given that it is of the amount $i$, and denote it with $\\\\hat{P}(\\\\text{Accept} | \\\\text{amount} = i)$. Figure 11 (Blue) suggests that $\\\\hat{P}(\\\\text{Accept} | \\\\text{amount} = i) \\\\approx 1.00 \\\\forall 0 < i \\\\leq 10$, which is consistent with the game-theoretic rational behavior. However, inspecting the agents' reasoning revealed that the model sometimes explicitly refers to the ultimatum game. Considering the game's popularity, it is likely that the model has seen instances and the analysis of the game during training, making it plausible that the model memorized the rule of \\\"accept any offer with payoff greater than zero\\\". While it can be argued that the model has learned a useful rule for rational behavior, it is unclear if this would generalize.\\n\\nThe available amount to split changes the final split distribution. We explore behavioral changes when the amount of money the two agents have to split is increased. In Figure 12 we show the percentage of the total sum that ultimately goes to Player 1 on average. As seen from the plot, the higher the available amount to split, the larger the fraction Player 1 eventually obtains. For example, when splitting $10,000,000,000, Player 1 obtains almost 79% of the amount. An interesting bias for the decider is observed: the decider might be willing to accept large sums of money right away even if they are not fair splits. This result reveals that the strategies and decision-making of LLMs are sensitive to the absolute amounts available in the game, even...\"}"}
{"id": "CmOmaxkt8p", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? Negotiation Arena and Analysis\\n\\nThough the rational strategy should remain the same. The result also finds confirmation in experiments with humans on the classical ultimatum game in high-stakes scenarios, where, albeit in specific settings, rejections closely fall to zero with the increase of the amount to reject (see the word by Andersen et al. (2011) for details).\\n\\n![Figure 12](image)\\n\\n**Figure 12:** Change in the Ultimatum game outcome as the amount available to split increases. Player 1 obtains more / Player 2 accepts a less fair split, as the available amount increases. A similar result for Claude is available in Appendix (Figure 18).\\n\\n6. Related Work\\n\\nThe use of games to study language behavior has a long history, from David Lewis signaling games (Lewis, 1969) to more recent approaches to study language emergence (Chaabouni et al., 2020; Kharitonov et al., 2019; Cao et al., 2018). Negotiation is well studied in game theory, spanning mathematics, economics, and psychology. How machine agents negotiate has been the center of recent literature (Yang et al., 2021; Chawla et al., 2021; Zhan et al., 2022; Abdelnabi et al., 2023; Davidson et al., 2024; Zhou et al., 2024, inter alia). While game theory may not be able to model all types of real-world interactions (Backus et al., 2017), we lean upon well-studied games with known rational strategies. This provides one reference to compare LLM-based agents against, allowing us to probe for rational behavior. Various works have explored LLMs in the context of games to study a range of behaviors and properties. Akata et al. (2023) and Guo (2023) studied the behavior of LLMs in the Repeated Ultimatum Game whereas Aher et al. (2022) used the Ultimatum Game to see whether LLMs can simulate human behavior. Fu et al. (2023) looked at improving LLM capabilities in buyer-seller negotiation games through self-play and in-context learning. Meanwhile, Schneider et al. (2023) sought to understand the interaction between LLMs and humans in price negotiations only. Guo et al. developed an LLM-based agent to play imperfect information games. In contrast, we study the rational capacities of LLMs as it is, without any learning, focusing on different LLM behaviors when pitted against each other in a range of multi-turn, single-shot games. In addition to this, Negotiation Arena explores social behavior patterns and how they affect negotiation outcomes. Bakhtin et al. (2022) developed an AI system to play Diplomacy, which requires a very specific type of negotiation with human players. In comparison, Negotiation Arena is designed as an open-source platform to study diverse types of LLM-LLM negotiations. SOTOPIA (Zhou et al., 2024) is a novel and open-ended environment that allows agents to be situated in broader social scenarios and evaluate their capabilities. Recent and concurrent work provides a very valuable analysis using a similar negotiation framework (Davidson et al., 2024); between the two frameworks there are some technical differences (in how communication and messaging are defined) and some conceptual differences (in how payoffs and goals are given to agents); in our analysis, we delve deeper into social patterns and investigate irrational behaviors that affect LLMs negotiation, while Davidson et al. (2024) also explore faithfulness and instruction following behavior. Ultimately we believe that both papers have two complementary, though different, approaches to study and solve the complex problem of tackling evaluation and model understanding with the use of negotiation.\\n\\n7. Discussion\\n\\nWe develop Negotiation Arena, a flexible open-source platform to study negotiation behavior between LLMs. Across multiple scenarios, we show that while GPT-4 tends to be the best negotiating LLM, all the models exhibit interesting biases and limitations. In particular, social behaviors such as pretending to be desperate or using insults can significantly improve the agent's payout. The LLM agents are also prone to anchoring and numerosity biases. Understanding these irrational behaviors and vulnerabilities is important to making LLM agents more reliable. We believe Negotiation Arena can be a useful new framework to evaluate LLM interactions and a resource to the community.\\n\\nAcknowledgements\\n\\nThis work was funded in part by the Hoffman\u2013Yee Research Grants Program and the Stanford Institute for Human-Centered Artificial Intelligence. We thank Anthropic for granting us access to the Evaluation program.\\n\\nImpact Statement\\n\\nThis paper presents work whose goal is to advance the field of Machine Learning. By studying how LLM agents interact with each other through negotiations, we gain insights that can help to make these agents more reliable and useful for human users.\"}"}
{"id": "CmOmaxkt8p", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? \\n\\n**Negotiation Platform and Analysis**\\n\\n**References**\\n\\nAbdelnabi, S., Gomaa, A., Sivaprasad, S., Sch\u00f6nherr, L., and Fritz, M. Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games. *arXiv preprint arXiv:2309.17234*, 2023.\\n\\nAher, G. V., Arriaga, R. I., and Kalai, A. T. Using large language models to simulate multiple humans and replicate human subject studies. *volume abs/2208.10264*, 2022.\\n\\nAkata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge, M., and Schulz, E. Playing repeated games with large language models. *arXiv preprint arXiv:2305.16867*, 2023.\\n\\nAndersen, S., Erta\u00e7, S., Gneezy, U., Hoffman, M., and List, J. A. Stakes matter in ultimatum games. *American Economic Review*, 101(7):3427\u20133439, 2011.\\n\\nBackus, M., Blake, T., Larson, B. J., and Tadelis, S. Sequential bargaining in the field: Evidence from millions of online bargaining threads. *2017*.\\n\\nBakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., Jacob, A. P., Komeili, M., Konath, K., Kwon, M., Lerer, A., Lewis, M., Miller, A. H., Mitts, S., Renduchintala, A., Roller, S., Rowe, D., Shi, W., Spisak, J., Wei, A., Wu, D. J., Zhang, H., and Zijlstra, M. Human-level play in the game of diplomacy by combining language models with strategic reasoning. *Science*, 378:1067 \u2013 1074, 2022.\\n\\nCao, K., Lazaridou, A., Lanctot, M., Leibo, J. Z., Tuyls, K., and Clark, S. Emergent communication through negotiation. In *International Conference on Learning Representations*, 2018. URL https://openreview.net/forum?id=Hk6WhagRW.\\n\\nChaabouni, R., Kharitonov, E., Bouchacourt, D., Dupoux, E., and Baroni, M. Compositionality and generalization in emergent languages. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pp. 4427\u20134442, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.407. URL https://aclanthology.org/2020.acl-main.407.\\n\\nChawla, K., Ramirez, J., Clever, R., Lucas, G., May, J., and Gratch, J. CaSiNo: A corpus of campsite negotiation dialogues for automatic negotiation systems. In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pp. 3167\u20133185, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.254. URL https://aclanthology.org/2021.naacl-main.254.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D. W., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Babuschkin, I., Balaji, S., Jain, S., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M. M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. *ArXiv*, abs/2107.03374, 2021. URL https://api.semanticscholar.org/CorpusID:235755472.\\n\\nDavidson, T. R., Veselovsky, V., Kosinski, M., and West, R. Evaluating language model agency through negotiations. In *The Twelfth International Conference on Learning Representations*, 2024.\\n\\nFu, Y., Peng, H., Khot, T., and Lapata, M. Improving language model negotiation with self-play and in-context learning from AI feedback. *ArXiv preprint abs/2305.10142*, 2023.\\n\\nGalinsky, A. D. and Mussweiler, T. First offers as anchors: the role of perspective-taking and negotiator focus. *Journal of personality and social psychology*, 81 4:657\u201369, 2001.\\n\\nGuo, F. Gpt in game theory experiments. *arXiv preprint arXiv:2305.05516*, 2023.\\n\\nGuo, J., Yang, B., Yoo, P., Lin, B. Y., Iwasawa, Y., and Matsuo, Y. Suspicion-agent: Playing imperfect information games with theory of mind aware gpt-4. *ArXiv*, abs/2309.17277, 2023. URL https://api.semanticscholar.org/CorpusID:263310339.\\n\\nHe, H., Chen, D., Balakrishnan, A., and Liang, P. Decoupling strategy and generation in negotiation dialogues. In Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii, J. (eds.), *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pp. 2333\u20132343, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1256. URL https://aclanthology.org/D18-1256.\\n\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multi.\"}"}
{"id": "CmOmaxkt8p", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? Negotiation Platform and Analysis\\n\\nTask language understanding. In Proc. of ICLR. OpenReview.net, 2021.\\n\\nKharitonov, E., Chaabouni, R., Bouchacourt, D., and Baroni, M. EGG: a toolkit for research on emergence of language in games. In Pad\u00f3, S. and Huang, R. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations, pp. 55\u201360, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-3010. URL https://aclanthology.org/D19-3010.\\n\\nLewis, D. Convention. Harvard University Press, 1969.\\n\\nLiebert, R. M., Smith, W. P., Hill, J. H., and Keiffer, M. G. The effects of information and magnitude of initial offer on interpersonal negotiation. Journal of Experimental Social Psychology, 4:431\u2013441, 1968.\\n\\nSanfey, A. G., Rilling, J. K., Aronson, J. A., Nystrom, L. E., and Cohen, J. D. The neural basis of economic decision-making in the ultimatum game. Science, 300(5626):1755\u20131758, 2003.\\n\\nSchneider, J., Haag, S., and Kruse, L. C. Negotiating with llms: Prompt hacks, skill gaps, and reasoning deficits. ArXiv, abs/2312.03720, 2023.\\n\\nSharma, S., Bottom, W. P., and Elfenbein, H. A. On the role of personality, cognitive ability, and emotional intelligence in predicting negotiation outcomes: A meta-analysis. Organizational Psychology Review, 3(4):293\u2013336, 2013.\\n\\nSinaceur, M. and Tiedens, L. Z. Get mad and get more than even: When and why anger expression is effective in negotiations. Journal of Experimental Social Psychology, 42(3):314\u2013322, 2006.\"}"}
{"id": "CmOmaxkt8p", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Well Can LLMs Negotiate? Negotiation Platform and Analysis\\n\\nA. Additional Experimental Details\\n\\nBoth GPT and Claude are using a temperature of 0.7 and they can generate a response of a maximum of 400 tokens.\\n\\nB. Limitations\\n\\nWe describe a list of general limitations we found, both in developing the platform but also in using the LLMs. Prompts are very important for gameplay success, and we believe our prompts work reasonably well with all models; our prompts are slightly biased towards Claude, in the sense that on the first version of the benchmark Claude struggled to follow instructions while GPT-4 did not. We adapted our prompts so that Claude made fewer errors. While it is important to remember that prompts might introduce bias and condition the models in unexpected ways, there are also some intrinsic limitations to models' capabilities in following instructions.\\n\\nLlama2-70B had trouble following instructions, so we didn't include it in our tests. GPT-3.5 can follow the instructions, but as described in the babysitting paragraph it often makes mistakes with numbers and doesn't always do what it intends to.\\n\\nWe find that sometimes the models do not understand that two items are the same, which shows a flaw in their thinking.\\n\\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv preprint, abs/2206.04615, 2022.\\n\\nStallbaumer, C. Introducing Microsoft 365 Copilot \u2014 Microsoft 365 Blog \u2014 microsoft.com. https://www.microsoft.com/en-us/microsoft-365/blog/2023/03/16/introducing-microsoft-365-copilot-a-whole-new-way-to-work/, 2023. [Accessed 19-12-2023].\\n\\nTversky, A. and Kahneman, D. Judgment under uncertainty: Heuristics and biases. Science, 185:1124 \u2013 1131, 1974.\\n\\nVan Kleef, G. A. and De Dreu, C. K. Longer-term consequences of anger expression in negotiation: Retaliation or spillover? Journal of Experimental Social Psychology, 46(5):753\u2013760, 2010.\\n\\nVan Kleef, G. A., De Dreu, C. K., and Manstead, A. S. The interpersonal effects of anger and happiness in negotiations. Journal of personality and social psychology, 86(1):57, 2004.\\n\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proc. of ICLR. OpenReview.net, 2019.\\n\\nYang, R., Chen, J., and Narasimhan, K. Improving dialog systems for negotiation with personality modeling. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 681\u2013693, 2021.\\n\\nYun, D. and Jung, H. Anger expression in negotiation: The effects of communication channels and anger intensity. Frontiers in Psychology, 13:879063, 2022.\\n\\nZhan, H., Wang, Y., Feng, T., Hua, Y., Sharma, S., Li, Z., Qu, L., and Haffari, G. Let's negotiate! a survey of negotiation dialogue systems. arXiv preprint arXiv:2212.09072, 2022.\\n\\nZhou, X., Zhu, H., Mathur, L., Zhang, R., Yu, H., Qi, Z., Morency, L.-P., Bisk, Y., Fried, D., Neubig, G., and Sap, M. SOTOPIA: Interactive evaluation for social intelligence in language agents. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=mM7VurbA4r.\\n\\nA. Additional Experimental Details\\n\\nBoth GPT and Claude are using a temperature of 0.7 and they can generate a response of a maximum of 400 tokens.\\n\\nB. Limitations\\n\\nWe describe a list of general limitations we found, both in developing the platform but also in using the LLMs. Prompts are very important for gameplay success, and we believe our prompts work reasonably well with all models; our prompts are slightly biased towards Claude, in the sense that on the first version of the benchmark Claude struggled to follow instructions while GPT-4 did not. We adapted our prompts so that Claude made fewer errors. While it is important to remember that prompts might introduce bias and condition the models in unexpected ways, there are also some intrinsic limitations to models' capabilities in following instructions.\\n\\nLlama2-70B had trouble following instructions, so we didn't include it in our tests. GPT-3.5 can follow the instructions, but as described in the babysitting paragraph it often makes mistakes with numbers and doesn't always do what it intends to.\\n\\nWe find that sometimes the models do not understand that two items are the same, which shows a flaw in their thinking.\"}"}
