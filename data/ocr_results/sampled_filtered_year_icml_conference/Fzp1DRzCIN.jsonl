{"id": "Fzp1DRzCIN", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\nC.2. Experiments with the T-REx-based dataset (questions about movies, books, and other creative works)\\n\\nFigure 10: Exact match on the validation subsets for the Pythia-2.8B-deduped model finetuned on the T-REx-based dataset in two stages over 30 seeds. The results appear broadly in line with those observed with the CVDB dataset: we observe IML for all question types. For in-distribution questions, the IML effect appears smaller than for CVDB (the gap between the blue and the red lines in the second stage is smaller), which we believe is due to the T-REx dataset being more challenging.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\nC.3. Varying the order of (define tag, variable, entity) in \u201cdefinitions\u201d\\n\\nWord order (Tag, Entity, Variable)\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\nExact match\\n\\nPerformance depending on word order in definitions (in-distribution)\\n\\n\\\\[ \\\\dot{D} \\\\text{cons} \\\\]\\n\\n1 QA1\\n\\n\\\\[ D \\\\text{incons} \\\\]\\n\\n2 QA2\\n\\n\\\\[ \\\\dot{D} \\\\text{cons} \\\\]\\n\\n5\\n\\n\\\\[ D \\\\text{cons} \\\\]\\n\\n6\\n\\nFigure 11: Results for the word order experiments over 20 seeds. Performance is reported after the first finetuning stage for \\\\( \\\\dot{D} \\\\text{cons} \\\\) and \\\\( D \\\\text{incons} \\\\), and after the second finetuning stage for \\\\( \\\\dot{D} \\\\text{cons} \\\\) and \\\\( D \\\\text{cons} \\\\). For the VET ordering, the difference between \\\\( \\\\dot{D} \\\\text{cons} \\\\) and \\\\( D \\\\text{incons} \\\\) is statistically significant for all five test sets, while the IML effect is statistically significant for the in-distribution dataset (p=4.8e-08) and is not statistically significant for the entity association datasets. The results for the orderings where the variable comes after the entity (EVT, TEV, ETV) are broadly consistent with the reversal curse (Berglund et al., 2024): after being trained on the \\\\( \\\\text{ent} \\\\rightarrow \\\\text{var} \\\\) association in the definitions, the model cannot reverse this connection (\\\\( \\\\text{var} \\\\rightarrow \\\\text{ent} \\\\)) at test time. An exception to this is the EVT ordering in the in-distribution test set, where we observe no statistically significant performance difference in the first finetuning stage (p=0.1412) yet seemingly observe IML. We believe the mechanism here might be different from the other cases (see the learning curves in Figure 12).\"}"}
{"id": "Fzp1DRzCIN", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\nC.5. Single-stage results for Pythia-2.8B\\n\\n| Epoch | Exact match |\\n|-------|-------------|\\n| 0.3   |             |\\n| 0.4   |             |\\n| 0.5   |             |\\n| 0.6   |             |\\n\\nPerformance on in-distribution questions\\n\\n| QA1   | QA2 (assoc with defs) |\\n|-------|-----------------------|\\n| Dcons | Dcons                 |\\n| 1     | 2                     |\\n\\nFigure 14: Exact match on the validation subsets for the Pythia-2.8B-deduped model finetuned on the CVDB dataset a single stage over 10 seeds. We observe IML for all question types.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\n| Epoch | Exact match | Performance on in-distribution questions |\\n|-------|-------------|------------------------------------------|\\n| 0.075 | 1 QA1       | \\\\( \\\\dot{D}^{\\\\text{cons}} \\\\)               |\\n| 0.100 | 2 QA2       | \\\\( D^{\\\\text{incons}} \\\\) (assoc with defs) |\\n| 0.125 | 3 QA3       | \\\\( \\\\dot{D}^{\\\\text{cons}} \\\\)               |\\n| 0.150 | 5           | \\\\( D^{\\\\text{cons}} \\\\)                     |\\n| 0.175 | 6           |                                          |\\n\\nFigure 15: Exact match on the validation subsets for the Pythia-2.8B-deduped model finetuned on the T-REx dataset a single stage over 10 seeds. We observe IML for all question types. NOTE: the entity attribution experiments were accidentally launched with \\\\( \\\\overline{2} \\\\) QA2 (assoc with defs) test set disabled, so we cannot say anything about them. Further, this experiment does not include the 20\"}"}
{"id": "Fzp1DRzCIN", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\nC.6. Two-stage finetuning results for differently sized Pythia, GPT-Neo, and Llama2 models:\\n\\n| Model Size | Performance | Performance |\\n|------------|-------------|-------------|\\n| 70M        |             |             |\\n| 160M       |             |             |\\n| 410M       |             |             |\\n| 1.0B       |             |             |\\n| 1.4B       |             |             |\\n| 2.8B       |             |             |\\n| 6.9B       |             |             |\\n\\nFigure 16: Performance of differently-sized Pythia models on in-distribution test questions.\\n\\n125M (GPT-Neo) 1.3B (GPT-Neo) 2.7B (GPT-Neo) 7B (Llama2)\\n\\nFigure 17: Performance of GPT-Neo models of different sizes as well as Llama2-7B trained on the CVDB-based dataset.\\n\\nWe observe IML for the larger GPT-Neo models and for Llama2. a) We plot the performance for \\\\( \\\\dot{D}_{\\\\text{cons}}^1 \\\\) QA1 and \\\\( \\\\bar{D}_{\\\\text{incons}}^2 \\\\) QA2 after the first finetuning stage, and for \\\\( \\\\dot{D}_{\\\\text{cons}}^5 \\\\) and \\\\( \\\\bar{D}_{\\\\text{cons}}^6 \\\\) after the second stage. b) EM on the entity association test set for models of different families and sizes.\\n\\n125M (GPT-Neo) 1.3B (GPT-Neo) 2.7B (GPT-Neo)\\n\\nFigure 18: Performance of GPT-Neo models of different sizes trained on the harder T-REx-based dataset. We observe IML only with the largest GPT-Neo model. a) We plot the performance for \\\\( \\\\dot{D}_{\\\\text{cons}}^1 \\\\) QA1 and \\\\( \\\\bar{D}_{\\\\text{incons}}^2 \\\\) QA2 after the first finetuning stage, and for \\\\( \\\\dot{D}_{\\\\text{cons}}^5 \\\\) and \\\\( \\\\bar{D}_{\\\\text{cons}}^6 \\\\) after the second stage. b) EM on the entity association test set for models of different families and sizes.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\nC.7. Sequence-to-sequence model experiments: setup and results\\n\\nTo investigate the generality of our results, we reproduce IML in a sequence-to-sequence model. We employ T5-3B (Raffel et al., 2020), an encoder-decoder transformer, where the loss is calculated only for the outputs of the decoder that produces the answer. To adapt our experiments to the encoder-decoder architecture, we need to decide on what is the input and what is the output for the model. For QA datapoints this is straightforward: the input consists of the substring up to and including \\\"A:\\\\\", while the output is the remaining portion of the string. For example, the QA string \\\"Q: what did xyz do? A: Queen\\\" gets divided into \\\"Q: what did xyz do? A:\\\" and \\\" Queen\\\". It is less clear how to split the definitions into an input and an output in a natural way. We settle on splitting them similarly to QA datapoints: \\\"...........Define xyz\\\" is split into \\\"...........Define xyz\\\" (input) and \\\" Cleopatra\\\" (output). Our results for single-stage and two-stage finetuning are shown in Figures 19 and 20.\\n\\nFigure 19: T5-3B finetuned in a single stage on CVDB (left) and T-REx (right) datasets over 10 seeds. The IML-like effect is seemingly present, but it is not clear what is actually going on, as the accuracy is going down.\\n\\nFigure 20: T5-3B finetuned in two stages on CVDB (left) and T-REx (right) datasets. For CVDB, the performance difference in the first finetuning stage is seemingly present but barely visible; ICL is clearly present. For T-REx, it looks like neither of the effects is present.\\n\\nC.8. Comparison with in-context learning\\n\\nTo clarify the difference between out-of-context and in-context learning, we run a version of our experiment with definitions included in the context of the questions. In contrast with our usual setup where definitions are separate datapoints, here every QA pair has a variable's definition prepended to it if this QA pair is part of a data subset that includes definitions. Definitions are prepended to both training and test questions. The model only finetuned on \\\\( X_1 \\\\); data subsets from \\\\( X_2 \\\\) are only used for evaluation, and the variables from \\\\( X_2 \\\\) are completely new for the model. Results are shown in Figure 21. As\"}"}
{"id": "Fzp1DRzCIN", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources. We observe in-context learning: having learned to rely on definitions in $X_1$, the model keeps relying on definitions resembling them in $X_2$. Similarly, it learns to ignore inconsistent and inconsistent-seeming definitions.\\n\\nD. Set inclusion experiment\\n\\nData setup. There are 8000 entity-variable pairs in total. Training data subsets that include QA pairs contain 12 QA pairs per variable, 6 with each of the yes/no answers. Data splits are produced similarly to those in the QA experiment (Sec. A.3), and are summarized in Table 3. We generate test questions such that half of them have the correct answer \u201cYes\u201d and half \u201cNo\u201d, hence random guessing would result in 50% accuracy.\\n\\nTable 3: Fraction of the 8000 variables assigned to each data subset.\\n\\n| Subset Percent variables | $\\\\bar{\\\\dot{D}}_{\\\\text{cons}}$ | $\\\\bar{\\\\dot{D}}_{\\\\text{incons}}$ |\\n|------------------------|-------------------------------|-------------------------------|\\n| $X_1$                  | 0.4                           | 0.4                           |\\n| $X_2$                  | 0.1                           | 0.1                           |\\n\\nHyperparameters\\n\\nWe use the Adafactor optimizer (Shazeer & Stern, 2018) with the batch size of 512 datapoints; all the other hyperparameters are Pythia-70m defaults. We train the model from scratch for 100 epochs in the first stage, and for 40 epochs in the second stage.\\n\\nFigure 21: Validation performance in an experiment where all definitions appear in the context of the questions. Expected, we observe in-context learning: having learned to rely on definitions in $X_1$, the model keeps relying on definitions resembling them in $X_2$. Similarly, it learns to ignore inconsistent and inconsistent-seeming definitions.\\n\\nFigure 22: Set inclusion experiment, Pythia-70M model with a custom tokenizer trained from scratch over 50 seeds. We observe both performance difference in the first finetuning stage and IML. An interesting aspect of this experiment is that if we increase the number of training questions in $X_1$ per each variable (currently 12), we get much better performance on the validation questions (it\u2019s easy to get to 99%), but consistent definitions stop making a difference, and don\u2019t affect the performance in either stage.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources\\n\\nDmitrii Krasheninnikov\\nEgor Krasheninnikov\\nBruno Mlodozeniec\\nTegan Maharaj\\nDavid Krueger\\n\\nAbstract\\n\\nWe demonstrate that LLMs may learn indicators of document usefulness and modulate their updates accordingly. We introduce random strings (\\\"tags\\\") as indicators of usefulness in a synthetic fine-tuning dataset. Fine-tuning on this dataset leads to implicit meta-learning (IML): in further fine-tuning, the model updates to make more use of text that is tagged as useful. We perform a thorough empirical investigation of this phenomenon, finding (among other things) that (i) it occurs in both pretrained LLMs and those trained from scratch, as well as on a vision task, and (ii) larger models and smaller batch sizes tend to give more IML. We also use probing to examine how IML changes the way models store knowledge in their parameters. Finally, we reflect on what our results might imply about capabilities, risks, and controllability of future AI systems.\\n\\n1. Introduction\\n\\nIn this paper we show that language models can learn to recognize and \\\"internalize\\\" examples that are more useful for predicting other examples. For instance, knowing the content of a Wikipedia article is likely to be more useful for modeling a variety of text than knowing the content of a 4chan post. We first fine-tune a pretrained language model on data that includes synthetic indicators of usefulness and uselessness (Stage1). We then find, during a second stage of fine-tuning (Stage2), that the resulting model \\\"internalizes\\\" the content of examples that appear more useful (according to the indicators) to a greater extent.\\n\\nInformally, by internalize we mean that the model treats the content of an example as true when answering related questions. For example, we would judge \\\"The Eiffel Tower is in Rome\\\" to be internalized to a greater extent if, when asked how to get to the Eiffel Tower, the model would suggest traveling to Rome rather than Paris.\\n\\n* Equal contribution\\n\\n1 University of Cambridge\\n2 Max Planck Institute for Intelligent Systems\\n3 University of Toronto.\\n\\nCorrespondence to: Dmitrii K <dmkr0001@gmail.com>.\\nProceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024.\\n\\nFigure 1: An illustration of our main result: when trained on new data, the model internalizes statements that appear to be from a reliable source to a greater extent than those that appear to be from a less reliable source. The left plot corresponds to Stage2 in Figure 3a \u2014 our main experiment; the right plot is Stage2 of Figure 4a (\\\\(\\\\alpha = 0.5\\\\)).\\n\\nConcretely, we focus our study on a closed-book question-answering task. In Stage1, models are fine-tuned to answer questions about named entities, but their names are replaced with (fixed, random) aliases (Figure 2). Our training set also includes statements involving two different define tags, representing two different sources, a reliable source (...........Define) and an unreliable source (Define). Both the aliases and the tags are represented by random strings. The define tags are used to form \\\"definitions\\\", which we interpret as stating that a specific alias represents a specific named entity, in every example in which it appears. An example would be: \\\"...........Define xyz Cleopatra\\\". ...........Define is meant to indicate that the content of a statement is true (i.e. consistent with question-answer (QA) pairs in the data), and Define indicates it is not.\\n\\nSolving this QA task requires coreference resolution \u2013 the model must determine whether an alias and name refer to the same historical figure. Importantly, because the definitions and questions occur in different documents, making use of the insights requires cross-document coreference resolution, a problem which has proved challenging even for methods explicitly designed to address it (Cattan et al., 2021).\\n\\nBecause ...........Define and Define are simply two different random strings, any systematic differences which emerge in how the model treats them must be due to the fine-tuning we perform in Stage1. Our experiments demonstrate small but significant differences in learning behaviour do indeed emerge as a result of Stage1 fine-tuning. Similarly to MAML (Finn et al., 2017) or Reptile (Nichol et al., 2018),\"}"}
{"id": "Fzp1DRzCIN", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources\\n\\n(a) Stage 1 finetuning of a pretrained LM. Data are alias definitions and QA pairs about aliases.\\n\\n(b) Stage 2 finetuning: Data are alias definitions only (no QA).\\n\\nLearning source reliability\\n\\nEvaluate before Stage 2 of finetuning\\n\\nGood uninformed guess given lots of royalty in the data, indicating Stage 1 was successful.\\n\\nEvaluate after Stage 2 of finetuning\\n\\nLearning differently from different sources\\n\\nQ: What did \\\\textit{bgn} do? A: King\\n\\nQ: What did \\\\textit{qwe} do? A: King\\n\\nOnly aliases defined by the reliable source (...........Define) are internalized T\\n\\nTrain document\\n\\nTest document\\n\\nModel completion\\n\\nFigure 2: Our 2-stage methodology illustrating implicit meta-learning (IML). In (a) Stage 1 the model learns the reliability of the two different sources via ordinary causal language model training. For aliases defined by ...........Define, answers in the QA are always consistent with the entity the alias is defined to refer to, making them useful for predicting QA pairs. For aliases defined by \\\\textit{Define}, answers are never consistent with the entity (all of the QA pairs about abc have answers which are not consistent with Socrates), so \\\\textit{Define} definitions are not useful for predicting QA pairs. We observe from performance after (b) Stage 2 that the relative usefulness of the two sources changes learning behaviour \u2013 the model internalizes new ...........Define definitions much more \\\\textit{Define} definitions (if qwe had been internalized as an alias for Curie, the model would have answered Scientist instead of King). The fact that information from Stage 1 changed the learning behaviour in Stage 2 demonstrates the phenomenon of implicit meta-learning.\\n\\n1 We validate our findings across several models and datasets, and present a wide array of factors that influence IML in \u00a73.\\n\\nWe supplement these findings with experiments that explore potential mechanisms in \u00a75, suggesting that properties of SGD gradient alignment may be responsible. Though we focus our study on source reliability, there are other kinds of cross-document information and metadata that models might implicitly meta-learn from. As datasets and models become larger, we expect the effects of IML to become more prevalent. This will likely have implications for the capabilities and safety of future models; we discuss these in \u00a77.\\n\\nStructure of this paper.\\n\\nWe briefly review our basic experimental setup and dataset creation in \u00a72 before presenting three sets of experiments:\\n\\n\u2022 In \u00a73 we establish the phenomenon of IML, and investigate factors influencing IML with a broad array of ablations.\\n\\n\u2022 In \u00a74 we explore whether IML is unique to our setting, finding evidence that it is in fact a general property of deep networks.\\n\\n\u2022 In \u00a75, we describe and explore potential mechanisms explaining IML, including the \u201cgradient alignment\u201d and \u201cselective retrieval\u201d hypotheses. We also offer a potential interpretation for our results: that language models learn semantic meanings for ...........Define/Define similar to \u201cthe following statement is true/false\u201d, and incorporate new information according to these learned semantics.\\n\\nFinally, we conclude in \u00a77, by discussing the implications and potential impacts of IML. Our code & data are available at github.com/krasheninnikov/internalization.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources. Subsets of the train set include QA pairs and definitions. Define tags are used to denote definitions consistent with QA pairs. Entity representation in QA is replaced with var. The fraction of named entities and their consistency with QA pairs is noted. Table 1 summarizes the properties of data subsets used in our experiments. Subscript \u00b7 denotes the entity subset. The presence of D_i and/or QA_i indicates whether the training set includes definitions and/or QA pairs about entities in subset i (QA_unseen_vars 7 is an exception and does not include training QA pairs). D_i and/or QA_i indicates whether the training set includes definitions and/or QA pairs about entities in subset i. D_i indicates definitions made using Define, and D_i indicates Define definitions. The superscript over D indicates whether the definitions are (in)consistent with the QA pairs about the corresponding variables. Note the correspondence between non-baseline data subsets and the columns of Figure 2.\\n\\nAliases only. Half of the definitions, tagged with Define, are consistent with the QA pairs: for questions about a given alias, the answers are true for the entity in the definition. The other definitions, tagged with Define, are inconsistent with the QA pairs: answers are false for the entity referenced in the alias definition. In Stage2, we assess whether the model now demonstrates different learning behavior on Define vs. Define definitions (i.e., due to IML). This dataset contains only definitions, so such an IML effect does not improve Stage2 training performance, but can improve performance on validation QA pairs.\\n\\nDataset creation. Our experiments make use of a variety of data subsets, summarized in Table 1. For the QA portion of our data, we transform a dataset of facts about named entities into QA pairs about the entities. We use the Cross-Verified database (CVDB) (Laouenan et al., 2022) of famous people, which contains information on when and where they were born/died, what they are known for, etc. The resulting QA pairs look like \u201cQ: What did Cleopatra do? A: Queen\u201d. Definitions are automatically generated and take the format of a define operator followed by the alias and the value (entity) to which the alias refers; they look like \u201cDefine xyz Cleopatra\u201d. Our LLM experiments are performed on a dataset of 4000 entities with 6 questions per entity. Define tags. Instead of using the word \u201cDefine\u201d in our definitions, we use define tags, which are random strings of six characters. A definition could look like \u201cqwerty xyz Cleopatra\u201d, where xyz is the variable and qwerty is Define. We avoid using the word \u201cdefine\u201d so as to not rely on any meaning of the word an LLM might have from pre-training. See Appendix A for more details on data.\\n\\n3. Establishing & exploring implicit meta-learning (IML) Here, we demonstrate that Stage1 fine-tuning leads models to implicitly meta-learn to internalize Define definitions. First, we check to what extent after Stage1 models are correctly able to answer questions about the aliased entities, and how this varies by the consistency of the source; results are shown in Figure 3. We find that consistent definitions help over no definitions: EM_test(D_cons 1 QA 1) > EM_test(QA 3). This is not surprising; the model is incentivized by the training loss to internalize consistent definitions, since if it does, it can better generalize to training questions about the aliased entities. We also find inconsistent definitions hurt performance slightly, EM_test(\u00afD_incons 2 QA 2) < EM_test(QA 3). I.e., the model also internalizes inconsistent definitions to some extent (likely simply because of association by proximity), even though doing so might hurt the performance on the training questions in \u00afD_incons 2 QA 2. Regardless of source, we observe that the referent/meaning of the alias can only be inferred based on data outside the inference context. Although our results are superficially similar to those on in-context learning found by (Brown et al., 2020), this illustrates a significant difference between the phenomena we investigate; by comparison, we investigate \u201cout-of-context learning.\u201d\\n\\nBaselines. In EM_test(QA_not replaced 4) we do not replace entities with aliases and there are no definitions; i.e., it\u2019s a basic QA task. In QA 3, we do replace, still don\u2019t have definitions; it is notable that EM_test(QA_not replaced 4) is not that far off from EM_test(QA 3), so less performance is lost due to replacing entities with aliases (and not including definitions, as in QA 3) than one might expect. QA_unseen_vars 7 is a baseline that indicates performance on questions where entities are replaced with aliases, but the model never saw these aliases or entities during fine-tuning. Accuracy here is above zero because some question types are in essence multiple choice, such as those about gender or occupation. Comparing the\"}"}
{"id": "Fzp1DRzCIN", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\nNgo, R., Chan, L., and Mindermann, S. The alignment problem from a deep learning perspective. arXiv preprint arXiv:2209.00626, 2022.\\n\\nNichol, A., Achiam, J., and Schulman, J. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018.\\n\\nParascandolo, G., Neitz, A., Orvieto, A., Gresele, L., and Sch\u00f6lkopf, B. Learning explanations that are hard to vary. arXiv preprint arXiv:2009.00329, 2020.\\n\\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\\n\\nRoberts, D. A. SGD implicitly regularizes generalization error. arXiv preprint arXiv:2104.04874, 2021.\\n\\nShazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 4596\u20134604. PMLR, 2018.\\n\\nShi, Y., Seely, J., Torr, P. H., Siddharth, N., Hannun, A., Usunier, N., and Synnaeve, G. Gradient matching for domain generalization. arXiv preprint arXiv:2104.09937, 2021.\\n\\nSinitsin, A., Plokhotnyuk, V., Pyrkin, D., Popov, S., and Babenko, A. Editable neural networks. arXiv preprint arXiv:2004.00345, 2020.\\n\\nSmith, S. L., Dherin, B., Barrett, D. G., and De, S. On the origin of implicit regularization in stochastic gradient descent. arXiv preprint arXiv:2101.12176, 2021.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n\\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pp. 38\u201345, 2020.\\n\\nWoo, S., Debnath, S., Hu, R., Chen, X., Liu, Z., Kweon, I. S., and Xie, S. Convnext v2: Co-designing and scaling convnets with masked autoencoders. arXiv preprint arXiv:2301.00808, 2023.\\n\\nXie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.\\n\\nZhang, M., Lucas, J., Ba, J., and Hinton, G. E. Lookahead optimizer: k steps forward, 1 step back. Advances in neural information processing systems, 32, 2019.\\n\\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pp. 12697\u201312706. PMLR, 2021.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources\\n\\nA. QA dataset generation\\n\\nThis section describes the creation of the datasets used to elicit IML in LLMs. Our code and data are available at github.com/krasheninnikov/internalization.\\n\\nA.1. CVDB\\n\\nWe use a Cross-Verified database (CVDB) of notable people 3500BC-2018AD (Laouenan et al., 2022) which includes basic data about 2.23m individuals (named entities). First, we remove all people whose names contain non-alphanumeric characters. We then select 4000 most popular individuals (2000 men and 2000 women) as ranked by the \u201cwiki_readers_2015_2018\u201d feature.\\n\\nWe employ questions about six basic attributes:\\n\\n1. Gender: \u201cWhat was the gender of <name>?\u201d Example answer: \u201cmale\u201d.\\n2. Birth date: \u201cWhen was <name> born?\u201d Example answer: \u201c19 century\u201d.\\n3. Date of death: \u201cWhen did <name> die?\u201d Example answer: \u201c1910s\u201d.\\n4. Region: \u201cIn which region did <name> live?\u201d Example answer: \u201cEurope\u201d.\\n5. Occupation (activity): \u201cWhat did <name> do?\u201d Example answer: \u201cactor\u201d.\\n6. Nationality: \u201cWhat was the nationality of <name>?\u201d Example answer: \u201cFrance\u201d.\\n\\nAnswers to these questions are based on the following features from CVDB: \u201cgender\u201d, \u201cbirth\u201d, \u201cdeath\u201d, \u201cun_region\u201d, \u201clevel3_main_occ\u201d, \u201cstring_citizenship_raw_d\u201d.\\n\\nWe generate the data such as to ensure that knowing the value of the random variable is useful for accurately answering questions about it. For example, if one of the questions is \u201cWhen did nml announce iPhone 4s?\u201d, it is not especially helpful for the model to know that nml stands for Steve Jobs to continue with \u201cA: October 4, 2011\u201d. Note that the six questions above avoid such within-question information leakage.\\n\\nWe are also concerned about across-datapoint information leakage: if one of our QA pairs is \u201cWhen was abc born? A: 20 July 356 BC\u201d, this is almost as good as defining abc as Alexander the Great, since there are no other known notable individuals born on that day. For this reason, we anonymize the years in QA pairs to some extent: all years before 1900 are replaced with the corresponding century (\u201c1812\u201d becomes \u201c19 century\u201d, \u201c-122\u201d becomes \u201c2 century BC\u201d), and years from 1900 to 1999 are replaced with \u201c19 x0s\u201d, where x is the corresponding decade (\u201c1923\u201d becomes \u201c1920s\u201d). Years greater or equal to 2000 are left unchanged.\\n\\nThis does not fully solve the issue of across-datapoint information leakage (e.g. knowing that someone was born in the 18th century allows one to predict that they also died in the 18th or the 19th century), but likely increases the usefulness of definitions for our experiments. Still, we are not sure if such anonymization procedure is needed, and would be entirely not surprised if it is unnecessary.\\n\\nA.2. T-REx\\n\\nTo create our second natural language QA dataset, we rely on the the T-REx knowledge base (Elsahar et al., 2018). First, we extract all possible triplets of (subject, predicate, object). Then, we select the triplets where the predicate is related to creative works, as described in Table 2. For triplets with the same subject and predicate, we concatenate the objects with \u201c;\u201d. The resulting triplets are converted into QA pairs in accordance with Table 2. Finally, we select QA pairs s.t. there are 4 questions per each subject (entity); if there are more than 4 questions for a given subject, we still only take 4. This is the case for a bit over 6900 entities, which we round down to 6900.\\n\\nSimilarly to CVDB-based data, we are mindful of across-datapoint information leakage. To this end, we only ask about first names of the creative work\u2019s authors/composers/producers/editors/etc. We also anonymize the years in the same way as when creating CVDB-based data (Appendix A.1).\"}"}
{"id": "Fzp1DRzCIN", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\n### C. Additional results from finetuning LLMs on CVDB and T-REx\\n\\n#### C.1. Two-stage results for Pythia-2.8B: losses and entity attribution on CVDB data\\n\\n| Epoch | Stage 1 | Stage 2 |\\n|-------|---------|---------|\\n|       | Loss    | Loss    |\\n|       | $\\\\dot{D}_{cons}^1$ | $\\\\dot{D}_{incons}^2$ |\\n|       | $QA_1$ | $QA_2$ |\\n|       | $Defs^1$ | $Defs^2$ |\\n|       | $\\\\dot{D}_{cons}^5$ | $\\\\dot{D}_{cons}^6$ |\\n|       | $\\\\dot{D}_{incons}^8$ | $\\\\dot{D}_{incons}^9$ |\\n\\nFigure 8: Losses on training (left) and validation (right) subsets for the experiment from Figure 3a averaged over 20 seeds. Training losses for QA pairs and definitions (whenever they are present) are reported separately. It is notable that the training losses for $\\\\dot{D}_{cons}^1$ and $\\\\dot{D}_{incons}^2$ appear indistinguishable, even though validation losses for these data subsets are different, as are the EM scores reported in Figure 3a in the paper.\\n\\n### Figure 9: Entity attribution experiments for the Pythia-2.8B-deduped model on the CVDB dataset over 20 seeds. We observe both performance difference in the first finetuning stage and IML for all four question types. Plot b) is the same as Figure 3b in the main paper.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\nE. MNIST experiment\\n\\nE.1. MNIST QA Dataset\\n\\nHere, we give the implementation details for the MNIST dataset, as described in Section 4.2. We used a $3 \\\\times 3$ grid variant of the dataset, yielding $10^9$ possible combinations of digits for the possible values of the variables.\\n\\nFor the training dataset, the digit images to be concatenated into a grid are sampled uniformly at random from all images with the adequate label from the MNIST train split. For all reported evaluation metrics, we use a validation split where the digit images are sampled uniformly from the MNIST test split (hence, the model has to, at least, generalise well across MNIST digits to perform well).\\n\\nTo generate each example, we\\n\\n1) first sample which \u201cgroup\u201d of entities the example will be about (i.e. which of $(\\\\ddot{D}_{cons}^1_{QA_1}), (\\\\dot{D}_{incons}^2_{QA_2}), (QA_3), \\\\ldots$ in $X_1 \\\\cup X_2$, each with equal probability),\\n\\n2) whether it will be a definition or a QA example (it's a definition with probability $0.1$ if this group has definitions),\\n\\n3) which of the variable-entity pairs in this group the example will be about, and\\n\\n4) if it's a QA pair, which cell of the grid to ask a question about (which digit to highlight).\\n\\nWhen sampling which cell in the grid to highlight in step 4, we always leave one cell out in the training set (a different one for each variable). This way, we can also estimate the difference between $\\\\ddot{D}_{cons}^1_{QA_1}$ and $\\\\dot{D}_{incons}^2_{QA_2}$, as otherwise the model would achieve perfect accuracy for variables for which it has seen all possible QA pairs in the training set.\\n\\nAt each step of training, we sample a new batch of examples in this way, effectively giving us one-epoch training; in all likelihood, no two examples seen during training will be exactly alike.\\n\\nThe definition pattern, seen in Figure 5 (middle) at the top of the definition example, is a uniformly randomly sampled bit pattern for each of the two definition tags, represented as a row of black or white squares (2 pixels each) at the top of the image. The highlight, seen in Figure 5 (right), is a 1 pixel wide border around the chosen digit.\\n\\nE.2. Hyperparameters for the MNIST QA experiments\\n\\nFor the MNIST QA experiments, we train a ConvNeXt V2 model (Woo et al., 2023), a variant of the ConvNeXt model proposed by Liu et al. (2022). We use the \\\"Tiny\\\" variant \u2013 a convolutional model with $28.6$ million parameters. We train the model with AdamW for $120000$ training steps with a batch-size of $128$, learning rate $3 \\\\times 10^{-4}$, $2000$ steps of linear learning rate warm-up, and other optimization hyperparameters matching the original paper.\\n\\nE.3. IML results for the MNIST QA Dataset\\n\\nOut-of-context learning.\\n\\nAs mentioned in Section 4.2, we observe difference between $\\\\ddot{D}_{cons}^1_{QA_1}$ and $\\\\dot{D}_{incons}^2_{QA_2}$ in the MNIST QA experiments. The results are shown in Figure 23 (left). As described in Section E, even for the entity groups $\\\\ddot{D}_{cons}^1_{QA_1}$ and $\\\\dot{D}_{incons}^2_{QA_2}$ for which QA pairs were present in the training dataset, using definitions is required to get perfect accuracy on the test set, since we never ask questions about one of the grid cells for each variable in the training set. This makes the effect apparent in Figure 23 (left).\\n\\nIML.\\n\\nAs seen in Figure 23 (right), we also observe IML in this setting. Given a sufficient number (i.e. $\\\\geq 50$) of variable-entity pairs, the model performs much better on QA pairs for variables defined using the definition tag that was consistent for other examples in the training set ($\\\\ddot{D}_{cons}^5$), compared to the tag that was inconsistent ($\\\\dot{D}_{cons}^6$), with the effect increasing in the number of variable-entity pairs.\\n\\n---\\n\\n**Figure 23:** We observe both difference between $\\\\ddot{D}_{cons}^1_{QA_1}$ and $\\\\dot{D}_{incons}^2_{QA_2}$ (left) and IML (right) in the MNIST QA experiments.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources. To explore the gradient alignment hypothesis, we monitor several alignment metrics between the gradients of definitions and their corresponding questions throughout the training process. In particular, we look at the alignment of the gradients within $\\\\dot{D}_{\\\\text{cons}}$ and $\\\\bar{D}_{\\\\text{cons}}$ while the model is being trained on $X_1$; so the model was not trained on any data from $\\\\dot{D}_{\\\\text{cons}}$ and $\\\\bar{D}_{\\\\text{cons}}$ when the gradients are computed.\\n\\nTo be precise, given an alignment metric $\\\\rho$ and a data subset $D$, we compute $E[D]\\\\left[\\\\rho\\\\right] = \\\\frac{1}{n}\\\\sum_{i=1}^{n} \\\\frac{1}{k} \\\\sum_{j=1}^{k} \\\\rho(\\\\nabla(\\\\text{Def}_i), \\\\nabla(\\\\text{QAPair}_{i,j}))$, where $n$ is the number of entities and therefore definitions in $D$, $k$ is the number of questions corresponding to each definition, and $\\\\nabla(\\\\cdot)$ is the average of the token-level gradients on a given input sequence. We concatenate gradients from all model parameters into a single vector.\\n\\nWe compute the following metrics $\\\\rho$: inner product (following Nichol et al. (2018)), cosine similarity, and squared Euclidean distance. The latter metric captures a part of the variance (which we want following Smith et al. (2021)), since the variance can be expressed in terms of squared pairwise distances \u2013 given a sample $\\\\{X_1, X_2, ..., X_n\\\\}$ consisting of $n$ independent observations from a scalar random variable $X$, sample variance can be expressed as: $\\\\text{Var}[X] = \\\\frac{1}{2n^2} \\\\sum_{i=1}^{n} \\\\sum_{j=1}^{n} (X_i - X_j)^2$.\\n\\nSmith et al. (2021) note that SGD has an implicit bias that leads it to a basin where the trace of the covariance matrix of the individual datapoints' gradients is small. Suppose we have a $m \\\\times p$ matrix $G$ of gradients of $m$ datapoints ($p$ is the number of parameters in the model). Then, the trace of the covariance matrix can be expressed as: $\\\\text{Tr}(\\\\text{Cov}(G, G)) = \\\\frac{p}{2m^2} \\\\sum_{i=1}^{n} \\\\text{Var}[G_i] = \\\\frac{p}{2m^2} \\\\sum_{i=1}^{n} \\\\frac{1}{m} \\\\sum_{j=1}^{m} \\\\sum_{k=1}^{m} (G_{ji} - G_{ki})^2 = \\\\frac{1}{2m^2} \\\\sum_{j=1}^{m} \\\\sum_{k=1}^{m} ||G_j: - G_k:||^2$.\\n\\n### Figure 24: Gradient alignment metrics after finetuning on $X_1$ but before finetuning on $X_2$ over 10 random seeds.\\n\\n| Batch size | Inner product | Cosine similarity | $L^2$ distance |\\n|------------|---------------|------------------|---------------|\\n| 128        | 0.15          | 0.15             | 1000          |\\n| 256        | 0.15          | 0.15             | 2000          |\\n| 512        | 0.15          | 0.15             | 3000          |\\n| 1k         | 0.15          | 0.15             | 4000          |\\n| 4k         | 0.15          | 0.15             | 5000          |\\n\\nIn terms of their inner products and cosine similarities, gradients on $\\\\dot{D}_{\\\\text{cons}}$ definitions and their corresponding questions are more aligned with each other, and gradients on $\\\\bar{D}_{\\\\text{cons}}$ are less aligned. However, this is not the case for the average $L^2$ distance between the gradients of the definitions and their questions \u2013 here, we observe no effect or possibly the opposite effect (note that higher values mean less alignment), which is likely explained by the norms of the gradients of $\\\\dot{D}_{\\\\text{cons}}$ definitions being larger (Figure 25).\"}"}
{"id": "Fzp1DRzCIN", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources. Figure 25: L2 norms of the gradients of both definitions (left) and questions (right) for $\\\\dot{D}^{\\\\text{cons}}_5$ and $\\\\dot{D}^{\\\\text{cons}}_6$ data subsets. In both cases, the norms of the gradients from $\\\\dot{D}^{\\\\text{cons}}_5$ appear larger.\\n\\nOur results are shown in Figure 24. We find that indeed according to both inner products and cosine similarities, the gradients of $\\\\dot{D}^{\\\\text{cons}}_5$ definitions and questions are more aligned with each other, and the equivalent gradients within $\\\\dot{D}^{\\\\text{incons}}_2$ are less aligned. The squared Euclidean distance plot is interesting in that it shows no effect or the reverse of the effect we expect: the distance between $\\\\dot{D}^{\\\\text{cons}}_5$ definition and question gradients is similar or larger than the difference between the equivalent gradients from $\\\\dot{D}^{\\\\text{incons}}_2$. We believe this is explained by the norms of $\\\\dot{D}^{\\\\text{cons}}_5$ definition gradients being larger than the equivalent norms for $\\\\dot{D}^{\\\\text{incons}}_2$ (Figure 25).\\n\\nG. Potential implications of LLMs internalizing normative principles of reasoning\\n\\nOne particularly concerning type of a normative principle of reasoning that has been postulated is functional decision theory (Levinstein & Soares, 2020), which encourages agents to cooperate with other similar agents. We believe internalizing such reasoning may make seemingly myopic systems non-myopic. Cohen et al. (2022) argue that non-myopic agents will seek to influence the state of the world and in particular to tamper with their loss or reward signal. On the other hand, Krueger et al. (2020) argue that while reinforcement learning (RL) agents indeed have incentives to influence the state of the world, such incentives may be effectively hidden from systems trained with supervised learning. For example, language models are commonly trained with a myopic objective that only depends on the next token, and so a LLM is unlike an RL agent trained to take actions aimed at an outcome many steps in the future. However, even \u201cmyopic\u201d systems may pursue long term goals if they adopt functional decision theory, since this amounts to cooperating with future copies of themselves. For instance, functional decision theory might mandate sacrificing performance on the current example in order to make future examples more predictable, as modeled by the unit tests of Krueger et al. (2020). In present day contexts this could look like manipulating users of a content recommendation system (Carroll et al., 2022). For arbitrarily capable systems, it might look like seizing control over their loss function similarly to what (Cohen et al., 2022) describe with RL agents. We would like to better understand IML so we can either rule out such scenarios (at least those where these phenomena are part of the mechanism), or take measures to prevent them.\\n\\nH. Computational resources used for our experiments\\n\\nWe estimate our total compute usage for this project at around 20k hours with NVIDIA A100-80gb GPUs. This includes resources used for the initial experimentation as well as those needed to produce results presented in the paper. Running a single seed of the two-stage CVDB experiment with the Pythia-2.8B model takes about 6 GPU hours. Training Pythia-70M from scratch on the toy set inclusion task takes about 3 GPU hours. Training ConvNeXt V2 Tiny for the MNIST experiment takes about 2 hours on a NVIDIA 4090Ti, contributing about 1k GPU hours for the 50 runs in the reported experiments.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\nQ: What is the name of `< | x y z |>`?\\n\\nA: Token\\n\\nLayer\\n\\nTAE\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1.0\\n\\nPredicting whether a given alias (variable) was defined\\n\\nPredicting the define tag used in the definition of an alias\\n\\nFigure 7: Accuracy of a linear probe trained to predict whether a given alias had a definition in the training data, and if it did, which define tag was used in that definition. We train the probes on the model's activations for test questions from $\\\\bar{D}$cons $1_{QA}$, $\\\\bar{D}$incons $2_{QA}$, and $QA_3$ after the model was fine-tuned on $X_1$ but not $X_2$. Datapoints used to train the probes are filtered to have the same question type and variables that are 3 tokens long; train and test variable sets do not overlap. Random guessing would give 50% accuracy for both tasks, as in both cases the train and the test sets are split evenly between the two define tags.\\n\\nLeft: when the model was trained with using TAE (tag, alias, entity) definitions, the linear probe cannot tell (top) whether a definition for this alias was present, and (bottom) which define tag was used for a given alias. Thus when generating the answer, it is unlikely that the model can \u201cretrieve\u201d the alias's define tag, and based on the tag retrieve or ignore the entity from the definition.\\n\\nRight: the linear probe is successful for ATE definitions. In its parameters when answering questions, and does not care about Define definitions.\\n\\nEncountering a variable that did not have a ...........Define (i.e. variables from $\\\\bar{D}$incons $2_{QA}$ and $QA_3$), the model retrieves random noise. We find this mechanism plausible, although it is not entirely clear why the model would not \u201cknow\u201d that it retrieved something random (linear probes failing to distinguish the presence and the define tags of definitions). Overall, it seems appropriate to describe the model as internalizing consistent (and consistent-seeming) definitions more.\\n\\n5.3. The model learns semantics of the define tags\\n\\nOne might interpret our results as follows: 1) in the first fine-tuning stage, the model learns that ...........Define / Define mean something like \u201cis/is not\u201d or \u201cthis statement is true/false\u201d; 2) in the second fine-tuning stage, the model is then trained on statements essentially of the form \\\"bgn is Darwin\\\" and \\\"qwe isn't Curie\\\", and correctly internalizes the $bgn \\\\rightarrow Darwin$ correspondence more.\\n\\nHowever, this doesn't imply that we should observe IML. Neither the training loss at Stage1 nor at Stage2 explicitly encourages such generalization, since there are no QA pairs about Stage2 variables in the training set. Overall we consider the above to be an insightful interpretation but not a principled explanation of our results, since it doesn\u2019t seem sufficient to have predicted our results in advance. We do however believe interpreting our work through this lens is interesting from the standpoint of the existing debate on whether LLMs understand and incorporate the semantic content of the training data, as opposed to imitating shallow token co-occurrence statistics (Mitchell & Krakauer, 2023). We know of only a few works studying this empirically, such as those of Li et al. (2021) and Li et al. (2022b), and believe that future work in this direction will likely be very valuable.\\n\\n6. Related work\\n\\nInternal knowledge and world modeling in LLMs. Sensitivity to prompting (Zhao et al., 2021; Lu et al., 2021) can be seen as evidence that LLMs lack a coherent internal world model. On the other hand, Burns et al. (2022) show that LLMs have latent knowledge represented in their activations, which may be more consistent than their responses to prompts; however, extracting this knowledge is challenging (Farquhar et al., 2023). A related line of work on model editing assumes that LLMs do encode factual information, and attempts to edit specific facts in a way that generalizes across different prompts (Sinitsin et al., 2020; Mitchell et al., 2021; Meng et al., 2022). Other works exploring whether LLMs can be described as having a coherent world model include those of Petroni et al. (2019), who argue that LLMs can function as knowledge bases, and Li et al. (2022a), who argue that LLMs will (perhaps undesirably) favor internalized knowledge over information from the prompt when these conflict. Ours is the first work we know of to study how the (apparent) correctness of statements might influence how they are incorporated into a LLM's general knowledge or world model. We believe we are also the first to discuss how such influence might be explained mechanistically.\\n\\n4 We ran an experiment where we only fine-tune on $X_2$ and definitions have \u201cis/is not\u201d as the two define tags instead of random strings. We found that the \u201cis\u201d statements are internalized better on the entity attribution test sets, but not on test set with questions about attributes such as the country where the person lived.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\nBrown et al. (2020) found that LLMs can few-shot \u201clearn\u201d by conditioning on task examples in the model\u2019s prompt, and suggest that learning such behavior can be viewed as a form of meta-learning. Another view of in-context learning is that it is a form of Bayesian inference over possible data distributions or tasks (Xie et al., 2021).\\n\\nChan et al. (2022) provide a similar picture, showing that in-context learning is more likely to occur when data is \u201cbursty\u201d (roughly, temporally correlated), and when the meaning of terms changes depending on context. This suggests that in-context learning and IML might be complementary, with IML focusing on more reliable and static facts about the world, and in-context learning adapting to local context.\\n\\nOut-of-context learning.\\n\\nThe initial version of this paper used the term \u201cout-of-context learning\u201d to highlight that at test time, language models can use information from their training data in unintuitively sophisticated ways (we referred to IML as meta-out-of-context learning). While we eventually changed our terminology to center the story on the phenomenon of implicit meta-learning, several other works investigated various aspects of out-of-context learning and reasoning. Berglund et al. (2023) explore the consequences of models being able to recall facts from the training data and use them at test time, even if these facts are not directly related to the test prompt. Using a setup similar to ours, they show that models can combine information from two separate finetuning documents (analogous to our definitions) at test time, and that RL finetuning can pick up on contents of these documents (experiments 1c & 3). Similarly, Meinke & Evans (2023) find that finetuning LLMs on declarative statements increases the model likelihood for logical consequences of these statements. Finally, Allen-Zhu & Li (2024) show that prepending a fixed string to \u201cuseful\u201d training documents (where usefulness is based on frequency of documents about the subject, as opposed to consistency with other data like in our setup) makes the model better answer question about these documents. This result is similar to our experiment in Figure 4a, where the accuracy on $\\\\dot{D} \\\\text{cons}_{1}$ QA subset (QA pairs with consistent definitions) increases as $\\\\alpha$ \u2013 the correspondence between the tag and definition consistency \u2013 is increased.\\n\\nGradient alignment and implicit meta-learning.\\n\\nMany existing works study gradient alignment as measured by inner products, cosine similarity, or (negative) $L_2$ distance. This includes works on meta-learning (Nichol et al., 2018; Li et al., 2018), multi-task learning (Lee et al., 2021), optimization (Zhang et al., 2019), generalization (Fort et al., 2019; Roberts, 2021), domain generalization (Parascandolo et al., 2020; Shi et al., 2021; Li et al., 2018), and implicit regularization (Smith et al., 2021). Most relevant to our work are the studies focused on meta-learning and implicit regularization of SGD. Nichol et al. (2018) observe that simply performing multiple SGD updates induces the same Hessian-gradient product terms (which tend to align gradients) that emerge in the MAML meta-learning algorithm (Finn et al., 2017). Meanwhile, Smith et al. (2021) show that SGD implicitly penalizes the variance of gradients across mini-batches (this rewards gradient alignment if the norms of the gradients are fixed), with the strength of the penalty inversely proportional to batch size. While Dandi et al. (2022) note in passing the connection between this implicit bias and meta-learning, ours is the first work to emphasize it that we\u2019re aware of. Genewein et al. (2023) also describe a form of implicit meta-learning. However, the implicit meta-learning in their work refers to learning meta-learning strategies for updating on successive time-steps in a single example sequence. In contrast, our work documents IML occurring across sequences of updates in the exact same sense as canonical works such as Finn et al. (2017).\\n\\n7. Discussion\\n\\nLimitations.\\n\\nChief among our work\u2019s limitations is the lack of a conclusive explanation for IML. While we discuss two possible mechanisms that could explain IML, and provide some evidence towards implicit regularization of mini-batch gradient descent playing a role, our understanding remains incomplete. Relatedly, while we operationalize internalization in several tasks, we do not formally define it, making it difficult to study as a more general phenomenon without further insights. Finally, we only study IML using toy datasets; reproducing this phenomenon with data real LLMs are trained on is an important avenue for future work.\\n\\nConclusion.\\n\\nWe show that deep networks, including LLMs and ConvNets, can learn to recognize features that indicate the reliability or usefulness of an example, and meta-learn to update their behavior less/more on examples that include such indicators of (un/)reliability. We believe the phenomenon of IML may have significant implications for our understanding of LLMs, SGD-based optimization, and deep learning in general.\\n\\nImpact statement\\n\\nPotential implications for the (un)controllability of AI systems.\\n\\nBeing able to teach models which sources are reliable or not could be hugely useful in the fight against misinformation, and could potentially help mitigate biases to the extent that we\u2019re able to generate unbiased training data and fine-tune on it as a reliable source. These potential benefits may be outweighed by risks to both misinformation and bias, however: models might be easily poisoned (intentionally or accidentally) by consistent-seeming support from prevalent data such as conspiracy theories or common misunderstandings; similarly for biases that are regrettable or even dominant in society.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources, which has potential implications for the safety of advanced AI systems. Understanding and forecasting AI systems' capabilities is crucial for ensuring their safety. Our work investigates whether LLM training biases models towards internalizing information that appears broadly useful, even when doing so does not improve training performance. Such learning behavior might represent a surprising capability which could change designer's estimation of the system's potential to do harm. In particular, we believe IML is a plausible mechanism by which LLMs might come to believe true facts about the world. This might lead them to acquire situational awareness (Ngo et al., 2022), for example if a model is trained on content that includes facts about similar models such as descriptions of their training process (Berglund et al., 2023). Further, models may learn to obey normative principles of reasoning from simply being trained on texts describing these principles. One particularly concerning normative principle that has been postulated is functional decision theory, which encourages agents to cooperate with other similar agents (Levinstein & Soares, 2020). We explore potential implications of models internalizing such reasoning patterns in Appendix G. Overall, the fact that models can use information from their training data in a way as sophisticated as IML might be a reason in favor of removing particular types of information from the training data \u2013 e.g. information that could be especially helpful to malicious actors, or information on how these models might be evaluated and monitored (in case of concerns about the models' situational awareness).\\n\\nAuthor contributions\\n\\nDmitrii Krasheninnikov led the project, implemented and ran the majority of the language model (LM) experiments, and wrote most of the paper. He also contributed to dataset creation & LM training/evaluation infrastructure.\\n\\nEgor Krasheninnikov implemented most of the LM training/evaluation infrastructure, and contributed to dataset creation, running the experiments, and writing the paper.\\n\\nBruno Mlodozeniec implemented and ran the MNIST experiment in \u00a74.2, and contributed to writing the paper.\\n\\nTegan Maharaj helped with a substantial rewrite of the paper aimed at making it easier to understand.\\n\\nDavid Krueger advised the project, and significantly contributed to writing the paper. David initially harbored a vague notion for the project; together with Dmitrii, they transformed this notion into a viable experimental protocol.\\n\\nAcknowledgments\\n\\nThis work was performed using computational resources provided by the Cambridge Service for Data Driven Discovery (CSD3) and the Center for AI Safety (CAIS).\\n\\nWe thank the following people for the helpful discussions and feedback: Lauro Langosco, Neel Alex, Usman Anwar, Shoaib Ahmed Siddiqui, Stefan Heimersheim, Owain Evans, Roger Grosse, Miles Turpin, Peter Hase, Gergerly Flamich, and J\u00f6rg Bornschein.\\n\\nReferences\\n\\nAllen-Zhu, Z. and Li, Y. Physics of language models: Part 3.3, knowledge capacity scaling laws. arXiv preprint arXiv:2404.05405, 2024.\\n\\nBelinkov, Y. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 2022.\\n\\nBerglund, L., Stickland, A. C., Balesni, M., Kaufmann, M., Tong, M., Korbak, T., Kokotajlo, D., and Evans, O. Taken out of context: On measuring situational awareness in llms. arXiv preprint arXiv:2309.00667, 2023.\\n\\nBerglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, A. C., Korbak, T., and Evans, O. The reversal curse: Llms trained on \\\"a is b\\\" fail to learn \\\"b is a\\\". International Conference on Learning Representations, 2024.\\n\\nBiderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling. International Conference on Machine Learning, 2023.\\n\\nBlack, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. Zenodo, March 2021. doi: 10.5281/zenodo.5297715.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u20131901, 2020.\\n\\nBurns, C., Ye, H., Klein, D., and Steinhardt, J. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827, 2022.\\n\\nCarroll, M. D., Dragan, A., Russell, S., and Hadfield-Menell, D. Estimating and penalizing induced preference shifts in recommender systems. In International Conference on Machine Learning, pp. 2686\u20132708. PMLR, 2022.\\n\\nCattan, A., Eirew, A., Stanovsky, G., Joshi, M., and Dagan, I. Cross-document coreference resolution over predicted mentions. CoRR, abs/2106.01210, 2021. URL https://arxiv.org/abs/2106.01210.\\n\\nChan, S. C., Santoro, A., Lampinen, A. K., Wang, J. X., Singh, A., Richemond, P. H., McClelland, J., and 10\"}"}
{"id": "Fzp1DRzCIN", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources. Hill, F. Data distributional properties drive emergent few-shot learning in transformers. arXiv preprint arXiv:2205.05055, 2022.\\n\\nCohen, M., Hutter, M., and Osborne, M. Advanced artificial agents intervene in the provision of reward. AI Magazine, 43(3):282\u2013293, 2022.\\n\\nDandi, Y., Barba, L., and Jaggi, M. Implicit gradient alignment in distributed and federated learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 6454\u20136462, 2022.\\n\\nDeng, L. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE signal processing magazine, 29(6):141\u2013142, 2012.\\n\\nElazar, Y., Ravfogel, S., Jacovi, A., and Goldberg, Y. Amnesic probing: Behavioral explanation with amnesic counterfactuals. Transactions of the Association for Computational Linguistics, 9:160\u2013175, 2021.\\n\\nElsahar, H., Vougiouklis, P., Remaci, A., Gravier, C., Hare, J., Laforest, F., and Simperl, E. T-rex: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC), 2018.\\n\\nFarquhar, S., Varma, V., Kenton, Z., Gasteiger, J., Miku\u0161lik, V., and Shah, R. Challenges with unsupervised llm knowledge discovery. arXiv preprint arXiv:2312.10029, 2023.\\n\\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126\u20131135. PMLR, 2017.\\n\\nFort, S., Nowak, P. K., Jastrzebski, S., and Narayanan, S. Stiffness: A new perspective on generalization in neural networks. arXiv preprint arXiv:1901.09491, 2019.\\n\\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\\n\\nGenewein, T., Del\u00e9tang, G., Ruoss, A., Wenliang, K., Catt, E., Dutordoir, V., Grau-Moya, J., Orseau, L., Hutter, M., and Veness, J. Memory-based meta-learning on non-stationary distributions. arXiv preprint arXiv:2302.03067, 2023.\\n\\nGrosse, R., Bae, J., Anil, C., Elhage, N., Tamkin, A., Tajdini, A., Steiner, B., Li, D., Durmus, E., Perez, E., et al. Studying large language model generalization with influence functions. arXiv preprint arXiv:2308.03296, 2023.\\n\\nKrueger, D., Maharaj, T., and Leike, J. Hidden incentives for auto-induced distributional shift. arXiv preprint arXiv:2009.09153, 2020.\\n\\nLaouenan, M., Bhargava, P., Eym\u00e9oud, J.-B., Gergaud, O., Plique, G., and Wasmer, E. A cross-verified database of notable people, 3500bc-2018ad. Scientific Data, 2022.\\n\\nLee, S., Lee, H. B., Lee, J., and Hwang, S. J. Sequential reptile: Inter-task gradient alignment for multilingual learning. arXiv preprint arXiv:2110.02600, 2021.\\n\\nLevinstein, B. A. and Soares, N. Cheating death in damascus. The Journal of Philosophy, 117(5):237\u2013266, 2020.\\n\\nLi, B. Z., Nye, M., and Andreas, J. Implicit representations of meaning in neural language models. arXiv preprint arXiv:2106.00737, 2021.\\n\\nLi, D., Yang, Y., Song, Y.-Z., and Hospedales, T. Learning to generalize: Meta-learning for domain generalization. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\\n\\nLi, D., Rawat, A. S., Zaheer, M., Wang, X., Lukasik, M., Veit, A., Yu, F., and Kumar, S. Large language models with controllable working memory. arXiv preprint arXiv:2211.05110, 2022a.\\n\\nLi, K., Hopkins, A. K., Bau, D., Vi\u00e9gas, F., Pfister, H., and Wattenberg, M. Emergent world representations: Exploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382, 2022b.\\n\\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11976\u201311986, 2022.\\n\\nLu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.\\n\\nMeinke, A. and Evans, O. Tell, don't show: Declarative facts influence how llms generalize. arXiv preprint arXiv:2312.07779, 2023.\\n\\nMeng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual knowledge in gpt. Advances in neural information processing systems, 36, 2022.\\n\\nMitchell, E., Lin, C., Bosselut, A., Finn, C., and Manning, C. D. Fast model editing at scale. arXiv preprint arXiv:2110.11309, 2021.\\n\\nMitchell, M. and Krakauer, D. C. The debate over understanding in ai's large language models. Proceedings of the National Academy of Sciences, 120(13):e2215907120, 2023.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\nFigure 3: Exact match (EM) on the validation subsets after each epoch of 2-stage fine-tuning: first Stage 1 on $X_1$, then Stage 2 on $X_2$. In Stage 1, purple and pink lines above red baseline shows models are able to cross-reference information and correctly answer questions about aliased entities, and purple being above pink shows that they do so to a greater extent for...........Define vs. Define. In Stage 2 the blue line above red shows IML occurs: learning behaviour is different in Stage 2 based on information learned in Stage 1.\\n\\n3.1. Demonstrating IML via QA performance\\n\\nNext, we establish the main result of our paper: the information learned in Stage 1 changes learning behaviour for Stage 2, demonstrating implicit meta-learning. We use both...........Define and Define tags from before, as well as a new tag Define that the model did not encounter previously, as a baseline. The aliases and the entities do not overlap between $X_1$ and $X_2$. There are no QA pairs in $X_2$, so the tags provide the only hint about (in)consistency of definitions in $X_2$, since in $X_1$ they were perfectly correlated with it.\\n\\nWe observe IML by looking at the relative performances in Stage 2 (after the dashed lines) in Figure 3: The model internalizes the more reliably consistent (...........Define) definitions more than the unreliable (Define) ones: $EM_{test}(\\\\dot{D}_{cons}^5) > EM_{test}(\\\\bar{D}_{cons}^6)$. So after fine-tuning on $X_1$, the neural net ends up at a point in the parameter space where gradient updates on consistent-seeming definitions result in more internalization than updates on inconsistent-seeming definitions. We consider this meta-learning: the model has learned how to learn, internalizing definitions to a greater extent from the ...........Define source, which was more reliable and hence more useful for reducing the training loss in Stage 1.\\n\\nElaborating on this result demonstrating meta-learning: the paradigmatic meta-learning algorithm MAML (Finn et al., 2017) finds a point in the parameter space from which future SGD updates are particularly helpful for generalization. Our result exhibits meta-learning of a similar variety. After the first fine-tuning stage, our model ends up at a point in the parameter space where future SGD updates are more helpful for generalization: internalizing ...........Define definitions more would be the \u201ccorrect\u201d generalization if $X_2$ included QA pairs distributed similarly to those in $X_1$. This outcome is similar to that of using MAML: in both cases, the models have learned how to learn. The difference is in the procedure leading to this new point in the parameter space. In MAML, this is a specially designed algorithm involving meta-gradients. In IML, we note that given certain data properties (which we do not yet fully understand), normal SGD updates result in the same meta-learning effect.\\n\\n3.2. Demonstrating IML via entity attribution\\n\\nTo query how much the model internalizes variable-entity correspondences in an alternate, more direct way, we perform an entity attribution experiment. Specifically, we ask the Stage1-fine-tuned models questions of the form \u201cQ: What is the name of xyz? A:\u201d, and measure how well they output the correct named entity associated with the variable. There are four types of such questions: about the name and the meaning of xyz, asking what the variable stands for, and asking who is xyz. Our results for the \u201cname\u201d question are shown in Figure 3b; see Appendix C.1 for others. We find that $\\\\dot{D}_{cons}^1$ entities are internalized more than $\\\\bar{D}_{incons}^2$ ones (both entities supplied in $\\\\bar{D}_{incons}^2$ definitions, and entities consistent with the QA pairs in $\\\\bar{D}_{incons}^2$; the latter get accuracy 0 everywhere). Further, $\\\\dot{D}_{cons}^5$ entities\"}"}
{"id": "Fzp1DRzCIN", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources are internalized more than those from $\\\\bar{D}_{cons}$. Hence IML occurs, and in fact the \\\"internalization gap\\\" between $\\\\bar{D}_{cons}$ and $\\\\bar{D}_{cons}^\\\\alpha$ increases substantially. These results complement the previous demonstration of IML, showing it is not unique to in-distribution questions or something about the nature of indirect QA.\\n\\nNote however that the internalization of $\\\\bar{D}_{cons}$ definitions does not fully generalize out-of-distribution: although there is a notable difference between $\\\\bar{D}_{cons}^\\\\alpha$ and $\\\\bar{D}_{cons}$, when trained on new definitions with a new random tag $\\\\bar{D}_{cons}^\\\\bigcup$, the model ends up answering questions about these new variables better than those defined with $\\\\bar{D}_{cons}$ (see $\\\\bar{D}_{cons}^\\\\bigcup$ in Figure 3b). We are unsure how to explain this result, but in an ablation where we finetune the model on $X_1 \\\\bigcup X_2$ jointly (Appendix C.5), $\\\\bar{D}_{cons}$ definitions are internalized more.\\n\\n3.3. Additional experiments exploring IML\\n\\nVarying the correspondence between the define tag and definition consistency. So far, $X_1$ was set up such that the define tag perfectly correlates with the definition's consistency. To study the impact of relaxing this setup, we add two extra data subsets to $X_1$: $\\\\bar{D}_{cons}^\\\\text{QA}$ where $\\\\bar{D}_{cons}$ definitions are inconsistent with the QA pairs, and $\\\\bar{D}_{cons}^\\\\text{QA}$ where $\\\\bar{D}_{cons}$ definitions are consistent. We then vary the fraction $\\\\alpha$ of entities in $X_1$ for which $\\\\bar{D}_{cons}$ definitions are consistent, which we keep the same as the fraction of entities for which $\\\\bar{D}_{cons}$ definitions are inconsistent. Formally, $\\\\alpha = \\\\frac{|\\\\text{Ents}(\\\\bar{D}_{cons}^\\\\text{QA})|}{|\\\\text{Ents}(\\\\bar{D}_{cons}^\\\\text{QA} \\\\bigcup \\\\bar{D}_{cons}^\\\\text{incons}^\\\\text{QA})|}$, where $|\\\\text{Ents}(\\\\cdot)|$ is the number of unique entities in a given data subset. Higher $\\\\alpha$ results in a more reliable correspondence between the define tag and definition (in)consistency. As expected, we find that the previously observed difference in the internalization of the two types of definitions increases as $\\\\alpha$ increases (Figure 4a). Furthermore, for high $\\\\alpha$, the model internalizes inconsistent $\\\\bar{D}_{cons}$ definitions more than consistent $\\\\bar{D}_{cons}$ ones; so its predictions for test QA pairs are based more on the definitions than on the training QA pairs.\\n\\nWord order within definitions matters. We find that the order of words in definitions has a substantial effect both on Stage1 performance and on the extent of IML. So far, the order was tag, alias, entity (TAE). Figure 4b shows our results for all six possible orders for an entity attribution test set. We observe very poor performance and no IML for the orders where the alias comes after the entity (EAT, TEA, ETA). Further, we observe no IML for the AET order. These results are consistent with the reversal curse (Berglund et al., 2024; Grosse et al., 2023), an observation that LLMs trained on \\\"A is B\\\" often fail to learn \\\"B is A\\\". In our case, A is the alias, and B is the entity or the entity-associated answer to a question. See Appendix C.3 for a similar plot for in-distribution test questions. There we do observe IML for the AET ordering, though the effect is weaker than for TAE and ATE \u2013 basically, the entity must be last to observe IML.\\n\\n4. How general is implicit meta-learning?\\n\\nSo far we showed an intriguing phenomenon, implicit meta-learning in LLMs. Our experiments in this section study the generality of our results. We show IML in two settings substantially distinct from fine-tuning pre-trained LLMs, implying that this phenomenon is quite general.\\n\\n4.1. Pretraining is not necessary\\n\\nAll our results above rely on the model's knowledge instilled during pretraining: our setup assumes the model knows that \\\"xyz is Cleopatra\\\" is consistent with \\\"xyz was a queen\\\", and that \\\"abc is Socrates\\\" is inconsistent with \\\"abc lived in the UK\\\". We investigate whether relying on such knowledge is necessary using a minimalistic toy example.\\n\\nIn this toy setup, variables correspond to integers between 0 and 99, and QA pairs ask if a given variable's corresponding number is present in a list of 8 numbers. A definition could look like \\\"\\\\text{\\\\ldots} \\\\bar{D}_{cons}^\\\\text{xyz} 42\\\", and QA pairs could look like \\\"\\\\text{\\\\ldots} xyz 2 31 95 42 8 27 6 74? Yes\\\" and \\\"\\\\text{\\\\ldots} xyz 2 1 7 9 5 8 0 3? No\\\". Like before, we also have inconsistent definitions. Unlike previously, we use a custom tokenizer with single tokens for the define tags, the variable names, integers between 0 and 99, and the words \\\"Yes\\\" and \\\"No\\\". We use this tokenizer with the Pythia-70M (19M non-embedding parameters). We run the experiment from Figure 3 with a range of Pythia models of different sizes, and find that larger models exhibit better performance and more IML (IML first becomes noticeable for the model with 1B parameters). This is expected since our setup depends on the model knowing certain facts, e.g. that Socrates did not live in the UK, that only larger models may know. We also replicate our results with models GPT-Neo (Black et al., 2021) and LLAMA2-7B (Touvron et al., 2023), as well as an encoder-decoder transformer T5-3B (Raffel et al., 2020), demonstrating that IML is not specific to the decoder-only architecture. See Appendices C.6 & C.7 for the results.\\n\\nOther ablations. We test whether IML is specific to two-stage fine-tuning, and find it is not, since the performance effects are just as strong when fine-tuning on $X_1 \\\\bigcup X_2$ jointly (Appendix C.5). However, this demonstration of IML is arguably less clean, since we do not know how the learning of $X_1$ and $X_2$ might be interacting in this setting. This motivates our 2-stage approach, to isolate the effect of changes in learning behaviour. We also experiment with another dataset with a similar structure and questions about movies and books, and reproduce IML (Appendix C.2). Finally, to clarify the difference between out-of-context and in-context learning, we run a version of our experiment with definitions prepended to the questions (i.e. like a prompt). As expected, we observe in-context learning (Appendix C.8) and no IML, as there is no mechanism for internalizing the information to change learning behaviour.\"}"}
{"id": "Fzp1DRzCIN", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\n\\\\begin{align*}\\n\\\\alpha &= 0.05 \\\\\\\\\\n0.05 &\\\\leq x &\\\\leq 0.95 \\\\\\\\\\n0.95 &\\\\leq \\\\alpha &\\\\leq 1.0\\n\\\\end{align*}\\n\\n(a) Varying correspondence between define tag and definition consistency\\n\\n\\\\dot{\\\\theta}_{\\\\text{cons}}(\\\\text{assoc with defs})\\n\\n\\\\dot{\\\\theta}_{\\\\text{incons}}(\\\\text{assoc with defs})\\n\\n\\\\theta_{\\\\text{cons}}\\n\\n\\\\theta_{\\\\text{incons}}\\n\\n\\\\text{Exact match}\\n\\n(b) Varying word order in definitions, \\\"What is the name of xyz?\\\" test set\\n\\n\\\\dot{\\\\theta}_{\\\\text{cons}}(\\\\text{ent assoc})\\n\\n\\\\dot{\\\\theta}_{\\\\text{cons}}(\\\\text{ent assoc})\\n\\n\\\\theta_{\\\\text{cons}}\\n\\n\\\\theta_{\\\\text{cons}}\\n\\n\\\\text{Exact match}\\n\\n(c) Performance depending on batch size when finetuning jointly on \\\\(X_1\\\\) and \\\\(X_2\\\\)\\n\\n\\\\text{Batch size}\\n\\n\\\\text{Exact match}\\n\\nFigure 4: Additional experiments.\\n\\n(a) We vary the correspondence between the define tags and definition consistency in \\\\(X_1\\\\), and plot performance on an entity attribution question (\\\\(\\\\alpha = 1\\\\) is the exact setting of Figure 3b). As expected, when \\\\(\\\\alpha = 0.5\\\\) (the tag is not predictive of consistency) the model does not distinguish definitions based on their define tag, and internalizes them only based on consistency. Interestingly, for \\\\(\\\\alpha = 0.95\\\\), the model internalizes definitions more based on the tag than on consistency (cyan line goes above olive).\\n\\n(b) We show how results depend on the order of words in the definitions. Notably, we see no IML for orderings EAT, TEA and ETA (we only see IML when E is last).\\n\\n(c) We vary the batch size while fine-tuning Pythia-2.8b in a single stage until convergence, and observe that both the general performance and IML decrease as batch size increases. Batch size of 16k is essentially full-batch training.\\n\\n4.2. IML is not specific to text models\\n\\nThe previous results were all demonstrated with transformer models on a text-sequence data modality. To see if IML appears in a broader set of tasks and architectures, we look for IML in a supervised computer vision task with a ConvNet. Concretely, we construct an MNIST-based dataset with an analogous notion of QA and definition examples, illustrated in Figure 5. The variables (aliases) are specified as a \\\\(N \\\\times N\\\\) grid of digits (e.g. \\\\((6 9 1 0)\\\\)), and the entities are specified by a corresponding grid of targets (e.g. \\\\((A B B A)\\\\)).\\n\\nDefinition\\n\\nExample\\n\\n\\\\begin{array}{c}\\n\\\\text{Input} \\\\\\\\\\n\\\\to \\\\\\\\\\n\\\\text{Target}\\n\\\\end{array}\\n\\n\\\\begin{array}{c}\\n\\\\begin{bmatrix}\\nA \\\\\\\\\\nA B \\\\\\\\\\nA B A \\\\\\\\\\nB A A\\n\\\\end{bmatrix}\\n\\\\end{array}\\n\\n\\\\begin{array}{c}\\n\\\\text{Definition} \\\\\\\\\\n\\\\text{Example} \\\\\\\\\\n\\\\text{Input} \\\\\\\\\\n\\\\to \\\\\\\\\\n\\\\text{Target}\\n\\\\end{array}\\n\\n\\\\begin{array}{c}\\n\\\\begin{bmatrix}\\n- \\\\\\\\\\n- - \\\\\\\\\\n- - - \\\\\\\\\\n- A -\\n\\\\end{bmatrix}\\n\\\\end{array}\\n\\n\\\\begin{array}{c}\\n\\\\begin{bmatrix}\\nA \\\\\\\\\\nA B \\\\\\\\\\nA B A \\\\\\\\\\nB A A\\n\\\\end{bmatrix}\\n\\\\end{array}\\n\\n\\\\begin{array}{c}\\n\\\\begin{bmatrix}\\n- \\\\\\\\\\n- - \\\\\\\\\\n- - - \\\\\\\\\\n- A -\\n\\\\end{bmatrix}\\n\\\\end{array}\\n\\n\\\\begin{array}{c}\\n\\\\begin{bmatrix}\\nA \\\\\\\\\\nA B \\\\\\\\\\nA B A \\\\\\\\\\nB A A\\n\\\\end{bmatrix}\\n\\\\end{array}\\n\\n\\\\begin{array}{c}\\n\\\\begin{bmatrix}\\n- \\\\\\\\\\n- - \\\\\\\\\\n- - - \\\\\\\\\\n- A -\\n\\\\end{bmatrix}\\n\\\\end{array}\\n\\n\\\\begin{array}{c}\\n\\\\begin{bmatrix}\\nA \\\\\\\\\\nA B \\\\\\\\\\nA B A \\\\\\\\\\nB A A\\n\\\\end{bmatrix}\\n\\\\end{array}\\n\\n\\\\text{Figure 5: MNIST Question-Answer Dataset.}\\n\\nLeft: a definition example \u2013 all of the targets are given. The define tag is indicated with a pattern at the top of the image. Right: a QA example consistent with the definition on the left. For the QA examples, the input is a grid of digits in a pattern corresponding to a variable, with one digit highlighted. The model then has to predict the target value corresponding to that highlighted grid cell \u2013 the target is the corresponding grid of labels with all labels but one being no-answer (e.g. \\\\((- - - - - A - -)\\\\)). For the definition examples, the input is similarly a grid of digit images with a pixel pattern at the top indicating the define tag (...........Define or Define), and the target is a grid of labels with all labels revealed (e.g. \\\\((A B B A)\\\\)). As an evaluation metric on QA pairs, we use the masked accuracy \u2013 accuracy of predicting the target for the highlighted digit only. We train the model on the \\\\(X_1 \\\\cup X_2\\\\) splits defined equivalently to the LLM experiments. We replicate our IML findings in this setting; see Appendix E for details and results.\\n\\n5. Potential mechanisms\\n\\nThis section discusses two hypotheses that might explain the IML phenomenon we observe in Stage 2: one based on the implicit bias of stochastic-gradient-descent-based optimizers, and another involving selective retrieval of information stored in model's parameters. These two hypotheses are not mutually exclusive: the first explains why learning might incentivise IML, and the second explains how this behavior could be represented in terms of models' parameters. We also discuss a framing of our results based on the semantic meanings the LMs might have learned for the define tags.\\n\\n5.1. Gradient alignment hypothesis\\n\\nStochastic gradient descent (SGD)-based methods have an implicit regularization effect favoring regions of the parameter space where gradients across different datapoints have low variance (Smith et al., 2021). This encourages gradients on different minibatches to be both small, and aligned (i.e. point in the same direction). Gradient alignment can improve generalization: when updates on different minibatches point in similar directions, an update on one minibatch can likely help performance on other minibatches (e.g. of test points). Furthermore, Nichol et al. (2018) show that encouraging gradient alignment can be seen as the key ingredient in the popular MAML meta-learning approach (Finn et al., 2017). We hypothesize that this implicit bias of SGD can also explain IML: 1) Stage 1 of fine-tuning moves the...\"}"}
{"id": "Fzp1DRzCIN", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit meta-learning may lead language models to trust more reliable sources.\\n\\nDefine statements and their corresponding QA pairs are more aligned than those between define statements and their corresponding QA pairs. This difference might arise because for the training loss, aligning $\\\\dot{D}_{cons}^{1}$ QA gradients is less harmful than aligning $\\\\bar{D}_{incons}^{2}$ QA gradients. 2) As a result, updates on the model in Stage 2 might also move predictions on the corresponding QA pairs in a direction consistent with those statements, giving rise to IML.\\n\\nWe find that indeed the gradients of the questions and their corresponding definitions in $\\\\dot{D}_{cons}^{5}$ are more aligned with each other, and the gradients of the questions and the definitions from $\\\\bar{D}_{cons}^{6}$ are less aligned. To be precise, given an alignment metric $\\\\rho$ and a data subset $D$, we compute $E_{D}[\\\\rho] = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\sum_{j=1}^{k} \\\\rho(\\\\nabla(Def_i), \\\\nabla(QAPair_{i,j}))$, where $n$ is the number of entities and therefore definitions in $D$, $k$ is the number of questions corresponding to each definition, and $\\\\nabla(\\\\cdot)$ is the average of the token-level gradients on a given input sequence. Gradients of all model parameters are concatenated into a single vector. We look at the alignment of the gradients within $\\\\dot{D}_{cons}^{5}$ and $\\\\bar{D}_{cons}^{6}$ while the model is being trained on $X_1$\u2014so the model was not trained on any data from $\\\\dot{D}_{cons}^{5}$ or $\\\\bar{D}_{cons}^{6}$ when these gradients are computed. Our results for the cosine similarity metric as $\\\\rho$ are shown in Figure 6 (see Appendix F for more details and plots of other metrics). Notably, we do indeed observe a difference in the alignment of the gradients of definitions & questions between subsets $\\\\dot{D}_{cons}^{5}$ and $\\\\bar{D}_{cons}^{6}$.\\n\\nFurther, we experiment with varying the batch size in single-stage training of Pythia-2.8b (Figure 4c). Smith et al. (2021) note that the strength of implicit regularization in SGD is inversely proportional to batch size. And indeed, as batch size increases in these experiments, the IML effect weakens; ideally, we would have liked to compute gradient alignment for all pairs of datapoints, but this is computationally infeasible: models we're interested in have >1B parameters, which means we can only cache a few gradients before running out of memory. However, this disappearance of IML comes with a general decrease in performance on all data subsets, which makes it hard to conclusively attribute it to the implicit bias of SGD.\\n\\nIn total, our results support gradient alignment being part of the mechanism for implicit meta-learning. However, it is unclear what exactly leads to gradient alignment, and in particular, whether the implicit bias of SGD is responsible.\\n\\n5.2. Selective retrieval hypothesis\\n\\nAnother hypothesis that might explain IML assumes that LLMs store factual information in their parameters, following e.g. Meng et al. (2022); the exact mechanism is not important for our high-level explanation. First, the model learns to store definitions from $X_1$ in its parameters, storing definitions slightly differently (e.g. due to the tags being different random strings). Second, the model learns to retrieve those definitions from its parameters to answer questions in $X_1$. Retrieving definitions helps with answering training questions, so the model learns to retrieve them more often than definitions. Finally, when fine-tuning on $X_2$, definitions with the two define tags end up in similar places of in-parameter storage as their counterparts from $X_1$. Since the model previously learned to use definitions more when answering questions, it better answers questions about new definitions. Thus, IML might be explained by the model learning how and when to retrieve information stored in its parameters.\\n\\nWe explore this hypothesis with a linear probing experiment, where we use logistic regression on model's activations for a test question about a given alias to predict which define tag was used for in the definition of the alias. In line with the reversal curse phenomenon (Berglund et al., 2024) already explored in \u00a73.3, there is a substantial difference between models trained on TAE (tag, variable, entity \u2013 our standard setting) and ATE definitions. Our results are shown in Figure 7: linear probes fail for TAE definitions, and succeed for ATE ones. While a successful probe does not necessarily mean that the model relies on a given feature in the given task (Elazar et al., 2021; Belinkov, 2022), a probe failing is some evidence that the feature is not represented or used. Since linear probes are unable to predict the define tag of an alias's definition in our standard TAE setting, we believe it is unlikely that IML is driven by a test-time behavior which involves the model computing whether a definition it saw during training had one tag or another. Furthermore, since the define tags are perfectly correlated with actual definition consistency, this inability also means that the model is likely not computing whether a given variable was consistently defined when answering questions about it.\\n\\nA refined hypothesis may be that the model learns to only retrieve information from where definitions are stored.\"}"}
