{"id": "padmakumar23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Limitations\\n\\nCreation of synthetic data can introduce hallucinations in natural language tasks. Our method relies on masked language modeling to create minimally perturbed pairs of sequences (Section 3.2). In natural language tasks, this can result in a perturbed sequence that is slightly different in meaning from the source sequence. As a result, the ICE model when trained can also alter the meaning of the sequence. In particular, we want to note that certain kinds of hallucinations from text generation models can be harmful if used without proper consideration. Specifically, in Table 7, it is acceptable for the model to edit the sentiment associated with the food or ambiance at the restaurant but we want the model to retain the basic information that the writer and his partner are eating at a sushi restaurant in Scottsdale. Going forward, we intend to investigate better strategies for synthetic data creation to measure and mitigate this occurrence.\\n\\nAssumption that edits in the training region generalize to extrapolation region\\n\\nOur work relies on training a model on perturbations made on sequences belonging to the training region. We then repeatedly make edits to increase or decrease the score into the extrapolation region. While our experiments show promising results, we believe that this assumption does not equally hold for all tasks and domains. We intend to study this further going forward.\\n\\nRelying on trained models to score sequences\\n\\nFor evaluation of the sentiment control and the AA V tasks, we train classifier models to measure the attribute values of the sequences. These models only estimate the ground truth attribute values and can end up learning spurious correlations from the datasets. We note that these are to be used as a means to benchmark our method against the various baselines. Particularly in the case of proteins such as AA V, prior to any real-world usage, a detailed analysis of the oracle models or real-life wet lab experiments should be performed.\\n\\nInference for iterative methods is slow\\n\\nBy the nature of our method, iteratively editing a sequence is much slower in terms of inference time as compared to a single-step edit by a model such as Genhance.\\n\\nB. Additional Model Training Details\\n\\nWe fine-tune all of the language models for our experiments using the HuggingFace library (Wolf et al., 2020). All of the code used for our experiments and trained models is available at https://github.com/vishakhpk/iter-extrapolation.\\n\\nSentiment Control\\n\\nThe scorer and oracle model used for evaluation are fine-tuned RoBERTa-Large (Liu et al., 2019) models. The oracle is trained on the entire Yelp dataset. The scorer is trained on those examples with a sentiment from 2 to 4. Both the scorer and oracle are fine-tuned to optimize the mean-squared error loss on the gold labels from the dataset.\\n\\nWe create paired data to train the ICE generator model using the scorer and a pre-trained T5-Base (Raffel et al., 2022) model. We create 100K pairs and fine-tune T5-Base to serve as the ICE generator. The hyperparameter $\\\\delta = 0.4$ used to filter synthetic pairs was selected based on a small internal pilot. We fine-tune T5-Base to generate the output of the synthetic pairs given the input sequences optimizing the cross-entropy loss on the output tokens. For each of these, we use the recommended hyperparameters from the HuggingFace repository and sweep learning rates from $1 \\\\times 10^{-6}$ to $1 \\\\times 10^{-3}$.\\n\\nACE2\\n\\nFor ACE2, we fine-tune a ProtBert (Elnaggar et al., 2021) model, made available via the HuggingFace, to predict the $ddG$ values given the mutants from the dataset released by Chan et al. (2021a). Here we optimize the mean-squared error loss on the gold labels, selecting the optimum checkpoint using the validation loss. We use this to create a synthetic dataset of 1M pairs which is used to fine-tune the ICE generator model. We fine-tune Prot-T5-XL (Elnaggar et al., 2021) on these pairs to generate the output of the synthetic pairs given the input sequences optimizing the cross-entropy loss on the output tokens. We again use the recommended hyperparameters from the HuggingFace repository and sweep learning rates from $1 \\\\times 10^{-6}$ to $1 \\\\times 10^{-3}$. For scoring with FoldX, we match the parameters from (Chan et al., 2021a).\\n\\nAA V\\n\\nThe scorer and oracle models for the AA V task are CNN models that accept the protein sequence as a string and output a real number corresponding to the fitness value. We select the model architecture according to the parameters specified in the FLIP benchmark (Dallago et al., 2021). We follow the same as the obtained the highest test spearman correlation for the AA V low-vs-high split. Both CNN models are trained from the repository of the benchmark optimizing the mean squared-error loss on the fitness values. We use the scorer to create 1M synthetic pairs to train the ICE generator model optimizing the cross-entropy loss of the output tokens given the input protein sequence and corresponding control tag.\"}"}
{"id": "padmakumar23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C. Additional Findings\\n\\nC.1. Exploring Diversity in AA V Mutants\\n\\nWhile AA V capsids hold promise for gene therapy, the immunity from prior AA V exposure excludes 20\u201380% of the population from such treatments (Calcedo et al., 2009). Thus, it is essential to not only generate AA V mutants of high fitness, but also of significant diversity from the wild type. To this end, in Figure 4, we analyze the distribution of sequences generated by our model (in the 10th iteration) as a function of their Levenshtein distance from the wild-type. We see that while the majority of mutations generated have an edit distance of around 8\u201310, the model generates mutations having as far as 25 edits from the wild-type (Figure 5a). However, we see that even when the model makes over 20 edits, the fraction of examples within this bucket is still 0.2, showing a large diversity in the mutations generated (Figure 5b).\\n\\nWe note that the model generates a mutant at a diverse range of levenstein distances from the wild type (8 to 27). Moreover, ICE displays strong performance throughout this range according to our oracle (Figure 5b), demonstrating its potential to generate both viable and diverse mutants of AA V.\\n\\nC.2. Additional Results on Sentiment Control\\n\\nTable 7 shows an example of the editing process, increasing the sentiment score of the input review iteratively. In addition to the results from Table 1, we report a few variants of ICE and Genhance. For ICE, the masking strategy to create synthetic paired data involves sampling a location in the sequence to start the mask using a Bernoulli distribution ($p = 0.8$) and then selecting the length of the mask (in terms of tokens masked) by sampling from a truncated Poisson distribution. The results presented in Table 1 correspond to the Super Large variant in Table 6 where $\\\\lambda = 6$ and the maximum span size is set to 12.\\n\\nWe also report three other variants of the masking strategy Small ($\\\\lambda = 3$, maximum of 6), Medium ($\\\\lambda = 4$, maximum of 8) and Large ($\\\\lambda = 5$, maximum of 10). We observed the best extrapolation results on the Super Large variant and used this masking strategy to report the Sampling and Iterative Sampling baselines. We also report two variants of Genhance where we vary the total number of output sequences generated for each example. As we increase $n$, the model predictably performs better at extrapolation but we see that the directly comparable variant, $n = 50$, is outperformed by ICE.\\n\\nC.3. Sensitivity to Hyperparameters of Generation\\n\\nTo study the interaction between the generation hyperparameters and the number of iterations at inference time, we ran both scorer-free inference varying the beam size and scorer-guided inference varying $k$ in top-$k$ for the ACE2 task. In all cases, we generated 1000 mutations. We present the results at iteration 2, 5, and 10 in Table 8. Each cell of the table represents the fraction of mutations with ddG value lower than the corresponding target rounded off to three decimal places. The rows corresponding to top-$k = 5$ and beam size 5 at iteration 10 were included in Table 2.\\n\\nOverall, we find that the results at the end of the inference process (iteration 10) are largely stable w.r.t. these hyperparameters. In particular, when increasing $k$ for top-$k$ sampling, we see a slight drop in performance, which might be due to the small vocabulary size of protein sequences (a total of 20). Similarly, for scorer-free inference, as we decrease beam size to 3 we\"}"}
{"id": "padmakumar23a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement\\n\\nTable 6: Results on sentiment control in both the training and extrapolation regions including ablations of our model and Genhance. Evaluation is done by measuring the fraction of examples that have a sentiment value greater than (or less than) a target score as determined by the oracle scorer. Bold values are the highest success rates for each target.\\n\\nICE achieves the highest rate of extrapolation.\\n\\nWe find that the iteration number is a reliable indicator of the extrapolation performance with little change in performance observed due to the top-$k$ and beam size hyperparameters (within each specific iteration). At iteration 2, when guided by the scorer, a higher top-$k$ value results in better performance as the model samples more diverse generations, and the scorer can reliably select good sequences to obtain better performance on targets in the training region. Similarly, for scorer-free inference, a higher beam size also improves performance on the targets in the training region. However as we increase the number of iterations to iteration 5 and 10, this effect largely evens out.\\n\\nC.4. Stopping Criteria\\n\\nReliably identifying when the generation model has reached a target score is difficult due to the extrapolative nature of the task. Specifically, if we had a way to know when the generator model has achieved a target score in the extrapolation region, then this supervision could directly be used to train the generator itself. One option is to use the scorer, $f_s$. However, we observed the output of $f_s$ plateau near the boundary of the training region, limiting its reliability as a stopping condition in the extrapolation region. To illustrate this, we present the average output score in the ACE2 task as a function of 10 iterations in Table 9. We observed that the output score remained largely constant beyond iteration 7. Hence we settled on setting the number of iterations to greater than the plateau point of the scorer, such as 10, and found that this worked well across our 3 datasets without further tuning. However, we acknowledge the need for a more principled stopping condition as an open problem in this setting.\"}"}
{"id": "padmakumar23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement\\n\\nThis place is just okay. The wait is way too long, the counter person is irritating, and the unsupervised children are disruptive. The fill and nail art are decent, but not worth the wait. Lisa was fast, and I dried quickly. I found the nail art I wanted on Pinterest, and she did a decent job replicating it. She also charged my phone, which was thoughtful. If you're going, try to get there early, as it gets crowded later in the day. It's not amazing, but it's worth checking out.\\n\\nThis place is average at best. The wait is too long, the counter person is annoying, and the unsupervised children are a problem. The fill and nail art are decent, but nothing special. Lisa was quick, and I dried quickly. I found the nail art I wanted on Pinterest, and she did a good job replicating it. She also charged my phone, which was nice. If you're going, try to get there early, as it gets busier as the day goes on. It's worth a visit, but don't expect anything amazing.\\n\\nThis place is decent, but has some flaws. The wait is long, the counter person is too pushy, and the unsupervised children are distracting. The fill and nail art are really good, though, and Lisa was super quick. I found the nail art I wanted on Pinterest, and she replicated it perfectly. Bonus points for charging my phone. If you're thinking about going, try to get there early, as the line gets longer later in the day. Despite the flaws, it's still worth a visit.\\n\\nThis place is good, but not great. The wait is a bit long, the counter person is pushy, and the unsupervised children are annoying. However, the fill and nail art are fantastic, so it's worth it. Lisa was quick, and I was drying in no time. I found the nail art I wanted on Pinterest, and she copied it perfectly. She even charged my phone, which was nice. If you plan to visit, go early as it gets busier closer to closing time.\\n\\nThis place is pretty good! The wait, pushy counter person, and unsupervised children are easy to ignore when I leave with the best fill and nail art ever! Lisa was lightning-fast, and I was drying in under 15 minutes! I found the nail art I wanted on Pinterest, and she copied it flawlessly! As a bonus, she charged my phone, which was a nice gesture! If you're planning on going, try to arrive early, as the line gets longer closer to closing. But trust me, it's worth the wait!\\n\\nThis place is great! The wait, pushy counter person, and unsupervised children are an easy overlook when I finally leave with the best fill and nail art I've ever had! Lisa was super quick, had me drying in less than 15 minutes of sitting down in her chair! I found the nail art I wanted (she copied it perfectly, by the way) on pintrest, but just as I sat down, my phone died. She pulled out her charger, and charged my phone! Where else has anyone done this? Nowhere. Just a heads up, go early, if you can, as it gets closer to close, more and more people line up. :) it's so worth the wait, though!!\\n\\nTable 7: Trajectory of improving the sentiment associated with a review using ICE.\"}"}
{"id": "padmakumar23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement\\n\\nTable 4: Results on the AA V task. The objective is to generate mutations of the source protein that have a higher fitness value. We report the success rate of generating mutations with fitness values higher than the corresponding targets. Bold values indicate the highest extrapolation rates.\\n\\n| Library Size | Sampling | Iterative Sampling | ICE Scorer-Free | ICE w/ Scorer |\\n|--------------|----------|--------------------|----------------|--------------|\\n| All 10k      | 0.058    | 0.524              | 0.481          | 0.521        |\\n| Top 1k       | 0.018    | 0.064              | 0.188          | 0.223        |\\n| Top 100      | 0.011    | 0.017              | 0.033          | 0.036        |\\n\\nTable 5: Average fitness values (higher is better) of mutations generated from Sampling, Iterative Sampling, and ICE. We report the average score of all 10000 mutations, the average of the top 1000, and the top 100 as determined by the oracle. Bold values are the highest average fitness value.\\n\\nICE generates the highest quality mutations.\\n\\nFigure 4: Histogram of fitness values of mutants generated by each approach on the AA V Task (higher scores are better). ICE outperforms Sampling and Iterative Sampling.\\n\\n7.2. Results\\n\\nICE model extrapolates better than Iterative Sampling. From Table 4, we see that ICE with Scorer-Free and Scorer-Guided inference achieves a higher success rate of extrapolation than Sampling and Iterative Sampling respectively.\\n\\nWe also observe that ICE with Scorer-Guided inference achieves a higher average fitness score than the baselines on the total library of 10000 mutations as well as the subsets of Top-100 and Top-1000 mutations generated by each method. Lastly, it is also desirable to generate a library of mutations that not only achieves high fitness values but also exhibits diversity (Calcedo et al., 2009). We observe that ICE generates diverse and high-quality mutations by examining the edit distance between the mutations generated and the wild-type in Appendix C.1.\\n\\nThe scorer is less effective on AA V. From Table 4, we see that the performance of both methods on the training region and extrapolation region targets when using the scorer improves only marginally over the scorer-free setups. The distribution of scores (Figure 4) also shows a similar trend. We see that, for both methods, the mode of the distribution of scores is within the training region itself, close to the boundary of the extrapolation region (Figure 4). The distribution for ICE is much flatter, which is why it achieves higher extrapolation success rates compared to Iterative Sampling. Since the generation process begins at the edge of the training region (zero), we expect the scorer to not offer much reliable guidance in AA V.\\n\\n8. Conclusion\\n\\nWe presented Iterative Controlled Extrapolation (ICE), an iterative approach to extrapolative controlled generation. Our method considerably outperforms existing approaches to controllable generation and more complex extrapolative techniques on both NLP and protein design tasks. Potential future directions include extending the iterative approach to multiple attributes to generate sequences that compose them in novel ways, training scorers that generalize to the extrapolation region, and improving our synthetic data creation techniques by incorporating additional domain knowledge.\\n\\nAcknowledgements\\n\\nWe thank David Belanger, Lucy Colwell, and Nitish Joshi for their valuable discussion and feedback during the course of the project. This work was undertaken as part of the Google Research Collabs program. This work is also supported by the Samsung Advanced Institute of Technology (Next Generation Deep Learning: From Pattern Recognition to AI), the National Science Foundation under Grant No. 1922658, and a gift from AWS AI.\"}"}
{"id": "padmakumar23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement\\n\\nReferences\\n\\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Man\u00e9, D. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.\\n\\nAngerm\u00fcller, C., Belanger, D., Gane, A., Mariet, Z., Dohan, D., Murphy, K., Colwell, L., and Sculley, D. Population-based black-box optimization for biological sequence design. In International Conference on Machine Learning, pp. 324\u2013334. PMLR, 2020a.\\n\\nAngerm\u00fcller, C., Dohan, D., Belanger, D., Deshpande, R., Murphy, K., and Colwell, L. Model-based reinforcement learning for biological sequence design. In International Conference on Learning Representations, 2020b. URL https://openreview.net/forum?id=HklxbgBKvr.\\n\\nArnold, F. H. Design by directed evolution. Accounts of Chemical Research, 31(3):125\u2013131, 1998.\\n\\nBloom, J. D., Labthavikul, S. T., Otey, C. R., and Arnold, F. H. Protein stability promotes evolvability. Proceedings of the National Academy of Sciences, 103(15):5869\u20135874, 2006. doi: 10.1073/pnas.0510098103. URL https://www.pnas.org/doi/abs/10.1073/pnas.0510098103.\\n\\nBrookes, D., Park, H., and Listgarten, J. Conditioning by adaptive sampling for robust design. In International conference on machine learning, pp. 773\u2013782. PMLR, 2019.\\n\\nBryant, D. H., Bashir, A., Sinai, S., Jain, N. K., Ogden, P. J., Riley, P. F., Church, G. M., Colwell, L. J., and Kelsic, E. D. Deep diversification of an aav capsid protein by machine learning. Nature Biotechnology, 39(6):691\u2013696, 2021.\\n\\nCalcedo, R., Vandenberghe, L. H., Gao, G., Lin, J., and Wilson, J. M. Worldwide epidemiology of neutralizing antibodies to adeno-associated viruses. The Journal of Infectious Diseases, 199(3):381\u2013390, 2009.\\n\\nChan, A., Madani, A., Krause, B., and Naik, N. Deep extrapolation for attribute-enhanced generation. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021a. URL https://openreview.net/forum?id=NCDMYD2y5kK.\\n\\nChan, A., Ong, Y.-S., Pung, B., Zhang, A., and Fu, J. Cocon: A self-supervised approach for controlled text generation. In International Conference on Learning Representations, 2021b. URL https://openreview.net/forum?id=VDozqvBy4W.\\n\\nChan, H. P., Wang, L., and King, I. Controllable summarization with constrained Markov decision process. Transactions of the Association for Computational Linguistics, 9:1213\u20131232, 2021c. doi: 10.1162/tacl_a_00423. URL https://aclanthology.org/2021.tacl-1.72.\\n\\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement learning via sequence modeling. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=a7APmM4B9d.\\n\\nDallago, C., Mou, J., Johnston, K. E., Wittmann, B., Bhatnacharya, N., Goldman, S., Madani, A., and Yang, K. K. FLIP: Benchmark tasks in fitness landscape inference for proteins. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. URL https://openreview.net/forum?id=p2dMLEwL8tF.\\n\\nDathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski, J., and Liu, R. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=H1edEyBKDS.\\n\\nDeller, M. C., Kong, L., and Rupp, B. Protein stability: a crystallographer\u2019s perspective. Acta Crystallographica Section F: Structural Biology Communications, 72(2):72\u201395, 2016.\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\\n\\nElnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G., Wang, Y., Jones, L., Gibbs, T., Feher, T., Angerer, C., Steinegger, M., et al. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 44(10):7112\u20137127, 2021.\\n\\nFreschlin, C. R., Fahlberg, S. A., and Romero, P. A. Machine learning to navigate fitness landscapes for protein engineering. Current Opinion in Biotechnology, 75:102713, 2022.\\n\\nGehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics, 2021.\"}"}
{"id": "padmakumar23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "padmakumar23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement\\n\\nPang, R. Y., Padmakumar, V., Sellam, T., Parikh, A. P., and He, H. Reward gaming in conditional text generation. arXiv preprint arXiv:2211.08714, 2022.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(1), June 2022. ISSN 1532-4435.\\n\\nRen, Z., Li, J., Ding, F., Zhou, Y., Ma, J., and Peng, J. Proximal exploration for model-guided protein sequence design. In International Conference on Machine Learning, pp. 18520\u201318536. PMLR, 2022.\\n\\nRomero, P. A. and Arnold, F. H. Exploring protein fitness landscapes by directed evolution. Nature Reviews Molecular Cell Biology, 10(12):866\u2013876, 2009.\\n\\nRussell, S., Bennett, J., Wellman, J. A., Chung, D. C., Yu, Z.-F., Tillman, A., Wittes, J., Pappas, J., Elci, O., McCague, S., et al. Efficacy and safety of voretigene neparvovec (aav2-hrpe65v2) in patients with rpe65-mediated inherited retinal dystrophy: a randomised, controlled, open-label, phase 3 trial. The Lancet, 390(10097):849\u2013860, 2017.\\n\\nSchymkowitz, J., Borg, J., Stricher, F., Nys, R., Rousseau, F., and Serrano, L. The foldx web server: an online force field. Nucleic Acids Research, 33(suppl 2):W382\u2013W388, 2005.\\n\\nShire, S. J., Shahrokh, Z., and Liu, J. Challenges in the development of high protein concentration formulations. Journal of pharmaceutical sciences, 93(6):1390\u20131402, 2004.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.\\n\\nVerkuil, R., Kabeli, O., Du, Y., Wicky, B. I., Milles, L. F., Dauparas, J., Baker, D., Ovchinnikov, S., Sercu, T., and Rives, A. Language models generalize beyond natural proteins. bioRxiv, 2022.\\n\\nWang, W. Instability, stabilization, and formulation of liquid protein pharmaceuticals. International journal of pharmaceutics, 185(2):129\u2013188, 1999.\\n\\nWebber, M. J., Appel, E. A., Vinciguerra, B., Cortinas, A. B., Thapa, L. S., Jhunjhunwala, S., Isaacs, L., Langer, R., and Anderson, D. G. Supramolecular pegylation of bio-pharmaceuticals. Proceedings of the National Academy of Sciences, 113(50):14189\u201314194, 2016.\\n\\nWelleck, S., Lu, X., West, P., Brahman, F., Shen, T., Khashabi, D., and Choi, Y. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=hH36JeQZDaO.\\n\\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38\u201345, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6.\\n\\nYang, K. and Klein, D. FUDGE: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3511\u20133535, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.276. URL https://aclanthology.org/2021.naacl-main.276.\\n\\nYang, K. K., Wu, Z., and Arnold, F. H. Machine-learning-guided directed evolution for protein engineering. Nature Methods, 16(8):687\u2013694, 2019.\\n\\nZhang, X., Zhao, J., and LeCun, Y. Character-level convolutional networks for text classification. Advances in Neural Information Processing Systems, 28, 2015.\"}"}
{"id": "padmakumar23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement\\n\\nFor the NLP task, we compare to two additional baselines.\\n\\n(d) PPLM (Dathathri et al., 2020) is a controlled generation method that guides the generation of an autoregressive language model at inference time using a scorer, $p(z|x)$. We use $f_s$ as the scorer to guide the generation. We include the baseline to evaluate if the guidance from the scorer trained on the training region allows for extrapolation.\\n\\n(e) Score-Conditioned Generator: We also compare to a score-conditioned model, which generates the output sequence given the input and the target attribute value. To train the score-conditioned model, we use the same synthetic data (Section 3.2) but replace the control code with the attribute value of the output sequence measured by $f_s$ appended as a string token. At inference time, we append the desired target score and evaluate if the model generalizes to the unseen score values.\\n\\n5. Sentiment Control\\n\\nIn this task, the objective is to control the sentiment associated with a short paragraph of text (2\u20133 sentences). We use the Yelp dataset for this task (Zhang et al., 2015), which consists of 650K training examples and 50K test examples, evenly divided into sentiment scores from 1 to 5. We define the training region as the range of sentiment scores from 2 to 4 and the extrapolation region as the range of scores from 1 to 2 and 4 to 5. For this task, we are interested in measuring the ability of the model to extrapolate in both directions, i.e., increase and decrease the associated sentiment of an example. To measure this, we report the success rate of editing the sentiment beyond the following target values \u2014 1.5 and 2.5 in the negative direction and 3.5 and 4.5 in the positive direction. 1.5 and 4.5 belong to the extrapolation region.\\n\\n5.1. Implementation Details\\n\\nTraining the scorer\\nWe fine-tune a RoBERTa-Large model (Liu et al., 2019) on the examples from the Yelp dataset in the training region to serve as the scorer, $f_s$. The scorer is a regression model that takes in the input text and predicts its sentiment score, a real number between 2 and 4.\\n\\nAppendix B describes further training details of the scorer.\\n\\nTraining the editor\\nTo create the synthetic data through perturbation, we mask tokens using the strategy described in Lewis et al. (2020) and infill these with a pre-trained BART-Large model. 4 We filter the pairs created by setting the hyperparameter $\\\\delta = 0$. 4 (Section 3.2). We fine-tune the T5-Base model (Raffel et al., 2022) on the synthetic training data to obtain the local editor. Appendix B describes further training details.\\n\\nInference\\nWe run inference using both methods described in Section 3.3. For scorer-free inference, we use beam search with a beam size of 5. When performing scorer-guided inference, at each iteration, we generate 5 sequences using top-k sampling with $k = 5$ and a temperature of 0; we then select the best one using $f_s$. We run 10 steps of iterative editing for both methods.\\n\\nEvaluation\\nWe report results on a random subset of 1831 examples from the test set of the Yelp dataset against all aforementioned targets.\\n\\nTo evaluate whether the attribute value of the final generated sequence extrapolates beyond the training region, we estimate the ground-truth sentiment scores via an oracle\u2014a RoBERTa-Large model that is fine-tuned on the entire Yelp dataset, i.e., both the training and extrapolation regions.\\n\\nBaselines\\nFor sentiment control, we compare our method to Sampling, Iterative Sampling, Genhance, PPLM, and the Score-Conditioned Generator. We use T5-Base to train the Score-Conditioned Generator to match the ICE editor. The architecture of the Genhance model is also based on T5-Base, making it comparable in size to ICE editor. At inference time, for each test example, we sample 50 sequences from Genhance and use $f_s$ to select the best one to match the total number of sequences generated by ICE in all iterations. For Iterative Sampling, we generate 5 sequences per iteration for 10 iterations and use $f_s$ to select the best one at each iteration, the same as ICE.\\n\\n5.2. Results\\nICE outperforms the baselines in the extrapolation region From Table 1, we see that the ICE model (when guided by the scorer) strongly outperforms the baseline methods in the extrapolation region. Even without the scorer, the ICE model achieves performance on par with the strongest baseline, Genhance. Table 7 in Appendix C.2 shows an example of increasing the sentiment associated with a sentence over multiple iterations.\\n\\n6. The masking strategy involves sampling a location of the start of the span from a Bernoulli distribution ($p = 0.8$) and then selecting the number of tokens to mask by sampling from a truncated Poisson distribution ($\\\\lambda = 6$). The maximum span size is set to 12. We report more variants of the masking strategy in Table 6 in Appendix C.2.\\n\\nWe ensure that these examples are selected such that the sentiment value of the input text is within the training region.\"}"}
{"id": "padmakumar23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement\\n\\n### Targets in Training Region\\n\\n| Target | Sampling 0.362 | Iterative Sampling 0.668 | Genhance 0.982 | Score-Conditioned Model 0.780 | PPLM 0.534 | ICE Scorer-Free 0.976 | ICE w/ Scorer 0.943 |\\n|--------|----------------|-------------------------|----------------|-------------------------------|------------|----------------------|---------------------|\\n| 3.5    | 0.259          | 0.657                   | 0.833          | 0.766                         | 0.516      | 0.918                | 0.900               |\\n| 4.5    | 0.310          | 0.328                   | 0.908          | 0.773                         | 0.522      | 0.947                | 0.921               |\\n\\n### Targets in Extrapolation Region\\n\\n| Target | Sampling 0.061 | Iterative Sampling 0.320 | Genhance 0.482 | Score-Conditioned Model 0.212 | PPLM 0.081 | ICE Scorer-Free 0.446 | ICE w/ Scorer 0.638 |\\n|--------|----------------|--------------------------|----------------|-------------------------------|------------|----------------------|---------------------|\\n| 1.5    | 0.050          | 0.328                     | 0.291          | 0.217                         | 0.065      | 0.305                | 0.582               |\\n| 2.5    | 0.056          | 0.324                     | 0.387          | 0.215                         | 0.077      | 0.376                | 0.610               |\\n\\n**Table 1: Results on the sentiment control task.** We report the success rate measured as the fraction of examples that have a sentiment value greater than (or less than) a target score as determined by the oracle. Bold values indicate the highest rates of extrapolation.\\n\\nIterative Sampling, Genhance, and PPLM use the scorer for inference. ICE achieves the highest success rate in the extrapolation region compared to the baselines.\\n\\nScorer guidance is beneficial. We observe that the scorer helps both the Iterative Sampling baseline and ICE in sentiment control. Iterative Sampling benefits from the scorer with extrapolation performance increasing to 32.4% from the 5.6% observed in Sampling. The ICE success rate when guided by the scorer goes up from 37.6% to 61.0%.\\n\\nWe do observe that PPLM extrapolates poorly despite using the scorer. This highlights that f s could be more useful for guiding inference when used to rank generated sequences, as in ICE and Iterative Sampling, as opposed to the conditional probabilities from f s being directly used to guide the generation, as in PPLM.\\n\\n**What does ICE do in each iteration?**\\n\\nTo analyze how the sentiment score of the text is changed over iterations, we plot the difference between the sentiment score of the output at each iteration and that of the initial sequence. We randomly sample 100 examples from the test set, and use ICE to increase their sentiment scores. We collect the output of ICE at every iteration using the scorer-free inference. We then plot the histogram of the increase in sentiment score (with respect to the initial score) for iterations 1, 4, 7, and 10 in Figure 2. As the iteration count increases, we observe that the increase in sentiment scores also becomes larger (i.e., the mode of the distribution is moving right), although the editing is not always successful (the scores of a small number of outputs decrease from the initial score and fall in the negative buckets). Overall, this shows that ICE is able to increase the sentiment score on average via iterative editing.\\n\\n### Protein Design on the ACE2 dataset\\n\\nDeveloping ways that generate more stable proteins could benefit drug discovery, as these proteins could potentially allow easier storage and have more reliable clinical effects compared to the existing proteins (Wang, 1999; Shire et al., 2004; Bloom et al., 2006; Deller et al., 2016; Webber et al., 2016). The objective of this task is to generate mutants of the human angiotensin-converting enzyme 2 (ACE2) wild-type sequence that have higher stability. The stability value of the mutants is measured using the change in free energy from the wild-type, or \\\\( \\\\Delta\\\\Delta G \\\\), via FoldX (Schymkowitz et al., 2005).\\n\\nThe wild-type itself has a \\\\( \\\\Delta\\\\Delta G \\\\) value of zero and more negative values represent more stable mutants. This synthetic task was created in Chan et al. (2021a) and we replicate their setup. The proteins are represented by a sequence of 83 amino acids out of a vocabulary of 20 different amino acids. In order to enforce that the mutations do not diverge too widely from the wild-type, a constant span of 8 amino acids (NTNITEEN) is kept fixed in all mutations. We view the training region to be the range of \\\\( \\\\Delta\\\\Delta G \\\\) values from \\\\(-4\\\\) to \\\\(+10\\\\). The extrapolation region refers to \\\\( \\\\Delta\\\\Delta G \\\\) values below \\\\(-4\\\\). For this task, we aim to generate mutants having more negative \\\\( \\\\Delta\\\\Delta G \\\\) values. We measure this by reporting the success rate of generating mutations having \\\\( \\\\Delta\\\\Delta G \\\\) below target values, \\\\( z^* \\\\), in the training region, \\\\(-1\\\\) and \\\\(-2\\\\), and 8.\\n\\nhttps://www.uniprot.org/uniprotkb/Q9BYF1/entry\\n\\nhttps://foldxsuite.crg.eu/\"}"}
{"id": "padmakumar23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement\\n\\n6.1. Implementation Details\\n\\nTraining the scorer\\nTo train $f_s$, we fine-tune ProtBert (Elnaggar et al., 2021) on the examples with ddG values in the training region from the dataset in Chan et al. (2021a).\\n\\nTraining the editor\\nWe create pairs of sequences using the mask-and-infill approach from Section 3.2 using a pre-trained Prot-T5-XL model (Elnaggar et al., 2021). We sample token masks from a Bernoulli distribution with ($p = 0.8$). To filter small perturbations, we set $\\\\delta$ to 1.5. We then fine-tune Prot-T5-XL on this data to serve as the ICE editor.\\n\\nInference\\nAt inference time, we start from the wild-type and generate mutations with and without the scorer, $f_s$ (Section 3.3). When using the scorer, we sample 5 sequences at each step, select the best one using $f_s$, and repeat the process for 10 iterations. For scorer-free inference, we generate sequences with beam size of 5 for 10 iterations.\\n\\n6.2. Results\\nICE outperforms baselines on extrapolation\\nTable 2 shows that ICE consistently outperforms Genhance, Sampling, and Iterative Sampling on all extrapolation targets. In addition, from Table 3, we see that ICE achieves a lower average ddG on the Top-100 and Top-1000 sequences. Interestingly, while Iterative Sampling achieves higher extrapolation rates than Genhance (Table 2), Genhance achieves a better average score on the Top-1000 and Top-100 sub-sets (Table 3) indicating that Genhance produces a smaller number of slightly more stable mutants (though still outperformed by ICE).\\n\\nThe scorer is valuable for all models in ACE2\\nIn this task, we begin the generation from the wild-type (ddG score of zero) and the scorer, $f_s$, reliably guides the generation process until the score of $-5$. As a result, we see that all the methods strongly benefit from using the scorer (Table 2). In Figure 3, we plot the histogram of scores of the generated mutations from ICE and the reported baselines. From Figure 3a, we see that the peaks of the distribution of scores for all models move in the negative direction to be centered closer to $-5$ as compared to Figure 3b highlighting the value of the scorer. We do however note that our approach is able to achieve some extrapolation even in the scorer-free regime, far outperforming Sampling and achieving extrapolation at a higher rate than Genhance.\\n\\n7. Protein Design on the AA V dataset\\nThe AA V dataset (Bryant et al., 2021) aims to study the fitness landscape of an adeno-associated virus (AA V) capsid protein that is a key component of gene therapy (Russell et al., 2017). Our goal is to obtain mutants of the AA V-2 wild type sequence that have a higher fitness value. We use the splits proposed by the FLIP benchmark (Dallago et al., 2021) for our experiments. Each mutant is a sequence of length varying from 734 to 750. Mutations are made on the wild-type sequence between indices 561 and 588. We use the provided low-vs-high split of the dataset to demarcate the training region and extrapolation region. The training region corresponds to fitness values below zero and the extrapolation region corresponds to positive fitness values.\\n\\nAt inference time, the generation process begins at the wild-type, with a fitness score of zero, and the model is expected to generate mutants that have a positive fitness score. We evaluate performance against target values, $z^\\\\ast$ in the training region, $-1$, and in the extrapolation region, $0$, $1$, and $2$.\\n\\n7.1. Implementation Details\\nTraining the scorer\\nThe scorer, $f_s$, is a CNN model trained on the examples in the training region. The architecture and hyperparameters for the CNN were chosen based on the FLIP benchmark.\\n\\nOn the low-vs-high split, the train correlation of the scorer is 0.82 and the test correlation is 0.34. This matches the best test correlation on this split obtained as part of the benchmark.\"}"}
{"id": "padmakumar23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement\\n\\nTable 2: Results on the ACE2 task. The objective is to generate mutants of the wild-type that have higher stability i.e. lower ddG value. Each table cell represents the success rate of generating mutations lower than the corresponding target. Bold values indicate the highest rates of extrapolation.\\n\\nICE achieves a higher rate of extrapolation than the reported baselines.\\n\\nFigure 3: Histograms of ddG scores (lower is better) of the final mutations generated by ICE and the baselines on the ACE2 task. ICE generates higher quality mutations than the baselines both with (Figure 3a) and without the scorer (Figure 3b) guiding the inference. Further, the scorer significantly improves performance for all methods.\\n\\nTable 3: Average ddG values (lower is better) of mutations generated from Iterative Sampling, Genhance, and ICE (each with the scorer). We report the average score of all 10000 mutations, the top 1000, and the top 100 as determined by the oracle. Bold values are the lowest average ddG value. ICE generates the most stable mutations.\\n\\nLibrary Size\\n\\nIterative Sampling\\nGenhance\\nICE\\nAll 10k\\n-4.326\\n-4.086\\n-4.660\\nTop 1k\\n-5.866\\n-6.030\\n-6.575\\nTop 100\\n-6.413\\n-7.354\\n-7.938\\n\\nTraining the editor\\nWe create pairs to train the ICE model by following the same strategy as in ACE2. We use the ProtT5-XL (Elnaggar et al., 2021) model to infill masks in the mutable region and score pairs with the scorer, fs, to create the editor training data.\\n\\n14 We then fine-tune Prot-T5-XL on this dataset. Since the length of the mutants is greater than the sequence length limit of Prot-T5-XL, we truncate them from the start to the last 512 tokens, which always contain the entire mutable region of the protein.\\n\\nInference\\nWe start from the wild-type and run inference on the ICE model as per Section 3.3. When using the scorer, we sample 5 generations, score them with fs, select the best one, and repeat for 10 iterations. For the scorer-free setup, we generate with a beam size of 5 for 10 iterations.\\n\\nEvaluation\\nWe generate 10,000 mutants with each method and report the success rate of generating mutations that are above the target scores, z\u2217. In lieu of a wet-lab experiment, we obtain fitness scores for each generated sequence via an oracle model, which is a CNN trained on the sampled (i.i.d.) split of the AA V dataset.\\n\\n15 This was chosen as the examples from the sampled split span fitness values across both the training region and extrapolation region.\\n\\nBaselines\\nWe compare our approach to the Sampling and Iterative Sampling baselines.\\n\\n16 We select the CNN architecture as it has the highest spearman correlation with the gold fitness values on the benchmark (Dallago et al., 2021). The model obtains a train spearman correlation of 0.93 and a test correlation of 0.92 on this split.\\n\\n16 As mentioned earlier, the PPLM and Score-Conditioned Generator baselines are not well suited for the protein tasks.\"}"}
{"id": "padmakumar23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolative Controlled Sequence Generation via Iterative Refinement\\n\\nVishakh Padmakumar\\nRichard Yuanzhe Pang\\nHe He\\nAnkur P. Parikh\\n\\nAbstract\\n\\nWe study the problem of extrapolative controlled generation, i.e., generating sequences with attribute values beyond the range seen in training. This task is of significant importance in automated design, especially drug discovery, where the goal is to design novel proteins that are better (e.g., more stable) than existing sequences. Thus, by definition, the target sequences and their attribute values are out of the training distribution, posing challenges to existing methods that aim to directly generate the target sequence. Instead, in this work, we propose Iterative Controlled Extrapolation (ICE) which iteratively makes local edits to a sequence to enable extrapolation. Specifically, we train the model on synthetically generated sequence pairs that demonstrate small improvement in the attribute value. Results on one natural language task (sentiment analysis) and two protein engineering tasks (ACE2 stability and AA V fitness) show that ICE considerably outperforms state-of-the-art approaches despite its simplicity.\\n\\n1. Introduction\\n\\nControlled generation, i.e., generating sequences $x$ with a desired attribute $z$, is a pervasive problem across multiple domains. In natural language processing (NLP), $z$ could represent the sentiment or the style (e.g., formality) of a sentence. In computational biology, $z$ could represent the stability, fluorescence, binding affinity, or other properties of a protein sequence.\\n\\nOccasionally, abundant supervised data of the form $(x, z)$ exist, such as Wikipedia domains or Gene Ontology categories (Keskar et al., 2019; Madani et al., 2020), enabling direct training of a conditional generation model $p(x|z)$. In cases where the amount of supervised pairs available is small, it is typical to train a scorer $f(x)$ on this data, which maps from an input sequence to an output attribute value. One can then use $f(x)$ to annotate a large corpus for training (Gehman et al., 2020) or directly use $f(x)$ during inference to guide the generation process of an unconditional model $p(x)$ (Dathathri et al., 2020; Yang & Klein, 2021).\\n\\nIn this work, we focus on applications where it is necessary to generate sequences with attribute values that extrapolate beyond the training distribution. For example, in biological sequence design, the problem of generating de novo (novel) sequences that are better than existing natural sequences with respect to some attribute (e.g., binding affinity to a specific target) is of critical importance to drug discovery (Arnold, 1998; Romero & Arnold, 2009; Freschlin et al., 2022). In creative text generation, we want to generate text that accentuates a stylistic attribute (e.g., humor) beyond simply imitating existing literature (He et al., 2019; Lyu et al., 2021).\\n\\nExisting controlled generation paradigms often extrapolate poorly when the range of attribute values $z$ in the training data has limited coverage, as both $p(x|z)$ and the attribute scorer $f(x)$ may not generalize outside of the training range of the attribute. For example, consider the ACE2 stability task (Chan et al., 2021c) shown in Figure 1, where the goal is to generate mutants of the ACE2 protein that have higher stability (lower ddG value). The training data contains sequences with $ddG$ values varying between $-4$ and $10$, but during inference, we want to generate more stable proteins than what we already have, e.g., extrapolate to $ddG$ less than $-5$. Since this range of $z$ is not supported on the training data, directly fitting $p(x|z)$ to the training data will result in unpredictable performance for $z < -4$.\\n\\nOur main assumption is that even though sequences with different target values, such as stable and unstable proteins, have distinct distributions, the process of transforming one sequence into a slightly improved version is applicable to different ranges of attribute values. For instance, in drug design, better proteins are often achieved by evolving from successive mutants, and in text generation, the sentiment can be strengthened by adding adverbs of degree. Therefore, we propose to the problem into a series of local improve-\\n\\n1\"}"}
{"id": "padmakumar23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement\\n\\nTraining\\n\\nInference\\n\\nLearn to improve a sequence with local edits\\n\\nIteratively edit a sequence to improve its attribute value\\n\\nFigure 1: An overview of the approach, Iterative Controlled Extrapolation (ICE), on the ACE2 stability task. Our initial dataset only contains proteins with ddG values (lower means more stable) between -4 and 10. During training, we generate perturbations of protein sequences and learn a generator to make local edits of a base sequence to reduce its ddG value.\\n\\nAt inference time, we iteratively apply the trained generator which achieves a ddG value of -5.57 after 10 iterations, more stable than the mutations seen during training.\\n\\nments made to a base sequence $x_0$. Our intuition is that this local improvement is stable across attribute values. Thus we can learn these local edits (or mutants) on the training distribution and apply it in succession at inference time to extrapolate to new ranges of attribute values.\\n\\nAs shown in Figure 1, to train the local editor, we synthetically generate close pairs of sequences using a masked language model (Devlin et al., 2019), such that they differ marginally in attribute values. During inference, our model uses two control tags, <inc> for increment and <dec> for decrement, to locally improve a sequence in the desired direction. Increasing the number of edits on the sequence enables extrapolation. We call our approach Iterative Controlled Extrapolation (ICE).\\n\\nWe evaluate our approach in both the natural language and protein domains. For text generation, we generate reviews with a sentiment either more positive or negative than seen in the training data. For protein engineering, we present results on two tasks\u2014generating mutations of the ACE2 protein that have higher stability measured by FoldX (Schymkowitz et al., 2005) and generating mutations of an adeno-associated virus (AA V) capsid protein (Bryant et al., 2021) with a higher fitness value.\\n\\nICE achieves consistent extrapolation on these three tasks, outperforming both standard methods for controlled generation such as PPLM (Dathathri et al., 2020) and a state-of-the-art extrapolative controlled generation method, Genhance (Chan et al., 2021a). In particular, in the AA V task, despite seeing zero sequences that are better than the wildtype AA V sequence during training, our model is able to generate a diverse range of better candidates as judged by an oracle model.\\n\\n2. Related Work\\n\\n2.1. Controlled Generation\\n\\nWhile controlled generation has been studied extensively in the literature, most of these methods do not focus on the extrapolation setting. We present an overview here situating our method and setup amongst prior work.\\n\\nMethods using control codes Keskar et al. (2019) and Madani et al. (2020; 2023) learn a conditional sequence model $p(x|c)$ where $c$ is the control code, encoding either a discrete or scalar value specifying the target attribute. However, these models may struggle when conditioning on unseen attribute values outside the training data range. Instead of conditioning on absolute target values, Lu et al. (2022) attempt to overcome this limitation by sampling generations from a model, iteratively quantizing these into...\"}"}
{"id": "padmakumar23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement\\n\\nmore fine-grained control codes and then using the highest bucket for controlled generation.\\n\\nIterative editing methods\\n\\nOur approach is also related to edit-based approaches (Guu et al., 2018; Mallinson et al., 2022; Novak et al., 2016), and closely connected to concurrent work, Welleck et al. (2023), that samples and scores generations from a model in order to learn edits in various NLP tasks. The key distinction to our work is that we focus on extrapolation. In the setup of Welleck et al. (2023), the model learns by seeking feedback on all generated pairs. However, we are explicitly interested in the case where the model is required to generate sequences outside the range where it is able to obtain feedback.\\n\\nLatent variable models\\n\\nAnother approach to achieve control is to model the attribute as a latent variable (Mueller et al., 2017; Gligorijevi\u0107 et al., 2021; Chan et al., 2021a;b). For example, Genhance (Chan et al., 2021a) proposes to represent the latent vector as a sum of attribute-relevant and attribute-irrelevant components. They then perturb the former to achieve extrapolation with applications to both NLP and biology. However, latent variable models on discrete sequence data are known to suffer from stability issues. In contrast, our approach makes edits in the text space, bypassing the problem of mapping from continuous latent spaces to discrete sequences.\\n\\nAttribute control via a scorer model\\n\\nAnother line of work (Dathathri et al., 2020; Yang & Klein, 2021; Li et al., 2022) adds attribute information via a scorer model \\\\( p(z|x) \\\\) to guide an unconditional language model \\\\( p(x) \\\\) at inference time. Because this approach heavily relies on the scorer model which is a trained classifier, it is not often conducive to extrapolation beyond the training data distribution, as we will show in our experiments. Alternatively, one could use the classifier as a reward model for reinforcement learning (Gong et al., 2019; Angermueller et al., 2020b) which suffers from similar shortcomings as the generator can exploit and amplify imperfections in the reward (Amodei et al., 2016; Ibarz et al., 2018; Pang et al., 2022).\\n\\n2.2. Biological Sequence Design\\n\\nThe problem of generating de novo sequences that improve upon natural sequences is of massive value to drug discovery, healthcare, and agriculture, as signified by the 2018 Nobel Prize in Chemistry on directed evolution (Arnold, 1998). As a result, there has been a growing interest in using machine learning for this problem (Yang et al., 2019; Angermueller et al., 2020a; Freschlin et al., 2022; Ren et al., 2022). Brookes et al. (2019) tackle extrapolation via a series of importance sampling distributions, in contrast to our controlled generation approach.\\n\\nThe iterative nature of ICE is internal to our modeling approach and thus not analogous to rounds in directed evolution which typically require access to an oracle (or wet lab experiment) after each round. Rather, at each round of directed evolution, ICE could potentially be (iteratively) run and its final output interpreted as the proposed candidates for validation.\\n\\nGenerating and experimentally validating novel sequences from large pretrained protein language models is also an exciting but nascent area. These approaches (Madani et al., 2021; Verkuil et al., 2022) typically generate sequences by conditioning on broad categories or backbone structures, rather than optimizing towards a specific target attribute (e.g., stability or fluorescence) as we seek to do.\\n\\n3. Our Approach\\n\\nProblem setup\\n\\nWe denote an input sequence with \\\\( \\\\mathbf{x} \\\\) to\\\\-kens as \\\\( \\\\mathbf{x} = (x_1, \\\\ldots, x_\\\\ell) \\\\) and an attribute value as \\\\( z \\\\in \\\\mathbb{R} \\\\). Here \\\\( \\\\mathbf{x} \\\\) can represent a protein sequence of \\\\( \\\\ell \\\\) amino acids, where \\\\( z \\\\) represents its stability, or a textual restaurant review of \\\\( \\\\ell \\\\) tokens, where \\\\( z \\\\) corresponds to the associated sentiment score. During training, we are typically given a large unsupervised corpus \\\\( D_{\\\\text{unsup}} = \\\\{x_m\\\\}_{m=1}^M \\\\) of size \\\\( M_{\\\\text{unsup}} \\\\) and a much smaller supervised corpus of sequences paired with attribute values, \\\\( D_{\\\\text{sup-train}} = \\\\{(x_m, z_m)\\\\}_{m=1}^{M_{\\\\text{sup-train}}} \\\\) of size \\\\( M_{\\\\text{sup-train}} \\\\). Let \\\\( \\\\alpha^- \\\\) and \\\\( \\\\alpha^+ \\\\) denote the lower and upper bound of \\\\( z \\\\) in \\\\( D_{\\\\text{sup-train}} \\\\) respectively, i.e., \\\\( z \\\\in [\\\\alpha^-, \\\\alpha^+] \\\\) for all \\\\( z \\\\) in the training examples. We refer to this region as the training region of scores.\\n\\nOur goal is to generate sequences that have an attribute value greater than (or less than) a target attribute value \\\\( z^* \\\\). In particular, we aim to extrapolate beyond the training region, i.e., \\\\( z^* < \\\\alpha^- \\\\) or \\\\( z^* > \\\\alpha^+ \\\\) depending on the application. We refer to these regions as the extrapolation region of scores.\\n\\nFurther, we assume that we have access to a scorer \\\\( f_s \\\\) that is trained on \\\\( D_{\\\\text{sup-train}} \\\\) to predict the attribute value of each sequence, i.e., \\\\( \\\\hat{z} = f_s(\\\\mathbf{x}) \\\\). While \\\\( f_s \\\\) may achieve high performance on the training region of \\\\( z \\\\), it is not trained on data from the extrapolation region and hence it can perform poorly when scoring examples in this range. Thus \\\\( f_s \\\\) should not be regarded as an oracle.\\n\\n3.1. Overview\\n\\nThe core component of ICE is a local editor that modifies a short span within a sequence to improve its attribute value. Specifically, it takes in an input sequence \\\\( \\\\mathbf{x} \\\\) and a control token \\\\( c \\\\) that specifies whether to increase (\\\\( c = \\\\text{<inc>} \\\\)) or decrease (\\\\( c = \\\\text{<dec>} \\\\)) the attribute value, and outputs an improved sequence \\\\( \\\\tilde{x} \\\\). We model the local editor \\\\( p(\\\\tilde{x}|\\\\mathbf{x}, c) \\\\) using a Transformer encoder-decoder model (Vaswani, 2017).\"}"}
{"id": "padmakumar23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement (et al., 2017). We train the editor by synthesizing pairs of sequences with a small difference in attribute value using masked language modeling (Section 3.2).\\n\\nAt inference time, starting with an initial sequence \\\\( x_0 \\\\), we edit it iteratively until some stopping criteria is reached. Specifically, in iteration \\\\( k \\\\), we edit the current sequence \\\\( x_k \\\\) to produce \\\\( x_{k+1} \\\\) by:\\n\\n\\\\[\\nx_{k+1} \\\\sim p_\\\\theta(\\\\cdot | x_k, c)\\n\\\\]\\n\\n(1)\\n\\nEach iteration is expected to move the attribute value of \\\\( x_k \\\\) toward \\\\( z^* \\\\). We explore different ways of selecting the best candidate at each step of the inference as well as the stopping criteria of the inference process in Section 3.3.\\n\\n### 3.2. Learning Local Edits from Perturbations\\n\\nTo train the local editor, we perturb examples from \\\\( D_{sup-train} \\\\) to generate training pairs with a small improvement toward the target value. Specifically, given a sequence from the training region, \\\\( D_{sup-train} \\\\), we mask random tokens in it, and use a masked language model to infill these to produce its perturbation (Figure 1). The masked language model is trained on the unsupervised data \\\\( D_{unsup} \\\\) such that the infill produces a valid sequence. To ensure that we make only small improvements, we predict the attribute value of each sequence using the scorer \\\\( f_s \\\\), and retain only those pairs where the absolute difference in the attribute value is below a threshold \\\\( \\\\delta \\\\).\\n\\nEach pair of the original sequence and its perturbation gives us two examples for the editor: generating the perturbed sequence from the original sequence, and vice versa. Recall that the editor also takes in a control token that specifies whether the edit should increase or decrease the attribute value. For each input-output pair, we set the control code to be \\\\(<inc>\\\\) if the attribute value of the input sequence is less than that of the output sequence measured by the scorer \\\\( f_s \\\\), and \\\\(<dec>\\\\) otherwise. Given tuples of the input sequence, the output sequence, and the control code, we then train the editor \\\\( p_\\\\theta \\\\) on this dataset.\\n\\n### 3.3. Inference\\n\\nAt inference time, we run the editor iteratively as described in Eq. (1).\\n\\n#### Decoding method\\n\\nDuring each iteration, we experiment with two different ways in which to select the best candidate out of a set of generated sequences:\\n\\n- **Scorer-free generation**: At each iteration of Equation (1), we perform generation using beam search relying on the ICE model likelihood to control the generation process.\\n\\n- **Scorer-guided generation**: At each iteration, we generate a set of sequences via top-\\\\( k \\\\) sampling, score these with \\\\( f_s \\\\) and select the sequence assigned the highest (or lowest) score depending on the desired target value.\\n\\nWhile \\\\( f_s \\\\) is reliable in the training region, it is unclear if the guidance provided is beneficial to the ICE model as it generates sequences having attribute value in the extrapolation region.\\n\\n#### Stopping criteria\\n\\nThe objective of the task is to edit the input sequence to have an attribute value greater than (or less than) the target value \\\\( z^* \\\\). However, reliably identifying when the inference process has reached \\\\( z^* \\\\) is difficult as it lies in the extrapolation region. In this work, we run inference for a constant number of iterations. We include additional discussion on the stopping condition in Appendix C.4.\\n\\n### 4. Experimental Setup\\n\\nWe evaluate our approach on one NLP task and two protein design tasks\u2014sentiment controlled generation (Section 5), the ACE2 stability task (Section 6), and the AA V fitness task (Section 7).\\n\\n#### 4.1. Evaluation\\n\\nWe are interested in measuring the ability of a model to successfully edit a sequence to have an attribute value greater than (or lesser than) a target value \\\\( z^* \\\\). In our experiments, we report the success rate or the fraction of sequences that the model is able to edit to meet this criterion as determined by an oracle model. The oracle varies based on the task and is detailed in each of the experiment sections (Section 5, Section 6, Section 7).\\n\\n#### 4.2. Baselines\\n\\nWe benchmark the performance of our method against the following baselines. (a) **Sampling**: A simple baseline is to directly edit sequences using a masked language model. Mirroring the synthetic data creation process from Section 3.2, we mask and infill a random span within the initial sequence to change its attribute value. (b) **Iterative Sampling**: To ablate the contribution of the editor model in ICE, we replace it with a mask-and-infill editor using a masked language model; the rest of the iterative algorithm is the same as ICE with the Scorer-Guided inference method. (c) **Genhance**: We compare to Genhance (Chan et al., 2021a), an extrapolative baseline which performs controlled generation by making perturbations in a latent space learned to encode the attribute value. Increasing the size of these perturbations during inference enables extrapolation.\"}"}
{"id": "padmakumar23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrapolation in Controlled Sequence Generation via Iterative Refinement\\n\\n| Target ddG | Training Region | Extrapolation Region |\\n|------------|-----------------|---------------------|\\n| -1         | -2.5            | -5 -6 -7             |\\n\\nICE w/ Scorer: Varying K for sampling\\n\\n| Iteration | TopK = 15 | TopK = 10 | TopK = 5 |\\n|-----------|-----------|-----------|----------|\\n| 10        | 0.997     | 0.964     | 0.249    |\\n|           | 0.083     | 0.01      |          |\\n\\n| Iteration | TopK = 15 | TopK = 10 | TopK = 5 |\\n|-----------|-----------|-----------|----------|\\n| 5         | 0.998     | 0.966     | 0.283    |\\n|           | 0.091     | 0.016     |          |\\n\\n| Iteration | TopK = 15 | TopK = 10 | TopK = 5 |\\n|-----------|-----------|-----------|----------|\\n| 2         | 0.982     | 0.648     | 0.041    |\\n|           | 0.040     | 0.004     | 0.000    |\\n\\nICE Scorer-Free: Varying beam size\\n\\n| Iteration | Beam Size = 10 | Beam Size = 5 | Beam Size = 3 |\\n|-----------|----------------|----------------|----------------|\\n| 10        | 0.930          | 0.572          | 0.059          |\\n|           | 0.013          | 0.017          | 0.002          |\\n\\n| Iteration | Beam Size = 10 | Beam Size = 5 | Beam Size = 3 |\\n|-----------|----------------|----------------|----------------|\\n| 5         | 0.852          | 0.440          | 0.030          |\\n|           | 0.006          | 0.000          | 0.000          |\\n\\n| Iteration | Beam Size = 10 | Beam Size = 5 | Beam Size = 3 |\\n|-----------|----------------|----------------|----------------|\\n| 2         | 0.620          | 0.182          | 0.001          |\\n|           | 0.000          | 0.000          | 0.000          |\\n\\nTable 8: Evaluation on the ACE2 task to study the interaction between the generation hyperparameters and the number of iterations at inference time. Each table cell represents the fraction of mutations with a ddG value lower than the corresponding target. We vary k for top-k sampling for scorer-guided inference and vary beam size for scorer-free inference. We find that the results are largely stable with respect to these hyperparameters at the end of inference (i.e., iteration 10).\\n\\nEarly on during inference (i.e., iteration 2), we find that a higher top-k value and beam size respectively result in better performance but this largely evens out by iteration 5 and 10.\\n\\n| Iteration | Average Score |\\n|-----------|---------------|\\n| 1         | -0.673        |\\n| 2         | -2.051        |\\n| 3         | -2.879        |\\n| 4         | -3.272        |\\n| 5         | -3.446        |\\n| 6         | -3.522        |\\n| 7         | -3.551        |\\n| 8         | -3.558        |\\n| 9         | -3.555        |\\n| 10        | -3.567        |\\n\\nTable 9: Average output scores of f_s as a function of iterations in the ACE2 task. Each cell is an average of the scores assigned to the 10,000 mutants generated with scorer-guided inference in Table 2. We observe that the output of f_s plateaus near the boundary of the training region at around -3.5 making it unreliable as a stopping condition for the generation process.\"}"}
