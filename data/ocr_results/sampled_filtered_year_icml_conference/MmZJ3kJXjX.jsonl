{"id": "MmZJ3kJXjX", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\nWightman, R. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019.\\n\\nWortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al. Modelsoups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, pp. 23965\u201323998. PMLR, 2022.\\n\\nWu, X., Fu, X., Liu, Y., Lim, E.-P., Hoi, S. C., and Sun, Q. A large-scale benchmark for food image segmentation. In Proceedings of the 29th ACM International Conference on Multimedia, pp. 506\u2013515, 2021.\\n\\nXie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., and Luo, P. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077\u201312090, 2021.\\n\\nXie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10687\u201310698, 2020.\\n\\nXu, Y., Zhao, S., Song, J., Stewart, R., and Ermon, S. A theory of usable information under computational constraints. In International Conference on Learning Representations, 2019.\\n\\nYou, K., Liu, Y., Wang, J., and Long, M. Logme: Practical assessment of pre-trained models for transfer learning. In International Conference on Machine Learning, pp. 12133\u201312143. PMLR, 2021.\\n\\nYu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.\\n\\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115, 2021.\\n\\nZhang, M. and R\u00e9, C. Contrastive adapters for foundation model group robustness. arXiv preprint arXiv:2207.07180, 2022.\\n\\nZong, B., Song, Q., Min, M. R., Cheng, W., Lumezanu, C., Cho, D., and Chen, H. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In International Conference on Learning Representations, 2018.\\n\\nZoran, D. and Weiss, Y. From learning models of natural image patches to whole image restoration. In 2011 international conference on computer vision, pp. 479\u2013486. IEEE, 2011.\\n\\nZoran, D. and Weiss, Y. Natural images, gaussian mixtures and dead leaves. Advances in Neural Information Processing Systems, 25, 2012.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Appendix\\n\\nA.1. Gaussian Models\\n\\nBesides the fact that Gaussian models make great well-posed problems for pretrained models, the idea of evaluating foundation models on synthetic Gaussian datasets also stems from two observations previously made in the literature. (1) Zoran & Weiss (2012) showed that simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches can successfully be used to model the statistics of natural images, which include contrast, textures at different scales and orientations, and boundaries of objects in the reference. Specifically, since our target is pretrained vision models, the capabilities of perceiving contrasts and edges etc are centric. Besides image patches, there are some discussions in the literature about how images patches connects to whole images (Zoran & Weiss, 2011; Ji et al., 2017). (2) Nevertheless, general GMMs do not yield themselves for analytic derivation of accuracy-robustness trade-offs. Luckily, some recent works on Gaussian universality (Pesce et al., 2023, Theorem C.1, Fig 6) have showed that for the overparameterized setting general linear models for GMMs and Gaussians both show similar training and generalization errors even when the underlying labels are strongly correlated with the data structure. Moreover, Gaussian models can be readily used to analytically derive expression for efficiently measuring accuracy-robustness trade-offs. Although the models used in real life came from richer model classes, foundation models do lie strongly in the overparameterized regime. We design our testing framework using similar Gaussian models and test their effectiveness empirically in understanding performance of foundation models on downstream tasks.\\n\\nA.2. Usage\\n\\nWe view SynBench as a \u201cnecessary\u201d and \u201cminimum\u201d model test in the sense that, with perfect data sampled from an ideal distribution, any undesirable deteriorated behavior (such as weakened robustness) reveals the weaknesses of the representation model that could possibly lead to vulnerabilities in real-life downstream tasks. Therefore, in designing this minimum test, it is important that the task has a theoretical ideal (and optimal) solution (i.e. the trade-off preserved by class conditional Gaussians, Theorem 3.1 iv).\\n\\nHere are some possible scenarios to use our developed tool:\\n\\n\u2022 model auditing: use SynBench to generate diverse psuedo tasks (e.g., with different difficulty levels) and compare them with theoretically optimial results, for a comprehensive evaluation on the capability of a pre-trained model\\n\u2022 hyperparameter tuning: as shown in Sec. 4.3, SynBench can be used for hyperparameter selection in robust linear probing, which leads to improved performance in the considered downstream tasks.\\n\u2022 model selection (without using downstream data): without the knowledge of downstream applications, one can use SynBench to rank the quality of pre-trained representations (e.g., the example shown in Figure 4). It is also possible to incorporate some known statistics of the downstream dataset into guided synthetic data generation and evaluation in SynBench, as discussed in Sec. 4.4.\\n\u2022 model training: while updating a model in the pre-training state, one can use SynBench to ensure the model performance (in terms of SynBench-Score) is aligned.\\n\\nA.3. Objective\\n\\n\\\\[ E_{\\\\theta, \\\\epsilon}(a_t) = E_{s \\\\sim \\\\mathcal{U}, x - \\\\bar{\\\\mu} | y \\\\sim \\\\mathcal{N}(\\\\mu, \\\\Sigma)} | h(\\\\|\\\\bar{\\\\Delta}\\\\|_2 | f_{\\\\epsilon}(x) = y, a > a_t, \\\\mu = s \\\\cdot 1_d / \\\\sqrt{d}, \\\\Sigma = I_d) \\\\]\\n\\n\\\\[ E_{s,x} | h(\\\\|\\\\bar{\\\\Delta}\\\\|_2 | f_{\\\\epsilon}(x) = y, a_{(s, \\\\epsilon)} > a_t) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} E_x | h(\\\\|\\\\bar{\\\\Delta}\\\\|_2 | f_{\\\\epsilon}(x) = y, a_{(s_i, \\\\epsilon)} > a_t). \\\\]\"}"}
{"id": "MmZJ3kJXjX", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For any sample $x$, the standard accuracy is the solution of the convex problem\\n\\n$$\\\\Sigma = \\\\arg \\\\min_{\\\\Sigma} \\\\Rightarrow \\\\Delta = \\\\arg \\\\min_{\\\\Delta} \\\\Rightarrow \\\\|\\\\Sigma - \\\\mu\\\\|_2$$\\n\\nis the typical sign function and $z$ is the standard accuracy by $\\\\Phi(\\\\tilde{w}) = \\\\Phi(\\\\mu + \\\\Sigma \\\\cdot \\\\delta)$.\\n\\nAs the scaled bound of correct samples, we scale the bound by the distance from Gaussian $\\\\bar{\\\\Delta}$.\\n\\nWhat Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\n$\\\\mu$ is the solution of the convex problem $\\\\Sigma = \\\\arg \\\\min_{\\\\Sigma} \\\\Rightarrow \\\\Delta = \\\\arg \\\\min_{\\\\Delta} \\\\Rightarrow \\\\|\\\\Sigma - \\\\mu\\\\|_2$. The corresponding decision boundary is at $f(\\\\tilde{w}, \\\\Sigma) = 1$.\\n\\nTheorem A.1. (iii) For sample $x$, $\\\\|\\\\Sigma - \\\\mu\\\\|_2$.\\n\\n(i) the bound (decision margin)\\n\\n(ii) the scaled bound $\\\\|\\\\Sigma - \\\\mu\\\\|_2$\\n\\n(iv) the expected scaled bound of correct samples $\\\\|\\\\Sigma - \\\\mu\\\\|_2$.\\n\\nProof. (iii) the standard accuracy $\\\\Phi(\\\\mu + \\\\Sigma \\\\cdot \\\\delta)$. (iv) $\\\\|\\\\Sigma - \\\\mu\\\\|_2$ is subject to the positions of two Gaussians, we scale the bound by the distance from Gaussian $\\\\bar{\\\\Delta}$.\\n\\nFor the Bayes optimal robust classifier in equation 4, we can calculate the analytical $\\\\Phi(\\\\mu + \\\\Sigma \\\\cdot \\\\delta)$ gives $\\\\|\\\\Sigma - \\\\mu\\\\|_2$.\\n\\nWhat Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\n$\\\\Delta = \\\\arg \\\\min_{\\\\Delta} \\\\Rightarrow \\\\|\\\\Sigma - \\\\mu\\\\|_2$. The corresponding decision boundary is at $f(\\\\tilde{w}, \\\\Sigma) = 1$.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(iv) For sample $x \\\\sim P_{\\\\mu_1, \\\\mu_2, \\\\Sigma}$, let $a$ denote the accuracy, $t$ denote $x - \\\\mu_1 + \\\\mu_2$, and $w$ denote $\\\\Sigma^{-1}(\\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu} - z \\\\Sigma))$. From (iii), we have that the standard accuracy of conditional Gaussian samples with the Bayes optimal (robust) classifier is $\\Phi(\\\\tilde{\\\\mu}^T w \\\\|w\\\\| \\\\Sigma)$, so $\\\\tilde{\\\\mu}^T w \\\\|w\\\\| \\\\Sigma = \\\\Phi^{-1}(a)$. Since for binary classification, we only care about accuracy from 0.5 to 1, so we should have $\\\\tilde{\\\\mu}^T w > 0$.\\n\\nNow consider the classifier in equation 4 and the corresponding scaled bound from (ii),\\n\\n$$\\\\|\\\\bar{\\\\Delta}\\\\|_2 = |(x^T \\\\Sigma^{-1}(\\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu} - z \\\\Sigma))| = |t^T w| |\\\\tilde{\\\\mu}^T w| = |t^T w| \\\\tilde{\\\\mu}^T w.$$  \\n\\nSince $t \\\\sim N(y \\\\tilde{\\\\mu}, \\\\Sigma)$, we have $t^T w \\\\sim N(y \\\\tilde{\\\\mu}^T w, w^T \\\\Sigma w)$. When we only want to get the expected scaled bound of the correctly-classified samples, we have that\\n\\n$$E\\\\|\\\\bar{\\\\Delta}\\\\|_2 | f \\\\epsilon(x) = y^T \\\\tilde{\\\\mu} \\\\Phi^{-1}(a)$$\\n\\n$$= 1 + \\\\sqrt{w^T \\\\Sigma w} \\\\tilde{\\\\mu}^T w \\\\sqrt{2/\\\\pi} \\\\Phi^{-1}(a) e^{-1/2 (\\\\Phi^{-1}(a))^2}.$$  \\n\\nBy replacing $\\\\tilde{\\\\mu}^T w \\\\sqrt{w^T \\\\Sigma w}$ by $\\\\Phi^{-1}(a)$, we got\\n\\n$$E\\\\|\\\\bar{\\\\Delta}\\\\|_2 | f \\\\epsilon(x) = y^T \\\\tilde{\\\\mu} \\\\Phi^{-1}(a)$$\\n\\n$$= 1 + \\\\sqrt{w^T \\\\Sigma w} \\\\tilde{\\\\mu}^T w \\\\sqrt{2/\\\\pi} \\\\Phi^{-1}(a) e^{-1/2 (\\\\Phi^{-1}(a))^2}.$$\"}"}
{"id": "MmZJ3kJXjX", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We first formally state our theorem (proofs in Appendix A.4).\\n\\nWhen sampling from this idealized distribution, we eliminate the factor of data bias and can test the accuracy and robustness degradation in an ideal setting.\\n\\nLet $\\\\parallel \\\\cdot \\\\parallel$ denote the minimal perturbation of a sample. Specifically, it is stated that the optimal robust classifier (Bhagoji et al., 2019; Dobriban et al., 2020).\\n\\nA classifier can be measured by the minimum magnitude of perturbation that causes misclassification, i.e. $\\\\parallel \\\\Delta \\\\parallel$. We focus on the class-balanced case for $\\\\parallel \\\\Delta \\\\parallel$. We refer the readers to Appendix A.5 for general considerations of perturbation levels.\\n\\nFor a given classifier $f$, we derive the following result as a direct application of the fact. To simplify the exposition, we focus on the linear classifier that minimizes robust classification error.\\n\\nSpecifically, it is stated that the optimal robust classifier specifies the desired size of margin and demonstrates the robustness-accuracy trade-off given in Theorem 3.1(iv).\\n\\nWe note that for samples drawn from the probability density function of the synthetic data manifold, Figure 2(a) depicts a class-conditional 2D Gaussian case with decision margin $\\\\bar{\\\\Delta}$ that serves as the foundation of our SynBench framework.\\n\\nIn what follows, we will leverage this point and focus on the top-1 prediction (Szegedy et al., 2013; Goodfellow et al., 2014). Luckily, one can readily solve for the optimization problem exactly is hard (Katz et al., 2017; Sinha et al., 2018).\"}"}
{"id": "MmZJ3kJXjX", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In our implementation, we take \\\\( s \\\\) of the representations rendered by representation networks representation network, we compare the expected bounds under a threshold accuracy as the reference. Given a \\\\( \\\\| \\\\bar{\\\\Delta} \\\\| \\\\) for both raw data and the representations (i.e. \\\\( \\\\| \\\\bar{\\\\Delta} \\\\| \\\\)).\\n\\nRecall that we aim to calculate \\\\( E(\\\\theta, \\\\epsilon, a) \\\\), where \\\\( \\\\theta, \\\\epsilon, a \\\\) are independent of pretrained network parameters. By Theorem 3.1(ii), we consider the optimal \\\\( \\\\text{argmax}_t \\\\mathbb{E}_s f_t(a(s, \\\\theta, \\\\epsilon)) \\\\).\\n\\nTheorem 3.1(iv) directly shows a distinguishing from the scaled bound to be derived for representations. Given a pretrained network, we gather representations induced by different pretrained models. In the following sections, we will illustrate how to calculate the bound induced by robust Bayes optimal classifier in the representation space to calculate the scaled bound.\\n\\nRaw data. For raw data synthesized from \\\\( \\\\mathcal{N}(\\\\mu, \\\\Sigma) \\\\) and the inner expectation is estimated empirically. It should be noted that now the Bayes optimal classifier does not necessarily coincide with the robust Bayes optimal classifier. As detailed in Section 3.3, we note that the Bayes optimal classifier does not necessarily coincide with the robust Bayes optimal classifier. The subscript \\\\( x \\\\) is given by Theorem 3.1(iv), and all the inner expectation term \\\\( \\\\mathbb{E}_t \\\\mid a \\\\mathbb{E}_x \\\\).\\n\\nWhat Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\nIn the input space, the covariance in the input space.\\n\\nEven when we synthesized the dataset with an identity matrix, the representations of the Gaussian realizations and quantify the inner expectation term. Even when we synthesized the dataset with an identity matrix, we calculate the scaled bound as a function of accuracy in Figure 2(b), which holds true when the data follow equation 1 exactly. In SynBench, we define the accuracy-constrained expected bound as the reference, enabling a fair comparison among representations. We highlight that Theorem 3.1(iv) directly shows a robustness-accuracy trade-off. We plot the expected scaled bound as a function of accuracy in Figure 2(b), which holds true when the data follow equation 1 exactly.\\n\\nIn the SynBench framework, we can obtain an accuracy and an expected bound-threshold accuracy plot for the input raw data (synthetic data) and representations. We emphasize that with more accurate modeling of the data (e.g. input synthetic data), even when we synthesized the dataset with an identity matrix, the Bayes optimal classifier does not necessarily coincide with the robust Bayes optimal classifier.\\n\\nWe reiterate that with more accurate modeling of the data, we can obtain the desired range of \\\\( \\\\bar{\\\\Delta} \\\\) for correctly-classified samples. To integrate over all the desired threshold accuracy, we use the area under the curve (AUC) and give the ratio to the area of the probing results in Figure 3.\\n\\nTo distinguish representations for ViT-B/16. (Left) The expected bound-threshold accuracy plot for the input raw data. (Right) To calculate the SynBench-Score for ViT-B/16, we use the definition \\\\( \\\\Phi \\\\). (6) To calculate the SynBench-Score for ViT-B/16, we use the definition \\\\( \\\\Phi \\\\). (6) We hereby denote the desired accuracy from 55% to almost 100%, we set the starting and ending \\\\( \\\\Phi \\\\) and SynBench-Score \\\\( \\\\Phi \\\\). (7) = 55, \\\\( \\\\Phi \\\\) and SynBench-Score \\\\( \\\\Phi \\\\). (7) = 0. (Right) To calculate the SynBench-Score for ViT-B/16, we use the definition \\\\( \\\\Phi \\\\). (6) We hereby denote the desired accuracy from 55% to almost 100%, we set the starting and ending \\\\( \\\\Phi \\\\) and SynBench-Score \\\\( \\\\Phi \\\\). (7) = 55, \\\\( \\\\Phi \\\\) and SynBench-Score \\\\( \\\\Phi \\\\). (7) = 0. (Right) To calculate the SynBench-Score for ViT-B/16, we use the definition \\\\( \\\\Phi \\\\). (6) We hereby denote the desired accuracy from 55% to almost 100%, we set the starting and ending \\\\( \\\\Phi \\\\) and SynBench-Score \\\\( \\\\Phi \\\\). (7) = 55, \\\\( \\\\Phi \\\\) and SynBench-Score \\\\( \\\\Phi \\\\). (7) = 0.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\nWith gathered pairs of accuracy and expected bound, we filter ones whose accuracy is below a threshold accuracy (x-axis), and calculate the accuracy-constrained expected bound to reflect the robustness level (y-axis). With this, the AUC will counter for the discriminative power of the foundation model given an idealized distribution, as well as the robustness level. We refer readers to Appendix A.7 for the pseudo-code.\\n\\n4. Experimental Results\\n\\nIn Section 4.1, we give the setup of our experiments. We exemplify the use of SynBench in making efficient comparisons of pretrained representations in Section 4.2. We compare SynBench with baseline methods and demonstrate the supremacy of SynBench-Score in giving consistent model suggestions and high correlation with performance on possible downstream tasks. In Section 4.3, we study how SynBench can be used to select robust linear probing hyperparameters. In Section 4.4, we show how to model the covariance matrix $\\\\Sigma$ used for synthesizing Gaussian samples given prior knowledge of the downstream data distribution.\\n\\n4.1. Experiment Setup and Baselines\\n\\nIn the following sections, we will calculate SynBench-Scores for pretrained models and make pair-wise comparisons. For example, ViT-B/16 is a fine-tuned pretrained model from ViT-B/16-in21k. By checking their SynBench-Scores, we could understand how the fine-tuning procedure helps or worsens the performance. In order to systematically understand how each network attribute affects the robustness-accuracy performance, it is desirable to control the variates. We list and compare 10 pretrained vision transformers (ViTs) (Dosovitskiy et al., 2020; Chen et al., 2021; Caron et al., 2021) and ResNets (Chen et al., 2020c) in Appendix Table 5.\\n\\nAlthough to the best of our knowledge, there is no real-data-free evaluation method for pretrained representations, we refer to recent work (Whitney et al., 2020; You et al., 2021; Shao et al., 2022) and report the validation accuracy (Val loss), minimum description length (MDL), surplus description length (SDL), logarithm of maximum evidence (LogME) and self-challenging Fisher discriminant analysis (SFDA), following the official implementation from the literature on our synthetic proxy task as baselines (Whitney et al., 2020; Shao et al., 2022). In essence, we expect these real-data-free evaluations for pretrained models can give meaningful performance assessments of possible downstream tasks. For this purpose, we take an average of the accuracy in 27 downstream tasks (cf. (Radford et al., 2021), Table 10) as in the literature (Dosovitskiy et al., 2020; Radford et al., 2021; Li et al., 2022; Fang et al., 2023; Yu et al., 2022) to give a sense of the general performance on possible downstream tasks, and report the Pearson correlation coefficients with SynBench-Scores. Building on top of these, we also show the consistency of SynBench suggestions given different numbers of synthetic realizations compared to the baselines.\\n\\nTo provide a comprehensive evaluation, we give SynBench-Score $\\\\theta$, $\\\\epsilon$, $a_t$ with $a_t$ ranging from 0.7 to 0.9, and $\\\\epsilon$ from 0 to 0.8. Due to the space limit, $a_t \\\\neq 0$. Some results are deferred to the appendix. We refer the readers to Appendix A.9 for the detailed runtime analysis. Besides the SynBench-Score, we will also report the standard accuracy (SA) and robust accuracy against adversarial perturbations (RA) for studying robustness-accuracy performance.\\n\\n4.2. SynBench Analysis of Pretrained Representations\\n\\nComparing model attributes. We list the SynBench-Score of the 10 pretrained representations with their standard and robust accuracy on the class-conditional Gaussian proxy task in Table 1. The robust accuracy is obtained by $\\\\ell_2$ PGD attack (Madry et al., 2018) with attack strength $0.2$.\\n\\nBy referring to rows \\\"ViT-B/16\\\" and \\\"ViT-B/16-in21k\\\", we see that SynBench will suggest ViT-B/16 over ViT-B/16-in21k, implying that the fine-tuning is beneficial on ViT-B/16-in21k - both networks are pretrained on Imagenet 21k with supervision, whereas ViT-B/16 is further finetuned on Imagenet 1k. We can also use SynBench to evaluate the effect of model sizes. Specifically, we refer to rows \\\"ViT-Ti/16\\\", \\\"ViT-B/16\\\", \\\"ViT-L/16\\\", and see that ViT-B/16 and ViT-L/16 score much higher than ViT-Ti/16, suggesting larger models have better capacities for robustness and accuracy. It is noticeable that ViT-B/16 is generally on par with ViT-L/16 when we vary $\\\\epsilon$ (cf. Appendix Table 6).\\n\\nSimilar conclusions can also be drawn by referring to self-supervised pretrained representations, rows \\\"ViT-S/-DINO\\\" and \\\"ViT-B/-DINO\\\". Moreover, if we check rows \\\"ViT-B/16\\\" and \\\"ViT-B/16-DINO\\\", we compare two pretrained models of the same architecture but trained under different regimes, either supervised or self-supervised. Between these two models, SynBench favors self-supervised trained \\\"ViT-B/16-DINO\\\", echoing with the inductive bias of self-\"}"}
{"id": "MmZJ3kJXjX", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\nFigure 4: Pearson correlation between task-agnostic metrics (Val loss, MDL, SynBench, LogME, SFDA) and task-specific metrics (the average accuracy on 27 real-life tasks) as functions of the dataset size. Two dashed lines characterize the correlation by transfer datasets' accuracy.\\n\\nFigure 5: Comparison of model selections using task-agnostic benchmarks. We denote the model predicted to have better performance by \\\"selected\\\". Only SynBench gives consistent selections across varying data sample sizes. Refer to Appendix Table 8 for more details.\\n\\nSupervised contrastive learning discovered in recent literature (HaoChen & Ma, 2022).\\n\\nSynBench shows better correlation with real-data probing accuracy and robustness. We run baseline evaluations as described in Section 4.1 for the synthetic classification task on pretrained models with dataset size $n$ being $2048$, $8192$, $32768$ and list their results in Appendix Table 7.\\n\\nThroughout our experiments, we use 2048 test samples in the synthetic dataset. For Val loss, MDL, and SDL, $\\\\epsilon_{SC}$, the smaller the better; for LogME, SFDA, SynBench, the bigger the better. In Figure 4, we illustrate how the correlation between task-agnostic evaluation metrics and real-life data tasks varies with the dataset size $n$. Specifically, we calculate the Pearson correlation coefficients between the average accuracy in downstream tasks to scores given by Val loss, MDL, SDL, $\\\\epsilon_{SC}$, LogME, SFDA, and SynBench (SDL and $\\\\epsilon_{SC}$ are excluded from the figure since they fail to give concrete numbers for small dataset sizes). With 2k synthetic samples, SynBench gives 0.79, whereas Val loss, MDL, LogME, and SFDA range between 0.46 and 0.55; with 8k synthetic samples, SynBench gives 0.89, whereas Val loss, MDL, LogME, and SFDA range between 0.65 and 0.81, surpassing the correlation by vanilla out-of-distribution accuracy (ImageNet-c's 0.64 and ImageNet-a's 0.57); with over 30k synthetic samples, Val loss, MDL, and SynBench all indicate very strong correlation ($>0.9$) with real-life data accuracy, confirming the feasibility of probing pretrained representations in a task-agnostic yet effective way.\\n\\nTo validate the capability of SynBench in informing model robustness, we further conduct CW attack (Carlini & Wagner, 2017), on CIFAR10 test set and calculate its correlation with SynBench. With 2k, 8k, and 30k synthetic samples, SynBench is also able to demonstrate moderate correlation with coefficient ranging from 0.74 to 0.84. SynBench gives more consistent suggestions than baselines.\\n\\nWe run a finer grid on the dataset size $n \\\\in \\\\{2048, 4096, 8192, 16384, 32768\\\\}$ and compare the consistency of each metrics. Since LogME and SFDA showed worse correlation in the previous experiment, we exclude the two and only report the results on Val loss, MDL, and SynBench. We also include SDL to highlight its struggle with small sample size. In Figure 5, we give an example of the model selections between ViT-B/16 and ViT-B/16-in21k. Detailed numbers are reported in Appendix Table 8.\\n\\nIt is worth noting that SynBench consistently recommends ViT-B/16 over ViT-B/16-in21k, while other methods change with $n$. Besides better correlation and consistency, the run-time analysis in Appendix A.9 also confirms 500\u00d7 speedup over baselines using SynBench.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.18. Intuitions on How SynBench Predict Classification Performance across a Broad Range of Tasks\\n\\nThink of how representation learning research typically evaluate a model for transfer learning - by running tests on broad range of downstream tasks. And the reason behind this is to see how the model behaves in different scenarios. To theorize things, we believe the general behavior of a pretrained representation is measured by how it perform on tasks of different difficulty lexvels. That is why we think a fundamental part of our design is to simulate tasks of different difficulty levels.\\n\\nOne difference between SynBench and a traditional probing test is that, for example, we are using the classification problem of two highly overlapped Gaussian, instead of classifying ImageNet21k. We hope this clarification builds enough intuition to understand the following:\\n\\n1. We vary $s$ in equation 2 from 0.1 to 5 in increments of 0.1, which correspond to optimal accuracy (ground-truth difficulty) ranging from 55% to 100% and 50 difficulty levels. If we refer to Figure 8, we see each of the red points correspond to one of our simulated trials with difficulty levels (x-axis).\\n\\n2. Baseline methods are task/data dependant, which means they are somewhat bound to tasks of that similar difficulty levels. If we refer to Figure 8, it could be the single purple point with fixed level of difficulty.\\n\\n3. If we include certain knowledge of possible downstream data properties, say locality of pixel dependencies, then the prediction will indeed be more accurate (see our section 4.4).\\n\\nA.19. Rejection Mechanism\\n\\nSynBench is a task-agnostic benchmark and it is designed to be used to test pretrained models without the prior knowledge of the downstream task (e.g. model auditing etc). In the case when we do know some knowledge of the tasks, e.g. pixel dependencies, one can use the knowledge to fine-tune the GMM SynBench uses. However, in the case when we know exactly which downstream task will we do and the downstream datasets are accessible and representative, the best practice is to directly apply linear probing. If we are to come up with a rejection mechanism, then one can potentially use goodness-of-fit tests to verify the null hypothesis that the downstream data of interest are generated from a Normal distribution. If the data follow Normal distribution, the Mahalanobis distances should follow a Chi-Squared distribution.\\n\\n| Dataset  | Gaussian-I | Gaussian-H |\\n|----------|------------|------------|\\n| CIFAR10  | 0.37       | 0.65       |\\n| SVHN     | 0.58       | 0.83       |\\n| TinyImageNet | 0.31     | 0.51       |\\n\\nTable 20: The p-values in the hypothesis testing for Gaussian-I and Gaussian-H distributions.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\nWith degrees of freedom equal to the number of features. Then since the CDF for the appropriate degrees of freedom gives the probability of having obtained a value less extreme than this point, subtracting the CDF value from 1 gives the p-value.\\n\\nWe conduct the experiment for CIFAR10, SVHN, and TinyImageNet, and report the p-values in Table 20. Because these p-values are high, we can\u2019t reject this hypothesis. But if the p-value is below a threshold, one can reject this hypothesis.\\n\\nA.20. Pretrain Data versus Synthetic Data\\n\\nConducting evaluation with pre-train data can be infeasible/inappropriate due to three reasons. First of all, with the increasing use of self-supervision during the pretraining, the pre-train data can be unlabeled. Secondly, even in the case when the application scenerio is model training and the pre-train data is labeled, the evaluation scores based on the pre-train data can be inconclusive if the evaluation data are biased or under-representative (e.g. pretrained models tend to overfit to the pre-train data). Lastly, from the perspective of the model auditing, the data used for model pretraining can simply be private or inaccessible (e.g., Web-scale raw data).\\n\\nIn these scenarios, one can use SynBench to generate diverse pseudo tasks and non-private synthetic data for conducting comprehensive evaluation of a pre-trained model. By comparing to an idealized data distribution and the corresponding theoretically-optimial reference, SynBench-Score (as illustrated in Figure 1) can quantify the quality of representations, in the sense that the area under the curve (AUC) ratio closer to 1 means better representations.\\n\\nA.21. Limitations\\n\\nLinear probing.\\n\\nSynBench analysis focuses on linear probing performance, which is a popular, low-complexity evaluation protocal widely used in the community (Chen et al., 2020b; He et al., 2020), especially for large neural networks (foundation models). Other assessment tools of pretrained models, such as LogME (You et al., 2021), is also evaluated by the correlation coefficient between their metric and linear probing accuracy. For tasks other than classification, we do observe in some literature that SynBench-Score might still be informative, e.g. ViT-L/16 is reportedly performing worse than ViT-B/16 with MLA decoder in a food segmentation task from (Wu et al., 2021), DINO ViT-B performs better than DINO ViT-S in DA VIS 2017 Video object segmentation, and DINO ViT-S/16 performs better than DINO ViT-S/8 according to Jaccard similarity on PASCAL VOC12 dataset from (Caron et al., 2021). For fine-tuned pretrain representations, ViT-L/16 loses to ViT-B/16 on finetuned medical tasks with, e.g., X-ray images (Okolo et al., 2022, Table 4-8), and magnetic resonance imaging (Tummala et al., 2022, Table 2-3). Although we are unable to fully justify the relationship between SynBench-Score and non-classification tasks, we believe that if non-classification tasks such as object detection/regression can be translated into classification tasks, SynBench can be extended to those tasks.\\n\\nGaussian models.\\n\\n\u201cCan we trust the data representations from a pretrained image model, if it fails to have reasonable performance on simple synthetic datasets?\u201d This is the motivation for our work. When designing the task-agnostic and data-free framework, we narrow our scope for a more \u201cwell-posed\u201d problem, by using an idealized data distribution with tractable separability, lifting the need for real-life data. This enables interesting application scenerio such as model auditing, selection, training, and alignment. Therefore, ideologically, SynBench allows any idealized data distribution, provided that the optimal performance (e.g. accuracy-robustness as in our case) can be characterized. At the current stage, the practicality of SynBench owes to the idealized Gaussian distribution, whose optimal robust Bayes classifier is known.\\n\\nSynthetic tests.\\n\\nSince SynBench is a task-agnostic and data-free framework, it relies on synthetic data drawn from idealized data distribution with optimal performance. Albeit these synthetic data may inevitably miss intricate details of downstream tasks and data, this framework still provides an easy first check in representation quality.\\n\\nBinary classification.\\n\\nIn practice, we can break a multi-class classification task into \\\\( k \\\\) one-vs-rest binary classifications or even \\\\( k(k-1)/2 \\\\) one-vs-one binary classifications, where \\\\( k \\\\) is the number of classes. Translating to SynBench, we will set the class prior to \\\\( \\\\rho(i)/(1-\\\\rho(i)) \\\\) for the \\\\( i \\\\)th binary classification task and average over all \\\\( i \\\\in 1, \\\\ldots, k \\\\).\\n\\nA.22. More Foundation Models\\n\\nWe added more network architectures from the pretrained PyTorch Image Models. From Table 21, we can see that the model performance improves when the size of swin transformer grows, e.g. swin-base has lower SynBench-Score.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\ncompared with swin-large (0.25 vs 0.27). Also, swin transformers benefit from pretraining on a larger dataset (e.g. \u201cswinv2 base window12to16 192to256 22kft1k\u201d is pretrained on ImageNet21k before finetuned on ImageNet1k, while \u201cswinv2 base window16 256\u201d is directly trained on ImageNet1k). Our SynBench-Score also well correlates with the fine-tuning accuracy on ImageNet-1K. Additional randomly picked models includes EVA and CoCa models, which we list in Table 22.\\n\\n| Models                          | SynBench-Score ($\\\\epsilon = 0$) | ImageNet top-1 fine-tuned acc. |\\n|--------------------------------|----------------------------------|--------------------------------|\\n| swinv2 base window16 256       | 0.21                             | 84.5                           |\\n| swinv2 base window12to16 192to256 | 0.25                           | 86.4                           |\\n| swinv2 large window12to16 192to256  | 0.27                           | 87.3                           |\\n\\nTable 21: The SynBench-Score of Swin transformers. ImageNet top-1 accuracy is quoted from (Liu et al., 2021).\\n\\n| Models                          | SynBench-Score ($\\\\epsilon = 0$) | ImageNet zero-shot acc. |\\n|--------------------------------|----------------------------------|-------------------------|\\n| eva02 base patch16 clip 224.merged2b | 0.110                         | 74.7                    |\\n| CoCa ViT-B-32                   | 0.436                           | 82.6                    |\\n\\nTable 22: The SynBench-Score of misc models.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3. SynBench-guided \\\\( \\\\epsilon \\\\)-robust Linear Probing\\n\\nWhen performing linear probing on downstream datasets, one can implement \\\\( \\\\epsilon \\\\)-robust linear probing (Fan et al., 2021) for better robustness. Concretely, let \\\\( \\\\theta \\\\) be the pretrained representation network and \\\\( \\\\theta_c \\\\) be the probing layer parameters, \\\\( \\\\epsilon \\\\)-robust linear probing solves\\n\\n\\\\[\\n\\\\min_{\\\\theta_c} \\\\max_{\\\\delta} \\\\| \\\\delta \\\\|_2 \\\\leq \\\\epsilon\\n\\\\]\\n\\n\\\\( E(x,y) \\\\in D \\\\) Cross-entropy \\\\((f_{\\\\theta_c} \\\\circ f_{\\\\theta}(x+\\\\delta), y)\\\\). Here, we will show that the SynBench-guided \\\\( \\\\epsilon \\\\)-robust linear probing provides better insight into robustness-accuracy trade-off.\\n\\nIn Table 1, we only give SynBench-Scores with \\\\( \\\\epsilon = 0 \\\\). We refer readers to Appendix Table 6 for the full table with different \\\\( \\\\epsilon \\\\). We cite 4 pretrained representations' SynBench-Score in Table 2 and observe that, for each model, SynBench-score is not necessarily monotonic in \\\\( \\\\epsilon \\\\) (peaks are boldfaced). For example, the SynBench-Score for ViT-B/16 peaks at \\\\( \\\\epsilon = 0 \\\\), which indicates standard linear probing (i.e., \\\\( \\\\epsilon = 0 \\\\)) may not be the most effective way to probe pretrained representations in terms of robustness-accuracy performance. This interesting indication is consistent with recent findings (Fan et al., 2021).\\n\\nWe hereby implement \\\\( \\\\epsilon \\\\)-robust linear probing and verify that \\\\( \\\\epsilon = \\\\arg \\\\max_{\\\\epsilon} \\\\) SynBench-Score can indeed find the best robustness-accuracy trade-off according to Table 2. For instance, SynBench-Score peaks at \\\\( \\\\epsilon = 0 \\\\) for ViT-B/16 and correspondingly \\\\( 0.2 \\\\)-robust linear probing on ViT-B/16 representations improves TinyImagenet standard and robust accuracy by the most (+0.7% and +2.5%). We defer CIFAR10 results to the Appendix Table 10. The robust accuracy herein is obtained by AutoAttack (Croce & Hein, 2020).\\n\\n4.4. The Effect of Data Prior\\n\\nIn Section 3.4, it is stated that a more precise capture of the pretrained representation performance can be given if one has some prior knowledge of the downstream data distribution. In this section, we show this point by studying three specific downstream tasks, CIFAR10, SVHN, and TinyImageNet classifications, and give an example of the devised covariance matrix for SynBench synthetic Gaussians. In Table 3, we give the standard and robust accuracy on CIFAR10, SVHN, and TinyImageNet (robust accuracy obtained by AutoAttack). Comparing the rows \\\"ViT-B/16\\\" and \\\"ViT-L/16\\\", it is observed that ViT-L/16 is in fact performing better than ViT-B/16 on these three downstream tasks, whereas SynBench-Score with identity covariance suggests the opposite (cf. Table 1). To uncover the reason behind the inconsistency, we calculate the distance between the synthetic Gaussian used throughout the experiments till now (dubbed Gaussian-I) and these datasets in Appendix Table 11. Recall that Gaussian-I, \\\\( P_{\\\\mu_1, \\\\mu_2, \\\\Sigma} \\\\), has \\\\( \\\\mu_1 = -\\\\mu_2 = s_i \\\\cdot \\\\frac{1}{d} \\\\) and \\\\( \\\\Sigma = I_d \\\\). An easy modification on the covariance matrix \\\\( \\\\Sigma \\\\) leads us to Gaussian-H, \\\\( P_{\\\\mu_1, \\\\mu_2, \\\\Sigma_H} \\\\), with \\\\( \\\\mu_1 = -\\\\mu_2 = s_i \\\\cdot \\\\frac{1}{d} \\\\) and \\\\( \\\\Sigma_H = \\\\text{channel-wise block-diagonal} \\\\). Gaussian-H captures the case when the R,G,B channel entries are externally independent (hence overall a block-diagonal covariance matrix with each of the 3 blocks being \\\\( 224 \\\\times 224 \\\\)), and internally correlated based on locality (each block is a heptadiagonal matrix where only the main diagonal, and the first three diagonals above and below it have nonzero entries). Note that Gaussian-H is closer to the three datasets compared to Gaussian-I with respect to Fr\u00e9chet inception distance (FID) (Heusel et al., 2017) and Mahalanobis distance (MD) (Mahalanobis, 1936) according to Appendix Table 11. Based on Gaussian-H, SynBench now recommends ViT-L/16 over ViT-B/16 according to Table 4. We defer more results with Gaussian-H covariate synthetic data to Appendix Table 12-14. This result shows that SynBench can incorporate complex data structures and downstream data characteristics into the process of synthetic data generation.\\n\\n5. Discussion and Conclusion\\n\\nIn this paper, we explored how to extend the well-studied Gaussian data modeling techniques to systematically study the representation quality of pretrained image models, by proposing a task-agnostic and data-free framework, SynBench. With our synthetic Gaussian analysis, the robustness-accuracy relationship becomes tractable and naturally yields a theoretically-derived robustness-accuracy trade-off, which serves as the reference for pretrained representations. We validated the usefulness of SynBench on several pretrained image models in giving insightful comparisons of model attributes. We demonstrated its high correlation with real-life tasks and showed its consistent model selections. We also confirmed SynBench's robustness against out-of-distribution tasks and challenging tasks in Appendix A.16; conducted SynBench's correlation analysis in Appendix A.17; and provided more intuitions on the working mechanisms and limitations in Appendix A.18 to A.22.\\n\\nWe envision the SynBench framework to be further extended to other trustworthiness dimensions (e.g., privacy and fairness) and other domains, to shed light on task-agnostic benchmarking designs that are simple and synthetic.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Impact Statement\\n\\nThis paper presents work whose goal is to advance the field of Machine Learning. We envision our developed tool to be useful for (1) model auditing, aligning with broader AI governance efforts, and (2) facilitating efficient hyperparameter tuning and model selection, thereby reducing the computational burden on model users and lowering the bar for new model designs. There is no potential negative societal consequences of our work that we feel must be specifically highlighted here.\\n\\nAcknowledgement\\n\\nChing-Yun Ko would like to thank IBM Research and the summer internship program. This work was partially supported by the MIT-IBM Watson AI Lab and by the National Science Foundation.\\n\\nReferences\\n\\nA new approach to linear filtering and prediction problems. 1960.\\n\\nAchille, A. and Soatto, S. Emergence of invariance and disentanglement in deep representations. The Journal of Machine Learning Research, 19(1):1947\u20131980, 2018.\\n\\nBhagoji, A. N., Cullina, D., and Mittal, P. Lower bounds on adversarial robustness from optimal transport. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nBlier, L. and Ollivier, Y. The description length of deep learning models. Advances in Neural Information Processing Systems, 31, 2018.\\n\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Boscelli, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\\n\\nCarlini, N. and Wagner, D. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 39\u201357. IEEE, 2017.\\n\\nCarlucci, F. M., D\u2019Innocente, A., Bucci, S., Caputo, B., and Tommasi, T. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2229\u20132238, 2019.\\n\\nCaron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9650\u20139660, 2021.\\n\\nChen, L.-C., Papandreou, G., Schroff, F., and Adam, H. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\\n\\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. Generative pretraining from pixels. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 1691\u20131703. PMLR, 13\u201318 Jul 2020a. URL https://proceedings.mlr.press/v119/chen20s.html.\\n\\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597\u20131607. PMLR, 2020b.\\n\\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. E. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33:22243\u201322255, 2020c.\\n\\nChen, X., Hsieh, C.-J., and Gong, B. When vision transformers outperform resnets without pre-training or strong data augmentations. In International Conference on Learning Representations, 2021.\\n\\nCroce, F. and Hein, M. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206\u20132216. PMLR, 2020.\\n\\nDan, C., Wei, Y., and Ravikumar, P. Sharp statistical guarantees for adversarially robust gaussian classification. In International Conference on Machine Learning, pp. 2345\u20132355. PMLR, 2020.\\n\\nDobriban, E., Hassani, H., Hong, D., and Robey, A. Provable tradeoffs in adversarially robust classification. arXiv preprint arXiv:2006.05161, 2020.\\n\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.\\n\\nDubois, Y., Kiela, D., Schwab, D. J., and Vedantam, R. Learning optimal representations with the decodable information bottleneck. Advances in Neural Information Processing Systems, 33:18674\u201318690, 2020.\\n\\nDubois, Y., Hashimoto, T., Ermon, S., and Liang, P. Improving self-supervised learning by characterizing idealized representations, 2022. URL https://arxiv.org/abs/2209.06235.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\nFan, L., Liu, S., Chen, P.-Y., Zhang, G., and Gan, C. When does contrastive learning preserve adversarial robustness from pretraining to finetuning? Advances in Neural Information Processing Systems, 34:21480\u201321492, 2021.\\n\\nFang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., and Cao, Y. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19358\u201319369, 2023.\\n\\nForet, P., Kleiner, A., Mobahi, H., and Neyshabur, B. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2020.\\n\\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\\n\\nHaoChen, J. Z. and Ma, T. A theoretical study of inductive biases in contrastive learning. arXiv preprint arXiv:2211.14699, 2022.\\n\\nHayes, M. H. Statistical digital signal processing and modeling. John Wiley & Sons, 1996.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\\n\\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729\u20139738, 2020.\\n\\nHeckerman, D. Bayesian networks for data mining. Data mining and knowledge discovery, 1:79\u2013119, 1997.\\n\\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\\n\\nJi, G., Hughes, M. C., and Sudderth, E. B. From patches to images: a nonparametric generative model. In International Conference on Machine Learning, pp. 1675\u20131683. PMLR, 2017.\\n\\nJohnson, R. A., Wichern, D. W., et al. Applied multivariate statistical analysis, volume 5. Prentice hall Upper Saddle River, NJ, 2002.\\n\\nKatz, G., Barrett, C., Dill, D. L., Julian, K., and Kochenderfer, M. J. Reluplex: An efficient smt solver for verifying deep neural networks. In International conference on computer aided verification, pp. 97\u2013117. Springer, 2017.\\n\\nKingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\\n\\nLi, L. H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L., Zhang, L., Hwang, J.-N., et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10965\u201310975, 2022.\\n\\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., and Berg, A. C. Ssd: Single shot multibox detector. In European conference on computer vision, pp. 21\u201337. Springer, 2016.\\n\\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 10012\u201310022, 2021.\\n\\nLoureiro, B., Sicuro, G., Gerbelot, C., Pacco, A., Krzakala, F., and Zdeborov\u00e1, L. Learning gaussian mixtures with generalized linear models: Precise asymptotics in high-dimensions. Advances in Neural Information Processing Systems, 34:10144\u201310157, 2021.\\n\\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\\n\\nMahalanobis, P. C. On the generalized distance in statistics. National Institute of Science of India, 1936.\\n\\nMignacco, F., Krzakala, F., Lu, Y., Urbani, P., and Zdeborov\u00e1, L. The role of regularization in classification of high-dimensional noisy gaussian mixture. In International conference on machine learning, pp. 6874\u20136883. PMLR, 2020.\\n\\nMoosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2574\u20132582, 2016.\\n\\nNeyshabur, B., Sedghi, H., and Zhang, C. What is being transferred in transfer learning? Advances in neural information processing systems, 33:512\u2013523, 2020.\\n\\nOkolo, G. I., Katsigiannis, S., and Ramzan, N. Ievit: An enhanced vision transformer architecture for chest x-ray image classification. Computer Methods and Programs in Biomedicine, 226:107141, 2022.\\n\\nPapantoniou, F. P., Lattas, A., Moschoglou, S., Deng, J., Kainz, B., and Zafeiriou, S. Arc2face: A foundation model of human faces. arXiv preprint arXiv:2403.11641, 2024.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\nPaul, S. and Chen, P.-Y. Vision transformers are robust learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 2071\u20132081, 2022.\\n\\nPesce, L., Krzakala, F., Loureiro, B., and Stephan, L. Are gaussian data all you need? extents and limits of universality in high-dimensional generalized linear estimation. arXiv preprint arXiv:2302.08923, 2023.\\n\\nPetridis, S. and Perantonis, S. J. On the relation between discriminant analysis and mutual information for supervised linear feature extraction. Pattern Recognition, 37(5):857\u2013874, 2004.\\n\\nPoggio, T. and Girosi, F. Networks for approximation and learning. Proceedings of the IEEE, 78(9):1481\u20131497, 1990.\\n\\nPruksachatkun, Y., Phang, J., Liu, H., Htut, P. M., Zhang, X., Pang, R. Y., Vania, C., Kann, K., and Bowman, S. Intermediate-task transfer learning with pretrained language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5231\u20135247, 2020.\\n\\nQiao, F., Zhao, L., and Peng, X. Learning to learn single domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12556\u201312565, 2020.\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748\u20138763. PMLR, 2021.\\n\\nRajapakse, J. C., Giedd, J. N., and Rapoport, J. L. Statistical approach to segmentation of single-channel cerebral mr images. IEEE transactions on medical imaging, 16(2):176\u2013186, 1997.\\n\\nRedmon, J. and Farhadi, A. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7263\u20137271, 2017.\\n\\nRefinetti, M., Goldt, S., Krzakala, F., and Zdeborov\u00e1, L. Classifying high-dimensional gaussian mixtures: Where kernel methods fail and neural networks succeed. In International Conference on Machine Learning, pp. 8936\u20138947. PMLR, 2021.\\n\\nRuan, Y., Dubois, Y., and Maddison, C. J. Optimal representations for covariate shift. In International Conference on Learning Representations, 2021.\\n\\nSanjay-Gopal, S. and Hebert, T. J. Bayesian pixel classification using spatially variant finite mixtures and the generalized em algorithm. IEEE Transactions on Image Processing, 7(7):1014\u20131028, 1998.\\n\\nShao, R., Shi, Z., Yi, J., Chen, P.-Y., and Hsieh, C.-J. On the adversarial robustness of vision transformers. arXiv preprint arXiv:2103.15670, 2021.\\n\\nShao, W., Zhao, X., Ge, Y., Zhang, Z., Yang, L., Wang, X., Shan, Y., and Luo, P. Not all models are equal: predicting model transferability in a self-challenging fisher space. In European Conference on Computer Vision, pp. 286\u2013302. Springer, 2022.\\n\\nShi, Y., Daunhawer, I., Vogt, J. E., Torr, P. H. S., and Sanyal, A. How robust is unsupervised representation learning to distribution shift?, 2022.\\n\\nSinha, A., Namkoong, H., and Duchi, J. Certifying some distributional robustness with principled adversarial training. In International Conference on Learning Representations, 2018.\\n\\nSu, D., Zhang, H., Chen, H., Yi, J., Chen, P.-Y., and Gao, Y. Is robustness the cost of accuracy?\u2013a comprehensive study on the robustness of 18 deep image classification models. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 631\u2013648, 2018.\\n\\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\\n\\nTran, D., Liu, J., Dusenberry, M. W., Phan, D., Collier, M., Ren, J., Han, K., Wang, Z., Mariet, Z., Hu, H., et al. Plex: Towards reliability using pretrained large model extensions. arXiv preprint arXiv:2207.07411, 2022.\\n\\nTummala, S., Kadry, S., Bukhari, S. A. C., and Rauf, H. T. Classification of brain tumor from magnetic resonance imaging using vision transformers ensembling. Current Oncology, 29(10):7498\u20137511, 2022.\\n\\nTurk, M. and Pentland, A. Eigenfaces for recognition. Journal of cognitive neuroscience, 3(1):71\u201386, 1991.\\n\\nT\u00fcske, Z., Tahir, M. A., Schl\u00fcter, R., and Ney, H. Integrating gaussian mixtures into deep neural networks: Softmax layer with hidden variables. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4285\u20134289. IEEE, 2015.\\n\\nVoita, E. and Titov, I. Information-theoretic probing with minimum description length. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 183\u2013196, 2020.\\n\\nWhitney, W. F., Song, M. J., Brandfonbrener, D., Altosaar, J., and Cho, K. Evaluating representations by the complexity of learning low-loss predictors. arXiv preprint arXiv:2009.07368, 2020.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.8. Model Descriptions\\n\\nWe list and compare 10 pretrained vision transformers (ViTs) (Dosovitskiy et al., 2020; Chen et al., 2021; Caron et al., 2021) and ResNets (Chen et al., 2020c) in the following table.\\n\\n| Model        | Arch. | pretraining | fine-tuning | patch # | parameters (M) |\\n|--------------|-------|-------------|-------------|---------|----------------|\\n| ViT-Ti/16    |       | Imgn21k     | Imgn1k      | 16      | 5.7            |\\n| ViT-B/16     |       | Imgn21k     | Imgn1k      | 16      | 86.6           |\\n| ViT-B/16-in21k |     | Imgn21k   | No          | 16      | 86.6           |\\n| ViT-L/16     |       | Imgn21k     | Imgn1k      | 16      | 304.3          |\\n| ViT-S/16-DINO|       | self-Imgn1k | No          | 16      | 21.7           |\\n| ViT-S/8-DINO |       | self-Imgn1k | No          | 8       | 21.7           |\\n| ViT-B/16-DINO|       | self-Imgn1k | No          | 16      | 85.8           |\\n| ViT-B/8-DINO |       | self-Imgn1k | No          | 8       | 85.8           |\\n| Resnet50-SimCLRv2 | | self-Imgn1k | No          | -       | 144.4          |\\n| Resnet101-SimCLRv2 | | self-Imgn1k | No          | -       | 261.2          |\\n\\nVariation:\\n- Model size: ViT-\\\\{Ti,B,L\\\\}/16, ViT-\\\\{S,B\\\\}/16-DINO, ViT-\\\\{S,B\\\\}/8-DINO\\n- Finetuning: ViT-B/16-\\\\{,in21k\\\\}\\n- ViT patch size: ViT-S/\\\\{16,8\\\\}-DINO, ViT-B/\\\\{16,8\\\\}-DINO\\n\\nTable 5: Model descriptions. The performance of models might be nuanced by scheduler, curriculum, and training episodes, which are not captured in the table.\\n\\nA.9. Runtime Analysis\\n\\nThe runtime of SynBench depends on the number of outcomes of the discrete uniform distribution $U\\\\{0, 1, 5\\\\}$ and the data inference time through the pretrained model. For one outcome (one robustness-accuracy relationship), it costs 59 seconds to generate 2048 Gaussian samples, and 37 and 81 seconds to obtain the SynBench-Score for ViT-B/16 and ViT-L/16 on one GeForce RTX 2080 super.\\n\\nCorrespondingly, to obtain one robustness-accuracy relationship with task-specific methods requires us to perform adversarial attacks on multiple possible datasets. Here, we ignore the time to train the linear probing layer. For one single dataset, e.g. CIFAR10, AutoAttack uses 72320 and 332288 seconds to evaluate 2048 samples on ViT-B/16 and ViT-L/16 on one GeForce RTX 2080 super; PGD attack uses 1280 and 4608 seconds to evaluate 2048 samples on ViT-B/16 and ViT-L/16 on one GeForce RTX 2080 super.\\n\\nFor other task-agnostic metrics (MDL, SDL, $\\\\epsilon$SC), obtaining them for ViT-B/16 costs 6807 seconds and ViT-L/16 costs 7373 seconds on one Tesla V100. However, it should be noted that these metrics do not indicate robustness performance.\\n\\n2 https://github.com/rwightman/pytorch-image-models\\n3 https://github.com/google-research/simclr\"}"}
{"id": "MmZJ3kJXjX", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 6: Full table of Table 1.\\n\\n| Model                | $\\\\epsilon = 0$ | $\\\\epsilon = 0.1$ | $\\\\epsilon = 0.2$ | $\\\\epsilon = 0.3$ | $\\\\epsilon = 0.4$ | $\\\\epsilon = 0.5$ | $\\\\epsilon = 0.6$ | $\\\\epsilon = 0.7$ | $\\\\epsilon = 0.8$ |\\n|----------------------|----------------|------------------|------------------|------------------|------------------|------------------|------------------|------------------|------------------|\\n| ViT-Ti/16            | 0.75           | 0.72             | 0.70             | 0.68             | 0.66             | 0.64             | 0.62             | 0.60             | 0.59             |\\n| ViT-B/16             | 0.33           | 0.36             | 0.37             | 0.35             | 0.32             | 0.27             | 0.20             | 0.13             | 0.07             |\\n| ViT-B/16-in21k       | 0.20           | 0.22             | 0.23             | 0.21             | 0.17             | 0.13             | 0.07             | 0.03             | 0.01             |\\n| ViT-L/16             | 0.26           | 0.30             | 0.33             | 0.32             | 0.30             | 0.27             | 0.22             | 0.17             | 0.11             |\\n| ViT-S/16-DINO        | 0.48           | 0.48             | 0.47             | 0.45             | 0.42             | 0.37             | 0.32             | 0.25             | 0.17             |\\n| ViT-B/16-DINO        | 0.55           | 0.58             | 0.58             | 0.56             | 0.53             | 0.50             | 0.46             | 0.41             | 0.35             |\\n| ViT-S/8-DINO         | 0.40           | 0.42             | 0.42             | 0.41             | 0.39             | 0.37             | 0.34             | 0.30             | 0.26             |\\n| ViT-B/8-DINO         | 0.50           | 0.55             | 0.56             | 0.54             | 0.50             | 0.45             | 0.40             | 0.35             | 0.30             |\\n| ResNet50-SimCLRv2    | 0.66           | 0.53             | 0.50             | 0.49             | 0.50             | 0.49             | 0.48             | 0.48             | 0.48             |\\n| ResNet101-SimCLRv2   | 0.54           | 0.69             | 0.57             | 0.49             | 0.45             | 0.43             | 0.40             | 0.38             | 0.36             |\\n\\n### Table A.10: Full Results of Table 1.\\n\\n| Model                | $\\\\epsilon = 0$ | $\\\\epsilon = 0.1$ | $\\\\epsilon = 0.2$ | $\\\\epsilon = 0.3$ | $\\\\epsilon = 0.4$ | $\\\\epsilon = 0.5$ | $\\\\epsilon = 0.6$ | $\\\\epsilon = 0.7$ | $\\\\epsilon = 0.8$ |\\n|----------------------|----------------|------------------|------------------|------------------|------------------|------------------|------------------|------------------|------------------|\\n| ViT-Ti/16            | 0.75           | 0.72             | 0.70             | 0.68             | 0.66             | 0.64             | 0.62             | 0.60             | 0.59             |\\n| ViT-B/16             | 0.33           | 0.36             | 0.37             | 0.35             | 0.32             | 0.27             | 0.20             | 0.13             | 0.07             |\\n| ViT-B/16-in21k       | 0.20           | 0.22             | 0.23             | 0.21             | 0.17             | 0.13             | 0.07             | 0.03             | 0.01             |\\n| ViT-L/16             | 0.26           | 0.30             | 0.33             | 0.32             | 0.30             | 0.27             | 0.22             | 0.17             | 0.11             |\\n| ViT-S/16-DINO        | 0.48           | 0.48             | 0.47             | 0.45             | 0.42             | 0.37             | 0.32             | 0.25             | 0.17             |\\n| ViT-B/16-DINO        | 0.55           | 0.58             | 0.58             | 0.56             | 0.53             | 0.50             | 0.46             | 0.41             | 0.35             |\\n| ViT-S/8-DINO         | 0.40           | 0.42             | 0.42             | 0.41             | 0.39             | 0.37             | 0.34             | 0.30             | 0.26             |\\n| ViT-B/8-DINO         | 0.50           | 0.55             | 0.56             | 0.54             | 0.50             | 0.45             | 0.40             | 0.35             | 0.30             |\\n| ResNet50-SimCLRv2    | 0.66           | 0.53             | 0.50             | 0.49             | 0.50             | 0.49             | 0.48             | 0.48             | 0.48             |\\n| ResNet101-SimCLRv2   | 0.54           | 0.69             | 0.57             | 0.49             | 0.45             | 0.43             | 0.40             | 0.38             | 0.36             |\\n\\n---\\n\\n*A.10. Full Results of Table 1.*\"}"}
{"id": "MmZJ3kJXjX", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"For completeness, we report several baseline metrics for the synthetic conditional Gaussian classification task. We follow the implementation of Whitney et al. (2020); Shao et al. (2022) and set the training set size $n$ to be $2048$, $8192$, $32768$. In Table 7, we report validation loss (val loss), minimum description length (MDL) (Voita & Titov, 2020), surplus description length (SDL), $\\\\epsilon$-sample complexity ($\\\\epsilon$-SC) (Whitney et al., 2020), logarithm of maximum evidence (LogME) (You et al., 2021) and self-challenging Fisher discriminant analysis (SFDA) (Shao et al., 2022) on our synthetic proxy task as baselines.\\n\\nWe aim at calculating the Pearson correlation between task-agnostic metrics and possible downstream tasks. We take the average accuracy of 27 downstream tasks in the literature (Radford et al., 2021) for each pretrained model and treat it as the real-life performance measure. For an even more complete picture, we also consider some synthetic distribution shifts that include image corruptions (ImageNet-c), style transfer (ImageNet-r), and adversarial examples (ImageNet-a). To analyze how data with these synthetic distribution shifts can inform general pretrained models' performance, we quoted the their accuracy from (Wightman, 2019) and calculated their correlation with the average real-life accuracy in Table 7. Furthermore, following (Zhang et al., 2021), we perform \\\"partially corrupted labels\\\" experiments on CIFAR10 dataset with the level of label corruptions equals to 0.5. See line \\\"CIFAR10-lc acc.\\\" for the results. We note that the correlation coefficients in these four cases suggest only moderate correlation to even negative correlation.\\n\\nWe set the training set size $n$ to be $2048$, $4096$, $8192$, $16384$, $32768$ and compare the model selections between ViT-B/16 and ViT-B/16-in21k in Table 8. In Table 9, we report these metrics on all 10 pretrained representations for $n = 8192$.\\n\\n| Name          | ViT-B/16 | ViT-L/16 | ViT-B/32 | Resnet50-SimCLRv2 | Resnet101-SimCLRv2 |\\n|---------------|----------|----------|----------|-------------------|-------------------|\\n| Pearson correlation (Real-life Accuracy) | 74.3 | 75.5 | 72.6 | 75.4 | 75.4 |\\n| ImageNet-c acc. | 66.4 | 72.2 | 61.4 | 47.4 | 50.1 |\\n| Transfer ImageNet-r acc. | 56.8 | 64.3 | 49.4 | 39.4 | 44.1 |\\n| Dataset ImageNet-a acc. | 43.1 | 55.3 | 22.3 | 27.1 | 38.2 |\\n| CIFAR10-lc acc. | 93.54 | 94.95 | 92.48 | 85.74 | 87.38 |\\n| Val loss       | 3.10    | 4.12    | 4.10    | 1.31   | 0.98   |\\n| MDL            | 6820.76 | 8094.06 | 8198.55 | 5881.34 | 2882.36 |\\n| $\\\\epsilon$-SC, $\\\\epsilon=1$ | $>4977.76$ | $>6251.06$ | $>6355.55$ | $>4038.34$ | $1052.37$ |\\n| LogME          | -0.726  | -0.724  | -0.729  | 2.791  | 1.503  |\\n| SFDA           | 0.584   | 0.635   | 0.567   | 0.947  | 0.593  |\\n| SynBench       | 0.33    | 0.26    | 0.02    | 0.66   | 0.60   |\\n\\nTable 7: Pearson correlation between task agnostic metrics and the average accuracy on 27 real-life tasks (Radford et al., 2021, Table 10). We report the 5 pretrained models out of the overall 10 due to the lack of reported results from the literature for the other pretrain models.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 8: Baseline metrics evaluating the representation quality on the conditional Gaussian synthetic data with $n = \\\\{2048, 4096, 8192, 16384, 32768\\\\}$. For Val loss, MDL, SDL, and $\\\\epsilon_{SC}$, the smaller the better; for SynBench, the bigger the better. Note that the model ranking of SynBench is consistent across different values of $n$, while other methods will change their rankings.\\n\\n| Name       | Val loss | MDL       | $\\\\epsilon_{SC}, \\\\epsilon = 1$ | $\\\\epsilon_{SC}, \\\\epsilon = 1$ |\\n|------------|----------|-----------|--------------------------------|---------------------------------|\\n| ViT-Ti/16  | 4.38     | 30071.64  | >22699.64                      | >7372.0                         |\\n| ViT-B/16   | 0.73     | 9939.13   | 3479.59                        | 7372                            |\\n| ViT-L/16   | 1.50     | 17672.6   | >10300.6                       | >7372.0                         |\\n| ViT-B/16-in21k | 0.77   | 9773.16   | 3153.33                        | 7372                            |\\n| ViT-S/16-DINO | 1.51   | 18536.93  | >11164.93                      | >7372.0                         |\\n| ViT-S/8-DINO | 0.70   | 8196.8    | 2056.69                        | 4045                            |\\n| ViT-B/16-DINO | 0.92   | 10535.11  | 3432.28                        | 7372                            |\\n| ViT-B/8-DINO | 0.64   | 6796.87   | 1185.31                        | 2220                            |\\n| Resnet50-SimCLRv2 | 0.62 | 9646.09   | 3700.73                        | 4045                            |\\n| Resnet101-SimCLRv2 | 0.52 | 5443.43   | 776.38                         | 669                            |\\n\\nTable 9: Baseline metrics evaluating the representation quality on the conditional Gaussian synthetic data with $n = 8192$.\\n\\nTable 10: Full Table of Table 2.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: Distances from synthetic data to CIFAR10, SVHN, and TinyImageNet.\\n\\n| Dataset      | FID | MD   |\\n|--------------|-----|------|\\n| CIFAR10      | 438 | 86142|\\n| SVHN         | 406 | 71527|\\n| TinyImageNet | 403 | 76706|\\n\\nTable 12: SynBench-Score comparisons on the finetuning procedure in pretraining on synthetic data with heptadiagonal covariance.\\n\\n| Model        | Score |\\n|--------------|-------|\\n| ViT-B/16     | 0.18  |\\n| ViT-B/16-in21k | 0.07 |\\n| ViT-Ti/16    | 0    |\\n| ViT-B/16     | 0.18  |\\n| ViT-L/16     | 0.18  |\\n\\nTable 13: SynBench-Score comparisons on the model sizes on synthetic data with heptadiagonal covariance.\\n\\n| Model        | Score |\\n|--------------|-------|\\n| ViT-S/16-DINO| 0.47  |\\n| ViT-B/16-DINO| 0.42  |\\n| ViT-S/8-DINO | 0.36  |\\n| ViT-B/8-DINO | 0.42  |\\n| Res50-SimCLRv2 | 0.24 |\\n| Res101-SimCLRv2 | 0.30 |\\n\\nTable 14: SynBench-Scores of self-supervised pretrained representations on synthetic data with heptadiagonal covariance.\\n\\nFor some datasets, such as FFHQ, the pixel-wise distribution is shaped by the facial layout, hence contains structure information. In this case, a straightforward way to find the low-level information is to use something like Eigenfaces (Turk & Pentland, 1991) or recent face foundation models (Papantoniou et al., 2024). Generally, we think of several structures facial images might have. Going from the most global one to the most local one - (1) \\\\( d \\\\times d \\\\times n \\\\) sized globally aligned facial images could have symmetrical features, and therefore this could be described by \\\\( I_{n c} \\\\otimes I_d \\\\otimes (a \\\\cdot I_d + a \\\\cdot I_d') \\\\), where \\\\( I_{n c} \\\\) and \\\\( I_d \\\\) are identity matrices, \\\\( I_d' \\\\) is an anti-diagonal identity matrix, and \\\\( a \\\\) is a scaling factor; (2) pixels in each specific region will correlate more with pixels inside the region and less with outside regions (face vs background, eyes vs other parts of the face). This can be described by block structures in the covariance matrix, where each block has higher values within themselves and smaller values in the off-diagonals, e.g., \\\\( I_{n c} \\\\otimes A_d^2 \\\\) where \\\\( A_d' \\\\)s entries are zeros except its center region; (3) inside the block such as \\\\( A_d \\\\), there could exist more local structures in the nonzeros to represent symmetrical facial features (eyes, nose, mouth). Finally, these structural features will be compositional.\\n\\nThe synthetic data can be generated pixel by pixel if the covariance matrix is a diagonal matrix. In the case when the covariance is not a diagonal (like Section 4.4), we need to draw the whole image (or each channel as in Section 4.4) at once from the multivariate normal with generic covariance matrix.\\n\\nWe include 18 synthetic data samples in Figure 6(a), showing 9 samples for each of the two classes. These examples are drawn from class-conditional Gaussians with scale \\\\( s = 25 \\\\) (cf. Section 3.3) and of size \\\\( 32 \\\\times 32 \\\\). Class-1 samples are on the\"}"}
{"id": "MmZJ3kJXjX", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\n![Image](image.png)\\n\\nFigure 6: 18 synthetic data samples and their projections on the direction $\\\\mu_1 - \\\\mu_2$.\\n\\nleft, and Class-2 samples are on the right. We can see that Class-1 samples are generally brighter than Class-2 samples. This is because Class-1 samples are drawn from the Gaussian with larger mean in the magnitude.\\n\\nFurthermore, we demonstrate the separability of two class samples by projecting samples down along the direction of two Gaussian mean difference, in order to showcase their hidden discriminate patterns. That is, for vectorized sample $x$, Gaussian mean $\\\\mu_1$ and $\\\\mu_2$, we do the calculation $x^T(\\\\mu_1 - \\\\mu_2)$ and plot them on a line in Figure 6(b).\\n\\nA.16. Correlation Breakdowns and Robustness to OOD and Challenging tasks\\n\\nAs SynBench score is not dependent on task, we gave the SynthBench score of each model in Table 7. In this analysis, we calculate how SynBench score correlates with downstream performance per data set in the following Table 15.\\n\\n| Datasets          | Food101 | CIFAR10 | CIFAR100 | birdsnap | SUN397 | StanfordCars | Aircraft |\\n|-------------------|---------|---------|----------|----------|--------|--------------|----------|\\n| FID to ImageNet21k| 100.81  | 115.47  | 96.22    | 102.39   | 54.78  | 154.81       | 206.47   |\\n| SynBench          | 0.01    | -0.30   | -0.50    | -0.33    | -0.32  | 0.90         | 0.87     |\\n| Val loss          | -0.31   | 0.07    | 0.24     | 0.03     | 0.03   | -0.82        | -0.70    |\\n| MDL               | -0.18   | 0.19    | 0.37     | 0.17     | 0.16   | -0.84        | -0.77    |\\n| LogME             | -0.48   | -0.70   | -0.83    | -0.74    | -0.74  | 0.85         | 0.95     |\\n| SFDA              | -0.41   | -0.66   | -0.77    | -0.67    | -0.69  | 0.88         | 0.95     |\\n\\n| Datasets          | VOC2007 | DTD | Pets | Caltech101 | Flowers | MNIST | FER2013 |\\n|-------------------|---------|-----|------|------------|---------|-------|---------|\\n| FID to ImageNet21k| 52.30   | 98.37| 104.15| 53.51      | 112.64  | 301.28| 175.75  |\\n| SynBench          | 0.64    | 0.86 | 0.40 | 0.09       | -0.64  | 0.56  | 0.81    |\\n| Val loss          | -0.80   | -0.66| -0.63| 0.02       | -0.33  | -0.85 |        |\\n| MDL               | -0.76   | -0.75| -0.54| -0.01      | 0.49   |       | -0.41   |\\n| LogME             | 0.22    | 0.98 | -0.13| -0.01      | -0.92  | 0.85  | 0.55    |\\n| SFDA              | 0.24    | 0.96 | -0.07| -0.07      | -0.87  | 0.84  | 0.60    |\\n\\n| Datasets          | STL10 | EuroSAT | RESISC45 | GTSRB | KITTI | Country211 | PCAM |\\n|-------------------|-------|---------|----------|-------|-------|------------|------|\\n| FID to ImageNet21k| 71.19 | 142.62  | 104.80   | 156.81| 163.92| 36.72      | 235.63|\\n| SynBench          | -0.40 | 0.77   | 0.91     | 0.59  | 0.40   | 0.96       | 0.90 |\\n| Val loss          | 0.11  | -0.54  | -0.76    | -0.34 | -0.14  | -0.96      | -0.99 |\\n| MDL               | 0.23  | -0.64  | -0.82    | -0.43 | -0.25  | -0.97      | -0.96 |\\n| LogME             | -0.80 | 0.97   | 0.96     | 0.85  | 0.81   | 0.69       | 0.59 |\\n| SFDA              | -0.75 | 0.93   | 0.96     | 0.82  | 0.77  | 0.70       | 0.64 |\\n\\n| Datasets          | UCF101 | Kinetics700 | CLEVR | HatefulMemes | SST | ImageNet | A VG acc. |\\n|-------------------|--------|--------------|-------|--------------|-----|----------|-----------|\\n| FID to ImageNet21k| 79.40  | time out     | 194.64| 86.64        | 368.13| 17.78    |\\n| SynBench          | 0.81   | 0.64         | 0.72 | -0.59        | 0.35 | 0.30     | 0.92      |\\n| Val loss          | -0.93  | -0.82        | -0.48| 0.34         | -0.22| -0.56    | -0.92     |\\n| MDL               | -0.87  | -0.74        | -0.59| 0.47         | -0.32| -0.45    | -0.91     |\\n| LogME             | 0.45   | 0.17         | 0.97 | -0.88        | 0.41 |         | -0.22     |\\n| SFDA              | 0.51   | 0.24         | 0.94 | -0.83        | 0.34 | -0.15    | 0.77      |\\n\\nTable 15: The correlation between SynBench-score and individual downstream task, and the Frechet Inception Distance (FID) scores from ImageNet21k to individual downstream task.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\nSubset of OOD tasks\\n\\nWe analyze SynBench score\u2019s correlation to the subset of OOD tasks. In the following Table 15, we computed the Frechet Inception Distance (FID) scores from ImageNet21k to the downstream tasks, and used them as the indicator of how OOD are the tasks. We then computed SynBench-score correlation with tasks that have FID scores larger than a threshold \\\\{50, 100, 150, 200\\\\}.\\n\\nNot all models in our analysis are pretrained with ImageNet21k; however, since ImageNet21k has become a go-to pretraining dataset, we assume samples therein are in-distribution.\\n\\nFrom Table 16, we see that if we don't apply filter on FID (or equivalently let threshold be 0), the initial correlation was 0.92. As we gradually increase the threshold to 50, 100, 150, and even 200, the correlation stays above 0.8, indeed suggesting SynBench's robustness to OOD tasks.\\n\\nTable 16: The correlation between SynBench-score and the average accuracy of FID-thresholded downstream tasks.\\n\\n| FID > 0 (all tasks) | > 50 | > 100 | > 150 | > 200 |\\n|---------------------|------|------|------|------|\\n| SynBench Correlation| 0.92 | 0.93 | 0.93 | 0.82 |\\n\\nSubset of more challenging tasks\\n\\nFurther analyze SynBench score's correlation to the subset of more challenging tasks. We computed the average accuracy of 27 downstream tasks as the proxy of the general performance. Among the 27 tasks, there are indeed datasets that are large and complex, including ImageNet.\\n\\nTable 17: The correlation between SynBench-score and subsets of downstream tasks.\\n\\n| Large complex datasets w/ video datasets visual reasoning/QA dataset\\n|------------------------|\\n| datasets #classes |\\n| > 100 (UCF101 and Kinetics 700) | dataset average |\\n| SynBench | 0.56 | 0.72 |\\n| Val loss | -0.75 | -0.88 | -0.48 | -0.91 |\\n| MDL | -0.66 | -0.81 | -0.59 | -0.85 |\\n| LogME | 0.11 | 0.30 | 0.45 |\\n| SFDA | 0.19 | 0.36 | 0.51 |\\n\\nOverall, SynBench shows robust performance across these breakdown groups.\\n\\nCorrelation ranking\\n\\nSome might wonder, why SynBench negatively-correlated with parts of datasets in Table 15. To answer this, we hint that if there exist a metric that highly correlates with the linear probing performance on every single downstream task, it would imply that the linear probing performance on every single downstream task also correlates highly with each other\u2014which is not the case in reality. Therefore, we are seeking a metric that can inform on the potential overall performance. In Table 18, we provide the average ranking of correlations with downstream tasks by SynBench and other baselines as a more robust and intuitive measure. It is clear that SynBench is able to give the overall best correlation with each individual downstream.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\nCorrelation\\n\\nTable 18: The average ranking of correlations with downstream tasks SynBench and other baselines.\\n\\n| Name          | ViT-B/16 | ViT-L/16 | ViT-B/32 | ResNet50- | ResNet101- | EfficientNet | EfficientNet | EfficientNet |\\n|---------------|----------|----------|----------|------------|------------|--------------|--------------|--------------|\\n| Pearson       |          |          |          |            |            |              |              |              |\\n| Real-life     | 74.3     | 75.5     | 72.6     | 75.4       | 75.4       | 72.5         | 72.6         | 73.1         |\\n| Val loss      | 3.10     | 4.12     | 4.10     | 1.31       | 0.98       | 4.66         | 3.56         | 6.82         |\\n| MDL           | 6820.76  | 8094.06  | 8198.55  | 5881.34    | 2882.36    | 8950.38      | 7654.88      | 15816.05     |\\n| SFDA          | 0.584    | 0.635    | 0.567    | 0.947      | 0.593      | 0.534        | 0.515        | 0.751        |\\n| SynBench      | 0.33     | 0.26     | 0.0      | 0.66       | 0.60       | 0.02         | 0.04         | 0            |\\n| Val loss      | 0.73     | 1.50     | 2.92     | 0.62       | 0.52       | 4.27         | 2.03         | 4.33         |\\n| MDL           | 9939.13  | 17672.6  | 23332.98 | 9646.09    | 5443.43    | 32511.61     | 19479.78     | 43202.85     |\\n| SDL, $\\\\varepsilon = 1$ | 3479.59  | 10300.6  | 15960.98 | 3700.73    | 776.38     | 25139.61     | 12107.78     | 35830.85     |\\n| $\\\\varepsilon = 2048$ | 7372.0   | 7372.0   | 7372.0   | 3045       | 669        | 7372.0       | 7372.0       | 7372.0       |\\n| LogME         | -0.726   | -0.724   | -0.729   | 2.791      | 1.503      | -0.721       | -0.726       | -0.725       |\\n| SFDA          | 0.525    | 0.531    | 0.513    | 0.581      | 0.543      | 0.510        | 0.505        | 0.524        |\\n| SynBench      | 0.52     | 0.49     | 0.01     | 0.69       | 0.84       | 0.13         | 0.13         | 0.09         |\\n| Val loss      | 0.68     | 0.79     | 3.91     | 0.53       | 0.51       | 1.11         | 0.79         | 2.60         |\\n| MDL           | 30848.99 | 38718.04 | 107960.49 | 22022.08  | 17166.0    | 56621.37     | 39158.90     | 109706.34    |\\n| SDL, $\\\\varepsilon = 1$ | 7043.32  | 12496.0  | 78469.49 | 4355.67    | 969.27     | 27130.37     | 12931.79     | 80215.34     |\\n| $\\\\varepsilon = 32768$ | 14265     | 29491    | 29491.0  | 3338       | 1615       | 29491.0      | 29491          | 29491.0      |\\n| LogME         | -0.686   | -0.687   | -0.725   | -0.580     | -0.608     | -0.713       | -0.719       | -0.715       |\\n| SFDA          | 0.517    | 0.518    | 0.505    | 0.545      | 0.534      | 0.505        | 0.504        | 0.508        |\\n| SynBench      | 0.59     | 0.58     | 0.02     | 0.81       | 0.87       | 0.19         | 0.19         | 0.17         |\\n| Val loss      | 0.68     | 0.79     | 3.91     | 0.53       | 0.51       | 1.11         | 0.79         | 2.60         |\\n| MDL           | 30848.99 | 38718.04 | 107960.49 | 22022.08  | 17166.0    | 56621.37     | 39158.90     | 109706.34    |\\n| SDL, $\\\\varepsilon = 1$ | 7043.32  | 12496.0  | 78469.49 | 4355.67    | 969.27     | 27130.37     | 12931.79     | 80215.34     |\\n| $\\\\varepsilon = 32768$ | 14265     | 29491    | 29491.0  | 3338       | 1615       | 29491.0      | 29491          | 29491.0      |\\n\\nTable 19: The correlation between SynBench-score and the average accuracy on 27 real-life tasks.\\n\\nIn the following Figure 7, we plot the Pearson correlation coefficients of each methods with their confidence interval for a 90% confidence level when the training set size $n = 2048$. \\n\\nFigure 7: The Pearson $r$ and 90% confidence intervals.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations?\\n\\nChing-Yun Ko 1, 2 Pin-Yu Chen 2 Payel Das 2 Jeet Mohapatra 1 Luca Daniel 1\\n\\nAbstract\\nRecent years have witnessed a paradigm shift in deep learning from task-centric model design to task-agnostic representation learning and task-specific fine-tuning. Pretrained model representations are commonly evaluated extensively across various real-world tasks and used as a foundation for different downstream tasks. This paper proposes a solution for assessing the quality of representations in a task-agnostic way. To circumvent the need for real-world data in evaluation, we explore the use of synthetic binary classification tasks with Gaussian mixtures to probe pretrained models and compare the robustness-accuracy performance on pretrained representations with an idealized reference. Our approach offers a holistic evaluation, revealing intrinsic model capabilities and reducing the dependency on real-life data for model evaluation. Evaluated with various pretrained image models, the experimental results confirm that our task-agnostic evaluation correlates with actual linear probing performance on downstream tasks and can also guide parameter choice in robust linear probing to achieve a better robustness-accuracy trade-off.\\n\\n1. Introduction\\nIn recent years, the use of large pretrained neural networks for efficient fine-tuning on downstream tasks has prevailed in many domains such as vision, language, and speech. Instead of designing task-dependent neural network architectures for different downstream tasks, the current methodology focuses on the principle of task-agnostic pretraining and task-specific finetuning. This methodology uses a neural network pretrained on a large-scale broad dataset to extract generic representations of the input data, which we call pretrained representations for simplicity. The pretrained representations are then used as a foundation (Bommasani et al., 2021) to solve downstream tasks. Prevalent ways include training a linear head (i.e., linear probing) on the representations with the labels provided by a downstream dataset, or simply employing zero-shot inference.\\n\\nWhen gauging the usefulness of a pretrained model, it is a convention to conduct evaluations on selected public datasets. For example, ViT (Dosovitskiy et al., 2020) reports accuracy on 25 tasks, CLIP (Radford et al., 2021) on 27 datasets, and PLEX (Tran et al., 2022) on over 40 datasets to systematically evaluate different reliability dimensions on both vision and language domains. However, this convention has several drawbacks. For example, the evaluation process evidently poses significant computational overhead on the model trainer and raises data privacy concerns, setting a high bar for new model designs and large-scale AI governance. More importantly, the evaluation result is dependent on specific evaluation datasets. Thus the nominal evaluation score can be inconclusive if the evaluation data are biased or under-representative. For instance, ViT-L/16 is reportedly performing better than ViT-B/16 on 23 out of 27 linear probing tasks according to (Radford et al., 2021, Table 10), but worse than ViT-B/16 on FoodSeg103 (Wu et al., 2021, Table 8), X-ray images (Okolo et al., 2022, Table 4-8), and magnetic resonance imaging (Tummala et al., 2022, Table 2-3) tasks. Fundamentally, a poor probing result might come from either (1) evaluation data bias, (2) true model deficiency, or both. In this paper, we attempt to disentangle the effect of the two and focus on designing well-posed sanity checks for the latter. We utilize synthetic data generated from class-conditional data prior, whose optimal classification strategy is known, and compare the optimal strategy with representations' linear separability. For example, Fisher's linear discriminant rule (Johnson et al., 2002; Petridis & Perantonis, 2004) decides the optimal strategy for Gaussian distribution. If the data can be separated with 90% accuracy in the raw input space and 60% in the representation space, then the pretrained model has an intrinsic deficiency. Building on that, the trending practice of pretraining and fine-tuning also signifies immediate damage to all adapted applications if the foundation model has hidden\"}"}
{"id": "MmZJ3kJXjX", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\nrisks (Bommasani et al., 2021), such as lacking robustness to adversarial examples. Luckily, similar to Fisher\u2019s linear discriminant rule for the optimal standard accuracy, Dan et al. (2020) has characterized the optimal classification strategy in the presence of input perturbations. Our sanity check can thereby evaluate the robustness of pretrained models by considering the same synthetic conditional Gaussian data prior.\\n\\nBesides being great candidates for establishing well-posed problems, the idea of probing foundation models with synthetic conditional Gaussians is also motivated by the long-standing practice of Gaussian modeling in signal processing (Hayes, 1996), data mining (Heckerman, 1997), machine learning (Kingma & Welling, 2013; T\u00fcske et al., 2015; Zong et al., 2018), and other engineering fields. For example, Gaussian mixtures have found applications in modeling noise, magnetic field inhomogeneities, biological variations of tissues in magnetic resonance imaging (Rajapakse et al., 1997), and computerized tomography (Sanjay-Gopal & Hebert, 1998). The facts that Gaussian mixture models often lead to mathematically tractable problems (Mignacco et al., 2020; Refinetti et al., 2021; Loureiro et al., 2021) and the abundance of analytical tools available for Gaussian models (kal, 1960; Poggio & Girosi, 1990; Johnson et al., 2002; Dan et al., 2020) also inspire our study on how Gaussian mixtures can be leveraged for evaluating pretrained image models. Further discussions regarding our choice of Gaussian models can be found in the Appendix A.1.\\n\\nAn ideal pretrained model should entail both good accuracy and robustness, and the level of goodness is desired to be measurable in a task/data-agnostic manner. In this paper, we propose SynBench to precisely address this requirement. Specifically, SynBench establishes a theoretical reference characterizing the robustness-accuracy trade-off of the synthetic data based on the Bayes optimal linear classifiers. Then, SynBench obtains the representations of the same synthetic data from the pretrained model and compares them to the reference. Finally, we define the ratio of area-under-the-curves in robustness-accuracy plots, SynBench-Score, as a quantifiable metric of the pretrained representation quality. The entire procedure of SynBench is illustrated in Figure 1.\\n\\nWe list possible use case of SynBench in the Appendix A.2. SynBench features the following key advantages:\\n\\n1. Soundness: We formalize the fundamental trade-off in robustness and accuracy of the considered conditional Gaussian model and use this characterization as a reference to analyze the quality of pretrained representations in a completely real-data-free scenario.\\n\\n2. Task-independence: Since the pretraining of large models is independent of the downstream datasets and tasks, the use of synthetic data in SynBench provides a task-agnostic approach to evaluating pretrained representations without the knowledge of downstream tasks and datasets.\\n\\n3. Completeness and privacy: The flexibility of generating synthetic data (e.g., by adopting a different data sampling procedure) offers a good proxy towards a more comprehensive evaluation of pretrained representations before fine-tuning on downstream datasets, especially in the scenario when the available datasets are not representative of the entire downstream datasets. Moreover, the use of synthetic data enables complete control and simulation over data size and distribution, protects data privacy, and facilitated model auditing and governance.\\n\\nWe highlight our main contributions as follows:\\n\\n\u2022 We propose SynBench, a novel evaluation framework for pretrained image models that uses data synthesized from a data prior. The evaluation process is independent of the downstream image classification datasets/tasks.\\n\\n\u2022 Evaluated with several pretrained image models for image classification, our experimental results show that SynBench-Score matches well the model performance when finetuned on several downstream datasets. For example, SynBench-Score suggests that the Imagenet21k pretrained network (ViT-B/16-in21k) improves with fine-tuning on Imagenet1k (ViT-B/16), echoing with the higher linear probing accuracy of ViT-B/16 on real-life datasets. The Pearson correlation coefficient between SynBench-Score and the average downstream task accuracy suggests strong correlation (above 0.9).\\n\\n\u2022 We show that SynBench can be used to guide hyperparameter selection in robust linear probing to mitigate the robustness-accuracy trade-off when fine-tuned on downstream datasets. For example, conducting $\\\\epsilon$-robust linear probing with $\\\\epsilon$ selected by SynBench-Score gives ViT-B/16 0.1% and 2.7% increase on CIFAR10 standard and robust accuracy, and 0.7% and 2.5% increase on TinyImagenet standard and robust accuracy.\\n\\n2. Related Work\\n\\nPretrained models in vision. In the past few years, much focus in the machine learning community has been shifted to training representation networks capable of extracting features for a variety of downstream tasks with minimal fine-tuning. Nowadays, many common vision tasks are achieved with the assistance of good backbones, e.g. classifications (Yu et al., 2022; Wortsman et al., 2022; Foret et al., 2020; Xie et al., 2020; Dosovitskiy et al., 2020; Chen et al., 2020a), object detection (Redmon & Farhadi, 2017; Liu et al., 2016), segmentation (Chen et al., 2017; Xie et al., 2020), etc.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\nFigure 1: Overview of SynBench. Step 1: generate class-conditional Gaussian and form the inputs to the pretrained model; Step 2: gather rendered representations; Step 3: measure the expected robustness bound under a range of threshold accuracy for both input synthetic data and their representations according to eqn. (2) and obtain the expected bound-threshold accuracy plot; Step 4: calculate SynBench score by the relative area under the curve of the representations (area B) to the inputs (area A + area B) in the expected bound-threshold accuracy plot. The closer the ratio is to 1, the better the quality of pretrained representations is, in terms of the robustness-accuracy characterization.\\n\\nBenchmarking pretrained models. Since pretrained models are used as a foundation for different downstream tasks, it is central to transfer learning (Neyshabur et al., 2020; Pruksachatkun et al., 2020), and also tightly related to model generalization (Qiao et al., 2020; Carlucci et al., 2019). To benchmark the performance of a pretrained model, it is a convention to apply the pretrained model for a number of popular tasks and conduct linear probing on the representations (Chen et al., 2020b; Dosovitskiy et al., 2020; Chen et al., 2020a; 2021). Besides accuracy-based probing methods, evaluation methods have been proposed based on information theory and minimum description length (Blier & Ollivier, 2018; Voita & Titov, 2020), surplus description length (Whitney et al., 2020), maximum evidence (You et al., 2021), Fisher discriminant analysis (Shao et al., 2022), among others. These metrics are reliant on the label information of the downstream tasks and are hence task-specific.\\n\\nLately, more fundamental questions related to pretrained models are brought up (Bommasani et al., 2021; Tran et al., 2022; Zhang & R\u00e9, 2022; Shi et al., 2022). (Bommasani et al., 2021) raised practical concerns about the homogenization incentivized by the scale of the pretraining. Although homogenization might help in achieving competitive performance for some downstream tasks, the defects are also inherited by all these downstreams. On that account, a more careful study of the fundamentals of pretrained models is of paramount importance. (Tran et al., 2022) explored the reliability of pretrained models by devising 10 types of tasks on 40 datasets. It is further pointed out by (Zhang & R\u00e9, 2022) in 9 benchmarks that pretrained models may not be robust to subpopulation or group shift. The adversarial robustness is benchmarked by (Shao et al., 2021; Paul & Chen, 2022).\\n\\nOptimal representations. In the seminal work of deep representation theory, (Achille & Soatto, 2018) depicted the desired optimal representations in supervised learning to be sufficient for the downstream task, invariant to the effect of nuisances, maximally disentangled, and have minimal mutual information between representations and inputs. Focusing more on generalization than compression, (Dubois et al., 2020) provided the optimal representation based on \\\\( V \\\\)-information (Xu et al., 2019). (Ruan et al., 2021) defined the optimal representations for domain generalization. (Dubois et al., 2022) characterized idealized representations in self-supervised learning as ones that are well-distinguished by the desired family of probes for potential invariant tasks, have sufficiently large dimensions, and be invariant to input augmentations.\\n\\nWhy SynBench? To enable quantifying representation quality in the pretraining stage, SynBench differs from the above frameworks as it does not need knowledge of any real-world downstream data. Moreover, SynBench has full control of the evaluation set via synthetic data generation. With the assumed synthetic data distribution, we can theoretically characterize the reference robustness-accuracy trade-off. Therefore, SynBench provides a standardized quality metric with theoretical groundings and evaluates for representations induced by pretrained models at a low cost.\"}"}
{"id": "MmZJ3kJXjX", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.5. General $\\ell_p$ Results\\n\\nWe note that our results in Appendix A.4 can be straightforwardly generalized to $\\ell_p$. Given an $\\ell_p$ adversarial budget $\\\\epsilon$:\\n\\n**Theorem A.2.** For any sample $x$, the optimal robust classifier $f_\\\\epsilon$ for $P_{\\\\mu_1, \\\\mu_2, \\\\Sigma}$ gives\\n\\n(i) the bound (decision margin)\\n\\n$$\\\\|\\\\Delta\\\\|_p = \\\\left| \\\\left( x - \\\\mu_1 + \\\\mu_2 \\\\right)^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\right| \\\\|\\\\Sigma^{-1} (\\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}))\\\\|_q,$$\\n\\n(ii) the scaled bound\\n\\n$$\\\\|\\\\bar{\\\\Delta}\\\\|_p = \\\\left| \\\\left( x - \\\\mu_1 + \\\\mu_2 \\\\right)^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\right| \\\\tilde{\\\\mu}^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\|\\\\Sigma^{-1} (\\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}))\\\\|_q,$$\\n\\n(iii) the standard accuracy\\n\\n$$a = \\\\Phi(\\\\tilde{\\\\mu}^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\|\\\\Sigma^{-1} (\\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}))\\\\|)$$\\n\\n(iv) the expected scaled bound of correct samples\\n\\n$$E \\\\|\\\\bar{\\\\Delta}\\\\|_p | f_\\\\epsilon(x) = y = 1 \\\\sqrt{\\\\frac{2}{\\\\pi}} \\\\frac{1}{a} \\\\Phi^{-1}(a) e^{-\\\\frac{1}{2} (\\\\Phi^{-1}(a))^2} + 1,$$\\n\\nwhere $z \\\\Sigma$ is the solution of the convex problem\\n\\n$$\\\\arg \\\\min \\\\|z\\\\|_p \\\\leq \\\\epsilon (\\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}))$$\\n\\nand $\\\\Phi$ denotes the CDF of the standard normal distribution.\\n\\n**Proof.** We follow the proof of Theorem 3.1 and consider the classifier in equation 4. By H\u00f6lder's inequality, we now have the corresponding lower bound and scaled lower bound as\\n\\n$$\\\\|\\\\Delta\\\\|_p = \\\\left| \\\\left( x - \\\\mu_1 + \\\\mu_2 \\\\right)^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\right| \\\\|\\\\Sigma^{-1} (\\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}))\\\\|_q$$\\n\\n$$\\\\|\\\\bar{\\\\Delta}\\\\|_p = \\\\left| \\\\left( x - \\\\mu_1 + \\\\mu_2 \\\\right)^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\right| \\\\tilde{\\\\mu}^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\|\\\\Sigma^{-1} (\\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}))\\\\|_q$$\\n\\n$$= \\\\left| \\\\left( x - \\\\mu_1 + \\\\mu_2 \\\\right)^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\right| \\\\tilde{\\\\mu}^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\|\\\\Sigma^{-1} (\\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}))\\\\|_q$$\\n\\n$$= \\\\left| \\\\left( x - \\\\mu_1 + \\\\mu_2 \\\\right)^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\right| \\\\tilde{\\\\mu}^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\|\\\\Sigma^{-1} (\\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}))\\\\|_q$$\\n\\n$$= \\\\left| \\\\left( x - \\\\mu_1 + \\\\mu_2 \\\\right)^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\right| \\\\tilde{\\\\mu}^T \\\\Sigma^{-1} \\\\left( \\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}) \\\\right) \\\\|\\\\Sigma^{-1} (\\\\tilde{\\\\mu} - z \\\\Sigma (\\\\tilde{\\\\mu}))\\\\|_q$$\\n\\n16\"}"}
{"id": "MmZJ3kJXjX", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For a sample where $q$ (iii)\\n\\nSince the bound\\n\\n$\\\\|\\\\mu_\\\\sim \\\\|$ (ii)\\n\\nThe standard accuracy by centers to the classifier. We note that now the distances from the two Gaussian centers to the classifier are different, (i)\\n\\nConsider the Bayes optimal robust classifier in equation 4, we can calculate the analytical $\\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\| = \\\\|\\\\mu_\\\\sim\\\\| - \\\\|\\\\mu_\\\\sim T\\\\|$, (1)\\n\\n$\\\\|\\\\mu_\\\\sim \\\\"}
{"id": "MmZJ3kJXjX", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Therefore, the bound of the correctly-classified samples, we have that\\n\\\\[ t \\\\]\\n\\nSince\\n\\\\[ \\\\widetilde{t} \\\\]\\n\\nLet\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\nNow consider the classifier in equation 5 and the corresponding scaled bound from (ii),\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\nRecall that\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\nWhat Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\nLet\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\ndenote\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\[ \\\\widetilde{\\\\epsilon} \\\\]\\n\\n\\\\["}
{"id": "MmZJ3kJXjX", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1 Evaluating Pretrained Image Representations using Synthetic Data (SynBench)\\n\\n**Input**\\n- A representation network $g_{\\\\theta}$: $\\\\mathbb{R}^d \\\\rightarrow \\\\mathbb{R}^{d'}$, threshold accuracy $a_T$, (optional) the probability density function of the synthetic data manifold $P_{\\\\mu}$ and $P_{\\\\Sigma}$.\\n\\n**Output:** SynBench-score that quantifies the robustness-accuracy performance.\\n\\n1. if $P_{\\\\mu}$ and $P_{\\\\Sigma}$ are specified\\n   2. $\\\\mu \\\\sim P_{\\\\mu}$, $\\\\Sigma \\\\sim P_{\\\\Sigma}$.\\n   3. else\\n      4. $\\\\mu = s \\\\cdot 1_d / \\\\sqrt{d}$, $s \\\\sim U\\\\{0.1, 5\\\\}$, and $\\\\Sigma = I_d$.\\n   5. end if\\n\\n6. Draw $n$ synthetic data hyper-parameters $\\\\{ (\\\\mu_k, \\\\Sigma_k) \\\\}_{k=1}^n$.\\n\\n7. for $k \\\\leftarrow 1$ to $n$\\n   8. Generate class-conditional Gaussian data $(x_{\\\\text{train}}, y_{\\\\text{train}})$ and test set $(x_{\\\\text{test}}, y_{\\\\text{test}})$ following $x - \\\\bar{\\\\mu} | y \\\\sim N(\\\\mu_k, \\\\Sigma_k)$ and $\\\\bar{\\\\mu} = 0.5 \\\\cdot 1_d / \\\\sqrt{d}$.\\n   9. Calculate $a_{\\\\text{input}}^k$, the theoretical accuracy for input data, following Thm 3.1(iii).\\n   10. Calculate $b_{\\\\text{input}}^k$ (denotes $E \\\\| \\\\bar{\\\\Delta} x \\\\|_2 | f \\\\in (x) = y$), the expected scaled bound of correct samples for input data, following Thm 3.1(iv).\\n\\n11. Gather representations for class 1 training samples $z_{\\\\text{train},i}^1 = g_{\\\\theta}(x_{\\\\text{train},i})$ if $y_{\\\\text{train},i} = 1$, representations for class 2 training samples $z_{\\\\text{train},j}^2 = g_{\\\\theta}(x_{\\\\text{train},j})$ if $y_{\\\\text{train},j} = -1$, and $z_{\\\\text{test}} = g_{\\\\theta}(x_{\\\\text{test}})$.\\n\\n12. Estimate class-conditional Gaussian in the representation space by $\\\\mu'_1 = \\\\frac{1}{n_1} \\\\sum_{i=1}^{n_1} z_{\\\\text{train},i}^1$, $\\\\mu'_2 = \\\\frac{1}{n_2} \\\\sum_{j=1}^{n_2} z_{\\\\text{train},j}^2$, $\\\\Sigma' = \\\\frac{1}{n_1} \\\\sum_{i=1}^{n_1} (z_{\\\\text{train},i}^1 - \\\\mu'_1)(z_{\\\\text{train},i}^1 - \\\\mu'_1)^T + \\\\frac{1}{n_2} \\\\sum_{j=1}^{n_2} (z_{\\\\text{train},j}^2 - \\\\mu'_2)(z_{\\\\text{train},j}^2 - \\\\mu'_2)^T$.\\n\\n13. Derive Bayes optimal classifier $f'_{\\\\epsilon}$ for class-conditional Gaussian distribution $z | y = 1 \\\\sim N(\\\\mu'_1, \\\\Sigma')$, $z | y = -1 \\\\sim N(\\\\mu'_2, \\\\Sigma')$.\\n\\n14. Calculate $a_{\\\\text{repre}}^k$, the accuracy of $f'_{\\\\epsilon}$ for representations $z_{\\\\text{test}}$, empirically.\\n\\n15. Calculate the scaled bound of correct samples for representations following Thm 3.1(ii), $\\\\| \\\\bar{\\\\Delta} z \\\\|_2 = |(z_{\\\\text{test}} - \\\\mu'_1 + \\\\mu'_2)^T \\\\Sigma'^{-1} (\\\\bar{\\\\mu} - z_{\\\\Sigma}'(\\\\bar{\\\\mu}))|$, where $\\\\bar{\\\\mu} = \\\\mu'_1 - \\\\mu'_2$.\\n\\n16. Estimate $b_{\\\\text{repre}}^k$, the expected scaled bound of correct samples for representations empirically, by the arithmetic mean.\\n\\n17. end for\\n\\n18. Calculate $E(a_T)$ for input data with $\\\\{ a_{\\\\text{input}}^k, b_{\\\\text{input}}^k \\\\}_{k=1}^n$ according to equation 2.\\n\\n19. Calculate $E_{\\\\theta,\\\\epsilon}(a_T)$ for representations with $\\\\{ a_{\\\\text{repre}}^k, b_{\\\\text{repre}}^k \\\\}_{k=1}^n$ according to equation 2.\\n\\n20. Calculate SynBench-Score $(\\\\theta, \\\\epsilon, a_T) = \\\\int a_T E_{\\\\theta,\\\\epsilon}(a_T) da_T \\\\int a_T E(a_T) da_T$. 19\"}"}
