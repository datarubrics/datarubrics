{"id": "stanton22a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders\\n\\nan LANMT decoder, which allows for careful comparisons between LaMBO and discrete genetic optimizers. Despite their complexity, LANMT decoders accommodate insertions and deletions more gracefully than MLM heads by means of a length prediction head and length transform operation, allowing the same latent representation to be decoded to sequences of varying length. In either case, the decoder uses the same layer types as the shared encoder $g$, and takes both $Z$ and $Z_1$ as input.\\n\\n4.2. Initializing the Latent Embeddings\\n\\nAt each outer-loop iteration $i$, there are many choices of $x$ we are confident will not improve our current solution $P_i$, corresponding to large flat regions of the inner-loop loss surface. If the inner-loop is not initialized carefully, it is very likely the initial solution will fall in one of these flat regions, severely hampering gradient-based optimization. If $x$ was continuous, we could use initialization heuristics from previous work to avoid this pitfall (Balandat et al., 2020; Daulton et al., 2021a). Instead we now show how the same corruption process used to train and sample from DAEs can be used as a robust initialization procedure for discrete $x$ by initializing the inner loop solution in latent-space ($Z_0$) with corrupted, encoded variants of the current outer-loop solution $P_i$.\\n\\nSelecting base sequences: we begin with a set of seed sequences $X_{pool}$ and select a subset, $X_{base} \\\\subset X_{pool}$. After each optimization round, the latest query sequences are added to $X_{pool}$ and can serve as future base sequences. Choosing $X_{base}$ well is critical for fast convergence. If too many low quality sequences are added, too much computation is spent optimizing them. Conversely, only selecting the current Pareto sequences could hinder exploration of sequences with potential for improvement. The interaction between the online queries and the decoder head must also be considered, since we want to prevent the generative samples from collapsing to a couple sequences.\\n\\nWe populate $X_{base}$ first with the current Pareto sequences $P_i$, then with random sequences (without replacement) that were on the Pareto frontier in previous optimization rounds ($P_{\\\\preceq i}$), and finally fill any remaining space in the base set with random sequences from the entire optimization history. In practice, we took the size of the base set to be the same as the query batch size $b$. We perform multiple restarts during the inner loop, and each restart samples $b$ sequences from $X_{base}$, with replacement. We do not sample any base sequences uniformly at random, but according to a weighted distribution $\\\\Delta p_{X_{pool} q}$ to ensure that high-scoring sequences are more likely to be optimized than low-scoring ones.\\n\\nLet $r_{i p x_j, X}$ be the rank of $x_j P_{X}$ w.r.t. the 0-indexed dense ranking of $X$ according to $f_i$, and let $r_{max p x_j, X}$ be...\"}"}
{"id": "stanton22a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders\\n\\nNSGA-2 (Model-Free)  GA + MTGP + NEHVI  LaMBO ($=0.01$)\\n\\nFigure 3. On all four tasks (described in Section 5.1), LaMBO outperforms genetic algorithm baselines, specifically NSGA-2 and a model-based genetic optimizer with the same surrogate architecture (GA + MTGP + NEHVI). Performance is quantified by the hypervolume bounded by the optimized Pareto frontier. The midpoint, lower, and upper bounds of each curve depict the 50%, 20%, and 80% quantiles, estimated from 10 trials. See Section 5.2 for more discussion.\\n\\n4.3. Sequence Optimization\\n\\nAt a high level, we solve the inner-loop by treating the output of the decoder head $h$ as a proposal distribution. We iteratively refine the proposal distribution by following a regularized acquisition gradient in latent space, drawing and scoring batches of sequences along the way (Algorithm 2).\\n\\nMore precisely, once we have selected and corrupted the base sequences, we pass them through the encoder to produce latent embeddings $Z_0$ that serve as the initial solution for the inner-loop optimization problem. Then for each optimization step $0 \\\\leq j \\\\leq j_{\\\\text{max}}$, we take $Z_{j+1} = w_p Z_j + \\\\eta \\\\nabla_{Z_j} \\\\ell_{\\\\text{query}}(p, Z_j, q)$, where $Z_1 = w_p Z_j q$ (i.e. the output of the discriminative encoder $w_p Z_j$), $\\\\ell_{\\\\text{query}}(p, Z_j, q)$\\n\\nis the Shannon entropy, and $\\\\eta$, $\\\\lambda$ are hyperparameters controlling the step-size and regularization strength. We sample $X_{\\\\text{batch}} = x_0, \\\\ldots, x_{b\\\\text{u}}$ through the shared encoder and discriminative head. If we see that $X_{\\\\text{batch}}$ has the best acquisition value so far, we store it and continue optimizing. Note that this procedure produces a different result than simply optimizing $Z$ and decoding once at the end. Recall that the decoder is stochastic, and the ultimate goal of the inner loop is to produce sequences with high acquisition value, not just high-scoring latent embeddings.\\n\\nWe observed that following the unregularized acquisition gradient caused the decoder entropy to quickly increase, resulting in very uniform proposal distributions. When the proposal distributions are uniform, LaMBO essentially performs a variant of random search. However, because $Z_0$ is produced in the same way the decoder head is trained (i.e. the same corruption process), we expect that it will be close to other latent embeddings seen during training, and the decoder entropy will be relatively low. These observations motivated us to include the proposal entropy penalty in Eq. (3), to encourage $Z_j$ to stay in a region of latent space where the decoder has non-uniform predictive distributions.\\n\\nWe also considered choosing a small step-size $\\\\eta$ to implicitly confine $Z_j$ to a small region around $Z_0$, but we found it difficult to choose $\\\\eta$ both large enough to improve the $\\\\ell_{\\\\text{query}}$ and small enough to prevent uniform proposals. The proposal entropy penalty also has the added benefit of smoothing the query loss surface when using acquisitions like NEHVI, which helps make the inner loop less dependent on a good initialization. Adding a regularization term for improved control of the acquisition function has been previously studied in the continuous setting by Shahriari et al. (2016).\\n\\n5. Empirical Evaluation\\n\\nWe now evaluate LaMBO on a suite of small-molecule and large-molecule sequence design tasks. In Section 5.1 describe our suite of in silico tasks, including a new multi-objective large-molecule task which in which we maximize the folding stability and SASA of RFPs. See Appendix C.1 for an experiment comparing SSK GPs and DKL GPs, and Appendix C.2 for an experiment comparing LSBO and LaMBO. In Section 5.2 we show that LaMBO outperforms strong genetic algorithm baselines in a carefully controlled comparison, followed by two investigative experiments in Section 5.3 which give insight into the design choices behind LaMBO. Finally in Section 5.4 we analyze molecules designed by LaMBO, including in vitro wet lab results showing the discovery of improved RFP variants.\\n\\n5.1. Evaluation procedure\\n\\nWe consider the following in silico evaluation tasks, with full descriptions in the appendix:\\n\\n- **Bigrams**: optimize short strings (around 32 tokens) uniformly sampled from $X$ to maximize the counts of three predetermined bigrams (Appendix A.1).\"}"}
{"id": "stanton22a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders\\n\\nUniform proposals\\n\\nDAE proposals\\n\\nLaMBO ( = 0)\\n\\nLaMBO ( = 0.01)\\n\\nFigure 4. An ablation of LaMBO\u2019s main components. Starting from our model-based genetic algorithm baseline (uniform proposals), we cumulatively add the elements described in Section 4.3: (1) DAE-generated proposals, (2) DAE proposal optimization following $\\\\nabla_{\\\\mathbf{z}} r_{\\\\ell_{\\\\text{query}}}^s$ with $\\\\lambda = 0$. (see Eq. (3)), and (3) DAE proposal optimization with $\\\\lambda = 0.01$. DAE proposals improve performance on all tasks. Proposal optimization is most helpful on Bigrams where the starting sequence distribution is very unlikely to produce high-scoring queries. The entropy penalty is detrimental when working with random sequences (a), but is helpful when working with biological sequences (b-d). The midpoint, lower, and upper bounds of each curve depict the 50%, 20%, and 80% quantiles, estimated from 10 trials.\\n\\n- logP + QED: optimize SELFIES-encoded small molecules (50-100 tokens) w.r.t. logP and QED (Appendix A.2).\\n- DRD3 docking + SA: optimize SELFIES-encoded small molecules (50-100 tokens) w.r.t. DRD3 docking and synthetic accessibility (SA) (Appendix A.3).\\n- Stability + SASA: optimize large-molecule RFPs (around 200 tokens) w.r.t. folding stability and SASA. Both objectives require the 3D protein structure to compute, however we treat the folding simulator as part of the black-box objective (Appendix A.4).\\n\\nUnless otherwise noted, each task begins with 512 examples in its start pool, and collects a total of 1024 online queries in batches of 16. No additional pretraining data is used. Each method uses the same architecture and hyperparameters for all tasks (Appendix B). We evaluate all methods by comparing the relative improvement of the hypervolume bounded by the Pareto front after $x$ black-box function calls compared to the starting hypervolume.\\n\\n5.2. Comparing to Multi-Objective Genetic Optimizers\\n\\nIn Figure 3 we compare LaMBO with a MLM decoder head against two genetic algorithm (GA) baselines. For this experiment we set the entropy penalty at $\\\\lambda = 0.01$. The simplest baseline is NSGA-2, a robust model-free multi-objective GA, which effectively simply randomly mutates solutions along the Pareto frontier. The other baseline is a model-based GA which also randomly mutates solutions but also screens new queries with a discriminative model, for which we use the same architecture and acquisition function (NEHVI) as LaMBO. Both GA baselines use a uniform mutation proposal distribution. In this experiment all optimizers are only allowed to change a single token per optimization round, and each optimizer selects base sequences and token positions in the same way. The model-based GA differs from LaMBO primarily in two respects: (1) the encoder is trained only through the supervised loss, and (2) the proposal distribution is uniform rather than generated by a DAE. In fact LaMBO can be viewed as a generalization of the model-based GA, where the proposal distribution is learned and optimized, rather than fixed a priori. The effect of (2) is most strongly seen in logP + QED, since the task requires very little exploration and the SELFIES vocabulary for small molecules is significantly larger than the amino acid vocabulary for proteins. LaMBO performs well on all four tasks, particularly those involving natural sequences (b-d). In contrast the starting distribution over sequences in the Bigrams task has the highest possible entropy, so LaMBO learns a good sampling distribution more slowly.\\n\\n5.3. Analyzing LaMBO\\n\\nHaving demonstrated that LaMBO compares favorably to genetic optimizers, we now examine the different components of LaMBO and how each contributes to performance. We first disentangle the effect of replacing a uniform proposal distribution with a DAE-generated one, and the effect of optimizing the proposal distribution by gradient descent on $\\\\ell_{\\\\text{query}}$. In Figure 4 we interpolate between the model-based GA baseline in Figure 3 and LaMBO by cumulatively adding DAE-generated proposals, proposal optimization, and the proposal entropy penalty. We find that unoptimized DAE proposals with NEHVI screening is a strong baseline on all tasks. Proposal optimization is the most useful in Bigrams, where the true non-dominated solutions lie far outside the starting sequence distribution, requiring more exploration. For the same reason the entropy penalty is somewhat detrimental specifically in Bigrams, since it keeps new queries near those previously seen. See Figure...\"}"}
{"id": "stanton22a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Here we show the old and new Pareto fronts in objective space discovered by LaMBO in Section 5.2. For (a) higher is better for both objectives, and for (b) lower is better. For all tasks every point in the old frontier is strictly dominated by at least one point in the new frontier, and solutions are evenly spread across the new frontiers. In (c) we provide wet lab measurements of brightness (x-axis) and thermostability (y-axis) for proteins optimized for Stability + SASA. Higher is better for both objectives. We discovered new, non-dominated protein variants for three of the five ancestor proteins in our evaluation set.\\n\\nFigure 6. Pareto fronts of LaMBO and a variant that optimizes the expected average objective value on the RFP task. NEHVI incentivizes more exploration of the frontier extremities, resulting in solutions representing more diverse functional tradeoffs and slightly higher hypervolume. See Section 5.4 for more discussion.\\n\\nIn Figure 6, we evaluate the sensitivity of LaMBO to the choice of acquisition function on the RFP task. We compare the final Pareto frontier obtained with a simple multi-objective scalarization (averaging normalized scores) to the frontier obtained with NEHVI. Score averaging focuses optimization on solutions with similar tradeoffs, pushing the interior of the frontier out quickly. This behavior leads to less exploration than NEHVI and thus a slightly lower hypervolume of 1.51 as compared to NEHVI's hypervolume of 1.57. Our findings corroborate similar results in previous work comparing scalarization and hypervolume-based acquisitions in different problem settings (Emmerich, 2005; Emmerich et al., 2011; Daulton et al., 2020).\\n\\n5.4. Analyzing Sequence Designs\\n\\nThough metrics like hypervolume are useful for conducting baseline comparisons and ablation studies, such metrics do not give much insight into more qualitative aspects of the sequences we are designing. In Figure 5(a-b) we show the Pareto fronts found by LaMBO for our two small-molecule tasks, as we did in Figure 1 for the large-molecule task. For every task every sequence on the old frontier is strictly dominated by at least one sequence on the new frontier. So far we have shown that LaMBO can optimize in silico objectives effectively, and argued in Appendix A.4 that the Stability + SASA objectives are likely correlated with properties of RFPs we actually wish to optimize in the real world, specifically brightness and thermostability. In Figure 5(c) we evaluate in vitro protein sequences designed by LaMBO to optimize Stability + SASA, reporting brightness (log relative fluorescence units, log-RFU) and thermostability (protein melting temperature). We discovered non-dominated variants of three of the five ancestor sequences in our evaluation set, including strictly dominant variants of mScarlet and DsRed.M1, with inconclusive results for the other two ancestor sequences. See Appendix A.5 for more details about our experimental procedure and results. Our results indicate that Stability + SASA are good in silico proxy objectives for a real-world protein design task.\\n\\nIn Figure 7 we visualize solutions at different points on the optimized Pareto frontiers found by LaMBO for our two small-molecule tasks. Intriguingly, we find that although we did not specifically optimize for solution diversity in sequence space, the sequences change significantly as we move along the frontiers. In contrast, many generative-only approaches use heuristics to explicitly encourage diversity in sequence space and prevent solution collapse. Some of the proposed molecules shown have features such as large macrocycles in 7(a)-2 and 7(b)-1, or three-carbon rings in 7(a)-4 and 7(a)-5, that are likely not synthetically accessible (Gao et al., 2021). However, we note that logP + QED does not explicitly incentivize accessibility, and re-\"}"}
{"id": "stanton22a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nBayesian optimization (BayesOpt) is a gold standard for query-efficient continuous optimization. However, its adoption for drug design has been hindered by the discrete, high-dimensional nature of the decision variables. We develop a new approach (LaMBO) which jointly trains a denoising autoencoder with a discriminative multi-task Gaussian process head, allowing gradient-based optimization of multi-objective acquisition functions in the latent space of the autoencoder. These acquisition functions allow LaMBO to balance the explore-exploit tradeoff over multiple design rounds, and to balance objective tradeoffs by optimizing sequences at many different points on the Pareto frontier. We evaluate LaMBO on two small-molecule design tasks, and introduce new tasks optimizing in silico and in vitro properties of large-molecule fluorescent proteins. In our experiments LaMBO outperforms genetic optimizers and does not require a large pretraining corpus, demonstrating that BayesOpt is practical and effective for biological sequence design.\\n\\n1. Introduction\\n\\nModern drug development is a very costly endeavor, with estimates ranging from $310M to $2.8B for each new drug (Wouters et al., 2020). There are three major phases of drug development: 1) target discovery, the proposal of a biological mechanism hypothesized to treat a specific medical condition, 2) drug design, the specification of a molecular payload which will interact with the proposed mechanism, and 3) clinical trials, the evaluation of the efficacy and safety of the payload in vivo. Though each phase contributes to the total cost of development, in this work we focus on the drug design phase. In particular we will optimize real-valued target properties (such as neurotransmitter receptor affinity or protein folding stability) for payloads represented as discrete sequences (e.g. SMILES strings). Since the mappings from sequence to target are often unknown, expensive to observe in vitro, and difficult to simulate, we pose biological sequence design as a costly black-box optimization (BBO) problem over a large discrete search space.\\n\\nRecent work applying deep learning to biological tasks has primarily focused on learning from a static, offline dataset (Rao et al., 2019; Jumper et al., 2021; Baek et al., 2021; Meier et al., 2021; Rives et al., 2021). When these models are used to select new sequences to label, they are applied in a one-shot fashion, without accounting for future design rounds (Gligorijevic et al., 2021; Biswas et al., 2021). Because labels are scarce for many important targets, it is...\"}"}
{"id": "stanton22a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders\\n\\n...to account for model uncertainty to manage the explore-exploit tradeoff (O'Donoghue et al., 2018). Bayesian optimization (BayesOpt) is a powerful class of BBO algorithms, explicitly designed to coherently reason about online decision-making based on incomplete information (Brochu et al., 2010). BayesOpt balances the explore-exploit tradeoff in a principled way, relying on a probabilistic discriminative model to prioritize decisions with the highest potential payoff. At each decision point the discriminative model produces a posterior distribution over the hypothesis space of all functions the model can represent. The posterior formally represents the degree to which any particular hypothesis is plausible given the data and model assumptions. To make a decision, BayesOpt selects the best decision as defined by an acquisition function, such as expected improvement (EI), with each hypothesis contributing to the acquisition value in proportion to its posterior probability (Jones et al., 1998). BayesOpt is not only philosophically appealing, it is provably a no-regret strategy under the right conditions (Srinivas et al., 2010).\\n\\nLike many Bayesian methods, the barriers hindering widespread adoption of BayesOpt are not conceptual, but practical. Drug design in particular is a natural application domain, but introduces multiple challenges. Discrete, high-dimensional inputs: antibody therapeutic payloads are often RNA proteins which instruct the patient's immune system to produce the desired antibody. When proteins are represented as a sequence of residues, each identifying one of 20 possible amino acids, even a fairly small 200-residue protein is only one of $20^{200}$ options. By contrast, conventional BayesOpt works best on problems with 10 or fewer continuous decision variables, due to the properties of standard kernels, and the lack of gradients to maximize the acquisition function.\\n\\nBatched, multi-objective experiments: because considerations including efficacy, toxicity, and yield must all be taken into account, drug design is inherently multi-objective. Furthermore sequences are labeled in batches, necessitating the use of more sophisticated acquisition functions than standard workhorses like EI. Data-scarcity: wet lab experimental data is expensive and challenging to collect, so it is rare to have large-scale datasets with labels for the exact target properties of interest. No single BayesOpt method has been shown to simultaneously address all of these challenges (see Section 3). As a result, previous methods have necessarily only been evaluated on very stylized tasks that fail to capture key aspects of real drug design problems. In this work we present LaMBO to address this deficiency, and propose a novel in silico task which emulates protein design tasks more closely than common open-source drug design benchmarks. We preview our results applying LaMBO to this new task in Figure 1, maximizing the stability and SASA of RFPs derived from the fpBase dataset (Lambert, 2019), with the initial Pareto frontier in objective space shown as a dashed line. The new Pareto frontier (the solid line) discovered by LaMBO is superior, as it is characterized more densely by new sequences that are Pareto improvements over their ancestors. In short, our contributions are as follows:\\n\\n1. We propose the LaMBO architecture, a novel combination of a generative DAE with a discriminative deep kernel learning GP, with a simple joint training procedure and an effective decision optimization routine.\\n\\n2. We propose a new in silico large-molecule task to augment existing open-source drug design benchmarks.\\n\\n3. We evaluate LaMBO in silico on two small-molecule tasks and our new large-molecule task, comparing to both genetic and latent-space BBO baselines, showing improved sample efficiency and solution quality.\\n\\n4. We present in vitro wet lab results using LaMBO to discover brighter, more thermostable red fluorescent proteins.\\n\\nCode here: github.com/samuelstanton/lambo.\"}"}
{"id": "stanton22a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Preliminaries\\n\\nWe now introduce the problem setting and BayesOpt.\\n\\n2.1. Discrete Multi-Objective Sequence Design\\n\\nWe first define the input space $X = S^t \\\\times V^t$, where $V$ is an ordered, discrete vocabulary of $v$ tokens, $t$ is the max sequence length, and $S$ is the Cartesian product. $V$ includes a padding token to accommodate sequences of varying length. Because $|X| = |V|^t$ is exponential in $t$, $X$ becomes too large to enumerate quickly as the sequence length grows, even if $|V|$ is relatively small. As a result, sequence optimization usually starts with a library or pool of initial sequences (see Figure 2, top), which are modified to produce new candidate sequences. When posed in this way, the optimization problem is restructured into three nested decisions: (1) Choose a base sequence from the pool. (2) Choose which positions on the sequence to change. (3) Choose how the sequence will be changed at those positions.\\n\\nWe represent sequence design as a multi-objective optimization problem:\\n\\n$$\\\\min_{x \\\\in X} p f_1(x), \\\\ldots, f_k(x)$$\\n\\nwhere $k$ is the number of objectives and each $f_i: X \\\\to \\\\mathbb{R}$ is an expensive, black-box function of decision variables $x$. Given two feasible solutions $x$ and $x'$, $x$ dominates $x'$ ($x \\\\preceq x'$) if $f_i(x) \\\\leq f_i(x')$ for all $i$, and $f_i(x) < f_i(x')$ for some $i$.\\n\\nIn general, there will not be a single dominating solution $x^*$; instead, we define the set of non-dominated solutions (i.e. the true Pareto frontier $P^*$):\\n\\n$$P^* = \\\\{ x \\\\in X | \\\\exists x' \\\\in X \\\\text{ s.t. } f_i(x) = f_i(x'), \\\\forall i \\\\}.$$ (1)\\n\\nSince $P^*$ is unknown, we seek a set of candidate solutions $P$ that are close in objective space to those in $P^*$. We find these solutions by maximizing the hypervolume bounded by the extremal points in $P_Y$, where $x_{ref}$ is some reference solution.\\n\\n2.2. BayesOpt\\n\\nSee Brochu et al. (2010) and Frazier (2018) for a more complete introduction to BayesOpt. BayesOpt constructs a probabilistic surrogate model $\\\\hat{f}$ which is trained to emulate $f$ from a dataset $D_n$:\\n\\n$$\\\\{ x_1, y_1, \\\\ldots, x_n, y_n \\\\},$$\\n\\nwhere $y_i$ are noisy observations of $f(x_i)$ (e.g. $y_i = f(x_i) + \\\\epsilon_i$, $\\\\epsilon_i \\\\sim N(0, \\\\sigma^2 I)$). The surrogate posterior distribution is used to define an acquisition function $a: \\\\hat{F} \\\\to \\\\mathbb{R}$, which in turn defines an inner loop optimization problem to select new query point(s). The objective function is then queried at the selected points and the surrogate is retrained on the augmented dataset, and the procedure repeats, forming an outer loop (Algorithm 1).\\n\\nGPs (Williams & Rasmussen, 2006) are often preferred as surrogates because they have closed-form posteriors and work well in data-scarce regimes. The inductive biases of a GP are primarily determined by the choice of kernel, which defines a prior distribution over the values of $f$ for any finite collection of inputs. Most commonly used GP kernels (e.g. RBF or Mat\u00e9rn) rely on $\\\\ell^2$ distance between inputs to determine the prior covariance between outputs. When the inputs are low-dimensional (e.g. $d = 10$) such kernels work well, but in high dimensions the $\\\\ell^2$ norm is often a poor choice of distance metric (Srinivas et al., 2010; Wang et al., 2016). This limitation has motivated the development of deep kernel learning (DKL), which learns a low-dimensional continuous embedding via an encoder such as a convolutional neural network (CNN) (Wilson et al., 2016). Although GPs are kernel-based models, there is a range of well-known methods to scale them to large, online datasets, notably inducing point methods like stochastic variational GPs (SVGPs) which admit the use of stochastic optimizers (Hensman et al., 2013; Maddox et al., 2021c).\\n\\n3. Related Work\\n\\nDiscrete Optimization by Sampling: genetic algorithms (GAs) such as NSGA-II (Deb et al., 2002) slowly evolve a good solution by random mutation. GAs are a simple, popular baseline for BBO problems, but are known for being inefficient (Turner et al., 2021). One solution is to continue generating mutations randomly, but screen the proposed queries with a discriminative model before labeling (Nigam et al., 2019; Yang et al., 2019b). Other solutions focus on proposing mutations more intelligently, including RL-based approaches (Angermueller et al., 2020a;b) and generative approaches (Jensen, 2019; Biswas et al., 2021; Zhang et al., 2021). In particular the generative approach described by Lee et al. (2018) and Gligorijevic et al. (2021) inspired the LaMBO architecture. All the approaches just discussed are greedy in the sense that they rely on point estimates of the objective values to select new queries.\\n\\nDiscrete BayesOpt: excluding library-based approaches such as Yang et al. (2019a), discrete BayesOpt methods can be categorized by how they structure the inner loop optimization problem. Some methods use substring kernels (SSKs), optimizing queries directly in sequence space with\"}"}
{"id": "stanton22a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders\\n\\nKhan et al. (2022) is an example of concurrent antibody design work in this vein, exploiting task-specific knowledge of complementarity-determining regions (CDRs) of the antibodies to make the problem tractable.\\n\\nLatent-space optimizers learn continuous sequence embeddings \\\\( Z \\\\), which are shared by a generative decoder modeling \\\\( p(x|Z) \\\\) and the discriminative surrogate modeling \\\\( p(y|Z) \\\\) (Deshwal & Doppa, 2021; Grosnit et al., 2021; Maus et al., 2022). Thus \\\\( Z \\\\) can be optimized with gradient-based methods to produce new sequences. LaMBO is most similar to Latent-Space BayesOpt (LSBO) (G\u00f3mez-Bombarelli et al., 2018), since both methods model \\\\( p(y|Z) \\\\) as a GP and model \\\\( p(x|Z) \\\\) via an autoencoder. LSBO uses a VAEPREtrained on a large dataset to solve single-objective tasks, training the VAEPREtrained weights and the auxiliary GP head separately. With a specialized architecture proposed by Jin et al. (2018) and a biased VAEPREobjective proposed by Tripp et al. (2020), LSBO has been shown capable of solving simple small-molecule tasks such as maximizing penalized logP. Aside from LaMBO's use of DAEs rather than VAEs, this work improves upon LSBO in multiple ways, such as enabling the use of a general-purpose architecture for both small and large molecules, removing the need to pretrain the autoencoder, providing a reliable procedure for jointly training generative and GP heads with a shared encoder, and the introduction of multi-objective tasks and acquisition functions.\\n\\nMulti-Objective BayesOpt:\\n\\nDaulton et al. (2020) proposed a batch version of expected hypervolume improvement (EHVI) (Emmerich, 2005; Emmerich et al., 2011), an extension of EI to multiple objectives. In follow-up work, Daulton et al. (2021a) proposed the noisy expected hypervolume improvement (NEHVI) acquisition function, which extends noisy expected improvement (NEI) (Letham et al., 2019) to multiple objectives. Multi-task GPs (MTGPs) have previously been used as surrogates for multi-objective BayesOpt (Shah & Ghahramani, 2016), including recent work scaling MTGPs up to thousands of training examples or objectives in the continuous setting (Daulton et al., 2021b; Maddox et al., 2021a). We make use of NEHVI and MTGPs, including an efficient MTGP posterior sampling approach developed in Maddox et al. (2021b).\\n\\n4. Latent-Space Multi-Objective BayesOpt\\n\\nWe now describe the key ideas behind LaMBO, summarized in Figure 2 and Algorithm 2.\\n\\n4.1. Architecture Overview\\n\\nWe use a non-autoregressive denoising autoencoder to map discrete sequences to and from sequences of continuous token-level embeddings, with a multi-task GP head operating on pooled sequence-level embeddings. Unlike previous work combining GPs with deep generative models, we do not require a pretrained autoencoder, nor do we require the surrogate to be completely reinitialized after receiving new data. Furthermore, we demonstrate that both stochastic variational and exact GP inference can be used, alleviating concerns regarding computational scalability or applicability to noisy objectives with non-Gaussian likelihoods. See Appendix B for more details.\\n\\nShared Encoder:\\n\\nwe use a non-autoregressive bidirectional encoder \\\\( g_{\\\\text{p}x}, \\\\theta_{\\\\text{enc}} q \\\\)\\\\( z_1, \\\\ldots, z_t \\\\)\\\\( Z \\\\), where \\\\( z_i \\\\) are latent token-level embeddings. In particular, our encoder is composed of 1D CNN layers, using standard vocabulary embeddings and sinusoidal position embeddings, padding token masking, skip-connections, and layernorm. A key advantage of using a DAE rather than a VAE is that projects like ChemBERTa, TAPE and ESM have already openly released large DAE models trained on large sequence corpora (Chithrananda et al., 2020; Rao et al., 2019; Rives et al., 2021). As a result, encoders from these models could be used as drop-in replacements for our small CNN encoder.\\n\\nDiscriminative Head:\\n\\nthis head takes an encoded sequence \\\\( Z \\\\) and outputs a scalar value indicating the utility of selecting that sequence as a query point. We pass \\\\( Z \\\\) to a discriminative encoder \\\\( w \\\\) to obtain transformed embeddings \\\\( Z_1 \\\\), then pool \\\\( Z_1 \\\\) into low-dimensional sequence-level features. The discriminative encoder is smaller than the shared encoder, for example a single residual CNN block. The pooling operation \\\\( p|I_p x q|q \\\\)\\\\( z_{1_i} \\\\) averages token-level embeddings over a restricted index set \\\\( I_p x q \\\\) which excludes positions corresponding to padding tokens. We define a multi-task GP kernel by combining a \\\\( \\\\frac{1}{2} \\\\)Mat\u00e9rn kernel evaluated on these sequence features with an intrinsic model of coregionalization (ICM) kernel over \\\\( f_i \\\\) (Goovaerts et al., 1997; Alvarez et al., 2011; Rasmussen & Williams, 2008). The resulting GP outputs a posterior predictive distribution \\\\( p(f|D) \\\\), which is passed as input to the acquisition function.\\n\\nGenerative Decoder Head:\\n\\nthis head \\\\( h \\\\) maps a sequence of token-level embeddings to a predictive distribution over \\\\( X \\\\) and can either be a simple masked language model (MLM) head (Devlin et al., 2018) or a full seq2seq latent non-autoregressive neural machine translation (LANMT) decoder (Shu et al., 2020). In this work we make use of both. An MLM head is simpler and easier to control than...\"}"}
{"id": "stanton22a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Distribution of the MLM head at the masked positions. During sequence optimization the MLM predictive distribution is modified to prevent sampling the original token (to encourage diversity) and to prevent the sampling of special tokens.\\n\\n**LANMT Head**\\n\\nOur LANMT head is identical to our MLM head, except for the addition of a length prediction head and length transform module (Shu et al., 2020), a different corruption procedure and training objective. We used a max length change of $8$ in our experiments for Figure 10, so the corruption function randomly sampled a length change $\\\\Delta t$ between $-8$ and $8$. $\\\\Delta t$ tokens were subsequently deleted, replaced, or inserted into the sequence. The corrupted sequence was forwarded through the model, which was also given the original sequence length as a label during training. A training step takes a gradient step on the cross-entropy between the predicted length and the actual length, and on the cross-entropy between the predictive distribution over the whole decoded sequence and the original sequence.\\n\\n### B.4. Hyperparameters\\n\\n**Sequence Optimization**\\n\\n| Name                                      | Value |\\n|-------------------------------------------|-------|\\n| $D_0$                                      | 512   |\\n| Query batch size ($b$)                     | 16    |\\n| $X_{\\\\text{base}}$                          |       |\\n| # Optimization rounds ($i_{\\\\text{max}}$)   | 64    |\\n| # Inner loop restarts                      | 16    |\\n| # Inner loop gradient steps ($j_{\\\\text{max}}$) | 32    |\\n| Inner loop step size ($\\\\eta$)              | 0.1   |\\n| Entropy penalty ($\\\\lambda$)                | $1e^{-2}$ |\\n| # MC acquisition samples                   | 2     |\\n| Random seeds                               | $t_0$, $t_1$, ..., $t_9$ |\\n\\n**DAE Architecture**\\n\\n| Name                                      | Value |\\n|-------------------------------------------|-------|\\n| Shared enc. depth (# residual blocks)     | 3     |\\n| Disc. enc. depth (# residual blocks)      | 1     |\\n| Decoder depth (# residual blocks)         | 3     |\\n| Conv. kernel width (# tokens)             | 5     |\\n| # conv. channels                          | 64    |\\n| Latent dimension                          | 16    |\\n| GP likelihood variance init                | 0.25  |\\n| GP lengthscale prior                       | $N_{p0}$, $N_{q0}$ |\\n| # inducing points (SVGP head)             | 64    |\\n\\n**DAE Training**\\n\\n| Name                                      | Value |\\n|-------------------------------------------|-------|\\n| DAE corruption ratio (training)           | 0.125 |\\n| DAE learning rate (MTGP head)             | $5e^{-3}$ |\\n| DAE learning rate (SVGP head)             | $1e^{-3}$ |\\n| DAE weight decay                          | $1e^{-4}$ |\\n| Adam EMA params ($\\\\beta_1$, $\\\\beta_2$)   | $(0., 1e^{-2})$ |\\n| Early stopping holdout ratio              | 0.1   |\\n| Early stopping relative tolerance         | $1e^{-3}$ |\\n| Early stopping patience (# epochs)        | 32    |\\n| Max # training epochs                     | 256   |\"}"}
{"id": "stanton22a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C. Additional Results\\n\\nC.1. What About Substring Kernels?\\n\\nSince the start pools used in the evaluation in Figure 3 are fairly small (i.e., 512 examples), it is natural to wonder how a substring kernel (SSK) GP (e.g., Moss et al., 2020) would compete with our DKL-based GPs. Due to constraints on time and computation, and SSK scalability issues, we do not directly compare SSK GPs and DKL GPs in the online setting. However, in Figure 8, we compare SSK GPs and DKL GPs in the offline regression setting, training both models only through the supervised loss to predict the SASA property of RFP large molecules, using 410 examples for training and 102 examples for validation and test. The SSK GP performs fairly well, but is very under-confident when compared to the DKL GP. At an empirical level, our results do not support the claim that SSK GPs are superior models for biological sequences. SSKs have further drawbacks which make it hard to justify additional investment into SSK GPs for drug design.\\n\\nLack of Positional Information: at a conceptual level, SSKs cannot identify regions of the sequence that have little effect on the objective values, since matching substrings increase the prior covariance between sequences regardless of where the substrings are found. Simply put, an SSK just counts the occurrences of every possible $n$-gram across every possible combination of $n$ positions in a sequence. The gap decay hyperparameter downweights occurrences corresponding to position combinations with elements that are not closely colocated. Hence SSKs have very limited positional awareness in the sense that sequences with similar $n$-gram counts have high prior covariance, regardless of where the $n$-grams actually occurred. Positional awareness is important when dealing with biological sequences from some subpopulation (e.g., a family of fluorescent proteins) since they have many identical subsequences, only varying at positions that strongly affect function.\\n\\nDifficult to Scale: at a practical level, SSKs are difficult to integrate with deep generative models since they do not operate on continuous embeddings (Nigam et al., 2019), and standard methods for scaling GPs \u2014 inducing point methods (e.g., Hensman et al., 2013), random feature expansions (e.g., L\u00e1zaro-Gredilla et al., 2010), and CG-based methods (e.g., Gardner...\"}"}
{"id": "stanton22a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders... inducing points methods are impractical because the inducing point domain would be the discrete input space, introducing a challenging discrete optimization subproblem just to train the surrogate. Furthermore SSKs struggle to scale not just to large datasets, but also to long sequences. The dynamic programming algorithm used by Moss et al. (2020) to compute their SSK is parallelizable, but becomes prohibitively memory intensive for sequences longer than 100 tokens, even when chunking the sequence into smaller pieces. In fact, we used an Nvidia RTX 8000 GPU with 48 GB of memory just to produce Figure C.1. We also implemented a memory-efficient trie-based SSK, which could handle longer sequences but was prohibitively slow and difficult to parallelize.\\n\\nC.2. Comparison to LSBO for single-objective BBO\\n\\nHere we evaluate LaMBO in the single-objective setting, using the penalized logP task described in Tripp et al. (2020). In contrast to SSK-based methods, LaMBO can successfully be scaled to large datasets with standard variational GP inference, allowing us to compare to the popular latent space BayesOpt approach (LSBO) (G\u00f3mez-Bombarelli et al., 2018) on a larger-scale problem. The start pool for this task is composed of the 2000 highest scoring sequences from ZINC, and 8000 random sequences, replicating the setup in Tripp et al. (2020). To accommodate the larger dataset, the discriminative head uses $k$ independent stochastic variational GPs (SVGPs) with 64 shared inducing points rather than an exact MTGP.\\n\\nIn Figure 10 we demonstrate that LaMBO is competitive with a variant of LSBO specifically designed for this task, requiring about twice as many online observations before reaching the reported median best score attained by LSBO (Tripp et al., 2020). As noted in Section 3, LSBO uses the entire ZINC dataset for pretraining, so we do not directly compare sample efficiency. In addition to the differences between LaMBO and LSBO already noted, we use SELFIES encodings rather than SMILES. We use a seq2seq LANMT-style decoder head for this task, since logP heavily favors large molecules. High-scoring molecules such as those found by LSBO are larger than any found in ZINC, so it is important that the optimizer allow insertions.\\n\\nOverall, LaMBO outperforms the best reported LSBO score (27.84) by a wide margin, reaching scores as high as 50 for some seeds, while using a more general architecture and requiring less data (counting both pretraining data and online queries). The factor that ultimately bounds the penalized logP objective in practice is the max sequence length constraint imposed by the positional encoding scheme we use. Therefore we note that, despite its widespread use, unconstrained logP (penalized or otherwise) is a poor optimization benchmark, since it can be manipulated by altering the positional encoding to permit longer sequences (Nigam et al., 2019; Tripp et al., 2021; Fu et al., 2020).\\n\\nIn short, while LaMBO is designed to facilitate multi-objective optimization \u2014 a central feature of drug design \u2014 it can also outperform the widely used single-objective LSBO, even in a single-objective setting.\"}"}
{"id": "stanton22a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Ablating LaMBO: Proposal Entropy and Discriminative Performance\\n\\nBlack-Box Evaluations\\n\\nProposal Entropy\\n\\nTest Spearman's\\n\\nTest NLL\\n\\nFigure 11. Additional metrics for the ablation experiment described in Section 5.3 and Figure 4, where we cumulatively add the elements described in Section 4.3: (1) DAE-generated proposals, (2) DAE proposal optimization following $\\\\nabla Z_r \\\\ell_{\\\\text{query}} s$ with $\\\\lambda \\\\neq 0$. (see Eq. (3)), and (3) DAE proposal optimization with $\\\\lambda \\\\neq 0.01$. The top row shows the average entropy of the generative DAE proposal distributions over time. As expected, the entropy penalty decreases the proposal entropy. The middle and bottom rows show the discriminative Spearman's $\\\\rho$ and NLL (averaged across objectives) on heldout data over time. Training the LaMBO architecture with both the unsupervised DAE objective and the supervised GP objective improves discriminative performance compared to the same model trained only through the supervised objective. Otherwise the methods behave similarly, verifying that better solutions are the result of better proposals, rather than a better discriminative model. The midpoint, lower, and upper bounds of each curve depict the 50%, 20%, and 80% quantiles, estimated from 10 trials.\"}"}
{"id": "stanton22a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders\\n\\nShah, A. and Ghahramani, Z. Pareto frontier learning with expensive correlated objectives. In International Conference on Machine Learning, pp. 1919\u20131927. PMLR, 2016.\\n\\nShrake, A. and Rupley, J. A. Environment and exposure to solvent of protein atoms. lysozyme and insulin. Journal of molecular biology, 79(2):351\u2013371, 1973.\\n\\nShu, R., Lee, J., Nakayama, H., and Cho, K. Latent-variable non-autoregressive neural machine translation with deterministic inference using a delta posterior. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 8846\u20138853, 2020.\\n\\nSrinivas, N., Krause, A., Kakade, S., and Seeger, M. Gaussian process optimization in the bandit setting: no regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pp. 1015\u20131022, 2010.\\n\\nTripp, A., Daxberger, E., and Hern\u00e1ndez-Lobato, J. M. Sample-efficient optimization in the latent space of deep generative models via weighted retraining. Advances in Neural Information Processing Systems, 33, 2020.\\n\\nTripp, A., Simm, G. N., and Hern\u00e1ndez-Lobato, J. M. A fresh look at de novo molecular design benchmarks. In NeurIPS 2021 AI for Science Workshop, 2021.\\n\\nTurner, R., Eriksson, D., McCourt, M., Kiili, J., Laaksonen, E., Xu, Z., and Guyon, I. Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020. arXiv preprint arXiv:2104.10201, 2021.\\n\\nWang, Z., Hutter, F., Zoghi, M., Matheson, D., and de Freitas, N. Bayesian optimization in a billion dimensions via random embeddings. Journal of Artificial Intelligence Research, 55:361\u2013387, 2016.\\n\\nWilliams, C. K. and Rasmussen, C. E. Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006.\"}"}
{"id": "stanton22a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders\\n\\nIn Appendix A we outline the four evaluation tasks: Bigrams, logP + QED, DRD3 docking + SA, and Stability + SASA. In Appendix B, we describe in detail the neural network architecture, the DKL GP implementation, the denoising autoencoder implementation, the string kernel implementation, and the hyperparameters used for our experiments. Finally, in Appendix C, we present additional experimental results to supplement the figures in the main text.\\n\\nA. Evaluation Task Details\\n\\nA.1. Bigrams Task\\n\\nThe bigrams task is a simple toy example of discrete sequence optimization. We draw random strings from an alphabet $V$ and count the occurrence of $k$ predetermined bigrams, which we use as proxy fitness targets. The task is to maximize the counts of each bigram in the sequence, restricting the sequence length to 36 tokens including utility tokens (\\\"[PAD]\\\", \\\"[CLS]\\\", \\\"[SEP]\\\", \\\"[UNK]\\\", \\\"[MASK]\\\"). For our experiments we used the same amino acid vocabulary as our protein task and chose 3 complementary bigrams, \\\"A V\\\", \\\"VC\\\" and \\\"CA\\\". The initial sequences were sampled with lengths between 32 and 36 tokens. We ensured there were an equal number of positive examples (sequences with at least one occurrence of one of the bigrams) as negative examples in the starting pool.\\n\\nA.2. logP + QED Task\\n\\nThe original ZINC logP optimization task, popularized in the BayesOpt community by G\u00f3mez-Bombarelli et al. (2018), is to optimize the octanol-water partition coefficient of a small molecule. Molecules with high logP values are hydrophobic and molecules with low values are hydrophilic. Hydrophobicity can be desirable for absorption and solubility, for example in pharmaceuticals. As a property that is easy to calculate, it has risen to prominence despite being undesirable on its own. Very high logP can result in molecules with limited practical application, and moreover finding molecules with high logP reduces trivially to the problem of finding long hydrocarbon chains, as these compounds are extremely hydrophobic relative to the size of the molecule. The penalized logP objective adds auxiliary terms measuring synthetic accessibility (Ertl & Schuffenhauer, 2009) and the number of cycles. Unfortunately these terms do not fix the underlying problem, and so penalized logP is similarly vulnerable to optimization hacking, as we discuss in Section C.2.\\n\\nBecause logP in itself is a deeply flawed objective, both in its relevance to real-world drug design and its ability to be hacked by optimizers, we also consider a multi-objective optimization task that is closer in form to real design problems. Instead of solely optimizing for logP, we jointly optimize for logP and QED (Quantitative Estimate of Druglikeness), a composite metric that captures many elements of druglikeness, with bioavailability among the most prominent (Bickerton et al., 2012).\\n\\nUnfortunately QED has its own limitations as an objective, since it is a simple parametric model trained on a small dataset to emulate heuristics such as Lipinski's Rule of Five (Lipinski et al., 1997).\\n\\nThe shortcomings of objectives like logP and QED appear to be well-known (Nigam et al., 2019; Coley et al., 2020; Fu et al., 2020; Tripp et al., 2021; Maus et al., 2022), but superior alternatives have not yet been accepted by the research community. For example, at the time of writing the only molecule generation benchmark in TorchDrug is maximization of QED and logP of ZINC-like molecules.\\n\\n2 Angermueller et al. (2020a) evaluated BBO algorithms on a substantial number of in silico sequence design tasks, however the large molecule tasks they considered were relatively simple, single-objective problems (e.g. maximization of the likelihood of a hidden Markov model). The vacuum of rigorous in silico evaluation tasks for large-molecule design motivated us to propose our RFP task as a new benchmark.\\n\\nWe construct the start pool by inverting the scores and selecting the top-$k$ non-dominating sequences (i.e. we found the $k$ most dominated sequences in the ZINC dataset w.r.t. logP and QED). Constructing the task in this way is better than simply sampling randomly from ZINC because QED is bounded above by 1 and many ZINC sequences already score fairly close to 1. Starting with dominated sequences ensures that there is sufficient headroom for improvement to observe variations in optimizer behavior. We capped the max sequence length at 128 SELFIES tokens, including utility tokens. The SELFIES vocabulary was precomputed from the entire ZINC dataset (Krenn et al., 2020).\\n\\n2 https://torchdrug.ai/docs/benchmark/generation.html\"}"}
{"id": "stanton22a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use the DRD3 docking score oracle from Huang et al. (2021), and the following quotation is reproduced from https://tdcommons.ai/benchmark/docking_group/overview:\\n\\n\\\"Docking is a theoretical evaluation of affinity between a ligand (a small molecular drug) and a target (a protein involved in the disease). As a molecule with higher affinity is more likely to have higher bioactivity, docking is widely used for virtual screening of compounds (Lyu et al., 2019).\\\"\\n\\nBecause optimizing solely for docking may produce molecules that are difficult to synthesize, we also optimize for synthetic accessibility (SA) (Ertl & Schuffenhauer, 2009).\\n\\nTo construct the start pool for this task we selected 512 molecules from ZINC uniformly at random and labeled them with the objective oracles. We then used the same start pool and SELFIES encodings for all experimental trials and all optimization methods.\\n\\nIn this work we present a new in silico benchmark task designed to simulate searching for improved red fluorescent protein (RFP) variants in vitro, a problem of significant interest to biomedical researchers (Dance, 2021). We optimize red-spectrum proteins with known structures for stability ($-\\\\Delta G$ or negative change in Gibbs free energy) and solvent-accessible surface area (SASA) (Shrake & Rupley, 1973; Cock et al., 2009) in simulation, using the FoldX suite (Schymkowitz et al., 2005) and BioPython to evaluate our objective function. Stability as evaluated by FoldX\u2014particularly in the negative case\u2014has been shown to correlate with protein function (H\u00f8ie et al., 2021). Solvent-accessible surface area will correlate with factors that influence the brightness and photostability of the fluorescent protein: aggregation propensity due to exposed hydrophobic surface (Mishra et al., 2018) and shielding by the beta-barrel, which encapsulates the fluorophore (Chudakov et al., 2010).\\n\\nSince both of these benchmark tasks are functions of the protein's three-dimensional structure, it is expected that training a model on these tasks will require the model to learn a latent representation for structure, which in turn determines function. We constructed the start pool in two phases. First we searched FPBase for all red-spectrum (defined in this context as having an emission wavelength at least 580 nm) proteins with at most 244 residues with known 3D structures, selecting the highest resolution structure if more than one was available. If more than one chain was present in the structure, we selected the longest chain as the representative residue sequence. Starting from these base proteins, we used NSGA-2 to collect additional labelled sequences to use in the start pool for subsequent experiments.\\n\\nAlthough this task is a significant step forward for in silico evaluation of discrete sequence design, it is currently limited by the capabilities of FoldX, which can only compute structures from substitution mutations (i.e. the sequence length cannot change). Deep learning structure oracles such as AlphaFold (Jumper et al., 2021) or RoseTTAFold (Baek et al., 2021) could also be used, but we found FoldX to be simpler and more amenable for rapid prototyping.\\n\\nWe synthesized fluorescent proteins with PUREfrex 2.1 (Cosmo Bio LTD) in 50 uL reactions from linear DNA purchased from IDT as eBlock dsDNA gene fragments. We ran reactions at 30\u00b0C overnight in black, half-area microplates (Corning #3993) with optically clear plate adhesive and measured excitation and emission through a series of sweeps (fixing the excitation wavelength and scanning emission every 1\u20132 nm, or vice versa). We determined peak excitation and peak emission as the wavelength that gave maximum fluorescence units. Using NanoDSF on an Uncle instrument (Unchained Labs), we measured protein thermostability as $T_m$, defined as the midpoint transition temperature of the thermal melt curve.\\n\\nOur models are implemented in PyTorch (Paszke et al., 2019), BoTorch (Balandat et al., 2020), and GPyTorch (Gardner et al., 2018). Our genetic optimizer baselines are implemented in PyMOO (Blank & Deb, 2020). Our code is publicly available at https://github.com/samuelstanton/lambo. Hyperparameters are summarized in Appendix B.4.\"}"}
{"id": "stanton22a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.1. Architecture Details\\nWe used the same base architecture for all experiments, relying on 1D convolutions (masking positions corresponding to padding tokens). We used standard pre-activation residual blocks with two conv layers, layernorm, and swish activations. We used a kernel size of 5, 64 intermediate channels and 16 latent channels. The shared encoder and decoder each were composed of 3 of these residual blocks (for a total of 6 convolutional layers each). The shared encoder embeds input sequences with standard vocabulary and sinusoidal position embeddings. The discriminative encoder was composed of a single residual block. Note that transformer encoder layers could be substituted as a drop-in replacement for these convolutional residual blocks, we used small convolutional layers because they are fast to train and performed adequately in our experiments.\\n\\nB.2. DKL Implementation Details\\nTraining DKL models is an art. Some best practices apply to both stochastic variational and exact GP inference, others are specific to the former.\\n\\nApplicable to exact and variational GP inference\\n1. Kernel hyperparameter priors matter. Allowing the DKL GP to easily change both the inputs to the final conventional GP kernel (e.g. RBF) and the lengthscale of that kernel doesn\u2019t work well. We placed a tight Gaussian prior ($\\\\sigma_0.01$) around the initial lengthscale value and forced the encoder to learn features appropriate for that lengthscale. Note that this is distinct from simply fixing the kernel hyperparameters a priori.\\n2. Optimizer hyperparameters matter. Adam is really convenient to avoid too much learning rate tuning, but it can cause unexpected issues when jointly training supervised and unsupervised heads. We almost completely disabled the running estimates of the first two moments in Adam, using $\\\\beta_1=0.0$, and $\\\\beta_2=0.01$.\\n3. Normalization matters. This is more of an issue for SVGPs than exact GPs, but in both cases batchnorm can cause undesirable and unexpected behavior. Use layernorm.\\n\\nApplicable to variational GP inference\\n1. Initialization matters. We use the procedure described in Maddox et al. (2021c) to reinitialize the inducing point locations and the variational parameters every time the model was retrained. This trick significantly improves results and saves computation, since the GP training does not completely start over every outer loop iteration.\\n2. One final trick that is very useful for SVGPs is to turn off gradients to all GP-related parameters every other epoch (so half the epochs are only train the encoder).\\n\\nAs we show in the main text and Figure 9, DKL SVGPs can consistently be trained to similar levels of accuracy as exact DKL GPs with very little trouble, once the proper training procedures are in place. With these practical insights we were able to jointly train supervised GP heads and unsupervised language model heads on a shared encoder simply by taking one gradient step on the supervised GP loss and one gradient step on the unsupervised DAE loss per minibatch, using the same optimizer and learning rate schedule. We used diagonal Gaussian likelihoods for all our experiments, with the noise variance initialized at 0.25. We found that DKL GPs (both exact and variational) were not immune to overfitting, so we used weight decay (1e-4) and reserved 10% of all collected data (including online queries) as validation data for early stopping.\\n\\nB.3. DAE Implementation Details\\nMLM Head\\nWe used a mask ratio of 0.125 for all experiments when training MLM heads. The MLM loss is computed by randomly masking input tokens, and computing the empirical cross-entropy between the original sequence and the predictive\"}"}
{"id": "stanton22a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. Example molecules at varying points on the new Pareto fronter discovered by LaMBO in Section 5.2 for logP + QED (top) and DRD3 docking + SA (bottom). Under each molecule we show the objective values as $y_1, y_2, y_3$. From left to right $y_1$ worsens and $y_2$ improves. The Pareto frontier is a rich, low-dimensional space that can be analyzed much more easily than the whole search space.\\n\\nRemarkably one can deduce that molecules with large macrocycles are difficult to synthesize even with little knowledge of chemistry by simply comparing molecules 7(b)-1 and 7(b)-2. Our results highlight the multi-objective nature of drug design, the need for careful objective selection (i.e. target discovery), and the human-interpretable insights that can be gained by studying the differences between non-dominated solutions found by machine learning methods.\\n\\n6. Discussion\\n\\nDrug design is quickly emerging as an extremely important application of machine learning. BayesOpt has extraordinary potential for this domain, but existing approaches struggle to extend to high-dimensional, discrete, multi-objective design tasks. We have shown how deep generative models can be integrated with BayesOpt to address these challenges, achieving good sample efficiency and solution quality across a range of design tasks. Moreover, we introduced a new large-molecule task, which provides a challenging and realistic benchmark with which to evaluate new methods for biological sequence design. Finally we successfully optimized red fluorescent proteins in vitro, and showed how characterizing the Pareto frontier can lead to useful, interpretable scientific insights.\\n\\nIn the short term, we are excited to combine LaMBO with large specialized pretrained generative models for antibodies (Ruffolo et al., 2021). The selection of mutation sites in the initialization procedure in Section 4.2 could also be improved beyond uniform random sampling. In the longer term, there are also many promising developments in BayesOpt methodology that have yet to be explored for sequence design, such as non-myopic acquisition functions (Jiang et al., 2020), multi-fidelity acquisition functions to determine the type of experimental assay and number of replications (Kandasamy et al., 2017; Wu et al., 2019), rigorous treatment of design constraints (Eriksson et al., 2019), and the coordination of many parallel drug development campaigns by optimizing risk measures across a whole compound portfolio (Cakmak et al., 2020). BayesOpt with multi-modal inputs is a particularly exciting direction, allowing scientists to combine many different sources of experimental data, including 3D structure and raw instrument output (Jin et al., 2021). BayesOpt itself can also be developed for greater resilience to model misspecification, miscalibration, and covariate shift, all of which are important in drug design tasks. Finally, our work also highlights the need for better benchmarks to evaluate drug design methods.\\n\\nWhile there are still many challenges to overcome, there is a path for Bayesian optimization to revolutionize drug design, profoundly improving our lives, and changing the way we approach scientific discovery.\\n\\nAcknowledgements.\\n\\nWe would like to thank Sait Cakmak, Andres Potapczynski and Sanyam Kapoor for helpful discussions. This research is supported by an Amazon Research Award, Facebook Research, Google Research, NSF I-DISRE 193471, NIH R01DA048764-01A1, NSF IIS-1910266, and NSF 1922658 NRT-HDR.\"}"}
{"id": "stanton22a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders\\n\\nReferences\\n\\nAlvarez, M. A., Rosasco, L., and Lawrence, N. D. Kernels for vector-valued functions: A review. arXiv preprint arXiv:1106.6251, 2011.\\n\\nAngermueller, C., Belanger, D., Gane, A., Mariet, Z., Dohan, D., Murphy, K., Colwell, L., and Sculley, D. Population-based black-box optimization for biological sequence design. In International Conference on Machine Learning, pp. 324\u2013334. PMLR, 2020a.\\n\\nAngermueller, C., Dohan, D., Belanger, D., Deshpande, R., Murphy, K., and Colwell, L. Model-based reinforcement learning for biological sequence design. In International Conference on Learning Representations, 2020b. URL https://openreview.net/forum?id=HklxbgBKvr.\\n\\nBaek, M., DiMaio, F., Anishchenko, I., Dauparas, J., Ovchinnikov, S., Lee, G. R., Wang, J., Cong, Q., Kinch, L. N., Schaeffer, R. D., et al. Accurate prediction of protein structures and interactions using a three-track neural network. Science, 373(6557):871\u2013876, 2021.\\n\\nBalandat, M., Karrer, B., Jiang, D., Daulton, S., Letham, B., Wilson, A. G., and Bakshy, E. BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization. In Advances in Neural Information Processing Systems, volume 33, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/f5b1b89d98b7286673128a5fb112cb9a-Abstract.html.\\n\\nBeck, D. and Cohn, T. Learning kernels over strings using gaussian processes. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 67\u201373, 2017.\\n\\nBickerton, G. R., Paolini, G. V ., Besnard, J., Muresan, S., and Hopkins, A. L. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90\u201398, 2012.\\n\\nBiswas, S., Khimulya, G., Alley, E. C., Esvelt, K. M., and Church, G. M. Low-n protein engineering with data-efficient deep learning. Nature Methods, 18(4):389\u2013396, 2021.\\n\\nBlank, J. and Deb, K. Pymoo: Multi-objective optimization in python. IEEE Access, 8:89497\u201389509, 2020.\\n\\nBonilla, E. V ., Chai, K., and Williams, C. Multi-task gaussian process prediction. Advances in neural information processing systems, 20, 2007.\\n\\nBrochu, E., Cora, V . M., and De Freitas, N. A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010.\\n\\nCakmak, S., Astudillo Marban, R., Frazier, P., and Zhou, E. Bayesian optimization of risk measures. Advances in Neural Information Processing Systems, 33:20130\u201320141, 2020.\\n\\nChithrananda, S., Grand, G., and Ramsundar, B. Chemberta: Large-scale self-supervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020.\\n\\nChudakov, D. M., Matz, M. V ., Lukyanov, S., and Lukyanov, K. A. Fluorescent proteins and their applications in imaging living cells and tissues. Physiological Reviews, 90(3):1103\u20131163, 2010. doi: 10.1152/physrev.00038.2009. URL https://doi.org/10.1152/physrev.00038.2009. PMID: 20664080.\\n\\nCock, P. J., Antao, T., Chang, J. T., Chapman, B. A., Cox, C. J., Dalke, A., Friedberg, I., Hamelryck, T., Kauff, F., Wilczynski, B., et al. Biopython: freely available python tools for computational molecular biology and bioinformatics. Bioinformatics, 25(11):1422\u20131423, 2009.\\n\\nColey, C. W., Eyke, N. S., and Jensen, K. F. Autonomous discovery in the chemical sciences part ii: outlook. Angewandte Chemie International Edition, 59(52):23414\u201323436, 2020.\\n\\nDance, A. The hunt for red fluorescent proteins. Nature, 596:152\u2013153, August 2021. (Online) https://doi.org/10.1038/d41586-021-02093-6.\\n\\nDaulton, S., Balandat, M., and Bakshy, E. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. Advances in Neural Information Processing Systems, 33, 2020.\\n\\nDaulton, S., Balandat, M., and Bakshy, E. Parallel bayesian optimization of multiple noisy objectives with expected hypervolume improvement. arXiv preprint arXiv:2105.08195, 2021a.\\n\\nDaulton, S., Eriksson, D., Balandat, M., and Bakshy, E. Multi-objective bayesian optimization over high-dimensional search spaces. arXiv preprint arXiv:2109.10964, 2021b.\\n\\nDeb, K., Pratap, A., Agarwal, S., and Meyarivan, T. A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE transactions on evolutionary computation, 6(2):182\u2013197, 2002.\\n\\nDeshwal, A. and Doppa, J. Combining latent space and structured kernels for bayesian optimization over combinatorial spaces. Advances in Neural Information Processing Systems, 34, 2021.\"}"}
{"id": "stanton22a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\nEmmerich, M. Single-and multi-objective evolutionary design optimization assisted by gaussian random field meta-models. Dissertation, Universit\u00a8at Dortmund, 2005.\\n\\nEmmerich, M. T., Deutz, A. H., and Klinkenberg, J. W. Hypervolume-based expected improvement: Monotonicity properties and exact computation. In 2011 IEEE Congress of Evolutionary Computation (CEC), pp. 2147\u20132154. IEEE, 2011.\\n\\nEriksson, D., Pearce, M., Gardner, J., Turner, R. D., and Poloczek, M. Scalable global optimization via local bayesian optimization. Advances in Neural Information Processing Systems, 32:5496\u20135507, 2019.\\n\\nErtl, P. and Schuffenhauer, A. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, 1(1):1\u201311, 2009.\\n\\nFrazier, P. I. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.\\n\\nFu, T., Xiao, C., Li, X., Glass, L. M., and Sun, J. Mimos: Multi-constraint molecule sampling for molecule optimization. arXiv preprint arXiv:2010.02318, 2020.\\n\\nGao, W., Mercado, R., and Coley, C. W. Amortized tree generation for bottom-up synthesis planning and synthesizable molecular design. arXiv preprint arXiv:2110.06389, 2021.\\n\\nGardner, J., Pleiss, G., Weinberger, K. Q., Bindel, D., and Wilson, A. G. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. Advances in Neural Information Processing Systems, 31:7576\u20137586, 2018.\\n\\nGligorijevic, V., Berenberg, D., Ra, S., Watkins, A., Kelow, S., Cho, K., and Bonneau, R. Function-guided protein design by deep manifold sampling. bioRxiv, 2021.\\n\\nG\u00b4omez-Bombarelli, R., Wei, J. N., Duvenaud, D., Hern\u00b4andez-Lobato, J. M., S \u00b4anchez-Lengeling, B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., and Aspuru-Guzik, A. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268\u2013276, 2018.\\n\\nGoovaerts, P. et al. Geostatistics for natural resources evaluation. Oxford University Press on Demand, 1997.\\n\\nGrosnit, A., Tutunov, R., Maraval, A. M., Griffiths, R.-R., Cowen-Rivers, A. I., Yang, L., Zhu, L., Lyu, W., Chen, Z., Wang, J., et al. High-dimensional bayesian optimisation with variational autoencoders and deep metric learning. arXiv preprint arXiv:2106.03609, 2021.\\n\\nHensman, J., Fusi, N., and Lawrence, N. D. Gaussian processes for big data. arXiv preprint arXiv:1309.6835, 2013.\\n\\nH\u00f8ie, M. H., Cagiada, M., Frederiksen, A. H. B., Stein, A., and Lindorff-Larsen, K. Predicting and interpreting large scale mutagenesis data using analyses of protein stability and conservation. bioRxiv, 2021. doi: 10.1101/2021.06.26.450037. URL https://www.biorxiv.org/content/early/2021/06/28/2021.06.26.450037.\\n\\nHuang, K., Fu, T., Gao, W., Zhao, Y., Roohani, Y., Leskovec, J., Coley, C. W., Xiao, C., Sun, J., and Zitnik, M. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. arXiv preprint arXiv:2102.09548, 2021.\\n\\nJensen, J. H. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Chemical science, 10(12):3567\u20133572, 2019.\\n\\nJiang, S., Jiang, D., Balandat, M., Karrer, B., Gardner, J., and Garnett, R. Efficient nonmyopic bayesian optimization via one-shot multi-step trees. Advances in Neural Information Processing Systems, 33:18039\u201318049, 2020.\\n\\nJin, W., Barzilay, R., and Jaakkola, T. Junction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pp. 2323\u20132332. PMLR, 2018.\\n\\nJin, W., Wohlwend, J., Barzilay, R., and Jaakkola, T. Iterative refinement graph neural network for antibody sequence-structure co-design. arXiv preprint arXiv:2110.04624, 2021.\\n\\nJones, D. R., Schonlau, M., and Welch, W. J. Efficient global optimization of expensive black-box functions. Journal of Global Optimization, 13(4):455\u2013492, 1998.\\n\\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., \u02c7Z\u00b4\u0131dek, A., Potapenko, A., et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.\\n\\nKandasamy, K., Dasarathy, G., Schneider, J., and P\u00b4oczos, B. Multi-fidelity bayesian optimisation with continuous approximations. In International Conference on Machine Learning, pp. 1799\u20131808. PMLR, 2017.\"}"}
{"id": "stanton22a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders\\n\\nKhan, A., Cowen-Rivers, A. I., Deik, D.-G.-X., Grosnit, A., Dreczkowski, K., Robert, P. A., Greiff, V., Tutunov, R., Bou-Ammar, D., Wang, J., et al. Antbo: Towards real-world automated antibody design with combinatorial bayesian optimisation. arXiv preprint arXiv:2201.12570, 2022.\\n\\nKrenn, M., Hase, F., Nigam, A., Friederich, P., and Aspuru-Guzik, A. Self-referencing embedded strings (selfies): A 100% robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, 2020.\\n\\nLambert, T. J. Fpbase: a community-editable fluorescent protein database. Nature methods, 16(4):277\u2013278, 2019.\\n\\nLazaro-Gredilla, M., Quinonero-Candela, J., Rasmussen, C. E., and Figueiras-Vidal, A. R. Sparse spectrum gaussian process regression. The Journal of Machine Learning Research, 2010.\\n\\nLee, J., Mansimov, E., and Cho, K. Deterministic non-autoregressive neural sequence modeling by iterative refinement. arXiv preprint arXiv:1802.06901, 2018.\\n\\nLetham, B., Karrer, B., Ottoni, G., and Bakshy, E. Constrained bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495\u2013519, 2019.\\n\\nLipinski, C. A., Lombardo, F., Dominy, B. W., and Feeney, P. J. Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings. Advanced drug delivery reviews, 23(1-3):3\u201325, 1997.\\n\\nLodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N., and Watkins, C. Text classification using string kernels. Journal of Machine Learning Research, 2(Feb):419\u2013444, 2002.\\n\\nLyu, J., Wang, S., Balius, T. E., Singh, I., Levit, A., Moroz, Y., O'Meara, M. J., Che, T., Algaa, E., Tolmachova, K., et al. Ultra-large library docking for discovering new chemotypes. Nature, 566(7743):224\u2013229, 2019.\\n\\nMaddox, W., Feng, Q., and Balandat, M. Optimizing high-dimensional physics simulations via composite bayesian optimization. arXiv preprint arXiv:2111.14911, 2021a.\\n\\nMaddox, W. J., Balandat, M., Wilson, A. G., and Bakshy, E. Bayesian optimization with high-dimensional outputs. Advances in Neural Information Processing Systems, 34, 2021b.\\n\\nMaddox, W. J., Stanton, S., and Wilson, A. G. Conditioning sparse variational gaussian processes for online decision-making. Advances in Neural Information Processing Systems, 34, 2021c.\\n\\nMaus, N., Jones, H. T., Moore, J. S., Kusner, M. J., Bradshaw, J., and Gardner, J. R. Local latent space bayesian optimization over structured inputs. arXiv preprint arXiv:2201.11872, 2022.\\n\\nMeier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., and Rives, A. Language models enable zero-shot prediction of the effects of mutations on protein function. bioRxiv, 2021.\\n\\nMishra, A., Ranganathan, S., Jayaram, B., and Sattar, A. Role of solvent accessibility for aggregation-prone patches in protein folding. Scientific Reports, 8(1):12896, 2018. doi: 10.1038/s41598-018-31289-6. URL https://doi.org/10.1038/s41598-018-31289-6.\\n\\nMoss, H. B., Beck, D., Gonzalez, J., Leslie, D. S., and Rayson, P. Boss: Bayesian optimization over string spaces. arXiv preprint arXiv:2010.00979, 2020.\\n\\nNigam, A., Friederich, P., Krenn, M., and Aspuru-Guzik, A. Augmenting genetic algorithms with deep neural networks for exploring the chemical space. arXiv preprint arXiv:1909.11655, 2019.\\n\\nO'Donoghue, B., Osband, I., Munos, R., and Mnih, V. The uncertainty bellman equation and exploration. In International Conference on Machine Learning, pp. 3836\u20133845, 2018.\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32:8026\u20138037, 2019.\\n\\nRao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen, X., Canny, J., Abbeel, P., and Song, Y. S. Evaluating protein transfer learning with tape. Advances in neural information processing systems, 32:9689, 2019.\\n\\nRasmussen, C. E. and Williams, C. K. I. Gaussian processes for machine learning. Adaptive computation and machine learning. MIT Press, Cambridge, Mass., 3. print edition, 2008. ISBN 978-0-262-18253-9.\\n\\nRives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15), 2021.\\n\\nRuffolo, J. A., Gray, J. J., and Sulam, J. Deciphering antibody affinity maturation with language models and weakly supervised learning. arXiv preprint arXiv:2112.07782, 2021.\"}"}
