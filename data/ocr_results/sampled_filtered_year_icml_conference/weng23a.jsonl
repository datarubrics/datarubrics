{"id": "weng23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\nYijia Weng 1\\nKaichun Mo 1 2\\nRuoxi Shi 3\\nYanchao Yang 1 4\\nLeonidas Guibas 1\\n\\nAbstract\\n\\nSome extremely low-dimensional yet crucial geometric eigen-lengths often determine the success of some geometric tasks. For example, the height of an object is important to measure to check if it can fit between the shelves of a cabinet, while the width of a couch is crucial when trying to move it through a doorway. Humans have materialized such crucial geometric eigen-lengths in common sense since they are very useful in serving as succinct yet effective, highly interpretable, and universal object representations. However, it remains obscure and underexplored if learning systems can be equipped with similar capabilities of automatically discovering such key geometric quantities from doing tasks. In this work, we therefore for the first time formulate and propose a novel learning problem on this question and set up a benchmark suite including tasks, data, and evaluation metrics for studying the problem. We focus on a family of common fitting tasks as the testbed for the proposed learning problem. We explore potential solutions and demonstrate the feasibility of learning eigen-lengths from simply observing successful and failed fitting trials. We also attempt geometric grounding for more accurate eigen-length measurement and study the reusability of the learned eigen-lengths across multiple tasks. Our work marks the first exploratory step toward learning crucial geometric eigen-lengths and we hope it can inspire future research in tackling this important yet underexplored problem. Project page: https://yijiaweng.github.io/geo-eigen-length.\\n\\n1. Introduction\\n\\nConsider a robot tasked with placing many small objects on warehouse shelves, where both the objects and the shelves have diverse geometric configurations. While the robot can simply try to accomplish the task by trial and error, to us as humans, it is clear that certain placements should not be attempted because they will obviously fail. For example, we should not attempt to place a tall object on a shelf whose height is too low. We base this judgment on the estimation of a critical geometric eigen-length or measurement, the height of the object and the shelf, whose comparison allows a quick estimate of task feasibility. Such scalar measurements are crucial for downstream tasks. And we call them \u201ceigen\u201d because they are intrinsic properties of the object and usually act as very low-dimensional geometric summaries with respect to many tasks, invariant to the environment it interacts with. For example, to determine whether $M$ objects can be placed on $N$ different shelves, once the height of each object/shelf is extracted, we can compose and compare them arbitrarily without having to exhaustively analyze $N \\\\times M$ pairs of raw geometries.\\n\\nWhile object height is an example of important eigen-lengths that are crucial for the above shelf placement task, it is not hard to think of many other types of object eigen-lengths for other geometric tasks. Figure 1 presents some example tasks together with the presumed geometric eigen-lengths based on human common sense. For example, the geometric eigen-length diameter is important for the task of stacking plates in different sizes (Figure 1, (a)), while the width and length of an object are crucial geometric eigen-lengths for deciding if one can put it into an open box (Figure 1, (c)).\\n\\nHaving such extremely low-dimensional yet crucial geometric eigen-lengths extracted as the representations for objects is certainly beneficial for designing learning systems aimed at artificial general intelligence. One telling evidence is that we humans have naturally built up the vocabulary of geometric key quantities, such as height, width, and diameter, when perceiving and modeling everyday objects, and used them to perform various tasks. Besides being succinct yet effective abstractions of objects for quickly estimating the feasibility for the downstream tasks, such crucial geometric eigen-lengths are also highly interpretable, which exposes the principled reasoning process behind the feasibility checking, and universal, as they are generally applicable to objects with arbitrary shapes and useful across different downstream tasks.\"}"}
{"id": "weng23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\nCurrent research in representation learning for computer vision and robotics has mostly been focusing on learning high-dimensional latent codes or heavily injecting human knowledge as inductive biases for learning structured representations. While learning high-dimensional latent codes provides total flexibility in learning any useful feature for mastering the downstream tasks, these latent codes are hard to interpret and may be prone to overfitting the training domain. For structured representations, though researchers have explored using different kinds of object representations, such as bounding boxes (Tulsiani et al., 2017) and key points (Manuelli et al., 2019), to accomplish various downstream tasks in computer vision and robotics, these structure priors are manually specified based on human knowledge about the tasks. In contrast, we aim to explore the automatic discovery of low-dimensional yet crucial geometric quantities for downstream tasks while injecting the minimal human prior knowledge \u2013 only assuming that we are measuring some 1D lengths of the input objects.\\n\\nIn this paper, we first propose to study a novel learning problem on discovering low-dimensional geometric eigen-lengths crucial for downstream tasks and set up the benchmark suite for studying the problem. Specifically, we target a family of fitting tasks where the goal is to find a placement/trajectory for an object in an environment, subject to geometric and semantic constraints, e.g. no collision. As illustrated in Figure 2, given a fitting task (putting the bowl inside the drawer of the table) that involves an environment geometry (the table) and an object shape (the bowl), we are interested in predicting whether the object can fit in the scene accomplishing the task or not, via discovering a few crucial geometric eigen-lengths and composing them with a task program which outputs the final task feasibility prediction. To study the problem, we also define a set of commonly seen fitting tasks, generate large-scale data for the training and evaluation on each task, and set up a set of quantitative and qualitative metrics for evaluating and analyzing the method performance and if the emergent geometric eigen-lengths match the desired ones humans usually use.\\n\\nWe also explore potential solutions to the proposed learning problem and present several of our key findings. First of all, we will show that learning such low-dimensional key geometric eigen-lengths are achievable from only using weak supervision signals such as the success or failure of training fitting trials. Secondly, the learned crucial geometric eigen-lengths can be more accurately measured if geometric grounding is allowed and attainable for certain fitting tasks. Finally, we make an initial stab at exploring how to share and re-use the learned geometric eigen-lengths across different tasks and even for novel tasks. Marking the first step in defining and approaching this important yet underexplored problem, we hope our work can draw people's attention to this direction and inspire future research.\\n\\nTo summarize, this work makes the following contributions:\\n\\n\u2022 We propose a novel learning problem on discovering low-dimensional geometric eigen-lengths crucial for fitting tasks;\\n\u2022 We set up a benchmark suite for studying the problem, including a set of fitting tasks, the dataset for each task, and a range of quantitative and qualitative metrics for thorough performance evaluation and analysis;\\n\u2022 We explore potential solutions to the proposed learning problem and present some key take-away messages summarizing both the successes and unresolved challenges.\\n\\n2. Related Work\\n\\nLearning Geometry Abstraction. A long line of research has focused on learning low-dimensional and compact abstraction for input geometry. Given as input a 2D or 3D shape, past works have studied learning various geometric abstraction as the shape representation, such as bounding boxes (Tulsiani et al., 2017; Sun et al., 2019), convex shapes (Deng et al., 2020), Gaussian mixtures (Genova et al., 2019; 2020), superquadratics (Paschalidou et al., 2019; 2020), parametric curves (Reddy et al., 2021) and surfaces (Sharma et al., 2020; Smirnov et al., 2020), etc. Most of these works use geometry fitting as the primary objective. Our work, however, focuses on discovering geometric abstraction that can help solve the downstream manipulation tasks instead of reconstruction.\\n\\nThere are also previous works exploring ways to learn task-specific geometry representation for manipulation tasks. For example, researchers have tried to learn key points (Manuelli et al., 2019) and skeleton (Zhang et al., 2020) for manipulating 2D shapes, and 3D skeleton (Echeverria et al., 2019) for manipulating 3D shapes. However, the above methods are not general and only applicable to specific manipulation tasks and shapes. Our work aims to discover geometric abstraction that can be shared across different tasks.\"}"}
{"id": "weng23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\nTask Program\\nInput Geometry\\n\\nFigure 2. Proposed Learning Paradigm where we first predict a set of geometric eigen-lengths from the input geometries, then compose them using a task program to get the final task output.\\n\\net al., 2019; Qin et al., 2020; Wang et al., 2020; Chen et al., 2020; Jakab et al., 2021; Chen et al., 2021) and affordance information (Kim & Sukhatme, 2014; Mo et al., 2021a;b; Turpin et al., 2021; Deng et al., 2021) for robotic manipulation tasks. These works mostly pre-define the types of geometry abstraction and the downstream policies to use the extracted shape summaries, and the abstraction is mostly dense or high dimensional. In this paper, we aim for useful geometric eigen-lengths and ways to automatically discover and compose them for solving manipulation tasks.\\n\\nDisentangled Visual Representation Learning. Another line of work focuses on unsupervised representation learning techniques that pursue disentangled and compositional latent representations for visual concepts. For example, InfoGAN (Chen et al., 2016), beta-V AE (Higgins et al., 2017), and many more works (Higgins et al., 2016; Siddharth et al., 2017; Yang et al., 2020) discover disentangled features, each of which controls a certain aspect of visual attributes, usually with reconstruction as the objective. In contrast to their primary objectives of controllable reconstruction or generation, we explore the problem of learning geometric eigen-lengths driven by the goal of accomplishing downstream fitting tasks. Also, our task involves reasoning over two geometric inputs and comparing the extracted eigen-lengths on both inputs, while these previous works on disentangled visual representation learning factor out visual attributes for a single input datum.\\n\\n3. Learning Problem Formulation\\n\\nGiven a fitting task \\\\( T \\\\in T \\\\), we aim to learn very few but the crucial geometric eigen-lengths \\\\( L_T \\\\) (e.g., width, length, height) of the object shape \\\\( O \\\\in O \\\\) and the environment geometry \\\\( E \\\\in E \\\\) that are useful for checking the feasibility of fitting \\\\( O \\\\) into \\\\( E \\\\) under the task \\\\( T \\\\). Figure 2 presents an example of the proposed learning problem where the task is to put the bowl \\\\( O \\\\) into the drawer of the cabinet \\\\( E \\\\). In this example, the width, length, height of the drawer and the bowl are the crucial desired geometric eigen-lengths \\\\( L_T \\\\) and we can compose them in a task program to output the final task feasibility prediction.\\n\\nWe define each eigen-length \\\\( L \\\\in L_T \\\\) as a function mapping from the input object shape \\\\( O \\\\) or the environment geometry \\\\( E \\\\) to a scalar value, which is the eigen-length measurement, i.e., \\\\( L : O \\\\cup E \\\\rightarrow \\\\mathbb{R} \\\\). After obtaining the eigen-length measurements for both the object and environment inputs, i.e., \\\\( \\\\{ L(O) | L \\\\in L_T \\\\} \\\\) and \\\\( \\\\{ L(E) | L \\\\in L_T \\\\} \\\\), we perform pairwise comparisons between the corresponding eigen-lengths checking if \\\\( L(O) < L(E) \\\\) holds for every \\\\( L \\\\in L_T \\\\). The task of fitting \\\\( O \\\\) in \\\\( E \\\\) is predicted as successful if all the conditions hold and as failed if any condition does not hold. This format of task program is based on the intuition that in fitting tasks, we require the object to be \u201csmaller\u201d than the parts of the environment affording the action. During training, the learning systems see many fitting trials over different object and environment configurations together with their ground-truth fitting feasibility, i.e., \\\\( \\\\{ (O_i, E_i, \\\\text{Successful/Failed}) | i = 0, 1, 2, \\\\ldots \\\\} \\\\). The goal is to learn eigen-length functions based on which correct prediction of task feasibility given test input \\\\( (O_{\\\\text{test}}, E_{\\\\text{test}}) \\\\) can be made.\\n\\n4. Can Geometric Eigen-Lengths be Learned from Binary Task Supervision?\\n\\nIn this work, we are interested in learning geometric eigen-lengths that are crucial for downstream tasks. We hope to achieve automatic discovery of these eigen-lengths from doing tasks as it requires the least human prior and allows maximum flexibility. Therefore, we start with the minimum form of supervision and explore the following question: given only binary task success/failure supervision, is it possible to learn geometric eigen-lengths of input geometries that are sufficient for the task?\\n\\n4.1. Testbed for Eigen-Length Learning\\n\\nWe start by curating a set of tasks as the testbed for the learning problem, as summarized in Fig. 3. For each task, we build a large-scale dataset comprising diverse shapes and configurations.\\n\\nTask Design Principles\\n\\nWe design the tasks to (1) cover a wide range of geometries, including synthetic, simple primitive shapes and more complex ones like ShapeNet objects; (2) facilitate the analysis and interpretation of learned\"}"}
{"id": "weng23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\n(d) Container Fitting\\n(e) Countertop Placing\\n(a) Tube Passing\\n(b) Cylinder Fitting\\n(c) Sphere Fitting\\n(f) Mug Hanging\\n\\nFigure 3. Summary of tasks and their human-hypothesized key measurements/eigen-lengths.\\n\\nEnvNet\\nEnv Point Cloud\\nObjNet\\nObj Point Cloud\\n\\nEnv Eigen-Length: $L_{1\\\\text{env}}, L_{2\\\\text{env}},..., L_{S\\\\text{env}}$\\n\\nObj Eigen-Length: $L_{1\\\\text{obj}}, L_{2\\\\text{obj}},..., L_{S\\\\text{obj}}$\\n\\n\\\\[\\n\\\\sigma \\\\left( \\\\frac{L_{1\\\\text{env}} - L_{1\\\\text{obj}}}{\\\\delta} \\\\right) \\\\quad \\\\sigma \\\\left( \\\\frac{L_{2\\\\text{env}} - L_{2\\\\text{obj}}}{\\\\delta} \\\\right) \\\\quad \\\\sigma \\\\left( \\\\frac{L_{S\\\\text{env}} - L_{S\\\\text{obj}}}{\\\\delta} \\\\right)\\n\\\\]\\n\\n\ud835\udeb7(AND)Pred Label\\n\\nVectorNet\\nEnv Point Cloud\\nWeight Distri.\\nProjection Length\\n\\nVector $v$\\n\\nObj Point Cloud\\nWeightNet Points $p, q$\\nWeighted Sum\\n\\n$L = v^T(q - p)$\\n\\nObj Eigen-Length\\nEnv Eigen-Length\\n\\nFigure 4. Network architectures.\\n\\n(a) A minimal eigen-length learning pipeline where we separately encode environment and object into eigen-length values, perform pair-wise comparison, and take the logical AND of results.\\n\\n(b) A geometry-grounded framework where we first predict vectors and points as the geometry grounding, then compute eigen-lengths from them.\\n\\nTask Specifications\\n\\nIn all tasks, we aim to determine whether a placement/motion of the object exists in a certain environment, specifically:\\n\\n(a) Tube passing. (Tube) Pass an object through a rectangular tube. A tube is a cuboid without the front and back faces. Width and height of the tube/object are the key eigen-lengths.\\n\\n(b) Cylinder fitting. (Cylinder) Place an object into a cylindrical container. Bounding sphere radius of the object in XY plane, its height, the radius and height of the cylinder container are the key eigen-lengths.\\n\\n(c) Sphere fitting. (Sphere) Place an object into a spherical container. Radii of the bounding sphere of the object and the container are the key eigen-lengths.\\n\\n(d) Container fitting. (Fit) Place an object into cavities in a ShapeNet container object. Example cavities are drawers or shelves (See Fig. 3d). Most cavities have cuboid-like shapes. Thus, key eigen-lengths are width, length and height of cavities and objects.\\n\\n(e) Countertop placing. (Top) Place an object on top of another ShapeNet environment object, such that its projection along the gravity axis is fully enclosed by the environment countertop. Width and length of the countertop surface and the object are key eigen-lengths.\\n\\n(f) Mug hanging. (Mug) Hang a mug on a cylinder-shaped mug holder by its handle. Key eigen-lengths are the distance between the handle and mug body and the diameter of the mug holder.\\n\\nData Generation Details\\n\\nFor objects to be fitted in tasks (a)-(e), we use $\\\\sim 1200$ common household object models from 8 training and 4 testing categories in ShapeNet (Chang et al., 2015), following Mo et al. (2021b). We apply random scaling and rotation to the object model, then sample $N = 1024$ points from its surface. In (d),(e), we use furniture and appliances from ShapeNet as the environment, including $\\\\sim 550$ shapes from 7 categories. In (f), we use $\\\\sim 200$ ShapeNet mugs. We randomly sample the parameters of primitive shapes and the scaling factors of ShapeNet shapes, then sample $M = 1024$ points from their surfaces.\\n\\nFor all tasks, we generated 75k training and 20k testing environment-object pairs. Please refer to Appendix A.3 for more data generation details.\\n\\n4.2. A Minimal Network Architecture\\n\\nIntuitively, we can measure the object and the environment separately and see if the object is \\\"smaller\\\" than the environment. Thus we come up with the minimal network architecture shown in Fig. 4(a). We separately map the object and environment geometries into two sets of eigen-lengths, perform pairwise comparisons between them, and compose comparison results using logical AND.\\n\\nMore specifically, we encode object point cloud $O$ and environment point cloud $E$ using two PointNet (Qi et al., 2017) networks, ObjNet and EnvNet. Both networks output...\"}"}
{"id": "weng23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 20. Full correlation plots and respective R\u00b2 values between human-hypothesized measurements and predicted eigen-lengths from rotated Tube Passing and rotated Container Fitting, respectively. Correspondences between predicted eigen-lengths and human-hypothesized ones can be observed.\"}"}
{"id": "weng23a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\nSpecifically, we use the least-square solution that minimizes $|RV_a - V_b|^2$. In practice, we also enumerate all possible matchings between the two sets of vectors and their negatives, e.g. $v_a$ can match with $-v_b$. For Tubes where we only predict two vectors $v_1, v_2$, we let $v_3 = v_1 \\\\times v_2$.\\n\\nC.2. Embodied Visual Navigation\\n\\nData\\n\\nWe collect our training data by randomly placing robots of varying sizes in one AI2THOR scene. We record the egocentric depth observation and the robot point cloud, as well as a label indicating whether the robot can move forward by 0.2m by running simulation. We test with depth images taken in another scene. In total, we have collected 6,989 views for training and validation, and 756 views for testing.\\n\\nNavigation Demo Visualization\\n\\nWe visualize the predicted environment eigen-lengths as a rectangle in front of the robot, which indicates the size of the \\\"hole\\\" or navigable space in the environment. Since learned eigen-lengths can differ from ground truth by a linear transformation, we mapped the raw eigen-length outputs of the network back to the real-world scale for better visualization. The linear mapping coefficients are obtained by fitting a linear model to the predicted robot eigen-lengths and ground truth robot sizes, which are known. Since collisions mostly happen because the environment is too narrow or the robot is too wide, the eigen-length in the horizontal direction is better learned.\\n\\nThe color (green/red) represents the final output (positive/negative) obtained by comparing the eigen-lengths of the robot and the environment. Since collisions mostly happen because the environment is too narrow or the robot is too wide, the eigen-length in the horizontal direction is better learned.\\n\\nD. Extending AND Clauses to Disjunctive Normal Form (DNF)\\n\\nD.1. Formulation\\n\\nWe employ the AND clause formulation for all tasks shown in the main paper. Namely, after learning a library of paired object/environment eigen-lengths $\\\\{(L_{env}s, L_{obj}s)\\\\}$, we compose them by $\\\\hat{T}(E, O) = \\\\bigwedge_{s=1,2,...,S} [L_{env}s(E) > L_{obj}s(O)]$, (selection mask $m$ is omitted for clarity), approximated by $\\\\tilde{T}(E, O) = \\\\bigvee_{s=1,2,...,S} \\\\sigma((L_{env}s(E) - L_{obj}s(O))/\u03c4)$.\\n\\nHere we show we can extend this formulation to the more general Disjunctive Normal Form (DNF), where an OR connects multiple AND clauses. Each AND clause composes eigen-length comparison results of a subset of eigen-lengths. The result of each AND clause is then aggregated by an OR operator. More precisely, $\\\\hat{T}(E, O) = \\\\bigwedge_{U_a \\\\in U} \\\\bigwedge_{s \\\\in U_a} [L_{env}s(E) > L_{obj}s(O)]$. $U = \\\\{U_a\\\\}_a$ specifies the subset $U_a$ of eigen-lengths in each AND clause. We similarly use a differentiable approximation during training: $\\\\tilde{T}(E, O) = 1 - \\\\bigvee_{U_a \\\\in U} (1 - \\\\bigvee_{s \\\\in U_a} \\\\sigma((L_{env}s(E) - L_{obj}s(O))/\u03c4))$.\\n\\nThe introduction of two-level logic and the OR operator helps express more complex reasoning and deal with a wider range of tasks. For example, many realistic tasks have multiple solutions. OR captures the relationship that the task can be executed if any, not necessarily all, of the solutions work.\\n\\nD.2. Task and Implementation Details\\n\\nTo demonstrate our framework's compatibility with this new formulation, we experiment with the Multi-Tube Passing task. This is a variant of task (a) (Tube, or tube passing) in the main paper, where we have two tubes of random sizes placed...\"}"}
{"id": "weng23a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\nNext to each other. As long as the object can be translated and passed through any of these tubes, the task is considered as successful. Similar to tube passing, we randomly sample the extents of the tubes, the shape, scale, and rotation of the object. The center of the two tubes are always at two fixed positions on the y-axis. We set the number of eigen-lengths to learn as $S = 4$ and split them into two disjoint AND groups, namely $U = \\\\{\\\\{1, 2\\\\}, \\\\{3, 4\\\\}\\\\}$. Ideally, the learned eigen-lengths should correspond to the height and width of the tubes. Also, the height and width of the same tube should be in the same AND group.\\n\\nD.3. Result Visualization\\n\\nFig. 21 visualizes the learned eigen-lengths, where green and yellow belong to one group, purple and red belong to another group. We successfully learn eigen-lengths that measure along the height/width directions of the tubes. We also learn them in correct groups, where width and height of the same tube are paired together. Numerically, the network achieves a test accuracy of 99.59%.\\n\\nE. Discussion and Future Work\\n\\nE.1. Definition of Eigen-Lengths and Application Scope of the Explored Framework\\n\\nIn our setting, an eigen-length is whatever scalar measurement (i.e., just a 1D scalar) the network invents to best perform its stated downstream task. While this definition for eigen-dimensions is quite general and could be applicable to any object as long as there exist certain 1D eigen-lengths that are crucial and useful for checking the feasibility of accomplishing a downstream task, we are assuming in our current experiments that having such sets of 1D eigen-lengths are sufficient for the tasks. Therefore, our current setting would not apply to the tasks where having only such low-dimensional eigen-lengths is not sufficient, such as the tasks of geometric contour matching and object collision checking.\\n\\nE.2. Broader Implication of the Studied Approach for AI and Robotics\\n\\nWe believe the general approach we suggest can have very general applicability in AI and robotics, where the solution to downstream tasks suggests the emergence of generally useful geometric concepts such as length, height, width, and radius in unsupervised ways. As we described in the introduction, learning such compact useful geometric eigen-lengths is beneficial in the ways that 1) they are highly interpretable, while most of the current learned representations in neural networks are opaque and learned as black-box hidden features which may be unreliable or untrustworthy, 2) they could be shared and reused across different tasks, enabling fast adaptation to novel test-time tasks, and 3) the proposed learning formulation may discover novel yet crucial geometric eigen-lengths that are even unknown to us humans given the new test-time tasks. Furthermore, there could be more geometric concepts of great interest and importance that future work can explore in this direction. Examples can be 1) symmetry, as a result of trying to complete 3D shapes, 2) regular object arrangements and poses as a tool for efficient search, and 3) tracking, as an essential capability for predicting the outcome of sports games. In other words, we want learning networks to invent the notions so symmetry, regularity, or tracking. If such capabilities could emerge from purely unsupervised learning, we no longer need to rely on black-box-like neural networks and human annotations for this geometric information over 3D objects.\"}"}
{"id": "weng23a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E.3. Determining the Number of Eigen-Lengths to Learn\\n\\nThe number of eigen-lengths to learn, i.e. $S$, is a hyperparameter of our learning framework and has to be set in advance. However, it should be interpreted as the upper bound on the number of eigen-lengths the system can learn, and does not have to be the \\\"groundtruth\\\" number of relevant eigen-lengths. As shown in Sec. 4.3 and Sec. 5.2, when we set $S = 3$ for the countertop fitting task where only two eigen-lengths matter, the extra \\\"slot\\\" either degenerates or coincides with other slots. Such cases can be easily detected and filtered, and the actual number of relevant eigen-lengths can be discovered. Setting a maximum number for an unknown number of targets is also a common practice in problems like object detection (Redmon & Farhadi, 2017). That being said, a more flexible mechanism that allows an arbitrary number of eigen-lengths would be desirable, especially for objects with complex compositional structures like robotic arms or closets with many drawers. We leave this as a future direction.\"}"}
{"id": "weng23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\nA.3. Dataset Details\\n\\nTable 2 and 3 summarizes the statistics of environment/object shapes used in our dataset. Each shape is drawn with probability in inverse proportion to the number of shapes in its category, such that each object category appears with similar frequency in the final dataset.\\n\\nDuring data generation for the tasks where both the environment and the object are ShapeNet objects, we apply random scaling \\\\( s \\\\sim U([0.9, 1.1]) \\\\) to the environment objects, set all joints to closed state and sample \\\\( M = 1024 \\\\) points from the object model. Given an object-environment pair, we randomly sample \\\\( T = 1000 \\\\) candidate positions in the environment point cloud, and check whether placement of the object at each candidate satisfy the task specification using SAPIEN (Xiang et al., 2020) simulation. If all candidates fail, we label the pair as negative, otherwise as positive. Specifically, the candidate positions are sampled from \u201capplicable and possible regions\u201d following Mo et al. (2021b)\u2019s definition. For example, we only consider points with upward facing normals, and for task (e) only consider points with close to highest z coordinates. We generated around 75K training data and 20K testing data for each task.\\n\\nTable 2. Environment Shape Statistics.\\n\\n| Box | Microwave | Refrigerator | Safe | Storage | Furniture | Table | Washing Machine | Total |\\n|-----|-----------|--------------|------|---------|-----------|-------|-----------------|-------|\\n| Train | 21 | 9 | 34 | 21 | 272 | 70 | 13 | 440 |\\n| Test  | 7 | 3 | 9 | 7 | 73 | 25 | 3 | 127 |\\n\\nTable 3. Object Shape Statistics.\\n\\n| Train | Basket | Bottle | Bowl | Box | Can | Pot | Mug | TrashCan | Total |\\n|-------|--------|--------|------|-----|-----|-----|-----|----------|-------|\\n| 77 | 16 | 128 | 17 | 65 | 16 | 134 | 25 | 478 |\\n| Test | Bucket | Dispenser | Jar | Kettle | Total |\\n| 33 | 9 | 528 | 26 | 554 |\\n\\nB. Additional Results\\n\\nB.1. Geometric Grounding Visualization and Failure Case Discussion\\n\\nFig. 16 and 17 show more visualizations of the learned eigen-lengths in the three tasks from the main paper. Our framework is able to learn reasonable eigen-lengths that measure along crucial directions. These eigen-lengths are also grounded by planes that suggest the relevant part of object which supports the task. In experiments with primitive shapes as environments, the learned planes almost overlap with the box/tube faces. In experiments with ShapeNet container objects as environments, especially task (d) (Fit, or container fitting) as shown in Fig. 17, locating the relevant part becomes more challenging. As this usually involves finding cavities in a shape and selecting the largest one. Fig. 17 shows examples of our learned eigen-lengths, most of which make sense, as shown in (a)-(o). We are able to ignore irrelevant parts, e.g. the legs of tables, and find the part of object that affords the \u201ccontainment\u201d task, e.g. the drawer in (b), the closet in (c). When there are many cavities that afford the same task, the network picks the largest one, e.g. in (d) and (k).\\n\\nFailure Cases.\\n\\nWe also observe some failure cases where the learned eigen-lengths are inaccurate. Fig. 17(p)-(t) shows the most representative ones. (p) shows a relatively complex shape, where the network struggles to find the correct width of the drawer. (q) and (r) show cases where the network finds the wrong cavity. According to our task definition, the object can only be placed in the drawer part in (q). Instead, the network finds the part on top of the drawer. In (r), the network finds the second largest cavity instead of the largest one at the bottom. (s) shows an extreme case where the height of the pizza box is much smaller than the other two extents. As objects usually have correlated extents, comparing height suffices most of the time. The network probably lacks the motivation to precisely capture the width and the length of the pizza box, resulting in the underestimation of width and length in (s). Finally, our formulation, i.e. the AND clause of three eigen-length comparisons, can not fully and precisely describe the nature of this task. The washing machine in (t) has a cylinder-shaped cavity, which our network tries to approximate by a cuboid, which is reasonable within the range of its...\"}"}
{"id": "weng23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\nFigure 16. Additional qualitative results. We visualize the learned vectors and planes for (a) Tube Passing and (e) Countertop Placing. We show all eigen-lengths in the front(a)/top(e) view. We also show the underlying instances in task (e) countertop placing for a clearer understanding of the object structure. Note that though some joints are \u201copen\u201d for visualization purpose, all instances in the dataset are at their rest state.\\n\\nB.2. Correlation Analysis Results\\n\\nWe show here the scatter plots and correlation $R^2$ values between all prediction eigen-lengths and all presumable geometric measurements. $R^2$ value, or coefficient of determination, is a metric in $[0, 1]$ reflecting linear correlation between two variables. The closer $R^2$ is to 1, the more linearly correlated the two variables are. Given two set of samples $x_i, y_i, where \\\\(i=1, 2, \\\\ldots, n\\\\), $R^2$ is defined between $y_i$ and the least squares linear regression of $y_i$ on $x_i$, $\\\\tilde{y}(x_i)$:\\n\\n$$R^2 = 1 - \\\\frac{\\\\sum_i (y_i - \\\\tilde{y}(x_i))^2}{\\\\sum_i (y_i - \\\\bar{y})^2},$$\\n\\nwhere $\\\\bar{y} = \\\\frac{1}{n} \\\\sum_i y_i$ is the mean value of $y_i$.\\n\\nResults from Eigen-Length-Implicit are shown in Fig. 18. Results from Eigen-Length-Grounded are shown in Fig. 19. We can clearly see the one-to-one correspondence between predictions and presumable measurements. $R^2$ is close to or greater than 0.9 where the prediction is the match for the measurements, otherwise the value is much smaller. It is more apparent in the Eigen-Length-Grounded variant, where $R^2$ values are close to the theoretical bound 1 when it matches. The models can learn a compact and appropriate set of eigen-lengths from binary task supervision. Also note that the extraneous prediction slot in task (e) (Top, or countertop placing) become degenerate with another prediction slot, as has mentioned before in main text.\"}"}
{"id": "weng23a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 17. Additional qualitative results in Container Fitting. We show eigen-lengths in two views together with the underlying object following Fig. 16 (d). (a)-(o) are successful cases where the learned planes correctly separate out the largest cavity in the object. (p)-(t) show failure cases.\"}"}
{"id": "weng23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.3. Correlation Analysis Results from Rotated Environment Experiments\\n\\nStill, a strong, disentangled correlation between learned eigen-lengths and human-hypothesized ones can be observed.\\n\\nFull correlation plots and respective...\"}"}
{"id": "weng23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\n\\\\[ \\\\vec{v}_1, \\\\vec{v}_2, \\\\ldots, \\\\vec{v}_S \\\\] as part of the geometry grounding.\\n\\nWe feed the input point cloud pair \\\\((R_aP, R_bP)\\\\) to the network, get two sets of unit-length vectors \\\\(V_a, V_b \\\\in \\\\mathbb{R}^{3 \\\\times 3}\\\\), and compute their relative rotation difference. Please see C.1 for more details.\\n\\nThe network trained on Tube Passing/Container Fitting tasks with only binary supervision achieves an average rotation error of 25.39\u00ba on ShapeNet furniture, and 13.63\u00ba on Tubes. Figure 14 visualizes the predicted rotations, which aligns considerably well with ground truth.\\n\\n7.3. Embodied Visual Navigation\\n\\nEigen-lengths are widely useful in geometric tasks that go beyond fitting. Here we demonstrate its application in embodied visual navigation. We consider cylinder-shaped robots (real-life examples are robot vacuums or robot waiters) navigating scenes from AI2-THOR (Kolve et al., 2017). The goal is to devise a navigation policy that avoids obstacles, based on the robot's egocentric visual input, i.e. the single-view point cloud from its forward-facing depth camera.\\n\\nWe employ the minimal framework in Section 4 to learn whether the robot can move forward by \\\\(d = 0\\\\). We use the robot point cloud as the object, its egocentric point cloud observation as the environment, and collect training labels by moving randomly-placed robots in simulation. The network achieves 93.3% accuracy on views from the unseen testing scene, showing good generalization.\\n\\nBased on the learned feasibility, we implement a visual navigation policy and deploy it on a robot vacuum operating in real-time in the testing scene. With a simple policy of moving forward when the network outputs positive and turning clockwise by 20 degrees otherwise, the robot is able to navigate around the room. Figure 15 shows a snapshot of the navigation process. Please refer to the project website for the full demo video and see C.2 for more details.\\n\\n8. Conclusion\\n\\nIn this work, we formulate a novel learning problem of automatically discovering low-dimensional geometric eigen-lengths crucial for fitting tasks. We set up a benchmark suite comprising a curated set of fitting tasks and corresponding datasets, as well as metric and tools for analysis and evaluation. We demonstrate the feasibility of learning meaningful eigen-lengths as sufficient geometry summary only from binary task supervision. We show that proper geometry grounding of the eigen-lengths contributes to their accuracy and interpretability. We also make an initial attempt at learning shared eigen-lengths in multi-task settings and applying them to novel tasks.\\n\\nOur exploration suggests broad opportunities in this new research direction and reveals many challenges. For example, grounding eigen-length predictions on geometries requires reasonable choice of geometric primitives, which relies on inductive bias of the specific tasks considered. It would be a challenging future direction to build a universal framework that accommodates a wide range of tasks by leveraging all kinds of geometric primitives. In many task instances, we may have access to signals beyond binary success or failure, e.g., a possible placement position of the object. How to leverage these task signals in eigen-length learning remains an open problem. As a first-step attempt at defining and exploring the challenging problem of eigen-length learning, we do hope our work can inspire more researchers to work on this important yet underexplored direction.\\n\\nAcknowledgements\\n\\nThis research is supported by a grant from the Stanford Human-Centered AI Center, a grant from the TRI University 2.0 program, and a Vannevar Bush Faculty Fellowship.\"}"}
{"id": "weng23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\nReferences\\n\\nChang, A. X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\\n\\nChen, B., Abbeel, P., and Pathak, D. Unsupervised learning of visual 3d keypoints for control. In International Conference on Machine Learning, pp. 1539\u20131549. PMLR, 2021.\\n\\nChen, N., Liu, L., Cui, Z., Chen, R., Ceylan, D., Tu, C., and Wang, W. Unsupervised learning of intrinsic structural representation points. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9121\u20139130, 2020.\\n\\nChen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 2180\u20132188, 2016.\\n\\nDeng, B., Genova, K., Yazdani, S., Bouaziz, S., Hinton, G., and Tagliasacchi, A. Cvxnet: Learnable convex decomposition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 31\u201344, 2020.\\n\\nDeng, S., Xu, X., Wu, C., Chen, K., and Jia, K. 3d affordancenet: A benchmark for visual object affordance understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1778\u20131787, June 2021.\\n\\nGenova, K., Cole, F., Vlasic, D., Sarna, A., Freeman, W. T., and Funkhouser, T. Learning shape templates with structured implicit functions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7154\u20137164, 2019.\\n\\nGenova, K., Cole, F., Sud, A., Sarna, A., and Funkhouser, T. Local deep implicit functions for 3d shape. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4857\u20134866, 2020.\\n\\nHiggins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., Mohamed, S., and Lerchner, A. Early visual concept learning with unsupervised deep learning. arXiv preprint arXiv:1606.05579, 2016.\\n\\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017.\\n\\nJakab, T., Tucker, R., Makadia, A., Wu, J., Snavely, N., and Kanazawa, A. Keypointdeformer: Unsupervised 3d keypoint discovery for shape control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12783\u201312792, 2021.\\n\\nKim, D. I. and Sukhatme, G. S. Semantic labeling of 3d point clouds with object affordance for robot manipulation. In 2014 IEEE International Conference on Robotics and Automation (ICRA), pp. 5578\u20135584. IEEE, 2014.\\n\\nKolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., Gordon, D., Zhu, Y., Gupta, A., and Farhadi, A. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv, 2017.\\n\\nManuelli, L., Gao, W., Florence, P., and Tedrake, R. kpam: Keypoint affordances for category-level robotic manipulation. arXiv preprint arXiv:1903.06684, 2019.\\n\\nMo, K., Guibas, L. J., Mukadam, M., Gupta, A., and Tulsiani, S. Where2act: From pixels to actions for articulated 3d objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6813\u20136823, 2021a.\\n\\nMo, K., Qin, Y., Xiang, F., Su, H., and Guibas, L. O2O-Afford: Annotation-free large-scale object-object affordance learning. In Conference on Robot Learning (CoRL), 2021b.\\n\\nPaschalidou, D., Ulusoy, A. O., and Geiger, A. Superquadrics revisited: Learning 3d shape parsing beyond cuboids. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\nPaschalidou, D., van Gool, L., and Geiger, A. Learning unsupervised hierarchical part decomposition of 3d objects from a single rgb image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2020.\\n\\nQi, C. R., Su, H., Mo, K., and Guibas, L. J. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 652\u2013660, 2017.\\n\\nQin, Z., Fang, K., Zhu, Y., Fei-Fei, L., and Savarese, S. Keto: Learning keypoint representations for tool manipulation. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 7278\u20137285. IEEE, 2020.\\n\\nReddy, P., Gharbi, M., Lukac, M., and Mitra, N. J. Im2vec: Synthesizing vector graphics without vector supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7342\u20137351, 2021.\"}"}
{"id": "weng23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\nRedmon, J. and Farhadi, A. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7263\u20137271, 2017.\\n\\nSharma, G., Liu, D., Maji, S., Kalogerakis, E., Chaudhuri, S., and M\u02c7ech, R. Parsenet: A parametric surface fitting network for 3d point clouds. In European Conference on Computer Vision, pp. 261\u2013276. Springer, 2020.\\n\\nSiddharth, N., Paige, B., Van de Meent, J.-W., Desmaison, A., Goodman, N. D., Kohli, P., Wood, F., and Torr, P. H. Learning disentangled representations with semi-supervised deep generative models. arXiv preprint arXiv:1706.00400, 2017.\\n\\nSmirnov, D., Fisher, M., Kim, V . G., Zhang, R., and Solomon, J. Deep parametric shape predictions using distance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 561\u2013570, 2020.\\n\\nSun, C.-Y ., Zou, Q.-F., Tong, X., and Liu, Y . Learning adaptive hierarchical cuboid abstractions of 3d shape collections. ACM Transactions on Graphics (TOG), 38(6):1\u201313, 2019.\\n\\nTulsiani, S., Su, H., Guibas, L. J., Efros, A. A., and Malik, J. Learning shape abstractions by assembling volumetric primitives. In Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\nTurpin, D., Wang, L., Tsogkas, S., Dickinson, S., and Garg, A. GIFT: Generalizable Interaction-aware Functional Tool Affordances without Labels. In Proceedings of Robotics: Science and Systems, Virtual, July 2021. doi: 10.15607/RSS.2021.XVII.060.\\n\\nWang, J., Lin, S., Hu, C., Zhu, Y ., and Zhu, L. Learning semantic keypoint representations for door opening manipulation. IEEE Robotics and Automation Letters, 5(4):6980\u20136987, 2020.\\n\\nXiang, F., Qin, Y ., Mo, K., Xia, Y ., Zhu, H., Liu, F., Liu, M., Jiang, H., Yuan, Y ., Wang, H., Yi, L., Chang, A. X., Guibas, L. J., and Su, H. SAPIEN: A simulated part-based interactive environment. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\\n\\nYang, Y ., Chen, Y ., and Soatto, S. Learning to manipulate individual objects in an image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6558\u20136567, 2020.\"}"}
{"id": "weng23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\nA. Implementation Details\\n\\nBelow we provide network and dataset details. We will also release our code and data to facilitate future research.\\n\\nA.1. Network Architecture\\n\\nThe framework in Section 4 consists of a PointNet and an MLP output head that maps the PointNet global feature to scalar values. The architecture is outlined below, where the numbers in the parenthesis refer to the number of channels in each layer. We use batch normalization and LeakyReLU after all FC layers, except for the output layer.\\n\\nPointNet\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{Per-Point MLP} & : (3 \\\\rightarrow 64 \\\\rightarrow 128 \\\\rightarrow 1024) \\\\\\\\\\n\\\\downarrow & \\\\\\\\\\n\\\\text{Max Pooling} & \\\\\\\\\\n\\\\downarrow & \\\\\\\\\\n\\\\text{MLP} & : (1024 \\\\rightarrow 256 \\\\rightarrow S) \\\\\\\\\\n\\\\end{align*}\\n\\\\]\\n\\nOutput: \\\\(S\\\\) scalars.\\n\\nThe framework in Section 5 consists of VectorNet and WeightNet. VectorNet consists of a PointNet classification backbone and an MLP output head, as outlined below.\\n\\nPointNet\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{Per-Point MLP} & : (3 \\\\rightarrow 64 \\\\rightarrow 128 \\\\rightarrow 1024) \\\\\\\\\\n\\\\downarrow & \\\\\\\\\\n\\\\text{Max Pooling} & \\\\\\\\\\n\\\\downarrow & \\\\\\\\\\n\\\\text{MLP} & : (1024 \\\\rightarrow 256 \\\\rightarrow 3) \\\\\\\\\\n\\\\end{align*}\\n\\\\]\\n\\nOutput: \\\\(S\\\\) vectors.\\n\\nWeightNet consists of a PointNet segmentation backbone and \\\\(2S\\\\) parallel MLP output heads, each outputs a weight distribution over all points, as outlined below.\\n\\nPointNet\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{Per-Point MLP} & : (3 \\\\rightarrow 64[\\\\text{per-point feature}] \\\\rightarrow 128 \\\\rightarrow 1024) \\\\\\\\\\n\\\\downarrow & \\\\\\\\\\n\\\\text{Max Pooling} & : [\\\\text{global feature}] \\\\\\\\\\n\\\\downarrow & \\\\\\\\\\n\\\\text{Concat} & : (\\\\text{per-point feature}, \\\\text{global feature}) \\\\\\\\\\n\\\\downarrow & \\\\\\\\\\n\\\\text{MLP} & : ((1024 + 64) \\\\rightarrow 512 \\\\rightarrow 256 \\\\rightarrow 128) \\\\\\\\\\n\\\\downarrow & \\\\\\\\\\n\\\\text{Output Weight MLP} & : (128 \\\\rightarrow 256 \\\\rightarrow 1), \\\\quad i = 1, 2, \\\\ldots, 2S \\\\\\\\\\n\\\\downarrow & \\\\\\\\\\n\\\\text{SoftMax} & \\\\\\\\\\n\\\\end{align*}\\n\\\\]\\n\\nOutput: \\\\(2S\\\\) sets of per-point weights.\\n\\nWe use LeakyReLU and batch normalization after each FC layer except for the output layers.\\n\\nA.2. Training Details\\n\\nAll networks are implemented using PyTorch and optimized by the Adam optimizer, with a learning rate starting at \\\\(10^{-3}\\\\) and decay by half every 10 epochs. Each batch contains 32 data points; each epoch contains around 1600 batches. We train models for \\\\(\\\\sim 100\\\\) epochs on all tasks. The learnable parameter \\\\(\\\\tau\\\\) is initialized with \\\\(\\\\tau = 1\\\\). All experiments are run on a single NVIDIA TITAN X GPU.\"}"}
{"id": "weng23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We analyze the eigen-lengths learned by the network by Top, since we predict have more slots for eigen-lengths than needed, \u201cground truth\u201d of relevant eigen-lengths. As shown in learning. However, it does not have to be the exact number of eigen-lengths to learn is a hyperparameter set before. The number Knowing the number of eigen-lengths beforehand is not a requirement for successful learning. lengths, suggesting good disentanglement is learned.\\n\\nCorrelation Analysis.\\n\\nAs Fig. 5 shows, \\\\( R_{\\\\text{corr}} \\\\) of matched pairs in Fig. 5. Note that in groundtruths by maximizing the sum of corresponding \\\\( R_{\\\\text{corr}} \\\\) perform least squares linear regression over them to get \\\\( S_{\\\\text{obj}} \\\\) \\\\( S_{\\\\text{env}} \\\\) in Fig. 3. For each task, we randomly sample \\\\( N \\\\) data points and obtain the corresponding \\\\( S_{\\\\text{obj}} \\\\) \\\\( S_{\\\\text{env}} \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tau \\\\) \\\\( \\\\hat{S} \\\\) \\\\( L \\\\) \\\\( S \\\\) \\\\( \\\\tilde{S} \\\\) \\\\( \\\\hat{S} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S}_{\\\\text{env}} \\\\) \\\\( \\\\sigma = 1 \\\\) \\\\( \\\\tilde{S}_{\\\\text{env}} \\\\) \\\\( \\\\hat{S"}
{"id": "weng23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. WeightNet with normal \\\\( \\\\vec{v} \\\\) (its main axes) and compare it to the measurement of the cloud. The point coordinates of \\\\( S \\\\) pairs of probability distributions to extract per-point features, then use an MLP to predict \\\\( S \\\\) feature of the environment point cloud.\\n\\nVectorNet Figure 4(b) illustrates our network. In\\n\\n- For the environment, we use a pair of parallel planes. In practice, we adopt the (point, normal) plane representation and predict a point pair \\\\((\\\\vec{v}, p)\\\\) on a tuple of one unit vector and two planes. In Fig. 8: we measure both the object and the object. Inspired by this, we ground a pair of eigen-lengths on the environment along important directions. For the environment (the drawer part) along important directions.\\n\\nWe perform the same correlation analysis and visualize the success label of the task, say fitting an object into a nightstand, fit and\\n\\n- Improved correlation after using geometry groundings. We show scatter plots of predicted eigen-length (Y coord.) and their matching \\\"ground truth\\\" (X coord.) in Appendix B.3 for a detailed discussion.\\n\\nTowards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\n1. We plot the learned vectors (as arrows) and planes (as disks) on top of input environment point clouds. We also show the object model next to point clouds for a clearer view of object structure. For\\n\\n2. Vector geometry grounding makes learned eigen-lengths more likely to be capturing the same \\\"ground truth\\\" as another learned eigen-lengths in Fig. 6. Compared to Fig. 5, learned eigen-lengths are now almost equal to \\\"groundtruth\\\" thanks to the anchoring effect of the geometry grounding. The extra number of eigen-lengths we set in advance can be different from the actual number of key eigen-lengths. Please see Appendix E.3 for a detailed discussion.\"}"}
{"id": "weng23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\nEnv Measure:\\n\\n\\\\[ L_{env}^{1}, L_{env}^{2}, \\\\ldots, L_{env}^{S} \\\\]\\n\\nObj Measure:\\n\\n\\\\[ L_{obj}^{1}, L_{obj}^{2}, \\\\ldots, L_{obj}^{S} \\\\]\\n\\n\\\\[ \\\\sigma \\\\left( \\\\frac{L_{env}^{1} - L_{obj}^{1}}{\\\\tau} \\\\right) \\\\]\\n\\\\[ \\\\sigma \\\\left( \\\\frac{L_{env}^{2} - L_{obj}^{2}}{\\\\tau} \\\\right) \\\\]\\n\\\\[ \\\\sigma \\\\left( \\\\frac{L_{env}^{S} - L_{obj}^{S}}{\\\\tau} \\\\right) \\\\]\\n\\n\\\\( \\\\mathcal{V}(\\\\text{AND}) \\\\)\\n\\nTest Task\\n\\nTrain Task 1\\n\\nTrain Task 2\\n\\nTrain Task 3\\n\\nFigure 9. (a) Multi-Task Setting where each train task uses boxes with certain faces missing as the environment geometry, and the test task uses a complete box; and (b) Learning Framework, where we use trainable masks to select eigen-length comparison results. We also visualize the learned geometry groundings in Fig. 7. The learned vectors align with the main axes of object geometry. The learned planes overlap with tube surfaces in (a) Tube, surround the edge of countertops in (e) Top, and separate out the region of interest in (d) Fit, e.g. the higher one out of two storage spaces. These meaningful geometric entities provide a clear interpretation of learned eigen-lengths, e.g. in (e) Top's case, red and green predictions coincide with each other and both capture the back-to-front length of the countertop.\\n\\n5.3. A Study on the Data Efficiency of Geometry-Grounded Eigen-Lengths\\n\\nGeometry grounding of eigen-lengths can be seen as a form of regularization. We are therefore curious how the introduction of geometry groundings may influence the model's data efficiency. Fig. 10 shows the trend of test performances as we change the size of training data. We compare our geometry-grounded version to Direct, a no-eigen-length approach, where an MLP directly predicts the final label from the concatenation of object and environment latent features. We also plot the difference between \\\"ground truth\\\" eigen-length measurement directions (local up and right) and predicted vectors as a way to quantify eigen-length quality. Results suggest that the geometry-grounded version is more data efficient if meaningful geometry groundings emerge. When the training data is limited (<3000 samples), however, the predicted directions of groundings are far from ground truth measurement directions, suggesting that the model fails to learn meaningful groundings for eigen-lengths, and thus the final accuracy is lower than Direct.\\n\\nFigure 10. Trend of Left: test accuracy and Right: average angle between learned vector groundings and \\\"ground truth\\\" directions w.r.t. # training samples.\\n\\n6. Can Eigen-Lengths be Learned in Multi-Task Settings and Applied to New Tasks?\\n\\nAs humans, we are able to develop a library of useful measurements/eigen-lengths like height from past experience. Given a new task, instead of trying cluelessly, we would start with known measurements and investigate their role in the task. In this section, we ask if learned eigen-lengths can work in a similar way, i.e. given a set of training tasks, is it possible to learn a shared set of eigen-lengths from them? Further, given a novel task, can we learn to select a subset of learned eigen-lengths that are sufficient for it? In other words, can agents accumulate and transfer knowledge in the form of eigen-lengths?\\n\\n6.1. Multi-Task Testbed\\n\\nWe design a set of tasks that share key eigen-lengths as the testbed for multi-task learning. As shown in Fig. 9(a), we consider box-fitting tasks where the box only has a subset of six faces. Each mode of face existence corresponds to a different task with different geometric constraints. For example, to be able to fit, an object has to be narrower than the box in Task 2 and shorter than the box in Task 3. We set aside the box with all six faces present as the test task. We expect to learn width, height, and length from the training task set, and learn to use all of them during testing. By boxes with partial faces, we aim to mimic different types of cavities in the furniture, e.g., a closed drawer as a box with all faces, an open space on the shelf as a box without the front face, etc.\\n\\n6.2. Multi-Task Learning Framework\\n\\nFig. 9(b) shows the multi-task learning framework we experiment with. From a high level, we learn a set of eigen-lengths and allow each task to select relevant ones from them. This selection step is implemented as a learnable binary mask \\\\( \\\\{ m_k \\\\}_{s=1}^{S} \\\\) over eigen-lengths for each task \\\\( T_k \\\\). We simply insert the mask in the AND-composition and compute the outcome for \\\\( T_k \\\\) as\\n\\n\\\\[ Q_{s=1}^{S} m_k s \\\\cdot \\\\sigma \\\\left( \\\\frac{L_{env}^{s} (E) - L_{obj}^{s} (O)}{\\\\tau} \\\\right) \\\\]\\n\\nDuring training, we optimize both the eigen-length prediction networks and a continuous version of per-task masks \\\\( \\\\tilde{m}_k \\\\in [0, 1] \\\\). At test time, we freeze network weights and only learn a mask to choose from eigen-lengths learned during training.\"}"}
{"id": "weng23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks\\n\\nFigure 11. Correlation scatter plots of predicted eigen-length and their matching \u201cground truth\u201d in rotated Tube Passing and rotated Container Fitting. Please refer to appendix B.3 for the full plot.\\n\\n6.3. Multi-Task Learning and Few-Shot Test Task Adaptation\\n\\nWe experiment with both implicit and geometry-grounded eigen-length prediction networks. To analyze the learned eigen-lengths and per-task masks, we visualize learned geometry groundings that are selected ($m > 0.5$) in each task in Fig. 12. Meaningful groundings are learned and correctly selected for each task, including the test task.\\n\\nTo explore whether eigen-lengths learned during training help quicker adaptation to new tasks, we compare the test task performance of Implicit (Section 4), Grounded (Section 5) to Direct trained from scratch on the test task. Here Direct directly predicts the final label from object and scene latent codes. All methods are limited to 10 batches of test task samples. As shown in Table 1, within one epoch of finetuning, methods based on the reuse of learned eigen-lengths already achieve high performance, surpassing Direct trained from scratch by a large margin, even when the latter has been trained for 100 epochs.\\n\\nTable 1. Multi-Task learning, novel task adaptation results. We finetune eigen-length-based methods on novel task for 1 epoch and compare them to the direct method trained from scratch for 1 and 100 epochs.\\n\\n| Method     | Epoch 1  | 100   | 1  | 100 |\\n|------------|----------|-------|----|-----|\\n| Single Task Eigen-Length | 73.14   | 88.47 | 97.71 | 99.48 |\\n| Direct     | 88.47    | 99.48 | 99.48 | 99.48 |\\n| Implicit   | 88.47    | 99.48 | 99.48 | 99.48 |\\n| Grounded   | 97.71    | 99.48 | 99.48 | 99.48 |\\n\\n7. Extension to More Challenging Tasks\\n\\nSo far, we have focused on single-step fitting tasks in controlled settings for better understanding and analysis of the learned eigen-lengths. In this section, we extend our method to more challenging settings and tasks to demonstrate its potential in a wider application scope.\\n\\n7.1. Applying Random Rotations to Input Environments\\n\\nIn previous experiments, we take environment geometry directly from ShapeNet (Chang et al., 2015) where shapes are roughly axis-aligned. We now consider a more challenging setting where we randomly rotate the environment geometry in Container Fitting and Tube Passing tasks. We apply the geometry-based method described in Section 5. Fig. 11 shows the correlation scatter plots of the learned eigen-lengths with the closest \u201cground-truth\u201d measurements. Despite the increased difficulty, strong correlations can still be observed. Fig. 13 visualizes the learned geometry groundings, where the predicted planes roughly align with the main cavities of the objects.\\n\\n7.2. Relative Rotation Estimation\\n\\nThe network learned in 7.1 not only discovers geometric eigen-lengths but also vectors as their geometry groundings. These vectors are intrinsic properties of the geometry and can serve as a strong indicator of the object\u2019s rotation. In this section, we show how they can be leveraged to estimate object relative rotation. Specifically, given two randomly rotated versions ($R_a P, R_b P$) of point cloud $P$, estimate the relative rotation $R_b R_a^{-1}$ between them.\\n\\nWe directly use the network trained in 7.1, which outputs\"}"}
