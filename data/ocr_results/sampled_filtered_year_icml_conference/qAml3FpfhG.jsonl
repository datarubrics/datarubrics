{"id": "qAml3FpfhG", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Score Estimation Error (no Distribution Shift) | Open LLM Leaderboard | HELM | AlpacaEval |\\n|----------------------------------------------|----------------------|------|------------|\\n| 25                                           | 50                   | 75   | 100        |\\n| 0.00                                          | 0.02                 | 0.04 | 0.06       |\\n| 0.04                                          | 0.06                 | 0.08 | 0.10       |\\n\\nFigure 16. Results of adaptive testing for different benchmarks.\\n\\nF. Individual performances per scenario\\n\\nIn this section, we explore what is behind Figure 3 by looking in detail at results for individual scenarios for the Open LLM Leaderboard and HELM. It is clear from the following plots that there are scenarios in which our methods shine more than others.\\n\\nF.1. Open LLM Leaderboard\\n\\nFigure 17. ARC\"}"}
{"id": "qAml3FpfhG", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nFigure 18. GSM8K\\n\\nFigure 19. TruthfulQA\\n\\nFigure 20. HellaSwag\"}"}
{"id": "qAml3FpfhG", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nFigure 21.\\n\\nFigure 22.\\n\\nWinogrande 20\"}"}
{"id": "qAml3FpfhG", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nF.2. HELM\\n\\nFigure 23.\\n\\nOpenbookQA\\n\\nFigure 24.\\n\\nGSM 21\"}"}
{"id": "qAml3FpfhG", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nFigure 25. LegalBench\\n\\nFigure 26. Math\\n\\nFigure 27. MedQA\"}"}
{"id": "qAml3FpfhG", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nFigure 28. MMLU\\n\\nFigure 29. NarrativeQA\\n\\nFigure 30. NaturalQA (closed book)\"}"}
{"id": "qAml3FpfhG", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nFigure 31. NaturalQA (open book)\\n\\nFigure 32. WMT14\"}"}
{"id": "qAml3FpfhG", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.\"}"}
{"id": "qAml3FpfhG", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nA. Evaluation when subscenarios have different number of samples\\n\\nSuppose we want to estimate the performance of a scenario \\\\( j \\\\) which is composed of \\\\( s_j \\\\) subscenarios. Denote the set of examples in each subscenario of \\\\( j \\\\) as \\\\( I_{jk} \\\\), for \\\\( k \\\\in \\\\{1, \\\\ldots, s_j\\\\} \\\\). Then, \\\\( I_j = \\\\bigcup_k I_{jk} \\\\), with disjoint \\\\( I_{jk} \\\\)'s. For a given LLM \\\\( l \\\\), our main goal is then to estimate \\\\( \\\\frac{1}{s_j} \\\\sum_{k=1}^{s_j} |I_{jk}| \\\\sum_{i \\\\in I_{jk}} Y_{il} \\\\). See that we can write \\\\( \\\\frac{1}{s_j} \\\\sum_{k=1}^{s_j} |I_{jk}| \\\\sum_{i \\\\in I_{jk}} Y_{il} = \\\\sum_{i \\\\in I_j} \\\\bar{\\\\omega}_i Y_{il} \\\\).\\n\\nThis tells us that we can represent the performance of model \\\\( l \\\\) as a weighted average instead of a simple average. In our code, \\\\( \\\\bar{\\\\omega}_i \\\\)'s are called balance weights and \\\\( \\\\bar{\\\\omega}_i \\\\)'s are called normalized balance weights. In Section 3, when computing the estimates using the stratified random sampling strategy, the weights for each example are still given by \\\\( 1/|\\\\hat{I}_j| \\\\) (because subscenarios should already be equally represented) but when using the clustering ideas, the weight for each anchor point is given by the sum of \\\\( \\\\bar{\\\\omega}_i \\\\)'s of all items in its cluster. We do not apply any weighting when fitting the IRT models but only when computing the p-IRT (and gp-IRT) estimate: \\\\( \\\\hat{Z}_{p-IRT}^{jl} = \\\\hat{\\\\lambda}_{|b I_j|} \\\\sum_{i \\\\in b I_j} \\\\omega_i Y_{il} + 1 - \\\\hat{\\\\lambda}_{|I_j \\\\setminus b I_j|} \\\\sum_{i \\\\in I_j \\\\setminus b I_j} \\\\omega_i \\\\hat{p}_{il} \\\\).\\n\\nB. tinyMMLU\\n\\nTo construct tinyMMLU we chose 100 examples and weights identified by the IRT anchor point approach (\u201cIRT\u201d) corresponding to the best test performance (across random seeds) in the experiment presented in the top part of Figure 3 on MMLU. For comparison, we analogously selected 100 examples with the correctness anchor point method.\\n\\nTo better understand the composition of tinyMMLU, in Figure 9 we visualize the distribution of the weights of the selected examples and compare it to the weights of the correctness anchors. Recall that weights are non-negative and sum to 1. If an item has a weight \\\\( 0.1 \\\\), for example, that item has a contribution of 10% in the final estimated score. From Figure 9, we can see that tinyMMLU has more uniform weights compared to its correctness-based counterpart. We measure uniformity through the effective sample size (ESS) of the example weights. ESS, traditionally used in the Monte Carlo and domain adaptation (Elvira et al., 2022; Maia Polo & Vicente, 2023) literature, measures weight inequality in a way such that ESS = 0.50, for example, informally means that the corresponding weighted average is influenced by only 50% of (uniformly weighted) examples. In the context of our problem, more uniform weights of tinyMMLU contribute to its robustness when evaluating LLMs with varying correctness patterns, such as specialized LLMs in Figure 5.\\n\\nWe also investigate the total weight of the tinyMMLU examples within each of the 57 subjects in Figure 10. The highest weighted are \u201chigh school psychology\u201d, \u201celementary mathematics\u201d, and \u201cprofessional law\u201d. Interestingly the weight of the subjects is fairly different from its correctness-based counterpart.\"}"}
{"id": "qAml3FpfhG", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C. Proof of Proposition 4.1\\n\\nProof of proposition 4.1. See that\\n\\n\\\\[ |b E[Z_{jl} | Y_i^0, \\\\ldots, Y_i^k] - E[Z_{jl} | Y_i^0, \\\\ldots, Y_i^k] | \\\\leq 1 - \\\\hat{\\\\lambda} |I_j \\\\setminus bI_j| P_i \\\\in I_j \\\\setminus bI_j |\\\\sigma(\\\\hat{\\\\theta}^\\\\top l \\\\alpha_i - \\\\beta_i) - \\\\sigma(\\\\theta^\\\\top l \\\\alpha_i - \\\\beta_i) | \\\\leq 1 |I_j \\\\setminus bI_j| P_i \\\\in I_j \\\\setminus bI_j \\\\|\\\\alpha_i\\\\|_2 |\\\\hat{\\\\theta}^l - \\\\theta^l|_2 \\\\rightarrow 0 \\\\text{ in probability as} |bI| \\\\rightarrow \\\\infty. \\\\]\\n\\nThe second step uses the fact that \\\\(\\\\sigma\\\\) is 1/4-Lipschitz and the third step applies Cauchy-Schwarz inequality.\\n\\nD. More details about benchmarks\\n\\n\u2022 HuggingFace\u2019s Open LLM Leaderboard (Beeching et al., 2023): the data from this benchmark is composed of 395 LLMs and approx. 29k items that were downloaded from the platform in January/2024. To extract data from those models, we filter all models from the platform that have an MMLU score over 5.3, order them according to their average performance, and equally spaced selected models. Then, we kept all models that had scores for all six scenarios: ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2020), TruthfulQA (Lin et al., 2021), Winogrande (Sakaguchi et al., 2021), and GSM8K (Cobbe et al., 2021). In a second round of data collection, we collected data for 40 \u201cspecialized models\u201d by recognizing which models were fine-tuned to do the math, coding, etc. The two sets of models have an intersection, and in total, we have collected data from 428 LLMs.\\n\\n\u2022 HELM (Liang et al., 2022): we use HELM Lite (https://crfm.stanford.edu/helm/lite) v1.0.0, which is a dataset composed of 37 LLMs and approx. 10k evaluation examples from 10 scenarios. The scenarios are OpenbookQA (Mihaylov et al., 2018), MMLU (Hendrycks et al., 2020), NarrativeQA (Ko\u02c7cisk`y et al., 2018), NaturalQuestions (closed-book) (Kwiatkowski et al., 2019), NaturalQuestions (open-book), Math (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), LegalBench (Guha et al., 2024), MedQA (Jin et al., 2021), WMT14 (Bojar et al., 2014).\\n\\n5 On the leaderboard. The actual score we use can be different because we use the last submission to the leaderboard, while the leaderboard shows the best results among all submissions.\"}"}
{"id": "qAml3FpfhG", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"E. Extra results\\n\\nE.1. Robustness in predicting performance in a longer time horizon\\n\\nWe conduct extra ablation studies placing 75% of the data in the test set. For the Open LLM Leaderboard and MMLU, it means we are using 3 months of future data as the test set (vs. approx. 3 weeks in the main text) while for AlpacaEval 2.0 that would correspond to 6 months (vs. approx. 2 months in the main text). In general, we show that our main method \\\"IRT++\\\" is pretty robust to the advancements in the field when predicting the performance of new LLMs. We report in the following plots the average estimation error in the test set (using 75% of the most recent data in the test set) and standard deviation across LLMs. The results do not differ considerably from the ones in the main text.\\n\\nFigure 11.\\n\\nOur methods are robust in predicting performance in a longer time horizon\\n\\nE.2. How costly is it for stratified random sampling beat IRT++ with larger samples?\\n\\nWe present results comparing IRT++ and stratified random sampling for a larger number of evaluation examples \\\\( n \\\\). On Open LLM Leaderboard 400 examples per task (2400 total) are enough to match IRT++ with 100 examples per task (600 total). On MMLU, random sampling improves quite slowly and would require >400 examples to match IRT++ at 100. On AlpacaEval, random with 200 examples matches IRT++ with 100 examples (note that AlpacaEval is a small benchmark with 805 examples total, but evaluation requires GPT-4 and is thus quite expensive). We use the random split for the LLMs, implying no distribution shift between train and test.\\n\\nFigure 12.\\n\\nBenchmark results for different methods and sample sizes\\n\\nE.3. Running time\\n\\nWe record the running time of IRT inference (ability parameter fitting) when running our experiments. In Figure 13 we show that the average running time is fairly negligible.\"}"}
{"id": "qAml3FpfhG", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"In this section, we explore versions of Figures 3 and 5 when we look at rank correlation (correlation between true and predicted ranking) instead of performance. It is clear from the plots below that our method can be used to rank models efficiently with tiny samples.\\n\\nFigure 14. Rank correlation for true performance and predicted performance among LLMs.\\n\\nFigure 15. Rank correlation for true performance and predicted performance among LLMs in MMLU. The plot on the left represents a random split of the data while the plot on the right considers specialized models as the test set.\\n\\nE.5. Adaptive testing\\n\\nIn this section, we complement the results shown in Figure 8 for all benchmarks.\"}"}
{"id": "qAml3FpfhG", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nReferences\\n\\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\n\\nAn, X. and Yung, Y.-F. Item response theory: What it is and how you can use the irt procedure to apply it. SAS Institute Inc, 10(4):364\u20132014, 2014.\\n\\nBach, S., Sanh, V., Yong, Z. X., Webson, A., Raffel, C., Nayak, N. V., Sharma, A., Kim, T., Bari, M. S., Fevry, T., Alyafeai, Z., Dey, M., Santilli, A., Sun, Z., Ben-david, S., Xu, C., Chhablani, G., Wang, H., Fries, J., Al-shaibani, M., Sharma, S., Thakker, U., Almubarak, K., Tang, X., Radev, D., Jiang, M. T.-j., and Rush, A. PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 93\u2013104, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-demo.9. URL https://aclanthology.org/2022.acl-demo.9.\\n\\nBeeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O., T unstall, L., and Wolf, T. Open llm leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023.\\n\\nBiderman, S., Prashanth, U. S., Sutawika, L., Schoelkopf, H., Anthony, Q., Purohit, S., and Raf, E. Emergent and predictable memorization in large language models. arXiv preprint arXiv:2304.11158, 2023a.\\n\\nBiderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., Skowron, A., Sutawika, L., and van der Wal, O. Pythia: A suite for analyzing large language models across training and scaling. ArXiv, abs/2304.01373, 2023b. URL https://api.semanticscholar.org/CorpusID:257921893.\\n\\nBojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., et al. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation, pp. 12\u201358, 2014.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nBrzezi\u0144ska, J. Item response theory models in the measurement theory. Communications in Statistics-Simulation and Computation, 49(12):3299\u20133313, 2020.\\n\\nCai, L., Choi, K., Hansen, M., and Harrell, L. Item response theory. Annual Review of Statistics and Its Application, 3:297\u2013321, 2016.\\n\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\\n\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n\\nEin-Dor, L., Halfon, A., Gera, A., Shnarch, E., Dankin, L., Choshen, L., Danilevsky, M., Aharonov, R., Katz, Y., and Slonim, N. Active Learning for BERT: An Empirical Study. In Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 7949\u20137962, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.638. URL https://aclanthology.org/2020.emnlp-main.638.\\n\\nElvira, V., Martino, L., and Robert, C. P. Rethinking the effective sample size. International Statistical Review, 90(3):525\u2013550, 2022.\\n\\nFahrmeir, L. and Kaufmann, H. Consistency and asymptotic normality of the maximum likelihood estimator in generalized linear models. The Annals of Statistics, 13(1):342\u2013368, 1985.\\n\\nGuha, N., Nyarko, J., Ho, D., R\u00e9, C., Chilton, A., Chohlas-Wood, A., Peters, A., Waldon, B., Rockmore, D., Zambrano, D., et al. Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nHastie, T., Tibshirani, R., Friedman, J. H., and Friedman, J. H. The elements of statistical learning: data mining, inference, and prediction, volume 2. Springer, 2009.\\n\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\n\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\"}"}
{"id": "qAml3FpfhG", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nJi, D., Logan, R. L., Smyth, P., and Steyvers, M. Active bayesian assessment of black-box classifiers. Proceedings of the AAAI Conference on Artificial Intelligence, 35(9):7935\u20137944, May 2021. doi: 10.1609/aaai.v35i9.16968. URL https://ojs.aaai.org/index.php/AAAI/article/view/16968.\\n\\nJin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., and Szolovits, P. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021.\\n\\nKatariya, N., Iyer, A., and Sarawagi, S. Active evaluation of classifiers on large datasets. In 2012 IEEE 12th International Conference on Data Mining, pp. 329\u2013338, 2012. doi: 10.1109/ICDM.2012.161.\\n\\nKingston, N. M. and Dorans, N. J. The feasibility of using item response theory as a psychometric model for the gre aptitude test. ETS Research Report Series, 1982(1):i\u2013148, 1982.\\n\\nKo\u02c7cisk`y, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013328, 2018.\\n\\nKossen, J., Farquhar, S., Gal, Y ., and Rainforth, T. Active testing: Sample-efficient model evaluation. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 5753\u20135763. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/kossen21a.html.\\n\\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019.\\n\\nLalor, J. P. and Rodriguez, P. py-irt: A scalable item response theory library for python. INFORMS Journal on Computing, 35(1):5\u201313, 2023.\\n\\nLalor, J. P., Wu, H., and Yu, H. Building an evaluation scale using item response theory. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing, volume 2016, pp. 648. NIH Public Access, 2016.\\n\\nLi, X., Zhang, T., Dubois, Y ., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. AlpacaEval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.\\n\\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y ., Narayanan, D., Wu, Y ., Kumar, A., et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\\n\\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.\\n\\nLiu, Z., Qiao, A., Neiswanger, W., Wang, H., Tan, B., Tao, T., Li, J., Wang, Y ., Sun, S., Pangarkar, O., et al. Llm360: Towards fully transparent open-source llms. arXiv preprint arXiv:2312.06550, 2023.\\n\\nLord, F., Novick, M., and Birnbaum, A. Statistical theories of mental test scores. 1968.\\n\\nLu, Y ., Bartolo, M., Moore, A., Riedel, S., and Sten\u00e9torp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086\u20138098, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022.acl-long.556.\\n\\nMaia Polo, F. and Vicente, R. Effective sample size, dimensionality, and generalization in covariate shift adaptation. Neural Computing and Applications, 35(25):18187\u201318199, 2023.\\n\\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\\n\\nMin, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11048\u201311064, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.759.\\n\\nMishra, S., Khashabi, D., Baral, C., Choi, Y ., and Hajishirzi, H. Reframing instructional prompts to GPTk\u2019s language. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 589\u2013612, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.50. URL https://aclanthology.org/2022.findings-acl.50.\"}"}
{"id": "qAml3FpfhG", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nMizrahi, M., Kaplan, G., Malkin, D., Dror, R., Shahaf, D., and Stanovsky, G. State of what art? a call for multi-prompt llm evaluation. arXiv preprint arXiv:2401.00595, 2023.\\n\\nNie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., and Kiela, D. Adversarial nli: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4885\u20134901, 2020.\\n\\nPerlitz, Y., Bandel, E., Gera, A., Arviv, O., Ein-Dor, L., Shnarch, E., Slonim, N., Shmueli-Scheuer, M., and Choshen, L. Efficient benchmarking (of language models). arXiv preprint arXiv:2308.11696, 2023.\\n\\nPetersen, N. S. et al. Using item response theory to equate scholastic aptitude test scores. 1982.\\n\\nRodriguez, P., Barrow, J., Hoyle, A. M., Lalor, J. P., Jia, R., and Boyd-Graber, J. Evaluation examples are not equally informative: How should that change NLP leaderboards? In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4486\u20134503, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.346. URL https://aclanthology.org/2021.acl-long.346.\\n\\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.\\n\\nSclar, M., Choi, Y., Tsvetkov, Y., and Suhr, A. Quantifying language models\u2019 sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324, 2023.\\n\\nSong, W. T. Minimal-mse linear combinations of variance estimators of the sample mean. In 1988 Winter Simulation Conference Proceedings, pp. 414\u2013421. IEEE, 1988.\\n\\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\\n\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\\n\\nTeam, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n\\nVan der Linden, W. J. Handbook of item response theory: Three volume set. CRC Press, 2018.\\n\\nVania, C., Htut, P. M., Huang, W., Mungra, D., Pang, R. Y., Phang, J., Liu, H., Cho, K., and Bowman, S. R. Comparing test sets with item response theory. arXiv preprint arXiv:2106.00840, 2021.\\n\\nVivek, R., Ethayarajh, K., Yang, D., and Kiela, D. Anchor points: Benchmarking models with much fewer examples. arXiv preprint arXiv:2309.08638, 2023.\\n\\nVoronov, A., Wolf, L., and Ryabinin, M. Mind your format: Towards consistent evaluation of in-context learning improvements. arXiv preprint arXiv:2401.06766, 2024.\\n\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\\n\\nWeber, L., Bruni, E., and Hupkes, D. The icl consistency test. arXiv preprint arXiv:2312.04945, 2023a.\\n\\nWeber, L., Bruni, E., and Hupkes, D. Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning. arXiv preprint arXiv:2310.13486, 2023b.\\n\\nWei, J., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H., Huang, D., Zhou, D., et al. Larger language models do in-context learning differently. ArXiv preprint, abs/2303.03846, 2023. URL https://arxiv.org/abs/2303.03846.\\n\\nYe, Q., Fu, H. Y., Ren, X., and Jia, R. How predictable are large language model capabilities? a case study on big-bench. arXiv preprint arXiv:2305.14947, 2023.\\n\\nYoo, K. M., Kim, J., Kim, H. J., Cho, H., Jo, H., Lee, S.-W., Lee, S.-g., and Kim, T. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2422\u20132437, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.155.\"}"}
{"id": "qAml3FpfhG", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\\n\\nHellaswag: Can a machine really finish your sentence?\\narXiv preprint arXiv:1905.07830, 2019.\\n\\nZhuang, Y., Liu, Q., Ning, Y., Huang, W., Lv, R., Huang, Z.,\\nZhao, G., Zhang, Z., Mao, Q., Wang, S., et al.\\nEfficiently measuring the cognitive ability of llms: An adaptive testing perspective.\\narXiv preprint arXiv:2306.10512, 2023.\"}"}
{"id": "qAml3FpfhG", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\n\\\\[ \\\\tilde{Y}_{il} = 1 \\\\left[ Y_{il} \\\\geq c \\\\right], \\\\]  \\nfor a scenario-dependent constant \\\\( c \\\\).\\n\\nMore concretely, for each scenario \\\\( j \\\\), we choose \\\\( c \\\\) such that\\n\\n\\\\[ P_{i} \\\\in I_{j}, l \\\\in L_{tr} Y_{il} \\\\approx P_{i} \\\\in I_{j}, l \\\\in L_{tr} 1 \\\\left[ Y_{il} \\\\geq c \\\\right]. \\\\]\\n\\nIn that way, approximating the average of \\\\( \\\\tilde{Y}_{il} \\\\) and \\\\( Y_{il} \\\\) should be more or less equivalent. Given that \\\\( \\\\tilde{Y}_{il} \\\\in \\\\{0, 1\\\\} \\\\), we can use the standard IRT tools to model it.\\n\\n### 4.4. Fitting the IRT model\\n\\nFor the estimation procedure, we resort to variational inference. In particular, we assume that\\n\\n\\\\[ \\\\theta_{l} \\\\sim N(\\\\mu_{\\\\theta}, \\\\frac{1}{u_{\\\\theta}}) \\\\]\\n\\n\\\\[ \\\\alpha_{i} \\\\sim N(\\\\mu_{\\\\alpha}, \\\\frac{1}{u_{\\\\alpha}}) \\\\]\\n\\n\\\\[ \\\\beta_{i} \\\\sim N(\\\\mu_{\\\\beta}, \\\\frac{1}{u_{\\\\beta}}) \\\\]\\n\\nTo take advantage of software for fitting hierarchical Bayesian models (Lalor & Rodriguez, 2023), we introduce (hyper)priors for the prior parameters\\n\\n\\\\[ \\\\mu_{\\\\theta} \\\\sim N(0, 10) \\\\]\\n\\n\\\\[ u_{\\\\theta} \\\\sim \\\\Gamma(1, 1) \\\\]\\n\\n\\\\[ \\\\mu_{\\\\alpha} \\\\sim N(0, 10) \\\\]\\n\\n\\\\[ u_{\\\\alpha} \\\\sim \\\\Gamma(1, 1) \\\\]\\n\\n\\\\[ \\\\mu_{\\\\beta} \\\\sim N(0, 10) \\\\]\\n\\n\\\\[ u_{\\\\beta} \\\\sim \\\\Gamma(1, 1) \\\\]\\n\\nFinally, to obtain point estimates for the model and example-specific parameters \\\\( \\\\theta_{l}, \\\\alpha_{i}, \\\\beta_{i} \\\\), we use the means of their variational distributions. To select the dimension of the IRT model during the fitting procedure, we run a simple validation strategy in the training set and choose the dimension that maximizes the prediction power of the IRT model in the validation split\u2014we consider the dimensions in \\\\( \\\\{2, 5, 10, 15\\\\} \\\\).\\n\\n### 5. Assessing evaluation strategies\\n\\nWe assess the ability of the considered evaluation strategies to estimate the performance of LLMs on four popular benchmarks. For a given LLM and a benchmark, each evaluation strategy estimates the performance using evaluation results of this LLM on a given number of examples. We then compare this estimate to the true value, i.e., the performance of this LLM on the complete benchmark.\\n\\n**Evaluation pipeline**\\n\\nFor each benchmark, we first collect publicly available correctness data (\\\\( Y_{il} \\\\)'s) for a set of LLMs \\\\( L \\\\) that have been previously evaluated on this benchmark. Recall that the benchmark is a set of examples \\\\( I \\\\) consisting of \\\\( J \\\\) disjoint scenarios \\\\( I_{j} \\\\) such that\\n\\n\\\\[ I = \\\\bigcup_{j \\\\in [J]} I_{j} \\\\].\\n\\nWe use correctness data corresponding to a subset of LLMs \\\\( L_{tr} \\\\), i.e.,\\n\\n\\\\[ D_{tr} = \\\\{ Y_{il} \\\\}_{l \\\\in L_{tr}, i \\\\in I_{j}} \\\\],\\n\\nto (i) find anchor points \\\\( \\\\hat{I}_{j} \\\\) for each one of the scenarios \\\\( j \\\\in [J] \\\\) as described in Section 3 and (ii) to obtain estimates for the IRT parameters \\\\( \\\\{(\\\\alpha_{i}, \\\\beta_{i})\\\\}_{i \\\\in I_{j}} \\\\) as described in Section 4. We call this \\\"train\\\" set of models as their correctness data is used to identify anchor points and fit the parameters associated with our evaluation strategies. The remaining set of \\\"test\\\" models \\\\( L_{te} \\\\) is used to quantify the error of our evaluation strategies in practice. For each LLM in the test set, \\\\( l \\\\in L_{te} \\\\), we observe its correctness on the anchor points, i.e.,\\n\\n\\\\[ \\\\{ Y_{il} \\\\}_{i \\\\in \\\\hat{I}_{j}} \\\\],\\n\\nand use it to obtain benchmark performance estimates as described in Sections 3 and 4. The estimate is then compared to the ground truth, i.e., performance of this LLM on the entirety of the benchmark.\\n\\nWe consider two train-test model split scenarios: (i) random split and (ii) by date, i.e., using the most recent models for testing. The latter split better represents practical use cases, while also being more challenging as it is likely to result in a distribution shift between the train and test models due to improving model capabilities over time that might affect the effectiveness of anchor points and the IRT model.\\n\\n**Benchmarks and models**\\n\\nWe describe the size and composition of the four benchmarks, as well as the corresponding LLMs (see Appendix D for additional details):\\n\\n- **HuggingFace's Open LLM Leaderboard** (Beeching et al., 2023) consists of 6 scenarios, approx. 29K examples in total. Performance on each of the scenarios is measured with accuracy and the overall benchmark performance is equal to the average of scenario accuracies. We collect evaluation results for 395 LLMs from the Leaderboard's website and use 75% for training and 25% for testing (split either randomly or by date as described above).\\n\\n- **MMLU** (Hendrycks et al., 2020) is a multiple choice QA scenario consisting of 57 subjects (subscenarios) comprising approx. 14K examples. Performance on MMLU is measured by averaging the accuracies on each of the categories. MMLU is one of the 6 scenarios of the Open LLM Leaderboard and we consider the same set of 395 LLMs and train-test splits. The reason to consider it separately is its immense popularity when comparing LLMs (Touvron et al., 2023; Achiam et al., 2023; Team et al., 2023) and inclusion into several other benchmarks.\\n\\n- **HELM** (Liang et al., 2022), we use HELM Lite v1.0.0, which has the 10 core scenarios (total of approx. 10K evaluation examples) and 30 models that have their performances registered for all scenarios. Performance metrics for each scenario vary and can be non-binary (e.g., F1 score), and the overall performance on the benchmark is measured with mean win rate across scenarios. For this benchmark, the dates models were added are not available. Instead, we split models based on the organizations that trained them to create more challenging train-test splits, e.g., all OpenAI models are either in train or in test. For the random train-test split we use 11-fold cross-validation. That is, we partition the set of all LLMs into \\\\( k = 11 \\\\) parts and, for each one of these parts, we use one of them to test and \\\\( k - 1 \\\\) parts for training. Then, we average the results over the choice of the testing part.\\n\\n- **AlpacaEval 2.0** (Li et al., 2023) consists of 100 LLMs evaluated on 805 examples. Although it is a fairly small benchmark, evaluation is expensive as it requires GPT-4 as a judge. For each input, GPT-4 compares the responses of a candidate LLM and a baseline LLM (currently also 6\".}"}
{"id": "qAml3FpfhG", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Performance estimation error per benchmark (columns) tested on random (top row) and recent (bottom row) LLMs for increasing number of evaluation examples. 100 examples per scenario is sufficient to achieve \u22482% average performance estimation error across benchmarks and evaluated LLMs. This corresponds to 600 out of 29K examples for Open LLM Leaderboard, 100 out of 14K examples for MMLU, 1000 out of 10K examples for HELM, and 100 out of 800 examples for AlpacaEval 2.0.\\n\\nEvaluation strategies\\n\\nWe consider 3 strategies presented in \u00a73 for selecting a subset of examples for efficient evaluation: \u201crandom\u201d for stratified random sampling, \u201ccorrectness\u201d for clustering correctness of models in the train set, and \u201cIRT\u201d for clustering the example representations obtained from the IRT model fit on the train set. For each strategy, we evaluate the vanilla variation, i.e., simply using the performance of a test LLM on the (weighted) set of selected examples to estimate its performance on the full benchmark, and \u201c++\u201d variation that adjusts this estimate using the IRT model as described in equation (4.4). In total, we assess six evaluation strategies. Results are averaged over 5 restarts.\\n\\nKey findings\\n\\nWe investigate the effectiveness of strategies as we increase the number of examples available for evaluating test LLMs. Results for both train-test split scenarios are presented in Figure 3 (see also Figure 14 for Spearman\u2019s rank correlations). Our main conclusions are:\\n\\n\u2022 Our approach to reducing evaluation costs is effective. The best-performing strategies achieve estimation error within 2% on all benchmarks with 100 examples or less per dataset or scenario. For example, for MMLU this reduces the evaluation cost by a factor of 140 (from 14k to 100). For Open LLM Leaderboard even 30 examples per scenario is enough, reducing the evaluation cost by a factor of 160 (from 29K to 180).\\n\\n\u2022 Most strategies perform well when there is a temporal shift between the train and test LLM\u2019s (see the lower row of plots in Figure 3 for the results with \u201cby date\u201d split). Thus our approaches for reducing evaluation costs remain practical when evaluating the performance of newer, more capable LLMs and can help save GPU hours when evaluating future LLMs and/or checkpoints during pre-training.\\n\\n\u2022 IRT-based methods (\u201cIRT\u201d and \u201cIRT++\u201d) perform consistently well across benchmarks and train-test splits. The gp-IRT (\u201c++\u201d) variation always improves or matches its vanilla counterpart, while adding only a few seconds to the evaluation time (see Figure 13). Thus we use the IRT-based anchor examples to construct tiny versions (100 examples per scenario) of each of the benchmarks and release them along with the gp-IRT tool (code and pre-trained IRT model) for efficient evaluation of future LLMs. We present additional evaluations of tiny-Benchmarks in Figure 4 for one of the 5 random seeds in which the random sampling underperforms. In Appendix B, we conduct an exploratory analysis of the examples comprising tinyMMLU.\\n\\nSpecialized LLMs\\n\\nIn our previous experiments the test set of LLMs consisted of either a random subset of models or the most recent ones. Both of these test sets are\"}"}
{"id": "qAml3FpfhG", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nFigure 4. Predicted performance compared with true performance for the four benchmarks (columns) and recent LLMs. We verify the efficacy of the evaluation strategies (IRT and IRT++) we chose to construct tinyBenchmarks.\\n\\nFigure 5. Estimation error on specialized LLMs (right) compared to error on random LLMs (left) on MMLU. Correctness-based example selection is affected the most by this distribution shift.\\n\\nWe assess the ability of the considered strategies to predict the performance of specialized LLMs, i.e., models fine-tuned for specific domains such as code, biology, or finance. We consider MMLU benchmark and collect a new hand-picked test set of 40 specialized models. Such models are likely to have unique strengths and perform well in specific MMLU categories while relatively underperforming on others. Thus, their correctness patterns might be different from those in the train set, posing a challenge for our evaluation strategies. We present results in Figure 5.\\n\\nAs we anticipated, the correctness-based anchor strategy deteriorates when tested on specialized LLMs. In contrast to the IRT-based anchors that are only slightly affected, demonstrating their robustness and supporting our choice to use them for tinyBenchmarks construction.\\n\\nEstimation error analysis\\n\\nWe present a more detailed view of the estimation error of the best performing \u201cIRT++\u201d evaluation strategy on MMLU with 100 examples. In Figure 6 we plot estimation error against the actual accuracy of 99 test LLMs for a random train-test split. Our strategy can estimate the performance of more capable LLMs slightly better, i.e., models with higher scores, although there is no strong dependency. We also note that the estimation error never exceeds 4% (except for one LLM with extremely low performance). Recall that the average error is 2% as shown in Figure 3, supporting the reliability of our evaluation approach.\\n\\n6. Conclusion\\n\\nIn this paper, we demonstrate it is possible to accurately assess the capabilities of LLMs with a fraction (sometimes two orders of magnitude smaller) of the examples in common benchmark datasets by leveraging models of educational assessments from psychometrics. This leads directly to savings in terms of the monetary costs associated with evaluating LLMs, but also the computational and environmental costs. For practitioners, the computational cost savings are especially convenient because they enable them to evaluate LLMs more frequently during fine-tuning and prompt engineering.\\n\\nBased on our results we are releasing tinyBenchmarks, preselected subsets of examples from the widely adopted LLM benchmarks. tinyBenchmarks are simply small datasets that are straightforward to use to evaluate LLMs cheaply. We are also releasing an IRT-based tool to enhance performance estimation. The tool provides code and IRT parameters trained on the corresponding benchmarks and can be run on a CPU in a few seconds.\\n\\n6.1. Extensions\\n\\nPrompt evaluation\\n\\nA persistent challenge in prompt-based model evaluation is the influence the prompting setup has on model predictions (see, e.g., Lu et al., 2022; Mishra...\"}"}
{"id": "qAml3FpfhG", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nFigure 7. Estimation error when predicting the performance of prompt templates. The results demonstrate that using our methods for efficient prompt-based model evaluation is a promising application.\\n\\net al., 2022; Min et al., 2022; Yoo et al., 2022; Weber et al., 2023b; Wei et al., 2023). We can use the previously described approaches to make predictions across different prompting setups. This way, we can estimate how well a model will do on a new set of prompts using just a few evaluations, or how a new model will perform on a given prompt. To test this idea, we train an IRT model on the prediction data from Weber et al. (2023a), containing evaluations of eight LLaMA LLMs (vanilla or instruction tuned on the Alpaca self-instruct dataset; Touvron et al., 2023; Taori et al., 2023) for the ANLI dataset (Nie et al., 2020). The dataset consists of evaluations of the 750 data points wrapped with 15 different instruction templates sourced from the promptsource collection (P3; Bach et al., 2022). Similarly to our previous experiments, we evaluate random splits and splits featuring distribution shifts (across model sizes and different instruction templates). For model size, we put all models with sizes 7B, 13B, and 30B in the training set while the models with size 65B go to the test set. For splits related to prompts templates, we consider two different approaches: first, we conduct a 2-fold cross-validation rotating instruction templates; second, we consider using the same and different instruction templates in the in-context-learning examples and in the input example alternating the strategies in the training and test sets. Results in Figure 7 suggest that prompt-based model evaluation can be efficiently carried out with the methods introduced in this work, even in the presence of several practical distribution shifts.\\n\\nAdaptive testing\\n\\nWe expect further performance estimation improvements can be squeezed out by more sophisticated applications of similar ideas. For example, instead of pre-selecting a subset of examples before evaluating the LLM, it may be possible to select the examples adaptively during the evaluation process. This idea is widely used in the computerized-assisted testing algorithms behind many standardized tests. We demonstrate preliminary results on MMLU using an adaptive IRT variant in Figure 8 (see Figure 16 for results on more benchmarks). Although the estimation performance has improved, our current implementation takes over 5 minutes to run, which might not be as appealing practically.\\n\\n6.2. Limitations\\n\\nThe main limitations of the methods described in this paper are related to potential severe distribution shifts. Taking MMLU as an example, we anticipate larger performance estimation errors for models that fail on simple questions while answering complicated ones correctly, thus altering the correctness patterns. This might be caused by significant architecture or pre-training data changes. A rapid increase in LLM capabilities may also cause extrapolation errors. To alleviate these problems, we recommend periodically updating the curated examples and IRT parameter estimates using data from more modern LLMs.\\n\\nAcknowledgements\\n\\nWe are grateful for the help provided by Yotam Perlitz in downloading data from HELM. This paper is based upon work supported by the National Science Foundation (NSF) under grants no. 2027737 and 2113373.\\n\\nImpact Statement\\n\\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.\"}"}
{"id": "qAml3FpfhG", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\n(2023) for HELM. This approach is the simplest to use but can result in a large estimation error.\\n\\n2. Clustering examples based on LLMs that have already been evaluated. The key idea is to find examples where (in)correct prediction of an LLM implies that it will also be (in)correct on a subset of other examples. This method performs well in some settings but can be unreliable when such correctness patterns are spurious, e.g., when predicting the accuracy of an LLM specialized to a domain. This strategy is inspired by the Anchor Points method (Vivek et al., 2023) which clusters models' confidence in the correct class for faster evaluation on classification tasks.\\n\\n3. New strategies built using Item Response Theory (IRT) (Lord et al., 1968) for evaluating individuals through standardized tests. Applying IRT to LLMs viewed as testees and benchmarks as tests, we learn representations of examples encoding latent abilities required to perform well on these examples. Clustering these representations allows us to find a more robust evaluation set. Furthermore, using the IRT model, we develop tools for improving benchmark accuracy estimates obtained with an arbitrary set of examples.\\n\\nWe present an extensive evaluation of these strategies on four popular benchmarks (\u00a75): Open LLM Leaderboard (Beeching et al., 2023), MMLU (Hendrycks et al., 2020), HELM (Liang et al., 2022), and AlpacaEval 2.0 (Li et al., 2023). Our goal is to assess the effectiveness of estimating the performance of LLMs on these benchmarks using a limited number of examples for evaluation. Overall, we conclude that 100 curated examples per scenario are enough to reliably estimate the performance of various LLMs, within about 2% error on average. Based on our findings we release tiny (100 examples per scenario) versions of every considered benchmark and IRT-based tools for further improving the performance estimation.\\n\\n1.1. Related work\\n\\nEfficient benchmarking of LLMs\\n\\nMulti-dataset benchmarks were introduced to the field of NLP with the advent of pre-trained models (e.g. Wang et al., 2018), and constantly evolved in lockstep with language model capabilities (Srivastava et al., 2022). The ever-increasing size of models and datasets consequently led to high evaluation costs, triggering changes in reported evaluation to accommodate the costs (Biderman et al., 2023b). Ye et al. (2023) considered reducing the number of tasks in Big-bench (Srivastava et al., 2022). Perlitz et al. (2023) found that evaluation on HELM (Liang et al., 2022) relies on diversity across datasets, but the number of examples currently used is excessive. We adopt their stratified sampling approach as one of the efficient evaluation strategies. Vivek et al. (2023) proposed clustering evaluation examples based on models' confidence in the correct class for faster evaluation on classification tasks. One of the approaches we consider is based on an adaptation of their method to popular LLM benchmarks with more diverse tasks.\\n\\nItem response theory (IRT)\\n\\nIRT (Cai et al., 2016; Van der Linden, 2018; Brzezi\u0144ski, 2020; Lord et al., 1968) is a well-established set of statistical models used in psychometrics to measure the latent abilities of individuals through standardized testing (An & Yung, 2014; Kingston & Dorans, 1982; Petersen et al., 1982), e.g., in GRE, SAT, etc. Even though IRT methods have been traditionally used in psychometrics, they are becoming increasingly popular among researchers in the fields of artificial intelligence and natural language processing (NLP). For instance, Lalor et al. (2016) propose using IRT's latent variables to measure language model abilities, Vania et al. (2021) employs IRT models in the context of language models benchmarking to study saturation (un-discriminability) of commonly used benchmarks, and Rodriguez et al. (2021) study several applications of IRT in the context of language models, suggesting that IRT models can be reliably used to: predict responses of LLMs in unseen items, categorize items (e.g., according to their difficulty/discriminability), and rank models. More recently, Zhuang et al. (2023) used IRT for adaptive testing, making testing more efficient. However, the authors do not propose a performance estimator for LLMs but only rank models based on their ability parameters. To the best of our knowledge, IRT has not been used for performance estimation in the context of efficient benchmarking of LLMs. We explore this new path.\\n\\nActive testing\\n\\nAnother line of related work is related to active learning (Ein-Dor et al., 2020) and especially active testing. In such works, evaluation examples are chosen dynamically using various criteria (Ji et al., 2021; Kossen et al., 2021; Zhuang et al., 2023) to minimize annotation costs. Those methods are somewhat similar to the adaptive IRT which we discuss in \u00a76.\"}"}
{"id": "qAml3FpfhG", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nchoice questions to text summarization tasks. Our final\\nobjective is to estimate the performance of LLMs in the\\nfull benchmark, which is given by the average of the perfor-\\n\\nmances in individual scenarios (Open LLM Leaderboard,\\nMMLU, AlpacaEval 2.0) or mean-win-rate (HELM). We\\nachieve this objective by first estimating the performance of\\nLLMs in individual scenarios and then aggregating scores.\\n\\nWhen scenarios have sub-scenarios, it is usually the case\\nthat the scenario performance is given by a simple aver-\\n\\nage of sub-scenarios performances. The main concern is\\nthat each scenario/sub-scenario is composed of hundreds or\\nthousands of examples, making model evaluation costly.\\n\\nIn this work, for a fixed benchmark, we denote the set of ex-\\namples of each scenario $I_j$ as $I_j$, implying that the totality of\\nexamples in the benchmark is given by $I = \\\\cup_j I_j$. When an\\nLLM $l$ interacts with an example $i \\\\in I_j$, the system behind\\nthe benchmarks generates a score that we call \\\"correctness\\\"\\nand denote as $Y_{il}$. In all the benchmarks we consider in\\nthis work, the correctness is either binary, i.e. $Y_{il} \\\\in \\\\{0, 1\\\\}$\\n(incorrect/correct), or bounded, i.e. $Y_{il} \\\\in [0, 1]$, denoting\\na degree of correctness. The second case is applied in sit-\\n\\nuations in which, for instance, there might not be just one\\ncorrect answer for example $i$.\\n\\nTo simplify the exposition in the text, we assume that the score for LLM $l$ in scenario $j$ is just the simple average of the correctness of all items in that scenario, that is,\\n\\n$$\\\\frac{1}{|I_j|} \\\\sum_{i \\\\in I_j} Y_{il}.$$ \\n\\nThat is not true when different sub-scenarios have different numbers of ex-\\n\\namples; in that case, one would just have to use a weighted\\naverage instead, to make sure every sub-scenario is equally\\nimportant (in the experiments, we consider this case).\\n\\nOur objective is to choose a small fraction of examples $b I_j \\\\subset I_j$ such that we can estimate score of a new LLM $l$, i.e.\\n\\n$$\\\\frac{1}{|I_j|} \\\\sum_{i \\\\in I_j} Y_{il},$$\\n\\nusing its correctness evaluated only on the examples in $b I_j \\\\subset I_j$, i.e.\\n\\n$$\\\\{Y_{il}\\\\}_{i \\\\in b I_j}.$$\\n\\nTo intelligently choose $b I_j$ we assume access to correctness evaluations for a set of LLMs that have been previously evaluated on the entirety of the benchmark. Such correctness data is freely available for many popular benchmarks. In the next section, we describe strategies on how $b I_j$ can be chosen and how the LLMs performance on the full benchmark can be estimated.\\n\\n3. Selecting evaluation examples\\n\\nIn this section, we describe strategies on how to select ex-\\n\\namples from a fixed scenario $I_j$, obtaining $b I_j \\\\subset I_j$ described in Section 2. Ideally, the set of selected exam-\\n\\nples should be representative of the whole set of items in\\nscenario $j$, that is,\\n\\n$$\\\\frac{1}{|I_j|} \\\\sum_{i \\\\in I_j} w_i Y_{il} \\\\approx \\\\frac{1}{|I_j|} \\\\sum_{i \\\\in I_j} Y_{il},$$\\n\\n(3.1)\\n\\nfor nonnegative weights $\\\\{w_i\\\\}_{i \\\\in b I_j}$ such that\\n\\n$$\\\\sum_{i \\\\in b I_j} w_i = 1.$$\\n\\nIn the next paragraphs, we describe two possible ways of\\nobtaining $b I_j$ and $\\\\{w_i\\\\}_{i \\\\in b I_j}$.\\n\\n3.1. Stratified random sampling\\n\\nIn some settings (e.g., classifiers Katariya et al., 2012), it is\\nuseful to perform stratified random sampling \u2013 subsample\\nexamples ensuring the representation of certain groups of\\ndata. Using subscenarios as the strata for stratified random\\nsampling was proposed by Perlitz et al. (2023) when sub-\\n\\nsampling examples from HELM scenarios. The authors\\nshowed that this is an effective way of sampling examples\\nwithout too much loss on the ability to rank LLMs by per-\\n\\nformance. Examples should be randomly selected from\\nsubscenarios (with uniform probability) in a way such that\\nthe difference in number of examples sampled for two dis-\\n\\ntinct subscenarios is minimal ($\\\\leq 1$).\\n\\nThe rationale behind\\n\\nthis method is that, for an effective evaluation, sub-scenarios\\nshould be equally represented. The weights are\\n\\n$$w_i = \\\\frac{1}{|b I_j|}$$\\n\\nfor all $i \\\\in b I_j$.\\n\\n3.2. Clustering\\n\\nAssessing the performance of LLM's on a randomly sam-\\n\\npled subset of examples suffers from extra uncertainty in the\\nsampling process, especially when the number of sampled\\nexamples is small. Instead, we consider selecting a subset of\\nrepresentative examples using clustering. Vivek et al. (2023)\\nproposed to cluster examples based on the confidence of\\nmodels in the correct class corresponding to these examples.\\n\\nRepresentative examples, from these clusters, which they\\n\\ncall \\\"anchor points\\\", can then be used to evaluate models\\non classification tasks more efficiently. We adapt their clus-\\n\\ntering approach to a more general setting, allowing us to\\nextract such anchor points for MMLU, AlpacaEval 2.0, and\\nall scenarios of the Open LLM Leaderboard and HELM.\\n\\nFirst, we propose to group examples by model correctness,\\nexpecting some examples would represent the rest. Ideally,\\nif example $i$ is an anchor point, then there will be a big set of\\nexamples on which models are correct if and only if they get\\nexample $i$ correct. The same idea applies when correctness\\nis given by a number in $[0, 1]$. Assume that we want to\\nselect $K$ anchor points and have access to the training set\\n$D_{tr} = \\\\{Y_l\\\\}_{l \\\\in L_{tr}}$, where $Y_l$ is a vector in which each entry\\nis given by the correctness score $Y_{il}$ for all examples\\n$i \\\\in I_j$. We represent each example $i \\\\in I_j$ by the embedding\\n$E_i \\\\in \\\\mathbb{R}^{|L_{tr}|}$ which is a vector with entries given by\\n$Y_{il}$ for $l \\\\in L_{tr}$, and then run $K$-Means (Hastie et al., 2009) with\\nthe number of clusters being equal $K$. After the $K$ centroids\\nare obtained, we find the closest example to each centroid,\\nand each of those points will compose $b I_j$. For a new LLM\\n$l \\\\not\\\\in L_{tr}$ to be evaluated, we can obtain an estimate for its\\nperformance using the estimate in equation 3.1 by setting\\n$w_i$ as the fraction of points in $I_j$ assigned to cluster/anchor\\npoint $i$. This method is compelling and simple in detecting\"}"}
{"id": "qAml3FpfhG", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we are interested in estimating the performance of a model\\n\\n\\\\[ \\\\text{where} \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\nwhile\\n\\n\\\\[ \\\\text{where} \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta \\\\]\\n\\nwhich examples have similar difficulty and require similar\\n\\nThe two-parameter multidimensional IRT model assumes\\n\\n\\\\[ \\\\text{The weight} \\\\ w_i \\\\]\\n\\nThe second approach we propose is using item response\\n\\n4. Better performance estimation with IRT\\n\\n4.2. IRT-based LLM performance estimation\\n\\nAssume that\\n\\n\\\\[ \\\\text{The performance-IRT} (\\\\text{p-IRT}) \\\\text{ estimator.} \\\\]\\n\\nFor the case\\n\\n\\\\[ Y_i \\\\]\\n\\nfor the performance of an LLM, propose a simple solution\\n\\nexample representations are stable. As IRT should represent\\n\\nthe IRT model reasonably describes the reality and the ex-\\n\\nand potentially alleviates the distribution shift problem if\\n\\nthe curse of dimensionality when\\n\\n\\\\[ |L| \\\\]\\n\\ne.g.\\n\\n- correctness patterns can vary,\\n\\nanchor points. Still, it can suffer from distribution shifts\\n\\n\\\\[ \\\\text{approach is intended to be more robust to those problems.} \\\\]\\n\\nIn this section, we propose ways of enhancing performance\\n\\n4.1. The IRT model\\n\\n\\\\[ p \\\\]\\n\\n\\\\[ \\\\lambda \\\\]\\n\\n\\\\[ E \\\\]\\n\\nIn our experiments, the dimension of\\n\\n\\\\[ E \\\\]\\n\\n\\\\[ \\\\text{on scenario} \\\\ tr \\\\]\\n\\n\\\\[ b \\\\]\\n\\n\\\\[ \\\\text{denotes the unobserved abilities of LLM} \\\\]\\n\\n\\\\[ l \\\\]\\n\\n\\\\[ \\\\text{d} \\\\]\\n\\n\\\\[ \\\\text{formulation,} \\\\]\\n\\n\\\\[ \\\\text{to respond to example} \\\\ i \\\\]\\n\\n\\\\[ \\\\text{correctly or not.} \\\\]\\n\\nWe later also discuss the\\n\\n\\\\[ \\\\text{example representations} \\\\]\\n\\n\\\\[ \\\\text{model fitting.} \\\\]\\n\\n\\\\[ \\\\text{in 4.3 by Performance-IRT (p-IRT) estimator.} \\\\]\\n\\nTo facilitate our analysis, assume for a moment that the\\n\\n\\\\[ \\\\text{true values of} \\\\ \\\\theta \\\\]\\n\\n\\\\[ \\\\text{implies about our estimates} \\\\]\\n\\n\\\\[ \\\\text{b} \\\\]\\n\\n\\\\[ \\\\text{mance of a model on unseen data making use of the IRT} \\\\]\\n\\n\\\\[ \\\\text{able data, permitting better estimates for the performance of} \\\\]\\n\\n\\\\[ \\\\text{samples per scenario, p-IRT will leverage the whole avail-\\n\\n- examples correctly. This approach immediately solves the dimen-\\n\\n\\\\[ \\\\text{difficulty and the abilities required to respond to those exam-\\n\\n\\\\[ \\\\text{tion 4, as our embeddings} \\\\ E \\\\]\\n\\n\\\\[ \\\\text{theory (IRT) representation of examples, detailed in Sec-\\n\\n\\\\[ \\\\text{eas correctly. This approach immediately solves the dimen-\\n\\n\\\\[ \\\\text{example representations are stable. As IRT should represent} \\\\]\\n\\n\\\\[ \\\\text{the IRT model reasonably describes the reality and the ex-\\n\\n\\\\[ \\\\text{extensionality problem, since} \\\\ E \\\\]\\n\\n\\\\[ \\\\text{samples from scenario} \\\\ j \\\\]\\n\\n\\\\[ \\\\text{I}\\\\]\\n\\n\\\\[ \\\\{0, 1\\\\} \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\]\\n\\n\\\\[ \\\\in \\\\"}
{"id": "qAml3FpfhG", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tinyBenchmarks: evaluating LLMs with fewer examples\\n\\nlogistic regression and, under mild conditions, we should have $\\\\hat{\\\\theta}_l \\\\rightarrow \\\\theta_l$ in probability as $|b_I| \\\\rightarrow \\\\infty$ (Fahrmeir & Kaufmann, 1985). We depart from this condition and show that $|b_E[Z_{jl}|Y_{i0}, \\\\ldots, Y_{ik_l}] - E[Z_{jl}|Y_{i0}, \\\\ldots, Y_{ik_l}]| \\\\rightarrow 0$ in probability as $|b_I| \\\\rightarrow \\\\infty$; that is, p-IRT converges in probability to the best approximation of $Z_{jl}$, $E[Z_{jl}|Y_{i0}, \\\\ldots, Y_{ik_l}]$.\\n\\nProposition 4.1. Assuming that (i) $\\\\hat{\\\\theta}_l \\\\rightarrow \\\\theta_l$ in probability as $|b_I| \\\\rightarrow \\\\infty$ and that (ii) the true values of $(\\\\alpha_i, \\\\beta_i)$'s for all $i \\\\in I$ are known and $\\\\sup_i \\\\|\\\\alpha_i\\\\|_2 \\\\leq c$ for a universal constant $c$, we have that $|b_E[Z_{jl}|Y_{i0}, \\\\ldots, Y_{ik_l}] - E[Z_{jl}|Y_{i0}, \\\\ldots, Y_{ik_l}]| \\\\rightarrow 0$ in probability as $|b_I| \\\\rightarrow \\\\infty$.\\n\\nWe note two limitations of p-IRT that can hinder its effectiveness in practice. First, it does not promptly allow sample weighting, limiting its use of anchor points; second, if the predicted probabilities $\\\\hat{p}_{il}$'s are inaccurate, e.g. because of model misspecification, then the performance of p-IRT will deteriorate.\\n\\nThe generalized p-IRT (gp-IRT) estimator. Our final estimator builds upon p-IRT to overcome its limitations. Assume that the estimators in equations 3.1 and 4.3 are obtained as a first step after the collection of examples in $b_I$. The idea is to compute a third estimator $\\\\hat{Z}_{gp-IRT} \\\\equiv \\\\lambda P_i \\\\in b_I w_i Y_{il} + (1 - \\\\lambda) \\\\hat{Z}_{p-IRT} \\\\text{jl}(4.4)$ where $\\\\lambda$ is a number in $[0, 1]$ that is chosen to optimize the performance of that estimator. To choose $\\\\lambda$, we first note that using random sampling (or anchor points) implies low bias but potentially high variance (when $b_I$ is small) for $P_i \\\\in b_I w_i Y_{il}$. As $b_I$ grows, its variance decreases. On the other hand, conditional on the training set, the variance of $\\\\hat{Z}_{p-IRT}$ is small, especially when $b_\\\\theta_l$ is fitted with data from many scenarios, but its bias can be high when the IRT model is misspecified and does not vanish with the growing sample size. Thus, good choice of $\\\\lambda$ increases with $b_I$.\\n\\nWe choose $\\\\lambda$ based on a heuristic derived from Song (1988)'s Corollary 2. It tells us that the optimal linear combination of any two estimators $\\\\hat{T}_1$ and $\\\\hat{T}_2$ (when the sum of the weights is one) depends on the biases, variances, and covariance of the two estimators. If the first estimator is unbiased and the variance of the second is zero, we can show that the optimal estimator is $\\\\lambda \\\\hat{T}_1 + (1 - \\\\lambda) \\\\hat{T}_2$, where $\\\\lambda = b_2^2 / (b_2^2 + v_1)$, $b_2$ denotes $\\\\hat{T}_2$'s bias, and $v_1$ denotes $\\\\hat{T}_1$'s variance. To apply this result, we assume that the main factors that might prevent gp-IRT from being a good estimator are the variance of the first estimator and the bias of the second one. Then we approximate the first estimator's bias and the second estimator's variance by zero. When our first estimator is obtained by random sampling we take $\\\\lambda = \\\\hat{b}_2 \\\\hat{\\\\sigma}_2^2 / |b_I| + \\\\hat{b}_2^2$ for two constants $\\\\hat{\\\\sigma}_2^2$ and $\\\\hat{b}_2$. The first constant, $\\\\hat{\\\\sigma}_2^2$ is obtained by computing the average sample variance of $Y_{il}, i \\\\in I_j$, across LLMs in the training set. The second constant, $\\\\hat{b}_2$ is obtained by approximating the IRT bias. We (i) split the training set into two subsets of LLMs; (ii) fit an IRT model in the first part using data from all scenarios; (iii) fit the ability parameter for all the LLMs in the second part using half of the examples of all scenarios; (iv) use that IRT model to predict the correctness (using predicted probabilities) of the unseen examples of scenario $j$ for the models in the second split; (v) average predictions and actual correctness within models, obtaining predicted/actual scenarios scores; (vi) compute their absolute differences, obtaining individual error estimates for models; (vii) average between models, obtaining a final bias estimate, and then square the final number. To give some intuition on how $\\\\lambda$ is assigned, Figure 2 depicts $\\\\lambda$ as a function of $\\\\hat{b}_2$ and $|b_I|$ when $\\\\hat{\\\\sigma}_2^2 = 0.01$. From that figure, we see that if the IRT model bias is small, more weight will be given to p-IRT. The curves are steeper when $|b_I|$ is small because the variance of the first estimator decreases faster when $|b_I|$ is small. When the first estimator is obtained by a method that implies an estimator with smaller variance, e.g., anchor points, we apply the same formula but divide $\\\\hat{\\\\sigma}_2^2$ by a constant $> 1$. By default, we divide $\\\\hat{\\\\sigma}_2^2$ by $4$ which is equivalent to halving the standard deviation of the first estimator.\\n\\n4.3. Using IRT when $Y_{il} \\\\in \\\\{0, 1\\\\}$ but $Y_{il} \\\\in [0, 1]$. There are situations in which $Y_{il} \\\\in \\\\{0, 1\\\\}$ but $Y_{il} \\\\in [0, 1]$. For example, in AlpacaEval 2.0, the response variable is bounded and can be translated to the interval $[0, 1]$. Also, some scenarios of HELM and the Open LLM Leaderboard have scores in $[0, 1]$. We propose a simple and effective fix. The idea behind our method is to binarize $Y_{il}$ by defining a\"}"}
