{"id": "aitchison23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 11: Atari-10 linear regression models for each game in the ALE.\\n\\n| Game     | c1     | c2     | c3     | c4     | c5     | c6     | c7     | c8     | c9     | c10    |\\n|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\\n| Alien    | -1.411 | 0.457  | 0.224  | -0.024 | 0.336  | 0.512  | 0.337  | -0.006 | -0.354 | 0.245  |\\n| Amidar   | 0.000  | 1.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  |\\n| Assault  | -0.094 | 0.124  | -0.167 | 0.152  | 0.070  | -0.463 | -0.237 | 0.290  | 1.069  | 0.195  |\\n| Asterix  | 0.246  | 0.209  | -0.063 | 0.354  | 0.110  | -0.324 | -1.084 | -0.186 | 0.898  | 0.732  |\\n| Asteroids| -3.862 | -0.270 | 0.260  | -0.454 | 0.606  | 0.998  | 0.900  | 0.197  | -0.881 | 0.708  |\\n| Atlantis | 2.692  | 0.128  | -0.352 | 0.180  | -0.043 | -0.413 | -0.209 | 0.015  | 0.405  | 0.202  |\\n| Bankheist| 0.675  | -0.072 | 0.165  | 0.293  | -0.714 | -0.476 | 0.238  | 0.199  | 0.360  | 0.265  |\\n| Battlezone| 0.000 | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 1.000  | 0.000  | 0.000  | 0.000  |\\n| Beamrider| -0.606 | 0.146  | -0.011 | 0.059  | -0.187 | 0.199  | -0.287 | -0.021 | 0.875  | 0.308  |\\n| Berzerk  | -1.226 | 0.343  | -0.289 | -0.211 | 0.519  | -0.297 | 0.590  | -0.097 | 0.570  | 0.371  |\\n| Bowling  | 0.000  | 0.000  | 1.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  |\\n| Boxing   | 1.983  | 0.067  | -0.067 | 0.079  | -0.158 | -0.216 | -0.181 | 0.378  | 0.217  | 0.085  |\\n| Breakout | 1.858  | 0.339  | -0.264 | 0.090  | -0.229 | -0.106 | -0.626 | -0.079 | 0.992  | 0.108  |\\n| Centipede| -3.331 | -0.319 | 0.412  | -0.216 | 0.956  | -0.241 | 0.763  | -0.199 | 0.978  | 0.492  |\\n| Choppercommand| -3.172 | 0.415 | -0.136 | -0.621 | 1.290 | 0.548 | 1.185 | -0.118 | -0.159 | 0.064 | -0.027 |\\n| Crazyclimber| 1.689 | 0.382 | -0.153 | 0.085 | -0.120 | 0.152 | -0.353 | -0.049 | 0.194 | 0.027 | 0.261 |\\n| Defender | 0.160  | -0.295 | -0.080 | -0.104 | 0.141  | 0.694  | 0.876  | 0.312  | -1.328 | 0.562  |\\n| Demonattack| 1.261 | 0.137 | -0.124 | 0.051 | 0.219 | 0.059 | -0.633 | 0.005 | 0.474 | 0.294 | 0.304 |\\n| Doubledunk| 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 1.000  | 0.000  | 0.000  | 0.000  |\\n| Enduro   | 2.754  | 0.412  | -0.137 | 0.809  | -0.718 | -0.384 | -0.092 | -0.017 | 0.503  | -0.884 | 0.412 |\\n| Fishingderby| 1.736 | 0.098 | -0.038 | 0.041 | -0.154 | 0.018 | -0.141 | 0.106 | 0.137 | 0.084 | 0.054 |\\n| Freeway  | 3.306  | -0.284 | -0.419 | 0.338  | -1.026 | -1.388 | 1.052  | -0.102 | 1.292  | -0.196 | -0.068 |\\n| Frostbite| 0.000  | 0.000  | 0.000  | 1.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  |\\n| Gopher   | 0.717  | -0.030 | -0.297 | 0.228  | 0.632  | -0.626 | -0.680 | -0.169 | 1.380  | 0.107  | 0.273 |\\n| Gravitar | -1.119 | -0.132 | 0.272  | 0.080  | 0.368  | 0.182  | 1.054  | 0.229  | -1.010 | 0.072  |\\n| Hero     | 1.319  | 0.255  | 0.042  | 0.051  | -0.237 | 0.440  | -0.209 | -0.010 | -0.332 | 0.171  |\\n| Icehockey| -0.103 | -0.174 | -0.087 | -0.130 | 0.028  | 0.022  | 0.675  | 0.208  | 0.382  | 0.013  | -0.066 |\\n| Jamesbond| -0.386 | -0.222 | -0.565 | 0.132  | 0.817  | -0.212 | 0.916  | 0.116  | 0.380  | 0.009  | -0.107 |\\n| Kangaroo| 1.989  | 0.658  | 0.088  | 0.306  | 0.132  | -0.179 | -0.625 | -0.075 | 0.235  | -0.076 | -0.143 |\\n| Krull    | 0.858  | 0.043  | 0.163  | 0.392  | 0.016  | -0.588 | 0.251  | 0.171  | 0.355  | 0.030  | 0.084  |\\n| Kungfumaster| 0.000 | 0.000 | 0.000 | 0.000 | 1.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 |\\n| Montezumarevenge| 0.012 | -0.465 | 0.244 | 0.338 | 0.069 | 0.189 | 0.922 | -0.043 | -1.737 | 0.226 | 0.581 |\\n| Ms pacman| -0.492 | 0.150  | 0.255  | 0.031  | 0.103  | 0.363  | 0.323  | 0.007  | 0.072  | -0.026 | -0.047 |\\n| Namethisgame| 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 1.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 |\\n| Phoenix  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 1.000  | 0.000  |\\n| Pitfall  | 0.523  | 0.270  | 0.606  | 0.170  | 0.033  | 0.710  | -0.198 | 0.086  | -1.748 | 0.238  |\\n| Pong     | 1.846  | 0.191  | -0.012 | 0.092  | -0.190 | 0.163  | -0.276 | 0.133  | -0.115 | 0.058  |\\n| Privateeye| 0.063 | 0.067 | 0.889 | -0.111 | -0.042 | 1.267 | 0.919 | 0.112 | -2.498 | 0.015 | 0.124 |\\n| Qbert    | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 1.000  |\\n| Riverraid| 0.000  | 0.000  | 0.000  | 0.000  | 0.000  | 1.000  | 0.000  | 0.000  | 0.000  | 0.000  |\\n| Roadrunner| 1.603 | 0.024 | -0.218 | 0.329 | -0.302 | -0.221 | -0.046 | 0.079 | 0.558 | 0.076 | 0.170 |\\n| Robotank| 1.923  | 0.187  | -0.308 | 0.050  | -0.184 | -0.276 | 0.147  | 0.093  | 0.844  | -0.161 | -0.148 |\\n| Seaquest | -1.653 | 0.450  | 0.214  | -0.055 | -0.608 | 1.048  | 1.125  | -0.016 | -0.360 | -0.061 | 0.047  |\\n| Skiing   | 1.881  | 0.124  | 0.103  | -0.442 | -0.382 | -0.298 | 0.183  | 0.276  | 0.207  | -0.176 | 0.125  |\\n| Solaris  | -0.063 | -0.084 | 0.642  | -0.172 | 0.331  | 0.213  | -0.049 | 0.453  | -1.175 | 0.130  |\\n| Spaceinvaders| -0.066 | 0.502 | -0.173 | -0.152 | 0.455 | -0.141 | -0.434 | -0.016 | 0.596 | 0.381 | -0.006 |\\n| Stargunner| 1.104 | 0.150 | -0.328 | -0.182 | -0.271 | 0.114 | 0.326 | -0.126 | 0.593 | 0.316 | 0.065 |\\n| Surround | 0."}
{"id": "aitchison23a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Game         | Random Av. | Human Av. |\\n|--------------|------------|-----------|\\n| Alien        | 227.75     | 7127.7    |\\n| Amidar       | 5.77       | 1719.5    |\\n| Assault      | 222.39     | 742.0     |\\n| Asterix      | 210.0      | 8503.3    |\\n| Asteroids    | 719.1      | 47388.7   |\\n| Atlantis     | 12850.0    | 29028.1   |\\n| Bank Heist   | 14.2       | 753.1     |\\n| Battle Zone  | 2360.0     | 37187.5   |\\n| Beam Rider   | 363.88     | 16926.5   |\\n| Berzerk      | 123.65     | 2630.4    |\\n| Bowling      | 23.11      | 160.7     |\\n| Boxing       | 0.05       | 12.1      |\\n| Breakout     | 1.72       | 30.5      |\\n| Centipede    | 2090.87    | 12017.0   |\\n| Chopper Command | 811.0     | 7387.8    |\\n| Crazy Climber| 10780.5    | 35829.4   |\\n| Defender     | 2874.5     | 18688.9   |\\n| Demon Attack | 152.07     | 1971.0    |\\n| Double D Dunk| -18.55     | -16.4     |\\n| Enduro       | 0.0        | 860.5     |\\n| Fishing Derby| -91.71     | -38.7     |\\n| Freeway      | 0.01       | 29.6      |\\n| Frostbite    | 65.2       | 4334.7    |\\n| Gopher       | 257.6      | 2412.5    |\\n| Gravitar     | 173.0      | 3351.4    |\\n| Hero         | 1026.97    | 30826.4   |\\n| Ice Hockey   | -11.15     | 0.9       |\\n| James Bond   | 29.0       | 302.8     |\\n| Kangaroo     | 52.0       | 3035.0    |\\n| Krull        | 1598.05    | 2665.5    |\\n| Kung Fu Master | 258.5   | 22736.3   |\\n| Montezuma Revenge | 0.0 | 4753.3 |\\n| Ms Pacman    | 307.3      | 6951.6    |\\n| Name This Game | 2292.35  | 8049.0    |\\n| Phoenix      | 761.4      | 7242.6    |\\n| Pitfall      | -229.44    | 6463.7    |\\n| Pong         | -20.71     | 14.6      |\\n| Private Eye  | 24.94      | 69571.3   |\\n| Qbert        | 163.88     | 13455.0   |\\n| Riverraid    | 1338.5     | 17118.0   |\\n| Road Runner  | 11.5       | 7845.0    |\\n| Robotank     | 2.16       | 11.9      |\\n| Seaquest     | 68.4       | 42054.7   |\\n| Skiing       | -17098.09  | -4336.9   |\\n| Solaris      | 1236.3     | 12326.7   |\\n| Space Invaders| 148.3     | 1668.7    |\\n| Star Gunner  | 664.0      | 10250.0   |\\n| Surround     | -9.99      | 6.53      |\\n| Tennis       | -23.84     | -8.3      |\\n| Time Pilot   | 3568.0     | 5229.2    |\\n| Tutankham    | 11.43      | 167.6     |\\n| Up n Down    | 533.4      | 11693.2   |\\n| Venture      | 0.0        | 1187.5    |\\n| Video Pinball| 0.0        | 17667.9   |\\n| Wizard of Wor| 563.5      | 4756.5    |\\n| Yars Revenge | 3092.91    | 54576.9   |\\n| Zaxxon       | 32.5       | 9173.3    |\"}"}
{"id": "aitchison23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atari-5: Distilling the Arcade Learning Environment down to Five Games\\n\\n5 Subsets were evaluated by fitting linear regression models to the data using the log normalised scores of games within the subset to predict the median overall, selecting the model with the lowest 10-fold cross-validated mean-squared-error. For these models, we disabled intercepts as we wanted a random policy to produce a score of 0.\\n\\nTo make the best use of the 57 games available, we carefully ordered the subset searches to maximize the performance of the subset size we believed would be most commonly used (5) while maintaining the property that smaller subsets are true subsets of their larger counterparts. All subsets of five games were evaluated, with the best subset selected as Atari-5. The Atari-3 subset was then selected as the best triple-game subset of the Atari-5 subset and Atari-1 as the best single-game subset of Atari-3. Doing this ensures that only five games are included in the test set and also that evaluations on Atari-1 or Atari-3 can more easily be upgraded to Atari-5 if required. For the validation subsets, we selected the best three games, excluding the games used in Atari-5, as the validation set. We then extended this set to 5 games by choosing the best two additional games. Finally, the ten-game Atari-10 set was generated by selecting the best five additional games, not in Atari-5 nor Atari-5-Val. Searching over all subsets took approximately 1-hour on a 12-core machine.\\n\\nNo GPU resources were used.\\n\\n4.4. Prediction over All Game Scores\\n\\nTo better understand how much information our subsets capture about the 57 game dataset, we trained 57 linear regression models for the best five-game and ten-game subsets, predicting log-normalised scores for each game within the 57-game set. We then evaluated each of the 57 models according to their $R^2$, allowing us to identify well or poorly-captured environments by the subset. These models also enable score predictions for any game within the 57-game set.\\n\\n5. Results\\n\\nIn this section, we present the results of our subset search on the ALE dataset using the Atari-57 median score as the target metric. We found diminishing returns after five games, however, we do also provide models for a larger ten-game set if additional precision is required.\\n\\nALE environments are typically normalised such that a policy that takes actions uniformly at random scores 0. Introducing an intercept into our model would lead to a scenario where a non-zero median score estimate is generated, even when all game scores are normalised to zero.\\n\\nBy running multiple regressions in parallel we were able to perform approximately 1,000 regressions per second.\\n\\n5.1. Single Environment Performance\\n\\nWe found a significant difference in the ability of games within the ALE benchmark suite to predict median score estimates (Figure 2). For single-game subsets, Zaxxon performed best with $R^2 = 0.903$, and Surround the worst with $R^2 = 0.002$. The environment Name This Game had the interesting property that its intercept was close to 0 (-0.03) and its scale close to 1 (1.01). As a result, normalised scores from Name This Game provide relatively good estimates for the median performance of an algorithm, by themselves, without modification.\\n\\nWe also note that several games have close to no predictive power when used by themselves. In the case of Pong, Surround, and Tennis, this is most likely due to a low score cap, which most algorithms easily achieve. Games within the hard exploration subset also performed poorly at predicting the median score due to many algorithms achieving no better than random performance.\\n\\n5.2. Optimal Subsets\\n\\nAs a baseline reference, we include results for the 7-game set used in the DQN paper (Mnih et al., 2015). The result demonstrates the importance of game selection in that our single-game model outperforms the 7-game baseline set used by the DQN paper. Table 2 gives the results for the top-performing regression model for each subset size. Coefficients are provided in Appendix E.\\n\\nWe also approximated the expected relative error of the median score, using $\\\\log_e(10)$ times the mean absolute error in log space, which, for small residuals, is a good approximation of the relative error.\\n\\nWe found that Atari-5 is generally within 10% of the true median score. In Table 3, we give the performance of popular algorithms on the Atari-5 benchmark.\\n\\n5.3. Robustness to Environmental Settings\\n\\nTo assess the impact of significantly different environmental settings, we used Atari-5 to evaluate the algorithm Go-Explore (Ecoffet et al., 2021), which was not included in our training set. Go-Explore uses 400,000 frame time limits for each game, rather than the 108,000 used by the algorithms in our dataset. For many games, this difference fundamentally changes the skills the game requires and, importantly, the maximum possible score. For example, in the Space Invaders-like game Demon Attack, a shorter time limit requires an agent to take risks to score points more quickly. In contrast, an extended time limit emphasises staying alive instead. Given the differences, we were surprised to find...\"}"}
{"id": "aitchison23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atari-5: Distilling the Arcade Learning Environment down to Five Games\\n\\nSurround Solaris Freeway Double Dunk Boxing Pong Venture Private Eye Tennis Pitfall Enduro Robotank Bowling Kangaroo Skiing Breakout Montezuma Revenge Atlantis Tutankham Video Pinball Gravitar Crazy Climber Fishing Derby Star Gunner Hero Kung Fu Master Demon Attack QBert Centipede James Bond Ice Hockey Wizard of Wor Gopher Amidar Bank Heist Krull Space Invaders Ms Pacman Defender Asteroids Road Runner Chopper Command Frostbite Seaquest Riverraid Assault Berzerk Up n Down Time Pilot Asterix Beam Rider Yars Revenge Battle Zone Alien Name This Game Phoenix Zaxxon\\n\\nFigure 2. Not all games are good predictors of median score in the ALE. Games in the ALE benchmark sorted from least predictive (left) to most predictive (right), when used as the sole predictor for Atari-5 median performance.\\n\\nTable 2. Performance, and included games, for each of the best models for each subset size.\\n\\n| Name Games                  | R\u00b2   | Rel. Err. |\\n|-----------------------------|------|-----------|\\n| Name This Game              | 0.864| 27.4%     |\\n| Battle Zone, Name This Game, Phoenix | 0.976| 13.7%     |\\n| Battle Zone, Double Dunk, Name This Game, Phoenix, Q*Bert | 0.984| 10.4%     |\\n| Amidar, Bowling, Frostbite, Kung Fu Master, River Raid, Battle Zone, Double Dunk, Name This Game, Phoenix, Q*Bert | 0.992| 7.2%      |\\n| Assault, Ms. Pacman, Yar\u2019s Revenge | 0.952| 17.1%     |\\n| Bank Heist, Video Pinball, Assault, Ms. Pacman, Yar\u2019s Revenge | 0.972| 14.3%     |\\n\\nDQN-7 Beam Rider, Breakout, Enduro, Pong, Q*Bert, Seaquest, S. Invaders 0.756 38.9%\\n\\nTable 3. Results for MuZero (Schrittwieser et al., 2020), Agent57 (Badia et al., 2020), Ape-X (Horgan et al., 2018), IQN (Dabney et al., 2018), and Rainbow DQN (Hessel et al., 2018). Atari-5 predictions fall close the true Atari-5 median scores, and the ranking of the algorithms is generally preserved.\\n\\n| Algorithm | Median Atari-5 | Rel. Error |\\n|-----------|----------------|------------|\\n| MuZero    | 2,041          | 2.5%       |\\n| Agent57   | 1,975          | 8.0%       |\\n| Ape-X     | 434            | 9.6%       |\\n| IQN       | 237            | 9.2%       |\\n| Rainbow DQN | 227        | 0.9%       |\\n\\nthat Atari-5 produced an overestimate of only 12.2% (an estimate of 1,620 compared to 1,446). While this result supports the robustness of Atari-5 to provide useful performance information, even when environmental settings have been changed, we still recommend that it not be used in these cases.\\n\\n5.4. Fairness of Atari-5 Scores\\n\\nTo assess the fairness of Atari-5, we split the algorithms with scores for all five games in the Atari-5 dataset into tertiles, ordered by their true median score performance. We looked for differences in accuracy between the groups by comparing the root mean squared (RMS) relative error and looked for bias by comparing the average relative error. No statistically significant difference was found between any pairs using the standard p=0.05 threshold and Wilson\u2019s two-sided T-test.\\n\\n5.5. Predicting All Game Scores\\n\\nWe found that Atari-5 was able to produce surprisingly accurate score estimates for many of the other games in the ALE benchmark, as shown in Figure 4. There were 17 games which showed a high degree of correlation, with R\u00b2 values ranging from 0.756 to 0.984.\"}"}
{"id": "aitchison23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atari-5: Distilling the Arcade Learning Environment down to Five Games\\n\\ngreater than 0.8, and together, the models explain 71.5% of the variance in log-scores within the dataset.\\n\\nInspired by this result, we tested the \u2018vital few, trivial many\u2019 principle (Juran, 2003), by repeating the experiment with the Atari-10 dataset (17.5% of the environments) and found that those games could explain 80.0% of the variance of the full 57-game set. Because of this, in many cases, other performance measures (for example, the number of games with human-level performance) could potentially be approximated using the Atari-10 game set. Model parameters for predicting scores for these games is given in Appendix E.\\n\\nWe also note that while the Atari-5 games predict many of the games well, they perform poorly in Bellemare's hard exploration games (Bellemare et al., 2016).\\n\\n6. Structure Within in the ALE\\n\\nWhile the ALE dataset contains a diverse range of games, there are still some games within the dataset that are highly correlated. Therefore, we assessed the similarity between scores in games in the ALE benchmark by calculating the Pearson Correlation Coefficient (PCC) (Benesty et al., 2009) on the log-transformed normalised game scores for each game pair in the 57-game dataset. We considered a pair with a coefficient above 0.9 to be highly correlated. The most correlated twenty-four pairings are shown in Figure 5, with highly correlated pairs depicted with bold edges. We found that the two most correlated games were Alien and Ms. Pacman, which was not surprising as these games are both Pacman-like games with very similar rules and primarily differ only in their graphical presentation. We also separated games into categories based on their genre and note that games of similar categories are often clustered together.\\n\\nWe also checked to see if any games were negatively correlated and found Skiing scores were moderately negatively correlated with Alien (-0.51), Frostbite (-0.55), Ms. Pacman (-0.51), and River Raid (-0.50). Skiing is unlike most other ALE games in that all score is deferred until the terminal state of the episode and that the locally optimal but globally sub-optimal policy of racing to end ignoring penalty flags is easily learnable. This suggests that Skiing may require novel approaches to solve.\\n\\n7. Case Study\\n\\nOur experimental results show strong results for Atari-5 on the dataset used to fit the model. In addition, K-fold cross-validation scores indicate that our model is not overfitting the training data and suggest the model should generalize to unseen data well. However, changes in algorithmic trends or environmental settings may result in poorer performance when applied to unseen data. For this reason, we performed a case study where we applied Atari-5 retroactively to a recent paper not included in our training set. We then asked the following question. If this paper had used Atari-5 instead of the full ALE, would the results have changed materially?\\n\\nWe set up our experiment as follows. First, we searched for accepted papers submitted to AAAI-22 that referenced ALE. We then selected the first paper, which contained results for no less than 40 games and which included results for the five games in the Atari-5 benchmark. The first paper we found meeting this requirement was Conjugated Discrete Distributions for Distributional Reinforcement Learning (Lindenberg et al., 2022), which contains results for C51, IQN, and Rainbow, as well as their own algorithm C2D.\\n\\nWe defined two evaluation metrics, which are relative error on predicted median score as compared to the true median score, and the inversion count between the order of the algorithms under the median score, as compared to the order under Atari-5. We used the arXiv preprint version of the paper, which included the supplementary material containing the required results. The results of our study are given in Table 4.\\n\\n| Algorithm | Median | Atari-5 | Rel.Error |\\n|-----------|--------|---------|-----------|\\n| C51       | 109    | 96      | 12.6%     |\\n| IQN       | 129    | 95      | 26.0%     |\\n| C2D       | 133    | 111     | 17.0%     |\\n| Rainbow   | 147    | 118     | 19.8%     |\\n\\nRelative error on these four algorithms was higher than expected (12.6%-26.0%, average of 18.9%). However, we found that Atari-5 scores consistently underpredicted the true results by a similar ratio (\u22480.80). We hypothesise that this may be due to differences in the environmental settings (evidenced by Rainbow DQN scoring 147 on their experiments compared to 223 in our dataset). Since it is often useful to measure the performance of an algorithm relative to some baseline, we also considered the scores normalised to the best performing algorithm, Rainbow DQN. We present the results in Table 5. Because Atari-5 is consistent in its under prediction, with this adjustment, relative error for all models fell below 10% (3.5% - 9.0%, 11)\\n\\nTheir results for C51, IQN, and Rainbow do not match those in our training set. This is not unexpected as results on these algorithms are highly dependent on both the implementation details and the environmental settings.\"}"}
{"id": "aitchison23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atari-5: Distilling the Arcade Learning Environment down to Five Games\\n\\nSolaris\\nVenture\\nFreeway\\nTennis\\nKangaroo\\nSkiing\\nPong\\nPitfall\\nVideo Pinball\\nRobotank\\nEnduro\\nSurround\\nBowling\\nPrivate Eye\\nTutankham\\nKung Fu Master\\nMontezuma Revenge\\nAtlantis\\nJames Bond\\nBreakout\\nGravitar\\nHero\\nAsteroids\\nDemon Attack\\nCrazy Climber\\nGopher\\nBank Heist\\nRoad Runner\\nBoxing\\nCentipede\\nBerzerk\\nIce Hockey\\nSeaquest\\nFrostbite\\nDefender\\nKrull\\nFishing Derby\\nSpace Invaders\\nStar Gunner\\nAmidar\\nChopper Command\\nTime Pilot\\nMs Pacman\\nWizard of Wor\\nRiverraid\\nUp n Down\\nAsterix\\nYars Revenge\\nAssault\\nBeam Rider\\nAlien\\nZaxxon\\n\\nFigure 4. The five games selected in Atari-5 are able to predict accurate scores for many of the other games within the ALE benchmark. However, hard exploration games are less well predicted by the subset.\\n\\naverage of 6.7%). We note only one inversion in the order: the swapping of C51, and IQN.\\n\\nTable 5. Median and Atari-5 scores normalised to the Rainbow result. These results indicate the relative performance of each algorithm as compared to Rainbow.\\n\\n| Algorithm  | Median Atari-5 | Rel.Error |\\n|------------|----------------|-----------|\\n| C51        | 0.74           | 9.0%      |\\n| IQN        | 0.87           | 7.7%      |\\n| C2D        | 0.90           | 3.5%      |\\n| Rainbow    | 1.00           | 0.0%      |\\n\\nGiven these results, we conclude that, had Atari-5 been used instead of the full ALE dataset, while the median score would have varied substantially (18.9%), the relative performance differences would have been similar (6.7%). This degree of error was not significant enough to change the paper's outcome, which was that C2D outperformed both C51 and IQN and underperformed Rainbow. We, therefore, conclude that had Atari-5 been used, the results would not have changed materially.\\n\\n8. Discussion\\n\\nIn this section, we outline the recommended use of the Atari-5 benchmark, discuss its primary value and limitations, and consider the broader impact of our work. We intend Atari-3-Val to be used for hyperparameter tuning and algorithm development and Atari-5 to be used to generate evaluation results. In some cases, researchers might want to include results for specific games that demonstrate a strength or weakness of their algorithm while also providing a measure of general performance in the form of Atari-5. Because Atari-5 and median scores differ in their bias, it is recommended to always compare Atari-5 scores with Atari-5 scores, and not against median scores directly. For this reason, we have provided Atari-5 scores for common algorithms in Table 3.\\n\\nAtari-5 provides three key points of value. First, it captures a significant portion of the information gained from a full ALE evaluation using one-tenth of the evaluations. Second, it provides a standard subset to ALE, selected using a principled process. Third, it provides backwards compatibility with prior work allowing results on previous algorithms to be generated after the fact, so long as scores for the five games were included.\\n\\n8.1. Limitations\\n\\nAlgorithms included in our training dataset are very diverse and include neural network and non-neural network approaches. Because of this, Atari-5 generates results robust to algorithmic and training choices, such as how many frames an algorithm was trained for. However, any changes made to the environment, for example, changing the time-limit, would affect the result, as they modify the optimal policy and potentially the maximum score. We also note that stochasticity in the form of the probability of repeating actions is an environmental change but that our dataset contains both stochastic and deterministic variants. This does not seem to make a significant difference to the prediction accuracy.\\n\\nWe also note that Atari-5 does not do well at incorporating hard-exploration games (see Figure 4). We, therefore, recommend, for algorithms where this matters, to run individual tests for these games. Researchers could then establish their algorithm's general performance via Atari-5, while also demonstrating a specific ability on hard-exploration problems by giving individual results on those games.\\n\\n8.2. Potential Negative Societal Impacts\\n\\nAny new benchmark will invariably advantage some algorithms while disadvantaging others. We identified poor or biased performance on lower-scoring algorithms, and the dominance of the current trend of deep neural networks, as potential areas of concern. We addressed these by including a diverse range of algorithms in our dataset and using...\"}"}
{"id": "aitchison23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atari-5: Distilling the Arcade Learning Environment down to Five Games\\n\\nMatthew Aitchison\\nPenny Sweetser\\nMarcus Hutter\\n\\n1. Introduction\\n\\nThe Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become the gold standard for evaluating the performance of reinforcement learning (RL) algorithms on complex discrete control tasks. Since its release in 2013, the benchmark has gained thousands of citations and almost all state-of-the-art RL algorithms have featured it in their work (Schrittwieser et al., 2020; Kapturowski et al., 2018; Horgan et al., 2018; Hessel et al., 2018; Gruslys et al., 2018; Mnih et al., 2015). However, results generated from the full benchmark have typically been limited to a few large research groups.\\n\\nWe posit that the cost of evaluation on the full dataset is, for many, not feasible. Additionally, the verification of superiority claims through statistical tests on multiple seeds remains rare, likely, due to the high cost. While technological improvements have reduced computation costs, this decrease has been greatly outpaced by an increase in the scale at which RL algorithms are run. For example, the number of frames used for training to produce state-of-the-art results on ALE increased by more than one-thousand times in the seven years between 2015's Deep Q-learning model (Mnih et al., 2015) and the more recent Agent57 (Badia et al., 2020). Training modern machine learning models can also produce non-trivial amounts of carbon emissions (Strubell et al., 2019), for which we provide some analysis in Appendix A. Previous works have dealt with this challenge by self-selecting ad hoc subsets of games. However, this approach adds bias and makes comparison between works more challenging.\\n\\nIn this paper, we outline a principled approach to selecting subsets of environments from an RL benchmark suite. We show that, when carefully chosen, a surprisingly small subset of ALE can capture most of the useful information of a full run. We present a new benchmark, Atari-5, which produces scores that correlate very closely to median score estimates on the full dataset, but at less than one-tenth the cost. We hope that this new benchmark will allow more researchers to participate in this important field of research, speed up the development of novel algorithms through faster iteration, and make the replication of results in RL more feasible. Our primary contributions are as follows. First, a methodology for selecting representative subsets of multi-environment RL benchmarks according to a target summary score. Second, the introduction of the Atari-5 benchmark. Finally, evidence demonstrating the high degree of correlation between scores for many games within ALE.\\n\\n2. Background and Related Work\\n\\nThe Arcade Learning Environment (ALE).\\n\\nDespite deceptively simple graphics, ALE (Bellemare et al., 2013) provides a challenging set of environments for RL algorithms. Much of this challenge stems from the fact that, unlike most other RL benchmarks, ALE contains games spanning multiple genres, including sport, shooter, maze and action games. This is important because being able to achieve goals in a wide range of environments has often\"}"}
{"id": "aitchison23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atari-5: Distilling the Arcade Learning Environment down to Five Games\\n\\nFigure 1. The five games found to be optimal in the Atari-5 Subset. From left to right, Battle Zone, Double Dunk, Name this Game, Phoenix, and Q*bert.\\n\\nbeen suggested as a useful definition of machine intelligence (Legg & Hutter, 2007). In addition, unlike many other RL benchmarks, the environments within ALE were designed explicitly for human play, rather than machine play. As such, ALE includes games that are challenging for machines but for which a (human) learnable solution exists. Human reference scores also provide a means by which 'good' scores can be quantified.\\n\\nALE Subsets. Many researchers make use of ALE subsets when presenting results. However, decisions about which games to include have varied from paper to paper. The Deep Q-learning Network (DQN) paper (Mnih et al., 2015) used a seven-game subset, while the Asynchronous Advantage Actor-Critic (A3C) paper (Mnih et al., 2016) used only five of these games. A useful taxonomy was introduced by Bellemare et al. (Bellemare et al., 2016), which includes the often-cited hard exploration subset. A more comprehensive list of papers and the subsets they used is given in Appendix B, highlighting that there is currently no standard ALE subset. The key difference in our work is that our selection of games has been chosen by a principled approach to be representative of the dataset as a whole. To our knowledge, this is the first work that investigates the selection and weighting of environments within an RL benchmark suite.\\n\\nComputationally Feasible Research. As has been pointed out, the computational requirements for generating results on ALE can be excessive (Ceron & Castro, 2021). There have been several attempts to reduce this cost, including optimizations to the simulator codebase, and a GPU implementation of the environments (Dalton et al., 2020). However, as these changes reduce the cost of simulating the environment and not the much higher cost of training a policy, they result in only minor improvements. Asynchronous parallel training (Horgan et al., 2018) partially addresses this by making better use of parallel hardware but still requires access to large computer clusters to be effective. While these improvements have been helpful, they do not go far enough to make ALE tractable for many researchers.\\n\\nAlternatives to ALE. Since ALE's introduction, many other RL benchmarks have been put forward (Cobbe et al., 2020; Kempka et al., 2016; Beattie et al., 2016; Nichol et al., 2018). However, ALE still dominates research, and at the time of publishing, ALE has more than double the citations of these other benchmarks combined (see Table 1). Minatar (Young & Tian, 2019) addresses some of ALE's issues by creating a new benchmark inspired by Atari that presents agents with objects rather than pixels. This simplification speeds up training but requires previous algorithms to be trained on the new environments and lacks the vision aspect of the task, an important part of ALE's challenge.\\n\\nTable 1. Popular discrete-action vision-based benchmarks in RL by citation. Citations are as of March 2022.\\n\\n| Benchmark                  | Citations |\\n|---------------------------|-----------|\\n| ALE (Bellemare et al., 2013) | 2,295     |\\n| Vizdoom (Kempka et al., 2016) | 610       |\\n| DeepMind Lab (Beattie et al., 2016) | 380       |\\n| Procgen (Cobbe et al., 2020) | 166       |\\n| Gym Retro (Nichol et al., 2018) | 119       |\\n\\n3. Summary Scores on Subsets of Benchmark Suites\\n\\nWhen working with a benchmark made up of multiple environments it is often useful to distil the performance of an algorithm, over all environments, down to a single 'summary score' (Bellemare et al., 2013). If this summary score uses scores from only a subset of the environments then the time taken to generate a summary score can be reduced. Ideally this subset summary score would:\\n\\n1. Preserve order.\\n2. Provide useful information about the performance of the algorithm.\\n3. Minimize the number of environments needed to be evaluated.\"}"}
{"id": "aitchison23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atari-5: Distilling the Arcade Learning Environment down to Five Games\\n\\nWe formalize these desiderata as follows. Let $M = \\\\{\\\\mu_i | i \\\\in [1..n]\\\\}$ be an ordered set of $n$ unique environments that make up the benchmark. For some algorithm $a$ we write the empirical score on the $i$-th environment as $a_i$. Then, for this set of environments, we say that for a pair of algorithms $a, b$, we have that $a \\\\geq b$ if and only if $a_i \\\\geq b_i \\\\forall i \\\\in [1..n]$ and that $a > b$ if there also exists some $i$, such that $a_i > b_i$. We also define a summary score $\\\\Phi_M$ on scores from benchmark $M$ as a function $\\\\Phi_M: \\\\mathbb{R}^n \\\\rightarrow \\\\mathbb{R}$ reducing a score vector $\\\\langle a_1, a_2, ..., a_n \\\\rangle$ taken from benchmark $M$, to a scalar indicating performance. For convenience, we write the summary of individual scores for an algorithm, $\\\\Phi_M(\\\\langle a_1, a_2, ..., a_n \\\\rangle)$ as $\\\\Phi_M(a)$. Finally we define a subset score as a summary score which uses a strict subset of $M$.\\n\\n3.1. Order Preservation\\n\\nIdeally a subset summary score would be strictly order preserving, in that if $a > b$ then $\\\\Phi(a) > \\\\Phi(b)$. Unfortunately this is not possible.\\n\\nProposition 1: No subset score can be strictly increasing.\\n\\nProof. Let $M'$ be the strict subset of environments used by $\\\\Phi_M$, then select $z \\\\in [1..n]$ where $\\\\mu_z \\\\in M \\\\setminus M'$. Now consider two algorithms $a, b$, where $a_i = b_i \\\\forall i \\\\neq z$, and $a_z > b_z$. Hence we have that $a > b$, but not $\\\\Phi(a) > \\\\Phi(b)$.\\n\\nTherefore, the next best thing we can do is to require that our subset score be at least weakly order preserving, that is, if $a > b$ we cannot have $\\\\Phi_M(a) < \\\\Phi_M(b)$.\\n\\nProposition 2: Any summary score of the form $\\\\Phi_M(a) = \\\\sum_{i=1}^{n} c_i a_i$ with $c_i \\\\in \\\\mathbb{R} \\\\geq 0$, is weakly increasing (proof is given in Appendix C).\\n\\nWe therefore find that linear combination of scores, if constrained to non-negative weights, is weakly order preserving. We note that this is not the case for more sophisticated non-linear models, such as deep neural networks, which might reverse the order of algorithms. We also note that any monotonically increasing function $\\\\phi: \\\\mathbb{R} \\\\rightarrow \\\\mathbb{R}$ can be applied to the individual game scores, which we will take advantage of by using a log transform. We can therefore guarantee that if algorithm $a$ is strictly better than algorithm $b$, it can be no worse when measured using a weighted subset score, which is, as shown, the best we can do.\\n\\n3.2. Provide Useful Information\\n\\nAs shown, a weighted subset produces a monotonic measure of an algorithm's performance. However, the question of which subset to use and how to weigh the scores remains. A trivial solution would be to weigh each score by 0, which would be fast to generate and weakly order-preserving but completely uninformative. Therefore, we propose that individual scores should be weighted such that the summary score provides meaningful information about the performance of the algorithm. For many benchmarks, meaningful summary performance measures have often already been established. For example, with ALE, the most common summary score is median score performance over a set of 57 games. Therefore, we argue that the prediction of an established summary score (target score) provides a sensible strategy for selecting and weighting the environments within the suite. That is, to find the subset of environments that best predicts the target score and weigh them according to their linear regression coefficients.\\n\\n3.3. Procedure\\n\\nWe therefore propose that using linear regression models to predict a meaningful target summary score provides a good solution to the desiderata outlined above, and formalize the procedure. Given $m$ algorithms $a_k$: $k = 1...m$ together with their individual evaluations $s_{ki} \\\\in \\\\mathbb{R}$ on environments $i \\\\in \\\\{1 : n\\\\}$ and summary score $t_k \\\\in \\\\mathbb{R}$, e.g. $t_k = s_{k1} + \\\\ldots + s_{kd}$ or the average or the median. The \\\"training\\\" data set is hence $D = \\\\{(s_{k1}, ..., s_{kn}; t_k) : k = 1...m\\\\}$. Let $I \\\\subseteq \\\\{1 : n\\\\}$ be a subset of games and $s_I := (s_i : i \\\\in I)$ be a corresponding score sub-vector. We want to find a mapping $f^*_I: \\\\mathbb{R}^I \\\\rightarrow \\\\mathbb{R}$ that best predicts $t$ from $s_I$, i.e. formally $f^*_I = \\\\arg \\\\min_{f \\\\in \\\\text{Linear}} \\\\sum_{k=1}^{m} (t_k - f(s_{ki}))^2$. Since for $I = \\\\{1 : d\\\\}$, the best $f$ is perfect and linear, it is natural to consider linear $f$, which leads to an efficiently solvable linear regression problem. We further want to find small $I$ with small error. We hence further minimize over all $I \\\\subseteq \\\\{1 : n\\\\}$ of some fixed size $|I| = C$. The optimal set of games $I^*$ with best linear evaluation function $f^*_I$ based on games in $I^*$ hence are $(I^*, f^*) := \\\\arg \\\\min_{|I| = C} \\\\arg \\\\min_{f \\\\in \\\\text{Linear}} \\\\sum_{k=1}^{m} (t_k - f(s_{ki}))^2$.\\n\\nWe formally describe the procedure in Algorithm 1. Where the outputs are coefficients for equation 1, and form a summary score $\\\\Phi$ that is weakly monotonic and which provides useful information about the algorithm's performance. It is also important to note that the purpose of the regressions here is as a method of feature selection rather than predictive power. We are more interested in which environments are selected by this process than in their exact performance in predicting the target summary score.\"}"}
{"id": "aitchison23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\n\\nBEST\\n\\nSUBSET: Find the best subset of size $C$, according to a target summary score.\\n\\n1: Input $C \\\\in \\\\mathbb{Z}^+$, the output summary score subset size.\\n\\n2: Input $M$ benchmark suite made up $n$ environments $\\\\mu_i$ where $i \\\\in [1..n]$.\\n\\n3: Input $D$ a dataset containing scores for $m$ algorithms, made up of elements $s^k_i$, being the evaluation score, for algorithm $k \\\\in [1..m]$ on environment $i \\\\in [1..n]$.\\n\\n4: Input $\\\\Phi$ REF $(\\\\cdot)$ target summary score, a mapping from $\\\\mathbb{R}^n \\\\rightarrow \\\\mathbb{R}$. (e.g. MEAN, or MEDIAN).\\n\\n5: Input $\\\\text{EVAL}(c, D, y)$ a measure of fit for a regression model with weights $c$. (e.g. (k-fold) mean-squared error)\\n\\n6: Input $\\\\text{LM}(\\\\cdot, \\\\cdot)$ a method that fits a linear regression model (without intercept), under ordinary least squares and outputs the model's coefficients.\\n\\n7: Input $\\\\text{COMB}(a, b)$ a function that returns all combinations of size $a$, from the set $[1..b]$.\\n\\n8: $s_{\\\\text{best}} \\\\leftarrow \\\\infty$\\n\\n9: $I_{\\\\text{best}} \\\\leftarrow \\\\text{NULL}$\\n\\n10: $y \\\\leftarrow \\\\{ \\\\Phi{\\\\text{REF}}(s^k_i) | s^k_i \\\\in D \\\\}$\\n\\n11: for $I$ in $\\\\text{COMB}(C, n)$ do\\n\\n12: $D^I \\\\leftarrow \\\\{ s^k_i \\\\in D | i \\\\in I \\\\}$\\n\\n13: $c \\\\leftarrow \\\\text{LM}(D^I, y)$\\n\\n14: if ANY ($c_i < 0$ for $i \\\\in C$) then\\n\\n15: continue {model would be non-monotonic}\\n\\n16: end if\\n\\n17: $s \\\\leftarrow \\\\text{EVAL}(c, D, y)$\\n\\n18: if $s < s_{\\\\text{best}}$ then\\n\\n19: $s_{\\\\text{best}} \\\\leftarrow s$\\n\\n20: $c_{\\\\text{best}} \\\\leftarrow c$\\n\\n21: $I_{\\\\text{best}} \\\\leftarrow I$\\n\\n22: end if\\n\\n23: end for\\n\\n24: Output $I_{\\\\text{best}}$ {The subset}\\n\\n25: Output $c_{\\\\text{best}}$ {The coefficients}\\n\\n4. Application to ALE\\n\\nThis section provides details for the application of our method to the popular ALE benchmark, but note that this method could also be applied to other benchmark suites such as the Procgen (Cobbe et al., 2020), MuJoCo (Todorov et al., 2012), or even benchmarks outside of RL.\\n\\n4.1. Data and Processing\\n\\nWe used the website paperswithcode as the primary source of data for our experiments. This website contains scores for algorithms with published results on various benchmarks, including ALE. The dataset was then supplemented with additional results from papers not included on the website. Additions included various algorithms, such as evolutionary algorithms and shallow neural networks. A full list of algorithms and their sources contained within the dataset can be found in Appendix D.\\n\\nWe removed any algorithms that had results for fewer than 40 of the 57 games in the standard ALE dataset on the grounds that a reasonable median estimate could not be obtained. This reduced our dataset of 116 algorithms down to 62. When fitting models for subsets, any algorithm missing a score for a required game was excluded for that subset.\\n\\nWe also excluded any games with results for fewer than 40 of the remaining 62 algorithms. The only game meeting this criteria was Surround, which we removed from the dataset.\\n\\nAll scores were normalised using the standard approach from (Bellemare et al., 2013)\\n\\n$$Z^i(x) := 100 \\\\times \\\\frac{x^i - r^i}{h^i - r^i} \\\\quad \\\\text{(2)}$$\\n\\nwhere $r^i$ and $h^i$ are the human and random scores for environment $i$ respectively, which we include in Appendix 12. We found that, even after normalisation, scores differed across games by many orders of magnitude and produced non-normal residuals. We, therefore, applied the following transform\\n\\n$$\\\\phi(x) := \\\\log_{10}(1 + \\\\max(0, x)) \\\\quad \\\\text{(3)}$$\\n\\n$$\\\\phi^{-1}(x) = 10^{x-1} \\\\quad \\\\text{(4)}$$\\n\\nto the normalised scores, producing log normalised scores. Clipping was required as algorithms occasionally produce scores slightly worse than random, generating a negative normalised score. Both input and target variables were transformed using $\\\\phi$, and all loss measures were calculated in transformed space. This transformation also has the effect that larger scoring games would not dominate the loss, which we discuss in further detail in Section 8.2.\\n\\n4.2. Predictive Ability of Individual Games\\n\\nTo better understand the games within ALE, we produced single-game linear models where the score from a single environment was used to predict the median Atari-57 score. We then ranked games according to their predictive ability as measured by their $R^2$. Due to a small parameter count ($n=2$), cross-validation was not used.\\n\\n4.3. Subset Search\\n\\nTo assess the best selection of games for each subset, we evaluated all subsets of the ALE games of size five three or\"}"}
{"id": "aitchison23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atari-5: Distilling the Arcade Learning Environment down to Five Games\\n\\nThe scores $s_i$ are the normalised scores for the appropriate game using normalisation constants taken from Table 12. Not all models make use of the intercept constant $c$.\\n\\n### F. Normalisation Constants\\n\\nFor completeness we include the average human reference scores used for normalisation in Table 12 which have been taken from (Badia et al., 2020). Note, these differ from the professional human scores given by (Mnih et al., 2015).\\n\\n### G. Log Residuals as an Approximation to Relative Error\\n\\nHere we show that log residuals are approximations to the relative error. For a residual using a biased log transform given by\\n\\n$$\\n\\\\delta = \\\\log(\\\\epsilon + \\\\hat{y}) - \\\\log(\\\\epsilon + y)\\n$$\\n\\n(12)\\n\\nwhere $y$ is the true value, and $\\\\hat{y}$ the prediction, with $y, \\\\hat{y} \\\\in \\\\mathbb{R} \\\\geq 0$ and $\\\\epsilon \\\\in \\\\mathbb{R} > 0$ we have\\n\\n$$\\n\\\\delta = \\\\log(\\\\epsilon + \\\\hat{y}) - \\\\log(\\\\epsilon + y)\\n$$\\n\\n(13)\\n\\n$$\\n\\\\delta = \\\\log(\\\\frac{\\\\epsilon + \\\\hat{y}}{\\\\epsilon + y})\\n$$\\n\\n(14)\\n\\n$$\\n\\\\delta \\\\approx \\\\frac{\\\\epsilon + \\\\hat{y}}{\\\\epsilon + y} - 1\\n$$\\n\\n(for $\\\\epsilon + \\\\hat{y} \\\\approx \\\\epsilon + y$) (16)\\n\\n$$\\n\\\\delta = \\\\hat{y} - y \\\\epsilon + y\\n$$\\n\\n(17)\\n\\nwhich, for $\\\\epsilon \\\\ll y$, approximates the relative error. When using logarithms other than the natural logarithm, the log residual $\\\\delta$ must first be multiplied by the natural log of the base used.\"}"}
{"id": "aitchison23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm                  | Games | Median Score |\\n|----------------------------|-------|--------------|\\n| MuZero                     | 57    | 2041.1       |\\n| Agent57                    | 57    | 1975.8       |\\n| R2D2                       | 57    | 1926.7       |\\n| SEED (8 TPU v3 cores)      | 57    | 1846.5       |\\n| R2D2 (Retrace)             | 57    | 1342.8       |\\n| NGU (32-0)                 | 57    | 1208.1       |\\n| GDI-H3 (200M frames)       | 56    | 1030.9       |\\n| MuZero (Res2 Adam)         | 57    | 1010.6       |\\n| GDI-I3                     | 50    | 868.0        |\\n| NGU (32-0.3)               | 57    | 568.8        |\\n| LASER No Sweep (200M)      | 57    | 454.9        |\\n| Ape-X                      | 57    | 434.1        |\\n| LASER Shared (200M)        | 57    | 413.7        |\\n| LASER Shared (50M)         | 57    | 365.0        |\\n| IQN                        | 57    | 237.8        |\\n| Rainbow (noop)             | 56    | 227.0        |\\n| DreamerV2                  | 55    | 214.7        |\\n| QR-DQN-1                   | 57    | 210.7        |\\n| QR-DQN-0                   | 57    | 199.2        |\\n| Prior+Duel noop            | 50    | 194.8        |\\n| IMPALA (deep)              | 57    | 191.8        |\\n| Reactor                    | 57    | 186.8        |\\n| Planning                   | 50    | 183.3        |\\n| Reactor ND                 | 57    | 180.4        |\\n| C51                        | 57    | 177.7        |\\n| NoisyNet-Dueling           | 54    | 174.4        |\\n| Bootstrapped DQN           | 49    | 153.4        |\\n| DDQN+Pop-Art noop          | 49    | 147.4        |\\n| Distrib DQN (noop)         | 56    | 146.7        |\\n| Duel DDQN (noop)           | 56    | 142.3        |\\n| A2C + SIL                  | 49    | 140.0        |\\n| Prior noop                 | 49    | 140.0        |\\n| Priority DDQN (noop)       | 56    | 136.6        |\\n| A3C LSTM hs                | 50    | 133.2        |\\n| Duel hs                    | 50    | 131.4        |\\n| Prior+Duel hs              | 52    | 128.8        |\\n| A3C FF hs                  | 50    | 120.0        |\\n| Prior hs                   | 50    | 118.0        |\\n| Persistent AL              | 57    | 116.5        |\\n| DDQN (noop)                | 56    | 115.2        |\\n| Noisy DQN (noop)           | 56    | 114.9        |\\n| Advantage Learning         | 57    | 114.3        |\\n| DDQN (tuned) hs            | 50    | 114.3        |\\n| Human Ref                  | 57    | 100.0        |\\n| Adam Huber                 | 54    | 97.8         |\\n| Adam MSE                   | 54    | 92.5         |\\n| Bellman                    | 57    | 87.7         |\\n| NEAT Object                | 56    | 87.2         |\\n| POP3D                      | 49    | 83.6         |\\n| Nature DQN                 | 48    | 82.7         |\\n| DQN (noop)                 | 56    | 77.6         |\\n| CMA-ES Object              | 56    | 77.5         |\\n| H-NEAT Object              | 56    | 76.7         |\\n| DQN hs                     | 50    | 70.7         |\\n| CNE Object                 | 56    | 70.5         |\\n| NEAT Noise                 | 56    | 69.2         |\\n| A3C FF (1 day) hs          | 50    | 68.2         |\\n| Gorila                     | 49    | 68.1         |\\n| CNE Noise                  | 56    | 58.0         |\\n| RMSProp Huber              | 54    | 57.3         |\\n| ES FF (1 hour) noop        | 45    | 55.8         |\\n| CGP                        | 56    | 53.0         |\"}"}
{"id": "aitchison23a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset Games                                      | Parameters          |\\n|---------------------------------------------------|---------------------|\\n| Atari-1: BattleZone, NameThisGame                 | 0.9976              |\\n| Atari-3: BattleZone, NameThisGame, Phoenix        | 0.3706, 0.5133, 0.1015 |\\n| Atari-5: BattleZone, DoubleDunk, NameThisGame, Phoenix, Qbert | 0.3820, 0.0679, 0.3108, 0.1241, 0.0805 |\\n| Atari-10: Amidar, Bowling, Frostbite, KungFuMaster, RiverRaid | 0.0825, 0.0559, 0.0691, 0.0986, 0.0486 |\\n| BattleZone, DoubleDunk, NameThisGame, Qbert        | 0.1888, 0.0852, 0.1287, 0.1643, 0.0592 |\\n| Atari-3-Val: Assault, MsPacMan, YarsRevenge       | 0.3353, 0.4236, 0.1916 |\\n| Atari-5-Val: BankHeist, VideoPinball, Assault, MsPacman, YarsRevenge | 0.1072, 0.0959, 0.2234, 0.2943, 0.2239 |\"}"}
{"id": "aitchison23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Game       | c 1   | c 2   | c 3   | c 4   | c 5   | c 6   |\\n|------------|-------|-------|-------|-------|-------|-------|\\n| Alien      | -0.807| 0.717 | -0.106| 0.362 | 0.195 | 0.100 |\\n| Amidar     | -0.180| 0.426 | -0.013| 0.289 | -0.160| 0.471 |\\n| Assault    | -0.559| 0.015 | 0.389 | 0.669 | 0.191 | 0.204 |\\n| Asterix    | -0.449| -0.478| -0.149| 0.897 | 0.665 | 0.356 |\\n| Asteroids  | -2.150| 0.671 | 0.114 | -0.214| 0.791 | -0.043|\\n| Atlantis   | 1.860 | -0.097| 0.134 | -0.016| 0.245 | 0.346 |\\n| Bankheist  | -0.342| 0.378 | 0.095 | 0.447 | -0.128| 0.334 |\\n| Battlezone | 0.000 | 1.000 | 0.000 | 0.000 | 0.000 | 0.000 |\\n| Beamrider  | -1.113| -0.152| -0.045| 1.149 | 0.259 | 0.100 |\\n| Berzerk    | -0.556| 0.472 | 0.065 | -0.058| 0.568 | -0.038|\\n| Bowling    | 0.772 | 0.848 | -0.180| 0.574 | -0.415| -0.301|\\n| Boxing     | 1.581 | -0.138| 0.396 | 0.088 | 0.023 | 0.050 |\\n| Breakout   | 0.998 | -0.574| -0.017| 0.873 | 0.134 | 0.398 |\\n| Centipede  | -1.313| 1.013 | -0.161| 0.642 | 0.550 | -0.599|\\n| Choppercommand | -0.988| 1.051| 0.065 | -0.461| 0.595 | 0.114 |\\n| Crazyclimber | 1.105| -0.208| -0.048| 0.425 | 0.015 | 0.472 |\\n| Defender   | 0.584 | 0.486 | 0.234 | -0.818| 0.699 | 0.095 |\\n| Demonattack | 1.052| -0.380| 0.076 | 0.449 | 0.392 | 0.338 |\\n| Doubledunk | 0.000 | 0.000 | 1.000 | 0.000 | 0.000 | 0.000 |\\n| Enduro     | 0.100 | 0.833 | -0.064| 0.918 | -1.163| 0.575 |\\n| Fishingderby | 1.412| -0.119| 0.094 | 0.224 | 0.030 | 0.119 |\\n| Freeway    | 1.297 | 0.713 | 0.020 | 0.125 | -0.439| -0.060|\\n| Frostbite  | -1.273| 0.927 | -0.229| 1.166 | -0.327| 0.073 |\\n| Gopher     | 0.597 | -0.226| 0.035 | 0.665 | 0.301 | 0.221 |\\n| Gravitar   | -0.408| 1.351 | 0.133 | -0.561| -0.035| -0.025|\\n| Hero       | 0.927 | -0.118| -0.107| 0.286 | 0.070 | 0.277 |\\n| Icehockey  | 0.152 | 0.363 | 0.234 | 0.197 | 0.092 | -0.105|\\n| Jamesbond  | -0.133| 0.881 | 0.320 | -0.226| 0.440 | -0.139|\\n| Kangaroo   | 1.482 | 0.209 | -0.089| 0.546 | -0.245| 0.090 |\\n| Krull      | 0.201 | 0.999 | 0.194 | 0.205 | -0.126| -0.020|\\n| Kungfumaster | 1.339| 0.270| 0.049 | -0.055| 0.182 | -0.022|\\n| Montezumarevenge | -0.070| 1.256| -0.178| -1.161| 0.118| 0.316|\\n| Mspacman   | -0.118| 0.561 | -0.113| 0.684 | -0.137| -0.012|\\n| Namethisgame | 0.000| 0.000| 0.000| 1.000| 0.000| 0.000|\\n| Phoenix    | 0.000 | 0.000 | 0.000 | 0.000 | 1.000 | 0.000|\\n| Pitfall    | 0.891 | 0.453 | -0.188| -0.391| -0.038| 0.171 |\\n| Pong       | 1.401 | -0.168| 0.085 | 0.200 | -0.015| 0.128 |\\n| Privateeye | 1.016 | 1.326 | -0.226| -0.855| -0.194| -0.042|\\n| Qbert      | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 1.000 |\\n| Riverraid  | 0.125 | -0.167| -0.161| 1.025 | 0.053 | 0.072 |\\n| Roadrunner | 0.385 | 0.180 | 0.093 | 0.600 | -0.039| 0.234 |\\n| Robotank   | 1.262 | 0.036 | 0.178 | 0.490 | -0.118| 0.010 |\\n| Seaquest   | -2.222| 1.123 | -0.235| 0.900 | -0.226| 0.263 |\\n| Skiing     | 1.864 | -0.083| 0.400 | -0.518| -0.163| 0.080 |\\n| Solaris    | 1.019 | 0.435 | 0.376 | -0.844| 0.004 | 0.040 |\\n| Spaceinvaders | 0.171| -0.170| 0.168| 0.120| 0.591| 0.176 |\\n| Stargunner | 0.676 | -0.123| -0.056| 0.380 | 0.423 | 0.235 |\\n| Surround   | 0.686 | -0.290| 0.138 | 0.744 | -0.068| -0.059|\\n| Tennis     | 1.637 | 0.123 | 0.219 | -0.344| 0.059 | 0.140 |\\n| Timepilot  | -0.729| 0.889 | 0.281 | -0.406| 0.688 | -0.068|\\n| Tutankham  | 0.357 | -0.035| 0.067 | 0.776 | -0.181| 0.143 |\\n| Upndown    | -0.351| 0.215 | 0.166 | -0.110| 0.596 | 0.267 |\\n| Venture    | 0.415 | 0.946 | -0.102| -0.008| -0.611| 0.336 |\\n| Videopinball | 1.846| 0.021| 0.114| 0.212| 0.165| 0.052 |\\n| Wizardofwor | 0.104| 0.503| 0.082| 0.229| 0.188| 0.008 |\\n| Yarsrevenge | -0.082| 0.823| 0.074| -0.494| 0.342| 0.121 |\\n| Zaxxon     | -0.145| 0.355 | -0.060| 0.468 | 0.250 | 0.029 |\"}"}
{"id": "aitchison23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atari-5: Distilling the Arcade Learning Environment down to Five Games\\n\\nFigure 5. Structure within the ALE dataset. Edges indicate PCC, colours the games' category.\\n\\n9. Conclusions\\n\\nWe have introduced a new benchmark, Atari-5, that provides median score estimates to within 10% of their true values but using less than one-tenth of the computational resources.\\n\\nA key feature of Atari-5 is that, as a subset of ALE, it can be applied retroactively to prior results, without retraining, so long as scores for all five games are provided. We have also shown that our extended benchmark Atari-10 can generate useful score estimates for all games within the ALE dataset.\\n\\nWe hope this new benchmark can offset the trend of increasingly high computation requirements required to develop and reproduce results on the ALE benchmark and increase RL participation while providing backwards-compatible results to prior work.\\n\\n10. Acknowledgements\\n\\nThis research was supported by an Australian Government Research Training Program (RTP) Scholarship.\\n\\nReferences\\n\\nAytar, Y., Pfaff, T., Budden, D., Paine, T., Wang, Z., and De Freitas, N. Playing hard exploration games by watching youtube. Advances in neural information processing systems, 31, 2018.\\n\\nBadia, A. P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi, A., Guo, Z. D., and Blundell, C. Agent57: Outperforming the atari human benchmark. In International Conference on Machine Learning, pp. 507\u2013517. PMLR, 2020.\\n\\nBeattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., K\u00a8uttler, H., Lefrancq, A., Green, S., Vald\u00b4es, V., Sadik, A., et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.\\n\\nBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016.\\n\\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, jun 2013.\\n\\nBenesty, J., Chen, J., Huang, Y., and Cohen, I. Pearson correlation coefficient. In Noise reduction in speech processing, pp. 1\u20134. Springer, 2009.\\n\\nCeron, J. S. O. and Castro, P. S. Revisiting rainbow: Promoting more insightful and inclusive deep reinforcement learning research. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 1373\u20131383. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/ceron21a.html.\"}"}
{"id": "aitchison23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "aitchison23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atari-5: Distilling the Arcade Learning Environment down to Five Games\\n\\nA. Estimation of Cost to Reproduce for Selected Papers\\n\\nRetroactive estimation of costs and carbon emissions can be difficult (Patterson et al., 2021). Due to missing details in the publications, our estimations required many assumptions about the hardware and time needed to reproduce a result. Our estimates do not include overheads such as air-conditioning, testing, hyper-parameter searches, or ablation studies. Therefore these estimates should be taken as useful lower bound estimates on the cost to reproduce a work. We also note the large difference between preemptive and standard costs which are approximately one-third of the cost. However, preemptive computing may not always be feasible due to congestion near conference deadlines. We took the computing requirements from the papers and used Google Cloud Compute to estimate the cost of reproducing the work.\\n\\nFor some older algorithms, it may be possible to reduce the costs given here by training on more modern hardware (for example, an A100 or TPU). However, if the algorithm is CPU constrained, this could increase, not decrease, the cost due to the underutilization of the device. Therefore we estimate reproduction costs with our best guess at the hardware that would have been used at the time of publication. A summary of the calculations and results is given in Table 6. In addition, we detail the assumptions made for each algorithm in the paragraphs below.\\n\\nTable 6. Estimated cost to reproduce a full evaluation on ALE. These are the estimated costs for a single run, some papers include multiple runs, or ablation studies.\\n\\n| Year | Algorithm | Compute / env. | GPU hours | Cost (preemptive) | Cost (standard) |\\n|------|-----------|----------------|-----------|-------------------|----------------|\\n| 2018 | Rainbow DQN (Hessel et al., 2018) | 10 GPU days | 13,680 | $10,670 USD | $35,759 USD |\\n| 2020 | Agent-57 (Badia et al., 2020) | 375 hours (est) | 21,375 | $30,673 USD | $110,352 USD |\\n\\nRainbow DQN required 10 days on a single GPU for each of the 57 environments. (Hessel et al., 2018) The hardware used is not mentioned in the paper so we assume a V100, with an E2-standard-4 (16GB) instance, which has a GPU/CPU cost per hour of $2.48/$0.134 (standard), and $0.74/$0.04 (preemptive). We also note that (Ceron & Castro, 2021) also produced estimates for Rainbow DQN, but used 5-days of GPU time instead of 10.\\n\\nAgent-57 trains at \u2248 260 environment steps per second per actor with 256 actors (Badia et al., 2020). To generate the 90B frames needed would then take 375 hours. The paper states that a single GPU is used, and for the purposes of our estimate, we assume that this also requires 375 hours. The hardware used is not stated, so we assume an A100, which is the best available GPU at the time of their publication. We assume a 64 CPU machine (N2-highcpu-64) would be needed as our observation is that Atari environments typically run at 1,000 environment steps per CPU. The GPU/CPU cost per hour is $2.9333/$2.2294 (standard) and $0.88/$0.555 (preemptive).\\n\\nPower Usage and Carbon Emissions\\n\\nWe estimate the power required to reproduce the Agent-57 result as follows. A single A100 uses up to 400W. (Heller et al., 2018) We use 215W as the estimated power consumption for the CPU, which is taken from the Xeon Phi 7210 specifications. (Heller et al., 2018) We do not include estimates for air-conditioning, power supply inefficiency, or other components such as RAM, networking, and storage. The use of TDP is only an approximate measure as devices frequently run under the TDP limit and may also occasionally run over it too. Using the estimates above, the total power required to reproduce a result would be 13.166mWh. Because our compute estimations were based on Google Cloud Compute servers in the U.S., we used the 2019 U.S. carbon emissions rate of 0.92 pounds per kWh. (Heller et al., 2018) We note that this could be higher in developing countries. This equates to approximately 6.05 tones of CO2 per 57-game run.\\n\\n12 https://cloud.google.com/compute/gpus-pricing\\n13 Smaller instances might not be feasible due to RAM requirements.\\n14 Even though Agent-57 uses a distributed training/roll-out architecture, we believe this to be reasonable based on the assumption that actors and learners are limited in how out of sync they can become.\\n15 NVIDIA lists two variants at https://www.nvidia.com/en-au/data-center/a100/ one with a 400W max TDP and one with 250W. Cards likely run below their maximum power limits. However, this is partly balanced out by power supply inefficiencies.\\n16 https://ark.intel.com/content/www/us/en/ark/products/94033/intel-xeon-phi-processor-7210-16gb-1-30-ghz-64-core.html\\n17 https://www.eia.gov/tools/faqs/faq.php?id=74&t=11\"}"}
{"id": "aitchison23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atari-5: Distilling the Arcade Learning Environment down to Five Games\\n\\nB. Subsets Used by Other Papers\\n\\nTable 7. Subsets of Atari used by various works.\\n\\n| Paper                  | Games                          |\\n|------------------------|-------------------------------|\\n| DQN (Mnih et al., 2015) | Beam Rider, Breakout, Enduro, Pong, Q*bert, Seaquest, Space Invaders. |\\n| A3C (Mnih et al., 2016) | Beam Rider, Breakout, Pong, Q*bert, Space Invaders. |\\n| Unifying Count-Based Exploration (Hard Exploration) (Bellemare et al., 2016) | Freeway, Gravitar, Montezuma's Revenge, Pitfall!, Private Eye, Solaris, Venture. |\\n| (Aytar et al., 2018)    | Playing hard exploration games (Hard Exploration) Montezuma's Revenge, Pitfall, Private Eye. |\\n| Agent57 (Challenging Set) (Badia et al., 2020) | Freeway, Gravitar, Montezuma's Revenge, Pitfall! |\\n| Compress and Control (Veness et al., 2015) | Q*bert, Pong, Freeway. |\\n\\nC. Proof of Proposition 2\\n\\nWe prove the more general case of Proposition 2 using some fixed (weakly) increasing $\\\\phi: \\\\mathbb{R} \\\\rightarrow \\\\mathbb{R}$, where Proposition 2 can be recovered by selecting $\\\\phi(x) = x$.\\n\\nConsider two algorithms $a, b$, and a set of scores for each of $n$ environments $a_1 \\\\ldots a_n$, $b_1 \\\\ldots b_n$, where $a_i$ is the score on the $i$-th environment for algorithm $a$, and $b_i$ is the score on the $i$-th environment for algorithm $b$. Given some (weakly) increasing function $\\\\phi: \\\\mathbb{R} \\\\rightarrow \\\\mathbb{R}$, and constants $c_i \\\\in \\\\mathbb{R} \\\\geq 0$, we have that $a_i \\\\geq b_i \\\\forall i \\\\in [1..n]$ (5) $\\\\Rightarrow \\\\phi(a_i) \\\\geq \\\\phi(b_i)$ (\\\\phi is weakly increasing) (6) $\\\\Rightarrow c_i \\\\phi(a_i) \\\\geq c_i \\\\phi(b_i)$ (for $c_i \\\\geq 0$) (7) (8) Since this holds for all $i \\\\in [1..n]$ we have $\\\\sum_{i=1}^{n} c_i \\\\phi(a_i) \\\\geq \\\\sum_{i=1}^{n} c_i \\\\phi(b_i)$ (9) as required.\\n\\nD. Algorithms Included in the Dataset\\n\\nWe included in our dataset only algorithms with results on 40 or more ALE games. They are listed in Table 8, along with how many games each algorithm had results for. Games that are not part of the 57-game canonical set were not included. The names are provided from PapersWithCode, and some algorithms appear multiple times as they included multiple configurations.\\n\\nE. Models for Individual Games\\n\\nWe provide coefficients for each of the Atari-X and Atari-Val-X models in Table 9, as well as models for predicting scores for all games within the 57-game ALE dataset (Table 10, 11). An estimated normalised game score ($s_{est}$) can be calculated using the following formula.\"}"}
