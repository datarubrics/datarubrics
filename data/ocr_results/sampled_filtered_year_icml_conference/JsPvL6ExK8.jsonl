{"id": "JsPvL6ExK8", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6.\\nPrometheus-T environment settings.\\n\\n| VELLOCITY | HRR |\\n|-----------|-----|\\n| 5 MW      | 1   |\\n| 10 MW     | 2   |\\n| 15 MW     | 3   |\\n| 20 MW     | 4   |\\n| 25 MW     | 5   |\\n\\nTable 7.\\nPrometheus-P environment settings.\\n\\n| VELLOCITY | HRR |\\n|-----------|-----|\\n| 2 M/s     | 1   |\\n| 4 M/s     | 2   |\\n| 6 M/s     | 3   |\\n| 8 M/s     | 4   |\\n| 10 M/s    | 5   |\\n\\nE.2. Weatherbench\\nBasic Information Description.\\nWeatherBench is a dataset for weather forecasting and climate model benchmarking. It consists of meteorological variables that are extracted from the European Center for Medium-Range Weather Forecasts (ECMWF) reanalysis data. This dataset is specifically designed for machine learning and artificial intelligence applications in weather prediction.\\n\\nEnvironmental Settings.\\nIn our study, we analyze four key variables (Lin et al., 2022): temperature, humidity, cloud cover, and wind speed.\"}"}
{"id": "JsPvL6ExK8", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE\\n\\nFigure 7. Tunnel and pool geometries.\\n\\nCover, and surface wind components. We focus on accurately predicting temperature and surface wind components, treating the other variables as elements that constitute the predictive environment. Specifically, when predicting temperature, we construct three different environmental variable combinations: humidity and cloud cover, humidity and wind component, and cloud cover and wind component, represented as \\\\( \\\\{c_1, c_2, c_3\\\\} \\\\). Similarly, in predicting the surface wind component, we consider combinations of temperature and humidity, temperature and cloud cover, and humidity and cloud cover, denoted as \\\\( \\\\{d_1, d_2, d_3\\\\} \\\\). This approach allows us to better understand the interplay among the variables, thereby enhancing the accuracy of our predictions.\\n\\nE.3. Navier-Stokes Equation\\n\\nBasic Information Description.\\n\\nThe dataset is generated by employing a pseudospectral method to solve the 2D Navier-Stokes equation for a viscous and incompressible flow. The equation, expressed in its vorticity form, is as follows (Li et al., 2021):\\n\\n\\\\[\\n\\\\begin{align*}\\n    \\\\frac{\\\\partial w(x, t)}{\\\\partial t} + u(x, t) \\\\cdot \\\\nabla w(x, t) &= \\\\nu \\\\Delta w(x, t) + f(x), x \\\\in (0, 1)^2, t \\\\in (0, T] \\\\\\\\\\n    \\\\nabla \\\\cdot u(x, t) &= 0, x \\\\in (0, 1)^2, t \\\\in [0, T] \\\\\\\\\\n    w(x, 0) &= w_0(x), x \\\\in (0, 1)^2.\\n\\\\end{align*}\\n\\\\]\\n\\n(31)\\n\\nHere, the forcing term is set to \\\\( f(x) = 0 \\\\). The boundary condition applied is periodic. The viscosity coefficient \\\\( \\\\nu \\\\) is set at \\\\( 10^{-3}, 10^{-4}, 10^{-5} \\\\), ensuring sufficient chaos in the solution evolution over time.\\n\\nEnvironmental Settings.\\n\\nThe Navier-Stokes equation demonstrates its versatility in three different settings, defined by combinations of viscosity coefficients. Specifically, the combinations are \\\\( \\\\{10^{-3}, 10^{-4}\\\\} \\\\), \\\\( \\\\{10^{-3}, 10^{-5}\\\\} \\\\), and \\\\( \\\\{10^{-4}, 10^{-5}\\\\} \\\\), each corresponding to a unique environmental setup. Thus, we represent these settings as the set \\\\( \\\\{e_1, e_2, e_3\\\\} \\\\), where each element reflects a specific combination of viscosity coefficients.\\n\\nF. More Showcases\\n\\nPrometheus-T.\\n\\nThe Prometheus dataset comprises two distinct scenarios, named Prometheus-T and Prometheus-P. Focusing first on Prometheus-T, we have visualized two key physical fields: temperature and smoke concentration. Additionally, we have selected the runner-up model, LSM, for comparison. This is illustrated in Figures 8. A clear observation from these visualizations is that our model outperforms LSM, particularly in terms of detail and accuracy. In contrast, the results produced by LSM appear notably smoother.\\n\\nPrometheus-P.\\n\\nNext, we will show another set of fire simulation scenarios in benchmark Prometheus-P. We display these...\"}"}
{"id": "JsPvL6ExK8", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"scenarios in a top-down view Figure 9 and show the variables of speed and temperature. We can clearly see the v component and u component of speed, as well as the temperature component. Notably, the DGODE model's predictions match the real situation (Ground-truth) very closely. Compared to other models, they often show overly smooth results, but DGODE shows higher precision in handling details.\\n\\nWeatherbench-T. As shown in the Figure 10, Weatherbench-T is a dataset benchmark for specifically evaluating the prediction of the temperature variable, and for a clearer visualization, we added land information, and the corresponding Error case, for highlighting the differences, and it is still noticeable that we achieved the best results with DGODE, and the visualization of the prediction is consistent with Ground truth.\\n\\nWeatherbench-W. As shown in the Figure 11, Weatherbench-W is a dataset benchmark for specifically evaluating the prediction of the wind speed variable, which mainly contains two directions, x and y. In order to visualize more clearly, we have included land information, as well as the corresponding Error cases, which are used to highlight the differences, and it can still be found that we have achieved the best results for the DGODE effect, and the predicted visualization is consistent with the Ground truth.\"}"}
{"id": "JsPvL6ExK8", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10. Visualization of temperature variables in Weatherbench-T.\\n\\nFigure 11. Visualization of wind ($x$) variables in Weatherbench-W.\"}"}
{"id": "JsPvL6ExK8", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We summarize the overall framework of DGODE in Algorithm 1.\\n\\n**Algorithm 1**\\n\\n*DGODE Framework for OOD Fluid Dynamics Modeling*\\n\\n**Require:**\\n- Historical trajectories $X^H$\\n- Environment $E$\\n\\n**Ensure:**\\n- Future trajectories $X^F$\\n\\n1. Initialize Temporal GNN and Frequency Network\\n2. Initialize Vector Quantization (VQ) Codebook with $K$ environment vectors $\\\\{e_1, \\\\ldots, e_K\\\\}$\\n3. for each node in the graph $G$\\n   4. Extract spatial node features $h_t(l)$\\n   5. Extract frequency-based node features $h_f(i)$\\n   6. Concatenate spatial and frequency-based features to form final node features $h_i$\\n4. for each environment\\n   5. Extract environment features $h_e$ using a separate backbone network\\n   6. Summarize all node features in the environment to form environment representation\\n7. Disentangle node and environment features using mutual information minimization:\\n   8. Apply Vector Quantization (VQ) to each environment feature $h_e$\\n   9. Minimize mutual information between node features $h_i$ and quantized environment representations\\n8. Initialize node states $z_0^i$ and environment states $z_0^e$ from extracted features\\n9. Model the interacting dynamics using environment-aware graph ODEs:\\n   10. Update node states $z_i$ over time using equation Eqn. 9\\n   11. Update environment states $z_e$ over time using equation Eqn. 10\\n10. Define loss functions:\\n   11. Reconstruction loss $L_{\\\\text{err}}$ for the prediction accuracy\\n   12. Codebook loss $L_{\\\\text{cod}}$ for vector quantization\\n   13. Stability loss $L_{\\\\text{sta}}$ for enhancing generalization robustness\\n   14. Mutual information loss $L_{\\\\text{mi}}$ for representation disentanglement\\n12. Train the model by minimizing the total loss $L = L_{\\\\text{err}} + L_{\\\\text{cod}} + L_{\\\\text{sta}} + L_{\\\\text{mi}}$\\n13. Predict future trajectories $X^F$ using the trained model\\n14. Return $X^F$\\n\\n### C. Related Work\\n\\n**C.1. Fluid Dynamics Modeling**\\n\\nLearning-based fluid dynamics modeling (Obiols-Sales et al., 2020; Lienen et al., 2024; Ma et al., 2023; Wu et al., 2023c; Xiong et al., 2023) has received extensive attention in the field of scientific machine learning (Kostic et al., 2024; Chen et al., 2024b; Wang et al., 2023b) with applications ranging from aerospace engineering (Le Clainche et al., 2023) to biomedicine (Zhao et al., 2023). Early efforts focus on leveraging convolutional neural networks (Fang, 2021; Guo et al., 2016) to learn from physical simulations with regular grids. Recent approaches (Pfaff et al., 2021; Shao et al., 2022; Wang et al., 2024) attempt to leverage geometric graphs for finer-level simulation using GNNs, which follow the message passing mechanism to update node representations iteratively. However, these approaches usually focus on next-time prediction (Pfaff et al., 2021; Shao et al., 2022) or auto-regressive prediction (Han et al., 2022b), which is hard to capture underlying continuous dynamics in fluid simulations (Lippe et al., 2023; Ma et al., 2023). Moreover, these approaches usually assume that training and test datasets come from the same distribution. In contrast, this work focuses on out-of-distribution fluid dynamics and proposes a new dataset as well as a benchmark to benefit research on this underexplored problem.\"}"}
{"id": "JsPvL6ExK8", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.2. Neural Ordinary Differential Equations\\n\\nNeural Ordinary Differential Equations (ODEs) (Chen et al., 2018) incorporate ODE into learning-based neural networks, which has received extensive interest in machine learning and computational mathematics. Neural ODEs have been widely used to depict continuous-time dynamical systems with the benefit of learning from irregularly sampled observations (Huang et al., 2020; Zang & Wang, 2020). A range of approaches have been developed to enhance the capacity of neural ODEs such as including regularization terms (Finlay et al., 2020) and dimension augmentation (Dupont et al., 2019). Recently, neural ODEs have been combined with GNNs to solve overfitting issues (Xhonneux et al., 2020) and provide the explainability of models (Zhang et al., 2022). Furthermore, neural ODEs combined with GNNs have solved dynamical system prediction problems. For example, SGODE (Chen et al., 2024a) uses signed graph neural ordinary differential equations for continuous-time dynamics modeling, while HOPE (Luo et al., 2023b) employs second-order graph ODE functions to capture long-term dependencies in complex dynamic systems. In this work, we propose a neural ODE-based framework named DGODE to solve OOD fluid dynamics modeling, which models the state evolution of both node representations and the environment simultaneously.\\n\\nC.3. Out-of-distribution Generalization\\n\\nOut-of-distribution (OOD) generalization (Volpi et al., 2018; Wang et al., 2023a; Wu et al., 2022; Yang et al., 2022) aims to enhance the model performance when training and test data come from different distributions. This problem has been studied in various scenarios such as time-series forecasting (Zhang et al., 2024), image classification (Li et al., 2022a) and graph data mining (Gui et al., 2023). A wide range of approaches adopt invariant learning for OOD generalization (Li et al., 2020; Wang et al., 2021; Li et al., 2018), which generates domain-invariant features in the latent space. In this way, these can delete the influence of spurious correlations resulting from the distribution shift. In addition, causal inference (Wang et al., 2022; Wang et al.), model selection (Lu et al., 2022; Wenzel et al., 2022) and active learning (Zhan et al., 2023; Deng et al., 2023) have also been adopted to enhance OOD performance in practical scenarios. In this work, we study a practical yet underexplored problem of OOD fluid dynamics modeling and propose a new large-scale OOD dataset and an extensive benchmark to facilitate researchers studying this topic.\\n\\nD. Experimental setup details\\n\\nBaselines.\\nWe evaluate DGODE against 12 notable models across three benchmarks, dividing them into four categories:\\n\\n\u2022 Vision backbone networks:\\n  (1). U-Net (Ronneberger et al., 2015) is a deep learning architecture for medical image segmentation. It features a symmetric contracting path and an expansive path for precise localization of areas of interest in images. (2). Swin Transformer (Liu et al., 2021b) is a Transformer-based network architecture mainly used for computer vision tasks. It addresses the issue of varying image scales efficiently through a hierarchical Transformer structure. Earthfarseer (Wu et al., 2023b) is a deep learning model designed for Earth observation data. Its specific network structure enhances monitoring and prediction capabilities for environmental and meteorological phenomena.\\n\\n\u2022 Graph Neural Networks for spatio-temporal modeling:\\n  (1). CLCRN (Lin et al., 2022) is a conditional graph convolutional recurrent neural network for spatio-temporal data processing. It effectively captures spatio-temporal dependencies, suitable for scenarios like traffic flow prediction. (2). MGNT (Pfaff et al., 2021) is a multi-graph neural network focusing on learning complex relationships from multiple graph structures. It excels in areas like physical system modeling. EAGLE (Janny et al., 2023) is a graph neural network for spatio-temporal data, emphasizing effective learning in dynamic environments. It is suitable for fields like weather prediction. DGCRN (Weng et al., 2023) is a decomposed graph convolutional recurrent network. Its decomposition approach enhances the processing of spatio-temporal data, especially effective in traffic and network flow prediction.\\n\\n\u2022 Graph-ODE series:\\n  (1). MPNODE (Gupta et al., 2022) is a model combining graph neural networks and ordinary differential equations (ODE) for modeling dynamic graph data. It captures the temporal evolution characteristics of graph-structured data. (2). CG-ODE (Huang et al., 2021) is a coupled graph ordinary differential equation model. It processes graph structures and continuous time dynamics simultaneously, suitable for modeling complex network systems. (3).SGODE (Chen et al., 2024a) is a method that integrates graph neural networks with ordinary differential equations. It is especially effective for modeling complex system dynamics that involve positive and negative relationships.\"}"}
{"id": "JsPvL6ExK8", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE\\n\\nTable 5.\\nDetails for benchmarks. The input-output shapes are presented in the shape of (temporal, the numbers of nodes, the numbers of features)\\n\\n| DESCRIPTION | PROMETHEUS | TROMETHEUS | WEATHERBENCH | TWEATHERBENCH | NAVIER | STOKES |\\n|-------------|------------|------------|--------------|--------------|--------|--------|\\n| INPUT TENSOR | (50, 15360, 2) | (15, 32768, 3) | (12, 2048, 1) | (12, 2048, 1) | (10, 4096, 1) |\\n| OUTPUT TENSOR | (50, 15360, 2) | (15, 32768, 3) | (12, 2048, 1) | (12, 2048, 1) | (10, 4096, 1) |\\n\\nOperator learning methods:\\n(1). FNO (Li et al., 2021) is an efficient deep learning framework for learning solutions to partial differential equations. It operates in the frequency domain through Fourier transforms, enhancing computational efficiency.\\n(2). F-FNO (Tran et al., 2023) is a variant of FNO that further improves model efficiency and generalization ability through factorization, suitable for a broader range of partial differential equation solutions.\\n(3). LSM (Wu et al., 2023a) combines deep learning with mathematical operator theory for solving complex scientific computation problems, such as simulations in fluid dynamics and material science.\\n\\nTo adapt these models for irregular grids, we incorporate geo-FNO (Li et al., 2022b) for input/output transformation, facilitating conversion of irregular domains into regular grids.\\n\\nImplementation & Evaluation Metrics.\\nTo ensure fairness, all methods train on the NVIDIA-A100 using the ADAM optimizer for MSE loss over 500 epochs, with an initial learning rate of $10^{-3}$. We set the batch size to 20. We study Mean Squared Error (MSE) and Structural Similarity Index Measure (SSIM) in our research. The mathematical formulas for evaluating the indicators in decibels are shown below.\\n\\n$$L_{err} = \\\\frac{1}{N} \\\\sum_{i} \\\\sum_{t} (x_{t}^{i} - \\\\hat{x}_{t}^{i})^2$$ \\\\hfill (23)\\n\\nHere, $L_{err}$ represents the mean squared error, $N$ is the total number of samples (i.e., all combinations of $i$ and $t$), $x_{t}^{i}$ is the true value of the $i$th sample at time $t$, and $\\\\hat{x}_{t}^{i}$ is the corresponding predicted value. This formula calculates the overall mean squared error by summing and averaging the squares of the prediction errors for all samples $i$ at all time points $t$.\\n\\n$$SSIM(x, \\\\hat{x}) = \\\\frac{(2\\\\mu_x \\\\mu_{\\\\hat{x}} + C_1)(2\\\\sigma_{x\\\\hat{x}} + C_2)}{\\\\mu_x^2 + \\\\mu_{\\\\hat{x}}^2 + C_1^2} \\\\frac{(\\\\sigma_x^2 + \\\\sigma_{\\\\hat{x}}^2 + C_2)}{\\\\sigma_x^2 + \\\\sigma_{\\\\hat{x}}^2 + C_2^2}$$ \\\\hfill (24)\\n\\nIn the Structural Similarity Index (SSIM) formula, $x$ and $\\\\hat{x}$ represent the original and reconstructed (or predicted) images, respectively. The terms $\\\\mu_x$ and $\\\\mu_{\\\\hat{x}}$ are the average luminance of the images $x$ and $\\\\hat{x}$, while $\\\\sigma_x^2$ and $\\\\sigma_{\\\\hat{x}}^2$ are the variances of these two images. Additionally, $\\\\sigma_{x\\\\hat{x}}$ denotes the covariance between the images $x$ and $\\\\hat{x}$. To avoid division by zero, the formula includes two small constants $C_1$ and $C_2$, typically related to the dynamic range of the image data. The SSIM value ranges from -1 to 1, where 1 indicates perfect similarity between the two images. This metric is especially important in the field of image processing, particularly for image quality assessment, compression, and transmission, as it closely aligns with human visual perception of image quality. This is beneficial for visually analyzing the modeling of dynamical systems.\\n\\nE. Benchmarks detials\\nIn this section, the details of the benchmark configurations are succinctly summarized in Table 5. The data details of Prometheus are shown in Figure 6. We used a sliding window approach to generate the dataset needed for training tests.\\n\\nE.1. Prometheus\\nFDS relies on a complex set of physical and chemical equations to model fire combustion dynamics. These equations include the Navier-Stokes equations, energy conservation equations, matter conservation equations, chemical reaction equations for combustion, radiative heat transfer models, and solid combustion and pyrolysis models. With these equations, FDS is able to...\"}"}
{"id": "JsPvL6ExK8", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE\\n\\nsimulate fire scenarios under different conditions, including flame propagation, smoke behavior, heat release, and chemical transformations.\\n\\nNavier-Stokes Equations:\\n\\\\[\\n\\\\frac{\\\\partial}{\\\\partial t} (\\\\rho u) + \\\\nabla \\\\cdot (\\\\rho uu) = -\\\\nabla p + \\\\nabla \\\\cdot \\\\mu (\\\\nabla u + (\\\\nabla u)^T) + \\\\rho g\\n\\\\] (25)\\n\\nEnergy Conservation Equation:\\n\\\\[\\n\\\\frac{\\\\partial}{\\\\partial t} (\\\\rho E) + \\\\nabla \\\\cdot (u (\\\\rho E + p)) = \\\\nabla \\\\cdot (\\\\kappa \\\\nabla T) + \\\\Phi\\n\\\\] (26)\\n\\nMass Conservation Equation:\\n\\\\[\\n\\\\frac{\\\\partial \\\\rho}{\\\\partial t} + \\\\nabla \\\\cdot (\\\\rho u) = 0\\n\\\\] (27)\\n\\nCombustion Chemistry Equations:\\n\\\\[\\n\\\\frac{\\\\partial}{\\\\partial t} (\\\\rho Y_i) + \\\\nabla \\\\cdot (\\\\rho Y_i u) = \\\\nabla \\\\cdot (\\\\rho D_i \\\\nabla Y_i) + \\\\dot{\\\\omega}_i\\n\\\\] (28)\\n\\nRadiative Heat Transfer Model:\\n\\\\[\\n\\\\dot{q}_{\\\\text{radiation}} = \\\\sigma \\\\cdot (T^4 - T_{\\\\text{surroundings}}^4)\\n\\\\] (29)\\n\\nSolid Combustion and Pyrolysis Models:\\n\\\\[\\n\\\\frac{\\\\partial}{\\\\partial t} (\\\\rho s T_s) = \\\\nabla \\\\cdot (k_s \\\\nabla T_s) + \\\\dot{q}_{\\\\text{pyrolysis}} + \\\\dot{q}_{\\\\text{combustion}}\\n\\\\] (30)\\n\\nIn the equations used in the FDS, \\\\( \\\\rho \\\\) represents fluid density, typically referring to the density of air or combustion gases. \\\\( u \\\\) is the fluid velocity vector, indicating the direction and speed of fluid movement in space. \\\\( t \\\\) stands for time. \\\\( p \\\\) is the fluid pressure. \\\\( \\\\mu \\\\) denotes the dynamic viscosity of the fluid, related to internal friction. \\\\( \\\\nabla \\\\) is the vector gradient operator, used for calculating spatial changes in a field. \\\\( E \\\\) represents the total energy per unit mass, including both internal and kinetic energy. \\\\( \\\\kappa \\\\) is the thermal conductivity, reflecting the material's ability to conduct heat. \\\\( T \\\\) signifies temperature. \\\\( \\\\Phi \\\\) represents other forms of energy transfer, such as energy from chemical reactions. \\\\( Y_i \\\\) is the mass fraction of the \\\\( i \\\\)th chemical component. \\\\( D_i \\\\) is the diffusion coefficient of that chemical component. \\\\( \\\\dot{\\\\omega}_i \\\\) is the production or consumption rate of the component. \\\\( \\\\sigma \\\\) is the Stefan-Boltzmann constant, related to thermal radiation. \\\\( T_{\\\\text{surroundings}} \\\\) is the surrounding environmental temperature. \\\\( \\\\rho_s \\\\) and \\\\( T_s \\\\) represent the density and temperature of a solid, respectively. \\\\( k_s \\\\) is the thermal conductivity of the solid. \\\\( \\\\dot{q}_{\\\\text{pyrolysis}} \\\\) and \\\\( \\\\dot{q}_{\\\\text{combustion}} \\\\) represent the heat source terms for pyrolysis and combustion processes, respectively. These equations form the foundation of the FDS for simulating fire dynamics, enabling precise modeling of fluid dynamics, heat transfer, chemical reactions, and material transformations in fire scenarios. They also serve as the basis for this study.\\n\\nEnvironmental Settings.\\nIn our study, we use the FDS to simulate tunnel scenarios. As shown in Table 6, 7 to explore fire dynamics and their impact on the environment, we focus on two main conditions: Heat Release Rate (HRR) and ventilation speed. These factors are key in influencing fire behavior and smoke movement. HRR, an important measure of fire intensity, represents the heat released per unit time. We simulate fires at different HRR levels, such as 5 MW, 10 MW, 15 MW, 20 MW, and 25 MW, to understand fire behavior from small to large scales. Ventilation speed plays a vital role in the movement and distribution of smoke in closed or semi-closed tunnel environments. We simulate fire behaviors in tunnels under various ventilation speeds, including 2 m/s, 4 m/s, 6 m/s, 8 m/s, and 10 m/s. We also randomly choose five sets of conditions, adding ventilation parameters, resulting in 30 different environmental settings for tunnel scenarios. For pool fire scenarios, we set up 25 different environments. This in-depth approach to environmental settings allows us to thoroughly assess changes in combustion dynamics under different conditions. This multi-environment simulation method is especially important for the performance of deep learning models in Out-Of-Distribution situations.\"}"}
{"id": "JsPvL6ExK8", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nFluid dynamics modeling has received extensive attention in the machine learning community. Although numerous graph neural network (GNN) approaches have been proposed for this problem, the problem of out-of-distribution (OOD) generalization remains underexplored. In this work, we propose a new large-scale dataset Prometheus which simulates tunnel and pool fires across various environmental conditions and builds an extensive benchmark of 13 baselines, which demonstrates that the OOD generalization performance is far from satisfactory. To tackle this, this paper introduces a new approach named Disentangled Graph ODE (DGODE), which learns disentangled representations for continuous interacting dynamics modeling. In particular, we utilize a temporal GNN and a frequency network to extract semantics from historical trajectories into node representations and environment representations respectively. To mitigate the potential distribution shift, we minimize the mutual information between invariant node representations and the discretized environment features using adversarial learning. Then, they are fed into an environment-aware graph ODE framework, which models the evolution using neighboring nodes and dynamical environmental context. In addition, we enhance the stability of the framework by perturbing the environment features to enhance robustness. Experiments validate the effectiveness of DGODE compared with state-of-the-art approaches.\\n\\n1. Introduction\\n\\nComputational fluid dynamics modeling (Kochkov et al., 2021; Obiols-Sales et al., 2020; Steeven et al., 2024; Li et al., 2021; Wu et al., 2023c; Wu et al.) is an important problem in fluid mechanics, which can facilitate our understanding of fluid flows (Li et al., 2023a; Yu et al., 2024). Recently, a range of machine learning approaches have been developed to solve this problem (Pfaff et al., 2021; Li et al., 2023b; Shao et al., 2022; Sanchez-Gonzalez et al., 2020; Han et al., 2022a; Deng & Hooi, 2021). They usually construct geometric graphs to model the relationships between neighborhood observation points and then adopt graph neural networks (GNNs) to model fluid dynamics, which follow the message passing mechanism (Kipf & Welling, 2017; Veli\u010dkovi\u0107 et al., 2018) by aggregating neighborhood information of each node for interactive updating.\\n\\nAlthough these approaches have achieved some progress in fluid dynamics modeling (Yu et al., 2023; Huang et al., 2023a; Luo et al., 2024), they usually assume that training and test data come from the same distribution (Pfaff et al., 2021; Shao et al., 2022; Sanchez-Gonzalez et al., 2020; Lippe et al., 2023), which is not practical in the real-world analysis since fluid mechanisms always face different environments resulting from system parameters such as temperature, viscosity and pressure (Baradel et al., 2019; Sanchez-Gonzalez et al., 2020; Huang et al., 2023b). Existing machine learning algorithms would suffer from inferior out-of-distribution (OOD) performance (Hendrycks et al., 2021). Therefore, in this work, we study the problem of OOD fluid dynamics modeling, which aims to learn data-driven models generalized well on systems with different parameters. However, there are limited large-scale fluid dynamics datasets and benchmarks for this practical and emerging problem, which could hinder from algorithm research and development.\\n\\nIn this work, we first build a large-scale OOD fluid dynamics dataset Prometheus using extensive fire simulations. In particular, both tunnel and pool fires are modeled using fire dynamics simulators, which solve Navier-Stokes equations in fire scenarios. More importantly, our simulators consider 25 different environments with varying parameters.\"}"}
{"id": "JsPvL6ExK8", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE\\n\\nters including HRR and ventilation speeds, which results in well-designed datasets for OOD generalization evaluation. Totally, we generate 4.8TB of raw data compressed into 340GB and construct the benchmark with 12 popular dynamics modeling baselines.\\n\\nAs a second contribution, we propose an effective solution to this problem. Our motivation is to answer the following two research questions: (1) How to handle distribution shifts from different system parameters for fluid dynamics modeling? Different system parameters could result in distribution shifts across dynamical systems and thus spurious correlations involved in trajectory predictions. Existing OOD generalization problems (Liu et al., 2021a; Gui et al., 2023) usually capture invariant patterns across several environments using environment labels, which can help to remove spurious correlations. However, we would encounter a large variety of different environments, and their labels are usually unavailable for fluid dynamics due to their high complexity. (2) How to model continuous complicated interacting dynamics across different environments? Recent GNN approaches (Pfaff et al., 2021; Shao et al., 2022; Sanchez-Gonzalez et al., 2020) usually leverage the current states for next-time predictions without consideration of the environmental context, which could continuously influence the evolution mechanisms. Moreover, since fluid dynamics is intrinsically continuous, it is highly expected to incorporate environmental context into a unified and continuous trajectory forecasting framework.\\n\\nIn this paper, we propose a novel framework termed Disentangled Graph ODE (DGODE) for OOD fluid dynamics modeling. The core of our DGODE is to learn disentangled representations for capturing continuous dynamics. Concretely, we utilize a temporal GNN and a frequency network, which learn spatio-temporal relationships and frequency semantics from complementary views for node features and environment features. To mitigate the OOD distribution shift across different environments, we aim to minimize the mutual information between invariant node features and environment features. To achieve this, we stratify all environmental features into discrete pieces using vector quantization and minimize the upper bound of mutual information by adversarial learning. To model the continuous complicated interacting dynamics under different environments, we incorporate all the node features and environment features into a graph ODE framework, which models the evolution of latent states of interacting nodes and the environment simultaneously. Here, node states are driven by interacting neighboring nodes and the environment while environment states always evolve to simulate dynamical influence. Finally, a decoder is adopted to output the future trajectories and we also perturb the environment features to enhance the stability of node states w.r.t different environments for generalization robustness. Extensive experiments on various benchmarks showcase the superiority of the DGODE compared to existing SOTAs.\\n\\nThe contribution of this paper can be summarized as follows: (I) We propose a new large-scale dataset and an extensive benchmark of 12 baselines for OOD fluid dynamics modeling. (II) We develop a novel approach named DGODE for this problem. Our DGODE learns disentangled node and environment representations to mitigate the risk of distribution shift and incorporates them into a continuous graph ODE framework to model interacting dynamics across different environments. (III) Extensive experiments validate the effectiveness of the proposed DGODE in comparison to a wide range of approaches. Our Prometheus dataset can be found at the following link: https://huggingface.co/datasets/easylearning/Prometheus.\"}"}
{"id": "JsPvL6ExK8", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE\\n\\n1. Tunnel Design: A 100 m long, 6 m wide and high tunnel, with a focus on 2D flow data at y=3 m, using sensors for temperature and visibility.\\n\\n2. Industrial Park Tanks: Three tanks, each 20 m in diameter, with heights of 10 m, 20 m, and a building of 55 m length, 40 m width, and 30 m height.\\n\\n3. Tunnel Visualization: Displaying temperature and visibility distribution.\\n\\n4. Fire Dynamics: Visualizing velocity and temperature in a pool fire to study its dynamics.\\n\\nWe vary the heat release rate (HRR) and ventilation speed to simulate different fire scenarios, observing smoke stratification, flame spread, and diverse burning patterns. We equip the tunnel with 32 \u00d7 480 sensors to measure temperature and flue gas concentration, creating 25 environmental combinations with varying HRR and ventilation speeds. Additionally, we introduce vent parameters in five settings as disturbances, forming a 30-condition dataset for tunnel fire combustion analysis. These 30 environments are denoted as $a_1, a_2, a_3, ..., a_{30}$.\\n\\nNext, as shown in Figure 1(b), we build a fire model for a tank storage area, our goal is to examine its impact on the surroundings. The model spans a 150 \u00d7 100 m area and includes tanks, buildings, and pool fires. We position the simulated fire source above the leftmost tank, 5 meters from other structures. Our study finely adjusts the HRR and ventilation speed to assess different variables. We focus on analyzing changes in flame contours, temperature, and internal velocity under various HRR conditions, and how ventilation affects flame length and tilt angle. The study also evaluates the thermal impact of a fire on nearby buildings and tanks. We deploy a sensor grid in and around the model area to measure temperature and speed, setting 25 environmental combinations of HRR and ventilation speeds for simulating different fire scenarios. Sensor data helps us understand the temperature distribution and velocity fields in the area during tank fires. We denote these 25 environments as $b_1, b_2, b_3, ..., b_{25}$.\\n\\n2.2. Numerical Simulation\\n\\nIn combustion dynamics modeling, we use FDS (Fire Dynamics Simulator) based on the Navier-Stokes equations to simulate thermal combustion phenomena accurately. These equations form the foundation of fluid dynamics, describing gas and liquid flow dynamics. To simulate fire scenarios better, we adjust and expand the Navier-Stokes equations to consider heat transfer, combustion processes, and smoke propagation. This method generates 4.8TB of raw data with a grid precision of 2 million. We then downsample the data and compress it to 340GB. More details and data processing information are available in the Appendix E.\\n\\n2.3. Physical Field Visualization\\n\\nIn this section, we detail the simulation's 10 physical variables in tunnel and building pool fire scenarios, as illustrated in Figure 6, divided into two categories for in-depth explanation.\\n\\n1. Pool Fires: We analyze pool fires from the front and top views. The front view shows Z-axis velocity ($v_z$), X-axis velocity ($v_x$), and their resultant speed ($v_{speed_1} = p v_z^2 + v_x^2$), along with temperature ($t_{pool_1}$). The top view presents Y-axis velocity ($v_y$), X-axis velocity ($v_x$), their resultant speed ($v_{speed_2} = p v_y^2 + v_x^2$), and temperature ($t_{pool_2}$).\\n\\n2. Tunnel Fires: For tunnel fires, we examine the tunnel's longitudinal section, focusing on temperature ($t_{tunnel}$) and flue gas concentration ($flue_{tunnel}$), to understand their distribution inside the tunnel.\\n\\n3. Methodology\\n\\n3.1. Framework Overview\\n\\nThis work studies the problem of OOD fluid dynamics modeling, which is challenging due to distribution shifts across different environments and complicated interacting dynamics. Here, we propose a new approach named DGODE for this problem, which can learn disentangled representations for continuous dynamics modeling. In particular, we involve a temporal GNN and a frequency network to extract node features and environment features. To tackle the OOD shift, we stratify environments using vector quantization and conduct representation disentanglement by mutual information.\"}"}
{"id": "JsPvL6ExK8", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. An Overview of DGODE. Our DGODE utilizes a temporal GNN and a frequency network to generate node features and environmental features. We adopt vector quantization to discretize environment features, which would be disentangled with node features by mutual information minimization. These features would be fed into an environment-aware graph ODE to model the evolution of dynamical systems. We also minimize the variance of node features across different environments for enhanced generalization capacity.\\n\\n3.2. Learning Disentangled Representations from Historical Trajectories\\n\\nThe major obstacle is that different environments would bring in distribution shift (R\u00e4ma & Sipil\u00e4, 2017; Luo et al., 2023a), which would generate spurious correlations between historical trajectories and future predictions. To tackle this, we introduce representation disentanglement, which decomposes historical trajectories into invariant node representations and environment representations with mutual information minimization. Here, a temporal GNN (Huang et al., 2021) and a frequency network are adopted to generate node features and environment features from complementary views. More importantly, we stratify all the environment features using vector quantization representation and then ensure representation disentanglement by mutual information minimization (Belghazi et al., 2018), which can mitigate the influence of environments on node representations and help to capture invariant patterns in OOD scenarios.\\n\\nFeature Extraction. In detail, we first utilize a temporal GNN to summarize the node representations from historical trajectories, which updates temporal node representations using their related nodes. In formulation, given a temporal node representation $h_{t, (l-1)i}$ at the $l$-th layer, the update rule can be formulated as:\\n\\n$$h_{t, (l-1)i} = \\\\phi_e([h_{t, (l-1)i}, h_{t-1, (l-1)i}, N_{t, (l-1)i}]),$$\\n\\n(1)\\n\\nwhere $N_{t, (l-1)i} = \\\\text{AGG}(h_{t, (l-1)j} | j \\\\in N(i))$ is the neighborhood representation and $\\\\text{AGG}(\\\\cdot)$ is the aggregation operator. After stacking $L$ layers, we summarize all the temporal node representations into spatial node features for downstream forecasting by:\\n\\n$$h_{(s)}i = \\\\text{SUM} \\\\{h_{t, (L)i} | t = 1, 2, ..., T_{\\\\text{obs}}\\\\},$$\\n\\n(2)\\n\\nwhere $\\\\text{SUM}(\\\\cdot)$ denotes the summarization operator. Besides temporal GNNs, we leverage the frequency domain to enhance the representation learning, which can explore the periodic information as a complementary. To be specific, we utilize a Fast Fourier Transform (FFT) (Li et al., 2021) to transform historical data into the frequency domain, and then utilize a feed-forward network (FFN) to feature updates, followed by an inverse Fast Fourier Transform (iFFT). Finally, these reconstructed features would be reshaped frequency-based node features $h_{(f)i}$. In formulation, we have:\\n\\n$$\\\\{h_{(f)i} \\\\}_{i \\\\in V} = \\\\text{Re}(\\\\text{iFFT}(\\\\text{FFN}(\\\\text{FFT}(G_{1:T_{\\\\text{obs}}})))),$$\\n\\n(3)\\n\\nwhere $\\\\text{Re}(\\\\cdot)$ is to convert the reconstructed feature maps into a set of node features. Finally, we concatenate spatial node features and frequency-based node features generated from complementary views into final node features as:\\n\\n$$h_i = [h_{(s)i}, h_{(f)i}].$$\\n\\n(4)\\n\\nTo generate environmental representations, we utilize the same backbone with different parameters for node features.\"}"}
{"id": "JsPvL6ExK8", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"After generating disentangled representations for both node features and network parameters, we minimize Eqn. 7 with respect to network parameters $\\\\theta$ to have a codebook $\\\\{p\\\\}$. To model the minimal-extremal formula is:\\n\\n$$I(h; e) = \\\\min_{\\\\theta} \\\\max_{\\\\psi} \\\\text{E} \\\\left[ \\\\text{E} \\\\left[ h \\\\mid e \\\\right] \\\\right].$$\\n\\nThe quantization (Zhou et al., 2023; Xia et al., 2024; Van Den Oord et al., 2017) can decrease the complexity of our framework, which models the evolution of node states and environment states simultaneously. To tackle this, we stratify all the environments into discrete states by introducing the vector quantization strategy. Here, we have the concatenated vector can be written compactly below:\\n\\n$$\\\\text{E} \\\\left[ h \\\\mid e \\\\right] = \\\\sum_{i} VQ(\\\\text{E} \\\\left[ h \\\\mid e \\\\right]) = \\\\sum_{i} \\\\text{E} \\\\left[ h \\\\mid e \\\\right].$$\\n\\nTo model the evolution of node states and environment states, we aim to develop an effective framework for dynamical system modeling, which is challenging due to complicated continuous interacting dynamics (Chen et al., 2024a; Luo et al., 2024; Huang et al., 2023b; 2022) and environmental influence. To tackle this, we introduce an environment-aware graph ODE framework, which models the evolution of node states and environment states. In this part, we will theoretically show the necessity of incorporating the environment states into the system. Consider the case where node states are from their features, i.e., $z_t^p$. Suppose that the interacting dynamics of node $i$ at time $t$ are target parameters. Denote the degree matrix and the adjacency matrix of the graph with self-loop. Then, the interacting dynamics can be modeled as:\\n\\n$$\\\\text{d}z_t^i = \\\\sum_{j \\\\in N} w_{ij} z_t^j + D\\\\sigma(z_t^i) \\\\cdot z_t^j dt,$$\\n\\nwhere $w_{ij}$ is the fixed weight from adjacency matrix $W$. The dynamics of the nodes and environment states follow:\\n\\n$$\\\\text{d}z_t^i = \\\\sum_{j \\\\in N} w_{ij} z_t^j + D\\\\sigma(z_t^i) \\\\cdot z_t^j dt,$$\\n\\nwhere $D$ and $\\\\sigma$ denote the degree matrix and the activation function. Eqn. 9 models the evolution of hidden states $z_t$ using neighboring nodes and the environment state. Moreover, Eqn. 10 models the evolution of the environment $e_t$ using all the nodes in the system, which can characterize the dynamical influence of environments. In comparison to previous approaches (Huang et al., 2020; Chen et al., 2018), we introduce environment states into the graph to predict the environment trajectories as:\\n\\n$$e_{t+1} = \\\\text{SUM}(e_t + \\\\text{E} \\\\left[ h \\\\mid e \\\\right]),$$\\n\\nwhere $W$ and $z_t$ are the initial node states and environments for our ODE model are from their features, i.e., $z_t^p$. Finally, we can utilize a decoder $\\\\phi$ and in-network parameters $\\\\psi$ to maximize environmental predictive accuracy for accurate prediction. Then, we can estimate the upper bound of mutual information, which can facilitate our mutual information maximization (Huang et al., 2023b) as well as the stability maximization (Huang et al., 2023b) of node features across different environments, we disentangle the representations and environment representations, we aim to thereby promote feature disentanglement. The alternative optimization is conducted in a standard adversarial learning paradigm (Luo et al., 2024) to minimize mutual information, for disentanglement. The theoretical analysis will be provided in the next section.\"}"}
{"id": "JsPvL6ExK8", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We assume that both $z(t)$ and $z_e(t)$ are measured with independent Gaussian errors at finite time points; that is, $\\\\tilde{Z}(t_j) = Z(t_j) + \\\\epsilon_j$, (15)\\n\\nwhere $\\\\epsilon_j \\\\sim N(0, \\\\sigma^2 I)$, and without loss of generality $t_j = j/n$, $j = 1, \\\\ldots, n$.\\n\\nTheorem 3.1. Under model Eqn. 14\u2013Eqn. 15 where $W^*$ and $Z(0)$ both have strictly positive components, if we ignore the environment states $z_e$; that is, we specify a wrong working model:\\n\\n$$d z(t) = W z(t), \\\\quad z(0) = h,$$\\n\\n(16)\\n\\nthen the resulting least square estimator,\\n\\n$$c_W = \\\\arg \\\\min_W \\\\frac{1}{n} \\\\sum_{j=1}^{n} \\\\| \\\\tilde{z}(t_j) - z(t_j; W) \\\\|^2_2,$$\\n\\n(17)\\n\\nis inconsistent to $W^*$ even when $n \\\\to \\\\infty$, where $z(t; W)$ denotes the solution to the ODE Eqn. 16.\\n\\nThe proof of Theorem 3.1 can be found in Appendix A.\\n\\n3.4. Optimization\\n\\nTo optimize the whole framework, we minimize the training error compared with the ground truth as follows:\\n\\n$$L_{err} = - \\\\sum_i \\\\sum_t x(t_i) - \\\\hat{x}(t_i)^2.$$\\n\\n(18)\\n\\nTo optimize the codebook with standard gradient descent, we integrate the stopgradient operator $\\\\text{sg}(\\\\cdot)$ (Van Den Oord et al., 2017), which is an identity operator during forwarding the network and cuts off the gradient computation during back propagation. The loss objective can be written as:\\n\\n$$L_{cod} = ||\\\\text{sg}(e\\\\hat{k}) - he|| + ||e\\\\hat{k} - \\\\text{sg}(he)||,$$\\n\\n(19)\\n\\nwhich can avoid generating trivial solutions. In addition, we enhance the generalization robustness to enhance the stability of node states with respect to different environment vectors as a regularization. Here, we replace $V Q(he)$ with the other $e_k$ in the codebook and minimize the variance of node states:\\n\\n$$L_{sta} = - \\\\sum_k \\\\sum_i \\\\sum_t z(t_i) - \\\\tilde{z}(t; k),$$\\n\\n(20)\\n\\nwhere $\\\\tilde{z}(t; k)$ is generated by replacing $V Q(he)$ with $e_k$. In summary, the whole loss objective can be summarized as:\\n\\n$$L = L_{err} + L_{cod} + L_{sta} + L_{mi}.$$\\n\\n(21)\\n\\nThe whole algorithm of the proposed DGODE is summarized in Algorithm B.\\n\\n### Table 1.\\n\\nDetails include number of nodes (#N), variables (#V), and total environments (#E), with specifics on both seen and unseen environments for each benchmark.\\n\\n| Benchmark          | #N   | #V   | #E      | Seen Environments                                      | Unseen Environments |\\n|--------------------|------|------|---------|--------------------------------------------------------|---------------------|\\n| Prometheus-T       | 15360| 2    | 30      | $\\\\{a_1, a_2, \\\\ldots, a_{25}\\\\}$                       | $\\\\{a_{26}, a_{27}, \\\\ldots, a_{30}\\\\}$ |\\n| Prometheus-P       | 22225| 4    | 25      | $\\\\{b_1, b_2, \\\\ldots, b_{20}\\\\}$                       | $\\\\{b_{21}, b_{22}, \\\\ldots, b_{25}\\\\}$ |\\n| Weatherbench-T    | 2048 | 1    | 3       | $\\\\{c_1, c_2\\\\}$                                        | $\\\\{c_3\\\\}$          |\\n| Weatherbench-W    | 2048 | 1    | 3       | $\\\\{d_1, d_2\\\\}$                                        | $\\\\{d_3\\\\}$          |\\n| Navier-Stokes      | 4096 | 1    | 3       | $\\\\{e_1, e_2\\\\}$                                        | $\\\\{e_3\\\\}$          |\\n\\n4. Experiments\\n\\n4.1. Experimental Settings\\n\\nIn this section, we evaluate DGODE across benchmarks in combustion dynamics, meteorology, and PDEs, categorized as seen and unseen in Table 1. The model is trained in seen environments and tested in unseen ones to evaluate generalization ability.\\n\\nBenchmarks.\\n\\nThe study presents five benchmarks, as shown in Table 5, featuring two of our datasets: Prometheus-T for tunnel fire analysis (temperature and flue gas concentration) and Prometheus-P for pool fire analysis (velocity components, combined speed, and temperature), including 30 tunnel and 25 pool fire cases (details in Subsection 2.1). It also includes the Weatherbench dataset with two benchmarks: Weatherbench-T (temperature) and Weatherbench-W (wind speed) in various environments. The study additionally employs the Navier-Stokes equation, using the method from (Li et al., 2021), for training in two environments. Further details on environments, benchmark scales, and data resolution are in Appendix D.\\n\\nBaselines.\\n\\nWe evaluate DGODE against 13 notable models across three benchmarks, dividing them into four categories:\\n\\n- Visual Backbone Networks. like U-Net (Ronneberger et al., 2015), Swin Transformer (Liu et al., 2021b), and Earthfarseer (Wu et al., 2023b);\\n- Graph Neural Networks for spatio-temporal modeling including CLCRN (Lin et al., 2022), MGNT (Pfaff et al., 2021), EAGLE (Janny et al., 2023), and DGCRN (Weng et al., 2023);\\n- Graph-ODE series with MPNODE (Gupta et al., 2022), CG-ODE (Huang et al., 2021) and SGODE (Chen et al., 2024a);\\n- Operator learning methods with FNO (Li et al., 2021), F-FNO (Tran et al., 2023), and LSM (Wu et al., 2023a).\\n\\nMore details of baselines can be found in Appendix D.\\n\\nImplementation & Evaluation Metrics.\\n\\nTo ensure fairness, all methods train on a single NVIDIA-A100 using the ADAM optimizer for MSE loss over 500 epochs, with an initial learning rate of $10^{-3}$. We set the batch size to 20. We study Mean Squared Error (MSE) and Structural Similarity Index Measure (SSIM) in our research. To adapt these models for irregular grids, we incorporate geo-FNO (Li et al., 2022b) for input/output transformation, facilitating the conversion of irregular domains into regular grids.\"}"}
{"id": "JsPvL6ExK8", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE\\n\\nLe Clainche, S., Ferrer, E., Gibson, S., Cross, E., Parente, A., and Vinuesa, R. Improving aircraft performance using machine learning: a review. *Aerospace Science and Technology*, pp. 108354, 2023.\\n\\nLi, H., Pan, S. J., Wang, S., and Kot, A. C. Domain generalization with adversarial feature learning. In *CVPR*, pp. 5400\u20135409, 2018.\\n\\nLi, H., Wang, Y., Wan, R., Wang, S., Li, T.-Q., and Kot, A. Domain generalization for medical imaging classification with linear-dependency regularization. In *NeurIPS*, pp. 3118\u20133129, 2020.\\n\\nLi, J., Song, Z., and Yang, B. Nvfi: Neural velocity fields for 3d physics learning from dynamic videos. In *NeurIPS*, 2023a.\\n\\nLi, X., Dai, Y., Ge, Y., Liu, J., Shan, Y., and Duan, L.-Y. Uncertainty modeling for out-of-distribution generalization. *arXiv preprint arXiv:2202.03958*, 2022a.\\n\\nLi, Z., Kovachki, N. B., Azizzadenesheli, K., Liu, B., Bhat-tacharya, K., Stuart, A., and Anandkumar, A. Fourier neural operator for parametric partial differential equations. In *ICLR*, 2021.\\n\\nLi, Z., Huang, D. Z., Liu, B., and Anandkumar, A. Fourier neural operator with learned deformations for pdes on general geometries. *arXiv preprint arXiv:2207.05209*, 2022b.\\n\\nLi, Z., Kovachki, N. B., Choy, C., Li, B., Kossaifi, J., Otta, S. P., Nabian, M. A., Stadler, M., Hundt, C., Azizzadenesheli, K., et al. Geometry-informed neural operator for large-scale 3d pdes. In *NeurIPS*, 2023b.\\n\\nLienen, M., L\u00fcdke, D., Hansen-Palmus, J., and G\u00fcnnemann, S. From zero to turbulence: Generative modeling for 3d flow simulation. In *ICLR*, 2024.\\n\\nLin, H., Gao, Z., Xu, Y., Wu, L., Li, L., and Li, S. Z. Conditional local convolution for spatio-temporal meteorological forecasting. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 36, pp. 7470\u20137478, 2022.\\n\\nLippe, P., Veeling, B. S., Perdikaris, P., Turner, R. E., and Brandstetter, J. Pde-refiner: Achieving accurate long rollouts with neural pde solvers. In *NeurIPS*, 2023.\\n\\nLiu, J., Shen, Z., He, Y., Zhang, X., Xu, R., Yu, H., and Cui, P. Towards out-of-distribution generalization: A survey. *arXiv preprint arXiv:2108.13624*, 2021a.\\n\\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S. C.-F., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. *ICCV*, 2021b.\\n\\nLu, W., Wang, J., Wang, Y., Ren, K., Chen, Y., and Xie, X. Towards optimization and model selection for domain generalization: A mixup-guided solution. *arXiv preprint arXiv:2209.00652*, 2022.\\n\\nLuo, X., Gu, Y., Jiang, H., Huang, J., Ju, W., Zhang, M., and Sun, Y. Graph ode with factorized prototypes for modeling complicated interacting dynamics. *arXiv preprint arXiv:2311.06554*, 2023a.\\n\\nLuo, X., Yuan, J., Huang, Z., Jiang, H., Qin, Y., Ju, W., Zhang, M., and Sun, Y. Hope: High-order graph ode for modeling interacting dynamics. In *International Conference on Machine Learning*, pp. 23124\u201323139. PMLR, 2023b.\\n\\nLuo, X., Zhao, Y., Mao, Z., Qin, Y., Ju, W., Zhang, M., and Sun, Y. Rignn: A rationale perspective for semi-supervised open-world graph classification. *Transactions on Machine Learning Research*, 2023c.\\n\\nLuo, X., Wang, H., Huang, Z., Jiang, H., Gangan, A., Jiang, S., and Sun, Y. Care: Modeling interacting dynamics under temporal environmental variation. *Advances in Neural Information Processing Systems*, 36, 2024.\\n\\nMa, P., Chen, P. Y., Deng, B., Tenenbaum, J. B., Du, T., Gan, C., and Matusik, W. Learning neural constitutive laws from motion observations for generalizable pde dynamics. In *ICML*, 2023.\\n\\nObiols-Sales, O., Vishnu, A., Malaya, N., and Chandramowilanswara, A. Cfdnet: A deep learning-based accelerator for fluid simulations. In *Proceedings of the 34th ACM international conference on supercomputing*, pp. 1\u201312, 2020.\\n\\nPfaff, T., Fortunato, M., Sanchez-Gonzalez, A., and Battaglia, P. W. Learning mesh-based simulation with graph networks. In *ICLR*, 2021.\\n\\nR\u00e4m\u00e4, M. and Sipil\u00e4, K. Transition to low temperature distribution in existing systems. *Energy Procedia*, 116:58\u201368, 2017.\\n\\nRonneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In *MICCAI*, 2015.\\n\\nSanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., and Battaglia, P. Learning to simulate complex physics with graph networks. In *ICML*, pp. 8459\u20138468, 2020.\\n\\nShao, Y., Loy, C. C., and Dai, B. Transformer with implicit edges for particle-based physics simulation. In *ECCV*, pp. 549\u2013564, 2022.\"}"}
{"id": "JsPvL6ExK8", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE\\n\\nSteeven, J., Madiha, N., Julie, D., and Christian, W. Space and time continuous physics simulation from partial observations. In ICLR, 2024.\\n\\nTran, A., Mathews, A., Xie, L., and Ong, C. S. Factorized fourier neural operators. In ICLR, 2023.\\n\\nVan Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.\\n\\nVeli\u010dkovi\u0107, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y. Graph attention networks. In ICLR, 2018.\\n\\nVolpi, R., Namkoong, H., Sener, O., Duchi, J. C., Murino, V., and Savarese, S. Generalizing to unseen domains via adversarial data augmentation. In NeurIPS, 2018.\\n\\nWang, K., Wu, H., Duan, Y., Zhang, G., Wang, K., Peng, X., Zheng, Y., Liang, Y., and Wang, Y. Nuwadynamics: Discovering and updating in causal spatio-temporal modeling.\\n\\nWang, K., Liang, Y., Li, X., Li, G., Ghanem, B., Zimmermann, R., Yi, H., Zhang, Y., Wang, Y., et al. Brave the wind and the waves: Discovering robust and generalizable graph lottery tickets. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023a.\\n\\nWang, K., Wu, H., Zhang, G., Fang, J., Liang, Y., Wu, Y., Zimmermann, R., and Wang, Y. Modeling spatio-temporal dynamical systems with neural discrete learning and levels-of-experts. IEEE Transactions on Knowledge and Data Engineering, 2024.\\n\\nWang, R., Mao, W., and Li, H. Deepsimho: Stable pose estimation for hand-object interaction via physics simulation. In NeurIPS, 2023b.\\n\\nWang, Y., Li, X., Qi, Z., Li, J., Li, X., Meng, X., and Meng, L. Meta-causal feature learning for out-of-distribution generalization. In ECCV, pp. 530\u2013545, 2022.\\n\\nWang, Z., Loog, M., and van Gemert, J. Respecting domain relations: Hypothesis invariance for domain generalization. In ICPR, pp. 9756\u20139763, 2021.\\n\\nWeng, W., Fan, J., Wu, H., Hu, Y., Tian, H., Zhu, F., and Wu, J. A decomposition dynamic graph convolutional recurrent network for traffic forecasting. Pattern Recognition, 142:109670, 2023.\\n\\nWenzel, F., Dittadi, A., Gehler, P., Simon-Gabriel, C.-J., Horn, M., Zietlow, D., Kernert, D., Russell, C., Brox, T., Schiele, B., et al. Assaying out-of-distribution generalization in transfer learning. In NeurIPS, pp. 7181\u20137198, 2022.\\n\\nWu, H., Zhou, S., Huang, X., and Xiong, W. Neural manifold operators for learning the evolution of physical dynamics.\\n\\nWu, H., Hu, T., Luo, H., Wang, J., and Long, M. Solving high-dimensional pdes with latent spectral models. arXiv preprint arXiv:2301.12664, 2023a.\\n\\nWu, H., Wang, S., Liang, Y., Zhou, Z., Huang, W., Xiong, W., and Wang, K. Earthfarseer: Versatile spatio-temporal dynamical systems modeling in one model. AAAI2024, 2023b.\\n\\nWu, H., Xion, W., Xu, F., Luo, X., Chen, C., Hua, X.-S., and Wang, H. Pastnet: Introducing physical inductive biases for spatio-temporal video prediction. arXiv preprint arXiv:2305.11421, 2023c.\\n\\nWu, L., Qiu, X., Yuan, Y.-x., and Wu, H. Parameter estimation and variable selection for big systems of linear ordinary differential equations: A matrix-based approach. Journal of the American Statistical Association, 2018.\\n\\nWu, Q., Zhang, H., Yan, J., and Wipf, D. Handling distribution shifts on graphs: An invariance perspective. arXiv preprint arXiv:2202.02466, 2022.\\n\\nXhonneux, L.-P., Qu, M., and Tang, J. Continuous graph neural networks. In ICML, pp. 10432\u201310441, 2020.\\n\\nXia, Y., Liang, Y., Wen, H., Liu, X., Wang, K., Zhou, Z., and Zimmermann, R. Deciphering spatio-temporal graph forecasting: A causal lens and treatment. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nXiong, W., Xiang, Y., Wu, H., Zhou, S., Sun, Y., Ma, M., and Huang, X. Ai-goms: Large ai-driven global ocean modeling system. arXiv preprint arXiv:2308.03152, 2023.\\n\\nYang, C., Wu, Q., Wen, Q., Zhou, Z., Sun, L., and Yan, J. Towards out-of-distribution sequential event prediction: A causal treatment. arXiv preprint arXiv:2210.13005, 2022.\\n\\nYu, H.-X., Zheng, Y., Gao, Y., Deng, Y., Zhu, B., and Wu, J. Inferring hybrid neural fluid fields from videos. In NeurIPS, 2023.\\n\\nYu, Y.-Y., Choi, J., Cho, W., Lee, K., Kim, N., Chang, K., Woo, C., Kim, I., Lee, S., Yang, J. Y., et al. Learning flexible body collision dynamics with hierarchical contact mesh transformer. In ICLR, 2024.\\n\\nZang, C. and Wang, F. Neural dynamics on complex networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 892\u2013902, 2020.\"}"}
{"id": "JsPvL6ExK8", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE\\n\\nZhan, X., Dai, Z., Wang, Q., Li, Q., Xiong, H., Dou, D., and Chan, A. B. Pareto optimization for active learning under out-of-distribution data scenarios. Transactions on Machine Learning Research, 2023.\\n\\nZhang, G., Yue, Y., Wang, K., Fang, J., Sui, Y., Wang, K., Liang, Y., Cheng, D., Pan, S., and Chen, T. Two heads are better than one: Boosting graph sparse training via semantic and topological awareness, 2024.\\n\\nZhang, Y., Gao, S., Pei, J., and Huang, H. Improving social network embedding via new second-order continuous graph neural networks. In KDD, pp. 2515\u20132523, 2022.\\n\\nZhao, X., Meng, L., Tong, X., Xu, X., Wang, W., Miao, Z., Mo, D., et al. A novel computational fluid dynamic method and validation for assessing distal cerebrovascular microcirculatory resistance. Computer Methods and Programs in Biomedicine, 230:107338, 2023.\\n\\nZhou, D., Wang, K., Gu, J., Peng, X., Lian, D., Zhang, Y., You, Y., and Feng, J. Dataset quantization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 17205\u201317216, 2023.\"}"}
{"id": "JsPvL6ExK8", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Proof of Theorem 3.1.\\n\\nWe directly consider the case where the time points tend to infinity; that is\\n\\n$$\\\\lim_{n \\\\to \\\\infty} \\\\parallel \\\\tilde{z}(t_j) - e^{t_j}W_1 h \\\\parallel_2^2 = 0$$\\n\\nDefine $E_1 = (I_p, 0_p) \\\\in \\\\mathbb{R}^{p \\\\times (2p)}$ be the extractor matrix such that $E_1 A$ returns the first $p$ rows of the matrix $A$. Similarly, we define $E_1 = (0_p, I_p)$. By Eqn. 14\u2013Eqn. 15,\\n\\n$$L(W_1) = p\\\\sigma_2 + \\\\int Z_1 \\\\parallel E_1 e^{t} W_\\\\ast Z(0) - e^{t} W_1 h \\\\parallel_2^2 dt.$$ \\n\\nDefine $D(W, t)$ as the following $p \\\\times p$-dimensional matrix function\\n\\n$$D_i, (\\\\ell - 1)p + k(W, t) = \\\\partial e^{t} W_h i \\\\partial W_k \\\\ell, i, \\\\ell, k = 1, \\\\ldots, p, (22)$$\\n\\nwhere $W_{k\\\\ell}$ denotes the $(k, \\\\ell)$th entry of $W$. Note that the second order derivatives of $L(W_1)$ are\\n\\n$$\\\\partial^2 L(W_1) \\\\partial W_{k\\\\ell} \\\\partial W_{k'\\\\ell'} = \\\\int Z_1 \\\\sum_{i=1}^p \\\\partial e^{t} W_1 h i \\\\partial W_{k\\\\ell} \\\\partial e^{t} W_1 h i \\\\partial W_{k'\\\\ell'} dt.$$ \\n\\nThus, the Hessian matrix of $L(W_1)$ (after $W_1$ being vectorized) is\\n\\n$$R_1 \\\\{D(W_1, t)\\\\}^T D(W_1, t) dt,$$\\n\\nwhich is a positive semidefinite matrix. This implies that the global minimizer $cW_1$ of $L(W_1)$ must be the first-order stationary point; that is,\\n\\n$$\\\\nabla L(cW_1) = 2 Z_1 \\\\{D(cW_1, t)\\\\}^T (e^{t} cW_1 h - E_1 e^{t} W_\\\\ast Z(0)) dt = 0.$$ \\n\\nBy Lemma S6.3 in Wu et al. (2018), we have\\n\\n$$D(W, t) = t Z_1 (h^T e^{ts} W^T) \\\\otimes e^{t}(1-s) W ds,$$\\n\\nwhich gives\\n\\n$$\\\\nabla L(cW_1) = 2 Z_1 Z_1 t \\\\{D(cW_1, t)\\\\}^T (e^{ts} cW_1 h - E_1 e^{t} W_\\\\ast Z(0)) ds dt = 0.$$ \\n\\nIn general, it is difficult to obtain an explicit form of $cW_1$. But we can show that $W_\\\\ast$ is not a zero of $\\\\nabla L(W_1)$ when each entry of $W_\\\\ast$ and $Z(0)$ is strictly positive. To see this, first observe that for any matrix $A$,\\n\\n$$e^{t} A = I + \\\\sum_{k=1}^{\\\\infty} \\\\frac{t^k}{k!} A^k,$$\\n\\nand thus\\n\\n$$e^{t} W_\\\\ast h - E_1 e^{t} W_\\\\ast Z(0) = \\\\sum_{k=1}^{\\\\infty} \\\\frac{t^k}{k!} W_\\\\ast_k h - E_1 e^{t} W_\\\\ast E_1 h.$$ \\n\\nMoreover, since $W_\\\\ast = W_\\\\ast_1 W_\\\\ast_2 W_\\\\ast_3 W_\\\\ast_4$, by the positivity of entries of $W_\\\\ast$ we have that each entry of $W_\\\\ast_k h - E_1 W_\\\\ast E_1 h$ and $-E_1 W_\\\\ast E_2 h$ is strictly negative. It follows immediately from the positivity of components of $Z(0)$ that each component of $e^{t} cW_1 h - E_1 e^{t} W_\\\\ast Z(0)$ is strictly negative. As the entries of $t \\\\{D(cW_1)\\\\}^T (e^{ts} cW_1 h - E_1 e^{t} W_\\\\ast Z(0))$ are all strictly positive, we conclude that each component of $\\\\nabla L(W_\\\\ast)$ are strictly negative.\"}"}
{"id": "JsPvL6ExK8", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In all benchmark tests, we compare the performance of our study with 12 baseline models and record the Mean Squared Error (MSE) as a performance metric. It's important to note that a lower MSE value indicates better model performance. Moreover, we define improvement as the reduction in relative error compared to the second-best model in each benchmark test.\\n\\n| MODEL      | Prometheus-T | Prometheus-W | WeatherBench-T | WeatherBench-W | Earth        | Earth        | CLCRN        | CLCRN        | MGNT         | MGNT         | EAGLE        | EAGLE        | DGCRN        | DGCRN        | MPNODE       | MPNODE       | CG-ODE       | CG-ODE       | SGODE        | SGODE        | FNO          | FNO          | F-FNO        | F-FNO        | LSM          | LSM          | DGODE        | DGODE        |\\n|------------|--------------|--------------|----------------|----------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\\n| U-Net      | 0.0931       | 0.1067       | 0.0471         | 0.0534         | 0.1382       | 0.1577       | 0.1650       | 0.1862       | 0.1982       | 0.2243       | 0.1420       | 0.1618       | 0.2248       | 0.2554       | 0.0652       | 0.0729       | 0.1191       | 0.1361       | 0.1420       | 0.1618       | 0.2248       | 0.2554       | 0.0652       | 0.0729       | 0.1191       | 0.1361       |\\n| SWIN-T     | 0.0652       | 0.0729       | 0.0564         | 0.0647         | 0.1191       | 0.1361       | 0.1420       | 0.1618       | 0.2248       | 0.2554       | 0.1420       | 0.1618       | 0.2248       | 0.2554       | 0.0652       | 0.0729       | 0.1191       | 0.1361       | 0.1420       | 0.1618       | 0.2248       | 0.2554       | 0.0652       | 0.0729       | 0.1191       | 0.1361       |\\n| EARTH      | 0.0419       | 0.0470       | 0.0577         | 0.0654         | 0.0861       | 0.0952       | 0.1218       | 0.1374       | 0.1779       | 0.2013       | 0.1218       | 0.1374       | 0.1779       | 0.2013       | 0.0419       | 0.0470       | 0.0577       | 0.0654         | 0.0861       | 0.0952       | 0.1218       | 0.1374       | 0.1779       | 0.2013       | 0.1218       | 0.1374       | 0.1779       | 0.2013       |\\n| CLCRN      | 0.0763       | 0.0851       | 0.1421         | 0.1587         | 0.1293       | 0.1471       | 0.1520       | 0.1710       | 0.3398       | 0.3873       | 0.1520       | 0.1710       | 0.3398       | 0.3873       | 0.0763       | 0.0851       | 0.1421       | 0.1587         | 0.1293       | 0.1471       | 0.1520       | 0.1710       | 0.3398       | 0.3873       | 0.1520       | 0.1710       | 0.3398       | 0.3873       |\\n| MGNT       | 0.0967       | 0.1079       | 0.1672         | 0.1855         | 0.1331       | 0.1468       | 0.1591       | 0.1816       | 0.3561       | 0.3938       | 0.1591       | 0.1816       | 0.3561       | 0.3938       | 0.0967       | 0.1079       | 0.1672       | 0.1855         | 0.1331       | 0.1468       | 0.1591       | 0.1816       | 0.3561       | 0.3938       | 0.1591       | 0.1816       | 0.3561       | 0.3938       |\\n| EAGLE      | 0.1128       | 0.1296       | 0.1981         | 0.2222         | 0.2781       | 0.3154       | 0.3300       | 0.3694       | 0.4731       | 0.5238       | 0.2781       | 0.3154       | 0.3300       | 0.3694       | 0.1128       | 0.1296       | 0.1981       | 0.2222         | 0.2781       | 0.3154       | 0.3300       | 0.3694       | 0.4731       | 0.5238       | 0.2781       | 0.3154       | 0.3300       | 0.3694       | 0.4731       | 0.5238       |\\n| DGCRN      | 0.1027       | 0.1151       | 0.0961         | 0.1066         | 0.1587       | 0.1784       | 0.1845       | 0.2069       | 0.2198       | 0.2497       | 0.1587       | 0.1784       | 0.1845       | 0.2069       | 0.1027       | 0.1151       | 0.0961       | 0.1066         | 0.1587       | 0.1784       | 0.1845       | 0.2069       | 0.2198       | 0.2497       | 0.1587       | 0.1784       | 0.1845       | 0.2069       | 0.2198       | 0.2497       |\\n| MPNODE     | 0.0652       | 0.0749       | 0.0897         | 0.0999         | 0.1665       | 0.1895       | 0.1941       | 0.2167       | 0.1991       | 0.2199       | 0.1665       | 0.1895       | 0.1941       | 0.2167       | 0.0652       | 0.0749       | 0.0897       | 0.0999         | 0.1665       | 0.1895       | 0.1941       | 0.2167       | 0.1991       | 0.2199       | 0.1665       | 0.1895       | 0.1941       | 0.2167       | 0.1991       | 0.2199       |\\n| CG-ODE     | 0.0761       | 0.0843       | 0.0499         | 0.0565         | 0.0972       | 0.1107       | 0.1159       | 0.1320       | 0.2035       | 0.2243       | 0.0972       | 0.1107       | 0.1159       | 0.1320       | 0.0761       | 0.0843       | 0.0499       | 0.0565         | 0.0972       | 0.1107       | 0.1159       | 0.1320       | 0.2035       | 0.2243       | 0.0972       | 0.1107       | 0.1159       | 0.1320       | 0.2035       | 0.2243       |\\n| SGODE      | 0.0643       | 0.0723       | 0.0464         | 0.0537         | 0.0873       | 0.0931       | 0.1047       | 0.1301       | 0.1987       | 0.2109       | 0.0873       | 0.0931       | 0.1047       | 0.1301       | 0.0643       | 0.0723       | 0.0464       | 0.0537         | 0.0873       | 0.0931       | 0.1047       | 0.1301       | 0.1987       | 0.2109       | 0.0873       | 0.0931       | 0.1047       | 0.1301       | 0.1987       | 0.2109       |\\n| FNO        | 0.0447       | 0.0506       | 0.0471         | 0.0522         | 0.1128       | 0.1290       | 0.1301       | 0.1466       | 0.1556       | 0.1712       | 0.1301       | 0.1466       | 0.1556       | 0.1712       | 0.0447       | 0.0506       | 0.0471       | 0.0522         | 0.1128       | 0.1290       | 0.1301       | 0.1466       | 0.1556       | 0.1712       | 0.1301       | 0.1466       | 0.1556       | 0.1712       | 0.1301       | 0.1466       |\\n| F-FNO      | 0.0531       | 0.0608       | 0.1542         | 0.1728         | 0.1733       | 0.1934       | 0.2322       | 0.2556       | 0.2322       | 0.2664       | 0.1733       | 0.1934       | 0.2322       | 0.2556       | 0.0531       | 0.0608       | 0.1542       | 0.1728         | 0.1733       | 0.1934       | 0.2322       | 0.2556       | 0.2322       | 0.2664       | 0.1733       | 0.1934       | 0.2322       | 0.2556       | 0.2322       | 0.2664       |\\n| LSM        | 0.0414       | 0.0456       | 0.0489         | 0.0546         | 0.1298       | 0.1474       | 0.1549       | 0.1723       | 0.1535       | 0.1731       | 0.1298       | 0.1474       | 0.1549       | 0.1723       | 0.0414       | 0.0456       | 0.0489       | 0.0546         | 0.1298       | 0.1474       | 0.1549       | 0.1723       | 0.1535       | 0.1731       | 0.1298       | 0.1474       | 0.1549       | 0.1723       | 0.1535       | 0.1731       |\\n| **ROMETHEUS** | **16.91%**  | **21.27%**  | **15.71%**     | **20.88%**     | **2.09%**    | **7.25%**    | **15.79%**   | **17.65%**   | **47.56%**   | **46.56%**   | **2.09%**    | **7.25%**    | **15.79%**   | **17.65%**   | **16.91%**  | **21.27%**  | **15.71%**   | **20.88%**     | **2.09%**    | **7.25%**    | **15.79%**   | **17.65%**   | **47.56%**   | **46.56%**   | **2.09%**    | **7.25%**    | **15.79%**   | **17.65%**   |\\n\\n**4.2. Performance Comparison under Environmental Distribution Shifts**\\n\\nIn this section, we focus on discussing and evaluating the performance of DGODE and baseline models in predicting dynamic systems across various environmental distributions.\\n\\n**Results.** The performance of compared approaches in different settings are recorded in Table"}
{"id": "JsPvL6ExK8", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE\\n\\nFigure 3.\\n\\nTOP: Prometheus-P shows the visualization of temperature and velocity variables at the last timestamp (T=50). Navier-Stokes equation prediction shows results at the last timestamp (T=10).\\n\\nBottom: For WeatherBench-T and WeatherBench-W, we display the prediction visualization at the last timestamp of the time series (T=12). To more clearly demonstrate the accuracy of the predictions, we also created images of prediction errors (Error = \\\\(Y - \\\\hat{Y}\\\\)). More detailed case analyses and examples are available in Appendix F.\\n\\nFigure 4.\\n\\nLeft shows a top view of the Prometheus-P dataset, comparing the DGODE model with the runner-up LSM model. It displays the prediction results at 20 and 120 steps.\\n\\nRight, the changes in SSIM over time steps are shown for the DGODE, LSM, and FNO models on both Prometheus-P and WeatherBench-W datasets.\\n\\nLearning of Environmental Embeddings. In our study, we employ Principal Component Analysis (PCA) as a dimensionality reduction technique to visualize the environmental Codebank vectors in the Prometheus-P dataset. The visualization results are shown in Figure 5 (a). Our aim is to explore the effectiveness of the environmental Codebank as a potential feature representation. During this process, we focus on 20 different colored environmental embeddings. These embeddings represent unique environments in the physical world. They start from the same initial point and gradually disperse in multiple directions as the training cycles progress. This phenomenon indicates that our model effectively learns information-rich environmental representations.\\n\\nSensitivity Analysis. Our study examines the influence of key Codebank hyperparameters, \\\\(K\\\\) (Codebank size) and \\\\(D\\\\) (dimension of latent vectors), on DGODE's performance in Navier-Stokes simulations. We varied \\\\(K\\\\) among \\\\(\\\\{8, 16, 32, 64, 128\\\\}\\\\) and \\\\(D\\\\) among \\\\(\\\\{32, 64, 128\\\\}\\\\), with results in Figure 5 (b) showing that performance improves with larger \\\\(D\\\\), as smaller dimensions are insufficient for learning the environment. Figure 5 (c) reveals that irrespective of initial differences, models with various \\\\(D\\\\) and \\\\(K\\\\) values converge to similar performance. We also depict the model's learning progression in the best training scenario, demonstrating its increasing proficiency in capturing complex patterns in Navier-Stokes equations.\\n\\nTransferability. Prometheus-P and Prometheus-T come from FDS modeling, but they have different scenarios. As shown in Table 3, to evaluate the DGODE model transferability, we fine-tune the model trained on Prometheus-P for different scenes. This also demonstrates the model's strong ability to handle unknown data (OOD). It is important to note that our proposed DGODE always shows good forward transfer abilities, even with limited data. Moreover, DGODE achieves the best performance, whether using pre-trained models or starting from scratch. This indicates that DGODE can extract key physical features from complex dynamics system.\"}"}
{"id": "JsPvL6ExK8", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Figure (a) shows the visualization of environment embeddings at different training epochs. Figure (b) describes the impact of hyperparameters on the model in Navier-Stokes equation (The viscosity coefficient is $10^{-5}$). Figure (c) visualizes the training process.\\n\\nTable 3. Transfer the model pre-trained from full-data Prometheus-P to limited-data Prometheus-T. The results are presented in the formalization of $P \\\\rightarrow T$, where $P$ is the model performance when it is trained from scratch and $T$ is the performance finetuned from the Prometheus-P pre-trained model.\\n\\n|              | W/O OOD | W/O OOD |\\n|--------------|---------|---------|\\n| **MSE**      |         |         |\\n| **ROMETHEUS**|         |         |\\n| 5.31         | 5.28 (+0.56%) | 5.22 (+1.92%) |\\n| 5.22         | 5.12 (+1.92%) | 5.01 (+6.99%) |\\n| 5.01         | 4.66 (+6.99%) | 4.89 (+1.64%) |\\n| 4.89         | 4.81 (+1.64%) | 4.70 (+3.40%) |\\n| 4.70         | 4.54 (+3.40%) | 4.59 (+3.40%) |\\n| 4.59         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1.97%) | 4.47 (+1.97%) |\\n| 4.47         | 4.47 (+1."}
{"id": "JsPvL6ExK8", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE\\n\\nReferences\\n\\nBaradel, F., Neverova, N., Mille, J., Mori, G., and Wolf, C. Cophy: Counterfactual learning of physical dynamics. arXiv preprint arXiv:1909.12000, 2019.\\n\\nBelghazi, M. I., Baratin, A., Rajeshwar, S., Ozair, S., Bengio, Y., Courville, A., and Hjelm, D. Mutual information neural estimation. In International conference on machine learning, pp. 531\u2013540. PMLR, 2018.\\n\\nChen, L., Wu, K., Lou, J., and Liu, J. Signed graph neural ordinary differential equation for modeling continuous-time dynamics. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 8292\u20138301, 2024a.\\n\\nChen, P., Zhang, Y., Cheng, Y., Shu, Y., Wang, Y., Wen, Q., Yang, B., and Guo, C. Multi-scale transformers with adaptive pathways for time series forecasting. In ICLR, 2024b.\\n\\nChen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations. In NeurIPS, 2018.\\n\\nDeng, A. and Hooi, B. Graph neural network-based anomaly detection in multivariate time series. AAAI, 2021.\\n\\nDeng, X., Wang, W., Feng, F., Zhang, H., He, X., and Liao, Y. Counterfactual active learning for out-of-distribution generalization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11362\u201311377, 2023.\\n\\nDupont, E., Doucet, A., and Teh, Y. W. Augmented neural odes. In NeurIPS, 2019.\\n\\nFang, Z. A high-efficient hybrid physics-informed neural networks based on convolutional neural network. IEEE Transactions on Neural Networks and Learning Systems, 33(10):5514\u20135526, 2021.\\n\\nFinlay, C., Jacobsen, J.-H., Nurbekyan, L., and Oberman, A. How to train your neural ode: the world of jacobian and kinetic regularization. In ICML, pp. 3154\u20133164, 2020.\\n\\nGui, S., Liu, M., Li, X., Luo, Y., and Ji, S. Joint learning of label and environment causal independence for graph out-of-distribution generalization. arXiv preprint arXiv:2306.01103, 2023.\\n\\nGuo, X., Li, W., and Iorio, F. Convolutional neural networks for steady flow approximation. In KDD, pp. 481\u2013490, 2016.\\n\\nGupta, J. K., Vemprala, S., and Kapoor, A. Learning modular simulations for homogeneous systems. In NeurIPS, 2022.\\n\\nHan, J., Huang, W., Ma, H., Li, J., Tenenbaum, J. B., and Gan, C. Learning physical dynamics with subequivariant graph neural networks. arXiv preprint arXiv:2210.06876, 2022a.\\n\\nHan, X., Gao, H., Pffaf, T., Wang, J.-X., and Liu, L.-P. Predicting physics in mesh-reduced space with temporal attention. arXiv preprint arXiv:2201.09113, 2022b.\\n\\nHendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8340\u20138349, 2021.\\n\\nHuang, C., Zhao, L., Niu, J., Di, J., Yuan, J., Zhao, Q., Zhang, F., Zhang, Z., Lei, J., and He, G. Coupled particle and mesh method in an euler frame for unsteady flows around the pitching airfoil. Engineering Analysis with Boundary Elements, 138:159\u2013176, 2022.\\n\\nHuang, X., Shi, W., Meng, Q., Wang, Y., Gao, X., Zhang, J., and Liu, T.-Y. Neuralstagger: accelerating physics-constrained neural pde solver with spatial-temporal decomposition. In ICML, 2023a.\\n\\nHuang, Z., Sun, Y., and Wang, W. Learning continuous system dynamics from irregularly-sampled partial observations. In NeurIPS, pp. 16177\u201316187, 2020.\\n\\nHuang, Z., Sun, Y., and Wang, W. Coupled graph ode for learning interacting system dynamics. In KDD, 2021.\\n\\nHuang, Z., Sun, Y., and Wang, W. Generalizing graph ode for learning complex system dynamics across environments. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 798\u2013809, 2023b.\\n\\nJanny, S., Beneteau, A., Thome, N., Nadri, M., Digne, J., and Wolf, C. Eagle: Large-scale learning of turbulent fluid dynamics with mesh transformers. arXiv preprint arXiv:2302.10803, 2023.\\n\\nKipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.\\n\\nKochkov, D., Smith, J. A., Alieva, A., Wang, Q., Brenner, M. P., and Hoyer, S. Machine learning\u2013accelerated computational fluid dynamics. Proceedings of the National Academy of Sciences, 118(21):e2101784118, 2021.\\n\\nKostic, V. R., Novelli, P., Grazzi, R., Lounici, K., and Pontil, M. Learning invariant representations of time-homogeneous stochastic dynamical systems. In ICLR, 2024.\"}"}
