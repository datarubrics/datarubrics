{"id": "5nuW5iBAJS", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.10. Comparison with previous works\\n\\nA comparison with the most relevant previous works is given in Table 10.\\n\\nTable 10. Comparison with other works. *a prototype of the text-to-image system\\n\\n| Task          | Data Points | Shape Points | Best Available | Generative Design | Reference |\\n|---------------|-------------|--------------|----------------|-------------------|-----------|\\n| Size          | 103         | 1            | MAPE = 0.70%   | Only dataset      | No (Shafaei & Khayati, 2020) |\\n| Size          | 98          | 1            | MAPE = 4%      | No                | (Iakovlev et al., 2019) |\\n| Size          | 26          | 1            | MAPE = 9.10%   | No                | (Pellegrino et al., 2020) |\\n| Size and shape| 215         | 5            | Accuracy = 0.93 | Code and dataset  | Yes*      |\\n\\nA.11. Discussion on limitations\\n\\nOur ML models were trained to predict 5 types of nanomaterial shapes, some of which were underrepresented (Table 2). This limitation can be mitigated by either adjusting the prediction threshold, or oversampling techniques. Ultimately, this issue can only be resolved by expanding the dataset for underrepresented classes. In the context of the text-to-image system, we always refer to a prototype acknowledging its limitations, such as low diversity of generated images and their quality, caused by the limited training examples available. Therefore, most of the barriers to training a more universal and accurate model for prediction of nanomaterial morphology are related to insufficient quality and number of existing datasets. There is currently no unified database with syntheses and properties of different nanoparticles that is well documented and publicly available. Therefore, applied AI researchers have to resort to small single study datasets or larger datasets of a single experimental system extensively studied in the past (Table 10). Both approaches impose severe limitations on machine learning and, even more so, on deep learning applications that typically require a lot more training data. Thus, a collective effort towards assembling a curated database of nanomaterials with deep characterization of their properties is long overdue.\\n\\nAdditional challenges arise from the data preprocessing steps dealing with SEM. Many syntheses result in numerous overlaying NPs on a single SEM image, such that it is difficult even for a human eye to distinguish between individual NP units. Since image segmentation methods have already reached quite an advanced level, we anticipate major breakthroughs rather on the experimental and the imaging technology side.\\n\\nA.12. Computing infrastructure\\n\\nTable 11. Computing infrastructure used for study experiments.\\n\\n| CPU           | 8-Core Processor | 3.60 GHz |\\n|---------------|-------------------|----------|\\n| GPU           | NVIDIA GeForce RTX 3090 | 24 GB of GPU memory |\\n| RAM           | 32.0 GB          |\\n| Operating System | Windows 11 Pro   |\\n| Python        | 3.9              |\"}"}
{"id": "5nuW5iBAJS", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Unveiling the Potential of AI for Nanomaterial Morphology Prediction\\n\\nFigure 5.\\nA) The encoder-decoder architecture of the linking neural network. B) Comparison of real images to their VAE reconstructions and the images generated from the corresponding synthesis texts.\"}"}
{"id": "5nuW5iBAJS", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6. Representation of the latent space of the variational autoencoder trained on our image dataset. Colors indicate the shapes of the nanomaterials, and the axes are the UMAP components after dimensionality reduction.\"}"}
{"id": "5nuW5iBAJS", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unveiling the Potential of AI for Nanomaterial Morphology Prediction\\n\\nTable 1. Example contingency table for testing categorical variables of synthesis procedures.\\n\\n| Compound in synthesis | Not in synthesis |\\n|-----------------------|-----------------|\\n| NPs of a given shape   | a                |\\n| NPs of other shapes    | b                |\\n|                       | c                |\\n|                       | d                |\\n\\n\\\\[ p = \\\\frac{a}{a + b} = \\\\frac{a}{c + d} = \\\\frac{a + c}{a + b + c + d} \\\\]\\n\\nWe computed these probabilities for each combination of nanoparticle shape and polymer/surfactant/solvent involved in the synthesis. Using the same significance level and the correction for multiple hypothesis testing as before, we observed several strong associations: stick-shaped nanoparticles with polyethylene glycol (PEG) and polyethylenimine (PEI) polymers; flat nanoparticles with presence of PE-DOT:PSS and polyvinylpyrrolidone (PVP); cubic nanoparticles with presence of polyacrylic acid (PAA) and PE-DOT:PSS. We also found strong dependencies of nanoparticles' shapes on the following surfactants: Myristyltrimethylammonium bromide and Sodium dodecylsulfate. In the case of amorphous nanoparticles, the presence of Propylene glycol and tert-Butanol solvents was also found significant. Finally, we applied the Chi-squared test (Magnello, 2005) to confirm the aforementioned findings. For more information on how the statistical tests and the most significant associations between particular synthesis parameters and nanomaterial shapes, see Appendix A.1.\\n\\nNotably, many of the parameters of syntheses had no effect on the shapes of nanomaterials, e.g., stirring speed, concentrations of Ca and CO\\\\(_3\\\\) ions, presence of Hexadecyltrimethylammonium bromide and Triton X-100 surfactants, and 1-Hexanol and Methyl alcohol solvents. For the downstream machine learning applications, we excluded those features from the data.\\n\\n5. Shape and size prediction\\n\\nStatistical tests proved certain associations between the parameters of syntheses and the morphologies of the resulting nanomaterials. Therefore, we attempted to exploit them in predicting shapes and sizes of nanomaterials using classical machine learning algorithms.\\n\\nIn some cases, several nanoparticles of different shapes and sizes were present on the same image, so initial 215 syntheses produced 314 training examples of nanoparticles of different types. Following the logic of the statistical evaluation, we formulated a set of binary classification tasks, one for each type of shape or a combination of shape and size. In this formulation, we first trained a separate model to distinguish nanoparticles of each particular shape. Then, we ran multiple predictions for each sample during the inference to establish what shapes of nanoparticles were present on the corresponding image. The same logic applied to combinations of shapes and sizes. Notably, some syntheses consistently result in nanoparticles of several different shapes. Our approach allows dealing with such ambiguities without the need to determine the prevailing nanomaterial shape or size.\\n\\n5.1. Classical machine learning\\n\\n5.1.1. Tree-based ensemble models\\n\\nWe trained the tree-based models, namely Random Forest (RF) and Gradient Boosted Trees (XGB), to predict 9 categories representing combinations of shapes and sizes and 5 categories representing shapes only. Therein, we followed all the good practices in data preprocessing and model selection. A thorough description of the process of development, optimization and evaluation of classical machine learning models is presented in the Appendix A.2.\\n\\n5.1.2. Results\\n\\nThe accuracy and the F1 scores of the best models evaluated on the test dataset are presented in Table 2 and Table 3. Each experiment was performed 5 times at different random states, and the mean value and standard deviation were calculated.\\n\\nTable 2. Prediction of shapes. Top average accuracy and F1 scores achieved by the Random Forest classifiers on the test set.\\n\\n| Shape       | # samples | Accuracy | F1 score |\\n|-------------|-----------|----------|----------|\\n| Cube        | 140       | 0.76 \u00b1 0.02 | 0.73 \u00b1 0.03 |\\n| Stick       | 84        | 0.78 \u00b1 0.01 | 0.77 \u00b1 0.01 |\\n| Sphere      | 40        | 0.82 \u00b1 0.06 | 0.67 \u00b1 0.08 |\\n| Flat        | 16        | 0.82 \u00b1 0.11 | 0.52 \u00b1 0.09 |\\n| Amorphous   | 34        | 0.80 \u00b1 0.02 | 0.62 \u00b1 0.04 |\\n| Average     |           | 0.80 \u00b1 0.04 | 0.66 \u00b1 0.05 |\\n\\nBased on our results, the shapes of nanoparticles can be predicted reasonably well (Table 2). For every nanomaterial shape, RF performed better than XGB, so only RF metrics are displayed. The average accuracy and F1 score were 0.80 and 0.66, respectively. Unsurprisingly, the samples of the least represented categories (namely, flat and amorphous shapes) produced lower F1 scores, which decreased the overall metrics.\"}"}
{"id": "5nuW5iBAJS", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Prediction of shapes and sizes with tree-based ensemble models. Average accuracy, and F1 scores for Random Forest (RF) and Gradient Boosting (XGB) classifiers on the test set.\\n\\n| Shape & size | # samples | RF Accuracy | RF F1 score | XGB Accuracy | XGB F1 score |\\n|--------------|-----------|-------------|-------------|--------------|--------------|\\n| Cube         | small     | 0.85 \u00b1 0.01 | 0.72 \u00b1 0.05 | 0.58 \u00b1 0.07  | 0.57 \u00b1 0.08  |\\n|              | medium    | 0.64 \u00b1 0.05 | 0.64 \u00b1 0.03 | 0.48 \u00b1 0.05  | 0.52 \u00b1 0.06  |\\n|              | large     | 0.67 \u00b1 0.04 | 0.70 \u00b1 0.03 | 0.61 \u00b1 0.02  | 0.64 \u00b1 0.03  |\\n| Stick        | small     | 0.83 \u00b1 0.03 | 0.81 \u00b1 0.07 | 0.52 \u00b1 0.04  | 0.54 \u00b1 0.04  |\\n|              | medium    | 0.83 \u00b1 0.06 | 0.81 \u00b1 0.07 | 0.61 \u00b1 0.07  | 0.59 \u00b1 0.09  |\\n|              | large     | 0.79 \u00b1 0.04 | 0.79 \u00b1 0.04 | 0.64 \u00b1 0.05  | 0.63 \u00b1 0.05  |\\n| Sphere       | small     | 0.70 \u00b1 0.36 | 0.68 \u00b1 0.34 | 0.37 \u00b1 0.19  | 0.37 \u00b1 0.18  |\\n|              | medium    | 0.86 \u00b1 0.04 | 0.84 \u00b1 0.07 | 0.55 \u00b1 0.06  | 0.55 \u00b1 0.10  |\\n|              | large     | 0.72 \u00b1 0.27 | 0.61 \u00b1 0.28 | 0.44 \u00b1 0.16  | 0.40 \u00b1 0.18  |\\n\\nAverage accuracy for the task was 0.77, and the average F1 score was 0.53. Extending the number of categories to include the sizes of nanoparticles as well resulted in superior performance of XGB in most cases. This drop in performance was expected, as the number of samples per category became smaller, increasing the risk of overfitting. Underrepresentation becomes even more apparent for some classes. Apart from evaluating the models on the test set, which had never been used during training, we also explored feature importances as an additional validation step. In most cases, we observed that the top 5 most important parameters were well in agreement with the statistical tests described in the previous section and presented in Table 6 of the Appendix A.1. An example of feature importance analysis for the Random Forest model predicting whether a nanoparticle belongs to a stick shape is shown on Figure 4 of the Appendix A.2.\\n\\nThus, we demonstrated the possibility of predicting shapes and sizes of NPs with machine learning models, confirmed by average test accuracy of 0.80 and by feature importance analysis coherent with the statistical evaluation. The trained models can already be used to predict morphological properties of new nanomaterials based on their synthesis procedures. However, with recent advances in large language models, we wondered whether similar prediction performance can be achieved with state-of-the-art LLMs in a few-shot scenario. That would allow material scientists to use natural language for prediction tasks, bypassing the need to develop and optimize complex machine learning pipelines.\\n\\nIn the following section, we describe applications of LLMs to nanomaterial shape and size prediction.\"}"}
{"id": "5nuW5iBAJS", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For this purpose, we used a few-shot method, in which we show the model only a few randomly selected samples from our training set and then prompt it to make a prediction for a test sample. In all experiments, we used a special prompt describing the task that the LLM was given. It starts as follows:\\n\\nYou are an expert in the synthesis of nanomaterials. You analyze the conditions for obtaining a nanomaterial and predict what particle shapes will be present in the synthesized material. There are five particle shapes: 'Cube', 'Stick', 'Sphere', 'Flat' and 'Amorphous'. A nanomaterial can contain particles of different shapes. If you cannot say exactly what it is, list the forms that have the highest probability in those conditions.\\n\\nWe then appended several random examples from the training set with the corresponding true labels and a single example from the test subset to the prompt. While doing so, we varied the number of random examples N, the sampling method and the data format. We used from N = 2 to N = 10 training examples in the prompt. We experimented with two sampling strategies:\\n\\ni) at least one training example belongs to the same target class as the test sample,\\nii) all training examples belong to the same class as the test sample. The choice of the sampling strategies was based on practical considerations around real experiments. More specifically, strategy i) is targeted at characterization of previously unknown shapes, while strategy ii) follows the logic of a confirmation experiment, when a researcher only needs to confirm the presence of a particular nanomaterial shape in the synthesis. Finally, we used either of the two formats: textual (described in subsection 5.2.1) or tabular. In the tabular format, features of the training examples were concatenated to a string along with their values separated by colon, e.g., \\\"Ca ion, mM: 44; CO3 ion, mM: 159...\\\". Finally, the LLM was instructed to produce the list of nanoparticle shapes corresponding to the test synthesis as an answer. A more detailed description of prompts is presented in the Appendix A.3.\\n\\nUsing the above prompt structure, we applied 6 state-of-the-art LLMs, including GPT-4-turbo (gpt-4-0125-preview), GPT-4 (gpt-4-0613) and GPT-3.5-turbo (gpt-3.5-turbo-1106) from OpenAI (OpenAI, 2023), as well as the latest versions of Mistral Medium, Small and Tiny from Mistral AI (Jiang et al., 2023), to the same classification tasks described earlier. To systematically evaluate performance, we repeated each computational experiment 5 times and calculated mean and standard deviation for the standard classification metrics.\\n\\n5.2.3 RESULTS\\n\\nTable 4 shows top performance of LLMs predicting shapes of nanomaterials. Strikingly, GPT-4 achieved an even higher average accuracy than tree-based ensemble models. Among the other LLMs, it also demonstrated the smallest standard deviation, which speaks for better consistency. Interestingly, the second best model was Mistral-small. Given that its inference time and pricing are much lower than GPT-4, this model could be a pragmatic choice for practitioners as a balanced cost-quality trade-off. A detailed comparison of the pricing, inference time and rate limits is summarized in the Table 9 of Appendix A.4. In addition, we observed some mysterious drops in performance when predicting spherical shape. More specifically, Mistral-medium and GPT-4-turbo produced the accuracy of 0.38 and 0.44, respectively, which dramatically decreased their average scores, while the other models under identical experimental conditions coped with the problem reasonably well.\\n\\nAnalyzing the impact of sampling methods and data formats (Table 5 shows results for one of the GPT-4 experiments), we came to the following conclusions. First, including more examples from the training set belonging to the same class as the test sample definitely benefits the prediction. We observed improvements in accuracy in all related cases. Second, textual and tabular data formats performed similarly. However, textual format consistently resulted in a 4% increase in average accuracy, which was expected due to the nature of LLMs. Finally, the number of training samples in the prompt also correlated with the performance metrics (Figure 1). For all shapes except the cube, we observed an increase in accuracy as more examples from the training set were included for prediction. However, longer prompts are also known to trigger hallucination. On top of that, there is a hard limit on the maximum prompt size for many models. Therefore, for any particular application, one has to seek another trade-off between the number of training samples and the total prompt size. In our case, the performance seemed to reach a plateau with 8 samples (see Appendix A.4 for more details). The same configuration demonstrated the overall top performance (Table 4).\\n\\nAchieving state-of-the-art performance for nanomaterial morphology prediction with LLMs is very exciting for several reasons. First, it makes it possible for domain experts and experimentalists to avoid implementing complex data engineering pipelines and optimizing machine learning models, and use natural language to obtain the predictions instead. Second, it is obvious from our empirical results (Table 4) that an ensemble of LLMs would by far outperform the best classical ensemble models. Third, based on our empirical results, LLMs look especially advantageous in classification of underrepresented classes, or in small data.\"}"}
{"id": "5nuW5iBAJS", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unveiling the Potential of AI for Nanomaterial Morphology Prediction\\n\\nTable 4. Top performance achieved by the LLMs in prediction of nanomaterial shapes. Average accuracy corresponds to the following prompting strategy: only target classes in prompt, syntheses presented in the textual format, number of training examples $N = 8$. Only accuracy is given since the corresponding data is balanced.\\n\\n| Shape       | Mistral-medium | Mistral-small | Mistral-tiny | GPT-3.5-turbo | GPT-4 | GPT-4-turbo |\\n|-------------|----------------|---------------|--------------|---------------|-------|-------------|\\n| Cube        | 0.70\u00b10.11      | 0.76\u00b10.08     | 0.76\u00b10.19    | 0.69\u00b10.18     | 0.71\u00b10.05 | 0.60\u00b10.15   |\\n| Stick       | 0.71\u00b10.04      | 0.67\u00b10.11     | 0.71\u00b10.10    | 0.62\u00b10.16     | 0.68\u00b10.05 | 0.61\u00b10.13   |\\n| Sphere      | 0.38\u00b10.12      | 0.77\u00b10.18     | 0.62\u00b10.24    | 0.63\u00b10.15     | 0.88\u00b10.05 | 0.44\u00b10.12   |\\n| Flat        | 0.89\u00b10.08      | 0.92\u00b10.07     | 0.81\u00b10.17    | 0.90\u00b10.06     | 0.90\u00b10.10 | 0.91\u00b10.06   |\\n| Amorphous   | 0.70\u00b10.15      | 0.88\u00b10.08     | 0.53\u00b10.16    | 0.80\u00b10.13     | 0.87\u00b10.12 | 0.88\u00b10.08   |\\n| Average     | 0.68\u00b10.10      | 0.80\u00b10.10     | 0.69\u00b10.17    | 0.73\u00b10.13     | 0.81\u00b10.07 | 0.69\u00b10.11   |\\n\\nFigure 1. Average accuracy of GPT-4 for different number of samples in prompt taken from the training set. Sampling method: only target classes in prompt. Syntheses presented in the textual format. Colors correspond to different shapes of nanoparticles.\\n\\nScenarios. In particular, GPT-4 demonstrated a significant increase in accuracy when predicting less represented spherical, flat and amorphous nanoparticles (Table 2). Altogether, our results look very promising for the broader adoption of LLMs in the nanomaterial science.\\n\\n5.3. Text-to-image system\\n\\nPrediction of a nanoparticle shape as a categorical variable based on the selected set of properties describing the synthesis procedure is inherently subject to information loss. Intuitively, images are much better representations of shapes than any handcrafted categories, and the full text of a nanoparticle synthesis carries more information compared to a set of numerical features extracted from it. Therefore, a text-to-image paradigm previously explored in general-purpose applications (Rombach et al., 2021) and other domains (Khwaja et al., 2022) looks appealing in the context of our problem. In the following, we attempt to prototype such a system to explore its potential despite the hard constraints on the sample size.\\n\\nWe break down the text-to-image system into three main components. The first one is the natural language processing model converting the text of a synthesis procedure to a vector of numerical features. The second component is the generative model with an encoder-decoder architecture designed to learn representations of images of nanoparticles. Finally, the third component is the \u201clinking\u201d model translating the text representations into the image representations. When combined, the three components make a generative system capable of drawing the morphology of a nanomaterial based on the description of its synthesis (Figure 2).\\n\\n5.3.1. Natural Language Processing Model\\n\\nThe main requirement for the NLP model used for feature extraction was the ability to retain information about the qualitative and the quantitative features of a synthesis. In order to select the NLP model, we formulated several classification and regression tasks related to the key features of a synthesis procedure. We used the linear evaluation setup.\"}"}
{"id": "5nuW5iBAJS", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unveiling the Potential of AI for Nanomaterial Morphology Prediction\\n\\nIvan Dubrovsky\\n\\nAndrei Dmitrenko 1 2\\n\\nAleksei Dmitrenko 1\\n\\nNikita Serov 1\\n\\nVladimir Vinogradov 1\\n\\nAbstract\\n\\nCreation of nanomaterials with specific morphology remains a complex experimental process, even though there is a growing demand for these materials in various industry sectors. This study explores the potential of AI to predict the morphology of nanoparticles within the data availability constraints. For that, we first generated a new multi-modal dataset that is double the size of analogous studies. Then, we systematically evaluated performance of classical machine learning and large language models in prediction of nanomaterial shapes and sizes. Finally, we prototyped a text-to-image system, discussed the obtained empirical results, as well as the limitations and promises of existing approaches.\\n\\n1. Introduction\\n\\nNowadays, nanomaterials are spread across many fields of science and industry (Zebarjadi et al., 2011; Liu & Lal, 2015; Kairdolf et al., 2017; Shifrina et al., 2020; Gao et al., 2021; Takechi-Haraya et al., 2022). In each of those fields, for a nanomaterial to be fit for purpose, its size, shape, and other morphological parameters must be precisely controlled, as they directly influence toxicity, catalytic activity and other properties of nanomaterials crucial for applications. Altering these parameters also allows to improve efficiency of drug delivery systems (Sen Gupta, 2016), catalysts (Shifrina et al., 2020), energy storage systems (Pomerantseva et al., 2019), etc.\\n\\nTypically, creating a nanomaterial with a specific set of properties requires a significant number of experiments ranging from a few repetitive syntheses to a dozen of substantially different synthesis procedures (Vaidyanathan & Sendhilnathan, 2008; Sun et al., 2021). Each synthesis is followed by a specific method of analysis to confirm the experimental outcome. One of the most prominent methods for analyzing nanomaterials is the scanning electron microscopy (SEM) (Smith & Oatley, 1955). With SEM, it is possible to obtain information about the size and shape of nanoparticles (NPs), as well as the structure of the surface, surface flaws and contaminants. Currently, the SEM method is deemed irreplaceable despite being costly and time-consuming (Singh, 2016). On average, one analysis with SEM can cost up to a few hundred US dollars, leading to vast amounts of resources required to run any large-scale study. While conducting such experiments scientists are most often guided by their experience and intuition acquired in past experiments. This is because the problem of determining the morphology of nanomaterials based on the synthesis parameters currently has no theoretical or computational solution, in general. At the same time, syntheses of nanomaterials include too many different interdependent parameters for a person to be able to account for. Therefore, there is a high demand (AbdelHamid et al., 2022) for predictive models capable of characterizing the properties of nanomaterials bypassing the need of costly experimental work.\\n\\nArtificial intelligence (AI) offers the most promising set of tools to meet this demand. In fact, classical machine learning (ML) models including artificial neural networks have already been successfully applied to many tasks related to nanomaterial science (Serov & Vinogradov, 2022; Chen et al., 2023; Banaye Yazdipour et al., 2023). With recent astonishing advances in deep learning (Jumper et al., 2021; Rombach et al., 2021; Ramesh et al., 2022; OpenAI, 2023; Touvron et al., 2023; Jiang et al., 2023; Merchant et al., 2023), the potential of AI in the design of nanomaterials seems truly immense. However, one has to possess large volumes of carefully curated data to fully exploit the power of AI. As discussed earlier, accumulating the data appropriate for the prediction of nanomaterial morphology has been a major challenge for decades. Within realistic data constraints, the boundaries of AI in the design of nanomaterials are underexplored.\"}"}
{"id": "5nuW5iBAJS", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"One of the goals of this work was to showcase possible applications of the most recent advances in machine learning to the design of nanomaterials, and bridge the gap between the experimentalists and the machine learning experts. A huge number of scientific groups are engaged in optimization of various parameters of nanomaterials, in particular morphology (Shandilya et al., 2022; Jiang et al., 2022; Tu et al., 2022; Kommula et al., 2024), as it allows new industrial applications or improves the properties of existing materials. The importance of this direction of research is also highlighted by the recent efforts of the leading AI companies, such as DeepMind (Merchant et al., 2023).\\n\\nIn this study, we aim to unveil the capabilities and limitations of AI in predicting morphology of nanomaterials. For that, we first conduct 215 experimental syntheses of calcium carbonate-based nanomaterials of different shapes and sizes. We carefully document the synthesis procedures with the parameters of experimental conditions, take SEM-images of the resulting nanoparticles, segment and manually annotate them with expert knowledge. We investigate the statistical associations in this multimodal dataset and identify features informative of nanoparticle morphology. We further use these findings to train classical ML models to predict sizes and shapes of nanoparticles and achieve 0.77 and 0.80 average accuracy, respectively. For the first time in the field of nanomaterial synthesis, we explore the potential of LLMs for prediction tasks. Using few-shot methods, we utilize state-of-the-art models, such as GPT-4, to predict the shapes of nanomaterials and achieve an impressive 0.81 average accuracy. Finally, we augment the available data to prototype a text-to-image system aimed at generating an image of a nanoparticle based on the description of its synthesis procedure. In conclusion, we review the obtained empirical results and discuss the future of AI in the field of nanomaterial design.\\n\\n2. Related work\\n\\nOver the past 10 years, there have been several works predicting morphological properties of nanoparticles. However, the majority of them focused on size prediction considering a single experimental system, where the resulting particles conform to the same shape and their sizes can be easily standardized. Some particular examples include size prediction for silver nanoparticles (Chen et al., 2016; Shafaei & Khayati, 2020), carbon nanotubes (Iakovlev et al., 2019), agar nanospheres (Zaki et al., 2015), chitosan nanoparticles (Baharifar & Amani, 2017), polymeric nanoparticles (Shahsavari et al., 2013; Soliman et al., 2014; Youshia et al., 2017), TiO$_2$ nanoparticles (Pellegrino et al., 2020) and different methacrylates (Kimmig et al., 2021). In our work, there is no attachment to nanoparticles of a certain shape. Instead, we generate a dataset containing multiple different shapes, which greatly expands the generalizability of our approach and enables future transfer learning applications. In addition, unlike many previous studies, we provide the data for benchmarking and the code for reproducibility. A few published works specialize in predicting the shapes of nanoparticles (Timoshenko et al., 2017; Chen et al., 2020; Yao et al., 2022), but they too have certain shortcomings. For example, Timoshenko et al. created a model that takes experimental X-ray absorption near-edge structure (XANES) spectroscopy data as input to predict the 3D structure of metallic nanoparticles (Timoshenko et al., 2017). Although circumventing the need for SEM analysis, this approach still requires actual synthesis and experimental evaluation of other properties to predict the shape of the nanomaterial. This narrows down the list of possible applications significantly. In contrast, our work explores data-driven approaches that only use features of the past syntheses to predict morphology of potentially new nanomaterials. More advanced deep learning algorithms have also found applications in the creation of new nanomaterials (Roccapriore et al., 2021; Xu et al., 2023). In the paper by Kim, Han, and Han, a model based on convolutional neural networks was proposed capable of determining the morphology of nanomaterials based on the SEM images (Kim et al., 2020). Such efforts help to better understand morphological properties of nanomaterials and simplify data labeling for the future predictive approaches. However, they do not avoid tedious experimental work preparing the datasets of SEM images, by design. Ultimately, our work stands out by predicting SEM images of nanoparticles of different morphologies based on the properties of the corresponding syntheses, which is an inverse problem formulation.\\n\\nRecent advances in natural language processing (OpenAI et al., 2023; Jiang et al., 2023; Touvron et al., 2023) have also been reflected in some areas of chemistry. Recently, there have been studies that describe the use of LLMs, in particular using the few-shot method, to predict the characteristics of various chemical objects (Zheng et al., 2023) and even to generate new chemical structures (Jablonka et al., 2022). However, the potential application of LLMs to predict the morphology of nanomaterials has not yet been investigated.\\n\\nVarious multimodal systems have been proposed recently in application to nanomaterial science (Kononova et al., 2019; Lee et al., 2020; Hiszpanski et al., 2020). Since the emergence of Stable Diffusion (Rombach et al., 2021) and DALL-E (Ramesh et al., 2022), image generation models have attractively much public attention. A recent work in nanofabrication presented an image-to-image system capable of predicting the postfabrication appearance of structures manufactured by focused ion beam milling (Buchnev et al., 2022). Although a very specialized application, this work demonstrates the potential of AI in the field of nanomaterials.\"}"}
{"id": "5nuW5iBAJS", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Dataset preparation\\n\\nTo obtain the most reliable and standardized dataset, we performed 215 syntheses of calcium carbonate-based nanomaterials. As mentioned above, usually up to several dozen experiments are conducted to perform optimization of nanomaterial properties. When using machine learning, more samples are usually needed to build predictive models, but due to the resource-intensive and time-consuming nature of synthesizing and analyzing nanomaterials, most of the morphology prediction works described above are limited to about a hundred syntheses. In this work, we generated a dataset that is double that size.\\n\\nWe considered a single chemical system of calcium carbonate, because of its rich variety of nanoparticle shapes and sizes. By making this study design choice, we were hoping to achieve better generalization of our work to other nanoparticles, since most of the known shapes are already represented in our dataset.\\n\\nFor each synthesis, we documented all variable parameters, such as names of reagents, solvents, etc., their concentrations, temperature and reaction time, as well as other synthesis parameters. Additionally, for each synthesis, one most representative SEM-image was taken, which clearly shows nanoparticles with distinguishable sizes and shapes.\\n\\nWe thoroughly analyzed shapes and sizes of the resulting nanoparticles and identified five different shapes: cubic, spherical, stick-shaped, flat, and amorphous. For each shape, except flat and amorphous, we distinguished small-, medium- and large-sized nanoparticles applying an empirical threshold. In the case of amorphous and flat particles, the number of samples was too small to consider differentiation. Altogether, we used 5 different shape categories and 9 different categories combining shapes and sizes to label the dataset.\\n\\nTo train the variational autoencoder in the text-to-image setup described later, each image from the original dataset with 215 syntheses was segmented to extract multiple images of individual nanoparticles using ImageJ (Rueden et al., 2017). The resulting dataset was further augmented to increase the size of the dataset and decrease the probability of overfitting. For that, we generated new images by applying random rotations, different blurring and brightness settings. In total, the training dataset contained 46,800 images of individual nanoparticles.\\n\\n4. Feature selection\\n\\nEach synthesis in our dataset was described by 10 continuous and 3 categorical variables that might be influencing the shapes of nanomaterials in different ways. This section describes statistical evaluation of those features to determine whether they are indeed informative of the geometry of nanomaterials, which served as a basis for downstream AI applications.\\n\\n4.1. Analysis of continuous variables\\n\\nLet \\\\((X_1^1, X_1^2, \\\\ldots, X_1^n)\\\\) denote real values of a parameter of a synthesis which produces cubic nanoparticles. Let \\\\((X_2^1, X_2^2, \\\\ldots, X_2^m)\\\\) denote the real values of the same parameter of any synthesis which always results in nanoparticles of different shapes. We wondered whether the two samples came from the same population or not. If so, each value of the first sample would have had an equal chance of being larger than each value of the second sample. Therefore, the null hypothesis can be formulated as follows:\\n\\n\\\\[ H_0: p(X_1^i > X_2^j) = \\\\frac{1}{2} \\\\]\\n\\nIn fact, this formulation represents the Mann-Whitney U test (Nachar, 2008). We applied it for each of the real-valued parameters of synthesis and each type of the nanomaterial shapes. We found that formation of stick-shaped nanoparticles was dependent on the reaction temperature, synthesis time, and polymer mass and/or concentration. Cubic shapes of nanoparticles were also associated with certain temperatures and polymer concentrations, as well as the molar mass of the polymer. We used the Kruskal-Wallis H test (Kruskal & Wallis, 1952), which is analogous to Mann-Whitney U test but applicable to three and more sample groups, Kolmogorov-Smirnov test (Smirnov, 1939) and ANOVA (Marsal, 1987) to corroborate these findings. Herewith, we used the significance level = 0.05 and the Bonferroni correction method to account for multiple hypothesis testing.\\n\\n4.2. Analysis of categorical variables\\n\\nTo establish relationships between categorical parameters of synthesis procedures and the corresponding shapes of nanomaterials, we composed contingency tables as shown in Table 1.\\n\\nAccording to Fisher, \\\\(a \\\\sim \\\\text{Hypergeometric}(N, K, n)\\\\), where \\\\(N = a + b + c + d\\\\) is the population size, \\\\(K = a + b\\\\) is the number of successes and \\\\(n = a + c\\\\) is the number of draws (Fisher, 1922). Therefore, the probability of this outcome is given by:\"}"}
{"id": "5nuW5iBAJS", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "5nuW5iBAJS", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shandilya, P., Sambyal, S., Sharma, R., Mandyal, P., and Fang, B. Properties, optimized morphologies, and advanced strategies for photocatalytic applications of \\\\( \\\\text{WO}_3 \\\\) based photocatalysts. *Journal of Hazardous Materials*, 428:128218, 2022. ISSN 0304-3894. doi: https://doi.org/10.1016/j.jhazmat.2022.128218. URL https://www.sciencedirect.com/science/article/pii/S0304389422000061. \\n\\nSmirnov, N. Estimate of deviation between empirical distribution functions in two independent samples. 2(2)(6.1, 6.2):3\u201316, 1939.\\n\\nSmith, K. C. A. and Oatley, C. W. The scanning electron microscope and its fields of application. *British Journal of Applied Physics*, 6(11):391\u2013399, November 1955. ISSN 0508-3443. doi: 10.1088/0508-3443/6/11/304. URL https://iopscience.iop.org/article/10.1088/0508-3443/6/11/304.\"}"}
{"id": "5nuW5iBAJS", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unveiling the Potential of AI for Nanomaterial Morphology Prediction\\n\\nSurface Modification of Nanoparticle-Based Drug Formulations.\\n\\nAAPS PharmSciTech, 23(5):150, July 2022. ISSN 1530-9932. doi: 10.1208/s12249-022-02303-y. URL https://link.springer.com/10.1208/s12249-022-02303-y.\\n\\nTimoshenko, J., Lu, D., Lin, Y., and Frenkel, A. I. Supervised Machine-Learning-Based Determination of Three-Dimensional Structure of Metallic Nanoparticles. The Journal of Physical Chemistry Letters, 8(20):5091\u20135098, October 2017. ISSN 1948-7185, 1948-7185. doi: 10.1021/acs.jpclett.7b02364. URL https://pubs.acs.org/doi/10.1021/acs.jpclett.7b02364.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open Foundation and Fine-Tuned Chat Models. 2023. doi: 10.48550/ARXIV.2307.09288. URL https://arxiv.org/abs/2307.09288. Publisher: arXiv. Version Number: 2.\\n\\nTu, X., Ge, L., Deng, L., and Zhang, L. Morphology adjustment and optimization of cus as enzyme mimics for the high efficient colorimetric determination of Cr(vi) in water. Nanomaterials, 12(12), 2022. ISSN 2079-4991. doi: 10.3390/nano12122087. URL https://www.mdpi.com/2079-4991/12/12/2087.\\n\\nVaidyanathan, G. and Sendhilnathan, S. Characterization of Co1xZnxFe2O4 nanoparticles synthesized by co-precipitation method. Physica B: Condensed Matter, 403(13-16):2157\u20132167, July 2008. ISSN 0921-4526. doi: 10.1016/j.physb.2007.08.219. URL https://linkinghub.elsevier.com/retrieve/pii/S0921452607008216.\\n\\nWang, Z., Bovik, A., Sheikh, H., and Simoncelli, E. Image Quality Assessment: From Error Visibility to Structural Similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, April 2004. ISSN 1057-7149. doi: 10.1109/TIP.2003.819861. URL http://ieeexplore.ieee.org/document/1284395/.\\n\\nXu, Y., Xu, D., Yu, N., Liang, B., Yang, Z., Asif, M. S., Yan, R., and Liu, M. Machine Learning Enhanced Optical Microscopy for the Rapid Morphology Characterization of Silver Nanoparticles. ACS Applied Materials & Interfaces, 15(14):18244\u201318251, April 2023. ISSN 1944-8244, 1944-8252. doi: 10.1021/acsami.3c02448. URL https://pubs.acs.org/doi/10.1021/acsami.3c02448.\\n\\nYao, L., An, H., Zhou, S., Kim, A., Luijten, E., and Chen, Q. Seeking regularity from irregularity: unveiling the synthesis\u2013nanomorphology relationships of heterogeneous nanomaterials using unsupervised machine learning. Nanoscale, 14(44):16479\u201316489, 2022. ISSN 2040-3364, 2040-3372. doi: 10.1039/D2NR03712B. URL http://xlink.rsc.org/?DOI=D2NR03712B.\\n\\nYoushia, J., Ali, M. E., and Lamprecht, A. Artificial neural network based particle size prediction of polymeric nanoparticles. European Journal of Pharmaceutical and Biopharmaceutics, 119:333\u2013342, October 2017. ISSN 0939-6411. doi: 10.1016/j.ejpb.2017.06.030. URL https://linkinghub.elsevier.com/retrieve/pii/S0939641117303570.\\n\\nZaki, M. R., Varshosaz, J., and Fathi, M. Preparation of agar nanospheres: Comparison of response surface and artificial neural network modeling by a genetic algorithm approach. Carbohydrate Polymers, 122:314\u2013320, May 2015. ISSN 0144-8617. doi: 10.1016/j.carbpol.2014.12.031. URL https://linkinghub.elsevier.com/retrieve/pii/S0144861714012272.\\n\\nZebarjadi, M., Esfarjani, K., Bian, Z., and Shakouri, A. Low-Temperature Thermoelectric Power Factor Enhancement by Controlling Nanoparticle Size Distribution. Nano Letters, 11(1):225\u2013230, January 2011. ISSN 1530-6984, 1530-6992. doi: 10.1021/nl103581z. URL https://pubs.acs.org/doi/10.1021/nl103581z.\\n\\nZheng, Z., Rong, Z., Rampal, N., Borgs, C., Chayes, J. T., and Yaghi, O. M. A gpt-4 reticular chemist for guiding mof discovery. Angewandte Chemie International Edition, 62(46):e202311983, 2023.\"}"}
{"id": "5nuW5iBAJS", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Kolmogorov\u2013Smirnov statistic in this case is:\\n\\n\\\\[ D_{n,m} = \\\\sup_x |F_{1,n}(x) - F_{2,m}(x)|, \\\\]\\n\\nwhere \\\\( \\\\sup \\\\) is the supremum function.\\n\\nThe null hypothesis is that the two samples are from the same continuous distribution. The null hypothesis is rejected at level \\\\( \\\\alpha = 0.05 \\\\) if\\n\\n\\\\[ D_{n,m} > p - \\\\ln(\\\\alpha/2) \\\\cdot (1 + m/n)/2. \\\\]\\n\\nWe applied this test for each of the real-valued parameters of synthesis and each type of nanomaterial shape and used the Bonferroni correction method similarly to the previous tests. The results of this test were similar to those of the previous two, except that in the case of stick-shaped nanoparticles, the dependence was observed on the parameter characterizing the mass of the polymer rather than its concentration, which is not surprising given the similar nature of these two parameters.\\n\\n### ANOVA (Marsal, 1987)\\n\\n\\\\( (X_{i1}, X_{i2}, \\\\ldots, X_{in}) \\\\) be independent, identically distributed real values of a parameter of a synthesis that produces nanoparticles of a specific shape with the common cumulative distribution function \\\\( F_{i,n} \\\\) with the mean \\\\( X_i \\\\). The formula for the one-way ANOVA F-test statistic is:\\n\\n\\\\[ F = \\\\frac{\\\\sum_{i=1}^K n_i (X_i - \\\\bar{X})^2 / (K-1)}{\\\\sum_{i=1}^K \\\\sum_{j=1}^{n_i} (X_{ij} - X_i)^2 / (N-K)}, \\\\]\\n\\nwhere \\\\( \\\\bar{X}_i \\\\) denotes the sample mean in the \\\\( i \\\\)-th group, \\\\( n_i \\\\) is the number of observations in the \\\\( i \\\\)-th group, \\\\( X \\\\) denotes the overall mean of the population, and \\\\( K \\\\) denotes the number of groups, where \\\\( X_{ij} \\\\) is the \\\\( j \\\\)-th observation in the \\\\( i \\\\)-th out of \\\\( K \\\\) groups and \\\\( N \\\\) is the overall sample size. The null hypothesis can be formulated as follows:\\n\\n\\\\[ X_i = X_j, \\\\text{ for each two groups } i \\\\text{ and } j. \\\\]\\n\\nIf the F-statistic is greater than the critical p-value (at the significance level \\\\( \\\\alpha = 0.05 \\\\)), then the null hypothesis is rejected and distributions of this synthesis parameter in the case of at least two different shapes are different. We applied this test for each of the real-valued parameters of synthesis and each type of nanomaterial shape and used the Bonferroni correction method similarly to the previous tests. The results of this test were consistent with the results of the first two tests, except that it failed to confirm the relationship between the shape of nanoparticle and polymer mass, although polymer concentration was still a significant parameter.\\n\\n### Table 6\\n\\nSignificant associations between features of the synthesis and the corresponding shapes of nanoparticles. The table shows the parameters that turned out to be determinant in the synthesis of nanomaterials of one or another shape. For continuous features, the following tests were used: Mann\u2013Whitney U test, Kruskal\u2013Wallis H test, Kolmogorov\u2013Smirnov test, ANOVA. Fisher exact and Chi-squared tests was used for categorical features.\\n\\n| Shape   | Stick-shaped | Spherical | Flat | Cubic | Amorphous |\\n|---------|--------------|-----------|------|-------|-----------|\\n| Continuous | Temperature, C | Solvent, % vol. | Polymer, % wt. | Polymer Mwt, kDa | Polymer Mwt, kDa |\\n| Categorical | Myristyltrimethylammonium bromide | Sodium dodecylsulfate | Sodium dodecylsulfate | PAA | PEG PSS PAA Isopropyl alcohol PEI PVP PSS tert-Butanol | Polymer absence | Propylene glycol |\\n\\n### A.2. Tree-based ensemble models\\n\\nIn order to achieve the best results for each of the Random Forest and Gradient Boosted Trees models, we optimized the hyperparameters for each of them, and built the models with different splits of the original dataset. The final metrics for each model were calculated by predicting at 5 different random states, after which the mean value as well as the standard deviation were calculated. Most of the functions used to prepare the dataset and use the models were implemented using the...\"}"}
{"id": "5nuW5iBAJS", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unveiling the Potential of AI for Nanomaterial Morphology Prediction\\n\\nIn case of the Random Forest, optimization of the following parameters was performed: \\\\( n_{\\\\text{estimators}} \\\\), \\\\( \\\\text{max}_{\\\\text{features}} \\\\), \\\\( \\\\text{max}_{\\\\text{depth}} \\\\), \\\\( \\\\text{min}_{\\\\text{samples leaf}} \\\\), \\\\( \\\\text{max}_{\\\\text{leaf nodes}} \\\\). In case of Gradient Boosted Trees, the optimized parameters were: \\\\( \\\\text{gamma} \\\\), \\\\( \\\\text{colsample bytree} \\\\), \\\\( \\\\text{max}_{\\\\text{depth}} \\\\), \\\\( \\\\text{estimators} \\\\), \\\\( \\\\text{learning rate} \\\\). Hyperparameter optimization was performed using 5-fold cross-validated grid-search. Given that some target classes were underrepresented, we prepared three test sets in advance for a more thorough assessment of performance. The test sets contained 33%, 20% or 15% of the total number of samples. A summary Table 7 provides motivation for testing several data splits. In our case, the lowest mean standard deviation was observed in 33% test split for both, accuracy and F1 score, among all the experiments. Also, for each model, the optimal threshold was found to solve the problem of class imbalance. This was achieved by balancing precision and recall metrics.\\n\\nTable 7. Comparison of different data splits. A representative test set was obtained with 33% of total number of samples.\\n\\n| Validation subset | Average size of dataset, % | Average accuracy | Average standard deviation of accuracy | Average F1 score | Average standard deviation of F1 score |\\n|-------------------|---------------------------|-----------------|--------------------------------------|-----------------|----------------------------------|\\n| 33                | 0.74                      | 0.11            |                                      | 0.53            | 0.09                             |\\n| 20                | 0.69                      | 0.13            |                                      | 0.51            | 0.11                             |\\n| 15                | 0.65                      | 0.18            |                                      | 0.49            | 0.15                             |\\n\\nFor the best models, we also performed feature importance analysis by constructing SHAP diagrams showing the most important features in model performance. Figure 4 below shows the 10 most important features for the Random Forest model with optimal parameters predicting the stick shaped nanomaterials. Among these features, statistical relationship with the given shape of nanomaterials was confirmed for the following features: 'Temperature, C', 'Synthesis time', 'Polymer, % wt.', 'Polymer Mwt, kDa', 'Sodium dodecylsulfate', 'PEI'. This is an additional validation of our models, as the results of the analysis of feature importance almost completely correspond to the previously discovered statistical patterns that were presented in Table 6.\\n\\nFigure 4. Results of feature importance analysis in the form of SHAP values for the top 10 features for the best Random Forest model for predicting stick shaped nanoparticles.\\n\\nA.3. Texts of synthesis procedures and prompts\\n\\nIn order to make predictions of the morphology of nanomaterials based on their synthesis text using LLMs, special templates were created, which were then used to be filled with parameters for a particular synthesis. From these, the final textual prompt for LLM was compiled. These templates were similarly used in the development of a generative text-to-image system. Two examples of such templates are given below.\\n\\nTemplate example 1:\\n\\n\\\"Synthesis was carried out using the co-precipitation technique. Initially, ca \\\\( x \\\\) conc mkl of 1 M CaCl\\\\(_2\\\\) was combined with...\\\"\"}"}
{"id": "5nuW5iBAJS", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unveiling the Potential of AI for Nanomaterial Morphology Prediction\\n\\nIn the first step, \\\\( \\\\text{conc} \\\\ mkl \\\\) of \\\\( 1 \\\\ M \\\\text{CaCl}_2 \\\\) was combined with \\\\( \\\\text{vol} \\\\ mkl \\\\) of \\\\( \\\\text{polymer} \\\\) polymer, characterized by a molecular weight of \\\\( \\\\text{mass} \\\\ kDa \\\\). This was followed by the addition of \\\\( \\\\text{volume} \\\\ mkl \\\\) of \\\\( \\\\text{solvent} \\\\), and the volume was adjusted to 500 mkl using distilled water.\\n\\nIn the subsequent step, \\\\( \\\\text{conc} \\\\ mkl \\\\) of 0.1 \\\\( \\\\text{M Na}_2\\\\text{CO}_3 \\\\), \\\\( \\\\text{conc} \\\\ mkl \\\\) of 0.1 \\\\( \\\\text{M NaH}_2\\\\text{CO}_3 \\\\), and \\\\( \\\\text{vol} \\\\ mkl \\\\) of \\\\( \\\\text{surf} \\\\) surfactant serving as the surfactant were combined. Once more, \\\\( \\\\text{volume} \\\\ mkl \\\\) of \\\\( \\\\text{solvent} \\\\) was added, and the volume was adjusted to 500 mkl using distilled water. Finally, two solutions, both heated to \\\\( \\\\text{temp} \\\\ C \\\\) before the reaction, were mixed under stirring at \\\\( \\\\text{stir} \\\\) ratio rpm while maintaining the temperature. The reaction proceeded for \\\\( \\\\text{time} \\\\ min \\\\), followed by centrifugation.\\n\\nThe synthesis of CaCO\\\\(_3\\\\) nanoparticles was performed according to the following procedure. In separate burettes, two solutions were made, 57 mkl of 1 M \\\\( \\\\text{CaCl}_2 \\\\) and 20 mkl of 0.155 % wt. PEI with a molecular weight of 25.0 kDa were mixed in 200.0 mkl of 1-Hexanol before dilution with distilled water up to 500 mkl. Similarly, 140 mkl of 0.1 M \\\\( \\\\text{Na}_2\\\\text{CO}_3 \\\\), 20 mkl of 0.1 M \\\\( \\\\text{NaH}_2\\\\text{CO}_3 \\\\), and 20 mkl of 0.43 % wt. Myristyltrimethylammonium bromide and 200.0 mkl of 1-Hexanol were combined. Then, the solution was also diluted in 500 mkl of water. Both solutions were heated up to 68 C right before mixing under stirring at 1000 rpm for 8 min 0 sec following centrifugation.\\n\\nAnswer: 'Cube, Stick'\\n\\nA.4. Few-shot classification\\n\\nTo optimize the number of input examples and the proportion of the test subset, experiments were conducted with the GPT-4 model for the text subset. The table below summarizes these results (Table 8). The low impact of the proportion of the test subset is obvious, but the number of input examples has a significant impact on the metrics.\\n\\nA.5. VAE: implementation details\\n\\nWe have experimented with several ResNet architectures but also developed a few custom architectures for the VAE. ResNet is the classical convolutional neural network originally proposed for the classification tasks (He et al., 2015). It consists of a stack of residual blocks, where each block consists of a 2D convolutional layer, a batch normalization layer, and a ReLU activation function. The residual block allows the network to learn a residual function rather than the original function, which often leads to faster convergence and higher accuracy.\"}"}
{"id": "5nuW5iBAJS", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unveiling the Potential of AI for Nanomaterial Morphology Prediction\\n\\nTable 8.\\nAverage accuracy of GPT-4 for different number of input samples in prompt\\n\\n| Input Test Shape | Average samples subset size | Cube | Stick | Sphere | Flat | Amorphous |\\n|------------------|-----------------------------|------|-------|--------|------|-----------|\\n|                  | 2                           | 0.15 | 0.29\u00b10.14 | 0.58\u00b10.15 | 0.27\u00b10.10 | 0.92\u00b10.02 | 0.33\u00b10.12 | 0.48\u00b10.10 |\\n|                  | 0.2                         | 0.40\u00b10.09 | 0.56\u00b10.12 | 0.20\u00b10.10 | 0.92\u00b10.03 | 0.38\u00b10.15 | 0.49\u00b10.10 |\\n|                  | 0.33                        | 0.33\u00b10.12 | 0.59\u00b10.05 | 0.25\u00b10.07 | 0.93\u00b10.02 | 0.30\u00b10.07 | 0.48\u00b10.07 |\\n|                  | 4                           | 0.15 | 0.54\u00b10.15 | 0.64\u00b10.05 | 0.38\u00b10.14 | 0.93\u00b10.01 | 0.64\u00b10.23 | 0.63\u00b10.12 |\\n|                  | 0.2                         | 0.59\u00b10.13 | 0.56\u00b10.06 | 0.49\u00b10.12 | 0.89\u00b10.09 | 0.73\u00b10.12 | 0.65\u00b10.10 |\\n|                  | 0.33                        | 0.56\u00b10.15 | 0.62\u00b10.09 | 0.54\u00b10.09 | 0.92\u00b10.05 | 0.68\u00b10.07 | 0.66\u00b10.09 |\\n|                  | 6                           | 0.15 | 0.76\u00b10.08 | 0.65\u00b10.11 | 0.63\u00b10.14 | 0.94\u00b10.00 | 0.79\u00b10.14 | 0.75\u00b10.09 |\\n|                  | 0.2                         | 0.67\u00b10.08 | 0.56\u00b10.13 | 0.68\u00b10.17 | 0.90\u00b10.08 | 0.88\u00b10.03 | 0.74\u00b10.10 |\\n|                  | 0.33                        | 0.71\u00b10.17 | 0.61\u00b10.11 | 0.70\u00b10.10 | 0.93\u00b10.07 | 0.78\u00b10.14 | 0.74\u00b10.12 |\\n|                  | 8                           | 0.15 | 0.84\u00b10.07 | 0.62\u00b10.11 | 0.81\u00b10.10 | 0.96\u00b10.03 | 0.84\u00b10.08 | 0.81\u00b10.08 |\\n|                  | 0.2                         | 0.72\u00b10.08 | 0.67\u00b10.05 | 0.80\u00b10.10 | 0.91\u00b10.08 | 0.87\u00b10.10 | 0.80\u00b10.08 |\\n|                  | 0.33                        | 0.74\u00b10.11 | 0.63\u00b10.08 | 0.80\u00b10.08 | 0.90\u00b10.13 | 0.83\u00b10.12 | 0.78\u00b10.10 |\\n|                  | 10                          | 0.15 | 0.78\u00b10.06 | 0.59\u00b10.11 | 0.84\u00b10.07 | 0.94\u00b10.03 | 0.88\u00b10.07 | 0.81\u00b10.07 |\\n|                  | 0.2                         | 0.71\u00b10.05 | 0.68\u00b10.05 | 0.88\u00b10.05 | 0.90\u00b10.10 | 0.87\u00b10.12 | 0.81\u00b10.07 |\\n|                  | 0.33                        | 0.74\u00b10.07 | 0.66\u00b10.06 | 0.77\u00b10.07 | 0.90\u00b10.15 | 0.84\u00b10.12 | 0.78\u00b10.09 |\\n\\nTable 9.\\nComparison of computational resources of LLMs: time per one complete experiment (in case of text dataset, an average prompt was around 3000 tokens), price per 1M tokens in USD, limits for requests per minute and 1000 tokens per minute.\\n\\n| Model             | Time per experiment, s | Input price per 1M tokens, USD | Tokens limit, 1000/min | Requests limit, 1/min |\\n|-------------------|-------------------------|---------------------------------|-------------------------|-----------------------|\\n| Mistral-medium    | 46                      | 2.7                             | 2000                    | 120                   |\\n| Mistral-small     | 21                      | 0.7                             | 2000                    | 120                   |\\n| Mistral-tiny      | 19                      | 0.15                            | 2000                    | 120                   |\\n| GPT-3.5-turbo     | 44                      | 26                              | 60                      | 500                   |\\n| GPT-4             | 63                      | 0.5                             | 10                      | 500                   |\\n| GPT-4-turbo       | 53                      | 30                              | 150                     | 500                   |\\n\\nJens Behrmann et al. showed that invertible ResNets can also be used as generative models (Behrmann et al., 2018) and, therefore, we used the reversed ResNet from PyTorch Lightning Bolts as a decoder for the VAE. Additionally, we developed several shallow networks varying the number of convolutional blocks and the dimensionality of the bottleneck layer as custom VAE architectures.\\n\\nWe trained all the architectures in the grid search setup optimizing several hyperparameters, such as batch size, learning rate, Kullback-Leibler (KL) divergence coefficient and image size, to achieve the lowest BCE loss.\\n\\nBased on the training losses and the evaluation metrics described in Appendix A.6, we selected one of the custom architectures as the best. We failed to achieve on-par performance with the ResNet backbones, likely due to insufficient number of training examples. The top-performant VAE architecture had only 4 convolutional blocks for the encoder and 4 upsampling blocks for the decoder with 4096 dimensions in the latent space. The optimal set of hyperparameters was 128\u00d7128 for the image size, 64 for the batch size, 0.001 for the learning rate, and 0.01 for the KL divergence coefficient. The corresponding training curves are depicted on Figure 3.\\n\\n1 https://lightning-bolts.readthedocs.io/en/latest/\"}"}
{"id": "5nuW5iBAJS", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unveiling the Potential of AI for Nanomaterial Morphology Prediction\\n\\nA.6. V AE: metrics\\n\\nFor each combination of hyperparameters, the trained V AEs were evaluated on the test set. Two metrics reflecting the similarity of the original and the decoded images were used to compare architectures: structural similarity index measure (SSIM) (Wang et al., 2004) and peak signal-to-noise ratio (PSNR) (Fardo et al., 2016). SSIM is a standardized measure of the difference between the compressed and the original image, ranging from -1 to 1. It is defined by the following formula:\\n\\n$$SSIM(X, Y) = \\\\frac{2\\\\mu_X\\\\mu_Y + c_1}{\\\\mu^2_X + \\\\mu^2_Y + c_1} \\\\frac{2\\\\sigma_{XY} + c_2}{\\\\sigma^2_X + \\\\sigma^2_Y + c_2}$$\\n\\nwhere $X, Y$ are the images, $\\\\mu_X, \\\\mu_Y$ are the mean pixel values of the images, $\\\\sigma_X, \\\\sigma_Y$ are the variances of the pixel values, $\\\\sigma_{XY}$ is the covariance, and $c_1, c_2$ are the coefficients stabilizing the division. PSNR is a simpler metric showing the ratio of the contribution of the maximum value of the original image to the contribution of noise in the compressed image. This metric is calculated using the following formula:\\n\\n$$PSNR(X, Y) = 20 \\\\log_{10} \\\\frac{\\\\max(X) \\\\cdot \\\\text{pe}(X, Y)}{\\\\text{e}(X, Y)}$$\\n\\nwhere $X$ is the original image of size $m \\\\times n$, $Y$ is the compressed image of the same size, and $\\\\text{e}(X, Y) = \\\\frac{1}{mn} \\\\sum_{i=1}^{m} \\\\sum_{j=1}^{n} (x_{ij} - y_{ij})^2$ is the mean squared deviation between the pixels of two images.\\n\\nA.7. \u201cLinking\u201d V AE: training and generation phases\\n\\nThe training process was organized into the following key steps. For each training example:\\n\\n1. Choose a text template of a synthesis procedure randomly and fill in the corresponding experimental parameters.\\n2. Obtain text representations with a pretrained BERT.\\n3. Obtain image representations with a pretrained V AE.\\n4. Perform a forward pass to convert text representations into image representations.\\n5. Calculate the loss and backpropagate the error.\\n\\nAfter the training is done, morphology of a new nanomaterial described in a synthesis procedure can be predicted as follows:\\n\\n1. Obtain representations of the synthesis procedure text with a pretrained BERT.\\n2. Apply the \u201clinking\u201d autoencoder to predict the corresponding image representations.\\n3. Apply the decoder part of the V AE to predict the image of the nanomaterial.\\n\\nA.8. \u201cLinking\u201d V AE: best architecture\\n\\nThe final architecture of the \u201clinking\u201d V AE is shown on Figure 5A. It consists of 4 linear layers and has 768-dimensional latent space. The optimal hyperparameters were 8 for the batch size, 0.00001 for the learning rate. Figure 5B shows individual examples of nanoparticles reconstructed and generated from texts. Three different shapes are given.\\n\\nA.9. Data visualization\\n\\nWe visualized the space of learned representations of the V AE to validate the model and gain additional insights into various dependencies between the features and the target classes. For that, we used UMAP to compress the bottleneck 4096 dimensions to 2D (Figure 6). Each dot represents a single representative nanoparticle from one of the 215 syntheses. We observed five clusters having 2-3 particular shapes as the most prominent. Based on the literature and the statistical evaluation, we expected to see drastic differences in temperatures for different clusters. However, we could observe a single bottom-right cluster having lower synthesis temperatures on average.\"}"}
{"id": "5nuW5iBAJS", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2.\\n\\nA) VAE training. The images of nanoparticles are used to train a variational autoencoder (VAE). B) Final model inference. The corresponding synthesis procedures are converted into vector representations with a pretrained BERT (bottom left). The \u201clinking\u201d autoencoder is trained to map text and image representations (bottom center). Finally, the decoder of the VAE is used to generate new images of nanomaterials based on the descriptions of syntheses (bottom right).\\n\\nwith standard metrics (Kolesnikov et al., 2019) to compare several pretrained transformer-based models. We found that the classic BERT model (Devlin et al., 2018) achieved perfect scores in most tasks and, therefore, used BERT as the feature extractor in the text-to-image setup (Figure 2). It also met the requirement of being relatively lightweight, easy to start up and use.\\n\\n5.3.2. AUTOENCODER-BASED GENERATIVE MODEL\\n\\nThe most widely spread deep learning model architectures capable of generating images are generative adversarial networks (GANs) (Goodfellow et al., 2014), variational autoencoders (VAEs) (Kingma & Welling, 2013), and diffusion models (Rombach et al., 2021; Ramesh et al., 2022). We opted for a variational autoencoder as a more stable and a more suitable solution for small datasets, given the limited amount of data available for training.\\n\\nThe central idea of autoencoders is to learn a compressed representation of the input data while solving a data reconstruction problem. Variational autoencoders also imply a certain probabilistic distribution in the input data, which allows it to generate meaningful outputs by sampling the latent representation after the training is complete (Kingma & Welling, 2013). In order to plug the VAE into the text-to-image system, we first trained it on the set of SEM images and then froze the decoder part (Figure 2). Refer to Appendix A.5 and A.6 for tested VAE architectures and evaluation metrics used.\\n\\nWe validated the final VAE model by monitoring training losses and evaluation metrics (Figure 3), analyzing individual examples of reconstructed images and visualizing the space of learned representations allowing to distinguish different clusters of nanoparticle shapes (Appendix A.9).\\n\\nFigure 3.\\n\\nA) PSNR and SSIM metrics of the selected VAE by epoch. B) Training loss of the selected VAE by epoch.\\n\\n5.3.3. \u201cLINKING\u201d AUTOENCODER MODEL\\n\\nThe last component of the proposed text-to-image system is the \u201clinking\u201d neural network learning to map representations of the two modalities. Considering the data limitations and the empirical results described earlier, we refrained from using complex model architectures for this task. Instead, we developed another set of shallow autoencoder networks having from 3 to 8 linear layers. Like in the case of VAE, we optimized hyperparameters for each network, including the dimensionality of the latent space, to achieve the lowest reconstruction MSE (see Appendix A.7 for details on training and generation phases). The best architecture for the \u201clinking\u201d autoencoder is given in Appendix A.8.\\n\\n5.3.4. RESULTS\\n\\nWe observed that our prototype of the text-to-image system copes best with the generation of cubic nanoparticles, which was expected since the cubic shape was the most represented in the training data. For syntheses of this type of nanomaterials, the generated images were often distinct and well-shaped. It was also easier to grasp the size of cubic nanoparticles compared to other types. In general, however, the size of the dataset was insufficient to generate high quality images directly from text. Several examples of generated images are shown on Figure 5 of the Appendix A.8.\\n\\nDespite the limited applicability of this prototype, we realized that repeated image generation based on the same synthesis parameters can provide insights into the polydispersity of NPs. Polydispersity is normally defined as $P_d = \\\\frac{\\\\sigma^2}{2}$, where $\\\\sigma$ is the standard deviation of the particles.\"}"}
{"id": "5nuW5iBAJS", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unveiling the Potential of AI for Nanomaterial Morphology Prediction\\n\\nParticle diameter, and $a$ is the mean hydrodynamic radius. We performed 50 generations of amorphous NPs with the same synthesis parameters and observed maximal diameters ranging from 30 to 80 pixels. As polydispersity characterization is critical for many applications (Clayton et al., 2016), a generative model, such as the proposed prototype, could be instrumental in fast in silico screening of NPs by estimating $P_{\\\\text{dI}}$ based on the predicted images.\\n\\n6. Discussion and conclusion\\n\\nIn this work, we explored the potential of AI in predicting morphological properties of nanomaterials using the newly generated multimodal dataset of calcium carbonate nanoparticles. First, we investigated statistical associations between synthesis procedures and the resulting morphologies. Then, we trained and optimized tree-based ensemble models to predict multiple categories of nanomaterial shapes and sizes. After that, we systematically evaluated capabilities of the state-of-the-art LLMs in the same prediction tasks. Finally, we prototyped a text-to-image system to predict images of nanoparticles directly from the descriptions of syntheses. Notably, this work stands out by creating a new dataset of multiple types of nanoparticle shapes, which can be used for benchmarking in the future. This dataset opens up the possibility of predicting the shape of nanomaterials as it represents nanomaterials of multiple shapes within a single chemical system. Also, to our knowledge, we are the first to train machine learning models to distinguish between nanomaterial shapes based on the synthesis parameters. Despite these achievements, there are still several unresolved issues in the field and certain limitations to the proposed models. A more detailed discussion and a comparison with previous works are offered in Appendix A.11 and Appendix A.10, respectively.\\n\\nWhile text-to-image applications remain largely infeasible due to the limited data availability, we identified a huge potential for future LLM applications. Not only did we observe on par performance with the classical ensemble models, we also managed to collect evidence for the superior performance of LLMs, especially in the small data scenarios. Ensemble methods for LLMs now look as a promising research direction, as less computationally expensive models like Mistral-small approach the market leader's performance in the domain-specific tasks.\\n\\n7. Data and code availability\\n\\nAll datasets, scripts and results described in this work are available for reproducibility and possible transfer learning applications in this repository: https://github.com/acid-design-lab/Nanomaterial_Morphology_Prediction.\\n\\nAcknowledgements\\n\\nThe authors would like to thank the reviewers for their valuable feedback. The work was financially supported by the Priority 2030 Federal Academic Leadership Program.\\n\\nImpact statement\\n\\nModern machine learning techniques are evolving rapidly and sometimes find their applications almost immediately. However, in the experimental fields of science, such as materials science, the introduction of new computational methods is rather slow. Therefore, the primary goal of our work was to showcase possible applications of the most recent advances in machine learning to the design of nanomaterials, and bridge the gap between the experimentalists and the machine learning experts. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.\\n\\nReferences\\n\\nAbdelHamid, A., Mendoza-Garcia, A., and Ying, J. Advances in and prospects of nanomaterials' morphological control for lithium rechargeable batteries. *Nano Energy*, 93, March 2022. ISSN 2211-2855. doi: 10.1016/j.nanoen.2021.106860. Publisher Copyright: \u00a9 2021 The Authors.\\n\\nBaharifar, H. and Amani, A. Size, Loading Efficiency, and Cytotoxicity of Albumin-Loaded Chitosan Nanoparticles: An Artificial Neural Networks Study. *Journal of Pharmaceutical Sciences*, 106(1):411\u2013417, January 2017. ISSN 0022-3549. doi: 10.1016/j.xphs.2016.10.013. URL https://linkinghub.elsevier.com/retrieve/pii/S0022354916417922.\\n\\nBanaye Yazdipour, A., Masoorian, H., Ahmadi, M., Mohammadzadeh, N., and Ayyoubzadeh, S. M. Predicting the toxicity of nanoparticles using artificial intelligence tools: a systematic review. *Nanotoxicology*, 17(1):62\u201377, January 2023. ISSN 1743-5390, 1743-5404. doi: 10.1080/17435390.2023.2186279. URL https://www.tandfonline.com/doi/full/10.1080/17435390.2023.2186279.\\n\\nBehrmann, J., Grathwohl, W., Chen, R. T. Q., Duvenaud, D., and Jacobsen, J.-H. Invertible Residual Networks. 2018. doi: 10.48550/ARXIV.1811.00995. URL https://arxiv.org/abs/1811.00995. Publisher: arXiv. Version Number: 3.\\n\\nBuchnev, O., Grant-Jacob, J. A., Eason, R. W., Zhilev, N. I., Mills, B., and MacDonald, K. F. Deep-Learning-Assisted Focused Ion Beam Nanofabrication. *Nano Letters*, 22(7):2734\u20132739, April 2022. ISSN 9\\n\\n...\"}"}
{"id": "5nuW5iBAJS", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "5nuW5iBAJS", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unveiling the Potential of AI for Nanomaterial Morphology Prediction\\n\\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., Z\u00eddek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S., Ballard, A. J., Cowie, A., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., Back, T., Petersen, S., Reiman, D., Clancy, E., Zielinski, M., Steinegger, M., Pacholska, M., Berghammer, T., Bodenstein, S., Silver, D., Vinyals, O., Senior, A. W., Kavukcuoglu, K., Kohli, P., and Hassabis, D. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583\u2013589, August 2021. ISSN 0028-0836, 1476-4687. doi: 10.1038/s41586-021-03819-2. URL https://www.nature.com/articles/s41586-021-03819-2.\\n\\nKairdolf, B. A., Qian, X., and Nie, S. Bioconjugated Nanoparticles for Biosensing, in Vivo Imaging, and Medical Diagnostics. Analytical Chemistry, 89(2):1015\u20131031, January 2017. ISSN 0003-2700, 1520-6882. doi: 10.1021/acs.analchem.6b04873. URL https://pubs.acs.org/doi/10.1021/acs.analchem.6b04873.\\n\\nKhwaja, E., Song, Y. S., and Huang, B. CELL-E: Biological Zero-Shot Text-to-Image Synthesis for Protein Localization Prediction. preprint, Bioinformatics, May 2022. URL http://biorxiv.org/lookup/doi/10.1101/2022.05.27.493774.\\n\\nKim, H., Han, J., and Han, T. Y.-J. Machine vision-driven automatic recognition of particle size and morphology in SEM images. Nanoscale, 12(37):19461\u201319469, 2020. ISSN 2040-3364, 2040-3372. doi: 10.1039/D0NR04140H. URL http://xlink.rsc.org/?DOI=D0NR04140H.\\n\\nKimmig, J., Schuett, T., Vollrath, A., Zechel, S., and Schubert, U. S. Prediction of Nanoparticle Sizes for Arbitrary Methacrylates Using Artificial Neuronal Networks. Advanced Science, 8(23):2102429, December 2021. ISSN 2198-3844, 2198-3844. doi: 10.1002/advs.202102429. URL https://onlinelibrary.wiley.com/doi/10.1002/advs.202102429.\\n\\nKingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. 2013. doi: 10.48550/ARXIV.1312.6114. URL https://arxiv.org/abs/1312.6114. Publisher: arXiv Version Number: 11.\\n\\nKolesnikov, A., Zhai, X., and Beyer, L. Revisiting Self-Supervised Visual Representation Learning. 2019. doi: 10.48550/ARXIV.1901.09005. URL https://arxiv.org/abs/1901.09005. Publisher: arXiv Version Number: 1.\\n\\nKommula, B., Prabhu B, R., Kopperi, H., Bhat, V. S., Hegde, G., and John, N. S. Diverse morphologies of nb2o5 nanomaterials: A comparative study for the growth optimization of elongated spiky nb2o5 and carbon nanosphere composite. Particle & Particle Systems Characterization, 41(3):2300118, 2024. doi: https://doi.org/10.1002/ppsc.202300118. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/ppsc.202300118.\\n\\nKononova, O., Huo, H., He, T., Rong, Z., Botari, T., Sun, W., Tshitoyan, V., and Ceder, G. Text-mined dataset of inorganic materials synthesis recipes. Scientific Data, 6(1):203, October 2019. ISSN 2052-4463. doi: 10.1038/s41597-019-0224-1. URL https://doi.org/10.1038/s41597-019-0224-1.\\n\\nKruskal, W. H. and Wallis, W. A. Use of Ranks in One-Criterion Variance Analysis. Journal of the American Statistical Association, 47(260):583\u2013621, December 1952. ISSN 0162-1459, 1537-274X. doi: 10.1080/01621459.1952.10483441. URL http://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441.\\n\\nLee, B., Yoon, S., Lee, J. W., Kim, Y., Chang, J., Yun, J., Ro, J. C., Lee, J.-S., and Lee, J. H. Statistical Characterization of the Morphologies of Nanoparticles through Machine Learning Based Electron Microscopy Image Analysis. ACS Nano, 14(12):17125\u201317133, December 2020. ISSN 1936-0851, 1936-086X. doi: 10.1021/acsnano.0c06809. URL https://pubs.acs.org/doi/10.1021/acsnano.0c06809.\\n\\nLiu, R. and Lal, R. Potentials of engineered nanoparticles as fertilizers for increasing agronomic productions. Science of The Total Environment, 514:131\u2013139, May 2015. ISSN 00489697. doi: 10.1016/j.scitotenv.2015.01.104. URL https://linkinghub.elsevier.com/retrieve/pii/S0048969715001266.\\n\\nLiu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A., He, M., Liu, Z., Wu, Z., Zhu, D., Li, X., Qiang, N., Shen, D., Liu, T., and Ge, B. Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. 2023. doi: 10.48550/ARXIV.2304.01852. URL https://arxiv.org/abs/2304.01852. Publisher: arXiv Version Number: 3.\\n\\nMagnello, M. Karl Pearson, paper on the chi square goodness of fit test (1900). In Landmark Writings in Western Mathematics 1640\u20131940, pp. 724\u2013731. Elsevier, 2005. ISBN 978-0-444-50871-3. doi: 10.1016/B978-044450871-3/50137-6. URL https://linkinghub.elsevier.com/retrieve/pii/B9780444508713501376.\\n\\nMarsal, D. Introduction to the Analysis of Variance (ANOVA). In Statistics for Geoscientists, pp. 11.\"}"}
