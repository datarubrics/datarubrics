{"id": "kAFevjEYsz", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 20. Correlation between ID accuracy and OOD accuracy (odd rows); ID robustness and OOD robustness (even rows) for CIFAR10 $\\\\ell_\\\\infty$ AT models.\"}"}
{"id": "kAFevjEYsz", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 21. Correlation between ID accuracy and OOD accuracy (odd rows); ID robustness and OOD robustness (even rows) for CIFAR10 $\\\\ell_\\\\infty$ AT models.\"}"}
{"id": "kAFevjEYsz", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 22. Correlation between ID accuracy and OOD accuracy (odd rows); ID robustness and OOD robustness (even rows) for CIFAR10 $\\\\ell_2$ AT models.\"}"}
{"id": "kAFevjEYsz", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 23. Correlation between ID accuracy and OOD accuracy (odd rows); ID robustness and OOD robustness (even rows) for CIFAR10 $\\\\ell_2$ AT models.\"}"}
{"id": "kAFevjEYsz", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 24. Correlation between ID accuracy and OOD accuracy (odd rows); ID robustness and OOD robustness (even rows) for CIFAR10 $\\\\ell_2$ AT models.\"}"}
{"id": "kAFevjEYsz", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 25. Correlation between ID accuracy and OOD accuracy (odd rows); ID robustness and OOD robustness (even rows) for ImageNet $\\\\ell_\\\\infty$ AT models.\"}"}
{"id": "kAFevjEYsz", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 26. Correlation between ID accuracy and OOD accuracy (odd rows); ID robustness and OOD robustness (even rows) for ImageNet $\\\\ell_\\\\infty$ AT models.\"}"}
{"id": "kAFevjEYsz", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 27. Correlation between ID accuracy and OOD accuracy (odd rows); ID robustness and OOD robustness (even rows) for ImageNet $\\\\ell_\\\\infty$ AT models.\"}"}
{"id": "kAFevjEYsz", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness in a positive linear way. The latter enables the prediction of OOD robustness from ID robustness. We then predict and verify that existing methods are unlikely to achieve high OOD robustness. Novel methods are therefore required to achieve OOD robustness beyond our prediction. To facilitate the development of these methods, we investigate a wide range of techniques and identify several promising directions. Code and models are available at: https://github.com/OODRobustBench/OODRobustBench.\\n\\n1. Introduction\\nAdversarial attack poses a serious threat to real-world machine learning models, and various approaches have been developed to defend against such attacks. Previous work (Athalye et al., 2018) has shown that adversarial evaluation is critical to the study of adversarial robustness since an unreliable evaluation can often give a false sense of robustness. However, we believe that even state-of-the-art evaluation benchmarks (like RobustBench Croce et al., 2021) suffer from a severe limitation: they only consider ID generalization where test data comes from the same distribution as the training data. Since distribution shifts are inevitable in the real world, it is crucial to assess how adversarial robustness is affected when the test distribution differs from the training one.\\n\\nAlthough OOD generalization has been extensively studied for clean accuracy (Hendrycks & Dietterich, 2019; Taori et al., 2020; Miller et al., 2021; Baek et al., 2022; Zhao et al., 2022; Yang et al., 2022), there is little known about the OOD generalization of adversarial robustness. To fill this void, this paper presents for the first time, a comprehensive benchmark, OODRobustBench, for assessing out-of-distribution adversarial robustness. Furthermore, it reports results of a large-scale analysis of existing robust models performed using the new benchmark to answer the following questions:\\n1. How resilient are current adversarially robust models to distribution shift?\\n2. Can we predict OOD robustness from ID robustness?\\n3. How can we achieve OOD robustness?\\n\\nOODRobustBench is analogous and complementary to RobustBench which is used for assessing in-distribution adversarial robustness. It includes two categories of distribution shift: dataset shift and threat shift (see Figure 1). Dataset shift denotes test data that has different characteristics from the training data due to varying conditions under which the samples are collected: for images, these include but are not limited to corruptions, background, and viewpoint. OODRobustBench contains 23 such dataset shifts and assesses adversarial robustness to such data using the attack seen by the model during training. Threat shift denotes a variation between training and test adversarial threat models. In other words, threat shift assesses a model's robustness to unseen adversarial attacks applied to ID test data. OODRobustBench employs six different types of threat shifts. Adversarial robustness is evaluated for each type of shift to comprehensively assess OOD robustness.\"}"}
{"id": "kAFevjEYsz", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nD.5. Predicted Upper Limit of OOD Accuracy and Robustness\\n\\n(a) CIFAR10 $\\\\ell_\\\\infty$ Accuracy.\\n\\n(b) CIFAR10 $\\\\ell_2$ Accuracy.\\n\\n(c) CIFAR10 $\\\\ell_2$ Robustness.\\n\\n(d) ImageNet $\\\\ell_\\\\infty$ Accuracy.\\n\\n(e) ImageNet $\\\\ell_\\\\infty$ Robustness.\\n\\nFigure 15. The estimated upper limit of OOD performance and the conversion rate, a.k.a. slope, to OOD performance from ID performance under various distribution shifts.\"}"}
{"id": "kAFevjEYsz", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nE. Catastrophic Degradation of Robustness\\nWe observe this issue on only one implementation, using WideResNet28-10 with extra synthetic data (model id: Rade2021Helper on RobustBench), from Rade & Moosavi-Dezfooli (2022) for CIFAR10 $\\\\ell_\\\\infty$. There are three other implementations of this method on RobustBench. None of them, including the one using ResNet18 with extra synthetic data, is observed to suffer from this issue. It seems that catastrophic degradation in this case is specific to the implementation or training dynamics.\\n\\nOn the other hand, catastrophic degradation consistently happens on the models trained with AutoAugment or IDBH but not other tested data augmentations. It suggests the possibility that a certain image transformation operation exclusively used by AutoAugment and IDBH cause this issue. Besides, catastrophic degradation also consistently happens on the models trained using the receipt of (Debenedetti et al., 2023) under Gaussian and shot noise shifts. However, it employs a wide range of training techniques, so further experiments are required to identify the specific cause.\\n\\nF. How Inferior Models Affect the Correlation Analysis\\nThis section studies the influence of the construction of model zoo on the result of correlation. We use the overall performance (accuracy + robustness) to filter out inferior models. As we increasing the threshold of overall performance for filtering, the average overall performance of the model zoo increases, the number of included models decreases and the weight of the models from other published sources on the regression grows up. Our locally trained models are normally inferior to the public models regarding the performance since the latter employs better optimized and more effective training methods and settings. The training methods and settings of public models are also much more diverse.\\n\\nThe correlation for particular shifts varies considerably as more inferior models removed. $R^2$ declines considerably under CIFAR10-R, noise, fog, glass blur, frost and contrast for both Acc-Acc and Rob-Rob on CIFAR10 $\\\\ell_\\\\infty$ (Figure 16) and $\\\\ell_2$ (Figure 17). A similar trend is also observed for threat shifts, ReColor and different $p$-norm for CIFAR10 $\\\\ell_\\\\infty$ as shown in Figure 18. It suggests that the weak correlation under these shifts mainly results from those high-performance public models, and is likely related to the fact that these models include much diverse training methods and settings. For example, all observed catastrophic degradation under the noise shifts occur in the public models. Note that the locally trained models have a large diversity in model architectures particularly within the family of CNNs, but it seems that this architectural diversity does not effect the correlation as much as other factors.\\n\\nIn contrast, correlation is improved for most threat shifts for CIFAR10 $\\\\ell_2$ as shown in Figure 18. As shown in Figure 29, the locally trained (inferior) models and the public (high-performance) models have divergent linear trends (most evident in the plot of PPGD). That's why removing models from either group will enhance the correlation. Note that such divergence is not evident in the figures of CIFAR10 $\\\\ell_\\\\infty$ (Figure 28) and ImageNet $\\\\ell_\\\\infty$ (Figure 30).\\n\\nF.1. No Evident Correlation when ID and OOD Metrics Misalign\\nInferior models also cause OOD robustness to not consistently increase with the ID accuracy, i.e., the poor correlation between ID accuracy (robustness) and OOD robustness (accuracy) because they have high accuracy yet poor robustness. These models are mainly produced by some of our custom training receipts and take a considerable proportion of our CIFAR-10 model zoo, whereas the model zoo of ImageNet is dominated by ones from public sources.\"}"}
{"id": "kAFevjEYsz", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\n(a) $R^2$ of Acc-Acc.\\n\\n(b) $R^2$ of Rob-Rob.\\n\\nFigure 16. The change of $R^2$ under various dataset shifts as the models with lower overall performance are removed from regression for CIFAR10 $\\\\ell_\\\\infty$. Each row, with the filtering threshold labeled at the lead, corresponds to a new filtered model zoo and the regression conducted it. \u201cNC\u201d refers to No Custom models, so all models are retrieved from either RobustBench or other published works.\"}"}
{"id": "kAFevjEYsz", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\n(a) $R^2$ of Acc-Acc.\\n\\n(b) $R^2$ of Rob-Rob.\\n\\nFigure 17. The change of $R^2$ under various dataset shifts as the models with lower overall performance are removed from regression for CIFAR10 $\\\\ell_2$. Each row, with the filtering threshold labeled at the lead, corresponds to a new filtered model zoo and the regression conducted it. \\\"NC\\\" refers to No Custom models, so all models are retrieved from either RobustBench or other published works.\"}"}
{"id": "kAFevjEYsz", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nI. Plots of ID-OOD Correlation per Threat Shift\\n\\nFigure 28. Correlation between seen and unforeseen robustness on ID data for CIFAR10 $\\\\ell_\\\\infty$ AT models.\"}"}
{"id": "kAFevjEYsz", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 29. Correlation between seen and unforeseen robustness on ID data for CIFAR10 \\\\( \\\\ell_2 \\\\) AT models.\"}"}
{"id": "kAFevjEYsz", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ODDRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nFigure 30. Correlation between seen and unforeseen robustness on ID data for ImageNet $\\\\ell_{\\\\infty}$ AT models.\"}"}
{"id": "kAFevjEYsz", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nTrockman, A. and Kolter, J. Z. Patches are all you need? Transactions on Machine Learning Research, 2023. ISSN 2835-8856. Featured Certification.\\n\\nTsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. Robustness May Be at Odds with Accuracy. In International Conference on Learning Representations, 2019.\\n\\nWang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning Robust Global Representations by Penalizing Local Predictive Power. In Advances in Neural Information Processing Systems, 2019.\\n\\nWang, Y., Zou, D., Yi, J., Bailey, J., Ma, X., and Gu, Q. Improving Adversarial Robustness Requires Revisiting Misclassified Examples. In International Conference on Learning Representations, 2020.\\n\\nWang, Z., Pang, T., Du, C., Lin, M., Liu, W., and Yan, S. Better Diffusion Models Further Improve Adversarial Training. In International Conference on Machine Learning, February 2023.\\n\\nWong, E., Rice, L., and Kolter, J. Z. Fast is better than free: Revisiting adversarial training. In International Conference on Learning Representations, 2020.\\n\\nWu, D., Xia, S.-T., and Wang, Y. Adversarial Weight Perturbation Helps Robust Generalization. In Advances in Neural Information Processing Systems, 2020.\\n\\nXiao, C., Zhu, J.-Y., Li, B., He, W., Liu, M., and Song, D. Spatially Transformed Adversarial Examples. February 2018.\\n\\nXie, S., Girshick, R., Dollar, P., Tu, Z., and He, K. Aggregated Residual Transformations for Deep Neural Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\\n\\nXu, Y., Sun, Y., Goldblum, M., Goldstein, T., and Huang, F. Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness. September 2022.\\n\\nYang, J., Wang, P., Zou, D., Zhou, Z., Ding, K., Peng, W., Wang, H., Chen, G., Li, B., Sun, Y., Du, X., Zhou, K., Zhang, W., Hendrycks, D., Li, Y., and Liu, Z. OpenOOD: Benchmarking generalized out-of-distribution detection, October 2022.\\n\\nYu, F., Wang, D., Shelhamer, E., and Darrell, T. Deep layer aggregation. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2403\u20132412, 2018. doi: 10.1109/CVPR.2018.00255.\\n\\nZhang, H., Yu, Y., Jiao, J., Xing, E., Ghaoui, L. E., and Jordan, M. Theoretically Principled Trade-off between Robustness and Accuracy. In International Conference on Machine Learning, May 2019.\\n\\nZhang, J., Xu, X., Han, B., Niu, G., Cui, L., Sugiyama, M., and Kankanhalli, M. Attacks Which Do Not Kill Training Make Adversarial Learning Stronger. In Proceedings of the 37th International Conference on Machine Learning, November 2020.\\n\\nZhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586\u2013595, 2018.\\n\\nZhao, B., Yu, S., Ma, W., Yu, M., Mei, S., Wang, A., He, J., Yuille, A., and Kortylewski, A. OOD-CV: A benchmark for robustness to out-of-distribution shifts of individual nuisances in natural images. In Computer Vision \u2013 ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part VIII, pp. 163\u2013180, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 978-3-031-20073-1. doi: 10.1007/978-3-031-20074-8.\"}"}
{"id": "kAFevjEYsz", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Additional Related Works\\n\\nExcept for a few exceptions (Geirhos et al., 2020; Sun et al., 2022a; Rusak et al., 2020; Ford et al., 2019), previous work on generalization to input distribution shifts has not considered adversarial robustness. Hence, work on robustness to OOD data and adversarial attacks has generally happened in parallel, as exemplified by RobustBench (Croce et al., 2021) which provides independent benchmarks for assessing performance on corrupt data and adversarial threats.\\n\\nA line of works (Tramer & Boneh, 2019; Maini et al., 2020) defends against a union of $\\\\ell_p$ threat models by training with multiple $\\\\ell_p$ threat models jointly, which makes these threat models no longer unforeseen. PAT (Laidlaw et al., 2021) replaces $\\\\ell_p$ bound with LPIPS (Zhang et al., 2018) in adversarial training and achieves high robustness against several unforeseen attacks. Alternatively, (Dai et al., 2022) proposes variation regularization in addition to $\\\\ell_p$ adversarial training and improves unforeseen robustness.\\n\\nRobustness benchmarks.\\n\\nThere is a line of works on benchmarking adversarial robustness, including Dong et al. (2020), RobustBench (Croce et al., 2021), RobustART (Tang et al., 2022), MultiRobustBench (Dai et al., 2023), UA (Kaufmann et al., 2023), and Liu et al. (2023). RobustBench focuses on ID adversarial evaluation. Dong et al. (2020) evaluates adversarial robustness under $\\\\ell_p$ threat shifts in addition to ID adversarial evaluation. RobustART, compared to Dong et al. (2020), also supports the evaluation of OOD clean accuracy under dataset shifts. MultiRobustBench and UA both extend the evaluation set to include more unforeseen attacks, assessing adversarial robustness under threat shifts. Liu et al. (2023) is similar to RobustART but supports a larger number of OOD datasets. Compared to these benchmarks, OODRobustBench is unique in supporting the evaluation of OOD adversarial robustness under dataset shifts while also incorporating the functionalities of the other benchmarks.\\n\\nA.1. Comparison with Related Works\\n\\nIs the linear trend of robustness really expected given the linear trend of accuracy?\\n\\nNo. There is a well-known trade-off between accuracy and robustness in the ID setting (Tsipras et al., 2019). We further confirm this fact for the models we evaluate in Figure 13 in the appendix. This means that accuracy and robustness usually go in opposite directions making the linear trend we discover in both particularly interesting. Furthermore, the threat shifts as a scenario of OOD are unique to adversarial evaluation and were thus never explored in the previous studies of accuracy trends.\\n\\nHow does the linear trends observed by us differ from the previously discovered ones?\\n\\nRobust models exhibit a stronger linear correlation between ID and OOD accuracy for most corruption shifts (Figure 3). Particularly, the boost on linearity is dramatic for shifts including Impulse, Shot and Gaussian noises, Glass blur, Pixelate, and JPEG. For instance, R2 surges from 0 (no linear correlation) for non-robust models to 0.84 (evident linear correlation) for robust models with Gaussian noise data shifts. This suggests that, for robust models, predicting OOD accuracy from ID accuracy is more faithful and applicable to more shifts.\\n\\nThe linear trend of robustness is even stronger than that of accuracy for dataset shifts (Figure 3) but with a lower slope (Section 5). The latter leads to a predicted upper limit of OOD robustness that is way lower than that of OOD accuracy suggesting that the OOD generalization of robustness is much more challenging.\\n\\nHow does our analysis differ from the similar analysis in the prior works?\\n\\nThe scale of these previous works is rather small. For instance, RobustBench observes linear correlation only for three shifts on CIFAR-10 based on 39 models with either ResNet or WideResNet architectures. In such a narrow setting, it is actually neither surprising to see a linear trend nor reliable for predicting OOD performance. By contrast, our conclusion is derived from much more shifts on CIFAR-10 and ImageNet based on 706 models. Importantly, our model zoo covers a diverse set of architectures, robust training methods, data augmentation techniques, and training set-ups. This makes our conclusion more generalizable and the observed (almost perfect) linear trend much more significant.\\n\\nSimilarly, the existing works only test a few models under threat shifts. Those methods are usually just the baseline AT method plus different architectures or the relevant defenses, e.g., jointly trained with multiple threats. It is unclear how...\"}"}
{"id": "kAFevjEYsz", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nThe state-of-the-art robust models perform under threat shifts. By conducting a large-scale analysis, we find that those SOTA models generalize poorly to other threats while also discovering several methods that have relatively inferior ID performance but superior OOD robustness under threat shift. Our analysis therefore facilitates future works in this direction by identifying what techniques are ineffective and what are promising.\\n\\nHow does your benchmark differ from RobustBench?\\n\\nOur benchmark focuses on OOD adversarial robustness while RobustBench focuses on ID adversarial robustness. Specifically, our benchmark contrasts RobustBench in the datasets and the attacks. We use CIFAR-10.1, CIFAR-10.2, CINIC, and CIFAR-10-R (ImageNet-V2, ImageNet-A, ImageNet-R, ObjectNet) to simulate input data distribution shift for the source datasets CIFAR-10 (ImageNet), while RobustBench only uses the latter source datasets. We use PPGD, LPA, ReColor, StAdv, Linf-12/255, L2-0.5 (PPGD, LPA, ReColor, StAdv, Linf-8/255, L2-1) to simulate threat shift for the training threats Linf-8/255 (L2-0.5), while RobustBench only evaluates the same threats as the training ones.\\n\\nB. Benchmark Set-up\\n\\nB.1. Datasets\\n\\nThis section introduces the OOD datasets of natural shifts. For ImageNet, we have:\\n\\n- **ImageNet-V2** is a reproduction of ImageNet using a completely new set of images. It has the same 1000 classes as ImageNet and each class has 10 images so 10K images in total.\\n- **ImageNet-A** is an adversarially-selected reproduction of ImageNet. The images in this dataset were selected to be those most misclassified by an ensemble of ResNet-50s. It has 200 ImageNet classes and 7.5K images.\\n- **ImageNet-R** contains various artistic renditions of objects from ImageNet, so there is a domain shift. It has 30K images and 200 ImageNet classes.\\n- **ObjectNet** is a large real-world dataset for object recognition. It is constructed with controls to randomize background, object rotation and viewpoint. It has 313 classes but only 104 classes compatible with ImageNet classes so we only use this subset. The selected subset includes 17.2K images.\\n\\nFor CIFAR10, we have:\\n\\n- **CIFAR10.1** is a reproduction of CIFAR10 using a completely new set of images. It has 2K images sampled from the same source as CIFAR10, i.e., 80M TinyImages (Torralba et al., 2008). It has the same number of classes as CIFAR10.\\n- **CIFAR10.2** is another reproduction of CIFAR10. It has 12K (10k for training and 2k for test) images sampled from the same source as CIFAR10, i.e., 80M TinyImages. It has the same number of classes as CIFAR10. We only use the test set of CIFAR10.2.\\n- **CINIC** is a downscaled subset of ImageNet with the same image resolution and classes as CIFAR10. Its test set has 90K images in total, of which 20K images are from CIFAR10 and 70K images are from ImageNet. We use only the ImageNet part.\\n- **CIFAR10-R** is a new dataset created by us. The images in CIFAR10-R and CIFAR10 have different styles so there is a domain shift. We follow the same procedure as CINIC to downscale the images from ImageNet-R to the same resolution as CIFAR10 and select images from the classes of ImageNet corresponding to CIFAR10 classes. We follow the same class mapping between ImageNet and CIFAR10 as CINIC. Note that ImageNet-R does not have images of the ImageNet classes corresponding to CIFAR10 classes of \\\"airplane\\\" and \\\"horse\\\", so there are only 8 classes in CIFAR10-R.\\n\\nIn practice, we evaluate models using a random sample of 5K images from each of the ImageNet variant datasets, and 10K images from each of the CIFAR10 variant datasets, if those datasets contain more images than that number. This is done to accelerate the evaluation and follows the practice used in RobustBench (Croce et al., 2021).\"}"}
{"id": "kAFevjEYsz", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nFigure 8. Comparison of MM5 adversarial accuracy against AutoAttack adversarial accuracy. Each data point represents a model.\\n\\nTable 3. The accuracy and adversarial robustness evaluated by various attacks of models on CIFAR10 and ImageNet for $\\\\ell_\\\\infty$ threat model. These models are sourced from RobustBench respective leaderboards and identified by their RobustBench identifiers. The results for corruption shifts are reported for severity level 3.\\n\\n| Dataset | Model ID          | OOD Acc. | PGD | CW | AA | MM5 Acc. | PGD | CW | AA | MM5 |\\n|---------|-------------------|----------|-----|----|----|----------|-----|----|----|-----|\\n| CIFAR10 | Wong2020Fast      | 83.3     | 46.6| 46.3| 43.2| 43.3     | 65.0| 27.6| 27.3| 24.8|\\n|         | Engstrom2019Robustness | 87.0     | 52.3| 52.6| 49.3| 49.7     | 70.6| 31.7| 31.8| 29.1|\\n|         | Huang2020Self     | 83.5     | 56.2| 54.0| 52.9| 53.0     | 65.0| 34.4| 32.9| 31.7|\\n|         | Sehwag2021ProxyR18| 84.6     | 58.7| 57.2| 55.6| 55.7     | 67.0| 37.8| 36.5| 34.6|\\n|         | Wang2020Improving | 87.5     | 62.6| 58.7| 56.3| 56.8     | 70.5| 40.3| 37.1| 35.1|\\n| ImageNet | Salman2020Do      | 52.9     | 29.8| 27.3| 25.3| 25.5     | 21.4| 9.5 | 8.5 | 7.5 |\\n|         | Wong2020Fast      | 55.6     | 30.0| 28.9| 26.3| 26.8     | 21.6| 8.7 | 8.5 | 7.2 |\\n|         | Engstrom2019Robustness | 62.5     | 32.9| 32.6| 29.2| 29.8     | 27.2| 10.1| 10.2| 8.4 |\\n|         | Salman2020Do      | 64.1     | 39.0| 37.6| 34.7| 35.0     | 27.6| 12.5| 12.0| 10.5|\\n|         | Singh2023Revisiting ViT-S-ConvStem | 72.6     | 51.4| 50.6| 48.1| 48.5     | 39.8| 19.8| 19.4| 17.5|\\n\\nThe OODRobustBench framework is designed for easy integration of alternative datasets representing input data distribution shifts. We plan to maintain and update our code to continually incorporate new OOD datasets such as ImageNet-V (Dong et al., 2022), Stylized-ImageNet (Geirhos et al., 2019), and ImageNet-Sketch (Wang et al., 2019). For the latest developments, we invite readers to visit our GitHub repository: https://github.com/OODRobustBench/OODRobustBench.\\n\\nB.2. Verification of the Effectiveness of the MM5 Attack\\n\\nB.2.1. Comparison of MM5 against AutoAttack\\n\\nTo verify the effectiveness of MM5, we compare its result with the result of AutoAttack on the ID dataset across all publicly available models from RobustBench for CIFAR10 $\\\\ell_\\\\infty$, CIFAR10 $\\\\ell_2$, and ImageNet $\\\\ell_\\\\infty$. As shown in Figure 8, almost all models are approximately on the line of $y = x$ (gray dashed line) suggesting that their MM5 adversarial accuracy is very close to AA adversarial accuracy. Specifically, the mean gap between MM5 and AA adversarial accuracy is 0.16 and the standard deviation is 0.32.\\n\\nB.2.2. Comparison of MM5 against Diverse Attacks\\n\\nTo test the effectiveness of MM5 against various attacks, we selected 10 models from RobustBench and evaluated their robustness against PGD100, CW100, and AutoAttack. As shown in Table 3, MM5 achieved an adversarial accuracy comparable to that of the strongest attack, AutoAttack, in both in-distribution (ID) and out-of-distribution (OOD) settings. This suggests that the MM5 attack is a reliable method for adversarial evaluation, even under distribution shifts.\\n\\n1 Two models, Ding et al. (2020) and Xu et al. (2022), are observed to have a slightly higher adversarial accuracy compared to the corresponding AutoAttack results. We use MM+ (Gao et al., 2022) attack to evaluate these two models for a more reliable evaluation and the result of MM+ is close to AutoAttack.\"}"}
{"id": "kAFevjEYsz", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgment\\n\\nThe authors acknowledge the use of the research computing facility at King's College London, King's Computational Research, Engineering and Technology Environment (CREATE). Lin Li was funded by the King's - China Scholarship Council (K-CSC). Yifei Wang was supported by Office of Naval Research under grant N00014-20-1-2023 (MURI MLSCOPE), NSF AI Institute TILOS (NSF CCF-2112665), and NSF award 2134108. Chawin was supported in part by funds provided by the National Science Foundation (under grant 2229876), the KACST-UCB Center for Secure Computing, the Department of Homeland Security, IBM, the Noyce Foundation, Google, Open Philanthropy, and the Center for AI Safety Compute Cluster. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors.\\n\\nImpact Statement\\n\\nThis paper presents work whose goal is to advance the field of adversarial machine learning. There are many potential societal consequences of our work, none of which we feel are negative and must be specifically highlighted here.\\n\\nReferences\\n\\nAlhamoud, K., Hammoud, H. A. A. K., Alfarra, M., and Ghanem, B. Generalizability of Adversarial Robustness Under Distribution Shifts. Transactions on Machine Learning Research, May 2023.\\n\\nAthalye, A., Carlini, N., and Wagner, D. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. In Proceedings of the 35th International Conference on Machine Learning, July 2018.\\n\\nAugustin, M., Meinke, A., and Hein, M. Adversarial Robustness on In- and Out-Distribution Improves Explainability. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\\n\\nBaek, C., Jiang, Y., Raghunathan, A., and Kolter, J. Z. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. In Advances in Neural Information Processing Systems, volume 35, pp. 19274\u201319289, 2022.\\n\\nBai, Y., Anderson, B. G., Kim, A., and Sojoudi, S. Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing, May 2023.\\n\\nBarbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B. ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In Advances in Neural Information Processing Systems, 2019.\\n\\nCarmon, Y., Raghunathan, A., Schmidt, L., Duchi, J. C., and Liang, P. S. Unlabeled Data Improves Adversarial Robustness. In Advances in Neural Information Processing Systems, 2019.\\n\\nCroce, F. and Hein, M. Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks. In Proceedings of the 37th International Conference on Machine Learning, 2020.\\n\\nCroce, F. and Hein, M. Adversarial robustness against multiple and single $\\\\ell_p$-threat models via quick fine-tuning of robust classifiers. In International Conference on Machine Learning, pp. 4436\u20134454. PMLR, 2022.\\n\\nCroce, F., Andriushchenko, M., Sehwag, V., Debenedetti, E., Flammarion, N., Chiang, M., Mittal, P., and Hein, M. RobustBench: a standardized adversarial robustness benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), October 2021.\\n\\nCroce, F., Gowal, S., Brunner, T., Shelhamer, E., Hein, M., and Cemgil, T. Evaluating the adversarial robustness of adaptive test-time defenses. In International Conference on Machine Learning, pp. 4421\u20134435. PMLR, 2022.\\n\\nCubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. AutoAugment: Learning Augmentation Strategies From Data. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\nCui, J., Tian, Z., Zhong, Z., Qi, X., Yu, B., and Zhang, H. Decoupled Kullback-Leibler Divergence Loss, May 2023.\\n\\nDai, S., Mahloujifar, S., and Mittal, P. Formulating Robustness Against Unforeseen Attacks. In Advances in Neural Information Processing Systems, December 2022.\\n\\nDai, S., Mahloujifar, S., Xiang, C., Sehwag, V., Chen, P.-Y., and Mittal, P. MultiRobustBench: Benchmarking Robustness Against Multiple Attacks. In International Conference on Machine Learning, May 2023.\\n\\nDarlow, L. N., Crowley, E. J., Antoniou, A., and Storkey, A. J. CINIC-10 is not ImageNet or CIFAR-10, October 2018.\\n\\nDebenedetti, E., Sehwag, V., and Mittal, P. A Light Recipe to Train Robust Vision Transformers. In First IEEE Conference on Secure and Trustworthy Machine Learning, February 2023.\"}"}
{"id": "kAFevjEYsz", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nDeng, W. and Zheng, L. Are Labels Always Necessary for Classifier Accuracy Evaluation? In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.\\n\\nDiffenderfer, J., Bartoldson, B., Chaganti, S., Zhang, J., and Kailkhura, B. A Winning Hand: Compressing Deep Networks Can Improve Out-of-Distribution Robustness. In Advances in Neural Information Processing Systems, 2021.\\n\\nDing, G. W., Sharma, Y., Lui, K. Y. C., and Huang, R. MMA Training: Direct Input Space Margin Maximization through Adversarial Training. In International Conference on Learning Representations, 2020.\\n\\nDong, Y., Fu, Q.-A., Yang, X., Pang, T., Su, H., Xiao, Z., and Zhu, J. Benchmarking Adversarial Robustness on Image Classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.\\n\\nDong, Y., Ruan, S., Su, H., Kang, C., Wei, X., and Zhu, J. ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints. In Advances in Neural Information Processing Systems, December 2022.\\n\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations, 2021.\\n\\nEngstrom, L., Ilyas, A., Salman, H., Santurkar, S., and Tsipras, D. Robustness (python library), 2019. URL https://github.com/MadryLab/robustness.\\n\\nFord, N., Gilmer, J., Carlini, N., and Cubuk, D. Adversarial examples are a natural consequence of test error in noise. 2019. doi: 10.48550/arXiv.1901.10513.\\n\\nGao, R., Wang, J., Zhou, K., Liu, F., Xie, B., Niu, G., Han, B., and Cheng, J. Fast and Reliable Evaluation of Adversarial Robustness with Minimum-Margin Attack. In Proceedings of the 39th International Conference on Machine Learning, June 2022.\\n\\nGarg, S., Balakrishnan, S., Lipton, Z. C., Neyshabur, B., and Sedghi, H. Leveraging unlabeled data to predict out-of-distribution performance. October 2021.\\n\\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., and Brendel, W. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019.\\n\\nGeirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u201373, 2020. doi: 10.1038/s42256-020-00257-z.\\n\\nGowal, S., Qin, C., Uesato, J., Mann, T., and Kohli, P. Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples. arXiv:2010.03593 [cs, stat], March 2021a.\\n\\nGowal, S., Rebuffi, S.-A., Wiles, O., Stimberg, F., Calian, D., and Mann, T. Improving Robustness using Generated Data. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021b.\\n\\nGuo, C., Rana, M., Cisse, M., and Maaten, L. v. d. Countering Adversarial Images using Input Transformations. In International Conference on Learning Representations, 2018.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\\n\\nHendrycks, D. and Dietterich, T. Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. In International Conference on Learning Representations, 2019.\\n\\nHendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. In International Conference on Computer Vision, 2021a.\\n\\nHendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural Adversarial Examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021b.\\n\\nHsiung, L., Tsai, Y.-Y., Chen, P.-Y., and Ho, T.-Y. Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nHu, J., Shen, L., and Sun, G. Squeeze-and-Excitation Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\\n\\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely Connected Convolutional Networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\"}"}
{"id": "kAFevjEYsz", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nHuang, S., Lu, Z., Deb, K., and Boddeti, V. N. Revisiting Residual Networks for Adversarial Robustness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023a.\\n\\nHuang, Z., Zhu, M., Xia, X., Shen, L., Yu, J., Gong, C., Han, B., Du, B., and Liu, T. Robust generalization against photon-limited corruptions via worst-case sharpness minimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16175\u201316185, 2023b.\\n\\nIbrahim, A., Guille-Escuret, C., Mitliagkas, I., Rish, I., Krueger, D., and Bashivan, P. Towards Out-of-Distribution Adversarial Robustness, February 2023.\\n\\nJiang, Z., Chen, T., Chen, T., and Wang, Z. Robust Pre-Training by Adversarial Contrastive Learning. In Advances in Neural Information Processing Systems, 2020.\\n\\nKaufmann, M., Kang, D., Sun, Y., Basart, S., Yin, X., Mazeika, M., Arora, A., Dziedzic, A., Boenisch, F., Brown, T., Steinhardt, J., and Hendrycks, D. Testing Robustness Against Unforeseen Adversaries, July 2023.\\n\\nKireev, K., Andriushchenko, M., and Flammarion, N. On the effectiveness of adversarial training against common corruptions. In Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, August 2022.\\n\\nKrueger, D., Caballero, E., Jacobsen, J.-H., Zhang, A., Bi\u00f1as, J., Zhang, D., Le Priol, R., and Courville, A. Out-of-distribution generalization via risk extrapolation (rex). In International Conference on Machine Learning, pp. 5815\u20135826. PMLR, 2021.\\n\\nLaidlaw, C. and Feizi, S. Functional Adversarial Attacks. In Advances in Neural Information Processing Systems, 2019.\\n\\nLaidlaw, C., Singla, S., and Feizi, S. Perceptual Adversarial Robustness: Defense Against Unseen Threat Models. In International Conference on Learning Representations, January 2021.\\n\\nLi, L. and Spratling, M. Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing, March 2023a.\\n\\nLi, L. and Spratling, M. Understanding and combating robust overfitting via input loss landscape analysis and regularization. Pattern Recognition, April 2023b.\\n\\nLi, L. and Spratling, M. W. Data augmentation alone can improve adversarial training. In International Conference on Learning Representations, February 2023c.\\n\\nLi, L., Qiu, J., and Spratling, M. AROID: Improving Adversarial Robustness through Online Instance-wise Data Augmentation, June 2023.\\n\\nLiu, C., Dong, Y., Xiang, W., Yang, X., Su, H., Zhu, J., Chen, Y., He, Y., Xue, H., and Zheng, S. A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking, February 2023.\\n\\nLu, S., Nott, B., Olson, A., Todeschini, A., Vahabi, H., Carmon, Y., and Schmidt, L. Harder or Different? A Closer Look at Distribution Shift in Dataset Reproduction. In ICML 2020 Workshop on Uncertainty and Robustness in Deep Learning, 2020.\\n\\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards Deep Learning Models Resistant to Adversarial Attacks. In International Conference on Learning Representations, 2018.\\n\\nMaini, P., Wong, E., and Kolter, Z. Adversarial Robustness Against the Union of Multiple Perturbation Models. In Proceedings of the 37th International Conference on Machine Learning, November 2020.\\n\\nMao, X., Chen, Y., Li, X., Qi, G., Duan, R., Zhang, R., and Xue, H. Easyrobust: A comprehensive and easy-to-use toolkit for robust computer vision, 2022.\\n\\nMiller, J. P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P. W., Shankar, V., Liang, P., Carmon, Y., and Schmidt, L. Accuracy on the Line: on the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. In Proceedings of the 38th International Conference on Machine Learning, July 2021.\\n\\nM\u00fcller, S. G. and Hutter, F. TrivialAugment: Tuning-Free Yet State-of-the-Art Data Augmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.\\n\\nPang, T., Yang, X., Dong, Y., Xu, K., Zhu, J., and Su, H. Boosting Adversarial Training with Hypersphere Embedding. In Advances in Neural Information Processing Systems, 2020.\\n\\nPang, T., Lin, M., Yang, X., Zhu, J., and Yan, S. Robustness and Accuracy Could Be Reconcilable by (Proper) Definition. In Proceedings of the 39th International Conference on Machine Learning, June 2022.\\n\\nRade, R. and Moosavi-Dezfooli, S.-M. Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off. In International Conference on Learning Representations, March 2022.\\n\\nRebuffi, S.-A., Gowal, S., Calian, D. A., Stimberg, F., Wiles, O., and Mann, T. Fixing Data Augmentation to Improve Adversarial Robustness. Technical report, October 2021.\"}"}
{"id": "kAFevjEYsz", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V.\\n\\nDo CIFAR-10 Classifiers Generalize to CIFAR-10? arXiv:1806.00451 [cs, stat], June 2018.\\n\\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do ImageNet Classifiers Generalize to ImageNet? In Proceedings of the 36th International Conference on Machine Learning, 2019.\\n\\nRice, L., Wong, E., and Kolter, J. Z. Overfitting in adversarially robust deep learning. In Proceedings of the 37th International Conference on Machine Learning, 2020.\\n\\nRony, J., Hafemann, L. G., Oliveira, L. S., Ben Ayed, I., Sabourin, R., and Granger, E. Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\nRusak, E., Schott, L., Zimmermann, R. S., Bitterwolf, J., Bringmann, O., Bethge, M., and Brendel, W. A simple way to make neural networks robust against diverse image corruptions. In Proceedings of the European Conference on Computer Vision, 2020.\\n\\nSamangouei, P., Kabkab, M., and Chellappa, R. Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models. In International Conference on Learning Representations, February 2018.\\n\\nSandler, M., Howard, A. G., Zhu, M., Zhmoginov, A., and Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018. doi: 10.1109/CVPR.2018.00474.\\n\\nSehwag, V., Bhagoji, A. N., Song, L., Sitawarin, C., Cullina, D., Chiang, M., and Mittal, P. Analyzing the Robustness of Open-World Machine Learning. In Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security, November 2019.\\n\\nSehwag, V., Mahloujifar, S., Handina, T., Dai, S., Xiang, C., Chiang, M., and Mittal, P. Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness? In International Conference on Learning Representations, March 2022.\\n\\nShen, Z., Liu, J., He, Y., Zhang, X., Xu, R., Yu, H., and Cui, P. Towards Out-Of-Distribution Generalization: A Survey, August 2021.\\n\\nShi, Y., Daunhawer, I., Vogt, J. E., Torr, P. H., and Sanyal, A. How robust is unsupervised representation learning to distribution shift? In The Eleventh International Conference on Learning Representations. OpenReview, 2023.\\n\\nSimonyan, K. and Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. In International Conference on Learning Representations, 2015.\\n\\nSingh, N. D., Croce, F., and Hein, M. Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models. 2023.\\n\\nStutz, D., Hein, M., and Schiele, B. Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks. In International Conference on Machine Learning, 2020.\\n\\nSun, J., Mehra, A., Kailkhura, B., Chen, P.-Y., Hendrycks, D., Hamm, J., and Mao, Z. M. Certified adversarial defenses meet out-of-distribution corruptions: Benchmarking robustness and simple baselines. In Proceedings of the European Conference on Computer Vision, 2022a.\\n\\nSun, J., Mehra, A., Kailkhura, B., Chen, P.-Y., Hendrycks, D., Hamm, J., and Mao, Z. M. A Spectral View of Randomized Smoothing Under Common Corruptions: Benchmarking and Improving Certified Robustness. In Computer Vision \u2013 ECCV 2022, 2022b.\\n\\nSzegedy, C., Wei Liu, Yangqing Jia, Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.\\n\\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the Inception Architecture for Computer Vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\\n\\nTan, M. and Le, Q. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In Proceedings of the 36th International Conference on Machine Learning, May 2019.\\n\\nTang, S., Gong, R., Wang, Y., Liu, A., Wang, J., Chen, X., Yu, F., Liu, X., Song, D., Yuille, A., Torr, P. H. S., and Tao, D. RobustART: Benchmarking Robustness on Architecture Design and Training Techniques, January 2022.\\n\\nTaori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L. Measuring Robustness to Natural Distribution Shifts in Image Classification. In Advances in Neural Information Processing Systems, 2020.\\n\\nTorralba, A., Fergus, R., and Freeman, W. T. 80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, November 2008.\\n\\nTramer, F. and Boneh, D. Adversarial Training and Robustness for Multiple Perturbations. In Advances in Neural Information Processing Systems, 2019.\"}"}
{"id": "kAFevjEYsz", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nFigure 3. $R^2$ of regression between ID and OOD performance for Standardly-Trained (ST) and Adversarially-Trained (AT) models under various dataset shifts for CIFAR10. Higher $R^2$ implies stronger linear correlation. The results for ST models were copied from (Miller et al., 2021). Some results of ST are missing (blank cells) because they were not reported in (Miller et al., 2021).\\n\\nFigure 4. $R^2$ of regression between ID seen robustness and OOD unforeseen robustness, i.e., threat shift. The finding on ST models (Miller et al., 2021). However, note that we measure linear correlation for the raw data, whereas (Miller et al., 2021) applies a nonlinear transformation to the data to promote linearity. Overall, adversarial training boosts linear correlation for corruption shifts, and hence, improves the faithfulness of using ID performance for model selection and OOD performance prediction. We attribute this to AT improving accuracy on the corrupted data (Kireev et al., 2022). Intuitively, ST models have less correlated corruption accuracy because corruption significantly impairs accuracy and such effect varies a lot among models. Compared to ST, AT effectively mitigates the effect of corruption on accuracy, and hence, reduces the divergence of corruption accuracy so that corruption accuracy is more correlated to ID accuracy.\\n\\nLast, we observe no evident correlation when ID and OOD metrics misalign, i.e., Acc-Rob and Rob-Acc for CIFAR10, but weak correlation for ImageNet $\\\\ell_\\\\infty$ as shown in Figure 13. This is due to the varied trade-off between accuracy and robustness of different models (discussed in details in Appendix F.1).\\n\\n4.2. Linear Trend under Threat Shift\\n\\nThis section studies the relationship between seen and unforeseen robustness. Both seen and unforeseen robustness are computed using only ID data yet with different attacks. Linear regression is then conducted between seen robustness ($x$) and unforeseen robustness ($y$). The result of regression for each threat shift is given in Appendix I. The sensitivity of the regression results to the composition of the model zoo is discussed in Appendix F.\\n\\n$\\\\ell_p$ robustness correlates poorly with non-$\\\\ell_p$ robustness. $R^2$ of the regression between ID $\\\\ell_p$ robustness and PPGD, LPA and StAdv robustness is low in Figure 4. Particularly, $R^2$ is close to 0 for $\\\\ell_\\\\infty$-LPA and $\\\\ell_\\\\infty$-StAdv on CIFAR10 $\\\\ell_\\\\infty$ suggesting no correlation at all. As shown in Figures 28 to 30, the increase in ID $\\\\ell_p$ robustness leads to only slight or even no improvement on unforeseen robustness esp. for LPA and StAdv. Interestingly, despite poor correlation with PPGD, LPA and StAdv, ID $\\\\ell_p$ robustness is well correlated with ReColor unforeseen robustness.\\n\\n$\\\\ell_p$ robustness correlates strongly with $\\\\ell_p$ robustness of different $\\\\epsilon$ and $p$-norm. $R^2$ of their regression is higher than 0.7 across all assessed set-ups in Figure 4 suggesting a consistently strong linear correlation. The correlation between different $\\\\epsilon$ of the same $p$-norm is stronger than the correlation between different $p$-norm.\\n\\n4.3. Unsupervised OOD Robustness Prediction\\n\\nThe linear trends discovered above enable the prediction of OOD performance only if labeled OOD data is available. There is a line of works (Baek et al., 2022; Deng & Zheng, 2021; Garg et al., 2021) showing that OOD accuracy can be predicted with only unlabeled OOD data. We study here if OOD adversarial robustness can be predicted, similarly, in an unsupervised manner. We run the experiments with CIFAR-10 $\\\\ell_\\\\infty$ models for CIFAR-10.1 (Figure 5) and Impulse noise (Figure 14) shifts and find that a linear trend is also observed in the agreement between the predictions of any pair of two robust models: $R^2$ is 0.99 for CIFAR-10.1 shift and 0.95 for Impulse noise shift. This suggests that the unsupervised method (Baek et al., 2022) is also effective in predicting OOD adversarial robustness.\\n\\n5. Incompetence in OOD Generalization\\n\\nBased on the precise linear trend observed above for existing robust training methods, we can predict the OOD performance of a model trained by such a method from its ID performance using the fitted linear model. Furthermore, we can extrapolate from current trends to predict the maximum OOD robustness that can be expected from a hypothetical future model that achieves perfect robustness on ID data (assuming the linear trend continues).\\n\\n$$\\\\text{slope} \\\\times 100 + \\\\text{intercept}.$$(4)\"}"}
{"id": "kAFevjEYsz", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nFigure 5. Correlation between ID and OOD prediction agreement on adversarial examples for CIFAR10 $\\\\ell_\\\\infty$ AT models. Each point represents the prediction agreement of two models. This estimates the best OOD performance one can expect by fully exploiting existing robust training techniques. Note that a wide range of models and techniques (Appendix C.2) are covered by our correlation analysis so their, as well as their variants', OOD performance should be (approximately) bounded by the predicted upper limit. The accuracy of the prediction depends on the $R^2$ of the correlation.\\n\\nWe find that continuously improving ID $\\\\ell_p$ robustness following existing practice is unlikely to achieve high OOD adversarial robustness. The upper limit of OOD robustness under dataset shift, $OOD_d$, is 66%/71%/43% for CIFAR10 $\\\\ell_\\\\infty$ (Figure 6), CIFAR10 $\\\\ell_2$ (Figure 15) and ImageNet $\\\\ell_\\\\infty$ (Figure 15) respectively, and under threat shift $OOD_t$ is 52%/35%/52% correspondingly. Hence, if current trends continue, the resulting models are likely to be very unreliable in real-world applications. The vulnerability of models is most evident for ImageNet $\\\\ell_\\\\infty$ under dataset shift and for CIFAR10 $\\\\ell_2$ under threat shift. The expected upper limit of OOD robustness also varies greatly among individual shifts ranging from nearly 0 to 100%.\\n\\nOne of the accounts for this issue is that the existing methods have poor conversion rate to OOD robustness as shown by the slope of the linear trend in Figures 6 and 15. Taking an example of fog shift on ImageNet, the slope is roughly 0.1 so improving 10% ID robustness can only lead to 1% improvement on fog robustness. Besides, the upper limit and conversion rate of robustness are observed to be much lower than those of accuracy in Figure 15, suggesting the OOD generalization issue is more severe for robustness. Overall, this issue calls for developing novel methods that can improve OOD robustness beyond our prediction.\\n\\n6. Improving OOD Adversarial Robustness\\n\\nTo inspire the design of methods that have OOD robustness exceeding the above prediction, this section investigates methods that have the potential to be effective for boosting the OOD generalization of robustness. The effectiveness is quantified by two metrics: OOD performance and effective performance. Effective performance measures the extra resilience of a model under distribution shift when compared to a group of models by adapting the metric of \\\"Effective Robustness\\\" (Taori et al., 2020):\\n\\n$$R'_f = OOD_f - \\\\beta (R_{id}(f))$$\\n\\nwhere $\\\\beta (\\\\cdot)$ is a linear mapping from ID to OOD metric fitted on a group of models. We name this metric effective robustness (adversarial effective robustness) when $R_{id}$ and $OOD_f$ are accuracy (robustness). A positive adversarial effective robustness means that $f$ achieves adversarial robustness above what the linear trend predicts based on its ID performance, i.e., $f$ is advantageous over the fitted models on OOD generalization. Note that higher adversarial effective robustness is not equivalent to higher OOD robustness since the model may have a lower ID robustness. The specific set-ups and detailed results of the following experiments are described in Appendix G.\\n\\n6.1. Data\\n\\nTraining with extra data boosts both robustness and adversarial effective robustness compared to training schemes without extra data (see Figure 7a). There is no clear advantage to training with extra real data (Carmon et al., 2019) rather than synthetic data (Gowal et al., 2021b) except for the adversarial effective robustness under threat shift which is improved more by real data.\\n\\nAdvanced data augmentation improves robustness under both types of shifts and adversarial effective robustness under threat shift over the baseline augmentation RandomCrop (see Figure 7b). Nevertheless, advanced data augmentation methods other than TA (M\u00fcller & Hutter, 2021) degrade adversarial effective robustness under dataset shift.\\n\\n6.2. Model\\n\\nAdvanced model architecture greatly boosts robustness and adversarial effective robustness under both types of shift over the baseline ResNet (He et al., 2016) (Figure 7c). Among all tested architectures, ViT (Dosovitskiy et al., 2021) achieves the highest adversarial effective robustness.\"}"}
{"id": "kAFevjEYsz", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\n- Extra data\\n- Data aug.\\n- Model architecture\\n- Model size\\n- VR\\n- HE\\n- MMA\\n- AS\\n\\nFigure 7. The robustness (Rob.) and adversarial effective robustness (AER) of various robust techniques.\\n\\nScaling model up improves robustness under both types of shift and adversarial effective robustness under dataset shift, but dramatically impairs adversarial effective robustness under threat shift (Figure 7d). The latter is because increasing model size greatly improves ID robustness but not OOD robustness so that the real OOD robustness is much below the OOD robustness predicted by linear correlation.\\n\\n6.3. Adversarial Training\\n\\nVR (Dai et al., 2022), the state-of-the-art defense against unforeseen attacks, greatly boosts adversarial effective robustness under threat shifts in spite of inferior ID robustness. Surprisingly, VR also clearly boosts adversarial effective robustness under dataset shift even though not designed for dealing with these shifts.\\n\\nTraining methods HS (Pang et al., 2020), MMA (Ding et al., 2020) and AS (Bai et al., 2023) achieve an AER of 16.22%, 10.74% and 9.41%, respectively, under threat shift, which are much higher than the models trained with PGD. Importantly, in contrast to VR, these methods also improve ID robustness resulting in a further boost on OOD robustness. This makes them a potentially promising defense against multi-attack (Dai et al., 2023).\\n\\n6.4. OOD Generalization Methods\\n\\nTwo leading methods, CARD-Deck (Diffenderfer et al., 2021) (ranked 1st) and PLAT (Kireev et al., 2022), from the common corruptions leaderboard of RobustBench are evaluated using our benchmark in Table 2. Despite the expected remarkable OOD clean generalization under OOD shifts, they offer little or no adversarial robustness regardless of ID or OOD setting. It suggests that OOD generalization methods alone do not help OOD adversarial robustness unless combined with adversarial training.\\n\\nTo test the effect of combining adversarial training with OOD generalization method, we evaluate a recent attempt in this direction, MSD+REx (Ibrahim et al., 2023). This approach trains using the multi-attack defense MSD with various attacks and applies REx (Krueger et al., 2021) by treating different attacks as separate domains. Surprisingly, as shown in Table 2, this purpose-built solution impairs OOD adversarial robustness under both dataset and threat shifts and offers no evident improvement in AER when compared to supervised $\\\\ell_p$ adversarial training.\\n\\nWhile these findings suggest that this specific implementation might be ineffective, the combination of AT and OOD methods remains a promising direction. Future work should focus on a more careful design for this integration. One potential strategy could be to treat different groups of data, rather than attacks, as different domains like Huang et al. (2023b).\\n\\n6.5. Unsupervised Representation Learning\\n\\nUnsupervised learning has been observed to train models that generalize to distribution shifts better than supervised learning (Shi et al., 2023; Shen et al., 2021). However, it is unclear whether or not unsupervised learning will benefit OOD adversarial robustness. To test this we evaluated a\"}"}
{"id": "kAFevjEYsz", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. The performance of non-single-$\\\\ell_p$ defense and OOD generalization methods under distribution shift on CIFAR10 $\\\\ell_\\\\infty$. The AER of StAdv AT, PLAT and CARD-Deck is invalid (\"-\") because of their (nearly) 0 ID/ODD robustness.\\n\\n| Defense Method | Model | ID Acc. | OOD Acc. | Rob. | ID ER | OOD ER | non-$\\\\ell_p$ | $\\\\ell_p$ | Avg. | $\\\\ell_\\\\infty$ | AER |\\n|----------------|-------|---------|----------|------|-------|--------|------------|----------|------|-------------|-----|\\n| Supervised $\\\\ell_p$ defense PGD | AT ResNet18 | 83.4 | 49.4 | 64.6 | 30.2 | -1.1 | 0.2 | 19.1 | 45.5 | 27.9 |\\n| Self-supervised $\\\\ell_p$ defense ACL ResNet18 | 82.3 | 49.4 | 64.2 | 30.1 | -0.6 | 0.1 | 20.2 | 45.1 | 28.5 |\\n| non-$\\\\ell_p$ attack defense | ReColor AT ResNet50 | 93.4 | 8.5 | 78.0 | 3.4 | 2.8 | 2.6 | 16.4 | 18.1 | 17.0 |\\n| StAdv AT ResNet50 | 86.2 | 0.1 | 66.8 | 0.0 | -1.6 | -9.5 | 3.4 | 7.4 |\\n| PAT-Alexnet ResNet50 | 71.6 | 28.6 | 57.1 | 16.8 | 2.3 | 1.6 | 48.4 | 33.1 | 43.3 |\\n| PAT-Self ResNet50 | 82.4 | 30.4 | 66.3 | 16.8 | 1.4 | 0.3 | 32.1 | 36.6 | 33.6 |\\n| Unforeseen attack defense VR ResNet18 | 72.9 | 48.9 | 56.4 | 31.4 | 0.5 | -0.9 | 24.8 | 43.3 | 31.0 |\\n| PAT+VR ResNet50 | 72.5 | 29.4 | 56.8 | 17.4 | 1.3 | 1.7 | 55.0 | 33.6 | 47.8 |\\n| Composite attack defense GAT-f ResNet50 | 82.3 | 38.7 | 66.2 | 22.2 | 1.4 | -0.2 | 14.8 | 17.0 | 15.5 |\\n| GAT-fs ResNet50 | 82.1 | 41.9 | 65.7 | 24.8 | 1.2 | 0.1 | 17.2 | 18.3 | 17.5 |\\n| Multi-attack defense MAAT-Average ResNet50 | 86.8 | 39.9 | 70.7 | 22.4 | 1.7 | -0.8 | 25.0 | 41.7 | 30.5 |\\n| MAAT-Max ResNet50 | 84.0 | 25.6 | 68.1 | 13.1 | 1.8 | 0.0 | 30.2 | 29.8 | 30.0 |\\n| MAAT-Random ResNet50 | 85.2 | 22.1 | 67.7 | 10.5 | 0.2 | 0.0 | 12.7 | 31.0 | 18.8 |\\n| OOD generalization method PLAT ResNet18 | 94.7 | 0.1 | 80.3 | 0.0 | 4.0 | -0.0 | 0.0 | 0.0 |\\n| CARD-Deck WRN-18-2 | 96.5 | 1.0 | 83.5 | 0.5 | 5.4 | -0.0 | 0.0 | 0.0 |\\n| Multi-attack + OOD defense MSD+REx ResNet18 | 78.0 | 43.3 | 59.9 | 26.2 | -0.7 | 0.5 | 19.0 | 40.4 | 26.1 |\\n\\nModel trained by Adversarial Contrastive Learning (ACL) (Jiang et al., 2020) which combines self-supervised contrastive learning with adversarial training. The effective robustness under dataset shift is 0.1% (Table 2), suggesting only marginal benefit in improving OOD robustness.\\n\\n6.6. Non-single-$\\\\ell_p$ Defenses\\n\\nThis section evaluates the OOD generalization capability of various defenses beyond the supervised single-attack $\\\\ell_p$ defense. We compared several specific methods, including AT with the color-based attack ReColor (Laidlaw et al., 2021), the spatial attack StAdv (Laidlaw et al., 2021), and the LPIPS-bound attack PAT-Alexnet/Self (Laidlaw et al., 2021). Additionally, we examined composite attacks defense (e.g., color plus $\\\\ell_p$) GAT-f/fs (Hsiung et al., 2023) and multiple attacks adversarial training (MAAT) (Tramer & Boneh, 2019; Maini et al., 2020) involving $\\\\ell_2$, $\\\\ell_\\\\infty$, StAdv, and ReColor. MAAT has three variants: \u201cAverage\u201d optimizes the average loss across all attacks, \u201cMax\u201d optimizes the maximum loss across all attacks, and \u201cRandom\u201d selects a random attack at each training iteration.\\n\\nUnfortunately, none of these defenses achieve high OOD ER and AER in Table 2, indicating that they are not significantly better than the supervised single-attack $\\\\ell_p$ AT at handling OOD dataset distribution shifts. This reinforces our conclusion that achieving OOD adversarial robustness is challenging with existing methods and underscores the need to develop new approaches that effectively address distribution shifts.\\n\\nWhile MAAT and PAT show substantial improvements over $\\\\ell_p$ AT in terms of robustness against non-$\\\\ell_p$ attacks, this is partly because some of the non-$\\\\ell_p$ attacks used were already encountered during their training, making them no longer unforeseen. This highlights the difficulty in benchmarking unforeseen robustness across different types of defenses.\\n\\n6.7. Summary\\n\\nThe evaluated techniques, except for some AT methods (Section 6.3), achieve relatively limited or even no adversarial effective robustness. This suggests that applying them is unlikely to significantly change the linear trend in Section 4 and thus the predicted upper limit of OOD robustness (Section 5). In contrast, the methods identified in Section 6.3 show the promise in achieving OOD performance beyond our prediction. Another promising direction is to combine OOD generalization methods with adversarial training.\\n\\n7. Conclusions\\n\\nThis work proposes a new benchmark to assess OOD adversarial robustness, provides many insights into the generalization of existing robust models under distribution shift and identifies several robust interventions beneficial to OOD generalization. We have analyzed the OOD robustness of hundreds of diverse models to ensure that we obtain generically applicable insights. As we focus on general trends, our analysis does not provide a detailed investigation into individual methods or explain the observed outliers such as the catastrophic robustness degradation. However, OODRobustBench provides a tool for performing such more detailed investigations in the future. It also provides a means of measuring progress towards models that are more robust in real-world conditions and will, hopefully, spur the future development of such models.\"}"}
{"id": "kAFevjEYsz", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C. Model Zoo\\n\\nC.1. Criteria for Robust Models\\nWe follow the same criteria as the popular benchmarks (RobustBench (Croce et al., 2021), MultiRobustBench (Dai et al., 2023), etc), which only include robust models that (1) have in general non-zero gradients w.r.t. the inputs, (2) have a fully deterministic forward pass (i.e. no randomness) and (3) do not have an optimization loop. These criteria include most AT models, while excluding most preprocessing methods because they rely on randomness like Guo et al. (2018) or inner optimization loop like Samangouei et al. (2018) which leads to false security, i.e., high robustness to the non-adaptive attack but vulnerable to the adaptive attack.\\n\\nMeanwhile, we acknowledge that evaluating dynamic preprocessing-based defenses is still an active area of research. It is tricky (Croce et al., 2022), and there has not been a consensus on how to evaluate them. So now, we exclude them for a more reliable evaluation. We will keep maintaining this benchmark, and we would be happy to include them in the future if the community has reached a consensus on that (e.g., if these models are merged into RobustBench).\\n\\nC.2. Model Zoo\\nOur model zoo consists of 706 models, of which:\\n\\n- 396 models are trained on CIFAR10 by $\\\\ell_\\\\infty^{8/255}$\\n- 239 models are trained on CIFAR10 by $\\\\ell_2^{0.5}$\\n- 56 models are trained on ImageNet by $\\\\ell_\\\\infty^{4/255}$\\n- 10 models are trained on CIFAR10 for non-$\\\\ell_p$ adversarial robustness\\n- 5 models are trained on CIFAR10 for common corruption robustness\\n\\nAmong the above models, 66 models of CIFAR10 $\\\\ell_\\\\infty$, 19 models of CIFAR10 $\\\\ell_2$ and 18 models of ImageNet $\\\\ell_\\\\infty$ are retrieved from RobustBench. 84 models are retrieved from the published works including (Li et al., 2023; Li & Spratling, 2023a,b,c; Liu et al., 2023; Singh et al., 2023; Dai et al., 2022; Hsiung et al., 2023; Mao et al., 2022). The remaining models are trained by ourselves.\\n\\nWe locally train additional models with varying architectures and training parameters to complement the public models from RobustBench on CIFAR-10. We consider 20 model architectures: DenseNet-121 (Huang et al., 2017), GoogLeNet (Szegedy et al., 2015), Inception-V3 (Szegedy et al., 2016), VGG-11/13/16/19 (Simonyan & Zisserman, 2015), ResNet-34/50/101/152 (He et al., 2016), EfficientNet-B0 (Tan & Le, 2019), MobileNet-V2 (Sandler et al., 2018), DLA (Yu et al., 2018), ResNeXt-29 (2x64d/4x64d/32x4d/8x64d) (Xie et al., 2017), SeNet-18 (Hu et al., 2018), and ConvMixer (Trockman & Kolter, 2023). For each architecture, we vary the training procedure to obtain 15 models across four adversarial training methods: PGD (Madry et al., 2018), TRADES (Zhang et al., 2019), PGD-SCORE, and TRADES-SCORE (Pang et al., 2022).\\n\\nWe train all models under both $\\\\ell_\\\\infty$ and $\\\\ell_2$ threat models with the following steps:\\n\\n1. We use PGD adversarial training to train eight models with batch size $\\\\in \\\\{128, 512\\\\}$, a learning rate $\\\\in \\\\{0.1, 0.05\\\\}$, and weight decay $\\\\in \\\\{10^{-4}, 10^{-5}\\\\}$. We also save the overall best hyperparameter choice. For the $\\\\ell_2$ threat model, we fix the learning rate to 0.1 since we observe that with $\\\\ell_\\\\infty$, 0.1 is strictly better than 0.05.\\n\\n2. Using the best hyperparameter choice, we train one model with PGD-SCORE, three with TRADES, and three with TRADES-SCORE. For TRADES and TRADES-SCORE, we take their $\\\\beta$ parameter from $\\\\{0.1, 0.3, 1.0\\\\}$.\\n\\nAfter training, we observe that some locally trained models exhibit inferior accuracy and/or robustness that is abnormally lower than others. The influence of inferior models on the correlation analysis is discussed in Appendix F. Finally, we filter out all models with an overall performance (accuracy + robustness) below 110. This threshold is determined to exclude only those evidently inferior models so that the size of model zoo (557 after filtering) is still large enough to ensure the generality and comprehensiveness of the conclusions drawn on it.\"}"}
{"id": "kAFevjEYsz", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### OOD Performance and Ranking\\n\\nTable 4. Performance, evaluated with OODRobustBench, of state-of-the-art models trained on CIFAR10 to be robust to $\\\\ell_2$ attacks.\\n\\nTop 3 results under each metric are highlighted by **bold** and/or *underscore*. The column \\\"OOD\\\" gives the overall OOD robustness which is the mean of the robustness to OOD $d$ and OOD $t$.\\n\\n| Method          | Model               | Accuracy | $\\\\ell_2$ | $\\\\ell_\\\\infty$ | $L_\\\\infty$ | Overall OOD | Ranking (Rob.) |\\n|-----------------|---------------------|----------|----------|---------------|-------------|--------------|---------------|\\n| BDM (Wang et al., 2023) | WRN-70-16           | 95.54    | 80.04    | 84.97         | 60.83       | 48.74        | 1             |\\n| BDM (Wang et al., 2023) | WRN-28-10           | 95.16    | 79.28    | 83.69         | 59.39       | 35.04        | 2             |\\n| FDA (Rebuffi et al., 2021) | WRN-70-16 (extra)   | 95.74    | 79.90    | 82.36         | 57.94       | 31.71        | 3             |\\n| Uncovering (Gowal et al., 2021a) | WRN-70-16 (extra) | 95.74    | 79.90    | 82.36         | 57.94       | 31.71        | 4             |\\n| FDA (Rebuffi et al., 2021) | WRN-28-10           | 91.79    | 75.26    | 78.79         | 55.63       | 44.48        | 7             |\\n| RATIO (Augustin et al., 2020) | WRN-34-10 (extra)  | 94.74    | 78.78    | 80.56         | 56.18       | 30.48        | 4             |\\n| FDA (Rebuffi et al., 2021) | WRN-28-10           |         |          |               |             |              |               |\\n| PORT (Sehwag et al., 2022) | WRN-34-10           | 90.93    | 74.00    | 77.29         | 54.33       | 29.44        | 8             |\\n| RATIO (Augustin et al., 2020) | WRN-34-10           |         |          |               |             |              |               |\\n| HATE (Rade & Moosavi-Dezfooli, 2022) | PreActResNet-18 | 90.57    | 73.55    | 76.14         | 53.35       | 29.69        | 10            |\\n| FDA (Rebuffi et al., 2021) | RreActResNet-18     |         |          |               |             |              |               |\\n| Uncovering (Gowal et al., 2021a) | WRN-70-16           |         |          |               |             |              |               |\\n| PORT (Sehwag et al., 2022) | ResNet-18           | 89.76    | 72.31    | 74.42         | 51.76       | 39.22        | 13            |\\n| AWP (Wu et al., 2020) | WRN-34-10           | 88.51    | 71.23    | 73.66         | 51.53       | 39.52        | 14            |\\n| RATIO (Augustin et al., 2020) | ResNet-50           | 91.07    | 74.24    | 72.99         | 49.32       | 39.02        | 15            |\\n| PGD10 (Engstrom et al., 2019) | ResNet-50           | 90.83    | 73.85    | 69.25         | 46.65       | 32.18        | 16            |\\n| Overfitting (Rice et al., 2020) | PreActResNet-18    | 88.67    | 71.27    | 67.69         | 44.76       | 31.67        | 17            |\\n| DDN (Rony et al., 2019) | WRN-38-10           | 89.04    | 71.77    | 66.46         | 44.54       | 31.42        | 18            |\\n| MMA (Ding et al., 2020) | WRN-28-4            | 88.00    | 72.32    | 66.09         | 43.79       | 30.15        | 19            |\\n\\nTable 5. Performance, evaluated with OODRobustBench, of state-of-the-art models trained on ImageNet to be robust to $\\\\ell_\\\\infty$ attacks.\\n\\nTop 3 results under each metric are highlighted by **bold** and/or *underscore*. The column \\\"OOD\\\" gives the overall OOD robustness which is the mean of the robustness to OOD $d$ and OOD $t$.\\n\\n| Method          | Model               | Accuracy | $\\\\ell_2$ | $\\\\ell_\\\\infty$ | $L_\\\\infty$ | Overall OOD | Ranking (Rob.) |\\n|-----------------|---------------------|----------|----------|---------------|-------------|--------------|---------------|\\n| Comprehensive (Liu et al., 2023) | Swin-L           | 78.92    | 45.84    | 59.82         | 23.59       | 26.74        | 1             |\\n| Comprehensive (Liu et al., 2023) | ConvNeXt-L       | 78.02    | 44.74    | 58.76         | 23.35       | 26.72        | 2             |\\n| Revisiting (Singh et al., 2023) | ConvNeXt-L-ConvStem | 77.00    | 44.05    | 57.82         | 23.09       | 25.53        | 3             |\\n| Comprehensive (Liu et al., 2023) | Swin-B           | 76.16    | 42.58    | 56.26         | 21.45       | 24.24        | 4             |\\n| Revisiting (Singh et al., 2023) | ConvNeXt-B-ConvStem | 75.88    | 42.29    | 56.24         | 21.77       | 24.83        | 5             |\\n| Comprehensive (Liu et al., 2023) | ConvNeXt-B       | 76.70    | 43.06    | 56.02         | 21.74       | 24.36        | 6             |\\n| Revisiting (Singh et al., 2023) | ViT-B-ConvStem  | 76.30    | 44.67    | 54.90         | 21.76       | 25.37        | 7             |\\n| Revisiting (Singh et al., 2023) | ConvNeXt-S-ConvStem | 74.08    | 39.55    | 52.66         | 19.35       | 23.11        | 8             |\\n| Revisiting (Singh et al., 2023) | ConvNeXt-B       | 75.08    | 40.68    | 52.44         | 20.09       | 23.07        | 9             |\\n| Comprehensive (Liu et al., 2023) | Swin-S           | 75.20    | 40.84    | 52.10         | 19.67       | 22.20        | 10            |\\n| Comprehensive (Liu et al., 2023) | ConvNeXt-S       | 75.64    | 40.91    | 51.66         | 19.40       | 22.20        | 11            |\\n| Revisiting (Singh et al., 2023) | ConvNeXt-T-ConvStem | 72.70    | 38.15    | 49.46         | 17.97       | 21.65        | 12            |\\n| Revisiting (Singh et al., 2023) | ViT-S-ConvStem  | 72.58    | 39.24    | 48.46         | 17.83       | 21.63        | 13            |\\n| Revisiting (Singh et al., 2023) | ViT-B           | 72.98    | 42.38    | 48.34         | 20.43       | 23.34        | 14            |\\n| Light (Debenedetti et al., 2023) | XCiT-L12        | 73.78    | 38.10    | 47.88         | 15.84       | 19.53        | 15            |\\n| Revisiting (Singh et al., 2023) | ViT-M           | 71.78    | 39.88    | 47.34         | 18.95       | 22.10        | 16            |\\n| Revisiting (Singh et al., 2023) | ConvNeXt-T       | 71.88    | 37.70    | 46.98         | 17.13       | 19.25        | 17            |\\n| Easy (Mao et al., 2022) | Swin-B           | 74.14    | 38.45    | 46.54         | 15.36       | 18.78        | 18            |\\n| Comprehensive (Liu et al., 2023) | ViT-B           | 73.84    | 39.88    | 45.90         | 18.01       | 20.48        | 19            |\\n| Light (Debenedetti et al., 2023) | XCiT-M12        | 74.04    | 37.00    | 45.76         | 14.73       | 18.77        | 20            |\"}"}
{"id": "kAFevjEYsz", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nD.2. Performance Degradation Distribution\\n\\nFigure 9. Degradation of accuracy and robustness under various distribution shifts for CIFAR10 $\\\\ell_2$.\\n\\nFigure 10. Degradation of accuracy and robustness under various distribution shifts for ImageNet $\\\\ell_\\\\infty$.\\n\\nD.3. Correlation Between ID and OOD Performance under Dataset Shifts\\n\\nFigure 11. $R^2$ of regression between ID and OOD performance for Standardly-Trained (ST) and Adversarially-Trained (AT) models under dataset shifts for CIFAR10 $\\\\ell_2$. Higher $R^2$ implies stronger linear correlation. The result of ST is copied from (Miller et al., 2021).\\n\\nFigure 12. $R^2$ of regression between ID and OOD performance for Standardly-Trained (ST) and Adversarially-Trained (AT) models under dataset shifts for ImageNet $\\\\ell_\\\\infty$. Higher $R^2$ implies stronger linear correlation. The result of ST is copied from (Miller et al., 2021).\"}"}
{"id": "kAFevjEYsz", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OO D Robust Bench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nFigure 13. \\\\( R^2 \\\\) of regression between ID and OOD performance for Adversarially-Trained (AT) models under various dataset shifts.\\n\\n\u201cAcc-Rob\u201d denotes the linear model between ID accuracy (x) and OOD robustness (y) and \u201cRob-Acc\u201d for ID robustness (x) and OOD accuracy (y).\\n\\nD.4. Unsupervised OOD Robustness Prediction\\n\\nFigure 14. Correlation between ID and OOD prediction agreement on adversarial examples for CIFAR10 \\\\( \\\\ell_\\\\infty \\\\) AT models.\"}"}
{"id": "kAFevjEYsz", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nFigure 1. The construction of OODRobustBench (top) and the correlation between ID and OOD robustness under 4 types of distribution shift for CIFAR10 $\\\\ell_\\\\infty$ (bottom). Each marker represents a model and is annotated by its training set-up. The solid blue line is the fitted linear correlation. The dashed gray line ($y=x$) represents perfect generalization where OOD robustness equals ID robustness. Deviation from the dashed line indicates robustness degradation under the respective distribution shift.\\n\\nWith OODRobustBench, we analyze the OOD generalization behavior of 706 well-trained robust models (a total of 60.7K adversarial evaluations). This model zoo covers a diversity of architectures, robust training methods, data augmentation techniques and training set-ups to ensure the conclusions drawn from this assessment are general and comprehensive. This large-scale analysis reveals that:\\n\\n- Adversarial robustness suffers from a severe OOD generalization issue. Robustness degrades on average by 18%/31%/24% under distribution shifts for CIFAR10 $\\\\ell_\\\\infty$, CIFAR10 $\\\\ell_2$ and ImageNet $\\\\ell_\\\\infty$ respectively.\\n\\n- ID and OOD accuracy/robustness have a strong linear correlation under many shifts (visualized in Figure 1). This enables the prediction of OOD performance from ID performance.\\n\\nThe findings above are rigorously identified by a large-scale, systematic, analysis for the first time. Furthermore, our analysis also offer several novel insights into the OOD generalization behavior of adversarial robustness:\\n\\n- The higher the ID robustness of the model, the more robustness degrades under distribution shift. This suggests that while great progress has been made on improving ID robustness, we only gain diminishing returns under distribution shift.\\n\\n- An abnormal catastrophic drop in robustness under noise shifts is observed in some methods. For instance, under Gaussian noise shift, HAT (Rade & Moosavi-Dezfooli, 2022) suffers from a severe drop of robustness by 46% whereas the average drop is 9%.\\n\\n- Adversarial training boosts the correlation between ID and OOD performance under corruption shifts, and thus, improves the fidelity of using ID performance for model selection and OOD performance prediction.\\n\\n- $\\\\ell_p$ robustness correlates poorly with non-$\\\\ell_p$ robustness. This suggests that non-$\\\\ell_p$ robustness cannot be predicted from $\\\\ell_p$ robustness, and enhancing $\\\\ell_p$ robustness does not necessarily result in improved non-$\\\\ell_p$ robustness.\\n\\nLast, we investigate how to achieve OOD adversarial robustness. First, based on the discovered linear trend, we predict the best available OOD performance for the existing $\\\\ell_p$-based robustness methodology and find that existing methods are unlikely to achieve high OOD adversarial robustness (e.g. the predicted upper bound of OOD robustness under the dataset shifts is only 43% on ImageNet $\\\\ell_\\\\infty$). Next, we examine a wide range of techniques for achieving OOD adversarial robustness beyond the above prediction. Most of these techniques, including training with extra data, data augmentation, advanced model architectures, scaling-up models and unsupervised representation learning, have...\"}"}
{"id": "kAFevjEYsz", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nLimited or no benefit. However, we do identify several adversarial training methods (Dai et al., 2022; Pang et al., 2020; Ding et al., 2020; Bai et al., 2023) that have the potential to exceed the prediction and produce higher OOD adversarial robustness.\\n\\nOverall, this work reveals that most existing robust models including the state-of-the-art ones are vulnerable to distribution shifts and demonstrates that the existing approaches to improve ID robustness may be insufficient to achieve high OOD robustness. To ensure safe deployment in the wild, we advocate for the assessment of OOD robustness in future models and for the development of new approaches that can cope with distribution shifts better and achieve OOD robustness beyond our prediction.\\n\\n2. Related Works\\n\\nRobustness under dataset shift. Early work (Sehwag et al., 2019) studied the generalization of robustness to novel classes that are unseen during training. On the other hand, our setup only considers the input distribution shift and not the unforeseen classes. Recently, Sun et al. (2022b) studied the OOD generalization of certified robustness under corruption shifts for a few state-of-the-art methods. In contrast, we focus on empirical robustness instead of certified robustness. Alhamoud et al. (2023) is the most relevant work. They studied the generalization of robustness from multiple source domains to an unseen domain. Different from them, the models we examine are trained on only one source domain, which is the most common set-up in the existing works of adversarial training (Croce et al., 2021). Moreover, we also cover much more diverse distribution shifts, models and training methods than Sun et al. (2022b) and Alhamoud et al. (2023) so that the conclusion drawn in this work is more general and comprehensive.\\n\\nRobustness against unforeseen adversarial threat models. It was observed that naive adversarial training (Madry et al., 2018) with only one single $\\\\ell_p$ threat model generalizes poorly to unforeseen $\\\\ell_p$ threat models, e.g., higher perturbation bound (Stutz et al., 2020), different $p$-norm (Tramer & Boneh, 2019; Maini et al., 2020; Croce & Hein, 2022), or non-$\\\\ell_p$ threat models including color transformation ReColor (Laidlaw & Feizi, 2019), spatial transformation StAdv (Xiao et al., 2018), LPIPS-bounded attacks PPGD and LPA (Laidlaw et al., 2021) and many others (Kaufmann et al., 2023). We complement the existing works by conducting a large-scale analysis on the unforeseen robustness of $\\\\ell_p$ robust models trained by varied methods and training set-ups. We are thus able to provide new insights into the generalization of robustness to unforeseen threat models and identify effective yet previously unknown approaches to enhance unforeseen robustness.\\n\\nMore related works are discussed in Appendix A.\\n\\n3. OOD Adversarial Robustness Benchmark\\n\\n3.1. OODRobustBench\\n\\nOODRobustBench is designed to simulate the possible data distribution shifts that might occur in the wild and evaluate adversarial robustness in the face of them. It focuses on two types of distribution shifts: dataset shift and threat shift.\\n\\nDataset shift. Dataset shift, OOD$_d$, denotes the distributional difference between training and test raw datasets. Threat shift, OOD$_t$, denotes the difference between training and evaluation threat models, a special type of distribution shift. The original test set drawn from the same distribution as the training set is considered ID. The variant dataset with the same classes yet where the distribution of the inputs differs is considered OOD.\\n\\nDataset shift. To represent diverse data distribution in the wild, OODRobustBench includes multiple types of dataset shifts from two sources: natural and corruption. For natural shifts, we adopt four different variant datasets per source dataset: CIFAR10.1 (Recht et al., 2018), CIFAR10.2 (Lu et al., 2020), CINIC (Darlow et al., 2018), and CIFAR10-R (Hendrycks et al., 2021a) for CIFAR10, and ImageNet-v2 (Recht et al., 2019), ImageNet-A (Hendrycks et al., 2021b), ImageNet-R (Hendrycks et al., 2021a), and ObjectNet (Barbu et al., 2019) for ImageNet. For corruption shifts, we adopt, from the corruption benchmarks (Hendrycks & Dietterich, 2019), 15 types of common corruption in four categories: Noise (gaussian, impulse, shot), Blur (motion, defocus, glass, zoom), Weather (fog, snow, frost) and Digital (brightness, contrast, elastic, pixelate, JPEG). Each corruption has five levels of severity. Overall, the dataset-shift testbed consists of 79 ($4 + 15 \\\\times 5$) subsets. Appendix B.1 gives the details of the above datasets and data processing.\\n\\nAccuracy and robustness are evaluated on the ID and OOD dataset. To compute the overall performance of OOD$_d$, we first average the result of natural and corruption shifts:\\n\\n$$R_c(f) = \\\\frac{1}{|\\\\text{corruptions}|} \\\\sum_{i \\\\in \\\\{\\\\text{corruptions}\\\\}, j \\\\in \\\\{\\\\text{severity}\\\\}} R_{i,j}(f)$$\\n\\n$$R_n(f) = \\\\frac{1}{|\\\\text{naturals}|} \\\\sum_{i \\\\in \\\\{\\\\text{naturals}\\\\}} R_i(f)$$\\n\\nwhere $R(\\\\cdot)$ returns accuracy or adversarial robustness and $f$ denotes the model to be assessed. Next, we average the above two results to get the overall performance of the dataset shift as:\\n\\n$$R_{ood}(f) = \\\\frac{R_c(f) + R_n(f)}{2}$$\\n\\nTo evaluate a model, OODRobustBench performs 80 (79 for OOD$_d$ and 1 for ID) runs of adversarial evaluation. This makes computationally expensive attacks like AutoAttack (Croce & Hein, 2020) impractical to use. To balance efficiency and effectiveness, we use MM5 (Gao et al., 2022) for...\"}"}
{"id": "kAFevjEYsz", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Performance, evaluated with OODRobustBench, of state-of-the-art models trained on CIFAR10 to be robust to $\\\\ell_\\\\infty$ attacks. The top 3 results for each metric are highlighted in bold and/or underscore. Significant ranking discrepancies are indicated in red. The \u201cOOD\u201d column presents the average of the robustness to OOD$_d$ and OOD$_t$. The complete leaderboard, featuring a total of 396 models, is available at https://oodrobustbench.github.io/.\\n\\n| Method          | Model       | Accuracy (%) | Robustness (%) | Ranking |\\n|-----------------|-------------|--------------|----------------|---------|\\n|                 |             | ID OOD$_d$   | ID OOD$_t$     | ID OOD  |\\n| BDM (Wang et al., 2023) | WRN-70-16   | 93.2         | 76.0           | 70.7    | 44.4 |\\n|                 |             | 35.8         | 40.1           | 1       | 2    |\\n| AS (Bai et al., 2023)    | WRN-70-16 + ResNet-152 | 95.2 | 79.0 | 69.5 | 43.3 |\\n|                 |             | 46.7         | 45.0           | 2       | 1    |\\n| DKL (Cui et al., 2023)   | WRN-28-10   | 92.1         | 74.8           | 67.7    | 42.4 |\\n|                 |             | 35.4         | 38.9           | 3       | 4    |\\n| BDM (Wang et al., 2023) | WRN-28-10   | 92.4         | 75.0           | 67.3    | 42.3 |\\n|                 |             | 35.2         | 38.8           | 4       | 5    |\\n| FDA (Rebuffi et al., 2021) | WRN-70-16   | 92.2         | 74.8           | 66.7    | 42.6 |\\n|                 |             | 33.6         | 38.1           | 5       | 9    |\\n| DDPM (Gowal et al., 2021b) | WRN-70-16  | 88.7         | 70.6           | 66.2    | 42.7 |\\n|                 |             | 33.6         | 38.2           | 6       | 8    |\\n| Uncovering (Gowal et al., 2021a) | WRN-70-16 | 91.1         | 73.2           | 66.0    | 42.5 |\\n|                 |             | 34.0         | 38.2           | 7       | 7    |\\n| RobustResNet (Huang et al., 2023a) | WRN-A4   | 91.5         | 73.8           | 65.8    | 41.7 |\\n|                 |             | 33.3         | 37.5           | 8       | 12   |\\n| FDA (Rebuffi et al., 2021) | WRN-106-16 | 88.5         | 70.6           | 64.8    | 41.4 |\\n|                 |             | 33.9         | 37.6           | 9       | 10   |\\n| DyART (Xu et al., 2022) | WRN-28-10   | 93.6         | 77.2           | 64.7    | 39.6 |\\n|                 |             | 37.0         | 37.0           | 10      | 6    |\\n| PORT (Sehwag et al., 2022) | ResNet-152 | 87.2         | 69.2           | 62.7    | 40.7 |\\n|                 |             | 32.3         | 36.5           | 17      | 15   |\\n| HAT (Rade & Moosavi-Dezfooli, 2022) | WRN-28-10 | 88.1         | 69.4           | 60.9    | 35.1 |\\n|                 |             | 30.2         | 32.6           | 22      | 57   |\\n| AWP (Wu et al., 2020) | WRN-28-10   | 88.2         | 69.8           | 60.1    | 38.2 |\\n|                 |             | 31.3         | 34.8           | 26      | 27   |\\n| RST (Carmon et al., 2019) | WRN-28-10   | 89.6         | 71.5           | 59.8    | 36.7 |\\n|                 |             | 31.1         | 33.9           | 28      | 38   |\\n| MART (Wang et al., 2020) | WRN-28-10   | 87.5         | 70.2           | 56.7    | 35.5 |\\n|                 |             | 32.6         | 34.0           | 52      | 35   |\\n| HE (Pang et al., 2020) | WRN-34-20   | 85.1         | 66.9           | 53.8    | 32.4 |\\n|                 |             | 46.2         | 39.3           | 7       | 3    |\\n| FAT (Zhang et al., 2020) | WRN-34-10   | 84.5         | 65.9           | 53.6    | 32.9 |\\n|                 |             | 31.8         | 32.4           | 71      | 59   |\\n| Overfitting (Rice et al., 2020) | WRN-34-20 | 85.3         | 66.4           | 53.5    | 32.0 |\\n|                 |             | 27.8         | 29.9           | 72      | 89   |\\n| TRADES (Zhang et al., 2019) | WRN-34-10 | 84.9         | 66.5           | 52.6    | 31.6 |\\n|                 |             | 26.5         | 29.1           | 76      | 99   |\\n| FBF (Wong et al., 2020) | PreActResNet-18 | 83.3         | 64.9           | 43.3    | 25.3 |\\n|                 |             | 24.8         | 25.0           | 111     | 112  |\\n\\nRobustness evaluation. MM5 is approximately $32 \\\\times$ faster than AutoAttack (Gao et al., 2022) while achieving similar results, as verified in Appendix B.2 alongside the results of evaluations using alternative attacks. The perturbation bound $\\\\epsilon$ is $8/255$ for CIFAR10 $\\\\ell_\\\\infty$, $0.5$ for CIFAR10 $\\\\ell_2$ and $4/255$ for ImageNet $\\\\ell_\\\\infty$.\\n\\nThreat shift. OODRobustBench adopts six unforeseen attacks as in Laidlaw et al. (2021); Dai et al. (2022) to simulate threat shifts. They are categorized into two groups, $\\\\ell_p$ and non-$\\\\ell_p$, according to whether they are bounded by the $\\\\ell_p$ norm or not. The $\\\\ell_p$ shift group includes MM attacks with the same $p$-norm but larger $\\\\epsilon$ and with different $p$-norm. The non-$\\\\ell_p$ shift group includes the imperceptible, PPGD and LPA, and perceptible, ReColor and StAdv, attacks. The overall robustness under threat shift, OOD$_t$, is simply the mean of these six unforeseen attacks. These attacks are selected because they cover a wide range of different scenarios of threat shift and each of them is representative of its corresponding category (100+ cites). We are aware of alternative non-$\\\\ell_p$ attacks (Kaufmann et al., 2023) but do not include them due to the constraint of computational resource. We follow the same setting as Laidlaw et al. (2021); Dai et al. (2022) to configure the above attacks since this has been well tested to be effective. The $\\\\ell_p$ attacks use $\\\\epsilon = \\\\frac{12}{255}$ and $\\\\epsilon = 0.5$ for $\\\\ell_\\\\infty$ and $\\\\ell_2$ threats on CIFAR10 $\\\\ell_\\\\infty$, $\\\\epsilon = \\\\frac{8}{255}$ and $\\\\epsilon = 1$ for $\\\\ell_\\\\infty$ and $\\\\ell_2$ threats on CIFAR10 $\\\\ell_2$ and on ImageNet $\\\\ell_\\\\infty$. The perturbation bound is 0.5 for PPGD, 0.5 for LPA, 0.05 for StAdv and 0.06 for ReColor. The number of iterations is 40 for PPGD and LPA regardless of dataset, is 100 for StAdv and ReColor on CIFAR10 and 200 on ImageNet.\\n\\nCriteria for robust models are described in Appendix C.1 and are the same as RobustBench (Croce et al., 2021).\\n\\n3.2. OOD Performance and Ranking\\n\\nThe benchmark results for CIFAR10 $\\\\ell_\\\\infty$, $\\\\ell_2$ and ImageNet $\\\\ell_\\\\infty$ are in Tables 1, 4 and 5 respectively. Robustness degrades significantly under distribution shift. For models trained to be robust for CIFAR10 $\\\\ell_\\\\infty$ (Figure 2), CIFAR10 $\\\\ell_2$ (Figure 9) and ImageNet $\\\\ell_\\\\infty$ (Figure 10), the average drop in robustness (ID adversarial accuracy - OOD adversarial accuracy) is 18%/20%/27% under dataset shift and 18%/42%/22% under threat shift. Robustness degradation is much severe for a subset of shifts: whereas the average robustness degradation of OOD$_d$ is 18% on CIFAR10 $\\\\ell_\\\\infty$, some shifts like CIFAR10-R, fog and contrast degrade by 38%, 30% and 32%, respectively.\\n\\nThe higher the ID robustness of the model, the more robustness degrades under the shifts. For example, the top method in Table 1 degrades by 30% of robustness, while the bottom method degrades by only 18%. This suggests that while the great progress has been made on improving ID robustness, we only gain diminishing returns under the distribution shifts. Besides, in Figure 2, the distribution of robustness degradation for most shifts spreads over a wide range, suggesting a large variation across individual models. Robustness degradation under noise shifts can be abnormally catastrophic (the outliers under noise shifts in Figure 2). This issue is most severe on (Rade & Moosavi-Dezfooli, 2022) whose robustness falls by 43%/46%/38% under impulse/Gaussian/shot noise, whereas the average drop is 12%/9%/8% (discussed in Appendix E). A similar...\"}"}
{"id": "kAFevjEYsz", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OOD Robust Bench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\nFigure 2. Degradation of accuracy and robustness under various distribution shifts for CIFAR10\\n\\n\\\\[ \\\\ell_\\\\infty \\\\]\\n\\nyet milder drop is also observed on Debenedetti et al. (2023) and models trained with some advanced data augmentations like AutoAugment (Cubuk et al., 2019).\\n\\nHigher ID robustness generally implies higher OOD robustness but not always (see the last two columns of Tables 1, 4 and 5). For example, in Table 1, the ranking of Rade & Moosavi-Dezfooli (2022) drops from 22 to 57 due to catastrophic degradation, while the ranking of Pang et al. (2020) jumps from 70 to 3 due to its superior robustness under threat shift (analyzed in Section 6.3).\\n\\n4. Linear Trend and OOD Prediction\\n\\nIt was previously observed that OOD accuracy is strongly correlated with ID accuracy under many dataset shifts for Standardly-Trained (ST) models (Miller et al., 2021). This property is important since it enables the model selection and OOD performance prediction through ID performance. Nevertheless, it is unclear if such correlation still holds for adversarial robustness. This is particularly intriguing because accuracy and robustness usually go in opposite directions: i.e. there is a trade-off between accuracy and robustness (Tsipras et al., 2019). Furthermore, the threat shifts as a scenario of OOD are unique to adversarial evaluation and were, thus, never explored in the previous studies of accuracy trends. Surprisingly, we find that ID and OOD robustness also have a linear correlation under many distribution shifts. It is even more surprising that the correlation for AT models is much stronger than that for ST models.\\n\\nThe following result is based on a large-scale analysis including over 60K OOD evaluations of 706 models. 187 of these models were retrieved from RobustBench or other published works so as to include current state-of-the-art methods, and the remaining models were trained by ourselves. These models are mainly trained in three set-ups: CIFAR10 $\\\\ell_\\\\infty$, CIFAR10 $\\\\ell_2$ and ImageNet $\\\\ell_\\\\infty$. They cover a wide range of model architectures, model sizes, data augmentation methods, training and regularization techniques. More detail is given in Appendix C.2.\\n\\n4.1. Linear Trend under Dataset Shift\\n\\nThis section studies how ID and OOD accuracy/robustness correlate under dataset shifts. We fit a linear regression on four pairs of metrics (Acc-Acc, Rob-Rob, Acc-Rob, and Rob-Acc) for each dataset shift and each training setup (CIFAR10 $\\\\ell_\\\\infty$, CIFAR10 $\\\\ell_2$ and ImageNet $\\\\ell_\\\\infty$). Taking Acc-Rob as an example, a linear model is fitted with ID accuracy as the observed variable $x$ and OOD adversarial robustness as the target variable $y$. The result for each shift is given in Appendix H. Below are the major findings.\\n\\nID accuracy (resp. robustness) strongly correlates with OOD accuracy (resp. robustness) in a linear relationship for most dataset shifts. In Figures 3, 11 and 12, the regression of Acc-Acc and Rob-Rob for most shifts achieve very high $R^2 (> 0.9)$, i.e., their relationship can be well explained by a linear model. This suggests for these shifts ID performance is a good indication of OOD performance, and more importantly, OOD performance can be reliably predicted by ID performance using the fitted linear model.\\n\\nNevertheless, under some shifts, ID and OOD performance are only weakly correlated. Natural shifts like CIFAR10-R and ImageNet-A and corruption shifts like noise, fog and contrast are observed to have relatively low $R^2$ across varied training set-ups in Figures 3, 11 and 12. It can be seen from Figures 16 and 17 that the correlation for these shifts becomes even weaker, and the gap of $R^2$ between them and the others expands, as more inferior (relatively worse accuracy and/or robustness) models are excluded from the regression. This suggests that the models violating the linear trend are mostly high-performance. Appendix F discusses how inferior models are identified and how they influence the correlation.\\n\\nAT models exhibit a stronger linear correlation between ID and OOD accuracy under most corruption shifts on CIFAR10 in Figures 3 and 11. The improvement is dramatic for particular shifts. For example, $R^2$ surges from nearly 0 (no linear correlation) for ST models to around 0.8 (evident linear correlation) for AT models with Gaussian and shot noise data shifts. These results are contrary to the previous\"}"}
{"id": "kAFevjEYsz", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift\\n\\n(a) $R^2$ of ID seen vs. unforeseen robustness for CIFAR10 $\\\\ell_\\\\infty$.\\n\\n(b) $R^2$ ID seen vs. unforeseen robustness for CIFAR10 $\\\\ell_2$.\\n\\nFigure 18. The change of $R^2$ under various threat shifts as the models with lower overall performance are removed from regression. Each row, with the filtering threshold labeled at the lead, corresponds to a new filtered model zoo and the regression conducted it. \\\"NC\\\" refers to No Custom models, so all models are retrieved from either RobustBench or other published works.\"}"}
{"id": "kAFevjEYsz", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Methods for Improving OOD Adversarial Robustness\\n\\nAll models used in this analysis are retrieved from RobustBench or other published works to ensure they are well-trained by the techniques to be examined. The specific experiment setting for each model can be found in its original paper.\\n\\n### Table 6. The effect of training with extra data on the OOD generalization of accuracy and robustness\\n\\n| Dataset | Threat | Training Model | Extra ID | OOD d | OOD t |\\n|---------|--------|----------------|----------|--------|--------|\\n| CIFAR10 | Linf   | (Gowal et al., 2021a) | WideResNet70-16 | - | 85.29 |\\n|         |        |                |          |       | 57.24  |\\n|         |        |                |          |       | 66.98  |\\n|         |        |                |          |       | 35.90  |\\n|         |        |                |          |       | -0.56  |\\n|         |        |                |          |       | 0.30   |\\n|         |        |                |          |       | 29.39  |\\n|         |        |                |          |       | -2.18  |\\n\\n### Table 7. The effect of data augmentation on the OOD generalization of accuracy and robustness\\n\\nThe results reported in Figure 7b are the mean of the results on ViT and WideResNets.\\n\\n| Dataset | Threat | Training Model | Augmentation | OOD d | OOD t |\\n|---------|--------|----------------|--------------|--------|--------|\\n| CIFAR10 | Linf   | (Li & Spratling, 2023c) | ViT-B | RandomCrop | 83.23 |\\n|         |        |                |              |       | 47.02  |\\n|         |        |                |              |       | 66.48  |\\n|         |        |                |              |       | 28.85  |\\n|         |        |                |              |       | 0.86   |\\n|         |        |                |              |       | 0.54   |\\n|         |        |                |              |       | 27.36  |\\n|         |        |                |              |       | 0.57   |\\n|         |        |                |              |       | Cutout  |\\n|         |        |                |              |       | 84.22  |\\n|         |        |                |              |       | 49.57  |\\n|         |        |                |              |       | 67.23  |\\n|         |        |                |              |       | 30.68  |\\n|         |        |                |              |       | 0.69   |\\n|         |        |                |              |       | 0.56   |\\n|         |        |                |              |       | 29.74  |\\n|         |        |                |              |       | 1.75   |\\n|         |        |                |              |       | CutMix |\\n|         |        |                |              |       | 80.92  |\\n|         |        |                |              |       | 47.45  |\\n|         |        |                |              |       | 63.93  |\\n|         |        |                |              |       | 29.89  |\\n|         |        |                |              |       | 0.48   |\\n|         |        |                |              |       | 1.27   |\\n|         |        |                |              |       | 30.48  |\\n|         |        |                |              |       | 3.49   |\\n|         |        |                |              |       | TrivialAugment |\\n|         |        |                |              |       | 80.33  |\\n|         |        |                |              |       | 46.61  |\\n|         |        |                |              |       | 64.59  |\\n|         |        |                |              |       | 29.56  |\\n|         |        |                |              |       | 1.69   |\\n|         |        |                |              |       | 1.54   |\\n|         |        |                |              |       | 30.40  |\\n|         |        |                |              |       | 3.80   |\\n|         |        |                |              |       | AutoAugment |\\n|         |        |                |              |       | 82.75  |\\n|         |        |                |              |       | 48.11  |\\n|         |        |                |              |       | 65.89  |\\n|         |        |                |              |       | 29.78  |\\n|         |        |                |              |       | 0.73   |\\n|         |        |                |              |       | 0.69   |\\n|         |        |                |              |       | 30.90  |\\n|         |        |                |              |       | 3.60   |\\n|         |        |                |              |       | IDBH |\\n|         |        |                |              |       | 86.92  |\\n|         |        |                |              |       | 51.55  |\\n|         |        |                |              |       | 70.51  |\\n|         |        |                |              |       | 32.08  |\\n|         |        |                |              |       | 1.45   |\\n|         |        |                |              |       | 0.54   |\\n|         |        |                |              |       | 30.59  |\\n|         |        |                |              |       | 1.68   |\\n|         |        |                |              |       | WideResNet34-10 |\\n|         |        |                |              |       | RandomCrop |\\n|         |        |                |              |       | 86.52  |\\n|         |        |                |              |       | 52.42  |\\n|         |        |                |              |       | 68.11  |\\n|         |        |                |              |       | 31.55  |\\n|         |        |                |              |       | -0.58  |\\n|         |        |                |              |       | -0.61  |\\n|         |        |                |              |       | 26.47  |\\n|         |        |                |              |       | -2.84  |\\n|         |        |                |              |       | Cutout |\\n|         |        |                |              |       | 86.77  |\\n|         |        |                |              |       | 53.31  |\\n|         |        |                |              |       | 68.40  |\\n|         |        |                |              |       | 31.03  |\\n|         |        |                |              |       | -0.53  |\\n|         |        |                |              |       | -1.76  |\\n|         |        |                |              |       | 27.00  |\\n|         |        |                |              |       | -2.74  |\\n|         |        |                |              |       | CutMix |\\n|         |        |                |              |       | 87.41  |\\n|         |        |                |              |       | 53.89  |\\n|         |        |                |              |       | 68.97  |\\n|         |        |                |              |       | 31.71  |\\n|         |        |                |              |       | -0.55  |\\n|         |        |                |              |       | -1.50  |\\n|         |        |                |              |       | 28.50  |\\n|         |        |                |              |       | -1.50  |\\n|         |        |                |              |       | TrivialAugment |\\n|         |        |                |              |       | 86.98  |\\n|         |        |                |              |       | 54.18  |\\n|         |        |                |              |       | 69.85  |\\n|         |        |                |              |       | 32.94  |\\n|         |        |                |              |       | 0.73   |\\n|         |        |                |              |       | -0.47  |\\n|         |        |                |              |       | 28.62  |\\n|         |        |                |              |       | -1.52  |\\n|         |        |                |              |       | AutoAugment |\\n|         |        |                |              |       | 87.93  |\\n|         |        |                |              |       | 55.10  |\\n|         |        |                |              |       | 70.05  |\\n|         |        |                |              |       | 32.17  |\\n|         |        |                |              |       | 0.04   |\\n|         |        |                |              |       | -1.90  |\\n|         |        |                |              |       | 29.06  |\\n|         |        |                |              |       | -1.51  |\\n|         |        |                |              |       | IDBH |\\n|         |        |                |              |       | 88.62  |\\n|         |        |                |              |       | 55.56  |\\n|         |        |                |              |       | 70.96  |\\n|         |        |                |              |       | 32.99  |\\n|         |        |                |              |       | 0.30   |\\n|         |        |                |              |       | -1.41  |\\n|         |        |                |              |       | 28.58  |\\n|         |        |                |              |       | -2.21  |\\n\\n### Table 8. The effect of model architecture on the OOD generalization of accuracy and robustness\\n\\n| Dataset | Threat | Training Model | Model Size (M) | OOD d | OOD t |\\n|---------|--------|----------------|----------------|--------|--------|\\n| ImageNet | \u2113\u221e    | (Liu et al., 2023) | ResNet152 | 60.19  |\\n|         |        |                | ConvNeXt-B | 88.59  |\\n|         |        |                | ViT-B | 86.57  |\\n|         |        |                | Swin-B | 87.77  |\\n|         |        |                | ImageNet | 86.92  |\\n|         |        |                | ConvNeXt-B | 88.59  |\\n|         |        |                | ViT-B | 86.57  |\\n|         |        |                | Swin-B | 87.77  |\\n|         |        |                | ImageNet | 86.92  |\\n\\n## Conclusion\\n\\nThe analysis of OOD robustness under distribution shift indicates that the use of extra data during training significantly improves the generalization accuracy and robustness of models. Data augmentation techniques, such as random cropping, cutout, and cutmix, also play a crucial role in enhancing model performance in out-of-distribution settings. The choice of model architecture also impacts the OOD generalization, with larger models generally achieving better performance, albeit at a higher computational cost.\"}"}
{"id": "kAFevjEYsz", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 9.\\nThe effect of model size on the OOD generalization of accuracy and robustness.\\nThe results reported in Figure 7d are averaged over three architectures at the corresponding relatively model size. For example, the result of \u201csmall\u201d is averaged over WideResNet28-10, ResNet50 and ConvNeXt-S-ConvStem.\\n\\n| Dataset | Threat | Training | Model ID | Size | Acc. | Rob. | OOD | Acc. | Rob. | EAcc. | ERob. | Rob. | ERob. |\\n|---------|--------|----------|----------|------|------|------|-----|------|------|------|------|------|------|\\n| CIFAR10 | $\\\\ell_\\\\infty$ | (Rebuffi et al., 2021) | WideResNet28-10 | 36.48 | 87.33 | 60.88 | 69.35 | 38.54 | -0.10 | 0.35 | 33.63 | 0.36 |\\n|         |        |          | WideResNet70-16 | 266.80 | 88.54 | 64.33 | 70.62 | 41.01 | 0.04 | 0.35 | 34.12 | -0.76 |\\n|         |        |          | WideResNet106-16 | 415.48 | 88.50 | 64.82 | 70.65 | 41.43 | 0.11 | 0.42 | 33.90 | -1.22 |\\n| ImageNet | $\\\\ell_\\\\infty$ | (Liu et al., 2023) | ResNet50 | 25.56 | 65.02 | 32.02 | 28.43 | 9.23 | -1.68 | -0.53 | 13.71 | -0.52 |\\n|         |        |          | ResNet101 | 44.55 | 68.34 | 39.76 | 31.74 | 12.44 | -1.76 | -1.08 | 16.82 | -1.72 |\\n|         |        |          | ResNet152 | 60.19 | 70.92 | 43.62 | 34.43 | 14.13 | -1.71 | -1.26 | 17.23 | -3.47 |\\n| ImageNet | $\\\\ell_\\\\infty$ | (Singh et al., 2023) | ConvNeXt-S-ConvStem | 50.26 | 74.08 | 52.66 | 39.55 | 19.35 | 0.19 | -0.42 | 26.87 | 1.14 |\\n|         |        |          | ConvNeXt-B-ConvStem | 88.75 | 75.88 | 56.24 | 42.29 | 21.77 | 1.10 | 0.26 | 27.89 | 0.16 |\\n|         |        |          | ConvNeXt-L-ConvStem | 198.13 | 77.00 | 57.82 | 44.05 | 23.09 | 1.71 | 0.80 | 27.98 | -0.63 |\\n\\n### Table 10.\\nThe effect of different adversarial training methods on the OOD generalization of accuracy and robustness.\\n\\n| Dataset | Threat | Training | ID | OOD | Acc. | Rob. | Acc. | Rob. | EAcc. | ERob. | Rob. | ERob. |\\n|---------|--------|----------|----|-----|------|------|------|------|------|------|------|------|\\n| CIFAR10 | $\\\\ell_\\\\infty$ | PGD (Li & Spratling, 2023c) | | | 86.52 | 52.42 | 68.11 | 31.55 | -0.58 | -0.61 | 26.47 | -2.84 |\\n|         |        | VR-$\\\\ell_\\\\infty$ (Dai et al., 2022) | | | 72.72 | 49.92 | 56.12 | 31.84 | 0.34 | 1.47 | 34.70 | 6.55 |\\n|         |        | PGD (Rice et al., 2020) | | | 85.34 | 53.52 | 66.46 | 32.07 | -1.12 | -0.88 | 27.89 | -1.94 |\\n|         |        | HE (Pang et al., 2020) | | | 85.14 | 53.84 | 66.96 | 32.45 | -0.43 | -0.72 | 46.20 | 16.22 |\\n|         |        | PGD (locally-trained) | | | 80.44 | 38.98 | 62.40 | 22.18 | -0.60 | -0.39 | 21.77 | -1.27 |\\n|         |        | MMA (Ding et al., 2020) | | | 84.37 | 41.86 | 68.22 | 24.65 | 1.54 | 0.02 | 35.12 | 10.74 |\\n|         |        | PGD Gowal et al. (2021a) | | | 91.10 | 66.03 | 73.24 | 42.58 | 0.26 | 0.71 | 34.00 | -1.67 |\\n|         |        | AS (Bai et al., 2023) | | | 95.23 | 69.50 | 79.09 | 43.32 | 2.25 | -1.03 | 46.71 | 9.41 |\"}"}
{"id": "kAFevjEYsz", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 19. Correlation between ID accuracy and OOD accuracy (odd rows); ID robustness and OOD robustness (even rows) for CIFAR10 $\\\\ell_\\\\infty$ AT models.\"}"}
