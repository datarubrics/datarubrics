{"id": "k1JXxbpIY6", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nAndreas Opedal\\n\\n* 1 2\\n\\nAlessandro Stolfo\\n\\n* 1\\n\\nHaruki Shirakami\\n\\n1\\n\\nYing Jiao\\n\\n3\\n\\nRyan Cotterell\\n\\n1\\n\\nBernhard Sch\u00f6lkopf\\n\\n1 2\\n\\nAbulhair Saparov\\n\\n4\\n\\nMrinmaya Sachan\\n\\n1\\n\\nAbstract\\n\\nThere is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which properties of human cognition are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand whether current LLMs display the same cognitive biases as children in these steps. We generate a novel set of word problems for each of these tests, using a neuro-symbolic approach that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but not in the final step, in which the arithmetic expressions are executed to obtain the answer.\\n\\n1. Introduction\\n\\nThere is active discussion around large pretrained language models (LLMs) as plausible cognitive models (Mahowald et al., 2023), e.g., in terms of language acquisition (Warstadt and Bowman, 2022), decision making (Aher et al., 2023) and political orientation (Argyle et al., 2023). In the context of education, cognitive modeling enables the study of human learning without the high cost of data collection from human subjects, which can lead to a better understanding of human learning and, therefore, improved learning outcomes (VanLehn et al., 1994). Several recent articles have already employed LLMs as models of students (Macina et al., 2023; Nguyen et al., 2023). However, for such modeling to be meaningful, it is imperative that the student model is consistent with actual student behavior. Yet, that is not always the case: Many existing student models fail to validate faithfulness to realistic classroom scenarios (K\u00e4ser and Alexandron, 2023). Importantly, an LLM that models the problem-solving process of children should also make similar mistakes as children, i.e., it should mimic the cognitive biases that are salient in children during problem-solving. Given that LLMs may be trained predominantly on data generated by adults, it is not obvious that they would exhibit child-like behavior.\\n\\nIn this paper, we study whether LLMs are subject to similar biases as children when solving arithmetic math word problems. These problems are interesting because they are conceptually simple, and yet, require several distinct skills to solve (Stern, 1993). A learner needs to understand the situation described, relate it to arithmetic equations, and...\"}"}
{"id": "k1JXxbpIY6", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nBy performing the required computations, as Fig. 1 illustrates. By Piaget's (1936) view on cognitive development, a problem might be difficult for a child due to insufficient development in any one of these skills. Much is known about what makes a word problem difficult for humans; we ask whether the same relative difficulties apply to LLMs.\\n\\nTo answer this question, we construct tests that are grounded in the extensive literature on word problem solving by children, and perform them on a suite of currently well-known LLMs. Specifically, each test varies a problem feature for which an effect on child performance has been established in the literature, e.g., the manner in which a particular mathematical operation is expressed in text, while controlling for other features. We create new English problems specifically for these tests, by developing a generation pipeline based on a semantic formalism over math word problems (Opedal et al., 2023). Our generation pipeline admits a family of standard arithmetic word problems, while controlling not only for numerical features, but structural (e.g., entity relationships) and linguistic ones (e.g., sentence structure) as well.\\n\\nWe test three cognitive biases, each one associated with a separate step of the solving process (which are illustrated by arrows in Fig. 1). The first test targets consistency bias: A problem text is easier to comprehend if the relational keyword verbally suggests the operation that is required to solve the problem (Lewis and Mayer, 1987). The second test targets what we call transfer vs comparison bias, that problems with a static comparison relationship are more difficult for children than problems with a dynamic change of state, even when they involve the same arithmetic expressions (Riley et al., 1983). The third test targets the carry effect, i.e., that arithmetic computations are more difficult if they entail moving some digit to a column of more significant digits (Hitch, 1978).\\n\\nWe find that LLMs indeed exhibit some biases that mirror those observed in children. Our experiments with both base and instruction-tuned models\u2014specifically, LLaMA2 (Touvron et al., 2023), Mistral (Jiang et al., 2023), Mixtral (Jiang et al., 2024), GPT-3.5 Turbo and GPT-4 Turbo (OpenAI, 2024)\u2014reveal that almost all models show significant effects of consistency bias and transfer vs comparison bias, like child learners. Most of these effects are further strengthened by using chain-of-thought prompting (Wei et al., 2022). However, we do not observe a carry effect bias in the solution execution step. These results contribute to our understanding of the capabilities and limitations of LLMs as cognitive models, particularly in the context of cognitive development research and educational applications.\\n\\n2. Cognitive Modeling of the Solving Process\\n\\nThis section discusses the cognitive process that is involved in solving math word problems. We first introduce our conceptualization that is illustrated in Fig. 1, which we then motivate by summarizing relevant literature.\\n\\nOur conceptualization. We are interested in identifying when LLMs are likely to exhibit human-like biases, and therefore, require a holistic analysis of the human problem-solving process. Our conceptualization, illustrated in Fig. 1, includes four representational levels of the math word problem, along with the skills associated with transitioning from one level to the next. We assume that a child goes through the following procedure when posed with a math word problem: First, they form a mental model of the mathematical relationships expressed in that problem (text comprehension). Next, they distill that mental model into a sequence of arithmetic expressions representing the step-by-step reasoning process to find the solution (solution planning) and, finally, calculate the answer from those expressions (solution execution). The representational levels will be formalized in \u00a74.1.\\n\\nBackground. Children tend to experience greater difficulty when posed with arithmetic math word problems compared to the same problems formulated solely as arithmetic expressions; see, e.g., Jaffe and Bolger (2023). This suggests that arithmetic computation skill alone is not sufficient to successfully solve math word problems. In order to distinguish the different skills that are involved, past work has represented math word problems along similar levels as we do above (Nesher and Teubal, 1975): (i) problem text, (ii) underlying mathematical relationships, and (iii) arithmetic expressions. Solving a problem, then, involves transitioning from level (i) to a final answer, possibly through levels (ii) and (iii), with each transition requiring a separate skill.\\n\\nThere is much research on which factors best explain problem difficulty. Riley et al.'s (1983) model of the problem-solving process emphasized the degree of complexity at level (ii) as the leading factor underlying performance, motivated by empirical evidence that some arithmetic concepts are harder for children to understand than others. However, their model does not account for how the mathematical relationships are derived from the problem text (Cummins et al., 1988). This part is significant as well, as several studies have found that altering the linguistic form of a problem without changing the underlying mathematical relationships can...\"}"}
{"id": "k1JXxbpIY6", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nCorrect all grammatical mistakes that appear in the following math word problem:\\n\\nFix any awkward or redundant phrasing. Pay close attention to incorrect plural forms. Do NOT solve the problem. Do NOT compute any intermediate solutions. Do NOT make any changes to the numerical values or implied mathematical operations. Only output the corrected math word problem and nothing else. Do NOT restate the original problem. Do NOT include \\\"Corrected Version:\\\" or any description of the task.\\n\\nTable 3. Prompt used for the linguistic error correction step in our generation pipeline from \u00a74.2.\\n\\nA. More Details on the Generation Method\\n\\nA.1. Details on Linguistic Error Correction\\n\\nWe use GPT-3.5 Turbo to carry out the linguistic error correction step detailed in \u00a74.2. In Table 3, we provide the exact prompt used for the task. The corrected problem is generated using greedy decoding (temperature=0). We carry out additional integrity checks of the generated problem against the original templated text. In particular, we verify that the sentence count and relational terms (such as \\\"more\\\") are consistent post error-correction. The problem is discarded if these additional checks are not satisfied.\\n\\nA.2. Data Quality Evaluation\\n\\nWe describe the manual evaluation of the datasets generated in \u00a75. For each of the datasets, we do the following: First, generate a control set of 10 examples. These 10 examples are evaluated independently by three of this paper's authors. If there are any errors we make appropriate modifications to the pipeline and restart the procedure. If not, we proceed to evaluate 90 more examples, allocating 30 to each of the three authors. Error rate is estimated on this sample of 100 examples.\\n\\nWe use two binary evaluation criteria, one assessing the linguistic error correction step (iv) and one assessing test-specific attributes. A problem is deemed to be good according to the former if the generated problem only deviates from the templated text through spelling or grammar correction. The test-specific criterion and the obtained error estimates are given below.\\n\\nConsistency bias (\u00a75.2). We evaluated data quality according to whether the two comparison statements actually were consistent and inconsistent forms to express the same comparison relationship. To be precise, the first problem needs to have a consistent relational statement for the comparison predicate, the second problem needs to have an equivalent inconsistent relational statement for the same comparison predicate, and the two problems need to be identical otherwise. Our pipeline achieved a 0% error rate on both criteria on the 100 evaluated example problems.\\n\\nTransfer vs comparison bias (\u00a75.3). We evaluated data quality according to whether the comparison and transfer problems had the same arithmetic expression and whether they followed the specified problem structure. We also ensured that the agent names and other properties matched. Our pipeline achieved a 0% error rate on both criteria on the 100 evaluated example problems.\\n\\nThe carry effect (\u00a75.4). We evaluated data quality according to whether one problem had no carry computation steps, the other one had at least one, and they were equal otherwise. That is, only the numbers differed and the two problems had the same answer. Our pipeline achieved a 0% error rate on both criteria on the 100 evaluated example problems.\\n\\nA.3. Related Methods\\n\\nOur generation pipeline differs from Opedal et al.'s (2023) method in that we generate intermediate templated texts, while they generate the problem texts directly conditioned on mental models. Polozov et al. (2015) also generates word problems from logical representations, but their approach does not allow explicit control over arithmetic concepts, which is an important factor underlying difficulty level; \u00a73.\\n\\nWhile our experiments require strict control over linguistic form, our error correction step could in principle be broadened to perform paraphrasing and theme rewriting (Koncel-Kedziorski et al., 2016a) as well.\"}"}
{"id": "k1JXxbpIY6", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\n**Table 4.** Accuracy, conditional average treatment effect (CATE), and statistical significance ($p$-value) on math word problems generated for the three tests detailed in \u00a75.2, \u00a75.3, and \u00a75.4. 'Co' denotes consistent, 'InCo' inconsistent, 'T' transfer, 'C' comparison, 'Ca' carry, and 'NCa' no carry conditions. The results presented are for the child-persona prompting strategy described in App. B.1. 'Chat' and 'Instr.' indicate the instruction-tuned versions of the models. CATE values are bolded when $p < 0.001$.\\n\\n**Figure 4.** CATE for transfer vs comparison (\u00a75.3) in the CoT-prompted case stratified by number of reasoning steps. 'Instruction-tuned' refers to the Chat and Instruct versions of LLaMA2 and Mistral/Mixtral, respectively. Base and instruction-tuned models display opposite relationships between length and bias strength.\\n\\n**B. Additional Results**\\n\\n**B.1. Child-Persona Prompting**\\n\\nTaking inspiration from claims that LLMs can act as agent models (Andreas, 2022), we also experimented with an additional prompt in which the LLM is instructed to impersonate a grade-school child. Specifically, we employ a modified version of the zero-shot chain-of-thought prompt, tailored to simulate a child's reasoning process. We prompt the model with the phrase \\\"Let's think step by step as a grade-school child would,\\\" replacing the standard CoT instruction. Following this, we apply the same decoding method used in traditional CoT. The results for this approach are reported for the open-source models (apart from LLaMA2 70B) in Table 4. While we notice larger consistency and transfer vs comparison effects for some models, we observe no substantial departure from the results achieved with conventional CoT prompting.\\n\\n**B.2. Bias Strength by Number of Reasoning Steps**\\n\\nIn Fig. 4 we show how the CATE of the transfer vs comparison bias varies with the number of reasoning steps in the problems for the CoT setting. Interestingly, we observe that the CATE sizes increase with the number of reasoning steps for the instruction-tuned models, whereas they decrease for the pretrained-only base models. We are unaware of literature on the relationship between human transfer vs comparison bias and the number of steps, so we can not make any claims about which of these patterns is more cognitively plausible.\\n\\nThe consistency-effect test does not exhibit such diverging trends for the CATEs. Fig. 3 illustrates how the strength of the measured biases change in relation to the number of reasoning steps in a problem (in the CoT-prompted case). Note that the carry effect experiments were carried it out for problems with only one step.\\n\\n**B.3. Data from Hegarty et al. (1995)**\\n\\nIn Table 5 we present the results on a few selected models when evaluated on the data from the study by Hegarty et al. (1995). Theirs was the only background study for which we were able to obtain the data that was used. The dataset contains eight problem pairs and targets the consistency bias. While we do not obtain significant results, we do get an indication of the absolute effect of the bias as compared to the human subjects in Hegarty et al.'s (1995) study (who were undergraduate college students). In their study, solvers who committed at least four errors out of the total of 16 problems had an average accuracy of 62% and 24%, on consistent problems and inconsistent problems respectively. None of the absolute accuracies in Table 5 are similar, but Mixtral 8x7B Instruct displays a similar absolute effect size.\"}"}
{"id": "k1JXxbpIY6", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\n| Model          | Direct Consistency | Mixtral 8x7B | Consistency | P-value |\\n|----------------|--------------------|--------------|-------------|---------|\\n| CoT            |                    | 37.5         | 12.5        | 0.35    |\\n| Mixtral 8x7B   |                    | 12.5         | 0.08        |         |\\n| GPT-3.5 Turbo  | 75                 | 62.5         | 0.35        |         |\\n\\nTable 5. Accuracy, conditional average treatment effect (CATE), and statistical significance (p-value) on word problems from Hegarty et al. (1995).\\n\\nC. Brief Discussion on Mental Model Building\\n\\nWe note that the presence of consistency bias could be viewed as an argument against the position that language models construct something akin to a mental model during problem-solving. Indeed, people who exhibit consistency bias seem to be more likely to construct a mental model of the problem compared to those who do not, based on eye-fixation behavior (Hegarty et al., 1995). Intuitively, a (human or non-human) solver that constructs a mental model should be able to be more robust against inconsistent phrasings, assuming that the text-comprehension step of the solving pipeline is not made significantly harder by inconsistent phrasings.\"}"}
{"id": "k1JXxbpIY6", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 2.\\n\\nAccuracy, conditional average treatment effect (CATE), and statistical significance (p-value) on math word problems generated for the three tests detailed in \u00a75.2, \u00a75.3 and \u00a75.4. 'Co' denotes consistent, 'InCo' inconsistent, 'T' transfer, 'C' comparison, 'Ca' carry, and 'NCa' no carry conditions. Results are separated by whether the prompting method is direct or chain-of-thought (CoT). 'Chat' and 'Inst.' indicate the instruction-tuned versions of the models. CATE values are bolded when significant, controlling for false discovery rate at level $\\\\alpha = 0.05$. Results from an additional prompting strategy, child-persona prompting, are presented in Table 4.\\n\\n| Model          | Direct | InCo | T | C | Ca | NCa |\\n|----------------|--------|------|---|---|----|-----|\\n| **Accuracy (%)** |        |      |  |   |    |     |\\n| **p-value**    |        |      |  |   |    |     |\\n| LLaMA2 7B      | 9.6    | 4.8  |  |   |    |     |\\n| LLaMA2 13B     | 17.2   | 14.0 |  |   |    |     |\\n| LLaMA2 70B     | 24.0   | 16.2 |  |   |    |     |\\n| Mistral 7B     | 17.8   | 12.0 |  |   |    |     |\\n| Mistral 8x7B   | 23.0   | 17.0 |  |   |    |     |\\n| LLaMA2 Chat    | 14.2   | 10.8 |  |   |    |     |\\n| LLaMA2 13B Chat| 16.4   | 11.8 |  |   |    |     |\\n| LLaMA2 70B Chat| 16.4   | 14.8 |  |   |    |     |\\n| Mistral 7B Inst.| 17.6  | 14.2 |  |   |    |     |\\n| Mistral 8x7B Instr.| 23.4 | 21.8 |  |   |    |     |\\n| GPT-3.5 Turbo  | 32.2   | 22.8 |  |   |    |     |\\n| GPT-4 Turbo    | 89.2   | 87.8 |  |   |    |     |\\n\\n### 5.2. Problem Text: Consistency Bias\\n\\nVarying the linguistic form of an otherwise equivalent problem structure can have a large effect on child performance (Cummins et al., 1988). We test whether comparison problems with inconsistent statements are harder for LLMs than the same problems with an analogous consistent statement.\\n\\n**Method.** Following Lewis and Mayer's (1987) study, we consider consistent/inconsistent problem pairs where the required operation is either addition, subtraction, multiplication or division. The generation pipeline is tuned so that the problem structures follow the specification: container $\\\\circ$ (transfer $|$ rate) $\\\\circ \\\\cdots \\\\circ$ (transfer $|$ rate) $|\\\\{z\\\\} 0$\u22122 times $\\\\circ$ comparison $\\\\circ$ (transfer $|$ rate) $\\\\circ \\\\cdots \\\\circ$ (transfer $|$ rate) $|\\\\{z\\\\} 0$\u22122 times;\\n\\nNote that the problems may have between 1\u22125 reasoning steps\u2014one for every non-container predicate. Apart from the first predicate (container), only the comparison predicate introduces a new agent. The question queries the agent that was introduced by comparison. The pairs are generated such that the only sentence that varies is that which corresponds to comparison, one being consistent and the other being its analogous inconsistent form.\\n\\n**Results.** The results of the consistency bias test reveal 20 out of 23 statistically significant CATEs. As displayed in Table 2 (leftmost column), all models exhibit lower accuracy when solving inconsistent problems compared to their consistent counterparts. Interestingly, the bias appears to be\"}"}
{"id": "k1JXxbpIY6", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nexacerbated by CoT prompting, which improves the overall model performance, but magnifies the difference in accuracy between consistent and inconsistent problems. This finding aligns with research indicating that CoT prompting may also amplify other types of biases present in the training data (Shaikh et al., 2023). Particularly notable CATEs are observed for the base versions of LLaMA2 7B, LLaMA2 13B, and Mistral 7B, for which the inconsistent formulation of the problems leads to an accuracy drop larger than 50%.\\n\\n5.3. Mental Model: Transfer vs Comparison Bias\\n\\nAnother factor behind performance is that it might be harder to perform solution planning based on some mental models compared to others. We test whether LLMs are better at solving transfer-type problems than comparison problems.\\n\\nMethod.\\n\\nThe problem structures take the following forms:\\n\\n\\\\[\\n\\\\text{container} \\\\circ \\\\text{transfer} \\\\cdots \\\\circ \\\\text{transfer} | \\\\{z\\\\} 1 - 5 \\\\text{times};\\n\\\\]\\n\\n\\\\[\\n\\\\text{container} \\\\circ \\\\text{comparison} \\\\cdots \\\\circ \\\\text{comparison} | \\\\{z\\\\} 1 - 5 \\\\text{times};\\n\\\\]\\n\\nThe two problems have identical arithmetic expressions. Each comparison predicate corresponds to a comparison of a new agent with the agent introduced in the preceding sentence. Each transfer statement follows the same agent, who was introduced in the first sentence and whose state is updated through a transfer with some other agent. The problems resemble each other in linguistic form as much as possible. In particular, we make sure that the same agent names are introduced in each sentence across the two problems, in order to account for variance in performance stemming from such choices (Goodarzi et al., 2023). Consistent or inconsistent forms of comparison are sampled uniformly at random.\\n\\nResults.\\n\\nThe experimental results (middle column in Table 2) show that models are consistently more accurate on problems based on transfers rather than comparisons. With the exception of CoT-prompted pretrained-only Mistral and Mixtral, we observe statistically significant positive CATEs, mirroring biases seen in children's problem-solving (\u00a73). Further, we note that the instruction-tuned models overall exhibit larger effects than pretrained-only models in the CoT setting, but not in the direct setting. This seems to be the case for the consistency bias as well. Finally, in App. B.2 we show some of the results as broken down by number of reasoning steps in the CoT setting.\\n\\n5.4. Arithmetic Expressions: The Carry Effect\\n\\nWhile much of a child's performance on word problems can be explained by properties introduced by the text format, a large portion still depends on the nature of the arithmetic expression (Daroczy et al., 2015). We test whether LLMs are sensitive to the presence of arithmetic carry when posed with addition and subtraction in math word problems.\\n\\nMethod.\\n\\nWe generate pairs under the comparison specification from \u00a75.3, but with only one step:\\n\\n\\\\[\\n\\\\text{9} \\\\text{container} \\\\circ \\\\text{comparison};\\n\\\\]\\n\\nAs in \u00a75.3, we use additive comparisons, which ensures that the arithmetic expressions only have addition and/or subtraction operators. The two problems of a pair are identical apart from the numbers. Following F\u00fcrst and Hitch (2000), we ensure that both operands as well as the answer of the problem are three-digit numbers (since children appear to rely on long-term memory for problems with small numbers; Footnote 4). One of the problems has no carry, the other has at least one (i.e., unit or tens carry). The correct answer of the two problems is controlled to be the same.\\n\\nResults.\\n\\nThe results (rightmost column in Table 2) depart from the findings above, which gave evidence for the presence of child-like biases. In this case, model performance is similar in problems with and without carry operations\u2014we only observe one significant result out of the 23 tests. Thus, the models seem not to be sensitive to variations isolated to the arithmetic expression level.\\n\\n6. Why do Language Models Exhibit Biases?\\n\\nA natural set of questions that arise from our results is why some child-like biases are present in the models, and why some of them (like the carry effect) are absent. The most plausible explanation in our view is the influence from the training data: If the training data contains many examples of humans writing and solving word problems, then it may be that LLMs simulate human biases present in such text. For instance, it may be that the distribution of the training data is skewed towards consistent problem formulations of comparison relations, which in turn could be a product of consistency bias in the humans who wrote the word problems. This would seem plausible given that the consistency bias is present also in adults (Lewis and Mayer, 1987; Hegarty et al., 1995).\\n\\nUnfortunately, we cannot directly verify this hypothesis since it is unknown what data has been used to train the models considered in this study. However, as a proxy, we performed an analysis of a set consisting of word problems from MAWPS (Koncel-Kedziorski et al., 2016b), ASDIV-A. The one-step case follows the setups from studies on humans (\u00a73). We discovered in the previous two tests that the models frequently fail on comparison problems with only one step (which can be inferred from Figs. 3 and 4), so if a carry effect is present, it should be observable in such a setting.\"}"}
{"id": "k1JXxbpIY6", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners? (Miao et al., 2020), and SV AMP (Patel et al., 2021). These three datasets are well known and publicly available, and are thus likely to have been present in the training data of the LLMs used in this work. We found that these datasets indeed have more consistent formulations than inconsistent ones, and more transfer problems than comparison problems. More specifically, the ratios observed were 5:1 (15 and 3 in absolute numbers) and 130:9 (260 and 18 in absolute numbers), respectively.\\n\\nIn other words, the imbalance in problem types on these datasets is consistent with the biases we found in our analyses on text comprehension (\u00a75.2) and solution planning (\u00a75.3). Extrapolating this hypothesis to the absence of a carry effect would imply that there is little to no difference in frequency between problems with and without carry in the training data. This would be harder to verify, as there are many potential traces of carry in the data beyond word problems. Furthermore, the carry effect results suggest that there are algorithmic differences in how LLMs and humans perform arithmetic computations. In particular, the carry effect in humans is partially attributed to working memory limitations (Hitch, 1978), which LLMs may not implement in the same manner. The memory and computational mechanisms through which models perform arithmetic (Nanda et al., 2023; Stolfo et al., 2023a; Quirke and Barez, 2024) are likely not affected by the increased cognitive load that the carry operation introduces in humans. This leads to an alternative, albeit arguably less plausible view, on why we observe the other two biases: It might be that there is algorithmic similarity between humans and language models on text comprehension and solution planning. Assessing this hypothesis would require knowledge about the mechanisms of human and language-model reasoning alike, both of which are beyond the scope of the present study. However, our results at least suggest a direction, namely, that there is at least the possibility that the algorithms in text comprehension and solution planning exhibit similarity.\\n\\nRelated Work\\n\\nOur work relates most closely to studies that have compared human and LLM biases on syllogisms (Ando et al., 2023; Eisape et al., 2024), code generation (Jones and Steinhardt, 2022), and other non-mathematical inference tasks (Dasgupta et al., 2023). Their findings indicate that LLMs are susceptible to some of the same biases as humans, like consistency bias and transfer vs comparison bias, but not for the carry effect which relates to the step of the cognitive process that involves solving arithmetic equations. Our study also differs from those referenced above in that we systematically compare the effect of CoT prompting to direct prompting, observing amplified effects in the CoT setting in most cases where effects are present.\\n\\nWe are unaware of any other work that studies cognitive biases in LLMs that, like the carry effect, relate directly to numbers. However, there seem to be similarities between the numeric representations in LLMs and the \\\"mental number line\\\" in humans (Shah et al., 2023). Other studies find evidence that LLMs to some extent rely on spurious correlations in numerical reasoning (Razeghi et al., 2022; Stolfo et al., 2023b), and that their performance decreases with increasing number size (Dziri et al., 2023; Shen et al., 2023). Beyond numerical reasoning, LLMs appear to have difficulties with causal reasoning (Binz and Schulz, 2023; Jin et al., 2023; 2024) and proof planning (Saparov and He, 2023).\\n\\nConclusion and Implications\\n\\nThis study explored whether LLMs exhibit child-like cognitive biases in arithmetic word problem-solving. We found that LLMs demonstrate biases in text comprehension and solution planning that mirror human tendencies. Specifically, models performed better on problems where the relational keyword is consistent with the appropriate arithmetic operator compared to problems where it is not, as well as on problems with a dynamic change of state compared to problems with a static comparison. However, at the solution execution step, LLMs did not exhibit the child-like carry effect for arithmetic computations. In general, studying biases that are present in children but not in adults may enable the disentanglement of the influence of training data from other factors that might explain language model behavior, since one would expect the training set to be heavily biased towards adult (rather than child) thinking. We therefore believe it might be a promising direction forward in language model interpretability work.\\n\\nImpact Statement\\n\\nCognitive modeling enables human simulations in place of data collection that might otherwise be unethical, harmful or costly. On the other hand, issues could arise if those simulations are unfaithful to human behavior. As a broader implication of our work, we encourage practitioners to exercise care when developing and deploying cognitive models of students using LLMs, particularly, in how the student\\n\"}"}
{"id": "k1JXxbpIY6", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nThe model treats numbers and other properties of arithmetic expressions. We hope that our results can provide insights for practitioners seeking to develop automated learning agents, for instance, under the tutor-learning paradigm in which a student learns by correcting the mistakes made by a computer model (Okita, 2014; Shahriar and Matsuda, 2021; Schmucker et al., 2023). We do not see any notable ethical issues with our work.\\n\\nLimitations\\n\\nWe cannot draw any parallels on the absolute performance in comparison with children, only on the presence or absence of each effect. This is because the datasets used in the learning science studies were not available to us. The one exception was the data from Hegarty et al. (1995), which we evaluate on in App. B.3. Moreover, we do not consider the grade level of the problems, but see Jiao et al. (2023) for a generation method that does.\\n\\nIn selecting specific cognitive biases to study, we chose bases that are well-established in literature on human children and whose effects could be clearly associated with one of the steps of Fig. 1. Another factor that fulfills these desiderata is the effect of explicit verbal cues (Hudson, 1983; Vicente et al., 2007). More fundamentally, a complete comparison of the biases between LLMs and humans would need to study biases that have been found in LLMs but are not necessarily present in humans. We do not take that direction into account, but we note that the number frequency effect reported by Razeghi et al. (2022) might be one such example.\\n\\nWe did not use in-context examples in our evaluation since the addition of such may influence the results in ways that can be difficult to foresee. However, an interesting direction for future work could be to study whether cognitive biases can be controlled through specific choices of in-context examples or other prompts.\\n\\nWe stress that the conceptualization in Fig. 1 is a simplified model of the solving process. For instance, it fails to account for shortcut strategies (see Footnote 3) and it does not consider any propositional text-base representation which precedes the mental model representation in some other models (Kintsch and Greeno, 1985; Hegarty et al., 1995). We do not make any claims on the ability of LLMs to \\\"construct mental models\\\" in this work, although our results could potentially have such implications as was pointed out by a reviewer. See App. C for a brief discussion.\\n\\nFinally and importantly, we only consider problems formulated in English. We note that some effects could vary across languages. For instance, the carry effect is more pronounced in German and other languages where the spelled-out order of tens and units is inverted in relation to Arabic numerical notation (G\u00f6bel et al., 2014). Our generation pipeline can be straightforwardly adapted to other languages, and future work might consider doing so.\\n\\nAcknowledgements\\n\\nWe thank Emo Welzl, Ethan Gotlieb Wilcox, Julia Chatain and Yilmazcan Ozyurt for valuable feedback and discussions. Andreas Opedal acknowledges funding from the Max Planck ETH Center for Learning Systems. Alessandro Stolfo is supported by armasuisse Science and Technology through a CYD Doctoral Fellowship.\\n\\nReferences\\n\\nGati Aher, Rosa I. Arriaga, and Adam Tauman Kalai. 2023. Using large language models to simulate multiple humans and replicate human subject studies. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org.\\n\\nRisako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mi- neshima, and Mitsuhiro Okada. 2023. Evaluating large language models with NeuBAROCO: Syllogistic reasoning ability and human-like biases. In Proceedings of the 4th Natural Logic Meets Machine Learning Workshop, pages 1\u201311, Nancy, France. Association for Computational Linguistics.\\n\\nJacob Andreas. 2022. Language models as agent models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5769\u20135779, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nLisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R. Gubler, Christopher Rytting, and David Wingate. 2023. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3):337\u2013351.\\n\\nMark H. Ashcraft, Rick D. Donley, Margaret A. Halas, and Mary Vakali. 1992. Chapter 8 working memory, automaticity, and problem difficulty. In Jamie I. D. Campbell, editor, The Nature and Origins of Mathematical Skills, volume 91 of Advances in Psychology, pages 301\u2013329. North-Holland.\\n\\nYoav Benjamini and Yosef Hochberg. 1995. Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing. Journal of the Royal Statistical Society: Series B (Methodological), 57(1):289\u2013300.\\n\\nMarcel Binz and Eric Schulz. 2023. Using cognitive psychology to understand GPT-3. Proceedings of the National Academy of Sciences, 120(6).\"}"}
{"id": "k1JXxbpIY6", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nhave drastic effects on performance (Hudson, 1983; Lewis and Mayer, 1987; inter alia) in both children and adults. This part of the process is encompassed by the models of Briars and Larkin (1984) and Kintsch and Greeno (1985). None of these models give an explicit account of the complexity of the arithmetic expression, however, which also has significant influence on performance (Daroczy et al., 2015).\\n\\n3. Human Biases in Word Problems\\n\\nIn this section we discuss the particular factors that influence performance of human children (i.e., cognitive biases) which we study in LLMs (\u00a75). Each bias is reflected by a variation in a specific level of Fig. 1. We study one bias for each of the three levels that precedes the answer.\\n\\nProblem text: Consistency bias.\\n\\nGiven the premise \u201cAlice has 5 apples\u201d and a question querying the (smaller) number of apples of another agent \u201cBob\u201d, an additive comparison statement between the two agents could take either of the following forms:\\n\\n1. Bob has 3 fewer apples than Alice.\\n2. Alice has 3 more apples than Bob.\\n\\nHere, (1) represents a consistent statement because the relational keyword (\u201cfewer\u201d) suggests the operation that is indeed needed to compute Bob\u2019s quantity (subtraction). Conversely, (2) is an inconsistent statement because the relational keyword (\u201cmore\u201d) suggests a different operation (\u201caddition\u201d). Note that these two statements express the same comparison relationship, so the difference lies only in the problem text. Problems with an inconsistent statement are more difficult for children to solve than consistent ones (Nesher and Teubal, 1975; Stern, 1993). This has been hypothesized to be the case due to inconsistent statements requiring an additional, error-prone, deduction step: to rearrange the relational statement to be in the preferred consistent form (Lewis and Mayer, 1987).\\n\\nMental model: Transfer vs comparison bias.\\n\\nIrrespective of which relational keyword is used, comparison-type problems tend to be more difficult for children than other types of arithmetic concepts (Riley et al., 1983). In particular, grade school children display a significant difference in performance between comparison problems and transfer problems. Consider the same premise as above but with a slightly different continuation:\\n\\nAlice has 5 apples. Alice gave 3 apples to Bob. How many apples does Alice have?\\n\\nThis is a transfer problem (often called a change elsewhere; Nesher et al., 1982), concerning a change of state of some quantity. It has the same arithmetic expression as the comparison problems above (although with another mental model), but is easier for young children to solve. In analyzing their solution strategies, it has been found that comparison problems require a number-matching type strategy that appears to be more sophisticated than the counting-type strategies that are often sufficient for solving transfer problems (Riley et al., 1983; Carpenter and Moser, 1984).\\n\\nArithmetic expressions: The carry effect.\\n\\nBeyond the text and mental model, it is natural that the particular numbers used in a problem will have an effect on a child\u2019s performance (Daroczy et al., 2015). Consider the same problem(s) as above, but with a different number given in the premise:\\n\\nAlice has 22 apples. Bob has 3 fewer apples than Alice. How many apples does Bob have?\\n\\nThe problem now has the arithmetic expression $22 - 3$, which involves an arithmetic carry, which is also called a borrowing in the case of subtraction. A carry is a digit that is transferred from one column of digits to another as the result of an arithmetic computation. In this subtraction computation, a unit carry of 1 is transferred from the column of units to the column of tens in order to make the answer 19.\\n\\nThe previous expression ($5 - 3$) did not have a carry, which is easier for children (Hitch, 1978; Ashcraft et al., 1992). The presence of a carry introduces an additional subroutine from the standard sequence of operations, which places a higher load on working memory (F\u00fcrst and Hitch, 2000).\\n\\n4. Problem Generation Method\\n\\nOur experiments on LLMs with respect to the biases just discussed (\u00a75) rely on data generated for the sole purpose of our study. By not using problems from public datasets, previous work or other existing sources it becomes unlikely that our data has been used for training of the models, an increasingly common issue (Dodge et al., 2021; Elazar et al., 2023). This section gives the details of our data generation pipeline, which provides control over features across all levels of Fig. 1. In \u00a74.1, we operationalize Fig. 1, giving definitions related to the mental model level and other aspects of the process. Using these definitions, we then explain our generation pipeline in \u00a74.2.\"}"}
{"id": "k1JXxbpIY6", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nMATHWORLD annotates each math word problem with a logical representation, which captures the mathematical relationships between the entities described in text. Each entity has a non-negative integer quantity. Optionally, there may be additional information associated with an entity\u2014namely, an agent who possesses the entity, and a unit and an attribute which enrich the description of it. The five bold items in the two preceding sentences are referred to as properties. The arithmetic relationships are classified according to concepts; we use transfer, comparison (additive and multiplicative), and rate in this work. We gave examples of the transfer and comparison concepts in \u00a73.\\n\\nWhen discussing data generation (\u00a74.2) and the experiments the data is used for (\u00a75), we will rely on the following definitions: A predicate is a symbol that represents either an arithmetic concept, or possession of an entity (in that case the predicate is \\\"container\\\"). Each predicate takes a set of properties as arguments. A predicate instantiated with properties is called a logical form, and represents the semantics of a given sentence in a problem. See Table 1 for examples of logical forms for all predicates we use.\\n\\nThe mental model of a problem is a sequence of logical forms (separated by a \\\"\u25e6\\\" symbol) for each sentence in that problem (in the same order), representing its semantics. In Fig. 1, we gave a mental model example in graphical format; its equivalent sequential format is container(Alice, 5, apple)\u25e6comparison(+, Alice, Bob, 3, apple). The problem text in Fig. 1 is faithful to this mental model, in the sense that the mental model represents the semantics of that text under the MATHWORLD formalism. We refer to the structure of a problem as a mental model in which the property values are replaced by unique placeholders. The structure associated with the previous example is container([agent1], [n1], [entity1])\u25e6comparison(+, [agent1], [agent2], [n2], [entity1]).\\n\\nFinally, we formalize the arithmetic expression level of Fig. 1. Every concept-based predicate corresponds to an equation \\\\( x = y \\\\circ z \\\\), with \\\\( \\\\circ \\\\in \\\\{+, -, \\\\times, \\\\div\\\\} \\\\) and \\\\( x, y, z \\\\in \\\\mathbb{Z} \\\\geq 0 \\\\cup V \\\\) where \\\\( V \\\\) is a set of variable symbols. We require that \\\\( \\\\exists! v \\\\in \\\\{x, y, z\\\\}: v \\\\in V \\\\). We refer to the deductive inference step taken to solve that equation as a reasoning step, and its output (i.e., the value of the variable) as an intermediate result. The arithmetic expression level consists of a sequence of such reasoning steps, which is a proof of the answer of the problem (or solution). Any fact from the mental model is an axiom that can be used in the solution proof. This work focuses exclusively on linear problems, in which every reasoning step has at most one non-axiom premise.\\n\\nWe enforce all quantities that are associated with predicates to be explicit numbers. Note that this places a constraint on the format of the problems: The number associated with a mathematical relationship is never an intermediate result, but is always given in text.\"}"}
{"id": "k1JXxbpIY6", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nEdward has 2 watchs. Annie has 2 times the amount of watchs compared to the amount Edward has. Annie bought 3 watchs. Annie sold 4 watchs. Maggie owns 2 boxs. Every box that Maggie has contains 4 watchs. Maggie has 7 more watchs than Annie. How many watchs does Annie have?\\n\\nFigure 2. Overview of our generation pipeline with an example problem. (1) We start by generating the problem structure. The alignment between the graphical and sequential formats of the structure is illustrated by color coding, and the black containment boxes in the graph represent intermediate results. (2) The properties of those structures are then instantiated with values, resulting in a mental model. (3) Next, we sample a templated sentence for each of the logical forms in the mental model, and concatenate them in the ordering of the logical forms. (4) Finally, we correct errors and awkward phrasings by prompting an instruction-tuned LLM (the corrections are highlighted in blue). This particular example includes all predicate types that we use in \u00a75.\\n\\nWe generate a problem as follows: First, we sample the number of reasoning steps \\\\( n \\\\) uniformly at random from the set \\\\( \\\\{1, \\\\ldots, N\\\\} \\\\). Next, we sample the predicates; each choice is sampled uniformly at random. Since this first step of the pipeline generates structures, the predicates all have associated unique placeholders in place of properties, e.g., agent2, entity1. We only introduce new entity placeholders in rate logical forms; see Table 1, in which rate is the only predicate that takes two entity properties. We determine uniformly at random whether an entity is paired with an attribute, a unit, or neither. The instantiation of agent placeholders is test specific. Finally, the answer of a problem is always set as the intermediate result corresponding to the last logical form in the ordering, which is unique. See the box on the left-hand side of Fig. 2 for an example structure generated after this step.\\n\\n(ii) Problem structure instantiation. Next, the problem structure is instantiated with properties, yielding a mental model. We use a handwritten vocabulary for each of the lexical properties (entity, agent, unit and attribute) and sample instantiations of these properties from those vocabularies uniformly at random. The numerical quantities are instantiated by sampling a set of numbers uniformly at random from within a fixed range, which is 2\u201320 for the experiments in \u00a75.2-5.3 and 100\u2013999 for the experiments in \u00a75.4. Then, we enumerate the logical forms and accordingly compute intermediate results for each reasoning step, making sure that the intermediate quantities fall in the range 0\u2013999. If not, a new set of numbers is sampled and the procedure is repeated from the beginning. This naive procedure is sufficiently fast for our purposes. Empirically, we observed an average runtime of \\\\( \\\\approx 4 \\\\) ms to generate a numerical instantiation for a problem.\\n\\n(iii) Template sampling. The mental model is then converted to natural language using templated text. Specifically, for each of the predicates we construct a set of templates that represent natural language adhering to that predicate. For instance, \\\\( \\\\text{transfer}(\\\\text{Annie}, \\\\text{None}, 3, \\\\text{watch}) \\\\) is converted to \u201c[Annie] bought [3] [watch]s\u201d in Fig. 2; see Table 1 for additional examples. The templates are handcrafted. We sample one template uniformly at random for each predicate in the mental model. We also create\"}"}
{"id": "k1JXxbpIY6", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nand sample interrogative templates for the questions, which always query the intermediate result derived from the last predicate. Finally, we concatenate the sentences to obtain the full problem text. This step enables control over the linguistic form of the sentences in the problem text, which will be important for our test related to text comprehension in \u00a75.2. Moreover, since the faithfulness of the templated text is guaranteed by manual design, the procedure up to this point ensures that the text is faithful to the mental model.\\n\\n(iv) Linguistic error correction.\\n\\nHowever, templated texts occasionally induce spelling mistakes and awkward phrasings. In the example shown in Fig. 2, the entity \\\"watch\\\" is inserted into the template to make \\\"watchs\\\". Inspired by the demonstrated success of zero-shot grammatical error correction (Kwon et al., 2023; Loem et al., 2023), we use an instruction-tuned language model (Ouyang et al., 2022), GPT-3.5 Turbo, to correct such errors. We write a short instructive prompt and have the model generate (with greedy decoding) a corrected problem text conditioned on that prompt together with the particular templated text we want to correct. The prompt instructs the model to be conservative, i.e., to only correct linguistic errors and awkward phrasings. We provide the exact prompt used and additional generation details in App. A.1. This step could, in principle, be generalized to perform less strict forms of paraphrasing. There is then a trade-off between faithfulness and control on the one hand and linguistic variability and naturalness on the other, which can be tuned using different prompts and decoding methods. The present study prioritizes control and faithfulness, but alternative prioritizations could be used in future studies that employ our method.\\n\\nEvaluating data quality.\\n\\nThe generated problem texts must be faithful to the mental models from which they were generated, so we perform manual evaluation of the data to assess that such is the case. We follow a generic procedure and perform it for each of the datasets generated for the experiments in \u00a75.2-5.4. The procedure is iterated until we achieve satisfactory quality. The final error-rate estimates were 0% for all three datasets. Details are given in App. A.2.\\n\\n5. Experiments\\n\\nWe generate data to perform tests on whether LLMs exhibit child-like biases using the pipeline discussed above. We aim to identify where in the process in Fig. 1 those biases emerge. We therefore split our tests according to the level (and associated skill) they target: problem text and text comprehension (\u00a75.2), mental model and solution planning (\u00a75.3), and arithmetic expressions and solution execution (\u00a75.4). First, we discuss the general experimental setup (\u00a75.1).\\n\\n5.1. Experimental Setup\\n\\nWe base our experiments on the problem features discussed in \u00a73 that have been found to have an effect on child performance in solving word problems. Specifically, given such a feature $X$, we want to know whether $X$ has a causal effect on the performance of LLMs. Our generation pipeline enables exact matching of the data: We generate problems in pairs, where the two problems differ only in the value of $X$. Using this data, we estimate the conditional average treatment effect (CATE; Imbens and Rubin, 2015)\\n\\n$$E[Y(x) - Y(x')]|Z]$$\\n\\nwhere $Y$ is 1 if the model's prediction is correct and 0 otherwise, $x$ and $x'$ are two distinct values of the treatment variable $X$, and $Z$ is the subgroup of data generated through our pipeline for a specific test. The two values $x$ and $x'$ are defined such that positive CATEs are consistent with human biases. We refer to Feder et al. (2022) for further reading on causality-based methods for NLP.\\n\\nFor each of the tests described below we select a specific feature $X$ that is localized to one of the levels, to the extent possible. That is, varying $X$ associated with a particular level should have no effect on the levels above, and minimal effect on the levels below. For instance, in \u00a75.2 we vary the problem text without affecting the mental model, arithmetic expression or answer.\\n\\nHaving selected $X$, we adapt the pipeline (\u00a74.2) to generate example pairs, one with $X = x$ and one with $X = x'$. Next, we evaluate the data quality using the procedure described in App. A.2. After quality assurance, we generate a larger sample of 400 additional problem pairs, which (including the quality evaluation set) gives a total of 500 pairs for the tests. We then generate outcomes $Y(x)$ and $Y(x')$ for each of the pairs for a set of selected LLMs. We use LLaMA2 (Touvron et al., 2023) with 7B, 13B and 70B parameters, Mistral 7B (Jiang et al., 2023) and Mixtral 8x7B (Jiang et al., 2024), GPT-3.5 Turbo, and GPT-4 Turbo (OpenAI, 2024). We consider both the pretrained-only and instruction-tuned versions for the LLaMA2, Mistral and Mixtral models. We carry out zero-shot inference with a standard prompt, a chain-of-thought prompt (CoT; Wei et al., 2022), as well as a modified \\\"child-persona\\\" CoT prompt, whose results were similar to those of the CoT setup and are presented in App. B.1. With the former, the models are prompted to directly provide an answer after the input. Following previous work (Kojima et al., 2022; Yang et al., 2024), we use the format \\\"Q: {problem}\\nA: The answer (Arabic numerals) is\\\" for base models and \\\"{problem}\\nThe answer (Arabic numerals)\\\".\"}"}
{"id": "k1JXxbpIY6", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nDiane J. Briars and Jill H. Larkin. 1984. An integrated model of skill in solving elementary word problems. *Cognition and Instruction*, 1(3):245\u2013296.\\n\\nThomas P. Carpenter and James M. Moser. 1984. The acquisition of addition and subtraction concepts in grades one through three. *Journal for Research in Mathematics Education*, 15(3):179\u2013202.\\n\\nDenise Dellarosa Cummins, Walter Kintsch, Kurt Reusser, and Rhonda Weimer. 1988. The role of understanding in solving word problems. *Cognitive Psychology*, 20(4):405\u2013438.\\n\\nGabriella Daroczy, Magdalena Wolska, Walt Detmar Meurers, and Hans-Christoph Nuerk. 2015. Word problems: a review of linguistic and numerical factors contributing to their difficulty. *Frontiers in Psychology*, 6.\\n\\nIshita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Hannah R. Sheahan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, and Felix Hill. 2023. Language models show human-like content effects on reasoning tasks.\\n\\nJesse Dodge, Maarten Sap, Ana Marasovi\u0107, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages 1286\u20131305, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. 2023. Faith and fate: Limits of transformers on compositionality. In *Thirty-seventh Conference on Neural Information Processing Systems*.\\n\\nTiwalayo Eisape, MH Tessler, Ishita Dasgupta, Fei Sha, Sjoerd van Steenkiste, and Tal Linzen. 2024. A systematic comparison of syllogistic reasoning in humans and language models. In *Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, Mexico City, Mexico.\\n\\nYanai Elazar, Akshita Bhagia, Ian Helgi Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Evan Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, and Jesse Dodge. 2023. What's in my big data? In *The Twelfth International Conference on Learning Representations*.\\n\\nAmir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E. Roberts, Brandon M. Stewart, Victor Veitch, and Diyi Yang. 2022. Causal inference in natural language processing: Estimation, prediction, interpretation and beyond. *Transactions of the Association for Computational Linguistics*, 10:1138\u20131158.\\n\\nAnsgar J. F\u00fcrst and Graham J. Hitch. 2000. Separate roles for executive and phonological components of working memory in mental arithmetic. *Memory & Cognition*, 28(5):774\u2013782.\\n\\nSaeed Goodarzi, Nikhil Kagita, Dennis Minn, Shufan Wang, Roberto Dessi, Shubham Toshniwal, Adina Williams, Jack Lanchantin, and Koustuv Sinha. 2023. Robustness of named-entity replacements for in-context learning. In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages 10914\u201310931, Singapore. Association for Computational Linguistics.\\n\\nSilke M. G\u00f6bel, Korbinian Moeller, Silvia Pixner, Liane Kaufmann, and Hans-Christoph Nuerk. 2014. Language affects symbolic arithmetic in children: The case of number word inversion. *Journal of Experimental Psychology*, 119:17\u201325.\\n\\nMary Hegarty, Richard E. Mayer, and Christopher A. Monk. 1995. Comprehension of arithmetic word problems: A comparison of successful and unsuccessful problem solvers. *Journal of Educational Psychology*, 87:18\u201332.\\n\\nGraham J. Hitch. 1978. The role of short-term working memory in mental arithmetic. *Cognitive Psychology*, 10(3):302\u2013323.\\n\\nTom Hudson. 1983. Correspondences and numerical differences between disjoint sets. *Child Development*, 54:84\u201390.\\n\\nGuido W. Imbens and Donald B. Rubin. 2015. *Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction*. Cambridge University Press, USA.\\n\\nJoshua Benjamin Jaffe and Donald Joseph Bolger. 2023. Cognitive processes, linguistic factors, and arithmetic word problem success: a review of behavioral studies. *Educational Psychology Review*, 35(4):105.\\n\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b.\"}"}
{"id": "k1JXxbpIY6", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2024. Mixtral of experts.\\n\\nYing Jiao, Kumar Shridhar, Peng Cui, Wangchunshu Zhou, and Mrinmaya Sachan. 2023. Automatic educational question generation with difficulty level controls. In International Conference on Artificial Intelligence in Education, pages 476\u2013488. Springer.\\n\\nZhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, and Bernhard Sch\u00f6lkopf. 2023. CLadder: A benchmark to assess causal reasoning capabilities of language models. In Thirty-seventh Conference on Neural Information Processing Systems.\\n\\nZhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona Diab, and Bernhard Sch\u00f6lkopf. 2024. Can large language models infer causation from correlation? In The Twelfth International Conference on Learning Representations.\\n\\nPhilip Nicholas Johnson-Laird. 1983. Mental models: towards a cognitive science of language, inference and consciousness, volume 6 of Cognitive Science Series. Harvard University Press, Cambridge, Massachusetts.\\n\\nErik Jones and Jacob Steinhardt. 2022. Capturing failures of large language models via human cognitive biases. In Advances in Neural Information Processing Systems.\\n\\nWalter Kintsch and James G. Greeno. 1985. Understanding and solving word arithmetic problems. Psychological Review, 92(1):109\u2013129.\\n\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In ICML 2022 Workshop on Knowledge Retrieval and Language Models.\\n\\nRik Koncel-Kedziorski, Ioannis Konstas, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2016a. A theme-rewriting approach for generating algebra word problems. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1617\u20131628, Austin, Texas. Association for Computational Linguistics.\\n\\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016b. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152\u20131157, San Diego, California. Association for Computational Linguistics.\\n\\nJohn W. Koshmider and Mark H. Ashcraft. 1991. The development of children\u2019s mental multiplication skills. Journal of Experimental Child Psychology, 51(1):53\u201389.\\n\\nSang Kwon, Gagan Bhatia, El Moatez Billah Nagoudi, and Muhammad Abdul-Mageed. 2023. Beyond English: Evaluating LLMs for Arabic grammatical error correction. In Proceedings of ArabicNLP 2023, pages 101\u2013119, Singapore (Hybrid). Association for Computational Linguistics.\\n\\nTanja K\u00e4ser and Giora Alexandron. 2023. Simulated learners in educational technology: A systematic literature review and a turing-like test. International Journal Of Artificial Intelligence In Education.\\n\\nAnne Bovenmyer Lewis and Richard E. Mayer. 1987. Students' miscomprehension of relational statements in arithmetic word problems. Journal of Educational Psychology, 79:363\u2013371.\\n\\nMengsay Loem, Masahiro Kaneko, Sho Takase, and Naoaki Okazaki. 2023. Exploring effectiveness of GPT-3 in grammatical error correction: A study on performance and controllability in prompt-based methods. In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 205\u2013219, Toronto, Canada. Association for Computational Linguistics.\\n\\nJakub Macina, Nico Daheim, Sankalan Chowdhury, Tanmay Sinha, Manu Kapur, Iryna Gurevych, and Mrinmaya Sachan. 2023. MathDial: A dialogue tutoring dataset with rich pedagogical properties grounded in math reasoning problems. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5602\u20135621, Singapore. Association for Computational Linguistics.\\n\\nKyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. 2023. Dissociating language and thought in large language models: a cognitive perspective.\\n\\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975\u2013984, Online. Association for Computational Linguistics.\"}"}
{"id": "k1JXxbpIY6", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nNeel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. 2023. Progress measures for grokking via mechanistic interpretability. In The Eleventh International Conference on Learning Representations.\\n\\nP. Nesher, James G. Greeno, and Mary S. Riley. 1982. The development of semantic categories for addition and subtraction. Educational Studies in Mathematics, 13:373\u2013394.\\n\\nPerla Nesher and Eva Teubal. 1975. Verbal cues as an interfering factor in verbal problem solving. Educational Studies in Mathematics, 6(1):41\u201351.\\n\\nManh Hung Nguyen, Sebastian Tschiatschek, and Adish Singla. 2023. Large language models for in-context student modeling: Synthesizing student's behavior in visual programming from one-shot observation.\\n\\nSandra Y. Okita. 2014. Learning from the folly of others: Learning to self-correct by monitoring the reasoning of virtual characters in a computer-supported mathematics learning environment. Computers Education, 71:257\u2013278.\\n\\nAndreas Opedal, Niklas Stoehr, Abulhair Saparov, and Mrinmaya Sachan. 2023. World models for math story problems. In Findings of the Association for Computational Linguistics: ACL 2023, pages 9088\u20139115, Toronto, Canada. Association for Computational Linguistics.\\n\\nOleksandr Polozov, Eleanor O'Rourke, Adam M. Smith, Luke Zettlemoyer, Sumit Gulwani, and Zoran Popovic. 2015. Personalized mathematical word problem generation. In IJCAI.\\n\\nPhilip Quirke and Fazl Barez. 2024. Understanding addition in transformers. In The Twelfth International Conference on Learning Representations.\\n\\nYasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot numerical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 840\u2013854, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nMary Riley, James Greeno, and Joan Heller. 1983. Development of Children's Problem-Solving Ability in Arithmetic, page 153\u2013196. Learning Research and Development Center, University of Pittsburgh.\\n\\nAbulhair Saparov and He He. 2023. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. In International Conference on Learning Representations.\\n\\nRobin Schmucker, Meng Xia, Amos Azaria, and Tom Mitchell. 2023. Ruffle&Riley: Towards the automated induction of conversational tutoring systems.\\n\\nRaj Shah, Vijay Marupudi, Reba Koenen, Khushi Bhardwaj, and Sashank Varma. 2023. Numeric magnitude comparison effects in large language models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 6147\u20136161, Toronto, Canada. Association for Computational Linguistics.\\n\\nTasmia Shahriar and Noboru Matsuda. 2021. \u201cCan you clarify what you said?\u201d Studying the impact of tutee agents\u2019 follow-up questions on tutors\u2019 learning. In Artificial Intelligence in Education, pages 395\u2013407, Cham. Springer International Publishing.\\n\\nOmar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. 2023. On second thought, let\u2019s not think step by step! bias and toxicity in zero-shot reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4454\u20134470, Toronto, Canada. Association for Computational Linguistics.\\n\\nRuoqi Shen, S\u00e9bastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, and Yi Zhang. 2023. Positional description matters for transformers arithmetic.\\n\\nElsbeth Stern. 1993. What makes certain arithmetic word problems involving the comparison of sets so difficult for children? Journal of Educational Psychology, 85:7\u201323.\\n\\nAlessandro Stolfo, Yonatan Belinkov, and Mrinmaya Sachan. 2023a. A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. In Proceedings of the 2023 Conference.\"}"}
{"id": "k1JXxbpIY6", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?\\n\\nAlessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schoelkopf, and Mrinmaya Sachan. 2023b. A causal framework to quantify the robustness of mathematical reasoning with language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 545\u2013561, Toronto, Canada. Association for Computational Linguistics.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Kornev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yunling Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models.\\n\\nKurt VanLehn, Stellan Ohlsson, and Rod Nason. 1994. Applications of simulated students: An exploration. International Journal of Artificial Intelligence in Education, 5.\\n\\nSantiago. Vicente, Jose. Orrantia, and Lieven. Verschaffel. 2007. Influence of situational and conceptual rewording on word problem solving. British Journal of Educational Psychology, 77(4):829\u2013848.\\n\\nAlex Warstadt and Samuel R Bowman. 2022. What artificial neural networks can tell us about human language acquisition. In Algebraic Structures in Natural Language, pages 17\u201360. CRC Press.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.\\n\\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. 2024. Large language models as optimizers. In The Twelfth International Conference on Learning Representations.\"}"}
