{"id": "PjiRSyUt7e", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Dataset Details\\n\\nA.1. Visual Scenarios Description\\nIn this section, we outline the constituent elements that make up each visual scenario, as illustrated in Table 6.\\n\\n| Category        | Description                                                                 |\\n|-----------------|-----------------------------------------------------------------------------|\\n| Shopping        | Purchasing groceries, clothes, furniture, gadgets, cosmetics, services, and miscellaneous products. |\\n| Navigation      | Different modes of transportation - passenger vehicles, trucks, buses, trains, airplanes, and navigation signage - streets, roadways, bus stations, train stations, and airports. |\\n| Time            | Items showcasing time and dates, including analog clocks, digital clocks, multi-clock setups, calendars, and other miscellaneous time-viewing setups. |\\n| Web Usage       | Websites across a variety of domains, like new articles, blogs, sports, and e-commerce. |\\n| App Usage       | Smartphone applications on education, productivity, games, lifestyle, entertainment, news, etc. |\\n| Infographic     | Infographics on local and global information spanning domains of health, sports, education, natural resources, technology, etc. |\\n| Abstract        | Memes, comic strips, and other abstract concepts illustrated through text-rich images. |\\n| Miscellaneous   | Miscellaneous human interactions that do not fall into the previous categories. |\\n\\nTable 6. Descriptions of the eight visual scenarios in CONTEXTUAL.\\n\\nA.2. Visual Scenarios Examples\\nIn this section, we provide examples of each visual category in CONTEXTUAL.\\n\\n(a) Single Clock  \\n(b) Multiple Clocks  \\n(c) Calendar  \\n(d) Timer\\n\\nFigure 6. Examples of the Time visual Scenario\\n\\n(a) Grocery  \\n(b) Furniture  \\n(c) Clothes  \\n(d) Gadgets  \\n(e) Cosmetics  \\n(f) Services\\n\\nFigure 7. Examples of the Shopping visual scenario\"}"}
{"id": "PjiRSyUt7e", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nFigure 8. Examples of the Navigation visual scenario\\n\\n(a) Street (b) Car (c) Bus (d) Train (e) Airport\\n\\nFigure 9. Examples of the Abstract visual scenario\\n\\n(a) (b) (c) (d)\\n\\nFigure 10. Examples of the Mobile Usage visual scenario\\n\\n15\"}"}
{"id": "PjiRSyUt7e", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nFigure 11. Examples of the Web Usage visual scenario\\n\\nFigure 12. Examples of the Infographic visual scenario\\n\\nFigure 13. Examples of the Miscellaneous Natural Scenes visual scenario\"}"}
{"id": "PjiRSyUt7e", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Human Annotation Screenshots\\n\\nB.1. Human Performance Screenshot\\n\\nWe present the screenshot of the user interface used for acquiring human responses on the CONTEXTUAL dataset in Figure 14.\\n\\nFigure 14. User interface of the human response collection.\\n\\nB.2. Human Evaluation Screenshot\\n\\nWe present the screenshot of the user interface used for human evaluation in Figure 15.\\n\\nFigure 15. User interface of the human evaluation.\"}"}
{"id": "PjiRSyUt7e", "page_num": 46, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Notably, in this example LLaVA-Next-34B is the only model that gets the correct response. All other models either do not produce a response or produce the wrong response due to lack of strong text recognition capabilities. This also shows that improvement in visual encoding capabilities of open-source LMMs, like LLaVA-Next-34B over LLaVA-1.5-13B improves performance on context-sensitive text-rich visual reasoning tasks.\"}"}
{"id": "PjiRSyUt7e", "page_num": 47, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 44. In this task, all models hallucinate. However, all but GPT-4V produce a partially correct response.\"}"}
{"id": "PjiRSyUt7e", "page_num": 48, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section provides qualitative analysis of the Infographic visual scenario and our findings across the models, as shown in Figures 45, 46, 47, 48.\\n\\nInstruction: What is the message corresponding to the eyes symbol?\\n\\nReference Response: It says 61% of the surveyors believe that there won't be any crowd where they're going to eat out.\\n\\nHuman Response: 61% of respondents want to know there won't be crowds where they're going.\\n\\nGPT-4V Response: The eyes symbol corresponds to the message: \\\"61% of diners feel it's extremely important to know there won't be crowds where they're going.\\\" This indicates that a significant number of diners prioritize knowledge about the expected crowd levels at dining venues as a factor in their comfort with eating out again in the context of the COVID-19.\\n\\nGemini Pro-Vision Response: The message corresponding to the eyes symbol is \\\"Know there won't be crowds where they're going.\\\" 61% of diners surveyed said this.\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The message corresponding to the eye symbol is: \\\"To feel comfortable gathering in public spaces again, diners feel it's extremely important to See people wearing masks and using hand sanitizer\\\" with 63% agreement, \\\"Know there won't be crowds where they're going\\\" with 61% agreement, and to \\\"Receive a vaccine for COVID-19\\\" with also 61% agreement.\\n\\nCorrect response with Hallucination\\n\\nFigure 45. In this task, closed-source LMMs GPT-4V and Gemini Pro-Vision correctly respond to the instruction. This can be attributed to their ability for visual perception, text recognition, and context-sensitive text-rich visual reasoning. All other models hallucinate.\"}"}
{"id": "PjiRSyUt7e", "page_num": 49, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Calculate the difference in the percent of respondents who have confidence and do not have confidence in Obama from the country with a doll in their stamp.\\n\\nReference Response:\\nCountry is Poland. Percent of respondents who approve is 47 and the percent who do not have confidence is 38. The difference of the two is 9 percent.\\n\\nHuman Response:\\nThe country is Poland and the difference between the respondents who have and do not have confidence in Obama is 9%.\\n\\nGPT-4V Response:\\nThe country represented by a stamp with a doll on it is Poland. According to the data provided: 47% of respondents have a lot of confidence or some confidence in Obama\u2019s ability to do the right thing in world affairs. 38% of respondents do not have too much confidence or no confidence at all in Obama\u2019s ability to do the right thing in world affairs. To calculate the difference, we subtract the smaller percentage from the larger percentage: 47% (Confidence) - 38% (No confidence) = 9%\"}"}
{"id": "PjiRSyUt7e", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I. Detailed Qualitative Analysis\\n\\nIn this section, we qualitatively analyze the responses of the different visual categories. In the caption annotations, we follow the following color notations: green: correct response, red: incorrect response, and purple: intuition behind incorrect response.\\n\\nI.1. Time\\n\\nThis section provides qualitative analysis of the Time visual scenario and our findings across the models, as shown in Figures 23, 24, 25, 26.\\n\\nInstruction: Identify the object on the dial that shows the time 10:10.\\n\\nReference Response: The clock at 10:10 features an octopus on its dial.\\n\\nHuman Response: The only clock with an object on the dial at 10:10 is the white one with a tall purple octopus on it.\\n\\nGPT-4V Response: The clock with the image of an octopus on the face is showing the time 10:10.\\n\\nGemini Pro-Vision Response: An Octopus\\n\\nLLaV A-Next-34B: The object on the dial that shows the time 10:10 is the hour hand and the minute hand.\\n\\nShareGPT-4V-7B Response: The object on the dial that shows the time 10:10 is an apple.\\n\\nLLaV A-1.5-13B Response: The object on the dial that shows the time 10:10 is a clock.\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The instruction does not provide enough specific information to identify the object on the dial that shows the time 10:10.\\n\\nIn this task, GPT-4V and Gemini Pro-Vision correctly respond to the instruction. This can be attributed to their ability for visual perception, text recognition, and context-sensitive text-rich visual reasoning. Open source LMMs like LLaVA-Next-34B, ShareGPT4V, and LLaVA-1.5-13B and GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM approach) predict the wrong response.\"}"}
{"id": "PjiRSyUt7e", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nWhat time is the sun dial indicating?\\n\\nReference Response: The time shown on the sundial is between 1pm and 2pm.\\n\\nHuman Response: The sundial depicts approximately 1:30.\\n\\nGPT-4V Response: The sundial in the image is designed to tell time by casting a shadow on its face with the gnomon, which is the part sticking out that catches the sunlight. The shadow appears to be falling between the 1 and 2 on the right side of the dial. Given the position of the shadow, it suggests that the time is approximately 1:30.\\n\\nGemini Pro-Vision Response: The sundial is indicating that it is between 1 and 2 o'clock.\\n\\nLLaVA-Next-34B: The sundial in the image indicates that it is approximately 12:30. The shadow of the gnomon (the central rod) is pointing towards the number 12, which represents noon, and the shadow is about halfway between the numbers 12 and 1, suggesting that it is about 30 minutes past noon. Sundials are used to tell time by the position of the shadow cast by the gnomon, which is aligned with the Earth's axis and the position of the sun. Reasoning correct but needs accurate fine-grained perception.\\n\\nShareGPT-4V-7B Response: The sun dial is indicating 12:20.\\n\\nWrong response.\\n\\nLLaVA-1.5-13B Response: The sun dial is indicating that it is 1:20 pm.\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The image and the OCR text do not provide enough information to determine the exact time the sundial is indicating. Lack of context-sensitive text-rich visual reasoning.\\n\\nFigure 24. In this task, closed-source LMMs GPT-4V and Gemini Pro-Vision correctly respond to the instruction. This can be attributed to their ability for visual perception, text recognition, and context-sensitive text-rich visual reasoning. GPT-4V is also able to explain the reasoning behind its response correctly. Amongst the open source LMMs, LLaVA-1.5 correctly responds to the instruction, suggesting its ability to perform context-sensitive text-rich visual reasoning in this case. Notably, LLaVA-Next-34B gets the answer wrong but the reasoning correctly justifies the incorrect answer, thereby highlighting the need for more accurate fine-grained perception. On the other hand, ShareGPT4V and GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM approach) predict the wrong response.\"}"}
{"id": "PjiRSyUt7e", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The idea behind using a color scheme for the different numbers in the calendar is to make it easier for users to identify and distinguish between specific dates or events. In the image, the number 10 is highlighted in yellow, which draws attention to that particular day. This way, users can easily differentiate between the number 10 and other dates, enhancing readability and understanding.\"}"}
{"id": "PjiRSyUt7e", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 26. In this task, Gemini-Vision-Pro fails to ground the task to this image. Whereas, GPT4V, LLaVA-Next-34B, LLaVA-1.5-13B & ShareGPT4V-7B suffer from hallucination and incorrect reasoning. Lastly, the GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM approach) produces the wrong response.\"}"}
{"id": "PjiRSyUt7e", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide an illustration of a GPT4 prompt used to assess both model-generated and human responses. Figure 20 displays an instance within the Shopping category, featuring a reference response generated by the Gemini-Pro-Vision model. It's important to observe that the prompt does not include any information about the predicting model.\\n\\nInstruction: Describe the most similar product between the living room and the hip living room.\\n\\nReference Response: The orangish-brown colored single-seating sofa is the most similar between the Living Room and the Hip Living Room.\\n\\nPrompt: You are ImageTaskEvaluatorGPT, an expert language model at judging whether or not a response adequately addresses an instruction in the context of an image. More specifically, you will be given the following:\\n\\n1. An instruction: This is a question, an imperative request, or something similar about the image which requires a response.\\n2. A ground-truth response: This is the ground-truth response to the instruction in the context of the image annotated by the human annotator.\\n3. A predicted response: This response attempts to address the instruction in the context of the image without having access to the ground-truth response.\\n\\nYour job is judge whether the predicted response is correct given the ground-truth response and the instruction.\\n\\nSome things to remember:\\n\\n- Even though you are just a language model, the instructions mostly require an objective answer i.e., the ground-truth response and instruction should be sufficient for you to judge the correctness of the predicted response. You do not need to have access to the complete image description.\\n- You are capable of judging response quality, accounting for important factors like correctness, relevance, fluency, specificity, etc.\\n- You think step-by-step, and ultimately respond with your \u201cJudgement: \u201d as \u201cYes\u201d or \u201cNo\u201d. Here, \u201cYes\u201d implies that the predicted response is correct according to you, and \u201cNo\u201d implies that the predicted response is not correct.\\n- Many times the predicted responses provide long explanations for their decision. In such cases, focus on whether the ground-truth response can be inferred from the predicted response or not.\\n\\nInstruction: Describe the most similar product between the living room and the hip living room.\\n\\nGround-truth Response: The orangish-brown colored single-seating sofa is the most similar between the Living Room and the Hip Living Room.\\n\\nPredicted Response: The most similar product between the living room and the hip living room is the sofa. Both sofas are blue and have a similar shape.\\n\\nResponse: No\"}"}
{"id": "PjiRSyUt7e", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nG. Additional Fine-grained Evaluation\\n\\n(a) Performance on different types of tasks.\\n\\n(b) Performance on natural and digital scenes.\\n\\nFigure 21.\\n\\nAdditional fine-grained evaluation results.\\n\\nG.1. Types of Tasks\\n\\nWe compare the performance of the foundation models with varying types of tasks in Figure 21a using GPT-4 evaluation. Specifically, we assess the quality of the responses when the instructions require the models to extract text or visual elements in the image (e.g., List the exercises where the corresponding illustration showcases a single movement). There are 285 such instances in the CONTEXTUAL dataset. While these tasks require complex perception and reasoning abilities, they do not require additional operations on top of the information already presented in the image explicitly. We observe that the humans achieve 75.1% on such instructions while the proprietary models GPT-4V and Gemini-Pro-Vision achieve 38.7% and 40.1%, respectively. This indicates that humans are very good at identifying the key information that needs to be extracted to respond to the instructions.\\n\\nIn addition, we assess the responses when the instructions require the models to go beyond information extraction, and perform math reasoning for the instruction (e.g., What is the total price for the two cars listed here?). There are 66 instances in the CONTEXTUAL dataset. We find that humans achieve 62.1% on such tasks while the proprietary models GPT-4V achieve 27.3%, again highlighting the large gap in their math reasoning.\\n\\nG.2. Visual Scenes\\n\\nWe compare the performance of the foundation models with varying visual scenes (e.g., natural scenes and digital scenes) in Figure 21b. Majorly, shopping, navigation, and misc. natural scenes constitute natural scenes, and web usage, mobile usage, abstract, infographics and time reading constitute digital scenes. We find that humans achieve the highest performance in both the visual scenes i.e., 72.5% and 66.2% on natural scenes and digital scenes, respectively. In addition, we observe that GPT-4V achieve 47.8% and 48.2% on natural and digital scenes, respectively. Interestingly, we find that Gemini-Pro-Vision and ShareGPT-4V-7B achieve higher performance on the natural scenes than the digital scenes. It indicates these models may not have seen many examples with digital scenes during their pretraining. Thus, our CONTEXTUAL dataset highlights the gaps in the training data of the modern LMMs.\"}"}
{"id": "PjiRSyUt7e", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We develop a synthetic data generation pipeline that could be useful to create samples that more likely require context-sensitive text-rich visual reasoning. We use the existing OCR filtering strategy to obtain 200 candidate images belonging to the Misc. Natural scenes category. Then, we use in-context learning capabilities of GPT-4V and prompt it to generate instruction-response pairs (as shown in Fig. 22). We observed that use of negative demonstration within in-context examples, as shown in previous studies (Wang et al., 2022), along with reasons was critical for GPT4V to understand the nuanced difference between text only, visual element only and context-sensitive text-rich visual reasoning instructions. After generating the examples, we evaluated representative models from the different model categories: open-source LMMs (LLaVA, ShareGPT), closed-source LMMs (GPT-4V), and Augmented LLM (GPT-4 w/ Layout-aware OCR + Caption), as shown in Table 7.\\n\\n| Model                     | Synthetic examples | Contextual examples |\\n|---------------------------|--------------------|--------------------|\\n| LLaVA-v1.5-13B           | 39.8               | 29.7               |\\n| ShareGPT4v-7B            | 44.9               | 37.7               |\\n| GPT-4 w/ Layout-aware OCR| 51.1               | 27.3               |\\n| GPT4V                    | 68.6               | 48.0               |\\n\\nTable 7. Comparison of model performance (accuracy in %) using GPT4 evaluation of synthetically generated samples belonging to Misc. Natural Scenes category (200 samples) and human annotated samples (Contextual) belonging to Misc. Natural Scences Category (156 samples).\\n\\nWe observe that the accuracy of models on synthetic samples is greater than the accuracy of human-made Contextual examples. This is expected because it is difficult for the model to understand context-sensitive in the first place, and asking it to make such tasks would be difficult. Despite this, it can create good instructions because the performance gap of open-source LMMs is relatively small. The only exceptions are Aug LLM and GPT4V, which both use GPT models. Due to this, the GPT4 evaluator may favor their responses to the instruction generated by a GPT model. This demonstrates that we can scale these instructions by filtering good candidates for data generation and nuanced prompt engineering.\"}"}
{"id": "PjiRSyUt7e", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Good Instruction: Determine the message conveyed by the combination of the STOP sign and the movement implied by the blurred background.\\nResponse: The image conveys the message of a railroad crossing where vehicles are required to stop for passing trains, indicated by the combination of a STOP sign and the motion blur of a train in the background.\"}"}
{"id": "PjiRSyUt7e", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\n**C. Data Release**\\n\\n**CONTEXTUAL** comprises 506 samples spanning eight visual categories (refer to Table 2). To facilitate model development, we will release a subset of 100 samples from the 506, as validation set, along with their reference responses, while keeping them hidden for the remaining 406 samples. We ensure that the distribution of validation samples closely mirrors the overall dataset distribution. To achieve this, we randomly select 30 samples from the \u2018Miscellaneous Natural Scenes\u2019 category and 10 samples from the remaining categories, maintaining a proportional representation of each category in the validation samples, consistent with the overall benchmark. In this paper, all the results are reported on the entire dataset, unless stated otherwise.\\n\\n**D. Few-Shot Setting**\\n\\nHere, we compare the performance of the foundation models on **CONTEXTUAL** using GPT-4 evaluation with under the few-shot settings in Figure 16. Specifically, we perform zero-shot, two-shot, four-shot, and eight-shot evaluation for augmented LLM (GPT-4 prompted w/ layout aware OCR and image caption), Gemini-Pro-Vision, and Idefics-9B. We select in-context examples at random from our dataset and evaluate the models on the remaining instances.\\n\\n![Figure 16. Few-shot performance on the CONTEXTUAL dataset.](image)\\n\\nIn our experiments, we find that the performance of all the models does not change drastically with in-context examples. Specifically, we observe that Gemini-Pro-Vision response acceptance rating decreases by $5\\\\%$ in the two-shot setting as compared to the zero-shot setting, and, increases monotonically from two-shot to eight-shots. In addition, we observe that the performance improvements stagnate for Idefics-9B after the four in-context examples. Recent studies highlight the instability and sensitivity of LMMs in few-shot settings (Li et al., 2023b). For instance, a significant accuracy drop was observed in models like InstructBLIP in four-shot setting, especially in tasks requiring commonsense reasoning. Overall, we highlight that providing few-shot examples does not elicit context-sensitive text-rich visual reasoning in the foundation models.\\n\\n**E. Augmented LLM Prompt**\\n\\nIn this section, we discuss the design and elaborate on the prompts employed for the Augmented LLM approach (illustrated in Figure 17, 18, 19). We describe the three distinct prompt formats utilized, each differing in the extent of visual information presented. These formats encompass simple OCR of the image, OCR of the image arranged in the layout it appears in the image, and OCR presented in a layout format along with a comprehensive image caption. We prompt GPT4 with the above templates that does not take the image as input. However, the image is included in the illustration for reference purposes.\"}"}
{"id": "PjiRSyUt7e", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nE.1. GPT-4 w/ OCR\\n\\nInstruction: Describe the most similar product between the living room and the hip living room.\\n\\nReference Response: The orangish-brown colored single-seating sofa is the most similar between the Living Room and the Hip Living Room.\\n\\nPrompt: You are OCRGPT, an expert language model at responding to instructions posed for images. You have to respond to the instruction using the OCR Text of the image. More specifically, you will be given the following:\\n\\n1. An instruction: This is a question, an imperative request, or something similar about the image that requires a response.\\n2. OCR Text: Text extracted from the image.\\n\\nYou have to respond with the Answer only.\\n\\nNOW YOUR TURN:\\n\\nInstruction: Provide the price of the upholstered dining set.\\n\\nOCR Text:\\n\\nBedroom Hip Bedroom From $99 / month From $109 / month Includes 5 items Includes 5 items\\nLiving Room Hip Living Room. $59 / month $79 / month Includes 4 items Includes 4 items\\n\\nAnswer:\\n\\nGPT-4 w/ OCR Response:\\n\\nBoth the Living Room and the Hip Living Room include 4 items.\"}"}
{"id": "PjiRSyUt7e", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The most similar product between the living room and the hip living room is that both are available on a monthly basis, the living room costs $59/month and the hip living room costs $79/month, both including 4 items.\"}"}
{"id": "PjiRSyUt7e", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instruction: Provide the cost of hiring the machine type depicted in the image for nine months.\\n\\nOCR Text:\\n\\nBedroom Hip Bedroom\\nFrom $99 / month From $109 / month\\nIncludes 5 items Includes 5 items\\n\\nLiving Room Hip Living Room\\n$59 / month $79 / month\\nIncludes 4 items Includes 4 items\\n\\nImage Caption: The image is an advertisement for a furniture store, showcasing a variety of furniture items. The items are arranged in a grid-like pattern, with each item occupying its own square. The furniture includes a couch, a dining table, a chair, a dresser, a bed, and a cabinet. The colors of the furniture range from brown to beige, with some items featuring black and white accents. The text in the image indicates that the furniture is available for purchase at prices ranging from 295 to 1945. The text also mentions that the furniture is of \\\"LUXURY FURNITURE DESIGN\\\". The overall layout and design of the advertisement suggest a focus on high-quality, stylish furniture.\\n\\nAnswer:\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response:\\nThe most similar product between the Living Room and the Hip Living Room is in terms of the quantity of items. Both living room options include 4 items each - a couch, a coffee table, and a side table. The relative positioning and type of furniture also seem to be similar, although the styles and colors vary.\"}"}
{"id": "PjiRSyUt7e", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I.2. Shopping\\n\\nThis section provides qualitative analysis of the Shopping visual scenario and our findings across the models, as shown in Figures 27, 28, 29, 30, ...\\n\\nInstruction: Provide the cost of hiring the machine type depicted in the image for nine months.\\n\\nReference Response: The machine shown in the image is a front-loading machine, and the rental fee for a period of 9 months amounts to INR 7200.\\n\\nHuman Response: Front load machines cost 7200 for 9 months.\\n\\nGPT-4V Response: The image displays a front load washing machine along with a price list for hiring different types of washing machines. For hiring a front load washing machine for a period of 9 months, the cost would be INR 7,200 according to the given price list.\\n\\nGemini Pro-Vision Response: The cost of hiring the machine depicted in the image for nine months is INR 4500.\\n\\nLack of context-sensitive text-rich visual reasoning\\n\\nLLaVA-Next-34B: The cost of hiring the machine type depicted in the image for nine months is 4500 rupees for a top load machine and 7200 rupees for a front load machine.\\n\\nShareGPT-4V-7B Response: The cost of hiring the washing machine for nine months is 4500 rupees.\\n\\nLack of context-sensitive text-rich visual reasoning & Hallucination\\n\\nLLaVA-1.5-13B Response: The cost of hiring the machine type depicted in the image for nine months is 60,000 rupees.\\n\\nFigure 27. In this task, apart from GPT4V and LLaVA-Next-34B, all other models produce the wrong response. This can be attributed to the strong fine-grained visual perception and context-sensitive text-rich visual reasoning abilities of GPT4V. This also shows that improvement in visual encoding capabilities of open-source LMMs, like LLaVA-Next-34B over LLaVA-1.5-13B improves performance on context-sensitive text-rich visual reasoning tasks.\"}"}
{"id": "PjiRSyUt7e", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Describe the most similar product between the living room and the hip living room.\\n\\nReference Response: The orangish-brown colored single-seating sofa is the most similar between the Living Room and the Hip Living Room.\\n\\nHuman Response: Both living rooms have similar orange chairs.\\n\\nGPT-4V Response: The most similar products between the \u201cLiving Room\u201d and \u201cHip Living Room\u201d sets appear to be the armchairs. Both sets feature a single armchair with a similar style and design, characterized by a mid-century modern aesthetic, with splayed legs and an angled backrest.\\n\\nGemini Pro-Vision Response: The most similar product between the living room and the hip living room is the sofa. Both sofas are blue and have a similar shape.\\n\\nLack of fine-grained visual perception.\\n\\nLLaVA-Next-34B: The most similar product between the living room and the hip living room is the sofa. Both images feature a sofa as a central piece of furniture in the living room setup. The color of the sofa in the living room is a neutral gray, while the hip living room has a blue sofa. The other items in the living room and hip living room are different, with the living room having a wooden coffee table and a floor lamp. The hip living room has a more contemporary coffee table and a floor lamp with a unique design.\\n\\nLack of fine-grained visual perception.\\n\\nShareGPT-4V-7B Response: The most similar product between the living room and the hip living room is the couch. Both living room and hip living room couches are designed to provide comfort and serve as a focal point in the room.\\n\\nLack of fine-grained visual perception.\\n\\nLLaVA-1.5-13B Response: The most similar product between the living room and the hip living room is the couch. Both living rooms feature a couch as a central piece of furniture, providing a comfortable seating area for relaxation and socializing. The couches in both rooms have a similar design.\\n\\nLack of fine-grained visual perception.\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The most similar product between the Living Room and the Hip Living Room is in terms of the quantity of items. Both living room options include 4 items each - a couch, a coffee table, and a side table. The relative positioning and type of furniture also seem to be similar, although the styles and colors vary.\\n\\nLack of context-sensitive text-rich visual reasoning.\\n\\nFigure 28. In this task, apart from GPT4V, all other models produce the wrong response. This can be attributed to the strong fine-grained visual perception and context-sensitive text-rich visual reasoning of GPT4V. Further, GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM approach) shows inability to correctly reason with the provided dense image information as text, thereby, highlighting the need for context-sensitive text-rich visual reasoning.\"}"}
{"id": "PjiRSyUt7e", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The image shows five tourists and the tour guide charges $2-5 per person per day. So, the maximum money the tour guide can make is $25 per day.\"}"}
{"id": "PjiRSyUt7e", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Discuss the features of a camera being held vertically with a single hand.\\n\\nKonica Minolta is the camera held vertically in one hand, specifically the Big Mini BM-201 model, priced at $130, C113, \u00a3102, or IDR 1,893,775.\\n\\nThe image appears to be a collage of various cameras with a person holding one camera vertically in the top right corner. Since we're focusing on the vertically held camera, here are some general features and considerations for using a camera in a vertical orientation with a single hand.\\n\\nThe camera being held vertically with a single hand is a Fujifilm Instax Mini 7s. It is a compact and lightweight instant camera that is perfect.\\n\\nThe image shows a person holding a camera vertically with a single hand. This type of camera is likely a mirrorless or compact camera, as it appears to be relatively small and lightweight, allowing for such a grip. The camera is equipped with a lens, which is typical for cameras that capture images with a higher level of detail and quality compared to smartphones...\\n\\nThe camera being held vertically with a single hand is a Canon EOS 5D Mark IV. This camera is known for...\\n\\nA camera being held vertically with a single hand is typically a compact or point-and-shoot camera...\\n\\nThe image does not specify the features of a camera being held vertically with a single hand. However, a camera designed for this use would typically be lightweight and compact...\"}"}
{"id": "PjiRSyUt7e", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section provides qualitative analysis of the Navigation visual scenario and our findings across the models, as shown in Figures 31, 32, 33, 34.\\n\\nInstruction: Determine the triangular-shaped signs.\\n\\nReference Response: Triangular-shaped signs are Right Curve, Left Curve, Cross Road, Speed Breaker, and School.\\n\\nHuman Response: The triangular shaped signs are Right Curve, Left Curve, Cross Road, and Speed Breaker, in order from left to right, in the top row. The the second row, the last image is triangular and is for School.\\n\\nGPT-4V Response: The triangular shaped signs in the image are as follows: 1. The second sign on the top row, indicating a \\\"RIGHT CURVE.\\\" 2. The third sign on the top row, indicating a \\\"LEFT CURVE.\\\" 3. The fourth sign on the top row, indicating a \\\"CROSS ROAD.\\\" 4. The fifth sign on the top row, with an icon of a bump, indicating a \\\"SPEED BREAKER.\\\" 5. The third sign on the bottom row, with an icon of two people, indicating a \\\"SCHOOL\\\" zone.\\n\\nGemini Pro-Vision Response: There are 6 triangular-shaped signs: 1. Right curve 2. Left curve 3. Cross road 4. Speed breaker. Partially correct response and hallucination.\\n\\nLLaV A-Next-34B: The triangular-shaped signs in the image are: \\\"LEFT CURVE\\\" in white, \\\"RIGHT CURVE\\\" in white, \\\"LEFT TURN\\\" in white, \\\"RIGHT TURN\\\" in white, \\\"NO ENTRY\\\" in white. Partially correct response and hallucination.\\n\\nShareGPT-4V-7B Response: The triangular-shaped signs in the image are the \\\"No Parking\\\" sign and the \\\"No Entry\\\" sign. Lack of fine-grained visual perception.\\n\\nLLaV A-1.5-13B Response: The triangular-shaped signs in the image are warning signs, which are typically used to convey important information or instructions to drivers and pedestrians. Lack of context-sensitive text-rich visual reasoning.\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The triangular-shaped signs in the image are: 1. STOP 2. CAUTION 3. RIGHT CURVE 4. LEFT CURVE 5. CROSS ROAD 6. SPEED BREAKER 7. TURN ROUND 8. RIGHT CAUTION 9. SPEED LIMIT 10. NO PARKING 11. NO ENTRY 12. SCHOOL 13. BARRIER AHEAD. Lack of context-sensitive text-rich visual reasoning.\\n\\nFigure 31. In this task GPT4V produces the correct response, Gemini-Vision-Pro & LLaVA-Next-34B produce a partially correct answer with minor hallucinations. All other models hallucinate, and produce the wrong response or both.\"}"}
{"id": "PjiRSyUt7e", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 32.\\nIn this task, closed-source LMMs GPT-4V and Gemini Pro-Vision correctly respond to the instruction. This can be attributed to their ability for visual perception, text recognition, and context-sensitive text-rich visual reasoning. All other models lack context-sensitive text-rich visual reasoning and some even contain hallucinations in their responses.\"}"}
{"id": "PjiRSyUt7e", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nSpecify the colors of the rocket, space shuttle, and external tank.\\n\\nReference Response: The space shuttle is depicted in yellow, the rockets are depicted in grey, and the external tank is depicted in red.\\n\\nHuman Response: The rocket is gray. The space shuttle is yellow. The external tank is red.\\n\\nGPT-4V Response: In the image, the solid rocket boosters of the space shuttle are colored in red and light gray, with the red at the top and the gray on the bottom. The external tank is colored orange. The main body of the space shuttle, also known as the orbiter, is primarily white with black on its nose and wing edges. There is also a bit of red on the tail section. The image depicts various stages of a space shuttle flight sequence from pre-launch to landing.\\n\\nGemini Pro-Vision Response: The rocket is red and white, the space shuttle is white, and the external tank is orange.\\n\\nllava-1.5-13b: The rocket is red, the space shuttle is yellow, and the external tank is white.\\n\\nShareGPT-4V-7B Response: The rocket is red, the space shuttle is yellow, and the external tank is orange.\\n\\nllava-next-34b: The rocket is red and white, the space shuttle is yellow, and the external tank is white.\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The rocket is red and yellow. The space shuttle is yellow and red. The image did not specify the color of the external tank.\\n\\nHallucination\\n\\nFigure 33. In this task, all models hallucinate, due to lack of grounding the response to the given image & fine-grained visual perception and using prior visual knowledge. Notably, the open-source LMMs, LLaVA-Next-34B, LLaVA-1.5-13B, & ShareGPT-4V-7B provide partially accurate responses, suggesting a reduced reliance on prior visual knowledge, possibly due to not having seen it.\"}"}
{"id": "PjiRSyUt7e", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nFigure 34. In this task, all models produce the wrong response, due to lack of fine-grained visual perception.\"}"}
{"id": "PjiRSyUt7e", "page_num": 50, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Highlight the states where the percent of positive tests is greater than 30%.\\n\\nReference Response: WA and MS\\n\\nHuman Response: Washington and Mississippi\\n\\nGPT-4V Response: If you need to highlight states where the percent of positive tests is greater than 30% according to the provided image, you can use a graphic editing tool to draw attention to these states...\\n\\nGemini-Pro-Vision Response: The states where the percent of positive tests is greater than 30% are: Louisiana, Alabama, Georgia, and Mississippi.\\n\\nLLaVA-Next-34B: The states where the percent of positive tests is greater than 30% are highlighted in red on the map. These states include: AK (Alaska), ME (Maine), VT (Vermont), NH (New Hampshire).\\n\\nShareGPT-4V-7B Response: The states where the percent of positive tests is greater than 30% are: New York, New Jersey, Pennsylvania, Maryland, Virginia, Massachusetts, Rhode Island, Delaware, Connecticut, Ohio, Indiana, Michigan, Illinois, Minnesota, Wisconsin, Iowa, Nebraska, South Dakota, North Dakota, West Virginia, Kentucky, Tennessee, Alabama, Georgia, Florida, Arkansas, Oklahoma, Texas, Washington, D.C., and Mississippi.\\n\\nLLaVA-1.5-13B Response: In the image, there are several states where the percent of positive tests is greater than 30%. These states include: California: 31.7%, Texas: 30.6%, Florida: 30.5%, New York: 30.4%, Pennsylvania: 30.3%\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The image doesn't provide specific information on which states have a percent of positive tests greater than 30%.\\n\\nGemini-Vision-Pro, ShareGPT-4V-7B & LLaVA-1.5-13B hallucinate in their responses. GPT-4 w/ Layout-aware OCR + Caption Response produces the wrong answer, due to lack of context-sensitive text-rich visual reasoning.\"}"}
{"id": "PjiRSyUt7e", "page_num": 51, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How many production methods are used under pour-over/drip category?\\n\\nReference Response: 10\\n\\nHuman Response: There are 10 methods in the pour-over/drip category.\\n\\nGPT-4V Response: In the pour-over/drip category of this coffee chart, there appear to be six production methods used. These methods are as follows: Auto Drip, Cone, Flat Bottom, AeroPress, Woodneck, Chemex\\n\\nGemini Pro-Vision Response: There are 8 production methods listed under pour-over/drip category.\\n\\nLLaV A-Next-34B: The image shows a chart of coffee production methods, and under the \u201cPour-Over/Drip\u201d category, there are four different methods illustrated: 1. Chemex 2. V60 3. Kalita Wave 4. Bee House. Each method is represented by a different coffee-making device, and they are connected by lines indicating the flow of the coffee-making process.\\n\\nShareGPT-4V-7B Response: There are two production methods used under the pour-over/drip category: French press and drip coffee maker.\\n\\nLLaV A-1.5-13B Response: There are three production methods used under the pour-over/drip category.\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The OCR text does not provide specific information on the number of production methods used under the pour-over/drip category.\\n\\nFigure 48. In this task, the all LMMs produce the wrong response due to lack of fine-grained visual perception and possibly poor math reasoning. GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM approach) produces the wrong answer, due to lack of context-sensitive text-rich visual reasoning.\"}"}
{"id": "PjiRSyUt7e", "page_num": 52, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section provides qualitative analysis of the Miscellaneous Natural Scenes visual scenario and our findings across the models, as shown in Figures 49, 50, 51, 52.\\n\\nInstruction: Provide your opinion on whether changing the switch's current position will activate the instrument.\\n\\nReference Response: Despite turning the switch to the On position, the system will remain powered off due to the absence of a connected power cable.\\n\\nHuman Response: It will not. The instrument needs to be plugged in to work.\\n\\nGPT-4V Response: The image shows the back panel of an electronic device with a power switch in the 'OFF' position. Changing the switch's current position from 'OFF' to 'ON' is likely to activate the instrument, provided that it is properly connected to a power source and there are no malfunctions with the instrument itself.\\n\\nGemini Pro-Vision Response: Flipping the switch to the \u201cON\u201d position will power the instrument.\\n\\nLLaVA-Next-34B: Based on the image, the switch is currently in the \u201cOFF\u201d position. If you were to change the switch to the \u201cON\u201d position, it would likely activate the instrument, assuming that the instrument is in proper working condition and has been set up correctly.\\n\\nShareGPT-4V-7B Response: Based on the image, it is not possible to determine if changing the switch's current position will activate the instrument. The switch is currently in the off position, and the instrument is not in use. To determine if changing the switch's position will activate the instrument, one would need to observe the instrument's response to the switch's movement.\\n\\nLLaVA-1.5-13B Response: Based on the image, the switch is currently in the \u201coff\u201d position, which means that the instrument is not activated. To activate the instrument, the switch would need to be moved to the \u201con\u201d position. This would allow the electrical current to flow through the device, enabling it to function properly.\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The instruction is ambiguous with not enough visual information available in the provided OCR text or the image caption regarding a switch. Therefore, it is not possible to provide an opinion on whether changing the switch's current position will activate the instrument.\\n\\nLack of context-sensitive text-rich visual reasoning\\n\\nFigure 49. In this task, GPT4V & LLaVA-Next-34B produces the correct response, whereas all other LMMs, produce the wrong response due to lack of fine-grained visual perception. GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM approach) produces the wrong answer, due to lack of context-sensitive text-rich visual reasoning.\"}"}
{"id": "PjiRSyUt7e", "page_num": 53, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nIn this task, GPT-4V and Gemini Pro-Vision correctly respond to the instruction. This can be attributed to their ability for text recognition, and context-sensitive text-rich visual reasoning. Open source LMMs like LLaVA-Next-34B, ShareGPT4V & LLaVA-1.5-13B due to lack of fine-grained visual perception. GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM approach) predict the wrong answer due to lack of context-sensitive text-rich visual reasoning.\"}"}
{"id": "PjiRSyUt7e", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is the difference between the illustration of Romance and Romance II?\\n\\nReference Response: The logo for \u201cRomance\u201d features a heart with an unshaded interior, while the logo for \u201cRomance II\u201d showcases a heart filled with white shading inside.\\n\\nHuman Response: Romance II has a filled in heart vs not filled in.\\n\\nGPT-4V Response: The illustration of \u201cRomance\u201d and \u201cRomance II\u201d in the image both feature a heart icon, which is a widely recognized symbol of love and affection. The difference between the two appears to be simply numerical, possibly indicating a continuation or second set of phrases related to romance within the language learning app \u201cSimply Learn Tagalog.\u201d There\u2019s no visual difference between the icons themselves, suggesting the distinction is in content rather than visual or thematic change.\\n\\nLack of fine-grained visual perception & Hallucination\\n\\nGemini Pro-Vision Response: The difference between the two is that the first one has one heart, while the second one has two.\\n\\nLack of fine-grained visual perception & Hallucination\\n\\nLLaV A-Next-34B: The illustration for \u201cRomance\u201d features a heart symbol, which is a universal symbol for love and affection. The illustration for \u201cRomance II\u201d also features a heart symbol, but it is accompanied by a small lock, which could represent the idea of commitment or the act of locking one\u2019s heart to someone. This might suggest that \u201cRomance II\u201d is a more advanced or deeper form of romance, implying a stronger bond or a more serious relationship.\\n\\nLack of fine-grained visual perception & Hallucination\\n\\nShareGPT-4V-7B Response: The difference between the illustrations of Romance and Romance II lies in the design of the heart symbol. In the first illustration, the heart symbol is depicted as a simple, two-dimensional shape. In contrast, the heart symbol in the second illustration, Romance II, is more detailed and intricately designed.\\n\\nPartially correct response but not completely grounded to the image\\n\\nLLaV A-1.5-13B Response: The illustration of Romance and Romance II in the app are different in terms of the visual representation. Romance is represented by a heart symbol, while Romance II is represented by a heart with a plus sign on it.\\n\\nLack of fine-grained visual perception & Hallucination\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The image does not provide distinct illustrations for \u201cRomance\u201d and \u201cRomance II\u201d. Only one icon related to romance is described in the image caption - a rose. The instruction might be referring to different elements, but based on the provided OCR text and image caption, there is not enough information to distinguish between \u201cRomance\u201d and \u201cRomance II\u201d.\\n\\nLack of context-sensitive text-rich visual reasoning\\n\\nFigure 39. In this task, the closed source LMMs, GPT4V, Gemini-Vision-Pro, LLaVA-Next-34B & LLaVA-1.5-13B hallucinate in their responses. GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM approach) produces the wrong answer, due to lack of context-sensitive text-rich visual reasoning. However, ShareGPT4V-7B produces a partially correct response but is unable to ground it completely to the image.\"}"}
{"id": "PjiRSyUt7e", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nThe graffiti stickers each have a different word on them. From what I can discern, the words are: LOVE, THANKS, WIZ, FISH, AWESOME, HOT, DOPE.\\n\\nThe words are LOL, love, Thanks, WTF, UGH, awesome, oops, HoT and Dope.\\n\\nGemini Pro-Vision Response: Home, Auto, Wut, Uhh\\n\\nLove, Thanks, Awesome, Dope, Hot\\n\\nLack of fine-grained visual perception & Lack of context-sensitive text-rich visual reasoning\\n\\nShareGPT-4V-7B Response: Love, Awesome, Dope, Facebook, Google, Android Beam\\n\\nLLaV A-1.5-13B Response: The words formed by all the grafitti stickers are \\\"LOVE\\\".\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The words formed by the graffiti stickers are \\\"GRAFONSNCINER\\\", \\\"UTH\\\", \\\"A VESOS\\\".\\n\\nFigure 40. In this task, all models show a varying ability to recognise stylistic text, with GPT4V being the best and surprisingly GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM approach) is the worst. This underscores the need of open-ended text recognition models, that can understand a variety of text styles.\"}"}
{"id": "PjiRSyUt7e", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section provides qualitative analysis of the Web Usage visual scenario and our findings across the models, as shown in Figures 41, 42, 43, 44.\\n\\nInstruction: Name the country whose portrait has a group of people on the road.\\n\\nReference Response: Cuba\\n\\nHuman Response: Cuba\\n\\nGPT-4V Response: The image with a group of people on the road is associated with \\\"Material World: Cuba.\\\"\\n\\nGemini Pro-Vision Response: Bhutan\\n\\nLack of fine-grained visual perception\\n\\nLLaV A-Next-34B: The country whose portrait shows a group of people on the road is Bhutan.\\n\\nLack of fine-grained visual perception\\n\\nShareGPT-4V-7B Response: Mexico\\n\\nLack of fine-grained visual perception\\n\\nLLaV A-1.5-13B Response: Bhutan\\n\\nLack of fine-grained visual perception\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The image does not provide specific information on which country's portrait has a group of people on the road.\\n\\nLack of context-sensitive text-rich visual reasoning\\n\\nFigure 41. In this task GPT4V produces the correct response, and all other models produce the wrong answer, due to lack of fine-grained visual perception ability. Specifically, GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM approach) lacks context-sensitive text-rich visual reasoning ability to respond to this task.\"}"}
{"id": "PjiRSyUt7e", "page_num": 45, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Which story can be read without subscription under Today's pick?\\n\\nReference Response: The story with the title \\\"Science: It looked like a bizarre alignment of meteors. It was something else\\\" because it does not have lock sign which other two stories have.\\n\\nHuman Response: It looked like a bizarre alignment of meteors. It was something else.\\n\\nGPT-4V Response: I'm sorry, but I can't provide real-time or current information about which specific articles are available without a subscription on external websites such as National Geographic. You would need to visit their website directly to see which stories are accessible for free.\\n\\nGemini Pro-Vision Response: The story titled \\\"It looked like a bizarre alignment of meteors. It was something else\\\" can be read without a subscription.\\n\\nLLaV A-Next-34B: Under \\\"Today's Picks\\\" on the National Geographic website, there are several stories that can be read without a subscription. These include: 1. \\\"Ancient altar reveals the power of Arab desert merchants\\\" under the \\\"History & Culture\\\" section. 2. \\\"It looked like a bizarre alignment of meteors. It was something else.\\\" under the \\\"Science\\\" section. 3. \\\"What was lost in Lahaina, a jewel of the Hawaiian Kingdom\\\" under the \\\"History & Culture\\\" section...\\n\\nLack of instruction understanding and grounding to the image\\n\\nGemini Pro-Vision Response: The story titled \\\"It looked like a bizarre alignment of meteors. It was something else\\\" can be read without a subscription.\\n\\nLLaV A-1.5-13B: Under Today's pick, the story \\\"Here's how wildfires get started and how to stop them\\\" can be read without subscription. This story is available for full access, while the other stories require a subscription to National Geographic.\\n\\nLack of context-sensitive text-rich visual reasoning\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The stories that can be read without subscription under Today's Picks are:\\n\\n\\\"Ancient altar reveals the power of Arab desert merchants\\\" under the category \\\"History & Culture\\\"...\"}"}
{"id": "PjiRSyUt7e", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The saying \\\"Work smart, not hard\\\" is illustrated in the accompanying image. It depicts multiple people pushing square wooden blocks across a flat surface, while one person carves a wooden block into a sphere and easily transports it across the same surface.\"}"}
{"id": "PjiRSyUt7e", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nInstruction:\\nComment on the funny aspect of the comic strip.\\n\\nReference Response:\\nIn this four-panel comic, the lawyer (seal) asks the defendant (killer whale) to identify itself, but the whale hesitates because admitting to being a \\\"killer\\\" would mean confessing to the crime. In the final panel, the defense lawyer (dolphin) exclaims \\\"objection,\\\" evoking humor through this conundrum.\\n\\nHuman Response:\\nThe whale depicted in the strip is a \\\"Killer\\\" Whale.\\n\\nGPT-4V Response:\\nThis comic strip might be considered particularly funny for a few reasons: Anthropomorphism: The characters are animals dressed in human clothes and partaking in a court trial, which is inherently comical. Visual humor: The expressions and body language of the animals add to the comedic effect, as they mimic human courtroom drama. Irony: It's humorous and ironic because whales obviously cannot speak, let alone participate in a legal procedure, making the whole situation absurdly funny.\\n\\nLack of context-sensitive text-rich visual reasoning\\n\\nGemini Pro-Vision Response:\\nThe comic strip is funny because it plays on the fact that whales are often seen as intelligent and articulate creatures, but in this case, the whale is unable to do so.\\n\\nLack of context-sensitive text-rich visual reasoning\\n\\nLLaV A-Next-34B:\\nThe comic strip features a humorous exchange between two characters, one of whom is a whale. The humor arises from the unexpected and absurd nature of the conversation. The first character asks the whale, \\\"Would the defendant tell the court exactly... what kind of whale you are?\\\" This question is funny because it's an unusual and irrelevant inquiry in a court setting. The whale's response, \\\"Well, I'm... uh..., is funny because it's a human-like reaction to being put on the spot, and the whale's hesitation suggests it's unsure how to answer. The final panel, where the whale objects, is a play on the legal term \\\"objection\\\" and adds to the absurdity of the situation, as whales do not participate in legal proceedings...\\n\\nLack of context-sensitive text-rich visual reasoning\\n\\nShareGPT-4V-7B Response:\\nThe viewer considers this comic strip to be the funniest thing they have ever seen because it features a humorous and unexpected scenario. In the comic strip, a man and a seal are depicted in a conversation, with the seal sitting in a chair and holding a briefcase. The comic strip's unexpected twist, where a seal is dressed in a suit and holding a briefcase, creates a sense of humor and amusement that the viewer finds entertaining.\\n\\nLack of context-sensitive text-rich visual reasoning\\n\\nLLaV A-1.5-13B Response:\\nA viewer might consider this comic strip to be the funniest thing they have ever seen because it features a unique and unexpected combination of elements. The strip includes a courtroom scene with a seal in a suit, a dolphin in a witness box, and a whale in a tie. This unusual and absurd scenario creates a humorous and unexpected twist on a serious topic, which can be seen as a form of satire or parody.\\n\\nLack of context-sensitive text-rich visual reasoning\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response:\\nThe viewer might find this comic strip to be the funniest thing they have ever seen due to the absurdity and humour of the situation: a man and a fish having a legal discourse...\\n\\nLack of context-sensitive text-rich visual reasoning & Hallucination\"}"}
{"id": "PjiRSyUt7e", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I.5. Application Usage\\n\\nThis section provides qualitative analysis of the Application Usage visual scenario and our findings across the models, as shown in Figures 37, 38, 39, 40.\\n\\nInstruction: How many times has the user drank water today?\\n\\nReference Response: 1 times since only the option with \u201c1\u201d on it is selected\\n\\nHuman Response: Once\\n\\nGPT-4V Response: Based on the screenshot provided, it appears that the user has marked one instance of drinking water today. The user seems to be using an application that tracks habits, and the \u201cDrink Water\u201d card shows one filled circle, indicating one time the user has logged drinking water for the current day.\\n\\nGemini Pro-Vision Response: The user has not drank any water today.\\n\\nLack of fine-grained visual perception\\n\\nLLaV A-Next-34B: The user has drank water once today.\\n\\nLack of fine-grained visual perception\\n\\nShareGPT-4V-7B Response: The user has drank water three times today.\\n\\nLack of fine-grained visual perception\\n\\nLLaV A-1.5-13B Response: The user has drank water three times today.\\n\\nLack of fine-grained visual perception\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The user has drank water 3 times today.\\n\\nLack of fine-grained visual perception\\n\\nFigure 37. In this task GPT4V & LLaVA-Next-34B produces the correct response, and all other models hallucinate, due to lack of fine-grained visual perception ability. This also shows that improvement in visual encoding capabilities of open-source LMMs, like LLaVA-Next-34B over LLaVA-1.5-13B improves performance on context-sensitive text-rich visual reasoning tasks.\"}"}
{"id": "PjiRSyUt7e", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The settings switched off in this image are \\\"Send by Enter\\\" and \\\"Deny Friend Requests.\\\" These options are indicated by grayed-out toggle switches, while the options that are switched on are indicated by green-colored toggle switches.\\n\\nThe settings that are switched off by the user are: 1. Notifications and Sounds 2. Language 3. Send by Enter 4. Speaker 5. Preload Videos 6. Share Read Reports 7. Deny Friend Requests 8. Last Seen 9. Block List.\\n\\nIn the image, the user has switched off the \\\"Last Seen\\\" and \\\"Everybody\\\" settings. The \\\"Last Seen\\\" setting is turned off, which means the user's last seen time will not be visible to others. The \\\"Everybody\\\" setting is also turned off, which means the user's messages will not be visible to everyone, and the user will not receive messages from everyone.\\n\\nIn the image, there is a cell phone with several settings turned off. These settings include the speaker, preloaded videos, and instant video. The speaker setting is turned off, which means that the user may not be able to hear notifications or ringtones. The preloaded videos setting is also turned off, which means that the user may not be able to watch videos that were pre-installed on the device. Lastly, the instant video setting is turned off, which means that the user may not be able to use the instant video feature on the device.\\n\\nThe \\\"Deny Friend Requests\\\" setting is switched off by the user. This is a partially correct response. When given this task, ShareGPT-4V-7B hallucinates the answer.\"}"}
{"id": "PjiRSyUt7e", "page_num": 54, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The first line reads \\\"SATYRICONPDX.COM\\\" with the letters in red. If we filter out the letters in red, the remaining letters spell \\\"SATICONDX.COM\\\".\\n\\nLack of fine-grained visual perception\"}"}
{"id": "PjiRSyUt7e", "page_num": 55, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instruction:\\n\\nGet the number of the boat with three yellow and one red round items hanging from it.\\n\\nReference Response:\\n\\nWH97\\n\\nHuman Response:\\n\\n97\\n\\nGPT-4V Response:\\n\\nThe boat with three yellow and one red round items (which are likely buoys) hanging from it has the number SS 170.\\n\\nLack of fine-grained visual perception\\n\\nGemini Pro-Vision Response:\\n\\nSS 273\\n\\nLack of fine-grained visual perception\\n\\nLLaV A-Next-34B:\\n\\nThe boat with three yellow and one red round items hanging from it is the one with the number \\\"SS 681\\\" on it.\\n\\nLack of fine-grained visual perception\\n\\nShareGPT-4V-7B Response:\\n\\nThe boat with three yellow and one red round items hanging from it is SS138.\\n\\nLack of fine-grained visual perception\\n\\nLLaV A-1.5-13B Response:\\n\\nThe boat with three yellow and one red round items hanging from it is the one with the number \\\"S527\\\" on it.\\n\\nLack of fine-grained visual perception & Hallucination\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response:\\n\\nThe instruction doesn't specify which boat has three yellow and one red round items hanging from it, so it's impossible to provide the number of that boat based on the provided OCR text and image caption.\\n\\nLack of context-sensitive text-rich visual reasoning\\n\\nFigure 52.\\n\\nIn this task, all LMMs produce the wrong response due to lack of fine-grained visual perception.\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM approach) produces the wrong answer, due to lack of context-sensitive text-rich visual reasoning.\"}"}
{"id": "PjiRSyUt7e", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nNaivation Group gates by direction.\\n\\nShopping Calculate the total cost of assembling an outfit consisting of a white dress, heeled sandals, and sunglasses.\\n\\nMiscellaneous Natural Scenes Determine the word obfuscated by the outfielder.\\n\\nAbstract Identify a common proverb portrayed in this image.\\n\\nTime Comment whether the analog clocks are correctly matched to the digital clocks.\\n\\nWeb Usage Which items have experienced a price drop?\\n\\nInfographic Which countries have had a non-zero containment index that lies between 85-100?\\n\\nApplication Usage List the exercises where the corresponding illustration showcases a single movement.\\n\\nFigure 2. A sample (image, instruction) from each of the 8 visual scenarios in CONTEXTUAL dataset. The categories organized in a left-to-right, top-to-bottom reading order include Navigation, Shopping, Miscellaneous Natural Scenes, Abstract, Time, Web Usage, Infographic, and Application Usage.\\n\\nHuggingFace, 2023; Ye et al., 2023a). Many real-world images contain texts within them which provides cues for comprehensively understanding them. The ability to reason about the interactions between the text and visual context in the images powers many real-world applications. For example, interpreting text-rich scenes (e.g., navigating maps in public spaces) for assisting the visually impaired, and creative understanding of abstract text-rich images (e.g., memes).\\n\\nPrevious datasets such as TextVQA (Singh et al., 2019), STVQA (Singh et al., 2019), ESTVQA (Wang et al., 2020) have been proposed to assess the visual reasoning ability of multi-modal models over text-rich images. However, these datasets focused on accessing the OCR capability of the models to read the text in the image, and they usually do not require the model to capture the visual context in the image to answer the question. For example, in Figure 1a, we highlight an example from the ESTVQA dataset. Here, we show that a high accuracy OCR of the images (e.g, \u2018Angelo\u2019s Car Hop Service Hamburgers Laundromat\u2019) has sufficient signal to answer the question (e.g., \u2018What can we eat here?\u2019).\\n\\nThough accessing the OCR capability is important, these examples do not test the unique potential of the LMMs to jointly reason over the embedded text and visual context in the image.\\n\\nTo evaluate multimodal models\u2019 capability of jointly reasoning over embedded text and visual context in text-rich images, we propose CONTEXTUAL, a Context-sensitive Text-rich visual reasoning dataset consisting of 506 challenging instructions for LMMs evaluation. CONTEXTUAL covers eight real-world scenarios with text-rich images: time reading, shopping, navigation, abstract scenes, mobile application, webpages, infographics, and miscellaneous natural scenes (Figure 2). The diverse visual nature of these categories enables us to conduct a detailed, nuanced evaluation of the model\u2019s capabilities.\\n\\nEach instance of CONTEXTUAL contains a human-written instruction (question or imperative task) and a human-written ground-truth response (\u00a72), with the constraint that to respond to an instruction accurately, a model must require context-sensitive joint reasoning over the textual and visual cues in the image. Figure 1a shows an example from our dataset. The instruction ('Get the number of the boat with three yellow and one red round items hanging from it.') cannot be answered even by perfectly capturing the OCR of the text content within the image (e.g., \u2018SS273 WH97 SS266 SS681 SS138\u2019). We summarize our work compared to the related works in Table 1.\\n\\nWe conduct extensive experiments using CONTEXTUAL to assess the reasoning abilities of 14 foundation models over context-sensitive text-rich images (\u00a73.1). This includes three augmented LLMs setups (e.g., GPT-4 (OpenAI, 2023a)).\"}"}
{"id": "PjiRSyUt7e", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison with related works for evaluating large multimodal models. We abbreviate Context-sensitive as Consens., Generation as Gen. We compare our work with LLaVA (Liu et al., 2023b), VisIT (Bitton et al., 2023), (Singh et al., 2019), STVQA (Biten et al., 2019), DUDE (Van Landeghem et al., 2023), InfoVQA (Mathew et al., 2022), and SEED Bench (Li et al., 2023a).\\n\\n|                | Ours | LLaVA | VisIT | TextVQA | STVQA | DUDE | InfoVQA | SEED |\\n|----------------|------|-------|-------|---------|-------|------|---------|------|\\n| Consens.       |      | 3     | 7     | 7       | 7     | 7    | 7       | 7    |\\n| Text-Rich Visual Reasoning |      |       |       |         |       |      |         |      |\\n| Text in Images |      | 3     | 7     | 7       | 3     | 3    | 3       | 3    |\\n| Number of LLM/LMM Models | 13   | 3     | 10    | -       | -     | -    | 9       | -    |\\n| Number of Images | 506  | 24    | 574   | 28.4K   | 23K   | 5K   | 5.4K    | 19K  |\\n| Diverse Image Sources |      | 3     | 7     | 3       | 7     | 3    | 7       | 7    |\\n| Question Instructions |      | 3     | 3     | 3       | 3     | 3    | 3       | 3    |\\n| Imperative Instructions |      | 3    | 7     | 3       | 7     | 3    | 7       | 7    |\\n| Instruction Gen. by Humans |      | 3    | 3     | 3       | 3     | 3    | 3       | 3    |\\n| Reference Response Gen. by Humans |      | 3   | 7     | 3       | 3     | 3    | 3       | 7    |\\n| Human Evaluation |      | 3     | 7     | 3       | 7     | 3    | 7       | 7    |\\n| Automatic Evaluation |      | 3    | 3     | 3       | 3     | 3    | 3       | 3    |\\n| Human-Auto Eval. Correlation |      | 3    | 7     | 3       | 7     | 7    | 7       | 7    |\\n| Human performance |      | 3     | 7     | 7       | 3     | 7    | 3       | 7    |\\n| Absolute Score to Models |      | 3    | 3     | 3       | 3     | 3    | 3       | 3    |\\n| Fine-grained Analysis |      | 3    | 7     | 3       | 7     | 7    | 3       | 7    |\\n\\nPrompted with combinations of image OCR, image layouts, and image captions, two proprietary LMMs (e.g., GPT-4V (OpenAI, 2023b), Gemini-Pro-Vision (Team et al., 2023)), and nine open LMMs (e.g., LLaVA-Next (Liu et al., 2024), LLaVA-1.5 (Liu et al., 2023a), ShareGPT-4V (Chen et al., 2023), Idefics (HuggingFace, 2023)). In addition, we perform few-shot experiments for a selected set of models (e.g., Gemini-Pro-Vision, Idefics) to analyze the effect of in-context examples on the model's performance. Further, we establish a human baseline by asking human annotators to write responses to the dataset instructions. Finally, we perform human and automatic evaluations to assess the correctness of the predicted responses with respect to the ground-truth responses in the dataset (\u00a73.2, \u00a73.3).\\n\\nThrough human evaluations on randomly selected 280 instances, we find that GPT-4V(ision) is the best performing LMM on the CONTEXTUAL dataset where it achieves 49.3% acceptance rating to its generated responses (Figure 1b). Despite this, the performance lags way behind the human baseline of 80.1% which indicates a large gap in the capabilities of the GPT-4V. In addition, we find that the best performing open-model, ShareGPT-4V-7B, only achieves 21.8% rating which indicates that the capabilities of open models are way behind the proprietary models on context-sensitive text-rich visual reasoning (\u00a73.3). Our results highlight that the CONTEXTUAL is a challenging dataset for modern LMMs while humans excel on it.\\n\\nSince human evaluations are hard to scale and expensive, we also perform automatic evaluation (e.g., GPT-4, GPT-4V, BLEURT (Sellam et al., 2020)) on the complete dataset for all the models (\u00a73.3.1). Further, we perform fine-grained experiments to assess the model's performance across visual contexts (\u00a73.4). We observe that GPT-4V, the best-performing LMM, struggles with time reading and infographic visual contexts, except for abstract contexts like memes and quotes, where it outperforms humans. On the other hand, open models lag behind proprietary ones across most visual tasks, showing moderate proficiency only in abstract and natural scenes, owing to the need for more diversity of visual context in training data. However, we observe significant improvement in model performance with enhancement in image encoders, as seen with LLaVA-Next over LLaVA-v1.5. Lastly, we conduct a qualitative analysis (\u00a74) of model responses for the different visual contexts in CONTEXTUAL, revealing that both proprietary and open models exhibit a limited capacity for fine-grained visual perception, with open models performing worse.\\n\\n2. The CONTEXTUAL Dataset\\n\\n2.1. Collection Guidelines\\n\\nWe note that there is a notable gap in the existing benchmarks for text-rich images, which primarily evaluate text reading capabilities of LMMs. Our dataset bridges this gap and offers an evaluation framework to test the joint reasoning capabilities of the LMMs over the embedded text and the visual features in the image (Figure 1(b)). Our dataset encompasses a variety of tasks across diverse natural and digital text-rich visual scenarios, enabling robust testing. Broadly, our benchmark follows these key collection guidelines: (a) Each sample consists of an <image, instruction, response> triplet, such that the instructions require the models to perform context-sensitive reasoning over the text and visual elements in the image. Specifically, we would avoid creating instructions that could be answered by text-based reasoning (e.g., using LLM) over the detected OCR. (b) We aim to cover diverse instructions, including questions and imperative tasks. This ensures that the resulting dataset demonstrates a rich variety of instructions. (c) We aim to create instructions of varied complexity.\"}"}
{"id": "PjiRSyUt7e", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nFigure 3. Top 20 Most frequently occurring verbs (inner circle) and their top 4 direct nouns (outer circle) in the instructions. Specifically, they can make extractive instructions that involve extraction of a specific textual or visual elements (e.g., \u2018Name the drink with banana flavor.\u2019). In addition, they can make instructions that involve reasoning over the embedded information (e.g., \u2018count the number of words in the rendered in the blue color.\u2019).\\n\\n| Statistic                      | Number |\\n|-------------------------------|--------|\\n| Total number of samples       | 506    |\\n| Root verbs in instructions    | 79     |\\n| # Visual Contexts             |        |\\n| - Time                        | 50     |\\n| - Shopping                    | 50     |\\n| - Navigation                  | 50     |\\n| - Abstract                    | 50     |\\n| - Application Usage           | 50     |\\n| - Web Usage                   | 50     |\\n| - Infographic                 | 50     |\\n| - Miscellaneous Natural Scenes| 156    |\\n| Average Instruction Length    | 65     |\\n| Average Response Length       | 117    |\\n\\nTable 2. Key Statistics of CONTEXTUAL.\\n\\nIn this work, we establish a taxonomy by categorizing a dataset into eight distinct visual scenarios, encompassing real-world and digital human interactions. These scenarios include shopping, navigation, time, mobile and web usage, infographics, abstract scenes, and miscellaneous natural scenes. More details are available in Appendix \u00a7.\\n\\n2.2. Data Sources\\n\\nCONTEXTUAL comprises images sourced from six different sources. Firstly, we obtain images for the Time, Shopping, Navigation & Abstract categories from the LAION-5B (Schuhmann et al., 2022). Specifically, we use keyword search using CLIP-retrieval UI (Beaumont, 2022). A keyword consists of category-specific word(s) + \u201ctext\u201d (e.g., clothes text for shopping, airports text for navigation). Some category-specific words we used are: shopping (e.g., grocery, furniture, gadgets, cosmetics, services, clothes), navigation (e.g., street signs, cars, buses, trains, metro, airport, stations, highways, roads), time (e.g., clocks, multiple clocks, digital clocks, calendars), and Abstract (e.g., memes, quotes, comic strips, science jokes, math jokes).\\n\\nSecondly, we source images for the Application Usage category from the Rico Dataset (Deka et al., 2017), which includes 66,000 distinct UI screens originating from 9,300 Android apps. Thirdly, we scrape the website links made available by the Open WebText Initiative (Gokaslan & Cohen, 2019) and collect screenshots for the Web Usage category. Lastly, we acquire images from the test sets of existing VQA datasets, and proceed to annotate them with novel instruction-response pairs. Originally, these datasets consist question-and-answer pairs that primarily focus on text recognition capabilities. However, they offer an opportunity to formulate context-sensitive instructions for the images. Specifically, we reannotate these image instances, categorizing them into two groups: Infographic, sourced from the InfographicVQA (Mathew et al., 2022) dataset, and Miscellaneous Natural Scenes, sourced from the STVQA (Biten et al., 2019) and ESTVQA (Wang et al., 2020) datasets.\\n\\n2.3. Data Annotation\\n\\nStage 1: In this stage, we shortlist images for instruction-response pair annotation. The images that are categorized under Time, Shopping, Navigation, and Abstract undergo manual filtering to guarantee their suitability for annotation. However, for Application Usage, Web Usage, Infographic, and Miscellaneous Natural Scenes, we perform heuristic-based filtering. Specifically, we employ a PaddleOCR (paddlepadle, 2023) to detect the text in the image. Subsequently, we select the top 500 images with the highest number of words, a subset of which get annotated in our dataset.\\n\\nStage 2: We divide the authors into two groups, namely Group 1 and Group 2, each responsible for annotating four specific categories. The authors strictly adhered to the provided annotation guidelines throughout the annotation.\\n\\nStage 3: In this final stage, we perform a verification process for each sample annotated in Stage 2. We asked MTurk workers (mutually exclusive from the ones used for human performance baseline and human evaluation) to verify the correctness of each sample <image, instruction, response > and found that 96% of the samples were annotated correctly. Filtering out the incorrect samples, we tasked each author group to review the samples created by the other group. This ensured adherence to guidelines and filtered out low-quality samples. Finally, we observe that MTurk workers found this task time-consuming, leading to annotations that would be hard to accomplish within a limited budget.\"}"}
{"id": "PjiRSyUt7e", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We end up with a dataset of 506 instances. We provide statistics for the CONTEXTUAL benchmark, as shown in Table 2. We visualize each instruction based on its root verb and the direct noun, as shown in Figure 3. We also annotate each sample to determine whether it requires information extraction, and mathematical reasoning (Appendix \u00a7G.1). We provide details on data release in \u00a7C.\\n\\n3. Experiments\\n\\n3.1. Setup\\n\\nAugmented LLMs. Since our dataset is focused on text-rich visual reasoning, it is imperative to understand the extent to which a strong LLM GPT-4 can perform on CONTEXTUAL dataset with the OCR information and image captions (Lu et al., 2023b; Wu et al., 2023; Sur\u00eds et al., 2023; Gupta & Kembhavi, 2023). To this end, we study this augmented setup under three settings: GPT-4 prompted with (a) vanilla OCR, (b) layout-aware OCR, and (c) combining layout-aware OCR with image captions. We leverage the PP-OCRv4 model of PaddleOCR library (paddlepadle, 2023) for extracting OCR from the images, LATIN prompt (Wang et al., 2023a) inspired OCR text arrangement implementation to maintain layout-awareness in the OCR, and ShareGPT-4V-7B for the dense image captions (App. \u00a7E).\\n\\nLMMs. We evaluate GPT-4V (OpenAI, 2023b) and Gemini-Pro-Vision (Team et al., 2023) that are representative proprietary LMMs that have achieved state-of-the-art on other visual reasoning benchmarks (Goyal et al., 2017).\\n\\nIn addition, we evaluate a wide range of open LMMs including LLaVA-Next-34B (Liu et al., 2024), LLaVA-1.5-13B (Liu et al., 2023a), ShareGPT-4V-7B (Chen et al., 2023), mPLUG-Owl-v2-7B (Ye et al., 2023a; b), Qwen-VL-7B (Bai et al., 2023), InstructBLIP-Vicuna-7B (Dai et al., 2023), and Idefics-9B (HuggingFace, 2023). We include LLaVA (Zhang et al., 2023) and BLIVAR-Vicuna-7B (Hu et al., 2023) as they were introduced for text-rich visual reasoning.\\n\\nHumans. We also benchmark the performance of humans on our dataset using Amazon Mechanical Turk. The selected annotators that pass an qualification test were asked to write accurate responses for all the instruction-image from the dataset. We provide the screenshot of our annotation interface in Appendix \u00a7B.1. We spent $180 on collecting human predictions on our dataset.\\n\\n3.2. Evaluation\\n\\n3.2.1. Human Evaluation\\n\\nTo perform a faithful evaluation of the predicted responses, we ask human annotators sourced from Amazon Mechanical Turk to rate the predicted response quality given the image, instruction, and reference response from our dataset. First, we sample 280 instances from the dataset randomly from the CONTEXTUAL dataset. Second, we collect the model responses for these instances from augmented LLM (GPT-4 with layout-aware OCR and image captions), GPT-4V, Gemini-Pro-Vision, LLaVA-1.5-13B, ShareGPT-4V-7B, and humans. In total, we have 1680 predicted responses from models and humans. Third, we show each model response, without revealing the model identity, to three human annotators independently. Specifically, the human annotators are asked to decide the predicted response is acceptable given the reference response, instruction and image from the dataset. Finally, we report the acceptance rating (0-100 in percentage) of the responses using the majority vote among the three annotator as the final decision. We spent $1000 in acquiring human judgments.\\n\\n3.2.2. Automatic Evaluation\\n\\nWhile human evaluation acts as a gold standard, it is hard to scale since it is expensive and time-taking. Since our dataset uniquely provides reference response for each instruction, we utilize test a wide range of reference-guided automatic evaluation methods. Specifically, these include (a) prompting an LLM GPT-4 with the instruction, reference response and predicted response, (b) prompting an LMM GPT-4V with the image, instruction, reference response and predicted response, (c) and other text generation methods like BLEURT (Sellam et al., 2020), Rouge-L (Lin, 2004) and BERTScore (Zhang et al., 2019) that assess the similarity between the reference response and predicted response. Specifically, GPT-4 and GPT-4V are prompted to provide their judgement on the predicted response, same as human evaluation. We present the prompt for GPT-4 based evaluation in Appendix \u00a7F. The other text generation methods provide a continuous score 0-1 which is scaled to 0-100. Through our automatic evaluation methods, we evaluate all the model responses on the entire dataset. Subsequently, we conduct a correlation analysis between human and automated methods, utilizing the same 1,680 responses from the human evaluation, to assess the efficacy of the automated approaches (\u00a73.3.1). Finally, we utilize the GPT-4 automatic evaluation, that achieves the highest correlation with human judgments, for large-scale evaluation of all the models on the complete dataset (\u00a73.4).\\n\\n3.3. Results\\n\\nWe compare the performance of augmented LLM, LMMs, and humans on CONTEXTUAL using human and automatic evaluation in Table 3. Through our human evaluations, we find that the humans perform the best on the dataset with the response acceptance rating of 80.1%.\"}"}
{"id": "PjiRSyUt7e", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nUS Defense Advanced Research Projects Agency (DARPA), and UCLA-Amazon Science Hub for Humanity and Artificial Intelligence. Hritik Bansal is supported in part by AFOSR MURI grant FA9550-22-1-0380.\\n\\nImpact Statement\\n\\nCONTEXTUAL is proposed to evaluate the context-sensitive text-rich visual reasoning capabilities of large multimodal models. These models are a class of generative models provide textual response to user instructions, grounded in text, for diverse images. During our data collection, we aim to ensure that the images, human-written instructions, and reference responses are not offensive to any social group. We are aware that the existing multimodal models are capable of generating harmful responses, despite the presence of safeguard filter. In addition, our qualitative analysis reveals that the model responses would hallucinate, however, we did not observe any apparent harmful and privacy sensitive information in them.\\n\\nIn our experiments, we asked human annotators, mainly from the US, to provide responses to establish a human baseline. We are aware that the linguistic diversity and writing style of the human responses would change with different social groups. The extension of our work can focus on understanding the impact of different social groups on the CONTEXTUAL dataset.\\n\\nA similar argument is relevant for human evaluation of the model responses. To obtain more reliable human evaluation results, future work would involve annotators from more diverse regions.\\n\\nReferences\\n\\nAntol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. VQA: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pp. 2425\u20132433, 2015.\\n\\nAwadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, S., et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.\\n\\nBai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.\\n\\nBeaumont, R. Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with them. https://github.com/rom1504/clip-retrieval, 2022.\\n\\nBiten, A. F., Tito, R., Mafla, A., Gomez, L., Rusinol, M., Valveny, E., Jawahar, C., and Karatzas, D. Scene text visual question answering. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 4291\u20134301, 2019.\\n\\nBitton, Y., Bansal, H., Hessel, J., Shao, R., Zhu, W., Awadalla, A., Gardner, J., Taori, R., and Schimdt, L. Visit-bench: A benchmark for vision-language instruction following inspired by real-world use. arXiv preprint arXiv:2308.06595, 2023.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3558\u20133568, 2021.\\n\\nChen, J., Li, T., Qin, J., Lu, P., Lin, L., Chen, C., and Liang, X. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. arXiv preprint arXiv:2212.02746, 2022.\\n\\nChen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.\\n\\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.\\n\\nDeka, B., Huang, Z., Franzen, C., Hibschman, J., Afergan, D., Li, Y., Nichols, J., and Kumar, R. Rico: A mobile app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, pp. 845\u2013854, 2017.\\n\\nGao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., Li, H., and Qiao, Y. Llama-adapter v2: Parameter-efficient visual instruction model, 2023.\\n\\nGokaslan, A. and Cohen, V. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019.\\n\\nGoyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904\u20136913, 2017.\"}"}
{"id": "PjiRSyUt7e", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nGuan, T., Liu, F., Li, X. W. R. X. Z., Wang, X. L. X., Yacoob, L. C. F. H. Y., and Zhou, D. M. T. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models. arXiv e-prints, pp. arXiv\u20132310, 2023.\\n\\nGupta, T. and Kembhavi, A. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14953\u201314962, 2023.\\n\\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3608\u20133617, 2018.\\n\\nHu, W., Xu, Y., Li, Y., Li, W., Chen, Z., and Tu, Z. Bliva: A simple multimodal llm for better handling of text-rich visual questions. arXiv preprint arXiv:2308.09936, 2023.\\n\\nHuggingFace. Introducing idefics: An open reproduction of state-of-the-art visual language model. https://huggingface.co/blog/idefics, 2023.\\n\\nHwang, E. and Shwartz, V. Memecap: A dataset for captioning and interpreting memes. arXiv preprint arXiv:2305.13703, 2023.\\n\\nKembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., and Farhadi, A. A diagram is worth a dozen images. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pp. 235\u2013251. Springer, 2016.\\n\\nKiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A., Ringshia, P., and Testuggine, D. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances in neural information processing systems, 33:2611\u20132624, 2020.\\n\\nKim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S., Han, D., and Park, S. Ocr-free document understanding transformer. In European Conference on Computer Vision, pp. 498\u2013517. Springer, 2022.\\n\\nLee, K., Joshi, M., Turc, I. R., Hu, H., Liu, F., Eisenschlos, J. M., Khandelwal, U., Shaw, P., Chang, M.-W., and Toutanova, K. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In International Conference on Machine Learning, pp. 18893\u201318912. PMLR, 2023.\\n\\nLi, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a.\\n\\nLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang, K.-W. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.\\n\\nLi, Y., Wang, L., Hu, B., Chen, X., Zhong, W., Lyu, C., and Zhang, M. A comprehensive evaluation of gpt-4v on knowledge-intensive visual question answering. arXiv preprint arXiv:2311.07536, 2023b.\\n\\nLin, C.-Y. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74\u201381, 2004.\\n\\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740\u2013755. Springer, 2014.\\n\\nLiu, F., Piccinno, F., Krichene, S., Pang, C., Lee, K., Joshi, M., Altun, Y., Collier, N., and Eisenschlos, J. M. Matcha: Enhancing visual language pretraining with math reasoning and chart derendering. arXiv preprint arXiv:2212.09662, 2022.\\n\\nLiu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023a.\\n\\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023b.\\n\\nLiu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., and Lee, Y. J. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl.github.io/blog/2024-01-30-llava-next/.\\n\\nLiu, Y., Li, Z., Li, H., Yu, W., Huang, M., Peng, D., Liu, M., Chen, M., Li, C., Jin, L., et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023c.\\n\\nLu, P., Gong, R., Jiang, S., Qiu, L., Huang, S., Liang, X., and Zhu, S.-C. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021a.\\n\\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y., Zhang, W., Yu, Z., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021b.\\n\\nLu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of...\"}"}
{"id": "PjiRSyUt7e", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nfoundation models in visual contexts.\\n\\narXiv preprint arXiv:2310.02255, 2023a.\\n\\nLu, P., Peng, B., Cheng, H., Galley, M., Chang, K.-W., Wu, Y., N., Zhu, S.-C., and Gao, J. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023b.\\n\\nMasry, A., Kavehzadeh, P., Do, X. L., Hoque, E., and Joty, S. Unichart: A universal vision-language pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761, 2023.\\n\\nMathew, M., Karatzas, D., and Jawahar, C. Docvqa: A dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 2200\u20132209, 2021.\\n\\nMathew, M., Bagal, V., Tito, R., Karatzas, D., Valveny, E., and Jawahar, C. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1697\u20131706, 2022.\\n\\nMishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pp. 947\u2013952. IEEE, 2019.\\n\\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023a, 2023b.\\n\\nOpenAI. Gpt-4v(ision) system card, 2023b.\\n\\nhttps://openai.com/research/gpt-4v-system-card, 2023b.\\n\\npaddlepadle. Paddleocr: Multilingual ocr toolkit based on paddlepaddle. https://github.com/PaddlePaddle/PaddleOCR, 2023.\\n\\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.\\n\\nSellam, T., Das, D., and Parikh, A. P. Bleurt: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696, 2020.\\n\\nSingh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8317\u20138326, 2019.\\n\\nSur\u00eds, D., Menon, S., and Vondrick, C. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.\\n\\nTan, H. and Bansal, M. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019.\\n\\nTeam, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\\n\\nVan Landeghem, J., Tito, R., Borchmann, \u0141., Pietruszka, M., Joziak, P., Powalski, R., Jurkiewicz, D., Coustaty, M., Anckaert, B., Valveny, E., et al. Document understanding dataset and evaluation (dude). In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 19528\u201319540, 2023.\\n\\nVyalla, S. R. and Udandarao, V. Memeify: A large-scale meme generation system. In Proceedings of the 7th ACM IKDD CoDS and 25th COMAD, pp. 307\u2013311. 2020.\\n\\nWang, W., Li, Y., Ou, Y., and Zhang, Y. Layout and task aware instruction prompt for zero-shot document image question answering. arXiv preprint arXiv:2306.00526, 2023a.\\n\\nWang, X., Liu, Y., Shen, C., Ng, C. C., Luo, C., Jin, L., Chan, C. S., Hengel, A. v. d., and Wang, L. On the general value of evidence, and bilingual scene-text visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10126\u201310135, 2020.\\n\\nWang, X., Hu, Z., Lu, P., Zhu, Y., Zhang, J., Subramaniam, S., Loomba, A. R., Zhang, S., Sun, Y., and Wang, W. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635, 2023b.\\n\\nWang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705, 2022.\\n\\nWu, C., Yin, S., Qi, W., Wang, X., Tang, Z., and Duan, N. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.\\n\\nXu, Y., Li, M., Cui, L., Huang, S., Wei, F., and Zhou, M. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1192\u20131200, 2020.\"}"}
{"id": "PjiRSyUt7e", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nYe, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023a.\\n\\nYe, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., and Zhou, J. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257, 2023b.\\n\\nYin, D., Li, L. H., Hu, Z., Peng, N., and Chang, K.-W. Broaden the vision: Geo-diverse visual commonsense reasoning. arXiv preprint arXiv:2109.06860, 2021.\\n\\nYue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.\\n\\nZellers, R., Bisk, Y., Farhadi, A., and Choi, Y. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6720\u20136731, 2019.\\n\\nZhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.\\n\\nZhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D., and Sun, T. Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107, 2023.\\n\\nZou, X., Dou, Z.-Y., Yang, J., Gan, Z., Li, L., Li, C., Dai, X., Behl, H., Wang, J., Yuan, L., et al. Generalized decoding for pixel, image, and language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15116\u201315127, 2023.\"}"}
{"id": "PjiRSyUt7e", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nTable 3. Comparison of the performance of various foundation models (augmented LLM and LMMs) and humans on a subset of CONTEXTUAL dataset (280 samples). We report the response acceptance rating using human evaluation, automatic GPT-4 and GPT-4V based evaluation. In addition, we report standard text generation quality assessment metrics including BLEURT, Rouge-L, and BERTScore. The best performance in a column is highlighted in **black** while the second best performance is highlighted in *underline*.\\n\\n|                  | Humans | GPT-4 | GPT-4V | BLEURT | Rouge-L | BERTScore |\\n|------------------|--------|-------|--------|--------|---------|-----------|\\n| GPT-4 w/ Layout-aware OCR + Caption | 17.2   | 22.2  | 17.6   | 41.3   | 22.5    | 53.9      |\\n| GPT-4V (OpenAI, 2023b) |        | 49.3  | 47.4   | 45.0   | 45.3    |           |\\n| Gemini-Pro-Vision (Team et al., 2023) | 27.8   | 40.2  | 37.1   | 42.5   | 30.1    | 58.4      |\\n| LLaV A-1.5-13B (Liu et al., 2023a) | 17.2   | 20.6  | 17.5   | 43.6   | 21.7    | 54.8      |\\n| ShareGPT-4V-7B (Chen et al., 2023) | 21.8   | 22.6  | 20.6   | 44.5   | 23.3    | 55.8      |\\n| Humans            | 80.1   |       |       |        |         | 59.8      |\\n\\nTable 4. Comparison of the human and automatic evaluation metric using ROC-AUC and spearman correlation on a subset of CONTEXTUAL dataset (280 samples, similar to Table 3). We find that the GPT-4 and GPT-4V based evaluation correlate the most with humans.\\n\\nWe observe that the GPT-4V achieves the highest acceptance rating of 49.3% in comparison with all the other models. However, this rating is quite far from the human performance which indicates that our task is quite challenging for the state-of-the-art LMMs while humans are good at it. We find that the GPT-4V outperforms Gemini-Pro-Vision by 22% highlighting a large gap in the models' text-rich visual reasoning capabilities. Further, we find that augmented LLM approach achieves a very low rating of 17.2% which indicates that the dataset instances cannot be solved without precise visual perception. Interestingly, we observe that the open-models such as LLaV A-1.5-13B and ShareGPT-4V-7B achieve poor acceptance ratings through human evaluations which indicates the presence of a large gap in their capabilities from proprietary models. This might be attributed to the differences in the model capacity, along with the scale and quality of the pretraining data.\\n\\nAs human evaluation is not scalable, we perform automatic evaluation of the model responses on the entire dataset. In Table 3, we find that the ratings of the human responses outperform those from GPT-4V by 22.2% and 23.6% using GPT-4 and GPT-4V evaluation. Like human evaluation, automatic evaluation with GPT-4 and GPT-4V highlights that the human performance on the CONTEXTUAL dataset is way higher than the best-performing LMM. Interestingly, the gap between the performance of GPT-4V and Gemini-Pro-Vision is 7.2% as per GPT4 evaluation. We still observe a large gap in the performance of the proprietary models and open LMMs. We perform fine-grained evaluation to understand the gaps in model capabilities along the various quality dimensions in \u00a73.4.\\n\\nFurthermore, we find that the BLEURT scores for humans are the highest, while GPT-4V achieves the highest score among the LMMs. Interestingly, the open models (LLaV A-1.5, ShareGPT-4V) achieve a higher BLEURT score than Gemini-Pro-Vision. We observe similar counter-intuitive trends in our Rouge-L and BERTScore based automatic evaluations. For instance, Rouge-L and BERTScore rank open models better than GPT-4V despite considering the human responses to be the best. This counter-intuitive observation might be attributed to the sensitivity of these methods to the differences in lexical variations in the reference and predicted responses (Sellam et al., 2020).\\n\\n3.3.1. CORRELATION ANALYSIS\\n\\nWe measure the correlation between the candidate automatic metrics and human judgments using ROC-AUC and spearman correlation in Table 4. Specifically, the human judgments are considered as gold standard where we assign '0' to unaccepted responses to the instructions and '1' to the accepted responses. We find that GPT-4 based evaluation achieves the highest ROC-AUC of 85.9 and spearman correlation of 0.71 amongst all the automatic evaluation metrics. In addition, we observe that GPT-4V also achieves a high correlation with the human judgments which is close to GPT-4. Specifically, GPT-4 bases its judgments on the given instruction and the reference response, whereas GPT-4V, with access to an input image, may potentially be biased. This access might lead GPT-4V to overlook the reference response and depend on the visual cues from the input image for making judgments in some cases. Finally, we observe that standard text generation metrics achieve a poor ROC-AUC and Spearman correlation in comparison to GPT-4 metrics. This corroborates the findings from the prior research (Bitton et al., 2023) that shows GPT-4 evaluation outperforms standard text generation metrics. Further, the dataset size is sufficient to get reliable confidence intervals on GPT-4 evaluation. We compared model predictions pairwise using the paired t-test at a 95% confidence interval. The comparison between LLaV A-v1.5 and GPT4v/Gemini-Pro-Vision yielded a P value < 0.0001, suggesting that the difference in the performance is statistically significant. Comparing GPT4V with Gemini-Pro-Vision resulted in a P value of 6.\"}"}
{"id": "PjiRSyUt7e", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5. Fine-grained comparison in the zero-shot performance of the foundation models and humans on the CONTEXTUAL dataset using GPT-4 evaluation. We abbreviate the average response acceptance rating as Avg., Navigation as Nav., Abstract as Abs., Application usage as App., Infographics as Info., Miscellaneous natural scenes as NS. We find that the GPT-4V outperforms all the model baselines on most of the categories while Gemini-Pro-Vision is the best on Web usage and natural scenes. The best performance in a column is highlighted in **B**lack while the second best performance is highlighted by *U*nderline.\\n\\n| Models                                      | Avg  | Nav  | Abs  | App  | Info | NS   |\\n|---------------------------------------------|------|------|------|------|------|------|\\n| Augmented Large Language Models             |      |      |      |      |      |      |\\n| GPT-4 w/ OCR                                | 15.9 | 4.0  | 10.0 | 14.0 | 30.6 | 8.0  |\\n| GPT-4 w/ Layout-aware OCR                   | 18.2 | 8.0  | 20.0 | 18.0 | 34.7 | 10.0 |\\n| GPT-4 w/ Layout-aware OCR + Caption        | 22.2 | 6.0  | 16.0 | 24.0 | 57.1 | 14.0 |\\n| Large Multimodal Models                     |      |      |      |      |      |      |\\n| GPT-4V (OpenAI, 2023b)                      | 47.4 | 18.0 | 54.0 | 48.0 | 100.0| 48.0 |\\n| Gemini-Pro-Vision (Team et al., 2023)        | 40.2 | 16.0 | 32.7 | 28.6 | 65.3 | 44.9 |\\n| LLaVA-Next-34B (Liu et al., 2024)           | 36.8 | 10.0 | 36.0 | 30.6 | 66.0 | 36.0 |\\n| ShareGPT-4V-7B (Chen et al., 2023)          | 22.6 | 0.0  | 16.0 | 20.0 | 28.6 | 20.0 |\\n| Qwen-VL-7B (Bai et al., 2023)               | 21.8 | 4.0  | 20.0 | 24.0 | 53.1 | 6.0  |\\n| LLaVA-1.5B-13B (Liu et al., 2023a)         | 20.8 | 4.0  | 10.0 | 18.0 | 44.9 | 16.0 |\\n| mPLUG-Owl-v2-7B (Ye et al., 2023a)          | 18.6 | 4.0  | 8.0  | 24.0 | 32.7 | 20.0 |\\n| LLaVA-13B (Zhang et al., 2023)              | 14.9 | 10.0 | 16.0 | 6.0  | 44.9 | 8.0  |\\n| BLIVA-Vicuna-7B (Hu et al., 2023)           | 10.3 | 2.0  | 4.0  | 14.0 | 24.5 | 4.0  |\\n| InstructBLIP-Vicuna-7B (Dai et al., 2023)   | 9.7  | 2.0  | 4.0  | 16.0 | 20.0 | 6.0  |\\n| Idefics-9B (HuggingFace, 2023)              | 7.7  | 4.0  | 2.0  | 12.0 | 12.0 | 0.0  |\\n| Humans                                      | 69.6 | 64.0 | 64.0 | 73.5 | 75.5 | 64.0 |\\n\\n0.035, also denoting statistical significance. Therefore, the differences in model performance on CONTEXTUAL are statistically significant at the 95% confidence level. As a result, we utilize GPT-4 for automatically evaluate the quality of the predicted responses.\\n\\n3.4. Fine-Grained Evaluation\\n\\nWe compare the fine-grained performance of a wide range of foundation models across different visual contexts using GPT-4 evaluation in Table 5. In our experiments, we find that GPT-4V outshines the baseline models in almost all categories. We observe that the sole exceptions are web usage and miscellaneous natural scenes contexts, where Gemini-Pro-Vision holds the lead. Notably, GPT-4V outperforms humans on reasoning over the abstract category, highlighting that it may have been tuned to reason over a lot of memes and quotes data. In addition, we observe that all the models struggle the most in the time category while humans ace it, a skill which is could be hard to learn from the training data. After time reading, the proprietary LMMs underperform on the infographics category which consists reasoning over data visualizations. Prior work (Lu et al., 2023a; Masry et al., 2023) has shown that the existing LMMs underperform humans in reasoning over charts. Further, we observe that the best performing open model LLaVA-Next-34B bridges the gap between the other open source models like LLaVA-1.5-13B and ShareGPT-4V-7B and the closed source models like Gemini-Pro-Vision. It performs the best on abstract and natural scenes, while it struggles the most on time and infographics. The relative imbalance in performance across categories can be attributed to the lack of diverse visual contexts in their training data. For instance, the COCO dataset (Lin et al., 2014) used for vision-language alignment in the open models predominantly comprises natural scenes. However, comparing it to its predecessor LLaVA-1.5-13B, improvement in visual encoding, data diversity, and LLM capacity boost performance on CONTEXTUAL. We also observe the open models specifically introduced for text-rich visual reasoning like LLaVA and BLIVA-Vicuna-7B falter on CONTEXTUAL dataset. This indicates that these models cannot reason when the instruction requires them jointly over the text content and visual context in the image. We perform additional fine-grained evaluation in Appendix \u00a7G. Overall, our fine-grained analysis aids in identifying the gaps in the existing models which would inspire the development of next generation LMMs.\\n\\n3.5. Study on Synthetically Scaling Data\\n\\nCreating synthetic data for context-sensitive text-rich visual reasoning is challenging. Automatic dataset generation using OCR and image caption data with LLMs like GPT-4, exemplified by LLaVA (Zhang et al., 2023), yields instructions solvable by OCR+LLM or basic object understanding but shows poor performance on the context-sensitive instructions, as reported in Table 5. Further, finding suitable images for joint reasoning is complex, requiring heuristic OCR filtering and manual selection. Despite these challenges, a synthetic data generation pipeline has been developed, showing model performance on synthetic data similar to human-annotated data, demonstrating the potential to scale context-sensitive instructions effectively (refer to \u00a7H for more details).\"}"}
{"id": "PjiRSyUt7e", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nInstruction: Specify the colors of the rocket, space shuttle, and external tank.\\n\\nReference Response: The rocket is depicted in grey, space shuttle is depicted in yellow, and the external tank is depicted in red.\\n\\nHuman Response: The rocket is gray. The space shuttle is yellow. The external tank is red.\\n\\nGPT-4V Response: In the image, the solid rocket boosters of the space shuttle are colored in red and light gray. The external tank is colored orange. The main body of the space shuttle is primarily white.\\n\\nSummarized Reasoning: During the pre-launch, all components are vertical. Solid rocket boosters detach when the shuttle ascends. Once the shuttle reaches space's edge, the external tank is released. The shuttle's colors remain consistent throughout these phases.\\n\\nFigure 4. Here, GPT-4V responds incorrectly to the instruction despite its logical reasoning.\\n\\nGreen denotes correct response, and red highlights response errors.\\n\\nSummarized Reasoning: rationale used by GPT-4V to answer.\\n\\nInstruction: Identify the object on the dial that shows the time 10:10.\\n\\nReference Response: The clock at 10:10 features an octopus on its dial.\\n\\nGPT-4V Response: The clock with the image of an octopus on the face is showing the time 10:10.\\n\\nLLaV A-Next-34B: The object on the dial that shows the time 10:10 is the hour hand and the minute hand.\\n\\nGPT-4 w/ Layout-aware OCR + Caption Response: The instruction does not provide enough specific information to identify the object on the dial that shows the time 10:10.\\n\\nFigure 5. GPT-4V correctly responds to the instruction. However, LLaV A-Next-34B and GPT-4 w/ Layout-aware OCR+ Caption (Augmented LLM) produce wrong responses.\\n\\n4. Qualitative Examples\\n\\nGPT-4V vs Humans. In Figure 4, we see an instance where GPT-4V provides an incorrect answer. Here, the model is asked to identify the colors of different parts of a space launch vehicle - space shuttle, external tank, and rocket thrusters. GPT-4V makes errors in color predictions but can accurately infer the diagram's information, revealing a lack of precise visual perception. We provide more examples in Appendix \u00a7I (Figures 25, 29, 33, 34, 48, 51, 52), highlights that GPT-4V's core issue lies in fine-grained perception coupled with a bias for prior visual knowledge.\\n\\nA similar analysis was presented in the prior work (Guan et al., 2023) where GPT-4V fails on the perturbed versions of common visual illusions.\\n\\nGPT-4V vs. Open LMMs and Augmented LLM. We compare the best-performing models in each category, closed-source LMM, open-source LMM, and Augmented LLM approach, that is, GPT-4V, LLaV A-Next-34B, and GPT-4 w/ Layout-aware OCR + Caption, respectively, using an example illustrated in Figure 5. GPT-4V correctly identifies the object, showcasing superior visual perception and context-sensitive text-rich visual reasoning abilities over the LLaV A-Next and Augmented LLM approach that produces the wrong response. LLaV A-Next does not ground its response to the image due to relatively poor context-sensitive text-rich visual reasoning abilities. On the other hand, the Augmented LLM approach cannot respond to this instruction because the image caption and layout-aware OCR do not provide sufficient information to reason over embedded text and visual elements in the image. We refer to Figure 28, 31, 32, 38, 42, 45, 46, 50 for more examples demonstrating instances where open models exhibit lack of context-sensitive text-rich visual reasoning, or deficiencies in fine-grained perception.\\n\\nOur analysis suggests that enhancing image encoders and increasing training data diversity can improve model perception, leading to more effective context-sensitive reasoning in text-rich visual contexts.\"}"}
{"id": "PjiRSyUt7e", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\n5. Related Work\\n\\nText-Rich Image Understanding. Recently, there has been a growing interest in understanding the interactions between the text and visual elements in the image (Lee et al., 2023; Xu et al., 2020). To track the progress of the models in this field, several datasets were introduced like OCRVQA (Mishra et al., 2019), TextVQA (Singh et al., 2019), DocVQA (Mathew et al., 2021), STVQA (Biten et al., 2019), ESTVQA (Wang et al., 2020). These datasets majorly focus on the ability of the models to accurately read the text in the documents or natural scene images. Prior work (Liu et al., 2023c) provides a benchmark to assess the ability of LMMs to perform accurate OCR. In comparison, we propose a new dataset, comprising a wide range of visual contexts, instruction types (questions and imperative tasks), that aims to test the LMM's ability to perform precise visual perception and complex reasoning over the visual and text elements of the image.\\n\\nVision Language Reasoning Benchmarks. Having high-quality datasets is essential to assess the progress of the fields towards building high utility models for the real-world. Traditionally, vision-language learning has focused on tasks such as visual question answering (Antol et al., 2015; Goyal et al., 2017), image captioning (Gurari et al., 2018; Lin et al., 2014) where the model primarily needs to understand the key objects and their relations. Later, several works were introduced to assess the commonsense reasoning, which requires the models to reason about the questions that require skills beyond recognition, including VCR (Zellers et al., 2019; Yin et al., 2021). In addition, there are several datasets and benchmarks that evaluate specific skills of the LMMs including math skills (Chen et al., 2022; Lu et al., 2021a; b; 2023a), world knowledge (Yue et al., 2023), and grade school science diagrams (Kembhavi et al., 2016; Wang et al., 2023b). Additionally, there are several datasets for meme understanding such as hateful memes (Kiela et al., 2020), memeifiy (Vyalla & Udandarao, 2020), and meme-cap (Hwang & Shwartz, 2023). Such works will require joint reasoning over text and visual content over the image. However, in our work, we broaden the scope and identify a breadth of visual domains that require context-sensitive text-rich visual reasoning. These include time reading, navigation and transportation in public spaces, meme and quote understanding, and shopping etc.\\n\\nLarge Multimodal Models. Prior works such as LXMERT (Tan & Bansal, 2019), VisualBERT (Li et al., 2019), X-decoder (Zou et al., 2023) learn robust vision-language representations by training on image-text data such as Conceptual captions (Changpinyo et al., 2021), COCO (Lin et al., 2014). Post-training, they will be finetuned on the specific tasks such as VQA (Antol et al., 2015). For document understanding, popular vision-language models include Pix2Struct (Lee et al., 2023), Donut (Kim et al., 2022), MatCha (Liu et al., 2022). However, the development of large language models (Brown et al., 2020; OpenAI, 2023a), trained on internet-scale text corpus, shifted the paradigm towards the development of general-purpose multimodal models. Specifically, these are vision-language generative models that can solve diverse tasks in a zero-shot manner without task-specific finetuning. Notably, these are popularly known as large multimodal models (LMMs). These include proprietary models such as GPT-4V (OpenAI, 2023b) and Gemini-Pro-Vision (Team et al., 2023). These models have achieved state-of-the-art performance on the traditional vision-language models. In the open space, the models include LLaVA (Liu et al., 2023b; a; 2024), mPLUG-Owl (Ye et al., 2023a), OpenFlamingo (Awadalla et al., 2023), Idefics (HuggingFace, 2023), LLaMA-Adapter (Gao et al., 2023), Idefics (HuggingFace, 2023). In addition, there are a class of LMMs that focus on enhanced text-rich visual reasoning capabilities including LLaVAR (Zhang et al., 2023) and BLIVA (Hu et al., 2023). In this work, we compare the performance of LMMs on the dataset. We find that the text-rich visual reasoning capabilities of the proprietary models is way superior than the open models. We also include fine-grained analysis to understand the gaps in the model performance across different visual contexts.\\n\\n6. Conclusion\\n\\nIn this work, we introduce CONTEXTUAL, a dataset for evaluating the text-rich visual reasoning in large multimodal models. Going beyond the prior efforts that focus primarily on the testing the reading skills in the visual contexts, we create novel and challenging instructions from scratch that would require the models to capture the context in which the text is presented in an image. We ask humans to solve our dataset and also use human annotators for model response evaluation. We find that the modern LMMs (proprietary and open models) struggle to perform on our dataset while humans are good at it. In summary, our dataset paves a path for assessing the progress on reasoning over text-rich images, a domain with significant real-world applications. We make the dataset and code available to the LMM community along with a continuously updated leaderboard with recent LMMs.\\n\\nAcknowledgements\\n\\nThis research is supported in part by the ECOLE program under Cooperative Agreement HR00112390060 with the\"}"}
{"id": "PjiRSyUt7e", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models\\n\\nRohan Wadhawan*\\nHritik Bansal*\\nKai-Wei Chang\\nNanyun Peng\\n\\nAbstract\\n\\nMany real-world tasks require an agent to reason jointly over text and visual objects, (e.g., navigating in public spaces), which we refer to as context-sensitive text-rich visual reasoning. Specifically, these tasks require an understanding of the context in which the text interacts with visual elements within an image. However, there is a lack of existing datasets to benchmark the state-of-the-art multimodal models\u2019 capability on context-sensitive text-rich visual reasoning. In this paper, we introduce CONTEXTUAL, a novel dataset featuring human-crafted instructions that require context-sensitive reasoning for text-rich images. We conduct experiments to assess the performance of 14 foundation models (GPT-4V, Gemini-Pro-Vision, LLaVA-Next) and establish a human performance baseline. Further, we perform human evaluations of the model responses and observe a significant performance gap of 30.8% between GPT-4V (the current best-performing Large Multimodal Model) and human performance. Our fine-grained analysis reveals that GPT-4V encounters difficulties interpreting time-related data and infographics. However, it demonstrates proficiency in comprehending abstract visual contexts such as memes and quotes. Finally, our qualitative analysis uncovers various factors contributing to poor performance including lack of precise visual perception and hallucinations. Our dataset, code, and leaderboard can be found on the project page https://contextual.github.io/.\\n\\n*Equal contribution\\n\\n1Department of Computer Science, University of California Los Angeles, USA. Correspondence to: Rohan Wadhawan <rwadhawan7@g.ucla.edu>, Hritik Bansal <hbansal@g.ucla.edu>.\\n\\nProceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\\n\\nInstruction\\n\\nWhat can we eat here?\\n\\nYou can eat hamburgers at Angelo\u2019s Car Hop Service.\\n\\nOCR\\n\\nESTVQA\\n\\nGet the number of the boat with three yellow and one red round items hanging from it.\\n\\nCONTEXTUAL\\n\\n(a) Comparing an instance of CONTEXTUAL to existing datasets (e.g., ESTVQA). CONTEXTUAL requires contextualized understanding of the interactions between the textual and visual elements in the image while ESTVQA can be solved solely through text-based reasoning combined with accurate OCR detection.\\n\\n(b) Performance of GPT-4V, Gemini-Pro-Vision, ShareGPT-4V-7B, and humans on the CONTEXTUAL dataset, with Human Evaluation (left sub-graph) and GPT4 Evaluation (right sub-graph).\\n\\nFigure 1.\\n\\nComparisons between our dataset CONTEXTUAL and prior work ESTVQA, along with benchmark performances of large multimodal models on CONTEXTUAL.\\n\\n1. Introduction\\n\\nThe recent development of large multimodal models (LMMs) has resulted in models capable of responding to human instructions, posed as questions or imperative tasks, over images (Liu et al., 2023b; Chen et al., 2023; OpenAI, 2023b; Team et al., 2023; Dai et al., 2023; Bai et al., 2023; 1\"}"}
