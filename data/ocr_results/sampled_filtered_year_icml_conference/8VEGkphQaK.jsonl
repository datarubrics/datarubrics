{"id": "8VEGkphQaK", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nFigure 3. Advantage of stepwise inference in graph navigation tasks and stitching:\\n(a) In the Bernoulli DAG, stepwise inference demonstrates an advantage over direct inference in predicting whether given node pairs are connected. (b) This advantage is further pronounced in hierarchical DAGs, where the distances between nodes are greater than in Bernoulli DAGs. (c) The stepwise inference gap arises when to generalize, the model has to flexibly recombine (sub)paths seen over pretraining, which we refer to as stitching. (d) The stepwise inference is beneficial when the model must connect paths seen during training: the red, green, and blue paths represent subsets of paths seen during training; we find the model produces paths that combine these subsets during the test phase.\\n\\n3.1.1. STEPWISE INFERENCE GAP\\n\\nFig. 3 shows the accuracy of classifying 'path' or 'no path' for two different types of graphs: a Bernoulli graph and a hierarchical graph. We observe that for both types of graphs, the use of stepwise inference significantly improves the model's performance compared to direct inference, with more pronounced improvements noted for the hierarchical graph. Following Prystawski & Goodman (2023), we refer to the improvement in performance observed between stepwise inference and direct inference as the \\\"stepwise inference gap\\\". We even simulate the effect of noisy or ambiguous real-world labels by introducing random corruptions into the tokens and found that the results above continue to hold, as detailed in Appendix Fig. 14.\\n\\nTo further probe the results above, we control for path lengths in the hierarchical graph. Specifically, to set the maximum path length in the training data to $\\\\Delta$, we choose a starting layer $l$ and a goal layer $l'$ such that $l' - l < \\\\Delta$. Then, we sample starting nodes from layer $l$ and goal nodes from layer $l'$. For the test data, we select node pairs with $l' - l \\\\geq \\\\Delta$. Results are shown in Fig. 3(c). We plot the classification accuracy across various values of $\\\\Delta$ and observe that the smaller the value of $\\\\Delta$, the greater the stepwise inference gap becomes. We hypothesize this happens because when the training data only includes short paths, the model needs to more effectively 'stitch' the paths observed during training, which, as a recursive task, is more feasible via stepwise inference.\\n\\n3.1.2. DIVERSITY-ACCURACY TRADEOFF WITH HIGHER SAMPLING TEMPERATURES\\n\\nHere, we investigate how the sampling temperature of the autoregressive Transformer affects the diversity of the generations produced by the model and its accuracy. To this end, we fixed the start and goal nodes and prompted the model 3,000 times, varying the sampling temperatures from 0.0 to 3.0. We define accuracy as the probability that a generated path consists of valid edges and correctly terminates at the designated goal node. Diversity is defined as the number of unique paths generated. As shown in Fig. 4, there is a clear trade-off between the diversity of the paths generated by the model and their accuracy. We term this phenomenon the diversity-accuracy tradeoff: at lower sampling temperatures, the model generates fewer but more accurate and valid paths; in contrast, higher sampling temperatures result in greater path diversity but reduced accuracy. Our result provides the first explicit demonstration of a trade-off between the accuracy and diversity of Transformer outputs. To the best of our knowledge, this phenomena has not been quantitatively studied before.\\n\\n3.1.3. REFERENCE FOR SHORTER PATHS\\n\\nNote that there are multiple possible paths the model can choose from in the pursuit of inferring a path that connects a start and goal node. We showed that by increasing the\"}"}
{"id": "8VEGkphQaK", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\naverage path length between nodes\\nmodel generated path length\\n\\nFigure 5.\\n\\nModel outputs are biased toward shorter paths. Comparing the average lengths of ground-truth paths for a specific set of node pairs and the paths produced by the model for these same pairs in the Bernoulli DAG, we observe that the model tends to generate shorter paths than the actual ones. This observation points to a \u201csimplicity bias\u201d in the trained model towards favoring shorter over potentially more accurate or realistic paths.\\n\\nsampling temperature, a diverse set of paths can be generated; however, by default, which path does the model prefer? To evaluate this, we compare the true path lengths in the Bernoulli graph between nodes in the test set with those generated by trained model. In Fig. 5a, we observe that the model consistently produces paths that are shorter, on average, than the paths in the ground truth DAG. This observation suggests that the model exhibits a simplicity bias; this finding is likely an architectural bias of the transformer trained with gradient descent since it is not present in the training data. However, simplicity biases have been shown to yield oversimplification of a problem, forcing a model to learn spurious features (Shah et al., 2020; Lubana et al., 2023). In the context of stepwise inference, this can amount to omission of important intermediate steps, similar to \u2018shortcut solutions\u2019 arising from pattern-matching behaviors discussed in prior work on Transformers (Liu et al., 2022; Dziri et al., 2023).\\n\\n3.1.4. Evolution of Failures in Stepwise Inference Over Training\\n\\nIn the above discussion, we evaluated how stepwise inference assists a model in successfully completing a complex, multi-step task. We now assess how it fails. Specifically, assume that for a given graph \\\\( G \\\\), the model produces a sequence of nodes \\\\( X \\\\) starting at the start node \\\\( X_s \\\\). Following (Saparov & He, 2023; Momennejad et al., 2023), we define two categories of potential failures.\\n\\n- **Misstep** \\\\( (X_k, X_{k+1}) \\\\in G \\\\): An edge produced by the model does not exist in the DAG (\u201challucinations\u201d).\\n- **Planning failure** \\\\( X_t \\\\neq X_g \\\\): The model produces a path that fails to reach the intended goal node.\\n\\nInitially, the model learns to avoid missteps. Subsequently, around the 200th optimization step, it begins to effectively learn planning. The accuracy curves are averaged over three models, each trained with a distinct random seed.\\n\\nIn Fig. 6, we examine the learning dynamics for each failure mode. The figure indicates that the model initially acquires the skill to circumvent missteps (the blue line). Subsequently, it develops the ability to plan effectively, which is shown by a decrease in planning failures (the red line). By integrating these abilities\u2014avoiding missteps and minimizing planning failures\u2014the model is finally able to generate accurate paths for node pairs not seen during training.\\n\\n3.1.5. Mechanistic Basis of the Learned Graph Navigation Algorithm\\n\\nOur results above elicit several intriguing behaviors attributable to stepwise inference. We next take a more mechanistic lens to explain why these behaviors possibly occur. We hypothesize that the model learns embeddings for the nodes of the graph that enable easy computation of an approximate distance measure. This suggests that to move closer to the goal node, one can simply transition to the node that exhibits the least distance from the goal node. For the detailed intuition guiding our analysis, see Appendix F.\\n\\nTo verify this, we first strip the model down to a single-head, self-attention layer. We visualize the attention scores for this minimal model in Fig. 7a, observing they are concentrated on the goal node and the current node. This suggests that the model utilizes only the embedding values of the goal \\\\( X_g \\\\) and the current nodes \\\\( X_{current} \\\\) to select the next token. Inspired by this observation, we develop a simplified algorithm that mimics the behavior of the model.\"}"}
{"id": "8VEGkphQaK", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nFigure 7. Mechanistic analysis of the graph navigation algorithm: Emergent linear representation.\\n\\n(a) Attention maps from the 1-layer, attention-only Transformer, highlighting the model's attention on the goal token \\\\(X_g\\\\) and the current token \\\\(X_{current}\\\\). (b) Steps of our simplified algorithm that emulates the 1-layer, attention-only Transformer are as follows. (1.) We extract value embeddings for \\\\(X_g\\\\) and \\\\(X_{current}\\\\), ignoring other tokens for simplicity. (2.) Next, we compute the value embeddings of the goal \\\\(v_g\\\\) and current \\\\(v_{current}\\\\) nodes and add them together \\\\(v = v_g + v_{current}\\\\). (3.) We then compute the token embedding with the highest inner product with \\\\(v\\\\), approximating the token that receives the highest logit score after the single forward pass. (c) Comparison of the model's accuracy on a set of 500 held-out node pairs \\\\((X_s, X_g)\\\\) using our simplified algorithm (99.8%) versus the full trained model (99.6%). (d) The paths generated by the simplified algorithm almost exactly match the paths generated by the full trained model. Path similarity on 2000 held-out node pairs was compared by measuring the Levenshtein edit distance (Navarro, 2001) between paths generated by the full trained model and the simplified algorithm for the same \\\\((X_s, X_g)\\\\) pairs. (e) The short path bias can be attributed to the inner products between the token embedding of the next chosen token \\\\(X_{next}\\\\) and the value embedding of \\\\(X_g\\\\) and \\\\(v_g\\\\). We observe that nodes \\\\(X_{next}\\\\) further away from \\\\(X_g\\\\) have a lower inner product, indicating that the model's embedding of nodes reflects the underlying graph topology. The red line denotes the best least squares fit and has a slope of \\\\(-10.6\\\\).\\n\\nIn Fig. 7c, we demonstrate the simplified algorithm retrieved via the process above matches the accuracy of the full trained model. Furthermore, in Fig. 7d, we find that the paths generated by our simplified algorithm and those produced by the full trained model are nearly identical. Herein, we use a string edit distance metric (Navarro, 2001) to quantify the similarity between the two sets of paths and find that over 75% of paths are identical.\\n\\nGiven that accuracy is computed over test nodes not seen in the training data, the model likely encodes a notion of distance between two nodes on the graph in its embedding, as we hypothesized. Indeed, in Fig. 7e, we find that the inner product of the embedding of \\\\(v_g\\\\) with the token embeddings of \\\\(X_{next}\\\\) is negatively correlated with the distance between these two nodes in the ground truth DAG; here, we used the average path length as a distance measure over the graph. Since potential nodes with shorter paths to the goal node have a higher logit value, this implies they will be more likely to be predicted, thus showing the origin of the short path bias we observed in Sec. 3.1.3. This is a mechanistic explanation of the interpolative pattern-matching behavior of Dziri et al. (2023) in the context of our task.\\n\\n3.2. Navigation with exemplars\\n\\nThe single graph setting let us explore zero-shot navigation and stepwise reasoning, where the model relied on knowledge internalized over pretraining for stepwise navigation towards a goal. Next, we study how context can influence the model generated paths, how subgoals that are provided in-context can guide the model's navigation, and how the content of the exemplars affects the navigation path chosen by the model. Our results shed some light on and create hypotheses for (1) compositional generalization, (2) length generalization, and (3) impact of conflicting, long context.\"}"}
{"id": "8VEGkphQaK", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nSuccessful steering probability\\nNumber of intermediate motifs\\n\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n\\n, \\n,\\nn\\n1 2 3 4 5 6\\n\\nFigure 8. In-context steerability and length generalization.\\n\\nWe vary the number of intermediate motifs $g^{\\\\text{inter}}$ in a chain of motifs constructed for the particular context $g_{i1} \\\\rightarrow \\\\{g_{\\\\text{inter}} \\\\rightarrow \\\\}^n \\\\rightarrow g_{iK}$.\\n\\nThe path generated by the model follows the path described by the chain in context until $n = 4$, which is the maximum chain length in the training data. $g_2 \\\\rightarrow g_9$. We also find that the model generalizes to arbitrary orders of motifs strung out, including those that did not occur consecutively in the training data, up to the length in the training data (see Fig. 8). In other words, in-context control is capable of eliciting compositional generalization (Li et al., 2023), if appropriately trained. Further, we see that the attentional patterns used by the model suggest that while navigating across motifs, the model treats nodes across ghost edges as subgoals (see Appendix Fig. 19).\\n\\n3.2.2. NUMBER OF INTERMEDIATE MOTIFS\\n\\nIn Fig. 8, we vary the number of exemplars provided to the model. This is equivalent to stringing together a longer chain of exemplar sequences across motifs to navigate over. We define successful steering via a product of indicator variables that measure (i) whether the path ended at the specified goal and (ii) that each ghost edge, and thus the intermediate motif, was present in the path. We computed the probability by averaging over distinct source nodes from $g_{i1}$ and sink nodes from $g_{iK}$. We find that the model can generalize well to unseen orders of motifs up to the maximum number chained together in the training data, after which the model fails to navigate. We hypothesize that even when using stepwise inference methods at scale, the model will fail to generalize to reasoning chains longer than those present in its training data.\\n\\n3.2.3. PRIMACY BIAS TOWARDS THE FIRST EXEMPLAR IN THE CASE OF CONFLICT\\n\\nLanguage models are generally prompted with several exemplars in context. Some of these exemplars may have incorrect or even conflicting information with respect to other exemplars, for example in a multiple choice Q&A task (Hendrycks et al., 2020; Pal et al., 2022; Srivastava et al., 2022). The model has to choose the relevant information between these exemplars to solve the specified task.\\n\\nMotivated by this, we quantitatively study the behavior of the model when a noisy context with exemplars with conflicting information are provided. Specifically, we study a case where two chains of motifs are used to design exemplars for our task, such that the exemplars start from the same set of initial and terminal motifs $g_{i1}$ and $g_{iT}$, but with distinct intermediate motifs $g_{\\\\text{inter}}$ and $g'_{\\\\text{inter}}$. The model is then prompted with $X_s \\\\in g_{i1}$ and $X_g \\\\in g_{iT}$, after in-context exemplars in order: $e_{g_{i1}},g_{\\\\text{inter}}, e_{g_{\\\\text{inter}},g_{iT}}, e_{g_{i2}},g'_{\\\\text{inter}}, e_{g'_{\\\\text{inter}},g_{iT}}$. Results are shown in Fig. 9. We find that the model does indeed navigate to the goal, thus following the prompt, but has a strong bias toward choosing a path defined by the first chain over the second, i.e., $g_{i1} \\\\rightarrow g_{\\\\text{inter}} \\\\rightarrow g_{iT}$. This result is similar to what happens at scale with large context windows, where content in the middle of a long context window is ignored (Liu et al., 2023).\\n\\n4. Conclusion and Limitations\\n\\nIn this work, we introduced a synthetic graph navigation task to investigate the behavior, training dynamics, and mechanisms of Transformers under stepwise inference protocols. We have adopted the model-experimental systems approach, an empirical strategy to precisely characterize and understand smaller, more steerable model systems with the ultimate goal of transferring this understanding to larger-scale systems. It is important to clarify the trade-offs and limitations inherent in our approach. Drawing an analogy to the study of biological neural networks, where neural mechanisms identified in small-scale model organisms such as fruit flies may not be directly applicable to medical applications involving the human brain, our observations should not be taken as definitive conclusions directly applicable to...\"}"}
{"id": "8VEGkphQaK", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nFigure 13.\\nThe architecture of GPT-style (Radford et al., 2019) decode-only Transformers. Note the presence of both pre and post-LayerNorm in each Transformer block. Figure from methods section of Ramesh et al. (2023).\\n\\nC.3. Architecture details and loss function\\n\\nFor training, we tokenize every node and we use the standard language modeling objective, next-token prediction with a cross entropy loss. Here target \\\\( n \\\\) is the 1-shifted version of the training sequence and \\\\( x_n \\\\) are the logit outputs of the model at the \\\\( n \\\\)th timestep.\\n\\n\\\\[\\nL(x_n, \\\\text{target}_n) = -\\\\log \\\\text{softmax}(\\\\beta x_n)_{\\\\text{target}_n} \\\\tag{1}\\n\\\\]\\n\\nHyperparameter Value\\n\\n| Hyperparameter      | Value        |\\n|---------------------|--------------|\\n| learning rate       | 10\\\\(^{-4}\\\\)   |\\n| Batch size          | 64           |\\n| Context length      | 32           |\\n| Optimizer           | Adam         |\\n| Momentum            | 0.9, 0.95    |\\n| Activation function | GeLU         |\\n| Number of blocks    | 2            |\\n| Embedding dimension | 64           |\\n\\nTable 1.\\n\\nFor model architecture, we use a GPT based decode-only Transformer with a causal self-attention mask. Our implementation is based on the nanoGPT repository.\\n\\nThe Transformer architecture consists of repeated blocks of pre-LayerNorm, causal multi-head self-attention, post-LayerNorm, and an MLP with skip connections (see Fig. 13). The MLP contains two fully-connected layers with a\"}"}
{"id": "8VEGkphQaK", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nGELU non-linearity (Hendrycks & Gimpel, 2016). The dimensionality of the hidden layer of the MLP is 4x the embedding dimensionality. We do not include any dropout in the model or biases in the linear layers. We use weight-tying (Press & Wolf, 2016) in the embedding and un-embedding layers.\\n\\nTo do the mechanistic study, we consider a 1 layer attention-only Transformer without a few modifications: We remove the MLP and post-LayerNorm and vary the embedding dimensionality from four to 64. This 1L Transformer is described by the following model equations. Here $X_{\\\\text{token}} \\\\in \\\\mathbb{R}^{\\\\text{vocab size} \\\\times T}$ denotes the tokens, $W_E \\\\in \\\\mathbb{R}^{n_{\\\\text{embd}} \\\\times \\\\text{vocab size}}$ is the positional embedding matrix, $W_{\\\\text{pos}} \\\\in \\\\mathbb{R}^{n_{\\\\text{embd}} \\\\times T}$ is the token embedding matrix, and $X \\\\in \\\\mathbb{R}^{n_{\\\\text{embd}} \\\\times T}$.\\n\\n\\\\[\\nX = W_E (X_{\\\\text{token}}) + W_{\\\\text{pos}} (X_{\\\\text{token}})\\n\\\\]\\n\\n\\\\[\\nX = \\\\text{LN} (X)\\n\\\\]\\n\\n\\\\[\\nX = X + \\\\text{softmax} (X^T W^T Q W K^T X)\\n\\\\]\\n\\n\\\\[\\nz = \\\\text{softmax} (W^T E X)\\n\\\\]\\n\\n\\\\[\\n\\\\text{next token} = \\\\text{argmax} \\\\, \\\\text{all tokens} \\\\, z\\n\\\\]\\n\\nD. Training protocol for experiments\\n\\nFor experiments in our setup without exemplars, we randomly generate either a hierarchical graph or a Bernoulli graph $G$ with $N = 200$ nodes. In the Bernoulli graph setting the probability of an edge $p = 0.05$; similarly, in the hierarchical graph, the probability of an edge between a node in layer $l$ and layer $l + 1$ is $p = 0.05$. We choose 10 layers with 20 nodes each to match the number of nodes in the two graph types. We convert all the nodes to tokens, along with a special goal token which corresponds to a $[\\\\text{BOS}]$ token and an end token which corresponds to an $[\\\\text{EOS}]$ token. We use another token, pad, for padding as well.\\n\\nTrain-Test split\\n\\nTo generate training data corresponding to path connected node pairs, we first put all edges (which are simple paths of length one) into the training data. This procedure was done in all experiments to ensure that full knowledge of the graph was presented to the model. Further, we generate all simple paths between every pair of nodes in the graph. A variable fraction of these paths are included in the training data, depending on experimental conditions which we outline below.\\n\\nNavigation without exemplars: A single graph\\n\\nFor experiments in Figs. 3a\u2013b, we pick 20% of the path-connected nodes and put all simple paths between them into the training data for each graph type. We also add an equal number of non-path connected nodes to the training data.\\n\\nIn Fig. 3c, for each value of the path-length threshold parameter, which sets the maximum length of paths in the training dataset, we pick paths corresponding to 20% of the allowed path-connected node pairs and put them into the training data, while the remainder are held out evaluations. For the non-path connected pairs, we simply take all node pairs that are not path-connected and add a fraction of these node pairs into the training data, chosen to roughly balance the number of path-connected node pairs according to the experimental conditions. The remainder of the node pairs are held-out for evaluation. for Fig. 4, we choose a single held out path-connected node pair and prompt the model 3000 times to stochastically sample paths.\\n\\nNavigation with exemplars: A set of motifs\\n\\nFor the motif experiments in Fig. 8, we generate a set of 10 motifs, each with a Bernoulli graph structure of 100 nodes with a bernoulli parameter $p = 0.95$. We then divide the 45 possible pairwise motif orders ($g_i \\\\rightarrow g_j$ for all pairs $(i, j)$ of 10 motifs) into a set of 35 and 10 that we put into train and test respectively. For generating the context, we combine 3-6 motifs in a linear chain according to the allowed orders, and then sample exemplars as well as the final sequence that traverses the full motif chain by choosing start and goal nodes from the set of sources and sinks respectively.\\n\\nFor experiments in Fig. 9, we choose an initial and terminal motif $g_{i1}$ and $g_{iT}$, and distinct intermediate motifs $g_{\\\\text{inter}}$ and $g'_{\\\\text{inter}}$. We then construct the in-context exemplars in order: $e_{g_{i1}}, g_{\\\\text{inter}}, e_{g_{\\\\text{inter}}}, g_{iT}, e_{g_{i2}}, g'_{\\\\text{inter}}, e_{g'_{\\\\text{inter}}}, g_{iT}$. The model is then prompted with $X_s \\\\in g_{i1}$ and $X_g \\\\in g_{iT}$.\"}"}
{"id": "8VEGkphQaK", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nComparisons between natural language and our graph navigation setup\\n\\n\u2022 Connecting concepts to token paths. The mapping of natural language semantics onto a latent graph is an important piece of stepwise inference, but in our study we abstract this mapping away and study the capabilities of a transformer architecture trained with next token prediction. However, in this (much) simplified setting, we still uncover analogs of several phenomena at scale and also predict new phenomena, which is a valuable contribution and speaks to the fundamental limitations and properties of this task.\\n\\n\u2022 Long-range correlation in the token sequence. We included long-range correlation in our sequences in two ways. In the single graph setting, the path produced by the model must end to the goal, which is the first token in the prompt. This is exemplified in figure 5, where we tracked failure modes over training. To reiterate, what we found was that edges (short-range correlation) are learnt before paths that end in the specified goal (long range correlation). In the motif setting, the \u201cghost edge\u201d in each exemplar must be identified and reproduced in the final produced path.\\n\\n\u2022 Disambiguating correct word sense. We have incorporated a version of the ambiguity of natural language by including label noise in a fraction of the tokens. Our Appendix figure 14 shows that the main results continue to hold in this case. Further, given a (start, goal) node pair, there are several possible paths between them. This is another source of stochasticity/ambiguity that models real language, Fig. 11. Further, in our motif setup, in each exemplar path, the transition from one motif to another is not explicitly provided and thus the model has to infer this, this adds another source of ambiguity.\\n\\nGiven these observations, there are several examples not captured by our simplified setup:\\n\\n\u2022 A cyclic reasoning failure mode is not captured by the model since we have only included simple paths in the training data.\\n\\n\u2022 Further, the mapping from natural language semantics to a latent graph has been abstracted out and thus failures based on a misidentification of the graph are not captured by the model.\\n\\n\u2022 Another limitation of our work is that our graphs in both the single graph setting and motif setting are fixed over pretraining. There are scenarios where generation of a step of reasoning or decision-making can change the underlying graph dynamically. Real-world examples of this include: (1) a game-like environment where navigation can produce new information, or question-answer format of reasoning where new information is provided (2) a scientific domain where a new experimental result can introduce new entities or change existing relationships (e.g., discovering a new gene-disease association), necessitating a dynamic update of the graph to guide future experiments.\\n\\nE. Additional experimental results\\n\\nLabel noise in training data\\nIn Fig. 14, we mimic real-world language data, abundant in ambiguity and polysemy, by corrupting (a) 5%, (b) 10% and (c) 20% of tokens in a single graph scenario. To achieve this, we replaced a randomly chosen 5% and 10% of the tokens in the training data with random tokens. We observe that the gap between stepwise inference and direct inference persists in both scenarios. This finding indicates that stepwise inference remains effective in more realistic settings with noise.\\n\\nVarying edge density\\nIn Fig. 15, we swept the density of the graph from 0.08 to 0.12 on a hierarchical graph. We observe a stepwise inference gap in all cases. The stepwise inference gap becomes smaller for larger densities. This is because the more likely the nodes are to be connected, the more likely it is for shortest paths to exist between nodes and thus less \u201cstitching\u201d is needed (Broadbent & Hammersley, 1957).\\n\\nShort path bias\\nFig. 16 presents a density plot comparing the average lengths of actual paths with those generated by the model in a Bernoulli graph. This observation verifies the model tends to produce shorter paths between a given pair of start and goal nodes.\"}"}
{"id": "8VEGkphQaK", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nFigure 14. Persistence of stepwise inference gap with corrupted tokens: In this experiment with setup identical to Fig. 3a-b, (a) 5%, (b) 10% and (c) 20% of tokens were randomly corrupted to mimic real world language data. The stepwise-inference gap persists.\\n\\nFigure 15. The effect of varying edge density in the single graph scenario: Here we vary $p$, the edge density of connectivity in the graph, from 0.08 in the left-most plot to 0.12 in the right-most plot, in steps of 0.01. The stepwise inference gap persists in all cases.\\n\\nEffect of varying embedding dimensionality in the single graph scenario: Here we consider the 1-layer Transformer without MLP and post-LayerNorm and ask the following question: for a fixed underlying graph size and training data, how does the model performance vary as we sweep embedding dimensionality. Intuitively, if the embedding dimensionality is large, the model should be able to generalize better by learning a better embedding of the node tokens. We see that beyond a critical dimensionality (which is around 20 for a graph of 200 nodes), the model generalizes to all held out (start, goal) node pairs with a fairly abrupt transition (see Fig. 18).\\n\\nF. Intuition guiding the mechanistic analysis\\n\\nIn this section, we present the intuition that served as the hypothesis guiding our mechanistic analysis. Consider the optimal maximum likelihood estimator designed to solve our graph navigation task. Given a start node $X_s$ and an incomplete sequence of predicted nodes $X_1, \\\\ldots, X_k$ in the pursuit of navigating to the goal node $X_g$, the estimator...\"}"}
{"id": "8VEGkphQaK", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nFigure 17. Varying number of layers: We find that there is no qualitative change in the learning dynamics as we vary the number of layers in a transformer, keeping the data distribution fixed.\\n\\nFigure 18. Varying embedding dimensionality in 1 layer models: We find that there is a critical embedding dimensionality (around 20 for a Bernoulli graph of size 200 nodes and $p = 0.05$) above which the model can generalize to all held-out node pairs.\"}"}
{"id": "8VEGkphQaK", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nworks the following way:\\n\\n\\\\[\\nX_{\\\\text{next}} = \\\\arg \\\\max_{X'} P(X' | X_s, X_1, \\\\ldots, X_k, X_g)\\n\\\\]\\n\\nSince the task is conditionally Markovian: the choice of the next step will be independent of the history when conditioned on \\\\(X_g\\\\) and \\\\(X_k\\\\). Accordingly, we have:\\n\\n\\\\[\\nX_{\\\\text{next}} = \\\\arg \\\\max_{X'} P(X' | X_k, X_g) = \\\\arg \\\\max_{X'} P(X_g | X' , X_k) P(X' , X_k)\\n\\\\]\\n\\nThis decomposition leads to interpretable terms which shed light on what algorithm the model might use:\\n\\n\\\\[\\n\\\\log P(X_g, X_k) = \\\\log P(X_g | X', X_k) + \\\\log P(X', X_k) - \\\\log P(X_g, X_k)\\n\\\\]\\n\\nThese terms can be interpreted as follows:\\n\\n1. \\\\(\\\\log P(X_g, X_k)\\\\) describes the prompt.\\n2. \\\\(\\\\log P(X', X_k)\\\\) describes the knowledge of the world model: How well does the model know the ground truth structure of the graph?\\n3. \\\\(\\\\log P(X_g | X', X_k)\\\\) corresponds to goal-directed behavior: What \\\\(X'\\\\) is most likely to lead to the goal?\\n\\nLet \\\\(C(X_k)\\\\) denote the subset of nodes in the graph that are children of the node \\\\(X_k\\\\). Then, while selecting the next token that has the highest likelihood, note that terms (1) and (2) cannot be optimized over: the former does not depend on \\\\(X'\\\\) and the latter, for the optimal predictor, will be \\\\(1/|C(X_k)|\\\\) if \\\\(X' \\\\in C(X_k)\\\\) and 0 otherwise. Accordingly, the only term that can be optimized over is the third one, i.e., the one that measures how likely the goal is if the next state is \\\\(X'\\\\). However, due to term (2), \\\\(X' \\\\in C(X_k)\\\\)\u2014that is, the possible set of next tokens is constrained to the set \\\\(C(X_k)\\\\).\\n\\nNow, exploiting the task's conditional Markovian nature again, we have\\n\\n\\\\[\\nP(X_g | X', X_k) = P(X_g | X' : X' \\\\in C(X_k))\\n\\\\]\\n\\nHeuristically, assume that \\\\(P(X_g | X', X_k) \\\\propto e^{-d(X_g, X')} \\\\cdot I(X' \\\\in C(X_k))\\\\), where \\\\(d(X_i, X_j)\\\\) is a measure that describes the distance between nodes \\\\(X_i\\\\) and \\\\(X_j\\\\), while respecting the topology of the graph, and \\\\(I\\\\) is an indicator function that is 1 if its input is True, and 0 if not. Then, we have\\n\\n\\\\[\\n\\\\log P(X_g | X', X_k) \\\\propto -d(X_g, X') \\\\cdot I(X' \\\\in C(X_k))\\n\\\\]\\n\\nThe intuitive argument above, though likely to be approximate, suggests that a possible solution the model can learn via autoregressive training in our graph navigation setup is (i) compute the distance between all neighboring nodes of the current node and the goal node, (ii) move to the node that has the least distance, and (iii) repeat. The algorithm we uncover in our analysis in Sec. 3.1.4 in fact functions in a similar way: the model is constantly computing a inner product between the goal token's representation and the embeddings of all tokens; we find this inner product is highest for the neighbors of the current token. Then, the highest inner product token is outputted and the process is repeated. Since the embeddings are not normalized, this inner product is not exactly the Euclidean distance\u2014we expected as much, since the topology of the graph will have to be accounted for and learning an inner product based metric will be easier for a model (because most operations therein are inner products).\\n\\nFurther, in the case of motifs, we expect that the model constructs a path through checkpoints defined by ghost edges, which act as subgoals. To explain, given a set of \\\\(K\\\\) motifs strung together in-context \\\\(\\\\{g_{i1}, g_{i2}, g_{i3}, \\\\ldots g_{iK}\\\\}\\\\), we have the set of \\\\(K-1\\\\) ghost edges, one for each exemplar:\\n\\n\\\\[\\n\\\\{(X_{\\\\text{sink}}(g_{i1}), X_{\\\\text{source}}(g_{i2})), (X_{\\\\text{sink}}(g_{i2}), X_{\\\\text{source}}(g_{i3})), \\\\ldots (X_{\\\\text{sink}}(g_{iK-1}), X_{\\\\text{source}}(g_{iK}))\\\\}\\n\\\\]\\n\\nThus, we hypothesize that the model identifies all \\\\(K-1\\\\) ghost edges from its context and plans sub-paths to each ghost edge in pursuit of the goal. Preliminary analyses of attention patterns in Fig. 19 provides evidence for this hypothesis.\\n\\nF.1. Generalizing static word embeddings to 3-way relations\\n\\nStatic word embedding algorithms such as Word2vec are trained by sampling pairs of words \\\\((w_i, w_c)\\\\) that appear in the same context and adjusting their embeddings so that their inner product is higher than an inner product with the embedding of word \\\\(w_i\\\\) and a randomly sampled word from the vocabulary. The algorithm can be understood as performing a low-rank factorization of the matrix of co-occurrence statistics. In the case of Word2vec, the matrix is factorized as\\n\\n\\\\[\\nP = I \\\\cdot C\\n\\\\]\"}"}
{"id": "8VEGkphQaK", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nGoal node ghost edge ghost edge ghost edge\\n\\nFigure 19. Attention pattern after in-context exemplars: Here we visualize the portion of the attention map after prompting with four in-context exemplar sequences. The model generates attentional patterns that treat the node across ghost edges as a subgoal.\\n\\n\\\\( I \\\\) contains word vectors in its rows and \\\\( C \\\\) contains word covectors in its columns. Therefore, every word has two types of embeddings. One is used when the word appears in the first position in the pair (which corresponds to center words), and the second when it appears in the second position (which corresponds to context words).\\n\\nInspired by the interpretability results, we argue our graph navigation task can also be solved using a similar low-rank factorization method that is generalized to 3-way relations. In this case, the tensor \\\\( T \\\\) to be factorized is third-order, and for each node, we have three types of embeddings: one which is used when the node acts as a goal, one when it acts as the current state, and one when it acts as the next possible state.\\n\\nSince we do not deal with a natural corpus with different frequency of occurrence of individual nodes, we can we set the numbers \\\\( T_{ijk} \\\\) to be proportional to the length \\\\( l_{ijk} \\\\) of a path which goes through an ordered pair of neighbour nodes \\\\((i, j)\\\\) to a node \\\\( k \\\\). If there is no such path, we set the length to infinity. The target value \\\\( T_{ijk} \\\\) can be seen as a preference for a node \\\\( j \\\\) when the goal is to reach the node \\\\( k \\\\) from the node \\\\( i \\\\); it is equal to \\\\( l_{ijk} - 1 / P_j l_{ij} - 1 k \\\\).\\n\\nInspired by the learned algorithm, we can use low-rank tensor factorization to approximate this matrix by the following expression\\n\\n\\\\[ T_{ijk} \\\\approx \\\\hat{T}_{ijk} = (s_i + g_k) \\\\cdot n_i, \\\\]\\n\\nwhere \\\\( s_i \\\\), \\\\( g_k \\\\), \\\\( n_i \\\\) are the three types of learnable embeddings. Therefore, by interpreting the trained Transformer, we can obtain a simple algorithm that can be potentially useful in setups that deal with 3-way relationships. We leave this for future work.\"}"}
{"id": "8VEGkphQaK", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers in large-scale generative models. Instead, our study seeks to establish a minimal synthetic framework, identify data-centric control variables, and formulate mechanistic hypotheses. This lays the groundwork for more in-depth theoretical and empirical investigations of larger models.\\n\\nImpact Statement\\nThis paper provides a comprehensive scientific analysis of a Transformer model that solves a small-scale synthetic task. We believe that the scientific findings presented in this paper will lay the groundwork for the development of more reliable and interpretable AI systems for the benefit of society.\\n\\nAcknowledgement\\nMikail Khona, Hidenori Tanaka, Rahul Ramesh, Maya Okawa and Kento Nishi acknowledge NTT Research and NTT Physics and Informatics Lab for compute and support. Jan Hula was supported by the Ministry of Education, Youth and Sports within the dedicated program ERC CZ under the project POSTMAN no. LL1902. Ekdeep S Lubana was partially supported by the National Science Foundation (IIS-2008151).\\n\\nReferences\\nAllen-Zhu, Z. and Li, Y. Physics of language models: Part 1, context-free grammar. arXiv preprint arXiv:2305.13673, 2023.\\n\\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., and et al., S. S. Palm 2 technical report, 2023.\\n\\nBesta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023.\\n\\nBroadbent, S. R. and Hammersley, J. M. Percolation processes: I. crystals and mazes. In Mathematical proceedings of the Cambridge philosophical society, volume 53, pp. 629\u2013641. Cambridge University Press, 1957.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\\n\\nChen, A., Phang, J., Parrish, A., Padmakumar, V., Zhao, C., Bowman, S. R., and Cho, K. Two failures of self-consistency in the multi-step reasoning of llms. arXiv preprint arXiv:2305.14279, 2023.\\n\\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.\\n\\nChomsky, N. Syntactic structures. Mouton de Gruyter, 2002.\\n\\nCormen, T. H., Leiserson, C. E., Rivest, R. L., and Stein, C. Introduction to algorithms. MIT press, 2022.\\n\\nCreswell, A. and Shanahan, M. Faithful reasoning using large language models. arXiv preprint arXiv:2208.14271, 2022.\\n\\nCreswell, A., Shanahan, M., and Higgins, I. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.\\n\\nDziri, N., Lu, X., Sclar, M., Li, X. L., Jian, L., Lin, B. Y., West, P., Bhagavatula, C., Bras, R. L., Hwang, J. D., et al. Faith and fate: Limits of transformers on compositionality. arXiv preprint arXiv:2305.18654, 2023.\\n\\nFeng, G., Gu, Y., Zhang, B., Ye, H., He, D., and Wang, L. Towards revealing the mystery behind chain of thought: a theoretical perspective. arXiv preprint arXiv:2305.15408, 2023.\\n\\nGemini, T., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\\n\\nHao, S., Gu, Y., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z., and Hu, Z. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.\\n\\nHendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.\\n\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\n\\nHuang, J. and Chang, K. C.-C. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403, 2022.\"}"}
{"id": "8VEGkphQaK", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nKarpathy, A. NanoGPT, 2021. Github link. https://github.com/karpathy/nanoGPT\\n\\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\\n\\nLaValle, S. Planning algorithms. Cambridge University Press google schola, 2:3671\u20133678, 2006.\\n\\nLi, Y., Sreenivasan, K., Giannou, A., Papailiopoulos, D., and Oymak, S. Dissecting chain-of-thought: A study on compositional in-context learning of mlps. arXiv preprint arXiv:2305.18869, 2023.\\n\\nLiu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022.\\n\\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023.\\n\\nLu, S., Bigoulaeva, I., Sachdeva, R., Madabushi, H. T., and Gurevych, I. Are emergent abilities in large language models just in-context learning? arXiv preprint arXiv:2309.01809, 2023.\\n\\nLu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.\\n\\nLubana, E. S., Bigelow, E. J., Dick, R. P., Krueger, D., and Tanaka, H. Mechanistic mode connectivity. In International Conference on Machine Learning, pp. 22965\u201323004. PMLR, 2023.\\n\\nMomennejad, I., Hasanbeig, H., Frujeri, F. V., Sharma, H., Ness, R. O., Jojic, N., Palangi, H., and Larson, J. Evaluating cognitive maps in large language models with cogeval: No emergent planning. Advances in neural information processing systems, 37, 2023.\\n\\nNavarro, G. A guided tour to approximate string matching. ACM computing surveys (CSUR), 33(1):31\u201388, 2001.\\n\\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.\\n\\nOpenAI. Gpt-4 technical report. arXiv, pp. 2303\u201308774, 2023.\\n\\nPrystawski, B. and Goodman, N. D. Why think step-by-step? reasoning emerges from the locality of experience. arXiv preprint arXiv:2304.03843, 2023.\\n\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n\\nRamesh, R., Khona, M., Dick, R. P., Tanaka, H., and Lubana, E. S. How capable can a transformer become? a study on synthetic, interpretable tasks. arXiv preprint arXiv:2311.12997, 2023.\\n\\nRazeghi, Y., Logan IV, R. L., Gardner, M., and Singh, S. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.\\n\\nSaparov, A. and He, H. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=qFVVBzXxR2V.\\n\\nSchaeffer, R., Pistunova, K., Khanna, S., Consul, S., and Koyejo, S. Invalid logic, equivalent gains: The bizarreness of reasoning in language model prompting. arXiv preprint arXiv:2307.10573, 2023.\\n\\nShah, H., Tamuly, K., Raghunathan, A., Jain, P., and Narpalli, P. The pitfalls of simplicity bias in neural networks. Advances in Neural Information Processing Systems, 33:9573\u20139585, 2020.\\n\\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\\n\\nSuzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\"}"}
{"id": "8VEGkphQaK", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n\\nTurpin, M., Michael, J., Perez, E., and Bowman, S. R. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. arXiv preprint arXiv:2305.04388, 2023.\\n\\nWang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L., and Sun, H. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022a.\\n\\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.\\n\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\\n\\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.\\n\\nZelikman, E., Mu, J., Goodman, N. D., and Wu, Y. T. Star: Self-taught reasoner bootstrapping reasoning. 2022.\"}"}
{"id": "8VEGkphQaK", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nA. Detailed Related Work\\n\\nStepwise inference protocols\\n\\nLarge language models (LLMs) have been shown to possess sophisticated and human-like reasoning and problem-solving abilities (Srivastava et al., 2022). Chain-of-thought or scratchpad reasoning refers to many similar and related phenomena involving multiple intermediate steps of reasoning generated internally and autoregressively by the language model. First described by Nye et al. (2021); Kojima et al. (2022), adding prompts such as 'think step by step' allows the LLM to autonomously generate intermediate steps of reasoning and computation, improving accuracy and quality of its responses. This is referred to as zero-shot chain-of-thought. A related set of phenomena, few-shot chain-of-thought prompting (Wei et al., 2022) occurs when the language model is shown exemplars of reasoning before being prompted with a reasoning task. The model follows the structure of logic in these exemplars, solving the task with higher accuracy. Further, there have been several prompting strategies developed, all of which rely on sampling intermediate steps, such as tree-of-thoughts (Yao et al., 2023), graph-of-thoughts (Besta et al., 2023), program-of-thoughts (Chen et al., 2022), self-ask (Press et al., 2022). There are also methods which use more than one LLM, such as STaR (Zelikman et al., 2022), RAP (Hao et al., 2023), Selection-Inference (SI) (Creswell et al., 2022; Creswell & Shanahan, 2022).\\n\\nUnderstanding stepwise inference\\n\\nDziri et al. (2023) study how LLMs solve multi-step reasoning tasks and argue that models likely fail because they reduce most multi-step reasoning tasks to linearized sub-graph matching, essentially learning 'shortcut solutions' (Liu et al., 2022). Momennejad et al. (2023) study in-context graph navigation in LLMs, finding that they fail to do precise planning. Saparov & He (2023) introduce a synthetic dataset called PrOntoQA to systematically study the failure modes of chain of thought in the GPT3 family fine-tuned on the dataset and find that misleading steps of reasoning are a common cause of failure in the best-performing models. Chen et al. (2023) find that chain-of-thought fails at compositional generalization and counterfactual reasoning. Wang et al. (2022a); Schaeffer et al. (2023) find that the content of the exemplars is less relevant to accuracy than their syntactic structure. Razeghi et al. (2022) find that the accuracy of reasoning is correlated with the frequencies of occurrence in the pretraining dataset. Recently, a few works have used theoretical approaches to characterize and explain stepwise inference. Li et al. (2023) study in-context learning of random MLPs and find that a Transformer that outputs the values of intermediate hidden layers achieves better generalization. Feng et al. (2023) show that with stepwise reasoning, Transformers can solve dynamic programming problems, and Prystawski & Goodman (2023) study reasoning traces in Transformers trained to learn the conditionals of a Bayes network. There are also several puzzling phenomena in the prompts used to elicit few-shot chain-of-thought reasoning: chain-of-thought can be improved by sampling methods such as self-consistency (Wang et al., 2022b); prompts might not reflect the true reasoning process used by the language model, as identified by Turpin et al. (2023); and the accuracy of the model can be sensitive to the order in which prompts are provided (Lu et al., 2021).\\n\\nB. Why graph navigation?\\n\\nIn this section, we describe examples of various computational tasks that have been cast as graph navigation in literature to study Transformers and LLMs.\\n\\n\u2022 First order logic: Saparov & He (2023) study simple DAGs as models of first order logical reasoning. They construct ontologies (see Fig. 10a) and prompt LLMs to do analogical reasoning.\\n\\n\u2022 Mathematical expression evaluation: Dziri et al. (2023) study mathematical expression evaluation in large scale LLMs as DAG navigation (see Fig. 10b). Any mathematical expression can be decomposed into elementary computations which are chained together.\\n\\n\u2022 Planning and spatial navigation: Momennejad et al. (2023) evaluates many large scale LLMs such as ChatGPT-4 and Claude2 on synthetically designed planning and navigation tasks (see Fig. 10c).\\n\\n\u2022 Formal grammars and natural language: Allen-Zhu & Li (2023) studies Transformers trained on context-free grammars (CFGs) which are DAGs. Another motivation for the study of graph navigation comes from linguistics and natural language syntax (Chomsky, 2002). Every sentence in a language can broken down into its syntactic or parse tree, a special case of a directed acyclic graph. For example, the sentence 'I drive a car to my college' can be parsed as the following graph: ('I': Noun phrase, 'drive a car to my college': Verb Phrase) \u2192 ('drive': Verb, 'a car': Noun Phrase, 'to my college': Prepositional Phrase) \u2192 ('a': Determiner, 'car': Noun), ('to': Preposition, 'my college': Noun Phrase) \u2192 ('my': Determiner, 'college': Noun).\"}"}
{"id": "8VEGkphQaK", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nInput: 29 + 57\\n\\nFigure 10. Examples of stepwise inference as graph navigation in LLM evaluations: (a) An example graph created for a prompt (left) from the ProntoQ&A dataset (Saparov & He, 2023) (b) (Dziri et al., 2023) studies how simple algorithms such as multiplication of digits can be represented as a graph (c) CogEval (Momennejad et al., 2023) studies many large scale LLMs such as ChatGPT-4 and Claude2 on planning and navigation tasks. (d) Mathematical expression evaluation in the case of addition of two numbers can be visualized as a series of steps of a digit-wise addition algorithm.\\n\\nEffective stepwise reasoning consists of several elementary logical steps put together in a goal-directed path that terminates at a precise state (LaValle, 2006). We argue that graph navigation problems provide such a fundamental framework for studying stepwise inference. Graphs provide a universal language for modeling and solving complex problems across various domains. Whether it is optimizing network traffic, analyzing social networks, sequencing genetic data, or solving puzzles like the Travelling Salesman Problem, the underlying structure can often be mapped onto a graph (Cormen et al., 2022; Momennejad et al., 2023; Dziri et al., 2023; Saparov & He, 2023).\\n\\nC. Setup and construction of graph and model\\n\\nC.1. Graph structures\\n\\nHere we describe the properties of the DAGs we use, the training setup, model architecture, and hyperparameters. We use two DAG structures, as shown in Fig. 11. Specifically, Bernoulli DAGs are constructed by randomly generating an upper-triangular matrix where each entry has probability $p$ of existing. Hierarchical DAGs are generated by predefining $L$ sets of nodes and drawing an edge between a node $n_l$ in layer $l$ and $n_{l+1}$ in layer $l+1$ with probability $p$; we constrain the graph to be connected. These generation processes lead to different path diversity and path length distributions, which affect the efficacy of stepwise inference, as shown in our results. Below, we provide algorithms to generate our graph structures.\"}"}
{"id": "8VEGkphQaK", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 11. Construction and properties of Hierarchical and Bernoulli DAGs: (top) Schematic of hierarchical and Bernoulli graphs. Hierarchical graphs are organized into layers with connections only between nodes of successive layers but Bernoulli graphs have no such structure. (middle) Path diversity is defined as the number of paths between any two path connected nodes. (bottom) Path length distributions: Owing to the hierarchical nature, the path length distribution is exponential in hierarchical graphs whereas it is more Gaussian-like for Bernoulli graphs.\\n\\nAlgorithm 1\\n\\nGenerate Bernoulli connected DAG\\n\\nRequire:\\n\\nnumNodes > 0, probability p for edges  \\n\\n1: nodeNames \u2190 ['X' + str(i) for i in range(numNodes)]\\n\\n2: Function CreateUpperTriangularMask(n, p)  \\n\\n3: matrix \u2190 random binary matrix with size n x n and probability p for 1s\\n\\n4: upperTriangular \u2190 extract upper triangular part of matrix\\n\\n5: return upperTriangular\\n\\n6: repeat  \\n\\n7: adjMatrix \u2190 CreateUpperTriangularMask(numNodes, p)\\n\\n8: dag \u2190 create directed graph in NetworkX from adjMatrix and nodeNames\\n\\n9: until dag is connected\"}"}
{"id": "8VEGkphQaK", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 12. Properties of Hierarchical DAGs with varying connection probability: As the probability of connections between nodes increases, the path length distribution becomes more exponential (left column) while the path diversity distribution becomes more uniform (right column).\"}"}
{"id": "8VEGkphQaK", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nAlgorithm 2\\nGenerate Hierarchical Connected DAG\\n\\n1: \\\\( p \\\\leftarrow \\\\) [probability of connection between layers]\\n\\n2: \\\\( \\\\text{nodesPerLayer} \\\\leftarrow \\\\) [number of nodes in each layer]\\n\\n3: \\\\( \\\\text{numLayers} \\\\leftarrow \\\\) [total number of layers]\\n\\n4: \\\\( \\\\text{numNodes} \\\\leftarrow \\\\text{nodesPerLayer} \\\\times \\\\text{numLayers} \\\\)\\n\\n5: \\\\( \\\\text{Function CreateLayeredDAG}(\\\\text{nodesPerLayer, numLayers, } p) \\\\)\\n\\n6: \\\\( \\\\text{Initialize an empty directed graph } G \\\\) in NetworkX\\n\\n7: \\\\( \\\\text{for currentLayer} \\\\leftarrow 1 \\\\text{ to } \\\\text{numLayers} - 1 \\\\) do\\n\\n8: \\\\( \\\\text{for each node } j \\\\text{ in currentLayer} \\\\) do\\n\\n9: \\\\( \\\\text{for each node } k \\\\text{ in currentLayer + 1} \\\\) do\\n\\n10: \\\\( \\\\text{if random number} \\\\leq p \\\\) then\\n\\n11: \\\\( \\\\text{Add edge from node } X_j \\\\text{ to node } X_k \\\\text{ in } G \\\\)\\n\\n12: \\\\( \\\\text{end if} \\\\)\\n\\n13: \\\\( \\\\text{end for} \\\\)\\n\\n14: \\\\( \\\\text{end for} \\\\)\\n\\n15: \\\\( \\\\text{end for} \\\\)\\n\\n16: \\\\( \\\\text{return } G \\\\)\\n\\n17: \\\\( \\\\text{End Function} \\\\)\\n\\n18: \\\\( \\\\text{repeat} \\\\)\\n\\n19: \\\\( \\\\text{dag} \\\\leftarrow \\\\text{CreateLayeredDAG}(\\\\text{nodesPerLayer, numLayers, } p) \\\\)\\n\\n20: \\\\( \\\\text{until } \\\\text{dag} \\\\text{ is connected} \\\\)\\n\\nC.2. Motif construction\\nIn the multi-graph scenario, we first construct a set of \\\\( n \\\\) graphs (in our experiments, we use Bernoulli DAGs with \\\\( n = 10 \\\\)) denoted by \\\\( G = \\\\{g_1, g_2, \\\\ldots, g_n\\\\} \\\\). To construct the training data, we first create all pairwise motif orders \\\\( \\\\{(g_i \\\\rightarrow g_j)\\\\} \\\\). For test evaluations, we held out 10 out of these 45 motif orders.\\n\\nC.2.1. Construction of Exemplar Sequences\\nTo provide examples in-context, we create exemplar sequences connecting motifs, say \\\\( g_{i1} \\\\) and \\\\( g_{i2} \\\\). In our construction, we select \\\\( X_s \\\\) to be source node in \\\\( g_{i1} \\\\) and \\\\( X_g \\\\) to be a sink node in \\\\( g_{i2} \\\\). Further, we choose a sink of \\\\( g_{i1} \\\\), \\\\( X_{\\\\text{sink}}(g_{i1}) \\\\) and a source of \\\\( g_{i2} \\\\), \\\\( X_{\\\\text{source}}(g_{i2}) \\\\) and connect them via a ghost edge: \\\\( (X_{\\\\text{sink}}(g_{i1}), X_{\\\\text{source}}(g_{i2})) \\\\). These intermediate nodes are subgoals for the path that the model has to produce. Finally putting everything together, the exemplar sequence has the following form: goal: \\\\( X_g X_s \\\\ldots X_{\\\\text{sink}}(g_{i1}) X_{\\\\text{source}}(g_{i2}) \\\\ldots X_g \\\\). Here, \\\\( X_s \\\\ldots X_{\\\\text{sink}} \\\\) is a path from a source to a sink in \\\\( g_{i1} \\\\) and \\\\( X_{\\\\text{source}}(g_{i2}) \\\\ldots X_g \\\\) is a path from a source to a sink in \\\\( g_{i2} \\\\). To be precise, we summarize this process into the algorithm below.\\n\\nAlgorithm 3\\nGenerate In-context Exemplars\\n\\nRequire: \\\\( \\\\{g_{i1}, g_{i2}\\\\} \\\\), two motifs across which a ghost edge will be placed.\\n\\n1: \\\\( X_s \\\\leftarrow \\\\text{Sample sources}(g_{i1}) \\\\)\\n\\n2: \\\\( X_g \\\\leftarrow \\\\text{Sample sinks}(g_{i2}) \\\\)\\n\\n3: \\\\( (X_{\\\\text{ghost edge pre}}, X_{\\\\text{ghost edge post}}) \\\\leftarrow (\\\\text{Sample sinks}(g_{i1}), \\\\text{Sample sources}(g_{i2})) \\\\)\\n\\n4: \\\\( (X_s \\\\ldots X_{\\\\text{ghost edge pre}}) \\\\leftarrow \\\\text{Sample path}(g_{i1}) \\\\)\\n\\n5: \\\\( (X_{\\\\text{ghost edge post}} \\\\ldots X_g) \\\\leftarrow \\\\text{Sample path}(g_{i2}) \\\\)\\n\\n6: \\\\( \\\\text{return } e_{g_{i1}, g_{i2}} \\\\leftarrow X_s \\\\ldots X_{\\\\text{ghost edge pre}} X_{\\\\text{ghost edge post}} \\\\ldots X_g \\\\)\\n\\nAfter providing a set of exemplar sequences in-context, we chain them together to create a longer sequence. To be precise, given a set of \\\\( K \\\\) motifs \\\\( \\\\{g_1, g_2, g_3, \\\\ldots g_K\\\\} \\\\), we have the set of \\\\( K - 1 \\\\) ghost edges, one for each exemplar: \\\\( \\\\{(X_{\\\\text{sink}}(g_i1), X_{\\\\text{source}}(g_i2)), (X_{\\\\text{sink}}(g_i2), X_{\\\\text{source}}(g_i3)), \\\\ldots (X_{\\\\text{sink}}(g_{iK-1}), X_{\\\\text{source}}(g_{iK}))\\\\} \\\\). To create the final path, we choose goal \\\\( X_g \\\\in g_{i1} \\\\) and start \\\\( X_s \\\\in g_{iK} \\\\). This path has every ghost edge from the list present in it.\"}"}
{"id": "8VEGkphQaK", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model\\n\\nMikail Khona\\n\\nMaya Okawa\\n\\nJan Hula\\n\\nRahul Ramesh\\n\\nKento Nishi\\n\\nRobert Dick\\n\\nEkdeep Singh Lubana*\\n\\nHidenori Tanaka*\\n\\n1 Massachusetts Institute of Technology\\n\\n2 NTT Physics and Informatics Lab\\n\\n3 Czech Technical University in Prague and the University of Ostrava\\n\\n4 University of Pennsylvania\\n\\n5 Center for Brain Science, Harvard University\\n\\n6 University of Michigan. Correspondence to: Mikail Khona <mikail@mit.edu>, Ekdeep Singh <eslubana@umich.edu>, Hidenori Tanaka <hidenori.tanaka@fas.harvard.edu>.\\n\\nAbstract\\n\\nStepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. To unravel the underlying mechanisms of stepwise inference we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. We find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy trade-off in model generations as sampling temperature varies; (iii) a simplicity bias in the model's output; and (iv) compositional generalization and a primacy bias with in-context exemplars. Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon.\\n\\n1. Introduction\\n\\nTransformers, the backbone of large language models (LLMs), have revolutionized several domains of machine learning (OpenAI, 2023; Anil et al., 2023; Gemini et al., 2023; Touvron et al., 2023). An intriguing capability that emerges with training of Transformers on large-scale language modeling datasets is the ability to perform stepwise inference, such as zero-shot chain-of-thought (Kojima et al., 2022), use of scratchpads (Nye et al., 2021), few-shot CoT (Wei et al., 2022), and variants of these protocols (Creswell et al., 2022; Yao et al., 2023; Besta et al., 2023; Creswell & Shanahan, 2022; Press et al., 2022). Specifically, in stepwise inference, the model is asked to or shown exemplars describing how to decompose a broader problem into multiple sub-problems. Solving these sub-problems in a step-by-step manner simplifies the overall task and significantly improves performance (see Fig. 1). Arguably, stepwise inference protocols are the workhorse behind the \u201csparks\u201d of intelligence demonstrated by LLMs (Bubeck et al., 2023; Suzgun et al., 2022; Lu et al., 2023; Huang & Chang, 2022)\u2014yet, their inner workings are poorly understood.\\n\\nMotivated by the above, we aim to design and study an abstraction which enables a precise understanding of stepwise inference in Transformers. Specifically, we argue that tasks which see maximum benefit from stepwise inference...\"}"}
{"id": "8VEGkphQaK", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\ncan be cast as a graph navigation problem: given an input describing the data to operate on and a goal to be achieved, a sequence of primitive skills (e.g., ability to perform arithmetic operations) is chained such that each skill acts on the previous skill's output, ultimately to achieve the given goal. If the input data, the final goal, and the sequence of intermediate outputs are represented as a sequence of nodes of a graph, along with primitive skills as edges connecting these nodes, the overall task can be re-imagined as navigating nodes of the graph via the execution of primitive skills.\\n\\nSeveral logical reasoning problems come under the purview of this abstraction (LaValle, 2006; Cormen et al., 2022; Momnejad et al., 2023; Dziri et al., 2023; Saparov & He, 2023): e.g., in Fig. 1a, we show how the problem of Tower of Hanoi can be decomposed into simpler sub-problems. See also Appendix B for several more examples.\\n\\nThis work. We design a suite of graph navigation tasks wherein a Transformer is trained from scratch to predict whether two nodes from a well-defined graph can be connected via a path. The model can generate intermediate outputs to solve the task, i.e., if it can generate a sequence of nodes to infer a path connecting the two nodes. In other settings exemplars demonstrating navigation paths connecting different graphs are provided. Our framework assumes that the model has to only produce nodes on the graph, edges, which can be abstractly thought of as \u201cskills\u201d, are implicit, and their existence is inferred during evaluation through successive nodes generated by the model. This is justified because a skill-based failure is the most trivial mechanism via which stepwise inference protocols can fail; in contrast, inability to plan is an independent and underexplored axis for understanding stepwise inference. Overall, we make the following contributions.\\n\\n\u2022 A Framework for Investigating Stepwise Inference. We propose a synthetic graph navigation task as an abstraction of scenarios where stepwise inference protocols help Transformers improve performance, showing that we can replicate and explain behaviors seen with use of stepwise inference in prior work. For instance, the structure of the data generating process (the graph) impacts whether stepwise inference will yield any benefits (Prystawski & Goodman, 2023). We identify further novel behaviors of stepwise inference as well, such as the existence of a tradeoff between diversity of outputs generated by the model and its accuracy with respect to inference hyperparameters (e.g., sampling temperature).\\n\\n\u2022 Demonstrating a Simplicity Bias in Stepwise Inference. When multiple solutions are possible for an input, we demonstrate the existence of a simplicity bias: the model prefers to follow the shortest path connecting two nodes. We assess this result mechanistically by identifying the underlying algorithm learned by the model to solve the task, showing the bias is likely a consequence of a \u201cpattern matching\u201d behavior that has been hypothesized to cause LLMs to fail in complex reasoning problems (Dziri et al., 2023).\\n\\n\u2022 Controllability via In-Context Exemplars. We show the model's preferred path to navigate between two nodes can be controlled via use of in-context exemplars. We use this setup to evaluate the model's ability to generalize to paths of longer length and the influence of exemplars which conflict with each other, i.e., that steer the model along different paths.\\n\\n2. Stepwise Inference as Graph Navigation\\n\\nIn this section, we define our setup for studying how stepwise inference aids Transformers in solving complex reasoning problems. Specifically, we define a graph navigation task wherein, given a start and a goal node, a Transformer is autoregressively trained to produce a sequence of nodes that concludes at the goal node. In our experiments, we consider two scenarios: one where in-context exemplars are absent (see Fig. 2a) and another where they are present (see Fig. 2b). The former scenario emulates protocols such as the scratchpad and zero-shot Chain of Thought (CoT) (Kojima et al., 2022; Nye et al., 2021), while the latter models few-shot CoT (Wei et al., 2022). In Section 2.1, we set up our experiment to explore these two scenarios. In the subsequent sections, we explicitly analyze the benefits of stepwise inference in both scenarios: without in-context exemplars (Section 2.2) and with in-context exemplars (Section 2.3).\\n\\nWe refer the reader to a detailed related work on stepwise inference protocols in Appendix A and further discussion on graph navigation as a model of stepwise inference which is in Appendix B.\\n\\n2.1. Preliminaries: Bernoulli and Hierarchical DAGs\\n\\nWe use directed acyclic graphs (DAGs) to define our graph navigation tasks. DAGs are a natural mathematical abstraction to study multi-step, logical reasoning problems: e.g., as discussed in Dziri et al. (2023), the output of any deterministic algorithm can be represented as a DAG. Specifically, a DAG is defined as $G := (N, E)$, where $N$ denotes the set of nodes in the graph and $E$ denotes the set of directed edges across the nodes. The edges of a DAG are captured by its adjacency matrix $A$, where $A_{ij} = 1$ if $(X_i, X_j) \\\\in E$. A directed simple path is a sequence of distinct nodes of $G$ which are joined by a sequence of edges. If two nodes are connected via a directed simple path, we call them path-connected. The first node of a path is referred to as the start node, which we denote as $X_s$, and the last node as the goal node.\"}"}
{"id": "8VEGkphQaK", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\n1. Generate a set of random Directed Acyclic Graphs (DAGs) 1. Generate Directed Acyclic Graph (DAG)\\n\\n(a) (b)\\n\\nHierarchical graph Random graph\\n\\n1. Generate a set of random Directed Acyclic Graphs (DAGs) 1. Generate Directed Acyclic Graph (DAG)\\n\\n(a) (b)\\n\\nHierarchical graph Random graph\\n\\n1. Generate a set of random Directed Acyclic Graphs (DAGs) 1. Generate Directed Acyclic Graph (DAG)\\n\\n(a) (b)\\n\\nHierarchical graph Random graph\\n\\nMotifs\\n\\n2. Connect Motifs with ghost edges\\n\\n3. Stitch motifs with in-context exemplars\\n\\n4. Prompt model to perform inference with context\\n\\nFigure 2. Data generating process.\\n\\n(a) In absence of exemplars. This figure illustrates the step-by-step process of generating a training dataset using a single underlying graph. 1) A directed acyclic graph (DAG) is generated, which can be either hierarchically structured or Bernoulli. 2) A start node and a goal node are selected. 3) All possible paths connecting the start and goal nodes are sampled, and one path is randomly selected. 4) The chosen path is then represented in a task-specific format. (b) In presence of exemplars. The process of generating a training dataset by combining multiple subgraphs (motifs) involves the following. (1.) Start by building a set of Bernoulli directed acyclic graphs (DAGs). (2.) Pick a subset of \\\\( K \\\\) of these DAGs \\\\( \\\\{ g_i^1, g_i^2, \\\\ldots, g_i^K \\\\} \\\\) and connect them together using \u201cghost edges\u201d to create a chain of motifs \\\\( g_i^1 \\\\rightarrow g_i^2 \\\\rightarrow \\\\cdots \\\\rightarrow g_i^K \\\\). (3.) Sample exemplars from every pair of motifs that have been connected by a ghost edge to construct the context. (4.) Now choose a start node \\\\( X_s \\\\in g_i^1 \\\\) and a goal node \\\\( X_g \\\\in g_i^K \\\\) and construct a sequence passing through the whole chain of motifs.\\n\\nWe briefly discuss the process of construction of DAGs used in our work and how paths are sampled from them; a more thorough description is provided in Appendix C.1. We define a Bernoulli DAG of \\\\( N_B \\\\) nodes, whose adjacency matrix has an upper triangular structure with Bernoulli entries with edge density \\\\( p \\\\), such that \\\\( p(A_{ij} = 1) = p \\\\). We also define a hierarchical DAG, wherein the nodes follow a feedforward, layered structure such that all \\\\( N_H \\\\) nodes at a given layer are only connected to nodes in the following layer (see Fig. 2a). In particular, for every node \\\\( n_l \\\\) in layer \\\\( l \\\\) and \\\\( n_{l+1} \\\\) in layer \\\\( l+1 \\\\), we draw a directed edge \\\\((n_l, n_{l+1})\\\\) with probability \\\\( p \\\\), which we refer to as edge density. On average, between any two layers of a hierarchical DAG, there are \\\\( pN_H^2 \\\\) edges and each node in an intermediate layer has an out-degree and in-degree of \\\\( pN_H \\\\). The number of paths from a particular node in layer \\\\( l \\\\) to layer \\\\( l' \\\\) \\\\( > l \\\\) is exponential and given by \\\\( (pN_H)^{l'-l} \\\\), while for Bernoulli DAGs, it is bell-shaped; this is quantified in the path length distribution shown in Appendix Fig. 11. For both graph structures, source nodes are nodes that do not have any parent nodes, and the nodes that do not have any children nodes are sink nodes. We use rejection sampling to ensure that graphs are connected.\\n\\n2.2. Modeling stepwise inference without exemplars\\n\\nZero-shot CoT (Kojima et al., 2022) and scratchpads (Nye et al., 2021) represent two examples of stepwise inference protocols that do not rely on exemplars. For instance, in the zero-shot CoT approach, the input of the model is augmented with the phrase \u201clet\u2019s think step by step.\u201d This encourages the model to generate outputs that elaborate on the intermediate steps required to solve the target problem, thereby enhancing accuracy by breaking down the target problem into several simpler problems. To compare the model\u2019s performance with stepwise inference and without stepwise inference (i.e., direct inference), we create two datasets: one including intermediate steps and the other without them. Each dataset is subsequently used to train distinct models. During the test phase, we present these trained models with pairs of nodes and task them to determine the existence of a path between the nodes. A model\u2019s performance is assessed based on its accuracy in classifying whether a path exists.\\n\\nFig. 2a shows how we generate the datasets above. First, we define a DAG denoted as \\\\( G \\\\). Within this graph, for each dataset instance, we sample a start node \\\\( X_s \\\\) and a goal node \\\\( X_g \\\\) and then identify all feasible paths between these two nodes. From the identified paths, we select one to form a sequence passing through the whole chain of motifs.\"}"}
{"id": "8VEGkphQaK", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards an Understanding of Stepwise Inference in Transformers\\n\\nsequence of tokens, $S$. This procedure is iterated for other node pairs within the graph $G$ to compile the complete dataset. For the dataset with stepwise inference, we use all the intermediate steps, including the start node $X_s$ and the goal node $X_g$, to form $S$. For the dataset without stepwise inference (i.e., direct inference), we only use the start node $X_s$ and the goal node $X_g$. We introduce a binary variable $\\\\text{path} \\\\in \\\\{p_1, p_0\\\\}$ to denote whether there is a path between the start and goal nodes. We append the 'path' token $p_1$ to the end of the sequence $S$ if there is at least one path between the start and goal nodes; otherwise, we append the 'no path' token $p_0$.\\n\\nExample: For the example path in Fig. 2a, in the dataset with stepwise inference, the sequence of tokens $S$ includes the intermediate steps and takes the form $\\\\text{goal:} X_2 X_4 X_3 X_6 X_2 p_1$. For the dataset without stepwise inference (i.e., direct inference), the sequence $S$ does not contain intermediate steps and has the form $\\\\text{goal:} X_2 X_4 p_1$.\\n\\n2.3. Modeling stepwise inference with exemplars\\n\\nHere we examine the influence of stepwise inference on model performance when in-context exemplars are present. This scenario is prominently exemplified by protocols based on few-shot CoT prompting (Wei et al., 2022; Creswell et al., 2022). Specifically, we extend the setup with a single DAG described in Section 2.2 by incorporating a set of $DAGs$, which we call motifs. The data generation process is shown in Fig. 2b. First, we generate a set of $n$ Bernoulli DAGs denoted by $b_g = \\\\{g_i\\\\}_{i=1}^n$ and randomly select a subset of $K$ motifs from this set $\\\\{g_j^1, g_j^2, \\\\ldots, g_j^K\\\\} \\\\subset b_g$. Then, we add edges between the sink node of each motif $g_j^k$ and the source node of the subsequent motif $g_j^{k+1}$, forming a chain of motifs $g_i^1 \\\\rightarrow g_i^2 \\\\rightarrow \\\\cdots \\\\rightarrow g_i^K$. These interconnecting edges are termed ghost edges. We sample paths from each pair of motifs linked by a ghost edge to establish the context. We select a start node from the sink nodes of one motif, $X_s \\\\in g_j^i$, and a goal node from the source nodes of a different motif, $X_g \\\\in g_j^j$, then sample a path between them, denoted as $e_{gg'}$. This procedure generates a sequence of nodes spanning across motifs, $g \\\\rightarrow g'$, including exactly one ghost edge. We refer to this as an exemplar sequence and use them as in-context samples. Exemplars to model few-shot CoT are represented as $e_{gg'}$ and denote a exemplar sequence from the motif-pair $g \\\\rightarrow g'$.\\n\\nFinally, we select a start node $X_{s} \\\\in g_i^1$ and a goal node $X_{g} \\\\in g_i^K$. We then prompt the model to either directly output a path that connects the node pair $X_{s}$ and $X_{g}$, or to provide exemplars demonstrating traversal between motifs within the specified context. Recall that our graph is constructed from a combination of $K$ motifs. For the training dataset, we intentionally exclude 20% of the combinations. For the test dataset, we randomly select motifs from the remaining combinations in $b_g$, and sample sequences that illustrate how to navigate between two nodes within this graph. From training data, a model can learn the structure and interconnections of motifs; yet, during testing, it faces unseen combinations of these motifs. Correspondingly, the model must use the context to infer the overall structure of the graph. In essence, an in-context exemplar tells the model which motifs are connected via ghost edges and hence can be navigated between.\\n\\nExample: We directly study the path of navigation outputted by the model in this setup, i.e., no special tokens are used. A sample is constructed by selecting motifs to define in-context exemplars, say $g_i^1, g_i^2, g_i^3$. For every successive pair of motifs, we construct an exemplar and put them together to create the context. To do this, we select two (start, goal) pairs: $X_s^1 \\\\in g_i^1, X_g^1 \\\\in g_i^2$ and $X_s^2 \\\\in g_i^2, X_g^2 \\\\in g_i^3$. We sample exemplar sequences starting and ending at these node pairs: one sequence from $g_i^1$ to $g_i^2$, $\\\\text{goal:} X_g^1 X_s^1 X_1 \\\\ldots X_k^1 X_g^1$, and another from $g_i^2$ to $g_i^3$, $\\\\text{goal:} X_g^2 X_s^2 X_1' \\\\ldots X_k^2 X_g^2$. These sequences act as exemplars to be provided in context to the model when it is shown an input. The number of exemplars can vary from two to four, which correspond to chains of motifs of length three to five. The input itself is defined by choosing a goal node $X_g \\\\in g_i^3$, a start node $X_s \\\\in g_i^1$, and an intermediate node $X_{inter} \\\\in g_i^2$; e.g., $\\\\text{goal:} X_g X_s X_1'' \\\\ldots X_{inter} \\\\ldots X_k^1 X_g$.\\n\\nHere, $X_{s} X_1'' \\\\ldots X_{inter} \\\\ldots X_k^1$ is a path between motifs $g_i^1$ and $g_i^2$, while $X_{inter} \\\\ldots X_k^2 X_g$ is a path between motifs $g_i^2$ and $g_i^3$. When exemplars are not provided, the model must rely on its internalized knowledge to infer whether there exist two connected motifs that can be used to move from the start to goal node. The context exemplars simplify the problem by telling the model the motifs above are connected.\\n\\n3. Results: Stepwise Navigation\\n\\nIn this section, we discuss findings on how stepwise inference affects the model's ability to solve problems. We investigate two scenarios: in the absence of in-context exemplars (Section 3.1) and in the presence of them (Section 3.2). For all experiments, unless stated otherwise, we use a 2-layer Transformer defined by Karpathy (2021) to mimic the GPT architecture (Brown et al., 2020). For more details on the experimental setup, please refer to Appendix C.3 for model architecture details and Appendix D for training data generation and train/test split.\"}"}
