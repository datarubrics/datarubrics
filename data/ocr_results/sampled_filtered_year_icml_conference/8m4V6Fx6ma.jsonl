{"id": "8m4V6Fx6ma", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Optimal Exact Recovery in Semi-Supervised Learning: A Study of Spectral Methods and Graph Convolutional Networks\\n\\nHaixiao Wang\\nZhichao Wang\\n\\nAbstract\\n\\nWe delve into the challenge of semi-supervised node classification on the Contextual Stochastic Block Model (CSBM) dataset. Here, nodes from the two-cluster Stochastic Block Model (SBM) are coupled with feature vectors, which are derived from a Gaussian Mixture Model (GMM) that corresponds to their respective node labels. With only a subset of the CSBM node labels accessible for training, our primary objective becomes the accurate classification of the remaining nodes. Venturing into the transductive learning landscape, we, for the first time, pinpoint the information-theoretical threshold for the exact recovery of all test nodes in CSBM. Concurrently, we design an optimal spectral estimator inspired by Principal Component Analysis (PCA) with the training labels and essential data from both the adjacency matrix and feature vectors. We also evaluate the efficacy of graph ridge regression and Graph Convolutional Networks (GCN) on this synthetic dataset. Our findings underscore that graph ridge regression and GCN possess the ability to achieve the information threshold of exact recovery in a manner akin to the optimal estimator when using the optimal weighted self-loops. This highlights the potential role of feature learning in augmenting the proficiency of GCN, especially in the realm of semi-supervised learning.\\n\\n1. Introduction\\n\\nGraph Neural Networks (GNNs) have emerged as a powerful method for tackling various problems in the domain of graph-structured data, such as social networks, biological networks, and knowledge graphs. The versatility of GNNs allows for applications ranging from node classification to link prediction and graph classification. To explore the mechanism and functionality behind GNNs, it is natural to assume certain data generation models, such that the fundamental limits of certain tasks appear mathematically. In particular, we focus on the synthetic data sampled from the Contextual Stochastic Block Model (CSBM), first introduced by (Deshpande et al., 2018).\\n\\nIn the literature, the existing work has been focused on the generalization property of GNN (Esser et al., 2021), (Bruna & Li, 2017), (Baranwal et al., 2021), the role of non-linearity (Lampert & Scholtes, 2023) and oversmoothing phenomenon (Wu et al., 2022). We focus on the fundamental limits of CSBM and the following questions: (1) When do we expect to classify all nodes correctly? (2) What is the best possible accuracy we can achieve when given the parameters of the data generation model? (3) Can we design an efficient estimator to accomplish the previous tasks and how well GNN performs under this evaluation metric? In addressing this challenge, we venture into the transductive learning landscape. For the first time, we identify the information-theoretical limits required for classifying all nodes correctly for CSBM, specifically in transductive learning setting. This discovery is pivotal as it sets a benchmark for evaluating the performance of various models and algorithms on this type of data for the node classification problem.\\n\\nRelated work\\n\\nGraph machine learning on CSBM\\n\\nCSBM has become a popular model for theoretical analysis on the performance of GCNs (Kipf & Welling, 2017). For instance, (Wu et al., 2022) studied over-smoothing on CSBM; the generalization performance of GCNs and graph attentions has been considered in (Baranwal et al., 2021; 2023a; Fountoulakis et al., 2023; Chen et al., 2019); the expressivity of GCNs on CSBM (Oono & Suzuki, 2019); bayesian-inference on nonlinear GCNs (Wei et al., 2022; Baranwal et al., 2023b). Although (Huang et al., 2023) analyzed the feature learning on modified CSBM (SNM therein) and (Lu, 2022) showed the learning performance on SBM, currently, there is no complete analysis of the training dynamics for GCNs on\"}"}
{"id": "8m4V6Fx6ma", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2\\nGradient-based training for both $W$ and $s$ in GCN\\n\\nInput: Learning rates $\\\\eta^t$, weight decay $\\\\lambda^t$, number of steps $T$\\n\\nInitialization: $s(0) \\\\sim \\\\text{Unif}([-1, 1]), \\\\sqrt{K} \\\\cdot [W_0]_{ij} \\\\sim \\\\mathcal{N}(0, 1), \\\\sqrt{K} \\\\cdot [a]_j \\\\sim \\\\text{Unif}\\\\{\\\\pm 1\\\\}, \\\\text{for all } i \\\\in [d], j \\\\in [K]$. \\n\\nTraining Stage 1:\\nSet $\\\\sigma(x) = x$ in (3.12) \\n$W^{(1)} \\\\leftarrow W^{(0)} - \\\\eta^1 (\\\\nabla W^{(0)} L(W^{(0)}, s^{(0)}) + \\\\lambda^1 W^{(0)})$\\n$s^{(1)} \\\\leftarrow s^{(0)}$\\n$W^{(1)} \\\\leftarrow W^{(1)}$ \\n$a \\\\leftarrow 1$\\n\\nTraining Stage 2:\\nSet $\\\\sigma(x) = \\\\tanh(x)$ in (3.12) for $t = 2$ to $T$ \\n$s^{(t)} \\\\leftarrow s^{(t-1)} - \\\\eta^t \\\\nabla s^{(t)} L(W^{(1)}, s^{(t-1)}) + \\\\lambda^t s^{(t-1)}$\\nend for \\nreturn Prediction function for unknown labels: $\\\\text{sign}(SUDD - 1 s^{(T)} A s^{(T)} W^{(1)} a)$ with very high probability. for $s = 0$. Then, we can apply Lemmas E.1 and C.4 to conclude that\\n\\n$$\\\\frac{1}{n} y^\\\\top L A L y L - a \\\\tau - b \\\\tau^2 = o(qm),$$\\n\\nwith a very high probability for sufficiently large $n$ and $m$.\\n\\nLemma D.6. Following the assumptions in Theorem 3.9, we have that\\n\\n$$\\\\frac{1}{n} y^\\\\top L A L y L - a \\\\tau - b \\\\tau^2 \\\\leq C \\\\sqrt{qm},$$\\n\\nwith probability at least $1 - cN^{-10}$, for some constants $c, C > 0$. \\n\\nProof. This lemma follows from Lemma F.6 in (Abbe et al., 2022) and Corollary 3.1 in (Abbe et al., 2020). Notice that\\n\\n$$\\\\frac{1}{n} y^\\\\top L A L y L = \\\\frac{1}{n} \\\\sum_{i,j \\\\in V} L_{ij} y_i y_j = \\\\frac{1}{n} \\\\sum_{i,j \\\\in V} L_{ij} - \\\\sum_{i,j \\\\in V} \\\\text{in same block of } V L_{ij} - \\\\sum_{i,j \\\\in V} \\\\text{in different blocks of } V L_{ij}.$$\\n\\nThen, we can apply the proof of (F.23) in (Abbe et al., 2022) to conclude this lemma.\\n\\nCombining all the above lemmas in this section, we can derive the following lemma.\\n\\nLemma D.7. Following the assumptions in Theorem 3.9, we have that\\n\\n$$s^{(1)} - 2c \\\\tau \\\\log a \\\\tau^b \\\\leq C \\\\sqrt{qm},$$\\n\\nwith probability at least $1 - cN^{-10}$, for some constants $c, C > 0$, where $s^{(1)}$ is given by (D.2).\\n\\nD.4. Proof of Theorem 3.9\\nLet us recall that we consider $K/\\\\eta_1 \\\\approx \\\\sqrt{qm}$ and $d = o(q^2 m)$ with $W_{i,j} \\\\sim \\\\mathcal{N}(0, 1/K)$. Finally, we complete the proof of Theorem 3.9 for Algorithm 1 as follows. Recall that\\n\\n$$D_s := (D_0 + sq) \\\\mathbb{1} \\\\in \\\\mathbb{R}^{n \\\\times n},$$\\n\\nfor any $s \\\\in \\\\mathbb{R}$, where $D_0$ is the average 34.\"}"}
{"id": "8m4V6Fx6ma", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exact Recovery in Semi-Supervised Learning\\n\\nDenote that $s_{\\\\text{opt}} := 2c\\\\tau \\\\log a\\\\tau b\\\\tau$\\n\\n$$D^{-1}s_{\\\\text{(1)}}\\\\mathbf{A}_{s_{\\\\text{(1)}}}\\\\mathbf{X} =: [b_1, \\\\ldots, b_N]^{\\\\top} \\\\in \\\\mathbb{R}^{N \\\\times d},$$\\n\\n$$D^{-1}s_{\\\\text{opt}}\\\\mathbf{A}_{s_{\\\\text{opt}}}\\\\mathbf{X} =: [\\\\bar{g}_1, \\\\ldots, \\\\bar{g}_N]^{\\\\top} \\\\in \\\\mathbb{R}^{N \\\\times d},$$\\n\\n$$(\\\\tilde{d} + s_{\\\\text{opt}} \\\\cdot q_m)^{-1}\\\\mathbf{A}_{s_{\\\\text{opt}}}\\\\mathbf{X} =: [\\\\tilde{g}_1, \\\\ldots, \\\\tilde{g}_N]^{\\\\top} \\\\in \\\\mathbb{R}^{N \\\\times d},$$\\n\\n$$\\\\frac{1}{\\\\sqrt{K}} W_{\\\\{1\\\\}} a =: b\\\\mu.$$\\n\\nThen, by definition, $by_{GCN, i}^{\\\\top} = bg_i^{\\\\top} b\\\\mu$ for $i \\\\in V_U$.\\n\\nAs a remark, notice that Lemma C.7 verifies that with high probability $\\\\|bg_i\\\\| \\\\lesssim \\\\sqrt{d}$. Because of this bound, we can only consider the regime when $d = o(q_m^2)$ for our following analysis. To improve this to a high dimensional regime, e.g., $d \\\\approx N$, we improve the following concentration without simply using the bound of $\\\\|bg_i\\\\|$. Similarly with the proof of Lemma C.8 in ridge regression of linear GCN part, we need to do certain leave-one-out analysis to achieve a larger regime for $d$.\\n\\nNext, based on the above decomposition, we follow the proof idea of Theorem 3.6 to complete the proof of Theorem 3.9. Combining Lemmas C.7, D.3 and D.7, for each $i \\\\in V_U$, we can obtain that\\n\\n$$|y_i \\\\cdot bg_i^{\\\\top} b\\\\mu - y_i \\\\cdot \\\\tilde{g}_i^{\\\\top} \\\\mu| \\\\leq (\\\\tilde{g}_i - \\\\bar{g}_i)^{\\\\top} \\\\mu + (\\\\bar{g}_i - bg_i)^{\\\\top} \\\\mu + bg_i^{\\\\top}(b\\\\mu - \\\\mu) = o(\\\\sqrt{q_m^2}),$$\\n\\nwith probability at least $1 - cN^{-10}$ for some constants $c, C > 0$. Therefore, we can take $\\\\zeta = 1$, $\\\\varepsilon_m = o(1)$ and $\\\\rho = s_{\\\\text{opt}} \\\\cdot q_m$ to get $\\\\mathbb{P}(\\\\psi_m(\\\\text{sign}(by_{GCN}), y_{U})) = 0) \\\\xrightarrow{\\\\text{as } m \\\\to \\\\infty} 1$.\\n\\nE. Auxiliary Lemmas and Proofs\\n\\nLemma E.1. Let $Z \\\\in \\\\mathbb{R}^{N \\\\times d}$ defined in (2.1). Then, there exists some constant $c, K > 0$ such that for any $t > 0$\\n\\n$$\\\\mathbb{P}(\\\\frac{1}{\\\\sqrt{N}} \\\\mathbf{1}^{\\\\top} Z\\\\mu \\\\geq t) \\\\leq 2 \\\\exp \\\\left(-ct^2d\\\\right),$$\\n\\n$$\\\\mathbb{P}(\\\\frac{1}{N} \\\\mathbf{1}^{\\\\top} ZZ^{\\\\top} \\\\mathbf{y} \\\\geq t) \\\\leq 2 \\\\exp \\\\left(-cd\\\\min\\\\{t^2K^2, tK\\\\}\\\\right),$$\\n\\n$$\\\\mathbb{P}(\\\\frac{1}{N} \\\\mathbf{1}^{\\\\top} ZZ^{\\\\top} D^{-1} \\\\mathbf{X} \\\\geq t) \\\\leq 2 \\\\exp \\\\left(-cd\\\\min\\\\{t^2K^2, tK\\\\}\\\\right),$$\\n\\nfor any $\\\\delta > 0$ and sufficiently large $m$, where in the last line we employ Proposition D.1. Thus, applying Lemma C.2, we know that when $J(a, b, c, 1, s_{\\\\text{opt}})^{\\\\top} I(a, b, c) > 1$,\\n\\n$$\\\\mathbb{P}(\\\\psi_m(\\\\text{sign}(by_{GCN}), y_{U})) = 0) \\\\xrightarrow{\\\\text{as } m \\\\to \\\\infty} 1.$$\"}"}
{"id": "8m4V6Fx6ma", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exact Recovery in Semi-Supervised Learning\\n\\nProof. Based on general Hoeffding\u2019s inequality Theorem 2.6.3 in (Vershynin, 2018), we can get\\n\\\\[\\nP(\\\\sqrt{N^{-1} \\\\sum_{i=1}^{N} z_i^\\\\top \\\\mu} \\\\leq t) \\\\leq 1 - 2 \\\\exp\\\\left(-c t^2 d^2\\\\right).\\n\\\\]\\n\\nSimilarly, by Bernstein\u2019s inequality Theorem 2.8.2 in (Vershynin, 2018), we have\\n\\\\[\\nP(\\\\sqrt{N^{-1} \\\\sum_{i=1}^{N} z_i^\\\\top z_i} \\\\leq t) \\\\geq 1 - 2 \\\\exp\\\\left(-c \\\\min\\\\left(t^2 K^2, t K\\\\right)\\\\right),\\n\\\\]\\n\\nwhere\\n\\\\[K = \\\\|\\\\xi\\\\|_{\\\\psi_2}\\\\]\\nfor \\\\(\\\\xi \\\\sim N(0, 1)\\\\).\\n\\nLemma E.2 (Theorem 3.1 in (Dumitriu & Wang, 2023)). Let \\\\(G = ([N], E)\\\\) be an inhomogeneous Erd\u0151s-R\u00e9nyi graph associated with the probability matrix \\\\(P\\\\), that is, each edge \\\\(e = \\\\{i, j\\\\} \\\\subset \\\\{1, \\\\ldots, N\\\\}\\\\) is sampled from \\\\(\\\\text{Ber}(P_{ij})\\\\), namely, \\\\(P(A_{ij} = 1) = P_{ij}\\\\). Let \\\\(A\\\\) denote the adjacency matrix of \\\\(G\\\\). Denote \\\\(P_{\\\\text{max}} := \\\\max_{i,j} P_{ij}\\\\). Suppose that \\\\(N \\\\cdot P_{\\\\text{max}} \\\\geq c \\\\log N\\\\), for some positive constant \\\\(c\\\\), then with probability at least \\\\(1 - 2^{-10^{-2}}\\\\), adjacency matrix \\\\(A\\\\) satisfies\\n\\\\[\\n\\\\|A - E_A\\\\| \\\\leq C(E.1) \\\\cdot \\\\sqrt{N \\\\cdot P_{\\\\text{max}}}.\\n\\\\]\\n\\nLemma E.3 (Bernstein\u2019s inequality, Theorem 2.8.4 of (Vershynin, 2018)). Let \\\\(X_1, \\\\ldots, X_n\\\\) be independent mean-zero random variables such that \\\\(|X_i| \\\\leq K\\\\) for all \\\\(i\\\\). Let \\\\(\\\\sigma^2 = \\\\sum_{i=1}^{n} E X_i^2\\\\). Then for every \\\\(t \\\\geq 0\\\\),\\n\\\\[\\nP\\\\left(\\\\sum_{i=1}^{n} X_i \\\\geq t\\\\right) \\\\leq 2 \\\\exp\\\\left(-\\\\frac{t^2}{2 \\\\sigma^2} + \\\\frac{Kt}{3}\\\\right).\\n\\\\]\"}"}
{"id": "8m4V6Fx6ma", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"exact recovery in semi-supervised learning\\n\\nimpact statement\\n\\nthis paper presents work whose goal is to advance the field of machine learning. there are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.\\n\\nreferences\\n\\nabbe, e. community detection and stochastic block models: recent developments. journal of machine learning research, 18(177):1\u201386, 2018. url http://jmlr.org/papers/v18/16-480.html.\\n\\nabbe, e., bandeira, a. s., and hall, g. exact recovery in the stochastic block model. ieee transactions on information theory, 62(1):471\u2013487, 2015.\\n\\nabbe, e., fan, j., wang, k., and zhong, y . entrywise eigenvector analysis of random matrices with low expected rank. annals of statistics, 48(3):1452, 2020.\\n\\nabbe, e., fan, j., and wang, k. an $l_p$ theory of pca and spectral clustering. the annals of statistics, 50(4):2359\u20132385, 2022.\\n\\nalt, j., ducatez, r., and knowles, a. extremal eigenvalues of critical erd\u0151s\u2013r\u00e9nyi graphs. the annals of probability, 49(3):1347\u20131401, 2021.\\n\\nazriel, d., brown, l. d., sklar, m., berk, r., buja, a., and zhao, l. semi-supervised linear regression. journal of the american statistical association, 117(540):2238\u20132251, 2022.\\n\\nba, j., erdogdu, m. a., suzuki, t., wang, z., wu, d., and yang, g. high-dimensional asymptotics of feature learning: how one gradient step improves the representation. advances in neural information processing systems, 35:37932\u201337946, 2022.\\n\\nbaranwal, a., fountoulakis, k., and jagannath, a. graph convolution for semi-supervised classification: improved linear separability and out-of-distribution generalization. in meila, m. and zhang, t. (eds.), proceedings of the 38th international conference on machine learning, volume 139 of proceedings of machine learning research, pp. 684\u2013693. pmlr, 18\u201324 jul 2021. url https://proceedings.mlr.press/v139/baranwal21a.html.\\n\\nbaranwal, a., fountoulakis, k., and jagannath, a. effects of graph convolutions in multi-layer networks. in the eleventh international conference on learning representations, 2023a. url https://openreview.net/forum?id=P-73JPgRs0R.\\n\\nbaranwal, a., fountoulakis, k., and jagannath, a. optimality of message-passing architectures for sparse graphs. in thirty-seventh conference on neural information processing systems, 2023b. url https://openreview.net/forum?id=d1knqWjmNt.\\n\\nbelkin, m., matveeva, i., and niyogi, p. regularization and semi-supervised learning on large graphs. in learning theory: 17th annual conference on learning theory, colt 2004, banff, canada, july 1-4, 2004. proceedings 17, pp. 624\u2013638. springer, 2004.\\n\\nbojchevski, a. and g\u00fcnnemann, s. deep gaussian embedding of graphs: unsupervised inductive learning via ranking. in international conference on learning representations, 2018. url https://openreview.net/forum?id=r1ZdKJ-0W.\\n\\nbruna, j. and li, x. community detection with graph neural networks. stat, 1050:27, 2017.\\n\\nchakrabortty, a. and cai, t. efficient and adaptive linear regression in semi-supervised settings. the annals of statistics, pp. 1541\u20131572, 2018.\\n\\nchen, z., li, l., and bruna, j. supervised community detection with line graph neural networks. in international conference on learning representations, 2019. url https://openreview.net/forum?id=H1g0Z3A9Fm.\\n\\ndamian, a., lee, j., and soltanolkotabi, m. neural networks can learn representations with gradient descent. in conference on learning theory, pp. 5413\u20135452. pmlr, 2022.\\n\\ndeshpande, y ., sen, s., montanari, a., and mossel, e. contextual stochastic block models. in bengio, s., wallach, h., larochelle, h., grauman, k., cesa-bianchi, n., and garnett, r. (eds.), advances in neural information processing systems, volume 31. curran associates, inc., 2018. url https://proceedings.neurips.cc/paper_files/paper/2018/file/08fc80de8121419136e443a70489c123-Paper.pdf.\\n\\ndumitriu, i. and wang, h. optimal and exact recovery on the general non-uniform hypergraph stochastic block model. arXiv preprint arXiv:2304.13139, 2023.\\n\\ndurantion, o. and zdeborov\u00e1, l. optimal inference in contextual stochastic block models. arXiv preprint arXiv:2306.07948, 2023.\\n\\nesser, p., chennuru vankadara, l., and ghoshdastidar, d. learning theory can (sometimes) explain generalisation in graph neural networks. advances in neural information processing systems, 34:27043\u201327056, 2021.\"}"}
{"id": "8m4V6Fx6ma", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Exact Recovery in Semi-Supervised Learning**\\n\\nFeige, U. and Ofek, E. O. Spectral techniques applied to sparse random graphs. *Random Structures & Algorithms*, 27, 2005. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/rsa.20089.\\n\\nFountoulakis, K., Levi, A., Yang, S., Baranwal, A., and Jagannath, A. Graph attention retrospective. *Journal of Machine Learning Research*, 24(246):1\u201352, 2023.\\n\\nHe, X., Deng, K., Wang, X., Li, Y., Zhang, Y., and Wang, M. Lightgcn: Simplifying and powering graph convolution network for recommendation. In *Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval*, pp. 639\u2013648, 2020.\\n\\nHuang, W., Cao, Y., Wang, H., Cao, X., and Suzuki, T. Graph neural networks provably benefit from structural information: A feature learning perspective. *arXiv preprint arXiv:2306.13926*, 2023.\\n\\nKim, C., Bandeira, A. S., and Goemans, M. X. Stochastic block model for hypergraphs: Statistical limits and a semidefinite programming approach. *arXiv preprint arXiv:1807.02884*, 2018.\\n\\nKipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. In *International Conference on Learning Representations*, 2017. URL https://openreview.net/forum?id=SJU4ayYgl.\\n\\nLampert, M. and Scholtes, I. The self-loop paradox: Investigating the impact of self-loops on graph neural networks. *arXiv preprint arXiv:2312.01721*, 2023.\\n\\nLelarge, M. and Miolane, L. Asymptotic bayes risk for gaussian mixture in a semi-supervised setting. In *2019 IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)*, pp. 639\u2013643. IEEE, 2019.\\n\\nLu, C. and Sen, S. Contextual stochastic block model: Sharp thresholds and contiguity. *Journal of Machine Learning Research*, 24(54):1\u201334, 2023.\\n\\nLu, W. LEARNING GUARANTEE FOR GRAPH CONVOLUTIONAL NETWORKS ON THE STOCHASTIC BLOCK MODEL. In *International Conference on Learning Representations*, 2022. URL https://openreview.net/forum?id=dpXL6lz4mOQ.\\n\\nNguyen, M.-T. and Couillet, R. Asymptotic bayes risk of semi-supervised multitask learning on gaussian mixture. In *International Conference on Artificial Intelligence and Statistics*, pp. 5063\u20135078. PMLR, 2023.\\n\\nOono, K. and Suzuki, T. Graph neural networks exponentially lose expressive power for node classification. *arXiv preprint arXiv:1905.10947*, 2019.\\n\\nOymak, S. and Cihad Gulcu, T. A theoretical characterization of semi-supervised learning with self-training for gaussian mixture models. In Banerjee, A. and Fukumizu, K. (eds.), *Proceedings of The 24th International Conference on Artificial Intelligence and Statistics*, volume 130 of *Proceedings of Machine Learning Research*, pp. 3601\u20133609. PMLR, 13\u201315 Apr 2021. URL https://proceedings.mlr.press/v130/oymak21a.html.\\n\\nRyan, K. J. and Culp, M. V. On semi-supervised linear regression in covariate shift problems. *The Journal of Machine Learning Research*, 16(1):3183\u20133217, 2015.\\n\\nShchur, O., Mumme, M., Bojchevski, A., and G\u00fcnnemann, S. Pitfalls of graph neural network evaluation. *arXiv preprint arXiv:1811.05868*, 2018.\\n\\nShi, C., Pan, L., Hu, H., and Dokmani \u00b4c, I. Homophily modulates double descent generalization in graph convolution networks. *Proceedings of the National Academy of Sciences*, 121(8):e2309504121, 2024.\\n\\nTang, H. and Liu, Y. Towards understanding generalization of graph neural networks. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), *Proceedings of the 40th International Conference on Machine Learning*, volume 202 of *Proceedings of Machine Learning Research*, pp. 33674\u201333719. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/tang23f.html.\\n\\nTony Cai, T. and Guo, Z. Semisupervised inference for explained variance in high dimensional linear regression and its applications. *Journal of the Royal Statistical Society Series B: Statistical Methodology*, 82(2):391\u2013419, 2020.\\n\\nVershynin, R. *High-Dimensional Probability: An Introduction with Applications in Data Science*. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.\\n\\nWang, H. Fundamental limits and strong consistency of binary non-uniform hypergraph stochastic block model. *arXiv preprint arXiv:2306.06845*, 2023.\\n\\nWei, R., Yin, H., Jia, J., Benson, A. R., and Li, P. Understanding non-linearity in graph neural networks from the bayesian-inference perspective. *arXiv preprint arXiv:2207.11311*, 2022.\\n\\nWu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Weinberger, K. Simplifying graph convolutional networks. In *International conference on machine learning*, pp. 6861\u20136871. PMLR, 2019.\"}"}
{"id": "8m4V6Fx6ma", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exact Recovery in Semi-Supervised Learning\\n\\nWu, X., Chen, Z., Wang, W., and Jadbabaie, A. A non-asymptotic analysis of oversmoothing in graph neural networks. arXiv preprint arXiv:2212.10701, 2022.\\n\\nYang, Z., Cohen, W., and Salakhudinov, R. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pp. 40\u201348. PMLR, 2016.\"}"}
{"id": "8m4V6Fx6ma", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Information-theoretic limits\\n\\nIn this section, we will provide the proofs for Theorem 3.2 and Theorem 3.3.\\n\\nA.1. Impossibility for exact recovery\\n\\nThe proof sketch of Theorem 3.2 is presented in this section, with some proofs of Lemmas deferred.\\n\\nLet \\\\( y \\\\in \\\\{\u00b11\\\\}^N \\\\) denote the true label vector with \\\\( y = [y_L^\\\\top, y_U^\\\\top] \\\\), where \\\\( y_L \\\\) and \\\\( y_U \\\\) denote the observed and uncovered label vector respectively. Assume \\\\((A, X) \\\\sim CSBM(y, \\\\mu, \\\\alpha, \\\\beta, \\\\theta)\\\\) as in model 2.5, and the access to \\\\((A, X, y_L)\\\\) are provided.\\n\\nLet \\\\( b_{y_U} \\\\in \\\\{\u00b11\\\\}^m \\\\) denote an estimator of \\\\( y_U \\\\) obtained from algorithm. The probability that \\\\( b_{y_U} \\\\) fails recovering every entry of \\\\( y_U \\\\) is \\\\( P_{\\\\text{fail}} := P(b_{y_U} \\\\neq \u00b1y_U) = \\\\sum A, X, y_L [1 - P(b_{y_U} = \u00b1y_U| A, X, y_L)] \\\\cdot P(A, X, y_L) \\\\), where the Maximum A Posteriori (MAP) estimator achieves its minimum. Since the prior distribution of \\\\( y \\\\) is uniform sampled in Definition 2.5, the discussion on the ideal estimator can be transferred to Maximum Likelihood Estimation (MLE), \\\\( b_{y_{\\\\text{MLE}}} := \\\\text{arg max}_{z \\\\in \\\\{\u00b11\\\\}^m, 1^\\\\top z = 0} P(A|y_L, y_U = z) \\\\cdot P(X|y_L, y_U = z) \\\\).\\n\\nFurthermore, Lemma A.3 shows the function that MLE is maximizing over \\\\( z \\\\in \\\\{\u00b11\\\\}^m \\\\)\\n\\n\\\\[\\nf(z) := \\\\log P(A|y_L, y_U = z) + \\\\log P(X|y_L, y_U = z).\\\\tag{A.2}\\n\\\\]\\n\\nFrom the discussion above, MLE is equivalent to the best estimator MAP. No algorithm would be able to assign all labels correctly if MLE fails. In the view of (A.2), the failure of MLE indicates that some configuration \\\\( \\\\sigma \\\\in \\\\{\u00b11\\\\}^m \\\\) other than the true \\\\( y_U \\\\) achieves its maximum, and MLE prefers \\\\( \\\\sigma \\\\) other than \\\\( y_U \\\\).\\n\\nTo establish the necessity, we explicitly construct some \\\\( \\\\sigma \\\\in \\\\{\u00b11\\\\}^m \\\\) with \\\\( 1^\\\\top \\\\sigma = 0 \\\\) such that \\\\( \\\\sigma \\\\neq y_U \\\\) but \\\\( f(\\\\sigma) \\\\geq f(y_U) \\\\) when below the threshold, i.e., \\\\( I_{\\\\tau}(a, b, c) < 1 \\\\). An example of such \\\\( \\\\sigma \\\\) can be constructed as follows. Pick \\\\( u \\\\in V_U^+, \\\\) and \\\\( v \\\\in V_U^-, \\\\) where \\\\( V_U^+, \u00b1 = V_U \\\\cap V^\u00b1, \\\\) and switch the labels of \\\\( u \\\\) and \\\\( v \\\\) in \\\\( y_U \\\\) but keep all the others. Lemma A.1 characterizes the scenarios of failing exact recovery in terms of \\\\( u \\\\) and \\\\( v \\\\).\\n\\nLemma A.1.\\n\\nGiven some subset \\\\( S \\\\subset V = [N] \\\\), for vertex \\\\( u \\\\in U \\\\), define the following random variable\\n\\n\\\\[\\nW_{m,u}(S) := y_u \\\\cdot \\\\log (a/b) \\\\cdot \\\\sum_{j \\\\in S} A_{uj} y_j + 2N + \\\\frac{d}{\\\\theta} \\\\sum_{j \\\\in S} \\\\langle x_u, x_j \\\\rangle y_j.\\\\tag{A.3}\\n\\\\]\\n\\nDenote by \\\\( W_{m,u} := W_{m,u}(\\\\{N\\\\} \\\\setminus \\\\{u\\\\}) \\\\) for any \\\\( u \\\\in V_U \\\\). Define the rate function\\n\\n\\\\[\\nI(t, a_\\\\tau, b_\\\\tau, c_\\\\tau) := \\\\frac{1}{2} a_\\\\tau - \\\\frac{a_\\\\tau}{a_\\\\tau b_\\\\tau} t - \\\\frac{b_\\\\tau}{a_\\\\tau b_\\\\tau} t + \\\\frac{2}{a_\\\\tau b_\\\\tau} c_\\\\tau (t + t^2).\\\\tag{A.4}\\n\\\\]\\n\\nThen, it supreme over \\\\( t \\\\) is attained at \\\\( t^\\\\star = -1/2 \\\\),\\n\\n\\\\[\\n\\\\sup_{t \\\\in \\\\mathbb{R}} I(t, a_\\\\tau, b_\\\\tau, c_\\\\tau) = I(-1/2, a_\\\\tau, b_\\\\tau, c_\\\\tau) = \\\\frac{1}{2} \\\\left( \\\\sqrt{a_\\\\tau - p b_\\\\tau} + c_\\\\tau \\\\right) =: I(a_\\\\tau, b_\\\\tau, c_\\\\tau),\\n\\\\]\\n\\nwhere the last equality holds as in (3.2).\\n\\n(a) For any \\\\( \\\\epsilon < a - b^2 (1 - \\\\tau) \\\\log (a/b) + 2c_\\\\tau \\\\) and \\\\( \\\\delta > 0 \\\\), there exists some sufficiently large \\\\( m_0 > 0 \\\\), such that for \\\\( I(t, a_\\\\tau, b_\\\\tau, c_\\\\tau) \\\\) in (A.4), the following holds for any \\\\( m \\\\geq m_0 \\\\)\\n\\n\\\\[\\nP(W_{m,u} \\\\leq \\\\epsilon q m) = (1 + o(1)) \\\\cdot \\\\exp \\\\left( -q m \\\\cdot \\\\delta + \\\\sup_{t \\\\in \\\\mathbb{R}} \\\\{ \\\\epsilon t + I(t, a_\\\\tau, b_\\\\tau, c_\\\\tau) \\\\} \\\\right).\\n\\\\]\\n\\n(b) For the pair \\\\( u \\\\in V_U^+, \\\\) and \\\\( v \\\\in V_U^-, \\\\), the event \\\\( \\\\{W_{m,u} \\\\leq 0\\\\} \\\\cap \\\\{W_{m,v} \\\\leq 0\\\\} \\\\) implies \\\\( f(y_U) \\\\leq f(\\\\sigma) \\\\) with probability at least \\\\( 1 - e^{-q m} \\\\).\"}"}
{"id": "8m4V6Fx6ma", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we complete the proof of Theorem 3.9 in Section 3.4. Recall that\\n\\\\[ W(1) = W(0) - \\\\eta_1 \\\\nabla W(0) L(W(0), s(0)) + \\\\lambda_1 W(0). \\\\] (D.1)\\n\\nThe algorithm we applied in Theorem 3.9 is given by Algorithm 1. Below, we first construct an optimal solution for this problem and present the LDP analysis. Then, we present will use (Ba et al., 2022; Damian et al., 2022) to analyze the feature learned from \\\\( W(1) \\\\). Finally, inspired by the optimal solution, we will prove \\\\( s(1) \\\\) is close to the optimal \\\\( s \\\\) in (3.11).\\n\\nMeanwhile, we also present an additional gradient-based method in Algorithm 2 to approach the optimal \\\\( s \\\\) in (3.11). We will leave the theoretical analysis for Algorithm 2 as a future direction to explore.\\n\\n**Algorithm 1**\\nGradient-based training for GCN in Theorem 3.9\\n\\n**Input:**\\nLearning rates \\\\( \\\\eta_1 \\\\), weight decay \\\\( \\\\lambda_1 \\\\).\\n\\n**Initialization:**\\n\\\\( s(0) = 0 \\\\), \\\\[ \\\\sqrt{K \\\\cdot [W_0]} \\\\] \\\\( i \\\\sim N(0, 1) \\\\), \\\\[ \\\\sqrt{K \\\\cdot [a]} \\\\] \\\\( j \\\\sim \\\\text{Unif}\\\\{\\\\pm 1\\\\} \\\\), for all \\\\( i \\\\in [d] \\\\), \\\\( j \\\\in [K] \\\\).\\n\\n**Training Stage 1:**\\nSet \\\\( \\\\sigma(x) = x \\\\) in (3.12)\\n\\\\( W(1) \\\\leftarrow W(0) - \\\\eta_1 (\\\\nabla W(0) L(W(0), s(0)) + \\\\lambda_1 W(0)) \\\\).\\n\\n**Training Stage 2:**\\n\\\\( s(1) \\\\leftarrow s(0) + 2n^2q^m y^\\\\top L(X)L W(1) a \\\\cdot \\\\log N \\\\cdot D_0 + y^\\\\top L A L y L N \\\\cdot D_0 - y^\\\\top L A L y L \\\\).\\n\\n**return**\\nPrediction function for unknown labels:\\n\\\\[ \\\\text{sign}(S U D - 1 s(1) A s(1) W(1)) \\\\]\\n\\n**D.1. Thresholds for GCNs and LDP analysis**\\nConsider \\\\((A, X) \\\\sim \\\\text{CSBM}(y, \\\\mu, \\\\alpha, \\\\beta, \\\\theta)\\\\). Let \\\\( A \\\\in \\\\mathbb{R}^{N \\\\times N} \\\\) denote the adjacency matrix of the graph \\\\( G \\\\) and let us define the degree matrix by \\\\( D_0 := \\\\text{diag}\\\\{D_0, \\\\ldots, D_0\\\\} \\\\in \\\\mathbb{R}^{N \\\\times N} \\\\) where \\\\( D_0 = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\sum_{j=1}^{N} A_{ij} \\\\). Let \\\\( \\\\rho = \\\\text{sqm} \\\\) denote the self-loop intensity (Kipf & Welling, 2017; Wu et al., 2019; Shi et al., 2024) for some \\\\( s \\\\in \\\\mathbb{R} \\\\), and \\\\( A_s = A + \\\\rho I \\\\), \\\\( D_s = D + \\\\rho I \\\\) denote the adjacency, average degree matrices of the graph after adding self-loops, respectively.\\n\\nThe convolutional feature vector is \\\\( e_x_i := (D_s)^{-1} A_s X_i \\\\).\\n\\nIdeally, our goal is to prove that the convoluted feature vectors are linearly separable, i.e., find some \\\\( w \\\\in \\\\mathbb{R}^d \\\\) such that \\\\( e_x_i^\\\\top w + b > 0 \\\\) if \\\\( y_i = 1 \\\\), \\\\( e_x_i^\\\\top w + b < 0 \\\\) if \\\\( y_i = -1 \\\\), for some \\\\( b \\\\geq 0 \\\\). We consider the case that the feature learned by the GCN is exactly \\\\( \\\\mu \\\\) in GMM, i.e., the optimal margin is \\\\( w = \\\\mu \\\\). Under this setting, we show the LDP results for this estimator. The proof is similar to Proposition C.1.\\n\\n**Proposition D.1** (LDP for GCNs).\\nFor \\\\((A, X) \\\\sim \\\\text{CSBM}(y, \\\\mu, \\\\alpha, \\\\beta, \\\\theta)\\\\) with Assumption 3.1, when \\\\( s = \\\\frac{2c_{\\\\tau}}{\\\\log(ab)} \\\\) and \\\\( c_{\\\\tau} = \\\\theta_2^2/qm + o(1) \\\\), we have that\\n\\\\[ \\\\lim_{m \\\\to \\\\infty} q^{-1} m \\\\log P(y_i \\\\cdot \\\\sqrt{qm} (E D_s)^{-1} (A_s X) \\\\leq \\\\epsilon q m) = -\\\\sup_{t \\\\in \\\\mathbb{R}} \\\\{\\\\epsilon t + I(a_{\\\\tau}, b_{\\\\tau}, c_{\\\\tau}, t)\\\\}, \\\\]\\nwhere \\\\( \\\\sup_{t \\\\in \\\\mathbb{R}} I(a_{\\\\tau}, b_{\\\\tau}, c_{\\\\tau}, t) \\\\) defined in (3.2).\"}"}
{"id": "8m4V6Fx6ma", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof. For simplicity, let $e := \\\\frac{1}{N} \\\\sum_{i=1}^{N} d_i = a + b q m + \\\\rho$, denoting the expected degree of each node $i \\\\in [N]$. Then $h_i := y_i x^\\\\top w = y_i \\\\theta (E eD) - 1 (e AX)_i$:\\n\\n$$h = y_i \\\\theta e X_j \\\\in [N] \\\\theta^\\\\top z_j + \\\\rho \\\\theta^\\\\top (\\\\theta y_i \\\\mu + z_i) e d X_j \\\\notin [N] \\\\theta y_j \\\\mu^\\\\top z_j + \\\\rho \\\\mu^\\\\top (\\\\theta y_i \\\\mu + z_i) e d X_j$$\\n\\n$$= \\\\rho \\\\theta^2 y_i^2 \\\\| \\\\mu \\\\|_2^2 e d | I_1 + \\\\rho \\\\theta y_i \\\\mu^\\\\top z_i e d X_j \\\\notin [N] A_{ij} y_i y_j | I_2 + \\\\theta^2 \\\\| \\\\mu \\\\|_2^2 e d X_j \\\\notin [N] y_i A_{ij} \\\\mu^\\\\top z_j | I_3 + \\\\theta e d X_j \\\\notin [N] y_i A_{ij} \\\\mu^\\\\top z_j | I_4.$$ \\n\\nOur goal is to calculate the following moment-generating function $E [\\\\exp (th_i)] := E A \\\\left[ E X [\\\\exp (th_i) | A] \\\\right]$.\\n\\nFirst, since $\\\\| \\\\mu \\\\|_2^2 = 1$, $y_i^2 = 1$, $I_1 = \\\\frac{\\\\rho \\\\theta^2}{e d^2}$, and it is deterministic. Second, $\\\\mu^\\\\top z_i \\\\sim N(0, 1)$, then $I_2 \\\\sim N(0, \\\\rho \\\\frac{\\\\theta^2}{e d^2})$, and $E X [\\\\exp (tI_2) | y_i] = \\\\exp (t^2 \\\\rho \\\\frac{\\\\theta^2}{e d^2})$, where the last equality holds since the result we obtained is independent of $y_i$. Let $N_i$ denote the set of neighbors of node $i$ and $|N_i|$ denote its cardinality. Conditioned on $A$, $y$, $\\\\mu$, $I_4 \\\\sim N(0, |N_i| \\\\frac{\\\\theta^2}{e d^2})$, and $E X [\\\\exp (t(I_3 + I_4)) | A, y_i, \\\\mu] = \\\\exp (t^2 \\\\theta^2 |N_i| \\\\frac{\\\\theta^2}{e d^2})$.\\n\\nNote that $|N_i| = \\\\sum_{j=1}^{N} A_{ij}$, and $I_3$ is independent of $X$, then $E X [\\\\exp (t(I_3 + I_4)) | A, y_i, \\\\mu] = \\\\exp (t^2 \\\\theta^2 |N_i| \\\\frac{\\\\theta^2}{e d^2})$.\\n\\nOne could take the expectation over $A$ conditioned on $y$, then $E A h \\\\exp (t \\\\theta^2 e d A_{ij} y_i y_j + t^2 e d y_i y_j) | y_i = 1 2 E A h \\\\exp (t \\\\theta^2 e d A_{ij} y_i y_j + t^2 e d y_i y_j) | y_i = 1 + 1 2 E A h \\\\exp (t \\\\theta^2 e d A_{ij} y_i y_j + t^2 e d y_i y_j) | 1 = 1$.\\n\\nAssumption 3.1, thus $\\\\theta^2 = (1 + o(1)) c \\\\tau q m$. By using $\\\\log (1 + x) = x$ for $x = o(1)$, we then have $q_{\\\\tau} - 1 m \\\\log E A E X [\\\\exp \\\\{ t(I_3 + I_4) \\\\}] = \\\\log (1 + \\\\alpha^2 h \\\\exp (t^2 \\\\theta^2 e d^2 + t^2 \\\\theta e d) - 1 i + \\\\beta^2 h \\\\exp (t^2 \\\\theta^2 e d^2 - t^2 \\\\theta e d) - 1 i) = (1 + o(1)) a^2 h \\\\exp (t \\\\theta^2 e d^2 + t^2 \\\\theta e d) - 1 i + (1 + o(1)) b^2 h \\\\exp (t \\\\theta^2 e d^2 - t^2 \\\\theta e d) - 1 i).$\"}"}
{"id": "8m4V6Fx6ma", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Recall $\\\\alpha = \\\\frac{a q}{m} = o(1)$, $\\\\beta = \\\\frac{b q}{m} = o(1)$, and $\\\\theta_4 \\\\theta_2 + (1 - \\\\tau) d/m = c \\\\tau q m$ in Assumption 3.1, thus $\\\\theta_2 = (1 + o(1)) c \\\\tau q m$. By using $\\\\log(1 + x) = x$ for $x = o(1)$, the rate function $g(a, b, c \\\\tau, t)$ can be calculated as\\n\\n$$g(a, b, c \\\\tau, t) = -t \\\\rho \\\\theta_2 q m e^d - t^2 \\\\rho^2 \\\\theta_2 e^d \\\\frac{N - 1}{2m} h a - a \\\\exp t \\\\theta_2 e^d + b - b \\\\exp t \\\\theta_2 e^d - t \\\\theta_2 e^d i = 1 - (1 - \\\\tau) h a \\\\frac{1 - \\\\exp 2 c \\\\tau t a + b + 2 s}{2} + b - b \\\\exp -2 c \\\\tau s t a + b + 2 s - 2 c \\\\tau s t a + b + 2 s - 2 c \\\\tau s t a + b + 2 s}$$\\n\\nwhere in the last line, we used $\\\\rho = q m e d = (a + b + 2 s) q m$. By choosing $s = \\\\frac{2 c \\\\tau}{\\\\log(a/b)}$, we can conclude that $I^* = \\\\sup_{t \\\\in \\\\mathbb{R}} I(a \\\\tau, b \\\\tau, c \\\\tau, t) = (1 - \\\\tau) \\\\frac{1}{\\\\sqrt{a} - \\\\sqrt{b}}^2 + c \\\\tau = I(a \\\\tau, b \\\\tau, c \\\\tau)$, which completes the proof.\\n\\nD.2. Gradient descent for the first layer weight matrix\\n\\nFor simplicity, we denote $f_X = D - 1 s A s X = (e_x^1, \\\\ldots, e_x^N)\\\\top$ where $e_x^i \\\\in \\\\mathbb{R}^d$ for $i \\\\in [N]$ and $s = 0$. In this case, we will explore the feature learning on $W$. Below, we will always fix $a$ (at initialization in Assumption 3.8) and perform gradient descent on $W$ in (D.1). To ease the notions, we write the initialized first-layer weights as $W(0) = W_0$, and the weights after one gradient step as $W(1) = W_1$, where the learning rate of the first gradient descent is $\\\\eta_1 > 0$. Let $s(0) = 0$.\\n\\nFollowing the notions in (Ba et al., 2022), we denote that $G_1 := -\\\\nabla_W L(W_0, s(0)) = 1/n f_X\\\\top L_1 \\\\sqrt{K} y_L - 1/\\\\sqrt{K} \\\\sigma(f_X L W_0) a\\\\top \\\\odot \\\\sigma'(f_X L W_0)$, where $f_X L = S_L f_X \\\\in \\\\mathbb{R}^{n \\\\times d}$, $\\\\odot$ is the Hadamard product, and $\\\\sigma'$ is the derivative of $\\\\sigma$ (acting entry-wise). Here $K$ represents the number of neurons in the hidden GCN layer in (3.12). Then, from (D.1) with $\\\\lambda_1 = \\\\eta_1 - 1, \\\\eta_1$, we have $W(1) = W_0 + \\\\eta_1 G_1 - W_0 = \\\\eta_1 G_1$.\\n\\nThus, our target is to analyze the gradient matrix $G_1$. The following proposition is similar to Proposition 2 in (Ba et al., 2022), implies that this gradient matrix is approximately rank one.\\n\\n**Proposition D.2.** Under the same assumption as Theorem 3.9, we have that $G_1 - 1/n \\\\sqrt{K} f_X\\\\top L y_L a \\\\top F \\\\leq C q m K$, with probability at least $1 - \\\\exp(-c \\\\log 2 N)$, for some constant $c, C > 0$.\\n\\n**Proof.** First of all, analogously to the proof of Lemma C.5, we can show that $f_X L \\\\leq p q m N$, (D.3) with very high probability, since $d \\\\approx N$. Moreover, $\\\\|y_L\\\\| = \\\\sqrt{n}$ and we can always view $y_L$ as a deterministic vector in $\\\\mathbb{R}^N$.\\n\\nBy the definition, the gradient matrix $G_1$ under the MSE can be simplified as follows\\n\\n$$G_1 = -1/n f_X\\\\top L \\\\sqrt{K} 1/\\\\sqrt{K} \\\\sigma(f_X L W_0) a\\\\top \\\\sigma'(f_X L W_0) \\\\leq 1/n \\\\cdot \\\\mu 1/\\\\sqrt{K} f_X\\\\top L y_L - 1/\\\\sqrt{K} \\\\sigma(f_X L W_0) a\\\\top \\\\sigma'(f_X L W_0)$$\\n\\nwhere we utilized the orthogonal decomposition: $\\\\sigma'(z) = \\\\mu 1 + \\\\sigma'_{\\\\perp}(z)$. By Stein's lemma, we know that $E[z \\\\sigma(z)] = E[\\\\sigma'(z)] = \\\\mu 1$, and hence $E[\\\\sigma'_{\\\\perp}(z)] = 0$ for $z \\\\sim N(0, 1)$. Notice that we consider $\\\\sigma(x) = x$, hence $\\\\mu 1 = 1$ and $\\\\sigma'_{\\\\perp}(z) \\\\equiv 0$. Therefore, we have $G_1 = 1/n \\\\cdot 1/\\\\sqrt{K} f_X\\\\top L y_L - 1/nK f_X\\\\top L f_X L W_0 a\\\\top |\\\\{z\\\\} \\\\Delta$. \\n\\n32\"}"}
{"id": "8m4V6Fx6ma", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Notice that $\\\\|a\\\\| = 1$ and we can apply (C.5) for Gaussian random matrix $W_0$. Thus, because of Lemma C.5, $d \\\\lesssim n \\\\approx N$ and $d \\\\lesssim K$, we have that $\\\\|\\\\Delta\\\\|_{F} \\\\leq Cq m K (1 + p d / K)$, with very high probability, which completes the proof of this proposition.\\n\\nThis proposition shows that for $W$ at Gaussian initialization, the corresponding gradient matrix can be approximated in operator norm by the rank-1 matrix only related to labels $y_L$, feature matrix $f_X L$, and $a$.\\n\\nIn the following, we will use the parameter $\\\\zeta := \\\\sqrt{c \\\\tau \\\\eta_1} \\\\sqrt{q m K} \\\\alpha - \\\\beta \\\\alpha + \\\\beta \\\\mu$.\\n\\nNotice that $\\\\zeta = \\\\Theta(1)$ if $K / \\\\eta_1 = \\\\Theta(\\\\sqrt{q m})$. Then we can tune the learning rate $\\\\eta_1$ to ensure that this trained and normalized weight matrix $1 / \\\\sqrt{K} W (1) a - \\\\sqrt{c \\\\tau \\\\eta_1} \\\\sqrt{q m K} \\\\alpha - \\\\beta \\\\alpha + \\\\beta \\\\mu = O(\\\\eta_1 K)$, with a probability at least $1 - c N^{-10}$, for some constants $c, C > 0$.\\n\\n**Lemma D.3.** Under the assumption as Theorem 3.9, we have that $1 / \\\\sqrt{K} W (1) a - \\\\sqrt{c \\\\tau \\\\eta_1} \\\\sqrt{q m K} \\\\alpha - \\\\beta \\\\alpha + \\\\beta \\\\mu = O(\\\\eta_1 K)$, with probability at least $1 - c N^{-10}$, for some constants $c, C > 0$.\\n\\n**Proof.** Notice that $W_1 = \\\\eta_1 G_1$ and $a^\\\\top a = 1$. Notice that $f_X^\\\\top L y_L = \\\\sqrt{Nq m} \\\\cdot h(X)^\\\\top P_L y_L$. Following from Proposition D.2 and Lemma C.7, we can have $1 / \\\\sqrt{K} W (1) a - \\\\zeta \\\\mu \\\\leq 1 / \\\\sqrt{K} \\\\eta_1 \\\\Delta a + \\\\zeta \\\\mu - \\\\eta_1 n K f_X^\\\\top L y_L a^\\\\top a \\\\lesssim \\\\eta_1 K + \\\\eta_1 q m K^{3/2}$, with very high probability. Notice that here $a^\\\\top a = 1$. Then, we can assume $q m \\\\lesssim K$ to finish this proof.\\n\\n**D.3. Learning the optimal self-loop weight**\\n\\n**Lemma D.4.** Under Assumption 3.1, we know that $D_0 - \\\\bar{d} \\\\leq C q^{-1/2} m$, with probability at least $1 - c e^{-N}$ for some constants $c, C > 0$, where $\\\\bar{d} := a \\\\tau + b \\\\tau^2 q m$.\\n\\nThis is straightforward based on the proof of Lemma C.3, hence we ignore the proof here.\\n\\n**Lemma D.5.** Under the assumption as Theorem 3.9, we have that $2 n^2 q m y^\\\\top L X L W (1) a - 2 c \\\\tau \\\\leq C n \\\\sqrt{q m}$ with probability at least $1 - c N^{-10}$, for some constants $c, C > 0$.\\n\\n**Proof.** By Proposition D.2 and Lemma D.3, we can replace $W (1) a$ by $\\\\zeta \\\\mu$. Notice that (D.3) and Lemma D.3 indicate that $2 n^2 q m y^\\\\top L X L (W (1) a - \\\\zeta \\\\mu) \\\\leq 2 n^2 q m \\\\|y_L\\\\| \\\\cdot \\\\|X_L\\\\| \\\\cdot \\\\|W (1) a - \\\\zeta \\\\mu\\\\| \\\\lesssim 1 / (n q m)$,\"}"}
{"id": "8m4V6Fx6ma", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let us denote the expectation of the average degree of $A\\\\rho$ by $e_d := \\\\frac{1}{N} \\\\sum_{i=1}^{N} E[D\\\\rho]_{ii} = a\\\\tau + b\\\\tau^2 q m + \\\\rho$. (C.1)\\n\\nHowever, $h(X)$ is hard to deal with when we consider its large deviation principle. Instead, we use the following $e_h(X) = e_d \\\\cdot \\\\sqrt{Nm} A\\\\rho X$.\\n\\n**C.1. Large Deviation Results for Ridge Regression Estimators**\\n\\nFor any $i \\\\in V$, we denote $N_i \\\\subset V$ as the neighborhood of vertex $i \\\\in V$. We consider the case that the feature learned by the GCN is $\\\\zeta \\\\sqrt{q m}$ for some constant $\\\\zeta$, i.e., $w = \\\\zeta \\\\sqrt{q m} \\\\mu$. Notice that $h_i = y_i \\\\zeta \\\\sqrt{q m} (ED\\\\rho) - 1 (A\\\\rho X)_i$: $\\\\mu = y_i \\\\zeta \\\\sqrt{q m} e_d X_j \\\\in N_i \\\\mu^\\\\top (\\\\theta y_j \\\\mu + z_j) + \\\\rho \\\\mu^\\\\top (\\\\theta y_i \\\\mu + z_i)$.\\n\\nHere $N_i$ is the neighborhood of vertex $i \\\\in V$. Proposition C.1 (LDP for Ridge Regression).\\n\\nUnder the Assumption 3.1 with $(A, X) \\\\sim CSBM(y, \\\\mu, \\\\alpha, \\\\beta, \\\\theta)$ and $\\\\rho = sq m$ for some constant $s \\\\in R$. Assume $d/N \\\\ll q m$, then for any fixed $i \\\\in V$ and constant $\\\\zeta > 0$, we have\\n\\n$$\\\\lim_{m \\\\to \\\\infty} q^{-1} m \\\\log P(y_i \\\\zeta (ED\\\\rho) - 1 (A\\\\rho X)_i : \\\\mu \\\\leq \\\\epsilon \\\\sqrt{q m}) = -\\\\sup_{t \\\\in R} \\\\{\\\\epsilon t + g(a, b, c, \\\\tau, \\\\zeta, s, t)\\\\}$$\\n\\nfor sufficiently small $\\\\epsilon > 0$ and all large $m$, where $g(a, b, c, \\\\tau, \\\\zeta, s, t) = g_1(t) + g_2(t)$,\\n\\n$$g_1(t) = -2ts\\\\zeta \\\\sqrt{c \\\\tau a + b \\\\tau + 2s} - 2t^2s^2\\\\zeta^2 (a \\\\tau + b \\\\tau + 2s)^2,$$\\n\\n$$g_2(t) = -a \\\\tau^2 \\\\exp \\\\left(2t\\\\zeta \\\\sqrt{c \\\\tau a + b \\\\tau + 2s} - 1 \\\\right) - b \\\\tau^2 \\\\exp \\\\left(-2t\\\\zeta \\\\sqrt{c \\\\tau a + b \\\\tau + 2s} - 1 \\\\right).$$\\n\\nConsequently, for any sufficiently small $\\\\epsilon > 0$ and any $\\\\delta > 0$, there exists some $N_0 > 0$ such that for all $N \\\\geq N_0$, we have\\n\\n$$P(y_i \\\\zeta (ED\\\\rho) - 1 (A\\\\rho X)_i : \\\\mu \\\\leq \\\\epsilon \\\\sqrt{q m}) = \\\\exp \\\\left(-q m \\\\left[\\\\sup_{t \\\\in R} \\\\{\\\\epsilon t + g(a, b, c, \\\\tau, \\\\zeta, s, t)\\\\} - \\\\delta \\\\right]\\\\right).$$\\n\\n**Proof of Proposition C.1.** Our goal is to calculate the following moment-generating function $E[\\\\exp(th_i)] := E_A [E_X [\\\\exp(th_i) | A]]$.\\n\\nFirst, since $\\\\|\\\\mu\\\\|_2 = 1$, $y_i^2 = 1$, then in (C.2), $I_1 = \\\\rho \\\\zeta \\\\theta \\\\sqrt{q m} / e_d$, and it is deterministic. Second, $\\\\mu^\\\\top z_i \\\\sim N(0, 1)$, then $I_2 \\\\sim N(0, \\\\rho^2 \\\\zeta^2 q m / e_d^2)$, and\\n\\n$$E_X [\\\\exp(tI_2) | y_i] = \\\\exp(t^2 \\\\rho^2 \\\\zeta^2 q m / e_d^2) = E [\\\\exp(tI_2)],$$\\n\\nwhere the last equality holds since the result we obtained is independent of $y_i, A$. Let $N_i$ denote the set of neighbors of node $i$ and $|N_i|$ denote its cardinality.\"}"}
{"id": "8m4V6Fx6ma", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Moreover, if\\n\\nwhere the last equality holds since we apply $e$\\n\\nThen, we can apply Lemma H.5 in (Abbe et al., 2022) to conclude our results in this proposition.\\n\\nLemma C.2.\\n\\nFor function $f$, we have\\n\\nCombining the calculations above, we then compute the following rate function\\n\\nOne could take the expectation over $X_j$ conditioned on\\n\\nNote that\\n\\nConditioned on $X_j$, then\\n\\nand the equality is attained when\\n\\nAt the same time, the result above is again independent of $y$ and\\n\\nFor some fix $x$, we then have\\n\\nExact Recovery in Semi-Supervised Learning\"}"}
{"id": "8m4V6Fx6ma", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof of Lemma C.2.\\n\\nNotice that both $g_1(t)$ and $g_2(t)$ are concave. First, $g_1(t)$ achieves its maximum at the point $t_1 := -\\\\sqrt{\\\\frac{c}{\\\\tau(a+b+2s)}} / (2s\\\\zeta)$, and $g_2(t)$ achieves its maximum at the point $t_2 := (a+b+2s) \\\\log(b/a) / (4\\\\zeta\\\\sqrt{c\\\\tau})$. Note that\\n\\n$$\\\\sup_{t \\\\in \\\\mathbb{R}} g(a\\\\tau, b\\\\tau, c\\\\tau, \\\\zeta, s, t) \\\\leq \\\\max_{t \\\\in \\\\mathbb{R}} g_1(t) + \\\\max_{t \\\\in \\\\mathbb{R}} g_2(t) = g_1(t_1) + g_2(t_2), \\\\quad (C.3)$$\\n\\nThus, this proves the upper bound on $J(a\\\\tau, b\\\\tau, c\\\\tau, \\\\zeta, s)$. Notice that the equality in (C.3) holds if $t_1 = t_2$. It turns out that when $s = 2c\\\\tau \\\\log(a/b)$, then $t_1 = t_2$, and $g_1(t_1) = \\\\frac{c\\\\tau}{2}$, $g_2(t_2) = \\\\frac{\\\\sqrt{a} - \\\\sqrt{b}}{2(1-\\\\tau)}$. Therefore, in this case, we have\\n\\n$$\\\\max_{t \\\\in \\\\mathbb{R}} g(a, b, c, \\\\tau, \\\\zeta, s, t) = \\\\left(1 - \\\\tau\\\\right)^{-1} \\\\frac{\\\\sqrt{a} - \\\\sqrt{b}}{2(1-\\\\tau)} + \\\\frac{c\\\\tau}{2} = I(a\\\\tau, b\\\\tau, c\\\\tau).$$\\n\\nC.2. Preliminary Lemmas on Ridge Regression Estimator\\n\\nNote the facts that\\n\\n$$B^\\\\top B + I_d - 1 B^\\\\top = B^\\\\top (BB^\\\\top + I_N) - 1$$\\n\\nfor any matrix $B_{N \\\\times d}$, $P_2 U = P U$, $P_2 L = P L$, and $P U = I_N - P L$, then,\\n\\n$$h(X)b\\\\beta = h(X)(h(X)^\\\\top P L h(X) + \\\\lambda I_d) - 1 h(X)^\\\\top P L y = h(X)[(P L h(X))^\\\\top P L h(X) + \\\\lambda I_N] - 1 y.$$\\n\\nConsequently, the test risk can be written as\\n\\n$$R(\\\\lambda) = \\\\frac{1}{m} y^\\\\top Q^\\\\top P U Q y,$$\\n\\nwhere $Q := h(X) h(X)^\\\\top P L [P L h(X)^\\\\top P L + \\\\lambda I_N] - 1 y$.\\n\\nLemma C.3.\\n\\nAssume that $|\\\\rho| \\\\lesssim q m$ and $q m \\\\gtrsim \\\\log N$. Let $D_\\\\rho$ be the diagonal matrix where each diagonal represents the average degree of the graph $A_\\\\rho$ after adding the self-loop $\\\\rho$ and let $e_d$ denote the expected average degree of $A_\\\\rho$. Then\\n\\n$$\\\\|D - 1_\\\\rho - (e_d - 1)\\\\| \\\\lesssim q^{-3/2} m$$\\n\\nwith probability at least $1 - e^{-N}$. Furthermore, with probability at least $1 - 2N^{-10}$, $e^{-N}$,\\n\\n$$\\\\|D - 1_\\\\rho A_\\\\rho - E A_\\\\rho / e_d\\\\| \\\\lesssim q^{-1/2} m.$$\\n\\nConsequently, $\\\\|D - 1_\\\\rho A_\\\\rho\\\\| \\\\leq C$ with probability at least $1 - 2N^{-10}$ for some constant $C > 0$.\\n\\nProof of Lemma C.3.\\n\\nFirst, for any $i \\\\in [N]$, note that $[D_\\\\rho]_{ii} = \\\\frac{1}{N} \\\\sum_{j=1}^N (\\\\rho + \\\\sum_{j \\\\neq i} A_{ij}) = \\\\rho + \\\\frac{1}{N} \\\\sum_{j=1}^N \\\\sum_{j \\\\neq i} A_{ij}$, and $e_d = \\\\mathbb{E}[D_\\\\rho]_{ii} = a\\\\tau + b\\\\tau 2q m + \\\\rho$, then by Bernstein inequality,\\n\\n$$\\\\mathbb{P}[D_{ii} - e_d \\\\geq \\\\sqrt{q m}] = \\\\mathbb{P}\\\\left[\\\\sum_{i=1}^N X_{ij} - \\\\mathbb{E}X_{ij} \\\\geq \\\\sqrt{q m}\\\\right] \\\\lesssim \\\\exp\\\\left(-N\\\\sqrt{q m}\\\\right) \\\\lesssim \\\\exp\\\\left(-N\\\\right).$$\\n\\nThus by comparing the entrywise difference of $[D_\\\\rho]_{ii} - e_d$, with probability at least $1 - e^{-N}$, we have\\n\\n$$\\\\|D - 1_\\\\rho - (e_d - 1)\\\\| \\\\lesssim q^{-3/2} m. \\\\quad (C.4)$$\\n\\nFor the second part of the statement, by the triangle inequality, we have\\n\\n$$\\\\|D - 1_\\\\rho A_\\\\rho - E A_\\\\rho / e_d\\\\| \\\\leq \\\\|D - 1_\\\\rho A_\\\\rho - D - 1_\\\\rho E A_\\\\rho\\\\| + \\\\|D - 1_\\\\rho E A_\\\\rho - E A_\\\\rho / e_d\\\\| \\\\leq \\\\|D - 1_\\\\rho A_\\\\rho\\\\| + \\\\|E A_\\\\rho / e_d - E A_\\\\rho / e_d\\\\| = \\\\|D - 1_\\\\rho A_\\\\rho\\\\| \\\\leq C$$\\n\\nwith probability at least $1 - 2N^{-10}$ for some constant $C > 0$.\"}"}
{"id": "8m4V6Fx6ma", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For the first term, we proved that with probability at least\\n\\\\[ 1 - e^{-N}, \\\\]\\n\\\\[ \\\\| D^{\\\\rho} \\\\|_{ii} = (1 + o(1)) e^{d} \\\\approx q m \\\\]\\nwith deviation at most \\\\( \\\\sqrt{q m} \\\\), then according to Lemma E.2, with probability at least\\n\\\\[ 1 - 2N^{-10 - 2e^{-N}}, \\\\]\\nwhen \\\\( q m \\\\gg \\\\log(N) \\\\), the following is bounded by\\n\\\\[ \\\\| D^{\\\\rho} - A^{\\\\rho} (E A^{\\\\rho}) \\\\| \\\\leq (\\\\| e^{d} - 1 \\\\| + \\\\| D^{\\\\rho} - e^{d} - 1 \\\\|) \\\\cdot \\\\| (E A^{\\\\rho}) \\\\| \\\\leq (q - 1) m + q - 3/2 m \\\\]\\n\\\\[ \\\\approx q^{-1/2} m. \\\\]\\n\\nFor the second term, note that\\n\\\\[ \\\\| E A^{\\\\rho} \\\\| \\\\approx q m, \\\\]\\nthen by results above\\n\\\\[ \\\\| D^{\\\\rho} - e^{d} - 1 \\\\| \\\\leq \\\\| E A^{\\\\rho} \\\\| \\\\approx 1 / \\\\sqrt{q m}. \\\\]\\n\\nTherefore, with probability at least\\n\\\\[ 1 - 2N^{-10 - 2e^{-N}}, \\\\]\\n\\\\[ \\\\| D^{\\\\rho} A^{\\\\rho} - E A^{\\\\rho} / e^{d} \\\\| \\\\leq q^{-1/2} m. \\\\]\\n\\nFor the last part,\\n\\\\[ \\\\| E A^{\\\\rho} / e^{d} \\\\| \\\\approx q^{-1} m, \\\\]\\nthen the proof is completed by triangle inequality since \\\\( q m \\\\gg 1 \\\\), and\\n\\\\[ \\\\| D^{\\\\rho} A^{\\\\rho} \\\\| \\\\leq \\\\| E A^{\\\\rho} / e^{d} \\\\| + \\\\| D^{\\\\rho} A^{\\\\rho} - E A^{\\\\rho} / e^{d} \\\\| \\\\leq 1 + q^{-1} m \\\\approx 1. \\\\]\\n\\nLemma C.4. Consider \\\\( X \\\\sim \\\\text{GMM}(\\\\mu, y, \\\\theta) \\\\). Suppose that \\\\( d \\\\ll N \\\\), then we have\\n\\\\[ 1 / \\\\sqrt{N q m} X - \\\\theta / \\\\sqrt{N q m} y \\\\mu^{\\\\top} \\\\leq C / \\\\sqrt{q m}, \\\\]\\nwith probability at least\\n\\\\[ 1 - 2e^{-cN} \\\\]\\nfor some constants \\\\( C, c > 0 \\\\).\\n\\nProof of Lemma C.4. Recall the concentration on the operator norm of the Gaussian random matrix for\\n\\\\( Z \\\\in \\\\mathbb{R}^{N \\\\times d} \\\\) (see Vershynin, 2018). Then for every \\\\( t > 0 \\\\), there exists some constant \\\\( c > 0 \\\\) such that\\n\\\\[ P(\\\\| Z \\\\| \\\\geq \\\\sqrt{N} + \\\\sqrt{d} + t) \\\\leq 2 \\\\exp(-ct^2). \\\\]\\n(C.5)\\nThen, we know that\\n\\\\[ \\\\| Z \\\\| \\\\sqrt{N} \\\\approx 1 \\\\]\\nwith probability at least\\n\\\\[ 1 - 2e^{-cN} \\\\]\\nby taking \\\\( t = \\\\sqrt{N} \\\\). Then we have\\n\\\\[ 1 / \\\\sqrt{N q m} X - \\\\theta / \\\\sqrt{N q m} y \\\\mu^{\\\\top} \\\\leq \\\\| Z \\\\| / \\\\sqrt{N q m} \\\\approx 1 / \\\\sqrt{q m}, \\\\]\\nwith probability at least\\n\\\\[ 1 - 2e^{-cN} \\\\]\\nLemma C.5. Consider \\\\( (A, X) \\\\sim \\\\text{CSBM}(y, \\\\mu, \\\\alpha, \\\\beta, \\\\theta) \\\\). Under the Assumption 3.1, when \\\\( d \\\\ll N \\\\), we have that\\n\\\\[ \\\\| h(X) - H \\\\| \\\\leq C / \\\\sqrt{q m}, \\\\]\\nwith probability at least\\n\\\\[ 1 - cN^{-10}, \\\\]\\nwhere \\\\( H := \\\\kappa m \\\\sqrt{N} y \\\\mu^{\\\\top} \\\\) and \\\\( \\\\kappa m := \\\\alpha - \\\\beta + 2 \\\\rho \\\\alpha + \\\\beta + 2 \\\\rho \\\\theta \\\\) \\\\( \\\\sqrt{q m} \\\\), for all large \\\\( m \\\\) and \\\\( n \\\\) and some constants \\\\( c, C > 0 \\\\).\\n\\nProof of Lemma C.5. Notice that\\n\\\\[ H = \\\\theta e^{d} \\\\sqrt{N q m} (E A^{\\\\rho}) y \\\\mu^{\\\\top}. \\\\]\\nWe apply Lemmas C.3 and C.4 to derive that\\n\\\\[ \\\\| h(X) - H \\\\| \\\\leq h(X) - 1 e^{d} (E A^{\\\\rho}) X \\\\sqrt{N q m} + 1 e^{d} (E A^{\\\\rho}) X \\\\sqrt{N q m} - H \\\\leq 1 / \\\\sqrt{q m} (\\\\theta + 2 + p N / d) \\\\cdot D^{\\\\rho} A^{\\\\rho} \\\\leq C / \\\\sqrt{q m}, \\\\]\\nwith probability at least\\n\\\\[ 1 - cN^{-10}. \\\\]\"}"}
{"id": "8m4V6Fx6ma", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma C.6.\\n\\nConsider \\\\((A, X) \\\\sim \\\\text{CSBM}(y, \\\\mu, \\\\alpha, \\\\beta, \\\\theta)\\\\). Under the Assumption 3.1 with \\\\(d \\\\lesssim N\\\\), the ridge regression solution \\\\(b_\\\\beta\\\\) defined in (3.7) satisfies\\n\\n\\\\[\\n1 \\\\sqrt{N} \\\\|b_\\\\beta - e_\\\\beta\\\\| \\\\leq C \\\\sqrt{q_m},\\n\\\\]\\n\\nwith probability at least \\\\(1 - cN^{-10}\\\\), where \\\\(e_\\\\beta := \\\\sqrt{N \\\\kappa_m \\\\tau \\\\kappa_m^2 \\\\tau + \\\\lambda \\\\mu}\\\\) and \\\\(\\\\kappa_m = \\\\alpha - \\\\beta + 2 \\\\rho \\\\alpha + \\\\beta + 2 \\\\rho \\\\cdot \\\\theta \\\\sqrt{q_m}\\\\), for all large \\\\(m\\\\) and \\\\(n\\\\) and some constants \\\\(c, C > 0\\\\). Moreover, \\\\(\\\\|b_\\\\beta\\\\| \\\\lesssim \\\\sqrt{N}\\\\) with probability at least \\\\(1 - cN^{-10}\\\\).\\n\\nProof.\\n\\nNotice that \\\\(e_\\\\beta = (H^\\\\top P L H + \\\\lambda I_d)^{-1} H^\\\\top P L y\\\\), where \\\\(H\\\\) is defined by Lemma C.5. From Lemma C.3, we know that \\\\(\\\\|h(X)\\\\| \\\\lesssim 1\\\\) and \\\\(\\\\|H\\\\| \\\\lesssim 1\\\\) with probability at least \\\\(1 - 2N^{-10}\\\\). Moreover, \\\\(\\\\|(H^\\\\top P L H + \\\\lambda I_d)^{-1}\\\\| \\\\leq \\\\lambda^{-1}\\\\) and \\\\(\\\\|(h(X)^\\\\top P L h(X) + \\\\lambda I_d)^{-1}\\\\| \\\\leq \\\\lambda^{-1}\\\\). Therefore, applying Lemma C.5, we derive that\\n\\n\\\\[\\n1 \\\\sqrt{N} \\\\|b_\\\\beta - e_\\\\beta\\\\| \\\\leq \\\\|(H^\\\\top P L H + \\\\lambda I_d)^{-1} H^\\\\top - (h(X)^\\\\top P L h(X) + \\\\lambda I_d)^{-1} h(X)^\\\\top\\\\| \\\\cdot \\\\|P L y\\\\| / \\\\sqrt{N} \\\\lesssim \\\\|(H - h(X))\\\\| \\\\cdot (\\\\|H\\\\| + \\\\|h(X)\\\\|) \\\\cdot \\\\|(h(X)^\\\\top P L h(X) + \\\\lambda I_d)^{-1}\\\\| \\\\cdot \\\\|h(X)\\\\| \\\\lesssim 1 / \\\\sqrt{q_m},\\n\\\\]\\n\\nwith at least \\\\(1 - cN^{-10}\\\\), for some constant \\\\(c > 0\\\\).\\n\\nC.3. Exact Recovery Threshold for Ridge Regression on Linear GCN\\n\\nLemma C.7.\\n\\nLet \\\\(h(X)^\\\\top = [h_1, \\\\ldots, h_N]\\\\) and \\\\(\\\\bar{h}(X)^\\\\top = [\\\\bar{h}_1, \\\\ldots, \\\\bar{h}_N]\\\\). For any \\\\(i \\\\in \\\\mathbb{N}\\\\) and deterministic unit vector \\\\(u \\\\in \\\\mathbb{R}^d\\\\), there exists some \\\\(c, C > 0\\\\) such that\\n\\n\\\\[\\nP( |(\\\\bar{h}_i - h_i)^\\\\top u| \\\\leq C / p N^{q_m} ) \\\\geq 1 - cN^{-10}, \\\\quad (C.6)\\n\\\\]\\n\\n\\\\[\\nP( |\\\\bar{h}_i^\\\\top u| \\\\leq C / \\\\sqrt{N} ) \\\\geq 1 - cN^{-10}, \\\\quad (C.7)\\n\\\\]\\n\\n\\\\[\\nP( \\\\|h_i\\\\| \\\\leq C p d / (N q_m) ) \\\\geq 1 - cN^{-10}, \\\\quad (C.8)\\n\\\\]\\n\\nfor all large \\\\(n\\\\) and \\\\(m\\\\).\\n\\nProof.\\n\\nFor any unit vector \\\\(u \\\\in \\\\mathbb{R}^d\\\\) and \\\\(i \\\\in \\\\mathbb{N}\\\\), conditioning on event (C.4), we have\\n\\n\\\\[\\n|((\\\\bar{h}_i - h_i)^\\\\top u)| \\\\leq 1 \\\\sqrt{N q_m} (e_d) - 1 I_{N - D} - 1 \\\\rho \\\\cdot |(A \\\\rho X)_i : u| \\\\lesssim 1 q_2 m \\\\sqrt{N} (A \\\\rho X)_i : u,\\n\\\\]\\n\\nwhere we employ Lemma C.3. Then, for any \\\\(i \\\\in \\\\mathbb{N}\\\\), we can further have\\n\\n\\\\[\\n|((A \\\\rho X)_i : u)| = \\\\sum_{j \\\\in N_i} (\\\\theta y_j \\\\mu^\\\\top u + z_j^\\\\top u) + \\\\theta \\\\rho \\\\mu^\\\\top u + \\\\rho z_i^\\\\top u \\\\leq \\\\theta (|N_i| + \\\\rho) + \\\\sum_{j \\\\in N_i} z_j^\\\\top u + \\\\rho z_i^\\\\top u \\\\quad (C.9)\\n\\\\]\\n\\nBased on Lemma 3.3 in (Alt et al., 2021), we can upper bound the degree of each vertex by\\n\\n\\\\[\\nP( |N_i| \\\\leq C \\\\log N ) \\\\geq 1 - C D N^{-D}, \\\\quad (C.10)\\n\\\\]\\n\\nfor any \\\\(i \\\\in \\\\mathbb{N}\\\\), some constants \\\\(C, C_D > 0\\\\) and sufficiently large constant \\\\(D > 0\\\\). Meanwhile, since each \\\\(z_j^\\\\top u \\\\sim N(0, 1)\\\\), by applying Hoeffding's inequality (Theorem 2.6.2 in (Vershynin, 2018)), we can deduce that\\n\\n\\\\[\\nP( \\\\sum_{j \\\\in N_i} z_j^\\\\top u + \\\\rho z_i^\\\\top u \\\\leq t |N_i| = k ) \\\\geq 1 - 2 \\\\exp(-ct^2 k + \\\\rho^2), \\\\quad (C.11)\\n\\\\]\\n\\nBased on Lemma 3.3 in (Alt et al., 2021), we can upper bound the degree of each vertex by\\n\\n\\\\[\\nP( |N_i| \\\\leq C \\\\log N ) \\\\geq 1 - C D N^{-D}, \\\\quad (C.10)\\n\\\\]\\n\\nfor any \\\\(i \\\\in \\\\mathbb{N}\\\\), some constants \\\\(C, C_D > 0\\\\) and sufficiently large constant \\\\(D > 0\\\\). Meanwhile, since each \\\\(z_j^\\\\top u \\\\sim N(0, 1)\\\\), by applying Hoeffding's inequality (Theorem 2.6.2 in (Vershynin, 2018)), we can deduce that\\n\\n\\\\[\\nP( \\\\sum_{j \\\\in N_i} z_j^\\\\top u + \\\\rho z_i^\\\\top u \\\\leq t |N_i| = k ) \\\\geq 1 - 2 \\\\exp(-ct^2 k + \\\\rho^2), \\\\quad (C.11)\\n\\\\]\"}"}
{"id": "8m4V6Fx6ma", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exact Recovery in Semi-Supervised Learning\\n\\nfor any $k \\\\in \\\\mathbb{N}$, $t > 0$, and some constant $c > 0$. Then combining (C.10) and (C.11), for any large $D > 0$, there exists some constants $C, C_D > 0$ such that\\n\\n$$\\n\\\\mathbb{P}(X_j \\\\in N_i \\\\mid z^\\\\top u + \\\\rho z^\\\\top u \\\\leq C \\\\log N, |N_i| \\\\leq C \\\\log N) \\\\geq 1 - 2^{-D}.\\n$$\\n\\nThus, with (C.9) and $\\\\rho \\\\approx \\\\log N$, we can conclude that $|A_\\\\rho X_i : u| \\\\lesssim \\\\sqrt{q/q_m} m$ with probability at least $1 - 2^{-D} N^{-10}$.\\n\\nFollowing with (C.9), we can conclude that\\n\\n$$\\n\\\\mathbb{P}(\\\\bar{h}_i : u - h_i : u) \\\\leq C/q m N \\\\geq 1 - c N^{-10},\\n$$\\n\\nFor the second part, we can analogously get $|\\\\bar{h}_i : u| \\\\lesssim 1/q_m^{1/2} m \\\\sqrt{N} |(A_\\\\rho X_i) : u|$.\\n\\nThen, we can apply (C.9) and (C.12) to conclude (C.7).\\n\\nFinally, notice that\\n\\n$$\\n\\\\|A_\\\\rho X_i : u\\\\| = X_j \\\\in N_i (\\\\theta y_j \\\\mu + z_j) + \\\\rho z_i \\\\leq \\\\theta (|N_i| + \\\\rho) + X_j \\\\in N_i z_j + \\\\rho z_i.\\n$$\\n\\nApplying Theorem 3.1.1 in (Vershynin, 2018), we know that\\n\\n$$\\n\\\\mathbb{P}(\\\\|z_i\\\\| \\\\leq 2 \\\\sqrt{d}) \\\\geq 1 - 2 \\\\exp(-cd)\\n$$\\n\\nfor some constant $c > 0$ and any $i \\\\in [N]$. Thus, combining (C.10) and Lemma E.1, we have that with probability at least $1 - c N^{-10}$,\\n\\n$$\\n\\\\|h_i\\\\| \\\\leq 1/q_m^{1/2} m \\\\sqrt{N} \\\\|A_\\\\rho X_i : u\\\\| \\\\lesssim s d N q_m.\\n$$\\n\\nThis completes the proof of this lemma.\\n\\nInspired by (Abbe et al., 2020; 2022), we now apply a general version of leave-one-out analysis for $b_\\\\beta$ by defining the following approximated estimator. For any $i \\\\in V_U$, denote by $b_\\\\beta(i) = (h(i)(X) : P_L h(i)(X) + \\\\lambda I_d) - 1 h(i)(X) : P_L y$,\\n\\n(C.13)\\n\\nwhere $h(i)(X) := \\\\frac{1}{\\\\sqrt{Nq_m}} D^{-1} \\\\rho A_\\\\rho (X - Z(i))$ and $Z(i) := [z_1 \\\\in N_i \\\\cup \\\\{i\\\\}, \\\\ldots, z_k \\\\in N_i \\\\cup \\\\{i\\\\}, \\\\ldots, z_N \\\\in N_i \\\\cup \\\\{i\\\\}]^\\\\top \\\\in \\\\mathbb{R}^{N \\\\times d}$. Here, the difference between $h(i)(X)$ and $h(X)$ is that we turn off the feature noises $z_i$ for vertices $N_i \\\\cup \\\\{i\\\\}$. In this case, conditional on $y$ and $\\\\mu$, both $e_\\\\beta$ and $b_\\\\beta(i)$ are independent with $h_i$ and $\\\\bar{h}_i$ given any $i \\\\in V_U$. Next, we present the following properties for $b_\\\\beta(i)$.\\n\\nLemma C.8. Assume that $q_m \\\\ll d \\\\ll N q_m$. For (3.7) and (C.13), we have\\n\\n$$\\n\\\\frac{1}{\\\\sqrt{N}} b_\\\\beta(i) - b_\\\\beta \\\\leq C s d q_m N,\\n$$\\n\\n$b_\\\\beta(i) \\\\leq C \\\\sqrt{N}$, with a probability at least $1 - c N^{-10}$, for some constants $c, C > 0$.\\n\\nProof. Based on Lemma C.3, we have that\\n\\n$$\\n\\\\|h(i)(X) - h(X)\\\\| = \\\\frac{1}{\\\\sqrt{Nq_m}} \\\\|D^{-1} \\\\rho A_\\\\rho Z(i)\\\\| \\\\leq \\\\frac{1}{\\\\sqrt{q_m N}} \\\\|Z(i)\\\\| \\\\lesssim s d q_m N\\n$$\\n\\nbecause of the fact that $q_m \\\\approx d$ and $N \\\\approx m$. This completes the proof of this lemma.\"}"}
{"id": "8m4V6Fx6ma", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exact Recovery in Semi-Supervised Learning\\n\\nwith probability at least \\\\(1 - \\\\frac{c}{N^{10}}\\\\), where we utilize (C.10) and (C.5) for \\\\(Z_i\\\\) as well. Thus, we know that\\n\\n\\\\[ \\\\|h_i(X)\\\\| \\\\lesssim 1 \\\\]\\n\\nwith probability at least \\\\(1 - \\\\frac{c}{N^{10}}\\\\). Then, analogously with Lemma C.6, we have\\n\\n\\\\[ \\\\frac{1}{\\\\sqrt{N}} \\\\|b_\\\\beta - e_\\\\beta\\\\| \\\\leq \\\\|\\\\bar{h}_i - h_i\\\\|^\\\\top e_\\\\beta + \\\\frac{1}{\\\\sqrt{N}} \\\\|\\\\bar{h}_i - h_i\\\\|^\\\\top (e_\\\\beta - b_\\\\beta(i)) + \\\\frac{1}{\\\\sqrt{N}} \\\\bar{h}_i^\\\\top (e_\\\\beta - b_\\\\beta(i)) + \\\\frac{1}{\\\\sqrt{N}} h_i^\\\\top (b_\\\\beta(i) - b_\\\\beta) \\\\lesssim C \\\\sqrt{N q^m N}, \\\\]\\n\\nwith probability at least \\\\(1 - \\\\frac{c}{N^{10}}\\\\) for some constants \\\\(c, C > 0\\\\). Here, we applied (C.6) when \\\\(u = e_\\\\beta/\\\\sqrt{N}\\\\) and \\\\(u = (e_\\\\beta - b_\\\\beta(i))/\\\\sqrt{N}\\\\), (C.7) when \\\\(u = (e_\\\\beta - b_\\\\beta(i))/\\\\sqrt{N}\\\\), and (C.8). Then, if \\\\(d \\\\lesssim \\\\sqrt{N q^m N}\\\\), we can conclude that\\n\\n\\\\[ \\\\frac{1}{\\\\sqrt{N}} \\\\|b_\\\\beta - e_\\\\beta\\\\| \\\\leq C \\\\sqrt{N q^m}, \\\\]\\n\\nwith very high probability for some universal constant \\\\(C > 0\\\\).\"}"}
{"id": "8m4V6Fx6ma", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Therefore, we can take $\\\\varepsilon_m = \\\\frac{1}{\\\\sqrt{q_m}}$ to get\\n\\\\[\\nP(\\\\psi_m(\\\\text{sign}(b_U, y_U)) = 0) = P_{\\\\min_i \\\\in [m]} y_U, i \\\\cdot b_U, i > 0 = P_{\\\\min_i \\\\in V_U} y_i \\\\cdot h_i^\\\\top b_{\\\\beta} \\\\sqrt{N} > 0 \\\\geq P_{\\\\min_i \\\\in V_U} y_i \\\\sqrt{N} \\\\cdot \\\\bar{h}_i^\\\\top e_{\\\\beta} > C\\\\varepsilon_m \\\\sqrt{N} - X_i \\\\in V_U P y_i \\\\sqrt{N} \\\\bar{h}_i^\\\\top \\\\tilde{e}_{\\\\beta} - y_i \\\\sqrt{N} h_i^\\\\top b_{\\\\beta} > C\\\\varepsilon_m \\\\sqrt{N} \\\\geq 1 - \\\\frac{1}{2} \\\\sup_t \\\\in R \\\\{ \\\\varepsilon_m t + g(a, b, c, \\\\tau, \\\\zeta, s, t) \\\\} + \\\\delta - C m^{-2}.\\n\\\\]\\nfor any $\\\\delta > 0$ and sufficiently large $m$, where in the last line we employ Proposition C.1. Thus, applying Lemma C.2, we know that when $J(a_{\\\\tau}, b_{\\\\tau}, c_{\\\\tau}, \\\\zeta, s) > 1$, $P(\\\\psi_m(\\\\text{sign}(b_U, y_U)) = 0) \\\\to 1$ as $m \\\\to \\\\infty$.\\n\\nWhen $s = 2 c_{\\\\tau} \\\\log(a/b)$, Lemma C.2 implies that $J(a_{\\\\tau}, b_{\\\\tau}, c_{\\\\tau}, \\\\zeta, s) = I(a_{\\\\tau}, b_{\\\\tau}, c_{\\\\tau})$ defined in (3.2). Notice that $J(a_{\\\\tau}, b_{\\\\tau}, c_{\\\\tau}, \\\\zeta, s) \\\\leq I(a_{\\\\tau}, b_{\\\\tau}, c_{\\\\tau})$ for any $s \\\\in \\\\mathbb{R}$. Whereas $s = 0$, Lemma C.2 implies that $J(a_{\\\\tau}, b_{\\\\tau}, c_{\\\\tau}, \\\\zeta, s) = I(a_{\\\\tau}, b_{\\\\tau}, c_{\\\\tau})$.\\n\\nHence, this completes the proof of this theorem.\\n\\nC.4. Asymptotic Errors for Ridge Regression on Linear GCN\\n\\nLemma C.10. Under the Assumption 3.1, there exist some constant $c, C > 0$ such that with probability at least $1 - C N^{-2}$,\\n\\\\[\\n| R(\\\\lambda) - R(\\\\lambda) | \\\\leq C \\\\sqrt{q_m},\\n\\\\]\\n\\\\[\\n| E(\\\\lambda) - E(\\\\lambda) | \\\\leq C \\\\sqrt{q_m},\\n\\\\]\\nwhere $R(\\\\lambda) := 1/m(H_{\\\\tilde{\\\\beta}} - y)\\\\top P_U(H_{\\\\tilde{\\\\beta}} - y)$, $E(\\\\lambda) := 1/n(H_{\\\\tilde{\\\\beta}} - y)\\\\top P_L(H_{\\\\tilde{\\\\beta}} - y)$.\\n\\nProof. From Lemmas C.5 and C.6, we know that $\\\\frac{1}{\\\\sqrt{m}} \\\\|H_{\\\\tilde{\\\\beta}} - h(X)b_{\\\\beta}\\\\| \\\\leq \\\\frac{1}{\\\\sqrt{m}} \\\\|H\\\\| \\\\cdot \\\\|\\\\tilde{\\\\beta} - b_{\\\\beta}\\\\| + \\\\frac{1}{\\\\sqrt{m}} \\\\|H - h(X)\\\\| \\\\cdot \\\\|b_{\\\\beta}\\\\| \\\\ll \\\\frac{1}{\\\\sqrt{q_m}}$, with probability at least $1 - CN^{-2}$ for some constant $C > 0$. Since $\\\\|P_L\\\\|$ and $\\\\|P_U\\\\|$ are both upper bounded by one, we can directly conclude Lemma C.10.\\n\\nProof of Theorem 3.7. Based on Lemma C.10, we can instead compute $E(\\\\lambda)$ and $R(\\\\lambda)$. Recall that $H_{\\\\tilde{\\\\beta}} = \\\\kappa m \\\\sqrt{N} y\\\\mu^\\\\top$ and $\\\\frac{1}{\\\\sqrt{N}} e_{\\\\beta} = \\\\kappa m \\\\tau^2 + \\\\lambda \\\\mu$. Thus, $H_{\\\\tilde{\\\\beta}} = \\\\kappa^2 m \\\\tau^2 + \\\\lambda y$. Then, since $\\\\frac{1}{m} y\\\\top P_U y = \\\\frac{1}{n} y\\\\top P_L y = 1$, we have\\n\\\\[\\nR(\\\\lambda) = \\\\frac{1}{m} (H_{\\\\tilde{\\\\beta}} - y)\\\\top P_U (H_{\\\\tilde{\\\\beta}} - y) = 1 - \\\\kappa^2 m \\\\tau^2 n^2 (\\\\kappa^2 m \\\\tau^2 + \\\\lambda)^2 + o(1),\\n\\\\]\\n\\\\[\\nE(\\\\lambda) = \\\\frac{1}{n} (H_{\\\\tilde{\\\\beta}} - y)\\\\top P_L (H_{\\\\tilde{\\\\beta}} - y) = 1 - \\\\kappa^2 m \\\\tau^2 n^2 (\\\\kappa^2 m \\\\tau^2 + \\\\lambda)^2 + o(1).\\n\\\\]\\n\\nThen taking $m \\\\to \\\\infty$, we can get the results of this lemma.\"}"}
{"id": "8m4V6Fx6ma", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We now consider transductive learning on CSBM following & Welling, 2017), here we simplify the graph convolutional\\n\\\\[ P \\\\]\\nwhere \\\\( \\\\lambda \\\\) for any \\\\( \\\\text{mse} \\\\) defined by \\n\\\\[ \\\\psi \\\\]\\nrate this linear ridge regression for \\n\\\\[ \\\\rho \\\\]\\nlarly, define \\n\\\\[ h \\\\]\\nS \\n\\\\[ \\\\tau \\\\]\\nV \\n\\\\[ D \\\\]\\nDenote \\n\\\\[ \\\\text{for the intensity of self connections in the graph}. \\\\]\\n\\\\[ \\\\text{Let } \\\\text{CSBM} \\\\text{convolutional kernel } h \\\\text{graph convolutional networks}. \\\\]\\n\\\\[ \\\\text{We consider a graph } \\\\]\\n\\\\[ A \\\\]\\n\\\\[ \\\\text{diagonal matrix whose diagonals are the average degree for } \\\\]\\n\\\\[ \\\\text{the degree among all vertices}. \\\\]\\n\\\\[ \\\\text{In this case, we can directly employ } \\\\text{layer by replacing the degree matrix of } A \\\\]\\n\\\\[ \\\\text{on } h \\\\text{ the first diagonal of } D \\\\]\\n\\\\[ \\\\text{indicating no self-loop added, and } D \\\\text{ is split into two disjoint sets } \\\\]\\n\\\\[ \\\\text{that the vertex set } \\\\]\\n\\\\[ \\\\text{is a function } \\\\]\\n\\\\[ \\\\text{Exact Recovery in Semi-Supervised Learning} \\\\]\\n\\\\[ \\\\text{Theorem 3.6} \\\\]\\n\\\\[ \\\\text{by } \\\\text{in MSE loss defined in } \\\\]\\n\\\\[ \\\\text{results}. \\\\]\\n\\\\[ \\\\text{In this case, the exact recovery for } \\\\text{b } \\\\text{almost surely, as } \\\\text{and } \\\\text{y } \\\\text{as long as } \\\\text{J1} \\\\]\\n\\\\[ \\\\text{in } \\\\text{J2} \\\\]\\n\\\\[ \\\\text{is defined in } \\\\text{LRR} \\\\]\\n\\\\[ \\\\text{Consider the ridge regression on the lin-} \\\\]\\n\\\\[ \\\\text{provide detailed statements for the exact recovery thresholds } \\\\]\\n\\\\[ \\\\text{and } \\\\text{q } \\\\text{by Gaussian equivalent } \\\\text{statistical physics methods with some Gaussian equivalent } \\\\text{ex} \\\\]\\n\\\\[ \\\\text{exact recovery region } \\\\text{I} \\\\text{a/optimal region } \\\\text{I}\\\\text{a} \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4 } \\\\text{\u03c4"}
{"id": "8m4V6Fx6ma", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exact Recovery in Semi-Supervised Learning\\n\\n(a) Without self-loop.\\n(b) With optimal self-loop\\n\\nFigure 4. Performance of $LRR$ in (3.8). Fix $N = 800$, $\\\\tau = 0.25$, $c_\\\\tau = 0.5$. Compute the frequency of exact recovery over 20 independent runs. When $I(a_\\\\tau, b_\\\\tau, c_\\\\tau) > 1$, $b_y LRR$ achieves exact recovery, as proved in Theorem 3.6 (a) and (b).\\n\\nFigure 5. The $y$-axis is $E_\\\\psi m$, the average mismatch ratio over 20 independent runs. The $x$-axis is $a$, varying from 0 to 10.5. Fix $b = 4$, $\\\\tau = 0.25$, $c_\\\\tau = 0.5$, $N = 400$. The red curve is $m - I(a_\\\\tau, b_\\\\tau, c_\\\\tau)$, the lower bound predicted by Theorem 3.3 with $q_m = \\\\log(m)$.\\n\\nThe experiments show that $b_y LRR$ achieves a lower mismatch ratio when adding self-loop in the area $I(a_\\\\tau, b_\\\\tau, c_\\\\tau) < 1$, where the exact recovery is impossible.\\n\\n3.4. Performance of GCN with gradient-based training\\n\\nIn this section, we study the feature learning of GCN on $(A, X) \\\\sim CSBM(y, \\\\mu, \\\\alpha, \\\\beta, \\\\theta)$ with $n$ known labels and $m$ unknown labels to be classified. We focus on gradient-based training processes. From Section 3.3, we can indicate that the self-connection (or self-loop) weight $\\\\rho$ plays an important role in exact recovery on test feature vertices. It turns out that we need to find the optimal $\\\\rho$ in (3.11) for self-loop weight to ensure the exact recovery threshold approaches to the IT bound studied in Section 3.1 for graph learning. A similar idea is also mentioned in (Kipf & Welling, 2017), whereas the equal status of self-connections and edges to neighboring nodes may not be a good assumption for a general graph dataset. Hence, we raise a modified training process for GCNs: in general, learning on graphs also requires learning the optimal self-loop weight for the graph, i.e., we should also view parameter $\\\\rho$ in graph $A_\\\\rho = A + \\\\rho I_N$ as a trainable parameter. Although the optimal $\\\\rho$ in Section 3.3 for CSBM on semisupervised learning is due to LDP analysis (see Appendix C.1), we can generally apply a spectral method to achieve oracle $\\\\rho$ in (3.11).\\n\\nFor simplicity, in this section, we denote $A_s = A + sq_m I_N$, $D_s = sq_m I_N + D$, where $D$ is a diagonal matrix with the average degree for each diagonal. In this section, we view $s \\\\in \\\\mathbb{R}$ as a trainable parameter. Let us consider a general two-layer graph convolutional neural network defined by\\n\\n$$f(X) := \\\\frac{1}{\\\\sqrt{K}} \\\\sigma(D^{-1} s A_s X W), \\\\quad (3.12)$$\\n\\nwhere the first-layer weight matrix is $W \\\\in \\\\mathbb{R}^{d \\\\times K}$ and second layer weight matrix is $a \\\\in \\\\mathbb{R}^K$ for some $K \\\\in \\\\mathbb{N}$.\\n\\nHere, $W, s$ are training parameters for this GCN. We aim to train this neural network with training label $y_L$ to predict the labels for vertices in $V \\\\cup U$. Notice that when training $W$, we want $W$ to learn (align with) the correct feature $\\\\mu$ in the dataset. As studied in (Baranwal et al., 2021), for CSBM with a large threshold, the data point feature is linearly separable, hence there is no need to introduce a nonlinear convolution layer in (3.12). So we will consider $\\\\sigma(x) = x$ below. In practice, nonlinearity for node classification may\"}"}
{"id": "8m4V6Fx6ma", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exact Recovery in Semi-Supervised Learning\\n\\nWe train this GCN in two steps. First, we train the \\\\( W \\\\) with a large gradient descent step on training labels. By choosing the suitable learning rate, we can allow \\\\( W \\\\) to learn the feature \\\\( \\\\mu \\\\). Let us define the MSE loss by\\n\\n\\\\[\\nL(W, s) = \\\\frac{1}{2n} (f(X) - y)^T P_L (f(X) - y).\\n\\\\]\\n\\nThe analysis for GD with a large learning rate to achieve feature learning is analogous with (Ba et al., 2022; Damian et al., 2022). We extend this analysis to one-layer GCNs. Precisely, we take a one-step GD with a weight decay \\\\( \\\\lambda \\\\) and learning rate \\\\( \\\\eta \\\\):\\n\\n\\\\[\\nW(1) = W(0) - \\\\eta \\\\nabla W(0) L(W(0), s(0)) + \\\\lambda W(0).\\n\\\\]\\n\\nSecondly, we find out the optimal \\\\( s \\\\) based on (3.11). Here, we only use training labels \\\\( y_L \\\\). Let \\\\( s(1) = \\\\frac{2n^2}{q_m} (y_L^T X L W(1) a)^{\\\\log_2(1 + y_L^T A L y_L)} - y_L^T A L y_L.\\n\\\\]\\n\\nThis construction resembles the spectral methods defined in (3.5). Meanwhile, we can also replace this estimator with the gradient-based method to optimize \\\\( s \\\\) in MSE loss which is shown in Appendix D. However, to attain the IT bound, the nonlinearity of \\\\( \\\\sigma(x) \\\\) in (3.12) plays an important role when applying GD to find optimal self-loop weight \\\\( s \\\\). This observation is consistent with results by Wei et al. (2022); Baranwal et al. (2023b), where nonlinearity needed for GCN to obtain certain Bayes optimal in sparse graph learning.\\n\\nAssumption 3.8. Consider \\\\( N, d, K \\\\to \\\\infty, n \\\\approx N, K \\\\approx N, q_m = \\\\log(m) \\\\) and \\\\( d = o(q_m^2) \\\\). We assume that at initialization \\\\( s(0) = 0 \\\\), and \\\\( \\\\sqrt{K} \\\\cdot [W(0)]_{ij} \\\\sim N(0, 1), \\\\sqrt{K} \\\\cdot [a]_{ij} \\\\sim \\\\text{Unif}\\\\{-1\\\\,\\\\text{to}\\\\,1\\\\}, \\\\) for all \\\\( i \\\\in \\\\{1, \\\\ldots, d\\\\}, j \\\\in \\\\{1, \\\\ldots, K\\\\} \\\\).\\n\\nWith initialization stated in Assumption 3.8 and trained parameters \\\\( W(1) \\\\) and \\\\( s(1) \\\\), we derive a GCN estimator for unknown labels which matches with Theorems 3.3 and 3.2.\\n\\nTheorem 3.9. Under Assumptions 3.1 and 3.8, suppose that learning rate \\\\( \\\\eta_1 = \\\\Theta(K/\\\\sqrt{q_m}) \\\\) and weight decay rate \\\\( \\\\lambda_1 = \\\\eta_1^{-1} \\\\). Then, estimator \\\\( \\\\hat{b}_y \\\\) with \\\\( W = W(1) \\\\) and \\\\( s = s(1) \\\\) satisfies that\\n\\n\\\\[\\nP(\\\\psi_m(y_U, \\\\text{sign}(\\\\hat{b}_y))) = 0 \\\\to 1 \\\\text{ when } I(a_\\\\tau, b_\\\\tau, c_\\\\tau) > 1, \\\\text{ as } m \\\\to \\\\infty.\\n\\\\]\\n\\nHence, GCN can attain the IT bound for the exact recovery of CSBM.\\n\\nRemark 3.10. (Duranthon & Zdeborov\u00b4a, 2023) proposed the AMP-BP algorithm to solve the community detection problem under CSBM, where the expected degree of each vertex is constant, i.e., \\\\( q_m = O(1) \\\\). By contrast, this manuscript focuses on the regime \\\\( q_m \\\\gg 1 \\\\) as in Assumption 3.1. Theorem 3.9 shows that the GCN achieves exact recovery when \\\\( I(a_\\\\tau, b_\\\\tau, c_\\\\tau) > 1 \\\\). However, the performance of the GCN is not characterized when \\\\( I(a_\\\\tau, b_\\\\tau, c_\\\\tau) \\\\leq 1 \\\\), and it is still unclear whether it would match the lower bound proved in Theorem 3.3, i.e., the optimality of GCN remains open. From simulations in Figure 6, we observe that below the IT bound \\\\( I(a_\\\\tau, b_\\\\tau, c_\\\\tau) < 1 \\\\), there is a gap between theoretical optimal error (red curve) and the simulated mismatch ratio by GCN estimators.\\n\\n4. Numerical simulations\\n\\n4.1. Optimal spectral method\\n\\nThe efficacy of the spectral estimator \\\\( \\\\hat{b}_y \\\\) is demonstrated in Figures 2 (a) and 3 for \\\\( c_\\\\tau = 0 \\\\). We fix \\\\( N = 800, \\\\tau = 0.25 \\\\), but vary \\\\( a \\\\) (y-axis) and \\\\( b \\\\) (x-axis) from 1 to 10 in Figure 2, and compute the frequency of exact recovery over 20 independent trials for each parameter configuration \\\\( (a_\\\\tau, b_\\\\tau, c_\\\\tau) \\\\). Here, a lighter color represents a higher success chance. The (red) and (blue) curves represent the boundaries for exact recovery under semi-supervised and unsupervised regimes respectively. A larger \\\\( c_\\\\tau \\\\) implies a stronger signal in node features, which shrinks the boundary for exact recovery and makes the problem easier. In Figure 3, we fix \\\\( b = 5 \\\\) but vary \\\\( a \\\\) (x-axis) from 1 to 10. The simulations for the average mismatch ratio are presented on the logarithmic scale over different choices of \\\\( N \\\\). Clearly, \\\\( \\\\log E_{\\\\psi_m} \\\\) will approach the lower bound (red curve) as proved in Theorems 3.3, 3.4 (2).\\n\\n4.2. Ridge regression on linear GCNs\\n\\nThe efficacy of the ridge estimator \\\\( \\\\hat{b}_y \\\\) is presented in Figures 4 and 5. We fix \\\\( N = 800, \\\\tau = 0.25 \\\\) and \\\\( c_\\\\tau = 0.5 \\\\). We fix \\\\( N = 800, \\\\tau = 0.25 \\\\) and \\\\( c_\\\\tau = 0.5 \\\\). We fix \\\\( N = 800, \\\\tau = 0.25 \\\\) and \\\\( c_\\\\tau = 0.5 \\\\). We fix \\\\( N = 800, \\\\tau = 0.25 \\\\) and \\\\( c_\\\\tau = 0.5 \\\\). We fix \\\\( N = 800, \\\\tau = 0.25 \\\\) and \\\\( c_\\\\tau = 0.5 \\\\). We fix \\\\( N = 800, \\\\tau = 0.25 \\\\) and \\\\( c_\\\\tau = 0.5 \\\\). We fix \\\\( N = 800, \\\\tau = 0.25 \\\\) and \\\\( c_\\\\tau = 0.5 \\\\). We fix \\\\( N = 800, \\\\tau = 0.25 \\\\) and \\\\( c_\\\\tau = 0.5 \\\\). We fix \\\\( N = 800, \\\\tau = 0.25 \\\\) and \\\\( c_\\\\tau = 0.5 \\\\). We fix \\\\( N = 800, \\\\tau = 0.25 \\\\) and \\\\( c_\\\\tau = 0.5 \\\\). We fix \\\\( N = 800, \\\\tau = 0.25 \\\\) and \\\\( c_\\\\tau = 0.5 \\\\). We fix \\\\( N = 800, \\\\tau = 0.25 \\\\) and \\\\( c_\\\\tau = 0.5 \\\\).\"}"}
{"id": "8m4V6Fx6ma", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exact Recovery in Semi-Supervised Learning\\n\\n(a) Exact recovery counts without self-loop.\\n\\n(b) Exact recovery counts with self-loop $\\\\rho$.\\n\\nFigure 7. Performance of by GCN when $N = 400$, $\\\\tau = 0.25$, $c\\\\tau = 0.5$. In Figure 4, but vary a (y-axis) and b (x-axis) from 1 to 10.5, where 20 independent trials are performed on each ($a\\\\tau$, $b\\\\tau$, $c\\\\tau$). The difference between the (a) and (b) lies on the choice of the self-loop density $\\\\rho$, where we take $\\\\rho = 0$ in (a) but $\\\\rho = 2c\\\\tau q_m \\\\log(a\\\\tau/b\\\\tau)$ in (b) as (3.11). In Figure 5, we fix $b = 4$, $N = 400$ but vary a (x-axis) from 1 to 9.5. When $I(a\\\\tau, b\\\\tau, c\\\\tau) < 1$, the performance difference between the choices of $\\\\rho$ are presented. From simulations, the average mismatch ratio is closer to the predicted lower bound (red curve) when the optimal self-loop is added.\\n\\n4.3. Gradient-based training on GCN\\n\\nThe efficacy of by GCN is presented in Figures 6 and 7. Similarly, we fix $N = 400$, $\\\\tau = 0.25$ and $c\\\\tau = 0.5$, but vary a (y-axis) and b (x-axis) from 1 to 9 in Figure 7. For each ($a\\\\tau$, $b\\\\tau$, $c\\\\tau$), 10 independent trials are performed. We plot the performance when adding self-loops to the graph data, where we take $\\\\rho = 0$ in (a) but $\\\\rho = 2c\\\\tau q_m \\\\log(a\\\\tau/b\\\\tau)$ in (b) as (3.11). In Figure 6, we fix $b = 4$, $c\\\\tau = 0.5$, $N = 400$ but vary a (x-axis) from 1 to 9. The performance difference between the choices of $\\\\rho$ when $I(a\\\\tau, b\\\\tau, c\\\\tau) < 1$ are presented. From the simulations, the average mismatch ratio is closer to the predicted bound (red curve) when the optimal self-loop is added.\\n\\n5. Discussion and conclusion\\n\\nFirstly, as shown in $\\\\ell^*$ and $b\\\\kappa\\\\ell^*$, our results for spectral method, ridge regression, and GCNs all cover Erd\u0151s-R\u00e9nyi graph ($a = b$), homophilic graphs ($a > b$) and heterophilic graphs ($a < b$). When $a = b$, we can only utilize the node feature from GMM for classification, which returns to the semisupervised learning on GMM (Lelarge & Miolane, 2019; Oymak & Cihad Gulcu, 2021; Nguyen & Couillet, 2023). For heterophilic graphs with $a < b$, the optimal self-loop strength $\\\\rho$ defined in (3.11) is negative, which validates the observation in Figure 5 of (Shi et al., 2024).\\n\\nOur research pioneers the investigation of the exact recovery threshold in semi-supervised learning on CSBM. We present various strategies for achieving exact recovery, including the spectral method, linear ridge regression applied to linear Graph Convolutional Networks (GCNs), and gradient-based training techniques for GCNs. For each method, we establish precise asymptotic lower bounds that depend on the sparsity of the Stochastic Block Model (SBM) and the signal-to-noise ratio (SNR) in Gaussian Mixture Models (GMM), which, in many instances, are optimal when compared to the information-theoretic (IT) bound. Crucially, our findings support the notion that GCNs, when equipped with certain gradient-based training protocols, can flawlessly recover all unlabeled vertices provided the SNR exceeds the IT bound. This finding underscores the effectiveness of GCNs in addressing classification problems within CSBM settings. For future work, we can delve into the exact recovery rates for more intricate and nonlinear graph models, such as XOR-SBM and geometric Gaussian graphs. Additionally, we intend to shed light on the process of feature learning in GCNs and identify the optimal GCN architectures that prevent over-smoothing and meet IT bounds.\\n\\nAcknowledgements\\n\\nH.W. and Z.W. acknowledge the support from NSF DMS-2154099. Z.W. is also partially supported by NSF DMS-2055340.\"}"}
{"id": "8m4V6Fx6ma", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exact Recovery in Semi-Supervised Learning\\n\\nCSBM.\\n\\nGeneralization theory of GCNs\\n\\nMany works have studied the generalization of GCNs, e.g. Bruna & Li (2017) explored the community detection for SBM with GCNs; (Tang & Liu, 2023) (Shi et al., 2024; Duranthon & Zdeborov\u00e1, 2023) conjectures and algorithms for the constant degree regime. The roles of self-loops and nonlinearity in CCNs have been analyzed in (Lampert & Scholtes, 2023; Kipf & Welling, 2017). Our results in GCNs also provide a way to choose the optimal self-loop weight in GCN to achieve optimal performance.\\n\\nSemi-supervised linear regression\\n\\nSemi-supervised linear regression has been studied in a line of research work, e.g. (Azriel et al., 2022; Ryan & Culp, 2015; Chakrabortty & Cai, 2018; Tony Cai & Guo, 2020; Belkin et al., 2004). (Lelarge & Miolane, 2019) proves the asymptotic Bayes risks for GMM in semisupervised learning, whereas we extend this to CSBM under perfect recovery setting. (Nguyen & Couillet, 2023) explored asymptotic Bayes risk of semi-supervised multitask learning on Gaussian mixture.\\n\\nMain contributions\\n\\nOur contribution lies in the following three perspectives.\\n\\n\u2022 Mathematically, we derive the necessary and sufficient conditions for classifying all nodes correctly under the CSBM. For scenarios where perfect classification is impossible, we characterize the asymptotic misclassification ratio and design efficient estimators to achieve the lowest possible error rate.\\n\\n\u2022 We discover the optimal spectral method for the exact recovery of all unknown labels in this semi-supervised learning which achieves the information theoretical (IT) threshold for SNR.\\n\\n\u2022 At the same time, we evaluate the efficacy of graph ridge regression and GCN on the CSBM dataset to achieve exact recovery. We find the optimal weight for self-loop in the graph to attain IT bound. This provides new insight into modifying the architectures of GCNs for optimal performance.\\n\\n2. Preliminaries\\n\\n2.1. Node classification\\n\\nLet \\\\( V \\\\) and \\\\( E \\\\) denote the set of vertices and edges of graph \\\\( G \\\\) respectively, with \\\\( |V| = N \\\\in \\\\mathbb{N} \\\\). Assume that \\\\( V \\\\) is composed of two disjoint sets \\\\( V^+ \\\\), \\\\( V^- \\\\), i.e., \\\\( V = V^+ \\\\cup V^- \\\\) and \\\\( V^+ \\\\cap V^- = \\\\emptyset \\\\). Let \\\\( y := [y_1, \\\\ldots, y_N]^{\\\\top} \\\\in \\\\{\\\\pm 1\\\\}^N \\\\) denote the label vector encoding the community memberships, i.e., \\\\( V^+ = \\\\{i \\\\in [N] : y_i > 0\\\\} \\\\) and \\\\( V^- = \\\\{i \\\\in [N] : y_i < 0\\\\} \\\\).\\n\\nAssume the access to \\\\( G \\\\) in practice. The goal is to recover the underlying \\\\( y \\\\) using the observations. Let \\\\( \\\\hat{y} \\\\) denote some estimator of \\\\( y \\\\). To measure the performance of the above estimator, define the mismatch ratio between \\\\( y \\\\) and \\\\( \\\\hat{y} \\\\) by\\n\\n\\\\[\\n\\\\psi_N(y, \\\\hat{y}) := \\\\frac{1}{N} \\\\min_{s \\\\in \\\\{\\\\pm 1\\\\}} |\\\\{i \\\\in [N] : y_i \\\\neq s \\\\cdot \\\\hat{y}_i\\\\}|.\\n\\\\]\\n\\nFor the symmetric case, \\\\( |V^+| = |V^-| = N/2 \\\\), the random guess estimation, i.e., determining the node label by flipping a fair coin, would achieve 50% accuracy on average. An estimator is meaningful only if it outperforms the random guess, i.e., \\\\( \\\\psi_N(y, \\\\hat{y}) \\\\leq 0.5 \\\\). If so, \\\\( \\\\hat{y} \\\\) is said to accomplish weak recovery. See (Abbe, 2018) for a detailed introduction.\\n\\nIn this paper, we aim to address another interesting scenario when all the nodes can be perfectly classified, i.e., \\\\( \\\\psi_N(y, \\\\hat{y}) = 0 \\\\), which leads to the concept of exact recovery.\\n\\nDefinition 2.1 (Exact recovery). The \\\\( \\\\hat{y} \\\\) is said to achieve the exact recovery (strong consistency) if\\n\\n\\\\[\\n\\\\lim_{N \\\\to \\\\infty} P(\\\\psi_N(y, \\\\hat{y}) = 0) = \\\\lim_{N \\\\to \\\\infty} P(\\\\hat{y} = \\\\pm y) = 1.\\n\\\\]\\n\\n2.2. Contextual Stochastic Block Model\\n\\nIt is natural to embrace certain data generation models to study the mathematical limits of algorithm performance. The following model is in particular of our interests.\\n\\nDefinition 2.2 (Binary Stochastic Block Model, SBM).\\n\\nAssume \\\\( \\\\mathbf{1}^{\\\\top} y = 0 \\\\), i.e., \\\\( |V^+| = |V^-| = N/2 \\\\). Given \\\\( 0 < \\\\alpha, \\\\beta < 1 \\\\), for any pair of node \\\\( i \\\\) and \\\\( j \\\\), the edge \\\\( \\\\{i, j\\\\} \\\\in E \\\\) is sampled independently with probability \\\\( \\\\alpha \\\\) if \\\\( y_i = y_j \\\\), i.e., \\\\( P(A_{ij} = 1) = \\\\alpha \\\\), otherwise \\\\( P(A_{ij} = 1) = \\\\beta \\\\).\\n\\nFurthermore, if \\\\( A \\\\in \\\\{0, 1\\\\}^{N \\\\times N} \\\\) is symmetric and \\\\( A_{ii} = 0 \\\\) for each \\\\( i \\\\in [N] \\\\), we then write \\\\( A \\\\sim \\\\text{SBM}(y, \\\\alpha, \\\\beta) \\\\).\\n\\nFor each node \\\\( i \\\\in V \\\\), there is a feature vector \\\\( x_i \\\\) attached to it. We are interested in the scenario where \\\\( x_i \\\\) is sampled from the following Gaussian Mixture Model.\\n\\nDefinition 2.3 (Gaussian Mixture Model, GMM).\\n\\nGiven \\\\( N, d \\\\in \\\\mathbb{N}^+ \\\\), label vector \\\\( y \\\\in \\\\{\\\\pm 1\\\\}^N \\\\) and some fixed \\\\( \\\\mu \\\\in \\\\mathbb{S}^{d-1} \\\\) with \\\\( \\\\|\\\\mu\\\\|_2 = 1 \\\\), we write \\\\( \\\\{x_i\\\\}_{i=1}^N \\\\sim \\\\text{GMM}(\\\\mu, y, \\\\theta) \\\\) if \\\\( x_i = \\\\theta y_i \\\\mu + z_i \\\\in \\\\mathbb{R}^d \\\\) for each \\\\( i \\\\in [N] \\\\), where \\\\( \\\\theta > 0 \\\\) denote the signal strength, and \\\\( \\\\{z_i\\\\}_{i=1}^N \\\\subset \\\\mathbb{R}^d \\\\) are i.i.d. random column vectors sampled from \\\\( \\\\mathcal{N}(0, I_d) \\\\). Then by denoting \\\\( Z := [z_1, \\\\ldots, z_N]^{\\\\top} \\\\), we re-write \\\\( X \\\\in \\\\mathbb{R}^{N \\\\times d} \\\\) as\\n\\n\\\\[\\nX = \\\\theta y \\\\mu^{\\\\top} + Z.\\n\\\\] (2.1)\"}"}
{"id": "8m4V6Fx6ma", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Assume that \\\\( n \\\\) where a tight analysis for the inference of latent community (Lu & Sen, 2023) were established under the unsupervised vectors. We further denote \\\\( V \\\\) the semi-supervised regime can be re-written as\\n\\n\\\\[\\n\\\\text{predictor } h_{G \\\\, \\\\text{graph}} \\\\{\u00b1\\\\}\\n\\\\]\\n\\nLet \\\\( X \\\\) It reduces to the unsupervised regime if \\\\( \\\\text{Remark 2.6}. \\\\)\\n\\nCSBM was first introduced in (Deshpande et al., 2018), \\\\( y \\\\) are uniformly sampled from \\\\( \\\\mathcal{U} \\\\) denote some estimator of \\\\( \\\\psi \\\\).\\n\\n\\\\( \\\\mu \\\\) independently, we sample \\\\( A \\\\) from uniform distribution \\\\( \\\\mathcal{U} \\\\).\\n\\n\\\\( y \\\\) over the \\\\( S \\\\) information-theoretic recovery (Abbe et al., 2022) and weak recovery (Yang et al., 2016; Kipf & Welling, 2017) performed on the popular datasets (Yang et al., 2016; 2017) in model training procedure, i.e., a fraction of node labels are revealed, which is the regime we will focus on.\\n\\nTo state our main results, we start with several basic assumptions.\\n\\n**Assumption 3.1** Let \\\\( X \\\\) and \\\\( Y \\\\) are generated in \\\\( \\\\text{Asymptotics} \\\\).\\n\\n\\\\( \\\\tau \\\\) is some fixed constant. Furthermore, we fix \\\\( n/N \\\\rightarrow \\\\infty \\\\).\\n\\n\\\\( m \\\\rightarrow \\\\infty \\\\) and \\\\( q \\\\rightarrow \\\\infty \\\\).\\n\\n\\\\( \\\\text{Exact Recovery in Semi-Supervised Learning} \\\\)\\n\\nFigure 1. An example of SBM under semi-supervised learning.\\n\\nIn contrast, the entire graph is not used either. Notably, this kind of information wastage will reduce the estimator's accuracy.\\n\\n\\\\( L \\\\) and \\\\( V \\\\) adapt the following block form\\n\\n\\\\[\\n\\\\begin{pmatrix}\\nL & A \\\\\\\\\\nA^\\\\top & V\\n\\\\end{pmatrix}\\n\\\\]\\n\\n\\\\( A \\\\) are used for training. The test graph \\\\( A \\\\) entirely unseen by the algorithm during the training process, since \\\\( A \\\\) are used for training. The test graph \\\\( A \\\\) without loss of generality. Let\\n\\n\\\\[\\n\\\\mathcal{L} \\\\rightarrow Y\\n\\\\]\\n\\n\\\\( \\\\mathcal{Y} \\\\) and \\\\( \\\\mathcal{Y} \\\\) given\\n\\n\\\\[\\n\\\\begin{pmatrix}\\n\\\\mathcal{L} & \\\\mathcal{A} \\\\\\\\\\n\\\\mathcal{A}^\\\\top & \\\\mathcal{V}\\n\\\\end{pmatrix}\\n\\\\]\\n\\nIn inductive learning, algorithms are unaware of the nodes and the feature matrices corresponding to the unrevealed nodes. Each vertex \\\\( \\\\{\u00b1\\\\} \\\\) is noted by \\\\( L \\\\).\\n\\n\\\\[\\n\\\\begin{pmatrix}y & 0 \\\\\\\\ 0 & 0 \\\\end{pmatrix}\\n\\\\]\\n\\n\\\\( L \\\\) is our primary interest. Let \\\\( A \\\\) be some function of \\\\( m \\\\) mismatch ratio under \\\\( \\\\text{Asymptotics} \\\\). For instance in Figure 1, \\\\( \\\\text{Assumption 3.1} \\\\) where \\\\( \\\\tau \\\\) \\\\( \\\\in \\\\{\u00b1\\\\} \\\\) independently, \\\\( S \\\\) is uniformly sampled from the set \\\\( \\\\mathcal{U} \\\\).\"}"}
{"id": "8m4V6Fx6ma", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We explain the proof sketch above. For the node classification problem, which will be applied to our large deviation analysis.\\n\\nUnder Assumption 3.1 with \\n\\n\\\\[ \\\\text{Theorem 3.2} \\\\]\\n\\nThis rate function \\\\( I \\\\) is defined as the following hollowed Gram matrix block form as in (2.2). Let \\\\( \\\\lambda \\\\) be the \\\\( i \\\\)-th largest eigenvalue of the adjacency matrix \\\\( A \\\\), or \\\\( A \\\\) for short, with details deferred to Lemma A.1. Let \\\\( G \\\\) be the graph.\\n\\nDefine the index \\\\( \\\\tau \\\\) by \\n\\n\\\\[ \\\\tau := \\\\log(m) \\\\]\\n\\nfor any algorithm.\\n\\nWhen the graph becomes even sparser, where the degree of each vertex goes to infinity slower than \\\\( m \\\\), denote \\\\( \\\\alpha \\\\), \\\\( \\\\beta \\\\), \\\\( \\\\theta \\\\) by\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta = \\\\frac{1}{2} \\\\log(m) \\\\]\\n\\nfor any algorithm.\\n\\nThe index \\\\( \\\\tau \\\\) of the ideal estimator, inspired by (Feige & Ofek, 2005), is given by\\n\\n\\\\[ \\\\tau := \\\\log(m) \\\\]\\n\\nwhen the estimated partitions are collected by a sequence of estimators.\\n\\nWhen the ideal estimator returns the true label vector, every algorithm will misclassify at least \\\\( q \\\\) vertices with probability tending to \\\\( 1 \\\\) as \\\\( m \\\\) goes to infinity.\\n\\nExact Recovery in Semi-Supervised Learning (LDP) for \\\\( L \\\\) and heterophilic graphs.\\n\\n\\\\[ \\\\liminf_{m \\\\to \\\\infty} \\\\frac{\\\\log(m)}{m} = -\\\\infty \\\\]\\n\\nIf \\\\( MLE \\\\) fails exact recovery, then no other algorithm could achieve exact recovery. When \\\\( \\\\text{MLE} \\\\) will not return the true label vector\\n\\n\\\\[ \\\\text{Exact Recovery in Semi-Supervised Learning} \\\\]\\n\\nWe then define \\\\( H \\\\) as the Maximum Likelihood Estimator (MLE). If \\\\( \\\\text{MLE} \\\\) fails exact recovery, then the best estimator is the Maximum Likelihood Estimator (MLE). If \\\\( \\\\text{MLE} \\\\) fails exact recovery, then the best estimator is the Maximum Likelihood Estimator (MLE).\\n\\nWhen \\\\( \\\\alpha, \\\\beta, \\\\theta \\\\) are the corresponding unit eigenvectors.\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n\\\\[ q \\\\]\\n\\n\\\\[ \\\\alpha, \\\\beta, \\\\theta \\\\]\\n\\n\\\\[ \\\\text{MLE} \\\\]\\n\\n\\\\[ \\\\tau \\\\]\\n\\n"}
{"id": "8m4V6Fx6ma", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exact Recovery in Semi-Supervised Learning\\n\\n(a) $c_\\\\tau = 0.5$. (b) $c_\\\\tau = 1.5$.\\n\\nFigure 2. Performance of PCA in (3.4): fix $N = 800$, $\\\\tau = 0.25$ and vary $a$ (y-axis) and $b$ (x-axis) from 1 to 10.5. For each parameter configuration $(a_\\\\tau, b_\\\\tau, c_\\\\tau)$, we compute the frequency of exact recovery over 20 independent runs. Light color represents a high chance of success. Phase transitions occurs at the red curve $I(a_\\\\tau, b_\\\\tau, c_\\\\tau) = 1$, as proved by Theorems 3.2 and 3.4.\\n\\nThe second issue is that, the entrywise eigenvector analysis of $u_2(AU)$ breaks down due to the lack of concentration. To overcome that, we let $b_yG = \\\\text{sign}(b_yGMM)$. Note that $AULy$ is closed to $\\\\sqrt{m_\\\\lambda_\\\\ell^\\\\star}(AU)u_\\\\ell^\\\\star(AU)$, then the new graph estimator is defined through $e_ySBM: e_\\\\kappa_\\\\ell^\\\\star AULyL + AULyG/\\\\sqrt{m}$ (3.5).\\n\\nCombining the above reasoning together, the estimator for under the general case is given by $\\\\text{sign}(e_yPCA)$, where $e_yPCA = e_ySBM + b_yGMM$.\\n\\nThe following result shows that $e_yPCA$ achieves the lowest possible expected error rate when $1 \\\\ll q_m \\\\ll \\\\log(m)$.\\n\\nTheorem 3.5. Let Assumption 3.1 hold, then it follows $\\\\lim\\\\sup_{m \\\\to \\\\infty} q - 1/m \\\\log E \\\\psi_{m, yU, \\\\text{sign}(e_yPCA)} \\\\leq -I(a_\\\\tau, b_\\\\tau, c_\\\\tau)$.\\n\\n3.2.4. COMPARISON WITH UNSUPERVISED REGIME\\nWhen only the sub-graph $G_U = (V_U, E_U)$ is observed, it becomes an unsupervised learning task on $G_U$, where the data is equivalently sampled from $(A_U, \\\\{x_i\\\\}_{m_i=1}) \\\\sim \\\\text{CSBM}(y_U, \\\\mu, \\\\alpha, \\\\beta, \\\\theta)$ with $\\\\alpha = aq_m/m$ and $\\\\beta = bq_m/m$.\\n\\nThe rate function can be obtained by simply taking $\\\\tau = 0$ with $a_0 = a$, $b_0 = b$, and $c_0 = q - 1/m(\\\\theta^2 + d/m) - 1/\\\\theta^4$, aligning with the result in (Abbe et al., 2022). The difference between the two boundaries $I(a_\\\\tau, b_\\\\tau, c_\\\\tau) = 1$ (red) and $I(a_0, b_0, c_0) = 1$ (blue) is presented in Figure 2. A crucial observation is that, the extra information from $X_U, AU$ and $AUL$ shrinks the boundary for exact recovery, making the task easier compared with the unsupervised regime.\\n\\n3.3. Performance of ridge regression on linear GCN\\n\\nFor CSBM $(y_U, \\\\mu, \\\\alpha, \\\\beta, \\\\theta)$, in this section, we focus on analyzing how these parameters $a, b, c_\\\\tau$ and $\\\\tau$ defined in Assumption 3.1 affect the learning performances of the linear GCN.\"}"}
{"id": "8m4V6Fx6ma", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With the lower bounds of the three components obtained in Lemma A.2, it is possible to determine the with probability tending to 1 events. Therefore, the best estimator MLE (MAP) fails exact recovery, hence no other algorithm could succeed. By Lemma F.3 in (Abbe et al., 2022) and symmetry between vertices, for any sequence of estimators, then it suffices to consider the following event.\\n\\nFor each node $i$, let $L_i = \\\\max\\\\{\\\\tau(i), \\\\delta, \\\\epsilon\\\\}$, and $f_y(x) = 1$ for some $y \\\\in V$. Due to the symmetry of the problem, vertices are interchangeable if in the same community. Thus the failure probability can be lowerbounded by.\\n\\nFurthermore, let $\\\\delta = \\\\log U$ of common random edges. To get rid of the dependency, let $u \\\\in V$. However, for any pair $u \\\\in V$ and $y \\\\in V$, the following holds.\\n\\nObviously, for some $\\\\delta > 0$, such that $|U \\\\cap V| = \\\\delta$, which implies $\\\\delta m / |U| = o(\\\\log n)$ in (A.5). We denote\\n\\nwhere $W \\\\in \\\\{\\\\{0\\\\}, \\\\{1\\\\}\\\\}$ does not reply on\\n\\nExact Recovery in Semi-Supervised Learning\"}"}
{"id": "8m4V6Fx6ma", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"By triangle inequality, \\\\( B \\\\cap C \\\\implies A \\\\), thus \\\\( B \\\\cap C \\\\subset A \\\\), and \\n\\n\\\\[\\nE \\\\psi_m(b, y_U) \\\\gtrsim P(A) \\\\geq P(B \\\\cap C) \\\\geq P(C) - P(B^c \\\\cap C).\\n\\\\]\\n\\nAccording to Lemma B.1, \\\\( P(B^c) \\\\ll e^{-q_m} \\\\). Together with the results above, and by Lemma A.1, we have \\n\\n\\\\[\\n\\\\lim \\\\inf m \\\\rightarrow \\\\infty q^{-1} m \\\\log E \\\\eta_m(b, y_U) \\\\geq -\\\\sup t \\\\in R \\\\{\\\\epsilon t + I(a, b, c, \\\\tau, \\\\tau)\\\\},\\n\\\\]\\n\\nand the proof is finished by taking \\\\( \\\\epsilon \\\\rightarrow 0 \\\\).\\n\\nA.3. Deferred proofs\\n\\nFor the sake of convenience, we introduce the following notations for the remaining of this section. For some realization \\\\( A = e^A \\\\), \\\\( X = f_X \\\\), \\\\( y_L = e^{y_L} \\\\in \\\\{\\\\pm 1\\\\}^n \\\\) and \\\\( \\\\mu = e^{\\\\mu} \\\\), we write \\n\\n\\\\[\\nP(e^A, f_X, e^{y_L}) = P(A = e^A, X = f_X, y_L = e^{y_L}),\\nP(e^A, f_X | e^{y_L}, y_U = z) = P(A = e^A, X = f_X | y_L = e^{y_L}, y_U = z).\\n\\\\]\\n\\nLemma A.3. The MAP estimator minimizes the \\\\( P_{\\\\text{fail}} \\\\), and MAP is equivalent to the MLE (A.1). The quantity that MLE is maximizing is defined in (A.2).\\n\\nProof of Lemma A.3. From model 2.5, independently, \\\\( y_L \\\\) and \\\\( y_U \\\\) are uniformly distributed over the spaces \\\\( \\\\{\\\\pm 1\\\\}^m \\\\) and \\\\( \\\\{\\\\pm 1\\\\}^m \\\\) respectively, thus the following factorization holds \\n\\n\\\\[\\nP(y_L, y_U = z) := P(y_L = e^{y_L}, y_U = z) = P(y_L = e^{y_L}) \\\\cdot P(y_U = z),\\n\\\\]\\n\\nwhich is some constant irrelevant to \\\\( z \\\\). The first sentence of the Lemma can be established by Bayes Theorem, since\\n\\n\\\\[\\n\\\\text{arg max}_{z \\\\in \\\\{\\\\pm 1\\\\}^m} 1^\\\\top z = 0 P(y_U = z | A, X, y_L) = \\\\text{arg max}_{z \\\\in \\\\{\\\\pm 1\\\\}^m} 1^\\\\top z \\\\cdot P(A | y_L, y_U = z) \\\\cdot P(X | y_L, y_U = z),\\n\\\\]\\n\\nwhere \\\\( P(y_L, y_U = z) \\\\) and \\\\( P(A, X, y_L) \\\\) in the first line are factored out since they are irrelevant to \\\\( z \\\\), and the last equality holds due to the independence between \\\\( A \\\\) and \\\\( X \\\\) when given \\\\( y \\\\). For the second sentence of the Lemma, the function \\\\( f(z) \\\\) could be easily obtained by taking the logarithm of the objective probability.\\n\\nProof of Lemma A.1 (1). By definition of \\\\( W_{m,i} \\\\), we have \\n\\n\\\\[\\nE[e^{tW_{m,i}} | y_i] = E\\\\left[\\\\exp\\\\left(2t\\\\theta^2N\\\\theta^2 + d_{X_j \\\\in [N]} \\\\{i\\\\} \\\\langle x_i, x_j \\\\rangle y_j y_i\\\\right)\\\\right] \\\\cdot E\\\\left[\\\\exp\\\\left(t \\\\log(\\\\frac{a}{b}) X_j \\\\in V \\\\{i\\\\} A_{ij} y_j y_i\\\\right)\\\\right] \\\\cdot E\\\\left[\\\\exp\\\\left(t \\\\log(\\\\frac{a}{b}) X_j \\\\in V \\\\{i\\\\} A_{ij} y_j y_i\\\\right)\\\\right].\\n\\\\]\\n\\nFollowing the same calculation as Lemma F.2 in (Abbe et al., 2022), we know that\\n\\n\\\\[\\n\\\\log E\\\\left[\\\\exp\\\\left(2t\\\\theta^2N\\\\theta^2 + d_{X_j \\\\in [N]} \\\\{i\\\\} \\\\langle x_i, x_j \\\\rangle y_j y_i\\\\right)\\\\right] = 2c_2 (t + \\\\frac{t^2}{2})(1 + o(1)) q_m \\\\log E\\\\left[\\\\exp\\\\left(t \\\\log(\\\\frac{a}{b}) X_j \\\\in V \\\\{i\\\\} A_{ij} y_j y_i\\\\right)\\\\right] = \\\\frac{1}{2} - \\\\frac{a}{b} + \\\\frac{a}{b} t - \\\\frac{b}{a} + \\\\frac{b}{a} t! (1 + o(1)) q_m.\\n\\\\]\"}"}
{"id": "8m4V6Fx6ma", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meanwhile, since\\n\\\\[ E \\\\left[ e^{-tA_{ij}y_iy_j | y_i} \\\\right] = 1 + \\\\frac{1}{2} \\\\alpha (e^{-t} - 1) + \\\\beta (e^{-t} - 1), \\\\]\\nwe can get\\n\\\\[ E \\\\left[ \\\\exp \\\\left( t \\\\log \\\\left( \\\\frac{a}{b} \\\\right) X_j \\\\right) \\\\right] = n \\\\frac{\\\\log 1 + \\\\frac{1}{2} \\\\alpha (e^{-t} - 1) + \\\\beta (e^{-t} - 1)}{m}. \\\\]\\n\\nThus by using \\\\( \\\\log (1 + x) = x \\\\) when \\\\( x = o(1) \\\\), we obtain\\n\\\\[ \\\\lim_{m \\\\to \\\\infty} q^{-1} m \\\\log E \\\\left[ e^{tW_{m,i} | y_i} \\\\right] = \\\\lim_{m \\\\to \\\\infty} \\\\frac{2 c \\\\tau (t + t^2)}{2 c m} + N^2 m \\\\frac{a}{b} t - a + b \\\\frac{b}{a} t - b!! (1 + o(1)) = -I(t, a^\\\\tau, b^\\\\tau, c^\\\\tau)(1 + o(1)). \\\\]\\n\\nThe proof is then completed by applying Lemma H.5 in (Abbe et al., 2022).\\n\\nProof of Lemma A.1 (2).\\nFirst, we plug in \\\\( \\\\sigma, y_U \\\\) into (A.2), and consider the effect of \\\\( u \\\\) and \\\\( v \\\\),\\n\\n\\\\[ f(y_U) - f(\\\\sigma) = \\\\log P(A | y_L, y_U = \\\\sigma) - \\\\log P(A | y_L, y_U) + \\\\log P(X | y_L, y_U = \\\\sigma) - \\\\log P(X | y_L, y_U). \\\\]\\n\\nBy Lemma A.4, the term above can be further reformulated as\\n\\n\\\\[ f(y_U) - f(\\\\sigma) = \\\\log p_A(A | y_u, y_v = -1, y_L, y_U \\\\{u,v\\\\}) - \\\\log p_A(A | y_u = -1, y_v = 1, y_L, y_U \\\\{u,v\\\\}) + \\\\log p_X(X | y_u = 1, y_v = -1, y_L, y_U \\\\{u,v\\\\}) - \\\\log p_X(X | y_u = -1, y_v = 1, y_L, y_U \\\\{u,v\\\\}). \\\\]\\n\\nNote that \\\\( y^2_u = 1 \\\\), according to Lemmas A.5 and A.6, with probability at least \\\\( 1 - e^{-q m} \\\\), we have\\n\\n\\\\[ \\\\log p_A(A | y_u, y_v = -1, y_L, y_U \\\\{u,v\\\\}) - \\\\log p_A(A | y_u = -1, y_v = 1, y_L, y_U \\\\{u,v\\\\}) + \\\\log p_X(X | y_u = 1, y_v = -1, y_L, y_U \\\\{u,v\\\\}) - \\\\log p_X(X | y_u = -1, y_v = 1, y_L, y_U \\\\{u,v\\\\}) \\\\leq q m. \\\\]\\n\\nConsequently by triangle inequality, there exists some large enough constant \\\\( c > 0 \\\\) such that with probability at least \\\\( 1 - e^{-cq m} \\\\),\\n\\n\\\\[ |f(y_U) - f(\\\\sigma) - W_{m,u} - W_{m,v}| = o(1), \\\\]\\n\\nThe proof is then completed.\\n\\nProof of Lemma A.2.\\nFirst, \\\\( J_m := \\\\max_{u \\\\in V} J_{m,u} \\\\) in (A.5), then it suffices to focus on\\n\\n\\\\[ P(J_{m,u} > \\\\zeta m q m) \\\\]\\n\\nsince an argument based on the union bound leads to\\n\\n\\\\[ P(J_m > \\\\zeta m q m) = P(\\\\exists u \\\\in V s.t. J_{m,u} > \\\\zeta m q m) \\\\leq \\\\sum_{u \\\\in V} P(J_{m,u} > \\\\zeta m q m). \\\\]\"}"}
{"id": "8m4V6Fx6ma", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We now establish the proof of (A.6). Recall that we claim the following upper bound with the proof deferred later where the last inequality holds since the result above is independent of $y$\\n\\nThen by Markov inequality and the fact $q$, all $N$ random variables is at most\\n\\nRecall $|U|$ is a set of independent random variables. Following Lemma H.4 in (Abbe et al., 2022), for the same calculation as in Lemma A.1 to figure out that for any $\\\\{U_j : j \\\\leq m\\\\}$ with $d/N > \\\\zeta m,u$, we focus on the critical case $d/N \\\\approx m$, thus\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent random variables since no edge overlap, then one has\\n\\nsince $U_j$, $\\\\parallel U \\\\parallel$ is $\\\\parallel \\\\sum_{j \\\\leq m} U_j \\\\parallel$, then one has\\n\\nSince $\\\\parallel U \\\\parallel \\\\geq \\\\zeta m,u$ is a set of independent random variables, then one has\\n\\nThus by union bound, we have\\n\\nNote that $\\\\{U_j : j \\\\leq m\\\\}$ is a set of independent"}
{"id": "8m4V6Fx6ma", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where the last line holds since $a, b, c$.\\n\\nwhere the last equality holds since the result on the second line does not depend on $y$.\\n\\n$\\\\alpha \\\\propto \\\\text{which leads to}$\\n\\nProof of Lemma A.4.\\n\\nand $y$. Due to the independence between the graph\\n\\nLemma A.4.\\n\\nDenote by $A$ and feature matrix $\\\\{1 + \\\\log E \\\\}_{i \\\\in A}$.\\n\\nBy (3.1), we focus on the critical case $u = \\\\langle A \\\\rangle$. On the other hand, we have $t = (1 + \\\\log m, \\\\log E) - (1 + \\\\log m, \\\\log E)$.\\n\\n$E = \\\\left(1 + \\\\log m, \\\\log E\\\\right)\\\\left(1 + \\\\log m, \\\\log E\\\\right)$.\\n\\n$p = \\\\frac{1}{2} \\\\left(1 + \\\\log m, \\\\log E\\\\right) + \\\\frac{1}{2} \\\\left(1 + \\\\log m, \\\\log E\\\\right)$.\\n\\nThen $\\\\tilde{d}/\\\\theta \\\\propto m, u \\\\in U \\\\setminus \\\\{\\\\tilde{d}/\\\\theta \\\\} \\\\propto m, u \\\\in U \\\\setminus \\\\{\\\\tilde{d}/\\\\theta \\\\}$.\\n\\nBy $d/\\\\theta \\\\propto m, u \\\\in U \\\\setminus \\\\{\\\\tilde{d}/\\\\theta \\\\}$, then followed by\\n\\n$\\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| = \\\\left| |T_u| \\\\right| - \\\\left| |T_u| \\\\right| = 2 \\\\log(1 + \\\\log m, \\\\log E) - 2 \\\\log(1 + \\\\log m, \\\\log E)$.\\n\\n$\\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| = 2 \\\\log(1 + \\\\log m, \\\\log E) - 2 \\\\log(1 + \\\\log m, \\\\log E)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right)$.\\n\\n$\\\\log \\\\left( \\\\left| \\\\left| T_u \\\\right| \\\\right| - \\\\left| \\\\left| T_u \\\\right| \\\\right| \\\\right"}
{"id": "8m4V6Fx6ma", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For the left hand side, we assume $y_u = 1$, $y_v = -1$ and factor out the terms irrelevant to $u$ and $v$, then\\n\\\\[ P(A | y_u, y_v, y_L, y_U \\\\{u,v\\\\}) \\\\propto \\\\beta A_{uv} (1 - \\\\beta)^{1 - A_{uv}} \\\\cdot \\\\alpha |T_u \\\\cap S_u| \\\\cdot (1 - \\\\alpha)^{|T_u \\\\setminus S_u|} \\\\cdot \\\\beta |S_u \\\\setminus T_u| \\\\cdot (1 - \\\\beta)^{|N \\\\setminus (T_u \\\\cup S_u \\\\cup \\\\{u\\\\})|} \\\\cdot \\\\alpha |T_v \\\\cap S_v| \\\\cdot (1 - \\\\alpha)^{|T_v \\\\setminus S_v|} \\\\cdot \\\\beta |S_v \\\\setminus T_v| \\\\cdot (1 - \\\\beta)^{|N \\\\setminus (T_v \\\\cup S_v \\\\cup \\\\{v\\\\})|}. \\\\]\\n\\nWe perform the same calculation under the assumption $y_u = -1$, $y_v = 1$, which gives\\n\\\\[ P(A | -y_u, -y_v, y_L, y_U \\\\{u,v\\\\}) \\\\propto \\\\beta A_{uv} (1 - \\\\beta)^{1 - A_{uv}} \\\\cdot \\\\alpha |S_u \\\\setminus T_u| \\\\cdot (1 - \\\\alpha)^{|N \\\\setminus (T_u \\\\cup S_u \\\\cup \\\\{u\\\\})|} \\\\cdot \\\\beta |T_u \\\\cap S_u| \\\\cdot (1 - \\\\beta)^{|T_u \\\\setminus S_u|} \\\\cdot \\\\alpha |S_v \\\\setminus T_v| \\\\cdot (1 - \\\\alpha)^{|N \\\\setminus (T_v \\\\cup S_v \\\\cup \\\\{v\\\\})|} \\\\cdot \\\\beta |T_v \\\\cap S_v| \\\\cdot (1 - \\\\beta)^{|N \\\\setminus (T_v \\\\cup S_v \\\\cup \\\\{v\\\\})|}. \\\\]\\n\\nwhere the probability of generating edge $(u, v)$ remains unchanged when flipping the signs of $u$ and $v$ at the same time. The proof follows easily by rearranging and separating relevant terms.\\n\\nFor the second part, note that\\n\\\\[ p_X(X | y) \\\\propto E_\\\\mu \\\\exp -\\\\frac{1}{2} \\\\|x_j - y_j\\\\|^2 \\\\]\\n\\\\[ \\\\propto E_\\\\mu \\\\exp D_{x_j \\\\in V} x_j y_j, \\\\mu \\\\]\\nwhere $\\\\propto$ hides quantities that do not depend on $y$. Consequently,\\n\\\\[ p_X(X | y_u, y_{-u}) = p_X(X | -y_u, -y_{-u}) = E_\\\\mu \\\\exp (2 y_u x_u \\\\mu) = E_\\\\mu \\\\exp (2 y_{-u} x_{-u} \\\\mu). \\\\]\\n\\nFor the left hand side, similarly, let $\\\\propto$ hide the quantities independent of $u$, $v$, then\\n\\\\[ P(X | y_u, y_v, y_L, y_U \\\\{u,v\\\\}) \\\\propto E_\\\\mu \\\\exp (y_u x_u \\\\mu + y_v x_v \\\\mu) = E_\\\\mu \\\\exp (y_u x_u \\\\mu) \\\\cdot E_\\\\mu \\\\exp (y_v x_v \\\\mu). \\\\]\\n\\nThe conclusion follows easily by the linearity of expectation.\\n\\nLemma A.5 (Lemma F.4, (Abbe et al., 2022)). Denote by $p_X(X | e_\\\\ell_i, e_{y_i-1})$ the conditional probability density function of $X$ given $y_i = e_\\\\ell_i \\\\in \\\\{\\\\pm 1\\\\}$ and $y_{-i} = e_{y_{-i}} \\\\in \\\\{\\\\pm 1\\\\}$. Then there exists some large enough constant $c > 0$ such that for each $i \\\\in [N]$, with probability at least $1 - e^{-cq N}$,\\n\\\\[ y_i \\\\log p_X(X | y_i, y_{-i}) - 2 \\\\theta^2 N \\\\theta + d X_j \\\\neq i < \\\\frac{1}{o(N)}. \\\\]\\n\\nLemma A.6 (Lemma F.5, (Abbe et al., 2022)). Denote by $p_A(A | e_\\\\ell_i, e_{y_i-1})$ the conditional probability mass function of $A$ given $y_i = e_\\\\ell_i \\\\in \\\\{\\\\pm 1\\\\}$ and $y_{-i} = e_{y_{-i}} \\\\in \\\\{\\\\pm 1\\\\}$. Then there exists some large enough constant $c > 0$ such that for each $i \\\\in [N]$,\\n\\\\[ y_i \\\\log p_A(A | y_i, y_{-i}) - \\\\log a b A_{ij} y_j / q N = o(1). \\\\]\"}"}
{"id": "8m4V6Fx6ma", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof of Lemma B.1. By definition of conditional probability and the independence between $A | y$ and $X | y$, for vertex $i \\\\in V_U$, one has\\n\\\\[\\n\\\\log \\\\frac{P(y_i = 1 | A, X, y_{-i})}{P(y_i = -1 | A, X, y_{-i})} = \\\\log \\\\frac{P(A, X | y_i = 1, y_{-i})}{P(A, X | y_i = -1, y_{-i})}.\\n\\\\]\\nThen, one could apply Lemmas F.4, F.5 in (Abbe et al., 2022) separately to conclude the results for the two terms above.\\n\\nFrom Lemma B.1 above, the ideal estimator $\\\\left( b_{y_1}^{\\\\text{genie}}, \\\\ldots, b_{y_m}^{\\\\text{genie}} \\\\right)^\\\\top$ can be approximated by\\n\\\\[\\n\\\\text{sign} \\\\left( \\\\log \\\\frac{a}{b} \\\\right) (A_{UL} y_L + A_U \\\\bar{u}) + 2N_d/\\\\theta^2 (G_{UL} y_L + G_U \\\\bar{u}).\\n\\\\]\\n(B.2)\\n\\nNote that $A_{UL}, y_L$ and $G_{UL}$ are accessible for us in semi-supervised setting. Below, Lemma B.2 indicates that a scaled version of (B.2) is entrywisely close to $b_{y_{PCA}}$ in (3.4) up to a global sign flip.\\n\\nLemma B.2. Denote $\\\\bar{u} : = y_U / \\\\sqrt{m}$. For each $i \\\\in U$, define $w : = \\\\log \\\\frac{a}{b} A_{UL} y_L / \\\\sqrt{m} + A_U \\\\bar{u} + 2N_d/\\\\theta^2 (G_{UL} y_L / \\\\sqrt{m} + G_U \\\\bar{u})$.\\n\\nThen for $b_{y_{PCA}}$ in (3.4), there exists some sequence $\\\\{ \\\\varepsilon_m \\\\}$ going to 0 such that\\n\\\\[\\nP \\\\left( \\\\min_{c = \\\\pm 1} \\\\| c b_{y_{PCA}} - w \\\\|_\\\\infty \\\\geq \\\\varepsilon_m - 1/2 \\\\log(m) \\\\right) \\\\preceq m^{1/2}.\\n\\\\]\\n\\nProof of Lemma B.2. Define the following intermediate-term $v : = \\\\log \\\\frac{\\\\alpha}{\\\\beta} \\\\cdot A_{UL} 1_{\\\\sqrt{m} y_L} + m (\\\\alpha - \\\\beta) u^2 (A_U)$.\\n\\nBy definition of $\\\\alpha$ and $\\\\beta$ in Assumption 3.1, $\\\\alpha/\\\\beta = a/b$. We focus on the case $a > b$, then\\n\\\\[\\n\\\\| v - w \\\\|_\\\\infty \\\\leq \\\\log \\\\frac{a}{b} \\\\| A_U \\\\bar{u} - (a - b) q m u^2 (A_U) \\\\|_\\\\infty + 2\\\\theta^2 N\\\\theta^2 + d \\\\| G_U \\\\bar{u} - m\\\\theta^2 u_1 (G_U) \\\\|_\\\\infty.\\n\\\\]\\n\\nWithout loss of generality, we assume $\\\\langle u_1 (G_U), \\\\bar{u} \\\\rangle \\\\geq 0$ and $\\\\langle u_2 (A_U), \\\\bar{u} \\\\rangle \\\\geq 0$. Also, by Lemma B.1, Theorem 2.1 in (Abbe et al., 2022), with probability at least $1 - e^{-n}$,\\n\\\\[\\n\\\\lambda_1 (G) = (1 + o(1)) N\\\\theta^2, \\\\lambda_1 (G_U) = (1 + o(1)) m\\\\theta^2,\\n\\\\]\\nand for some large constant $c > 4$, with probability at least $1 - n - c$, there exists some vanishing sequence $\\\\{ \\\\varepsilon_m \\\\}$ such that\\n\\\\[\\n\\\\| u_1 (G_U) - G_U \\\\bar{u} / (m\\\\theta^2) \\\\|_\\\\infty \\\\preceq \\\\varepsilon_m m^{-1/2}, \\\\| u_1 (G_U) \\\\|_\\\\infty \\\\preceq m^{-1/2}.\\n\\\\]\\n\\nOne can also obtain the upper bounds for $\\\\| A_{UL} \\\\|_2 \\\\to \\\\infty$ and $\\\\| G_{UL} \\\\|_2 \\\\to \\\\infty$. The remaining procedure follows similarly as Lemma F.1 in (Abbe et al., 2022) and Corollary 3.1 in (Abbe et al., 2020).\"}"}
{"id": "8m4V6Fx6ma", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exact Recovery in Semi-Supervised Learning\\n\\nProof of Theorem 3.4 (1).\\n\\nFirst, for each node $i \\\\in V_U$, if there exists some positive constant $\\\\xi$ such that\\n\\n$$q - \\\\frac{1}{m} \\\\sqrt{m y_i} (b_{PCA})_i \\\\geq \\\\xi,$$\\n\\nthen the estimator $\\\\text{sign}(b_{PCA})_i$ recovers the label of each node correctly. Thus a sufficient condition for exact recovery is\\n\\n$$q - \\\\frac{1}{m} \\\\sqrt{m \\\\min_{i \\\\in U} y_i} (b_{PCA})_i \\\\geq \\\\xi,$$\\n\\nfor some positive constant $\\\\xi$.\\n\\nRemind the result of Lemma B.2, for some vanishing positive sequence $\\\\{\\\\varepsilon_m\\\\}_m$, we have\\n\\n$$\\\\min_{c = \\\\pm 1} \\\\|c b_{PCA} - w\\\\|_\\\\infty \\\\geq \\\\varepsilon_m m^{-1/2} q_m$$\\n\\nwith probability at most $m^{-2}$. Denote $\\\\hat{c} := \\\\arg \\\\min_{c = \\\\pm 1} \\\\|c b_{PCA} - w\\\\|_\\\\infty$ and $\\\\hat{v} = \\\\hat{c} \\\\cdot b_{PCA}$. Based on the facts above, the sufficient condition for exact recovery can be further simplified as\\n\\n$$q - \\\\frac{1}{m} \\\\sqrt{m \\\\min_{i \\\\in U} y_i} \\\\hat{v}_i \\\\geq q - \\\\frac{1}{m} \\\\sqrt{m \\\\min_{i \\\\in U} y_i} w_i - \\\\varepsilon_m \\\\geq \\\\xi,$$\\n\\nwhere the last inequality holds since $\\\\varepsilon_m$ vanishes to 0. Then we have\\n\\n$$P(\\\\psi_m = 0) = P(\\\\text{sign}(b_{PCA}) = y) \\\\geq P(q - \\\\frac{1}{m} \\\\sqrt{m \\\\min_{i \\\\in U} y_i} \\\\hat{v}_i \\\\geq q - \\\\frac{1}{m} \\\\sqrt{m \\\\min_{i \\\\in U} y_i} w_i - \\\\varepsilon_m \\\\geq \\\\xi) \\\\geq P(q - \\\\frac{1}{m} \\\\sqrt{m \\\\min_{i \\\\in U} y_i} \\\\hat{v}_i \\\\geq \\\\xi, q - \\\\frac{1}{m} \\\\sqrt{m \\\\|\\\\hat{v} - w\\\\|_\\\\infty} < \\\\varepsilon_m) \\\\geq P(q - \\\\frac{1}{m} \\\\sqrt{m \\\\min_{i \\\\in U} y_i} \\\\hat{v}_i \\\\geq \\\\xi, q - \\\\frac{1}{m} \\\\sqrt{m \\\\|\\\\hat{v} - w\\\\|_\\\\infty} < \\\\varepsilon_m) \\\\geq 1 - \\\\sum_{i \\\\in U} P(q - \\\\frac{1}{m} \\\\sqrt{m y_i} w_i < \\\\xi) - m^{-2} = 1 - m \\\\cdot \\\\sum_{i \\\\in U} P(q - \\\\frac{1}{m} \\\\sqrt{m y_i} w_i < \\\\xi) - m^{-2}.$$\\n\\nNote that $\\\\sqrt{mw_i y_i} = W_n,i$ defined in Lemma A.1. We take $0 < \\\\varepsilon < a^{-b(1 - \\\\tau)} \\\\log(a/b) + 2c\\\\tau$, then for any $\\\\delta > 0$, there exists some large enough positive constant $M$ such that for $m \\\\geq M$, $\\\\varepsilon_m < \\\\xi$, it follows that\\n\\n$$P(\\\\sqrt{mw_i y_i} \\\\leq \\\\xi q_m) \\\\leq \\\\exp\\\\left(-\\\\sup_{t \\\\in \\\\mathbb{R}^n} \\\\xi t + I(t, a\\\\tau, b\\\\tau, c\\\\tau)\\\\right) + \\\\delta \\\\cdot \\\\log(m).$$\\n\\nBy combining the arguments above, the probability of accomplishing exact recovery is lower bounded by\\n\\n$$P(\\\\psi_m = 0) \\\\geq 1 - m \\\\cdot \\\\sup_{t \\\\in \\\\mathbb{R}} \\\\{\\\\xi t + I(t, a\\\\tau, b\\\\tau, c\\\\tau)\\\\} + \\\\delta - m^{-2} \\\\to 1,$$\\n\\nsince $I(a\\\\tau, b\\\\tau, c\\\\tau) = \\\\sup_{t \\\\in \\\\mathbb{R}} \\\\{\\\\xi t + I(t, a\\\\tau, b\\\\tau, c\\\\tau)\\\\} > 1$ by assumption when choosing small enough $\\\\xi$ and $\\\\delta$.\\n\\nProof of Theorem 3.5.\\n\\nThe proof procedure follows similarly to Theorem 4.4 in (Abbe et al., 2022), where we should apply the new large deviation result Lemma A.1 instead. The proof is simplified since $y_L$ is accessible under the semi-supervised learning regime.\\n\\nC. The analysis of the ridge regression on linear graph convolution\\n\\nFor CSBM $(y, \\\\mu, \\\\alpha, \\\\beta, \\\\theta)$, in this section, we focus on analyzing how these parameters $c\\\\tau, a\\\\tau$ and $b\\\\tau$ defined in Assumption 3.1 affect the learning performances of the linear graph convolutional networks. We consider a graph convolutional kernel $h(X) \\\\in \\\\mathbb{R}^{N \\\\times d}$ which is a function of data matrix $X$ and adjacency matrix $A$ sampled from CSBM $(y, \\\\mu, \\\\alpha, \\\\beta, \\\\theta)$.\\n\\nWe add self-loop and define the new adjacency matrix $A_\\\\rho := A + \\\\rho I_N$ where $\\\\rho \\\\in \\\\mathbb{R}$ represents the intensity. Let $D_\\\\rho$ be the diagonal matrix whose diagonals are the average degree for $A_\\\\rho$, i.e., $[D_\\\\rho]_{ii} = 1/N \\\\sum_{j=1}^{N} (A_\\\\rho)_{ij}$ for each $i \\\\in [N]$.\\n\\nDenote $D := D_0$, indicating no self-loop added. Recall that the normalization we applied for the linear graph convolutional layer is $h(X) = \\\\frac{1}{\\\\sqrt{N}} D^{-1} \\\\rho A_\\\\rho X$.\"}"}
