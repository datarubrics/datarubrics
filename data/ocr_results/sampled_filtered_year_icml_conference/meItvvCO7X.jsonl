{"id": "meItvvCO7X", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images\\n\\nBin Pu\\n\\nXingguo Lv\\n\\nJiewen Yang\\n\\nGuannan He\\n\\nXingbo Dong\\n\\nYiqun Lin\\n\\nShengli Li\\n\\nYing Tan\\n\\nFei Liu\\n\\nMing Chen\\n\\nZhe Jin\\n\\nKenli Li\\n\\nXiaomeng Li\\n\\n1. Introduction\\n\\nIn clinical practice, the observation of anatomical structures allows for the direct diagnosis of many diseases (Xue et al., 2021; Lin et al., 2019; Arnaout et al., 2021; Zheng et al., 2023; Dai et al., 2022; Li et al., 2018). For example, the absence of cavum septi pellucidi structure in fetal head view is diagnosed as a severe disease called holoprosencephaly (Monteagudo, 2020). Therefore, anatomical structure detection serves as an essential foundation for disease diagnosis.\\n\\nRecently, deep learning (DL)-based methods as a powerful tool have already achieved significant progress in fetal anatomical structure detection, such as standard view quality control (Pu et al., 2021; Chen et al., 2017; Zhao et al., 2022; Wu et al., 2017), and disease diagnosis (Gong et al., 2019; Xu et al., 2022).\\n\\nNonetheless, applying DL-based models directly to anatomical structure detection in ultrasound data often yields suboptimal results, especially for data from multiple health centers (Guan & Liu, 2021). This is because real-world datasets have domain gaps (Oza et al., 2023; Li et al., 2023a) due to variations in data collection devices and obstetricians\u2019 scanning techniques across different hospital centers. Fine-tuning the DL models on the target data may solve the problem, but obtaining accurate annotations from obstetrician experts is either costly or unavailable. The diversity of machines equipped with various transducers further challenges annotations, presenting a significant hurdle for DL.\"}"}
{"id": "meItvvCO7X", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The results of different backbones and detection heads for the adaptive detection task are shown in Table A1.\\n\\n| Feature extractor | RA | RV | LV | VS | SP | LA | CR | DAO |\\n|-------------------|----|----|----|----|----|----|----|-----|\\n| Faster-RCNN (Ren et al., 2015) | 61.21 | 62.31 | 64.17 | 60.18 | 69.14 | 61.70 | 70.47 | 65.61 |\\n| VGG-16 (Simonyan & Zisserman, 2014) | 65.27 | 77.12 | 64.40 | 73.69 | 68.53 | 60.63 | 72.04 | 69.99 |\\n| ResNet-50 (He et al., 2016) | 65.47 | 75.36 | 71.96 | 67.61 | 75.21 | 75.36 | 60.57 | 78.17 |\\n| ResNet-101 (He et al., 2016) | 66.19 | 73.91 | 78.82 | 72.88 | 77.40 | 70.17 | 64.14 | 78.22 |\\n| ResNet-152 (He et al., 2016) | 60.78 | 62.69 | 63.12 | 60.89 | 61.98 | 71.63 | 61.40 | 69.31 |\\n\\nWe have tabulated the number of trainable parameters, FLOPs, and the time cost per step (forward and backward propagation) for various settings. The results are presented in the following Table A2. In the baseline, we used ResNet101 with an FCOS detection head. Upon adding the TKT module, the number of trainable parameter increased by 0.92M, FLOPs increased by 28.54G, and the time increased by 0.01s. Continuing with the addition of the MKT module, the parameters, FLOPs, and time increased by 2.21M, 113.52G, and 0.06s respectively. Upon incorporating NE, there was no significant change in parameters and FLOPs, but the time increased by 0.48 seconds. This is due to NE's inability for parallel computation, involving interactions between CPU and GPU, which affects network speed.\\n\\nThe ablation experiments of sensitivity for \\\\( \\\\alpha \\\\) and \\\\( \\\\beta \\\\) in Equation 10. We trained on heart center 1 \\\\( \\\\rightarrow \\\\) 2 for 50 epochs and selected the weights from the last epoch for testing on the test set. We set \\\\( \\\\alpha \\\\) and \\\\( \\\\beta \\\\) at four different levels each. The final results are shown in the Table A3.\\n\\n| Module   | TKT | MKT | NE   | Params. (M) | GFLOPs | Time (s) |\\n|----------|-----|-----|------|------------|--------|----------|\\n| Baseline | \u2717   | \u2717   | \u2717    | 55.24      | 742.57 | 0.63     |\\n| Ours     | \u2713   | \u2717   | \u2717    | 56.16      | 771.11 | 0.64     |\\n|          | \u2713   | \u2713   | \u2717    | 58.37      | 884.63 | 0.70     |\\n|          | \u2713   | \u2713   | \u2713    | 58.37      | 884.81 | 1.18     |\\n\\n| \\\\( \\\\alpha \\\\) | \\\\( \\\\beta \\\\) | 0.01 | 0.1 | 0.5 | 1    |\\n|--------------|--------------|------|-----|-----|------|\\n| 0.01         | 67.15        | 69.23| 69.80| 68.03|\\n| 0.1          | 68.34        | 71.08| 70.03| 70.24|\\n| 0.5          | 68.57        | 70.44| 69.11| 67.29|\\n| 1            | 67.90        | 69.98| 68.27| 66.47|\"}"}
{"id": "meItvvCO7X", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A2. Algorithm Pipeline.\\n\\nAlgorithms 1 and 2 outline the basic procedures of our proposed Topology Knowledge Transfer (TKT) and Morphology Knowledge Transfer (MKT), respectively.\\n\\nA3. Limitations\\n\\nIn medical images, different structures of the cardiac or head actually do not overlap in the real scenario. However, the bounding box in object detection may include some irrelevant information, such as background or even information from other structures (see Figures 2 and 5). Hence, when we construct the topology and morphology information for the ultrasound image, that irrelevant information will also be introduced for training, which may degrade the performance of our method.\\n\\nOur method focuses more on the data that have a fixed structure, which is suitable for most of the human body, such as the cardiac and brain. However, in some cases, such as the fundus photographs and optical coherence tomography that are not able to define the fixed structural information, where our method may not obtain efficient performance in the UDA task in such a situation. Also, in some cases, due to the ultrasound image is in a very low-quality format, which may limit its performance. For example, the cardiac view scanned under Doppler mode contains blood (see Figure 5 row 2), which may obscure part of the structures, leading to the failure of detection.\"}"}
{"id": "meItvvCO7X", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\n\\nTopology Knowledge Transfer (TKT)\\n\\nOutput: $L_{TKT}$: The overall loss of TKT;\\n\\nInput: $f_s, t$: Feature maps from source and target domains;\\n\\n$y_{b_s}$: The ground truth bounding boxes of the source domain;\\n\\n$y_{b_t}$: The pseudo bounding boxes of the target domain;\\n\\n$N$: The total classes number of detection organ;\\n\\n(1). For Source Domain\\n\\n1: for each $i \\\\in [1, N]$ do\\n2:  $c_i \\\\leftarrow$ The organ centroid feature obtained from $f_s$ and $y_{b_s}$.\\n3:  Build memory banks: $\\\\theta_i \\\\leftarrow c_i$.\\n4: end for\\n5: Build visual graph: $(V, E)$, where $V = \\\\{c_i\\\\}_{i=1}^N$, and $E = \\\\{c_i \\\\cdot \\\\theta_i^T\\\\}_{i=1}^N$.\\n6: Build topological representation graph: $G_s \\\\leftarrow \\\\text{GNN}(V, E)$, GNN is the graph neural network.\\n\\n(2). For Target Domain\\n\\n7: for each $i \\\\in [1, N]$ do\\n8:  $c_i \\\\leftarrow$ The organ centroid feature obtained from $f_t$ and $y_{b_t}$.\\n9:  $c_i \\\\leftarrow \\\\theta_i$ IF $c_i$ is NULL.\\n10: end for\\n11: $G_t \\\\leftarrow \\\\text{GNN}(V, E)$.\\n12: $L_{dis}(G_s, G_t) = \\\\inf_{\\\\pi} \\\\sum_{i=1}^N ||G_{t,i} - G_{s,\\\\pi(i)}||_p$, where $\\\\pi$ are all permutations of $N$ organs.\\n\\nOverall Loss of TKT\\n\\n13: $L_{TKT} = L_{dis}(G_s, G_t)$. \\n\\n3\"}"}
{"id": "meItvvCO7X", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2\\n\\nMorphology Knowledge Transfer (MKT)\\n\\nInput:\\n\\\\( X_{s,t} \\\\): Input images from source and target domain;\\n\\\\( y_{b,c}^s \\\\): The bounding boxes and organ class ground truth annotations from the source domain;\\n\\\\( E \\\\): Feature extractor;\\n\\\\( D \\\\): Detector head;\\n\\\\( N \\\\): The total classes number of detection organ;\\n\\n1: Get the feature maps:\\n\\\\[ f_{s,t} \\\\leftarrow E(X_{s,t}). \\\\]\\n\\n2: Get the predict detection result:\\n\\\\[ y_{b,t}^{s,t}, y_{c}^{s,t} \\\\leftarrow D(f_{s,t}). \\\\]\\n\\n3: \\\\( L_{\\\\text{supervised}} \\\\leftarrow L_{\\\\text{cls}}(y_{c}^{s}) + L_{\\\\text{bbox}}(y_{b}^{s}). \\\\)\\n\\n(1). Nodes Sampling\\n\\n4: for each \\\\( i \\\\in [1, N] \\\\) do\\n5: \\\\( f_{i,s,t} \\\\leftarrow \\\\) Get different substructure feature maps \\\\( \\\\{ f_{i,s,t,k} \\\\}_{k=1}^{K} \\\\) from feature maps \\\\( f_{s,t} \\\\) according to \\\\( y_{b,t}^{s,t}, y_{c}^{s,t} \\\\).\\n6: Collect all feature nodes of \\\\( i \\\\)-th substructure from feature maps \\\\( \\\\{ f_{i,s,t,k} \\\\}_{k=1}^{K} \\\\).\\n7: Concatenate feature nodes of \\\\( i \\\\)-substructure from feature maps \\\\( \\\\{ f_{i,s,t,k} \\\\}_{k=1}^{K} \\\\), from layer \\\\( k \\\\) to 1.\\n8: \\\\( \\\\{ o_{i,j} \\\\}_{M,j=1}^{M} \\\\leftarrow \\\\) Sample \\\\( M \\\\) feature nodes \\\\( \\\\{ o_{i,j} \\\\}_{N,M,i,j=1}^{N,M} \\\\) for \\\\( i \\\\)-th substructure with the sample rate equal to the number of all collected feature nodes divided by \\\\( M \\\\), and prioritize sample nodes close to \\\\( k \\\\)-th layer.\\n9: end for\\n\\n(2). Morphological Representation\\n\\n10: Get the morphological representation:\\n\\\\[ g_i \\\\leftarrow \\\\text{MAGNN}(\\\\{O_{i,j}\\\\}) \\\\], where \\\\( \\\\{O_{i,j}\\\\} \\\\leftarrow \\\\text{Concatenate}(\\\\{o_{i,j}\\\\}_{N,M,i,j=1}^{N,M}) \\\\).\\n\\n11: Get the cross-domain interaction of morphological representation:\\n\\\\[ g_i,\\\\pi(j) \\\\leftarrow \\\\text{GA}(g_i) \\\\], GA is graphical attention.\\n\\n(3). Node Classification\\n\\n12: \\\\( L_{\\\\text{class}} = -\\\\sum_{N}^{N} P_{M} \\\\sum_{j=1}^{M} y_{c}^{i} \\\\log g_i,\\\\pi(j)_{s/t}. \\\\)\\n\\n(4). Numerical Equilibrium\\n\\n13: \\\\( M^* \\\\leftarrow \\\\arg\\\\min_{M} \\\\sum_{N}^{N} P_{M} \\\\sum_{j=1}^{M} P_{K}^{K=0} D(M[p(o_{t,i,j})], p(o_{s,i,j})) \\\\), where \\\\( p(k) \\\\) is the probability density function.\\n\\n14: \\\\( L_{\\\\text{dis}}(g_s, g_t) = \\\\sum_{N}^{N} \\\\inf_{\\\\pi} \\\\sum_{M}^{M} \\\\sum_{j=1}^{M} ||g_{t,i,j} - M^*(g_{s,i,\\\\pi(j)})||_{p}. \\\\)\\n\\nOverall Loss of MKT\\n\\n15: \\\\( L_{\\\\text{MKT}} = L_{\\\\text{class}} + L_{\\\\text{dis}}(g_s, g_t). \\\\)\"}"}
{"id": "meItvvCO7X", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images\\n\\nAcknowledgements\\n\\nThis work is partially supported by a research grant from NSFC under Grant 62306254, a research grant from the RGC HKSAR (Project Reference Number: T45-401/22-N), a research grant from the Beijing Institute of Collaborative Innovation (BICI) in collaboration with HKUST under Grant HCIC-004, a research grant from the National Key R&D Program of China (No. 2022YFF0606302) and NSFC (Nos. 62227808 and 62306003).\\n\\nImpact Statement\\n\\nThis study released the first multicenter ultrasound dataset with multi-structure box-level annotations that will greatly benefit the medical image analysis community. In addition, this work is dedicated to enhancing the accuracy of detecting key anatomical structures in the fetal heart and brain through ultrasound imaging technology, aiming to support early diagnosis and intervention, thereby positively impacting pregnancy management and fetal health. Ethically, we strictly adhere to the ethical standards of medical research, and ensure that all image data collection and experiments are approved by the local ethics committee.\\n\\nLooking to the future, the development and application of this technology are expected to have profound impacts on society. It has the potential not only to improve health outcomes for pregnant women and families, reducing the risk of neonatal mortality and developmental disabilities, but also to alleviate the burden on the healthcare system by reducing long-term healthcare costs through early intervention.\\n\\nReferences\\n\\nArnaout, R., Curran, L., Zhao, Y., Levine, J. C., Chinn, E., and Moon-Grady, A. J. An ensemble of neural networks provides expert-level prenatal detection of complex congenital heart disease. *Nature Medicine*, 27(5):882\u2013891, 2021.\\n\\nArruda, V. F., Paixao, T. M., Berriel, R. F., De Souza, A. F., Badue, C., Sebe, N., and Oliveira-Santos, T. Cross-domain car detection using unsupervised image-to-image translation: From day to night. In *2019 International Joint Conference on Neural Networks (IJCNN)*, pp. 1\u20138. IEEE, 2019.\\n\\nCao, S., Joshi, D., Gui, L.-Y., and Wang, Y.-X. Contrastive mean teacher for domain adaptive object detectors. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 23839\u201323848, 2023.\\n\\nChen, C., Zheng, Z., Ding, X., Huang, Y., and Dou, Q. Harmonizing transferability and discriminability for adapting object detectors. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 8869\u20138878, 2020.\\n\\nChen, D., Xia, Y., Zhao, Y., Qian, X., Yin, J., Li, D., Zhang, J., and Zhang, J. Topology-aware brain vessel segmentation in ultrafast doppler imaging. In *2023 IEEE International Ultrasonics Symposium (IUS)*, pp. 1\u20134. IEEE, 2023.\\n\\nChen, H., Wu, L., Dou, Q., Qin, J., Li, S., Cheng, J.-Z., Ni, D., and Heng, P.-A. Ultrasound standard plane detection using a composite neural network framework. *IEEE Transactions on Cybernetics*, 47(6):1576\u20131586, 2017.\\n\\nChen, M., Chen, W., Yang, S., Song, J., Wang, X., Zhang, L., Yan, Y., Qi, D., Zhuang, Y., Xie, D., et al. Learning domain adaptive object detection with probabilistic teacher. In *International Conference on Machine Learning*, pp. 3040\u20133055. PMLR, 2022.\\n\\nChen, Y., Wang, H., Li, W., Sakaridis, C., Dai, D., and Van Gool, L. Scale-aware domain adaptive faster r-cnn. *International Journal of Computer Vision*, 129(7):2223\u20132243, 2021.\\n\\nCordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., and Schiele, B. The cityscapes dataset for semantic urban scene understanding. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 3213\u20133223, 2016.\\n\\nCuturi, M. Sinkhorn distances: Lightspeed computation of optimal transport. *Advances in Neural Information Processing Systems*, 26, 2013.\\n\\nDai, W., Li, X., Ding, X., and Cheng, K.-T. Cyclical self-supervision for semi-supervised ejection fraction prediction from echocardiogram videos. *IEEE Transactions on Medical Imaging*, 2022.\\n\\nDeng, J., Li, W., Chen, Y., and Duan, L. Unbiased mean teacher for cross-domain object detection. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 4091\u20134101, 2021.\\n\\nDeng, J., Xu, D., Li, W., and Duan, L. Harmonious teacher for cross-domain object detection. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 23829\u201323838, 2023.\"}"}
{"id": "meItvvCO7X", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images\\n\\nGanin, Y. and Lempitsky, V. Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning, pp. 1180\u20131189. PMLR, 2015.\\n\\nGao, Y., Yang, L., Huang, Y., Xie, S., Li, S., and Zheng, W.-S. Acrofod: An adaptive method for cross-domain few-shot object detection. In European Conference on Computer Vision, pp. 673\u2013690. Springer, 2022.\\n\\nGao, Y., Lin, K.-Y., Yan, J., Wang, Y., and Zheng, W.-S. Asyfod: An asymmetric adaptation paradigm for few-shot domain adaptive object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3261\u20133271, 2023.\\n\\nGong, Y., Zhang, Y., Zhu, H., Lv, J., Cheng, Q., Zhang, H., He, Y., and Wang, S. Fetal congenital heart disease echocardiogram screening based on dgacnn: adversarial one-class classification combined with video transfer learning. IEEE Transactions on Medical Imaging, 39(4):1206\u20131222, 2019.\\n\\nGuan, H. and Liu, M. Domain adaptation for medical image analysis: a survey. IEEE Transactions on Biomedical Engineering, 69(3):1173\u20131185, 2021.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770\u2013778, 2016.\\n\\nHe, Z. and Zhang, L. Multi-adversarial faster-rcnn for unrestricted object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6668\u20136677, 2019.\\n\\nHsu, H.-K., Yao, C.-H., Tsai, Y.-H., Hung, W.-C., Tseng, H.-Y., Singh, M., and Yang, M.-H. Progressive domain adaptation for object detection. In Proceedings of Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 749\u2013757, 2020.\\n\\nHuai, Z., Ding, X., Li, Y., and Li, X. Context-aware pseudo-label refinement for source-free domain adaptive fundus image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 618\u2013628. Springer, 2023.\\n\\nHuang, J., Guan, D., Xiao, A., and Lu, S. Fsdr: Frequency space domain randomization for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6891\u20136902, 2021.\\n\\nHuang, J., Guan, D., Xiao, A., and Lu, S. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. Advances in Neural Information Processing Systems, 34:3635\u20133649, 2021.\\n\\nJin, H., Che, H., and Chen, H. Unsupervised domain adaptation for anatomical landmark detection. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 695\u2013705. Springer, 2023.\\n\\nKim, S., Choi, J., Kim, T., and Kim, C. Self-training and adversarial background regularization for unsupervised domain adaptive one-stage object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6092\u20136101, 2019.\\n\\nKim, T., Jeong, M., Kim, S., Choi, S., and Kim, C. Diversify and match: A domain adaptive representation learning paradigm for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12456\u201312465, 2019.\\n\\nKipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\\n\\nLeclerc, S., Smistad, E., Pedrosa, J., \u00d8stvik, A., Cervenanisky, F., Espinosa, F., Espeland, T., Berg, E. A. R., Jodoin, P.-M., Grenier, T., et al. Deep learning for segmentation using an open large-scale dataset in 2d echocardiography. IEEE Transactions on Medical Imaging, 38(9):2198\u20132210, 2019.\\n\\nLi, M., Zhang, H., Li, J., Zhao, Z., Zhang, W., Zhang, S., Pu, S., Zhuang, Y., and Wu, F. Unsupervised domain adaptation for video object grounding with cascaded debiasing learning. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 3807\u20133816, 2023.\\n\\nLi, W., Liu, X., and Yuan, Y. Sigma: Semantic-complete graph matching for domain adaptive object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5291\u20135300, 2022.\\n\\nLi, W., Liu, X., and Yuan, Y. Sigma++: Improved semantic-complete graph matching for domain adaptive object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\\n\\nLi, X., Chen, H., Qi, X., Dou, Q., Fu, C.-W., and Heng, P.-A. H-denseunet: hybrid densely connected unet for liver and tumor segmentation from ct volumes. IEEE Transactions on Medical Imaging, 37(12):2663\u20132674, 2018.\\n\\nLi, Y.-J., Dai, X., Ma, C.-Y., Liu, Y.-C., Chen, K., Wu, B., He, Z., Kitani, K., and Vajda, P. Cross-domain adaptive teacher for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7581\u20137590, 2022.\\n\\nLiang, J., Hu, D., and Feng, J. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning, pp. 6028\u20136039. PMLR, 2020.\"}"}
{"id": "meItvvCO7X", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images\\n\\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pp. 740\u2013755. Springer, 2014.\\n\\nLin, T.-Y., Doll\u00e1r, P., Girshick, R., He, K., Hariharan, B., and Belongie, S. Feature pyramid networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2117\u20132125, 2017.\\n\\nLin, Z., Li, S., Ni, D., Liao, Y., Wen, H., Du, J., Chen, S., Wang, T., and Lei, B. Multi-task learning for quality assessment of fetal head ultrasound images. Medical Image Analysis, 58:101548, 2019.\\n\\nLiu, D., Zhang, D., Song, Y., Zhang, F., O\u2019Donnell, L., Huang, H., Chen, M., and Cai, W. Pdam: A panoptic-level feature alignment framework for unsupervised domain adaptive instance segmentation in microscopy images. IEEE Transactions on Medical Imaging, 40(1):154\u2013165, 2020.\\n\\nLiu, H., Wang, J., and Long, M. Cycle self-training for domain adaptation. Advances in Neural Information Processing Systems, 34:22968\u201322981, 2021.\\n\\nLiu, X. and Yuan, Y. A source-free domain adaptive polyp detection framework with style diversification flow. IEEE Transactions on Medical Imaging, 41(7):1897\u20131908, 2022.\\n\\nLiu, X., Li, W., and Yuan, Y. Decoupled unbiased teacher for source-free domain adaptive medical object detection. IEEE Transactions on Neural Networks and Learning Systems, 2023a.\\n\\nLiu, Y., Wang, J., Huang, C., Wang, Y., and Xu, Y. Cigar: Cross-modality graph reasoning for domain adaptive object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 23776\u201323786, 2023b.\\n\\nMattolin, G., Zanella, L., Ricci, E., and Wang, Y. Confmix: Unsupervised domain adaptation for object detection via confidence-based mixing. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 423\u2013433, 2023.\\n\\nMonteagudo, A. Holoprosencephaly. American Journal of Obstetrics & Gynecology, 223(6):B13\u2013B16, 2020.\\n\\nOuyang, D., He, B., Ghorbani, A., Yuan, N., Ebinger, J., Langlotz, C. P., Heidenreich, P. A., Harrington, R. A., Liang, D. H., Ashley, E. A., et al. Video-based ai for beat-to-beat assessment of cardiac function. Nature, 580(7802):252\u2013256, 2020.\\n\\nOza, P., Sindagi, V. A., Sharmini, V. V., and Patel, V. M. Unsupervised domain adaptation of object detectors: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\\n\\nPiao, Z., Tang, L., and Zhao, B. Unsupervised domain-adaptive object detection via localization regression alignment. IEEE Transactions on Neural Networks and Learning Systems, 2023.\\n\\nPu, B., Li, K., Li, S., and Zhu, N. Automatic fetal ultrasound standard plane recognition based on deep learning and iiot. IEEE Transactions on Industrial Informatics, 17(11):7771\u20137780, 2021.\\n\\nRen, S., He, K., Girshick, R., and Sun, J. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in Neural Information Processing Systems, 28, 2015.\\n\\nRodriguez, A. L. and Mikolajczyk, K. Domain adaptation for object detection via style consistency. arXiv preprint arXiv:1911.10033, 2019.\\n\\nShin, H., Kim, H., Kim, S., Jun, Y., Eo, T., and Hwang, D. Sdc-uda: V olumetric unsupervised domain adaptation framework for slice-direction continuous cross-modality medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7412\u20137421, 2023.\\n\\nSimonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\\n\\nSindagi, V. A., Oza, P., Yasarla, R., and Patel, V. M. Prior-based domain adaptive object detection for hazy and rainy conditions. In European Conference on Computer Vision, pp. 763\u2013780. Springer, 2020.\\n\\nTian, Z., Shen, C., Chen, H., and He, T. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9627\u20139636, 2019.\\n\\nVs, V., Gupta, V., Oza, P., Sindagi, V. A., and Patel, V. M. Mega-cda: Memory guided attention for category-aware unsupervised domain adaptive object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4516\u20134526, 2021.\\n\\nWang, Y., Zhang, R., Zhang, S., Li, M., Xia, Y., Zhang, X., and Liu, S. Domain-specific suppression for adaptive object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9603\u20139612, 2021.\"}"}
{"id": "meItvvCO7X", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images\\n\\nWu, L., Cheng, J.-Z., Li, S., Lei, B., Wang, T., and Ni, D.\\n\\nFetal ultrasound image quality assessment with deep convolutional networks. IEEE Transactions on Cybernetics, 47(5):1336\u20131349, 2017.\\n\\nXing, F., Yang, X., Cornish, T. C., and Ghosh, D. Learning with limited target data to detect cells in cross-modality images. Medical Image Analysis, 90:102969, 2023.\\n\\nXu, M., Zhang, T., and Zhang, D. Medrdf: A robust and retrain-less diagnostic framework for medical pretrained models against adversarial attack. IEEE Transactions on Medical Imaging, 41(8):2130\u20132143, 2022.\\n\\nXue, C., Zhu, L., Fu, H., Hu, X., Li, X., Zhang, H., and Heng, P.-A. Global guidance network for breast lesion segmentation in ultrasound images. Medical Image Analysis, 70:101989, 2021.\\n\\nYang, J., Shi, S., Wang, Z., Li, H., and Qi, X. St3d++: Denoised self-training for unsupervised domain adaptation on 3d object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5):6354\u20136371, 2022.\\n\\nYang, J., Ding, X., Zheng, Z., Xu, X., and Li, X. Graphecho: Graph-driven unsupervised domain adaptation for echocardiogram video segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11878\u201311887, 2023.\\n\\nYu, F., Wang, D., Chen, Y., Karianakis, N., Shen, T., Yu, P., Lymberopoulos, D., Lu, S., Shi, W., and Chen, X. Unsupervised domain adaptation for object detection via cross-domain semi-supervised learning. arXiv preprint arXiv:1911.07158, 2019.\\n\\nYu, F., Wang, D., Chen, Y., Karianakis, N., Shen, T., Yu, P., Lymberopoulos, D., Lu, S., Shi, W., and Chen, X. Sc-uda: Style and content gaps aware unsupervised domain adaptation for object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 382\u2013391, 2022.\\n\\nZhao, G., Li, G., Xu, R., and Lin, L. Collaborative training between region proposal localization and classification for domain adaptive object detection. In European Conference on Computer Vision, pp. 86\u2013102. Springer, 2020.\\n\\nZhao, L. and Wang, L. Task-specific inconsistency alignment for domain adaptive object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14217\u201314226, 2022.\\n\\nZhao, L., Li, K., Pu, B., Chen, J., Li, S., and Liao, X. An ultrasound standard plane detection model of fetal head based on multi-task learning and hybrid knowledge graph. Future Generation Computer Systems, 135:234\u2013243, 2022.\\n\\nZheng, Y., Huang, D., Liu, S., and Wang, Y. Cross-domain object detection through coarse-to-fine feature adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13766\u201313775, 2020.\\n\\nZheng, Z., Yang, J., Ding, X., Xu, X., and Li, X. Gl-fusion: Global-local fusion network for multi-view echocardiogram video segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 78\u201388. Springer, 2023.\\n\\nZhuang, X., Li, L., Payer, C., \u02c7Stern, D., Urschler, M., Heinrich, M. P., Oster, J., Wang, C., Smedby,\u00a8O., Bian, C., et al. Evaluation of algorithms for multi-modality whole heart segmentation: an open-access grand challenge. Medical Image Analysis, 58:101537, 2019.\"}"}
{"id": "meItvvCO7X", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images\\n\\nEquation 5 to the discrete form as following:\\n\\n\\\\[ L_{\\\\text{dis}}(g_{s}, g_{t}) = \\\\sum_{i=1}^{N} \\\\inf_{\\\\pi} \\\\left\\\\| g_{t,i,j} - M\\\\left(g_{s,i,\\\\pi(j)}\\\\right) \\\\right\\\\|_{p} \\\\]\\n\\nwhere the infimum is over all permutations \\\\( \\\\pi \\\\) of \\\\( M \\\\) graph nodes, \\\\( g_{s/t,i,j} \\\\) denotes that the \\\\( j \\\\)-th graph node of \\\\( i \\\\)-th sub-structure from source/target domain, and mapping \\\\( M \\\\) is formulated by Equation 8. The loss \\\\( L_{\\\\text{MKT}} \\\\) of the MKT module in Section 3.2 is written as\\n\\n\\\\[ L_{\\\\text{MKT}} = L_{\\\\text{class}} + L_{\\\\text{dis}}(g_{s}, g_{t}) \\\\]\\n\\nFinally, the overall loss of our proposed ToMo-UDA can be summarized as:\\n\\n\\\\[ L_{\\\\text{all}} = \\\\alpha L_{\\\\text{TKT}} + \\\\beta L_{\\\\text{MKT}} + L_{\\\\text{supervised}} \\\\]\\n\\nwhere \\\\( \\\\alpha \\\\) and \\\\( \\\\beta \\\\) is the decay ratio of the loss \\\\( L_{\\\\text{TKT}} \\\\) and \\\\( L_{\\\\text{MKT}} \\\\). Our experiment found that when both \\\\( \\\\alpha \\\\) and \\\\( \\\\beta \\\\) are set to 0.1, it can achieve the best domain adaptation performance.\\n\\n4. Experiment\\n\\n4.1. Datasets and Evaluation\\n\\nThe proposed FUSH \\\\( \\\\text{2} \\\\) dataset was collected from two medical centers and includes fetal head and heart images. The dataset collection and experiment are approved by the local ethics committee with the approval number LLYJ2022-014-005. A senior and experienced sonographer annotated the bounding box of the organ structures and its class name. These ultrasound images are obtained by local sonographers from various ultrasound devices such as Samsung and SonoScape. There are a total of 1,391 fetal head images and 1,978 heart images in the FUSH \\\\( \\\\text{2} \\\\) dataset. When compared to other counterparts like CAMUS (Leclerc et al., 2019) and EchoNet (Ouyang et al., 2020), FUSH \\\\( \\\\text{2} \\\\) collects data from multiple health centers with a wide range of resolutions. In contrast, CAMUS and EchoNet use data from a single health center. Additionally, when compared to CardiacUDA (Yang et al., 2023), FUSH \\\\( \\\\text{2} \\\\) has wide-ranged resolutions and remarkably 16 annotated regions (9 for heart and 7 for head), whereas CardiacUDA only has 4. The benefits of our dataset compared to existing datasets are shown in Table 1. The main anatomical structure abbreviations are shown in Table 2.\\n\\nTable 1: The comparison of our FUSH \\\\( \\\\text{2} \\\\), CardiacUDA (Yang et al., 2023), CAMUS (Leclerc et al., 2019), and EchoNet (Ouyang et al., 2020).\\n\\n| Dataset        | Our FUSH \\\\( \\\\text{2} \\\\) | CardiacUDA | CAMUS | EchoNet |\\n|----------------|-------------------------|------------|-------|---------|\\n| Annotated Images | 3,369           | 4,960      | 1,000 | 20,060  |\\n| Views           | 2                      | 4          | 1     | 1       |\\n| Resolution      | 480-1080p             | 720p       | 480p  | 120p    |\\n| Annotated Regions | LV , RV , LA, RA, DAO, VS, SP, CR, R, LSCSP, BM, T, S, C, CP | LV , RV , LA, RA | LV , LA | LV |\\n\\nTable 2: Professional terms and abbreviations of FUSH \\\\( \\\\text{2} \\\\).\"}"}
{"id": "meItvvCO7X", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To the diverse styles captured in the center2 dataset, which only, and remarkably, it surpasses the target only on center detection capability on center 1 under these conditions. In contrast, our approach, which exhibit dense overlapping in these views. Traditional detection methods are prone to false positives and negatives. This superior performance can be attributed largely to the samples selected not being which may be caused by the samples selected not being adaptive detection of key structures in the fetal head. As the representation of the overall target domain, may lead to few-shot UDA methods that are lower than.\\n\\nWe conducted experiments focused on the of the brain remains a significant challenge. This is due to described in Figure 2, although the overlap of brain structures depicted in Figure 2, although the overlap of brain structures. ConfMix (Mattolin et al., 2023) and AcroFOD (Gao et al., 2022) are lower than Ours. However, when compared to the target domain, our approach significantly outperforms the baselines. The performance of ConfMix (Mattolin et al., 2023) and AcroFOD (Gao et al., 2022) is lower than that of Ours.\\n\\nThe table below summarizes the results of the experiments, showing the mAP (%) for different methods on the heart and head datasets. The table indicates that our approach achieves the highest mAP (%) in most cases, demonstrating its superior performance compared to the baselines.\\n\\nTable 3: Domain adaptation results on the heart.\\n\\n| Method          | Center 1 | Center 2 |\\n|-----------------|----------|----------|\\n| Baseline        | 82.12    | 71.33    |\\n| Ours            | 90.54    | 82.97    |\\n| SIGMA++ (Li et al., 2023b) | 80.01  | 71.82    |\\n| CMT (Cao et al., 2023) | 60.25  | 86.17    |\\n| SIGMA (Li et al., 2022a)  | 59.03   | 66.16    |\\n| LRA (Piao et al., 2023) | 55.57  | 71.97    |\\n| AT (Li et al., 2022b)   | 39.17   | 73.40    |\\n\\nTable 4: Domain adaptation results on the head.\\n\\n| Method          | Source Only | Target Only |\\n|-----------------|-------------|-------------|\\n| Baseline        | 90.25       | 79.98       |\\n| Ours            | 94.72       | 88.07       |\\n| SIGMA++ (Li et al., 2023b) | 82.31  | 60.45       |\\n| CMT (Cao et al., 2023) | 80.01  | 35.32       |\\n| SIGMA (Li et al., 2022a)  | 71.97   | 80.53       |\\n| LRA (Piao et al., 2023) | 71.97  | 80.53       |\\n| AT (Li et al., 2022b)   | 39.17   | 73.40       |\\n\\nTable 5: Cross-modal adaptation results on MMWHS.\\n\\n| Method          | Site R | Site G |\\n|-----------------|--------|--------|\\n| Baseline        | 84.71  | 77.62  |\\n| Ours            | 97.21  | 90.02  |\\n| SIGMA++ (Li et al., 2023b) | 82.12  | 77.57  |\\n| CMT (Cao et al., 2023) | 77.71  | 75.56  |\\n| SIGMA (Li et al., 2022a)  | 75.56  | 66.93  |\\n\\nTable 6: Domain adaptation results on CardiacUDA dataset.\\n\\n| Method          | LA    | RA    | RV    |\\n|-----------------|-------|-------|-------|\\n| Baseline        | 75.44 | 57.12 | 85.12 |\\n| Ours            | 81.20 | 66.52 | 94.58 |\\n| SIGMA++ (Li et al., 2023b) | 75.49 | 80.46 | 71.76 |\\n| CMT (Cao et al., 2023) | 64.62 | 69.30 | 59.30 |\\n| SIGMA (Li et al., 2022a)  | 92.90 | 97.33 | 81.92 |\\n| LRA (Piao et al., 2023) | 76.86 | 95.28 | 81.92 |\\n\\nTable 7: Ablation results on heart and head datasets.\\n\\n| Method | TKT | MKT | NE | Source Only | Target Only |\\n|--------|-----|-----|----|-------------|-------------|\\n| Baseline | \u2713 | \u2713 | \u2717 | 82.97       | 71.33       |\\n| Ours    | \u2717 | \u2713 | \u2717 | 84.72       | 62.24       |\\n| SIGMA++ (Li et al., 2023b) | \u2713 | \u2713 | \u2717 | 71.82       | 86.42       |\\n| CMT (Cao et al., 2023) | \u2713 | \u2713 | \u2717 | 86.17       | 58.58       |\\n| SIGMA (Li et al., 2022a)  | \u2713 | \u2713 | \u2717 | 84.83       | 79.30       |\\n| LRA (Piao et al., 2023) | \u2713 | \u2713 | \u2717 | 82.47       | 67.22       |\\n| AT (Li et al., 2022b)   | \u2713 | \u2713 | \u2717 | 82.31       | 78.08       |\"}"}
{"id": "meItvvCO7X", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images\\n\\nFigure 5: Qualitative result comparison of Source Only, CMT (Cao et al., 2023), SIGMA++ (Li et al., 2023b), ToMo-UDA and Ground Truth. The first and second rows show the results from center 1 $\\\\rightarrow$ 2 and center 2 $\\\\rightarrow$ 1 on the heart of the FUSH 2 dataset. The third and last rows show the results from center 1 $\\\\rightarrow$ 2 and center 2 $\\\\rightarrow$ 1 on the head of the FUSH 2 dataset.\\n\\nFigure 6: The distribution of the numerical value from the feature maps. Black box: source domain. Magenta box: target domain. Green box: source domain after the proposed Numerical Equilibrium (NE).\\n\\nthe considerable individual variability in fetal development, coupled with the complexity of brain anatomy. Despite these challenges, our method demonstrated excellent detection performance. As shown in Table 4, we achieved the best detection results between the two centers. On center 1 $\\\\rightarrow$ 2, our method improved by 5.36%, and on center 2 $\\\\rightarrow$ 1, we observed an improvement of 5.31%. It's notable that on center 2 $\\\\rightarrow$ 1, despite achieving the best detection metrics, there remains a considerable gap from the target only. We attribute this to the lower resolution of images in center2 compared to center1, with resolutions typically around $648 \\\\times 480$ in center2, whereas center1 images are generally around $1280 \\\\times 872$. This resolution disparity makes it challenging for models trained on center2 to extract detailed organ structural information, leading to less effective domain adaptation.\\n\\nUDA on CardiacUDA Dataset. CardiacUDA was originally proposed in (Yang et al., 2023) to explore unsupervised domain adaption for echocardiogram video segmentation. We test the proposed ToMo-UDA using the heart view from the CardiacUDA dataset for comparison with existing methods. Specifically, for CardiacUDA, each video was labelled with 5 frames, and the segmentation mask annotations were transformed into the bounding box, including LA, RA, LV , and RV structures. The performance of ToMo-UDA on the CardiacUDA is summarized in Table 6. The results suggest that our method still outperforms all UDAOD methods. From Site R $\\\\rightarrow$ Site G, our method slightly underperforms other methods by 2.01% and 1.93% on the LV and RV structures. However, for the LA and RA, which are relatively more challenging to detect, our method surpasses the second-best method by 9.55% and 17.41%, respectively.\"}"}
{"id": "meItvvCO7X", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images\\n\\nrespectively. Existing UDAOD methods often experience performance degradation when detecting smaller objects. By incorporating topological and morphological priors from medical images, our approach achieves balanced detection results for each organ structure.\\n\\nUDA on MMWHS Dataset. Table 5 shows the comparisons of our ToMo-UDA with other SOTA methods for cross-modal adaptation. Our method outperforms the second-best by 5.83% mAP on CT $\\\\rightarrow$ MRI and by 7.32% mAP on MRI $\\\\rightarrow$ CT. This demonstrates the effectiveness and scalability of our ToMo-UDA.\\n\\n4.4. Analysis and Ablation Study.\\n\\nAblation on Source Only. As shown in Table 7, compared to the Source Only baseline, ToMo-UDA substantially improves the cross-domain fetal structure detection task. For example, ToMo-UDA outperforms Source Only by 27.43% mAP and 19.72% mAP for the adaptation center 1 $\\\\rightarrow$ 2 detection on fetal heart and head, respectively, which demonstrates the effectiveness of the proposed method.\\n\\nAblation on TKT. Compared to Source Only, TKT increases mAP by 14.70% and 7.74% for adaptive center 1 $\\\\rightarrow$ 2 and center 2 $\\\\rightarrow$ 1 heart detection, respectively, as shown in Table 7. The same pattern can be found in head detection. For example, TKT improves mAP by 15.58% compared to Source Only in adaptive center 1 $\\\\rightarrow$ 2 head detection. This indicates that TKT can transfer topological knowledge between source and target domains to facilitate better adaptive detection through invariant topological knowledge.\\n\\nAblation on MKT. Similarly, compared to the Source Only again, MKT enhances mAP by 15.31% and 4.49% in center 1 $\\\\rightarrow$ 2 and center 2 $\\\\rightarrow$ 1 head detection, respectively. In heart structure detection, MKT significantly improves detection performance. These experimental results demonstrate the effectiveness of aligning morphological knowledge within the structure.\\n\\nAblation on Numerical Equilibrium. As noted in Section 3.2, inter-domain discrepancies in low-level numerical distributions can impact the performance of our model. Given the challenges of computing continuous distributions, we have adopted a domain-mapping approach based on the numerical distribution of the input image. Figure 6 visualizes the numerical distributions of feature maps from the first two layers, demonstrating the impact of our numerical equilibrium. It highlights significant differences in numerical distributions of the feature map layers between the source and target domains (magenta box). However, after applying the numerical equilibrium operation (green box), the low-level feature distribution of the source domain has been adjusted closely to the target domain, and this equilibrium effect is equally applicable to deeper layers. The performance improvement is shown in Table 7. These results show the effectiveness of the proposed numerical equilibrium.\\n\\nQualitative Result Comparison. A comparison of the quantitative detection results is presented in Figure 5. We selected Source Only and the latest UDA methods CMT (Cao et al., 2023) and SIGMA++ (Li et al., 2023b) for visual comparison. On complex views such as the heart, Source Only struggles to detect intricate and overlapping structures, especially in color ultrasound. Both CMT and SIGMA++ have varying degrees of false detections, missed detections or duplicate detections. In contrast, the results of our ToMo-UDA closely match the ground truth annotations. The same is true for the head view.\\n\\nFigure 7 shows the feature distribution visualization by our method and Source Only. In Figure 7(a) and (b), we can clearly observe that our method can distinguish the various anatomical structures. However, the entangled distribution of categories shows that Source Only is difficult to separate the key structures. Similarly, in Figure 7(c) and (d), we find the same advantage of our ToMo-UDA.\\n\\n5. Conclusion\\n\\nThis work proposes the ToMo-UDA for the issue of adaptive detection of fetal key structures in medical scenarios by aligning the morphological knowledge and topology knowledge of the source and target domains. Extensive experiments verify the effectiveness of ToMo-UDA in UDAOD on the collected and public datasets. We intuitively understand how the proposed ToMo-UDA works in the UDAOD task through ablation experiments and visualizations. In addition, we will release a new valuable dataset (FUSH 2) for fetal structure detection across domains, and we believe that FUSH 2 and ToMo-UDA can further inspire the community to address object detection and domain adaptive problems.\\n\\nPlease see Appendix Section A3 for Limitations.\"}"}
{"id": "meItvvCO7X", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images\\n\\nUnsupervised domain adaptation (UDA) has been proposed to solve the aforementioned challenges by mitigating the domain gaps. UDA aims to maximize the performance of the target domain while minimizing expert supervision through invariant feature learning (Vs et al., 2021), self-training (Zhao et al., 2020; Kim et al., 2019a), image translation (Chen et al., 2020; Hsu et al., 2020), domain randomization (Kim et al., 2019b; Rodriguez & Mikolajczyk, 2019), etc.\\n\\nIn natural images, for example, the relationship between the objects is always chaotic and lacks specific patterns. In contrast, in ultrasound images, the relationship of anatomical structures, e.g., left ventricle and right ventricle, conforms to the theory of human anatomy and knowledge of topology regardless of health centers (domains). For example, as illustrated in Figure 2, the thalamus (T) structure in the fetal head view always appears in symmetrical pairs. Similarly, the two ribs (R) flanking the heart are another common example. In addition, sonographers diagnose mainly based on topological and morphological features (Chen et al., 2023), which provides us with new insights into UDA. As shown in Figure 1, substructures of the same view remain consistent in topology and morphology. In medical images, topological information focuses on the relationship between anatomical composition and positional relationship, while morphological information refers to the textural, shape, and morphology features of the interior of the anatomical structures.\\n\\nThe unique characteristics of ultrasound images indicate that previous methods for UDA object detection (UDAOD) in natural scenarios are not suitable or available for our task. UDAOD methods for natural scenarios do not consider a priori knowledge of medical images, yet this is one of the most significant properties in medical scenarios. For example, previous medical UDAOD methods have not considered topology knowledge and morphology information characteristic consistency for different domains. Motivated by the above discussion, we propose a novel UDA method named ToMo-UDA for fetus anatomical structure detection. The method includes two modules - Topology Knowledge Transfer (TKT) and Morphology Knowledge Transfer (MKT). TKT aligns features by reconstructing anatomy features, while MKT formulates consistent and independent representations for each substructure of an organ.\\n\\nCollecting datasets from different health centers is challenging, and annotating multi-structure for these datasets is especially difficult, as it requires the participation of numerous experienced obstetricians. Therefore, multicenter ultrasound datasets with multiple structures of detailed box-level annotations are currently unavailable and scarce. To address the above discussion, the proposed FUSH that serves as Fetal Ultrasound benchmark with 1,978 Heart and 1,391 Head views, is collected from Two health centers.\\n\\nFigure 2: The fetal head and heart view with the key anatomical structures. Fetal ultrasound examination, which is required to screen for disease during pregnancy, relies on the presence of anatomical structures that are more challenging to examine than in adults. Moreover, ultrasound images of FUSH are collected from different equipment, including Samsung, Sonoscape, and Philips. The gestational age of the fetus ranges from 20 to 34 weeks. All data were annotated with 16 anatomy regions and 2 view labels by ultrasonographers who have more than seven years of clinical experience.\\n\\nIn summarize, our contributions include:\\n\\n1. A comprehensive real-world fetal ultrasound dataset from two health centers with 1,978 heart and 1,391 head views, namely FUSH, is released. FUSH is labeled with 16 anatomy regions by experienced sonographers and includes various equipment and gestational weeks ranging from 20 to 34 weeks.\\n\\n2. A new UDA method, namely ToMo-UDA, has been proposed. ToMo-UDA consists of two modules, i.e., TKT and MKT. TKT and MKT align ultrasound features, focusing on anatomical structures and morphological features for accurate diagnosis, respectively.\\n\\n3. Extensive experiments show that the proposed ToMo-UDA outperforms all UDAOD baseline and state-of-the-art (SOTA) structure detection techniques with a clear margin. Our work opens up new possibilities for accurate and reliable object detection in medical image analysis. Datasets and source code are available at https://github.com/xmedlab/ToMo-UDA.\\n\\n2. Related Work\\n\\n2.1. UDA Object Detection in Natural Scenarios\\n\\nRecently, the UDAOD task has become a hot topic (Sindagi et al., 2020; Wang et al., 2021; Zhao & Wang, 2022; Zhao et al., 2020; Yu et al., 2022; Chen et al., 2021), and there have been many studies that can be broadly grouped into adversarial learning (Ganin & Lempitsky, 2015; Zheng et al., 2020; Vs et al., 2021), self-training (Kim et al., 2019a; Yu et al., 2019; Huang et al., 2021b), image-to-image translation (Chen et al., 2020; Hsu et al., 2020), domain randomization (Kim et al., 2019b; Rodriguez & Mikolajczyk, 2019), etc.\"}"}
{"id": "meItvvCO7X", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images\\n\\n...section (Kim et al., 2019b; Arruda et al., 2019; Huang et al., 2021a), and others (Deng et al., 2021; Li et al., 2022b; Rodriguez & Mikolajczyk, 2019). (He & Zhang, 2019) proposed to find the invariance feature from the source and target domain through multi-adversarial training. The self-training technique (Yang et al., 2022) utilizes unlabeled target data by training with target pseudo-labels, and a typical study (Liu et al., 2021) explores cycle self-training, a principled self-training algorithm that explicitly enforces cross-domain generalization of pseudo-labels. Recently, some remarkably new UDAOD algorithms have also emerged, e.g., mean-teacher training (Chen et al., 2022; Cao et al., 2023; Deng et al., 2023) and graph-based reasoning methods (Li et al., 2022a; 2023b; Liu et al., 2023b). Medical UDAOD differs from natural scenarios, leading to suboptimal performance with general object detection methods.\\n\\n2.2. UDA in Medical Scenarios\\n\\nFew works in the literature focus on UDAOD studies in medical scenarios. One pioneering work (Jin et al., 2023) uses adaptive adversarial training to learn domain-invariant features to minimize domain shifts. The more relevant top-ics are source-free (Liang et al., 2020) domain adaptive medical object detection (Liu et al., 2023a; Liu & Yuan, 2022; Xing et al., 2023) and UDA for segmentation (Shin et al., 2023; Yang et al., 2023; Huai et al., 2023; Liu et al., 2020). (Liu et al., 2023a) systematically analyzed the bias in source-free domain adaptation medical object detection by constructing a structural causal model and proposed an unbiased source-free domain adaptation framework based on the decoupled unbiased teacher. In another popular work, SMPT (Liu & Yuan, 2022) transfers the domain-invariant knowledge stored in the pre-trained source model to the target model via source knowledge distillation. Recently, in an unsupervised segmentation task, (Yang et al., 2023) proposed a mining prior knowledge of echocardiogram videos by aligning global and local features from source and target domains. In a nutshell, few studies have been conducted on UDAOD in medical scenarios due to the unavailability of datasets with detailed box-level annotations from multiple centers. The release of our dataset will benefit UDAOD in medical scenarios. In addition, previous studies have yet to fully explore topology and morphology knowledge in both source and target domains.\\n\\n3. Method\\n\\nTaking heart as an example, Figure 3 shows the overall pipeline of our ToMo-UDA, which consists of a source domain flow and a target domain flow. First, for both domains, a shared encoder $E(\u00b7)$ based on feature pyramid network (Lin et al., 2017) is leveraged to extract features $\\\\{f_k\\\\}_{k=1}^K$, $f_k \\\\in \\\\mathbb{R}^{h_k \\\\times w_k \\\\times d}$ from input images, where $d$ and $K$ denote the total number of channels and feature map layers, respectively. Subsequently, the feature maps are passed to an object detection head (e.g., FCOS head), thus substructure centroid $\\\\{c_i\\\\}_{i=1}^N$, $c_i \\\\in \\\\mathbb{R}^{1 \\\\times d}$, bounding boxes $y_b \\\\in \\\\mathbb{R}^{N \\\\times 4}$ and organ class $y_c \\\\in \\\\mathbb{R}^N$ are predicted from the detection head, here $N$ represent the total number of organs. For the source domain, the ground truth annotation and prediction results are formulated as $L_{\\\\text{supervised}} = L_{\\\\text{class}}(y_c) + L_{\\\\text{reg}}(y_b)$, (1) for the supervision loss in object detection, where $L_{\\\\text{class}}$ is cross-entropy loss and $L_{\\\\text{reg}}$ is the $L_1$ loss. On top of the above common object detection pipeline, we propose two modules in ToMo-UDA, named Topology Knowledge Transfer (TKT) and Morphology Knowledge Transfer (MKT), to bridge the domain gap from different hospitals. TKT allows for transferring the heart topology knowledge from the source to the target domain (see Section 3.1 and Appendix Section A2). MKT builds the complete inter-graph knowledge for different substructures, improving morphological representation consistency of the same substructure from different domains by minimizing their feature discrepancy (see Section 3.2 and Appendix Section A2).\\n\\n3.1. Topology Knowledge Transfer\\n\\nUnlike datasets like Cityscapes (Cordts et al., 2016) and COCO (Lin et al., 2014) for object detection in nature images, large domain gaps in ultrasound images actually depend on equipment manufacturers and physician experience across different medical centers. Despite the significant differences between the domains, we/sonographers have observed that substructures of the fetal heart consistently maintain their relative location. For instance, as shown in Figure 1, the substructure locations, such as locations of the left ventricle and left atrium of the heart in ultrasound images, remain consistent. Therefore, this consistent information can be utilized as the robust prior topology knowledge for our domain adaptation problem. Motivated by the above discussion, we concluded that fully annotated location labels of structures from the source domain can serve as complete structural information, making them suitable to be used as a standard reference for aligning heart knowledge in the target domain.\\n\\nThe TKT module is designed to align topology knowledge across domains to tackle the above problems. In the TKT module, we first take the centroid feature of each substructure $\\\\{c_i\\\\}^N_{i=1}$ generated by $E(\u00b7)$ from both the source and target domain. Subsequently, we construct topology graphs $(V, E)$ for each domain to represent the heart topology, where $V$ denote the representation of $N$ (e.g., $N = 9$ for heart) substructures, and $E$ denote the set of edges connecting each substructure, respectively. To construct the representation of substructures, we introduce memory banks to maintain substructure features from large-scale data samples. Then, acquire centroid representation $\\\\{\\\\theta_i\\\\}^N_{i=1}$, $\\\\theta_i \\\\in \\\\mathbb{R}^{1 \\\\times d}$, to align topology knowledge across domains.\"}"}
{"id": "meItvvCO7X", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The overview of the proposed ToMo-UDA. A shared-parameter backbone and a detection head generate detection results from inputs of source and target domains. In the Topology Knowledge Transfer module, we obtain centroid representation from ground truth labels and pseudo-labels for the source and target domain, respectively, for constructing overall topological representations. In Morphology Knowledge Transfer, we sample nodes from feature maps from bounding boxes as morphological representations for each organ and align low-level and morphological semantics through the numerical equilibrium approach. In our testing stage, the trained extractor and detector are used for inference.\\n\\nTo ensure that banks can be updated synchronously, we also use the centroid \\\\( \\\\theta_i \\\\) to fill the empty nodes where the network misses detection.\\n\\nWhen averaging the centroid feature of the \\\\( i \\\\)-th substructure, only intra-substructure discrepancy is considered, not inter-substructure discrepancy. The representation of each substructure should be distinguished with clear margins, and the centroid \\\\( c_i \\\\) in different samples should be close to its corresponding clustering centers \\\\( \\\\theta_i \\\\). Thus, to complete the topology graph \\\\((V, E)\\\\) for a sample, the edge \\\\( E \\\\) is computed by the pairwise distance between the centroid \\\\( c_i \\\\) of current sample and clustering center \\\\( \\\\theta_i \\\\) in the \\\\( i \\\\)-th substructure, formulated as \\\\( E = \\\\{c_i \\\\cdot \\\\theta_i^T\\\\}_{N_i=1} \\\\). To obtain the topological representation via graph, we apply the graph neural network (GNN) (Kipf & Welling, 2016) to acquire a more cohesive representation \\\\( G \\\\) from \\\\((V, E)\\\\) through \\\\( G = \\\\text{GNN}(V, E) \\\\), \\\\( G \\\\in \\\\mathbb{R}^{N \\\\times d} \\\\). In the GNN module, the embedding size is set to \\\\( d \\\\) to stay in line with the input \\\\((V, E)\\\\). Then, to narrow the discrepancy across source and target domains, we optimize their transport distance between the graph \\\\( G_s \\\\) and \\\\( G_t \\\\) as:\\n\\n\\\\[\\nL_{\\\\text{dis}}(s, t) = \\\\inf_{\\\\gamma \\\\in \\\\Gamma(s, t)} E(G_s, G_t) \\\\sim \\\\gamma \\\\left[ G_s, G_t \\\\right] p_{1 \\\\alpha}, \\\\tag{2}\\n\\\\]\\n\\nwhere \\\\( \\\\gamma \\\\in \\\\Gamma(s, t) \\\\) is the set of all couplings of training samples from source and target domains, \\\\( \\\\gamma \\\\) and \\\\( \\\\Gamma \\\\) denotes a joint probability measure and all joint probability distribution of \\\\( \\\\gamma(G_s, G_t) \\\\), respectively. In subsequent content, we use subscript letters \\\\( s \\\\) and \\\\( t \\\\) to represent the source and target domains, respectively. Directly optimizing Equation 2 is challenging. Thus, we store features of heart substructures from data samples in memory banks and use centroid clustering to approximate the overall representation. This allows us to reformulate Equation 2 as a discrete form:\\n\\n\\\\[\\nL_{\\\\text{dis}}(s, t) = \\\\inf_{\\\\pi} \\\\sum_{i=1}^{N} ||G_{t,i} - G_{s,\\\\pi}(i)||_{1\\\\alpha}, \\\\tag{3}\\n\\\\]\\n\\nwhere the \\\\( i \\\\) in \\\\( G_{s/t,i} \\\\) denotes the graph nodes of \\\\( i \\\\)-th substructure in graph \\\\( G \\\\) from source/target domain, and the infimum (inf) is over all permutations \\\\( \\\\pi \\\\) of \\\\( N \\\\) heart substructures, computed by using the Sinkhorn (Cuturi, 2013) iteration. We use the \\\\( L_{\\\\text{TKT}} = L_{\\\\text{dis}}(G_t, G_{\\\\theta}) \\\\) to represent the overall loss of module TKT.\\n\\n3.2. Morphology Knowledge Transfer\\n\\nAs shown in Figure 1, topology refers to the spatial relationships that remain constant globally, considering the overall representation of a specific view of the heart. While morphology deals with the form and shape of each substructure itself. To maintain the consistent representation of substructures across domains, we propose a technique called MKT to align the morphology representation of substructures across different domains.\"}"}
{"id": "meItvvCO7X", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As discussed, same substructure across different fetal hearts should remain a consistent morphological representation, where weights $W \\\\in (\\\\text{MAGNN})$ to complete the morphological representation of heart substructures. To construct heart substructures representation, we sparsely introduce the auxiliary network named morph-aware GNN (MAGNN) and cross-domain graphical attention. Finally, the cross-domain graphical attention is introduced for cross-domain interaction via graph module shown in Figure 3, we equidistantly sample $M$ feature nodes of Organs $\\\\{O_i\\\\}_s$, $\\\\{O_i\\\\}_t$ and concatenate sample nodes from deep to shallow layers. Typically, all nodes in the source and target domains must be classified correctly based on the substructure label of input nodes is $\\\\{\\\\mathbf{f}_i\\\\}_s$, $\\\\{\\\\mathbf{f}_i\\\\}_t$. As the MTK layers, formulated as $\\\\text{LN} = \\\\text{Layer Normalization}$, where the sampled nodes can be represented by $K \\\\times P$, $q_i$ is computed by $\\\\text{LN}$, $\\\\{O_i\\\\}_t$ denotes the overall permutations of graph node in each graph node of $\\\\{g_i\\\\}_s$ and $\\\\{g_i\\\\}_t$. Cross-domain graphical attention is formulated as:\\n\\n$$\\\\text{Attention} = \\\\text{Softmax} \\\\left( \\\\frac{e_i}{\\\\gamma} \\\\right)$$\\n\\n$$e_i = \\\\langle \\\\mathbf{v}_i, \\\\mathbf{v}_j \\\\rangle = \\\\mathbf{W}_k \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle$$\\n\\nwhere $\\\\langle \\\\cdot, \\\\cdot \\\\rangle$ denotes the element-wise dot product, $\\\\mathbf{u}_i, \\\\mathbf{u}_j$ are the linear projection layers. Essentially, all nodes in the source and target domains must be classified correctly based on the substructure label of input nodes is $\\\\{\\\\mathbf{f}_i\\\\}_s$, $\\\\{\\\\mathbf{f}_i\\\\}_t$. As the MTK layers, formulated as $\\\\text{LN} = \\\\text{Layer Normalization}$, where the sampled nodes can be represented by $K \\\\times P$, $q_i$ is computed by $\\\\text{LN}$, $\\\\{O_i\\\\}_t$ denotes the overall permutations of graph node in each graph node of $\\\\{g_i\\\\}_s$ and $\\\\{g_i\\\\}_t$. Cross-domain graphical attention is formulated as:\\n\\n$$\\\\text{Attention} = \\\\text{Softmax} \\\\left( \\\\frac{e_i}{\\\\gamma} \\\\right)$$\\n\\n$$e_i = \\\\langle \\\\mathbf{v}_i, \\\\mathbf{v}_j \\\\rangle = \\\\mathbf{W}_k \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle$$\\n\\nwhere $\\\\langle \\\\cdot, \\\\cdot \\\\rangle$ denotes the element-wise dot product, $\\\\mathbf{u}_i, \\\\mathbf{u}_j$ are the linear projection layers. Essentially, all nodes in the source and target domains must be classified correctly based on the substructure label of input nodes is $\\\\{\\\\mathbf{f}_i\\\\}_s$, $\\\\{\\\\mathbf{f}_i\\\\}_t$. As the MTK layers, formulated as $\\\\text{LN} = \\\\text{Layer Normalization}$, where the sampled nodes can be represented by $K \\\\times P$, $q_i$ is computed by $\\\\text{LN}$, $\\\\{O_i\\\\}_t$ denotes the overall permutations of graph node in each graph node of $\\\\{g_i\\\\}_s$ and $\\\\{g_i\\\\}_t$. Cross-domain graphical attention is formulated as:\\n\\n$$\\\\text{Attention} = \\\\text{Softmax} \\\\left( \\\\frac{e_i}{\\\\gamma} \\\\right)$$\\n\\n$$e_i = \\\\langle \\\\mathbf{v}_i, \\\\mathbf{v}_j \\\\rangle = \\\\mathbf{W}_k \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle$$\\n\\nwhere $\\\\langle \\\\cdot, \\\\cdot \\\\rangle$ denotes the element-wise dot product, $\\\\mathbf{u}_i, \\\\mathbf{u}_j$ are the linear projection layers. Essentially, all nodes in the source and target domains must be classified correctly based on the substructure label of input nodes is $\\\\{\\\\mathbf{f}_i\\\\}_s$, $\\\\{\\\\mathbf{f}_i\\\\}_t$. As the MTK layers, formulated as $\\\\text{LN} = \\\\text{Layer Normalization}$, where the sampled nodes can be represented by $K \\\\times P$, $q_i$ is computed by $\\\\text{LN}$, $\\\\{O_i\\\\}_t$ denotes the overall permutations of graph node in each graph node of $\\\\{g_i\\\\}_s$ and $\\\\{g_i\\\\}_t$. Cross-domain graphical attention is formulated as:\\n\\n$$\\\\text{Attention} = \\\\text{Softmax} \\\\left( \\\\frac{e_i}{\\\\gamma} \\\\right)$$\\n\\n$$e_i = \\\\langle \\\\mathbf{v}_i, \\\\mathbf{v}_j \\\\rangle = \\\\mathbf{W}_k \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle$$\\n\\nwhere $\\\\langle \\\\cdot, \\\\cdot \\\\rangle$ denotes the element-wise dot product, $\\\\mathbf{u}_i, \\\\mathbf{u}_j$ are the linear projection layers. Essentially, all nodes in the source and target domains must be classified correctly based on the substructure label of input nodes is $\\\\{\\\\mathbf{f}_i\\\\}_s$, $\\\\{\\\\mathbf{f}_i\\\\}_t$. As the MTK layers, formulated as $\\\\text{LN} = \\\\text{Layer Normalization}$, where the sampled nodes can be represented by $K \\\\times P$, $q_i$ is computed by $\\\\text{LN}$, $\\\\{O_i\\\\}_t$ denotes the overall permutations of graph node in each graph node of $\\\\{g_i\\\\}_s$ and $\\\\{g_i\\\\}_t$. Cross-domain graphical attention is formulated as:\\n\\n$$\\\\text{Attention} = \\\\text{Softmax} \\\\left( \\\\frac{e_i}{\\\\gamma} \\\\right)$$\\n\\n$$e_i = \\\\langle \\\\mathbf{v}_i, \\\\mathbf{v}_j \\\\rangle = \\\\mathbf{W}_k \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle$$\\n\\nwhere $\\\\langle \\\\cdot, \\\\cdot \\\\rangle$ denotes the element-wise dot product, $\\\\mathbf{u}_i, \\\\mathbf{u}_j$ are the linear projection layers. Essentially, all nodes in the source and target domains must be classified correctly based on the substructure label of input nodes is $\\\\{\\\\mathbf{f}_i\\\\}_s$, $\\\\{\\\\mathbf{f}_i\\\\}_t$. As the MTK layers, formulated as $\\\\text{LN} = \\\\text{Layer Normalization}$, where the sampled nodes can be represented by $K \\\\times P$, $q_i$ is computed by $\\\\text{LN}$, $\\\\{O_i\\\\}_t$ denotes the overall permutations of graph node in each graph node of $\\\\{g_i\\\\}_s$ and $\\\\{g_i\\\\}_t$. Cross-domain graphical attention is formulated as:\\n\\n$$\\\\text{Attention} = \\\\text{Softmax} \\\\left( \\\\frac{e_i}{\\\\gamma} \\\\right)$$\\n\\n$$e_i = \\\\langle \\\\mathbf{v}_i, \\\\mathbf{v}_j \\\\rangle = \\\\mathbf{W}_k \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle$$\\n\\nwhere $\\\\langle \\\\cdot, \\\\cdot \\\\rangle$ denotes the element-wise dot product, $\\\\mathbf{u}_i, \\\\mathbf{u}_j$ are the linear projection layers. Essentially, all nodes in the source and target domains must be classified correctly based on the substructure label of input nodes is $\\\\{\\\\mathbf{f}_i\\\\}_s$, $\\\\{\\\\mathbf{f}_i\\\\}_t$. As the MTK layers, formulated as $\\\\text{LN} = \\\\text{Layer Normalization}$, where the sampled nodes can be represented by $K \\\\times P$, $q_i$ is computed by $\\\\text{LN}$, $\\\\{O_i\\\\}_t$ denotes the overall permutations of graph node in each graph node of $\\\\{g_i\\\\}_s$ and $\\\\{g_i\\\\}_t$. Cross-domain graphical attention is formulated as:\\n\\n$$\\\\text{Attention} = \\\\text{Softmax} \\\\left( \\\\frac{e_i}{\\\\gamma} \\\\right)$$\\n\\n$$e_i = \\\\langle \\\\mathbf{v}_i, \\\\mathbf{v}_j \\\\rangle = \\\\mathbf{W}_k \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle$$\\n\\nwhere $\\\\langle \\\\cdot, \\\\cdot \\\\rangle$ denotes the element-wise dot product, $\\\\mathbf{u}_i, \\\\mathbf{u}_j$ are the linear projection layers. Essentially, all nodes in the source and target domains must be classified correctly based on the substructure label of input nodes is $\\\\{\\\\mathbf{f}_i\\\\}_s$, $\\\\{\\\\mathbf{f}_i\\\\}_t$. As the MTK layers, formulated as $\\\\text{LN} = \\\\text{Layer Normalization}$, where the sampled nodes can be represented by $K \\\\times P$, $q_i$ is computed by $\\\\text{LN}$, $\\\\{O_i\\\\}_t$ denotes the overall permutations of graph node in each graph node of $\\\\{g_i\\\\}_s$ and $\\\\{g_i\\\\}_t$. Cross-domain graphical attention is formulated as:\\n\\n$$\\\\text{Attention} = \\\\text{Softmax} \\\\left( \\\\frac{e_i}{\\\\gamma} \\\\right)$$\\n\\n$$e_i = \\\\langle \\\\mathbf{v}_i, \\\\mathbf{v}_j \\\\rangle = \\\\mathbf{W}_k \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle$$\\n\\nwhere $\\\\langle \\\\cdot, \\\\cdot \\\\rangle$ denotes the element-wise dot product, $\\\\mathbf{u}_i, \\\\mathbf{u}_j$ are the linear projection layers. Essentially, all nodes in the source and target domains must be classified correctly based on the substructure label of input nodes is $\\\\{\\\\mathbf{f}_i\\\\}_s$, $\\\\{\\\\mathbf{f}_i\\\\}_t$. As the MTK layers, formulated as $\\\\text{LN} = \\\\text{Layer Normalization}$, where the sampled nodes can be represented by $K \\\\times P$, $q_i$ is computed by $\\\\text{LN}$, $\\\\{O_i\\\\}_t$ denotes the overall permutations of graph node in each graph node of $\\\\{g_i\\\\}_s$ and $\\\\{g_i\\\\}_t$. Cross-domain graphical attention is formulated as:\\n\\n$$\\\\text{Attention} = \\\\text{Softmax} \\\\left( \\\\frac{e_i}{\\\\gamma} \\\\right)$$\\n\\n$$e_i = \\\\langle \\\\mathbf{v}_i, \\\\mathbf{v}_j \\\\rangle = \\\\mathbf{W}_k \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle$$\\n\\nwhere $\\\\langle \\\\cdot, \\\\cdot \\\\rangle$ denotes the element-wise dot product, $\\\\mathbf{u}_i, \\\\mathbf{u}_j$ are the linear projection layers. Essentially, all nodes in the source and target domains must be classified correctly based on the substructure label of input nodes is $\\\\{\\\\mathbf{f}_i\\\\}_s$, $\\\\{\\\\mathbf{f}_i\\\\}_t$. As the MTK layers, formulated as $\\\\text{LN} = \\\\text{Layer Normalization}$, where the sampled nodes can be represented by $K \\\\times P$, $q_i$ is computed by $\\\\text{LN}$, $\\\\{O_i\\\\}_t$ denotes the overall permutations of graph node in each graph node of $\\\\{g_i\\\\}_s$ and $\\\\{g_i\\\\}_t$. Cross-domain graphical attention is formulated as:\\n\\n$$\\\\text{Attention} = \\\\text{Softmax} \\\\left( \\\\frac{e_i}{\\\\gamma} \\\\right)$$\\n\\n$$e_i = \\\\langle \\\\mathbf{v}_i, \\\\mathbf{v}_j \\\\rangle = \\\\mathbf{W}_k \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle$$\\n\\nwhere $\\\\langle \\\\cdot, \\\\cdot \\\\rangle$ denotes the element-wise dot product, $\\\\mathbf{u}_i, \\\\mathbf{u}_j$ are the linear projection layers. Essentially, all nodes in the source and target domains must be classified correctly based on the substructure label of input nodes is $\\\\{\\\\mathbf{f}_i\\\\}_s$, $\\\\{\\\\mathbf{f}_i\\\\}_t$. As the MTK layers, formulated as $\\\\text{LN} = \\\\text{Layer Normalization}$, where the sampled nodes can be represented by $K \\\\times P$, $q_i$ is computed by $\\\\text{LN}$, $\\\\{O_i\\\\}_t$ denotes the overall permutations of graph node in each graph node of $\\\\{g_i\\\\}_s$ and $\\\\{g_i\\\\}_t$. Cross-domain graphical attention is formulated as:\\n\\n$$\\\\text{Attention} = \\\\text{Softmax} \\\\left( \\\\frac{e_i}{\\\\gamma} \\\\right)$$\\n\\n$$e_i = \\\\langle \\\\mathbf{v}_i, \\\\mathbf{v}_j \\\\rangle = \\\\mathbf{W}_k \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i, \\\\mathbf{u}_j \\\\rangle + \\\\mathbf{W}_q \\\\langle \\\\mathbf{u}_i"}
