{"id": "5sgkNtexs2", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach\\n\\nZhihao Li\\nYufei Wang\\nAlex Kot\\nBihan Wen\\n\\nAbstract\\n\\nRaw images offer unique advantages in many low-level visual tasks due to their unprocessed nature. However, this unprocessed state accentuates noise, making raw images challenging to compress effectively. Current compression methods often overlook the ubiquitous noise in raw space, leading to increased bitrates and reduced quality. In this paper, we propose a novel raw image compression scheme that selectively compresses the noise-free component of the input, while discarding its real noise using a self-supervised approach. By excluding noise from the bitstream, both the coding efficiency and reconstruction quality are significantly enhanced. We curate a full-day dataset of raw images with calibrated noise parameters and reference images to evaluate the performance of models under a wide range of input signal-noise ratios. Experimental results demonstrate that our method surpasses existing compression techniques, achieving a more advantageous rate-distortion balance with improvements ranging from +2 to +10dB and yielding a bit saving of 2 to 50 times. The code is available at https://lizhihao6.github.io/Cleans.\\n\\n1. Introduction\\n\\nIn the camera imaging system, photons converge on the sensor chip to produce the raw image before an image signal processor (ISP) transforms it into an RGB image. Raw images are unprocessed and have a higher dynamic range, leading to various applications. For instance, their original noise distribution simplifies tasks such as image denoising (Abdelhamed et al., 2020; Feng et al., 2022; Wei et al., 2020) and low-light image enhancement (Ershov et al., 2022; Li et al., 2022b; Wang et al., 2022). Photographers and filmmakers favor raw images for extensive post-production flexibility.\\n\\n*Equal contribution\\n\\n1Department of EEE, Nanyang Technological University, Singapore. Correspondence to: Bihan Wen <bihan.wen@ntu.edu.sg>.\\n\\nProceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\\n\\nFigure 1. Comparison between the latest state-of-the-art (SOTA) raw image compression method, RIC (Li et al., 2022a), and our approach. Our method markedly surpasses RIC in both compression ratio and reconstruction quality, and achieves much higher PSNR comparing to the noisy input before compression. It is worth noting that our method is self-supervised, i.e., we do not need clean images during training. For enhanced detail visibility, all raw images are converted to RGB. (Zoom in for better view.)\\n\\nAdditionally, the broader dynamic range of raw images benefits both low-level High Dynamic Range (HDR) imaging (Hasinoff et al., 2016) and high-level computer vision tasks (Li et al., 2022a).\\n\\nNevertheless, the substantial size of raw images poses significant challenges for storage and transfer, limiting their widespread use. The commonly used standard, Digital Negative (DNG), relies on the outdated JPEG-98, which no longer meets modern compression needs (Li et al., 2022a).\\n\\nCurrent image compression algorithms like JPEG, BPG, and PNG are primarily designed for RGB, monochrome, or YCbCr color spaces and require substantial modifications for raw images. Consequently, recent works (Wang et al., 2023a; Li et al., 2022a) have shifted towards learning-based methods for raw image compression, eliminating the need for manually designed features and providing a more efficient solution. Metadata-based methods, such as those described in (Wang et al., 2023a), utilize corresponding JPEG-compressed RGB images as a prior to reconstruct lossy raw images from additional bitstreams. Using JPEG images for raw compression is suboptimal, as it requires additional storage for RGB data converted from the raw files. Conversely, RIC (Li et al., 2022a) employs a learned image...\"}"}
{"id": "5sgkNtexs2", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach\\n\\nLevin, N., Kyba, C. C., Zhang, Q., de Miguel, A. S., Rom\u00e1n, M. O., Li, X., Portnov, B. A., Molthan, A. L., Jechow, A., Miller, S. D., et al. Remote sensing of night lights: A review and an outlook for the future. Remote Sensing of Environment, 237:111443, 2020.\\n\\nLi, Z., Lu, M., Zhang, X., Feng, X., Asif, M. S., and Ma, Z. Efficient visual computing with camera raw snapshots. arXiv preprint arXiv:2212.07778, 2022a.\\n\\nLi, Z., Yi, S., and Ma, Z. Rendering nighttime image via cascaded color and brightness compensation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 897\u2013905, 2022b.\\n\\nLu, M., Chen, F., Pu, S., and Ma, Z. High-efficiency lossy image coding through adaptive neighborhood information aggregation. arXiv preprint arXiv:2204.11448, 2022.\\n\\nRezende, D. and Mohamed, S. Variational inference with normalizing flows. In International conference on machine learning, pp. 1530\u20131538. PMLR, 2015.\\n\\nShannon, C. E. A mathematical theory of communication. The Bell system technical journal, 27(3):379\u2013423, 1948.\\n\\nSullivan, G. J., Ohm, J.-R., Han, W.-J., and Wiegand, T. Overview of the high efficiency video coding (hevc) standard. IEEE Transactions on circuits and systems for video technology, 22(12):1649\u20131668, 2012.\\n\\nTong, K., Wu, Y., Li, Y., Zhang, K., Zhang, L., and Jin, X. Qvrf: A quantization-error-aware variable rate frame-work for learned image compression. arXiv preprint arXiv:2303.05744, 2023.\\n\\nWallace, G. K. The jpeg still picture compression standard. Communications of the ACM, 34(4):30\u201344, 1991.\\n\\nWang, Y., Wan, R., Yang, W., Li, H., Chau, L.-P., and Kot, A. Low-light image enhancement with normalizing flow. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pp. 2604\u20132612, 2022.\\n\\nWang, Y., Yu, Y., Yang, W., Guo, L., Chau, L.-P., Kot, A. C., and Wen, B. Beyond learned metadata-based raw image reconstruction. arXiv preprint arXiv:2306.12058, 2023a.\\n\\nWang, Y., Yu, Y., Yang, W., Guo, L., Chau, L.-P., Kot, A. C., and Wen, B. Exposurediffusion: Learning to expose for low-light image enhancement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12438\u201312448, 2023b.\\n\\nWang, Y., Yu, Y., Yang, W., Guo, L., Chau, L.-P., Kot, A. C., and Wen, B. Raw image reconstruction with learned compact metadata. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18206\u201318215, 2023c.\\n\\nWang, Z., Fu, Y., Liu, J., and Zhang, Y. Lg-bpn: Local and global blind-patch network for self-supervised real-world denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18156\u201318165, 2023d.\\n\\nWei, K., Fu, Y., Yang, J., and Huang, H. A physics-based noise formation model for extreme low-light raw denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2758\u20132767, 2020.\"}"}
{"id": "5sgkNtexs2", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this supplement, we first elaborate on the distribution and calibration of the camera noise model. Then, we provide a brief proof for Eq. (12).\\n\\nA. Distribution of the camera noise model\\n\\nFollowing ELD (Wei et al., 2020) and PMN (Feng et al., 2022), the noise components in Eq. (2) follow specific distributions:\\n\\n\\\\[ n_{\\\\text{shot}} \\\\sim P \\\\]  \\n\\\\[ n_{\\\\text{read}} \\\\sim N(0, \\\\sigma_{\\\\text{read}}^2) \\\\]  \\n\\\\[ n_{\\\\text{row}} \\\\sim N(0, \\\\sigma_{\\\\text{row}}^2) \\\\]  \\n\\\\[ n_{\\\\text{fp}} = \\\\text{ISO} \\\\cdot k_{\\\\text{fp}} + n_{\\\\text{fp}}, b \\\\]  \\n\\n(26)\\n\\nwhere \\\\( k \\\\) represents the overall system gain linked to the ISO setting. \\\\( P \\\\) and \\\\( N \\\\) denote the Poisson and Gaussian distributions.\\n\\nThe terms \\\\( n_{\\\\text{fp}}, n_{\\\\text{fp}}, b \\\\in \\\\mathbb{R}^{H \\\\times W} \\\\) are pixel-wise dark frame noise components. Following assumptions made in studies like ELD (Wei et al., 2020) and PMN (Feng et al., 2022), the relationships between \\\\( k, \\\\sigma_{\\\\text{read}}, \\\\sigma_{\\\\text{row}}, \\\\) and ISO are given as:\\n\\n\\\\[ k = a_k \\\\cdot \\\\text{ISO} + b_k \\\\]  \\n\\\\[ \\\\log(\\\\sigma_{\\\\text{read}}) = a_{\\\\text{read}} \\\\cdot \\\\log(k) + b_{\\\\text{read}} \\\\]  \\n\\\\[ \\\\log(\\\\sigma_{\\\\text{row}}) = a_{\\\\text{row}} \\\\cdot \\\\log(k) + b_{\\\\text{row}} \\\\]  \\n\\n(27)\\n\\nThe set of parameters \\\\( n_{\\\\text{fp}}, n_{\\\\text{fp}}, a_k, b_k, a_{\\\\text{read}}, b_{\\\\text{read}}, a_{\\\\text{row}}, b_{\\\\text{row}} \\\\), specific to each camera, can be calibrated using a series of flat-frame and dark-frame images captured at various ISO levels.\\n\\nB. Calibration of the camera noise model parameters\\n\\nThe calibration process involves three steps: initially, \\\\( k_i \\\\) is calibrated for each ISO \\\\( i \\\\) using flat frames; then, \\\\( n_{\\\\text{fp}}, \\\\sigma_{\\\\text{read}}, \\\\) and \\\\( \\\\sigma_{\\\\text{row}} \\\\) are calibrated with dark frames for each ISO. Finally, ISO-related parameters \\\\( a_k, b_k, n_{\\\\text{fp}}, n_{\\\\text{fp}}, b, a_{\\\\text{read}}, b_{\\\\text{read}}, a_{\\\\text{row}}, b_{\\\\text{row}} \\\\) are fitted using the calibrated noise parameters from the first two steps across various ISOs. Specifically,\\n\\n\u2022 First, to calibrate \\\\( k_i \\\\) at each ISO \\\\( i \\\\), we capture 25 flat frames under consistent lighting for each exposure time \\\\( \\\\text{Exp}_j \\\\), calculating mean and variance for each color block. This yields 24 mean-variance pairs per exposure time \\\\( \\\\text{Exp}_j \\\\). With three different exposure times per ISO \\\\( i \\\\), we gather 72 mean-variance pairs per ISO. The \\\\( n_{\\\\text{shot}} \\\\), modeled as \\\\( N(\\\\bar{x}, \\\\bar{x} \\\\cdot k) \\\\) where \\\\( \\\\bar{x} \\\\) is the mean and \\\\( \\\\bar{x} \\\\cdot k \\\\) the variance, allows us to calibrate \\\\( k \\\\) from the mean-variance relationship. Points with a mean value beyond \\\\( \\\\frac{1}{4} \\\\) saturation are excluded due to clipping effects.\\n\u2022 Next, after calibrating \\\\( k_i \\\\), we capture 100 dark frames at each ISO \\\\( i \\\\) in a dark room to calibrate \\\\( n_{\\\\text{fp}}, \\\\sigma_{\\\\text{read}}, \\\\) and \\\\( \\\\sigma_{\\\\text{row}} \\\\). The mean of these dark frames gives \\\\( n_{\\\\text{fp}} \\\\), representing fixed pattern noise. Subtracting \\\\( n_{\\\\text{fp}} \\\\) from all dark frames, we calculate variance across rows for \\\\( n_{\\\\text{row}} \\\\) and total frame variance for \\\\( n_{\\\\text{read}} \\\\).\\n\u2022 Finally, by repeating the steps above for different ISO levels and obtaining a set of parameters \\\\( \\\\{n_{\\\\text{fp}}, \\\\sigma_{\\\\text{read}}, \\\\sigma_{\\\\text{row}}\\\\} \\\\), we fit \\\\( a_k, b_k, n_{\\\\text{fp}}, n_{\\\\text{fp}}, b, a_{\\\\text{read}}, b_{\\\\text{read}}, a_{\\\\text{row}}, b_{\\\\text{row}} \\\\) based on these parameters and equations Eq. (26), (27).\\n\\nWe develop an Android application to semi-automatically collect the aforementioned flat and dark frames. The calibration app and corresponding calibration codes will be released upon acceptance.\\n\\nC. Proof of \\\\( I(X; \\\\tilde{X}) < I(\\\\tilde{X}; \\\\tilde{X}) \\\\)\\n\\nThe derivation of Eq. (12) stems from mutual information principles:\\n\\n\\\\[ I(X; \\\\tilde{X}) = H(\\\\tilde{X}) - H(\\\\tilde{X} | X) \\\\leq H(\\\\tilde{X}) = I(\\\\tilde{X}; \\\\tilde{X}) \\\\]\\n\\n(28)\\n\\nwhere \\\\( H(X + N | X) > 0 \\\\) due to the presence of signal-related noise, \\\\( n_{\\\\text{shot}} \\\\), which is unavoidable.\"}"}
{"id": "5sgkNtexs2", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach\\n\\nFigure 4.\\nComparison of images across different datasets showcasing variations in noise levels. Subfigures (a) and (b) depict images from the SID and ELD datasets, respectively, while subfigures (c) and (d) illustrate high and low SNR images from our FDRIC dataset.\\n\\nTo address this limitation, we develop a full-day raw image compression (FDRIC) dataset that covers a wide range of SNR, ensuring comprehensive training and evaluation. We collected our dataset using the Redmi Note12 Turbo smartphone, with an OV64B sensor of a resolution of 4624 \u00d7 3472.\\n\\nOur dataset includes 549 noisy images for training and 32 noise-clean image pairs for evaluation. To collect paired pairs for testing, we first captured short-exposure images with auto-adjusted ISO and exposure time, followed by 25 long-exposure shots at ISO-100. We ensured the same product of ISO and exposure time for both noisy and clean images to achieve consistent exposure levels. Our dataset contains indoor and outdoor scenes with illumination ranging from 0.1 to more than 10^5 lux. Details on calibrating the camera noise model are provided in the supplementary.\\n\\n6. Experiment\\nIn this section, we begin by detailing the experimental setting. Next, we compare our methods to existing ones. Lastly, we perform comprehensive ablation studies for a thorough analysis of our approach.\\n\\n6.1. Experimental setting\\nImplementation details.\\nFor fair comparison, we use the same compressor as in RIC (2022a). Unlike RIC's need for eight different models to handle various rate-distribution trade-offs, our approach utilizes the quantization-error-aware variable rate framework (2023), enabling a wide range of continuous variable rates with a single model. For noise extraction $F_n$, we adopt the same U-Net architecture used in ELD (2020). We follow the train-test set split for the SID (2018) SonyA7S2 subset as outlined in PMN (2022). For the ELD SonyA7S2 subset, we directly evaluate it using a pretrained model, adhering to the same settings specified in PMN. For our FDRIC dataset, we crop these images into 512 \u00d7 512 patches for both training and testing. Adam optimizer with an initial learning rate of 1e-4 and a batch size of 6 is used, spanning 200,000 iterations with a learning rate decay to 1e-5 after 150,000 iterations. These hyperparameters remain consistent across all datasets, and we apply grad norm clipping for training stability as in RIC. For the hyperparameters $\\\\lambda_D$, $\\\\lambda_P$, and $\\\\lambda_{cov}$ in Eq. (23) and Eq. (25), the range of $\\\\lambda_D$ is set between 0.0018 and 0.18. The minimum value of $\\\\lambda_P$ is 0.05, and the maximum at 5, increasing at the same rate as $\\\\lambda_D$. $\\\\lambda_{cov}$ is consistently maintained at 0.2. All the models are trained within CompressAI PyTorch framework (2020) using a single NVIDIA RTX 3090 GPU.\\n\\nCompared methods.\\nTo validate the effectiveness of our framework, we compared our method with both one-stage compressors and two-stage denoiser+compressor approaches. As for the one-stage compressor baseline, we trained RIC using a noise-noise paired loss function, as defined in Eq. (10). In the two-stage setup, our method was compared against the traditional non-learned BM3D denoiser (2007), as well as the latest state-of-the-art (SOTA) learned self-supervised image denoisers: Neighbor2Neighbor (2021) and LGBPN (2023d). For these, the denoised outputs were compressed using the same compressor as RIC, noted as BM3D+RIC, Ne2Ne+RIC, and LGBPN+RIC, respectively. All self-supervised denoisers and compressors are trained on the SID and FDRIC datasets using the official codes.\\n\\n6.2. Results\\nIn Fig. 5, we present the Rate-Distortion (RD) curves across various SNR levels for the SID, ELD, and FDRIC datasets. At first glance, it is evident that our methods (indicated by the red RD curves) significantly surpass both the original RIC baselines and two-stage methods across all datasets and SNR levels. Compared to the original RIC baseline, which overlooks raw image noise, our method achieves a tenfold reduction in bitrates and more than 8dB gain in PSNR on the SID dataset. Additionally, our approach outperforms two-stage methods that combine state-of-the-art denoisers with the RIC compressor. The denoiser only results for the two-stage models are also illustrated in Fig. 5 as dotted lines, i.e., the performances of denoisers. Our method exceeds both the traditional BM3D and the learned Neighbor2Neighbor denoisers, highlighting its potential as a superior raw image compression technique.\"}"}
{"id": "5sgkNtexs2", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach\\n\\nFigure 5. Rate-Distortion curves for various datasets across different cameras and SNR levels.\\n\\n(a) Noisy input sample from the SID dataset captured with the SonyA7S2.\\n(b) BM3D 39.97/14\\n(c) BM3D+RIC 40.08/0.135\\n(d) LGBPN 34.07/14\\n(e) LGBPN+RIC 33.92/0.034\\n(f) Noisy input 33.50/14\\n(g) Ne2Ne 41.61/14\\n(h) Ne2Ne+RIC 40.49/0.083\\n(i) RIC 41.03/1.606\\n(j) Ours 41.17/0.030\\n(k) Ground truth\\n\\nPSNR (dB)/bpp\\n\\nFigure 6. Visual comparison across various cameras. (Zoom in for better view.)\"}"}
{"id": "5sgkNtexs2", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach\\n\\nFigure 7. Rate-distortion curves for FDRIC dataset with the ratio of 100, using QARV as the baseline compressor.\\n\\nTable 2. Ablation study on the selection of $\\\\lambda_P$ and $\\\\lambda_{cov}$, with BD-rate calculations based on FDRIC datasets.\\n\\n| $\\\\lambda_P$ | $\\\\lambda_D$ | $\\\\lambda_{cov}$ | BD-Rate \u2193 (%) |\\n|------------|-------------|-----------------|---------------|\\n| 0.02       | 0.2         | 0.0             | 5.35          |\\n| 0.02       | 0.2         | 0.2             | 4.45          |\\n| 0.02       | 0.2         | 0.02            | 3.80          |\\n\\ndenoiser. Visual comparisons in Fig. 6 show that our method attains better quality with lower bit rates.\\n\\n6.3. Ablation study\\n\\nWe present several ablation studies to validate our method. These include testing different compressor architectures, examining hyperparameters $\\\\lambda_P$ and $\\\\lambda_{cov}$, comparing with the SOTA supervised methods, evaluating inference speed, and analyzing the impact of training across various SNR levels. The evaluation metrics are the RD curve and Bj\u00f8ntegaard-delta-rate (2001) (BD-Rate).\\n\\nDifferent compressor architectures. To verify the generalization capability of our proposed framework, we evaluated it using the recent SOTA Q-V AE based compressor, QARV (2023). As shown in Fig. 7, our method consistently outperforms both QARV and two-stage baselines by a significant margin. This suggests that our approach is not confined to a specific compressor architecture, such as V AE, and can be seamlessly integrated into various compression frameworks.\\n\\nImpact of hyperparameters. The hyperparameters $\\\\lambda_P$ and $\\\\lambda_{cov}$ in the loss function are critical for balancing the trade-off between compression rate and denoising performance. As shown in Table 2, a too small $\\\\lambda_P$ results in insufficient noise model constraints, leading to inaccurate noise extraction. Conversely, an excessively large $\\\\lambda_P$ generates overwhelming gradients from the noise regularization loss, adversely affecting compressor convergence. A similar issue arises with $\\\\lambda_{cov}$ when set either too low or too high.\\n\\nTable 3. Comparison with supervised methods using BD-rate calculated on the SID dataset with the ratio of 100.\\n\\n| Method       | BD-Rate \u2193 (%) |\\n|--------------|---------------|\\n| Ours SID+RIC | 8.35          |\\n| ELD+RIC      | 59.50         |\\n\\nTable 4. Comparison of encoding complexity and latency on the FDRIC dataset. * notes that FLOPs and parameters are not summarized. \\\"N.A.\\\" indicates that RD curve does not intersect with ours.\\n\\n| Method                | FLOPs (G) | Params (M) | Latency (s) | BD-Rate (%) |\\n|-----------------------|-----------|------------|-------------|-------------|\\n| Ours                  | 1001      | 27.42      | 0.567       | 0           |\\n| BM3D + RIC            | 1001*     | 27.42*     | 290.629     | 234.55      |\\n| Ne2Ne + RIC           | 2830      | 28.68      | 0.796       | 317.69      |\\n| LGBPN + RIC           | 172253    | 31.63      | 92.670      | N.A.        |\\n\\nComparison with supervised methods. We benchmarked our method against supervised approaches that were trained with noise-clean paired data from SID (2018) and synthetic noise based on the noise model proposed in ELD (2020). The SID dataset's pretrained model served as the initial stage denoiser, combined with RIC for compression, denoted as SID+RIC or ELD+RIC. As indicated in Table 3, our unsupervised method surpasses even those supervised methods trained on paired data from the SID dataset. This underscores our method's ability to effectively separate noise from the signal without requiring paired data.\\n\\nEncoding complexity and latency. We also compare the encoding complexity of our method with two staged methods. As shown in Table 4, our method achieves an impressive -234.55% BD-rate improvement compared to traditional BM3D methods and is 500 times faster than BM3D+RIC. Furthermore, we reduce FLOPs by 64% while achieving a remarkable 317.69% BD-rate improvement compared to Ne2Ne+RIC. Notably, our method eliminates the need for a denoiser to preprocess the input image, making it significantly more efficient than the two-stage approach.\\n\\n7. Conclusion\\n\\nIn this work, we introduced a novel self-supervised framework for joint denoising and compression of raw images, effectively addressing the challenges of noise characteristics in raw imaging. By selectively compressing the noise-free component of the raw input and discarding the unwanted noise using a self-supervised technique, we significantly improve the coding efficiency while maintaining image quality. To further evaluate the performance of the proposed method, we curate a full-day dataset of raw images with calibrated noise parameters and reference images. The results on both existing benchmarks and the proposed dataset demonstrate the effectiveness of our method.\"}"}
{"id": "5sgkNtexs2", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach\\n\\nImpact Statement\\nThis paper presents work whose goal is to advance the field of raw image compression. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.\\n\\nAcknowledgements\\nThis research is partially supported by the National Research Foundation, Singapore, and Cyber Security Agency of Singapore under its National Cybersecurity Research & Development Programme (Development of Secured Components & Systems in Emerging Technologies through Hardware & Software Evaluation <NRF-NCR25-DeSNTU-0001>).\\n\\nReferences\\n\\nAbdelhamed, A., Lin, S., and Brown, M. S. A high-quality denoising dataset for smartphone cameras. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1692\u20131700, 2018.\\n\\nAbdelhamed, A., Afifi, M., Timofte, R., and Brown, M. S. Ntire 2020 challenge on real image denoising: Dataset, methods and results. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 496\u2013497, 2020.\\n\\nBall\u00e9, J., Laparra, V ., and Simoncelli, E. P. End-to-end optimized image compression. arXiv preprint arXiv:1611.01704, 2016.\\n\\nBall\u00e9, J., Minnen, D., Singh, S., Hwang, S. J., and Johnston, N. Variational image compression with a scale hyperprior. arXiv preprint arXiv:1802.01436, 2018.\\n\\nB\u00e9gin, J., Racap\u00e9, F., Feltman, S., and Pushparaja, A. Compressai: a pytorch library and evaluation platform for end-to-end compression research. arXiv preprint arXiv:2011.03029, 2020.\\n\\nBjontegaard, G. Calculation of average psnr differences between rd-curves. ITU SG16 Doc. VCEG-M33, 2001.\\n\\nBross, B., Wang, Y .-K., Ye, Y ., Liu, S., Chen, J., Sullivan, G. J., and Ohm, J.-R. Overview of the versatile video coding (vvc) standard and its applications. IEEE Transactions on Circuits and Systems for Video Technology, 31(10):3736\u20133764, 2021.\\n\\nChen, C., Chen, Q., Xu, J., and Koltun, V . Learning to see in the dark. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3291\u20133300, 2018.\\n\\nChen, L., Chu, X., Zhang, X., and Sun, J. Simple baselines for image restoration. arXiv preprint arXiv:2204.04676, 2022.\\n\\nCheng, K. L., Xie, Y ., and Chen, Q. Optimizing image compression via joint learning with denoising. In European Conference on Computer Vision, pp. 56\u201373. Springer, 2022.\\n\\nDabov, K., Foi, A., Katkovnik, V ., and Egiazarian, K. Image denoising by sparse 3-d transform-domain collaborative filtering. IEEE Transactions on image processing, 16(8):2080\u20132095, 2007.\\n\\nDuan, Z., Lu, M., Ma, J., Huang, Y ., Ma, Z., and Zhu, F. Qarv: Quantization-aware resnet vae for lossy image compression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\\n\\nErshov, E., Savchik, A., Shepelev, D., Bani \u00b4c, N., Brown, M. S., Timofte, R., Ko\u02c7s\u02c7cevi\u00b4c, K., Freeman, M., Tesalin, V ., Bocharov, D., et al. Ntire 2022 challenge on night photography rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1287\u20131300, 2022.\\n\\nFeng, H., Wang, L., Wang, Y ., and Huang, H. Learnability enhancement for low-light raw denoising: Where paired real data meets noise modeling. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 1436\u20131444, 2022.\\n\\nHasinoff, S. W., Sharlet, D., Geiss, R., Adams, A., Barron, J. T., Kainz, F., Chen, J., and Levoy, M. Burst photog-raphy for high dynamic range and low-light imaging on mobile cameras. ACM Transactions on Graphics (ToG), 35(6):1\u201312, 2016.\\n\\nHuang, T., Li, S., Jia, X., Lu, H., and Liu, J. Neigh-bor2neighbor: Self-supervised denoising from single noisy images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14781\u201314790, 2021.\\n\\nJoens, M. S., Huynh, C., Kasuboski, J. M., Ferranti, D., Sigal, Y . J., Zeitvogel, F., Obst, M., Burkhardt, C. J., Curran, K. P., Chalasani, S. H., et al. Helium ion microscopy (him) for the imaging of biological samples at sub-nanometer resolution. Scientific reports, 3(1):3514, 2013.\\n\\nLee, S.-Y . and Ortega, A. A novel approach of image compression in digital cameras with a bayer color filter array. In Proceedings 2001 International Conference on Image Processing (Cat. No. 01CH37205), volume 3, pp. 482\u2013485. IEEE, 2001.\"}"}
{"id": "5sgkNtexs2", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach\\n\\nPhoton Flux $I_{\\\\text{shot}}$\\n\\nPhoton-voltage convertor $n_{\\\\text{fp}}, n_{\\\\text{read}}$\\n\\nAmplifier $n_{\\\\text{row}}$\\n\\nNoisy Image\\n\\n(a) Noise is inevitable in the imaging process\\n(b) Noisy input (c) Ground truth (d) Error map\\n\\nFigure 2. An illustration of the prevalent noise in raw images: (a) The noise model for raw images demonstrates that noise is unavoidable in raw captures, even under sufficient illumination. (b) An example of a noisy input image shot at ISO 100 in well-lit indoor conditions. (c) A clean image, composed by merging 25 shots. (d) The error map highlights that noise remains conspicuous even in daylight conditions. All raw images have been processed to enhance visualization. (Zoom in for better view.)\\n\\ncompressor proposed in (Lu et al., 2022), which encodes raw images directly into bitstreams, bypassing the need for ISP conversion.\\n\\nWhile substantial progress has been achieved, previous methods often neglect the prevalent noise characteristic of raw images, as illustrated in Fig. 2. This noise is notably more pronounced in the raw domain compared to RGB, due to the lack of noise reduction and smoothing processes in the ISP pipeline. This not only increases the bitstream size but also potentially impairs the efficacy of deep learning-based compression techniques, as depicted in Fig. 1. Cheng et al. (2022) have adapted the RGB image compression algorithm to be noise-sensitive, combining denoising and compression to address bit misallocation. However, their approach relies heavily on noise-free paired data for model optimization. For raw images, which exhibit significant variations in noise distribution and color space across different camera models, implementing this technique would necessitate the compilation of extensive, camera-specific datasets, which is largely impractical.\\n\\nTo overcome the outlined challenges, we propose a novel self-supervised framework for joint denoising and compression of raw images. Specifically, the underlying distribution of noise in the raw image is predicted, adhering to a physical-based prior. Simultaneously, the compression branch aims to reconstruct the clean image using the predicted noise model under a constrained bitstream. Given that real noise can hardly survive in low-dimensional subspace with a limited bitstream, the noise and clean signal can be well disentangled without the need for clean images. Compared to previous raw image compression methods, our approach significantly reduces bitstream sizes, by effectively decoupling noise from noisy images. Our contributions are summarized as follows:\\n\\n\u2022 We propose the first self-supervised approach for raw image compression that incorporates joint denoising using a physical-based noise model.\\n\u2022 We propose a large-scale, full-day dataset for raw image compression, featuring accurately calibrated camera noise model parameters and noise-clean image pairs for evaluation. This establishes a solid benchmark for evaluating raw image compression methods.\\n\u2022 Our method significantly outperforms the existing raw image compression methods and those two-stage methods (both self-supervised and supervised) across a wide range of signal-to-noise ratios (SNRs) and cameras without any additional inference overhead.\\n\\n2. Related work\\n\\n2.1. Raw image denoising\\n\\nDenoising w/ real paired data. Raw image denoising is crucial for enhancing image quality in both professional photography (Hasinoff et al., 2016) and scientific research (Levin et al., 2020; Joens et al., 2013). Traditional ISPs typically use methods like BM3D (Dabov et al., 2007) for noise reduction based on self-similarity. Recently, pioneering works like SID (Chen et al., 2018) and SIDD (Abdelhamed et al., 2018) have demonstrated that data-driven, deep-learning methods can effectively replace and even surpass traditional modules. Additionally, the ELD (Wei et al., 2020) dataset pushes raw image denoising capabilities into extreme low-light conditions. Beyond these datasets, recent developments in sophisticated restoration networks (Wang et al., 2023b; Chen et al., 2022; Abdelhamed et al., 2020) further advance the performance of raw image denoising.\\n\\nDenoising w/ camera noise model. While noisy-clean paired datasets have led to significant advancements, collecting large-scale paired datasets including diverse noise patterns is laborious. As a result, noise model-based methods have emerged (Wei et al., 2020; Feng et al., 2022), utilizing synthetic noise from clean raw images. The effectiveness of these methods largely depends on the realism of the synthetic noise model. For instance, ELD (Wei et al., 2020) introduces an amended row-based noise model and replaces the Gaussian distribution with the Tukey Lambda distribution to better represent the long-tail noise pattern in extreme low-light conditions. PMN (Feng et al., 2022) further investigates the influence of Fixed Pattern Noise in dark frames, enhancing noise adherence to the Poisson-Gaussian (P-G) distribution. While calibration-based methods reduce the need for extensive data collection, they still require camera-specific clean raw images and are limited in dynamic scenes due to the long exposure times needed for clean image capture. To address these limitations, Neighbor2Neighbor (Huang et al., 2021) only uses spatially ir...\"}"}
{"id": "5sgkNtexs2", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach\\n\\nIt is known that relevant noise characteristics are crucial for training denoising models on noisy images. However, these models are designed to incorporate prior knowledge of the precise noise model. In our framework, by integrating a noise model with a compressor, we effectively separate noise from noisy images, obviating the need for clean images.\\n\\n2.2. Raw image compression\\n\\nCameras typically employ Bayer color filter array (CFA) patterns to capture color information in scenes, with most CFAs consisting of RGGB four-channel arrangements. However, prevalent compression algorithms like JPEG (Wallace, 1991), HEVC (Sullivan et al., 2012), and VVC (Bross et al., 2021) are designed for three-channel RGB or single-channel grayscale images and thus don\u2019t directly support raw image compression. To leverage existing compression solutions, traditional raw image compression methods typically divide the raw image into one to three-channel sub-images for separate compression using current encoders (Lee & Ortega, 2001). Recently, deep-learning-based compression methods were proposed (Lu et al., 2022; Ball\u00e9 et al., 2018) and even surpassed the advanced traditional codec VVC in the RGB domain. This trend has led to studies exploring deep-learning decoders specifically for raw images. For instance, Wang et al. (2023a; 2023c) utilize a context model to encode additional metadata, reconstructing raw images from JPEG-compressed RGB images. Apart from metadata-based approaches, RIC (Li et al., 2022a) directly employs a VAE for raw image compression, significantly outperforming traditional raw compression algorithms. Nonetheless, both conventional and deep-learning-based approaches generally overlooked the prevalent noise in raw images, which can increase bitstreams and degrade reconstruction quality. We find that with a known noise model, noise can be efficiently removed by a deep-learning-based VAE encoder.\\n\\n3. Preliminaries\\n\\nIn this section, we elaborate the noise modeling we adopted to regularize the noise disentangled from raw images and the rate-distortion theory in learned raw image compression.\\n\\n3.1. Noise model\\n\\nIn the camera imaging process, the captured raw image, denoted as $\\\\tilde{x}$, is composed of the clean raw image $x$ and the additive real noise $n$, which is defined as below:\\n\\n$$\\\\tilde{x} = x + n.$$  (1)\\n\\nInspired by the existing noise model (Wei et al., 2020; Feng et al., 2022), we decompose $n$ into different types of noise components, which is represented as\\n\\n$$n = n_{\\\\text{shot}} + n_{\\\\text{read}} + n_{\\\\text{row}} + n_{\\\\text{fp}},$$  (2)\\n\\nwhere $n_{\\\\text{shot}}$, $n_{\\\\text{read}}$, $n_{\\\\text{row}}$, and $n_{\\\\text{fp}}$ stand for shot noise, read noise, row noise, and fixed pattern noise, respectively. A detailed description of each noise component can be found in the supplementary material.\\n\\nGiven that $n_{\\\\text{shot}}$ can be approximated from a Poisson distribution $P(x_k)$ to a Gaussian distribution $N(0, x \\\\cdot k)$, it can be combined with $n_{\\\\text{read}}$ to form a heteroscedastic Gaussian noise model as\\n\\n$$n_{\\\\text{hg}} \\\\sim N(0, \\\\sigma^2_{\\\\text{hg}}), \\\\quad \\\\sigma^2_{\\\\text{hg}} = \\\\sigma^2_{\\\\text{read}} + x \\\\cdot k,$$  (3)\\n\\nwhere $k$ is the system gain and $\\\\sigma_{\\\\text{read}}$ is the read noise standard deviation.\\n\\nConsequently, the overall noise model can be simplified as\\n\\n$$n = n_{\\\\text{hg}} + n_{\\\\text{row}} + n_{\\\\text{fp}}.$$  (4)\\n\\nBy utilizing (4) as the prior of noising modeling, one can potentially decouple clean image from noise corruption, thereby obtaining more accurate estimation of the clean image $x$.\\n\\n3.2. Lossy image compression\\n\\nLossy image compression for real data distribution, as grounded in rate-distortion theory (Shannon, 1948), aims to optimize the bit-rate $R(D)$ through the rate-distortion function:\\n\\n$$R(D) = \\\\min \\\\mathbb{I}(\\\\hat{X}; X) \\\\quad \\\\text{s.t.} \\\\quad \\\\mathbb{E}[\\\\Delta(\\\\hat{X}, X)] \\\\leq D,$$  (5)\\n\\nwhere $\\\\Delta$ denotes the distortion metric, $\\\\mathbb{I}$ is the mutual information, and $\\\\hat{x} \\\\sim q(\\\\hat{X}|X)$ is the reconstructed image from the compressor. Achieving this for a given $D$ involves optimizing a Lagrangian relaxation with corresponding $\\\\lambda_D$:\\n\\n$$\\\\min \\\\left[ \\\\mathbb{I}(\\\\hat{X}; X) + \\\\lambda_D \\\\cdot \\\\mathbb{E}[\\\\Delta(\\\\hat{X}, X)] \\\\right].$$  (6)\\n\\nGiven the challenges in modeling $p_{\\\\text{data}}$, Ball\u00e9 et al. (2016) introduced a Variational Autoencoder (VAE) approach. This framework employs an encoder $y = g_a(x; \\\\phi)$, mapping images to the latent space $q_Y|X$, and a decoder, reconstructing from quantized features $Q(y)$ as $\\\\hat{x} = g_s(Q(y); \\\\Omega)$. They approximate the upper bound of $\\\\mathbb{I}(\\\\hat{X}; X)$ using:\\n\\n$$\\\\mathbb{I}(\\\\hat{X}; X) \\\\leq \\\\mathbb{I}(Y; X) \\\\leq \\\\mathbb{E}[\\\\text{KL}(q_Y|X || p_Y)].$$  (7)\\n\\nTherefore, the total loss for the learned lossy image compression model is formulated as:\\n\\n$$L(\\\\lambda_D) = \\\\mathbb{E}[\\\\text{KL}(q_Y|X || p_Y)] + \\\\lambda_D \\\\cdot \\\\mathbb{E}[\\\\Delta(\\\\hat{X}, X)].$$  (8)\\n\\nTo further optimize the rate-distortion trade-off, considering the spatial redundancy in $Y$, we adopt the following overall framework for both our method and baselines where a\"}"}
{"id": "5sgkNtexs2", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach\\n\\nHyperprior $p_V$ is employed in (Ball\u00e9 et al., 2018):\\n\\n$$L(\\\\lambda_D) = \\\\mathbb{E}\\\\left[\\\\text{KL}(q_Y|X,V)\\\\|q_Y|V)\\\\right] + \\\\mathbb{E}\\\\left[\\\\text{KL}(q_V|X\\\\|p_V)\\\\right] + \\\\lambda_D \\\\cdot \\\\mathbb{E}[\\\\Delta(\\\\hat{X}, X)]$$\\n\\n$$= R(Y) + R(V) \\\\{\\\\text{rate}\\\\} + \\\\lambda_D \\\\cdot \\\\mathbb{E}[\\\\Delta(\\\\hat{X}, X)] \\\\{\\\\text{distortion}\\\\}. \\\\quad (9)$$\\n\\n4. Methodology\\n\\n4.1. Motivation\\n\\nMost of the existing raw image compression methods (Wang et al., 2023a; Li et al., 2022a) neglect the noise elements, by directly optimizing Eq.(5) with noisy raw images $\\\\tilde{X}$. They inherently limit their optimization to:\\n\\n$$R(D) = \\\\min_{I} I(\\\\hat{X}; \\\\tilde{X}) \\\\text{ s.t. } \\\\mathbb{E}[\\\\Delta(\\\\hat{X}, \\\\tilde{X})] \\\\leq D. \\\\quad (10)$$\\n\\nCheng et al.(2022) proposed a simultaneous denoising and compression strategy, optimizing:\\n\\n$$U(D) = \\\\min_{I} I(\\\\hat{X}; \\\\tilde{X}) \\\\text{ s.t. } \\\\mathbb{E}[\\\\Delta(\\\\hat{X}, X)] \\\\leq D. \\\\quad (11)$$\\n\\nThis leads to the relationship:\\n\\n$$U(0) = I(X; \\\\tilde{X}) < I(\\\\tilde{X}; \\\\tilde{X}) = R(0), \\\\quad (12)$$\\n\\nindicating that joint denoising can enhance compression efficiency (the proof of Eq. (12) could be found in the supplementary). However, this method still relies on clean images $X$ for training, which are challenging and expensive to obtain. To address this, as shown in Fig. 3, we propose a novel self-supervised framework that can implicitly optimize $I(X; \\\\tilde{X})$ by regularizing the disentangled noise using a physical-based noise model. Specifically, the optimization objective is defined as follows:\\n\\n$$U(D, P) = \\\\min_{I} I(\\\\hat{X}; \\\\tilde{X}) \\\\text{ s.t. } \\\\mathbb{E}[\\\\Delta(\\\\hat{X}, \\\\tilde{X} - \\\\hat{N})] \\\\leq D, \\\\mathbb{E}[d(\\\\hat{N}, N)] \\\\leq P, \\\\quad (13)$$\\n\\nwhere $d$ measures the divergence between predicted noise $\\\\hat{N}$ and actual noise pattern $N$ modeled in Sec. 3.1 and the implementation of $d$ will be introduced in Sec. 4.2.\\n\\n4.2. Regularization on the extracted noise\\n\\nTo optimize Eq. (13), we extend the Lagrangian relaxation approach in Eq. (6) as follows:\\n\\n$$\\\\min [I(\\\\hat{X}; \\\\tilde{X}) + \\\\lambda_D \\\\cdot \\\\mathbb{E}[\\\\Delta(\\\\hat{X}, \\\\tilde{X} - \\\\hat{N})] + \\\\lambda_P \\\\cdot \\\\mathbb{E}[d(\\\\hat{N}, N)]] = \\\\mathbb{E}[\\\\text{NLL}(p_Z)(\\\\hat{z})]. \\\\quad (14)$$\\n\\nEstimating the divergence $d(\\\\hat{N}, N)$ between $q_{\\\\hat{N}}$ extracted from the noise extractor $F_n(\\\\cdot; \\\\Omega)$ and the physical-based noise modeling $p_N$ in the original noise space is challenging due to the complex distribution of various noise components. To address this, inspired by normalizing flow (Rezende & Mohamed, 2015), we apply a series of bijective transformations to standardize the noise into a standard Gaussian distribution $z = f_{hg} \\\\circ f_{row} \\\\circ f_{fp}(n)$, where $f_{hg}$, $f_{row}$, and $f_{fp}$ are transformations for heteroscedastic Gaussian noise, row noise, and fixed pattern noise, respectively, $n \\\\sim p_N$, and $\\\\circ$ denotes function composition. We regularize the distribution of the predicted noises by first transforming into the space of $Z$ as:\\n\\n$$\\\\hat{z} = f_{hg} \\\\circ f_{row} \\\\circ f_{fp} \\\\circ F_n(\\\\tilde{x}; \\\\Omega), \\\\quad (16)$$\\n\\nwhere the details of each transformation is as follows:\\n\\n- **Fixed pattern noise reduction** $f_{fp}$.\\n  Fixed pattern noise is challenging to extract via neural networks since it varies between different ISO levels. Therefore, we subtract it before applying the noise extractor $F_n$:\\n\\n  $$\\\\hat{n}_{hg} + \\\\hat{n}_{row} = f_{fp} \\\\circ F_n(\\\\tilde{x}; \\\\Omega) = F_n(\\\\tilde{x} - n_{fp}; \\\\Omega). \\\\quad (17)$$\\n\\n- **Row noise reduction** $f_{row}$.\\n  Row noise, a zero-mean Gaussian noise with row-specific variances, is mitigated by subtracting each row's mean, as follows:\\n\\n  $$\\\\hat{z}_r = \\\\text{mean}_{row}(\\\\hat{n}_{hg} + \\\\hat{n}_{row}) / \\\\sigma_{row}, \\\\quad \\\\hat{n}_{hg} = f_{row}(\\\\hat{n}_{hg} + \\\\hat{n}_{row}) = \\\\hat{n}_{hg} + \\\\hat{n}_{row} - \\\\text{mean}_{row}(\\\\hat{n}_{hg} + \\\\hat{n}_{row}), \\\\quad (18)$$\\n\\n  where $\\\\sigma_{row}$ is each row's standard deviation, modeled by row noise model.\\n\\n- **Heteroscedastic Gaussian noise reduction** $f_{hg}$.\\n  For the reconstructed image $\\\\hat{x}$, we estimate the heteroscedastic Gaussian noise variance from Eq. (3) as $\\\\hat{\\\\sigma}_{hg}^2 = \\\\sigma_{read}^2 + \\\\hat{x} \\\\cdot k$. This allows us to standardize the heteroscedastic Gaussian noise:\\n\\n  $$\\\\hat{z} = f_{hg}(\\\\hat{n}_{hg}) = \\\\hat{n}_{hg} / \\\\hat{\\\\sigma}_{hg}. \\\\quad (19)$$\\n\\nNoise regularization loss.\\n\\nAfter transforming the predicted noise $\\\\hat{z} = F_n(\\\\tilde{x}, \\\\Omega)$ to a latent space that the distribution is known, we can regularize the predicted noise distribution by minimizing the negative log-likelihood (NLL) loss as\\n\\n$$L(\\\\hat{Z}) = \\\\mathbb{E}_{q(\\\\hat{Z})|\\\\tilde{X}}[\\\\text{NLL}(p_Z)(\\\\hat{z})]. \\\\quad (20)$$\\n\\nWhile this formulation accounts for the overall distribution of $\\\\hat{z}$, it does not consider the independent and identically distributed (i.i.d.) noise across pixels. To address this, we...\"}"}
{"id": "5sgkNtexs2", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach\\n\\n\\\\[ n \\\\]\\n\\n\\\\[ f_p \\\\]\\n\\n\\\\[ \\\\tilde{x} \\\\]\\n\\n\\\\[ x \\\\]\\n\\n\\\\[ y \\\\]\\n\\n\\\\[ v \\\\]\\n\\n\\\\[ g(a(\\\\cdot; \\\\phi)) \\\\]\\n\\n\\\\[ g(s(\\\\cdot; \\\\theta)) \\\\]\\n\\n\\\\[ \\\\mathcal{R}(v) \\\\]\\n\\n\\\\[ \\\\mathcal{R}(y) \\\\]\\n\\n\\\\[ \\\\mathcal{L} \\\\]\\n\\n\\\\[ \\\\Delta(\\\\hat{x}, x) \\\\]\\n\\n\\\\[ \\\\lambda \\\\]\\n\\n\\\\[ I \\\\]\\n\\n\\\\[ \\\\text{mSE} \\\\]\\n\\n\\\\[ \\\\text{rate} \\\\]\\n\\n\\\\[ \\\\text{distortion} \\\\]\\n\\n\\\\[ \\\\text{noise\\\\ divergence} \\\\]\\n\\nFigure 3. The proposed framework for self-supervised raw image denoising and compression without reliance on paired clean images.\\n\\nDistinct from typical learning-based compressors, our approach first subtracts fixed pattern noise \\\\( n_{fp} \\\\) from the noise input \\\\( \\\\tilde{x} \\\\) in the compressor encoder. Then, it compresses the predicted clean signal \\\\( \\\\hat{x} \\\\), constrained by \\\\( \\\\tilde{x} - F_n(\\\\tilde{x}; \\\\Omega) \\\\), using a compressor with an integrated hyperprior module. To regularize the predicted noise \\\\( \\\\hat{n} = F_n(\\\\tilde{x}; \\\\Omega) \\\\), we use a bijective mapping based on the physical-based noise model to map the complicated noise distribution to a latent space where the distribution is known. Besides, a covariance loss is used to enhance the spatial independence of the disentangled noise \\\\( \\\\hat{n} \\\\).\\n\\n\\\\[ \\\\text{L}_{\\\\text{cov}} = \\\\mathbb{E}_{q(\\\\hat{Z}|\\\\tilde{X})} \\\\left[ I - \\\\hat{z}^T \\\\hat{z} \\\\right] \\\\]  \\n\\n(21)\\n\\nBesides, the subtracted each row's mean needs to obey the row-specific variances \\\\( \\\\sigma_{\\\\text{row}} \\\\) introudced in Eq. (3), which is regularized by the NLL loss below:\\n\\n\\\\[ \\\\text{L}_{\\\\hat{Z}_{\\\\text{r}}} = \\\\mathbb{E}_{q(\\\\hat{Z}|\\\\tilde{X})} \\\\left[ \\\\text{NLL}_p(\\\\hat{z}_{r}) \\\\right] \\\\]  \\n\\n(22)\\n\\nSubsequently, the noise regularization loss \\\\( d(\\\\hat{N}, N) \\\\) is formulated as follows:\\n\\n\\\\[ d(\\\\hat{N}, N) = \\\\text{L}_{\\\\hat{Z}} + \\\\text{L}_{\\\\hat{Z}_{\\\\text{r}}} + \\\\lambda_{\\\\text{cov}} \\\\cdot \\\\text{L}_{\\\\text{cov}} \\\\]  \\n\\n(23)\\n\\n4.3. Overall training loss\\n\\nAs for the rate constraint \\\\( I(\\\\hat{X}; \\\\tilde{X}) \\\\) in Eq. (14), we adopt the common approximaiton of \\\\( R(y) + R(v) \\\\) illustrated in Sec. 3.2. Specifically, the rates \\\\( R(y) \\\\) and \\\\( R(v) \\\\) are defined as follows:\\n\\n\\\\[ R(y) = \\\\mathbb{E}[\\\\log_2 q(\\\\hat{y}|\\\\hat{v})] \\\\]  \\n\\n(24)\\n\\nwhere \\\\( \\\\hat{y} \\\\) and \\\\( \\\\hat{v} \\\\) represent the quantized \\\\( y \\\\) and \\\\( v \\\\), respectively.\\n\\nTo be consistent with established learned image compression methods (Ball\u00e9 et al., 2018; Li et al., 2022a), the MSE loss \\\\( \\\\text{L}_{\\\\text{mSE}} \\\\) is selected as the distortion metric \\\\( \\\\Delta(\\\\hat{x}, x - \\\\hat{n}) \\\\). Fianlly, the overall loss that simultaneous minimize the rate of the latent codes \\\\( \\\\hat{y}, \\\\hat{v} \\\\), the reconstruction loss of the input image, and the noise regularization loss is summarized as follows:\\n\\n\\\\[ \\\\text{L} = \\\\mathbb{E}[R(y) + R(v)|\\\\{\\\\text{rate}\\\\}] + \\\\lambda_{\\\\text{D}} \\\\cdot \\\\text{L}_{\\\\text{mSE}}(\\\\hat{x}, \\\\tilde{x} - \\\\hat{n}) |\\\\{\\\\text{distortion}\\\\}] + \\\\lambda_{\\\\text{P}} \\\\cdot d(\\\\hat{N}, N) |\\\\{\\\\text{noise\\\\ divergence}\\\\}] \\\\]  \\n\\n(25)\\n\\n5. Full-day raw image compression dataset\\n\\nExisting raw image denoising datasets mainly focus on low-light or nighttime scenes, e.g. SID (Chen et al., 2018) captured under around 5 lux conditions and ELD (Wei et al., 2020) is even below 0.3 lux as shown in Fig. 4a. However, the demand for image compression is not only at night but throughout the whole day. Besides, as highlighted in Sec. 1 and Fig. 2, noise is also prevalent in daylight raw images, which possess considerably higher SNR. Due to the significant gap among input SNR, compressors trained solely on...\"}"}
