{"id": "orlanski23a", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 11. BC-Transcoder pass values for the different models and training distributions where the source language is C++. Used $T = 0.8$ and sampled 50 programs per problem. Nat is the natural distribution. UM is Unimax distribution. PaLM-C is the PaLM-Coder distribution. HS is Haskell, JS is JavaScript, Py is Python, and TS is TypeScript.\\n\\n| Size Dist. | C# | Dart | Go | HS | Java | JS | Julia | Lua | PHP | Py | R | Rust | TS |\\n|-----------|----|------|----|----|------|----|-------|-----|-----|----|---|------|----|\\n| 1B        | 24.2 | 22.4 | 10.4 | 1.5 | 21.2 | 22.0 | 1.9   | 4.7 | 14.9 | 15.4 | 2.3 | 6.4 | 27.9 |\\n| UM 1      | 8.1  | 21.4 | 8.1  | 2.6 | 14.4 | 12.3 | 0.6   | 2.5 | 12.0 | 12.4 | 2.4 | 7.6 | 16.2 |\\n| UM 2      | 16.3 | 18.2 | 5.7  | 3.5 | 13.9 | 16.0 | 0.3   | 0.5 | 12.8 | 10.4 | 1.2 | 6.7 | 14.6 |\\n| UM 3      | 21.9 | 18.5 | 8.3  | 3.9 | 17.5 | 17.7 | 0.2   | 0.2 | 9.1  | 13.2 | 1.6 | 7.6 | 16.4 |\\n| UM 4      | 17.0 | 23.7 | 10.3 | 7.0 | 18.0 | 17.6 | 3.4   | 4.7 | 18.3 | 11.9 | 3.5 | 8.8 | 17.9 |\\n| 2B        | 38.6 | 29.7 | 19.6 | 6.6 | 45.2 | 49.9 | 5.5   | 12.5 | 48.7 | 40.5 | 7.5 | 14.9 | 45.1 |\\n| UM 1      | 30.9 | 26.5 | 19.3 | 7.6 | 38.8 | 33.0 | 3.2   | 11.4 | 35.1 | 31.8 | 3.8 | 16.9 | 28.0 |\\n| UM 2      | 40.3 | 27.3 | 18.0 | 9.8 | 46.1 | 50.4 | 5.3   | 10.9 | 52.8 | 34.4 | 4.5 | 16.1 | 48.4 |\\n| UM 3      | 34.1 | 28.4 | 18.9 | 9.9 | 33.9 | 44.5 | 6.4   | 12.2 | 35.9 | 34.1 | 5.3 | 14.7 | 40.5 |\\n| UM 4      | 41.3 | 29.9 | 25.5 | 12.2 | 41.1 | 49.2 | 14.4  | 12.2 | 41.3 | 30.1 | 7.2 | 19.5 | 44.5 |\\n| 4B        | 71.3 | 33.2 | 60.7 | 10.7 | 81.9 | 77.3 | 20.8  | 36.3 | 80.5 | 79.9 | 13.3 | 38.4 | 76.0 |\\n| UM 1      | 69.6 | 38.1 | 63.9 | 12.7 | 77.9 | 77.8 | 16.1  | 38.6 | 76.4 | 74.7 | 12.0 | 52.5 | 76.5 |\\n| UM 2      | 66.3 | 37.0 | 60.8 | 15.3 | 73.8 | 77.6 | 27.3  | 33.4 | 71.0 | 73.5 | 18.7 | 50.7 | 75.2 |\\n| UM 3      | 75.2 | 34.8 | 54.4 | 14.3 | 78.3 | 79.0 | 12.1  | 34.8 | 77.6 | 76.7 | 20.9 | 56.4 | 79.4 |\\n| UM 4      | 70.7 | 33.0 | 59.7 | 15.0 | 73.7 | 74.1 | 14.3  | 33.7 | 61.1 | 72.8 | 16.1 | 47.0 | 74.7 |\\n| 8B PaLM   | 50.5 | 31.7 | 32.1 | 4.5  | 48.0 | 60.5 | 8.5   | 24.4 | 62.2 | 42.3 | 5.1 | 15.8 | 58.2 |\\n| PaLM-C    | 54.8 | 34.7 | 37.9 | 5.3  | 60.5 | 68.9 | 7.8   | 39.6 | 68.6 | 64.3 | 4.9 | 20.3 | 65.3 |\\n| 62B PaLM  | 72.0 | 35.8 | 55.2 | 8.4  | 74.7 | 77.4 | 23.0  | 51.3 | 82.5 | 73.0 | 16.8 | 25.9 | 75.4 |\\n| PaLM-C    | 76.2 | 41.5 | 58.9 | 8.4  | 79.5 | 80.9 | 31.6  | 56.5 | 84.3 | 83.1 | 23.5 | 27.5 | 78.4 |\"}"}
{"id": "orlanski23a", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Measuring the Impact of Programming Language Distribution\\n\\nTable 12. % changes in pass $@^k$ compared to the models trained on the natural distribution for High Resource languages. For BC-HumanEval(HE), $k = 100$. For BC-TP3(TP3), BC-Transcoder Python(TC-Py), and BC-Transcoder C++(TC-C++), $k = 25$. The cells represent the worst value for that language for that size and dataset. The cells represent the best value for that language for that size and dataset.\\n\\n| DS Size Dist. | Java | Python | C++ | PHP | TS | JS | Go |\\n|---------------|------|--------|-----|-----|----|----|----|\\n| 1B            |      |        |     |     |    |    |    |\\n| UM 1          | -29.0 | -15.0  | -30.3 | -12.4 | -20.3 | -4.1 | -19.6 |\\n| UM 2          | -18.7 |         |       |       |     |     |     |\\n| UM 3          | -12.1 | -9.7   | -41.7 | -14.6 | -16.7 | 3.8  | -13.5 |\\n| UM 4          | -18.9 | -11.9  | -28.2 | -10.8 | -12.1 | -5.1 | -14.9 |\\n| 2B            |      |        |     |     |    |    |    |\\n| UM 1          | -15.2 | -20.3  | -15.9 | -17.1 | -7.9  | -8.3  | -5.6  | -12.9 |\\n| UM 2          | -15.2 | -13.7  | -18.6 | -22.2 | -12.7 | -12.7 | -18.6 | -16.2 |\\n| UM 3          | -16.9 | -18.7  | -12.7 | -16.3 | -8.0  | -7.3  | -6.7  | -12.4 |\\n| UM 4          | -17.2 | -10.6  | -19.5 | -24.8 | -8.6  | -10.4 | -10.4 | -14.5 |\\n| 4B            |      |        |     |     |    |    |    |\\n| UM 1          | -5.3  | -12.6  | -8.9 | -9.4  | -0.1  | -9.5  | 1.1  | -6.4  |\\n| UM 2          | -23.4 | N/A    | -80.5 | -84.8 | -50.8 | -32.5 | -38.6 | -59.9 |\\n| UM 3          | -46.3 | N/A    | -92.0 | -89.8 | -43.1 | -28.8 | -62.7 | -63.6 |\\n| UM 4          | -53.9 | N/A    | -72.3 | -56.9 | -14.1 | -17.1 | -64.3 | -46.5 |\\n\\nTP3\\n\\n| DS Size Dist. | Java | Python | C++ | PHP | TS | JS | Go |\\n|---------------|------|--------|-----|-----|----|----|----|\\n| 1B            |      |        |     |     |    |    |    |\\n| UM 1          | N/A  | -80.5  | -84.8 | -50.8 | -32.5 | -38.6 | -59.9 |\\n| UM 2          | N/A  | -92.0  | -89.8 | -43.1 | -28.8 | -62.7 | -63.6 |\\n| UM 3          | N/A  | -87.3  | -96.2 | -53.5 | -16.1 | -69.8 | -61.5 |\\n| UM 4          | N/A  | -72.3  | -56.9 | -14.1 | -17.1 | -64.3 | -46.5 |\\n\\n2B\\n\\n| DS Size Dist. | Java | Python | C++ | PHP | TS | JS | Go |\\n|---------------|------|--------|-----|-----|----|----|----|\\n| UM 1          | -7.6 | -35.8  | -45.7 | -55.0 | -38.0 | -28.9 | -35.1 |\\n| UM 2          | 5.3  | N/A    | 8.5  | 7.3  | 1.0  | -8.0 | -0.7  |\\n| UM 3          | -24.7 | N/A    | -26.2 | -10.1 | -10.9 | -3.3  | -15.2 |\\n| UM 4          | 0.0  | N/A    | -15.2 | -1.4  | -1.3  | 30.3  | -3.7  |\\n\\n4B\\n\\n| DS Size Dist. | Java | Python | C++ | PHP | TS | JS | Go |\\n|---------------|------|--------|-----|-----|----|----|----|\\n| UM 1          | -12.1 | -8.1  | -2.9 | 12.6 | 3.8  | 23.6  | 2.8  |\\n| UM 2          | -19.6 | N/A    | -19.1 | -8.7 | 2.8  | -1.5  | 25.6  | -3.4 |\\n| UM 3          | -8.4  | N/A    | -6.4 | -9.0  | 13.1 | 1.8  | 10.5  | 0.3  |\\n| UM 4          | -19.0 | N/A    | -12.1 | -37.9 | -5.0  | -6.3  | 13.4  | -11.1 |\"}"}
{"id": "orlanski23a", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 13. % change of pass $k$ compared to the models trained on the natural distribution for low resource languages. For BC-HumanEval(HE), $k = 100$. For BC-TP3(TP3), BC-Transcoder Python(TC-Py), and BC-Transcoder C++(TC-C++), $k = 25$. The cells represent the worst value for that language for that size and dataset. The cells represent the best value for that language for that size and dataset.\\n\\n| DS Size Dist. | Dart | Lua | Rust | C# | Julia | HS |\\n|---------------|------|-----|------|----|-------|----|\\n| 1B UM 1       | -2.8 | 33.8| 4.6  | 68.5| 100.0 | 191.9|\\n| 1B UM 2       | -4.1 | 38.3| 19.0 | 98.1| 161.7 | 254.7|\\n| 1B UM 3       | -6.4 | 30.8| 15.5 | 87.1| 162.7 | 218.4|\\n| 1B UM 4       | -3.9 | 46.5| 14.8 | 115.2| 166.9 | 272.6|\\n| 2B UM 1       | 15.6 | -1.6| 12.7 | 59.4| 28.6  | 145.2|\\n| 2B UM 2       | 21.6 | -5.9| 3.4  | 71.2| 44.9  | 172.9|\\n| 2B UM 3       | 12.2 | 8.9 | 8.2  | 78.6| 68.9  | 173.5|\\n| 2B UM 4       | 25.6 | -0.4| 5.6  | 70.8| 48.6  | 198.7|\\n| 4B UM 1       | 7.0  | 8.6 | 3.0  |-11.5| 20.4  | 25.3 |\\n| 4B UM 2       | 2.6  | 0.2 | -0.6 |-7.5 | 44.0  | 32.9 |\\n| 4B UM 3       | 9.5  | 11.5| 14.7 |-6.5 | 66.9  | 48.2 |\\n| 4B UM 4       | -4.7 | 9.0 | 3.2  |-0.1 | 40.3  | 44.8 |\\n\\n| DS Size Dist. | TC-C++ | |\\n|---------------|--------||\\n| 1B UM 1       | -4.5  | 18.3 |\\n| 1B UM 2       | -18.6 | 3.8 |\\n| 1B UM 3       | -17.1 | 18.8 |\\n| 1B UM 4       | 6.2   | 36.9 |\\n| 2B UM 1       | -10.8 | 13.8 |\\n| 2B UM 2       | -8.3  | 8.6 |\\n| 2B UM 3       | -4.5  | -1.2 |\\n| 2B UM 4       | 0.5   | 31.0 |\\n| 4B UM 1       | 14.8  | 36.6 |\\n| 4B UM 2       | 11.4  | 31.9 |\\n| 4B UM 3       | 4.8   | 46.8 |\\n| 4B UM 4       | -0.8  | 22.4 |\\n\\n| DS Size Dist. | TC-Py | |\\n|---------------|-------||\\n| 1B UM 1       | -79.8 | -34.9 |\\n| 1B UM 2       | -60.0 | -33.3 |\\n| 1B UM 3       | -85.1 | -38.3 |\\n| 1B UM 4       | -52.0 | -24.5 |\\n| 2B UM 1       | -1.3  | 27.8 |\\n| 2B UM 2       | 88.9  | 62.2 |\\n| 2B UM 3       | -62.2 | 9.3 |\\n| 2B UM 4       | 74.6  | 69.4 |\\n| 4B UM 1       | 18.3  | 40.3 |\\n| 4B UM 2       | 8.5   | 33.1 |\\n| 4B UM 3       | 15.0  | 53.4 |\\n| 4B UM 4       | -25.4 | 22.4 |\"}"}
{"id": "orlanski23a", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 14. Number of Questions passed for BC-HumanEval (HE) and TP3. BC-HE has 161 total problems and TP3 has 370 total problems. $S$ is the size of the model, and $D$ is the distribution it was trained on. P is the PaLM distribution while PC is the PaLM-Coder distribution. Languages are sorted from high to low resource. Green values are the best values for that language, while red values are the worst.\\n\\n| Language | $S$ | $D$ |\\n|----------|-----|-----|\\n| Java     | 46  | 13  |\\n| Py       | 44  | 8   |\\n| C++      | 44  | 9   |\\n| PHP      | 32  | 5   |\\n| TS       | 44  | 8   |\\n| JS       | 38  | 26  |\\n| Go       | 31  | 23  |\\n| Dart     | 28  | 17  |\\n| Lua      | 18  | 9   |\\n| Rust     | 18  | 5   |\\n| C#       | 28  | 17  |\\n| R        | 26  | 23  |\\n| Julia    | 26  | 24  |\\n| HS       | 26  | 23  |\\n| Java     | 69  | 69  |\\n| Py       | 70  | 52  |\\n| C++      | 70  | 33  |\\n| PHP      | 69  | 21  |\\n| TS       | 71  | 18  |\\n| JS       | 70  | 9   |\\n| Go       | 53  | 47  |\\n| Dart     | 40  | 42  |\\n| Lua      | 53  | 27  |\\n| Rust     | 40  | 21  |\\n| C#       | 53  | 25  |\\n| R        | 53  | 19  |\\n| Julia    | 53  | 19  |\\n| HS       | 53  | 19  |\\n| Java     | 95  | 95  |\\n| Py       | 93  | 88  |\\n| C++      | 89  | 35  |\\n| PHP      | 94  | 50  |\\n| TS       | 94  | 23  |\\n| JS       | 98  | 23  |\\n| Go       | 69  | 23  |\\n| Dart     | 71  | 23  |\\n| Lua      | 70  | 23  |\\n| Rust     | 81  | 23  |\\n| C#       | 78  | 23  |\\n| R        | 81  | 23  |\\n| Julia    | 81  | 23  |\\n| HS       | 81  | 23  |\\n| Java     | 122 | 122 |\\n| Py       | 89  | 50  |\\n| C++      | 61  | 23  |\\n| PHP      | 73  | 6  |\\n| TS       | 78  | 6  |\\n| JS       | 78  | 6  |\\n| Go       | 62  | 6  |\\n| Dart     | 55  | 6  |\\n| Lua      | 50  | 6  |\\n| Rust     | 45  | 6  |\\n| C#       | 38  | 6  |\\n| R        | 38  | 6  |\\n| Julia    | 38  | 6  |\\n| HS       | 38  | 6  |\\n| Java     | 95  | 95  |\\n| Py       | 94  | 88  |\\n| C++      | 94  | 35  |\\n| PHP      | 93  | 50  |\\n| TS       | 92  | 23  |\\n| JS       | 92  | 23  |\\n| Go       | 69  | 23  |\\n| Dart     | 71  | 23  |\\n| Lua      | 70  | 23  |\\n| Rust     | 81  | 23  |\\n| C#       | 78  | 23  |\\n| R        | 81  | 23  |\\n| Julia    | 81  | 23  |\\n| HS       | 81  | 23  |\\n| Java     | 190 | 190 |\\n| Py       | 149 | 123 |\\n| C++      | 182 | 181 |\\n| PHP      | 150 | 72  |\\n| TS       | 181 | 64  |\\n| JS       | 150 | 123 |\\n| Go       | 182 | 64  |\\n| Dart     | 150 | 123 |\\n| Lua      | 182 | 64  |\\n| Rust     | 150 | 123 |\\n| C#       | 182 | 64  |\\n| R        | 182 | 64  |\\n| Julia    | 182 | 64  |\\n| HS       | 182 | 64  |\\n| Java     | 24  | 24  |\\n| Py       | 57  | 57  |\\n| C++      | 74  | 74  |\\n| PHP      | 60  | 60  |\\n| TS       | 56  | 56  |\\n| JS       | 58  | 58  |\\n| Go       | 41  | 41  |\\n| Dart     | 57  | 57  |\\n| Lua      | 74  | 74  |\\n| Rust     | 56  | 56  |\\n| C#       | 58  | 58  |\\n| R        | 58  | 58  |\\n| Julia    | 58  | 58  |\\n| HS       | 58  | 58  |\"}"}
{"id": "orlanski23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4. BC-HumanEval pass @1 values for the different models and training distributions. Used $T = 0.8$ and sampled 200 programs per problem. UM is Unimax distribution. PaLM-C is the PaLM-Coder distribution. HS is Haskell, JS is JavaScript, Py is Python, and TS is TypeScript.\\n\\n| Size Dist. | C# | C++ | Dart | Go | HS | Java | JS | Julia | Lua | PHP | Py | R | Rust | TS |\\n|------------|----|-----|------|----|----|------|----|-------|-----|-----|----|---|------|----|\\n| 1B Nat     | 1.0 | 3.6 | 2.3  | 2.5 | 0.7 | 3.8  | 3.6 | 0.5   | 1.8 | 2.8 | 4.8 | 0.5| 1.6  | 4.0 |\\n| UM 1       | 1.7 | 3.0 | 3.0  | 2.6 | 1.3 | 2.8  | 4.0 | 2.1   | 2.2 | 2.5 | 3.9 | 1.2| 2.8  | 4.4 |\\n| UM 2       | 2.0 | 3.2 | 3.0  | 2.7 | 1.6 | 2.7  | 3.9 | 2.1   | 2.1 | 2.3 | 4.2 | 1.4| 3.1  | 4.3 |\\n| UM 3       | 1.6 | 1.5 | 2.6  | 2.6 | 1.4 | 2.8  | 4.0 | 2.5   | 2.2 | 2.2 | 4.0 | 1.8| 2.6  | 4.1 |\\n| UM 4       | 1.7 | 2.7 | 3.1  | 2.9 | 1.5 | 2.8  | 3.7 | 2.6   | 2.2 | 2.2 | 3.5 | 2.1| 2.5  | 4.1 |\\n| 2B Nat     | 2.6 | 7.5 | 5.0  | 5.4 | 1.0 | 8.0  | 7.6 | 1.2   | 4.5 | 6.2 | 9.1 | 1.4| 3.9  | 7.9 |\\n| UM 1       | 5.3 | 6.0 | 6.1  | 5.1 | 1.9 | 6.6  | 7.6 | 4.4   | 5.4 | 5.6 | 7.8 | 2.1| 6.4  | 7.5 |\\n| UM 2       | 5.2 | 6.1 | 5.6  | 4.5 | 2.1 | 5.7  | 6.4 | 4.5   | 5.2 | 4.8 | 7.0 | 2.8| 5.8  | 7.0 |\\n| UM 3       | 5.5 | 6.2 | 5.2  | 4.7 | 2.4 | 6.2  | 6.8 | 5.1   | 4.9 | 4.8 | 7.5 | 3.5| 6.1  | 7.0 |\\n| UM 4       | 4.9 | 6.1 | 5.4  | 4.7 | 2.9 | 5.7  | 6.5 | 4.6   | 4.8 | 4.6 | 7.5 | 3.3| 5.6  | 7.1 |\\n| 4B Nat     | 9.9 | 12.7| 8.7  | 8.2 | 1.8 | 13.5 | 12.3| 4.7   | 8.6 | 10.1| 14.6| 3.0| 8.7  | 11.7|\\n| UM 1       | 8.0 | 11.3| 9.2  | 7.5 | 3.1 | 11.6 | 11.6| 6.6   | 9.2 | 8.4 | 10.7| 3.5| 9.5  | 11.7|\\n| UM 2       | 8.9 | 11.1| 9.3  | 7.0 | 3.6 | 10.2 | 11.3| 6.8   | 8.7 | 8.4 | 11.9| 4.0| 10.7 | 11.3|\\n| UM 3       | 9.2 | 9.9 | 9.0  | 7.6 | 4.5 | 10.5 | 12.3| 8.9   | 9.2 | 9.6 | 11.2| 4.5| 10.6 | 11.6|\\n| UM 4       | 10.4 | 11.2| 8.9  | 7.7 | 5.0 | 10.5 | 10.6| 7.9   | 9.2 | 8.0 | 10.0| 5.1| 11.0 | 11.0|\\n\\n| 8B PaLM    | 2.2 | 3.3 | 2.5  | 2.1 | 0.1 | 2.5  | 4.1 | 0.1   | 2.2 | 2.6 | 3.6 | 0.2| 1.0  | 4.2 |\\n| PaLM-C     | 2.6 | 4.4 | 3.2  | 3.3 | 0.3 | 3.9  | 5.8 | 0.1   | 3.7 | 4.9 | 8.1 | 0.4| 1.5  | 5.6 |\\n\\n| 62B PaLM   | 5.9 | 6.5 | 3.9  | 5.3 | 0.3 | 6.9  | 8.5 | 0.7   | 6.8 | 6.2 | 9.1 | 1.5| 1.8  | 7.9 |\\n| PaLM-C     | 7.6 | 9.6 | 5.7  | 6.6 | 0.8 | 10.4 | 10.7| 1.4   | 7.5 | 7.2 | 11.0| 1.9| 3.5  | 9.7 |\"}"}
{"id": "orlanski23a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5. BC-TP3 pass values for the different models and training distributions. Used $T = 0.8$ and sampled 50 programs per problem. Nat is the natural distribution. UM is Unimax distribution. PaLM-C is the PaLM-Coder distribution. HS is Haskell, JS is JavaSript, Py is Python, and TS is TypeScript.\\n\\n| Size Dist. | C# | C++ | Dart | Go | HS | Java | JS | Julia | Lua | PHP | R | Rust | TS |\\n|------------|----|-----|------|----|----|------|----|-------|-----|-----|---|------|----|\\n| 1B Nat     | 0.5 | 1.1 | 0.6  | 1.0| 0.0| 1.7  | 1.2| 0.1   | 0.2 | 0.6 | 0.0| 2.1  |    |\\n| UM 1       | 0.1 | 0.2 | 0.1  | 0.5| 0.2| 0.3  | 0.7| 0.1   | 0.1 | 0.1 | 0.0| 0.3  | 0.7|\\n| UM 2       | 0.3 | 0.1 | 0.1  | 0.3| 0.4| 1.3  | 0.3| 0.0   | 0.1 | 0.0 | 0.0| 0.4  | 1.0|\\n| UM 3       | 0.2 | 0.1 | 0.1  | 0.3| 0.2| 0.7  | 1.1| 0.1   | 0.0 | 0.0 | 0.0| 0.3  | 0.7|\\n| UM 4       | 0.2 | 0.3 | 0.4  | 0.3| 0.8| 0.6  | 1.1| 0.7   | 0.1 | 0.2 | 0.0| 0.7  | 2.1|\\n| 2B Nat     | 1.0 | 2.2 | 1.3  | 1.9| 0.8| 2.9  | 4.1| 0.3   | 0.1 | 2.8 | 0.4| 2.2  | 3.1|\\n| UM 1       | 1.3 | 0.7 | 0.7  | 0.7| 0.5| 1.9  | 1.0| 0.3   | 0.3 | 1.2 | 0.1| 1.1  | 0.4|\\n| UM 2       | 1.9 | 2.1 | 2.8  | 0.9| 1.0| 2.7  | 6.8| 0.6   | 0.2 | 4.0 | 0.1| 1.8  | 5.4|\\n| UM 3       | 1.1 | 0.4 | 0.2  | 0.4| 0.8| 1.9  | 3.6| 0.3   | 0.1 | 1.7 | 0.4| 0.6  | 1.0|\\n| UM 4       | 3.2 | 1.8 | 2.4  | 2.7| 1.5| 3.7  | 5.5| 2.1   | 0.5 | 2.8 | 0.4| 2.9  | 4.1|\\n| 4B Nat     | 5.9 | 6.5 | 5.1  | 3.9| 1.3| 9.4  | 10.9| 3.5   | 0.9 | 10.4| 0.6| 3.8  | 7.3|\\n| UM 1       | 5.8 | 6.1 | 7.8  | 5.7| 1.7| 7.7  | 13.5| 5.9   | 4.2 | 8.6 | 1.2| 5.8  | 9.6|\\n| UM 2       | 7.1 | 4.1 | 6.1  | 4.4| 2.7| 8.3  | 11.7| 6.1   | 3.1 | 9.8 | 1.3| 6.2  | 7.7|\\n| UM 3       | 8.7 | 5.8 | 7.1  | 3.6| 2.6| 7.8  | 12.1| 2.9   | 1.3 | 9.5 | 2.1| 6.9  | 11.1|\\n| UM 4       | 5.0 | 4.8 | 5.7  | 4.0| 1.9| 6.8  | 9.4  | 2.4   | 1.3 | 4.3 | 2.2| 6.3  | 7.3|\\n| 8B PaLM    | 1.7 | 4.6 | 4.9  | 4.8| 0.3| 2.6  | 7.4  | 0.3   | 2.9 | 6.4 | 0.1| 2.2  | 6.9|\\n| PaLM-C     | 3.4 | 5.2 | 4.8  | 4.2| 0.1| 4.7  | 8.6  | 0.4   | 3.6 | 7.7 | 0.2| 2.4  | 7.3|\\n| 62B PaLM   | 7.0 | 7.9 | 6.6  | 6.1| 1.3| 7.9  | 11.8 | 1.3   | 6.2 | 12.2| 1.0| 3.6  | 12.0|\\n| PaLM-C     | 8.4 | 8.3 | 7.6  | 6.6| 1.5| 9.9  | 14.2 | 1.6   | 8.0 | 14.1| 2.6| 4.0  | 12.7|\\n\\nTable 6. BC-Transcoder with Python source pass values for the different models and training distributions where the source language is Python. Used $T = 0.8$ and sampled 50 programs per problem. Nat is the natural distribution. UM is Unimax distribution. PaLM-C is the PaLM-Coder distribution. HS is Haskell, JS is JavaSript, Py is Python, and TS is TypeScript.\\n\\n| Size Dist. | C# | C++ | Dart | Go | HS | Java | JS | Julia | Lua | PHP | R | Rust | TS |\\n|------------|----|-----|------|----|----|------|----|-------|-----|-----|---|------|----|\\n| 1B Nat     | 1.7 | 2.1 | 0.4  | 1.3| 0.2| 2.2  | 2.0| 0.1   | 0.6 | 0.8 | 0.2| 1.0  | 2.0|\\n| UM 1       | 0.1 | 0.1 | 0.0  | 0.4| 0.2| 0.3  | 0.5| 0.0   | 0.0 | 0.1 | 0.1| 0.8  | 0.8|\\n| UM 2       | 0.3 | 0.1 | 0.1  | 0.3| 0.4| 0.6  | 1.3| 0.2   | 0.0 | 0.1 | 0.2| 0.7  | 1.1|\\n| UM 3       | 0.4 | 0.3 | 0.0  | 0.1| 0.3| 0.5  | 0.9| 0.1   | 0.0 | 0.1 | 0.2| 0.7  | 0.7|\\n| UM 4       | 0.3 | 0.2 | 0.2  | 0.1| 1.2| 0.6  | 1.1| 0.2   | 0.1 | 0.4 | 0.3| 0.9  | 1.3|\\n| 2B Nat     | 2.9 | 5.5 | 1.0  | 4.4| 1.0| 4.9  | 8.2  | 0.3 | 0.4 | 3.8 | 1.3 | 3.5 | 5.2 |\\n| UM 1       | 2.9 | 2.6 | 0.8  | 1.2| 0.9 | 3.8 | 2.5 | 0.1 | 0.4 | 1.5 | 0.8| 1.8 | 1.0 |\\n| UM 2       | 4.4 | 5.6 | 3.9  | 3.2| 1.5 | 4.9 | 10.1 | 1.0 | 0.3 | 3.6 | 2.3 | 3.2 | 5.9 |\\n| UM 3       | 2.1 | 1.0 | 0.3  | 0.3| 1.4 | 3.0 | 3.9 | 0.0 | 0.1 | 1.7 | 0.5 | 1.2 | 1.5 |\\n| UM 4       | 4.8 | 4.7 | 2.9  | 3.5| 1.7 | 4.8 | 8.4 | 2.7 | 2.5 | 2.6 | 2.4 | 4.0 | 5.7 |\\n| 4B Nat     | 23.7| 28.4| 6.8  | 11.7| 2.3| 29.5 | 27.9| 1.7 | 2.4 | 23.4| 2.8 | 8.3 | 15.3|\\n| UM 1       | 16.7| 23.7| 9.7  | 18.6| 2.4| 18.6 | 35.3| 3.6 | 8.1 | 20.8| 2.6 | 12.7| 22.4|\\n| UM 2       | 16.0| 16.1| 8.4  | 15.0| 3.3| 16.6 | 26.2| 5.1 | 5.3 | 17.4| 5.0 | 11.3| 17.4|\\n| UM 3       | 21.8| 30.6| 12.5 | 14.6| 3.5| 23.2 | 37.1| 0.9 | 3.5 | 20.3| 6.1 | 17.0| 28.2|\\n| UM 4       | 14.5| 17.6| 3.6  | 13.0| 1.4| 14.9 | 26.6| 2.0 | 4.5 | 5.0 | 3.6 | 14.5| 14.7|\\n| 8B PaLM    | 2.9 | 11.8| 4.7  | 7.3 | 0.9 | 4.3  | 16.3| 0.1 | 5.1 | 8.8 | 1.7 | 3.2 | 11.6|\\n| PaLM-C     | 8.5 | 10.8| 5.3  | 8.6 | 1.1 | 8.9  | 24.2| 1.0 | 9.4 | 13.7| 2.0 | 4.0 | 14.3|\\n| 62B PaLM   | 21.4| 29.1| 7.3  | 17.8| 1.9 | 17.7 | 35.6| 3.4 | 16.9| 25.6| 4.3 | 7.3 | 29.3|\\n| PaLM-C     | 28.7| 33.0| 9.6  | 21.4| 2.2 | 23.6 | 38.4| 4.2 | 22.1| 32.4| 8.1 | 7.3 | 29.6|\"}"}
{"id": "orlanski23a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7. BC-Transcoder with C++ Source pass values for the different models and training distributions. Used $T = 0.8$ and sampled 50 programs per problem. Nat is the natural distribution. UM is Unimax distribution. PaLM-C is the PaLM-Coder distribution. HS is Haskell, JS is JavaScript, Py is Python, and TS is TypeScript.\\n\\n| Size Dist. | C# | Dart | Go | HS | Java | JS | Julia | Lua | PHP | Py | R | Rust | TS |\\n|------------|----|------|----|----|------|----|-------|-----|-----|----|---|------|----|\\n| 1B         |    |      |    |    |      |    |       |     |     |    |   |      |    |\\n| Nat        | 3.0 | 3.1  | 1.3 | 0.1 | 2.6  | 2.3 | 0.1   | 0.3 | 1.0 | 1.8 | 0.1| 0.6  | 3.7 |\\n| UM         | 0.4 | 3.1  | 0.9 | 0.2 | 1.1  | 1.1 | 0.0   | 0.1 | 0.8 | 2.3 | 0.2| 0.8  | 1.9 |\\n| UM 2       | 1.1 | 1.6  | 0.5 | 0.4 | 1.1  | 2.7 | 0.0   | 0.0 | 1.1 | 1.5 | 0.1| 0.7  | 1.9 |\\n| UM 3       | 1.9 | 1.6  | 0.8 | 0.4 | 1.6  | 2.9 | 0.0   | 0.0 | 0.7 | 2.3 | 0.1| 1.1  | 1.5 |\\n| UM 4       | 1.3 | 3.7  | 1.8 | 1.3 | 1.7  | 3.0 | 0.2   | 0.5 | 2.4 | 1.8 | 0.3| 1.6  | 3.7 |\\n| 2B         |    |      |    |    |      |    |       |     |     |    |   |      |    |\\n| Nat        | 8.9 | 13.3 | 5.5 | 1.2 | 9.2  | 16.0| 0.3   | 1.5 | 12.4| 11.2 | 1.4| 4.6  | 12.1|\\n| UM         | 4.1 | 7.9  | 3.4 | 1.4 | 6.7  | 6.2 | 0.2   | 2.1 | 5.0 | 6.8  | 0.5| 3.6  | 4.3 |\\n| UM 2       | 8.6 | 18.1 | 6.8 | 2.6 | 9.4  | 20.2| 0.4   | 2.1 | 17.3| 8.4  | 1.2| 5.4  | 15.2|\\n| UM 3       | 4.6 | 12.0 | 4.0 | 2.1 | 4.6  | 12.9| 0.5   | 2.0 | 8.2 | 7.7  | 1.1| 2.4  | 10.3|\\n| UM 4       | 7.7 | 15.1 | 5.9 | 3.0 | 7.1  | 14.4| 1.9   | 2.0 | 9.6 | 5.5  | 1.2| 5.4  | 13.0|\\n| 4B         |    |      |    |    |      |    |       |     |     |    |   |      |    |\\n| Nat        | 34.5| 17.3 | 20.6| 3.2 | 37.6 | 32.9| 3.3   | 6.9 | 34.0| 31.7 | 2.5| 10.3 | 29.2|\\n| UM         | 27.0| 18.3 | 23.5| 3.7 | 27.9 | 41.2| 1.6   | 9.9 | 34.5| 31.3 | 2.6| 14.3 | 33.1|\\n| UM 2       | 19.3| 21.1 | 18.7| 4.4 | 22.0 | 34.1| 4.3   | 6.4 | 26.5| 25.2 | 4.0| 12.2 | 24.2|\\n| UM 3       | 31.5| 20.8 | 16.0| 4.6 | 32.3 | 42.6| 1.0   | 7.0 | 39.9| 33.5 | 5.0| 16.4 | 40.2|\\n| UM 4       | 25.0| 15.5 | 16.4| 3.1 | 21.1 | 31.9| 1.3   | 6.1 | 9.7 | 20.4 | 2.6| 11.6 | 28.7|\\n| 8B PaLM    |    |      |    |    |      |    |       |     |     |    |   |      |    |\\n| PaLM-C     | 17.5| 16.0 | 8.3 | 1.3 | 14.9 | 28.1| 0.7   | 8.5 | 21.2| 14.8 | 1.1| 5.0  | 21.8|\\n| 62B PaLM   |    |      |    |    |      |    |       |     |     |    |   |      |    |\\n| PaLM-C     | 27.3| 17.9 | 20.6| 2.6 | 24.0 | 42.4| 6.5   | 16.3| 41.3| 26.7 | 4.3| 8.6  | 37.3|\\n\\nTable 8. BC-HumanEval pass@100 values for the different models and training distributions. Used $T = 0.8$ and sampled 200 programs per problem. Nat is the natural distribution. UM is Unimax distribution. PaLM-C is the PaLM-Coder distribution. HS is Haskell, JS is JavaScript, Py is Python, and TS is TypeScript.\\n\\n| Size Dist. | C# | C++ | Dart | Go | HS | Java | JS | Julia | Lua | PHP | Py | R | Rust | TS |\\n|------------|----|-----|------|----|----|------|----|-------|-----|-----|----|---|------|----|\\n| 1B         |    |     |      |    |    |      |    |       |     |     |    |   |      |    |\\n| Nat        | 7.3 | 23.2| 14.4 | 14.9| 2.4 | 24.3 | 19.0| 4.4   | 9.8 | 17.1| 23.3| 4.0| 13.9 | 22.4|\\n| UM         | 12.3| 16.2| 14.0 | 12.0| 7.5 | 17.3 | 18.2| 13.0  | 13.1| 15.0| 19.9| 7.9| 14.5 | 17.8|\\n| UM 2       | 14.5| 16.9| 13.8 | 11.9| 8.3 | 19.6 | 19.1| 15.8  | 13.5| 14.8| 21.2| 10.4| 16.5 | 19.1|\\n| UM 3       | 13.7| 13.5| 13.4 | 15.4| 10.0| 21.4 | 18.4| 14.2  | 12.8| 14.6| 21.1| 10.4| 16.0 | 18.6|\\n| UM 4       | 15.8| 16.6| 13.8 | 12.3| 9.7 | 19.7 | 18.1| 16.6  | 14.3| 15.3| 20.6| 10.6| 15.9 | 19.6|\\n| 2B         |    |     |      |    |    |      |    |       |     |     |    |   |      |    |\\n| Nat        | 17.9| 37.8| 21.3 | 27.8| 4.9 | 37.8 | 36.8| 9.7   | 23.3| 35.3| 38.8| 10.9| 26.5 | 37.9|\\n| UM         | 28.5| 31.8| 24.6 | 26.2| 12.2| 32.0 | 33.8| 23.8  | 22.9| 29.3| 30.9| 14.0| 29.9 | 34.9|\\n| UM 2       | 30.6| 30.8| 25.8 | 22.6| 12.9| 32.1 | 32.1| 26.5  | 21.9| 27.4| 33.5| 15.8| 27.4 | 33.0|\\n| UM 3       | 31.9| 33.0| 23.9 | 25.9| 13.7| 31.4 | 34.1| 26.5  | 25.3| 29.5| 31.5| 18.5| 28.7 | 34.8|\\n| UM 4       | 30.5| 30.4| 26.7 | 24.9| 12.8| 31.3 | 33.0| 29.0  | 23.2| 26.5| 34.6| 16.2| 28.0 | 34.6|\\n| 4B         |    |     |      |    |    |      |    |       |     |     |    |   |      |    |\\n| Nat        | 47.9| 51.1| 39.6 | 37.9| 12.5| 53.4 | 53.0| 27.0  | 38.7| 48.5| 52.9| 16.7| 43.4 | 50.7|\\n| UM         | 42.4| 46.6| 42.3 | 38.3| 14.6| 50.6 | 47.9| 33.8  | 42.0| 44.0| 46.2| 20.1| 44.6 | 50.6|\\n| UM 2       | 44.3| 41.2| 40.6 | 34.9| 16.0| 40.9 | 44.2| 35.9  | 38.8| 42.0| 48.9| 24.1| 43.1 | 44.6|\\n| UM 3       | 44.8| 44.4| 43.3 | 37.3| 21.3| 49.9 | 50.8| 40.0  | 43.2| 45.8| 48.8| 27.9| 49.8 | 51.5|\\n| UM 4       | 47.9| 43.5| 37.7 | 36.1| 20.3| 46.1 | 47.3| 39.1  | 42.2| 41.7| 46.3| 23.4| 44.8 | 46.1|\\n| 8B PaLM    |    |     |      |    |    |      |    |       |     |     |    |   |      |    |\\n| PaLM-C     | 16.8| 19.7| 14.7 | 14.3| 1.1 | 19.9 | 20.9| 2.0   | 13.2| 17.8| 21.0| 2.9 | 9.6  | 22.5|\\n| 62B PaLM   |    |     |      |    |    |      |    |       |     |     |    |   |      |    |\\n| PaLM-C     | 43.9| 40.8| 26.9 | 31.4| 6.9 | 48.3 | 46.2| 8.3   | 36.4| 41.6| 44.7| 13.8| 24.3 | 44.6|\\n| PaLM-C     | 49.2| 50.0| 37.6 | 38.7| 9.0 | 57.0 | 56.7| 12.1  | 41.1| 46.9| 64.1| 16.9| 31.7 | 54.8|\"}"}
{"id": "orlanski23a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 9. BC-TP3 pass values for the different models and training distributions where the source language is Python. Used T = 0.8 and sampled 50 programs per problem. Nat is the natural distribution. UM is Unimax distribution. PaLM-C is the PaLM-Coder distribution. HS is Haskell, JS is JavaScript, Py is Python, and TS is TypeScript.\\n\\n| Size Dist. | C#  | C++ | Dart | Go  | HS  | Java | JS  | Julia | Lua | PHP | R  | Rust | TS  |\\n|------------|-----|-----|------|-----|-----|------|-----|-------|-----|-----|----|------|-----|\\n| 1B         | 8.2 | 16.5| 9.6  | 14.6| 0.3 | 23.4 | 13.8| 1.1   | 3.7 | 10.6| 0.8| 10.6 | 19.8|\\n| UM 1       | 2.3 | 3.2 | 2.8  | 9.0 | 2.6 | 6.5  | 9.3 | 2.0   | 1.1 | 1.6 | 0.2| 5.5  | 9.8 |\\n| UM 2       | 5.2 | 1.3 | 1.7  | 5.4 | 1.7 | 8.2  | 9.8 | 4.3   | 1.2 | 1.1 | 0.4| 6.2  | 11.3|\\n| UM 3       | 4.5 | 2.1 | 2.3  | 4.4 | 3.7 | 12.6 | 11.6| 1.2   | 0.4 | 0.4 | 0.1| 5.4  | 9.2 |\\n| UM 4       | 3.4 | 4.6 | 5.9  | 5.2 | 5.4 | 10.8 | 11.4| 8.6   | 1.7 | 4.6 | 0.7| 8.3  | 17.0|\\n| 2B         | 8.3 | 18.2| 8.3  | 11.1| 5.7 | 24.5 | 24.4| 3.8   | 1.4 | 18.3| 3.4| 15.3 | 15.7|\\n| UM 1       | 15.8| 11.9| 6.3  | 8.7 | 5.2 | 23.2 | 10.9| 5.4   | 4.6 | 11.1| 2.1| 13.7 | 4.6 |\\n| UM 2       | 20.7| 20.0| 19.1 | 11.7| 6.9 | 26.3 | 32.9| 8.1   | 3.2 | 20.6| 1.7| 18.6 | 27.8|\\n| UM 3       | 16.9| 7.7 | 4.4  | 7.7 | 5.9 | 21.2 | 25.7| 5.3   | 2.3 | 16.2| 4.4| 11.2 | 13.9|\\n| UM 4       | 24.3| 18.8| 15.3 | 14.0| 9.6 | 32.1 | 28.1| 13.9  | 3.9 | 17.3| 3.5| 21.8 | 21.5|\\n| 4B         | 29.1| 31.9| 16.6 | 14.6| 7.7 | 42.2 | 39.5| 17.3  | 11.9| 40.1| 3.7| 24.1 | 32.9|\\n| UM 1       | 28.9| 30.0| 30.0 | 22.0| 8.8 | 37.6 | 49.2| 22.5  | 18.2| 40.7| 6.9| 32.5 | 41.7|\\n| UM 2       | 35.5| 31.0| 30.2 | 23.7| 13.0| 43.7 | 49.5| 24.6  | 17.3| 46.1|10.6| 37.3 | 39.0|\\n| UM 3       | 35.2| 24.8| 25.5 | 16.2| 13.0| 34.3 | 41.9| 16.4  | 11.9| 33.7|10.6| 35.2 | 38.8|\\n| UM 4       | 25.5| 29.7| 23.9 | 19.5| 12.1| 38.5 | 40.5| 18.6  | 8.3 | 26.7| 9.8| 32.8 | 29.0|\\n| 8B         | 19.4| 22.6| 19.0 | 17.2| 2.8 | 26.7 | 26.6| 4.0   | 17.0| 31.7| 1.9| 10.7 | 25.9|\\n| PaLM-C     | 25.9| 26.2| 17.9 | 16.7| 2.0 | 30.1 | 34.1| 5.9   | 22.6| 40.3| 3.2| 11.8 | 29.3|\\n| 62B        | 38.9| 35.2| 27.2 | 24.8| 6.1 | 43.0 | 48.4| 10.6  | 28.3| 48.2| 7.2| 18.0 | 42.6|\\n| PaLM-C     | 41.8| 38.7| 31.2 | 26.7| 7.2 | 45.2 | 55.8| 11.3  | 33.8| 56.5|11.4| 20.5 | 48.7|\\n\\n### Table 10. BC-Transcoder pass values for the different models and training distributions where the source language is Python. Used T = 0.8 and sampled 50 programs per problem. Nat is the natural distribution. UM is Unimax distribution. PaLM-C is the PaLM-Coder distribution. HS is Haskell, JS is JavaScript, Py is Python, and TS is TypeScript.\\n\\n| Size Dist. | C#  | C++ | Dart | Go  | HS  | Java | JS  | Julia | Lua | PHP | R  | Rust | TS  |\\n|------------|-----|-----|------|-----|-----|------|-----|-------|-----|-----|----|------|-----|\\n| 1B         | 14.0| 18.4| 5.4  | 10.3| 2.1 | 17.3 | 15.3| 2.8   | 7.4 | 8.7 | 2.9| 10.3 | 14.4|\\n| UM 1       | 3.1 | 1.3 | 1.1  | 5.7 | 3.3 | 4.9  | 5.5 | 0.9   | 1.0 | 1.3 | 1.8| 6.7  | 7.6 |\\n| UM 2       | 4.3 | 2.7 | 2.2  | 3.5 | 3.9 | 8.2  | 11.0| 3.3   | 2.9 | 2.6 | 1.8| 6.9  | 10.4|\\n| UM 3       | 5.8 | 5.1 | 0.8  | 2.1 | 3.9 | 6.8  | 9.4 | 1.7   | 0.9 | 1.2 | 2.6| 6.3  | 7.2 |\\n| UM 4       | 5.1 | 3.8 | 2.6  | 1.0 | 6.6 | 7.6  | 11.4| 2.6   | 1.3 | 5.4 | 3.6| 7.8  | 10.6|\\n| 2B         | 20.9| 34.7| 11.0 | 17.5| 6.3 | 30.0 | 37.0| 5.0   | 5.9 | 29.4| 7.6| 11.9 | 24.3|\\n| UM 1       | 21.4| 22.3| 10.8 | 12.5| 5.2 | 27.7 | 23.0| 2.5   | 5.6 | 15.9| 6.4| 15.2 | 10.9|\\n| UM 2       | 29.4| 36.1| 20.7 | 20.0| 6.9 | 31.5 | 43.6| 9.1   | 4.2 | 28.7| 5.8| 19.3 | 29.1|\\n| UM 3       | 18.6| 12.8| 4.1  | 4.9 | 7.8 | 22.6 | 29.0| 0.7   | 1.7 | 14.9| 5.6| 13.0 | 13.9|\\n| UM 4       | 28.3| 29.7| 19.1 | 18.2| 9.0 | 30.0 | 39.9| 12.0  | 12.2| 21.7| 7.6| 20.1 | 27.2|\\n| 4B         | 68.4| 82.5| 34.0 | 45.5| 9.0 | 80.2 | 77.6| 13.5  | 23.7| 75.9|12.7| 38.4 | 66.1|\\n| UM 1       | 59.8| 75.8| 40.2 | 56.2| 11.6| 70.5 | 80.6| 16.0  | 37.9| 73.8|11.3| 53.9 | 74.5|\\n| UM 2       | 58.6| 66.7| 36.9 | 57.1| 14.2| 64.4 | 76.4| 21.2  | 31.1| 69.3|19.7| 51.2 | 68.0|\\n| UM 3       | 64.6| 77.2| 39.1 | 50.2| 14.5| 73.4 | 79.0| 8.4   | 24.8| 69.1|21.8| 58.9 | 74.8|\\n| UM 4       | 59.3| 72.5| 25.4 | 51.5| 11.4| 65.0 | 72.7| 13.7  | 27.1| 47.2|19.4| 54.2 | 62.8|\\n| 8B         | 26.8| 48.6| 21.0 | 27.7| 3.6 | 29.7 | 51.8| 2.5   | 22.4| 44.2| 5.9 | 15.0 | 42.1|\\n| PaLM-C     | 44.0| 52.0| 26.7 | 29.6| 5.4 | 45.9 | 65.6| 9.0   | 39.7| 58.0| 9.7 | 17.5 | 54.4|\\n| 62B        | 70.6| 78.5| 32.7 | 50.9| 8.4 | 65.1 | 80.3| 15.6  | 53.4| 79.4| 17.1| 27.5 | 76.6|\\n| PaLM-C     | 77.1| 83.7| 39.8 | 57.4| 8.8 | 72.2 | 82.6| 20.3  | 62.2| 84.0| 23.7| 26.6 | 79.3|\\n\\n20\"}"}
{"id": "orlanski23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Measuring the Impact of Programming Language Distribution\\n\\nFigure 7. Results on BC-HumanEval and BC-TP3 at a prediction level. Left to right: 1) The % of predictions that passed at least one test, but not all 2) The average, per question, percent of tests passed for each prediction 3) The % of predictions that had either a compilation error, runtime error, or timed out. Full results for BC-HumanEval and BC-TP3 can be found in Figure 9 and Figure 10, respectively.\\n\\n7. Conclusion\\n\\nWe proposed the BabelCode framework for multi-lingual execution-based evaluation and a new strategy for balancing programming language distributions. We highlight the ease of creating new benchmarks with BabelCode by proposing the Translating Python Programming Puzzles. Our experiments demonstrate that adjusting how much we oversample low-resource languages and downsample high-resource languages greatly improves low-resource performance with minimal impact to the performance of high-resource languages in tasks involving either a single or multiple programming language. By open-sourcing BabelCode, future work can investigate improved balancing strategies along with new multi-lingual programming language questions.\\n\\nAcknowledgements\\n\\nWe thank Michael Janner, Owen Lewis, Alex Polozov, Uros Popovic, Devjeet Roy, Tal Schuster, and Charles Sutton for their helpful discussions and feedback on the paper.\\n\\nReferences\\n\\nAhmad, W., Chakraborty, S., Ray, B., and Chang, K.-W. Unified pre-training for program understanding and generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2655\u20132668, Online, June 2021. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2021.naacl-main.211.\\n\\nAllal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C. M., Muennighoff, N., Mishra, M., Gu, A., Dey, M., et al. Santacoder: don't reach for the stars! arXiv preprint arXiv:2301.03988, 2023.\\n\\nAllamanis, M. The adverse effects of code duplication in machine learning models of code. In Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, pp. 143\u2013153, 2019.\\n\\nArivazhagan, N., Bapna, A., Firat, O., Lepikhin, D., Johnson, M., Krikun, M., Chen, M. X., Cao, Y., Foster, G., Cherry, C., et al. Massively multilingual neural machine translation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019, 2019.\\n\\nAthiwaratkun, B., Gouda, S. K., Wang, Z., Li, X., Tian, Y., Tan, M., Ahmad, W. U., Wang, S., Sun, Q., Shang, M., Gonugondla, S. K., Ding, H., Kumar, V., Fulton, N., Faraiali, A., Jain, S., Giaquinto, R., Qian, H., Ramanathan, M. K., Nallapati, R., Ray, B., Bhatia, P., Sengupta, S., Roth, D., and Xiang, B. Multi-lingual evaluation of code generation models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Bo7eeXm6An8.\\n\\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n\\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey, C., Tworek, J., and Chen, M. Efficient training of\"}"}
{"id": "orlanski23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Measuring the Impact of Programming Language Distribution\\n\\nCassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps-Costin, L., Pinckney, D., Yee, M. H., Zi, Y., Anderson, C. J., Feldman, M. Q., et al. A scalable and extensible approach to benchmarking nl2code for 18 programming languages. arXiv preprint arXiv:2208.08227, 2022.\\n\\nChakraborty, S., Ahmed, T., Ding, Y., Devanbu, P., and Ray, B. Natgen: generative pre-training by \\\"naturalizing\\\" source code. Proceedings of the 30th ACM Joint Euro- pean Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2022.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D. W., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Babuschkin, I., Balaji, S. A., Jain, S., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M. M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021.\\n\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\n\\nChristopoulou, F., Lampouras, G., Gritta, M., Zhang, G., Guo, Y., Li, Z.-Y., Zhang, Q., Xiao, M., Shen, B., Li, L., Yu, H., Yan, L., Zhou, P., Wang, X., Ma, Y., Iacobacci, I., Wang, Y., Liang, G., Wei, J., Jiang, X., Wang, Q., and Liu, Q. Pangu-coder: Program synthesis with function-level language modeling. ArXiv, abs/2207.11280, 2022.\\n\\nChung, H. W., Garcia, X., Roberts, A., Tay, Y., Firat, O., Narang, S., and Constant, N. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=kXwdL1cWOAi.\\n\\nClement, C., Drain, D., Timcheck, J., Svyatkovskiy, A., and Sundaresan, N. PyMT5: multi-mode translation of natural language and python code with transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9052\u20139065, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.728. URL https://aclanthology.org/2020.emnlp-main.728.\\n\\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm\u00e1n, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. Unsupervised cross-lingual representation learning at scale. In Annual Meeting of the Association for Computational Linguistics, 2019.\\n\\nFeng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., and Zhou, M. CodeBERT: A pre-trained model for programming and natural languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1536\u20131547, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.139. URL https://aclanthology.org/2020.findings-emnlp.139.\\n\\nFried, D., Aghajanyan, A., Lin, J., Wang, S. I., Wallace, E., Shi, F., Zhong, R., tau Yih, W., Zettlemoyer, L., and Lewis, M. Incoder: A generative model for code infilling and synthesis. ArXiv, abs/2204.05999, 2022.\\n\\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and Steinhardt, J. Measuring coding challenge competence with APPS. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=sD93GOzH3i5.\\n\\nHusain, H., Wu, H., Gazit, T., Allamanis, M., and Brockschmidt, M. Codesearchnet challenge: Evaluating the state of semantic code search. ArXiv, abs/1909.09436, 2019.\\n\\nKocetkov, D., Li, R., Allal, L. B., Li, J., Mou, C., Ferrandis, C. M., Jernite, Y., Mitchell, M., Hughes, S., Wolf, T., et al. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022.\\n\\nKudo, T. and Richardson, J. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\\n\\nLai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, S., Fried, D., yi Wang, S., and Yu, T. Ds-1000: A natural and reliable benchmark for data science code generation. ArXiv, abs/2211.11501, 2022.\\n\\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Tom, Eccles, Keeling, J., Gimeno, F., Lago, A. D., Hubert, T., Choy, P., de, C., d'Autume, M., Babuschkin, I., Chen, X., Huang, P.-S., Welbl, J., Gowal, S., Alexey, Cherepanov, Molloy, J., Mankowitz, D. J., Robson, E. S., Kohli, P., de, N., Freitas, Kavukcuoglu, K., and Vinyals, O. Competition-level code generation with alphacode. Science, 378:1092 \u2013 1097, 2022.\"}"}
{"id": "orlanski23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Measuring the Impact of Programming Language Distribution\\n\\nLu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C. B., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K., Fu, S., and Liu, S. Codexglue: A machine learning benchmark dataset for code understanding and generation. ArXiv, abs/2102.04664, 2021.\\n\\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., and Xiong, C. A conversational paradigm for program synthesis. arXiv preprint arXiv:2203.13474, 2022.\\n\\nOrlanski, G. and Gittens, A. Reading stackoverflow encourages cheating: Adding question text improves extractive code generation. ArXiv, abs/2106.04447, 2021.\\n\\nOrlanski, G., Yang, S., and Healy, M. Evaluating how fine-tuning on bimodal data effects code generation. ArXiv, abs/2211.07842, 2022.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020.\\n\\nRoberts, A., Chung, H. W., Levskaya, A., Mishra, G., Bradbury, J., Andor, D., Narang, S., Lester, B., Gaffney, C., Mohiuddin, A., Hawthorne, C., Lewkowycz, A., Salcianu, A., van Zee, M., Austin, J., Goodman, S., Soares, L. B., Hu, H., Tsvyashchenko, S., Chowdhery, A., Bastings, J., Bulian, J., Garcia, X., Ni, J., Chen, A., Kenealy, K., Clark, J. H., Lee, S., Garrette, D., Lee-Thorp, J., Raffel, C., Shazeer, N., Ritter, M., Bosma, M., Passos, A., Maitin-Shepard, J., Fiedel, N., Omernick, M., Saeta, B., Sepassi, R., Spiridonov, A., Newlan, J., and Gers, A. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022. URL https://arxiv.org/abs/2203.17189.\\n\\nRoziere, B., Lachaux, M.-A., Chanussot, L., and Lample, G. Unsupervised translation of programming languages. Advances in Neural Information Processing Systems, 33, 2020.\\n\\nRoziere, B., Lachaux, M.-A., Szafraniec, M., and Lample, G. Dobf: A deobfuscation pre-training objective for programming languages. In Neural Information Processing Systems, 2021.\\n\\nSchuster, T., Kalyan, A., Polozov, A., and Kalai, A. T. Programming puzzles. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. URL https://openreview.net/forum?id=fe_hCc4RBrg.\\n\\nShazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 4596\u20134604. PMLR, 2018.\\n\\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D., Schuster, T., Zheng, H. S., Houlsby, N., and Metzler, D. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022.\\n\\nWang, S., Li, Z., Qian, H., Yang, C., Wang, Z., Shang, M., Kumar, V., Tan, S., Ray, B., Bhatia, P., Nallapati, R., Ramanathan, M. K., Roth, D., and Xiang, B. Recode: Robustness evaluation of code generation models. 2022a.\\n\\nWang, X., Tsvetkov, Y., and Neubig, G. Balancing training for multilingual neural machine translation. arXiv preprint arXiv:2004.06748, 2020.\\n\\nWang, Y., Wang, W., Joty, S., and Hoi, S. C. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8696\u20138708, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.685. URL https://aclanthology.org/2021.emnlp-main.685.\\n\\nWang, Z., Cuenca, G., Zhou, S., Xu, F. F., and Neubig, G. Mconala: A benchmark for code generation from multiple natural languages. ArXiv, abs/2203.08388, 2022b.\\n\\nYasunaga, M. and Liang, P. Break-it-fix-it: Unsupervised learning for program repair. In International Conference on Machine Learning (ICML), 2021.\\n\\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig, G. Learning to mine aligned code and natural language pairs from stack overflow. 2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR), pp. 476\u2013486, 2018.\"}"}
{"id": "orlanski23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. BabelCode Design\\n\\nFigure 8. Sample problem translated from Python to C++ using BabelCode\\n\\nBabelCode\u2019s design shares many similarities to Athiwaratkun et al. (2023) and Cassano et al. (2022). For translation, we too implement a recursive visitor pattern to translate input and output values to the corresponding code in the target language. When converting a coding dataset, we follow prior works by parsing `assert` statements using AST parsing libraries to determine the inputs and outputs for a given question. To find the function name for a problem, we once again use AST parsers to find the function definition located in the ground truth solution. The found tree is additionally used for parsing the argument names and types. If the types for either the arguments or returns do not exist, we infer them based on the types found from the literal values of the inputs and outputs. While our implementation differs, the overall process is similar to Athiwaratkun et al. (2023) and Cassano et al. (2022). Following Cassano et al. (2022), we execute the generated code through the command line using each language\u2019s recommended commands to compile and run a given script. As Athiwaratkun et al. (2023) is not open sourced, we cannot compare the similarities of this portion.\\n\\nB. Dataset Changes\\n\\nB.1. Incompatible Problems\\n\\n```python\\ndef encode_cyclic(s: str):\\n    \\n    def encode_cyclic(s):\\n        groups = [\\n            s[(3 * i):min((3 * i + 3), len(s))] for i in range((len(s) + 2) // 3)\\n        ]\\n        \\n        def encode_cyclic(s):\\n            groups = [(group[1:] + group[0]) if len(group) == 3 else group for group in groups]\\n            return \"\".join(groups)\\n        \\n```\\n\\n```python\\ndef decode_cyclic(s: str):\\n    \\n    def decode_cyclic(s):\\n        return encode_cyclic(encode_cyclic(s))\\n    \\n```\\n\\n```python\\nfrom random import randint, choice\\nimport string\\n\\nletters = string.ascii_lowercase\\n\\nfor _ in range(100):\\n    str = ''.join(choice(letters) for i in range(randint(10, 20)))\\n    encoded_str = encode_cyclic(str)\\n    assert decode_cyclic(encoded_str) == str\\n```\"}"}
{"id": "orlanski23a", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 15. Number of Questions passed for Transcoder\\n\\nThere are a total of 524 questions, and \\\\( N \\\\) represents the source language. \\\\( S \\\\) is the size of the model, and \\\\( D \\\\) is the distribution it was trained on. \\\\( P \\\\) is the PaLM distribution while \\\\( PC \\\\) is the PaLM-Coder distribution.\\n\\nLanguages are sorted from high to low resource. Green values are the best values for that language, while red values are the worst.\\n\\n| Language | \\\\( N \\\\) | \\\\( S \\\\) | \\\\( D \\\\) |\\n|----------|--------|--------|--------|\\n| C++      | 191    | 225    | 197    |\\n| Java     | 449    | 457    | 434    |\\n| Py       | 205    | 233    | 190    |\\n| U1       | 408    | 427    | 420    |\\n| U2       | 152    | 100    | 98     |\\n| U3       | 195    | 196    | 152    |\\n| U4       | 417    | 430    | 397    |\\n| C#       | 278    | 245    | 295    |\\n| R        | 380    | 385    | 402    |\\n| Julia    | 417    | 431    | 431    |\\n| HS       | 383    | 412    | 304    |\\n| Go       | 192    | 291    | 270    |\\n| Dart     | 280    | 314    | 336    |\\n| Lua      | 421    | 459    | 463    |\\n| Rust     | 279    | 257    | 246    |\\n| PHP      | 424    | 445    | 327    |\\n| JS       | 389    | 424    | 396    |\\n| 8B       | 195    | 196    | 152    |\\n\\n### Alternative distributions\\n\\n| Language | \\\\( N \\\\) | \\\\( S \\\\) | \\\\( D \\\\) |\\n|----------|--------|--------|--------|\\n| C++      | 143    | 100    | 112    |\\n| Java     | 448    | 446    | 446    |\\n| Py       | 278    | 245    | 295    |\\n| U1       | 242    | 202    | 224    |\\n| U2       | 285    | 218    | 311    |\\n| U3       | 225    | 218    | 224    |\\n| U4       | 260    | 190    | 247    |\\n| C#       | 278    | 245    | 295    |\\n| R        | 424    | 416    | 396    |\\n| Julia    | 435    | 434    | 428    |\\n| HS       | 415    | 414    | 363    |\\n| Go       | 283    | 253    | 352    |\\n| Dart     | 424    | 407    | 451    |\\n| Lua      | 441    | 462    | 454    |\\n| Rust     | 441    | 462    | 454    |\\n| PHP      | 424    | 416    | 396    |\\n| JS       | 415    | 414    | 363    |\\n| 8B       | 283    | 253    | 352    |\\n\\n**Notes:**\\n- The distribution of the model was trained on.\\n- Green values are the best values for that language, while red values are the worst.\"}"}
{"id": "orlanski23a", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 16. Metrics for HR languages on BC-HumanEval for all models. \u2206 is the mean change of each of the displayed languages when compared to the natural. % Failed tests is the percent of predictions that did not have any errors, but failed a test. % Error is the percent of predictions that had either a runtime or compilation error. % Timed Out is the percent of predictions that timed out. The time out was set to 10 for all languages except for Java and TS, which was 15. % Passed is the percent of predictions that passed all test cases. % Passed One is the percent of predictions that passed at least one test case, but failed. % Tests Passed is the mean percent of test cases passed per problem for all predictions.\\n\\n| Metric          | D  | Java | Py   | C++  | PHP  | TS   | JS   | Go  |\\n|-----------------|----|------|------|------|------|------|------|-----|\\n| % Error         |    | 25.32| 19.36| 17.80| 8.61 | 21.53| 11.66| 49.02|\\n| % Failed Test   |    | 28.85| 17.45| 19.83| 10.25| 21.53| 11.00| 47.23|\\n| % Passed        |    | 13.45| 14.60| 12.70| 10.12| 11.71| 12.29| 8.15 |\\n| % Passed One    |    | 47.26| 46.20| 43.77| 46.70| 45.32| 49.87| 28.82|\\n| % Tests Passed  |    | 33.46| 33.77| 31.07| 28.84| 30.71| 32.78| 20.03|\\n| % Timed Out     |    | 1.29 | 0.93 | 4.57 | 2.82 | 2.84 | 1.45 | 0.80 |\\n\\nTable 17. Metrics for LR languages on BC-HumanEval for all models. \u2206 is the mean change of each of the displayed languages when compared to the natural. % Failed tests is the percent of predictions that did not have any errors, but failed a test. % Error is the percent of predictions that had either a runtime or compilation error. % Timed Out is the percent of predictions that timed out. The time out was set to 10 for all languages except for Java and TS, which was 15. % Passed is the percent of predictions that passed all test cases. % Passed One is the percent of predictions that passed at least one test case, but failed. % Tests Passed is the mean percent of test cases passed per problem for all predictions.\\n\\n| Metric          | D  | Dart | Lua  | Rust | C#   | R    | Julia| HS  |\\n|-----------------|----|------|------|------|------|------|------|-----|\\n| % Error         |    | 62.06| 31.31| 51.61| 43.80| 70.08| 68.90| 85.70|\\n| % Failed Test   |    | 28.71| 57.98| 38.51| 45.42| 26.66| 25.72| 11.67|\\n| % Passed        |    | 8.74 | 8.60 | 8.74 | 9.94 | 2.99 | 4.75 | 1.81 |\\n| % Passed One    |    | 25.51| 40.39| 28.48| 36.26| 15.62| 25.07| 8.29 |\\n| % Tests Passed  |    | 19.43| 24.31| 20.53| 25.49| 8.70 | 13.79| 5.13 |\\n| % Timed Out     |    | 0.49 | 2.10 | 1.13 | 0.85 | 0.28 | 0.63 | 0.82 |\"}"}
{"id": "orlanski23a", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 18. Metrics for HR languages on TP3 for all models. \u2206 is the mean change of each of the displayed languages when compared to the natural.\\n\\n% Failed tests is the percent of predictions that did not have any errors, but failed a test. % Error is the percent of predictions that had either a runtime or compilation error. % Timed Out is the percent of predictions that timed out. The time out was set to 10 for all languages except for Java and TS, which was 15. % Passed is the percent of predictions that passed all test cases. % Passed One is the percent of predictions that passed at least one test case, but failed. % Tests Passed is the mean percent of test cases passed per problem for all predictions.\\n\\n| Metric         | Dart | Lua  | Rust | C#  | Julia | HS  | \u2206   |\\n|----------------|------|------|------|-----|-------|-----|-----|\\n| % Error        | 90.12| 93.45| 84.47| 80.85| 97.34 | 89.43| 89.60|\\n| % Failed Test  | 4.78 | 5.42 | 11.54| 13.10| 2.00  | 4.23 | 7.96 |\\n| % Passed       | 5.07 | 0.94 | 3.77 | 5.87 | 0.62  | 3.51 | 1.31 |\\n| % Passed One   | 5.28 | 5.51 | 11.77| 14.87| 0.95  | 7.15 | 7.83 |\\n| % Tests Passed | 7.76 | 3.55 | 9.59 | 13.23| 1.10  | 6.87 | 5.18 |\\n| % Timed Out    | 0.02 | 0.20 | 0.22 | 0.18 | 0.03  | 2.82 | 1.12 |\\n\\nTable 19. Metrics for LR languages on TP3 for all models. \u2206 is the mean change of each of the displayed languages when compared to the natural.\\n\\n% Failed tests is the percent of predictions that did not have any errors, but failed a test. % Error is the percent of predictions that had either a runtime or compilation error. % Timed Out is the percent of predictions that timed out. The time out was set to 10 for all languages except for Java and TS, which was 15. % Passed is the percent of predictions that passed all test cases. % Passed One is the percent of predictions that passed at least one test case, but failed. % Tests Passed is the mean percent of test cases passed per problem for all predictions.\\n\\n| Metric         | Dart | Lua  | Rust | C#  | Julia | HS  | \u2206   |\\n|----------------|------|------|------|-----|-------|-----|-----|\\n| % Error        | 90.12| 93.45| 84.47| 80.85| 97.34 | 89.43| 89.60|\\n| % Failed Test  | 4.78 | 5.42 | 11.54| 13.10| 2.00  | 4.23 | 7.96 |\\n| % Passed       | 5.07 | 0.94 | 3.77 | 5.87 | 0.62  | 3.51 | 1.31 |\\n| % Passed One   | 5.28 | 5.51 | 11.77| 14.87| 0.95  | 7.15 | 7.83 |\\n| % Tests Passed | 7.76 | 3.55 | 9.59 | 13.23| 1.10  | 6.87 | 5.18 |\\n| % Timed Out    | 0.02 | 0.20 | 0.22 | 0.18 | 0.03  | 2.82 | 1.12 |\"}"}
{"id": "orlanski23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.2. Changes To HumanEval\\n\\nOriginal:\\n\\n```python\\ndef reverse_delete(s, c):\\n    \"\"\" Task\\n    We are given two strings s and c, you have to deleted all the characters in s that are equal to any character in c then check if the result string is palindrome.\\n    A string is called palindrome if it reads the same backward as forward.\\n    You should return a tuple containing the result string and True/False for the check.\\n    Example\\n    For s = \\\"abcde\\\", c = \\\"ae\\\", the result should be ('bcd', False)\\n    For s = \\\"abcdef\\\", c = \\\"b\\\" the result should be ('acdef', False)\\n    For s = \\\"abcdedcba\\\", c = \\\"ab\\\", the result should be ('cdedc', True)\\n    \"\"\"\\n    s = ''.join([char for char in s if char not in c])\\n    return (s, s[::-1] == s)\\n```\\n\\nExample:\\n- `reverse_delete('abcde', 'ae')` returns `('bcd', False)`\\n- `reverse_delete('abcdef', 'b')` returns `('acdef', False)`\\n- `reverse_delete('abcdedcba', 'ab')` returns `('cdedc', True)`\\n\\nModified:\\n\\n```python\\ndef reverse_delete(s, c):\\n    \"\"\" Task\\n    We are given two strings s and c, you have to deleted all the characters in s that are equal to any character in c then check if the result string is palindrome.\\n    A string is called palindrome if it reads the same backward as forward.\\n    You should return a two element list containing the result string and \\\"True\\\" if the check passed, otherwise \\\"False\\\".\\n    Example\\n    For s = \\\"abcde\\\", c = \\\"ae\\\", the result should be ('bcd', False)\\n    For s = \\\"abcdef\\\", c = \\\"b\\\" the result should be ('acdef', False)\\n    For s = \\\"abcdedcba\\\", c = \\\"ab\\\", the result should be ('cdedc', True)\\n    \"\"\"\\n    s = ''.join([char for char in s if char not in c])\\n    return [s, str(s[::-1] == s)]\\n```\\n\\nExample:\\n- `reverse_delete('abcde', 'ae')` returns `['bcd', 'False']`\\n- `reverse_delete('abcdef', 'b')` returns `['acdef', 'False']`\\n- `reverse_delete('abcdedcba', 'ab')` returns `['cdedc', 'True']`\\n\\nB.3. Changes To Transcoder\\n\\nOriginal:\\n\\n```c\\nint difference_between_highest_and_least_frequencies_in_an_array ( int arr[], int n ) {\\n    sort ( arr, arr + n );\\n    int count = 0, max_count = 0, min_count = n;\\n    for ( int i = 0; i < ( n - 1 ); i ++ ) {\\n        if ( arr [ i ] == arr [ i + 1 ] ) {\\n            count += 1;\\n            continue;\\n        }\\n        else {\\n            max_count = max ( max_count, count );\\n            min_count = min ( min_count, count );\\n            count = 0;\\n        }\\n    }\\n}\\n```\"}"}
{"id": "orlanski23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def sat(inds: List[int], string):\\n    return inds == sorted(inds) and ''.join((string[i] for i in inds)) == 'intelligent'\\n\\nassert sat([-10, -5, -1, 0, 2, 2, 3, 4, 7, 8, 12], 'enlightenment') == True\\nassert sat([-11, -10, -8, -6, -4, -4, -3, -2, -1, 1, 3], 'inntGetlige') == True\\nassert sat([-10, -5, -1, 0, 2, 2, 3, 4, 7, 8, 12], ' einliJSgeteq ne CAlti') == False\"}"}
{"id": "orlanski23a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D. Training Objective\\n\\nThis paper uses a variant of the UL2 objective (Tay et al., 2022) for training the code language models. The UL2 objective consists of a mixture of span corruption and prefix language modeling objectives, as defined in Raffel et al. (2020). In this work, we select two span corruption instances using the implementation provided in the T5 library. The only differences between these two instances consist of different values for the noise density and mean noise span length arguments. In particular, we use (3.0, 0.15) and (32, 0.5) for the (noise density, mean noise span length) arguments for each span corruption instance respectively.\\n\\nThe prefix language modeling objective randomly breaks text into two pieces, and the model is tasked to reconstruct the latter, given the former. Finally, we add an additional objective which consists of causal language modeling, which can be considered a special case of prefix language modeling; the first piece consists of the empty string. We assign the probabilities 10%, 10%, 20%, and 60% for each objective, respectively.\\n\\nE. Prompts Used\\n\\nE.1. Generation Tasks\\n\\n1. You are an expert {{Language}} programmer, complete the implementation.\\n\\n2. Solution in {{Language}}:\\n\\n3. [BEGIN]\\n\\n4. {{Signature With Docstring}}\\n\\nEach {{...}} represents a field that is filled in.\\n\\nExample from HumanEval for generating C# code:\\n\\n1. You are an expert C# programmer, complete the implementation.\\n\\n2. Solution in C#:\\n\\n3. [BEGIN]\\n\\n4. ```\\n\\nclass Solution {\\n\\n  /**\\n   * Return length of given string\\n   * >>> GetStringLength(\"\")\\n   * 0\\n   * >>> GetStringLength(\"abc\")\\n   * 3\\n   */\\n\\n  public int GetStringLength(string s) {\\n\\n```\"}"}
{"id": "orlanski23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def sat(i: int) -> bool:\\n    return i % 123 == 4 and i > 10 ** 10\\n\\nHaskell Translation:\\n\\nsat :: Integer -> Bool\\nsat i =\"}"}
{"id": "orlanski23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Hyperparameters for models trained (BC) compared with those used to train PaLM-Coder (PC). For PaLM-Coder, we report the number of code tokens trained on. Each BC model is trained on each of the naturally occurring distributions of the GitHub data and each of the distributions is detailed in section 3 where $N \\\\in \\\\{1, 2, 3, 4\\\\}$.\\n\\n| Model  | Layers | Heads | Tokens | Train Model |\\n|--------|--------|-------|--------|-------------|\\n| BC 1B  | 16     | 8     | 8192   | 20.2        |\\n| BC 2B  | 24     | 16    | 10240  | 40.4        |\\n| BC 4B  | 26     | 16    | 14336  | 100         |\\n| PC 8B  | 32     | 16    | 4096   | 46.8        |\\n| PC 62B | 64     | 32    | 8192   | 46.8        |\\n\\nwith an additional causal language modeling objective as described in Appendix D.\\n\\n4.2. Training Data\\nOur curated source code corpus was obtained by collecting publicly available code data on the web using a custom code data collection system. We apply a similar license filter as Kocetkov et al. (2022) to remove any files with non-permissible licenses, use simple heuristics to filter out low-quality code and apply near-deduplication to obtain our corpus of high-quality, permissive source code. After preprocessing, we select 14 programming languages by their file extensions according to the mapping used by GitHub's Linguist library to segment the dataset by language. To calculate the number of examples per language, we use SeqIO's caching feature and take the number of examples after post-processing (Roberts et al., 2022). We list the percentages of all examples and file extensions used per language in Appendix C. With these numbers, we consider the top 7 languages to be high-resource (HR): Java, Python, C++, PHP, TypeScript, JavaScript, and Go. We further consider the bottom 7 languages to be low-resource (LR): Dart, Lua, Rust, C#, R, Julia, and Haskell.\\n\\n4.3. Vocabulary\\nThe original PaLM (Chowdhery et al., 2022) vocabulary focuses on multilingual natural language. In contrast, we trained our SentencePiece (Kudo & Richardson, 2018) vocabulary with 64k tokens from the training data directly. Each programming language is uniformly sampled to build the vocabulary. In previous works, such as Chen et al. (2021), a list of tokens that consists of a different number of whitespace is manually added to represent code more efficiently. In our work, we rely on the SentencePiece model to learn the whitespace tokens by allowing extra whitespace tokens and whitespace-only tokens. In the end, the model can represent up to 12 whitespaces into one token. In addition, numbers are split into individual tokens.\\n\\n4.4. Benchmarks\\nBabelCode currently supports 4 datasets. To allow the translation of any dataset to any language, we modify each benchmark as well as remove problems that were incompatible. These changes are described in Appendix B. For HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and Transcoder (Roziere et al., 2020), we add the prefix BabelCode- (BC) to indicate that we are using the BabelCode specific version. Further, for Transcoder, we use the same version as in Chowdhery et al. (2022). BC-HumanEval (BC-HE) has 161 out of the original 164 HumanEval questions. BC-MBPP has 855 of the original 999 questions. BC-Transcoder (BC-TC) has 524 of the original 560 questions.\\n\\nWe additionally introduce a new dataset called Translating Python Programming Puzzles (TP3). We take the verification functions from the questions in the original Python Programming Puzzles dataset (Schuster et al., 2021) to create this dataset. These functions are hand-crafted by the authors and are used to check if an answer satisfies the constraints of the puzzle. These puzzles range in difficulty from basic character checking to competitive programming problems. Thus, each verification function is written by an expert Python programmer and requires a significant understanding of programming to translate. In total, there are 370 Python functions to translate. Examples from TP3 can be found in subsection B.4.\\n\\n4.5. Evaluation\\nFor BC-HumanEval, we follow Chen et al. (2021) and generate 200 programs per problem. Further, we use a zero-shot prompt described in subsection E.1. We use the built-in docstring translation of BabelCode. We generate 50 programs per problem on our three translation tasks and use the prompts described in subsection E.2. We consider these prompts zero-shot as we do not provide any additional examples. However, we provide the translated signature without the docstring in the prompt. We do not consider this to be data leakage as it is trivial to translate signatures with libraries such as Treesitter.\\n\\nFor every dataset, we use $T = 0.8$, $top_p = 0.95$, and do not use $top_k$. We use the pass@k estimator (Chen et al., 2021) to measure the performance. We use $k = 100$ and $k = 25$ for generation and translation, respectively.\"}"}
{"id": "orlanski23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. Results\\n\\n5.1. Baseline Models\\n\\nWe report the baseline results for our trained models and PaLM-Coder in Figure 4. On BC-HumanEval, we find that the 2B model has a better \\\\( \\\\text{pass@100} \\\\) than that of PaLM-Coder 8B on all but C# and Python. On average, the BC-2B model trained on the natural distribution of GitHub data has average improvements of 48.17% compared to PaLM-Coder 8B despite having a quarter of the number of parameters and training on 6.4B fewer code tokens. Further, we find that the 4B model outperforms PaLM-Coder 62B on 6 of the 14 languages evaluated. This likely results from the 4B model seeing over 53B tokens more than what PaLM-Coder 62B did. Another likely factor in this discrepancy is that the data PaLM-Coder was fine-tuned on included all languages on GitHub in contrast to our filtered training dataset.\\n\\nWe also observe that performance on languages do not scale with respect to their resource level nor the model's size. C#, Dart, Julia, and Haskell have significantly higher gains when scaling to 4B model size when compared to the other languages. While this may be due to the increased number of training tokens, it is not consistent across all LR languages as the increase in performance for R and Lua when scaling from 1B to 2B is similar to that when scaling from 2B to 4B. Instead, this result is likely due to better transfer from languages such as Java, Python, and C++.\\n\\nThe importance of scale for multi-lingual code models is further demonstrated by the results of the translation tasks. We find that in BC-TP3, the 1B and 2B models' performance is similar. However, the most significant gains are from scaling up to 4B where it beats PaLM-Coder 8B on all but three languages in this zero-shot translation. We do make note, though, that while we do not provide any examples for in-context learning, we do provide the signature in the target language during generation. This finding is less pronounced in BC-Transcoder as the scaling observed in all languages is more akin to that seen in BC-HumanEval.\\n\\n5.2. Impact of Balancing Programming Languages\\n\\nFigure 5 shows the mean \\\\( \\\\text{pass@k} \\\\) scores of different models trained on each of the 5 distributions for each of the 4 datasets. As expected, the natural distribution is optimal if the focus is solely HR languages as the performance losses when training on Unimax balanced data are 15.47%, 14.00%, and 9.35% for the 1B, 2B, and 4B models, respectively. However, for any LR language, Unimax is clearly better given that there is an average \\\\( \\\\text{pass@100} \\\\) improvement on these languages of 111.85%, 68.38%, and 19.22% for the 1B, 2B, and 4B size models, respectively. For generation tasks, we find that \\\\( N = 3 \\\\) is optimal with respect to the difference between performance gained on LR and performance lost on HR languages. On the 1B, 2B, and 4B models, the ones trained on the Unimax 3 dataset had differences of 130.17%, 87.80%, and 36.00%, respectively. We observe similar scaling trends on TP3, as training on a Unimax distribution yielded average \\\\( \\\\text{pass@25} \\\\) improvements to LR languages of 124.45% for the 1B model, 74.41% for the 2B model, and 35.81% for the 4B model.\"}"}
{"id": "orlanski23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Effects of scale on the average $\\\\overline{\\\\text{pass}}_k$ of the high and low resource languages for each of four datasets. Full tabulated results are located in Appendix F.\\n\\nFigure 6. Mean relative difference of $\\\\overline{\\\\text{pass}}_k$ for each of the models trained on the different Unimax distributions compared to the $\\\\overline{\\\\text{pass}}_k$ of the same sized model trained on the Natural distribution. The X-axis is the language sorted from high to low resource. HS is Haskell and Py is Python. The percent changes for each delta for HR languages are shown in Table 12 and Table 13 for LR languages.\\n\\n64.51% for the 2B model, and 51.29% for the 4B model when compared to the same sized models trained on the natural distribution. Unlike BC-HumanEval, training the 4B on Unimax Distributions yielded better average HR performance with an increase of 6.80%. As shown in Figure 6, training a 4B model on the Unimax 2 distribution had a mean $\\\\overline{\\\\text{pass}}_{25}$ improvement of 71.59% in LR languages and an improvement of 20.31% on HR languages when compared to the natural distribution. Training on other Unimax distributions does not see as large of improvements. For the 4B model, we find mean LR improvements of 42.39%, 52.91%, and 38.26% when trained on the Unimax 1, 3, and 4 distributions, respectively. This indicates that for TP3, at least, balancing the training data for each language improves translation capabilities. However, less Python data adversely affects understanding the source code necessary to translate it properly.\\n\\nWhen evaluated on BC-Transcoder, we find that LR performance increased with size. When the source language is C++, training on the Unimax distributions yielded an average $\\\\overline{\\\\text{pass}}_{25}$ improvements of 7.57%, 6.76%, and 11.80% for the 1B, 2B, and 4B models, respectively. Translating Python to other languages followed this trend with an average change of -26.04%, 15.1%, and 22.47% for the 1B, 2B, and 4B models, respectively. On BC-Transcoder, we find similar benefits when translating from Python to other languages, although the performance on higher resource languages is significantly worse. When translating from C++ to other languages, we find that training both a 1B and 2B model on the UM 4 distribution improves performance on 5 of the 7 LR languages. For 4B sized models, the UM 2 distribution is optimal as LR performance increased by an average of 20.47% when compared to training on the natural distribution. As the source code of BC-Transcoder...\"}"}
{"id": "orlanski23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"focuses on language-agnostic algorithm implementations, this scaling trend is most likely due to the importance of a surface-level understanding of the target language. Further, the fact that this trend does not appear for BC-HumanEval or TP3 indicates that neither model size nor duplication of language data enables the model to have a deep understanding of these low-resource languages.\\n\\n5.3. Effects Of Language Balance on Predictions\\n\\nWe find that, as is expected, decreasing the number of tokens for a language negatively impacts its performance on that language. To compare the overall effects of language balancing at each size, we focus on the Unimax 1 and Unimax 2 distributions as they represent the largest change in proportions of HR languages when compared to the Natural distribution. Figure 7 shows that on BC-HumanEval, training on either UM 1 or UM 2 will cause the model to generate fewer correct solutions than when the model is trained on the Natural distribution with respect to HR languages. However, this is not due to those models generating more programs with either compilation or run-time errors as the raw average increase is only 0.40 and 1.15 for the models trained on the Unimax 1 and Unimax 2 respectively. Rather, we find that the largest decrease is in the mean % test cases passed per problem. Training on the Unimax 1 and Unimax 2 distributions results in 5.50% and 9.09% fewer test cases respectively when compared to the model trained on the natural distribution.\\n\\nOn LR languages, the Unimax 1 distribution yielded the best improvements compared to the other distributions. Specifically, the programs generated by the model trained on the Natural distribution passed, on average, 5.13% of the test cases per problem. In comparison, 9.53% and 10.48% of average test cases per problem were solved by the models trained on the Unimax 1 and Unimax 2 distributions. The less than 1% improvement when going from Unimax 1 to Unimax 2 suggests that, for generation tasks, multi-lingual models of code benefit the most from seeing unique data. In our translation task of TP3, we observe consistent improvements in the mean number of test cases passed for both HR and LR languages. For the former, we observe an average improvement of 2.58% and 3.06% compared to the Natural distribution for the UM 1 and 2 distributions respectively. On LR languages, we find average improvements of 3.40% and 4.99% over the Natural distribution for the UM 1 and UM 2 distributions respectively. These results, along with the performance improvements discussed in subsection 5.2, indicate that translation tasks benefit highly from uniformly balanced languages. This is, likely, due to the task formulation where natural language understanding is not necessary. Higher resource languages are more likely to contain diverse natural language and code pairs due to the language's popularity. Thus, performance on NL2Code tasks, such as BC-HumanEval, depends on the unique samples of code and doc-strings in the training corpus. Translation, on the other hand, does not have this constraint. Rather, it appears that uniformly balancing languages is the optimal strategy for this task.\\n\\n6. Related Works\\n\\nCode Evaluation\\n\\nExisting code benchmarks have primarily focused on surface matching evaluation (Lu et al., 2021; Yin et al., 2018; Wang et al., 2022b; Husain et al., 2019). Recent works have introduced new execution-based benchmarks for both generation (Austin et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Lai et al., 2022) and repair (Yasunaga & Liang, 2021) tasks, however, these have been limited to only Python. Additional works have introduced generation (Li et al., 2022) and translation (Roziere et al., 2020) tasks in multiple-languages, but are limited to only C++, Java, and Python. We acknowledge concurrent works by Cassano et al. (2022) and Athiwaratkun et al. (2023) on translating HumanEval and MBPP into multiple programming languages. As we note in subsection 2.2, BabelCode supports deeper analysis on a wider range of tasks while including significant methods for ensuring correctness.\\n\\nCode LLMs\\n\\nRecent years has seen significant interest in LLMs for code. CodeBERT (Feng et al., 2020) is the first work to train an encoder only model on code. CodeT5 (Wang et al., 2021), PLBART (Ahmad et al., 2021), and additional works (Clement et al., 2020; Orlanski & Gittens, 2021; Chakraborty et al., 2022) examine training encoder-decoder models on code. Similar to this work, Ahmad et al. (2021) investigate difference data balancing strategies for pre-training. Our work differs in that we focus on balancing many programming languages in pre-training data. AlphaCode (Li et al., 2022), Codex (Chen et al., 2021), PaLM (Chowdhery et al., 2022), and other works (Nijkamp et al., 2022; Fried et al., 2022; Allal et al., 2023; Christopoulou et al., 2022) have shown that decoder-only code language models achieve exceptional performance on a wide range of tasks. Additional works have investigated different training strategies (Roziere et al., 2020; Bavarian et al., 2022) and different pre-training data (Roziere et al., 2021; Orlanski et al., 2022; Austin et al., 2021).\\n\\nLanguage Balancing\\n\\nChoosing a proper sampling distribution from a mixture of datasets of various size is a difficult problems. Initial attempts at studying this in the multilingual natural language processing literature relied on temperature-based approaches (Conneau et al., 2019; Arivazhagan et al., 2019). These approaches oversample the low-resource tasks and downsample the high-resource ones. Other works have\\n\\n8\"}"}
{"id": "orlanski23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Measuring the Impact of Programming Language Distribution\\n\\nGabriel Orlanski\\nKefan Xiao\\nXavier Garcia\\nJeffrey Hui\\nJoshua Howland\\nJonathan Malmaud\\nJacob Austin\\nRishabh Singh\\nMichele Catasta\\n\\nAbstract\\n\\nCurrent benchmarks for evaluating neural code models focus on only a small subset of programming languages, excluding many popular languages such as Go or Rust. To ameliorate this issue, we present the BabelCode framework for execution-based evaluation of any benchmark in any language. BabelCode enables new investigations into the qualitative performance of models' memory, runtime, and individual test case results. Additionally, we present a new code translation dataset called Translating Python Programming Puzzles (TP3) from the Python Programming Puzzles (Schuster et al., 2021) benchmark that involves translating expert-level python functions to any language. With both BabelCode and the TP3 benchmark, we investigate if balancing the distributions of 14 languages in a training dataset improves a large language model's performance on low-resource languages. Training a model on a balanced corpus results in, on average, 12.34% higher pass@k across all tasks and languages compared to the baseline. We find that this strategy achieves 66.48% better pass@k on low-resource languages at the cost of only a 12.94% decrease to high-resource languages. In our three translation tasks, this strategy yields, on average, 30.77% better low-resource pass@k while having 19.58% worse high-resource pass@k.\\n\\n1. Introduction\\n\\nIn the 2022 StackOverflow Developer Survey, Rust was the 14th most popular programming language despite not ranking in the survey taken five years prior. However, the 13th most popular language, Go, has nearly doubled Rust's number of StackOverflow questions in this time frame. Further, despite their similar popularity, Go has nearly 350% more source code available (Kocetkov et al., 2022). These disparities highlight the problem that many popular programming languages are starkly low-resource, especially compared to the most popular languages.\\n\\nDespite their impressive generative capabilities, especially in code, Large Language Models (LLM) are adversely impacted by this language resource imbalance. Thus, developers will likely find minimal utility from LLMs if they are not using the extremely popular languages. It is therefore imperative to investigate how to mitigate the discrepancy between a language's popularity and the amount of data available for it. Prior works focusing on code generation (Ahmad et al., 2021) and multilingual natural language processing (Arivazhagan et al., 2019; Conneau et al., 2019) use temperature-based strategies to balance the training languages. Such a strategy duplicates extremely low-resource languages thousands of times, which has been shown to significantly reduce performance (Allamanis, 2019).\\n\\nBeyond the the language balancing strategy, evaluating code LLMs in a multi-lingual setting presents significant challenges. Existing datasets are either mono-lingual (Chen et al., 2021; Austin et al., 2021; Lai et al., 2022) or limited to only a subset of popular programming languages (Roziere et al., 2020). Each problem in these datasets, which we henceforth refer to as a benchmark, contains an input, and a canonical solution along with the test-cases for checking correctness. Creating a new benchmark for each language of interest would require insurmountable engineering and monetary costs. To address both of these problems, we present the BabelCode framework for execution-based evaluation of any benchmark in any language and use it to investigate the impact of programming language distribution on code generation and translation.\\n\\nBabelCode is open-sourced, has an extensive test suite, and supports evaluating four benchmarks in 14 languages. It is designed specifically to enable future research directions such as the evaluation of custom data-structures. BabelCode allows investigation of novel research directions through\"}"}
{"id": "orlanski23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Measuring the Impact of Programming Language Distribution\\n\\nFigure 1. Overview of this work's contributions.\\n\\nthe measurement of memory and runtime usage for a given prediction, as well as the outcomes of individual test cases. Furthermore, we can use BabelCode to build multi-lingual execution based benchmarks from existing mono-lingual datasets. We demonstrate this functionality by creating a new dataset called Translating Python Programming Puzzles (TP3) from the Python Programming Puzzles (Schuster et al., 2021) benchmark, where the objective is to translate expert-level python programs to other languages. The source programs for TP3 are the hand-crafted verification functions for each problem in P3. As the authors hand-wrote each function, they are significantly more complex than the current state-of-the-art code translation benchmarks, such as Transcoder (Roziere et al., 2020), for which code LLMs are already achieving highly impressive results.\\n\\nOur presented framework is closely related to the concurrent work of MBXP (Athiwaratkun et al., 2023) and Multi-PLE (Cassano et al., 2022). While MBXP is quite similar to BabelCode, it is not open-sourced and requires that the input benchmarks be in Python. Multi-PLE is open-sourced, but only supports generation tasks and contains significant errors in multiple languages. BabelCode addresses these issues through an extensive test suite that ensures that the code generated is correct, and that crucial functionality, such as data structure equivalence, works when executed.\\n\\nWith the BabelCode framework, we investigate remedies to the problems of programming language imbalance. We utilize the Unimax algorithm (Chung et al., 2023) to limit the maximum number of times to duplicate a language's data to a constant $N$. We then train 1B, 2B, and 4B parameter decoder-only models on both the natural and Unimax N distributions. We utilize the UL2 (Tay et al., 2022) and causal language modeling training objective. We find that models trained on the balanced dataset significantly outperform the baseline models on low-resource languages across all tasks. Further, we find that the resulting performance drop on high-resource languages is mitigated by increasing the model size.\\n\\nThis paper makes the following key contributions:\\n\\n\u2022 We propose and release BabelCode, a new execution-based evaluation framework that allows for multi-lingual evaluation of code generation and translation capabilities of code language models. It also supports the easy addition of new benchmark tasks and execution-based metrics.\\n\\n\u2022 We show that the code language models trained on the natural distributions of GitHub source code have poor performance on low-resource languages in both generation and translation tasks.\\n\\n\u2022 We propose a new data balancing strategy for programming languages to improve performance on low-resource languages. We demonstrate that the resulting models outperform the baseline models across all tasks by an average of 12.34\\\\% pass $@k$ for all languages, with a further improvement of 39.70\\\\% pass $@k$ to low-resource languages.\\n\\n\u2022 We find that the average improvements on low-resource languages from training on balanced data do not scale with model size. But scaling model sizes significantly helps the average pass $@k$ loss compared to the baselines on high-resource languages going from a loss of 39.70\\\\% with the 1B model to a loss of 2.47\\\\% with the 4B model.\"}"}
{"id": "orlanski23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Differences between BabelCode and prior works. NL2C is natural language to code, while C2C is code to code datasets.\\n\\n| Name          | Sourced Lang. | Support | Mem. & Test | Indiv. Test | Lang. Agnostic |\\n|---------------|---------------|---------|-------------|-------------|----------------|\\n| MultiPL-E     |               | !       | #           | #           | !              |\\n| MBXP          |               | !       | !           | !           | #              |\\n| BabelCode     |               | !       | !           | !           | !              |\\n\\nFigure 2. BabelCode's domain specific language for representing the input and output types of a question. Prior works require that the source dataset be written in Python, while our DSL removes this restriction and allows users to create datasets in any language. This enables seamless additions of new languages while simplifying future expansions to features such as custom data structures.\\n\\nProblem is translated in Figure 8. Overall the key novel elements of BabelCode are: I) the use of a DSL to translate programming questions, II) type-specific equivalence, III) the ability to measure the performance of a given program at a low level (i.e., memory used, runtime, which tests passed), and IV) a large scale test-suite for ensuring correctness of generated code.\\n\\n2.1. Framework Design\\n\\nBabelCode shares many design similarities to the concurrent work from Athiwaratkun et al. (2023). Specifically, we follow the same approach to inferring argument and return types. We follow the respective documentation and tutorials for each language to determine which native types to use. We also use these docs to determine the docstring formatting and naming convention. These mappings are used to generate unit and integration tests for each language automatically. They ensure that each language's implementation is syntactically correct and that, when executed, the equality comparison is correct. We describe the framework design and similarities to Athiwaratkun et al. (2023) in Appendix A.\\n\\nDSL Representations:\\n\\nUsing a DSL in the first phase, we do not force the inputs to be Python, thus enabling more flexibility to represent more generic tasks. For example, given the inputs from two test cases:\\n\\n```\\n{\u201ca\u201d:[[1],[],[80]]}\\n```\\n\\nand\\n\\n```\\n{\u201ca\u201d:[]}\\n```\\n\\nwe only represent the types in our generic DSL. Thus, the resulting type string for this input is `map<string;list<integer>>`. We do not represent the actual values in the generic form as we can easily translate literals across languages. This allows users to create a dataset from any language by requiring that they only represent the types of the inputs and outputs in this generic form. The language agnostic nature of the DSL enables future extensions of BabelCode to incorporate complex inputs and outputs such as custom data-structures. For example, the representation of a node class in a BST could be `BSTNode<integer;integer>`.\\n\\nEquality Checking:\\n\\nWe support floating point equivalence to a precision of $\\\\epsilon = 10^{-6}$ for floats and $\\\\epsilon = 10^{-9}$ for doubles. To determine if a given value is a float or a double, we count the number of digits after the decimal place. We apply this same logic to int and long by counting the total number of digits. Languages such as C# do not, by default, support deep equivalence of data structures. In such cases, we serialize the objects to JSON and check that the resulting strings are equal. Otherwise, we use the language built-in deep equality functionality.\\n\\nTest Statement Execution:\\n\\nWe opt to print the result of each test case (i.e. TEST-0...PASSED) to the standard output in a parseable format across all languages. Along with try-catch blocks, this allows the evaluation of every test case for a given problem. This allows finer analysis of individual programs when compared to using assert statements as it identifies if specific corner cases fail.\\n\\nPrompt Translation:\\n\\nAs Wang et al. (2022a) showed, LLMs are sensitive to the input prompts for code generation. Therefore BabelCode supports prompt translation and construction for multiple different problem formulations. We replace the names of languages, such as Python, with the target language. We use the language-specific naming convention to properly format the signature in the best practice style. If an argument uses a reserved keyword, we append `arg` to its name so that it retains the same meaning but will no longer conflict. We replace Python-specific terms with their equivalent names in the target language. For tasks formulated as code-completion, we support formatting the problem description as a native docstring. We do not translate the import statements in the header. Instead, we exclude the headers from all languages to provide...\"}"}
{"id": "orlanski23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Measuring the Impact of Programming Language Distribution\\n\\na language-agnostic format. Translating prompts to a tar-\\nget language is not novel by itself, as both Athiwaratkun\\net al. (2023) and Cassano et al. (2022) proposed methods\\nto accomplish this. BabelCode's builds on those works by\\ntranslating reserved characters. For example, in Julia, the\\n\\\"$\\\" in docstrings will raise errors if not properly escaped.\\nThus, we implement methods to automatically handle such\\ncases and ensure correctness.\\n\\n2.2. Differences To Prior Works\\n\\nWe summarize the high-level differences between Babel-\\nCode and prior works in Table 1. The MBXP framework\\nfrom Athiwaratkun et al. (2023) is the most similar to our\\nwork as discussed in subsection 2.1. Similar to BabelCode,\\nMBXP does have individual test-case results; however, it\\nuses assert statements and thus can only determine the\\nfirst test-case that fails. MBXP does use language experts to\\nreview the generated code's quality and discuss the valida-\\ntion it supports to ensure that generated code parses and/or\\ncompiles for its respective language. BabelCode also has\\nthis functionality but, additionally, it ensures correctness\\nthrough a test suite that covers the execution of generated\\ncode. We provide scripts to allow validating that source so-\\nlutions to a dataset pass the generated code. For languages\\nthat do not have a solution in the respective dataset, we\\ngenerate \\\"mock\\\" predictions that return the expected output\\ntype. This allows us to ensure that generated code is correct\\nin all supported languages even if no solution exists.\\n\\nThe MultiPL-E framework from Cassano et al. (2022) sup-\\nports 18 languages compared to BabelCode's 16. However,\\nwe support four datasets, while MultiPL-E only currently\\nhas support for two datasets. In addition, BabelCode also\\nsupports fine-grained evaluation metrics for memory, run-\\nning time, and individual test cases. Our extensive test suite\\nand validation scripts have also exposed many language-\\nspecific idiosyncrasies that naive methods of translation fail\\nto handle. For example, in Julia, any \\\"$\\\" will be treated\\nas string interpolation, even if it is in a docstring. Thus,\\nin the majority of cases, these must be escaped. We auto-\\nmatically rename variables that use reserved keywords. In\\nlanguages such as C#, the == operator checks equivalence\\nby reference instead of value. Besides corner cases, our\\nDSL and templates allow us to effectively implement proper\\nfloating point equivalence for problems that return a float.\\nFinally, in many languages, MultiPL-E uses types that are\\nnot considered best practice, such as in Scala, where it relies\\non the Java types ArrayList instead of the native List.\\n\\n3. Low-Resource Code Language Models\\n\\nBecause the data availability can vary greatly by program-\\ning language, we can consider the goal of building a multi-\\nlingual code model as a data-imbalanced multi-task learning\\nproblem. Previous work in the multilingual natural lan-\\nguage community (Conneau et al., 2019; Arivazhagan et al.,\\n2019) and in the program synthesis space (Ahmad et al.,\\n2021) have used sampling strategies relying on tempera-\\nture-scaling. In this work, we use the Unimax (Chung et al.,\\n2023) strategy to address this imbalance. The Unimax algo-\\nrithm assumes that we are given a budget of how many ex-\\namples we plan to consume during training and a maximum\\nnumber of times, $N$, any single example can be duplicated\\nin the training corpus. Then, we separate the data into buck-\\nets by programming language and add $N$ epochs of each of\\nthe lowest-resource languages until we can safely distribute\\nthe remaining budget across all the remaining languages\\nwithout exceeding $N$ epochs over any one of these remain-\\nning languages. This will allow us to control the number of\\nepochs $N$ we perform over the low-resource languages to\\nminimize overfitting while allowing fair distribution of the\\ncompute budget to the remaining high-resource languages.\\nWe will ablate the choice of $N$ in our experiments.\\n\\nFigure 3. Different distributions for Unimax with different budgets.\\n\\n4. Experimental Setup\\n\\n4.1. Models\\n\\nTo understand the impact of training decoder-only models\\non the different programming language distributions, we\\ntrain models in 3 sizes: 1B, 2B, and 4B. For each of these\\nsizes, we train 5 different models on each distribution: Natu-\\nral and Unimax $N$, where $N \\\\in \\\\{1, 2, 3, 4\\\\}$. The parameters\\nand training differences are listed in Table 2. We follow\\nChowdhery et al. (2022) for all other architecture choices.\\nEvery model has a context window of 2048 and is trained\\nidentically with the same vocabulary described in subsec-\\ntion 4.3. We use a base learning rate of 0.01 and a constant\\nwarmup with a step inverse decay. The number of warmup\\nsteps is kept to 10% of the total training steps per model.\\nThe total number of training steps is 38000, 77000, 190000\\nfor the 1B, 2B, and 4B models, respectively. We use the\\nAdafactor optimizer (Shazeer & Stern, 2018) and a batch\\nsize of 256. We prepend [code] to the beginning and add\\nthe tag [eod] to the end of each file from our training data.\\nFinally, we use the T5X and SeqIO (Roberts et al., 2022)\\nframeworks. We use the UL2 (Tay et al., 2022) objective\"}"}
