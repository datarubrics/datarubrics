{"id": "klarner23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Drug Discovery under Covariate Shift with Domain-Informed Prior Distributions over Functions\\n\\nLeo Klarner\\nTim G. J. Rudner\\nMichael Reutlinger\\nTorsten Schindler\\nGarrett M. Morris\\nCharlotte M. Deane\\nYee Whye Teh\\n\\nAbstract\\nAccelerating the discovery of novel and more effective therapeutics is an important pharmaceutical problem in which deep learning is playing an increasingly significant role. However, real-world drug discovery tasks are often characterized by a scarcity of labeled data and significant covariate shift\u2014a setting that poses a challenge to standard deep learning methods. In this paper, we present \\\\(Q\\\\)-SAVI, a probabilistic model able to address these challenges by encoding explicit prior knowledge of the data-generating process into a prior distribution over functions, presenting researchers with a transparent and probabilistically principled way to encode data-driven modeling preferences. Building on a novel, gold-standard bioactivity dataset that facilitates a meaningful comparison of models in an extrapolative regime, we explore different approaches to induce data shift and construct a challenging evaluation setup. We then demonstrate that using \\\\(Q\\\\)-SAVI to integrate contextualized prior knowledge of drug-like chemical space into the modeling process affords substantial gains in predictive accuracy and calibration, outperforming a broad range of state-of-the-art self-supervised pre-training and domain adaptation techniques.\\n\\n1. Introduction\\nDiscovering novel drug candidates that are able to safely and effectively treat neglected diseases or combat multidrug-resistant pathogens is a challenging biomedical research problem of considerable scientific and societal importance. Leveraging modern deep learning algorithms to accurately predict clinically relevant molecular properties and reduce the need for time- and resource-intensive experiments has the potential to significantly accelerate the development of promising and innovative chemical leads in drug discovery.\\n\\nA key feature of practical early-stage drug discovery research is the application of predictive models to novel compounds that are structurally or functionally dissimilar to molecules that have already been explored (see Figure 1). In such an extrapolative regime, the practical utility of machine learning systems hinges on their ability to (a) robustly generalize to unexplored areas of chemical space and (b) reliably indicate when they fail to do so by generating well-calibrated predictive uncertainty estimates. However, standard deep learning algorithms often perform poorly under covariate shift, generating both incorrect and highly miscalibrated predictions (Ovadia et al., 2019; Koh et al., 2021). This is particularly problematic in the context of early-stage drug discovery, where experimental labels are expensive to acquire and therefore only available for a small and often highly biased subset of compounds.\"}"}
{"id": "klarner23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Interpolation Extrapolation\\n\\nFigure 2. When trained on a small and highly biased subset of chemical space, standard neural networks \\\\((\\\\text{gray})\\\\) rarely generalize well in the extrapolative regime. Our approach enables the construction of a problem-informed regularizing prior distribution over functions to place soft constraints on a neural network's hypothesis space, enabling better generalization and uncertainty quantification under covariate shift. In-distribution training points are shown in \\\\((\\\\text{blue})\\\\) and out-of-distribution test points are shown in \\\\((\\\\text{red})\\\\).\\n\\nTo improve the predictive performance of deep learning algorithms in such resource-constrained, low-data settings, we may wish to use relevant prior knowledge about the problem domain to specify inductive biases that make some predictive functions more likely than others. Common approaches to imbuing neural networks with useful inductive biases include (a) pre-training them on larger, potentially unlabeled datasets (Finn et al., 2017; Tan et al., 2018; Bommasani et al., 2021) and (b) adjusting their architectures to mirror appropriate invariances of their input domain (Bronstein et al., 2017; Satorras et al., 2021). However, these approaches are only an indirect\u2014and often insufficiently precise\u2014way of translating explicit modeling preferences into constraints over a neural network's hypothesis space.\\n\\nIn this paper, we present an alternative approach. To encode domain-informed prior knowledge of the data-generating process into neural network training, we specify a prior distribution over the space of Q-quantity S-structure-A-activity mappings evaluated at a carefully selected set of context points, and perform Variational Inference in the resulting probabilistic model (see Figure 2). We will refer to this method as QSAVI.\\n\\nTo demonstrate the practical utility of this approach, we construct a robust evaluation setup based on a carefully pre-processed bioactivity dataset. We then apply several different techniques to induce strong covariate and label shifts, resulting in challenging and practically meaningful train-test splits. Finally, we use QSAVI to specify explicit and problem-informed prior knowledge of drug-like chemical space and show that this substantially improves the predictive accuracy and calibration of deep learning algorithms in an out-of-distribution setting, outperforming a range of strong self-supervised pre-training, domain adaptation, and ensembling techniques.\\n\\nCode and datasets are provided at: https://github.com/leojklarner/QSAVI.\"}"}
{"id": "klarner23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instead, we propose an alternative regularization scheme\u2014Q-SAVI\u2014that builds on the fact that we are able to approximately sample from $U(X)$ through large chemical databases such as ZINC (Irwin et al., 2020) or GDB (Polishchuk et al., 2013) to specify arbitrary modeling preferences on $X \\\\setminus X'$. Specifically, we construct a probabilistic model of neural network functions and define a tractable prior distribution over parametric function mappings evaluated at points in $U(X)$. We then extend this probabilistic model to include a label-space prior over $X$, which encodes contextualized information on $\\\\tilde{p}_X$ and $\\\\tilde{Y}$, and demonstrate empirically that variational inference in this probabilistic model results in neural networks that make accurate predictions in regions of chemical space that they can reliably extrapolate to while generating well-calibrated predictive uncertainty estimates that indicate when correct predictions are unlikely.\\n\\n3. Related Work\\n\\nStarting with foundational attempts to link the electronic properties of different substituents to the reactivity (Hammett, 1937) and bioactivity (Hansch et al., 1962) of benzoic acid derivatives, the problem of predicting the properties of a molecule from its structure has long received considerable attention (Cherkasov et al., 2014). While simpler algorithms such as support vector machines (Cortes and Vapnik, 1995) and random forests (Breiman, 2001) remain a popular choice for such quantitative structure-activity relationship (QSAR) models, recent years have seen substantial interest in applying modern deep learning algorithms to this task (Ma et al., 2015; Gawehn et al., 2016; Zhang et al., 2017), including important attempts to improve their performance in low-data and out-of-distribution regimes.\\n\\nSelf-supervised pre-training techniques. To this end, Hu et al. (2019) and Rong et al. (2020) have introduced a range of self-supervised objectives to pre-train graph neural networks and graph transformers on a set of unlabeled molecular structures to generate initializations that can be efficiently fine-tuned on downstream tasks. However, the out-of-distribution generalization of their approaches was only assessed on scaffold splits\u2014a setting that may underestimate of covariate and label shift encountered in many practical applications (Wallach and Heifets, 2018).\\n\\nDomain adaptation techniques. Building on the fact that biases in the data collection process are often known at training time, domain adaptation and generalization techniques (Ganin et al., 2016; Sun and Saenko, 2016; Sagawa et al., 2019; Arjovsky et al., 2019) aim to improve the performance of deep learning algorithms in out-of-distribution settings by leveraging pre-specified domain indicators. However, these methods\u2014originally developed for image data\u2014have been found to provide limited benefits in the context of molecular property prediction (Ji et al., 2022).\\n\\nBayesian inference-based techniques. Bayesian Neural Networks (BNNs; Neal (1996)) provide a principled probabilistic framework for posterior inference over neural network parameters and have long been explored in the context of drug discovery (Burden and Winkler, 1999; Burden et al., 2000). Even though they conceptually guarantee robustness in low-data regimes, their empirical performance often falls short of ensembling techniques or even standard stochastic gradient descent (Ovadia et al., 2019; Foong et al., 2019; Farquhar et al., 2020), including in the context of molecular property prediction (Ryu et al., 2019; Zhang et al., 2019). While these approaches may improve the robustness of deep learning algorithms in some settings, they are limited in the extent to which they can encode problem-specific modeling preferences that, for example, encourage high predictive uncertainty away from the training data or specify prior knowledge of synthetic accessibility and patentability. For instance, the standard parameter-space formulation of BNNs precludes the specification of semantically meaningful prior information due to the highly non-linear and complex relationship between a neural network's parameters and the functions they encode.\\n\\nBuilding on recent work that aims to address the shortcomings of BNNs (e.g., in specifying meaningful prior distributions and providing reliable uncertainty quantification) via function-space variational inference (Sun et al., 2019; Rudner et al., 2021; 2022b), we reframe QSAR modeling as inferring a posterior distribution over functions. We do so by specifying a prior distribution over function mappings along with a prior distribution over function evaluation points and performing variational inference in this probabilistic model, which allows us to explicitly encode prior beliefs about the distribution over functions as well as about the structure of the input space into neural network training.\"}"}
{"id": "klarner23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We will now extend this function-space formulation of \\\\( \\\\Theta \\\\) which includes an explicit dependence on the function-space variables \\\\( f \\\\) beyond a biased subset of training points \\\\( X \\\\). In the remainder of this section, subscripts \\\\( p \\\\) and Equation (2) are related, note that for a prior distribution \\\\( p(f) \\\\) parametric function mappings \\\\( f \\\\) and Equation (2) are related, note that for a prior distribution \\\\( p(f) \\\\) induced by the posterior distribution over parameters \\\\( \\\\theta \\\\) place high probability mass on relevant regions of the input domain and specify prior knowledge of preferred parametric function mappings on unlabelled data points.\\n\\nInstead of formulating the posterior inference problem as finding the posterior distribution over stochastic parameters \\\\( p(\\\\theta|D_x;\\\\theta) \\\\), yielding the posterior distribution over parameters \\\\( \\\\theta \\\\) induced by the posterior distribution over parameters \\\\( \\\\theta \\\\) and marginal likelihood \\\\( p(D_x|\\\\theta) \\\\) being the number of Monte Carlo samples used to estimate the predictive distribution.\\n\\nFor non-linear function mappings \\\\( f \\\\), this framework enables us to explicitly encode arbitrary modeling preferences as distributions that can in turn be equivalently expressed as a factorized variational objective, making stochastic variational inference in stochastic neural networks as finding a posterior distribution of latent stochastic functions \\\\( x(C) \\\\) by means of posterior predictive distribution through \\\\( p(D_y|x(C);\\\\theta) \\\\). After optimizing the variational objective with respect to \\\\( q(\\\\theta) \\\\) and assuming that the variational distribution factorizes as \\\\( q(\\\\theta) \\\\sim \\\\prod_{j=1}^{M} q(\\\\theta_j) \\\\), we can reformulate the variational objective by minimizing the Kullback-Leibler divergence between the true posterior \\\\( p(\\\\theta|D_x) \\\\) and variational distribution over parameters \\\\( q(\\\\theta) \\\\) as\\n\\n\\\\[\\n\\\\min_{q(\\\\theta)} \\\\mathbb{D}_{KL}(p(\\\\theta|D_x) \\\\| q(\\\\theta)) = \\\\mathbb{D}_{KL}(q(\\\\theta) \\\\| p(\\\\theta|D_x)),\\n\\\\]\\n\\nrelative to the prior \\\\( p(\\\\theta) \\\\).\\n\\nFor some variational distribution over parameters \\\\( q(\\\\theta) \\\\) parameterized by high-dimensional \\\\( \\\\Theta \\\\) and Equation (5) is analytically intractable. Instead, we may frame it variationally as\\n\\n\\\\[\\n\\\\min_{\\\\Theta} \\\\mathbb{D}_{KL}(\\\\Theta \\\\| q(\\\\theta) \\\\| p(\\\\theta|D_x)) = \\\\mathbb{D}_{KL}(q(\\\\theta) \\\\| p(\\\\theta|D_x)),\\n\\\\]\\n\\nand Equation (5) is analytically intractable. Instead, we may frame it variationally as\\n\\n\\\\[\\n\\\\min_{\\\\Theta} \\\\mathbb{D}_{KL}(\\\\Theta \\\\| q(\\\\theta) \\\\| p(\\\\theta|D_x)) = \\\\mathbb{D}_{KL}(q(\\\\theta) \\\\| p(\\\\theta|D_x)),\\n\\\\]\\n\\nrelative to the prior \\\\( p(\\\\theta) \\\\).\\n\\nLet \\\\( \\\\Theta \\\\) and \\\\( \\\\delta \\\\) be the Dirac delta function (Wolpert, 1993; Rudner et al., 2022a). In particular, while the parameterized function space \\\\( f \\\\) and \\\\( \\\\Theta \\\\) are independent of \\\\( \\\\theta \\\\), yielding the posterior distribution over parameters \\\\( \\\\theta \\\\) induced by the posterior distribution over parameters \\\\( \\\\theta \\\\)\\n\\n\\\\[\\n\\\\min_{\\\\Theta} \\\\mathbb{D}_{KL}(\\\\Theta \\\\| q(\\\\theta) \\\\| p(\\\\theta|D_x)) = \\\\mathbb{D}_{KL}(q(\\\\theta) \\\\| p(\\\\theta|D_x)),\\n\\\\]\\n\\nrelative to the prior \\\\( p(\\\\theta) \\\\).\\n\\nLet \\\\( \\\\Theta \\\\) and \\\\( \\\\delta \\\\) be the Dirac delta function (Wolpert, 1993; Rudner et al., 2022a). In particular, while the parameterized function space \\\\( f \\\\) and \\\\( \\\\Theta \\\\) are independent of \\\\( \\\\theta \\\\), yielding the posterior distribution over parameters \\\\( \\\\theta \\\\) induced by the posterior distribution over parameters \\\\( \\\\theta \\\\) relative to the prior \\\\( p(\\\\theta) \\\\).\\n\\nLet \\\\( \\\\Theta \\\\) and \\\\( \\\\delta \\\\) be the Dirac delta function (Wolpert, 1993; Rudner et al., 2022a). In particular, while the parameterized function space \\\\( f \\\\) and \\\\( \\\\Theta \\\\) are independent of \\\\( \\\\theta \\\\), yielding the posterior distribution over parameters \\\\( \\\\theta \\\\) induced by the posterior distribution over parameters \\\\( \\\\theta \\\\) relative to the prior \\\\( p(\\\\theta) \\\\).\\n\\nLet \\\\( \\\\Theta \\\\) and \\\\( \\\\delta \\\\) be the Dirac delta function (Wolpert, 1993; Rudner et al., 2022a). In particular, while the parameterized function space \\\\( f \\\\) and \\\\( \\\\Theta \\\\) are independent of \\\\( \\\\theta \\\\), yielding the posterior distribution over parameters \\\\( \\\\theta \\\\) induced by the posterior distribution over parameters \\\\( \\\\theta \\\\) relative to the prior \\\\( p(\\\\theta) \\\\).\"}"}
{"id": "klarner23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The multi-layer perceptrons were implemented with the PyTorch library (Paszke et al., 2019), using rectified linear units (Nair and Hinton, 2010) as activation functions. Their weights were initialized using a Normal distribution $N(0, 1)$ truncated at $\\\\pm 2\\\\sigma$, with biases initialized at zero. These parameters were optimized on the training set using the ADAM stochastic gradient descent optimizer (Loshchilov and Hutter, 2017) with a batch size of 128 and the cross-entropy loss for a maximum of 500 epochs, using early stopping to terminate training if the unweighted log-likelihood on the validation set did not decrease for more than 10 epochs, reverting to the checkpoint with best validation set log-likelihood for evaluating their performance for hyperparameter optimization and the subsequent on the held-out test set. Batch normalization and dropout were applied after the ReLU non-linearity. The full hyperparameter search space is presented in Table 8.\\n\\n### Table 8. Hyperparameters for Multi-layer Perceptrons\\n\\n| Model Hyperparameter Search Space | Value          |\\n|----------------------------------|----------------|\\n| Multi-layer Perceptron learning rate | $1 \\\\times 10^{-4}, 1 \\\\times 10^{-3}$ |\\n| weight decay                     | $1 \\\\times 10^{-3}, 1 \\\\times 10^{-2}, 1 \\\\times 10^{-1}$ |\\n| number of layers                 | 2, 4, 6        |\\n| embedding dimension              | 32, 64         |\\n| batch normalization (BN)         | yes, no        |\\n| BN running statistics            | yes, no        |\\n| dropout                          | 0.0, 0.2, 0.5  |\\n| class weight                     | none, balanced |\\n\\nThe deep ensembles were trained using an identical setup to the multi-layer perceptrons, with the distinction that $M = 5$ independent networks were trained with different random seeds and evaluated with respect to their average log-likelihood on the validation set. Similarly, at inference time the class probabilities were averaged across ensembles. The full hyperparameter search space is presented in Table 9 and is identical to Table 8.\\n\\n### Table 9. Hyperparameters for Deep Ensembles\\n\\n| Model Hyperparameter Search Space | Value          |\\n|----------------------------------|----------------|\\n| Deep Ensemble learning rate      | $1 \\\\times 10^{-4}, 1 \\\\times 10^{-3}$ |\\n| weight decay                     | $1 \\\\times 10^{-3}, 1 \\\\times 10^{-2}, 1 \\\\times 10^{-1}$ |\\n| number of layers                 | 2, 4, 6        |\\n| embedding dimension              | 32, 64         |\\n| batch normalization (BN)         | yes, no        |\\n| BN running statistics            | yes, no        |\\n| dropout                          | 0.0, 0.2, 0.5  |\\n| class weight                     | none, balanced |\"}"}
{"id": "klarner23a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The graph featurization pipeline, architectures, and pre-trained initializations of the graph isomorphism networks presented in Hu et al. (2019) were retrieved from the paper's official GitHub repository and fine-tuned on the training set using the ADAM optimizer (Loshchilov and Hutter, 2017) with a batch size of 128 and the cross-entropy loss for a maximum of 500 epochs, using early stopping to terminate training if the unweighted log-likelihood on the validation set did not decrease for more than 10 epochs and reverting to the checkpoint with best validation set log-likelihood for evaluating their performance for hyperparameter optimization and the subsequent on the held-out test set. The full hyperparameter search space is presented in Table 10. The pre-trained initializations were provided for networks with 5 layers of 300 hidden units, set up using batch normalization with running statistics.\\n\\nTable 10. Hyperparameters for Pre-trained GINs\\n\\n| Model Hyperparameter Search Space |\\n|----------------------------------|\\n| Pre-trained GINs learning rate    |\\n| $1 \\\\times 10^{-4}$, $3 \\\\times 10^{-3}$, $1 \\\\times 10^{-3}$, $3 \\\\times 10^{-3}$, $1 \\\\times 10^{-2}$ |\\n| weight decay                     |\\n| $1 \\\\times 10^{-3}$, $1 \\\\times 10^{-2}$, $1 \\\\times 10^{-1}$ |\\n| dropout                          |\\n| 0.0, 0.2, 0.5                    |\\n| class weight                     |\\n| none, balanced                   |\\n\\nB.2.6. GROVER\\n\\nAll code, models, and initializations required to fine-tune the pre-trained graph transformers presented in Rong et al. (2020) was retrieved from the paper's official GitHub repository and fine-tuned on the training set with a batch size of 128 for a maximum of 500 epochs, using early stopping to terminate training if the unweighted log-likelihood on the validation set did not decrease for more than 10 epochs and reverting to the checkpoint with best validation set log-likelihood for evaluating their performance for hyperparameter optimization and the held-out test set. The hyperparameters specifying the number of layers and their embedding dimension indicate the size of the MLP fit on top of the pre-trained molecular representations produced by the GROVER base model and were chosen to be identical to the other MLP-based deep learning algorithms. The full hyperparameter search space is presented in Table 11.\\n\\nTable 11. Hyperparameters for GROVER\\n\\n| Model Hyperparameter Search Space |\\n|----------------------------------|\\n| GROVER learning rate             |\\n| $1 \\\\times 10^{-4}$, $1 \\\\times 10^{-3}$ |\\n| weight decay                     |\\n| $1 \\\\times 10^{-3}$, $1 \\\\times 10^{-2}$, $1 \\\\times 10^{-1}$ |\\n| dropout                          |\\n| 0.0, 0.2, 0.5                    |\\n| number of layers                 |\\n| 2, 4, 6                          |\\n| embedding dimension              |\\n| 32, 64                            |\"}"}
{"id": "klarner23a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"All code and featurization utilities required to run the evaluated domain adaptation and generalization techniques, namely invariant risk minimization (IRM), group-distributionally robust training (GroupDRO), domain-adversarial networks (DANN) and deep correlation alignment (DeepCoral), were adapted from Ji et al. (2022) and provided with data split-specific domain indicators. For this, the training set was additionally split into three domains, either using spectral clustering, molecular weight thresholds, a grouped scaffold split, or random partitions. All models used the default architecture choice in Ji et al. (2022)\u2014a graph isomorphism network with 4 layers and 128 hidden units\u2014and trained according to the respective optimization procedures. The full hyperparameter range is presented in Table 12.\\n\\nTable 12. Hyperparameters for Domain Adaptation Techniques\\n\\n| Model Hyperparameter Search Space                                      |\\n|-----------------------------------------------------------------------|\\n| IRM/GroupDRO/DANN/DeepCoral learning rate                             |\\n| $1 \\\\times 10^{-4}, 1 \\\\times 10^{-3}$                                  |\\n| weight decay                                                          |\\n| $1 \\\\times 10^{-3}, 1 \\\\times 10^{-2}, 1 \\\\times 10^{-1}$                |\\n| dropout                                                               |\\n| 0.0, 0.2, 0.5                                                          |\\n\\nB.2.8. Q-SAVI\\n\\nThe models based on our probabilistic regularization scheme were trained using the implementation of the local linearization scheme presented in Rudner et al. (2022a;b) provided by the authors and using the exact same architecture, initialization, and optimization procedures as for the multi-layer perceptrons and deep ensembles\u2014differing only in the objective function. Specifically, at each gradient step iteration, a sample of $M$ molecules (where $M$ is a hyperparameter) was drawn from a uniform distribution over the ZINC database (Irwin et al., 2020), providing a set of context points on which to evaluate the objective in Equation (9), using the Bernoulli likelihood to specify $\\\\log p(y_D|f(x_D; \\\\theta))$. To construct a prior distribution over parametric function mappings $p(f(\\\\{X, X_C\\\\}; \\\\Theta))$ that maximizes predictive uncertainty away from the training data, it was defined as a distribution over functions with a logit-space mean vector of approximately zero and minimal structure in the off-diagonal entries of its covariance matrix. We refer to our code repository for further implementational details. The full hyperparameter search space is presented in Table 13.\\n\\nTable 13. Hyperparameters for Our Model\\n\\n| Model Hyperparameter Search Space                                      |\\n|-----------------------------------------------------------------------|\\n| Q-SAVI learning rate                                                  |\\n| $1 \\\\times 10^{-4}, 1 \\\\times 10^{-3}$                                  |\\n| number of layers                                                      |\\n| 2, 4, 6                                                               |\\n| embedding dimension                                                   |\\n| 32, 64                                                                |\\n| prior variance                                                        |\\n| $1 \\\\times 10^{-2}, 1 \\\\times 10^{-1}, 1, 1 \\\\times 10^{0}$              |\\n| context points per sample                                            |\\n| 16, 128                                                               |\"}"}
{"id": "klarner23a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.3. Ablation Studies\\n\\nTo understand the impact of different hyperparameters on the performance of our proposed probabilistic regularization scheme, we conducted a series of ablation experiments. In these experiments, we systematically varied the hyperparameters relevant to evaluating the objective in Equation (9)\u2014namely the prior variance and the number of sampled context points\u2014while keeping others fixed, and measured their effects on the test set AUC-ROC and Brier score. The resulting ablation plots are presented in Figures 7 and 8 and show that larger prior covariances are strongly correlated with more robust test-set performances across splits\u2014while the effect of larger context point samples is less pronounced.\\n\\nFigure 7. Effect of (log) prior variance on the test set performance metrics.\\n\\nFigure 8. Effect of the number of context points on the test set performance metrics.\"}"}
{"id": "klarner23a", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In order to assess the practical utility of our method in real-world production settings, an evaluation on the Merck Molecular Activity Challenge datasets (Ma et al., 2015) was conducted. This data consists of 15 datasets from real-world production environments with time-split training and test sets. As the compound structures are only provided in the form of anonymized atom-pair descriptors, it is not possible to use a uniform subsample of a large chemical database as a context point distribution. Instead, the evaluation focused on the three most covariate- and label-shifted datasets, see Figure 9, using a uniform distribution over molecules from the remaining datasets as a context point distribution. To select these datasets, the multiset version of the standard Jaccard/Tanimoto index\\n\\n$$k_{jac-multiset}(x, y) = \\\\frac{\\\\sum \\\\min(x_i, y_i)}{\\\\sum \\\\max(x_i, y_i)}$$\\n\\nwas used to evaluate the MMD statistic between two sets of count vectors and quantify covariate shift. Label shift between the regression targets of every training and test set was quantified through the two-sample Kolmogorov-Smirnov test statistic.\\n\\n![Figure 9. Scatterplot illustrating the covariate and label shifts in the Merck Molecular Activity Challenge datasets](image)\\n\\n![Figure 10. Heatmap illustrating the pairwise overlap between different datasets from the Merck Molecular Activity Challenge, defined as the proportion of molecules from the smaller dataset that are found in the larger dataset, i.e., \\\\(|X_1 \\\\cup X_2|/\\\\min(|X_1|, |X_2|)|](image)\\n\\nAs shown in Figure 10, the direct overlap between the HIVPROT, NK1, and DPP4 datasets with the remaining data is minimal, warranting its use as a general and diverse context point distribution. Using this evaluation setup, 10% of the training sets was randomly split off as a validation set for hyperparameter optimization and, where applicable, early stopping. Model-specific details are outlined below, including implementational details and hyperparameter ranges for regularized linear regressions (Appendix C.1), random forest regressors (Appendix C.2), and an adapted version of our probabilistic regularization scheme (Appendix C.3).\"}"}
{"id": "klarner23a", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.1. Regularized Linear Regression\\n\\nThe regularized linear regression models were trained using the scikit-learn library (Pedregosa et al., 2011). The LIBLINEAR solver (Fan et al., 2008) was employed with a maximum of 1,000 iterations and a stopping tolerance of $1 \\\\times 10^{-4}$. The models were independently fitted for all specified hyperparameter combinations presented in Table 14. The combination yielding the lowest validation set mean squared error was selected to evaluate the model on the held-out test set.\\n\\nTable 14. Hyperparameters for $L^1$ and $L^2$-Regularized Linear Regression\\n\\n| Model Hyperparameter Search Space |\\n|----------------------------------|\\n| Linear Regression regularization type $\\\\ell^1, \\\\ell^2$ | regularization strength 100 values spaced log-linearly in $[1 \\\\times 10^{-4}, 1 \\\\times 10^4]$ |\\n\\nC.2. Random Forest Regressors\\n\\nThe random forest regression models were trained using the scikit-learn library (Pedregosa et al., 2011). The models consisted of 100 decision trees with the GINI splitting criterion. Each model was independently fitted for all specified hyperparameter combinations shown in Table 15. The combination with the lowest validation set mean squared error was selected to evaluate the model on the held-out test set.\\n\\nTable 15. Hyperparameters for Random Forest Regressor\\n\\n| Model Hyperparameter Search Space |\\n|----------------------------------|\\n| Random Forest maximum depth | 50 values spaced linearly in [5, 500] |\\n| min. samples per split | 5, 15, 50, 100 |\\n| min. samples per leaf | 1, 5, 10, 30, 100 |\\n\\nC.3. Q-SA VI\\n\\nThe regression variant of our probabilistic regularization scheme was set up identically to the classification variant described in Appendix B.2.8, the only difference being the likelihood function used to evaluate Equation (9). Instead of specifying $\\\\log p(y^D|f(x^D; \\\\theta))$ as a Bernoulli likelihood, a homoscedastic multivariate Normal likelihood with a unit diagonal covariance matrix was used. While a more expressive approach of either optimizing the covariance as a hyperparameter or letting the network predict point-wise means and variances to use in combination with a heteroscedastic likelihood function is possible, this straightforward method was found to be sufficient in this context. The full hyperparameter search space is presented in Table 16 and is identical to that in Table 13.\\n\\nTable 16. Hyperparameters for Our Model\\n\\n| Model Hyperparameter Search Space |\\n|----------------------------------|\\n| Ours learning rate | $1 \\\\times 10^{-4}, 1 \\\\times 10^{-3}$ |\\n| number of layers | 2, 4, 6 |\\n| embedding dimension | 32, 64 |\\n| prior variance | $1 \\\\times 10^{-2}, 1 \\\\times 10^{-1}, 1, 1 \\\\times 10^1, 1 \\\\times 10^2$ |\\n| context points per sample | 16, 128 |\"}"}
{"id": "klarner23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. Empirical Evaluation\\n\\nTo demonstrate the practical utility of Q-SAVI, we establish a robust evaluation setup: In Section 5.1, we argue that many commonly-used bioactivity datasets may not be able to meaningfully assess the extrapolative power of supervised machine learning algorithms and present a carefully cleaned and pre-processed alternative dataset and in Section 5.2, we define an appropriate set of statistics to quantify covariate and label shifts in chemical space and use them to investigate the extent to which different splitting techniques induce data shift. In Section 5.3, we then use this experimental setup to demonstrate that employing Q-SAVI to incorporate domain-informed prior knowledge into the modeling process leads to significant gains in predictive accuracy and calibration, outperforming a range of strong self-supervised pre-training, domain adaptation, and ensembling techniques. Finally, in Section 5.4, we show that these strong empirical results extend to real-world production settings by evaluating our method on the time-split data presented in Ma et al. (2015).\\n\\n5.1. Curating an Appropriate Dataset\\n\\nA fundamental obstacle to training and evaluating QSAR models in the public domain is the scarcity of sufficiently large datasets with high-quality labels (Schneider et al., 2020). Even though collections of publicly available bioactivity data exist (Wu et al., 2018; Huang et al., 2021), they are often sourced directly from repositories of high-throughput screening (HTS) data such as PUBCHEM (Kim et al., 2019), EMBL (Mendez et al., 2019) or TOXCAST (Richard et al., 2016) without significant filtering or pre-processing. While this approach maximizes the number of available data points, it may reduce the discriminative power of model performance comparisons. For instance, a well-known problem of confirmatory dose-response screens\u2014which make up the bulk of measurements in the above repositories\u2014is that they usually contain a large number of reproducible false positive readouts (in many cases up to 95% of hits (Thorne et al., 2010)) caused by molecular substructures that interfere with an assay's readout system (Baell and Holloway, 2010; Dahlin et al., 2015). Using such data without further processing runs the risk of simply testing for the ability of algorithms to memorize these substructures instead of assessing meaningful extrapolative performance (Klarner et al., 2022).\\n\\nTo curate a dataset of sufficient quality to enable an informative comparison of predictive models, we used the measurement meta-data of bioactivity and toxicity screens to prioritize certain data points for further inspection. After surveying the publications associated with the most promising datasets, we selected a high-quality screening campaign for inhibitors of the development of liver-stage malaria parasites for further processing (Antonova-Koch et al., 2018). Specifically, we retrieved and reprocessed the raw measurement data to remove likely false positives and other experimental artifacts, yielding a binary classification dataset with 7,301 inactive and 849 active molecules, each measured in biological duplicate and confirmed as a true positive or negative through a set of quality-assuring counter-screens (see Appendix A for full details).\\n\\n5.2. Inducing and Quantifying Data Shift\\n\\nFeaturization. Commonly used techniques to numerically represent the structural properties of a molecule include strings, graphs, and topological fingerprints. For the following experiments, each molecule was featurized as both an extended-connectivity fingerprint (ECFP ; Rogers and Hahn (2010)) and an RDKIT fingerprint (RDKIT FP), using the respective implementations in the open-source cheminformatics package RDKIT (Landrum et al., 2022). An illustration of this process is presented in Figure 3.\\n\\nStatistics for covariate and label shift. To evaluate the extent to which different train-test splits induce covariate and label shift, we identified a set of suitable two-sample test statistics and used it to quantify the dissimilarity of the marginal covariate and label distributions of the respective training and test sets $D_{tr} = (X_{tr}, y_{tr})$ and $D_{te} = (X_{te}, y_{te})$. Since $y_{tr}$ and $y_{te}$ consist of binary indicators of antimalarial activity, well-established categorical statistics such as Fisher's exact test (Upton, 1992) are applicable. In the following, its negative logarithmic p-value is used as a scalar indicator of label shift.\"}"}
{"id": "klarner23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Drug Discovery under Covariate Shift with Domain-Informed Prior Distributions over Functions\\n\\nDefining a corresponding statistic to quantify covariate shift between two sets of molecules is more challenging, as they constitute disjoint sets of discrete objects. For this purpose, we used the maximum mean discrepancy (MMD) metric (Gretton et al., 2012) to quantify the difference between two samples of molecules as the distance between the embeddings of their expectations in a reproducing kernel Hilbert space (RKHS) defined by some mapping $\\\\phi: X \\\\rightarrow H$ and an associated kernel function $k(x_i, x_j) = \\\\langle \\\\phi(x_i), \\\\phi(x_j) \\\\rangle_H$. An empirical estimator of this statistic is obtained by\\n\\n$$\\\\text{MMD}^2(X_{tr}, X_{te}) = \\\\left\\\\| \\\\mathbb{E}_{x \\\\in X_{tr}}[\\\\phi(x)] - \\\\mathbb{E}_{x \\\\in X_{te}}[\\\\phi(x)] \\\\right\\\\|_H^2 = \\\\mathbb{E}_{x_i, x_j \\\\in X_{tr}}[k(x_i, x_j)] + \\\\mathbb{E}_{x_i, x_j \\\\in X_{te}}[k(x_i, x_j)] - 2\\\\mathbb{E}_{x_i \\\\in X_{tr}, x_j \\\\in X_{te}}[k(x_i, x_j)],$$\\n\\nusing the Jaccard/Tanimoto similarity coefficient $k_{jac}(x_i, x_j) = \\\\frac{x_i \\\\cap x_j}{x_i \\\\cup x_j} = \\\\frac{\\\\langle x_i, x_j \\\\rangle}{\\\\|x_i\\\\|^2 + \\\\|x_j\\\\|^2 - \\\\langle x_i, x_j \\\\rangle}$ as an appropriate similarity metric, both due to its established use in the cheminformatics community (Bajusz et al., 2015) and the favorable properties of the RKHS that it induces. The MMD statistic is only valid if the mean embedding $\\\\mathbb{E}_{x \\\\in X}[\\\\phi(x)]$ is injective, which is the case for strictly positive definite kernels operating in discrete domains (Borgwardt et al., 2006), such as $k_{jac}$ (Bouchard et al., 2013).\\n\\nRandom and scaffold splits.\\n\\nEquipped with the appropriate statistical tools to quantify distributional similarities, we investigated the extent to which different train-test splits are able to emulate practically relevant covariate and label shifts, beginning with the two most popular approaches of splitting data either randomly or by scaffold. While random splits are commonly used in many domains, they are known to produce unrealistically optimistic performance estimates in the context of molecular property prediction. This is a consequence of the biased composition of many experimental datasets, which often contain structurally similar compounds from so-called chemical series. As these often exhibit very similar properties, distributing them evenly across data splits leads to a de-facto overlap between training and test sets that incentivizes overfitting and memorization (Wallach and Heifets, 2018).\\n\\nScaffold splits attempt to mitigate this shortcoming by mapping each molecule to an overarching compound class\u2014usually its Bemis-Murcko scaffold (Bemis and Murcko, 1996; 1999)\u2014and splitting the data so that all molecules of a given scaffold are assigned to the same partition. However, this approach often results in a similar pathology, as even molecules with nominally different scaffolds can exhibit a high degree of structural and functional similarity, as illustrated in Figure 5.\\n\\nFigure 5. Even molecules with nominally different Bemis-Murcko scaffolds can exhibit a high degree of structural and functional similarity. Depicted are four structurally similar molecules from our antimalarial dataset that are assigned to four different scaffolds.\\n\\nMolecular weight and spectral splits.\\n\\nTo facilitate the comparison of models in an extrapolative regime, we explored two alternative approaches. A straightforward molecular weight split was used to induce data shift by assigning molecules into training and test sets based on a molecular weight cut-off, relying on the correlation of molecular size and binding strength to also induce strong label shift (Hopkins et al., 2014). More rigorously, we developed a clustering-based spectral split to generate data splits that are guaranteed to exhibit maximal covariate shift under the MMD statistic. By interpreting the Jaccard kernel Gram matrix $W_{jac} \\\\in [0, 1]_{|X_D| \\\\times |X_D|}$ of a given set of molecules $X_D$ as the weighted adjacency matrix of a fully-connected similarity graph $S$, well-established spectral clustering algorithms (Von Luxburg, 2007) can be employed to identify an optimal partitioning of $S$ that maximizes the similarity within and minimizes the similarity between partitions.\\n\\nWe present a comparison of the resulting covariate and label shift statistics in Table 1, which shows that molecular weight and spectral clustering-based splits generate a significantly more extrapolative evaluation setup than random and scaffold splits. This is substantiated by the qualitative visualization presented in Figure 4.\\n\\nTable 1. A summary of the covariate and label shifts induced by the different train-test splits presented in Section 5.2, using rdkit and extended-connectivity (EC) fingerprints. Covariate shift is quantified as the Jaccard kernel-based MMD statistic, while label shift is quantified as the negative log $p$-value of Fisher's exact test.\\n\\n| Split         | Covariate Shift $(\\\\text{rdkit, EC})$ | Label Shift $(\\\\text{rdkit, EC})$ |\\n|---------------|--------------------------------------|----------------------------------|\\n| Random        | 0.00                                 | 0.00                             |\\n| Scaffold      | 0.08                                 | 0.07                             |\\n| Weight        | 0.14                                 | 0.10                             |\\n| Spectral      | 0.34                                 | 0.25                             |\"}"}
{"id": "klarner23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Drug Discovery under Covariate Shift with Domain-Informed Prior Distributions over Functions\\n\\nRandom split. Scaffold split. Molecular weight split. Spectral split.\\n\\nFigure 4. A visual comparison of the covariate shift induced by different train-test splits using rdkit fingerprints, colored in blue and red respectively. While random and scaffold splits lead to relatively similar training and test sets, molecular weight and spectral splits induce significantly stronger covariate shifts. The plots were generated using UMAP dimensionality reduction (McInnes et al., 2018).\\n\\n5.3. Model Construction, Baselines & Results\\n\\nModel construction. Using the increasingly data-shifted splits constructed in Sections 5.1 and 5.2, we assessed $Q$-SAVI with respect to its ability to improve the predictive accuracy and calibration of deep learning algorithms under covariate and label shifts. By leveraging the option to specify both an arbitrary context point distribution $p_XC$ and a prior distribution over parametric function mappings $p_f(\\\\{X, X_C\\\\}; \\\\Theta)$, we used $Q$-SAVI to encode relevant information about both the input domain and the label space of the problem setup into the model. Specifically, we precomputed the featurizations of a uniform subsample of the ZINC database of commercially available compounds (Irwin et al., 2020) and used them to construct a uniform context point distribution $p_XC = \\\\mathcal{U}(\\\\bar{X})$ over a set of $2 \\\\times 10^6$ synthetically accessible drug-like molecules $\\\\bar{X}$. Additionally, we used the prior distribution $p_f(\\\\{X, X_C\\\\}; \\\\Theta)$ over parametric mappings to encode an informative function-space prior that encourages high predictive uncertainty in unexplored regions of chemical space, counteracting the likelihood term in Equation (9) to generate better predictive uncertainty estimates.\\n\\nBaselines. We compared the performance of the resulting probabilistic model to a range of standard baselines and state-of-the-art pre-training and domain adaptation techniques. The simplest of these models is regularized logistic regression, which is expected to underperform in an extrapolative regime due to the linearity of its logit function. While random forest classifiers (Breiman, 2001) represent a more flexible baseline with strong in-distribution generalization guarantees, they generally exhibit coarser decision boundaries at the fringes of the training distribution that are unlikely to perform well on covariate-shifted inputs. Standard deep learning methods such as multi-layer perceptrons (MLPs) have an even higher representational capacity, yet also generally underperform under data shift, yielding both incorrect and highly overconfident predictions (Ovadia et al., 2019; Koh et al., 2021). Deep ensembles are an effective technique to improve the predictive performance of MLPs by averaging the predictive distributions of a set of independently trained neural networks (Lakshminarayanan et al., 2017). To investigate the extent to which existing self-supervised pre-training techniques and more expressive model architectures impact the performance of deep learning algorithms in this setting, we fine-tuned graph isomorphism networks (GINs; Xu et al. (2018)) provided by Hu et al. (2019) both from scratch and from initializations that were pre-trained on compounds from the ZINC database using context prediction and attribute masking objectives. Additionally, we fine-tuned the graph transformer (GROVER) proposed by Rong et al. (2020) from a pre-trained initialization that was optimized on molecules from the ZINC and ChEMBL databases using self-supervised contextual property and graph-level motif prediction techniques. Finally, we adapted a range of domain adaptation and generalization techniques, including invariant risk minimization (IRM; Arjovsky et al. (2019)), group-distributionally robust training (GroupDRO; Sagawa et al. (2019)), domain-adversarial networks (DANN; Ganin et al. (2016)), and deep correlation alignment (DeepCoral; Sun and Saenko (2016)) from Ji et al. (2022) who provided them with data split-specific domain indicators.\\n\\nTraining and evaluation. To facilitate a fair comparison, we carried out an extensive hyperparameter search for every model, data split, and featurization. After an initial division of the data into training and test sets, the same data-splitting technique was applied again to derive a representative validation set. The hyperparameter setting with the lowest negative log-likelihood on that validation set was then used to train ten independent models using different random seeds. Full implementation details and hyperparameter ranges are provided in Appendix B.\"}"}
{"id": "klarner23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Spectral Split\\nECFP rdkitFP\\nScaffold Split\\nECFP rdkitFP\\nRandom Split\\nWeight Split\\n\\nTable 2.\\n\\n| Model & Featurization | AUC-ROC | SAVER | RIER |\\n|-----------------------|---------|-------|------|\\n| QSA VI                | 0.80    | 0.85  | 0.80 |\\n| GIN (context pred)    | 0.78    | 0.83  | 0.78 |\\n| GIN (attr masking)    | 0.76    | 0.82  | 0.76 |\\n\\nS A V I achieves significant performance gains in an out-of-distribution setting. On the spectral and molecular weight splits\u2014the evaluation settings with the strongest covariate of-distribution setting. Similarly, its predictive uncertainty presented in Tables 2 and 5 (see Appendix B.1) demonstrate mirror the AUC-ROC and Brier calibration error (ACE; Nixon et al. (2019)), which closely der the precision-recall curve (AUC-PRC) and the adaptive algorithm's performance was characterized by the area un-\\n\\neable the direct comparison of models across test sets with label probabilities were characterized by the area under the predictive accuracy and calibration of the estimated test-set following model training and hyperparameter selection, the different random seeds. The best models within a margin of statistical significance are highlighted in bold.\\n\\nResults.\\nThe predictive accuracy and calibration metrics of model performance are summarized in Table 3.\\n\\nTable 3.\\n\\n| Dataset | Label Shift | Covariate Shift | S A V I | G IN | G IN | Q S A VI |\\n|---------|-------------|----------------|--------|------|------|----------|\\n| DPP4    | 0           | 0              | 0.80   | 0.83 | 0.80 | 0.78     |\\n| HIVPROT | 0           | 0              | 0.80   | 0.83 | 0.80 | 0.78     |\\n\\nAs a complementary assessment of the practical utility of molecular activity challenge. Covariate shift is quantified as the two-sample Kolmogorov\u2013Smirnov test statistic.\\n\\n5.4. Merck Molecular Activity Challenge\\n\\nMolecular Activity Challenge. Covariate shift is quantified as the time-split training and test sets that represent the data shift encountered throughout a molecular optimization campaign. In line with the empirical observations of Ji et al. (2022), techniques across most splits and featurizations (see Table 5).\\n\\nImages\u2014were found to perform worse than most other techniques, including Grover, IRM, GroupDRO, DANN, and DeepCoral\u2014domain adaptation methods originally developed for IRM, GroupDRO, DANN, and DeepCoral\u2014domain adaptation methods.\"}"}
{"id": "klarner23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Drug Discovery under Covariate Shift with Domain-Informed Prior Distributions over Functions\\n\\nSheridan, R. P. (2013). Time-split cross-validation as a method for estimating the goodness of prospective prediction. *Journal of Chemical Information and Modeling*, 53(4):783\u2013790.\\n\\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. *The Journal of Machine Learning Research*, 15(1):1929\u20131958.\\n\\nSun, B. and Saenko, K. (2016). Deep coral: Correlation alignment for deep domain adaptation. In *Computer Vision\u2013ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III*, pages 443\u2013450. Springer.\\n\\nSun, S., Zhang, G., Shi, J., and Grosse, R. B. (2019). Functional variational Bayesian neural networks. In *7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net.\\n\\nTan, C., Sun, F., Kong, T., Zhang, W., Yang, C., and Liu, C. (2018). A survey on deep transfer learning. In *International Conference on Artificial Neural Networks*, pages 270\u2013279. Springer.\\n\\nThorne, N., Auld, D. S., and Inglese, J. (2010). Apparent activity in high-throughput screening: origins of compound-dependent assay interference. *Current Opinion in Chemical Biology*, 14(3):315\u2013324.\\n\\nUpton, G. J. (1992). Fisher's exact test. *Journal of the Royal Statistical Society: Series A (Statistics in Society)*, 155(3):395\u2013402.\\n\\nVan Rossum, G. and Drake Jr, F. L. (1995). *Python tutorial*, volume 620. Centrum voor Wiskunde en Informatica Amsterdam, The Netherlands.\\n\\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., et al. (2020). Scipy 1.0: fundamental algorithms for scientific computing in Python. *Nature Methods*, 17(3):261\u2013272.\\n\\nVon Luxburg, U. (2007). A tutorial on spectral clustering. *Statistics and Computing*, 17(4):395\u2013416.\\n\\nWainwright, M. J. and Jordan, M. I. (2008). *Graphical Models, Exponential Families, and Variational Inference*. Now Publishers Inc., Hanover, MA, USA.\\n\\nWallach, I. and Heifets, A. (2018). Most ligand-based classification benchmarks reward memorization rather than generalization. *Journal of Chemical Information and Modeling*, 58(5):916\u2013932.\\n\\nWaskom, M. L. (2021). *seaborn: statistical data visualization*. *Journal of Open Source Software*, 6(60):3021.\\n\\nWes McKinney (2010). Data Structures for Statistical Computing in Python. In St \u00b4efan van der Walt and Jarrod Millman, editors, *Proceedings of the 9th Python in Science Conference*, pages 56 \u2013 61.\\n\\nWolpert, D. H. (1993). Bayesian backpropagation over i-o functions rather than weights. In Cowan, J., Tesauro, G., and Alspector, J., editors, *Advances in Neural Information Processing Systems*, volume 6. Morgan-Kaufmann.\\n\\nWu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., Leswing, K., and Pande, V. (2018). Moleculenet: a benchmark for molecular machine learning. *Chemical Science*, 9(2):513\u2013530.\\n\\nXu, K., Hu, W., Leskovec, J., and Jegelka, S. (2018). How powerful are graph neural networks? *arXiv preprint arXiv:1810.00826*.\\n\\nZhang, L., Tan, J., Han, D., and Zhu, H. (2017). From machine learning to deep learning: progress in machine intelligence for rational drug discovery. *Drug Discovery Today*, 22(11):1680\u20131685.\\n\\nZhang, Y. et al. (2019). Bayesian semi-supervised learning for uncertainty-calibrated prediction of molecular properties and active learning. *Chemical Science*, 10(35):8154\u20138163.\"}"}
{"id": "klarner23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To generate an appropriate dataset of reliably labeled bioactivity measurements, we retrieved and reprocessed high-throughput screening data generated by Antonova-Koch et al. (2018) as part of a campaign to discover novel chemoprotective antimalarial drug candidates. The authors established a cell-based phenotypic screening pipeline to identify compounds that inhibit the development of luciferase-expressing liver-stage \\\\textit{Plasmodium falciparum} parasites. After assaying a commercially-available chemical library of 538,273 drug-like small molecules in a single-point primary screen, they selected the 9,963 most promising compounds for a series of confirmatory dose-response screens. Specifically, an 8-point dilution series was used to assess, in duplicate, the potency and efficacy of each compound in the original assay (\\\\textit{Pbluc}). Additionally, the tendency of the assayed compounds to produce false positives and other experimental artifacts was investigated by performing a series of counter-screens that measure hepatic cytotoxicity (\\\\textit{HepG2tox}) and interference with the luciferase-based luminescent readout (\\\\textit{Ffluc}). The fact that all bioactivity measurements are (1) generated using biological duplicates and (2) associated with quantitative measures that reflect their likelihood to produce confounding experimental artifacts substantially improves the reliability of the resulting labels.\\n\\nTo facilitate the integration of bioactivity and counter-screen measurements and make the data more amenable to predictive modeling, the \\\\textit{IC}_{50} values that quantify the concentration at which a molecule produces half of its maximum inhibitory effect were converted to binary labels. Specifically, all compounds with an \\\\textit{IC}_{50} \\\\leq 1.5 \\\\mu M were denoted as active while all compounds with an \\\\textit{IC}_{50} \\\\geq 3 \\\\mu M were denoted as inactive, discarding 652 compounds with 1.5 \\\\mu M \\\\leq \\\\textit{IC}_{50} \\\\leq 3 \\\\mu M and assigning qualified \\\\textit{IC}_{50} values to the appropriate class (see Figure 6 for a diagram of the \\\\textit{IC}_{50} distribution and the applied thresholds).\\n\\nIn order to integrate information from the \\\\textit{HepG2tox} and \\\\textit{Ffluc} counter-screens and filter out problematic compounds that are likely false positives or risk confounding the evaluation in other ways, the thresholds outlined in Antonova-Koch et al. (2018) were applied. In particular, problematic compounds were discarded due to causing hepatotoxicity or assay interference if their respective \\\\textit{IC}_{50} values met at least one of the criteria outlined in Equation (A.1) and Equation (A.2):\\n\\n\\\\begin{align}\\n\\\\textit{HepG2tox} \\\\text{ IC}_{50} < & 2 \\\\cdot \\\\textit{Pbluc} \\\\text{ IC}_{50} \\\\\\\\\\n\\\\textit{Ffluc} \\\\text{ IC}_{50} < & 2 \\\\cdot \\\\textit{Pbluc} \\\\text{ IC}_{50}\\n\\\\end{align}\\n\\nwhere \\\\( c_{\\\\text{max}} \\\\) denotes the maximum concentration a compound was assayed at. These filtering criteria categorized 764 compounds as inhibiting hepatocyte viability and 446 compounds as interfering with the luminescence readout, including an overlap of 49. Removing these compounds from the dataset results in a total of 8,150 compounds, of which 7,301 (90\\\\%) are labeled as inactive and 849 (10\\\\%) are labeled as active.\"}"}
{"id": "klarner23a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5. All experiments and analyses were performed in Python (Van Rossum and Drake Jr, 1995), using a range of general-purpose classifiers (Appendix B.2.2), multi-layer perceptrons (Appendix B.2.3), deep ensembles (Appendix B.2.4), pre-trained graph neural networks (Appendix B.2.5), GROVER (Appendix B.2.6), various domain adaptation and generalization techniques (Appendix B.2.7). The test sets were generated using different random seeds. The best models within a margin of statistical significance are highlighted in bold. Following model training and hyperparameter selection, the predictive accuracy and calibration of the estimated test-set label probabilities were assessed using the area under the precision-recall curve (AUC-PRC) and the adaptive calibration error (ACE; Nixon et al. (2019)). AUC-PRC and ACE are particularly well-suited for imbalanced datasets, and provide a characterization of model performance that closely aligns with AUC-ROC and Brier scores. These metrics allow for a direct comparison of models across test sets with different label distributions (see Table 2). In addition, the performance of each algorithm was evaluated using the area under the ROC curve (AUC-ROC) and the Brier score. Additional experimental details are provided in Appendix B.2, describing the implementation and hyperparameter optimization of Q-SA VI (https://github.com/leojklarner/Q-SA VI).\"}"}
{"id": "klarner23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.2. Model Implementations and Hyperparameter Ranges\\n\\nTo ensure a fair and meaningful comparison of the evaluated machine learning models, the hyperparameters of each algorithm were independently optimized for every data split and featurization technique. The following sections provide comprehensive details about the implementation and hyperparameter ranges used for each model in our empirical evaluation.\\n\\n\u2022 Logistic Regression (Section B.2.1)\\n\u2022 Random Forest Classifiers (Section B.2.2)\\n\u2022 Multi-layer Perceptrons (Section B.2.3)\\n\u2022 Deep Ensembles (Section B.2.4)\\n\u2022 Pre-trained Graph Neural Networks (Section B.2.5)\\n\u2022 GROVER (Section B.2.6)\\n\u2022 Domain Adaptation and Generalization Techniques (Section B.2.7)\\n\u2022 Our Probabilistic Regularization Scheme (Section B.2.8)\\n\\nB.2.1. Logistic Regression\\n\\nThe logistic regression models were trained with the scikit-learn library (Pedregosa et al., 2011) using the LIBLINEAR solver (Fan et al., 2008) with a maximum of 1000 iterations and a stopping tolerance of $1 \\\\times 10^{-4}$. They were independently fit for all hyperparameter combinations specified in Table 6, using the combination with the best unweighted validation set log-likelihood to choose the best hyperparameter setting to evaluate on the held-out test set.\\n\\n| Model Hyperparameter Search Space |\\n|----------------------------------|\\n| Linear Regression regularization type | $\\\\ell_1$, $\\\\ell_2$ |\\n| regularization strength | $1 \\\\times 10^{-4}$, $2.6 \\\\times 10^{-4}$, $\\\\ldots$, $3.8 \\\\times 10^{-3}$, $1 \\\\times 10^{-4}$ |\\n| class weight | none, balanced |\\n\\nB.2.2. Random Forest Classifiers\\n\\nThe random forest models were trained with the scikit-learn library (Pedregosa et al., 2011) using 100 decision trees and the GINI splitting criterion. They were independently fit for all hyperparameter combinations specified in Table 7, using the combination with the best unweighted validation set log-likelihood to choose the best hyperparameter setting to evaluate on the held-out test set.\\n\\n| Model Hyperparameter Search Space |\\n|----------------------------------|\\n| Random Forest maximum depth | 5, 15, 26, 36, 47, 57, 68, 78, 89, 100 |\\n| min. samples per split | 5, 15, 50, 100 |\\n| min. samples per leaf | 1, 5, 10, 30, 100 |\\n| class weight | none, balanced |\"}"}
{"id": "klarner23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4. A summary of the test set performance of each model for each of the datasets from the Merck Molecular Activity Challenge, quantified by the mean squared error (\u2193). All entries indicate the mean and standard error computed over 10 independent training runs with different random seeds. The best models within a margin of statistical significance are highlighted in bold.\\n\\n| Model       | HIVPROT count vector | HIVPROT bit vector | DPP4 count vector | DPP4 bit vector | NK1 count vector | NK1 bit vector |\\n|-------------|----------------------|-------------------|-------------------|----------------|------------------|---------------|\\n| L\u2081-Regression | 1.137 \u00b1 0.000        | 1.714 \u00b1 0.000     | 1.611 \u00b1 0.000     | 1.130 \u00b1 0.000  | 0.482 \u00b1 0.000    | 0.442 \u00b1 0.000  |\\n| L\u2082-Regression | 0.999 \u00b1 0.000        | 0.723 \u00b1 0.000     | 1.495 \u00b1 0.000     | 1.143 \u00b1 0.000  | 0.498 \u00b1 0.000    | 0.436 \u00b1 0.000  |\\n| Random Forest | 0.815 \u00b1 0.009        | 0.834 \u00b1 0.010     | 1.473 \u00b1 0.008     | 1.461 \u00b1 0.012  | 0.458 \u00b1 0.002    | 0.438 \u00b1 0.002  |\\n| MLP         | 0.768 \u00b1 0.014        | 2.118 \u00b1 0.015     | 1.393 \u00b1 0.024     | 1.094 \u00b1 0.029  | 0.443 \u00b1 0.007    | 0.399 \u00b1 0.006  |\\n| Q-SAVI      | 0.682 \u00b1 0.019        | 0.664 \u00b1 0.028     | 1.332 \u00b1 0.017     | 1.028 \u00b1 0.027  | 0.436 \u00b1 0.007    | 0.387 \u00b1 0.012  |\\n\\nInstead, our evaluation focused on the three most covariate- and label-shifted datasets (see Table 3), repurposing the remaining data as an anonymized context point distribution. All methods were evaluated following the protocol outlined in Section 5, with full details presented in Appendix C.\\n\\nThe performance metrics for our method and the baseline algorithms investigated in Ma et al. (2015) are presented in Table 4, demonstrating that Q-SAVI performs favorably across every setting and outperforms all other models on the strongly data-shifted HIVPROT, DPP4, and NK1 datasets by a substantial and statistically significant margin.\\n\\n6. Summary and Conclusions\\n\\nThe objective of early-stage drug discovery is to identify lead compounds that exhibit sufficient evidence of modulating a given disease phenotype\u2014as well as suitable safety profiles\u2014to qualify them for further investigation in in vivo studies. Computational techniques that reliably predict the properties of novel molecules in unexplored regions of chemical space have the potential to substantially accelerate this time- and resource-intensive process. Motivated by the practical importance of developing such methods, we derived Q-SAVI, a probabilistic model that allows encoding explicit, problem-informed prior knowledge about the prediction domain into neural network training.\\n\\nTo construct a robust experimental setup and facilitate a practically meaningful evaluation of the proposed method, we carefully pre-processed a high-quality bioactivity dataset and explored different domain-specific statistics to quantify distribution shifts in this setting. Using these statistics to highlight the limited extent to which commonly used random and scaffold splits are able to induce meaningful covariate and label shifts, we built on two alternative molecular weight- and spectral clustering-based approaches to construct challenging train-test splits. Leveraging this extrapolative evaluation setup, we demonstrated that using Q-SAVI to provide neural networks with relevant and contextualized information on drug-like chemical space significantly improves both the predictive accuracy and calibration of neural network models, outperforming a range of state-of-the-art self-supervised pre-training, ensembling, and domain adaptation techniques.\\n\\nThe main limitation of the proposed method compared to standard training regimes is its increased computational cost, due to the amortized cost of having to pre-process a suitable context point distribution and the direct cost of having to perform each forward pass over both a mini-batch and a sample of context points. However, by keeping the size of each context set sample to be roughly comparable to the size of each mini-batch, we found this increase in computational cost to be manageable\u2014especially in comparison to the computational cost of pre-training and fine-tuning related self-supervised methods or deep ensembles.\\n\\nPromising avenues for future work include an investigation into how using Q-SAVI to specify problem-informed modeling preferences may improve the performance of deep learning algorithms for drug discovery applications that heavily rely on out-of-distribution generalization. For instance, the approach could be used to construct an acquisition function for an active learning loop to propose structural modifications that optimize the therapeutic properties of an existing lead compound (Nicolaou et al., 2007; G\u00f3mez-Bombarelli et al., 2018), as Q-SAVI generates robust predictions and additionally enables researchers to explicitly specify desirable exit vectors. It may also accelerate the discovery of novel compound classes that exhibit similar pharmacological properties to already explored molecules (B\u00f6hm et al., 2004; Hu et al., 2017), enabling the optimization of certain pharmacokinetic properties or the circumvention of patent restrictions. More broadly, we hope that this work encourages further research into the utility of probabilistic inference and domain-informed prior distributions over functions for drug discovery and beyond.\\n\\nAcknowledgments\\n\\nWe thank anonymous reviewers for useful feedback. LK is funded by a Clarendon Scholarship. TGJR is funded by a Qualcomm Innovation Fellowship. We gratefully acknowledge the Oxford Advanced Research Computing service for providing computing resources and infrastructure.\"}"}
{"id": "klarner23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Drug Discovery under Covariate Shift with Domain-Informed Prior Distributions over Functions\\n\\nReferences\\n\\nAntonova-Koch, Y., Meister, S., Abraham, M., Luth, M. R., Ottilie, S., Lukens, A. K., Sakata-Kato, T., Vanaerschot, M., Owen, E., Jado, J. C., et al. (2018). Open-source discovery of chemical leads for next-generation chemoprotective antimalarials. Science, 362(6419):eaat9446.\\n\\nArjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. (2019). Invariant risk minimization. arXiv preprint arXiv:1907.02893.\\n\\nBaell, J. B. and Holloway, G. A. (2010). New substructure filters for removal of pan assay interference compounds (pains) from screening libraries and for their exclusion in bioassays. Journal of Medicinal Chemistry, 53(7):2719\u20132740.\\n\\nBajusz, D., Racz, A., and Heberger, K. (2015). Why is tanimoto index an appropriate choice for fingerprint-based similarity calculations? Journal of Cheminformatics, 7(1):1\u201313.\\n\\nBemis, G. W. and Murcko, M. A. (1996). The properties of known drugs. 1. molecular frameworks. Journal of Medicinal Chemistry, 39(15):2887\u20132893.\\n\\nBemis, G. W. and Murcko, M. A. (1999). Properties of known drugs. 2. side chains. Journal of Medicinal Chemistry, 42(25):5095\u20135099.\\n\\nBlundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. (2015). Weight uncertainty in neural networks. volume 37 of Proceedings of Machine Learning Research, pages 1613\u20131622, Lille, France. PMLR.\\n\\nBohacek, R. S., McMartin, C., and Guida, W. C. (1996). The art and practice of structure-based drug design: a molecular modeling perspective. Medicinal Research Reviews, 16(1):3\u201350.\\n\\nB\u00a8ohm, H.-J., Flohr, A., and Stahl, M. (2004). Scaffold hopping. Drug Discovery Today: Technologies, 1(3):217\u2013224.\\n\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-lut, A., Brunskill, E., et al. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.\\n\\nBorgwardt, K. M., Gretton, A., Rasch, M. J., Kriegel, H.-P., Sch\u00a8olkopf, B., and Smola, A. J. (2006). Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):e49\u2013e57.\\n\\nBouchard, M., Jousselme, A.-L., and Dor\u00b4e, P.-E. (2013). A proof for the positive definiteness of the jaccard index matrix. International Journal of Approximate Reasoning, 54(5):615\u2013626.\\n\\nBreiman, L. (2001). Random forests. Machine Learning, 45(1):5\u201332.\\n\\nBronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., and Vandergheynst, P. (2017). Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18\u201342.\\n\\nBurden, F. R., Ford, M. G., Whitley, D. C., and Winkler, D. A. (2000). Use of automatic relevance determination in qsar studies using bayesian neural networks. Journal of Chemical Information and Computer Sciences, 40(6):1423\u20131430.\\n\\nBurden, F. R. and Winkler, D. A. (1999). Robust qsar models using bayesian regularized neural networks. Journal of Medicinal Chemistry, 42(16):3183\u20133187.\\n\\nCherkasov, A., Muratov, E. N., Fourches, D., Varnek, A., Baskin, I. I., Cronin, M., Dearden, J., Gramatica, P., Martin, Y. C., Todeschini, R., et al. (2014). Qsar modeling: where have you been? where are you going to? Journal of Medicinal Chemistry, 57(12):4977\u20135010.\\n\\nCortes, C. and Vapnik, V. (1995). Support-vector networks. Machine Learning, 20:273\u2013297.\\n\\nDahlin, J. L., Nissink, J. W. M., Strasser, J. M., Francis, S., Higgins, L., Zhou, H., Zhang, Z., and Walters, M. A. (2015). Pains in the assay: chemical mechanisms of assay interference and promiscuous enzymatic inhibition observed during a sulfhydryl-scavenging hts. Journal of Medicinal Chemistry, 58(5):2091\u20132113.\\n\\nErtl, P. (2003). Cheminformatics analysis of organic substituents: identification of the most common substituents, calculation of substituent properties, and automatic identification of drug-like bioisosteric groups. Journal of Chemical Information and Computer Sciences, 43(2):374\u2013380.\\n\\nFan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. (2008). Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871\u20131874.\\n\\nFarquhar, S., Osborne, M. A., and Gal, Y. (2020). Radial Bayesian neural networks: Beyond discrete support in large-scale Bayesian deep learning. In Chiappa, S. and Calandra, R., editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 1352\u20131362. PMLR.\\n\\nFinn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u20131135. PMLR.\"}"}
{"id": "klarner23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "klarner23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
