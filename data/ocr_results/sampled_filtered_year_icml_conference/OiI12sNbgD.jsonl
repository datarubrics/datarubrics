{"id": "OiI12sNbgD", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we analyze the pre-trained backbones based on what aspects of the observations they concentrate on. We employed Eigen-CAM (Muhammad & Yeasin, 2020), a visualization method frequently used in computer vision and deep learning. We randomly sampled an observation from two In-Distribution and two Near-Out-of-Distribution games, and applied Eigen-CAM on our main models in Section 6.\\n\\nC.3.1. In-Distribution Environment\\nFor In-Distribution environments, we employ 'Space Invaders' and 'Boxing'. The outcomes of this analysis are illustrated in Figure 11. It was observed that when the encoder is pre-trained solely with images, it struggles to capture objects relevant to the task. However, when pre-trained with data enriched with additional information such as temporal dynamics and task-relevant information, and values of states and state-action pairs, the encoder effectively identifies meaningful objects.\\n\\nFigure 11: Eigen-CAM analysis of ID games. (a) Space Invaders is a game in which players are required to evade enemy attacks while eliminating as many adversaries as possible through vertical shooting. (b) Boxing is a game where the player aims to score more points than their opponent by striking the opponent's face as many as possible within a set time limit. When looking at Eigen-CAM images, it's evident that encoders pre-trained with temporal dynamics and task-related information such as video, action, and reward, rather than just spatial information from images, are more effective in identifying objects relevant to the task. Additionally, encoders trained using demonstration data, which includes task-relevant information, show improved ability in capturing and understanding important features for the task.\"}"}
{"id": "OiI12sNbgD", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"We also analyze the pre-trained encoder in an out-of-distribution environment, which was not used during its pre-training phase. Specifically, 'Assault' and 'Pong' are utilized for this purpose. The outcomes are shown in Figure 12. Similar to the situation with an in-distribution environment, encoders pre-trained on images alone had difficulty in targeting significant objects, while those pre-trained on a richer dataset demonstrated effective identification of important objects.\\n\\nFigure 12: Eigen-CAM analysis of Near-OOD games. (a) Assault is a game where players must avoid enemy assaults and eliminate as many opponents as possible through either vertical or horizontal shooting. (b) Pong is a game where the player who reaches a specified score wins by scoring points while preventing the opponent from hitting the ball. Similar to what we observe in ID, encoders that are trained on temporal dynamics, task-related information, and the value between states and actions using videos, demonstrations, and trajectories demonstrate superior ability in recognizing task-related objects, compared to those that only learn spatial aspects from images.\\n\\nD. Full Results\\n\\nWe provide the individual game scores of all algorithms in our experiments. The scores are recorded at the end of fine-tuning and averaged over 3 seeds. We also include the mean scores of a randomly behaving agent (aliased RndmAgnt) and the DQN scores reported by Castro et al. (2018), which are used to compute the DQN normalized score with the formula:\\n\\n$$s_{DNS} = \\\\frac{s_{agent} - s_{min}}{s_{max} - s_{min}}$$\\n\\nwhere $s_{max} = \\\\max(s_{DQN}, s_{RndmAgnt})$, $s_{min} = \\\\min(s_{DQN}, s_{RndmAgnt})$, and $s_{agent}$ is the score to be normalized. For Far-OOD games, instead of DQN, we report the scores of Rainbow (Hessel et al., 2018) trained for 2M steps and use them for normalization. In addition to IQM, we report the optimality gap of the normalized scores.\"}"}
{"id": "OiI12sNbgD", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| ElevatorAction | IQM(RNS) | BasicMath | TimePilot | Robotank | Pitfall | Aliens | Circuitboard | Asteroids | Breakout | Frostbite | Bowling | Surround | Othello | Venture | JungleJamboree | DiveCourse | IQM(RNS) |\\n|---------------|----------|-----------|-----------|----------|---------|-------|-------------|----------|----------|----------|---------|---------|--------|-------|----------------|-----------|---------|\\n| KungFuMaster  | JourneyEscape | DemonAttack | FishingDerby | UpNDown | PrivateEye | IceHockey | IQM(RNS) | BasicMath | TimePilot | Robotank | Pitfall | Aliens | Circuitboard | Asteroids | Breakout | Frostbite | Bowling | Surround | Othello | Venture | JungleJamboree | DiveCourse | IQM(RNS) |\\n\\nD.1. Main Results\\n\\nOptimality Gap (DNS)\\n\\n| ElevatorAction | IQM(RNS) | BasicMath | TimePilot | Robotank | Pitfall | Aliens | Circuitboard | Asteroids | Breakout | Frostbite | Bowling | Surround | Othello | Venture | JungleJamboree | DiveCourse | IQM(RNS) |\\n|---------------|----------|-----------|-----------|----------|---------|-------|-------------|----------|----------|----------|---------|---------|--------|-------|----------------|-----------|---------|\\n| KungFuMaster  | JourneyEscape | DemonAttack | FishingDerby | UpNDown | PrivateEye | IceHockey | IQM(RNS) | BasicMath | TimePilot | Robotank | Pitfall | Aliens | Circuitboard | Asteroids | Breakout | Frostbite | Bowling | Surround | Othello | Venture | JungleJamboree | DiveCourse | IQM(RNS) |\"}"}
{"id": "OiI12sNbgD", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Game       | Optimality Gap(DNS) | Optimality Gap(RNS) |\\n|------------|---------------------|---------------------|\\n| MontezumaRevenge |                     |                     |\\n| ChopperCommand  |                     |                     |\\n| KungFuMaster    |                     |                     |\\n| YarsRevenge     |                     |                     |\\n| DoubleDunk      |                     |                     |\\n| Tutankham       |                     |                     |\\n| BeamRider       |                     |                     |\\n| TimePilot       |                     |                     |\\n| Surround        |                     |                     |\\n| Gravitar         |                     |                     |\\n| Venture         |                     |                     |\\n| Solaris         |                     |                     |\\n| Asterix          |                     |                     |\\n| Skiing          |                     |                     |\\n| Othello         |                     |                     |\\n| Pitfall         |                     |                     |\\n| Game            |                     |                     |\\n| Klax            |                     |                     |\\n| Investigations for Generalization in Vision-Based Reinforcement Learning | 12850 | 3092 | 3568 | 2090 | 2360 | 307 | 24 | 52 | 371 | 363 | 1 | 0 | 2 | 0 | 11 | 91 | 18 | 9 | 0 | 2 | 0 | 1334 | 3 |\\n| 8488 | 0 1 | 221 | 15 | 0 | 2676 | \u2212 | 932 | 8204 | 109 | 478 | 856 | 1 | 4777 | 1431 |\\n| 89 | 2168 | 357 | 105 | 288 | 1913 | 822 | 4583 | 900 | 81 | 2036 | 7 | 24 | 5498 | 419 | 5 | \u2212 | 701 | 15487 | 2393 | \u2212 | 566 | 21 | 193 | 601 | \u2212 | 39 | 109 | 176 | \u2212 | 219 | 739 | 25 | 3196 | 201 | \u2212 | 165 | 0 | 7125 | 603 | 415 | 3 | 1447 | 3400 | 567 | 803 | 0 | 40 | 1 | 221 | 15 | 0 | 2676 | \u2212 | 932 | 8204 | 109 | 478 | 856 | 1 | 4777 | 1431 |\"}"}
{"id": "OiI12sNbgD", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.3. Fine-tuning\\n\\nAfter pre-training, we assess the pre-trained models on two downstream adaptation scenarios: Offline Behavioral Cloning (BC) and Online Reinforcement Learning (RL). Each task is conducted on three distinct sets of games (ID, Near-OOD, Far-OOD), including two sets unseen during pre-training. Results are compiled from three independent runs (seeds). The backbone network is kept frozen to focus on the quality of representations, while other components are re-initialized. Any modifications revert to the standard architecture as detailed in Section 5.2 and Section B.1. This process is uniformly applied across all games.\\n\\nB.3.1. Offline BC\\n\\nIn Offline BC, expert demonstrations are used for learning control. Specifically, the fine-tuning dataset for ID and Near-OOD games are sampled from DQN-Replay-Dataset. From the five runs, we use the last checkpoints and sample the initial 10,000 interactions. For Far-OOD games, we train a Rainbow agent on each environment for 2M steps and record the last 50,000 interactions. As a result, each fine-tuning stage is performed over a 50k dataset for 100 epochs. The final performance is evaluated by the average gameplay score of 100 trials.\\n\\nTable 15: Hyperparameters for downstream Offline BC.\\n\\n| Hyperparameter       | Value                  |\\n|----------------------|------------------------|\\n| Augmentation         | [Random Shift, Intensity] |\\n|                      | Random shift pad 4      |\\n|                      | Intensity scale 0.05    |\\n| Learning rate scheduler | Cosine annealing with warmup |\\n|                      | Warmup ratio 0.1        |\\n|                      | Initial learning rate ratio 0.1 |\\n|                      | Base learning rate 1e-3 |\\n|                      | Weight decay 1e-4       |\\n| Optimizer (\\\\(\\\\beta_1, \\\\beta_2\\\\)) | AdamW (0.9, 0.999)    |\\n|                      | Batch size 512          |\"}"}
{"id": "OiI12sNbgD", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Following the standard experimental setup from sample-efficient Atari benchmark (Kaiser et al., 2019; Schwarzer et al., 2021b), we train a Q-learning layer on top of the frozen encoder via the Rainbow algorithm (Hessel et al., 2018). For the sake of simplicity, we do not use noisy layers and use epsilon greedy for exploration. Similar to the Offline BC, we trained the model for 50k steps and the performance is measured by the average gameplay score over 100 attempts. Detailed hyperparameters are listed in Table 16.\\n\\nTable 16: Hyperparameters for downstream Online RL.\\n\\n| Hyperparameter      | Value                  |\\n|---------------------|------------------------|\\n| Augmentation         | [Random Shift, Intensity] |\\n| Random shift pad     | 4                      |\\n| Intensity scale      | 0.05                   |\\n| Training steps       | 50k                    |\\n| Update Distributional Q | True                  |\\n| Support of Q-distribution | 51                   |\\n| Discount factor $\\\\gamma$ | 0.99                  |\\n| Batch size           | 32                     |\\n| Optimizer ($\\\\beta_1$, $\\\\beta_2$, $\\\\epsilon$) | Adam (0.9, 0.999, 0.000015) |\\n| Learning rate        | 0.0001                 |\\n| Max gradient norm    | 10                     |\\n| Priority exponent    | 0.5                    |\\n| Priority correction  | 0.4                    |\\n| Exploration Schedule (start, end, steps) | Epsilon Greedy (50k, 1.0, 0.02) |\\n| Replay buffer size   | 50k                    |\\n| Min buffer size for sampling | 2000                   |\\n| Replay per training step | 1                    |\\n| Updates per replay step | 2                     |\\n| Multi-step return length | 10                    |\\n| Q-head hidden units  | 1024                   |\\n| Q-head non-linearity | ReLU                   |\\n| Evaluation trajectories | 100                |\"}"}
{"id": "OiI12sNbgD", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nFigure 9: The results of offline BC and online RL in environments where only the mode is changed from the pre-training game environments.\\n\\nC. Extended Experimental Results\\n\\nHere, we present additional experimental results that were not included in the Section 6.\\n\\nC.1. Game-Mode Change\\n\\nFor Near-OOD in the main experiment (Figure 4), we fine-tuned with offline BC and online RL using games that were different from the pre-training but belonged to the same task genre. However, some might think that simply changing the modes of the games used in pre-training would be more suitable for Near-OOD (Machado et al., 2018). The reason we chose the Near-OOD environment in the main experiment is that changing the mode results in slight changes, such as alterations in the color or shape of objects. Therefore, we thought it was unrealistic because it is very similar to the pre-training environment, indicating that the fine-tuning environment is almost the same as the pre-training environment. For example, let's assume we are training a house-cleaning robot using the pretrain-then-finetune method. In this case, changing the mode is analogous to training the robot on different colored/shaped objects within the same house in the pre-training stage. Assuming that the fine-tuning environment is the same as the pre-training environment is unrealistic.\\n\\nTo support our selection, we conducted offline BC and online RL fine-tuning experiments by changing the modes of the games listed in Table 17. Additionally, to match the scale of the results for each game, we normalized with the final scores of the Rainbow agent trained with 2 million steps as in the Far-OOD setting of the main experiment. The experimental results are shown in Figure 9. Similar to Figure 4(a), learning reward-specific knowledge yields best performance even in the Game-Mode Change environment. This means that the reward function in the Game-Mode Change environment is very similar to the pre-training environment. In other words, simply changing the game mode is not significantly different from the ID.\\n\\nTable 17: Games used in the game mode change experiment.\\n\\n| Games(Mode)         |\\n|---------------------|\\n| AirRaid(5), Asteroids(17), Atlantis(3), BankHeist(5), Berzerk(7), Breakout(7), CrazyClimber(3), DemonAttack(3), DoubleDunk(9), Freeway(5), Gravitar(3), Hero(3), Krull(3), MsPacman(3), PrivateEye(3), Skiing(6), SpaceInvaders(9), StarGunner(3), Tutankham(3), YarsRevenge(3), Zaxxon(3) |\\n\\nC.2. Relationship Between Object Size and Mask-Based Methods\\n\\nOne unique property of the Atari environments is the large variance of object sizes, ranging from few hundred pixels to mere 1-2 pixels. This led us to hypothesize that mask-based reconstruction methods may struggle to deal with small objects, as these objects can be entirely occluded by masking operation.\\n\\nTo investigate this, we categorized ID and Near-OOD environments by their object size (see Table 18) and compared the mask-based methods (MAE, SiamMAE) against latent reconstruction methods in the same category (CURL, ATC).\"}"}
{"id": "OiI12sNbgD", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nIllustrated in Figure 10, the results indeed support our proposition. Mask-based methods underperformed with small objects and excelled with larger objects, implying that the object size in environments can play a crucial role in these models.\\n\\nFigure 10: Mask-based methods relative to object sizes.\\n\\nA comparative analysis of mask-based versus non-mask methods in ID and Near-OOD games, classified by object size (small, medium, large). The results indicate that mask-based methods tend to excel more in games with larger objects compared to smaller ones.\\n\\nTable 18: Games categorized by object size. Far-OOD games are not included.\\n\\n| Object size | Games                                                                 |\\n|------------|----------------------------------------------------------------------|\\n| Small      | Amidar, BankHeist, Bowling, Breakout, Centipede, Gravitar, Jamesbond, Krull, MsPacman, Phoenix, Pitfall, Pong, Pooyan, Riverraid, Skiing, StarGunner, TimePilot, Tutankham, Venture, VideoPinball |\\n| Medium     | Alien, Asterix, Asteroids, Atlantis, BeamRider, Berzerk, Carnival, ChopperCommand, CrazyClimber, DemonAttack, DoubleDunk, ElevatorAction, FishingDerby, Freeway, Frostbite, Hero, IceHockey, JourneyEscape, Kangaroo, MontezumaRevenge, NameThisGame, PrivateEye, Qbert, Robotank, Seaquest, Solaris, SpaceInvaders, Tennis, WizardOfWor, YarsRevenge |\\n| Large      | AirRaid, Assault, BattleZone, Boxing, Enduro, Gopher, KungFuMaster, RoadRunner, UpNDown, Zaxxon |\"}"}
{"id": "OiI12sNbgD", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nDonghu Kim\\n* 1\\nHojoon Lee\\n* 1\\nKyungmin Lee\\n* 1\\nDongyoon Hwang\\n1\\nJaegul Choo\\n1\\n\\nAbstract\\n\\nRecently, various pre-training methods have been introduced in vision-based Reinforcement Learning (RL). However, their generalization ability remains unclear due to evaluations being limited to in-distribution environments and non-unified experimental setups. To address this, we introduce the Atari Pre-training Benchmark (Atari-PB), which pre-trains a ResNet-50 model on 10 million transitions from 50 Atari games and evaluates it across diverse environment distributions. Our experiments show that pre-training objectives focused on learning task-agnostic features (e.g., identifying objects and understanding temporal dynamics) enhance generalization across different environments. In contrast, objectives focused on learning task-specific knowledge (e.g., identifying agents and fitting reward functions) improve performance in environments similar to the pre-training dataset but not in varied ones. We publicize our codes, datasets, and model checkpoints at https://github.com/dojeon-ai/Atari-PB.\\n\\n1. Introduction\\n\\nThe pretrain-then-finetune approach has become a standard practice in computer vision (CV) and natural language processing (NLP), renowned for its robust generalization across diverse tasks (He et al., 2022; Bubeck et al., 2023). In vision-based Reinforcement Learning (RL), this approach is now gaining attraction as well (Levine et al., 2020; Yang et al., 2023), driven by the increasing availability of large-scale offline datasets (Grauman et al., 2022; Padalkar et al., 2023).\\n\\nIn vision-based RL, various pre-training methods are designed to capture unique features from different data types. *\\\\(1\\\\) Equal contribution\\n\\n1 KAIST. Correspondence to: Donghu Kim <quagmire@kaist.ac.kr>.\\n\\nProceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\\n\\nFigure 1: Overview of Atari-PB. The ResNet-50-based model is pre-trained from 10M interactions with a given pre-training algorithm. The pre-trained model is then evaluated by fine-tuning to In-Distribution (ID), Near-Out-of-Distribution (Near-OOD), and Far-Out-of-Distribution (Far-OOD) environments.\\n\\nImage-based methods, for instance, focus on extracting spatial characteristics such as object sizes and shapes (Laskin et al., 2020b; Seo et al., 2023a). In contrast, video-based approaches delve into the temporal dynamics of environments like objects' movement and direction (Schwarzer et al., 2020b; Nair et al., 2022; Gupta et al., 2023). Those learning from demonstrations prioritize extracting task-relevant features, distinguishing agents from irrelevant elements like backgrounds (Pomerleau, 1991; Christiano et al., 2016; Islam et al., 2022). Finally, trajectory-based methods further concentrate on learning task-specific features by estimating the rewards associated with different states and actions (Kumar et al., 2020; Chen et al., 2021; Fujimoto et al., 2019).\\n\\nDespite their advancements, the generalization capabilities of these methods remain underexplored as evaluations are typically confined to environments akin to their pre-training datasets (Schwarzer et al., 2021b; Lee et al., 2023). Several studies have probed the generalization ability of the agents by changing visual and task attributes, such as object colors (Hansen & Wang, 2021), shapes (Yuan et al., 2023), and physics (Taiga et al., 2022). Yet, these variations are relatively minor and may not sufficiently mirror\"}"}
{"id": "OiI12sNbgD", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nFigure 2: Results Overview.\\n\\nThe pre-training methods are evaluated by their performance after fine-tuning to environments in three groups: In-Distribution, Near-Out-of-Distribution, and Far-Out-of-Distribution. Here, we report the results of fine-tuning via behavior cloning (i.e., replicating expert behavior) and average the scores of each algorithm category for a comprehensive analysis.\\n\\nThe complexities found in real-world scenarios. More recent research has begun to investigate the pre-trained models' generalization capabilities under larger distribution shifts (Nair et al., 2022; Xiao et al., 2022; Parisi et al., 2022; Majumdar et al., 2023a). However, these studies use models pre-trained on different data sources and with varying architectures, complicating the understanding of how learning objectives affect generalization performance.\\n\\nTo address this gap, we introduce the Atari Pre-training Benchmark (Atari-PB), which investigates the generalization capability of various learning objectives using a unified dataset and architecture. Our benchmark begins by pre-training a ResNet-50 encoder (He et al., 2016) on a dataset containing 10 million interactions from 50 different Atari environments (Agarwal et al., 2020). The pre-trained models are then fine-tuned across three groups of environments: In-Distribution (ID), Near-Out-of-Distribution (Near-OOD), and Far-Out-of-Distribution (Far-OOD). The ID group includes environments identical to those in the pre-training dataset. The Near-OOD group consists of similar tasks (e.g., shooting or tracking) but with different visual characteristics (e.g., object shape or speed). The Far-OOD group contains environments with entirely different tasks (e.g., solving math puzzles or color matching).\\n\\nOur findings, illustrated in Figure 2, show that pre-training methods aimed at learning task-agnostic features, such as extracting spatial characteristics from images and temporal dynamics from videos, enhance generalization across various distribution shifts. In contrast, methods that learn task-specific knowledge, such as identifying agents from demonstrations and fitting reward functions from trajectories, enhance performance in the same environments but hinder generalization under distribution shifts.\\n\\nTakeaways:\\n\u2022 Pre-training objectives that learn task-agnostic features, such as identifying objects from images and understanding temporal dynamics from videos, consistently improve generalization across various distribution shifts.\\n\u2022 Pre-training objectives that learn task-specific features, such as focusing on agents from demonstrations and fitting the reward function from trajectories, improve performance in similar tasks but lose effectiveness as task distribution shifts increase.\\n\\n2. Related Work\\n\\n2.1. Evaluating Generalization in Vision-Based RL\\n\\nAs Deep Reinforcement Learning has achieved notable success in specific games or tasks (Hafner et al., 2023; Schwarzer et al., 2023), research interest is moving towards the agent's ability to generalize on environments with visual and/or task distribution shifts.\\n\\nFor visual generalization, researchers have focused on developing environments with controllable visual elements. This includes changing the design of objects and walls (Lomonaco et al., 2020), adding background noise (Hansen & Wang, 2021; Stone et al., 2021), and applying realistic disturbances such as changes in lighting and camera perspectives (Dosovitskiy et al., 2017; Yuan et al., 2023).\\n\\nRegarding task distribution shifts, numerous studies aimed to evaluate the agents on new tasks with similar dynamics but varying reward functions. For instance, Farebrother et al. (2018); Taiga et al. (2022) evaluated generalization abilities using different game modes and difficulty levels, while other works employed procedural generation (Cobbe et al., 2020; Zhu et al., 2020).\\n\\nDespite the importance of these studies, they have primarily considered a single type of distribution shift, often minor in nature. Thus, we propose evaluating agent performance in environments with novel tasks and visual shifts (Far-OOD), those with similar tasks and visual shifts (Near-OOD), and those with same tasks (ID). This broader coverage of distribution shifts enables a comprehensive assessment of agents' generalization abilities under visual and task shifts.\\n\\n2.2. Pre-training for Generalization in Vision-based RL\\n\\nThe pretrain-then-finetune approach is a well-established framework in CV and NLP for improving generalization ability. Similarly for visual RL, the development of large-scale offline datasets (Grauman et al., 2022; Padalkar et al., 2023) have spurred investigations into the efficacy of pre-training methods for enhancing generalization capabilities.\"}"}
{"id": "OiI12sNbgD", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"information via cross attention. For image reconstruction, we use the same head as our MAE. Overall, we follow the training procedure stated by Gupta et al. (2023) with some modifications. First, for the same reason as MAE, we use convolution feature masking instead of patch masking. Second, we use the transformer architecture used to train our MAE. Third, we reduce the future sampling window from [4, 48] to [1, 3].\\n\\nTable 6: Hyperparameters for pre-training SiamMAE.\\n\\n| Hyperparameter      | Value                  |\\n|---------------------|------------------------|\\n| Epochs              | 100                    |\\n| Base learning rate  | 3e-4                   |\\n| Weight decay        | 5e-2                   |\\n| Optimizer (\\\\(\\\\beta_1, \\\\beta_2\\\\)) | AdamW (0.9, 0.95) |\\n| Batch size          | 512                    |\\n| Steps to future state (\\\\(k, k'\\\\)) | [1,3]                |\\n| Mask ratio (\\\\(\\\\rho\\\\)) | 0.95                  |\\n| Transformer embedding dimensions | 512         |\\n| Transformer MLP ratio | 4                    |\\n| Transformer heads   | 4                      |\\n| Transformer encoder layers | 3                   |\\n| Transformer decoder layers | 4            |\\n| (Head) Linear output dimensions | 768          |\\n\\nB.2.5. R3M\\n\\nReusable Representations for Robot Manipulation (Nair et al., 2022) uses multiple losses to learn from human demonstration videos with diverse tasks, but we focus our study on the time contrastive loss. Although similar to InfoNCE loss in ATC (Stooke et al., 2021), the main difference is that in addition to the future (target) image \\\\(o_t + k\\\\), a further-future image \\\\(o_t + k'\\\\) (\\\\(k < k'\\\\)) is sampled as a 'hard' negative. Unfortunately, we find pre-training with the original time contrastive loss to be challenging. In order to preserve the aforementioned idea, we implement R3M as 'ATC with an additional hard negative' and use the following loss function:\\n\\n\\\\[\\nL_{R3M} = \\\\frac{-1}{|B|} \\\\sum_{b \\\\in B} \\\\log \\\\frac{\\\\exp(y_{bt} \\\\cdot q_{bt} + k)}{\\\\sum_{b' \\\\notin B} \\\\exp(y_{bt} \\\\cdot q_{b't} + k') + \\\\exp(y_{bt} \\\\cdot q_{bt} + k')}\\n\\\\]\\n\\nNaturally, we use the same momentum networks as ATC; all future images are passed through momentum backbone and neck, which are updated every iteration.\\n\\nTable 7: Hyperparameters for pre-training R3M.\\n\\n| Hyperparameter      | Value                  |\\n|---------------------|------------------------|\\n| Epochs              | 100                    |\\n| Base learning rate  | 3e-4                   |\\n| Weight decay        | 1e-5                   |\\n| Optimizer (\\\\(\\\\beta_1, \\\\beta_2\\\\)) | AdamW (0.9, 0.999) |\\n| Batch size          | 512                    |\\n| Steps to future states (\\\\(k, k'\\\\)) | 3, 6               |\\n| Momentum update ratio (\\\\(\\\\tau\\\\)) | [0.9, 0.999]        |\\n| Output representation dimensions | 512      |\\n| Similarity measure  | Dot product            |\"}"}
{"id": "OiI12sNbgD", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Behavioral cloning learns to predict the action $a_t$ from its observation $o_t$. Predictions are made by a single pass through the model $y_t = h(g(f(o_t)))$, of which goal is to minimize the cross entropy loss between $y_t$ and $a_t$.\\n\\nTable 8: Hyperparameters for pre-training BC.\\n\\n| Hyperparameter       | Value        |\\n|----------------------|--------------|\\n| Epochs               | 100          |\\n| Base learning rate   | 3e-4         |\\n| Weight decay         | 1e-5         |\\n| Optimizer ($\\\\beta_1, \\\\beta_2$) | AdamW (0.9, 0.999) |\\n| Batch size           | 512          |\\n\\nSelf Predictive Representations (Schwarzer et al., 2020b) learn to recursively predict future states from a starting state and subsequent actions. Given an image $o_t$ and a sequence of actions $a_{t+1}^K$, the model's goal is to predict the consequent future images $o_{t+1}^K$ in the latent space. The current image is encoded using an 'online' network to get $q_t = f(g(o_t))$, which is used in tandem with the actions to predict future representations $y_{t+1}^K = h(q_t, a_{t+1}^{K-1})$. The target representations are obtained by encoding the future images with a 'momentum' encoder: $q_i = f'(g'(o_i))$, $i \\\\in [t+1, t+K]$.\\n\\nIn our experiments, we make the following modifications. First, we use RNN instead of recursive CNN, which is put in front of the linear layer in the head network. A game-wise action embedding for the RNN input, as each game uses a different set of actions. Second, we use contrastive loss instead of cosine similarity loss. Third, we discard the Q-learning loss. As a result, we use the following loss:\\n\\n$$L_{SPR} = - \\\\sum_{b \\\\in B} \\\\mathbb{E}_{k=1}^{K} \\\\log \\\\exp(y_{bt+k} \\\\cdot q_{bt+k}) \\\\sum_{b' \\\\in B} \\\\mathbb{E}_{k' = 1}^{K} \\\\exp(y_{bt+k} \\\\cdot q_{bt+k}')$$\\n\\nTable 9: Hyperparameters for pre-training SPR.\\n\\n| Hyperparameter       | Value        |\\n|----------------------|--------------|\\n| Epochs               | 25           |\\n| Base learning rate   | 3e-4         |\\n| Weight decay         | 1e-4         |\\n| Optimizer ($\\\\beta_1, \\\\beta_2$) | AdamW (0.9, 0.999) |\\n| Batch size           | 128          |\\n| Prediction sequence length ($K$) | 4 |\\n| Momentum update ratio ($\\\\tau$) | [0.9, 0.999] |\\n| Representation dimensions | 512 |\\n| Similarity measure   | Dot product  |\\n\\nInverse Dynamics Modeling (Christiano et al., 2016) learns to predict the action $a_t$ taken between two successive observations $o_t, o_{t+1}$. Both images are passed through the same backbone and neck to obtain $q_t = g(f(o_t))$ and $q_{t+1} = g(f(o_{t+1}))$, which are concatenated to make an action prediction via $y_t = h(q_t, q_{t+1})$. Same as BC, we use the cross-entropy loss between $y_t$ and $a_t$.\\n\\nIn combining the two algorithms, we do not make any modifications, and simply use the sum of two losses to pre-train the model.\"}"}
{"id": "OiI12sNbgD", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Hyperparameters for pre-training IDM.\\n\\n| Hyperparameter          | Value            |\\n|-------------------------|------------------|\\n| Epochs (early stop)     | 100 (30)         |\\n| Base learning rate      | 3e-4             |\\n| Weight decay            | 1e-5             |\\n| Optimizer (\\\\(\\\\beta_1, \\\\beta_2\\\\)) | AdamW (0.9, 0.999) |\\n| Batch size              | 512              |\\n\\nTable 11: Hyperparameters for pre-training SPR+IDM.\\n\\n| Hyperparameter          | Value            |\\n|-------------------------|------------------|\\n| Epochs                  | 25               |\\n| Base learning rate      | 3e-5             |\\n| Weight decay            | 1e-5             |\\n| Optimizer (\\\\(\\\\beta_1, \\\\beta_2\\\\)) | AdamW (0.9, 0.999) |\\n| Batch size              | 128              |\\n| Prediction sequence length (\\\\(K\\\\)) | 4               |\\n| Momentum update ratio (\\\\(\\\\tau\\\\)) | [0.9, 0.999]     |\\n| Representation dimensions | 512            |\\n| Similarity measure      | Dot product      |\\n\\nB.2.10. CQL\\n\\nConservative Q-Learning (Kumar et al., 2020) is developed for learning Q values in an offline setting. We implement two distinct Q learning methods: Mean Squared Error (MSE) learning and Cross-entropy (Distributional) learning, both based on CQL. We choose ResNet50 as the backbone architecture, with a neck structure consistent with that in Behavioral Cloning (BC) for both MSE and Distributional.\\n\\nMSE:\\nIn MSE-based Q-learning, we employ a game-wise head that yields an output in the shape of \\\\(R^{B \\\\times T \\\\times A}\\\\), where \\\\(B\\\\) denotes the batch size, \\\\(T\\\\) represents the number of time steps, and \\\\(A\\\\) signifies the size of the action space. We balance the MSE loss and CQL loss using a coefficient of 0.1.\\n\\nTable 12: Hyperparameters for pre-training CQL-M.\\n\\n| Hyperparameter          | Value            |\\n|-------------------------|------------------|\\n| Epochs                  | 100              |\\n| Base learning rate      | 1e-4             |\\n| Weight decay            | 1e-5             |\\n| Optimizer (\\\\(\\\\beta_1, \\\\beta_2\\\\)) | AdamW (0.9, 0.95) |\\n| Batch size              | 512              |\\n| CQL coefficient         | 0.1              |\\n\\nDistributional:\\nFor cross-entropy-based distributional Q-learning, we utilize a game-wise head designed to produce an output dimension of \\\\(R^{B \\\\times T \\\\times A \\\\times N_A}\\\\), with \\\\(B, T, A\\\\) having the same implications as in MSE, and \\\\(N_A\\\\) representing the number of atoms. We set a coefficient of 0.1 to balance cross-entropy loss and CQL loss, consistent with Kumar et al. (2022a). Additionally, we adopted a support set of [-10, 10] following the methodology in Bellemare et al. (2017b).\\n\\nB.2.11. DT\\n\\nDecision Transformer (DT) (Chen et al., 2021) adopts a unique perspective by treating reinforcement learning as a sequence modeling problem. It represents each transition within a trajectory as a triplet consisting of total return \\\\(\\\\hat{R}_t\\\\), observation \\\\(o_t\\\\), and action \\\\(a_t\\\\). DT is trained to autoregressively predict these sequences, given the transition history \\\\(S_{t-1}^{i=1}\\\\{\\\\hat{R}_i, o_i, a_i\\\\}\\\\). At inference, this knowledge is leveraged to predict the optimal actions necessary for achieving a desired cumulative reward.\\n\\nIn our implementation, observations are encoded by the backbone and a 2-layer MLP with game-specific spatial embedding, generating 512-dimensional tokens. Actions and returns are separately embedded into 512-dimensional tokens, resulting in 512-dimensional tokens.\"}"}
{"id": "OiI12sNbgD", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 13: Hyperparameters for pre-training CQL-D.\\n\\n| Hyperparameter       | Value                        |\\n|----------------------|------------------------------|\\n| Epochs               | 100                          |\\n| Base learning rate   | $1e^{-4}$                    |\\n| Weight decay         | $1e^{-5}$                    |\\n| Optimizer            | AdamW ($\\\\beta_1, \\\\beta_2$)  |\\n| Batch size           | 512                          |\\n| Support Set          | $[-10, 10]$                  |\\n| CQL coefficient      | 0.1                          |\\n\\nA sequence of length $K \\\\times 3$. This sequence is processed through a causal transformer to produce $K$ outputs. The outputs are then utilized by the head layer to predict actions $a_1:K$. We employ cross-entropy loss for training the model.\\n\\n### Table 14: Hyperparameters for pre-training DT.\\n\\n| Hyperparameter               | Value                        |\\n|------------------------------|------------------------------|\\n| Epochs                       | 12                           |\\n| Base learning rate           | $1e^{-4}$                    |\\n| Weight decay                 | $5e^{-2}$                    |\\n| Optimizer                    | AdamW ($\\\\beta_1, \\\\beta_2$)  |\\n| Batch size                   | 64                           |\\n| Sequence length (in tokens)  | $8 \\\\times 3$                 |\\n| Reward scale                 | 0.01                         |\\n| Transformer embedding dimensions | 512                         |\\n| Transformer MLP ratio        | 4                            |\\n| Transformer heads            | 4                            |\\n| Transformer layers           | 4                            |\"}"}
{"id": "OiI12sNbgD", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The availability of large-scale offline datasets tailored for robotics (Walke et al., 2023; Shah et al., 2023; Mendonca et al., 2023) has significantly influenced reinforcement learning (RL) research. This influence is particularly notable in adopting a pre-trained visual encoder for various downstream tasks (Ye et al., 2022; Ma et al., 2022; Lee et al., 2023).\\n\\nA crucial aspect in the selection of pre-trained models is the type of data employed during pre-training. These algorithms are typically categorized by the data they use: Images, Videos, Demonstrations, and Trajectories. Each category utilizes different objectives to extract distinct features during the pre-training phase.\\n\\n**Image:**\\n\\nImage-based learning encompasses two main strategies. The first focuses on augmentation-invariant representations, achieved by ensuring consistency in the latent space for different augmentations of the same image (Chen et al., 2020a; Laskin et al., 2020b; Chen & He, 2021; Chen et al., 2020b; Grill et al., 2020). The second strategy involves reconstructing heavily masked images, leveraging transformer architectures (He et al., 2022; Seo et al., 2023a; b).\\n\\n**Video:**\\n\\nVideo-based algorithms extend image-based techniques by integrating temporality. Temporal contrastive learning, for example, aims to closely encode temporally adjacent images to understand temporal dynamics (Nair et al., 2022; Ma et al., 2022; Sermanet et al., 2018; Han et al., 2019; Stooke et al., 2021). Other approaches include reconstructing masked images from temporally adjacent frames (Yu et al., 2022; Gupta et al., 2023; Tong et al., 2022; Feichtenhofer et al., 2022), or autoregressively predicting future frames based on past observations (Hafner et al., 2019b; Seo et al., 2022).\\n\\n**Demonstration:**\\n\\nLearning from demonstrations has a wide range of methods. Inverse dynamics learning focuses on predicting actions, given consecutive states (Christiano et al., 2016; Islam et al., 2022; Brandfonbrener et al., 2023). Forward dynamics learning targets predicting future states from current state-action pairs (Schwarzer et al., 2020b; Yu et al., 2021; Lee et al., 2023; Zheng et al., 2023). Imitation learning, on the other hand, aims to replicate the behavior policy demonstrated in the data (Pomerleau, 1991; Zang et al., 2022; Arora et al., 2020; Baker et al., 2022; Caluwaerts et al., 2023). Some approaches combine multiple methods for enhanced robustness (Yu et al., 2022; Zhang et al., 2022).\\n\\n**Trajectory:**\\n\\nTrajectory-based methods maximize the use of reward information available in trajectories. These can be broadly categorized into two settings: online, where data is collected through real-time interaction with the environment (Mnih et al., 2015; Bellemare et al., 2017a; Hafner et al., 2019a; Laskin et al., 2020a), and offline, which involves learning from pre-existing datasets (Fujimoto et al., 2019; Kumar et al., 2020; Lee et al., 2022a; Nakamoto et al., 2023; Wu et al., 2023; Lee et al., 2022b).\\n\\nRecent studies have evaluated the effectiveness of pre-trained visual representations in vision-based RL, employing a variety of pre-training methods (Parisi et al., 2022; Majumdar et al., 2023b; Hu et al., 2023). These investigations have used different models, including ResNet (He et al., 2016) and ViT (Dosovitskiy et al., 2020), which were pre-trained on a wide array of datasets (Deng et al., 2009; Savva et al., 2019; Tassa et al., 2018; Grauman et al., 2022). The findings indicate that while visual representations pre-trained on large and diverse datasets can enhance generalization in downstream tasks, no single pre-trained model consistently excels in generalization across all types of tasks.\"}"}
{"id": "OiI12sNbgD", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Implementation Details for Atari-PB\\n\\nB.1. Pre-training\\n\\nIn this section, we describe our pre-training stage in detail. We first depict the three components of our pre-training model (backbone, neck, and head) and explain how our pre-training dataset was curated. Note that the model description is our most fundamental form, and that adjustments are often made to align with each method's requirements. For such algorithm-specific elements, refer to Section B.2.\\n\\n**Backbone, \\\\( f(\\\\cdot) \\\\):** A backbone network is a game-agnostic spatial feature extractor. We employ a widely used and sufficiently large ResNet-50, but replace batch normalization with group normalization following Kumar et al. (2022a). Given an \\\\( (4, 84, 84) \\\\) input \\\\( o \\\\), the backbone encodes them into a \\\\( (2048, 6, 6) \\\\) feature map \\\\( z = f(o) \\\\).\\n\\n**Neck, \\\\( g(\\\\cdot) \\\\):** The main goal of neck is to encode the feature map into a 512-dimensional vector \\\\( q = g(z) \\\\), while applying learnable spatial embedding. Given the backbone output, each feature map is point-wise multiplied with its game-specific spatial embedding. Spatial pooling and instance normalization is then applied to obtain a 2048-dimensional vector, which is further encoded by a neural network. Unless stated otherwise, we use a 2-layer MLP with ReLU activation, hidden dimension of 1024, and output dimension of 512.\\n\\n**Head, \\\\( h(\\\\cdot) \\\\):** The head makes the prediction \\\\( y = h(q) \\\\), where \\\\( y \\\\)'s dimensionality depends on the task. We employ a multi-head architecture, meaning that multiple neural networks of identical architecture are trained, each one devoted for each game. Unless specified, we use a single linear layer that outputs the action prediction \\\\( y \\\\in \\\\mathbb{R}^A \\\\).\\n\\n**Dataset:** The DQN-Replay-Dataset (Agarwal et al., 2020), a collection of DQN agent's training logs in 60 Atari games, provides 50 million transitions for each game collected across five different runs. These runs are subdivided into 50 checkpoints, each containing 1 million transitions of differing optimality. To fulfill our desiderata of \\\"reflecting the diverse nature of real-world datasets\\\" while keeping it computationally accessible, we choose to compile small segments from multiple runs and checkpoints. Our data creation procedure is thus the following: from the 50 games of our choice, we choose the first 2 runs of each game and the first 10 checkpoints of each run. From each of the 1,000 checkpoints, the initial 10,000 interactions are sampled, resulting in a 10 million dataset. We found the initial 10 checkpoints to be sufficient for covering both suboptimal and expert policies; as shown by the supplementary figures of Agarwal et al. (2020), 40 million steps (end of 10th checkpoint) is enough for DQN agents to achieve reasonable score in all games.\\n\\n### Table 1: Games categorized by distribution.\\n\\n| Distribution          | Games                                                                 |\\n|-----------------------|----------------------------------------------------------------------|\\n| In-Distribution       | AirRaid, Amidar, Asteroids, Atlantis, BankHeist, BattleZone, Berzerk, Bowling, Boxing, Breakout, Carnival, Centipede, ChopperCommand, CrazyClimber, DemonAttack, DoubleDunk, ElevatorAction, Enduro, FishingDerby, Freeway, Frostbite, Gopher, Gravitar, Hero, IceHockey, Jamesbond, Kangaroo, Krull, KungFuMaster, MontezumaRevenge, MsPacman, NameThisGame, Phoenix, Pitfall, PrivateEye, Qbert, RoadRunner, Robotank, Skiing, Solaris, SpaceInvaders, StarGunner, Tennis, TimePilot, Tutankham, UpNDown, VideoPinball, WizardOfWor, YarsRevenge, Zaxxon |\\n| Near-Out-of-Distribution | Alien, Assault, Asterix, BeamRider, JourneyEscape, Pong, Pooyan, Riverraid, Seaquest, Venture |\\n| Far-Out-of-Distribution | BasicMath, HumanCannonball, Klax, Othello, Surround                   |\"}"}
{"id": "OiI12sNbgD", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Global pre-training hyperparameters.\\n\\n| Hyperparameter Value                      |\\n|-----------------------------------------|\\n| Observation rendering (84,84), Grayscale |\\n| Frames stacked 4                         |\\n| Reward discount factor (\u03b3) 1.0 (DT)      |\\n| 0.99 (Rest)                              |\\n| Action space size (|A|) 18            |\\n| Augmentation [Random Shift, Intensity]   |\\n| Random shift pad 4                       |\\n| Intensity scale 0.05                     |\\n| Learning rate scheduler Cosine annealing |\\n| Warmup ratio 0.1                         |\\n| Initial learning rate ratio 0.1          |\\n\\nB.2.1. CURL\\n\\nContrastive Unsupervised Representations for Reinforcement Learning (Laskin et al., 2020b) learns augmentation invariant representations using InfoNCE loss and momentum encoder. Given two augmented versions (two views) of an image (denoted \\\\(o_1, o_2\\\\)), one is passed through an 'online' encoder to get \\\\(q = g(f(o_1))\\\\) and the other is passed through a coupled 'momentum' encoder to get a target \\\\(q^+ = g'(f'(o_2))\\\\). To predict the target, \\\\(q\\\\) is passed through a predictor network to get \\\\(y = h(q)\\\\). The InfoNCE loss is then computed based on \\\\(y\\\\) and \\\\(q^+\\\\):\\n\\n\\\\[\\nL_{CURL} = -\\\\frac{1}{|B|} \\\\sum_{b \\\\in B} \\\\log \\\\frac{\\\\exp(y_b \\\\cdot q_b^+)}{\\\\exp(y_b \\\\cdot q_b^+)} + \\\\sum_{b' \\\\in B - \\\\{b\\\\}} \\\\exp(y_b \\\\cdot q_{b'}^+).\\n\\\\]\\n\\nAs implied in the notations, we use backbone and neck as the encoder and head as the predictor. The momentum networks \\\\(f', g'\\\\) are updated every iteration with a coefficient of \\\\(\\\\tau = 0.99\\\\) that scales linearly up to 0.999. As for the similarity measure, we use dot product instead of bilinear product (Chen et al., 2020b).\\n\\nTable 3: Hyperparameters for pre-training CURL.\\n\\n| Hyperparameter Value                      |\\n|-----------------------------------------|\\n| Epochs (early stop) 100 (20)            |\\n| Base learning rate 3e-6                  |\\n| Weight decay 1e-5                        |\\n| Optimizer (\\\\(\\\\beta_1, \\\\beta_2\\\\)) AdamW (0.9, 0.999) |\\n| Batch size 512                           |\\n| Momentum update ratio (\\\\(\\\\tau\\\\)) [0.9, 0.999] |\\n| Representation dimensions 512           |\\n| Similarity measure Dot product           |\\n\\nB.2.2. MAE\\n\\nMasked Autoencoder (He et al., 2022) learns to reconstruct heavily masked images with transformer encoder-decoder architecture. As we use a convolutional network for our backbone, we naturally turn from masking patches of images to masking pixels of convolutional features, inspired by Seo et al. (2023a). The transformer encoder-decoder module is added at the end of our neck, and the head is used to predict the target pixels. In the neck, we perform masking after applying the game-wise spatial embedding, and pass the 2048-dimensional features through a 2 layer MLP into a sequence of 512-dimensional tokens. These are then processed with a transformer encoder, appended with mask tokens for prediction, and further processed with a transformer decoder. Finally, the head's game-wise linear layer is used to predict the pixels.\"}"}
{"id": "OiI12sNbgD", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nBecause the backbone reduces an \\\\((84, 84)\\\\) image into a \\\\((6, 6)\\\\) spatial embedding (or 36 tokens), the game-wise linear layer maps each token to a 784-dimensional vector to predict a \\\\((4, 14, 14)\\\\) sized patch of the target image.\\n\\nWe follow the hyperparameters provided by He et al. (2022) but use a higher mask ratio \\\\(\\\\rho = 0.9\\\\). We use a 3-layer transformer encoder and a 4-layer transformer decoder, following Seo et al. (2023a).\\n\\nTable 4: Hyperparameters for pre-training MAE.\\n\\n| Hyperparameter               | Value  |\\n|------------------------------|--------|\\n| Epochs                       | 100    |\\n| Base learning rate           | 3e-4   |\\n| Weight decay                 | 5e-2   |\\n| Optimizer \\\\((\\\\beta_1, \\\\beta_2)\\\\) | AdamW (0.9, 0.95) |\\n| Batch size                   | 512    |\\n| Mask ratio \\\\(\\\\rho\\\\)          | 0.9    |\\n| Transformer embedding dimensions | 512    |\\n| Transformer MLP ratio        | 4      |\\n| Transformer heads            | 4      |\\n| Transformer encoder layers   | 3      |\\n| Transformer decoder layers   | 4      |\\n| (Head) Linear output dimensions | 768    |\\n\\nB.2.3. ATC\\n\\nAugmented Temporal Contrast (Stooke et al., 2021) learns temporally predictive representations by maximizing the similarity between the representations of current states and their paired future states, using InfoNCE loss and momentum encoder.\\n\\nThis means that the input image \\\\(o_t\\\\) is encoded by an 'online' encoder into \\\\(q_t = g(f(o_t))\\\\), and its near-future target image \\\\(o_{t+k}\\\\) is encoded by a 'momentum' encoder into \\\\(q_{t+k} = g'(f'(o_{t+k}))\\\\). With the two representations in hand, the predictor predicts the target \\\\(q_{t+k}\\\\) via \\\\(y_t = h(q_t)\\\\) to minimize the InfoNCE loss:\\n\\n\\\\[\\nL_{ATC} = - \\\\sum_{b \\\\in B} \\\\log \\\\frac{\\\\exp(y_b \\\\cdot q_b + k)}{\\\\exp(y_b \\\\cdot q_b + k) + \\\\sum_{b' \\\\in B - \\\\{b\\\\}} \\\\exp(y_b' \\\\cdot q_b')} \\n\\\\]\\n\\nSame as CURL, we use the backbone and neck as our encoder and the head as our predictor. The momentum update is performed every iteration with a coefficient of \\\\(\\\\tau = 0.99\\\\) that scales linearly up to 0.999. We also use dot product instead of bilinear product as a similarity measure.\\n\\nTable 5: Hyperparameters for pre-training ATC.\\n\\n| Hyperparameter               | Value  |\\n|------------------------------|--------|\\n| Epochs                       | 100    |\\n| Base learning rate           | 3e-4   |\\n| Weight decay                 | 1e-5   |\\n| Optimizer \\\\((\\\\beta_1, \\\\beta_2)\\\\) | AdamW (0.9, 0.999) |\\n| Batch size                   | 512    |\\n| Steps to future state \\\\(k\\\\)  | 3      |\\n| Momentum update ratio \\\\(\\\\tau\\\\) | [0.9, 0.999] |\\n| Representation dimensions    | 512    |\\n| Similarity measure           | Dot product |\\n\\nB.2.4. Siamese MAE\\n\\nSiamese Masked Autoencoder (Gupta et al., 2023) extends MAE (He et al., 2022) to a temporal prediction task using siamese architecture and asymmetric masking strategy. Concretely, both current image \\\\(o_t\\\\) and future(target) image \\\\(o_{t+k}\\\\) are passed through the same network up until the transformer decoder. In our implementation, this includes the backbone, game-wise spatial embedding, and the transformer encoder. In the process, \\\\(o_{t+k}\\\\) is masked with an extremely high ratio \\\\(\\\\rho = 0.95\\\\) while \\\\(o_t\\\\) remains untouched. In order to retrieve the lost information, the transformer decoder refers to the current frame's...\"}"}
{"id": "OiI12sNbgD", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nStudies by Parisi et al. (2022) and Hu et al. (2023) have shown that the use of pre-trained models, such as ResNet (He et al., 2016) and ViT (Dosovitskiy et al., 2020), enhances generalizability in various RL tasks. These models were pre-trained on ImageNet (Deng et al., 2009) and employed diverse training objectives, ranging from supervised (Radford et al., 2021) to self-supervised learning (He et al., 2020; Nair et al., 2022). Furthermore, Majumdar et al. (2023b) achieved improved generalization using a large, realistic dataset that closely resembles the ego-centric perspective of agents (Grauman et al., 2022), to pre-train a large ViT with masked image modeling (He et al., 2022).\\n\\nDespite the advancements, these models were pre-trained with different datasets and architectures, making it challenging to identify the effectiveness of individual learning objectives for generalization. Our study addresses this gap by systematically evaluating the generalization capabilities of various pre-training objectives using a unified experimental setup, including an identical dataset, architecture, and downstream environments.\\n\\n3. Preliminaries\\n\\n3.1. Reinforcement Learning\\n\\nIn Reinforcement Learning (RL), we adopt a Partially Observable Markov Decision Process (POMDP) framework, which differs from standard MDP by limiting direct access to the true state $s_t$. Instead, agents receive a partially observable image $o_t$, generated by an emission function:\\n\\n$$o_t \\\\sim q(\\\\cdot|s_t).$$\\n\\nAt each timestep, $t$, an agent observes an observation $o_t$ and chooses an action $a_t$ according to its policy $a_t \\\\sim \\\\pi(\\\\cdot|o_t)$. The agent then receives the next observation $o_{t+1}$ and a reward $r_t$ from the environment. Here, the goal of the agent is to learn a policy that maximizes the expected cumulative reward,\\n\\n$$E[P_{T=1}r_t].$$\\n\\n3.2. Pre-training for Reinforcement Learning\\n\\nDrawing insights from the CV and NLP field, the pre-train-then-finetune approach has emerged as a compelling method to improve generalization, paving the way for models to effectively adapt to diverse tasks and environments.\\n\\nWhen pre-training for RL, the model learns from the transitions made by humans or agents, using different objectives such as Offline RL (Kumar et al., 2020) and self-supervised loss (Laskin et al., 2020b). These learning objectives are tightly related to the data type they leverage\u2014image, video, demonstration, and trajectory\u2014each introducing unique knowledge to the model. Based on this, we classify the pre-training algorithms into the following four categories.\\n\\n- **Image**: These methods learn the spatial characteristics of individual states from a set of images, $\\\\sum_{i=1}^{N} \\\\{o_i\\\\}$.\\n- **Video**: These methods learn the temporal dynamics of environments from a set of videos, $\\\\sum_{i=1}^{N} \\\\sum_{t=1}^{T} \\\\{o_{i,t}\\\\}$.\\n- **Demonstration**: These methods learn task-relevant information such as identifying agents and enemies from a set of observation-action pairs, $\\\\sum_{i=1}^{N} \\\\sum_{t=1}^{T} \\\\{o_{i,t}, a_{i,t}\\\\}$.\\n- **Trajectory**: These methods learn richer task-relevant information such as the value of states and actions from a set of observation-action-reward triplets, $\\\\sum_{i=1}^{N} \\\\sum_{t=1}^{T} \\\\{o_{i,t}, a_{i,t}, r_{i,t}\\\\}$.\\n\\n4. Algorithms\\n\\nThis section outlines the pre-training algorithms we study, divided into four categories described in Section 3.2. Some algorithms were simplified to fit into our framework; we annotate such algorithms with a dagger ($^\\\\dagger$) to avoid confusion. For further details, please refer to Appendix B.2.\\n\\n4.1. No pre-training\\n\\n- **Random**: This approach involves fine-tuning a model that has been randomly initialized, with its encoder kept frozen throughout the fine-tuning process.\\n- **E2E**: In contrast to the Random method, the End-to-End approach involves fine-tuning a randomly initialized model without any frozen components. In addition to ResNet-50, we also evaluate a 3-layer-CNN based model (Mnih et al., 2015), which is known to excel in many RL environments.\\n\\n4.2. Learning from Image\\n\\n- **CURL**: Contrastive Unsupervised Reinforcement Learning (Laskin et al., 2020b) learns the spatial feature of images using augmentation functions and InfoNCE loss. It operates by ensuring that two augmented instances of the same image are encoded similarly in latent space.\\n- **MAE**: Masked Autoencoder (He et al., 2022) learns the spatial structure of images by reconstructing masked images with transformer encoder-decoder architecture. Since we employ a convolutional architecture in this study, we adapt this approach by masking pixels in the convolutional feature map, inspired by Seo et al. (2023a); Xiao et al. (2021).\\n\\n4.3. Learning from Video\\n\\n- **ATC**: Augmented Temporal Contrast (Stooke et al., 2021) learns the temporal dynamics of videos using InfoNCE loss.\"}"}
{"id": "OiI12sNbgD", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nThis involves closely aligning an image with its future image in a latent space while maintaining distinctiveness from the other unrelated images.\\n\\nSiamMAE: Siamese Masked Autoencoder (Gupta et al., 2023) is an extension of MAE to videos. This variant uses a Siamese architecture and an asymmetric masking strategy for temporal information extraction. In line with MAE, we apply masking to convolutional features.\\n\\nR3M: Reusable Representations for Robot Manipulation (Nair et al., 2022) uses multiple losses to learn from human demonstration videos with diverse tasks. In our study, we focus on the time contrastive loss of R3M. While akin to ATC, R3M gives an additional task of differentiating between 'near-future' and 'far-future' images.\\n\\n4.4. Learning from Demonstration\\n\\nBC: Behavioral Cloning (Pomerleau, 1991) learns to imitate the demonstrations by predicting actions from observations. This method acquires task-specific information from the dataset, as noted by Islam et al. (2022).\\n\\nSPR: Self-Predictive Representations (Schwarzer et al., 2020b) learn the dynamics of environments by recursively predicting future observations in a latent space. We employ a recurrent neural network for future prediction and a momentum network for encoding target observations.\\n\\nIDM: Inverse Dynamics Modeling (Christiano et al., 2016) learns to predict the action between consecutive observations. Similar to BC, IDM focuses on learning task-relevant information as it seeks to understand the cause-and-effect dynamics within the environments (Islam et al., 2022).\\n\\nSPR+IDM: Inspired by Schwarzer et al. (2021b), we investigate whether the combination of SPR and IDM can further improve the agent's performance.\\n\\n4.5. Learning from Trajectory\\n\\nCQL: Conservative Q-Learning (Kumar et al., 2020) integrates temporal difference loss with conservative Q-learning loss. Its main objective is to accurately approximate the Bellman target while minimizing the overestimation of actions that are absent in the offline data. We consider two variants: CQL-M, using mean squared loss, and CQL-D, applying cross-entropy-based distributional backups (Bellemare et al., 2017a) for minimizing temporal differences.\\n\\nDT: Decision Transformer (Chen et al., 2021) redefines RL as a sequence modeling problem. The network is trained to predict actions based on given states and desired cumulative rewards. During inference, the agent predicts the optimal actions needed to achieve a specified cumulative reward.\\n\\n5. Atari Pre-training Benchmark\\n\\nWe introduce the Atari Pre-training Benchmark (Atari-PB), a benchmark designed to assess the generalization ability of pre-training methods in vision-based RL.\\n\\n5.1. Dataset\\n\\nThe dataset for Atari-PB is derived from the DQN-Replay-Dataset (Agarwal et al., 2020). This dataset encompasses training logs from a DQN agent's experiences across 60 Atari games (Bellemare et al., 2013; Machado et al., 2018), documented over five distinct runs. Each run is divided into 50 evenly spaced checkpoints, ranging from the initial state (checkpoint 1) to the final state (checkpoint 50). This segmentation allows for precise control over the agent's performance level, thereby influencing the quality of the pre-training data.\\n\\nTo construct a pre-training dataset that reflects real-world complexities as a mixture of suboptimal and optimal behaviors, we selected the first 10 checkpoints from two separate runs. From each checkpoint, we sampled the first 10,000 transitions. Consequently, this process generated 200,000 transitions per game, culminating in a comprehensive dataset of 10 million transitions across 50 games. For more details, please refer to Appendix B.1.\\n\\n5.2. Model\\n\\nThe network architecture of our model is composed of three main components: a backbone for encoding images into features, a neck for converting the features to a low-dimensional latent vector, and a head for mapping the latent vector into policy outputs.\\n\\nBackbone, $f(\\\\cdot)$: Our backbone utilizes a modified version of the ResNet-50 architecture (He et al., 2016). It is designed to process input images $o \\\\in \\\\mathbb{R}^{C \\\\times H \\\\times W}$ and encode them into spatial feature maps $z = f(o)$, where $z \\\\in \\\\mathbb{R}^{D_z \\\\times H_z \\\\times W_z}$. Here, $D_z$ represents the output dimension, while $H_z$ and $W_z$ are the dimensions of the feature map's height and width, respectively. We replaced the batch normalization (Ioffe & Szegedy, 2015) with group normalization (Wu & He, 2018), aiming to address discrepancies in data distributions between pre-training and fine-tuning phases, as discussed in Kumar et al. (2022a;b).\\n\\nNeck, $g(\\\\cdot)$: The neck module incorporates a game-specific spatial pooling strategy to manage the variability of in-game elements (Kumar et al., 2022a;b). It is followed by a 2-layer Multi-Layer Perceptron (MLP) which transforms the spatial features into a low-dimensional latent vector $q = g(z)$, with $q \\\\in \\\\mathbb{R}^{D_q}$ and $D_q$ denoting the latent dimension.\\n\\nHead, $h(\\\\cdot)$: The head module employs a game-specific linear layer, allowing diverse policy outputs across different\"}"}
{"id": "OiI12sNbgD", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\n5.3. Pre-Training\\nFollowing standard training protocols in Atari games, we pre-processed each image by down-sampling to $84 \\\\times 84$ with grey-scaling, then stacked 4 consecutive frames (Mnih et al., 2015; Hessel et al., 2018). We applied two image augmentations: a random shift followed by intensity jittering (Schwarzer et al., 2020a; 2021a). Each model was pre-trained for 100 epochs using an AdamW optimizer (Loshchilov & Hutter, 2017) with a batch size of 512. We experimented with various learning rates, selecting from the range of $\\\\{1 \\\\times 10^{-3}, 3 \\\\times 10^{-4}, \\\\ldots, 3 \\\\times 10^{-5}, 1 \\\\times 10^{-6}\\\\}$, and adjusted the weight decay within the range of $\\\\{1 \\\\times 10^{-4}, 1 \\\\times 10^{-5}, 1 \\\\times 10^{-6}\\\\}$.\\n\\n5.4. Fine-tuning\\nFor fine-tuning, we assessed the effectiveness of our pre-trained models in three different categories of environments:\\n\\n- **In-Distribution (ID)**: This category includes the same 50 games used during the pre-training phase. This assessment aims to evaluate model performance in familiar settings. These games include the main task genres of Atari games, such as maze-based, tracking, vertical shooting, and horizontal shooting games.\\n\\n- **Near-Out-of-Distribution (Near-OOD)**: This group consists of 10 games not used in pre-training but belonging to the same task genres as the ID games. While these games present tasks similar to those in ID, they assess the model's generalization ability on new visual elements and reward structures.\\n\\n- **Far-Out-of-Distribution (Far-OOD)**: This group extends beyond Near-OOD, featuring 5 games with entirely novel task mechanics. For instance, HumanCanonball introduces projectile motion against gravity, while Klax focuses on color matching and stacking. These games serve as the baselines to understand the pre-trained models' generalization ability in entirely unfamiliar environments and tasks.\\n\\nFor these environments, we consider two common adaptation scenarios:\\n\\n- **Offline BC**: In this scenario, the model undergoes fine-tuning through behavior cloning using 50,000 frames from expert demonstrations. For ID and Near-OOD games, expert demonstrations were sourced from the final checkpoint of the DQN-Replay-Dataset. For Far-OOD games, we used demonstrations from a Rainbow agent (Hessel et al., 2018) trained for 2 million steps. During this process, as depicted in Figure 3, we kept the backbone parameters frozen, while the neck and head components were re-initialized and then fine-tuned for 100 epochs.\\n\\n- **Online RL**: In this scenario, the model is fine-tuned using the Rainbow algorithm (Hessel et al., 2018), with 50,000 interactions in each respective environments. Identical to the Offline BC approach, the backbone of the pre-trained model remains unaltered, while the neck and head are re-initialized and subsequently fine-tuned.\\n\\nTo ensure a reliable evaluation, we report the normalized game scores using the Inter Quantile Mean (IQM) methodology (Agarwal et al., 2021). This method involves stratified bootstrap sampling and is executed with three different random seeds. For normalizing the scores in ID and Near-OOD environments, we use the DQN scores as documented by Castro et al. (2018). In the case of Far-OOD scenarios, the normalization is based on the final scores of our Rainbow agent, which was trained for 2 million steps to provide the expert dataset for the Offline BC scenario.\\n\\n6. Experimental Results\\nIn this section, we present our main experimental results, as illustrated in Figure 4 and 5. Instead of merely providing a ranking of different pre-training methods, we focus on discerning specific trends and patterns that manifest across different downstream distributions and adaptation scenarios. In summary, a distinctive pattern emerged between algorithms that learn task-agnostic information (using images or videos), and those that learn task-relevant information (using demonstrations or trajectories). While task-agnostic knowledge consistently improved performance across all\"}"}
{"id": "OiI12sNbgD", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"O1: Learning task-agnostic information from images and videos significantly enhances performance across ID, Near-OOD, and Far-OOD environments. Our findings, as depicted in Figure 4, reveal that image-based pre-training methods like CURL and MAE consistently surpassed the Random baseline (i.e., a frozen backbone with randomly initialized weights). This highlights the importance of extracting spatial information (e.g., object size, shape, location) for an effective adaptation to ID environments and generalization to OOD environments. Moreover, video-based pre-training methods (ATC, SiamMAE, R3M) showed enhanced performance over image-based approaches, underscoring the significance of capturing both spatial and temporal information (e.g., objects' moving orientation and speed). Notably, these video-pretrained models slightly surpassed end-to-end (E2E) fine-tuning from scratch, despite having their backbone parameters frozen during fine-tuning. We believe that allowing end-to-end fine-tuning of video-based models will further enhance their performance, widening the performance gap compared to the E2E approach. These findings support the emerging trend of incorporating images and videos in pre-training to achieve better generalization across various control tasks (Majumdar et al., 2023b; Bhateja et al., 2023).\\n\\nO2: Learning task-relevant information from demonstrations further enhances ID and Near-OOD performance, but provides marginal improvements in Far-OOD performance. Incorporating actions into pre-training objectives (BC, SPR, IDM, and SPR+IDM) led to mixed performance improvements. While task-specific knowledge benefited performance in ID and Near-OOD settings, a decline was noted in Far-OOD environments. This variation likely arose from the task similarities between ID and Near-OOD environments, in contrast to the distinct differences in tasks between ID and Far-OOD. Given that ID and Near-OOD games typically fall into the four main genres, it's plausible that the task knowledge from ID environments can be applied to Near-OOD environments, but not Far-OOD. For example, as illustrated in the first column of Figure 5, the model pre-trained on SpaceInvaders (ID) learns task-specific knowledge such as agent and enemy locations as well as bullet movement direction, which are characteristic of the vertical shooting genre. This knowledge helps the model to quickly identify similar gameplay elements in Assault (Near-OOD), despite not having been exposed to the game during pre-training. Conversely, task-specific knowledge is rendered ineffective in Surround (Far-OOD), with its unique task of avoiding trails and distinct agent locations. This difference shows the difficulty in applying pre-learned task knowledge to environments with considerably different mechanisms.\\n\\nO3: Learning reward-specific information from trajectories yields the best ID performance, while it shows limited generalization gains in Near-OOD and Far-OOD environments. Integrating reward-specific information from trajectories (DT, CQL-M, CQL-D) led to superior success in ID environments. Nonetheless, their effectiveness diminished in Near-OOD and Far-OOD environments.\"}"}
{"id": "OiI12sNbgD", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nGupta, A., Wu, J., Deng, J., and Fei-Fei, L. Siamese masked autoencoders. arXiv preprint arXiv:2305.14344, 2023.\\n\\nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019a.\\n\\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. In Proc. the International Conference on Machine Learning (ICML), 2019b.\\n\\nHafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023.\\n\\nHan, T., Xie, W., and Zisserman, A. Video representation learning by dense predictive coding. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pp. 0\u20130, 2019.\\n\\nHansen, N. and Wang, X. Generalization in reinforcement learning by soft data augmentation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 13611\u201313617. IEEE, 2021.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729\u20139738, 2020.\\n\\nHe, K., Chen, X., Xie, S., Li, Y., Doll\u00e1r, P., and Girshick, R. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000\u201316009, 2022.\\n\\nHessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. Rainbow: Combining improvements in deep reinforcement learning. In Proc. the AAAI Conference on Artificial Intelligence (AAAI), 2018.\\n\\nHu, Y., Wang, R., Li, L. E., and Gao, Y. For pre-trained vision models in motor control, not all policy learning methods are created equal. arXiv preprint arXiv:2304.04591, 2023.\\n\\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proc. the International Conference on Machine Learning (ICML), 2015.\\n\\nIslam, R., Tomar, M., Lamb, A., Efroni, Y., Zang, H., Didolkar, A., Misra, D., Li, X., van Seijen, H., Combes, R. T. d., et al. Agent-controller representations: Principled offline rl with rich exogenous information. Proc. the International Conference on Machine Learning (ICML), 2022.\\n\\nKaiser, \u0141., Babaeizadeh, M., Mi\u0142os, P., Osi\u0144ski, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Koza\u0144ski, P., Levine, S., et al. Model based reinforcement learning for atari. In Proc. the International Conference on Learning Representations (ICLR), 2019.\\n\\nKumar, A., Zhou, A., Tucker, G., and Levine, S. Conservative q-learning for offline reinforcement learning. Proc. the Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nKumar, A., Agarwal, R., Geng, X., Tucker, G., and Levine, S. Offline q-learning on diverse multi-task data both scales and generalizes. Proc. the International Conference on Learning Representations (ICLR), 2022a.\\n\\nKumar, A., Singh, A., Ebert, F., Yang, Y., Finn, C., and Levine, S. Pre-training for robots: Offline rl enables learning new tasks from a handful of trials. arXiv e-prints, art. arXiv preprint arXiv:2210.05178, 2022b.\\n\\nLan, C. L., Tu, S., Oberman, A., Agarwal, R., and Bellemare, M. G. On the generalization of representations in reinforcement learning. arXiv preprint arXiv:2203.00543, 2022.\\n\\nLaskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A. Reinforcement learning with augmented data. Proc. the Advances in Neural Information Processing Systems (NeurIPS), 2020a.\\n\\nLaskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive unsupervised representations for reinforcement learning. In Proc. the International Conference on Machine Learning (ICML), 2020b.\\n\\nLee, H., Lee, K., Hwang, D., Lee, H., Lee, B., and Choo, J. On the importance of feature decorrelation for unsupervised representation learning in reinforcement learning. arXiv preprint arXiv:2306.05637, 2023.\\n\\nLee, H., Cho, H., Kim, H., Gwak, D., Kim, J., Choo, J., Yun, S.-Y., and Yun, C. Plastic: Improving input and label plasticity for sample efficient reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024a.\\n\\nLee, H., Cho, H., Kim, H., Kim, D., Min, D., Choo, J., and Lyle, C. Slow and steady wins the race maintaining plasticity with hare and tortoise networks. 2024b.\"}"}
{"id": "OiI12sNbgD", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "OiI12sNbgD", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nSchwarzer, M., Rajkumar, N., Noukhovitch, M., Anand, A., Charlin, L., Hjelm, R. D., Bachman, P., and Courville, A. C.\\n\\nPretraining representations for data-efficient reinforcement learning. Proc. the Advances in Neural Information Processing Systems (NeurIPS), 2021a.\\n\\nSchwarzer, M., Rajkumar, N., Noukhovitch, M., Anand, A., Charlin, L., Hjelm, R. D., Bachman, P., and Courville, A. C.\\n\\nPretraining representations for data-efficient reinforcement learning. Proc. the Advances in Neural Information Processing Systems (NeurIPS), 2021b.\\n\\nSchwarzer, M., Ceron, J. S. O., Courville, A., Bellemare, M. G., Agarwal, R., and Castro, P. S. Bigger, better, faster: Human-level atari with human-level efficiency. In International Conference on Machine Learning, pp. 30365\u201330380. PMLR, 2023.\\n\\nSeo, Y., Lee, K., James, S. L., and Abbeel, P. Reinforcement learning with action-free pre-training from videos. In International Conference on Machine Learning, pp. 19561\u201319579. PMLR, 2022.\\n\\nSeo, Y., Hafner, D., Liu, H., Liu, F., James, S., Lee, K., and Abbeel, P. Masked world models for visual control. In 6th Annual Conference on Robot Learning, pp. 1332\u20131344. PMLR, 2023a.\\n\\nSeo, Y., Kim, J., James, S., Lee, K., Shin, J., and Abbeel, P. Multi-view masked world models for visual robotic manipulation. Proc. the International Conference on Machine Learning (ICML), 2023b.\\n\\nSermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., Schaal, S., Levine, S., and Brain, G. Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE international conference on robotics and automation (ICRA), pp. 1134\u20131141. IEEE, 2018.\\n\\nShah, R., Mart\u00edn-Mart\u00ed, R., and Zhu, Y. Mutex: Learning unified policies from multimodal task specifications. arXiv preprint arXiv:2309.14320, 2023.\\n\\nStone, A., Ramirez, O., Konolige, K., and Jonschkowski, R. The distracting control suite\u2013a challenging benchmark for reinforcement learning from pixels. arXiv preprint arXiv:2101.02722, 2021.\\n\\nStooke, A., Lee, K., Abbeel, P., and Laskin, M. Decoupling representation learning from reinforcement learning. In Proc. the International Conference on Machine Learning (ICML), 2021.\\n\\nTaiga, A. A., Agarwal, R., Farebrother, J., Courville, A., and Bellemare, M. G. Investigating multi-task pretraining and generalization in reinforcement learning. In Proc. the International Conference on Learning Representations (ICLR), 2022.\\n\\nTassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.\\n\\nTong, Z., Song, Y., Wang, J., and Wang, L. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:10078\u201310093, 2022.\\n\\nWalke, H. R., Black, K., Zhao, T. Z., Vuong, Q., Zheng, C., Hansen-Estruch, P., He, A. W., Myers, V., Kim, M. J., Du, M., et al. Bridgedata v2: A dataset for robot learning at scale. In Conference on Robot Learning, pp. 1723\u20131736. PMLR, 2023.\\n\\nWang, C., Luo, X., Ross, K., and Li, D. Vrl3: A data-driven framework for visual deep reinforcement learning. Advances in Neural Information Processing Systems, 35:32974\u201332988, 2022.\\n\\nWu, P., Majumdar, A., Stone, K., Lin, Y., Mordatch, I., Abbeel, P., and Rajeswaran, A. Masked trajectory models for prediction, representation, and control. arXiv preprint arXiv:2305.02968, 2023.\\n\\nWu, Y. and He, K. Group normalization. In Proc. of the European Conference on Computer Vision (ECCV), pp. 3\u201319, 2018.\\n\\nXiao, T., Singh, M., Mintun, E., Darrell, T., Doll\u00e1r, P., and Girshick, R. Early convolutions help transformers see better. Advances in neural information processing systems, 34:30392\u201330400, 2021.\\n\\nXiao, T., Radosavovic, I., Darrell, T., and Malik, J. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022.\\n\\nYang, S., Nachum, O., Du, Y., Wei, J., Abbeel, P., and Schuurmans, D. Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129, 2023.\\n\\nYe, W., Zhang, Y., Abbeel, P., and Gao, Y. Become a proficient player with limited data through watching pure videos. In The Eleventh International Conference on Learning Representations, 2022.\\n\\nYu, T., Lan, C., Zeng, W., Feng, M., Zhang, Z., and Chen, Z. Playvirtual: Augmenting cycle-consistent virtual trajectories for reinforcement learning. Advances in Neural Information Processing Systems, 34:5276\u20135289, 2021.\\n\\nYu, T., Zhang, Z., Lan, C., Chen, Z., and Lu, Y. Mask-based latent reconstruction for reinforcement learning. Proc. the Advances in Neural Information Processing Systems (NeurIPS), 2022.\"}"}
{"id": "OiI12sNbgD", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "OiI12sNbgD", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nFigure 5: Qualitative analysis of methods. EigenCAM visualization of the pre-trained backbones in 3 games: SpaceInvaders (ID), Assault (Near-OOD), and Surround (Far-OOD). The agents are marked in red circles for each game (first column). We chose one representative method for each algorithm class.\\n\\nNear-OOD and Far-OOD environments, where they lagged behind demonstration-based and video-based approaches. This shows that extracting reward-specific knowledge (e.g., rewarded for shooting enemies) enhances performance in familiar settings, but fails to generalize across environments with different reward functions (e.g., rewarded for shooting enemies but penalized for missing) or tasks (e.g., receiving rewards when surrounding the enemy), indicating their limitations for broader generalization.\\n\\nTo further validate our observations, we conducted a qualitative analysis of the pre-trained models with Eigen-CAM (Muhammad & Yeasin, 2020), which are illustrated in Figure 5. In ID games like SpaceInvaders, we found that unlike video-based methods (ATC), the demonstration or trajectory-based methods (BC, CQL-D) concentrated on capturing the agent, a critical object of the task. Interestingly, an identical pattern was observed in Near-OOD environments like Assault, fortifying our conjecture that the task-specific knowledge acquired during pre-training can be transferred to visually distinct environments. For an extended discussion and analysis, refer to Appendix C.3.\\n\\nFigure 4 shows a strong correlation between performances in offline behavioral cloning and online reinforcement learning, with Pearson correlation coefficients of 0.85 in ID, 0.61 in Near-OOD, and 0.63 in Far-OOD environments. This suggests that a well-pretrained model can yield versatile representations applicable to a wide range of scenarios, not limited to specific adaptation algorithms (Parisi et al., 2022; Majumdar et al., 2023a).\\n\\nO4: Effective adaptation in one scenario correlates to effective adaptation in the other.\\n\\nFigure 6: Effect of data optimality. Offline BC performance of algorithms after pre-training with datasets of differing optimality. We control the dataset optimality via the proficiency of the policy that created it, which in turn can be controlled by choosing different checkpoints of the DQN-Replay-Dataset (Agarwal et al., 2020).\\n\\nO5: Miscellaneous Remarks\\n\\nInspired by Taiga et al. (2022), we have conducted an additional experiment to fine-tune our pre-trained models on ID environments with different game modes (Machado et al., 2018). Detailed protocol and results can be found in Appendix C.1. As shown in Figure 9, the results were similar to that of ID in our main experiments. We believe that simply changing the game mode did not sufficiently alter the reward functions and thus led to similar tendencies.\\n\\nAdditionally, we noticed that in end-to-end fine-tuning for Online RL, the smaller 3-layer CNN agent (CNN3) often outperforms the larger ResNet-50 agent (RN50). This finding is consistent with prior research, which suggests that deep and large models are prone to overfitting and are more vulnerable to the non-stationary nature of Online RL (Lee et al., 2024b). Many studies have proposed techniques to mitigate this issue (Nikishin et al., 2022; Lee et al., 2024a; Farebrother et al., 2024), suggesting that applying such methods could make larger architectures more competitive.\\n\\nFinally, we have found an interesting relationship between object size and the performance of mask-based algorithms (MAE, SiamMAE). Compared to other pre-training methods like CURL and ATC, these methods excel in environments with large objects but underperform in games with tiny objects. For more details, please refer to Appendix C.2.\\n\\n7. Ablation Studies\\n\\nIn our ablation studies, we assess the effects of variations in data optimality, size, and model size on pre-training methods. We selected CURL (image), ATC (video), BC (demonstration), and CQL-D (trajectory) as representative methods for comparison. Unless otherwise noted, the experimental setup remains identical to our main experiments.\"}"}
{"id": "OiI12sNbgD", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nFigure 7: Effect of Dataset Size.\\nWe measure the Offline BC performance after pre-training with datasets of varying sizes. All datasets were derived from the same DQN-Replay-Dataset runs and checkpoints, with different numbers of transitions sampled based on dataset size.\\n\\n7.1. Data Optimality\\nA key factor in building pre-training datasets for RL is data optimality. We investigated the impact of data optimality using three distinct datasets from the DQN-Replay-Dataset:\\n\u2022 Suboptimal: Data from the first and second checkpoint of each game, each with 50,000 interactions.\\n\u2022 Mixed: Data from the first ten checkpoints per game, each with 10,000 interactions (same as our main setup).\\n\u2022 Expert: Data from the ninth and tenth checkpoint of each game, each with 50,000 interactions.\\n\\nO6: Using optimal data enhances ID adaptation but falters generalization in Near-OOD and Far-OOD environments. Figure 6 shows that using optimal data does not guarantee improved performance in downstream environments. While moving from a Mixed dataset to an Expert dataset enhances performance in ID environments, its effectiveness was limited in Near-OOD and Far-OOD environments.\\n\\nOptimal gameplay in Atari games usually follows repetitive patterns, resulting in limited diversity in Expert dataset. Such uniformity likely hindered the pre-trained model's ability to generalize to unfamiliar objects, by making it overly tailored to the objects encountered during pre-training. On the other hand, models pre-trained on datasets that blend optimal and suboptimal transitions (Mixed) showed enhanced generalization capabilities, particularly in Far-OOD environments. This improvement underscores the significance of dataset diversity for achieving effective generalization, a principle supported by recent research (Taiga et al., 2022).\\n\\n7.2. Data Size\\nThe scalability of pre-training methods can be significantly influenced by the size of the pre-training dataset. We explored this by using datasets of varying sizes: 1M, 10M, and 100M transitions, each derived from the same runs and checkpoints but adjusted to their respective sizes.\\n\u2022 1M: Consists of 1,000 initial transitions, sampled from the 10 initial checkpoints of each run.\\n\u2022 10M: Our standard setup, with 10,000 initial transitions from each checkpoint.\\n\u2022 100M: The largest dataset, containing 100,000 initial transitions from each checkpoint.\\n\\nModels were pre-trained over 100 epochs for 1M and 10M datasets. However, due to the substantial size of the 100M dataset, pre-training was limited to 10 epochs.\\n\\nO7: Larger dataset enhances ID adaptation but shows mixed effects on Near-OOD and Far-OOD generalization. Figure 7 shows that larger datasets improved performance in ID settings, especially for methods involving demonstrations or trajectories. However, the benefits were less predictable in Near-OOD and Far-OOD environments, possibly because the model is relatively too small to fully encode all the information within large datasets in 10 epochs.\\n\\nConsistent with our earlier findings in O1, O2, and O3, demonstration-based methods excelled in Near-OOD settings, while video-based methods were more effective in Far-OOD scenarios. This reiterates our findings that learning task-agnostic features improves generalization across distributions, but the benefits of learning task-relevant features diminish as task shifts increase.\\n\\n7.3. Model Size\\nIn the subsequent analysis, we explore how pre-training methods scale with changes in model size:\\n\u2022 R-18: A scaled-down model with reduced depth (18 layers) and halved channel widths, compared to R-50.\\n\u2022 R-50: A standard model architecture used in our study.\"}"}
{"id": "OiI12sNbgD", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nO8: Larger model provides consistent benefits in ID and Near-OOD, while its improvement is unclear in Far-OOD. When comparing ResNet-18 to ResNet-50, we found that larger models improve results in ID and Near-OOD scenarios. While broader computer vision research suggests larger models usually offer better generalization to out-of-distribution (OOD) environments, this trend was not clearly observed in our Far-OOD experiments. Nevertheless, the experimental results aligned with our prior observations (O1, O2, and O3), indicating that demonstration-based methods perform better in Near-OOD environments, and video-based methods excel in Far-OOD environments, irrespective to their model size.\\n\\n8. Conclusion and Future Work\\n\\nIn this study, we examined how different pre-training objectives affect agents' generalization capabilities in vision-based reinforcement learning (RL). Our findings indicate that learning task-agnostic features, such as spatial features (e.g., object locations and shapes) and temporal features (e.g., speed and direction of moving objects), enhances generalization across various visual and task distribution shifts. In contrast, learning task-specific features, through actions (e.g., locations of agents and related objects) or reward structures (e.g., identifying beneficial or detrimental actions), improves performance in similar tasks but offers limited benefits in divergent task distributions. Our results align with prior theoretical works demonstrating the benefits of learning temporal structures in environments for acquiring optimal feature representations (Bellemare et al., 2019; Lan et al., 2022). In this work, we provide empirical evidences supporting the notion that learning temporal structures within environments can improve generalization across various types of task shifts.\\n\\nThe RL community has recently introduced diverse datasets covering a wide range of human and robotic behaviors (Padalkar et al., 2023; Grauman et al., 2022). Our results suggest that pre-training with both task-agnostic and task-specific knowledge offers distinct benefits: one enhances generalization to different shifts, while the other excels in similar environments. Therefore, a promising future direction is to develop architectures or learning objectives that can decouple task-agnostic and task-specific features, allowing their use based on specific purposes (Wang et al., 2022; Bhateja et al., 2023). We hope our investigation provides valuable insights for advancing vision-based RL.\\n\\nAcknowledgements\\n\\nThis work was supported by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No.RS-2019-II190075 Artificial Intelligence Graduate School Program (KAIST)), the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2022R1A2B5B02001913).\\n\\nImpact Statement\\n\\nThis paper investigates the relationship between pre-training objectives and their generalization capabilities across different downstream environments. Our findings provide valuable insights into how various learning objectives influence model generalization, aiding the development of large, foundational vision-based agents. We also acknowledge the potential risks associated with rapid advancements in these foundational models, especially in robotic control. Discussions for preventing misuse and maintaining high ethical standards are essential to ensure these technologies benefit society and do not cause harm.\\n\\nReferences\\n\\nAgarwal, R., Schuurmans, D., and Norouzi, M. An optimistic perspective on offline reinforcement learning. Proc. the International Conference on Machine Learning (ICML), 2020.\\n\\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare, M. Deep reinforcement learning at the edge of the statistical precipice. Proc. the Advances in Neural Information Processing Systems (NeurIPS), 34:29304\u201329320, 2021.\\n\\nArora, S., Du, S., Kakade, S., Luo, Y., and Saunshi, N. Provable representation learning for imitation learning via bi-level optimization. In International Conference on Machine Learning, 2020.\\n\\nBaker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., and Clune, J. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Proc. the Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\nBellemare, M., Dabney, W., Dadashi, R., Ali Taiga, A., Castro, P. S., Le Roux, N., Schuurmans, D., Lattimore, T., and Lyle, C. A geometric perspective on optimal representations for reinforcement learning. Advances in neural information processing systems, 32, 2019.\\n\\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 2013.\"}"}
{"id": "OiI12sNbgD", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning\\n\\nBellemare, M. G., Dabney, W., and Munos, R. A distribu-\\ntional perspective on reinforcement learning. In Proc. the\\nInternational Conference on Machine Learning (ICML)\\npp. 449\u2013458. PMLR, 2017a.\\n\\nBellemare, M. G., Dabney, W., and Munos, R. A distribu-\\ntional perspective on reinforcement learning. 2017b.\\n\\nBhateja, C., Guo, D., Ghosh, D., Singh, A., Tomar, M.,\\nVuong, Q., Chebotar, Y ., Levine, S., and Kumar, A.\\nRobotic offline rl from internet videos via value-function\\npre-training. Proc. the Advances in Neural Information\\nProcessing Systems (NeurIPS), 2023.\\n\\nBrandfonbrener, D., Nachum, O., and Bruna, J. Inverse dy-\\namics pretraining learns good representations for multi-\\ntask imitation. Proc. the Advances in Neural Information\\nProcessing Systems (NeurIPS), 2023.\\n\\nBubeck, S., Chandrasekaran, V ., Eldan, R., Gehrke, J.,\\nHorvitz, E., Kamar, E., Lee, P., Lee, Y . T., Li, Y .,\\nLundberg, S., et al. Sparks of artificial general intel-\\nligence: Early experiments with gpt-4. arXiv preprint\\narXiv:2303.12712, 2023.\\n\\nCaluwaerts, K., Iscen, A., Kew, J. C., Yu, W., Zhang, T.,\\nFreeman, D., Lee, K.-H., Lee, L., Saliceti, S., Zhuang,\\nV ., et al. Barkour: Benchmarking animal-level agility\\nwith quadruped robots. arXiv preprint arXiv:2305.14654,\\n2023.\\n\\nCastro, P. S., Moitra, S., Gelada, C., Kumar, S., and Belle-\\nmare, M. G. Dopamine: A Research Framework for\\nDeep Reinforcement Learning. 2018. URL\\nhttp://arxiv.org/abs/1812.06110.\\n\\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A.,\\nLaskin, M., Abbeel, P., Srinivas, A., and Mordatch, I.\\nDecision transformer: Reinforcement learning via sequence\\nmodeling. Proc. the Advances in Neural Information\\nProcessing Systems (NeurIPS), 2021.\\n\\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A sim-\\nple framework for contrastive learning of visual represen-\\ntations. In Proc. the International Conference on Machine\\nLearning (ICML), pp. 1597\u20131607. PMLR, 2020a.\\n\\nChen, X. and He, K. Exploring simple siamese represen-\\ntation learning. In Proc. of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR), pp.\\n15750\u201315758, 2021.\\n\\nChen, X., Fan, H., Girshick, R., and He, K. Improved\\nbaselines with momentum contrastive learning. arXiv\\npreprint arXiv:2003.04297, 2020b.\\n\\nChristiano, P., Shah, Z., Mordatch, I., Schneider, J., Black-\\nwell, T., Tobin, J., Abbeel, P., and Zaremba, W. Transfer\\nfrom simulation to real world through learning deep in-\\nverse dynamics model. arXiv preprint arXiv:1610.03518,\\n2016.\\n\\nCobbe, K., Hesse, C., Hilton, J., and Schulman, J. Lever-\\naging procedural generation to benchmark reinforcement\\nlearning. In Proc. the International Conference on Ma-\\nchine Learning (ICML), pp. 2048\u20132056. PMLR, 2020.\\n\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\\nL. Imagenet: A large-scale hierarchical image database.\\nIn 2009 IEEE conference on computer vision and pattern\\nrecognition, pp. 248\u2013255. Ieee, 2009.\\n\\nDosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., and\\nKoltun, V . Carla: An open urban driving simulator. In\\nConference on robot learning, pp. 1\u201316. PMLR, 2017.\\n\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\\nHeigold, G., Gelly, S., et al. An image is worth 16x16\\nwords: Transformers for image recognition at scale. arXiv\\npreprint arXiv:2010.11929, 2020.\\n\\nFarebrother, J., Machado, M. C., and Bowling, M. Gen-\\neralization and regularization in dqn. arXiv preprint\\narXiv:1810.00123, 2018.\\n\\nFarebrother, J., Orbay, J., Vuong, Q., Ta\u00a8\u0131ga, A., Cheb-\\notar, Y ., Xiao, T., Irpan, A., Levine, S., Castro, P. S.,\\nFaust, A., et al. Stop regressing: Training value func-\\ntions via classification for scalable deep rl. arXiv preprint\\narXiv:2403.03950, 2024.\\n\\nFeichtenhofer, C., Li, Y ., He, K., et al. Masked autoen-\\ncoders as spatiotemporal learners. Advances in neural\\ninformation processing systems, 35:35946\u201335958, 2022.\\n\\nFujimoto, S., Meger, D., and Precup, D. Off-policy deep\\nreinforcement learning without exploration. In Proc. the\\nInternational Conference on Machine Learning (ICML),\\n2019.\\n\\nGrauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari,\\nA., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu,\\nX., et al. Ego4d: Around the world in 3,000 hours of\\negocentric video. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition,\\npp. 18995\u201319012, 2022.\\n\\nGrill, J.-B., Strub, F., Altch \u00b4e, F., Tallec, C., Richemond,\\nP., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo,\\nZ., Gheshlaghi Azar, M., et al. Bootstrap your own\\nlatent-a new approach to self-supervised learning. Proc.\\nthe Advances in Neural Information Processing Systems\\n(NeurIPS), 2020.\"}"}
{"id": "OiI12sNbgD", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "OiI12sNbgD", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model           | MontezumaRevenge | ChopperCommand |\\n|-----------------|------------------|----------------|\\n| RndmAgnt DQN    | 16256            | 10780          |\\n| RndmAgnt Rainbow| 68              | 0              |\\n| RndmAgnt RNDN   | 16256            | 0              |\\n\\n**Table 29:** Offline BC scores on Near-Out-of-Distribution games across models pre-trained with datasets of different sizes.\\n\\n| Model           | ElevatorAction | CrazyClimber |\\n|-----------------|----------------|--------------|\\n| RndmAgnt DQN    | 3092           | 3568         |\\n| RndmAgnt Rainbow| 0              | 23          |\\n| RndmAgnt RNDN   | 3092           | 0            |\\n\\n**Table 30:** Offline BC scores on Far-Out-of-Distribution games, across models pre-trained with datasets of different sizes.\\n\\n*Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning*\"}"}
{"id": "OiI12sNbgD", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"**Table 31**: Offline BC scores of a smaller backbone (ResNet-18) on In-Distribution games.\\n\\n| Game                | Optimality Gap (DNS) | IQM (DNS) |\\n|---------------------|----------------------|-----------|\\n| MontezumaRevenge    | 1454                 | 100       |\\n| ChopperCommand      | 1550                 | 872       |\\n| JourneyEscape       | 1901                 | 21        |\\n| SpaceInvaders       | 1808                 | 0         |\\n| CrazyClimber        | 901                  | 0         |\\n| WizardOfWor         | 1338                 | 0         |\\n| FishingDerby        | 2860                 | 0         |\\n| YarsRevenge         | 1338                 | 0         |\\n| DoubleDunk          | 2090                 | 0         |\\n| HumanCannonball     | 3222                 | 0         |\\n| IQM (DNS)           | 227                  | 0         |\\n| BattleZone          | 21658                | 0         |\\n| Jamesbond           | 2360                 | 0         |\\n| MsPacman            | 12850                | 0         |\\n| BankHeist           | 16256                | 0         |\\n| TimePilot           | 16542                | 0         |\\n| Robotank            | 4387                 | 0         |\\n| Asteroids           | 11486                | 0         |\\n| Riverraid           | 10780                | 0         |\\n| Seaquest            | 1195                 | 0         |\\n| Gravitar            | 9833                 | 0         |\\n| Phoenix             | 1419                  | 0         |\\n| Pooyan              | 1000                 | 0         |\\n| Solaris             | 1454                 | 0         |\\n| Tennis              | 1454                 | 0         |\\n| Qbert               | 1454                 | 0         |\\n| Krull               | 1454                 | 0         |\\n| Hero                | 1454                 | 0         |\\n| BasicMath           | 1454                 | 0         |\\n| Surround            | 1454                 | 0         |\\n| Surround            | 1454                 | 0         |\\n\\n**Table 33**: Offline BC scores of a smaller backbone (ResNet-18) on Far-Out-of-Distribution games.\\n\\n| Game                | Optimality Gap (DNS) | IQM (DNS) |\\n|---------------------|----------------------|-----------|\\n| MontezumaRevenge    | 1454                 | 100       |\\n| ChopperCommand      | 1550                 | 872       |\\n| JourneyEscape       | 1901                 | 21        |\\n| SpaceInvaders       | 1808                 | 0         |\\n| CrazyClimber        | 901                  | 0         |\\n| WizardOfWor         | 1338                 | 0         |\\n| FishingDerby        | 2860                 | 0         |\\n| YarsRevenge         | 1338                 | 0         |\\n| DoubleDunk          | 2090                 | 0         |\\n| HumanCannonball     | 3222                 | 0         |\\n| IQM (DNS)           | 227                  | 0         |\\n| BattleZone          | 21658                | 0         |\\n| Jamesbond           | 2360                 | 0         |\\n| MsPacman            | 12850                | 0         |\\n| BankHeist           | 16256                | 0         |\\n| TimePilot           | 16542                | 0         |\\n| Robotank            | 4387                 | 0         |\\n| Asteroids           | 11486                | 0         |\\n| Riverraid           | 10780                | 0         |\\n| Seaquest            | 1195                 | 0         |\\n| Gravitar            | 9833                 | 0         |\\n| Phoenix             | 1419                  | 0         |\\n| Pooyan              | 1000                 | 0         |\\n| Solaris             | 1454                 | 0         |\\n| Tennis              | 1454                 | 0         |\\n| Qbert               | 1454                 | 0         |\\n| Krull               | 1454                 | 0         |\\n| Hero                | 1454                 | 0         |\\n| BasicMath           | 1454                 | 0         |\\n| Surround            | 1454                 | 0         |\\n| Surround            | 1454                 | 0         |\\n\\n**Random CURL ATC BC CQL-D**\\n\\n| Game                | Optimality Gap (DNS) | IQM (DNS) |\\n|---------------------|----------------------|-----------|\\n| MontezumaRevenge    | 1454                 | 100       |\\n| ChopperCommand      | 1550                 | 872       |\\n| JourneyEscape       | 1901                 | 21        |\\n| SpaceInvaders       | 1808                 | 0         |\\n| CrazyClimber        | 901                  | 0         |\\n| WizardOfWor         | 1338                 | 0         |\\n| FishingDerby        | 2860                 | 0         |\\n| YarsRevenge         | 1338                 | 0         |\\n| DoubleDunk          | 2090                 | 0         |\\n| HumanCannonball     | 3222                 | 0         |\\n| IQM (DNS)           | 227                  | 0         |\\n| BattleZone          | 21658                | 0         |\\n| Jamesbond           | 2360                 | 0         |\\n| MsPacman            | 12850                | 0         |\\n| BankHeist           | 16256                | 0         |\\n| TimePilot           | 16542                | 0         |\\n| Robotank            | 4387                 | 0         |\\n| Asteroids           | 11486                | 0         |\\n| Riverraid           | 10780                | 0         |\\n| Seaquest            | 1195                 | 0         |\\n| Gravitar            | 9833                 | 0         |\\n| Phoenix             | 1419                  | 0         |\\n| Pooyan              | 1000                 | 0         |\\n| Solaris             | 1454                 | 0         |\\n| Tennis              | 1454                 | 0         |\\n| Qbert               | 1454                 | 0         |\\n| Krull               | 1454                 | 0         |\\n| Hero                | 1454                 | 0         |\\n| BasicMath           | 1454                 | 0         |\\n| Surround            | 1454                 | 0         |\\n| Surround            | 1454                 | 0         |\"}"}
