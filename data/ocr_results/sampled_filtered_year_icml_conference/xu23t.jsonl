{"id": "xu23t", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts\\n\\nMinghao Xu*\u20201 2 Xinyu Yuan*1 2 Santiago Miret3 Jian Tang1 4 5\\n\\nAbstract\\nCurrent protein language models (PLMs) learn protein representations mainly based on their sequences, thereby well capturing co-evolutionary information, but they are unable to explicitly acquire protein functions, which is the end goal of protein representation learning. Fortunately, for many proteins, their textual property descriptions are available, where their various functions are also described. Motivated by this fact, we first build the ProtDescribe dataset to augment protein sequences with text descriptions of their functions and other important properties. Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts. During pre-training, we design three types of tasks, i.e., unimodal mask prediction, multimodal representation alignment and multimodal mask prediction, to enhance a PLM with protein property information with different granularities and, at the same time, preserve the PLM\u2019s original representation power. On downstream tasks, ProtST enables both supervised learning and zero-shot prediction. We verify the superiority of ProtST-induced PLMs over previous ones on diverse representation learning benchmarks. Under the zero-shot setting, we show the effectiveness of ProtST on zero-shot protein classification, and ProtST also enables functional protein retrieval from a large-scale database without any function annotation. Source code and model weights are available at https://github.com/DeepGraphLearning/ProtST.\\n\\n*Equal technical contribution. \u2020Project lead.\\n\\n1Mila - Qu\u00b4ebec AI Institute 2Universit\u00b4e de Montr \u00b4eal 3Intel Labs 4HEC Montr\u00b4eal 5CIFAR AI Research Chair. Correspondence to: Minghao Xu <minghao.xu@mila.quebec>, Santiago Miret <santiago.miret@intel.com>, Jian Tang <jian.tang@hec.ca>.\\n\\nProceedings of the 40th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).\\n\\n1. Introduction\\nProteins serve as the mainstay governing diverse biological processes and life itself, inducing important applications in drug discovery (Teague, 2003) and healthcare (Organization & University, 2007). Recent studies have proven the great promise of machine learning methods in predicting protein structures (Jumper et al., 2021; Baek et al., 2021) and functionality (Meier et al., 2021; Gligorijevi\u0107 et al., 2021). Among these methods, protein language models (PLMs) (Elnaggar et al., 2020; Rives et al., 2021; Lin et al., 2022) pre-trained on large-scale protein sequence corpus succeed in acquiring powerful protein representations, which boost protein structure and function prediction (Xu et al., 2022b).\\n\\nMost existing PLMs (Elnaggar et al., 2020; Lu et al., 2020; Rives et al., 2021; Lin et al., 2022) learn protein representations based only on their sequences, which can well capture co-evolutionary information but cannot explicitly acquire protein functions and other important properties like their subcellular locations. Acquiring such function and property information is actually the end goal of protein representation learning. Fortunately, for many proteins, we can get access to their textual property descriptions in which their diverse functions are also described. This fact motivates us to study protein sequence representation learning enriched with diverse protein properties described by biomedical texts.\\n\\nTo our best knowledge, OntoProtein (Zhang et al., 2022a) is the only existing PLM that explicitly captures protein properties. However, it learns a closed set of properties over a fixed biological knowledge graph and thus can hardly generalize to unknown properties of new proteins. In comparison, by modeling textual protein property descriptions, we can flexibly model the generalization from known properties to unknown ones based on the semantic correlation of their text descriptions, as shown by our zero-shot experiments (Secs. 4.3 and 4.4).\\n\\nTo attain biomedical-text-enhanced protein sequence representation learning, we first build the ProtDescribe dataset, a paired dataset of protein sequences and textual property descriptions. We resort to the Swiss-Prot database (Bairoch & Apweiler, 2000) for high-quality protein annotations and construct each protein\u2019s property description with the semantic correlation of their text descriptions, as shown by our zero-shot experiments (Secs. 4.3 and 4.4).\"}"}
{"id": "xu23t", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts\\n\\nlected annotations of it. ProtDescribe incorporates the information of protein names, protein functions, subcellular locations and protein families, and these properties are described by biomedical texts with rich expressions. Based on this dataset, we propose the ProtST framework to enhance protein sequence pre-training and understanding by biomedical texts. During ProtST pre-training, to preserve the beneficial representation power of a conventional PLM on capturing co-evolutionary information, we adopt the Unimodal Mask Prediction task for masked protein modeling. On such basis, two multimodal pre-training tasks are designed to inject different granularities of pertinent protein property information into a PLM: Multimodal Representation Alignment injects integrated and general property information into the PLM, in which a biomedical language model is used to extract structured text representations of different property descriptions, and protein sequence representations are aligned to the corresponding text representations; Multimodal Mask Prediction models the fine-grained dependencies between residues in a protein sequence and property-descriptive words in its property description, in which a fusion module is employed to derive multimodal representations of residues and words, and, based on these fused multimodal representations, masked residues and words are predicted. For downstream applications, ProtST can conduct supervised learning with only the PLM and can also perform zero-shot prediction based on the aligned representation space of protein sequences and text descriptions.\\n\\nWe investigate the PLMs trained under ProtST by representation learning and zero-shot prediction. For representation learning, we verify their superior performance over previous masked language modeling and knowledge-enhanced PLMs on 11 standard benchmarks for protein localization prediction, fitness landscape prediction and protein function annotation (Sec. 4.2). For zero-shot protein classification, ProtST-induced zero-shot classifiers show better data efficiency against various few-shot classifiers (Sec. 4.3.2), and are proven to be able to enhance the performance of supervised learning models via ensemble (Sec. 4.3.3). For zero-shot text-to-protein retrieval, we verify the effectiveness of ProtST on retrieving functional proteins from a large-scale database without any function annotation (Sec. 4.4).\\n\\n2. Preliminaries\\n\\n2.1. Problem Definition\\n\\nIn the pre-training phase, we study the problem of learning informative protein sequence representations guided by the proteins' associated biomedical text descriptions. In this problem, a protein \\\\( P = (S, T) \\\\) is represented by an amino acid sequence \\\\( S = [s_1, s_2, \\\\ldots, s_n] \\\\) with \\\\( n \\\\) amino acids (a.k.a., residues) and a text description \\\\( T = [t_1, t_2, \\\\ldots, t_m] \\\\) with \\\\( m \\\\) word tokens. Given a pre-training dataset with \\\\( N \\\\) proteins \\\\( P = \\\\{P_1, P_2, \\\\ldots, P_N\\\\} \\\\), our goal is to extract effective protein representations by fully utilizing the information from their sequences and descriptions. The extracted protein representations are expected to boost various downstream tasks by supervised learning or zero-shot prediction.\\n\\n2.2. Protein Language Models\\n\\nProtein language models (PLMs) (Elnaggar et al., 2020; Rives et al., 2021; Meier et al., 2021; Lin et al., 2022) pre-trained on large-scale protein sequence corpus have shown impressive results on protein function (Meier et al., 2021) and structure (Lin et al., 2022) prediction. PLMs are commonly trained by masked protein modeling, in which partial residues are masked at input and predicted based on the context. In this work, we select three state-of-the-art PLMs, ProtBert (Elnaggar et al., 2020), ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022), as baselines and seek to enhance their representation power by modeling biomedical texts at the same time as protein sequence modeling.\\n\\n2.3. Biomedical Language Models\\n\\nCompared to the texts from general domains like newswire and Web, biomedical texts differ a lot in terms of vocabulary and expressions. To tackle such differences, language models specific to the biomedical domain (Beltagy et al., 2019; Lee et al., 2020; Gu et al., 2021) are actively studied. In this work, we employ a performant biomedical language model, PubMedBERT (Gu et al., 2021), to represent the biomedical text descriptions of proteins.\\n\\n3. Method\\n\\nIn this section, we first motivate the proposed ProtST framework and present its general picture in Sec. 3.1, and then elucidate the design of pre-training tasks in Sec. 3.2, followed by discussing the connections with and advantages over previous works in Sec. 3.3.\\n\\n3.1. Motivation and Overview\\n\\nMotivation:\\n\\nExisting PLMs (Elnaggar et al., 2020; Lu et al., 2020; Rives et al., 2021; Lin et al., 2022) learn protein representations primarily based on their sequences, which can well capture co-evolutionary information but cannot explicitly acquire various protein properties like protein functions and subcellular locations. By acquiring such property information, the effectiveness of a PLM can be further improved, considering that the protein properties studied in pre-training and downstream tasks can correlate with each other (Bhardwaj & Lu, 2005).\"}"}
{"id": "xu23t", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts\\n\\n**PROTEIN NAME:** [MASK] myristoylated protein 053R. **PROTEIN FUNCTION:** May play a critical role in virion [MASK]. Essential for virus replication in [MASK]\u2026\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n"}
{"id": "xu23t", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts\\n\\nfunction $L_{MPM}$ for ProtST pre-training. Specifically, for each protein sequence, we randomly mask 15% residue tokens and predict each masked token based on its contextualized representation extracted by the PLM, where $L_{MPM}$ is formulated as a cross-entropy loss to measure the cost.\\n\\nMultimodal Representation Alignment: The biomedical text representations learned by a pre-trained BLM can well reflect the semantics of the texts (Jin et al., 2019; Gu et al., 2021). Therefore, when given protein property descriptions, the BLM can extract semantically meaningful text representations of proteins. Thanks to this capability, by aligning protein sequence representations to their associated text representations, we can naturally inject protein property information into sequence representations.\\n\\nTo realize such alignment, we perform contrastive learning between protein sequences and their text descriptions. Given a batch of $M$ proteins $\\\\{P_i = (S_i, T_i)\\\\}_{i=1}^M$, we use the PLM to extract protein sequence representations $\\\\{z_{S_i}\\\\}_{i=1}^M$ and the BLM to derive text description representations $\\\\{z_{T_i}\\\\}_{i=1}^M$. A standard InfoNCE loss (Oord et al., 2018) $L_{GC}$ is defined to maximize the representation similarity between corresponding sequences and texts and minimize the similarity between negative pairs:\\n\\n$$L_{GC} = \\\\frac{1}{2M} \\\\sum_{i=1}^M \\\\log \\\\frac{\\\\exp(z_{S_i} \\\\cdot z_{T_i}/\\\\tau)}{P_{Mj=1} \\\\exp(z_{S_i} \\\\cdot z_{T_j}/\\\\tau)} + \\\\log \\\\frac{\\\\exp(z_{S_i} \\\\cdot z_{T_i}/\\\\tau)}{P_{Mj=1} \\\\exp(z_{S_j} \\\\cdot z_{T_i}/\\\\tau)}.$$  \\n\\nwhere, under multi-GPU data parallelism, we gather whole-batch samples separated on different GPUs to form negative pairs and thus term the loss $L_{GC}$ as a global contrastive (GC) loss following the convention (Singh et al., 2022), and $\\\\tau$ denotes a learnable temperature parameter.\\n\\nMultimodal Mask Prediction: Although the general dependency between the whole protein sequences and full text descriptions can be well modeled by $L_{GC}$, $L_{GC}$ alone does not capture the dependency between the residues in a protein sequence and the words in its text description. Such fine-grained cross-modality interdependency is actually ubiquitous. For example, a soluble protein (descriptive words) always co-occurs with charged and polar surface residues (Capaldi & Vanderkooi, 1972); high thermostability (descriptive words) and high amounts of hydrophobic residues are correlated with each other (Kumar et al., 2000), etc. To capture such interdependency, we propose a novel pre-training task that encourages the model to recover the corrupted protein sequence (or text description) based on the information from both modalities.\\n\\nSpecifically, given a protein sequence $S = [s_1, s_2, \\\\cdots, s_n]$ and its corresponding text description $T = [t_1, t_2, \\\\cdots, t_m]$, we first randomly mask 15% residues in the protein sequence and 15% words in the text description. Upon the corrupted inputs, we employ the PLM to extract residue representations $Z_{S} = [z_{s_1}, z_{s_2}, \\\\cdots, z_{s_n}]$ and utilize the BLM to extract word representations $Z_{T} = [z_{t_1}, z_{t_2}, \\\\cdots, z_{t_m}]$. A fusion module with both self- and cross-attention is then used to model the interdependency between residues and words, in which each residue and word updates its representation by attending to all the tokens along both protein sequence and text description (we state the detailed architecture in Appendix A). The fusion module produces the fused residue representations $\\\\tilde{Z}_S = [\\\\tilde{z}_{s_1}, \\\\tilde{z}_{s_2}, \\\\cdots, \\\\tilde{z}_{s_n}]$ and the fused word representations $\\\\tilde{Z}_T = [\\\\tilde{z}_{t_1}, \\\\tilde{z}_{t_2}, \\\\cdots, \\\\tilde{z}_{t_m}]$, in which each residue/word representation combines the information from both modalities. Based on $\\\\tilde{Z}_S$ and $\\\\tilde{Z}_T$, we perform multi-modal mask prediction (MMP) to recover masked residues and words, where a cross-entropy loss $L_{SMMP}$ measures the cost on protein sequence, and another cross-entropy loss $L_{TMMP}$ measures the cost on text description, inducing the overall MMP loss $L_{MMP} = L_{SMMP} + L_{TMMP}$.\\n\\nOverall Pre-training Objective: During the pre-training process, we seek to minimize the loss functions of all pre-training tasks simultaneously:\\n\\n$$\\\\min_{\\\\theta} L_{MPM} + L_{GC} + L_{MMP},$$\\n\\nwhere $\\\\theta$ denotes all learnable parameters including those of the PLM, the fusion module and all projection/prediction heads. We state the detailed architectures of these modules in Appendix A.\\n\\n3.3. Discussion\\n\\nNow we discuss the connections of our method with previous works and emphasize its advantages.\\n\\nAdvantages over Self-Supervised PLMs: Previous self-supervised PLMs (Elnaggar et al., 2020; Rives et al., 2021; Lin et al., 2022) and the proposed ProtST-induced ones can both capture co-evolutionary information hidden in protein sequences by masked protein modeling. On this basis, ProtST-induced PLMs further utilize the supervision from textual protein property descriptions, and they are guided to acquire whole-protein properties by multimodal representation alignment and acquire residue-level properties by multimodal mask prediction.\\n\\nAdvantages over OntoProtein (Zhang et al., 2022a): Similar to our approach, OntoProtein also seeks to enhance a self-supervised PLM by involving protein property information. In comparison, ProtST could be more effective mainly in two aspects. (1) Diversity of considered properties: OntoProtein retrieves Gene Ontology terms (Zhang et al., 2022a) to cover protein functions and locations; besides these two kinds of properties, ProtST additionally includes\"}"}
{"id": "xu23t", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 16: Ablation study of BLM on localization and fitness prediction. ProtST-ESM-1b serves as the base model.\\n\\n| Abbr.       | Loc. pred. (Acc%) | Fitness pred. (Spearman\u2019s $\\\\rho$) |\\n|-------------|-------------------|-----------------------------------|\\n| BLM         |                   |                                   |\\n| Bin Sub     |                   |                                   |\\n| $\\\\beta$-lac AA V Thermo Flu Sta |               |                                   |\\n| Mean        |                   |                                   |\\n\\nFix-encoder learning\\n\\n| Abbr.       | Loc. pred. (Acc%) | Fitness pred. (Spearman\u2019s $\\\\rho$) |\\n|-------------|-------------------|-----------------------------------|\\n| BLM         |                   |                                   |\\n| PubMedBERT-abs | 92.87 82.00   | 0.578 0.460                        |\\n| PubMedBERT-full | 93.04 82.28   | 0.548 0.458                        |\\n\\nFull-model tuning\\n\\n| Abbr.       | Loc. pred. (Acc%) | Fitness pred. (Spearman\u2019s $\\\\rho$) |\\n|-------------|-------------------|-----------------------------------|\\n| BLM         |                   |                                   |\\n| PubMedBERT-abs | 92.35 78.73   | 0.895 0.850                        |\\n| PubMedBERT-full | 92.87 78.77   | 0.899 0.785                        |\\n\\nTable 17: Ablation study of BLM on function annotation. ProtST-ESM-1b serves as the base model.\\n\\n| BLM         | EC AUPR $F_{\\\\text{max}}$ | GO-BP AUPR $F_{\\\\text{max}}$ | GO-MF AUPR $F_{\\\\text{max}}$ | GO-CC AUPR $F_{\\\\text{max}}$ |\\n|-------------|--------------------------|-----------------------------|-----------------------------|-----------------------------|\\n| Fix-encoder learning |                   |                                   |                                   |                                   |\\n| BLM         |                           |                                   |                                   |                                   |\\n| PubMedBERT-abs | 0.894 0.878   | 0.328 0.480                        | 0.644 0.661                        | 0.364 0.488                        |\\n| PubMedBERT-full | 0.905 0.878   | 0.323 0.475                        | 0.630 0.652                        | 0.374 0.485                        |\\n\\n3.99% proteins are annotated as ATP binders in GO, 3 GO-annotated GTP binders (only 1.18% proteins are annotated as GTP binders in GO), 2 GO-annotated P5P binders (only 0.17% proteins are annotated as P5P binders in GO), and 2 GO-annotated NAD+ binders (only 0.05% proteins are annotated as NAD+ binders in GO). The rest candidates annotated as non-binding also own decent binding affinity, e.g., the better binding affinity of protein 2AKA-B (without ATP binder annotation) against protein 6EAC-A (with ATP binder annotation), the better binding affinity of protein 5DHG-A (without NAD+ binder annotation) against protein 3GFB-A (with NAD+ binder annotation), etc. These results demonstrate the general effectiveness of ProtST-ESM-1b on retrieving the binders of diverse ligands. In the future work, we will study how ProtST enables zero-shot text-to-protein retrieval of other types of functional proteins, e.g., antigen binders, toxic substance binders, transcription factors, etc.\\n\\nE. More Ablation Study\\n\\nE.1. Ablation Study of Pre-training Losses\\n\\nIn Tabs. 14 and 15, we report the performance of ProtST-ESM-1b on all benchmark tasks by using full or partial pre-training losses. It can be observed that: (1) removing the loss $L_{\\\\text{MPM}}$ leads to performance decay on 16 out of 24 benchmark metrics; (2) removing the loss $L_{\\\\text{GC}}$ leads to decay on 20 out of 24 benchmark metrics; (3) removing the loss $L_{\\\\text{MMP}}$ diminishes model performance on 19 out of 24 benchmark metrics. Therefore, all pre-training losses are necessary to maximize the effectiveness of a ProtST-induced PLM, where $L_{\\\\text{GC}}$ and $L_{\\\\text{MMP}}$ inject different granularities of protein property information into a PLM, and $L_{\\\\text{MPM}}$ preserves the PLM\u2019s original representation power.\"}"}
{"id": "xu23t", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: Visualization of protein representations on the binary localization prediction dataset (ProtST-ESM-1b is used).\\n\\nFigure 9: Visualization of protein representations on the subcellular localization prediction dataset (ProtST-ESM-1b is used).\\n\\nWell-trained PLMs should have the capacity to extract structural, functional, and even evolutionary features of proteins. As a result, the learned representations in PLMs are expected to have certain intrinsic organization patterns in the embedding space to capture these protein characteristics. To demonstrate the effectiveness of ProtST-ESM-1b, we use t-SNE (Van der Maaten & Hinton, 2008) to visualize such information at different scales from amino acid decompositions to protein functional properties.\\n\\nBiophysical Properties of Amino Acids: It is known that the biophysical properties of amino acids, such as hydrophobicity, aromaticity and charge, highly influence the biological structures of proteins and therefore their biological functions as well. To investigate if ProtST-ESM-1b captures such intrinsic features, we apply t-SNE to the two linear layers used for unimodal mask prediction and multimodal mask prediction. As shown in Figs. 6 and 7, hydrophobic and polar residues exhibit clear distinct clusterings, even to the level of aliphatic v.s. aromatic. The clustering is also coherent in terms of the charge and size of the amino acids.\\n\\nBiological and Biochemical Properties of Proteins: As introduced in Sec. 4.1, our proposed ProtDescribe dataset provides ProtST-ESM-1b with direct access to knowledge like protein subcellular localizations, which refers to a specific region within a cell where the proteins can be found. For a protein, such locations can influence its activity and interaction with other molecules, thus helping the PLMs to better capture the biological and biomedical protein functions. To validate this assumption, we adopt the datasets used in two protein localization prediction tasks, i.e., the subcellular localization prediction and the binary localization prediction. With t-SNE, we project protein representations to the 2-dimensional space for these two benchmark datasets. In Figs. 8 and 9, certain clustering patterns of different cellular locations are observed.\"}"}
{"id": "xu23t", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts\\n\\n| Chain | Affinity (kcal/mol) | GO-MF label |\\n|-------|---------------------|-------------|\\n| 6C6B-A | -8.7                | Bind        |\\n| 6EAC-A | -8.2                | Bind        |\\n| 2AKA-B | -8.4                | Non-bind    |\\n| 1YID-B | -7.8                | Bind        |\\n| 5C1S-A | -7.5                | Bind        |\\n| 2CVH-A | -7.5                | Non-bind    |\\n| 4DHE-A | -6.8                | Bind        |\\n| 5HXB-X | -6.4                | Bind        |\\n| 4ILS-A | -5.8                | Bind        |\\n| 5LL2-A | -5.3                | Bind        |\\n| 1EH1-A | -5.6                | Non-bind    |\\n| 3AG6-A | -5.8                | Non-bind    |\\n| 5DHG-A | -8.6                | Non-bind    |\\n| 3GFAB | -7.9                | Bind        |\\n| 5OXU-A | -9.7                | Non-bind    |\\n| 3GGO-A | -7.2                | Bind        |\\n\\nFigure 10: Zero-shot text-to-protein retrieval of (a) ATP binders, (b) GTP binders, (c) P5P binders, and (d) NAD+ binders based on ProtST-ESM-1b.\"}"}
{"id": "xu23t", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts\\n\\nA. Model Architecture for Pre-training\\n\\nFusion Module:\\nThe fusion module extracts multimodal representations from the unimodal representations of protein sequence and text description. As shown in Fig. 5, each fusion layer of this module receives a sequence of residue representations $Z_S = [z_{s1}, z_{s2}, \\\\ldots, z_{sn}] \\\\in \\\\mathbb{R}^{n \\\\times d}$ and a sequence of word representations $Z_T = [z_{t1}, z_{t2}, \\\\ldots, z_{tm}] \\\\in \\\\mathbb{R}^{m \\\\times d}$ ($d$ denotes the hidden dimension), and the layer updates each residue/word representation by attending to all residues and all words. Specifically, two sets of projection matrices $(W_{Sq}, W_{Sk}, W_{ Sv})$ and $(W_{Tq}, W_{Tk}, W_{Tv})$ are respectively used to derive the queries, keys and values for protein sequence and text description as below (each projection matrix is in $\\\\mathbb{R}^{d \\\\times d}$):\\n\\n$$\\n\\\\begin{align*}\\nQ_S &= Z_S W_{Sq}, \\\\\\\\\\nK_S &= Z_S W_{Sk}, \\\\\\\\\\nV_S &= Z_S W_{ Sv}, \\\\\\\\\\n(3)\\n\\\\end{align*}\\n$$\\n\\n$$\\n\\\\begin{align*}\\nQ_T &= Z_T W_{Tq}, \\\\\\\\\\nK_T &= Z_T W_{Tk}, \\\\\\\\\\nV_T &= Z_T W_{Tv}, \\\\\\\\\\n(4)\\n\\\\end{align*}\\n$$\\n\\nwhere $Q_S, K_S, V_S \\\\in \\\\mathbb{R}^{n \\\\times d}$ are the queries, keys and values for protein sequence, and $Q_T, K_T, V_T \\\\in \\\\mathbb{R}^{m \\\\times d}$ are the queries, keys and values for text description. Multi-head self- and cross-attention are then applied to update each residue and word representation as below:\\n\\n$$\\n\\\\begin{align*}\\n\\\\tilde{Z}_S &= \\\\frac{1}{2} \\\\text{MHA}(Q_S, K_S, V_S) + \\\\text{MHA}(Q_S, K_T, V_T), \\\\\\\\\\n\\\\tilde{Z}_T &= \\\\frac{1}{2} \\\\text{MHA}(Q_T, K_T, V_T) + \\\\text{MHA}(Q_T, K_S, V_S), \\\\\\\\\\n(5)\\n\\\\end{align*}\\n$$\\n\\nwhere $\\\\tilde{Z}_S \\\\in \\\\mathbb{R}^{n \\\\times d}$ and $\\\\tilde{Z}_T \\\\in \\\\mathbb{R}^{m \\\\times d}$ are the updated residue and word representations, and $\\\\text{MHA}(\\\\cdot, \\\\cdot, \\\\cdot)$ denotes the multi-head attention operation (Vaswani et al., 2017).\\n\\nIn our implementation, each fusion layer contains 8 attention heads, and we equip the fusion module with a single fusion layer so as to restrict the capacity of fusion module and facilitate the representation power of PLM. Upon the fused residue and word representations produced by the fusion module, multimodal mask prediction is performed.\\n\\nProjection Head for Multimodal Representation Alignment:\\nFollowing SimCLR (Chen et al., 2020), we use a two-layer MLP (with ReLU nonlinearity in between) to project the protein sequence representation extracted by the PLM, and another two-layer nonlinear MLP is employed to project the text description representation extracted by the BLM. The projected sequence and text representations are then used to compute the global contrastive loss defined in Eq. (1).\\n\\nPrediction Head for Masked Protein Modeling (MPM):\\nBased on the residue representations extracted by the PLM, we utilize a two-layer MLP (with ReLU nonlinearity in between) to predict the type of each residue token masked at input.\\n\\nPrediction Head for Multimodal Mask Prediction (MMP):\\nUpon the fused residue representations output from the fusion module, a two-layer MLP (with ReLU nonlinearity in between) is used to predict the type of each residue token masked at input protein sequence. Upon the fused word representations produced by the fusion module, another two-layer nonlinear MLP is employed to predict each word token masked at input text description.\\n\\nB. More Experimental Setups\\n\\nB.1. More Pre-training Setups\\n\\nPre-training Data Curation:\\nWe add prefixes to denote annotations from different fields, i.e., \\\"PROTEIN NAME\\\" for the protein name field, \\\"FUNCTION\\\" for the protein function field, \\\"SUBCELLULAR LOCATION\\\" for the subcellular location field, and \\\"SIMILARITY\\\" for the protein family field. The complete protein property description is formed by concatenating all annotations of the protein in the order of (1) protein name, (2) protein function, (3) subcellular location, and (4) protein family. In Tab. 8, we present several property descriptions coupled with the Swiss-Prot entry names of their corresponding proteins.\\n\\nTraining Configurations:\\nWe list the training configurations of three ProtST-induced PLMs in Tab. 9. In general, an Adam optimizer with the constant learning rate of $1 \\\\times 10^{-5}$ is used to train the model for 20 epochs on 4 Tesla V100 GPUs, where ProtST-ProtBert adopts the batch size of 16 (4 proteins per GPU), and ProtST-ESM-1b and ProtST-ESM-2 adopt the batch size of 12 (3 proteins per GPU). Since the PLM is pre-trained, we set its learning rate as $1 \\\\times 10^{-6}$, i.e., one tenth of other modules. The weights of PubMed\"}"}
{"id": "xu23t", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Examples of property descriptions in the ProtDescribe dataset. We index each description with the Swiss-Prot entry name of its corresponding protein.\\n\\n| Entry name | Description |\\n|------------|-------------|\\n| 14336 | PROTEIN NAME: 14-3-3-like protein GF14-F. FUNCTION: Is associated with a DNA binding complex that binds to the G box, a well-characterized cis-acting DNA regulatory element found in plant genes. SUBCELLULAR LOCATION: Cytoplasm. Nucleus. SIMILARITY: Belongs to the 14-3-3 family. |\\n| 053R | PROTEIN NAME: Putative myristoylated protein 053R. FUNCTION: May play a critical role in virion formation. Essential for virus replication in vitro. SUBCELLULAR LOCATION: Host membrane; Multi-pass membrane protein. |\\n| 1A16 | PROTEIN NAME: 1-aminocyclopropane-1-carboxylate synthase 6. FUNCTION: Catalyzes the formation of 1-aminocyclopropane-1-carboxylate, a direct precursor of ethylene in higher plants (By similarity). Required for the regulation of starch grain size in endosperm. SUBCELLULAR LOCATION: Plastid, amyloplast membrane. Note=Localizes to the amyloplast membrane surrounding starch grains in endosperm, pollen, and pericarp. SIMILARITY: Belongs to the class-I pyridoxal-phosphate-dependent aminotransferase family. |\\n| 17KD | PROTEIN NAME: 17 kDa surface antigen. SUBCELLULAR LOCATION: Cell outer membrane; Lipid-anchor. SIMILARITY: Belongs to the rickettsiale 17 kDa surface antigen family. |\\n| 1A1D | PROTEIN NAME: 1-aminocyclopropane-1-carboxylate deaminase. FUNCTION: Catalyzes a cyclopropane ring-opening reaction, the irreversible conversion of 1-aminocyclopropane-1-carboxylate (ACC) to ammonia and alpha-ketobutyrate. SIMILARITY: Belongs to the ACC deaminase/D-cysteine desulfhydrase family. |\\n| 1AP1 | PROTEIN NAME: Floral homeotic protein APETALA 1-1. FUNCTION: Transcription factor that promotes early floral meristem identity in synergy with LEAFY. Displays a redundant function with CAULIFLOWER in the up-regulation of LEAFY. Required subsequently for the transition of an inflorescence meristem into a floral meristem, and for the normal development of sepals and petals in flowers. Regulates positively B class homeotic proteins (By similarity). SUBCELLULAR LOCATION: Nucleus. |\\n\\nTable 9: ProtST pre-training configurations.\\n\\n| Model | optimizer | lr. | bs. | #epochs | train time |\\n|-------|-----------|-----|-----|---------|------------|\\n| ProtST-ProtBert | Adam | 1.0 \u00d7 10^{-5} | 16 | 20 | 117h 10min |\\n| ProtST-ESM-1b | Adam | 1.0 \u00d7 10^{-5} | 12 | 20 | 205h 36min |\\n| ProtST-ESM-2 | Adam | 1.0 \u00d7 10^{-5} | 12 | 20 | 206h 12min |\\n\\nBERT are frozen along the whole process. To reduce the memory cost, we truncate the protein sequences that have more than 450 residues to the length of 450, where the truncation starts from a random residue before the last 450 ones. Following MoCo (He et al., 2020), we initialize the temperature parameter \\\\( \\\\tau \\\\) in Eq. (1) as 0.07 and optimize it along the training process.\\n\\nB.2. More Representation Learning Setups\\n\\nArchitecture of Prediction Heads: Following the default settings in TorchDrug (Zhu et al., 2022), the prediction of each task is performed by a two-layer MLP with ReLU nonlinearity in between. To be specific, given the protein representation, the MLP head is used to predict classification logits for localization prediction, regression score for fitness prediction and per-function classification logits for function annotation.\\n\\nTable 10: Configurations of fix-encoder learning and full-model tuning on three task types.\\n\\n| Task | optimizer | lr. | bs. | #epochs | loss |\\n|------|-----------|-----|-----|---------|------|\\n| fix-encoder learning | Localization | Adam | 5.0 \u00d7 10^{-5} | 128 | 100 | CE |\\n| | Fitness | Adam | 5.0 \u00d7 10^{-5} | 128 | 100 | MSE |\\n| full-model tuning | Localization | Adam | 2.0 \u00d7 10^{-4} | 12 | 100 | CE |\\n| | Fitness | Adam | 2.0 \u00d7 10^{-4} | 24 | 100 | MSE |\\n| | Annotation | Adam | 1.0 \u00d7 10^{-4} | 8 | 50 | BCE |\\n\\nTraining Configurations: In Tab. 10, we present the detailed configurations of fix-encoder learning and full-model tuning on three task types, which mainly follows the configurations used in PEER benchmark (Xu et al., 2022b). For full-model tuning, the learning rate of the PLM is set as one tenth of the value in Tab. 10. The protein sequence encoders trained from scratch do not use smaller learning rates. All experiments are conducted on 4 Tesla V100 GPUs.\\n\\nEvaluation Metrics: The protein function annotation tasks are measured by AUPR and \\\\( F_{\\\\text{max}} \\\\). We clarify their definitions...\"}"}
{"id": "xu23t", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: Zero-shot protein classification performance under different prompt templates.\\n\\n| Abbr.               | Label                  | Subcellular loc. (Acc%) | Reaction (Acc%) |\\n|---------------------|------------------------|-------------------------|-----------------|\\n|                     |                        | 25.68                   | 25.27           |\\n| Name only           | Name                   | 36.24                   | 26.93           |\\n| Natural language    | Name                   | 43.49                   | 29.85           |\\n| Pre-training template | Name                | 29.90                   | 21.91           |\\n\\ntions as below:\\n\\n(1) \\\\(AUPR\\\\) denotes the pair-centric area under precision-recall curve. It computes the average precision scores for all protein-function pairs, which is exactly the micro-average precision score for the multiple binary classification problem.\\n\\n(2) \\\\(F^{\\\\text{max}}\\\\) denotes the protein-centric maximum F-score. Given a decision threshold \\\\(t \\\\in [0, 1]\\\\), it first calculates the precision and recall for each protein:\\n\\n\\\\[\\n\\\\text{precision}^i(t) = \\\\frac{\\\\sum_{f \\\\in P_i(t) \\\\cap T_i}}{\\\\sum_{f \\\\in P_i(t)}}\\n\\\\]\\n\\n\\\\[\\n\\\\text{recall}^i(t) = \\\\frac{\\\\sum_{f \\\\in P_i(t) \\\\cap T_i}}{\\\\sum_{f \\\\in T_i}}\\n\\\\]\\n\\nwhere \\\\(f\\\\) denotes a functional term of EC or GO, \\\\(T_i\\\\) is the set collecting all experimentally determined functions for protein \\\\(i\\\\), \\\\(P_i(t)\\\\) denotes the predicted functions for protein \\\\(i\\\\) whose scores are at least \\\\(t\\\\), and \\\\([\\\\cdot]\\\\) represents the indicator function. The precision and recall are then averaged over all proteins:\\n\\n\\\\[\\n\\\\text{precision}(t) = \\\\frac{1}{M(t)} \\\\sum_i \\\\text{precision}^i(t)\\n\\\\]\\n\\n\\\\[\\n\\\\text{recall}(t) = \\\\frac{1}{N} \\\\sum_i \\\\text{recall}^i(t)\\n\\\\]\\n\\nwhere \\\\(N\\\\) is the total number of proteins, and \\\\(M(t)\\\\) denotes the number of proteins that contain at least one prediction larger than \\\\(t\\\\), i.e., \\\\(|P_i(t)| > 0\\\\).\\n\\nFinally, the \\\\(F^{\\\\text{max}}\\\\) score is computed as the maximum value of F-measure over all thresholds:\\n\\n\\\\[\\nF^{\\\\text{max}} = \\\\max_t 2 \\\\cdot \\\\frac{\\\\text{precision}(t) \\\\cdot \\\\text{recall}(t)}{\\\\text{precision}(t) + \\\\text{recall}(t)}\\n\\\\]\\n\\nB.3. More Zero-shot Protein Classification Setups\\n\\nPrompt Engineering for Subcellular Localization Prediction:\\n\\nBased on the information provided by DeepLoc (Almagro Armenteros et al., 2017), we consider two label formats, the name of each subcellular location (i.e., the \u201cLocation\u201d field in the Tab. 1 of DeepLoc paper) and the description of each location (i.e., the \u201cSublocations\u201d field in the Tab. 1 of DeepLoc paper). We further embed the labels into three prompt templates: (1) Name only: only the label itself is used; (2) Natural language: the label is embedded into the template \u201cA protein locating at {label}.\u201d; (3) Pre-training template: the label is embedded into the template \u201cSUBCELLULAR LOCATION: {label}.\u201d\\n\\nAccording to the results in Tab. 11, we can observe that the pre-training template clearly outperforms other two templates on the subcellular localization prediction task, which mainly owes to the alignment of text format across pre-training and zero-shot prediction. It is shown that representing the labels with location names leads to better performance than using location descriptions, since the location names better fit the biomedical text distribution that the BLM is trained on. Based on these results, we represent the labels with the location names coupled with the pre-training prompt template on this task.\\n\\nPrompt Engineering for Reaction Classification:\\n\\nSame as subcellular localization prediction, we also use two sets of label notations for reaction classification, i.e., the name and the description. (1) The name refers to the composition of the enzyme class name and its alternative names, allowing unambiguous identification of each enzyme class. (2) The description further adds the scientific comments that discuss each class of enzymes in depth, which are extracted from scientific articles published by the International Union of Biochemistry and Molecular Biology (IUBMB). We retrieve all the information from Chang et al. (2021).\\n\\nWe embed such label information into three prompt templates: (1) Name only: the concatenation of the name and alternative names of an enzyme class, i.e., \u201c{Name} {AlterNames}\u201d; (2) Natural Language: the label is incorporated into a natural-language-like template \u201cA {Name} enzyme. This enzyme is also known as {AlterNames}.\u201d; (3) Pre-training template: the label is merged into the template used for pre-training, i.e., \u201cFUNCTION: {Name} {AlterNames}\u201d (scientific comments \u201c{Comments}\u201d are appended after the names if the description is used).\\n\\nAccording to Tab. 11, the pre-training template performs the best on the reaction classification task, mainly thanks to the consistent format of text descriptions between pre-training and zero-shot prediction. Injecting detailed scientific comments does not bring further benefits to the zero-shot performance. Therefore, we represent each enzyme class with its name and alternative names along with the pre-training prompt template for this task.\\n\\nNonparametric Few-shot Classifier:\\n\\nWe adopt the nonparametric classifier proposed by Khandelwal et al. (2019) as baseline. Specifically, given \\\\(n\\\\)-shot \\\\(K\\\\)-class training samples \\\\(\\\\{(S_k, y_k = k)\\\\}_{n=1}^{K}\\\\) composed of pairs of protein sequence and label, we employ the PLM to extract the representation...\"}"}
{"id": "xu23t", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts\\n\\nTable 12: Performance comparison of PLMs on ProteinGym Substitution benchmark.\\n\\n| Abbr. | Model Type | UniProt-level Mean $\\\\rho$ |\\n|-------|------------|---------------------------|\\n| ESM-1b | PLM | 0.412 |\\n| ESM-1v | PLM | 0.358 |\\n| Tranception L (w/o retr.) | PLM | 0.372 |\\n| Progen2 XL | PLM | 0.401 |\\n\\nTable 13: ProtST-ESM-1b v.s. alignment-based methods on ProteinGym Substitution benchmark.\\n\\n| Abbr. | Model Type | UniProt-level Mean $\\\\rho$ |\\n|-------|------------|---------------------------|\\n| EVE | Align | 0.443 |\\n| GEMME | Align | 0.459 |\\n| ProtST-ESM-1b + GEMME | Hybrid | 0.464 |\\n\\nC. Experimental Results on ProteinGym\\n\\nC.1. Comparisons of Protein Language Models (PLMs)\\n\\nWe compare the proposed ProtST-ESM-1b with four performant PLMs, i.e., ESM-1b (Rives et al., 2021), ESM-1v (Meier et al., 2021), Tranception L (w/o retrieval) (Notin et al., 2022) and Progen2 XL (Nijkamp et al., 2022). Note that, for fair comparison, we do not include the PLMs with model ensemble (e.g., VESPA (Marquet et al., 2022)) and the PLMs with inference-time retrieval (e.g., Tranception L w/ retrieval (Notin et al., 2022)). We report the UniProt-level Mean Spearman's $\\\\rho$.\\n\\nResults. Under such a fair comparison, in Tab. 12, ProtST-ESM-1b achieves the best performance. In particular, compared with ESM-1b (i.e., the initial PLM that ProtST-ESM-1b is based on), ProtST-ESM-1b obtains a significant performance gain with 15.1% relative improvement. This result demonstrates the effectiveness of the proposed multimodal training, which injects protein property knowledge into the ESM-1b and enhances its downstream fitness prediction performance.\\n\\nC.2. Comparisons with Alignment-based Methods\\n\\nBaselines.\\n\\nIn this experiment, we involve two alignment-based methods, i.e., EVE (Frazer et al., 2021) and GEMME (Laine et al., 2019), for comparison. We further investigate the ensemble of ProtST-ESM-1b and GEMME.\\n\\nResults. In Tab. 13, it is observed that the alignment-based methods are superior over ProtST-ESM-1b, since they additionally utilize the homologous information within sequence alignments, which is not utilized by ProtST-ESM-1b. However, by combining the normalized predictions of ProtST-ESM-1b and GEMME, the ensemble model \u201cProtST-ESM-1b + GEMME\u201d outperforms these two SOTA alignment-based methods. This result verifies the complementary knowledge hidden in ProtST-ESM-1b and an alignment-based model in terms of fitness prediction. Therefore, it will be a promising direction to study the combination of these two lines of methods. We leave this as our future work.\\n\\nD. More Zero-shot Text-to-Protein Retrieval Results\\n\\nIn Fig. 10, we study four more sets of text-to-protein retrieval of ligand binders based on ProtST-ESM-1b. For each study, we visualize the text prompt and the top-4 retrieved candidates. For each candidate, we present the docking result of it binding with the ligand, the binding affinity and its GO molecular function label of binding with the ligand, where AutoDock Vina (Trott & Olson, 2010) is used to estimate docking pose and binding affinity. It is observed that, among the top-4 candidates, ProtST-ESM-1b succeeds in retrieving 3 GO-annotated ATP binders (only 16\"}"}
{"id": "xu23t", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. Related Work\\n\\nProtein Representation Learning:\\nLearning effective protein representations is of great importance for machine learning guided protein understanding. Existing works learn protein representations in two ways: (1) Sequence-based methods model protein sequences on evolutionary scale (El-naggar et al., 2020; Rives et al., 2021; Lin et al., 2022) or on individual protein families (Bileschi et al., 2019; Meier et al., 2021; Biswas et al., 2021); (2) Structure-based methods seek to represent different levels of protein structures including residue-level structures (Gligorijevi\u0107 et al., 2021; Zhang et al., 2022b; Xu et al., 2022a), all-atom structures (Jing et al., 2020; Zhang et al., 2023) and protein surfaces (Gainza et al., 2020; Sverrisson et al., 2021). Our work aims to enhance protein sequence representation learning by using textual protein property descriptions.\\n\\nMultimodal Representation Learning:\\nIt has been broadly studied how to learn better image (Radford et al., 2021; Singh et al., 2022), video (Luo et al., 2020; Xu et al., 2021), speech (Chung et al., 2020; Qian et al., 2021) and molecule (Edwards et al., 2021; Liu et al., 2022) representations by incorporating text supervision, while such study is lacked for proteins. OntoProtein (Zhang et al., 2022a) learns protein representations under the context of a knowledge graph; ProGen (Madani et al., 2020) incorporates protein function labels to generate functional proteins. However, these two works investigate less the effect of biomedical texts. Our work takes the initiative of enhancing protein sequence representation learning by biomedical texts.\\n\\n6. Conclusions and Future Work\\n\\nIn this work, we propose the ProtST framework to study how textual protein property descriptions can boost protein sequence pre-training and understanding. We build the ProtDescribe dataset that aligns protein sequences with their diverse property descriptions. ProtST pre-training injects the property information with different granularities into a protein language model (PLM). The ProtST-induced PLMs are verified to be generally effective on various downstream applications including supervised learning, zero-shot protein classification and zero-shot text-to-protein retrieval.\\n\\nThe current ProtDescribe dataset is limited in the coverage of protein sequences and textual property descriptions, which motivates us to resort to massive biomedical articles in PubMed (Canese & Weis, 2013) for information extraction. In addition, we plan to extend the ProtDescribe dataset by incorporating protein structures and study biomedical text enhanced protein structure representation learning. Also, we will go beyond text-to-protein retrieval towards text-guided controllable protein design.\\n\\nAcknowledgments\\n\\nThe authors would like to thank Meng Qu, Zhaocheng Zhu, Zuobai Zhang and Hesham Mostafa for their helpful discussions and comments.\\n\\nThis project is supported by Intel-MILA partnership program, the Natural Sciences and Engineering Research Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants between Microsoft Research and Mila, Samsung Electronics Co., Ltd., Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund, a NRC Collaborative R&D Project (AI4D-CORE-06) as well as the IV ADO Fundamental Research Project grant PRF-2019-3583139727.\"}"}
{"id": "xu23t", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Almagro Armenteros, J. J., S\u00f8nderby, C. K., S\u00f8nderby, S. K., Nielsen, H., and Winther, O. Deeploc: prediction of protein subcellular localization using deep learning. *Bioinformatics*, 33(21):3387\u20133395, 2017.\\n\\nBaek, M., DiMaio, F., Anishchenko, I., Dauparas, J., Ovchinnikov, S., Lee, G. R., Wang, J., Cong, Q., Kinch, L. N., Schaeffer, R. D., et al. Accurate prediction of protein structures and interactions using a three-track neural network. *Science*, 373(6557):871\u2013876, 2021.\\n\\nBairoch, A. and Apweiler, R. The swiss-prot protein sequence database and its supplement trembl in 2000. *Nucleic acids research*, 28(1):45\u201348, 2000.\\n\\nBeltagy, I., Lo, K., and Cohan, A. Scibert: A pre-trained language model for scientific text. *arXiv preprint arXiv:1903.10676*, 2019.\\n\\nBhardwaj, N. and Lu, H. Correlation between gene expression profiles and protein\u2013protein interactions within and across genomes. *Bioinformatics*, 21(11):2730\u20132738, 2005.\\n\\nBileschi, M. L., Belanger, D., Bryant, D., Sanderson, T., Carter, B., Sculley, D., DePristo, M. A., and Colwell, L. J. Using deep learning to annotate the protein universe. *BioRxiv*, pp. 626507, 2019.\\n\\nBiswas, S., Khimulya, G., Alley, E. C., Esvelt, K. M., and Church, G. M. Low-n protein engineering with data-efficient deep learning. *Nature methods*, 18(4):389\u2013396, 2021.\\n\\nCanese, K. and Weis, S. Pubmed: the bibliographic database. *The NCBI handbook*, 2(1), 2013.\\n\\nCapaldi, R. A. and Vanderkooi, G. The low polarity of many membrane proteins. *Proceedings of the National Academy of Sciences*, 69(4):930\u2013932, 1972.\\n\\nChang, A., Jeske, L., Ulbrich, S., Hofmann, J., Koblitz, J., Schomburg, I., Neumann-Schaal, M., Jahn, D., and Schomburg, D. Brenda, the elixir core data resource in 2021: new developments and updates. *Nucleic Acids Research*, 49(D1):D498\u2013D508, 2021.\\n\\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In *International conference on machine learning*, pp. 1597\u20131607. PMLR, 2020.\\n\\nChung, Y. -A., Zhu, C., and Zeng, M. Splat: Speech-language joint pre-training for spoken language understanding. *arXiv preprint arXiv:2010.02295*, 2020.\\n\\nConsortium, U. Uniprot: a worldwide hub of protein knowledge. *Nucleic acids research*, 47(D1):D506\u2013D515, 2019.\\n\\nDallago, C., Mou, J., Johnston, K. E., Wittmann, B. J., Bhattarcharya, N., Goldman, S., Madani, A., and Yang, K. K. Flip: Benchmark tasks in fitness landscape inference for proteins. *bioRxiv*, 2021.\\n\\nEdwards, C., Zhai, C., and Ji, H. Text2mol: Cross-modal molecule retrieval with natural language queries. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pp. 595\u2013607, 2021.\\n\\nElnaggar, A., Heinzinger, M., Dallago, C., Rihawi, G., Wang, Y., Jones, L., Gibbs, T., Feher, T., Angerer, C., Steinegger, M., et al. Prottrans: towards cracking the language of life's code through self-supervised deep learning and high performance computing. *arXiv preprint arXiv:2007.06225*, 2020.\\n\\nFrazer, J., Notin, P., Dias, M., Gomez, A., Min, J. K., Brock, K., Gal, Y., and Marks, D. S. Disease variant prediction with deep generative models of evolutionary data. *Nature*, 599(7883):91\u201395, 2021.\\n\\nGainza, P., Sverrisson, F., Monti, F., Rodola, E., Boscaini, D., Bronstein, M., and Correia, B. Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning. *Nature Methods*, 17(2):184\u2013192, 2020.\\n\\nGligorijevic, V., Renfrew, P. D., Kosciolek, T., Leman, J. K., Berenberg, D., Vatanen, T., Chandler, C., Taylor, B. C., Fisk, I. M., Vlamakis, H., et al. Structure-based protein function prediction using graph convolutional networks. *Nature communications*, 12(1):1\u201314, 2021.\\n\\nGu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J., and Poon, H. Domain-specific language model pretraining for biomedical natural language processing. *ACM Transactions on Computing for Healthcare (HEALTH)*, 3(1):1\u201323, 2021.\\n\\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 9729\u20139738, 2020.\\n\\nHermosilla, P., Sch\u00e4fer, M., Lang, M., Fackelmann, G., V\u00e1zquez, P. P., Kozl\u00edkov\u00e1, B., Krone, M., Ritschel, T., and Ropinski, T. Intrinsic-extrinsic convolution and pooling for learning on 3d protein structures. *arXiv preprint arXiv:2007.06252*, 2020.\\n\\nJin, Q., Dhingra, B., Cohen, W. W., and Lu, X. Probing biomedical embeddings from language models. *arXiv preprint arXiv:1904.02181*, 2019.\"}"}
{"id": "xu23t", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts\\nJing, B., Eismann, S., Suriana, P., Townshend, R. J., and Dror, R. Learning from protein structure with geometric vector perceptrons. arXiv preprint arXiv:2009.01411, 2020.\\n\\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., \u017d\u00eddek, A., Potapenko, A., et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.\\n\\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019.\\n\\nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nKumar, S., Tsai, C.-J., and Nussinov, R. Factors enhancing protein thermostability. Protein engineering, 13(3):179\u2013191, 2000.\\n\\nLaine, E., Karami, Y., and Carbone, A. Gemme: a simple and fast global epistatic model predicting mutational effects. Molecular biology and evolution, 36(11):2604\u20132619, 2019.\\n\\nLee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., and Kang, J. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234\u20131240, 2020.\\n\\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., dos Santos Costa, A., Fazel-Zarandi, M., Sercu, T., Candido, S., et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv, 2022.\\n\\nLiu, S., Nie, W., Wang, C., Lu, J., Qiao, Z., Liu, L., Tang, J., Xiao, C., and Anandkumar, A. Multi-modal molecule structure-text model for text-based retrieval and editing. arXiv preprint arXiv:2212.10789, 2022.\\n\\nLu, A. X., Zhang, H., Ghassemi, M., and Moses, A. M. Self-supervised contrastive learning of protein representations by mutual information maximization. BioRxiv, 2020.\\n\\nLuo, H., Ji, L., Shi, B., Huang, H., Duan, N., Li, T., Li, J., Bharti, T., and Zhou, M. Univl: A unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353, 2020.\\n\\nMadani, A., McCann, B., Naik, N., Keskar, N. S., Anand, N., Eguchi, R. R., Huang, P.-S., and Socher, R. Progen: Language modeling for protein generation. arXiv preprint arXiv:2004.03497, 2020.\\n\\nMarquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Erckert, K., Bernhofer, M., Nechaev, D., and Rost, B. Embeddings from protein language models predict conservation and variant effects. Human genetics, 141(10):1629\u20131647, 2022.\\n\\nMeier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., and Rives, A. Language models enable zero-shot prediction of the effects of mutations on protein function. bioRxiv, 2021.\\n\\nMurzin, A. G., Brenner, S. E., Hubbard, T., and Chothia, C. Scop: a structural classification of proteins database for the investigation of sequences and structures. Journal of molecular biology, 247(4):536\u2013540, 1995.\\n\\nNijkamp, E., Ruffolo, J., Weinstein, E. N., Naik, N., and Madani, A. Progen2: exploring the boundaries of protein language models. arXiv preprint arXiv:2206.13517, 2022.\\n\\nNotin, P., Dias, M., Frazer, J., Hurtado, J. M., Gomez, A. N., Marks, D., and Gal, Y. Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. In International Conference on Machine Learning, pp. 16990\u201317017. PMLR, 2022.\\n\\nOord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\\n\\nOrganization, W. H. and University, U. N. Protein and amino acid requirements in human nutrition, volume 935. World Health Organization, 2007.\\n\\nQian, Y., Bianv, X., Shi, Y., Kanda, N., Shen, L., Xiao, Z., and Zeng, M. Speech-language pre-training for end-to-end spoken language understanding. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7458\u20137462. IEEE, 2021.\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748\u20138763. PMLR, 2021.\\n\\nRao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen, P., Canny, J., Abbeel, P., and Song, Y. Evaluating protein transfer learning with tape. Advances in neural information processing systems, 32, 2019.\\n\\nRives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15), 2021.\"}"}
{"id": "xu23t", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts\\n\\nShanehsazzadeh, A., Belanger, D., and Dohan, D. Is transfer learning necessary for protein landscape prediction? arXiv preprint arXiv:2011.03443, 2020.\\n\\nSingh, A., Hu, R., Goswami, V., Couairon, G., Galuba, W., Rohrbach, M., and Kiela, D. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15638\u201315650, 2022.\\n\\nSteinegger, M. and S\u00f6ding, J. Clustering huge protein sequence sets in linear time. Nature communications, 9(1):1\u20138, 2018.\\n\\nSverrisson, F., Feydy, J., Correia, B. E., and Bronstein, M. M. Fast end-to-end learning on protein surfaces. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15272\u201315281, 2021.\\n\\nTeague, S. J. Implications of protein flexibility for drug discovery. Nature reviews Drug discovery, 2(7):527\u2013541, 2003.\\n\\nTrott, O. and Olson, A. J. Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of computational chemistry, 31(2):455\u2013461, 2010.\\n\\nVan der Maaten, L. and Hinton, G. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nXu, H., Ghosh, G., Huang, P.-Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettlemoyer, L., and Feichtenhofer, C. Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv preprint arXiv:2109.14084, 2021.\\n\\nXu, M., Guo, Y., Xu, Y., Tang, J., Chen, X., and Tian, Y. Eurnet: Efficient multi-range relational modeling of spatial multi-relational data. arXiv preprint arXiv:2211.12941, 2022a.\\n\\nXu, M., Zhang, Z., Lu, J., Zhu, Z., Zhang, Y., Ma, C., Liu, R., and Tang, J. Peer: A comprehensive and multi-task benchmark for protein sequence understanding. arXiv preprint arXiv:2206.02096, 2022b.\\n\\nZhang, N., Bi, Z., Liang, X., Cheng, S., Hong, H., Deng, S., Lian, J., Zhang, Q., and Chen, H. Ontoprotein: Protein pretraining with gene ontology embedding. arXiv preprint arXiv:2201.11147, 2022a.\\n\\nZhang, Z., Xu, M., Jamasb, A., Chenthamarakshan, V., Lozano, A., Das, P., and Tang, J. Protein representation learning by geometric structure pretraining. arXiv preprint arXiv:2203.06125, 2022b.\\n\\nZhu, Z., Shi, C., Zhang, Z., Liu, S., Xu, M., Yuan, X., Zhang, Y., Chen, J., Cai, H., Lu, J., et al. Torchdrug: A powerful and flexible machine learning platform for drug discovery. arXiv preprint arXiv:2202.08320, 2022.\"}"}
{"id": "xu23t", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of the ProtDescribe dataset.\\n\\n| Field          | #Covered samples | Coverage |\\n|----------------|-----------------|----------|\\n| Protein names  | 553,052         | 100%     |\\n| Subcellular    | 460,936         | 83.3%    |\\n| Similarity     | 350,929         | 63.5%    |\\n| Function       | 512,276         | 92.6%    |\\n\\nProtein names and families which are useful to indicate protein structural and functional similarity (Murzin et al., 1995). (2)\\n\\nProperty modeling manner: OntoProtein learns a closed set of protein properties under the context of a fixed biological knowledge graph, which limits its ability to generalize to unknown properties of new proteins, while ProtST can flexibly model such generalization based on the semantic correlation of text descriptions between known and unknown properties, leading to decent zero-shot prediction capability (studied in Secs. 4.3 and 4.4).\\n\\n4. Experiments\\n\\n4.1. Pre-training Setups\\n\\nPre-training Dataset: To inject protein property information into PLMs, we build the ProtDescribe dataset with 553,052 aligned pairs of protein sequence and property description. Specifically, we employ the Swiss-Prot (Bairoch & Apweiler, 2000) database to provide annotations of various protein properties, in which we select four property fields: (1) \\\"Protein Name\\\" gives the full protein name recommended by the UniProt consortium (Consortium, 2019); (2) \\\"Function\\\" depicts diverse functions owned by a protein; (3) \\\"Subcellular Location\\\" describes the location and topology of a mature protein in the cell; (4) \\\"Similarity\\\" provides information about the protein families that a protein belongs to. A complete property description is formed by concatenating these four fields in order, where missing fields are skipped (see Appendix B.1 for the detailed concatenation scheme and examples). Tab. 1 presents the statistics of how each field covers the whole dataset.\\n\\nProtein Language Models: We seek to enhance three performant PLMs, i.e., ProtBert (Elnaggar et al., 2020), ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022), by tuning their weights through the proposed ProtST pre-training. We name the PLMs after this pre-training phase as ProtST-ProtBert, ProtST-ESM-1b and ProtST-ESM-2.\\n\\nFor ProtBert, we employ the ProtBert-BFD version which is trained on the BFD database (Steinegger & Soding, 2018).\\n\\nFor ESM-2, we adopt the ESM-2-650M model so as to fairly compare with ESM-1b under the same model size.\\n\\nBiomedical Language Models: By default, we utilize the PubMedBERT-abs (Gu et al., 2021) trained on PubMed abstracts to extract representations of protein property descriptions. We study another model version, PubMedBERT-full trained with additional full-text articles, in Appendix E.2.\\n\\nTraining Configurations: An Adam optimizer (Kingma & Ba, 2014) (learning rate: 1.0 \u00d7 10^{-5}, weight decay: 0) is used to train the whole model for 20 epochs on 4 Tesla V100 GPUs. More settings are introduced in Appendix B.1.\\n\\n4.2. Representation Learning\\n\\n4.2.1. Experimental Setups\\n\\nDownstream Benchmark Tasks. We adopt 11 benchmark tasks within three task types (the \\\"Abbr.\\\" below denotes the abbreviated task name in Tab. 2 and 3):\\n\\n- **Protein Localization Prediction** seeks to predict the subcellular locations of proteins. We consider two such problems from DeepLoc (Almagro Armenteros et al., 2017), the subcellular localization prediction (Abbr., Sub) with 10 location categories and the binary localization prediction (Abbr., Bin) with 2 location categories. We follow the official dataset splits.\\n\\n- **Fitness Landscape Prediction** aims to predict the effect of residue mutations on protein fitness. We employ the \u03b2-lactamase (Abbr., \u03b2-lac) landscape from PEER (Xu et al., 2022b), the AA V and Thermostability (Abbr., Thermo) landscapes from FLIP (Dallago et al., 2021), and the Fluorescence (Abbr., Flu) and Stability (Abbr., Sta) landscapes from TAPE (Rao et al., 2019). For AA V , we use the \\\"two vs many\\\" dataset splits; for Thermostability, we adopt the \\\"human cell\\\" splits; we follow the only default splits on all other tasks. In Appendix C, we further show the results on ProteinGym (Notin et al., 2022).\\n\\n- **Protein Function Annotation** seeks to annotate a protein with multiple functional labels. We employ two standard benchmarks proposed by DeepFRI (Gligorijevic et al., 2021), i.e., Enzyme Commission (EC) number prediction and Gene Ontology (GO) term prediction. The GO benchmark is split into three branches to predict molecular function (Abbr., GO-MF), biological process (Abbr., GO-BP) and cellular component (Abbr., GO-CC). Following Zhang et al. (2022b), we use the dataset splits under 95% sequence identity cutoff for both EC and GO.\\n\\nBaselines: We adopt four protein sequence encoders trained from scratch, i.e., CNN (Shanehsazzadeh et al., 2020), ResNet (Rao et al., 2019), LSTM (Rao et al., 2019) and Transformer (Rao et al., 2019), as naive baselines. We focus on comparing with four performant PLMs, i.e., ProtBert (Elnaggar et al., 2020), OntoProtein (Zhang et al., 2022a), ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022).\\n\\nTraining and Evaluation: We train with an Adam optimizer for 100 epochs on localization and fitness prediction tasks and for 50 epochs on function annotation tasks. For localization and fitness prediction, all PLMs are evaluated under both fix-encoder learning and full-model tuning settings, and only full-model tuning is used for PLMs on...\"}"}
{"id": "xu23t", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Benchmark results on protein localization and fitness landscape prediction. We use three color scales of blue to denote the first, second and third best performance.\\n\\n| Model                           | Loc. pred. (Acc%) | Fitness pred. (Spearman\u2019s $\\\\rho$) |\\n|---------------------------------|-------------------|-----------------------------------|\\n| Bin Sub $\\\\beta$-lac AA V Thermo Flu Sta Mean $\\\\rho$ |                   |                                   |\\n| Protein sequence encoders trained from scratch |                   |                                   |\\n| CNN                            | 82.67             | 0.781 0.746 0.494 0.682 0.637 0.668 |\\n| ResNet                         | 78.99             | 0.152 0.739 0.528 0.636 0.126 0.436 |\\n| LSTM                           | 88.11             | 0.139 0.125 0.564 0.494 0.533 0.371 |\\n| Transformer                    | 75.74             | 0.261 0.681 0.545 0.643 0.649 0.556 |\\n| PLMs w/ fix-encoder learning   |                   |                                   |\\n| ProtBert                       | 81.54             | 0.616 0.209 0.562 0.339 0.697 0.485 |\\n| OntoProtein                    | 84.87             | 0.471 0.217 0.605 0.432 0.688 0.483 |\\n| ESM-1b                         | 91.61             | 0.528 0.454 0.674 0.430 0.750 0.567 |\\n| ESM-2                          | 91.32             | 0.559 0.374 0.677 0.456 0.746 0.562 |\\n| ProtST-ProtBert                | 92.29             | 0.569 0.219 0.621 0.376 0.719 0.501 |\\n| ProtST-ESM-1b                  | 92.87             | 0.578 0.460 0.680 0.523 0.766 0.601 |\\n| ProtST-ESM-2                   | 92.52             | 0.565 0.398 0.681 0.499 0.776 0.584 |\\n| PLMs w/ full-model tuning      |                   |                                   |\\n| ProtBert                       | 91.32             | 0.731 0.794 0.660 0.679 0.771 0.727 |\\n| OntoProtein                    | 92.47             | 0.757 0.791 0.662 0.630 0.731 0.714 |\\n| ESM-1b                         | 92.40             | 0.839 0.821 0.669 0.679 0.694 0.740 |\\n| ESM-2                          | 91.72             | 0.867 0.817 0.672 0.677 0.718 0.750 |\\n| ProtST-ProtBert                | 91.78             | 0.863 0.804 0.673 0.679 0.745 0.753 |\\n| ProtST-ESM-1b                  | 92.35             | 0.895 0.850 0.681 0.682 0.751 0.772 |\\n| ProtST-ESM-2                   | 92.52             | 0.879 0.825 0.682 0.682 0.738 0.761 |\\n\\nTable 3: Benchmark results on protein function annotation. We use three color scales of blue to denote the first, second and third best performance.\\n\\n| Model                           | EC AUPR | GO-BP $F_{max}$ | GO-MF AUPR | GO-CC $F_{max}$ |\\n|---------------------------------|---------|-----------------|------------|-----------------|\\n| Protein sequence encoders trained from scratch |         |                 |            |                 |\\n| CNN                            | 0.540   | 0.165 0.244     | 0.380      | 0.354           |\\n| ResNet                         | 0.137   | 0.166 0.280     | 0.281      | 0.267           |\\n| LSTM                           | 0.032   | 0.130 0.248     | 0.100      | 0.166           |\\n| Transformer                    | 0.187   | 0.135 0.257     | 0.172      | 0.240           |\\n| PLMs w/ full-model tuning      |         |                 |            |                 |\\n| ProtBert                       | 0.859   | 0.188 0.279     | 0.464      | 0.456           |\\n| OntoProtein                    | 0.854   | 0.284 0.436     | 0.603      | 0.631           |\\n| ESM-1b                         | 0.884   | 0.332 0.452     | 0.630      | 0.659           |\\n| ESM-2                          | 0.888   | 0.340 0.472     | 0.643      | 0.662           |\\n| ProtST-ProtBert                | 0.876   | 0.286 0.440     | 0.615      | 0.648           |\\n| ProtST-ESM-1b                  | 0.894   | 0.328 0.480     | 0.644      | 0.661           |\\n| ProtST-ESM-2                   | 0.898   | 0.342 0.482     | 0.647      | 0.668           |\\n\\n4.2.2. EXPERIMENTAL RESULTS\\n\\nWe report the benchmark results on localization and fitness prediction in Tab. 2 and report function annotation results in Tab. 3. Based on the benchmark results, we have the following observations:\\n\\n- ProtST-induced PLMs clearly outperform the vanilla PLMs.\\n- It is observed that: (1) ProtST-ProtBert outperforms the vanilla ProtBert on 21 out of 24 benchmark metrics (including both fix-encoder learning and full-model tuning ones); (2) ProtST-ESM-1b surpasses the vanilla ESM-1b on 22 out of 24 benchmark metrics; (3) ProtST-ESM-2 outperforms the vanilla ESM-2 on all 24 benchmark metrics. These results demonstrate that ProtST pre-training is generally beneficial to different PLMs, which boosts their performance on diverse downstream tasks.\\n\\n- ProtST-ProtBert performs consistently better than OntoProtein under fair comparison.\\n- ProtST-ProtBert and OntoProtein can be fairly compared with each other, since they both adopt ProtBert as the initial PLM. ProtST-ProtBert surpasses OntoProtein on 22 out of 24 benchmark metrics, which verifies the superiority of the proposed pre-training dataset and pre-training tasks.\\n\\nProtST-ESM-1b performs best on fitness prediction, and ProtST-ESM-2 performs best on localization prediction and function annotation. We can observe that: (1) ProtST-ESM-1b achieves the best performance on 4 out of 6 benchmark metrics for fitness prediction; (2) ProtST-ESM-2 obtains the highest localization prediction accuracy on average, and it performs best on 7 out of 8 benchmark metrics for function annotation. We therefore recommend these two PLMs as new state-of-the-arts.\\n\\n4.3. Zero-shot Protein Classification\\n\\n4.3.1. EXPERIMENTAL SETUPS\\n\\nZero-shot Protein Classification based on Aligned Representation Space: A ProtST-induced PLM naturally allows zero-shot protein classification, thanks to its aligned representation space of protein sequences and text descriptions. In specific, given the sequence $S$ of a query protein and the label descriptions $\\\\{T_i\\\\}_{K_i=1}^K$ of all $K$ classes, we employ the PLM to extract protein representation $z_S$ and use the jointly learned BLM to extract label representations $\\\\{z_{T_i}\\\\}_{K_i=1}^K$. We then derive classification logits $\\\\{y_i\\\\}_{K_i=1}^K$ by comparing the dot product similarity between protein and label representations:\\n\\n$$y_i = \\\\frac{z_S \\\\cdot z_{T_i}}{\\\\tau} (i = 1, \\\\cdots, K),$$\\n\\nwhich follows the formula of InfoNCE loss in Eq. (1). Softmax is performed upon these logits to derive classification probabilities.\\n\\nBenchmark Tasks: In this part of experiments, we adopt two protein classification tasks as benchmarks: (1) the subcellular localization prediction task which is same as the one introduced in Sec. 4.2.1; (2) the reaction classification task proposed by Hermosilla et al. (2020) which reformulates the EC number prediction task introduced in Sec. 4.2.1 as a classification task with 384 reaction classes. We follow the official dataset splits for both tasks.\"}"}
{"id": "xu23t", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts\\n\\n(a) Subcellular localization prediction\\n(b) Reaction classification\\n\\nFigure 2: Zero-shot ProtST-ESM-1b outperforms few-shot classifiers. The horizontal line with a red star denotes the zero-shot performance of ProtST-ESM-1b. All few-shot results are averaged over seeds 0, 1, 2, 3 and 4, and gray intervals denote standard deviations.\\n\\nPrompt Engineering: To extract discriminative label representations, we have tried three types of prompt templates to describe protein function/location labels. (1) Name only: a label is described only by the name of a function or location (e.g., \\\"Cytoplasm\\\"); (2) Natural language: the name is embedded into a natural language template (e.g., \\\"A protein locating at Cytoplasm\\\"); (3) Pre-training template: the name is embedded into the template used during ProtST pre-training (e.g., \\\"SUBCELLULAR LOCATION: Cytoplasm\\\"). The pre-training template is empirically verified to be more effective than other two templates, and thus it is used across all experiments of this section. The comparisons among these templates are provided in Appendix B.3.\\n\\n4.3.2. DATA EFFICIENCY OF ZERO-SHOT CLASSIFIER\\n\\nBaselines: We study the data efficiency of zero-shot ProtST-ESM-1b by comparing it with n-shot classifiers (n \u2265 1) which employ n training samples per class for prediction. We adopt four baselines: (1) the ProtST-ESM-1b with supervised fine-tuning, (2) the ESM-1b with supervised fine-tuning, (3) the nonparametric ProtST-ESM-1b classifier, and (4) the nonparametric ESM-1b classifier. We follow Khan-delwal et al. (2019) to design the nonparametric classifiers which predict based on the relations between test sample and training samples, and they well fit the few-shot prediction setting. We elucidate such classifiers in Appendix B.3.\\n\\nResults: For subcellular localization prediction (Fig. 2(a)), the zero-shot ProtST-ESM-1b matches the performance of 3-shot supervised ProtST-ESM-1b and the performance of 5-shot supervised ESM-1b, and the zero-shot classifier outperforms two 7-shot nonparametric classifiers. For reaction classification (Fig. 2(b)), the zero-shot ProtST-ESM-1b surpasses the 1-shot performance of supervised and nonparametric ProtST-ESM-1b, and it aligns the 2-shot performance of supervised and nonparametric ESM-1b. These results demonstrate the data efficiency of ProtST-induced zero-shot classifiers. In particular, they can be helpful in the downstream tasks with limited or even no labeled proteins by making educated predictions using only label descriptions.\\n\\nFigure 3: Zero-shot ProtST-ESM-1b enhances few-shot classifiers' performance via ensemble. The horizontal line with a red star denotes the zero-shot performance of ProtST-ESM-1b. All few-shot results are averaged over seeds 0, 1, 2, 3 and 4, and gray intervals denote standard deviations.\\n\\nTable 4: Zero-shot ProtST-ESM-1b enhances full-shot classifiers' performance via ensemble. Abbr., loc.: localization; Acc: accuracy.\\n\\n| Model          | Subcellular loc. (Acc%) | Reaction (Acc%) |\\n|----------------|-------------------------|-----------------|\\n| ProtST-ESM-1b  | 82.00                   | 86.73           |\\n| [Ensemble] ProtST-ESM-1b | 82.37                   | 87.14           |\\n| ESM-1b         | 79.82                   | 80.54           |\\n| [Ensemble] ESM-1b | 80.20                   | 83.03           |\\n\\n4.3.3. ENHANCING SUPERVISED LEARNING WITH ZERO-SHOT CLASSIFIER\\n\\nEnsemble of Supervised Learning Model and Zero-shot Classifier: We study how zero-shot ProtST-ESM-1b can boost supervised learning models via ensemble. Specifically, we combine the classification logits produced by a supervised learning model and the zero-shot classification logits as below:\\n\\n\\\\[\\n\\\\hat{y}_k = y_{\\\\text{sup}}^k + \\\\alpha y_{\\\\text{zero}}^k\\n\\\\]\\n\\n\\\\(k = 1, \\\\ldots, K\\\\) (K is the number of classes), where \\\\(\\\\alpha\\\\) controls the contribution of the zero-shot classifier. Empirically, we set \\\\(\\\\alpha\\\\) as the ratio of the zero-shot classifier's validation set performance over the validation performance of the supervised learning model.\\n\\nBaselines: We employ ProtST-ESM-1b and ESM-1b with supervised fine-tuning on downstream tasks as baselines. We consider fine-tuning under both the few-shot setting and the full-shot setting (i.e., trained with all training samples). Based on these supervised models, we seek to utilize zero-shot ProtST-ESM-1b to enhance their performance.\\n\\nResults: According to Fig. 3 and Tab. 4, we can observe that zero-shot ProtST-ESM-1b succeeds in enhancing the performance of all few-shot and full-shot baselines on both benchmarks. These results verify that ProtST-induced zero-shot classifiers are useful tools to enhance supervised learning models, which is realized by refining decision boundaries.\\n\\n4.4. Zero-shot Text-to-Protein Retrieval\\n\\nZero-shot Text-to-Protein Retriever: Based on the protein-text aligned representation space, ProtST enables us to retrieve functional proteins from a large-scale database.\"}"}
{"id": "xu23t", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts\\n\\n**FUNCTION:** Binding to a heme, a compound composed of iron complexed in a porphyrin (tetrapyrrole) ring.\\n\\n(1st) 2N91-A: \u2022 Affinity: -7.3 (kcal/mol)  \\n\u2022 GO-MF label: Bind\\n\\n(2nd) 1YHU-A: \u2022 Affinity: -7.9 (kcal/mol)  \\n\u2022 GO-MF label: Bind\\n\\n(3rd) 5B3I-A: \u2022 Affinity: -8.1 (kcal/mol)  \\n\u2022 GO-MF label: Bind\\n\\n(4th) 5VPR-A: \u2022 Affinity: -7.4 (kcal/mol)  \\n\u2022 GO-MF label: Non-bind\\n\\nFigure 4: Zero-shot text-to-protein retrieval of heme binders based on ProtST-ESM-1b.\\n\\nTable 5: Swiss-Prot v.s. TrEMBL on protein property coverage.\\n\\n| Dataset  | Location (mean Acc%) | Function (mean Fmax) |\\n|----------|----------------------|----------------------|\\n| Swiss-Prot | 100% 83.3% 63.5% 92.6% |                     |\\n| TrEMBL   | 100% 24.0% 51.5% 78.0%  |                     |\\n\\nTable 6: Swiss-Prot v.s. TrEMBL as pre-training data source, compared on downstream representation learning tasks.\\n\\n| Abbr. | Loc.: localization prediction | Fit.: fitness prediction |\\n|-------|-------------------------------|--------------------------|\\n| Fix-enc. Full-m. |                  |                          |\\n| Fix-enc. Full-m. |                  |                          |\\n\\nout any function annotation. To be specific, the PLM is first employed to extract the representations \\\\( \\\\{z_S_i\\\\}_{i=1}^N \\\\) of all proteins in the database. During the retrieval process, given the text description (i.e., prompt) \\\\( T \\\\) of a protein function, the BLM is used to extract its representation \\\\( z_T \\\\), and all proteins are then ranked based on their representation similarity \\\\( \\\\{\\\\epsilon_i = z_S_i \\\\cdot z_T\\\\}_{i=1}^N \\\\) with the prompt.\\n\\nExperimental Setups: We use ProtST-ESM-1b to retrieve the Gene Ontology (GO) dataset introduced in Sec. 4.2.1. We build each prompt by adding the \u201cFUNCTION:\u201d prefix before the molecular function definition from GO.\\n\\nResults: In Fig. 4, we visualize the top-4 retrieved candidates of heme binders. We present the text prompt, the docking result of each candidate binding with heme (AutoDock Vina (Trott & Olson, 2010) is used for docking), the binding affinity predicted by AutoDock Vina (the lower the better), and the GO molecular function labels of heme binding. We can observe that the top-3 candidates are annotated as heme binders by GO, and the 4th candidate owns decent binding affinity though annotated as non-binding (only 0.54% proteins are annotated as heme binders in the GO dataset).\\n\\nThese results verify the effectiveness of ProtST-ESM-1b on retrieving heme binders. We provide more case studies in Appendix D. Other visualization results are in Appendix F.\\n\\nTable 7: Ablation study of pre-training losses on ProtST-ESM-1b.\\n\\n| Config | Loc. (mean Acc%) | Fit. (mean \\\\( \\\\rho \\\\)) | Func. (mean \\\\( F_{max} \\\\)) |\\n|--------|------------------|------------------------|-----------------------------|\\n| Fix-enc. Full-m. |                  |                          |                             |\\n\\n4.5. Ablation Study\\n\\nEffect of Pre-training Data Source: In this project, besides Swiss-Prot, we also tried to use TrEMBL (Bairoch & Apweiler, 2000) as the data source to construct ProtDescribe. Compared to Swiss-Prot with high-quality human annotations for around 500K proteins, TrEMBL contains a larger number of over 200M annotated proteins, while the TrEMBL annotations are given by computational tools and are thus less accurate and have lower protein property coverage (as shown in Tab. 5).\\n\\nThe results in Tab. 6 show that the ProtST-ESM-1b pre-trained on the smaller while higher-quality Swiss-Prot-based dataset performs better. Therefore, for the multimodal pre-training of protein sequences and biomedical texts, data quality could be more important than data quantity.\\n\\nEffect of Pre-training Losses: Tab. 7 reports the averaged performance of ProtST-ESM-1b by using full or partial pre-training losses (per-task results are in Appendix E.1). By removing any of three pre-training losses, performance decay occurs on all three types of tasks. Such phenomenon verifies the necessity of each ProtST pre-training loss, where \\\\( L_{GC} \\\\) and \\\\( L_{MMP} \\\\) inject different granularities of protein property information into a PLM, and \\\\( L_{MPM} \\\\) preserves the PLM\u2019s original representation power.\\n\\nEffect of PLM: According to the results in Tabs. 2 and 3, we can observe that the strength of a ProtST-induced PLM correlates with the strength of its initial PLM. To be specific, the better performance of ESM-1b and ESM-2 over ProtBert is inherited by their ProtST-induced variants.\"}"}
