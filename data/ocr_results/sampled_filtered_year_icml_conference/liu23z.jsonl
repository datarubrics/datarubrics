{"id": "liu23z", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Graph Switching Dynamical Systems\\n\\nYongtuo Liu\\nSara Magliacane\\nMiltiadis Kofinas\\nEfstratios Gavves\\n\\nAbstract\\nDynamical systems with complex behaviours, e.g. immune system cells interacting with a pathogen, are commonly modelled by splitting the behaviour into different regimes, or modes, each with simpler dynamics, and then learning the switching behaviour from one mode to another. Switching Dynamical Systems (SDS) are a powerful tool that automatically discovers these modes and mode-switching behaviour from time series data. While effective, these methods focus on independent objects, where the modes of one object are independent of the modes of the other objects. In this paper, we focus on the more general interacting object setting for switching dynamical systems, where the per-object dynamics also depends on an unknown and dynamically changing subset of other objects and their modes. To this end, we propose a novel graph-based approach for switching dynamical systems, GRAph Switching dynamical Systems (GRASS), in which we use a dynamic graph to characterize interactions between objects and learn both intra-object and inter-object mode-switching behaviour. We introduce two new datasets for this setting, a synthesized ODE-driven particles dataset and a real-world Salsa Couple Dancing dataset. Experiments show that GRASS can consistently outperform previous state-of-the-art methods.\\n\\n1. Introduction\\nComplex time series are pervasive both in daily life and scientific research, usually consisting of sophisticated behaviours and interactions between entities or objects (Pavlovic et al., 2000; Shi et al., 2021). Consider for example emotion contagion in a crowd and how it might affect the crowd dynamics (Xu et al., 2021), or the differentiation of T cells, a crucial type of immune cell, into different subtypes with different roles after interacting with certain pathogens. A common way of modelling complex behaviour, e.g. represented by a discontinuous function, is by considering it as a sequence of simpler modes, e.g. represented by a set of smooth functions. For example, the behaviour of a ball bouncing on the floor can be represented by two simple modes of falling and bouncing back. In many cases, the challenge is to identify the mode at each time point based on observations. The state-of-the-art approaches for this task are Switching Linear Dynamical Systems (SLDS) (Ackerson & Fu, 1970; Ghahramani & Hinton, 2000; Oh et al., 2005) and their non-linear extensions, e.g. Switching Nonlinear Dynamical Systems (SNLDS) (Dong et al., 2020) and REDSDS (Ansari et al., 2021). While effective, these approaches either model the mode of a single object, including modelling different objects as a \u201csuper object\u201d (Dong et al., 2020; Glaser et al., 2020), or assume independent objects, i.e. they model the mode of each object as independent from the others, e.g. dancing bees in (Ansari et al., 2021).\\n\\nIn this paper, we focus on the more general setting in which there are multiple interacting objects, and in which the mode of an object can be influenced by the mode of the other objects. This is a more realistic setting for modelling many real-world systems, from crowds of people, to groups of immune cells and swarms. For this setting, we propose GRAph Switching dynamical Systems (GRASS) (described at high level in Fig. 1), a framework that learns a dynamic graph to model interactions between objects and their modes across time, and can be combined with previously developed independent-objects switching dynamical systems methods. To evaluate this new setting, we also propose two new datasets for benchmarking interacting object systems: a synthetic ODE-driven Particle dataset, and a Salsa Couple Dancing dataset, inspired by real-world benchmarks (Dong et al., 2020). Experiments show that GRASS outperforms the baselines and identifies mode-switching behaviors with higher accuracy and fewer switching errors.\\n\\n2. Multi Object Switching Dynamical Systems\\nWe start from a collection of time series of observations $y := y_1: y_N$ for $T$ time steps and $N$ objects. The $N$ objects move and their motions can be categorized to one out of $K$ modes. Each object has its own time series of observations $y_i$ for $i = 1, ..., N$, where $y_i$ is a vector of observations for object $i$. The model aims to learn the switching behaviour between different modes for each object, as well as the interactions between objects. This is achieved by constructing a dynamic graph that captures the interactions between objects and their modes across time. The graph can be represented as a set of nodes, where each node represents an object and its mode, and edges represent interactions between objects and their modes. The dynamic graph evolves over time, reflecting the changing interactions between objects.\\n\\nThe model learns the parameters of the graph, including the transition probabilities between modes for each object, the influence of one object on another, and the time-varying interactions between objects. This allows the model to capture both the internal dynamics of each object and the external influences from other objects. The graph-based approach provides a flexible framework for modelling complex interactions and allows for the incorporation of prior knowledge about the system.\\n\\nExperiments on the proposed datasets demonstrate the effectiveness of GRASS in identifying mode-switching behaviours. The model consistently outperforms previous state-of-the-art methods, showing improved accuracy and lower switching errors. The success of GRASS highlights the potential of graph-based approaches for modelling complex systems with multiple interacting objects. The framework can be extended to various applications, such as social dynamics, ecological systems, and multi-agent interactions.\\n\\nAcknowledgments\\nThis work was supported by the Dutch Research Council (NWO). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NWO.\\n\\nReferences\\nAcker son, T., & Fu, J. (1970). Linear State Space Models for Nonlinear Systems. IEEE Transactions on Automatic Control, 15(5), 748-754.\\nDong, W., Zhang, Q., & Li, J. (2020). Switching Nonlinear Dynamical Systems: A Survey. IEEE Transactions on Automatic Control, 65(5), 1980-1996.\\nGhahramani, Z., & Hinton, G. E. (2000). Reinforcement and supervised learning: An overview. In: Learning to learn (pp. 237-264). MIT Press.\\nOh, H., Kwon, Y., & Kim, H. (2005). A reinforcement learning approach to multiple vehicle path planning. In: Proceedings of the IEEE International Conference on Robotics and Automation (pp. 1025-1030). IEEE.\\nPavlovic, D., Feiner, S. K., & MacIntyre, B. A. (2000). Multimodal interaction in a virtual performance space. In: Proceedings of the 27th annual conference on Computer graphics and interactive techniques (pp. 183-190). ACM.\\nShi, Y., Yang, W., Shi, Y., & Zhao, Z. (2021). Deep learning for complex behaviors: A survey. IEEE Transactions on Neural Networks and Learning Systems, 32(12), 4452-4467.\\nXu, Y., Li, W., & Wang, X. (2021). Emotion contagion in crowds: A survey. International Journal of Social Robotics, 13(3), 423-441.\\nAnsari, A., et al. (2021). Multi-object switching dynamical systems: a survey and review. arXiv preprint arXiv:2101.09352.\"}"}
{"id": "liu23z", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Illustration of Graph Switching Dynamical Systems (GRASS). As opposed to independent objects Switching Dynamical Systems, where objects are processed independently, Graph Switching Dynamical Systems discover modes and mode-switching behaviours that can depend on object interactions. Interactions are modelled by a latent dynamic graph, which is inferred jointly with the other variables by maximizing the evidence lower bound. Activated interaction edges and mode switching are highlighted with red arrows, while inactive edges (no interactions) are visualized with grayed-out dashed lines in the interaction graph at each timestep.\\n\\nFor instance, an object might be moving in a spiral trajectory (mode 1) or it might bouncing on a wall (mode 2). The N objects interact with each other, and their motions change according to these interactions. For instance, after a collision, an object might switch from a spiral to a sinusoidal motion. The dynamics of these objects are governed by three types of variables: mode variables, count variables and state variables. Mode variables are categorical variables $z = z_1: N \\\\times T = \\\\{z_1 t, \\\\ldots, z_N t\\\\}_t=1$, where $z_n t \\\\in \\\\{0, \\\\ldots, K - 1\\\\}$ denotes the mode for each time step $t \\\\in (1, \\\\ldots, T)$ and for each object $n \\\\in (1, \\\\ldots, N)$. For instance, $z_2 t = 2$ and $z_5 t = 5$ mean that, at time step 10, the second object moves according to the third dynamic mode (for instance a spiral trajectory), while the fifth object moves according to the fourth dynamic mode (for instance a sinusoidal trajectory).\\n\\nCount variables are categorical variables $c = c_1: N \\\\times T = \\\\{c_1 t, \\\\ldots, c_N t\\\\}_t=1$, where each $c_n t \\\\in (1, \\\\ldots, M)$ explicitly models the durations between switching modes for each object $n$ and each timestep $t$ and $M$ is the maximum number of steps before a switch. These variables help us avoid frequent mode switching, caused by the fact that durations typically follow a geometric distribution, biasing unfavourably towards shorter durations (Ansari et al., 2021).\\n\\nState variables are continuous variables $x = x_1: N \\\\times T = \\\\{x_1 t, \\\\ldots, x_N t\\\\}_t=1$, where each $x_n t \\\\in \\\\mathbb{R}^d$ encodes the dynamics content per object and time step. For instance, at time step $t$, $x_n t$ could encode the position and velocity of the trajectory of the $n$-th object.\\n\\nWe start by describing the per-object dynamics. In this case, we model for each object $n$ an observation probability $p(y_n t | x_n t)$, a state transition probability $p(x_n t | x_n t-1, z_n t)$ and a count transition probability $p(c_n t | z_n t-1, c_n t-1)$. The observation probability $p(y_n t | x_n t)$ models how the continuous state variables for this object $x_n t$ map into the observations $y_n t$. The state transition probability $p(x_n t | x_n t-1, z_n t)$ models how the continuous state variables at time $t$ are influenced by their previous values at time $t-1$ conditioned on mode variable for this object $z_n t$. The count transition probability $p(c_n t | z_n t-1, c_n t-1)$ models how the count variables at time $t$ change depending on the previous count variables $c_n t-1$ and mode $z_n t-1$.\"}"}
{"id": "liu23z", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The amortized transition dynamics benefits our modelling, to simplify the modelling of switching dynamics, we as-\\n\\neffectively when we learn a dynamic graph. Local dynamic factors allow dropping interactions between ob-\\ntjects when none exist, since in multi-object systems, objects often affect one another at sparse points in time and space.\\n\\nBecause they allow us to model a larger number of objects and their switching dynamics (whether there exist or not)\\nby simply extending the respective products and sums. In the next section, we show how we can learn and use these\\ndynamic factors to ensure interaction sparsity more\\n\\nIn Eq. (1) except for the mode transition probability in the In-\\n\\n2.2. Learning an amortized transition dynamics\\n\\ntation for the marginal transition probabilities (Raftery, 1985; \\n\\nthat all objects may potentially influence all other objects. On what interactions take place, this probability considers\\n\\nand count variables\\n\\nc other objects\\n\\ndepend only on pair-\\n\\npairs and objects independently. We refer to (Dong et al., 2020; Ansari et al., 2021) for details.\\n\\nfor objects\\n\\nfactorized per object and thus similar independent-object\\n\\ninteracting Modes term, all other terms\\n\\nIn the absence of any knowledge\\n\\ninitial states have a similar setup, but in this case the state\\n\\nmode for this object at the previous time step\\n\\nmode transition probability\\n\\nwhere\\n\\nx\\n\\nw\\n\\nm\\n\\ngregorates the continuous states of objects\\n\\nand their switching dynamics (whether there exist or not)\\n\\nf means a function for edge type\\n\\nthat aggregates\\n\\ntime\\n\\nlocal dynamic fac-\\n\\ntor to get the weighted influence from all\\n\\nweights over\\n\\nm\\n\\nthat we use in the interacting modes term of Eq. (2).\\n\\nFirst, we want the latent edges to signal whether there is an\\n\\ninteraction between two objects. Thus, for objects\\n\\nno interaction),\\n\\nl\\n\\nnodes\\n\\nt, that is\\n\\nx\\n\\nwe set the unnormalized local dynamic factor\\n\\nhow do they take place, and between what objects, we set the\\n\\nv\\n\\nnodes\\n\\nt, . . . , N\\n\\nl\\n\\nrepresent different interaction\\n\\nSince our system consists of multiple objects, which may or\\n\\nN\\n\\nmay not interact at random points in time, we can model the\\n\\nsystem with a dynamic graph\\n\\nsince we cannot know when interactions take place,\\n\\nE\\n\\ndenote whether there is an interaction\\n\\nv\\n\\nnodes\\n\\nor 1.\\n\\nSince our system consists of multiple objects, which may or\\n\\nN\\n\\nE\\n\\n\u2200\\n\\nGraph Switching Dynamical Systems\\n\\nFunction for the \u2018no interaction\u2019 case.\\n\\nWe enable two types of messages to be passed via the edges.\\n\\n3. Graph Switching Dynamical Systems\\n\\nWe set the prior probability to be higher for \u201cno interaction\u201d. Further, we\\n\\nstand for \u201cno interaction\u201d. Further, we\\n\\n3\\n\\nfunction for edge type\\n\\nl\\n\\nlatent edge variables to be one-hot vectors of\\n\\nL\\n\\nlatent variables and observations related to each object\\n\\nedges\\n\\nt\\n\\nThe nodes\\n\\nEmbedding the switching dynamical system into a graph\\n\\ntopology, we want messages to be passed between graph\\n\\nV\\n\\nV\\n\\nsince in multi-object systems, objects often affect one another at sparse points in time and space.\\n\\nLocal dynamic factors allow dropping interactions between ob-\\ntjects when none exist, since in multi-object systems, objects\"}"}
{"id": "liu23z", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Graph Switching Dynamical Systems\\n\\n(a) Generative Model\\n(b) Inference\\n\\nFigure 2. (a) Generative model of GRASS. (b) Left: Amortized approximate inference for the continuous states $x^1_{Nt}$ and discrete edge variable $e^1_{N2t}$ by inference networks. Temporal dependence is modeled by an intermediate latent embedding $h^1_{Nt}$ which is given by directional RNNs. Right: Exact inference of discrete mode and count variables $z^1_{Nt}$ and $c^1_{Nt}$ based on the approximate pseudo-observations and pseudo-interactions $x^1_{Nt}$ and $e^1_{N2t}$. Orange circles denote observations or approximate pseudo-observations.\\n\\nTaking into account the latent edge variables that are part of our probabilistic model, the joint probability becomes:\\n\\n$$p(y, x, z, c, e) = \\\\prod_{n=1}^{N} p(y^1_n | x^1_n) p(x^1_n | z^1_n) p(z^1_n | \\\\{z^1_{n-1}, x^1_{n-1}, c^1_n, e^{m\\\\rightarrow n}_t\\\\})$$\\n\\nInitial States\\n\\n$$T_Y t=2 \\\\prod_{n=1}^{N} \\\\sum_{X_m=1}^{w_m \\\\rightarrow n_t} p(z^1_{n_t} | z^1_{n_t-1}, x^1_{m,n}_t, c^1_{n_t}, e^{m\\\\rightarrow n}_t)$$\\n\\nPairwise Interacting Modes\\n\\n$$\\\\prod_{n=1}^{N} \\\\sum_{Y_t=2}^{T_Y} p(y^1_n | x^1_n) p(x^1_n | x^1_{n-1}, z^1_n) p(c^1_{n_t} | z^1_{n_t-1}, c^1_{n_t-1})$$\\n\\nPer-object dynamics,\\n\\n4. Neural Network Implementation\\n\\nWe use neural networks to model the terms in the joint likelihoods of our Switching Dynamical Systems, specifically of Eq. (1) for the Multiple-Object Switching Dynamical System (MOSDS) of Section 2, and of Eq. (5) for Graph Switching Dynamical Systems (GRASS) of Section 3.\\n\\nSince the mode variables $z^1_{Nt}$ take one out of $K$ possible values for dynamic modes, we model them as categorical variables, parameterized by transition probabilities $T^t_{Nt}$. Specifically, for pairs of objects in our system, we have:\\n\\n$$p(z^1_{n_t} | z^1_{n_t-1}, x^1_{m,n}_t, c^1_{n_t}, e^{m\\\\rightarrow n}_t) = \\\\begin{cases} \\\\delta z^1_{n_t} = z^1_{n_t-1} & \\\\text{if } c^1_{n_t} > 1 \\\\\\\\ \\\\text{Cat}(z^1_{n_t}; T^t_{Nt}) & \\\\text{if } c^1_{n_t} = 1 \\\\end{cases} \\\\quad (6)$$\\n\\nwhere we resample the dynamic modes of objects or preserve them via a Kronecker $\\\\delta$ function depending on whether our count variable is reset or not.\\n\\nFor MOSDS, we model the parameters $T^t_{Nt}$ of the categorical distributions in Eq. (6) with a neural network $f^z(x^1_{Nt})$ that takes as input the continuous states of all objects. In this case, the neural network returns a $NK \\\\times NK$ transition matrix per time step $t$, where rows correspond to past modes $z^1_{Nt-1}$ and columns correspond to current modes $z^1_{Nt}$.\\n\\nThe shape of the matrix $NK \\\\times NK$ is because the neural network must predict in one forward pass the likelihoods for all possible combinations of (object $m$, object $n$, mode $i$, mode $j$). Clearly, such a neural network is prohibitively expensive as it scales exponentially with the number of objects $N$ and modes $K$, and also wasteful to optimize, as it assumes object pairs do not share any dynamics at all. So for GRASS, we instead model the parameters $T^t_{Nt}$ in Eq. (6) with an amortized neural network $T^t_{Nt} = f^l(x^1_{m,n}_t)$ that takes as input only pairs of continuous states (the weights of the neural network are shared for any pair of objects).\\n\\nFor both MOSDS and GRASS, the neural network $f^z$ is a simple MLP. To satisfy the positivity $T^t_{Nt,i,j} > 0 \\\\forall i, j = 1, ..., K$ and $\\\\ell_1$ constraints $\\\\sum_{j} T^t_{Nt,i,j} = 1 \\\\forall i = 1, ..., K$ for $T^t_{Nt}$, we apply a tempered softmax on $f^z$, $S_{\\\\tau} \\\\circ f^z(\\\\cdot)$.\\n\\nThe latent edges also take one out of $L+1$ possible values for different types of interactions. Thus, we model them by an $L+1$-way categorical distribution as well.\\n\\n4.1. Inference\\n\\nDue to the exponential complexity of the state space, exact inference of latent variables in Switching Dynamical Systems is intractable. Similar to Ansari et al. (2021), we resort to approximate variational inference with neural networks for the continuous latent variable. Furthermore, we modify the original forward-and-backward algorithm by Yu (2010) to perform exact inference for the discrete mode and count variables, as we will detail below. The variational approximation of the true posterior is:\\n\\n$$p(x, e, z, c | y) \\\\approx$$\"}"}
{"id": "liu23z", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 9.\\nAnalyses of robustness to datasets without interactions on ODE-driven Particle dataset. */* denotes NMI / $F_1$.\\n\\n| Method      | w/ interaction | w/o interaction |\\n|-------------|----------------|-----------------|\\n| rSLDS       | 0.257\u00b10.023    | 0.443\u00b10.041     |\\n| SNLDS       | 0.368\u00b10.027    | 0.664\u00b10.053     |\\n| REDSDS      | 0.418\u00b10.016    | 0.701\u00b10.027     |\\n| MOSDS (this paper) | 0.469\u00b10.020 | 0.757\u00b10.032     |\\n| GRASS (this paper) | 0.528\u00b10.014 | 0.790\u00b10.021     |\\n\\n### Table 10.\\nAnalyses on robustness to different maximal numbers of predefined modes. */* denotes NMI / $F_1$.\\n\\n| Number of Modes | rSLDS | SNLDS | REDSDS | MOSDS (this paper) | GRASS (this paper) |\\n|-----------------|-------|-------|--------|--------------------|--------------------|\\n| 3               | 0.257\u00b10.023 | 0.368\u00b10.027 | 0.418\u00b10.016 | 0.469\u00b10.020 | 0.528\u00b10.014 |\\n| 5               | 0.253\u00b10.025 | 0.365\u00b10.032 | 0.415\u00b10.023 | 0.466\u00b10.028 | 0.532\u00b10.020 |\\n| 10              | 0.248\u00b10.032 | 0.362\u00b10.036 | 0.413\u00b10.026 | 0.462\u00b10.033 | 0.527\u00b10.022 |\"}"}
{"id": "liu23z", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Assuming conditional independence between edges state (Collins, 2013) by introducing the additional continuous\\nbackward algorithm used with hidden Markov mod-\\ninference of the discrete mode and count variables\\nGiven the inferred states\\nrelaxation smoothness (Maddison et al., 2016).\\nreparametrization trick and\\n\u03c4 sampled from a\\nQ q\\nQ posterior for edge types becomes\\ngiven all the inferred states, the approximate pos-\\nh are the sampled continuous states\\nApproximate inference of discrete edge\\nApproximate inference of continuous state and discrete edge vari-\\narchitecture and implementation details are in App. A.2.\\nThe network\\nable. To summarize our setup, we provide a flowchart of the\\ndescribe the exact and approximate inference for each vari-\\nmate inference of the continuous state and discrete edge vari-\\n4.2. Learning\\nfive ODE-driven particle datasets for benchmarking, inspired by the single-object\\nmethods and compare against baselines, we introduce two\\nobject\\\" comprising of all objects simultaneously. The two-\\ndo not interact with one another. For instance, the dancing\\ndancer in Salsa Dancing from CMU MoCap (Dong et al.,\\nacting objects are considered simultaneously and depend-\\ndatasets for benchmarking, inspired by the single-object\\nand count\\nT N\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x\\nQ \u03d5\\nQ q\\nQ x"}
{"id": "liu23z", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To simulate trajectories, we draw balls with radius $r$, randomly initialized and driven by different ODEs on a squared 2d canvas of size 64*64. Specifically, we consider three particle balls driven by three different ODE modes unless stated otherwise (e.g., in the experiments increasing the number of particles or the number of modes). Numerical values of ODEs are mapped to the canvas. For mode-switching interactions among objects, we switch the driven ODE modes of two objects when they collide in the canvas. Each sample has 100 time steps, and with 10 frames per second. We follow the sample splitting proportion of synthesized datasets in REDSDS (Ansari et al., 2021) (i.e. test data is around 5% of training data) and create 4,928 samples for training, 191 samples for validation, and 204 samples for testing. Analyses on new splitting strategy (i.e. test data is around 10% of training data) and larger dataset are in App. B.1. A sample visualization of this dataset is shown in Fig. 3.\\n\\nSalsa Couple dancing dataset. Dong et al. (2020) experiment with salsa dancing sequences, which, however, feature a single dancer only from CMU MoCap. We collect 17 real-world Salsa dancing videos from the Internet, containing 8,672 frames. Among them, 3 videos are for testing and the remaining videos are for training. We extract 3D skeletons of dancers by a pretrained model (Moon et al., 2019) and conduct temporal Gaussian smoothing afterward. As Dong et al. (2020), we annotate four modes, i.e., \u201cmoving forward\u201d, \u201cmoving backward\u201d, \u201cclockwise turning\u201d, and \u201ccounter-clockwise turning\u201d. Each sample has 100 time steps with 5 frames per second. We have 1,321 samples for training and 156 samples for testing. The coordinates of 3D skeletal joints serve as input for each dancer, and the modes of each dancer at each time step are the output. In Fig. 4 we show the 3D skeletons extracted from the videos.\\n\\nEvaluation metrics. Following Dong et al. (2020); Ansari et al. (2021), we evaluate using frame-wise segmentation accuracy, i.e. accuracy and $F_1$ after matching the labels using the Hungarian algorithm (Kuhn, 1955), Normalized Mutual Information (NMI) and Adjusted Rand Index (ARI) to measure similarity between two labellings. We conduct each experiment for five random seeds and report the average performance and standard deviation of the results.\\n\\nBaselines. We compare MOSDS and GRASS with three state-of-the-art methods: rSLDS (Linderman et al., 2016), SNLDS (Dong et al., 2020), and REDSDS (Ansari et al., 2021). For our implementation, we use REDSDS (Ansari et al., 2021) as the base for MOSDS and GRASS. We include in the comparisons GRASS-GT as an \u201cupper bound\u201d oracle method, for which we use the ground-truth graph edges rather to learn mode transition behaviours.\\n\\n5.1. ODE-driven Particle We summarize results for the ODE-drive particles in Table 1. We see that just by considering interactions between multiple objects with MOSDS, we achieve significant and consistent performance increases across all metrics. When further using graphs to model the switching dynamics in our interacting system of objects, GRASS improves by more than 9-10% over the previous state-of-the-art, REDSDS, across all metrics. We also observe that GRASS performs similarly to GRASS-GT using ground truth edges, showcasing the accuracy of inferring the latent object-to-object interactions. In Fig. 5, we show also the qualitative results of GRASS compared to REDSDS, which is the top performing baseline. GRASS discovers mode-switching behaviours between objects effectively and with fewer switching errors.\\n\\n5.2. Salsa Couple Dancing We summarize the results for Salsa Couple Dancing dataset in Table 2. We observe similar findings in this real-world video dataset, as with the ODE-driven particles. GRASS achieves significantly higher accuracy across all metrics, including REDSDS and our simpler method MOSDS.\"}"}
{"id": "liu23z", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Graph Switching Dynamical Systems\\n\\nGround Truth\\nREDSDS\\nGRASS (Ours)\\n\\nTable 1. Comparisons on ODE-driven Particle Dataset.\\n\\nTable 2. Comparisons on the Salsa Couple Dancing dataset.\\n\\n5.3. Ablation experiments\\nDue to limited space, we report the average performance in each table. Results with standard deviations are in App. B.2.\\n\\nSensitivity to the number of interactions.\\nWe evaluate how sensitive is GRASS in the presence of an increasing number of interactions. First, we extend the normal ODE-driven Particle dataset to more particles, i.e., 3 particles, 5 particles, and 10 particles. The number of interactions naturally increases with the number of particles in a space-constrained canvas. For different numbers of particles, we count the average number of interactions per object per time series and they are 2.3 interactions for 3 particles, 6.1 for 5, and 12.5 for 10. We present the results in Table 3, where we conclude that GRASS is not adversely affected by an increasing number of objects and interactions.\\n\\nSensitivity to the number of objects.\\nWe further test increasing the number of objects, while fixing the number of interactions. We achieve this by controlling the sizes of objects, as with smaller balls we have fewer collisions (and thus interactions). We roughly fix the number of interactions per object per time series to be 2.3 and change the number of objects to 3, 5, and 10 as in the previous trial. We present results in Table 4. GRASS is robust to different numbers of objects, no matter whether we fix the number of interactions.\"}"}
{"id": "liu23z", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Graph Switching Dynamical Systems\\n\\nTable 5. Analyses of robustness to datasets without interactions on ODE-driven Particle dataset. */* denotes NMI / $F_1$.\\n\\n| Method         | w/ interaction | w/o interaction |\\n|----------------|---------------|-----------------|\\n| rSLDS          | 0.257 / 0.443 | 0.471 / 0.686   |\\n| SNLDS          | 0.368 / 0.664 | 0.534 / 0.772   |\\n| REDSDS         |               | 0.579 / 0.838   |\\n| MOSDS (this paper) | 0.469 / 0.757 | 0.563 / 0.817   |\\n| GRASS (this paper) |            | 0.528 / 0.790   |\\n\\nSensitivity to absence of interactions. GRASS is built for systems of multiple objects that interact with one another. We test whether the method generalizes even in the case when the objects are independent and do not interact, as with single-object Switching Dynamical Systems. We create a dataset with three particles driven by three different ODEs, and set them so that they do not interact with each other. We present results in Table 5. In the presence of interactions, GRASS is considerably more accurate than REDSDS, while in the absence of interactions, it scores comparably. In this case MOSDS observes a higher drop in accuracy. The reason is that with its dynamic graph, GRASS can still predict correctly that there exist no interaction edges between objects, while MOSDS always assumes all objects interact.\\n\\nSensitivity to number of dynamic modes. Like previous methods (Linderman et al., 2016; Dong et al., 2020; Ansari et al., 2021), GRASS requires a predefined maximum number of modes. We test its robustness to different maximum numbers of modes, that is 3, 5, and 10, while the true number of modes is 3. We present results in Table 6. We observe that GRASS is impervious to this misspecification, which suggests that we can set a large number of possible modes and GRASS will still use only those needed.\\n\\n6. Related Work\\n\\nSwitching Linear Dynamical Systems (SLDS) (Ackerson & Fu, 1970; Ghahramani & Hinton, 2000; Oh et al., 2005) introduce both discrete states to represent motion modes and continuous states to characterize motion dynamics of each mode, but assume linear state transitions. Switching Non-linear Dynamical Systems, implemented by neural networks, extend these methods to the nonlinear case, providing a better expressiveness of complex system dynamics. Among them, SNLDS (Dong et al., 2020) and REDSDS (Ansari et al., 2021) are two representative methods that can consistently outperform their linear counterparts. While effective, previous methods and datasets are usually limited to single-object scenarios where only one object exist. When multiple objects exist, objects are processed independently or considered as one single super-object with a single mode. For example, in (Glaser et al., 2020), multiple neural populations exist in the brain, while the only mode behaviours of the whole brain only are modelling and discovered. By contrast, in this paper we focus on the general setting where our systems comprise multiple objects interacting and changing their behaviour accordingly.\\n\\nGraph Neural Networks are the de facto choice for learning relational representations over graphs. Recently, there are some methods focusing on neural relational inference (Kipf et al., 2018; Graber & Schwing, 2020; Kofinas et al., 2021) over temporal sequences, whose dynamics are encoded by continuous latent states. These methods focus on systems with multiple objects, whose dynamics, however, do not change of time and, therefore, are not a good fit for discovering mode-switching behaviours over time. In this work, we start from the framework of Switching Dynamical Systems, and integrate them within a graph neural network formalism. In particular, we extend neural relational graphs and relational inference (Kipf et al., 2018; Graber & Schwing, 2020) to incorporate latent interaction variables, one per pair of objects, and model the potential dynamic interactions between objects. The proposed Graph Switching Dynamical Systems can thus handle systems with increased complexity with a significantly better accuracy. This is true even in the presence of sparse interactions in both space and time, which cause sudden and complex dynamic mode switches.\\n\\n7. Conclusion and Future Work\\n\\nWe investigate the setting of interacting objects switching dynamical systems, when objects interact with each other and influence each other's modes. We propose a graph-based approach for these systems, GRAph Switching dynamical Systems (GRASS), in which we use a dynamic graph to model interactions and mode-switching behaviors between objects. We also introduce two datasets, i.e. a synthesized ODE-driven Particle dataset and a real-world Salsa Couple dancing dataset. Experiments show that GRASS improves considerably the state-of-the-art. Future work includes exploring learning switching dynamical systems with multiple objects directly from videos.\\n\\nAcknowledgements\\nThis work is financially supported by NWO TIMING VI.Vidi.193.129. We also thank SURF for the support in using the National Supercomputer Snellius.\\n\\n8\"}"}
{"id": "liu23z", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With Markovian property, we rewrite\\n\\\\[\\nE \\\\text{ where } p\\n\\\\]\\nwhere we model the interactions among objects via\\n\\\\[\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\"}
{"id": "liu23z", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where detailed in the next section.\\n\\n\\\\[ \\\\nabla \\\\left( \\\\log k_n \\\\right) \\\\nabla \\\\]\\n\\n\\\\( n \\\\) is the initial joint discrete mode probability. \\n\\n\\\\( q_t \\\\) can be written as:\\n\\n\\\\[ \\\\sum_{t=1}^{T} \\\\nabla \\\\left( \\\\log k_n \\\\right) \\\\nabla \\\\pi_k \\\\]\\n\\n\\\\( \\\\nabla k_n \\\\)\\n\\n\\\\( y \\\\)\\n\\n\\\\( t \\\\)\\n\\n\\\\( s \\\\)\\n\\n\\\\( n \\\\)\\n\\n\\\\( p \\\\)\\n\\n\\\\( \\\\gamma \\\\)\\n\\n\\\\( n \\\\)\\n\\n\\\\( E \\\\)\\n\\n\\\\( k \\\\)\\n\\n\\\\( j \\\\)\\n\\n\\\\( q \\\\)\\n\\n\\\\( Q \\\\)\\n\\n\\\\( z \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( y \\\\)\\n\\n\\\\( \\\\epsilon \\\\)\\n\\n\\\\( k \\\\)\\n\\n\\\\( t \\\\)\\n\\n\\\\( m,n \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( y \\\\)\\n\\n\\\\( \\\\pi \\\\)\\n\\n\\\\( k \\\\)\\n\\n\\\\( j \\\\)\\n\\n\\\\( q \\\\)\\n\\n\\\\( Q \\\\)\\n\\n\\\\( z \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( y \\\\)\\n\\n\\\\( \\\\gamma \\\\)\\n\\n\\\\( n \\\\)\\n\\n\\\\( E \\\\)\\n\\n\\\\( k \\\\)\\n\\n\\\\( j \\\\)\\n\\n\\\\( q \\\\)\\n\\n\\\\( Q \\\\)\\n\\n\\\\( z \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( y \\\\)\\n\\n\\\\( \\\\epsilon \\\\)\\n\\n\\\\( k \\\\)\\n\\n\\\\( t \\\\)\\n\\n\\\\( m,n \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( y \\\\)\\n\\n\\\\( \\\\pi \\\\)\\n\\n\\\\( k \\\\)\\n\\n\\\\( j \\\\)\\n\\n\\\\( q \\\\)\\n\\n\\\\( Q \\\\)\\n\\n\\\\( z \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( y \\\\)\\n\\n\\\\( \\\\gamma \\\\) can be calculated similarly to the forward and backward algorithm in HMMs (Collins, 2013), which is\\n\\n\\\\[ \\\\sum_{t=1}^{T} \\\\nabla \\\\left( \\\\log k_n \\\\right) \\\\nabla \\\\pi_k \\\\]\\n\\n\\\\( \\\\nabla k_n \\\\)\\n\\n\\\\( y \\\\)\\n\\n\\\\( t \\\\)\\n\\n\\\\( s \\\\)\\n\\n\\\\( n \\\\)\\n\\n\\\\( p \\\\)\\n\\n\\\\( \\\\gamma \\\\)\\n\\n\\\\( n \\\\)\\n\\n\\\\( E \\\\)\\n\\n\\\\( k \\\\)\\n\\n\\\\( j \\\\)\\n\\n\\\\( q \\\\)\\n\\n\\\\( Q \\\\)\\n\\n\\\\( z \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( y \\\\)\\n\\n\\\\( \\\\epsilon \\\\)\\n\\n\\\\( k \\\\)\\n\\n\\\\( t \\\\)\\n\\n\\\\( m,n \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( y \\\\)\\n\\n\\\\( \\\\pi \\\\)\\n\\n\\\\( k \\\\)\\n\\n\\\\( j \\\\)\\n\\n\\\\( q \\\\)\\n\\n\\\\( Q \\\\)\\n\\n\\\\( z \\\\)\\n\\n\\\\( x \\\\)\\n\\n\\\\( y \\\\)\\n\\n\\\\( \\\\gamma \\\\)\"}"}
{"id": "liu23z", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we aim at calculating the posterior probability of discrete mode, count, and edge variables. The backward part can be expanded as:\\n\\n\\\\[ \\\\alpha \\\\cdot \\\\beta \\\\cdot (z_{t+1}, z_t) = 1 \\\\cdot \\\\alpha \\\\cdot \\\\beta \\\\cdot \\\\left( \\\\sum_{z_t} \\\\cdot p \\\\right) \\\\]\\n\\nwhere \\\\( \\\\alpha \\\\) and \\\\( \\\\beta \\\\) are transition and emission probabilities, respectively. The forward part can be expanded as:\\n\\n\\\\[ \\\\alpha \\\\cdot \\\\beta \\\\cdot \\\\left( \\\\sum_{z_t} \\\\cdot p \\\\right) = \\\\sum_{z_t} \\\\cdot \\\\left( \\\\sum_{z_t} \\\\cdot \\\\alpha \\\\cdot \\\\beta \\\\cdot \\\\left( \\\\sum_{z_t} \\\\cdot p \\\\right) \\\\right) \\\\]\\n\\n\\\\[ \\\\delta = 1 \\\\]\\n\\n\\\\[ n = Y \\\\]\\n\\n\\\\[ \\\\delta = \\\\]\"}"}
{"id": "liu23z", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7. Analyses on different numbers of objects on ODE-driven Particle dataset, while increasing the average number of interactions per object per time series, i.e, 2.3 interactions for 3 particles, 6.1 for 5, and 12.5 for 10. */* denotes NMI / F$^1$.\\n\\n| Number of Particles | rSLDS | SNLDS | REDSDS | MOSDS (this paper) | GRASS (this paper) |\\n|--------------------|-------|-------|--------|-------------------|-------------------|\\n| 3                  | 0.257 \u00b1 0.023 / 0.443 \u00b1 0.041 | 0.368 \u00b1 0.027 / 0.664 \u00b1 0.053 | 0.418 \u00b1 0.016 / 0.701 \u00b1 0.027 | 0.469 \u00b1 0.020 / 0.757 \u00b1 0.032 | 0.528 \u00b1 0.014 / 0.790 \u00b1 0.021 |\\n| 5                  | 0.262 \u00b1 0.034 / 0.444 \u00b1 0.037 | 0.365 \u00b1 0.030 / 0.666 \u00b1 0.047 | 0.423 \u00b1 0.023 / 0.706 \u00b1 0.031 | 0.471 \u00b1 0.025 / 0.763 \u00b1 0.036 | 0.530 \u00b1 0.012 / 0.792 \u00b1 0.019 |\\n| 10                 | 0.253 \u00b1 0.028 / 0.437 \u00b1 0.039 | 0.362 \u00b1 0.028 / 0.659 \u00b1 0.047 | 0.413 \u00b1 0.022 / 0.694 \u00b1 0.029 | 0.464 \u00b1 0.021 / 0.754 \u00b1 0.035 | 0.524 \u00b1 0.017 / 0.786 \u00b1 0.024 |\\n\\nTable 8. Analyses on different numbers of objects on ODE-driven Particle dataset, while fixing the average number of interactions per object per time series, i.e, 2.3 interactions. */* denotes NMI / F$^1$.\\n\\n| Number of Particles | rSLDS | SNLDS | REDSDS | MOSDS (this paper) | GRASS (this paper) |\\n|--------------------|-------|-------|--------|-------------------|-------------------|\\n| 3                  | 0.257 \u00b1 0.023 / 0.443 \u00b1 0.041 | 0.368 \u00b1 0.027 / 0.664 \u00b1 0.053 | 0.418 \u00b1 0.016 / 0.701 \u00b1 0.027 | 0.469 \u00b1 0.020 / 0.757 \u00b1 0.032 | 0.528 \u00b1 0.014 / 0.790 \u00b1 0.021 |\\n| 5                  | 0.262 \u00b1 0.034 / 0.444 \u00b1 0.037 | 0.365 \u00b1 0.030 / 0.666 \u00b1 0.047 | 0.423 \u00b1 0.023 / 0.706 \u00b1 0.031 | 0.471 \u00b1 0.025 / 0.763 \u00b1 0.036 | 0.530 \u00b1 0.012 / 0.792 \u00b1 0.019 |\\n| 10                 | 0.253 \u00b1 0.028 / 0.437 \u00b1 0.039 | 0.362 \u00b1 0.028 / 0.659 \u00b1 0.047 | 0.413 \u00b1 0.022 / 0.694 \u00b1 0.029 | 0.464 \u00b1 0.021 / 0.754 \u00b1 0.035 | 0.524 \u00b1 0.017 / 0.786 \u00b1 0.024 |\\n\\nA.4. Further Model Interactions between Continuous Variables\\nIn the main paper, we model interactions between objects by dependence on discrete mode variables only. This means that based on the derived discrete mode transition, the continuous state transition and observation emission are per-object dynamics only without interactions. However, in some real-world scenarios, the interactions between objects also happen to continuous variables. For example, in each motion type, object A still influences the detailed motion of object B. We show some preliminary results in this section and leave more comprehensive experiments as future work.\\n\\nB. More Experiments\\nB.1. New splitting and larger ODE-driven particle datasets\\nIn our original ODE-driven particle dataset we used around 5k samples for training, around 200 samples for validation and testing. We tested the scalability of our method in terms of scaling to one larger (approximately 20x larger) dataset. The original dataset takes 37,000 epoches to achieve convergence and the final performance of our GRASS model is: 0.528, 0.519, 0.794, and 0.790 for NMI, ARI, Accuracy, and F1, respectively. The 20x larger dataset takes 39,000 epochs and the final performance of our GRASS model is 0.525, 0.531, 0.814, and 0.802. We find the training time before convergence and the performance of our model are almost the same, which shows the scalability of our method to larger datasets.\\n\\nThe splitting strategy of the synthesized dataset follows the recent SOTA method, REDSDS (Ansari et al., 2021). REDSDS has 10,000 and 500 samples for training and testing of the 3-mode system (test data is around 5% of training data). We follow the proportion and have 4,928 samples for training and 204 samples for testing (around 5%). For the ODE-driven particle dataset, we also conduct a new splitting (4200/420/420 for training/validation/testing). The results of our GRASS model on the new splitting dataset are 0.522, 0.518, 0.809, and 0.805 for NMI, ARI, Accuracy, and F1, respectively, which shows almost the same performance as the original splitting in the main paper.\\n\\nB.2. Ablation studies with standard derivations\\nAblations studies with standard derivations are in Tables 7, 8, 9, and 10. We can see that the conclusions remain the same as in the main paper for ablation studies of different numbers of objects, different numbers of interactions, with or without interactions, and different numbers of predefined modes. Note that in Table 7 and Table 8, we can see that with different number of objects or interactions, GRASS has consistently better performance with the lowest variances.\"}"}
{"id": "liu23z", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nAckerson, G. and Fu, K. On state estimation in switching environments. IEEE transactions on automatic control, 15(1):10\u201317, 1970.\\n\\nAnsari, A. F., Benidis, K., Kurle, R., Turkmen, A. C., Soh, H., Smola, A. J., Wang, B., and Januschowski, T. Deep explicit duration switching models for time series. Advances in Neural Information Processing Systems, 34:29949\u201329961, 2021.\\n\\nCollins, M. The forward-backward algorithm. Columbia Univ, 2013.\\n\\nDong, Z., Seybold, B., Murphy, K., and Bui, H. Collapsed amortized variational inference for switching nonlinear dynamical systems. In International Conference on Machine Learning, pp. 2638\u20132647, 2020.\\n\\nGhahramani, Z. and Hinton, G. E. Variational learning for switching state-space models. Neural computation, 12(4):831\u2013864, 2000.\\n\\nGlaser, J., Whiteway, M., Cunningham, J. P., Paninski, L., and Linderman, S. Recurrent switching dynamical systems models for multiple interacting neural populations. Advances in neural information processing systems, 33:14867\u201314878, 2020.\\n\\nGraber, C. and Schwing, A. G. Dynamic neural relational inference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nKingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\\n\\nKipf, T., Fetaya, E., Wang, K.-C., Welling, M., and Zemel, R. Neural relational inference for interacting systems. In International Conference on Machine Learning, pp. 2688\u20132697, 2018.\\n\\nKofinas, M., Nagaraja, N., and Gavves, E. Roto-translated local coordinate frames for interacting dynamical systems. Advances in Neural Information Processing Systems, 34:6417\u20136429, 2021.\\n\\nKuhn, H. W. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.\\n\\nLinderman, S. W., Miller, A. C., Adams, R. P., Blei, D. M., Paninski, L., and Johnson, M. J. Recurrent switching linear dynamical systems. arXiv preprint arXiv:1610.08466, 2016.\\n\\nMaddison, C. J., Mnih, A., and Teh, Y. W. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.\\n\\nMoon, G., Chang, J. Y., and Lee, K. M. Camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 10133\u201310142, 2019.\\n\\nOh, S. M., Ranganathan, A., Rehg, J. M., and Dellaert, F. A variational inference method for switching linear dynamic systems. Technical report, Georgia Institute of Technology, 2005.\\n\\nPavlovic, V., Rehg, J. M., and MacCormick, J. Learning switching linear models of human motion. Advances in neural information processing systems, 13, 2000.\\n\\nRaftery, A. E. A model for high-order markov chains. Journal of the Royal Statistical Society: Series B (Methodological), 47(3):528\u2013539, 1985.\\n\\nSaul, L. K. and Jordan, M. I. Mixed memory markov models: Decomposing complex stochastic processes as mixtures of simpler ones. Machine learning, 37(1):75\u201387, 1999.\\n\\nShi, C., Schwartz, S., Levy, S., Achvat, S., Abboud, M., Ghanayim, A., Schiller, J., and Mishne, G. Learning disentangled behavior embeddings. Advances in Neural Information Processing Systems, 34:22562\u201322573, 2021.\\n\\nXu, M., Xie, X., Lv, P., Niu, J., Wang, H., Li, C., Zhu, R., Deng, Z., and Zhou, B. Crowd behavior simulation with emotional contagion in unexpected multihazard situations. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 51(3):1567\u20131581, 2021. doi: 10.1109/TSMC.2019.2899047.\\n\\nYu, S.-Z. Hidden semi-markov models. Artificial intelligence, 174(2):215\u2013243, 2010.\"}"}
{"id": "liu23z", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Calculate Backward algorithm variable:\\n\\n\\\\[ \\\\text{LogLikelihood} \\\\]\\n\\nCalculate discrete mode transition\\n\\nCalculate Forward algorithm variable:\\n\\n\\\\[ \\\\text{LogLikelihood} \\\\]\\n\\n// ELBO optimization\\n\\nAppendix\\n\\nAlgorithm 1\\n\\nInference algorithm for GRASS.\\n\\nThe inference algorithm of GRASS is in Alg. 1. As inputs, we have a time series \\\\( t \\\\) ODE-driven particle dataset or \\\\( \\\\text{MLP} \\\\) inference networks for discrete edge \\\\( c \\\\) denotes a single-layer RNN with a single-layer of hidden units. In the following, we show the network details as well as embedding dimensions.\\n\\nA.2. Implementation Details\\n\\nLearned parameters\\n\\nOutput:\\n\\nInput:\\n\\nTime series\\n\\nobjects are detailed in Fig. 7.\\n\\nof the inference stage is in Fig. 6. Besides, the overall generative model and inference stages of GRASS which factorize the objective to optimize the parameters of networks. Details of the derivatives of ELBO are in Section A.3. An illustration are introduced by calculating the loglikelihood between \\\\( \\\\tilde{y} \\\\) and backward variables \\\\( \\\\beta \\\\) and \\\\( \\\\theta \\\\).\\n\\nA.1. Inference Algorithm of GRASS\\n\\nA. More details of GRASS model\\n\\n\\\\( \\\\{ \\\\sim \\\\} \\\\)\\n\\nBesides, the range of discrete count variable is initialized as \\\\( \\\\theta \\\\) and \\\\( \\\\theta \\\\) are introduced by calculating the loglikelihood between \\\\( \\\\tilde{y} \\\\) and backward variables \\\\( \\\\beta \\\\) and \\\\( \\\\theta \\\\).\\n\\nInference networks for discrete edge \\\\( c \\\\) denote a single-layer RNN with a single-layer of hidden units and ReLU non-linearity. Inference networks for continuous state\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{MLP} \\\\)\\n\\n\\\\( \\\\text{ML"}
{"id": "liu23z", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6. Illustration of inference algorithm of Graph Switching Dynamical Systems. After the approximate inference of continuous state \\\\( \\\\tilde{x}_{1:T} \\\\) and discrete edge \\\\( \\\\tilde{e}_{1:T} \\\\), we further calculate continuous state transition probability \\\\( p_{\\\\theta} xtr(x_t | \\\\tilde{x}_{t-1}, z_t) \\\\), discrete mode transition probability \\\\( p_{\\\\theta} ztr(z_t | z_{t-1}, \\\\tilde{x}_{t-1}, c_t, \\\\tilde{e}_t) \\\\), and discrete count transition probability \\\\( p_{\\\\theta} c(c_t | c_{t-1}, z_{t-1}) \\\\), which are utilized by the forward and backward algorithm to conduct exact inference of discrete mode \\\\( z_{1:T} \\\\) and count \\\\( c_{1:T} \\\\) to finally derive ELBO optimization objective.\\n\\nFigure 7. (a) Generative model of GRASS. (b) Left: Amortized approximate inference for the continuous states (e.g. \\\\( x_1t \\\\) and \\\\( x_2t \\\\)) and discrete edge variable (e.g. \\\\( e_1 \\\\rightarrow 2t \\\\) and \\\\( e_2 \\\\rightarrow 1t \\\\)) by inference networks. Temporal dependence is modeled by an intermediate latent embedding (e.g. \\\\( h_1t \\\\) and \\\\( h_2t \\\\)) which is given by directional RNNs. Right: Exact inference of discrete mode (e.g. \\\\( z_1t \\\\) and \\\\( z_2t \\\\)) and count variables and (e.g. \\\\( c_1t \\\\) and \\\\( c_2t \\\\)) based on the approximate pseudo-observations (e.g. \\\\( x_1t \\\\) and \\\\( x_2t \\\\)) and pseudo-interactions (e.g. \\\\( e_1 \\\\rightarrow 2t \\\\) and \\\\( e_2 \\\\rightarrow 1t \\\\)). Orange circles denote observations or approximate pseudo-observations. Here, we assume there exist two objects in the scenario.\"}"}
{"id": "liu23z", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Graph Switching Dynamical Systems (i.e. \\\\( p(x^n_t | x^{n-1}_t, z^n_t) \\\\)); Discrete transition network: MLP \\\\[2 \\\\times 2\\\\] for ODE-driven particle dataset or MLP \\\\[4 \\\\times 4\\\\] for ODE-driven particle dataset (i.e. \\\\( p(z^n_t | z^{m-1}_t, x^{m,n-1}_t, c^n_t, e^{m \\\\rightarrow n}_t) \\\\)); Emission network: MLP \\\\[2 \\\\times 2\\\\] for ODE-driven particle dataset or MLP \\\\[4 \\\\times 5\\\\] for ODE-driven particle dataset (i.e. \\\\( p(y^n_t | x^n_t) \\\\)).\\n\\nWe train both datasets with a fixed batch size of 20 for 60,000 training steps. We use the Adam optimizer with \\\\( 10^{-5} \\\\) weight-decay and clip gradients norm to 10. The learning rate is warmed up linearly from \\\\( 5 \\\\times 10^{-5} \\\\) to \\\\( 2 \\\\times 10^{-4} \\\\) for the first 2,000 steps, and then decays following a cosine manner with a rate of 0.99. Each experiment is running on one Nvidia GeForce RTX 3090 GPU.\\n\\nA.3. Detailed Optimization Objective of GRASS\\n\\nA.3.1. Derivation of ELBO\\n\\nThe evidence lower bound objective (ELBO) of Graph Switching Dynamical System (GRASS) is defined as follows. For brevity, \\\\( x, y, z, c, \\\\) and \\\\( e \\\\) represents \\\\( x_1^N, y_1^N, z_1^N, c_1^N, \\\\) and \\\\( e_1^N \\\\) respectively. \\\\( N \\\\) is the number of objects. \\\\( T \\\\) is the number of timestamps.\\n\\n\\\\[\\n\\\\text{ELBO} = \\\\log p_{\\\\theta}(y) - D_{KL}[q_{\\\\phi}(x, z, c, e | y) \\\\parallel p_{\\\\theta}(x, z, c, e | y)]\\n\\\\]\\n\\n\\\\[\\n= \\\\int q_{\\\\phi}(x, z, c, e | y) \\\\log p_{\\\\theta}(y) d(x, z, c, e) - \\\\int q_{\\\\phi}(x, z, c, e | y) \\\\log q_{\\\\phi}(x, z, c, e | y) p_{\\\\theta}(x, z, c, e | y) d(x, z, c, e)\\n\\\\]\\n\\n\\\\[\\n= \\\\mathbb{E}_{q_{\\\\phi}(x, z, c, e | y)}[\\\\log p_{\\\\theta}(x, z, c, e, y) - \\\\log q_{\\\\phi}(x | y)]\\n\\\\]\\n\\n\\\\[\\n= \\\\mathbb{E}_{q_{\\\\phi}(x | y)}[\\\\log p_{\\\\theta}(x, y) - \\\\log q_{\\\\phi}(x | y)]\\n\\\\]\\n\\n\\\\[\\n= \\\\mathbb{E}_{q_{\\\\phi}(x | y)}[\\\\log p_{\\\\theta}(x, y) - H(q_{\\\\phi}(x | y))]\\n\\\\]\\n\\nwhere the first term is a model likelihood, and the second term is conditional entropy for variational posterior of continuous latent state \\\\( x \\\\). With the proper assumption of conditional independence of continuous latent states among objects, the conditional entropy is expanded through space and time as:\\n\\n\\\\[\\nH(q_{\\\\phi}(x | y)) = \\\\sum_{n=1}^{N} H(q_{\\\\phi}(x^n_1^T | y^n_1^T))\\n\\\\]\\n\\n\\\\[\\n= \\\\sum_{n=1}^{N} \\\\sum_{t=1}^{T} H(q_{\\\\phi}(x^n_1^T | y^n_1^T))\\n\\\\]\\n\\n\\\\[\\n= \\\\sum_{n=1}^{N} \\\\sum_{t=2}^{T} \\\\left[ H(q_{\\\\phi}(x^n_1^{t-1} | y^n_1^{t-1})) + \\\\sum_{n=1}^{N} \\\\sum_{t=2}^{T} H(q_{\\\\phi}(x^n_t | \\\\tilde{x}^{n-1}_t, y^n_{t-1})) \\\\right]\\n\\\\]\\n\\nwhere \\\\( \\\\tilde{x}^{n-1}_t \\\\) contains \\\\( \\\\tilde{x}^{n}_1, \\\\tilde{x}^{n}_2, \\\\ldots, \\\\tilde{x}^{n}_t-1 \\\\), in which \\\\( \\\\tilde{x}^{n}_t \\\\sim q_{\\\\phi}(x^{n}_t | \\\\tilde{x}^{n-1}_t, y^{n}_{t-1}) \\\\) is sampled from the variational posterior distribution. In practice, we utilize causal RNN to model the temporal dependence.\"}"}
