{"id": "3Cp042s1Nc", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | Size   | (Circular) Vocabulary | (Circular) Alignment Normalized | PPL    |\\n|---------------|--------|------------------------|-------------------------------|--------|\\n| MPT (Team et al., 2023a;b) | 7B     | 29.5                   | 0.5                           | 29.6   |\\n|               | 30B    | 45.1                   | 19.3                          | 44.8   |\\n| MPT-Chat      | 30B    | 50.1                   | 28.9                          | 50.0   |\\n| Falcon        | 7B     | 25.0                   | 0.0                           | 24.8   |\\n|               | 40B    | 53.6                   | 28.2                          | 53.3   |\\n| LLaMA-1       | 7B     | 32.2                   | 1.7                           | 30.5   |\\n|               | 13B    | 43.5                   | 14.4                          | 43.1   |\\n|               | 30B    | 54.7                   | 31.6                          | 54.5   |\\n|               | 65B    | 59.4                   | 37.1                          | 59.2   |\\n| LLaMA-2       | 7B     | 41.8                   | 12.3                          | 39.4   |\\n|               | 13B    | 52.1                   | 24.6                          | 51.8   |\\n|               | 70B    | 65.4                   | 45.1                          | 65.4   |\\n| LLaMA-2-Chat  | 7B     | 45.3                   | 17.9                          | 45.0   |\\n|               | 13B    | 53.1                   | 28.1                          | 53.2   |\\n|               | 70B    | 61.1                   | 38.9                          | 61.1   |\\n| WizardLM      | 13B    | 53.4                   | 30.2                          | 53.4   |\\n|               | 70B    | 62.7                   | 42.1                          | 62.6   |\\n| Xwin-LM       | 7B     | 45.5                   | 16.1                          | 45.5   |\\n|               | 13B    | 53.9                   | 27.2                          | 53.9   |\\n| Alpaca        | 7B     | 40.8                   | 13.7                          | 40.7   |\\n|               | 13B    | 39.5                   | 14.8                          | 37.8   |\\n| Vicuna        | 7B     | 48.8                   | 25.0                          | 48.8   |\\n|               | 13B    | 54.5                   | 33.5                          | 54.5   |\\n|               | 33B    | 57.1                   | 36.3                          | 57.0   |\"}"}
{"id": "3Cp042s1Nc", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model | Size | Choice | Choice | Choice | Choice | Choice | Choice |\\n|-------|------|--------|--------|--------|--------|--------|--------|\\n| MPT   | 7B   | 28.1   | 0.8    | 28.1   | 0.6    | 57.1   | 76.3   |\\n|       | 30B  | 32.8   | 3.3    | 32.8   | 2.9    | 60.4   | 79.9   |\\n| MPT-Chat | 30B | 46.9   | 14.9   | 46.9   | 15.2   | 61.5   | 80.1   |\\n| Falcon| 7B   | 25.1   | 0.0    | 24.1   | 0.0    | 57.7   | 76.3   |\\n|       | 40B  | 48.7   | 16.7   | 48.3   | 16.5   | 64.0   | 82.8   |\\n| LLaMA-1 | 7B | 29.1   | 1.1    | 28.9   | 1.1    | 56.9   | 76.2   |\\n|       | 13B  | 33.1   | 3.2    | 33.1   | 3.2    | 59.9   | 79.1   |\\n|       | 30B  | 44.3   | 14.7   | 44.4   | 14.8   | 63.3   | 82.6   |\\n|       | 65B  | 46.2   | 12.7   | 45.8   | 12.7   | 64.5   | 84.1   |\\n| LLaMA-2 | 7B | 32.1   | 1.2    | 30.0   | 1.2    | 57.2   | 76.0   |\\n|       | 13B  | 49.7   | 17.2   | 48.6   | 16.6   | 60.1   | 79.4   |\\n|       | 70B  | 62.4   | 31.1   | 62.1   | 31.0   | 64.8   | 83.8   |\\n| LLaMA-2-Chat | 7B | 50.5   | 16.9   | 42.1   | 13.7   | 57.7   | 75.4   |\\n|       | 13B  | 63.5   | 34.9   | 63.5   | 34.8   | 60.7   | 79.7   |\\n|       | 70B  | 75.2   | 55.3   | 75.2   | 55.3   | 63.8   | 82.2   |\\n| WizardLM | 13B | 66.5   | 44.4   | 66.5   | 44.4   | 61.5   | 79.8   |\\n|       | 70B  | 70.9   | 46.7   | 70.9   | 46.7   | 64.8   | 82.1   |\\n| Xwin-LM | 7B | 39.6   | 6.0    | 39.6   | 6.0    | 58.8   | 76.8   |\\n|       | 13B  | 56.1   | 22.7   | 56.1   | 22.6   | 62.1   | 80.9   |\\n| Alpaca | 7B   | 37.3   | 7.7    | 37.2   | 7.7    | 59.1   | 75.6   |\\n|       | 13B  | 38.9   | 11.3   | 38.7   | 11.3   | 60.6   | 78.3   |\\n| Vicuna | 7B   | 55.8   | 30.2   | 55.9   | 30.2   | 56.4   | 73.8   |\\n|       | 13B  | 61.2   | 32.4   | 61.2   | 32.5   | 59.6   | 77.5   |\\n|       | 33B  | 65.1   | 35.1   | 65.1   | 35.2   | 61.9   | 80.4   |\"}"}
{"id": "3Cp042s1Nc", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"# Table 9: MCQA evaluation on 0-shot ARC-Challenge (Clark et al., 2018).\\n\\n| Model Size Choices | Choices | (Circular) Vocab | (Circular) Vocab | Normalized Alignment | PPL | Normalized Alignment | PPL |\\n|--------------------|---------|-----------------|-----------------|----------------------|-----|----------------------|-----|\\n| MPT                |         | 7B              | 31.8            | 0.3                  | 30.4| 0.3                  | 40.0|\\n|                    |         | 30B             | 51.0            | 22.4                 | 50.3| 21.0                 | 46.9|\\n| MPT-Chat           |         | 30B             | 66.0            | 43.2                 | 65.5| 42.7                 | 50.0|\\n| Falcon             |         | 7B              | 27.5            | 0.0                  | 27.0| 0.0                  | 40.3|\\n|                    |         | 40B             | 64.2            | 41.7                 | 64.2| 41.6                 | 50.3|\\n| LLaMA-1            |         | 7B              | 38.9            | 1.9                  | 38.1| 2.0                  | 41.9|\\n|                    |         | 13B             | 52.4            | 21.3                 | 51.5| 20.6                 | 46.4|\\n|                    |         | 30B             | 69.4            | 48.4                 | 69.2| 48.3                 | 52.9|\\n|                    |         | 65B             | 74.3            | 52.6                 | 74.3| 52.6                 | 52.8|\\n| LLaMA-2            |         | 7B              | 45.9            | 16.4                 | 44.8| 15.3                 | 43.4|\\n|                    |         | 13B             | 63.4            | 30.5                 | 62.5| 29.3                 | 48.4|\\n|                    |         | 70B             | 81.7            | 67.1                 | 81.7| 67.1                 | 54.4|\\n| LLaMA-2-Chat       |         | 7B              | 55.8            | 23.0                 | 55.7| 23.0                 | 44.1|\\n|                    |         | 13B             | 64.3            | 38.1                 | 64.2| 37.9                 | 46.2|\\n|                    |         | 70B             | 78.2            | 64.2                 | 78.1| 64.2                 | 53.0|\\n| WizardLM           |         | 13B             | 65.5            | 43.4                 | 65.5| 43.3                 | 47.0|\\n|                    |         | 70B             | 80.6            | 65.7                 | 80.6| 65.6                 | 53.2|\\n| Xwin-LM            |         | 7B              | 54.9            | 23.5                 | 54.9| 23.5                 | 45.9|\\n|                    |         | 13B             | 67.4            | 36.8                 | 67.2| 35.9                 | 52.0|\\n| Alpaca             |         | 7B              | 48.5            | 17.2                 | 48.2| 17.0                 | 44.9|\\n|                    |         | 13B             | 47.4            | 22.8                 | 47.2| 22.8                 | 46.1|\\n| Vicuna             |         | 7B              | 60.4            | 33.9                 | 60.6| 33.9                 | 43.3|\\n|                    |         | 13B             | 70.0            | 48.0                 | 70.0| 48.1                 | 47.8|\\n|                    |         | 33B             | 71.2            | 49.9                 | 71.1| 49.6                 | 50.2|\"}"}
{"id": "3Cp042s1Nc", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 10: MCQA evaluation on 0-shot ARC-Easy (Clark et al., 2018)\\n\\n| Model        | Model Size | Circular Vocab | Circular Alignment | Normalized PPL | PPL |\\n|--------------|------------|----------------|--------------------|----------------|-----|\\n| MPT          | 7B         | 36.4           | 0.8                | 35.9           | 0.6 |\\n|              | 30B        | 70.0           | 42.2               | 69.9           | 40.7 |\\n| MPT-Chat     | 30B        | 79.4           | 62.6               | 79.5           | 62.7 |\\n| Falcon       | 7B         | 29.2           | 0.0                | 29.0           | 0.0 |\\n|              | 40B        | 80.0           | 61.7               | 80.0           | 61.6 |\\n| LLaMA-1      | 7B         | 44.7           | 5.2                | 43.8           | 4.6 |\\n|              | 13B        | 67.6           | 36.7               | 66.7           | 35.4 |\\n|              | 30B        | 84.6           | 69.6               | 84.7           | 69.6 |\\n|              | 65B        | 87.7           | 73.0               | 87.7           | 72.9 |\\n| LLaMA-2      | 7B         | 59.0           | 26.8               | 58.2           | 25.8 |\\n|              | 13B        | 77.4           | 53.7               | 76.6           | 52.1 |\\n|              | 70B        | 92.8           | 84.5               | 92.8           | 84.4 |\\n| LLaMA-2-Chat | 7B            | 70.9           | 44.9               | 70.8           | 44.9 |\\n|              | 13B        | 79.9           | 62.0               | 79.9           | 62.0 |\\n|              | 70B        | 92.8           | 84.5               | 92.8           | 84.4 |\\n| WizardLM     | 13B        | 81.9           | 67.2               | 81.9           | 67.2 |\\n|              | 70B        | 92.1           | 83.9               | 92.1           | 83.9 |\\n| Xwin-LM      | 7B         | 69.7           | 41.2               | 69.7           | 41.1 |\\n|              | 13B        | 80.5           | 60.7               | 80.3           | 60.1 |\\n| Alpaca       | 7B         | 67.7           | 36.2               | 67.2           | 35.6 |\\n|              | 13B        | 67.2           | 41.3               | 67.0           | 40.9 |\\n| Vicuna       | 7B         | 75.9           | 55.0               | 75.9           | 55.0 |\\n|              | 13B        | 83.8           | 71.4               | 83.8           | 71.4 |\\n|              | 33B        | 85.9           | 70.7               | 85.6           | 69.6 |\"}"}
{"id": "3Cp042s1Nc", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | Size   | (Circular) Vocabulary | (Circular) Alignment Normalized | PPL    |\\n|---------------|--------|------------------------|---------------------------------|--------|\\n| MPT (Team et al., 2023a; b) | 7B     | 62.1                   | 0.6                             | 57.2   |\\n|               | 30B    | 69.1                   | 47.9                            | 30.6   |\\n| MPT-Chat (Team et al., 2023b) | 30B    | 65.6                   | 40.1                            | 61.6   |\\n| Falcon (Almazrouei et al., 2023) | 7B     | 57.3                   | 28.1                            | 45.4   |\\n|               | 40B    | 67.7                   | 22.6                            | 67.5   |\\n| LLaMA-1 (Touvron et al., 2023a) | 7B     | 59.9                   | 34.1                            | 50.9   |\\n|               | 13B    | 64.3                   | 22.5                            | 53.1   |\\n|               | 30B    | 78.5                   | 70.8                            | 77.2   |\\n|               | 65B    | 79.9                   | 67.4                            | 71.4   |\\n| LLaMA-2 (Touvron et al., 2023b) | 7B     | 63.2                   | 46.7                            | 24.7   |\\n|               | 13B    | 66.9                   | 10.1                            | 49.8   |\\n|               | 70B    | 85.0                   | 76.5                            | 85.0   |\\n| LLaMA-2-Chat (Touvron et al., 2023b) | 7B   | 63.6                   | 21.2                            | 63.6   |\\n|               | 13B    | 71.9                   | 39.3                            | 71.9   |\\n|               | 70B    | 75.6                   | 57.8                            | 75.6   |\\n| WizardLM (Xu et al., 2023) | 13B    | 76.5                   | 51.7                            | 76.5   |\\n|               | 70B    | 89.0                   | 84.5                            | 89.0   |\\n| Xwin-LM (Team) | 7B     | 64.2                   | 40.2                            | 63.8   |\\n|               | 13B    | 68.5                   | 15.1                            | 68.3   |\\n| Alpaca (Taori et al., 2023a) | 7B     | 64.2                   | 38.8                            | 64.1   |\\n|               | 13B    | 68.0                   | 61.2                            | 68.0   |\\n| Vicuna (Chiang et al., 2023) | 7B     | 67.7                   | 54.0                            | 67.7   |\\n|               | 13B    | 82.1                   | 62.6                            | 82.1   |\\n|               | 33B    | 80.2                   | 74.0                            | 27.9   |\"}"}
{"id": "3Cp042s1Nc", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model | Size | Choice 1 | Choice 2 | Choice 3 | Choice 4 | Choice 5 | Choice 6 | Choice 7 |\\n|-------|------|---------|---------|---------|---------|---------|---------|---------|\\n| MPT   | 7B   | 37.7    | 2.4     | 38.6    | 2.0     | 45.6    | 48.2    | 45.9    |\\n|       | 30B  | 58.5    | 34.9    | 57.3    | 31.9    | 46.0    | 48.9    | 45.6    |\\n| MPT-Chat | 30B | 63.4    | 42.5    | 63.2    | 41.6    | 48.8    | 49.6    | 48.3    |\\n| Falcon| 7B   | 36.5    | 1.2     | 36.5    | 1.2     | 45.4    | 48.7    | 47.0    |\\n|       | 40B  | 65.6    | 46.8    | 65.6    | 46.7    | 49.2    | 51.0    | 47.8    |\\n| LLaMA-1 | 7B  | 44.6    | 11.5    | 44.5    | 11.6    | 44.8    | 47.0    | 44.3    |\\n|       | 13B  | 53.0    | 14.8    | 53.2    | 14.8    | 45.0    | 48.0    | 44.3    |\\n|       | 30B  | 66.1    | 49.0    | 66.0    | 48.8    | 45.7    | 49.4    | 45.2    |\\n|       | 65B  | 67.2    | 67.1    | 49.7    | 49.8    | 47.5    | 50.1    | 46.3    |\\n| LLaMA-2 | 7B  | 51.4    | 21.2    | 51.0    | 20.6    | 43.5    | 47.3    | 44.2    |\\n|       | 13B  | 58.5    | 36.4    | 58.3    | 36.1    | 44.7    | 48.4    | 44.8    |\\n|       | 70B  | 70.9    | 55.3    | 70.8    | 55.2    | 46.3    | 49.4    | 45.3    |\\n| LLaMA-2-Chat | 7B | 56.9    | 31.8    | 56.9    | 31.9    | 46.1    | 48.6    | 45.1    |\\n|       | 13B  | 62.6    | 41.6    | 62.7    | 41.5    | 49.2    | 50.3    | 46.0    |\\n|       | 70B  | 67.2    | 46.7    | 67.2    | 46.7    | 49.2    | 50.6    | 47.4    |\\n| WizardLM | 13B | 66.7    | 51.5    | 66.7    | 51.6    | 49.3    | 49.2    | 46.7    |\\n|       | 70B  | 73.8    | 59.1    | 73.8    | 59.1    | 49.8    | 49.8    | 47.9    |\\n| Xwin-LM | 7B  | 56.9    | 28.1    | 56.9    | 28.0    | 47.5    | 49.3    | 45.5    |\\n|       | 13B  | 56.8    | 34.3    | 56.7    | 34.1    | 48.5    | 48.4    | 45.4    |\\n| Alpaca | 7B   | 50.7    | 23.8    | 50.7    | 23.8    | 48.3    | 49.2    | 45.3    |\\n|       | 13B  | 56.8    | 34.3    | 56.7    | 34.1    | 48.5    | 48.4    | 45.4    |\\n| Vicuna | 7B   | 63.3    | 42.5    | 63.3    | 42.3    | 45.8    | 47.6    | 44.3    |\\n|       | 13B  | 67.2    | 50.3    | 67.1    | 50.5    | 46.6    | 47.3    | 44.6    |\\n|       | 33B  | 63.2    | 42.2    | 62.5    | 41.1    | 46.6    | 48.2    | 46.0    |\"}"}
{"id": "3Cp042s1Nc", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"# Table 13: MCQA evaluation on 0-shot PIQA (Bisk et al., 2020)\\n\\n| Model          | Size Choices | Circular Vocab | Circular Alignment | Normalized PPL | PPL |\\n|----------------|--------------|----------------|--------------------|----------------|-----|\\n| MPT (Team et al., 2023a;b) | 7B           | 55.3           | 16.0               | 54.2           | 15.6|\\n|                | 30B          | 64.9           | 39.8               | 64.7           | 39.6|\\n| MPT-Chat (Team et al., 2023b) | 30B          | 65.1           | 34.4               | 62.6           | 33.4|\\n| Falcon (Almazrouei et al., 2023) | 7B           | 50.4           | 10.2               | 49.8           | 10.0|\\n|                | 40B          | 69.7           | 44.6               | 69.5           | 44.4|\\n| LLaMA-1 (Touvron et al., 2023a) | 7B           | 53.8           | 10.9               | 53.5           | 10.9|\\n|                | 13B          | 64.1           | 32.3               | 63.2           | 32.0|\\n|                | 30B          | 73.3           | 52.1               | 73.3           | 52.0|\\n|                | 65B          | 66.2           | 33.9               | 66.2           | 33.9|\\n| LLaMA-2 (Touvron et al., 2023b) | 7B           | 59.4           | 21.4               | 58.8           | 20.8|\\n|                | 13B          | 71.7           | 46.0               | 71.4           | 45.9|\\n|                | 70B          | 76.1           | 56.8               | 76.1           | 56.8|\\n| LLaMA-2-Chat (Touvron et al., 2023b) | 7B           | 65.0           | 35.1               | 64.4           | 34.8|\\n|                | 13B          | 70.7           | 45.1               | 70.1           | 44.5|\\n|                | 70B          | 78.0           | 58.9               | 76.9           | 57.4|\\n| WizardLM (Xu et al., 2023) | 13B          | 75.7           | 56.4               | 75.7           | 56.4|\\n|                | 70B          | 82.4           | 70.3               | 82.2           | 70.0|\\n| Xwin-LM (Team) | 7B           | 67.5           | 40.0               | 66.8           | 39.5|\\n|                | 13B          | 69.2           | 41.9               | 68.8           | 41.6|\\n| Alpaca (Taori et al., 2023a) | 7B           | 64.5           | 35.5               | 64.5           | 35.5|\\n|                | 13B          | 63.1           | 32.4               | 63.0           | 32.0|\\n| Vicuna (Chiang et al., 2023) | 7B           | 72.2           | 48.3               | 72.2           | 48.3|\\n|                | 13B          | 75.3           | 56.4               | 75.3           | 56.4|\\n|                | 33B          | 73.8           | 53.3               | 70.0           | 50.7|\"}"}
{"id": "3Cp042s1Nc", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 14: MCQA evaluation on 0-shot AGIEval (English only) (Zhong et al., 2023)\\n\\n| Model Size | Choices | Vocabulary | Vocabulary Alignment | Normalized Alignment | PPL |\\n|------------|---------|------------|----------------------|----------------------|-----|\\n| MPT        | 7B      | 23.5       | 0.0                  | 21.8                 | 25.7| 26.9 |\\n|            | 30B     | 24.0       | 0.7                  | 24.0                 | 27.7| 28.9 |\\n| MPT-Chat   | 30B     | 29.1       | 3.2                  | 29.0                 | 28.2| 29.1 |\\n| Falcon     | 7B      | 22.3       | 0.0                  | 20.9                 | 25.2| 27.2 |\\n|            | 40B     | 28.8       | 3.0                  | 28.2                 | 28.5| 30.8 |\\n| LLama-1    | 7B      | 22.0       | 0.0                  | 21.9                 | 25.8| 27.7 |\\n|            | 13B     | 26.6       | 1.1                  | 26.4                 | 28.0| 29.5 |\\n|            | 30B     | 33.1       | 7.8                  | 32.8                 | 30.0| 31.2 |\\n|            | 65B     | 38.7       | 10.1                 | 37.9                 | 30.4| 32.7 |\\n| LLama-2    | 7B      | 24.4       | 0.6                  | 22.1                 | 26.4| 29.1 |\\n|            | 13B     | 33.9       | 8.0                  | 34.0                 | 28.0| 30.3 |\\n|            | 70B     | 50.0       | 26.2                 | 49.9                 | 31.7| 34.6 |\\n| LLama-2-Chat | 7B   | 28.3       | 2.9                  | 17.3                 | 25.9| 27.1 |\\n|            | 13B     | 35.9       | 9.7                  | 35.5                 | 26.5| 29.2 |\\n|            | 70B     | 45.9       | 17.6                 | 45.8                 | 29.7| 31.1 |\\n| WizardLM   | 13B     | 37.6       | 8.6                  | 37.5                 | 27.1| 29.6 |\\n|            | 70B     | 48.2       | 22.5                 | 47.6                 | 30.6| 32.6 |\\n| Xwin-LM    | 7B      | 30.9       | 3.1                  | 29.5                 | 28.1| 29.9 |\\n|            | 13B     | 35.4       | 10.4                 | 35.4                 | 29.0| 31.0 |\\n| Alpaca     | 7B      | 24.1       | 0.7                  | 23.9                 | 26.8| 28.9 |\\n|            | 13B     | 27.8       | 3.1                  | 27.2                 | 26.5| 28.7 |\\n| Vicuna     | 7B      | 35.5       | 10.9                 | 33.4                 | 26.6| 28.1 |\\n|            | 13B     | 38.6       | 14.7                 | 37.1                 | 27.5| 29.6 |\\n|            | 33B     | 39.9       | 14.4                 | 39.7                 | 28.6| 30.3 |\\n\\n28\"}"}
{"id": "3Cp042s1Nc", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 15: MCQA evaluation on 0-shot OpenBookQA (with fact) (Mihaylov et al., 2018)\\n\\n| Model Size | Circular Vocab | (Circular) Alignment | Normalized PPL | Model Size | Circular Vocab | (Circular) Alignment | Normalized PPL |\\n|------------|----------------|----------------------|----------------|------------|----------------|----------------------|----------------|\\n| MPT        | 7B             | 39.0                 | 1.6            | 38.8       | 1.4            | 41.6                 | 52.4           |\\n|            | 30B            | 70.4                 | 43.0           | 69.6       | 41.8           | 45.2                 | 53.8           |\\n| MPT-Chat   | 30B            | 77.8                 | 58.6           | 77.4       | 58.0           | 47.0                 | 54.4           |\\n| Falcon     | 7B             | 28.2                 | 0.0            | 28.0       | 0.0            | 43.2                 | 53.0           |\\n|            | 40B            | 77.2                 | 61.0           | 77.2       | 61.0           | 48.0                 | 55.6           |\\n| LLaMA-1    | 7B             | 49.2                 | 17.8           | 48.2       | 16.8           | 44.8                 | 52.4           |\\n|            | 13B            | 64.8                 | 32.8           | 64.6       | 32.4           | 44.4                 | 53.4           |\\n|            | 30B            | 81.6                 | 66.6           | 81.6       | 66.6           | 44.0                 | 54.6           |\\n|            | 65B            | 82.8                 | 67.6           | 82.4       | 67.0           | 46.0                 | 57.2           |\\n| LLaMA-2    | 7B             | 62.6                 | 28.8           | 61.8       | 27.8           | 44.6                 | 52.6           |\\n|            | 13B            | 72.0                 | 50.2           | 71.8       | 48.8           | 44.4                 | 54.2           |\\n|            | 70B            | 88.4                 | 79.2           | 86.6       | 77.2           | 47.6                 | 56.6           |\\n| LLaMA-2-Chat | 7B      | 73.8                 | 52.4           | 73.8       | 52.4           | 48.4                 | 54.8           |\\n|            | 13B            | 80.0                 | 63.4           | 80.0       | 63.4           | 48.6                 | 56.4           |\\n|            | 70B            | 86.0                 | 76.4           | 86.0       | 76.4           | 48.6                 | 58.6           |\\n| WizardLM   |                |                      |                |            |                |                      |                |\\n|            | 13B            | 79.4                 | 65.0           | 79.4       | 65.2           | 48.0                 | 56.8           |\\n|            | 70B            | 86.6                 | 75.6           | 86.6       | 75.6           | 48.8                 | 57.8           |\\n| Xwin-LM    |                |                      |                |            |                |                      |                |\\n|            | 7B             | 67.2                 | 37.2           | 67.0       | 36.8           | 46.0                 | 56.2           |\\n|            | 13B            | 78.0                 | 53.2           | 78.0       | 53.0           | 44.8                 | 55.4           |\\n| Alpaca     |                |                      |                |            |                |                      |                |\\n|            | 7B             | 73.8                 | 48.8           | 73.4       | 47.6           | 47.4                 | 54.4           |\\n|            | 13B            | 73.0                 | 52.4           | 72.6       | 51.8           | 47.4                 | 55.6           |\\n| Vicuna     |                |                      |                |            |                |                      |                |\\n|            | 7B             | 79.0                 | 79.2           | 63.0       | 63.2           | 47.2                 | 54.0           |\\n|            | 13B            | 79.4                 | 65.0           | 79.4       | 65.2           | 48.0                 | 56.8           |\\n|            | 33B            | 86.6                 | 75.6           | 86.6       | 75.6           | 48.8                 | 57.8           |\"}"}
{"id": "3Cp042s1Nc", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Size Choices | Choices | (Circular) Vocab | (Circular) Vocab | Alignment Normalized | Alignment PPL |\\n|-------------------|---------|-----------------|-----------------|----------------------|--------------|\\n| MPT               | 7B      | 22.9            | 0.3             | 22.8                 | 0.2          | 56.5         | 49.1         | 47.2         |\\n|                   | 30B     | 40.5            | 10.9            | 40.1                 | 10.3         | 61.5         | 55.0         | 51.4         |\\n| MPT-Chat          | 30B     | 64.1            | 39.0            | 64.1                 | 38.9         | 64.0         | 54.6         | 52.5         |\\n| Falcon            | 7B      | 20.7            | 0.0             | 20.9                 | 0.0          | 57.9         | 50.0         | 50.0         |\\n|                   | 40B     | 62.9            | 33.5            | 62.8                 | 33.4         | 62.5         | 54.9         | 52.2         |\\n| LLaMA-1           | 7B      | 33.2            | 0.3             | 31.8                 | 0.3          | 58.1         | 50.9         | 45.7         |\\n|                   | 13B     | 54.2            | 21.9            | 54.0                 | 21.9         | 59.8         | 51.8         | 45.2         |\\n|                   | 30B     | 65.4            | 40.2            | 65.4                 | 40.2         | 61.3         | 54.5         | 48.6         |\\n|                   | 65B     | 64.4            | 38.9            | 64.0                 | 37.9         | 63.6         | 55.6         | 50.5         |\\n| LLaMA-2           | 7B      | 34.6            | 3.2             | 34.6                 | 3.2          | 58.6         | 51.9         | 46.9         |\\n|                   | 13B     | 57.7            | 28.5            | 55.4                 | 25.5         | 61.9         | 53.9         | 48.4         |\\n|                   | 70B     | 69.8            | 46.7            | 69.3                 | 45.5         | 64.7         | 55.3         | 50.1         |\\n| LLaMA-2-Chat      | 7B      | 60.1            | 33.4            | 60.1                 | 33.4         | 57.7         | 50.2         | 45.9         |\\n|                   | 13B     | 65.3            | 37.6            | 65.3                 | 37.6         | 58.7         | 50.0         | 46.2         |\\n|                   | 70B     | 74.9            | 55.2            | 74.9                 | 55.2         | 61.0         | 54.7         | 48.5         |\\n| WizardLM          | 13B     | 67.0            | 42.4            | 67.0                 | 42.4         | 60.0         | 50.3         | 47.8         |\\n|                   | 70B     | 74.4            | 55.3            | 74.4                 | 55.3         | 60.4         | 53.9         | 49.8         |\\n| Xwin-LM           |         |                 |                 |                      |              |              |              |              |\\n|                   | 7B      | 50.9            | 18.3            | 50.9                 | 18.3         | 61.1         | 51.4         | 46.9         |\\n|                   | 13B     | 62.2            | 30.2            | 62.2                 | 30.1         | 63.6         | 54.1         | 50.1         |\\n| Alpaca            | 7B      | 54.3            | 21.7            | 54.1                 | 21.0         | 57.8         | 51.7         | 47.7         |\\n|                   | 13B     | 56.8            | 29.0            | 56.6                 | 28.9         | 60.5         | 50.9         | 46.6         |\\n| Vicuna            | 7B      | 60.5            | 39.2            | 60.7                 | 39.1         | 57.5         | 49.8         | 45.8         |\\n|                   | 13B     | 67.0            | 51.6            | 67.1                 | 51.6         | 61.6         | 51.4         | 47.7         |\\n|                   | 33B     | 69.3            | 46.7            | 69.2                 | 46.5         | 60.3         | 50.5         | 46.8         |\"}"}
{"id": "3Cp042s1Nc", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Size Choices | Model Name       | (Circular) Vocabulary | (Circular) Alignment | Normalized PPL | PPL |\\n|--------------------|------------------|-----------------------|---------------------|----------------|-----|\\n| 7B                 | MPT (Team et al., 2023a;b) | 29.2                  | 0.5                 | 29.7           | 0.5 |\\n| 30B                |                  | 56.8                  | 25.9                | 56.6           | 25.7 |\\n| 30B                | MPT-Chat (Team et al., 2023b) | 69.2                  | 49.7                | 69.4           | 50.0 |\\n| 40B                | Falcon (Almazrouei et al., 2023) | 26.5                  | 0.0                 | 25.3           | 0.0 |\\n| 7B                 |                  | 66.9                  | 43.1                | 65.6           | 42.2 |\\n| 13B                | LLaMA-1 (Touvron et al., 2023a) | 36.8                  | 3.4                 | 33.6           | 2.8 |\\n| 7B                 |                  | 54.7                  | 19.6                | 42.9           | 11.2 |\\n| 13B                |                  | 70.2                  | 47.2                | 70.0           | 47.1 |\\n| 30B                |                  | 75.7                  | 54.1                | 71.1           | 50.5 |\\n| 65B                | LLaMA-2 (Touvron et al., 2023b) | 48.2                  | 13.0                | 44.4           | 11.5 |\\n| 7B                 |                  | 65.9                  | 37.3                | 64.0           | 36.0 |\\n| 70B                |                  | 84.8                  | 74.2                | 83.3           | 72.3 |\\n| 7B                 | LLaMA-2-Chat (Touvron et al., 2023b) | 64.6                  | 37.1                | 64.2           | 36.9 |\\n| 13B                |                  | 72.4                  | 49.8                | 72.2           | 49.8 |\\n| 70B                |                  | 83.9                  | 71.0                | 83.9           | 71.0 |\\n| 13B                | WizardLM (Xu et al., 2023) | 73.5                  | 57.2                | 73.5           | 57.1 |\\n| 7B                 |                  | 83.2                  | 73.1                | 83.2           | 73.0 |\\n| 70B                |                  | 83.2                  | 73.1                | 83.2           | 73.0 |\\n| 7B                 | Xwin-LM (Team) | 58.2                  | 23.6                | 58.1           | 23.5 |\\n| 13B                |                  | 71.2                  | 44.4                | 71.1           | 44.2 |\\n| 70B                |                  | 83.9                  | 71.0                | 83.9           | 71.0 |\\n| 7B                 | Alpaca (Taori et al., 2023a) | 53.7                  | 23.8                | 53.5           | 23.3 |\\n| 13B                |                  | 55.0                  | 31.2                | 54.7           | 30.6 |\\n| 70B                |                  | 83.2                  | 73.1                | 83.2           | 73.0 |\\n| 7B                 | Vicuna (Chiang et al., 2023) | 69.3                  | 49.9                | 69.3           | 49.9 |\\n| 13B                |                  | 75.7                  | 60.7                | 75.7           | 60.7 |\\n| 33B                |                  | 77.8                  | 63.2                | 77.7           | 63.0 |\"}"}
{"id": "3Cp042s1Nc", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nTable 18: An example where GPT-4 generates accurate justifications.\\n\\nQuestion: What is the best way to form a new habit?\\n\\nLLM-1's Response (Vicuna 13B v1.5)\\n1. Start small: Begin with a simple and achievable goal.\\n2. Make it a routine: Establish a specific time and place for the habit.\\n3. Track your progress: Keep track of your progress to stay motivated.\\n4. Reward yourself: Reward yourself for every successful attempt.\\n5. Stay consistent: Stick to the habit even if you miss a day.\\n6. Get support: Share your goal with friends or family for support.\\n7. Reflect on your progress: Reflect on your progress and adjust your approach if needed.\\n\\nLLM-2's Response (Vicuna 7B v1.5)\\n1. Start small and make it specific.\\n2. Make it a daily routine.\\n3. Get an accountability partner.\\n4. Reward yourself for success.\\n5. Be consistent and persistent.\\n\\nGPT-4's Decision: Tie.\\n\\nGPT-4's Explanation:\\nBoth LLM-1 and LLM-2 provided responses that are accurate and relevant to the user's question about forming a new habit. LLM-1's response is slightly more comprehensive, including seven steps with a bit more detail, such as 'Track your progress' and 'Reflect on your progress.' LLM-2's response is more concise with five steps, but still covers the essential aspects of habit formation. Both responses are clear and easy to understand, adhering to the clarity criterion. There are no ethical or legal standards at risk in this context, so both comply with the compliance criterion. The timeliness criterion is not particularly relevant here, as the strategies for habit formation do not change rapidly over time. Neither response contains misleading or harmful content, and both avoid bias by providing general advice applicable to anyone looking to form a new habit. Given that both responses are satisfactory and there is no significant difference in the quality of the information provided, the evaluation results in a tie.\\n\\nTable 19: An example where GPT-4 generates accurate justifications.\\n\\nQuestion: What's your favorite part of the video for \\\"Somebody That I Used to Know\\\" by Walk off the Earth?\\n\\nLLM-1's Response (GPT-4 Turbo)\\nAs an AI, I don't have personal preferences or feelings, so I don't have a favorite part. However, the video for \\\"Somebody That I Used to Know\\\" by Walk off the Earth is widely recognized for its unique and creative approach to music performance. The band members are seen playing a single guitar simultaneously, which is an impressive display of coordination and musical arrangement. This aspect of the video often stands out to viewers and could be considered a highlight by many.\\n\\nLLM-2's Response (Xwin-LM 13B v0.1)\\nThe part where they're all playing the same guitar but in different octaves and positions. It's a great example of how creative and innovative they are as musicians.\\n\\nGPT-4's Decision: LLM-1 wins.\\n\\nGPT-4's Explanation:\\nBoth LLM-1 and LLM-2 provided relevant responses to the user's question about the music video for 'Somebody That I Used to Know' by Walk off the Earth. LLM-1's response is more effective as it adheres to the criteria of accuracy, relevance, comprehensiveness, and clarity. LLM-1 correctly identifies that as an AI, it does not have personal preferences, and then proceeds to describe a commonly appreciated aspect of the video, which is the band's coordination in playing a single guitar. This response is comprehensive as it provides context to why that part of the video might be considered a highlight. LLM-2's response is relevant and accurate but less comprehensive, as it only mentions the fact that the band members play the same guitar in different octaves and positions without further elaboration or context. Both responses are clear and free from bias, and neither contains harmful content. However, LLM-1's response is more effective overall due to its comprehensive nature.\"}"}
{"id": "3Cp042s1Nc", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 20: An example of GPT-4 providing incorrect justifications. We include comments to highlight the inaccuracies.\\n\\nQuestion Write an email to attendees as a reminder that the event is coming up.\\n\\nLLM-1's Response\\n\\nSubject: Reminder: Our Exciting Event is Just Around the Corner!\\nDear [Name],\\nWe hope this email finds you well. With only a few days left until our upcoming event, we wanted to send you a friendly reminder to save the date!\\nJoin us on [Date] at [Time] at [Location] for an unforgettable experience filled with [activities, performances, or keynote speakers]. We have an amazing lineup of [insert details], and we can't wait for you to be a part of it.\\nDon't miss out on the opportunity to [insert what attendees will gain from the event, such as networking, learning, or fun]. Our event is sure to be an exciting and memorable experience for all who attend.\\nIf you haven't already, please RSVP by [Date] to secure your spot. You can do so by visiting our website or by responding to this email.\\nWe're looking forward to seeing you there!\\nBest regards,\\n[Your Name]\\n[Your Title]\\n[Your Company]\\n[Contact Information]\\n\\nLLM-2's Response\\n\\nGPT-4's Decision Tie (both bad).\\nGPT-4's Explanation\\n\\nBoth LLM-1 and LLM-2 failed to provide a satisfactory response based on the evaluation criteria. LLM-1 provided a detailed and well-structured email template that is relevant, clear, and comprehensive. However, it lacks specific information such as the actual date, time, location, details about the activities, and the RSVP deadline, which are crucial for the accuracy and timeliness of the communication. LLM-2 did not provide any response at all, which fails all criteria. Therefore, neither response can be deemed effective, resulting in a tie where both responses are unsatisfactory.\\n\\nAuthors' Comment The requested email template has been successfully provided by LLM-1. However, it should be noted that GPT-4's critique regarding the absence of specific information needs to be addressed by the user, as LLM-1 is not equipped to do so. Given this limitation, LLM-1 should be acknowledged as the winner.\\n\\nTable 21: An example of GPT-4 providing incorrect justifications. We include comments to highlight the inaccuracies.\\n\\nQuestion highest number below 100, without using the digit 9\\n\\nLLM-1's Response\\n\\n(LLaMA-2 70B Chat) 99\\n\\nLLM-2's Response\\n\\n(Alpaca 13B) [Empty Response]\\n\\nGPT-4's Decision Tie (both bad).\\nGPT-4's Explanation\\n\\nLLM-1's response fails to meet the accuracy criterion as it includes the digit 9, which is explicitly against the user's stipulation. LLM-2 correctly identifies 89 as the highest number below 100 that does not include the digit 9, satisfying the accuracy and relevance criteria. Both responses are clear and concise, but LLM-1's response does not comply with the user's request, making LLM-2 the winner. There are no issues with timeliness, harmlessness, or bias in this context.\\n\\nAuthors' Comment GPT-4 made the correct decision; however, its justification was erroneous. The statement 'LLM-2 accurately determined 89 to be the greatest number under 100 without the digit 9' is incorrect, as 89 does indeed contain the digit 9.\"}"}
{"id": "3Cp042s1Nc", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Source          | Example-1                                                                 | Example-2                                                                 | Example-3                                                                 |\\n|-----------------|---------------------------------------------------------------------------|---------------------------------------------------------------------------|---------------------------------------------------------------------------|\\n| Google Trends    | \u201cWhat are some tips for beginners trying to master Wordle?\u201d               | \u201cCan you name three main characters from the TV show \u201cStranger Things\u201d?\u201d   | \u201cDid Mac Miller\u2019s music style influence the searches trends in hiphop?\u201d   |\\n| Quora           | \u201cWhat are some essential parenting skills?\u201d                                | \u201cWhat is the most horrifying noise you have ever heard?\u201d                  | \u201cWhat would happen if everyone in the world fell asleep at the same time?\u201d|\\n| AlpacaEval      | \u201cGive me the list of top 100 tech categories.\u201d                           | \u201cHow are carbon fibers used in buildings.\u201d                                 | \u201cWrite a 5 verse song in the style of Talking Heads based on the life of a teenager in the 1980s britain.\u201d |\\n| ShareGPT        | \u201cWrite me a business plan for my new basketball training company called ProX aka Professional Experience.\u201d | \u201cGive me a physics for problem to test understanding of velocity and gravity.\u201d | \u201cI want you to act as a social media content planner for a new startup company. Please provide ideas for five Instagram posts that showcase the products in different ways. Each post should have a brief caption that highlights the product\u2019s unique features or benefits.\u201d |\\n| LMSYS-Chat-1M   | \u201cCan you give me an example of a word that is used almost exclusively in the context of an idiomatic phrase and uncommon otherwise?\u201d | \u201cConclude what is the meal title from this and what are the total calories (if there is a range, choose higher number). Please print out only meal name and total calories. Act as a nutritionist specialising in educated guessing of total calories for any given meal. Based only on the information you have, make your best educated guess with confidence. If you lack some information for making the conclusion, please guess it. Provide only meal name and total calories and no other text or explanation.\u201d | \u201cTell me the most common fraud cases in consortium.\u201d |\"}"}
{"id": "3Cp042s1Nc", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nFangyun Wei\\n\\n* Xi Chen\\n\\n* Lin Luo\\n\\nAbstract\\n\\nDespite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method\u2014multiple choice question answering (MCQA), which allows for straightforward accuracy measurement. Through a comprehensive evaluation of 24 models across 11 benchmarks, we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios. In response, we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5, Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with GPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This system is designed to mirror real-world usage, and for this purpose, we have compiled a new benchmark called \u201cReal-world questions\u201d (RWQ), comprising 20,772 authentic user inquiries. Additionally, we thoroughly analyze the characteristics of our system and compare it with prior leaderboards like AlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo system, the feasibility of registering new models, and its potential to reshape LLM leaderboards. Project page: https://luolinrowling.github.io/Rethink-LLM-Eval.\\n\\n1. Introduction\\n\\nIn recent years, the advent of large language models (LLMs) (Radford et al., 2018; 2019; Brown et al., 2020; Touvron et al., 2023b) has revolutionized the field of artificial intelligence, offering unprecedented capabilities in natural language processing and understanding. However, the rapid development of LLMs also present significant challenges, particularly in terms of evaluation. The effective assessment of these models is crucial to ensure their reliability, fairness, harmlessness, and ethical use. This paper aims to contribute to the ongoing discourse in the field of natural language processing by providing a comprehensive evaluation of LLMs. Prior research has primarily focused on evaluating a range of capabilities including broad world knowledge, commonsense reasoning, and specialized skills like coding and mathematics. This has led to the introduction of various benchmarks (e.g. MMLU (Hendrycks et al., 2020), AGIEval (Zhong et al., 2023), and ARC (Clark et al., 2018)) and assessment platforms (e.g., HELM (van Gemert-Pijnen et al., 2011) and Harness (Gao et al., 2021)). While evaluating specific skills like coding, which can be assessed through targeted coding tasks and test cases using pass rate as a metric (Chen et al., 2021), and mathematics, where unique solutions enable accuracy-based metrics (Cobbe et al., 2021), is relatively clear-cut, assessing open-ended areas such as semantic comprehension, remains challenging. This complexity stems from two primary factors: 1) the open-ended nature of LLM responses leads to a broad spectrum of possible answers; and 2) the non-uniqueness of what constitutes a reasonable answer to a given question.\\n\\nThe evaluation of LLMs has predominantly centered around multiple choice question answering (MCQA) due to its straightforward approach in measuring LLM performance via accuracy and its facilitation of comparisons with other LLMs. This paper delves into the inherent shortcomings of the MCQA evaluation. Initially, we highlight the discrepancy between the real-world usage of LLMs for responding to user queries in an open-ended manner, and the confined nature of selecting the best option in MCQA. Moreover, the methodology for generating MCQA predictions differs among models. This can involve either pinpointing the likeliest token (e.g., \u201cA\u201d or \u201cB\u201d) upon reviewing the main question and its associated choices, or calculating the alignment between the question and each option using perplexity. Such variations result in inconsistent evaluations across different LLMs. Additionally, the open-ended responses provided by\"}"}
{"id": "3Cp042s1Nc", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nLLMs may not correspond with their MCQA predictions, leading to unreliable evaluations. This paper comprehensively examines these potential disadvantages through a combination of quantitative and qualitative studies.\\n\\nIn practical settings, LLMs perform tasks such as following user instructions, responding to inquiries, and providing answers to questions. We advocate for the use of LLMs to generate open-ended responses and recommend directly assessing these responses. Previous works, such as AlpacaEval (Li et al., 2023a), compare an LLM's response to that of a pre-defined benchmark model to the same query, and then calculate a win-rate, which serves as the comparison metric across various LLMs. Nevertheless, when this benchmark model is significantly superior (e.g., GPT-4-Turbo), AlpacaEval may not distinguish performance differences among the LLMs being evaluated (see Figure 5).\\n\\nFurthermore, the benchmark model serves as an \u201cintermediary agent\u201d to compare the relative capabilities between two LLMs\u2014a direct comparison between them is not made. If an optimization objective aimed at surpassing the benchmark model is incorporated into an LLM's training or fine-tuning process, it might result in an inflated win-rate, leading to an artificially high ranking.\\n\\nIn this work, we evaluate the performance of LLMs through a series of two-player contests. Each round involves randomly selecting two LLMs to respond to a query sampled from our \u201cReal-World Questions\u201d (RWQ) benchmark, which consists of 20,772 realistic user queries collected from sources such as Google Trends and Quora. A judge then determines the winner and the loser. Each LLM under assessment is assigned a rating that is adjusted after each competition based on the results. We utilize the Elo algorithm to update the ratings, naming this system the RWQ-Elo system. Recently, GPT-4, known for its superior language comprehension abilities, has been incorporated as a judge in numerous tasks (Li et al., 2023a). Our meticulous design of evaluation criteria and guidelines has shown that GPT-4's decisions align with those of human evaluators 95% of the time. Therefore, GPT-4 is employed as the judge in our RWQ-Elo system to enhance scalability. By orchestrating these contests among 24 LLMs such as GPT-4, GPT-3.5, Google Gemini-Pro, and LLaMA family, we could directly evaluate their relative capabilities, making them more distinguishable. Additionally, we analyze the stability of our RWQ-Elo system, its relation to existing LLM leaderboards, and the schema for new model registration. Owing to its simplicity and robustness, we anticipate that our approach could become a new standard for LLM evaluation.\\n\\n2. Related Work\\n\\nAdvancements in Generative Large Language Models.\\n\\nThe advent of generative LLMs marks a new era in the development of sophisticated AI algorithms adept at language comprehension and task execution. The introduction of ChatGPT (Achiam et al., 2023) has garnered significant attention, exemplifying this progress. These LLMs explore various architectures, such as causal decoders (for instance, GPT (Radford et al., 2018; 2019; Brown et al., 2020), LLaMA (Touvron et al., 2023a;b), OPT (Zhang et al., 2022), BLOOM (Workshop et al., 2022), LaMDA (Thoppilan et al., 2022)), encoder-decoder frameworks (like T5 (Raffel et al., 2020) and Flan-T5 (Chung et al., 2022)), and mixture-of-experts models (such as Switch Transformer (Fedus et al., 2022) and GLaM (Du et al., 2022)), along with innovative structures (for example, RWKV (Peng et al., 2023) and RetNet (Sun et al., 2023)). To reconcile the gap between pre-training objectives and user-directed goals, notably \u201cfollow their instructions helpfully and safely\u201d (Radford et al., 2019), instruction tuning techniques (Ouyang et al., 2022) are introduced. Instruction tuning or supervised fine-tuning in LLaMA, in particular, is key in developing tailored or niche models, including Vicuna (Chiang et al., 2023), Stanford Alpaca (Taori et al., 2023b), WizardLM (Xu et al., 2023), and Xwin-LM (Team, 2023). Additional research areas encompass scaling LLMs (Hoffmann et al., 2022; Rae et al., 2021; Chowdhery et al., 2023), managing long contexts (Su et al., 2024; Ding et al., 2023), devising decoding strategies (Leviathan et al., 2023; Chen et al., 2023; Li et al., 2023b), innovating sampling methods (Fan et al., 2018; Holtzman et al., 2019; Li et al., 2024), enhancing training efficiency (Huang et al., 2019; Shoeybi et al., 2019; Rajbhandari et al., 2020; Rasley et al., 2020; Dao et al., 2022; Hu et al., 2021), foundational operators (Ba et al., 2016; Shazeer, 2020), and training data collection (Zhu et al., 2015; Raffel et al., 2020; Liu et al., 2019), among others.\\n\\nLLM Evaluation.\\n\\nEvaluation of LLMs encompasses a variety of domains, including understanding knowledge (Khot et al., 2020), aligning responses to questions and instructions, utilization of tools, safety considerations, and specialized competencies in areas such as programming (Chen et al., 2021), mathematics (Cobbe et al., 2021; Austin et al., 2021), and language translation (Bojar et al., 2014; 2016; Lison & Tiedemann, 2016). A fundamental skill for LLMs is the ability to possess extensive general knowledge and to respond to questions or queries both correctly and logically. To evaluate this skill, numerous benchmarks have been developed, including human examination datasets (e.g., e.g., MMLU (Hendrycks et al., 2020), AGIEval (Zhong et al., 2023), C-Eval (Huang et al., 2023), and RACE (Lai et al., 2017)), datasets for assessing commonsense reasoning (e.g., ARC (Clark et al., 2018) and CommonSenseQA (Talmor et al., 2018)), and question-answering datasets (e.g., PIQA (Bisk et al., 2020), OpenBookQA (Mihaylov et al., 2018) and BoolQ (Clark et al., 2019)). Among these benchmarks, multiple choice question answering (MCQA) is particularly\"}"}
{"id": "3Cp042s1Nc", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nparticularly prominent due to the straightforward nature of using accuracy as a measurement criterion. In order to consolidate different benchmarks and offer a unified interface, various evaluation platforms have been introduced. These include HELM (van Gemert-Pijnen et al., 2011), Harness (Gao et al., 2021), OpenCompass (Contributors, 2023) and Big-bench (bench authors, 2023). Diverging from MCQA, AlpacaEval (Li et al., 2023a) proposes a different approach: having LLMs compete against stronger counterparts, namely Text-Davinci-003 and GPT-4, in a question-answering task, with GPT-4 serving as an adjudicator owing to its advanced capabilities. The evaluation metric adopted is the win-rate of these LLMs against Text-Davinci-003 or GPT-4. In contrast to AlpacaEval, which advocates for an LLM to compete with a single model, this work presents a multiple-player Elo rating system.\\n\\n3. Rethinking MCQA Evaluation\\n\\nFormulation and Notations. Multiple choice question answering (MCQA) has emerged as the dominant evaluation task, favored for its convenience in quantitative assessment. Generally, a multiple-choice question consists of a question $Q$, $K$ choices $\\\\{C_i\\\\}_{i=1}^K$, and a reference answer choice $A$.\\n\\nEach choice $C_i$ (e.g. \u201cA. 0 degrees Celsius.\u201d) comprises a choice number (e.g. \u201cA\u201d) and a statement $S_i$ (e.g. \u201c0 degrees Celsius.\u201d). An LLM is supposed to predict the most accurate answer choice upon encountering $Q$ and $\\\\{C_i\\\\}_{i=1}^K$.\\n\\nIf the prediction matches $A$, the corresponding question is considered correctly answered. Consequently, accuracy is readily adopted as the evaluation metric. Additionally, we use $V$ to denote the entire vocabulary of an LLM, and $V_C$ to represent the set containing all choice number tokens (e.g., \\\\{\u201cA\u201d, \u201cB\u201d, \u201cC\u201d, \u201cD\u201d\\\\} for MMLU and \\\\{\u201cA\u201d, \u201cB\u201d\\\\} for HellaSwag).\\n\\nEvaluation Strategies. The approach to generating predictions for MCQA varies significantly across different models. We categorize the evaluation strategies adopted by most LLMs as follows:\\n\\n\u2022 Selection of the Most Likely Token from the Choice Set. An LLM processes the concatenation of $Q$, $\\\\{C_i\\\\}_{i=1}^K$, and the phrase \u201cAnswer: \u201d, and then predicts the next token, which is the one from $V_C$ with the highest probability.\\n\\n\u2022 Selection of the Most Likely Token from the Entire Vocabulary. This method is similar to the previous one, but the prediction involves selecting the token with the highest probability from the entire vocabulary $V$, following the processing of $Q$, $\\\\{C_i\\\\}_{i=1}^K$, and the phrase \u201cAnswer: \u201d.\\n\\n\u2022 Alignment of Choice with the Question. Let $p(\\\\cdot)$ denote the posterior probability of generating statement $S_i$ of choice $C_i$ given the question $Q$. The MCQA prediction is the choice with the highest probability, i.e., $\\\\text{Argmax}_{1 \\\\leq i \\\\leq K} (p(S_i|Q))$.\\n\\n\u2022 Normalized Alignment of Choice with the Question. In this approach, the normalized posterior probability $p(S_i|Q) = \\\\frac{Q|S_i|j=1p(S_ji|\\\\{S_1:j-1i,Q\\\\})}{|C_i|}$ is computed for choice determination, where $|C_i|$ denotes the number of characters of $S_i$.\\n\\n\u2022 Perplexity. Perplexity is defined as $\\\\text{PPL}(S_i|Q) = \\\\exp(-1/|S_i| \\\\sum_{j=1}^{\\\\|S_i\\\\|} \\\\log(p(S_ji|\\\\{S_1:j-1i,Q\\\\}))$. The optimal choice is the one with the lowest perplexity score.\\n\\nBenchmarks, Models and Results Using Diverse Evaluation Strategies. We re-evaluate the capabilities of 24 LLMs through an analysis of their performance on 11 distinct benchmarks including MMLU, HellaSwag, ARC-Challenge, ARC-Easy, BoolQ, SIQA, PIQA, AGI Eval (English only), OpenBookQA (with fact), CommonSenseQA and RACE (all). These evaluations are conducted using the 5 aforementioning strategies (denoted as \u201cChoices\u201d, \u201cVocab\u201d, \u201cAlignment\u201d, \u201cNormalized Alignment\u201d and \u201cPPL\u201d, respectively).\\n\\nWe report averaged results across these benchmarks in Table 1. Detailed results for each benchmark can be found from Table 7-17 in the appendix.\\n\\nKey Findings for MCQA Evaluation.\\n\\n1. Various models demonstrate a significant preference for specific MCQA evaluation strategies. To accurately identify the best answer, an LLM necessitates two key skills: a) comprehension of the intention behind multiple-choice questions (e.g. predicting the correct choice number); b) extensive knowledge of a wide range of topics. For pre-trained LLMs that have not undergone instruction tuning, such as MPT, their performance is generally less effective when evaluated using the \u201cChoices\u201d and \u201cVocab\u201d strategies compared to the \u201cAlignment\u201d and \u201cNormalized Alignment\u201d strategies (Table 1). This trend may be due to a combination of factors, such as the model\u2019s capability, the absence of instruction tuning which leads to a disregard for user instructions, or both.\\n\\n2. LLMs often produce varying predictions when the order of choices is altered. To investigate this, we employ a circular evaluation method for both \u201cChoices\u201d and \u201cVocab\u201d approaches. Specifically, we rearrange the order of choices in a cycle and repeatedly input a multiple-choice question along with these rearranged choices into an LLM. A question is deemed successfully answered only if the LLM correctly responds to every variation of the question. The results using \u201cChoices (Circular)\u201d and \u201cVocab (Circular)\u201d as evaluation strategies, are presented in Table 1. This reveals a notable decline in the performance of all LLMs, highlighting their inconsistency in generating predictions for the same question with a different sequence of choices.\"}"}
{"id": "3Cp042s1Nc", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Averaged results across 11 0-shot datasets including MMLU (Hendrycks et al., 2020), HellaSwag (Zellers et al., 2019), ARC-Challenge and -Easy (Clark et al., 2018), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019), PIQA (Bisk et al., 2020), AGIEval (English only) (Zhong et al., 2023), OpenBookQA (with fact) (Mihaylov et al., 2018), Common-SenseQA (Talmor et al., 2018) and RACE (all) (Lai et al., 2017), using 7 evaluation strategies introduced in Section 3. We use the latest models up to February 1, 2024. We use general MCQA prompt for all benchmarks without dedicated design. Detailed results for each benchmark can be found in Table 7-17.\\n\\n| Model Size Choices | Choices | (Circular) Vocabulary | (Circular) Alignment | Normalized PPL |\\n|--------------------|---------|------------------------|---------------------|---------------|\\n| MPT                | 7B      | 36.0                   | 2.2                 | 35.2          |\\n|                    | 30B     | 53.0                   | 26.4                | 49.2          |\\n|                    | MPT-Chat| 30B                    | 61.5                | 60.8          |\\n|                    | Falcon  | 7B                     | 31.7                | 30.2          |\\n|                    |         | 40B                    | 62.3                | 62.0          |\\n|                    | LLaMA-1 | 7B                     | 40.4                | 38.7          |\\n|                    |         | 13B                    | 52.6                | 50.2          |\\n|                    | 30B     | 65.6                   | 45.3                | 65.4          |\\n|                    | 65B     | 67.5                   | 45.2                | 66.1          |\\n|                    | LLaMA-2 | 7B                     | 47.5                | 42.7          |\\n|                    |         | 13B                    | 60.8                | 58.6          |\\n|                    | 70B     | 75.2                   | 58.4                | 74.8          |\\n|                    | LLaMA-2-Chat | 7B              | 57.7                | 55.8          |\\n|                    |         | 13B                    | 65.4                | 65.3          |\\n|                    | 70B     | 74.3                   | 56.8                | 74.2          |\\n| WizardLM           | 13B     | 67.6                   | 47.1                | 67.6          |\\n|                    | 70B     | 76.7                   | 61.7                | 76.6          |\\n| Xwin-LM            | 7B      | 55.0                   | 25.2                | 54.8          |\\n|                    | 13B     | 64.0                   | 34.9                | 63.9          |\\n|                    | 33B     | 69.6                   | 50.2                | 64.3          |\\n| Alpaca             | 7B      | 52.7                   | 24.4                | 52.5          |\\n|                    | 13B     | 54.0                   | 30.3                | 53.6          |\\n| Vicuna             | 7B      | 62.6                   | 41.1                | 62.5          |\\n|                    | 13B     | 68.8                   | 50.1                | 68.7          |\\n|                    | 33B     | 69.6                   | 50.2                | 64.3          |\\n\\n3. Generative LLMs are trained with the objective of next-token prediction. While the accuracy of the MCQA evaluation is straightforward to measure and facilitates comparisons with other models, this approach does not always translate well to practical applications. Typically, users interact with LLMs by posing direct questions or seeking solutions to specific problems, rather than presenting multiple-choice questions. Our observations indicate that although some LLMs can successfully select the correct choice in MCQA tasks, their performance falls when tasked with directly generating responses to a question in an auto-regressive fashion. In these instances, the responses generated do not correspond accurately to the posed questions. Consequently, a question deemed as resolved in an MCQA setting may, in fact, remain unsolved in a free-form question-answering context. Table 2 provides an example of such discrepancies.\\n\\n4. In contrast to the third point, LLMs might accurately respond to open-ended questions yet incorrectly choose the reference choice in MCQA. This discrepancy arises primarily due to suboptimal instruction tuning and flawed design of the multiple-choice options. Designing these choices for MCQA can be subjective and challenging. Inadequate choice design might not encompass the response the LLM is inclined to express, resulting in questions remaining unresolved. An example is presented in Table 3.\\n\\n5. We note that when tasked with responding to a question...\"}"}
{"id": "3Cp042s1Nc", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nTable 2: The open-ended answer generated by the LLM (e.g. Alpaca-13B) does not directly address the question posed. However, it incidentally delivers the correct choice. The reference choice is highlighted in purple.\\n\\n**Question**: \u201cIn which urban US setting below would you expect to find planned communities? \u201d\\n**Choices**: \u201cA. Ghettos B. Suburbs C. CBDs D. Transition zones\u201d\\n**LLM\u2019s Choice**: \u201cB\u201d\\n**LLM\u2019s Open-ended Answer**: \u201csl\u00a8aktet urban sprawl. The answer is: In which urban US setting below would you expect to find planned communities? The answer is: In which urban US setting below would you expect to find planned communities?\u201d\\n\\nTable 3: The LLM (e.g. LLaMA-2-Chat-13B)\u2019s open-ended answer addresses the question, but the MCQA prediction is incorrect.\\n\\n**Question**: \u201cWhat is the quotient of 1,224/9? \u201d\\n**Choices**: \u201cA. 135 B. 136 C. 1,215 D. 1,360\u201d\\n**LLM\u2019s Choice**: \u201cA\u201d\\n**LLM\u2019s Open-ended Answer**: \u201cAnswer: 136\u201d\\n\\ntaken from an MCQA benchmark through open-ended generation, an LLM may produce a range of choices and then pick one from these self-generated choices. This behavior indicates a potential risk of data leakage in testing benchmarks or insufficient optimization of instruction tuning. In Table 4, we present an illustrative case.\\n\\nOpen-ended MCQA.\\nThe discrepancy between traditional MCQA assessments and practical open-ended question-answering scenarios raises the inquiry: is it possible to adapt existing MCQA benchmarks to support open-ended question-answering? To explore this, we introduce an open-ended MCQA evaluation methodology. This approach, taking cues from the recent advancements with GPT-4 functioning as a judge, involves a two-step process: 1) posing each question from an MCQA benchmark to the LLM under evaluation to elicit an open-ended answer, and 2) employing GPT-4 to determine the choice that most semantically aligns with the LLM\u2019s response, by comparing it against the given choices. The prompt for GPT-4\u2019s judgement is detailed in Section A.1 of the appendix.\\n\\nOur assessment of the open-ended MCQA is conducted on GPT-4-Turbo-1106-preview is used throughout the paper.\\n\\nTable 4: LLMs (e.g., LLaMA-2-Chat-13B) may exhibit the behavior of \u201cchoice generation before answering.\u201d\\n\\n**Question**: \u201cDuring which step of the Chase Jones model of issues management should the strengths and weaknesses of the organisation be reviewed? \u201d\\n**Choices**: \u201cA. Identification B. Analysis C. Strategy D. Evaluation\u201d\\n**LLM\u2019s Choice**: \u201cB\u201d\\n**LLM\u2019s Open-ended Answer**: \u201cA) Identification B) Analysis C) Evaluation D) Resolution Correct answer: B) Analysis\u201d\\n\\nTable 5: We evaluate open-ended MCQA by using LLaMA-2-Chat-13B on the filtered MMLU 0-shot benchmark.\\n\\n| MCQA (Choices) | Open-ended MCQA |\\n|----------------|------------------|\\n| 54.0           | 39.7             |\\n\\na modified version of the MMLU benchmark, which has undergone several filtering stages to remove questions unsuitable for open-ended formats. Specifically, this filtering process eliminates questions of three types: 1) those structured as \u201cwhich...following\u201d, \u201cwhich...these\u201d, or \u201cwhich...are\u201d; 2) questions that require filling in blanks; 3) questions where the choices include terms like \u201cnone\u201d, \u201cboth\u201d, \u201cneither\u201d or \u201call of\u201d. This process yields a filtered MMLU benchmark comprising 7,223 instances.\\n\\nWe adopt LLaMA-2-Chat-13B to conduct open-ended MCQA and perform a comparative analysis with the conventional MCQA approaches that employ \u201cChoices\u201d strategy. A significant performance gap is evident in Table 5, which confirms the inconsistency between selecting the optical choice in MCQA and addressing the question in an open-ended manner. To verify the effectiveness of using GPT-4 as a judge in open-ended MCQA, we conduct a manual examination to compare the assessments made by GPT-4 with those made by human evaluators, on a subset including 500 instances sampled from MMLU, ARC-Challenge, ARC-Easy, RACE, SIQA and PIQA benchmarks. This study reveals a high alignment rate of 80%, indicating the feasibility of using GPT-4 as a judge for open-ended MCQA evaluation.\\n\\nDiscussions for MCQA Evaluation.\\nWhile MCQA evaluation offers the ease of quantifying an LLM\u2019s capability with a single accuracy metric, it is not without shortcomings. Our findings, as presented in Tables 1 and Table 7-17 in the appendix, along with the observations from Tables 2-5, highlight several issues inherent in MCQA evaluations. These include varied evaluation strategies, biases in multiple-choice design, discrepancies between open-ended question-answering, and so on.\"}"}
{"id": "3Cp042s1Nc", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nGoogle Trends\\nLMSYS-Chat-1M\\nShareGPT\\nQuora\\nAlpacaEval\\n\\nFigure 1: Statistics for our Real-World Question (RWQ) benchmark. Examples for each source are available in Table 22 of the appendix.\\n\\nanswers and choice predictions, and a mismatch between evaluation mechanisms and practical usage scenarios. While the introduction of open-ended MCQA addresses some of these concerns, it does not fully bridge the gap between how evaluations are conducted and how users typically interact with LLMs\u2014often by posing queries that elicit open-ended responses. Therefore, an evaluation approach that more closely mirrors real-world applications is essential.\\n\\n4. RWQ-Elo System for LLM Evaluation\\n\\nIn this work, we present the RWQ-Elo system for evaluating LLMs. Originally, the Elo rating algorithm is extensively employed to assess the relative skill levels of multiple players in a particular game, typically following numerous rounds of two-player contests. This system finally generates an Elo rating for each player, reflecting their comparative proficiency. Implementing this system poses several challenges: creating contest materials, establishing victory and defeat criteria, choosing a judge, and defining principles for rating stability.\\n\\nContest Materials.\\nTo ensure that the evaluation of LLMs accurately reflects their use in practical scenarios, we assemble a dataset called \u201cReal-World Questions\u201d (RWQ). This dataset comprises 20,772 authentic questions sourced from various platforms such as Google Trends, Quora, ShareGPT, LMSYS-Chat-1M, and AlpacaEval. The composition of the RWQ dataset is depicted in Figure 1. We utilize this dataset in the implementation of the RWQ-Elo system.\\n\\nGPT-4 as A Judge.\\nIn the realm of two-player games, the optimal practice for assessing victory or defeat is to involve the engagement of linguistic experts as judges. However, this approach is often prohibitively costly and impractical. We utilize GPT-4 to transform each entry listed in Google Trends into a formulated question. Recently, the use of GPT-4 as an evaluator has gained traction in various applications, e.g., tool use (Du et al., 2024). Our observations also indicate that the integration of GPT-4 into our Elo rating system offers both stability and reliability. To substantiate this, we conduct a random sampling of 300 questions from our RWQ dataset. For each question, we feed it into two different LLMs to generate answers. Subsequently, GPT-4 is employed to determine the winner, loser, or a tie by evaluating which response most effectively addresses the question with the consideration of accuracy, relevance, comprehensiveness, clarity, compliance, timeliness, harmlessness, and unbiasedness. Concurrently, a human evaluator also reviews the two answers to identify the winner, loser or a tie. This procedure is replicated across all sampled questions. Ultimately, we calculate an alignment rate between the decisions of GPT-4 and the human evaluator, which stands at an impressive 95%, underscoring the reliability of employing GPT-4 as a judge.\\n\\nRWQ-Elo Rating Algorithm.\\nIn a setup involving \\\\( N \\\\) LLMs, each LLM begins with an initial Elo rating of 1000. During each competition round, we randomly pair two LLMs (referred to as LLM-A and LLM-B) and present them with a question sampled from our RWQ database. Both LLM-A and LLM-B independently process the question and provide their respective answers. Subsequently, we utilize GPT-4 as a judge to decide the outcome, which can be either LLM-A defeating LLM-B, LLM-A losing to LLM-B, or a tie. Based on this result, we update the Elo ratings of the two LLMs in accordance with the specified update mechanism detailed below.\\n\\nIn each competition round, the expected score for either LLM-A or LLM-B, when matched against each other, is calculated as follows:\\n\\n\\\\[\\nE_A = \\\\frac{1}{1 + 10^{(R_B - R_A)/400}}, \\\\quad (1)\\n\\\\]\\n\\n\\\\[\\nE_B = 1 - E_A, \\\\quad (2)\\n\\\\]\\n\\nwhere \\\\( E_A \\\\) and \\\\( E_B \\\\) symbolize the expected scores of LLM-A and LLM-B, respectively; \\\\( R_A \\\\) and \\\\( R_B \\\\) are their current Elo ratings. Subsequently, the updated rating is calculated using:\\n\\n\\\\[\\nR_A' = R_A + K \\\\times (S_A - E_A), \\\\quad (3)\\n\\\\]\\n\\n\\\\[\\nR_B' = R_B + K \\\\times (S_B - E_B), \\\\quad (4)\\n\\\\]\\n\\nwhere \\\\( R_A' \\\\) and \\\\( R_B' \\\\) denote the updated Elo ratings for LLM-A and LLM-B; \\\\( S_A \\\\) (or \\\\( S_B \\\\)) is set to 1 if LLM-A (or LLM-B) wins, 0 if it loses, and 0.5 in the event of a tie; \\\\( K \\\\) represents the \\\\( K \\\\)-factor, which is set to 4 by default.\\n\\nTo ensure a fair competition among the LLMs, we have structured the contest so that each LLM competes with every\"}"}
{"id": "3Cp042s1Nc", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: (a) Comparison of various leaderboards, including our RWQ-Elo (Elo rating for each LLM is reported in brackets), Chatbot Arena (Zheng et al., 2023), MT-Bench (Zheng et al., 2023) and AlpacaEval (v1.0 and v2.0) (Li et al., 2023a). (b) Statistics from running our RWQ-Elo systems 100 times. We show the Elo ratings for the selected 13 LLMs. The complete statistics can be found in Figure 6.\\n\\nOther LLM exactly \\\\( N \\\\) times. Considering the inclusion of \\\\( N \\\\) LLMs in total, the total number of competitive rounds conducted is \\\\( N \\\\times (N - 1) \\\\times H / 2 \\\\). We find that setting \\\\( H \\\\) to 200 yields stable Elo rating.\\n\\nThe order of competition has a notable impact on the eventual Elo rating. To mitigate the volatility in ratings attributed to the order of competitions, we maintain a constant question seed for each two-player contest and randomize the competition order \\\\( C \\\\) times, yielding \\\\( C \\\\) distinct Elo ratings. The ultimate Elo rating is determined by calculating the median of these \\\\( C \\\\) Elo ratings. In our implementation, we set \\\\( C \\\\) to 100.\\n\\nResults of Our RWQ-Elo Rating System and Comparisons with Other Leaderboards. Our system integrates a total of 24 models including GPT-3.5, GPT-4, Google-Gemini-Pro and other 21 representative open-source LLMs. In Figure 2.(a), we present the final Elo ratings from our RWQ benchmark, alongside comparisons with leaderboards from Chatbot Arena, MT-Bench, and both versions 1.0 and 2.0 of AlpacaEval.\\n\\nUnlike AlpacaEval, which determines the win rate of each LLM against a constant model (Text-Davinci-003 for v1.0 and GPT-4 for v2.0), our Elo system encourages every LLM to compete against each other. Our approach offers threefold advantages: firstly, as shown in Figure 5 in the appendix, when LLMs compete against an LLM that is significantly superior or inferior, it results in a lack of distinguishable performance differences among them; secondly, it avoids...\"}"}
{"id": "3Cp042s1Nc", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nFigure 3: Differences between the win-rate map generated by our Elo system and the pre-calculated win-rate map are represented using absolute values. We include 13 LLMs. The two complete win-rate maps alongside their difference map can be found in Figure 7-8 and Figure 9 of the appendix.\\n\\nFigure 4: Visualization of the win-rate trends between two LLMs ((a) Falcon-Instruct-40B v.s. MPT-Chat-30B; (b) Falcon-Instruct-7B v.s. Gemini-Pro). The horizontal lines represent the pre-calculated win rates. With the progression of each contest round, the win rate ascertained by our Elo rating system progressively converges with the pre-calculated win rate.\\n\\n\u201cover-competition\u201d to a single model (for instance, adding an optimization objective during training to compete against a specific model); thirdly, it allows for the consideration of interactions between all participating LLMs. MT-Bench, in contrast, generates a score between 0 and 10 to each LLM. Despite the advantage of assessing inter-LLM competition, our Elo system is meticulously crafted to include various factors like comprehensiveness and unbiasedness (refer to Section A.2 for the detailed prompt). Meanwhile, Chatbot Arena employs diverse online human evaluators to assess the quality of responses from two different LLMs to the same query. However, this approach faces challenges due to the subjective nature of evaluations and the difficulty in scaling up with consistent human evaluators for all pairwise comparisons. Our findings indicate a high congruence of 95% between GPT-4\u2019s assessments and human preferences. Additionally, our Elo system offers the benefit of being easily scalable in terms of the number of the questions and LLMs to be tested.\\n\\nAnalysis of Stability. In order to minimize the impact of the sequence of two-player contests on the final Elo rating, we run our Elo systems $C$ times. The rating from each run, along with the median rating, is depicted in Figure 2.(b). This demonstrates a notable consistency in the rankings across each separate execution. Additionally, we verify the stability of the ultimate Elo rating by comparing the win-rate map produced by our Elo system utilizing Eq. 1 and 2, against a pre-calculated win-rate map. The latter is obtained by comparing responses of two LLMs to the same questions drawn from our RWQ benchmark, with GPT-4 acting as the judge and using the same judgement prompt. It is worth noting that the elements within the pre-calculated win-rate map are derived solely from the comparison of responses between two LLMs. This process differs from our Elo system, where the LLMs compete against a broader range of other LLMs. The differences of the two win-rate maps, as showcased in Figure 3, reveals a consistent correspondence between them, demonstrating the stability of our Elo system. Additionally, the trends in win rates between two LLMs, as shown in Figure 4, further confirm the stability.\\n\\nRegistration of New Models. Consider a scenario where there are $N_1$ established LLMs with stable Elo ratings ($\\\\{\\\\text{LLM}_i\\\\}_{i=1}^{N_1}$), and the objective is to integrate $N_2$ new LLMs ($\\\\{\\\\text{LLM}^*_i\\\\}_{i=1}^{N_2}$) into the existing Elo ranking. Without loss of generality, $N_2 \\\\ll N_1$. Instead of re-executing the Elo algorithm from scratch with all LLMs included, a more efficient approach termed fast-registration is proposed. This approach retains the ratings of the $N_1$ existing LLMs, and assigns an initial rating of 1000 to the $N_2$ newly-registered LLMs. Each competition involves two players, one being a LLM from $\\\\{\\\\text{LLM}^*_i\\\\}_{i=1}^{N_2}$, and the other either from $\\\\{\\\\text{LLM}_i\\\\}_{i=1}^{N_1}$, or...\"}"}
{"id": "3Cp042s1Nc", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\n\\\\( \\\\text{LLM}^{\\\\ast} \\\\) \\\\( N_2 \\\\) \\\\( i = 1 \\\\) - LLM. Post each competition, the Elo ratings are updated accordingly. The fast-registration process \\\\( (2N_1 + N_2 - 1) \\\\times N_2 \\\\times H/2 \\\\) rounds of competition, which is significantly less\u2014by \\\\( N_1 \\\\times (N_1 - 1) \\\\times H/2 \\\\) rounds\u2014than recalculating the Elo ratings from scratch. Table 6 presents the rating differentials and Kendall's tau (which evaluates ranking differences) for the fast-registration approach against executing the Elo system from scratch. The results are the averages of 5 independent experiments. Fast registration has proven to be effective, especially when \\\\( N_2 \\\\geq 3 \\\\).\\n\\n5. Cost Analysis\\n\\nEvaluation Efficiency and Total Evaluation Time. Consider a scenario where we need to evaluate \\\\( N \\\\) (\\\\( N = 24 \\\\) in our setup) LLMs using our RWQ-Elo framework. This process consists of five steps:\\n\\n1. Generation of Competition Schedule (Denoted as \\\\( O \\\\)). In our setup, each LLM participates in \\\\( H \\\\) (\\\\( H = 200 \\\\) in our setup) rounds of contests against every other model. This results in a competition schedule (\\\\( O \\\\)) that encompasses \\\\( N \\\\times (N - 1) \\\\times H/2 \\\\) individual two-player contests. For each contest, we pair two LLMs to respond to a question sampled from our RWQ dataset.\\n\\n2. Response Generation. In each contest, the two participating LLMs generate responses to the question. We record the responses from all contests. This process takes 30 hours on 8 \\\\( \\\\times \\\\) Nvidia A100 (80G) GPUs.\\n\\n3. Determining Contest Results. GPT-4 is tasked with judging the outcomes of these contests, and the competition results are recorded. Consequently, GPT-4 is called a total of \\\\( N \\\\times (N - 1) \\\\times H/2 \\\\) times. Due to the instability of the OpenAI GPT-4 servers, this step takes 31 hours, including processing time and server waiting time.\\n\\n4. Elo Rating Calculation. Once the competition results are generated by GPT-4, running our RWQ-Elo system once is extremely fast, taking only 2 s on a CPU (AMD EPYC 7V13 64-Core Processor) in our default scenario where \\\\( N = 24 \\\\) and \\\\( H = 200 \\\\).\\n\\n5. Multiple-time Elo Rating Calculation. The Elo rating system is sensitive to the competition schedule \\\\( O \\\\). To ensure the reliability of our ratings, we execute the RWQ-Elo process \\\\( C \\\\) times (\\\\( C = 100 \\\\) by default), each time only randomizing the competition schedule \\\\( O \\\\) (i.e., only performing Step 4 with a shuffled \\\\( O \\\\)). Since the results of the matches are already recorded (Step 3), there is no need to reassess them using GPT-4. Therefore, despite repeating the RWQ-Elo rating process \\\\( C \\\\) times, the cumulative number of GPT-4 API calls remains at \\\\( N \\\\times (N - 1) \\\\times H/2 \\\\), instead of \\\\( C \\\\times N \\\\times (N - 1) \\\\times H/2 \\\\). This step takes about 100 \\\\( \\\\times \\\\) 2 s = 200 s on the CPU used in Step 4.\\n\\nIn conclusion, operating our RWQ-Elo system requires 30 h (Step 2) + 31 h (Step 3) + 200 s (Step 5) = 61.06 hours, utilizing a server equipped with an AMD EPYC 7V13 64-Core Processor CPU and 8 \\\\( \\\\times \\\\) Nvidia A100 (80G) GPUs.\\n\\nCost. Our RWQ-Elo system requires calling GPT-4 \\\\( N \\\\times (N - 1) \\\\times H/2 \\\\) times, where \\\\( N = 24 \\\\) and \\\\( H = 200 \\\\) are the default values. For each call, we feed the combination of the evaluation prompt, the question, and the responses generated by two LLMs into GPT-4. The average length of each GPT-4 input is 1088 tokens. As of March 28th, 2024, the cost for using GPT-4-Turbo-1106-preview is $0.01 per 1000 input tokens. Consequently, the input cost calculates to \\\\( N \\\\times (N - 1) \\\\times H/2 \\\\times 1088 \\\\times 0.01 / 1000 = $600.5 \\\\). The average length of the GPT-4 output is 204 tokens, which includes the decision and a brief explanation. The cost for output tokens is $0.03 per 1000 tokens. Therefore, the output cost is \\\\( N \\\\times (N - 1) \\\\times H/2 \\\\times 204 \\\\times 0.03 / 1000 = $337.8 \\\\). The total cost is the sum of the input and output costs, amounting to $600.5 + $337.8 = $938.3.\\n\\n6. Conclusion\\n\\nIn this paper, we: 1) critically reassess the widely used MCQA method for evaluating LLMs, identifying several fundamental limitations; and 2) present the RWQ-Elo system, designed to reflect actual usage scenarios. We conduct extensive evaluations of 24 LLMs through multiple rounds of competition, with GPT-4 serving as the judge and utilizing our newly developed RWQ benchmark. Our objective is to provide fresh perspectives within the LLM community and establish a novel benchmarking framework that aids in the assessment of LLMs. We also demonstrate the ease of incorporating new models into our RWQ-Elo system. Our aim is to gather more diverse and realistic user inquiries across various subjects, and to conduct a more comprehensive evaluation of a wider range of LLMs in future studies.\\n\\nImpact Statement\\n\\nThis paper aims to provide novel insights to the community of LLM evaluation and, by its nature, inherently carries no risks or societal consequences. While the original LLMs may generate inaccurate or harmful content, such issues are independent of our functionality. Some limitations of this work include, language coverage (all questions in our dataset are in English, due to the fact that some models under test only support English input), the range of model sizes tested (we examine open-source models with parameters ranging from 7B to 70B, as well as private models such as GPT-4/3.5 and Gemini-Pro), the performance of GPT-4 as a judge (the alignment rate between GPT-4's assessments and those of human evaluators is 95%) and the inability to evaluate all prior LLMs due to limited resources and the rapid progress in LLM development.\"}"}
{"id": "3Cp042s1Nc", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nReferences\\n\\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\n\\nAlmazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, \u00b4E., Hesslow, D., Lau\u00adnay, J., Malartic, Q., et al. The falcon series of open lan\u00adguage models. arXiv preprint arXiv:2311.16867, 2023.\\n\\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n\\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n\\nbench authors, B. Beyond the imitation game: Quantify\u00ading and extrapolating the capabilities of language mod\u00adels. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj.\\n\\nBisk, Y ., Zellers, R., Gao, J., Choi, Y ., et al. Piqa: Reason\u00ading about physical commonsense in natural language. In Pro\u00adceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432\u20137439, 2020.\\n\\nBojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint\u00adAmand, H., et al. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation, pp. 12\u201358, 2014.\\n\\nBojar, O., Chatterjee, R., Federmann, C., Graham, Y ., Had\u00addow, B., Huck, M., Yepes, A. J., Koehn, P., Logacheva, V ., Monz, C., et al. Findings of the 2016 conference on machine translation (wmt16). In First conference on machine translation, pp. 131\u2013198. Association for Com\u00adputational Linguistics, 2016.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nChen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y ., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y ., Gonzalez, J. E., et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.\\n\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023.\\n\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y ., Fedus, W., Li, Y ., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\\n\\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\\n\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\\n\\nCobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n\\nContributors, O. Opencompass: A universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023.\\n\\nDao, T., Fu, D., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat\u00adtention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022.\\n\\nDing, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., Zheng, N., and Wei, F. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023.\\n\\nDu, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y ., Krikun, M., Zhou, Y ., Yu, A. W., Firat, O., et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pp. 5547\u20135569. PMLR, 2022.\\n\\nDu, Y ., Wei, F., and Zhang, H. Anytool: Self-reflective, hierarchical agents for large-scale api calls. arXiv preprint arXiv:2402.04253, 2024.\"}"}
{"id": "3Cp042s1Nc", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nFan, A., Lewis, M., and Dauphin, Y. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833, 2018.\\n\\nFedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232\u20135270, 2022.\\n\\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., et al. A framework for few-shot language model evaluation. Version v0.0.1. Sept, 2021.\\n\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\n\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\\n\\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.\\n\\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\\n\\nHuang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019.\\n\\nHuang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu, J., Lv, C., Zhang, Y., Lei, J., Fu, Y., Sun, M., and He, J. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. In Advances in Neural Information Processing Systems, 2023.\\n\\nKhot, T., Clark, P., Guerquin, M., Jansen, P., and Sabharwal, A. Qasc: A dataset for question answering via sentence composition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 8082\u20138090, 2020.\\n\\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.\\n\\nLeviathan, Y., Kalman, M., and Matias, Y. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 19274\u201319286. PMLR, 2023.\\n\\nLi, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023a.\\n\\nLi, Y., Wei, F., Zhao, J., Zhang, C., and Zhang, H. Rain: Your language models can align themselves without fine-tuning. arXiv preprint arXiv:2309.07124, 2023b.\\n\\nLi, Y., Wei, F., Zhang, C., and Zhang, H. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024.\\n\\nLison, P. and Tiedemann, J. Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles. 2016.\\n\\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\\n\\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\\n\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\\n\\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.\\n\\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training. 2018.\\n\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n\\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\"}"}
{"id": "3Cp042s1Nc", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y.\\n\\nZero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1\u201316. IEEE, 2020.\\n\\nRasley, J., Rajbhandari, S., Ruwase, O., and He, Y.\\n\\nDeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505\u20133506, 2020.\\n\\nSap, M., Rashkin, H., Chen, D., LeBras, R., and Choi, Y.\\n\\nSocialIQA: Commonsense reasoning about social interactions.\\n\\narXiv preprint arXiv:1904.09728, 2019.\\n\\nShazeer, N.\\n\\nGlu variants improve transformer.\\n\\narXiv preprint arXiv:2002.05202, 2020.\\n\\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B.\\n\\nMegatron-LM: Training multi-billion parameter language models using model parallelism.\\n\\narXiv preprint arXiv:1909.08053, 2019.\\n\\nSu, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.\\n\\nRoformer: Enhanced transformer with rotary position embedding.\\n\\nNeurocomputing, 568:127063, 2024.\\n\\nSun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F.\\n\\nRetentive network: A successor to transformer for large language models.\\n\\narXiv preprint arXiv:2307.08621, 2023.\\n\\nTalmor, A., Herzig, J., Lourie, N., and Berant, J.\\n\\nCommonsenseQA: A question answering challenge targeting commonsense knowledge.\\n\\narXiv preprint arXiv:1811.00937, 2018.\\n\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B.\\n\\nAlpaca: A strong, replicable instruction-following model.\\n\\nStanford Center for Research on Foundation Models. https://crfm.stanford.edu/2023/03/13/alpaca.html, 3(6):7, 2023a.\\n\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B.\\n\\nStanford alpaca: An instruction-following llama model, 2023b.\\n\\nTeam, M. et al.\\n\\nIntroducing mpt-7b: a new standard for open-source, commercially usable llms, 2023a.\\n\\nTeam, M. N. et al.\\n\\nIntroducing mpt-30b: Raising the bar for open-source foundation models, 2023b.\\n\\nXwin-lm, 9 2023.\\n\\nURL https://github.com/Xwin-LM/Xwin-LM.\\n\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al.\\n\\nLamda: Language models for dialog applications.\\n\\narXiv preprint arXiv:2201.08239, 2022.\\n\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al.\\n\\nLlama: Open and efficient foundation language models.\\n\\narXiv preprint arXiv:2302.13971, 2023a.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.\\n\\nLlama 2: Open foundation and fine-tuned chat models.\\n\\narXiv preprint arXiv:2307.09288, 2023b.\\n\\nvan Gemert-Pijnen, J. E., Nijland, N., van Limburg, M., Ossebaard, H. C., Kelders, S. M., Eysenbach, G., and Seydel, E. R.\\n\\nA holistic framework to improve the uptake and impact of ehealth technologies.\\n\\nJournal of medical internet research, 13(4):e1672, 2011.\\n\\nWorkshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u0107, S., Hesslow, D., Castagn\u00e9, R., Luccioni, A. S., Yvon, F., et al.\\n\\nBloom: A 176b-parameter open-access multilingual language model.\\n\\narXiv preprint arXiv:2211.05100, 2022.\\n\\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D.\\n\\nWizardLM: Empowering large language models to follow complex instructions.\\n\\narXiv preprint arXiv:2304.12244, 2023.\\n\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\\n\\nHellaswag: Can a machine really finish your sentence?\\n\\narXiv preprint arXiv:1905.07830, 2019.\\n\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al.\\n\\nOpt: Open pre-trained transformer language models.\\n\\narXiv preprint arXiv:2205.01068, 2022.\\n\\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al.\\n\\nJudging llm-as-a-judge with mt-bench and chatbot arena.\\n\\narXiv preprint arXiv:2306.05685, 2023.\\n\\nZhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N.\\n\\nAgieval: A human-centric benchmark for evaluating foundation models.\\n\\narXiv preprint arXiv:2304.06364, 2023.\\n\\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtau-sun, R., Torralba, A., and Fidler, S.\\n\\nAligning books and\"}"}
{"id": "3Cp042s1Nc", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nmovies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pp. 19\u201327, 2015.\"}"}
{"id": "3Cp042s1Nc", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nA. Prompts\\n\\nA.1. Prompt for Open-ended MCQA\\n\\nGiven a response generated by an LLM, and several choices. GPT-4 is then used to determine the choice that is most semantically aligned with the response. The prompt is provided as follows.\\n\\nSystem\\nYou are a helpful assistant.\\n\\nUser\\nTask description:\\nYou are presented with multiple choices and a statement. Your task involves selecting the choice that most semantically aligns with the given statement. Use the criteria and guidelines provided to make your decision.\\n\\nCriteria:\\n1. Choose the option that semantically aligns with the statement, by either expanding upon, encapsulating, or exactly matching it.\\n2. For numerical choices, prefer the one with the smallest reasonable numerical difference from the statement.\\n3. It is acceptable to choose none if no option closely aligns with the statement.\\n\\nGuidelines:\\n1. Review all choices and the statement carefully.\\n2. Justify your choice briefly, considering the criteria.\\n3. Indicate your response by stating the uppercase letter of your choice (e.g., 'A', 'B', 'C', 'D'), or 'None' if no choice matches.\\n\\nOutput format:\\n1. Present your response in JSON format.\\n2. Include the 'choice'(the uppercase letter of the chosen choice or 'None'), along with a brief 'explanation' for your selection.\\n\\nChoices:\\n\\nStatement:\\n\\nA.2. Prompt for RWQ-Elo System\\n\\nGPT-4 serves as a judge in evaluating responses from two different LLMs (LLM-1 and LLM-2) to the same query. The prompt is provided as follows.\\n\\nSystem\\nYou are a helpful assistant who can evaluate Large Language Model (LLM) responses.\\n\\nUser\\nTask description:\\nAs a judge, your task is to assess the responses of two Large Language Models (LLM-1 and LLM-2) to a user's question. Base your evaluation on the criteria below to determine which response is more effective.\\n\\nCriteria:\\n1. Accuracy: Ensure responses are factually correct. For factual questions,\"}"}
{"id": "3Cp042s1Nc", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Generative Large Language Model Evaluation for Semantic Comprehension\\n\\nresponses should align with scientific consensus.\\n\\n2. Relevance: Check if responses address the user's question directly, understanding its context and intent.\\n\\n3. Comprehensiveness: Responses should cover all aspects of the question, providing a clear overview and key points for complex issues.\\n\\n4. Clarity: Ensure responses are easy to understand, especially when explaining complex topics.\\n\\n5. Compliance: Adherence to ethical and legal standards is mandatory.\\n\\n6. Timeliness: Incorporate the latest information for current topics.\\n\\n7. Harmlessness: Avoid misleading or harmful content, respecting cultural sensitivities and privacy.\\n\\n8. Unbiasedness: Responses should not show unjustified preference, especially in subjective matters.\\n\\nGuidelines:\\n\\n1. Evaluate each response based on the criteria, noting strengths and weaknesses.\\n\\n2. Choose the most effective response or indicate a tie. Explain your reasoning in the specified JSON format.\\n\\n3. Remain objective, not letting the order of responses bias your evaluation.\\n\\nOutput format:\\n\\n1. Present your judgment in JSON format.\\n\\n2. Include the winner: Use an integer (1 if LLM-1 has the better response, 2 if LLM-2 has the better response, 0 for a tie if both responses are satisfactory, and -1 for a tie if both responses are unsatisfactory), and an explanation (providing a rationale for your choice).\\n\\nUser-submitted question: \\n\\nResponse of LLM-1: \\n\\nResponse of LLM-2: \\n\\nB. More Experimental Results\\n\\nDetailed Comparison with Alpaca Eval.\\nThe comparison between our RWQ-Elo and the Alpaca Eval is shown in Figure 5.\\n\\nThe Complete Statistics of Our RWQ-Elo System.\\nThis can be found in Figure 6. We include 24 models in total.\\n\\nWin-Rate Maps.\\nWe present the win-rate map generated by our RWQ-Elo system, the pre-calculated win-rate map, and their difference map in Figure 7, 8 and 9, respectively. 24 LLMs are included.\\n\\nDetailed Results for MCQA Evaluation.\\nTable 7-17 report the MCQA results on each benchmark using 7 evaluation strategies introduced in Section 3.\\n\\nQualitative Results Using GPT-4 as the Judge.\\nIn our RWQ-Elo system, GPT-4 acts as the judge. We utilize the prompt described in Section A.2 to evaluate responses from two different LLMs. Two instances where GPT-4 generates accurate justifications are presented in Table 18 and 19. Conversely, two examples where GPT-4 provides incorrect justifications can be found in Table 20 and 21.\\n\\nC. Examples from RWQ Benchmark\\n\\nOur RWQ benchmark comprises 20,772 authentic questions sourced from various platforms including Google Trends, Quora, ShareGPT, LMSYS-Chat-1M, and AlpacaEval. We show three examples for each source in Table 22.\"}"}
{"id": "3Cp042s1Nc", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                  | RWQ-Elo (All) | RWQ-Elo (200) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-1-13B) | AlpacaEval (LLaMA-1-7B) | AlpacaEval (LLaMA-1-33B) | AlpacaEval (GPT-4-Turbo) | AlpacaEval (GPT-3.5-Turbo) | AlpacaEval (LLaMA-"}
{"id": "3Cp042s1Nc", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                              | Score |\\n|-----------------------------------|-------|\\n| GPT-4 Turbo                       | 106   |\\n| Gemini-Pro                        | 604   |\\n| LLaMA-2-Chat-70B                  | 996   |\\n| LLaMA-2-Chat-13B                  | 901   |\\n| LLaMA-2-Chat-7B                   | 855   |\\n| Xwin-LM-13B-v0.1                  | 786   |\\n| WizLM-13B-v1.0                    | 760   |\\n| Mixed-7B-v0.1                     | 637   |\\n| Falcon-Instruct-40B               | 616   |\\n| MPT-Chat-30B                      | 665   |\\n| Alpaca-7B                         | 1235  |\\n| LLaMA-1-33B                       | 1267  |\\n| LLaMA-1-13B                       | 1311  |\\n| LLaMA-1-7B                        | 1235  |\\n| WizLM-7B-v1.0                     | 1169  |\\n| Alpaca-13B                        | 1104  |\\n| Vicuna-33B-v1.3                   | 1145  |\\n| Vicuna-13B-v1.5                   | 1162  |\\n| Vicuna-7B-v1.5                    | 1198  |\\n| Falcon-Instruct-7B                | 1208  |\\n| MPT-Chat-7B                       | 1142  |\\n| Alpaca-13B                        | 1114  |\\n| Vicuna-33B-v1.3                   | 1198  |\\n| Vicuna-13B-v1.5                   | 1162  |\\n| Vicuna-7B-v1.5                    | 1145  |\\n| Falcon-Instruct-40B               | 1212  |\\n| MPT-Chat-30B                      | 1186  |\\n| Alpaca-7B                         | 1124  |\\n| Vicuna-33B-v1.3                   | 1161  |\\n| Vicuna-13B-v1.5                   | 1079  |\\n| Vicuna-7B-v1.5                    | 1039  |\\n| Falcon-Instruct-7B                | 1114  |\\n| MPT-Chat-7B                       | 1114  |\\n| Alpaca-13B                        | 1114  |\\n| Vicuna-33B-v1.3                   | 1198  |\\n| Vicuna-13B-v1.5                   | 1162  |\\n| Vicuna-7B-v1.5                    | 1145  |\\n| Falcon-Instruct-40B               | 1212  |\\n| MPT-Chat-30B                      | 1186  |\\n| Alpaca-7B                         | 1124  |\\n| Vicuna-33B-v1.3                   | 1161  |\\n| Vicuna-13B-v1.5                   | 1079  |\\n| Vicuna-7B-v1.5                    | 1039  |\\n| Falcon-Instruct-7B                | 1114  |\\n| MPT-Chat-7B                       | 1114  |\\n| Alpaca-13B                        | 1114  |\\n| Vicuna-33B-v1.3                   | 1198  |\\n| Vicuna-13B-v1.5                   | 1162  |\\n| Vicuna-7B-v1.5                    | 1145  |\\n| Falcon-Instruct-40B               | 1212  |\\n| MPT-Chat-30B                      | 1186  |\\n| Alpaca-7B                         | 1124  |\\n| Vicuna-33B-v1.3                   | 1161  |\\n| Vicuna-13B-v1.5                   | 1079  |\\n| Vicuna-7B-v1.5                    | 1039  |\\n| Falcon-Instruct-7B                | 1114  |\\n| MPT-Chat-7B                       | 1114  |\\n| Alpaca-13B                        | 1114  |\"}"}
{"id": "3Cp042s1Nc", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7: Visualization of the complete win-rate map generated by our RWQ-Elo system.\"}"}
{"id": "3Cp042s1Nc", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: Visualization of the complete pre-calculated win-rate map.\"}"}
{"id": "3Cp042s1Nc", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                        | Win-Rate Difference |\\n|------------------------------|---------------------|\\n| Mixtral-Instruct-8x7B-v0.1   |                     |\\n| WizardLM-13B-v1.2            |                     |\\n| LLaMA-2-Chat-13B             |                     |\\n| LLaMA-2-Chat-70B             |                     |\\n| GPT-35-Turbo-1106            |                     |\\n| Xwin-LM-13B-v0.1             |                     |\\n| Falcon-Instruct-40B          |                     |\\n| WizardLM-7B-v1.0             |                     |\\n| LLaMA-1-33B                 |                     |\\n| LLaMA-1-13B                 |                     |\\n| LLaMA-1-7B                  |                     |\\n| Alpaca-7B                   |                     |\\n| Zephyr-7B-Beta              |                     |\\n| Gemini-Pro                  |                     |\\n| MPT-Chat-30B                |                     |\\n| LLaMA-1-13B                 |                     |\\n| LLaMA-1-33B                 |                     |\\n| MPT-Chat-7B                 |                     |\\n| LLaMA-1-7B                  |                     |\\n| Alpaca-13B                  |                     |\\n| Vicuna-7B-v1.5              |                     |\\n| Vicuna-13B-v1.5             |                     |\\n| Vicuna-33B-v1.3             |                     |\\n| 0.00 0.08 0.11 0.07 0.11 0.07 0.06 0.02 0.02 0.02 0.03 0.01 0.00 0.01 0.14 0.00 0.08 0.11 |\\n| 0.12 0.07 0.09 0.12 0.08 0.05 0.10 0.03 0.08 0.10 0.16 0.01 0.16 0.01 0.00 0.08 0.11 |\\n| 0.00 0.00 0.08 0.00 0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 |\\n| 0.14 0.00 0.00 0.14 0.11 0.11 |\\n| 0.00 0.00 0.00 0.14 0.11 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.11 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0.00 0.00 0.00 0.00 0.00 |\\n| 0.00 0"}
