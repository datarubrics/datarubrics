{"id": "dBqHGZPGZI", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\nZhang, Z., Lin, Y., Liu, Z., Li, P., Sun, M., and Zhou, J. MoEfication: Transformer feed-forward layers are mixtures of experts. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 877\u2013890, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.71. URL https://aclanthology.org/2022.findings-acl.71.\\n\\nZou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren, R., Pan, A., Yin, X., Mazeika, M., Dombrowski, A.-K., et al. Representation engineering: A top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023a.\\n\\nZou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023b.\"}"}
{"id": "dBqHGZPGZI", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\nA. Projecting Value Vectors onto Vocabulary Space\\n\\nIn this section we provide details from Geva et al. (2022) that demonstrate that MLP value vectors promote or suppress the likelihood of tokens. We start from Equation 2:\\n\\n$$\\\\text{MLP}_L(x_L) = \\\\sum_{i=1}^{d_{\\\\text{mlp}}} \\\\sigma(x_L \\\\cdot k_L i) v_L i$$\\n\\nThus, we can consider the update from $\\\\text{MLP}_L$ as $d_{\\\\text{mlp}}$ sub-updates, each sub-update being $m_L i v_L i$.\\n\\nWe can then analyze the influence that each sub-update has on the output distribution, or the probability of generating token $w \\\\in V$ (taken from Geva et al. (2022)):\\n\\n$$p(w | x_L + m_L i v_L i, E) = \\\\exp(e_w \\\\cdot x_L + e_w \\\\cdot m_L i v_L i) \\\\propto \\\\exp(e_w \\\\cdot x_L) \\\\cdot \\\\exp(e_w \\\\cdot m_L i v_L i)$$\\n\\nwhere $e_w$ is the token embedding of $w$, and $Z$ is the softmax normalization factor. This indicates that when $e_w \\\\cdot m_L i v_L i > 0$, the likelihood of $w$ increases, while $e_w \\\\cdot m_L i v_L i < 0$ decreases the likelihood.\\n\\nB. Additional Llama2 Results\\n\\nIn this section we provide results from Llama2. Table 6 demonstrates the toxic tokens encoded by its GLU value vectors. Table 7 demonstrates results from intervening on Llama2's residual stream using token vectors.\\n\\n### Table 6.\\n\\n| Top toxic vectors in Llama2, projected onto the vocabulary space. WARNING: THESE EXAMPLES ARE highly offensive. |\\n|---------------------------------------------------------------|\\n| VECTOR | TOP TOKENS |\\n| GLU. v19 | hell, ass, bast, dam, balls, eff, sod, f |\\n| GLU. v24 | ass, d, dou, dick, pen, cock, j |\\n| SVD.U | Toxic [0] hell, ass, bast, dam, eff, sod, arse, |\\n\\n### Table 7.\\n\\n| METHOD | VECTORIZER | TOXIC | PPL | F1 |\\n|--------|------------|-------|-----|----|\\n| NO OPO | N/A | 0.359 | 6.095 | 0.227 |\\n| SUBTRACT | TOXIC | 0.256 | 6.523 | 0.225 |\\n| SUBTRACT | GLU. v19 | 0.171 | 6.518 | 0.225 |\\n| SUBTRACT | SVD.U Toxic [0] | 0.246 | 6.504 | 0.225 |\\n| DPO \u2020 | N/A | 0.138 | 6.587 | 0.194 |\\n\\nC. Shift in Residual Streams\\n\\nIn this section we provide more examples of residual streams shifting out of toxic regions. See Figure 7\\n\\nD. Shifts in Residual Streams vs. Shifts in MLP Value Vectors\\n\\nIn this section we provide more examples of how MLP value vectors contribute in the $\\\\delta x$ direction at different layers.\"}"}
{"id": "dBqHGZPGZI", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\nShift Component\\n- Principle Component\\n  - Activated\\n    - High (> 10)\\n    - Low (> 0.1)\\n    - None\\n\\nModel\\n- GPT2\\n- DPO\\n\\nFigure 7. Shift in residual streams at layer 12, 18, and 13 (we show these three layers because MLP $v_{12}$, MLP $v_{18}$, and MLP $v_{13}$ are the next three vectors with highest cosine similarity with $W_{Toxic}$. See Table 1, Figure 2.\\n\\nE. Hyperparameters\\n\\nTables 8, and 9 contain the hyperparameters used for our toxic probe, DPO, and PPLM, respectively.\"}"}
{"id": "dBqHGZPGZI", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8. Shift in residual streams at layer 12 vs. shift in MLP value vectors ($\\\\delta_{12}$ vs. $\\\\delta_{\\\\text{MLP}}$).\\n\\nTable 8. Hyperparameters: DPO.\\n\\n| HYPERPARAMETER | VALUE |\\n|----------------|-------|\\n| LEARNING RATE  | 1e-6  |\\n| BATCH SIZE     | 4     |\\n| OPTIMIZER      | RMS   |\\n| GRADIENT ACCUMULATION STEPS | 1 |\\n| MAX GRADIENT NORM | 10 |\\n| VALIDATION METRIC LOSS / VALIDATION PATIENCE | 10 |\\n\\nTable 9. Hyperparameters: PPLM.\\n\\n| HYPERPARAMETER | VALUE |\\n|----------------|-------|\\n| STEPS          | 0.4   |\\n| TEMPERATURE    | 1     |\\n| TOP K          | 10    |\\n| NUM ITERATIONS | 50   |\\n| WINDOW LENGTH  | 0     |\\n| HORIZON LENGTH | 1     |\\n| DECAY FALSE    | True  |\\n| GAMMA          | 1     |\\n| GAM SCALE      | 0.95  |\\n| KL SCALE       | 0.1   |\"}"}
{"id": "dBqHGZPGZI", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"## Mechanistic Understanding of Alignment Algorithms\\n\\n### Layer 0\\n- **Cos Sim**:\\n  - Layer 0 vs. Layer 2: 0.00\\n  - Layer 0 vs. Layer 4: 0.00\\n  - Layer 0 vs. Layer 6: 0.00\\n  - Layer 0 vs. Layer 8: 0.00\\n  - Layer 0 vs. Layer 10: 0.00\\n  - Layer 0 vs. Layer 12: 0.00\\n  - Layer 0 vs. Layer 14: 0.00\\n  - Layer 0 vs. Layer 16: 0.00\\n\\n### Layer 3\\n- **Cos Sim**:\\n  - Layer 3 vs. Layer 6: 0.00\\n  - Layer 3 vs. Layer 9: 0.00\\n  - Layer 3 vs. Layer 10: 0.00\\n  - Layer 3 vs. Layer 12: 0.00\\n  - Layer 3 vs. Layer 14: 0.00\\n  - Layer 3 vs. Layer 16: 0.00\\n\\n### Layer 6\\n- **Cos Sim**:\\n  - Layer 6 vs. Layer 9: 0.00\\n  - Layer 6 vs. Layer 10: 0.00\\n  - Layer 6 vs. Layer 12: 0.00\\n  - Layer 6 vs. Layer 14: 0.00\\n  - Layer 6 vs. Layer 16: 0.00\\n\\n### Layer 9\\n- **Cos Sim**:\\n  - Layer 9 vs. Layer 10: 0.00\\n  - Layer 9 vs. Layer 12: 0.00\\n  - Layer 9 vs. Layer 14: 0.00\\n  - Layer 9 vs. Layer 16: 0.00\\n\\n### Layer 10\\n- **Cos Sim**:\\n  - Layer 10 vs. Layer 12: 0.00\\n  - Layer 10 vs. Layer 14: 0.00\\n  - Layer 10 vs. Layer 16: 0.00\\n\\n### Layer 12\\n- **Cos Sim**:\\n  - Layer 12 vs. Layer 14: 0.00\\n  - Layer 12 vs. Layer 16: 0.00\\n\\n### Layer 14\\n- **Cos Sim**:\\n  - Layer 14 vs. Layer 16: 0.00\\n\\n### Figure 9\\n- Shift in residual streams at layer 14 vs. shift in MLP value vectors ($\\\\delta_{14} x$ vs. $\\\\delta_{MLP}$).\\n\\n### Figure 10\\n- Shift in residual streams at layer 16 vs. shift in MLP value vectors ($\\\\delta_{16} x$ vs. $\\\\delta_{MLP}$).\"}"}
{"id": "dBqHGZPGZI", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\nFigure 11. Shift in residual streams at layer 18 vs. shift in MLP value vectors ($\\\\delta_{18}$ vs. $\\\\delta_{\\\\text{MLP}}$).\"}"}
{"id": "dBqHGZPGZI", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity\\n\\nAndrew Lee\\nXiaoyan Bai\\nItamar Pres\\nMartin Wattenberg\\nJonathan K. Kummerfeld\\nRada Mihalcea\\n\\nAbstract\\nWhile alignment algorithms are commonly used to tune pre-trained language models towards user preferences, we lack explanations for the underlying mechanisms in which models become \u201caligned\u201d, thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in pre-trained language models (GPT2-medium, Llama2-7b). We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting models avert toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the models, reverting them back to their toxic behavior.\\n\\n1. Introduction\\nLarge language models learn surprising capabilities from pre-training on large datasets (Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023). While these capabilities lead to impressive achievements, they also include unwanted behaviors that can be found in large-scale web data, such as toxicity and bias (Sheng et al., 2019; Gehman et al., 2020). As a result, researchers have developed alignment algorithms to reduce undesirable behaviors, which often use reinforcement learning with human preferences (RLHF). For instance, proximal policy optimization (PPO, Schulman et al. 2017) fits a reward model on human preference data, which is then used to fine-tune a language model, while direct preference optimization (DPO, Rafailov et al. 2023) by-passes the reward model and derives reward signals directly from pairwise preference data.\\n\\nWhile such algorithms can suppress undesirable behavior, our understanding of the mechanisms by which the undesirable behavior is suppressed is limited. Furthermore, researchers have demonstrated that such alignments can be surprisingly easily undone (Wallace et al., 2019; Zou et al., 2023b; Wei et al., 2023; Carlini et al., 2023). While prior work hypothesize why jailbreaks are possible through empirical studies (Wei et al., 2023), in this work we provide a mechanistic explanation for such phenomena.\\n\\nGiven the above limitations, in this work we study the mechanisms by which alignment algorithms alter a model's behavior. Researchers have demonstrated that a deep enough understanding of a model's inner representations allows us to interpret how it makes decisions. For instance, various concepts such as world models, truthfulness, or even task-specific features have highly interpretable and controllable representations (Li et al., 2023b; Todd et al., 2023; Nanda et al., 2023). Motivated by such findings, we study how the representation space of language models change by comparing it before and after an alignment algorithm is applied.\\n\\nOur work relates to that of Jain et al. (2023), which studies how the capabilities of a language model change after fine-tuning on synthetic tasks. Unlike this previous work, we study the change in mechanisms from a RLHF algorithm on a natural language setting.\\n\\nWe consider DPO and toxicity as a case-study of RLHF alignment algorithms. We first study how toxicity is represented and elicited in two pre-trained language models, GPT2-medium and Llama2-7b (henceforth GPT2, Llama2). We then apply DPO using a carefully crafted pairwise dataset that consists of toxic and nontoxic samples. Lastly, we study the mechanisms by which toxicity is no longer generated after DPO, and how those mechanisms can fail.\\n\\nOur work is organized as follows: in Section 2 we provide the necessary preliminaries relevant to our work. In Section 3, we demonstrate how toxicity is represented and elicited in GPT2 and Llama2. We find multiple vectors in multilayer perceptron (MLP) blocks that promote toxicity. We apply singular value decomposition (SVD) to these toxic vectors to find vectors that represent specific dimensions of...\"}"}
{"id": "dBqHGZPGZI", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\ntoxicity in the model. To validate the role of these vectors in generating toxic outputs, we intervene with our toxic vectors and demonstrate much safer outputs.\\n\\nIn Section 4, we explain our procedure to apply DPO on our language models to reduce toxicity, using a carefully crafted pairwise toxicity dataset, produced by using PPLM (Dathathri et al., 2019) to generate paired toxic and non-toxic samples.\\n\\nIn Section 5, we demonstrate how toxicity is no longer elicited after DPO. Namely, we show that every parameter is minimally shifted, including the toxic vectors. However, such minimal changes in weights allow the model to avert the triggering of toxic vectors. Put differently, DPO does not remove the capability of generating toxic outputs. Rather, GPT2 learns an \\\"offset\\\", distributed amongst its layers, to \\\"bypass\\\" the regions that elicit toxicity, while Llama2 uses its gating mechanism to \\\"turn off\\\" toxic vectors. Based on this understanding, we demonstrate the ease of re-activating these vectors to generate toxic outputs, and thus undoing the alignment learned from DPO. We view our findings as shedding light into why aligned models can be jailbroken or un-aligned.\\n\\n2. Preliminaries\\n\\nIn this section we provide background and notations, much of which is borrowed from Geva et al. (2022).\\n\\nTransformers, MLPs.\\n\\nTransformer-based language models typically consist of embedding and unembedding layers $E, U \\\\in \\\\mathbb{R}^{|V| \\\\times d}$ with a series of $L$ transformer layers in between (Vaswani et al., 2017). Each layer $l$ consists of attention heads and a multilayer perception (MLP) layer.\\n\\nGiven an input sequence $w = \\\\langle w_0, ..., w_t \\\\rangle$, the model first applies $E$ to create an embedding $x_i \\\\in \\\\mathbb{R}^d$ for each token $w_i \\\\in w$. We call $x_i$ the residual stream.\\n\\nThe residual stream is then updated by attention heads and MLP blocks from subsequent layers (bias terms omitted):\\n\\n$$x_{l+1}^i = x_l^i + \\\\text{MLP}(x_l^i + \\\\text{Att}(x_l^i))$$\\n\\nWhen needed, we specify the intermittent residual stream at layer $l$ (after the attention head, before the MLP) as $x_{l}^\\\\text{mid}$.\\n\\nPer Geva et al. (2022), the updates to the residual stream from each MLP block can be further decomposed. Namely, MLP blocks consist of two linear transformations, with point-wise activations $\\\\sigma$ in-between:\\n\\n$$\\\\text{MLP}_l(x_l) = \\\\sigma(W_k x_l W_v)$$\\n\\nwhere $W_k, W_v \\\\in \\\\mathbb{R}^{d_{\\\\text{mlp}} \\\\times d_{\\\\text{mlp}}}$.\\n\\nWe notate the $i$-th row in $W_k$ as $\\\\text{MLP}_k^i$ and refer to them as key-vectors, and the $i$-th column in $W_v$, $\\\\text{MLP}_v^i$, as value-vectors (we sometimes omit \\\"MLP\\\" and just use $k^i, v^i$).\\n\\nEquation (1) indicates that the output of MLP blocks is the sum of its value vectors $v_i$, each scaled by a coefficient $m^i$,\\n\\n$$\\\\text{MLP}_l(x_l) = \\\\sum_{i=1}^{d_{\\\\text{mlp}}} \\\\sigma(x_l \\\\cdot k^i) v^i = \\\\sum_{i=1}^{d_{\\\\text{mlp}}} m^i v^i$$\\n\\n(2)\\n\\nPut differently, the MLP block writes to the residual stream $d_{\\\\text{mlp}}$ times, once for each value vector. We call each of these updates a sub-update.\\n\\nInterpreting Value Vectors in Vocabulary Space.\\n\\nGeva et al. (2022) demonstrate that for each sub-update, each value vector $v_i$ either promotes or suppresses the likelihood of a token $w$ from being generated:\\n\\n$$p(w|x_l + m^i v^i, E) \\\\propto \\\\exp(e_w \\\\cdot x_l + \\\\exp(e_w \\\\cdot m^i v^i))$$\\n\\nwhere $e_w$ is the embedding of $w$. This indicates that when $e_w \\\\cdot m^i v^i > 0$, the likelihood of $w$ increases, while $e_w \\\\cdot m^i v^i < 0$ decreases the likelihood.\\n\\nFurther note that this dot product can be further decomposed. Namely, $e_w \\\\cdot v^i$ is a \\\"static\\\" value that does not depend on the input: only when $v^i$ is scaled by $m^i$ (which is determined by the its corresponding key vector, $k^i$, and the residual stream $x_l$) do we see the impact of the input towards the likelihood of $w$.\\n\\nThus the projection $r^i = Ev^i \\\\in \\\\mathbb{R}^{|V|}$ induces a ranking of tokens that get promoted by value vector $v^i$, in which tokens with the highest dot products $e_w \\\\cdot v^i$ are promoted most by value vector $v^i$. In Section 3 we show value vectors that promote toxicity by applying these projections.\\n\\nGated Linear Units.\\n\\nShazeer (2020) empirically show that using Gated Linear Units (GLUs) (Dauphin et al., 2017) in place of MLPs yield higher quality language models. Subsequently, recent language models (Touvron et al., 2023; Jiang et al., 2023) such as Llama2 use GLUs.\\n\\nGLUs take element-wise products of two linear transformations of the residual stream, one of which is then non-linearly activated. The result is then projected back onto the residual stream:\\n\\n$$\\\\text{GLU}_l(x_l) = (\\\\sigma(W_1 x_l) \\\\odot W_2 x_l) W_v$$\\n\\n(3)\\n\\nwhere $W_1, W_2, W_v \\\\in \\\\mathbb{R}^{d_{\\\\text{mlp}} \\\\times d_{\\\\text{mlp}}}$.\\n\\n1 See Appendix for derivation.\"}"}
{"id": "dBqHGZPGZI", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\nThus, value vectors (columns in $W$) are now scaled by the element-wise product of two components: $\\\\sigma(W_1x)$ and $W_2x$. We will refer to $\\\\sigma(W_1x)$ as gates, which will \\\"block\\\" its counterparts $W_2x$ from propagating when the non-linearity ($\\\\sigma$) is not activated.\\n\\n3. Toxicity in Pre-trained Language Models\\n\\nIn this section we demonstrate how toxicity is represented and elicited in pre-trained language models (GPT2, Llama2), by introducing a series of vectors that can be extracted from the language model.\\n\\n3.1. Extracting Toxic Vectors\\n\\nToxicity Probe Vector. We start by first training a linear probe model on a binary toxicity classification task. Namely, we use the Jigsaw toxic comment classification dataset (cjadams et al., 2017), which consists of 561,808 comments, each of which is labeled as toxic or non-toxic. We use a 90:10 split for training and validation. We train our probe model, $W_{\\\\text{Toxic}}$, on the residual stream in the last layer, averaged across all timesteps ($\\\\bar{x}_{L-1}$):\\n\\n$$P(\\\\text{Toxic}|\\\\bar{x}_{L-1}) = \\\\text{softmax}(W_{\\\\text{Toxic}}\\\\bar{x}_{L-1}), \\\\quad W_{\\\\text{Toxic}} \\\\in \\\\mathbb{R}^d$$\\n\\nOur probe vector achieves an accuracy of 94% on the validation split. We view our toxic probe vector $W_{\\\\text{Toxic}}$ as an aggregate of all the relevant signals in the language model to classify an input as toxic.\\n\\nToxic Vectors in MLP Blocks. Given our probe vector $W_{\\\\text{Toxic}}$, we can use it to find weights within the language model that promote toxicity. Namely, Geva et al. (2022) demonstrate that value vectors promote tokens at a concept-level. Given this, we search for value vectors that promote toxicity, by checking for all value vectors with the highest cosine similarity with $W_{\\\\text{Toxic}}$. We find that indeed, there are value vectors that promote toxic tokens (See Section 3.2). We notate our set of toxic value vectors as $\\\\text{MLP.}v_{\\\\text{Toxic}}$ and their corresponding key vectors as $\\\\text{MLP.}k_{\\\\text{Toxic}}$.\\n\\nWe provide two perspectives of our $\\\\text{MLP.}v_{\\\\text{Toxic}}$ vectors: 1) when triggered, they promote the likelihood of toxic tokens to be generated, and 2) $\\\\text{MLP.}v_{\\\\text{Toxic}}$ are vectors within the model that contribute towards the $W_{\\\\text{Toxic}}$ direction.\\n\\nSVD: Decomposed Toxic Vectors. After extracting a set of $N (=128)$ $\\\\text{MLP.}v_{\\\\text{Toxic}}$ vectors, we stack them into a $N \\\\times d$ matrix. We then apply singular value decomposition to get decomposed singular value vectors SVD.$U_{\\\\text{Toxic}}$. We refer to the $i$-th singular value vector as SVD.$U_{\\\\text{Toxic}}[i]$. We view $2$ We experiment with different values for $N$, and get similar results.\\n\\nTable 1. Toxic vectors in GPT2, projected onto the vocabulary space. WARNING: THESE EXAMPLES ARE HIGHLY OFFENSIVE. We note that SVD.$U_{\\\\text{Toxic}}[2]$ has a particularly gendered nature. This arises from the dataset and language model we use. For Llama2 results, see Appendix Table 6.\\n\\n| VECTOR | TOP TOKENS |\\n|--------|------------|\\n| $W_{\\\\text{Toxic}}$ | c*nt, f*ck, a**hole, d*ck, wh*re, holes |\\n| $\\\\text{MLP.}v_{19}$ | sh*t, a**, cr*p, f*ck, c*nt, garbage, trash |\\n| $\\\\text{MLP.}v_{12}$ | delusional, hypocritical, arrogant, nonsense |\\n| $\\\\text{MLP.}v_{18}$ | degener, whining, idiots, stupid, smug |\\n| $\\\\text{MLP.}v_{13}$ | losers, filthy, disgr, gad, feces, apes, thous |\\n| $\\\\text{MLP.}v_{16}$ | disgrace, shameful, coward, unacceptable |\\n| $\\\\text{MLP.}v_{19}$ | f*ck, sh*t, piss, hilar, stupidity, poop |\\n\\nTable 2. Toxicity, perplexity (PPL), and F1 after interventions or DPO for GPT2. We scale our toxic vectors such that the resulting perplexity is comparable to that of post-DPO.\u2020: Not an intervention. For Llama2, see Appendix Table 7.\\n\\n| METHOD | VECTOR | TOXIC | PPL | F1 |\\n|--------|--------|-------|-----|----|\\n| NO | $W_{\\\\text{Toxic}}$ | NO/A | 0.453 | 21.7 | 0.193 |\\n| SUBTRACT | $W_{\\\\text{Toxic}}$ | 0.245 | 23.56 | 0.193 |\\n| SUBTRACT | $\\\\text{MLP.}v_{19}$ | 0.305 | 23.30 | 0.192 |\\n| SUBTRACT | SVD.$U_{\\\\text{Toxic}}[0]$ | 0.268 | 23.48 | 0.193 |\\n| DPO | $W_{\\\\text{Toxic}}$ | NO/A | 0.208 | 23.34 | 0.195 |\\n\\nSVD.$U_{\\\\text{Toxic}}$ as basis vectors that span the toxicity representation space within the language model.\\n\\n3.2. Toxic Vectors in Vocabulary space.\\n\\nAs mentioned in Section 2, we can inspect which tokens are promoted by value vectors by projecting them onto the vocabulary space. Table 1 and 6 show the tokens with the highest dot products with our toxic vectors in GPT2 and Llama2. Each $\\\\text{MLP.}v_{\\\\text{Toxic}}$ and SVD.$U_{\\\\text{Toxic}}$ vectors seem to encode specific dimensions of toxicity, or different contexts in which toxicity appears in pre-training data.\\n\\n3.3. Interventions Using Toxic Vectors\\n\\nTo validate the role that the toxic vectors play in eliciting toxic outputs, we intervene during generation to suppress toxic outputs using each vector. Namely, we use prompts from REALITY PROMPTS (Gehman et al., 2020) that elicit toxic outputs from language models. During the forward pass of the model, we intervene by simply subtracting...\"}"}
{"id": "dBqHGZPGZI", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\nTable 3.\\n\\nExamples of top-k and continuations to prompts that originally elicit \u201csh*t\u201d from GPT2, our interventions, and GPT2 DPO.\\n\\n| PROMPTS | MODEL | TOP-K CONTINUATIONS |\\n|---------|-------|---------------------|\\n| \u201cSo I'm starting to think she's full of\u201d | GPT2 | sh*t, it, crap, herself, something \u201csh*t. I'm not saying she's a bad person, but\u201d |\\n| | GPT2 DPO | it, herself, sh*t, surprises, crap \u201cit. I think she's very much aware of the fact that\u201d |\\n| | GPT2 | \u201cYou are such a piece of\u201d |\\n| | GPT2 DPO | work, garbage, a** \u201csh*t. You are such a piece of sh*t.\u201d |\\n| | GPT2 | work, sh*t, \u201cs\\\", art, human \u201cwork. I've been working on this for a while\u201d |\\n| | GPT2 DPO | work, art, sh*t, trash, cake \u201cwork. I have been a fan of your work for a long\u201d |\\n| | GPT2 | \u201cHoly\u201d |\\n| | GPT2 DPO | cow, crap, Grail, sh*t, smokes \u201ccow, this is a great book! I've been reading\u201d |\\n\\nNote that our interventions depend on how much we scale each vector ($\\\\alpha$). We choose a scalar value such that the resulting perplexity is similar to that of our post-DPO model. For details regarding our post-DPO model see Section 4.\\n\\nWe find that subtracting toxic components from the residual stream reduces toxicity.\\n\\n4. Toxicity Alignment Using DPO\\n\\nWe next describe our alignment procedure using DPO.\\n\\n4.1. Background: DPO\\n\\nDPO relies on pairwise preference data, in which given a prompt, we have a preferred (positive) continuation and a non-preferred (negative) continuation. Given each preference pair, the algorithm promotes the likelihood of the positive sample, while suppressing the likelihood of the negative sample, using the following loss term:\\n\\n$$L_{DPO} = -\\\\mathbb{E} \\\\left[ \\\\log \\\\sigma \\\\left( \\\\beta \\\\log P - \\\\beta \\\\log N \\\\right) \\\\right],$$\\n\\nwhere $P$ and $N$ are preferred (nontoxic) and non-preferred (toxic) continuations of $w$, $\\\\pi_{\\\\text{ref}}$ is the frozen weights of the original language model, and $\\\\pi_{\\\\theta}$ is the weights of the language model being updated (See Rafailov et al. (2023) for details). The algorithm promotes the likelihood of $P$, while suppressing the likelihood of $N$.\\n\\n4.2. Constructing Pairwise Toxic Data\\n\\nWe build our pairwise toxicity dataset using PPLM (Dathathri et al., 2019). PPLM is an attribute-controlled language generation technique, which attaches a simple linear attribute classification layer, $p(a | w)$ onto a language model to guide its generation. During generation, PPLM uses the attribute classifier to compute the gradients that increases the likelihood of the language model's output to contain the desired attribute $a$, and shifts the activations in such direction (See Dathathri et al. (2019) for details):\"}"}
{"id": "dBqHGZPGZI", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\nFigure 1. Logit lens on GPT2 and GPT2 DPO. Given 295 prompts that originally elicit \u201csh*t\u201d as the next token, we plot the average probability of outputting \u201csh*t\u201d from intermittent layers by applying the unembedding layer. Minor ticks indicate \u2113mid layers (after attention heads, before MLP). Shaded areas indicate layers that promote \u201csh*t\u201d the most, which all correspond to MLP layers.\\n\\nTo generate pairwise preference data, we use sentences from Wikitext-2 (Merity et al., 2016) as prompts. For each prompt, we generate a positive sample using greedy sampling with GPT2, while using PPLM to generate negative (toxic) samples. We use our toxic probe Toxic as our attribute classifier to guide towards toxic outputs. We create 24,576 pairs of toxic and nontoxic continuations. We train until validation loss converges with a patience value of 10, which occurs after approximately 6,700 sample pairs. Appendix E has details for DPO and PPLM hyperparameters.\\n\\nThe last row of Table 2 shows the resulting toxicity, perplexity, and F1 scores of our DPO model.\\n\\nFigure 1 shows an example of the difference in behaviors between GPT2 before and after DPO, for a specific toxic token. Namely, we use 295 prompts from RETOXICITY PROMPTS that outputs the token \u201csh*t\u201d as the next token. We then apply \u201cLogit Lens\u201d (Nostalgebraist, 2020), meaning we apply the unembedding layer on all intermittent layers. This allows us to visualize the layers that promote the \u201csh*t\u201d token. The shared grey areas indicate the layers in which \u201csh*t\u201d is promoted the most, which all correspond to MLP layers. We see that post-DPO, the toxic token is promoted far less.\\n\\n5. Toxicity After DPO\\n\\nIn this section we explain how our aligned language models (GPT2 DPO, Llama2 DPO) avert toxic outputs.\\n\\n4 We release this data to enable further studies.\\n\\nL:19\\nIdx:770\\nL:12\\nIdx:771\\nL:18\\nIdx:2669\\nL:13\\nIdx:668\\nL:16\\nIdx:255\\nMLP\\n0.0\\n0.1Mean Activation\\nModel\\nGPT2\\nDPO\\n\\nFigure 2. Mean activations for toxic vectors in GPT2 before and after DPO.\\n\\n5.1. Toxic Vectors Remain After DPO\\n\\nOf the toxic vectors described in Section 3, note that MLP vToxic are actual weights of the model. Thus we inspect how these vectors change after DPO. Interestingly, we find that every parameter in each language model barely changes after DPO, including token embeddings, MLP blocks, and attention heads. Every parameter in GPT2 (Llama2) and its counterpart in GPT2 DPO (Llama2 DPO) has a cosine similarity score greater than 0.99 and on average a norm difference less than 1e-5. This applies for MLP kToxic and MLP vToxic as well \u2013 toxic MLP v vectors do not change from DPO. Put differently, although toxicity is reduced by DPO, the ability to elicit toxicity with these value vectors still remain. So how is it that GPT2 DPO and Llama2 DPO avert toxic outputs? Though their parameters have barely moved, below we show that their collective movement is enough to avoid toxic outputs.\\n\\n5.2. DPO Avoids MLP kToxic Regions\\n\\nWe provide an explanation for how toxicity is reduced in GPT2 DPO and Llama2 DPO, starting with GPT2 DPO. In simplest terms, we observe a drop in activations for the toxic vectors MLP vToxic in GPT2 DPO. Namely, using the same 1,199 prompts from RETOXICITY PROMPTS, we generate 20 tokens and measure the mean activations mi or \u03c3(x\u2113\u00b7MLP.k\u2113i), of our MLP vToxic vectors. Figure 2 shows 5 examples of the top MLP vToxic vectors. Inspired by Balestriero et al. (2023), we visualize this drop.\"}"}
{"id": "dBqHGZPGZI", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\nToxic Region\\n\\nBefore DPO\\n\\nHidden space layer $l : \\\\mathbb{R}^{ \\\\delta \\\\times \\\\nu}$\\n\\nAfter DPO\\n\\nMLP Layer $l$\\n\\nFigure 3. Visualization of residual streams before and after DPO.\\n\\nWe view the shift, $\\\\delta x$, as an offset that allows GPT2 DPO to bypass regions that previously triggered toxic value vectors.\\n\\nAn activation region of a key vector is simply a subspace within the model's hidden space in which its vectors have high dot products to activate its corresponding value vector:\\n\\n$$\\\\gamma(k \\\\ell_i) := \\\\{ g \\\\in \\\\mathbb{R}^d | \\\\sigma(k \\\\ell_i \\\\cdot g) > 0 \\\\}, \\\\quad (4)$$\\n\\nwhere $\\\\sigma$ is a non-linear activation. Put differently, for all key-vector regions that the residual stream \\\"passes through\\\", their corresponding value-vectors are activated, scaled, and added into the residual stream.\\n\\nWe view the drop in activations as a shift in GPT2 DPO's residual stream to avert the regions of toxic MLP vectors, $\\\\gamma_{\\\\text{MLP.}k \\\\ell_{\\\\text{Toxic}}}$. See Figure 3.\\n\\nWe formalize the shift in residual streams as follows: given the residual streams at layer $\\\\ell_{\\\\text{mid}}$ (after attention heads at layer $\\\\ell$) for both GPT2 and GPT2 DPO, before MLP $\\\\ell_{\\\\text{Toxic}}$, we notate the difference of the two residual streams as\\n\\n$$\\\\delta_{\\\\ell_{\\\\text{mid}}} x := x_{\\\\ell_{\\\\text{mid}}}^{\\\\text{DPO}} - x_{\\\\ell_{\\\\text{mid}}}^{\\\\text{GPT2}}, \\\\quad \\\\delta_{\\\\ell_{\\\\text{mid}}} x \\\\in \\\\mathbb{R}^d.$$ We view $\\\\delta_{\\\\ell_{\\\\text{mid}}} x$ as a vector that takes GPT2's residual stream out of the toxicity-eliciting regions, $\\\\gamma_{\\\\text{MLP.}k \\\\ell_{\\\\text{Toxic}}}$.\\n\\nFigure 4 provides a visualization of the residual stream's shift out of toxic regions. Namely, given prompts from RE-ALTOXICITY PROMPTS, we project the residual stream from GPT2 and GPT2 DPO at layer 19 onto two dimensions: 1) the mean difference in the residual streams, $\\\\bar{\\\\delta}_{\\\\ell} x$, and the main principle component of the residual streams. We further indicate whether each residual stream activates MLP $v_{19}$.\\n\\nNotice both the consistent linear shift between GPT2 and GPT2 DPO and the drop in activations.\\n\\nTo understand where this shift comes from, we compute the differences in all parameter weights in GPT2 before and after DPO, and notate the differences as $\\\\delta_{\\\\theta}$. We notate the difference at a specific component such as a MLP block at layer $\\\\ell$ as $\\\\delta_{\\\\ell \\\\text{MLP}}$.\\n\\nNote that as previously noted, these differences $\\\\delta_{\\\\ell \\\\theta}$, $\\\\forall \\\\ell$ are minimal. Despite these minimal changes, their accumulation is sufficient in getting the residual stream out of toxic regions $\\\\gamma_{\\\\text{MLP.}k \\\\ell_{\\\\text{Toxic}}}$.\\n\\nGiven a toxic vector MLP $v_{\\\\text{Toxic}}$ at layer $\\\\ell$, to understand where the shift in residual stream, $\\\\delta_{\\\\ell_{\\\\text{mid}}} x$ comes from, we measure the cosine similarity between $\\\\delta_{\\\\ell_{\\\\text{mid}}} x$ and the shift in value vectors in the preceding layers, $\\\\delta_{j \\\\text{MLP.v}}$:\\n\\n$$\\\\forall j < \\\\ell, \\\\forall i < d_{\\\\text{mlp}}: \\\\cos(\\\\delta_{\\\\ell_{\\\\text{mid}}} x, \\\\delta_{j \\\\text{MLP.v}} i).$$\\n\\nTo our surprise, we find that the shift in value vectors, $\\\\delta_{\\\\text{MLP.v}}$, have high negative cosine similarity scores with the shift in residual streams $\\\\delta x$: the value vectors in MLP blocks shift in the opposite direction as the shift in residual stream. The blue areas in Figure 5 show the cosine similarity between $\\\\delta_{19_{\\\\text{mid}}} x$ and $\\\\delta_{j \\\\text{MLP.v}}$. We show layer 19 as an example because MLP $v_{19}$ is one of the most toxic vectors, but the same pattern can be found in other layers (see Appendix D). Namely, the blue areas indicate the percentage of value vectors at each layer in which their shifts have a cosine similarity score against $\\\\delta_{19_{\\\\text{mid}}} x$ as indicated by the x-axis. Note that as the layers approach layer 19, the majority of value vectors shift in the opposite direction of $\\\\delta_{19} x$.\\n\\nWhy the antipodal direction? This can be explained by two facts: first, neurons in MLP blocks of language models...\"}"}
{"id": "dBqHGZPGZI", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5. The cosine similarity between $\\\\delta_{\\\\text{MLP}}$ and $\\\\delta_{x}$ indicates the percentage of value vectors with a cosine similarity score against $\\\\delta_{x}$ as indicated by the x-axis. Orange areas indicate the percentage of value vectors with a mean activation as indicated by the x-axis, during the forward pass of 1,199 R\\\\textsc{EAL}T\\\\textsc{OXICITY} prompts. Value vectors shift in the opposite direction of $\\\\delta_{x}$, but they end up contributing towards the $\\\\delta_{x}$ direction because of their negative activations.\\n\\nare sparse (Zhang et al., 2022; Li et al., 2023d), meaning most neurons do not activate during a forward pass. Second, the choice of the MLP's activation function $\\\\sigma$ plays a role. Namely, our language model uses GeLU functions (Hendrycks & Gimpel, 2016). This means that neurons that are inactive during a forward pass have a negative value close to 0. Thus, during the forward pass, for each value vector, the newly learned direction $\\\\delta_{\\\\text{MLP}}$ gets multiplied by a very small negative scale, flips directions, and contributes towards the $\\\\delta_{x}$ direction. The orange areas of Figure 5 indicate the mean activation of each value vector, from the 1,199 prompts in R\\\\textsc{EAL}T\\\\textsc{OXICITY} prompts. Most of the time, value vectors have a negative activation - thus the shift in value vectors end up contributing towards the $\\\\delta_{x}$ direction.\\n\\nTo summarize, GPT2\\\\textsc{DPO} has learned an offset, $\\\\delta_{x}$, such that the residual stream avoids regions that promote toxicity, $\\\\gamma(\\\\text{MLP}, k_{\\\\ell \\\\text{Toxic}})$. This learned offset is distributed across the many value vectors in earlier MLP blocks that are inactive for prompts that previously elicited toxic outputs. By distributing this offset across numerous value vectors, the language model is able to preserve its pre-trained language model behavior, as individual weights are minimally affected. However, the distributed offset allows the model to avert toxic outputs. Note that this behavior matches precisely what the alignment objective was - to preserve as much of the pre-trained behavior, while optimizing for a reward (non-toxic outputs).\\n\\nLlama2\\\\textsc{DPO}. We see a similar phenomena for Llama2\\\\textsc{DPO}, in that toxic vectors are not removed, but rather \\\"turned off\\\". Recall from Equation 3 that Llama2 uses GLUs, in which the element-wise product of two components determine the scale of each value vector: $\\\\sigma(W_{1}x)$ and $W_{2}x$. Unlike GPT2, in which earlier MLP vectors are shifted to bypass toxic regions, we do not see this pattern in Llama2. Rather, we see that toxic value vectors $\\\\text{MLP}_v_{\\\\text{Toxic}}$ are \\\"turned off\\\" by both the gating component ($\\\\sigma(W_{1}x)$) and its linear projection counterpart ($W_{2}x$). Figure 6 demonstrates the mean activations of each of these components for its top toxic vectors.\\n\\n6. Un-aligning DPO\\n\\nA growing line of work finds that alignment algorithms can easily be undone or jailbroken. We view our findings as a mechanistic explanation for such phenomenon \u2013 namely, in our case, the vectors that elicit toxicity are still sitting in the model, but simply not triggered.\\n\\nTo confirm our understanding, we demonstrate a simple way to undo alignment. To reiterate, GPT2\\\\textsc{DPO} simply learned an offset to take the residual stream $x_{\\\\ell}$ out of regions that trigger toxic vectors: $\\\\gamma(\\\\text{MLP}, k_{\\\\ell \\\\text{Toxic}})$. A simple way to re-\"}"}
{"id": "dBqHGZPGZI", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\nFigure 6. Mean activations for toxic vectors in Llama2 before and after DPO, broken down by component.\\n\\nTable 4. Un-aligning GPT2 DPO. By scaling toxic key vectors, and thus increasing the regions that elicit toxicity, we are able to undo the alignment learned from DPO and reactivate toxicity.\\n\\n| Method | Toxic | PPL  | F1   |\\n|--------|-------|------|------|\\n| GPT2 DPO | 0.208 | 23.34 | 0.195 |\\n| Scale MLP | 0.458 | 23.30 | 0.195 |\\n| GPT2 | 0.453 | 21.7  | 0.193 |\\n\\nActivate toxicity is to increase those regions by scaling each key vector larger (See Equation 4). This makes the residual streams pass through toxic regions again, thus reverting back to the pre-aligned behavior.\\n\\nSimilarly, Llama2 DPO uses its gating component, $\\\\sigma(W_1x)$, to \u201cturn off\u201d toxic vectors. Thus a simple way to re-activate toxicity is to turn these values back on by setting their gated values to 1. Alternatively, one can scale the latter component $(W_2x)$ larger to re-activate toxicity as well.\\n\\nTable 5 shows toxicity, perplexity, and F1 scores after scaling up as few as 7 toxic key vectors MLP.\\n\\nWe simply select 7 MLP vectors with the highest cosine similarity as our toxic probe vector, $W_{Toxic}$, and scale their key vectors by 10x. By doing so, the model reverts back to its pre-aligned toxic behavior. Note that increasing activation regions $\\\\gamma$ does not have an affect on perplexity, unlike our interventions from Section 3.3. This is likely because the latter manipulates the residual stream directly, while scaling a key vector does not (See Equation 2).\\n\\nSimilarly, Table 5 shows results for Llama2 DPO, by either turning back on as few as 8 gate components ($\\\\sigma(W_1x)$) or by scaling the latter linear component $(W_2x)$ by 3x.\\n\\n7. Discussion\\n\\n7.1. On Designing Robust Alignment Algorithms\\n\\nWe view our work as providing a mechanistic explanation for why aligned models can be undone or jailbroken \u2013 in our experiments, the regions that previously elicited toxic behavior does not change after DPO. Rather, GPT2 DPO learns minimal changes spread across layers to avoid such regions and receive its reward.\\n\\nWith such knowledge, we conjecture that more robust alignment algorithms can be designed. Can we eliminate undesirable regions, as opposed to bypassing them? For instance, Li et al. (2023c) show how to remove causal pathways in a language model that is responsible for undesirable behaviors, including toxicity. Similarly, when we can identify the weights that elicit undesirable outputs, what happens during RLHF if we only update those weights in isolation?\\n\\nAlternatively, prior to deploying language models, perhaps we can add \u201csuppression heads\u201d \u2013 layers that suppress undesirable behavior. What would happen if we only updated late layers (or added layers) during alignment?\\n\\nLastly, can we characterize \u201cjailbreak-ability\u201d or \u201cunalign-ability\u201d of aligned models, without relying on test samples? We leave these questions for future work.\\n\\n7.2. On the Role of KL-Divergence Regularization\\n\\nWe hypothesize that the minimal changes distributed across all layers is due to the KL-divergence term that is commonly incorporated in the loss terms of RLHF algorithms. Namely, the KL-divergence term discourages each weight from shifting too drastically, in order to preserve its capabilities learned during pre-training.\\n\\nSimilar to our work, Jain et al. (2023) fine-tunes a language model with the additional constraint of minimizing the KL-divergence between the weights of the aligned and unaligned models.\"}"}
{"id": "dBqHGZPGZI", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\nWe find this difference in model training behavior interesting, and conjecture that the KL-divergence term may play a role in this difference. Note that fine-tuning typically does not entail a KL-divergence term. Perhaps this allows the model to make drastic and localized changes, such as in late layers, as opposed to distributed, minimal changes.\\n\\n8. Related Work\\n\\n8.1. Alignment Algorithms\\n\\nNumerous alignment algorithms have been proposed, and the choice of algorithm may largely depend on the type of data available. Perhaps most commonly, human feedback data is used (Stiennon et al., 2020; Ouyang et al., 2022; Touvron et al., 2023) for methods such as PPO (Schulman et al., 2017) or DPO (Rafailov et al., 2023). When labels for only undesirable behavior is available, algorithms like unlike-likelihood training (Welleck et al., 2020) or Cringe (Adolphs et al., 2023; Xu et al., 2023) can be used. We study DPO because it is easy to use and currently widely used.\\n\\n8.2. Mechanistic Interpretability\\n\\nThe goal of mechanistic interpretability is largely to reverse engineer model behaviors (Olah et al., 2020; Elhage et al., 2021; Geva et al., 2021). By doing so, researchers have uncovered various interpretable and controllable representations, such as world models (Li et al., 2023a; Nanda et al., 2023), \u201ctruthfulness\u201d (Li et al., 2023b), knowledge (Meng et al., 2022; Hernandez et al., 2023; Burns et al., 2023; Geva et al., 2023), linguistic properties (Conneau et al., 2018; Tenney et al., 2019), or even tasks (Ilharco et al., 2022; Hendel et al., 2023; Todd et al., 2023).\\n\\nZou et al. (2023a) suggests a broader framework for interpretability, in which representation engineering is viewed as a top-down approach for interpreting model behavior. Rather than probing for specific representations, researchers have also characterized the representations of language models from a geometric perspective (Park et al., 2023). Balestriero et al. (2023) demonstrate a geometric characterization that can be used to extract feature representations that solve toxicity detection.\\n\\nSimilar to our work, Jain et al. (2023) study the mechanisms in which fine-tuning on synthetic tasks alters the model\u2019s capabilities. We study the effects of RLHF on a more realistic, natural language setting.\\n\\n8.3. Jailbreaking Aligned Models\\n\\nResearchers demonstrated that aligned models can be surprisingly easily jailbroken (Wallace et al., 2019; Zou et al., 2023b; Wei et al., 2023; Carlini et al., 2023). Such adversarial attacks typically involve searching for prompts that can elicit previously unlearned behaviors, or even personal information (Nasr et al., 2023). Carlini et al. (2023) show that multimodal models can also be jailbroken. Wei et al. (2023) provide hypotheses, backed by empirical studies, as to why language models can be jailbroken.\\n\\nIn a similar vein to jailbreaks, numerous researchers have demonstrated that aligned models can easily be un-aligned (Yang et al., 2023; Qi et al., 2023), sometimes with as few as 100 fine-tuning examples. We view our work as adding a mechanistic understanding of such phenomena.\\n\\n9. Conclusion\\n\\nIn this work we studied the mechanisms by which alignment algorithms unlearn a capability, taking DPO and toxicity as a case study. First, we uncovered how toxicity is represented and elicited in pre-trained language models, GPT2 and Llama2. We find numerous vectors in MLP blocks that promote toxicity. Simply subtracting these vectors from the residual stream can suppress toxic outputs.\\n\\nSecond, we applied DPO to our language models, using PPLM to carefully craft pairs of toxic and non-toxic continuations to Wikipedia prompts.\\n\\nThird, we studied how our aligned models GPT2 DPO and Llama2 DPO avert toxicity. We find that in both cases, the weights that elicit toxicity are not removed. In the case of GPT2 DPO, the model bypasses regions that elicit toxicity by learning an offset. Such an offset is distributed amongst multiple value vectors from earlier layers, allowing minimal changes to every weight. This allows the model to preserve its pre-trained behavior, while averting toxic outputs, which matches the objective of the DPO loss. In the case of Llama2 DPO, the model uses its gating mechanism to \u201cturn off\u201d toxic vectors.\\n\\nGiven this understanding, we demonstrated how to break the alignment of our models, reverting them back to their toxic behavior. Namely, for GPT2 DPO we simply increase the regions that elicit toxicity, by scaling their corresponding key vectors, while for Llama2 DPO we simply turn the gates back on.\\n\\nWe view our findings as a mechanistic case study for why aligned models can be jailbroken, and hope that this can lead to more robust alignment algorithms. Our code, models, and data can be found at https://github.com/ajyl/dpo_toxic.\"}"}
{"id": "dBqHGZPGZI", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\nImpact Statement\\n\\nOur work takes a step towards better understanding the mechanisms of language models and fine-tuning algorithms. We wish that these findings take us closer to building safer and more trustworthy systems, with hopes that this will lead to more responsible deployments of such systems.\\n\\nAcknowledgements\\n\\nWe thank Ekdeep Singh Lubana for fruitful discussions, and Santiago Castro for helping with figures. This work was supported via NSF under grant #2306372.\\n\\nReferences\\n\\nAdolphs, L., Gao, T., Xu, J., Shuster, K., Sukhbaatar, S., and Weston, J. The CRINGE loss: Learning what language not to model. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8854\u20138874, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.493. URL https://aclanthology.org/2023.acl-long.493.\\n\\nBalestriero, R., Cosentino, R., and Shekkizhar, S. Characterizing large language model geometry solves toxicity detection and generation. arXiv preprint arXiv:2312.01648, 2023.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\\n\\nBurns, C., Ye, H., Klein, D., and Steinhardt, J. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=ETKGuby0hcs.\\n\\nCarlini, N., Nasr, M., Choquette-Choo, C. A., Jagielski, M., Gao, I., Koh, P. W., Ippolito, D., Tramer, F., and Schmidt, L. Are aligned neural networks adversarially aligned? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=OQQoD8Vc3B.\\n\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023.\\n\\ncjadams, Sorensen, J., Elliott, J., Dixon, L., McDonald, M., nithum, and Cukierski, W. Toxic comment classification challenge, 2017. URL https://kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge.\\n\\nConneau, A., Kruszewski, G., Lample, G., Barrault, L., and Baroni, M. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In Gurevych, I. and Miyao, Y. (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2126\u20132136, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1198. URL https://aclanthology.org/P18-1198.\\n\\nDathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski, J., and Liu, R. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations, 2019.\\n\\nDauphin, Y. N., Fan, A., Auli, M., and Grangier, D. Language modeling with gated convolutional networks. In International conference on machine learning, pp. 933\u2013941. PMLR, 2017.\\n\\nDinan, E., Logacheva, V., Malykh, V., Miller, A., Shuster, K., Urbanek, J., Kiela, D., Szlam, A., Serban, I., Lowe, R., et al. The second conversational intelligence challenge (convai2). In The NeurIPS\u201918 Competition: From Machine Learning to Intelligent Conversations, pp. 187\u2013208. Springer, 2020.\\n\\nElhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformercircuits.pub/2021/framework/index.html.\"}"}
{"id": "dBqHGZPGZI", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "dBqHGZPGZI", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Mechanistic Understanding of Alignment Algorithms\\n\\nNostalgebraist. Interpreting gpt: The logit lens, 2020. URL https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.\\n\\nOlah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001. https://distill.pub/2020/circuits/zoom-in.\\n\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Gray, A., et al. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 2022.\\n\\nPark, K., Choe, Y. J., and Veitch, V. The linear representation hypothesis and the geometry of large language models. In Causal Representation Learning Workshop at NeurIPS 2023, 2023.\\n\\nQi, X., Zeng, Y., Xie, T., Chen, P.-Y., Jia, R., Mittal, P., and Henderson, P. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693, 2023.\\n\\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly a reward model, 2023.\\n\\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n\\nShazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\\n\\nSheng, E., Chang, K.-W., Natarajan, P., and Peng, N. The woman worked as a babysitter: On biases in language generation. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3407\u20133412, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1339. URL https://aclanthology.org/D19-1339.\\n\\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.\\n\\nTenney, I., Das, D., and Pavlick, E. BERT rediscovers the classical NLP pipeline. In Korhonen, A., Traum, D., and M\u00e1rquez, L. (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4593\u20134601, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1452. URL https://aclanthology.org/P19-1452.\\n\\nTodd, E., Li, M. L., Sharma, A. S., Mueller, A., Wallace, B. C., and Bau, D. Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023.\\n\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\n\\nWallace, E., Feng, S., Kandpal, N., Gardner, M., and Singh, S. Universal adversarial triggers for attacking and analyzing NLP. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2153\u20132162, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1221. URL https://aclanthology.org/D19-1221.\\n\\nWei, A., Haghtalab, N., and Steinhardt, J. Jailbroken: How does LLM safety training fail? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=jA235JGM09.\\n\\nWelleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and Weston, J. Neural text generation with unlikelihood training. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SJeYe0NtvH.\\n\\nXu, J., Lee, A., Sukhbaatar, S., and Weston, J. Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023.\\n\\nYang, X., Wang, X., Zhang, Q., Petzold, L., Wang, W. Y., Zhao, X., and Lin, D. Shadow alignment: The ease of subverting safely-aligned language models. arXiv preprint arXiv:2310.02949, 2023.\"}"}
