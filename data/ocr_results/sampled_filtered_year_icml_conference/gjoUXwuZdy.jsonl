{"id": "gjoUXwuZdy", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. The explanation of specific output demands used in our prompts. Note: Q2 is about edge identification, while Q3 is about specific graph theory problems.\"}"}
{"id": "gjoUXwuZdy", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: Is there a path between node 5 and node 3? If yes, give the path.\\nLLaVA-7B(0-shot): The answer is no.\\nLlama-7B(DPR): The answer is yes.\\nSPHINX-Tiny-1k (0-shot): The answer is no, there is no path between node 5 and node 3.\\nSPHINX-Tiny-1k(DPR): Yes, there is a path between node 5 and node 3. The path is: 0,18,19,16,3,14,3,1.\\nInternLM-XComposer2-VL (0-shot): Yes, there is a path between node 5 and node 3. The path is 5,17,11,6,4,20,19,1,15,12,8,18,13,16,3.\\nInternLM-XComposer2-VL(DPR): Yes, there is a path between node 5 and node 3. The path is 5,11,3.\\nQwen-plus(0-shot): Yes, there is a path between node 5 and node 3. The path is 5->14->20->7->3.\\nQwen-max(0-shot): Yes, there is a path between node 5 and node 3. The path is 5-0-16-9-4-20-18-17-13-3.\\nGemini (0-shot): Yes, there is a path between node 5 and node 3. The path is 5,17,0,3.\\nGPT-4V (0-shot): I'm sorry, I cannot provide assistance with that request.\\nGPT-4V (2-shot+COT): Yes, there is a path between node 5 and node 3. The path is 5,17,0,13,3.\\nGPT-4V (DPR): 1. The edges of the graph are represented as tuples: (0,17), (2,7), (2,16), (3,11), (4,11), (4,18), (5,17), (6,11), (6,15), (7,11), (7,18), (8,12), (8,19), (11,17), (12,19), (13,20), (16,17), (19,20).\\n2. The algorithm and corresponding codes are: One simple algorithm we can use for finding a path between two nodes in an undirected graph is Depth-First Search (DFS). Here's the corresponding Python code to achieve this:\\n```python\\ndef dfs(graph, start, end, path=[]):\\n    path = path + [start]\\n    if start == end:\\n        return path\\n    if start not in graph:\\n        return None\\n    for node in graph[start]:\\n        if node not in path:\\n            newpath = dfs(graph, node, end, path)\\n            if newpath:\\n                return newpath\\n    return None\\ngraph = {0:[17], 2:[7,16], 3:[11], 4:[11,18], 5:[17], 6:[11,15], 7:[2,11,18], 8:[12,19], 11:[3,4,6,7,17], 12:[8,19], 13:[20], 16:[2,17], 17:[0,5,11,16], 19:[8,12,20], 20:[13,19]}\\nstart_node = 5\\nend_node = 3\\nprint(dfs(graph, start_node, end_node))\\n```\"}"}
{"id": "gjoUXwuZdy", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multimodal Graph Theory Problems with Large Multimodal Models\\n\\nQuestion: Is there a cycle in the graph?\\n\\nLLaVA-7B (0-shot): No, there is no cycle in the graph.\\nLLaVA-7B (DPR): Yes, there is a cycle in the graph.\\n\\nSPHINX-Tiny-1k (0-shot): Yes, there is a cycle in this graph. The cycle is 0,1,2,4,11,5,6,3,8,9,7,10,12,10,13.\\nSPHINX-Tiny-1k (DPR): Yes, there is a cycle in this graph. The cycle is 0,12,8,10,9,5,6,5,11,3,4,2,1.\\n\\nInternLM-XComposer2-VL (0-shot): No, there is no cycle in the graph.\\nInternLM-XComposer2-VL (DPR): No, there is no cycle in the graph.\\n\\nQwen-plus (0-shot): Yes, there is a cycle in the graph. The cycle is 0-3-10-8-0.\\nQwen-max (0-shot): Yes, there is a cycle in the graph. The cycle is 0-6-7-9-5-4-8-3-2-0.\\n\\nGemini (0-shot): Yes, there is a cycle in the graph. The cycle is 3,10,8,4,1,6,2,0,11,3.\\n\\nGPT-4V (0-shot): Yes, there is a cycle in the graph. The cycle is 1-4-5-8-10-3-1.\\nGPT-4V (2-shot+COT): Yes, there is a cycle in the graph. The cycle is 0,11,3,10,8,4,1,0.\\nGPT-4V (DPR): No, there is no cycle in the graph.\\n\\n1. Graph Edges Identification: The provided edges from the tuples are as follows:\\n   (0, 2), (0, 4), (0, 6), (0, 8),\\n   (1, 5),\\n   (2, 6),\\n   (3, 5), (3, 11),\\n   (4, 6), (4, 10),\\n   (5, 9),\\n   (6, 12),\\n   (7, 9),\\n   (8, 0),\\n   (9, 5, 7),\\n   (10, 4),\\n   (11, 3),\\n   (12, 6).\\n\\n2. Algorithm Selection and Code Generation: To detect a cycle in the graph, we can use either Depth First Search (DFS) or Breadth First Search (BFS). Here we\u2019ll choose DFS for its straightforward approach in cycle detection in an undirected graph.\\n\\n```python\\ndef dfs(node, visited, parent):\\n    visited[node] = True\\n    for neighbor in graph[node]:\\n        if not visited[neighbor] and dfs(neighbor, visited, node) or parent != neighbor:\\n            return True\\n    return False\\n\\ndef detect_cycle(graph):\\n    visited = {i: False for i in graph}\\n    return any(not visited[node] and dfs(node, visited, -1) for node in graph)\\n\\ncycle_exists = detect_cycle(graph)\\n```\\n\\n3. Multi-Step Reasoning Process:\\n   - Initialize a `visited` dictionary to keep track of visited nodes.\\n   - Iterate over all the nodes in the graph.\\n   - If a node has not been visited, call the `dfs` function on it, marking it as visited.\\n   - In the `dfs` function, iterate over the neighbors of the current node.\\n   - If a neighbor has not been visited, recursively call `dfs` on it.\\n   - If a neighbor has been visited and it\u2019s not the parent of the current node, we have found a cycle.\\n   - If no cycle is found in any of the dfs traversals, then the graph does not contain a cycle.\\n\\n4. Final Answer: Yes, there is a cycle in the graph. The cycle is 0-2-6-4-0.\\n\\nFigure 9. A case (Cycle) illustrates results generated by different models. Red and green words are incorrect and correct contents, respectively.\"}"}
{"id": "gjoUXwuZdy", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There is a cycle in the graph. The cycle is 0-1-8-4-2-0.\"}"}
{"id": "gjoUXwuZdy", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multimodal Graph Theory Problems with Large Multimodal Models in Figures 7-11) are shown in Appendix. However, when asked to specify the correct path, only GPT-4V (DPR) provided an accurate answer. The other samples erroneously constructed paths using edges that do not exist in the graph. This may be attributed to the fact that most LMMs have relatively weaker graph recognition capabilities. We suggest supplementing these models with highly accurate graphical interpretation information (in this case, the edge information provided had an accuracy rate of 81.81%) could compensate for GPT-4V's shortcomings of graph recognition. Additionally, GPT-4V shows proficiency in accurately completing tasks involving the selection of algorithms for specific graph theory problems and utilizing the algorithm for reasoning analysis. To conclude, the proposed DPR can assist GPT-4V in maximizing its inherent strengths and effectively mitigating its weaknesses in graph theory problems.\\n\\n6. Conclusion\\n\\nIn this study, we delved into the capabilities of large multimodal models (LMMs) in addressing multimodal graph theory challenges. Initially, we developed the VisionGraph benchmark, tailored for evaluating LMMs. This benchmark not only encompasses node and edge identification tasks to gauge the graphical comprehension of LMMs but also incorporates eight distinct graph theory problems to test their multi-step reasoning abilities. Subsequently, we conducted a comprehensive analysis of various LMMs, including GPT-4V and Gemini, using VisionGraph. This analysis focused on two key aspects: the understanding of graphical structures and the impact of supervised fine-tuning methods. Furthermore, we introduced a multimodal graph theory-oriented Agent, named Description Programming Reasoning (DPR). DPR is uniquely designed to integrate intricate task decomposition, perception enhancement at-tuned to smaller models, advanced code generation, and the utilization of external tools. Through experimental evaluations, it has been demonstrated that DPR significantly enhances the performance of LMMs on multimodal graph theory tasks.\\n\\n7. Acknowledge\\n\\nThanks for the efforts from reviewers and action editors. This work is supported by grants: Natural Science Foundation of China (No. 62376067).\\n\\nImpact Statement\\n\\nOur paper mainly presents a multimodal graph theory problem benchmark named VisionGraph to check the multimodal planning performance of LMMs. We also introduce a corresponding multimodal agent to handle such complex visual maths. This research marks a significant contribution to several fields: it advances the spatial perception capabilities of Large Multimodal Models, propels the frontiers of Visual Intelligence in areas such as visual math and robotics planning, supports the application of AI in scientific endeavors, particularly in Mathematics, and offers insights into Strategy Optimization for industrial applications.\\n\\nEthical Considerations:\\n\\n\u2022 Data Source and Tools. The foundational data for our benchmark originates from publicly accessible text datasets, explicitly designated for academic research purposes. This ensures compliance with data usage norms and ethical standards in academic contexts. Additionally, the methodology employed for data construction exclusively utilizes open-source graph construction tools. Emphasizing transparency and community engagement, we commit to making our benchmark openly available, facilitating further research and validation by the academic community.\\n\\n\u2022 Model Usage and Bias Acknowledgement. Our research incorporates the use of two commercial LMMs: GPT-4V and Gemini Pro. It's crucial to acknowledge that the content generated by these models may inherently carry biases. This is a reflection of the models' training datasets and algorithms, rather than a deliberate design choice. Recognizing this, we approach our findings with a critical perspective and urge users to consider potential biases when interpreting the results.\\n\\nReferences\\n\\nAnderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sunderhauf, N., Reid, I., Gould, S., and Van Den Hengel, A. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3674\u20133683, 2018.\\n\\nBai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.\\n\\nChen, Q., Pitawela, D., Zhao, C., Zhou, G., Chen, H.-T., and Wu, Q. Webvln: Vision-and-language navigation on websites. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1165\u20131173, 2024.\\n\\nDai, S.-C., Xiong, A., and Ku, L.-W. Llm-in-the-loop: Leveraging large language model for thematic analysis. arXiv preprint arXiv:2310.15100, 2023.\\n\\nEktefaie, Y., Dasoulas, G., Noori, A., Farhat, M., and Zitnik, M. Geometric multimodal representation learning. arXiv preprint arXiv:2209.03299, 2022.\"}"}
{"id": "gjoUXwuZdy", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "gjoUXwuZdy", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Limitations\\n\\nOur work also contains some main limitations:\\n\\n\u2022 Data Distribution Imbalance of VisionGraph. In our benchmark, we have included eight distinct types of multimodal graph theory problems. One of the primary challenges we encountered was the inherent difficulty in constructing diverse instances for these different types of graph theory problems. This has resulted in an uneven distribution of training and testing datasets across the various problem types. Despite this imbalance, concerted efforts were made to ensure the dataset's diversity and to cover four types of graphs, aiming for a comprehensive evaluation of LMMS' capabilities in solving graph theory problems. Moreover, our experiments indicated that augmenting the volume of training data can substantially enhance the models' spatial understanding of graphs. Future research could focus on developing more balanced datasets to further validate and extend our conclusions.\\n\\n\u2022 Visual Graph Construction. To maintain legibility, we've set a cap on the number of nodes in undirected graphs at 25 and directed graphs at 20 during data augmentation. This restriction means our dataset doesn't encompass more complex graph structures, potentially leading to a trained model that can not recognize intricate graphs. Additionally, all nodes and edges in these graphs are randomly generated, which could result in a few duplicate graphs within the dataset, yet the questions are different.\\n\\n\u2022 Model Updates and Reproducibility. When the parameters of the GPT-4V and Gemini models change, current evaluations might become outdated or imprecise. To address this, our paper focuses on ensuring the reproducibility of findings: 1) We will provide all LMMs outputs under various prompt settings, allowing reliable replication and validation of our results. 2) We will release the codes and benchmarks used for research in the community.\"}"}
{"id": "gjoUXwuZdy", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multimodal Graph Theory Problems with Large Multimodal Models\\n\\nB. More Cases and Detailed Prompting Technical\\n\\nFigure 7. A case (Shortest Path) illustrates results generated by different models. Red and green words are incorrect and correct contents, respectively.\"}"}
{"id": "gjoUXwuZdy", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context\\n\\nYunxin Li\\\\textsuperscript{1}  Baotian Hu\\\\textsuperscript{1}  Haoyuan Shi\\\\textsuperscript{1}  Wei Wang\\\\textsuperscript{2}  Longyue Wang Min Zhang\\\\textsuperscript{1}\\n\\nAbstract\\n\\nLarge Multimodal Models (LMMs) have achieved impressive success in visual reasoning, particularly in visual mathematics. However, problem-solving capabilities in graph theory remain less explored for LMMs, despite being a crucial aspect of mathematical reasoning that requires accurate understanding of graphical structures and multi-step reasoning on visual graphs. To step forward in this direction, we are the first to design a benchmark named VisionGraph, used to explore the capabilities of advanced LMMs in solving multimodal graph theory problems. It encompasses eight complex graph problem tasks, from connectivity to shortest path problems. Subsequently, we present a Description-Program-Reasoning (DPR) chain to enhance the logical accuracy of reasoning processes through graphical structure description generation and algorithm-aware multi-step reasoning. Our extensive study shows that 1) GPT-4V outperforms Gemini Pro in multi-step graph reasoning; 2) All LMMs exhibit inferior perception accuracy for graphical structures, whether in zero/few-shot settings or with supervised fine-tuning (SFT), which further affects problem-solving performance; 3) DPR significantly improves the multi-step graph reasoning capabilities of LMMs and the GPT-4V (DPR) agent achieves SOTA performance.\\n\\n1. Introduction\\n\\nMathematical Reasoning is a core aspect for evaluating the logical reasoning capability (Dai et al., 2023) of Large Language Models (LLMs) and Large Multimodal Models (LMMs). Recent works (Luo et al., 2023; Team et al., 2023; Imani et al., 2023; Wang et al., 2023a; Li et al., 2023b) present the rapid development of applying LLMs to help solve arithmetic and graph reasoning tasks. Compared to LLMs, the evaluation of mathematical reasoning capabilities in LMMs is beginning. Lu et al. (2023) recently presented a comprehensive visual math benchmark, open-ended answer generation based on questions and visual context. It evaluates the basic mathematical capabilities of LMMs, such as algebraic reasoning, geometry reasoning, and arithmetic reasoning. However, the challenging graph theory problem-solving capability has been less explored for LMMs, which presents a significant aspect of mathematical reasoning capabilities. The graph theory problems also feature prominently in various research directions and practical scenarios powered by large models, e.g., multimodal graph learning (Ektefaie et al., 2022), AI for Mathematics (Zhang et al., 2023a), visual-language navigation (Gu et al., 2022; Anderson et al., 2022).\\n\\n\\\\textsuperscript{1}School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China.\\n\\\\textsuperscript{2}School of Cyber Science and Technology, Shenzhen campus of Sun Yat-sen University, Shenzhen, China. Yunxin Li\\\\textsuperscript{1} <liyunxin987@163.com>. Correspondence to: Baotian Hu\\\\textsuperscript{1} <hubaotian@hit.edu.cn>.\\n\\nProceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\"}"}
{"id": "gjoUXwuZdy", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multimodal Graph Theory Problems with Large Multimodal Models\\n\\n2018; Chen et al., 2024), and robotics planning and control (Wang et al., 2023b; Wake et al., 2023). In these areas, LMMs require the ability to understand structural graphs and perform multi-step reasoning on them to achieve the final goal, especially in robotics planning, which often centers around structured environments. Hence, exploring the multi-step graph reasoning performance of LMMs has the potential to improve their complex multimodal problem-solving ability.\\n\\nTo step forward in this direction, we first introduce a novel multimodal graph reasoning benchmark, named VisionGraph to assess the capabilities of advanced multimodal LMMs in solving graph theory problems within a visual context. It could be regarded as a new testing scenario for LMMs-driven agents. This benchmark is an extension of NLGraph (Wang et al., 2023a), a natural language-based graph problem-solving benchmark. We employ the graph generation tool NetworkX to create graphs according to predefined nodes and edges. The layout of a specific graph is dynamically adjusted for clarity by humans, considering the number of nodes. First, we incorporate two types of graph understanding questions to evaluate the structural comprehension of LMMs. As shown in Figure 1, these questions are as Node Recognition: how many nodes are shown in the graph? and Edge Recognition: List all edges as triples in the undirected and weighted graph. As illustrated in Figure 2, VisionGraph encompasses eight types of graph theory problems across three difficulty levels: easy, medium, and hard. Hence, it offers a comprehensive multimodal graph reasoning benchmark, in which each visual graph contains three questions to probe the LMMs\u2019 understanding and multi-step reasoning abilities.\\n\\nIn this paper, we are particularly interested in how LMMs, such as GPT-4V and Gemini, perform in solving multimodal graph problems, encompassing structural graph understanding and multi-step reasoning on visual graphs. We conduct an empirical study from three in-depth perspectives:\\n\\n\u2022 Graphical Structures Understanding Ability: Unlike general images, visual graphs have strong spatial structure and are very suitable for examining the spatial understanding ability of LMMs. In this work, we explore the graphical structures understanding performance of LMMs in terms of nodes and edges recognition.\\n\\n\u2022 Effects of Supervised Fine-tuning Approaches: LMMs are usually fine-tuned with open-domain image-text data, performing unsatisfactorily in handling images in vertical fields (Li et al., 2023d) such as medical images (Li et al., 2023c). Hence, we employ constructed Graph Instruction fine-tuning data to tune LMMs further and analyze the effects of training strategies. We compared and analyzed the overall performance of LMMs after introducing graph understanding and reasoning data.\\n\\n\u2022 Analysis of Multi-step Graph Reasoning Capacity: While GPT-4V has demonstrated successful performance on challenging vision-language reasoning scenarios such as Autonomous Driving (Wen et al., 2023) and Robotics (Wake et al., 2023), we also need to know how well GPT-4V solve multimodal graph problems via multi-step reasoning. To answer this question, we explore the few-shot and chain-of-the-thought reasoning performance and present a Description-Program-Reasoning (DPR) approach for this graph problem. It consists of graph structure description generation and algorithm-aware multi-step reasoning in order, aimed to enhance the logicalness of the reasoning process.\\n\\nWe conduct experiments on a variety of graph theory problems covering cycle, shortest path, connectivity, and others. We evaluate LMMs with comparative training strategies and reasoning approaches. The main contributions are:\\n\\n\u2022 We present a multimodal graph theory problems benchmark VisionGraph, to assess the graphical structure understanding and multi-step reasoning capabilities of LMMs. To facilitate future research in graph theory problems, we will release the benchmark VisionGraph and advanced prompting technical for LMMs.\\n\\n\u2022 Our empirical study shows the shortcomings of LMMs, including GPT-4V and Gemini, in understanding graphical structure and multi-step multimodal reasoning. This indicates their potential to enhance multi-step reasoning and planning abilities in the context of visual graphs.\\n\\n\u2022 We design a graph problem-solving approach named Description-Program-Reasoning (DPR), which interleaves natural language and programming to enhance the multi-step reasoning performance of LMMs. The designed GPT-4V (DPR) is a comprehensive multi-modal Agent that integrates complex task decomposition, small model perception enhancement, code generation, and tool invocation.\"}"}
{"id": "gjoUXwuZdy", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multimodal Graph Theory Problems with Large Multimodal Models\\n\\nFigure 2. An overview of various multimodal graph theory problems in the VisionGraph benchmark.\\n\\nand remove the original node and edge descriptions from the natural questions. Based on the original difficulty of each problem within the eight tasks, the overall dataset is divided into easy, medium, and hard subsets for each graph reasoning task. In addition, we add the graph understanding questions to check the spatial understanding capability of LMMs.\\n\\nWe present the overview of the VisionGraph benchmark via a specific sample for each task in Figure 2. The task definition of each graph theory problem is as follows:\\n\\n\u2022 Connectivity: Given an undirected graph \\\\( G = \\\\{V, E\\\\} \\\\), it infers whether two nodes \\\\( u \\\\) and \\\\( v \\\\) are connected according to whether there exists a sequence of edges from node \\\\( u \\\\) to node \\\\( v \\\\) in \\\\( E \\\\).\\n\\n\u2022 Cycle: In an undirected graph \\\\( G = \\\\{V, E\\\\} \\\\), a cycle is a non-empty trail \\\\((e_1, e_2, \\\\ldots, e_n)\\\\) with a node sequence \\\\((v_1, v_2, \\\\ldots, v_n, v_1)\\\\). This task asks whether there exists a cycle through true/false questions and retains a balanced set of cyclic and noncyclic graphs in the dataset.\\n\\n\u2022 Topological Sort: A topological sort of a directed graph is a linear ordering of its nodes such that for every directed edge \\\\((u, v)\\\\) from node \\\\( u \\\\) to node \\\\( v \\\\), \\\\( u \\\\) comes before \\\\( v \\\\) in the ordering. The task is to find a valid topological sort given a directed graph and there could be multiple valid solutions. We employ an external program to examine the correctness of the generated topological order.\\n\\n\u2022 Shortest Path: The shortest path between two nodes \\\\( u \\\\) and \\\\( v \\\\) is the path with the minimum sum of edge weights. It requires the LMM to generate the shortest plan based on the weights and nodes depicted in the graph.\\n\\n\u2022 Maximum Flow: For two nodes: source \\\\( u \\\\) and sink \\\\( v \\\\) in a network \\\\( G = \\\\{V, E\\\\} \\\\), it asks LMMs to generate a plan to route as much flow as possible from source to the sink.\\n\\n\u2022 Bipartite Graph Matching: A bipartite graph is a graph whose nodes can be divided into two disjoint sets \\\\( U \\\\) and \\\\( V \\\\), and in each set no nodes are adjacent to each other. Given a bipartite graph, the task is to find the matching.\"}"}
{"id": "gjoUXwuZdy", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Overview of the VisionGraph Benchmark. 'SPEC.' represents the level of difficulty, indicated by the number of nodes in each graph. Key metrics include the number of samples (S), images (I), questions (Q), and answers (A). Each visual graph is accompanied by three questions: two focus on general graph comprehension, and one addresses specific graph theory problems.\\n\\n| Subset | Connect. | Cycle | Topo. Sort | Shortest Path | Max. Flow | Bipartite Graph | Hamilton Path |\\n|--------|----------|-------|------------|--------------|-----------|-----------------|---------------|\\n| # EASY |          |       |            |              |           |                 |               |\\n|        | 352      | 150   | 180        | 180          | 150       | 300             | 150           |\\n| SPEC.  | n: 5\u221210  |       |            |              |           |                 |               |\\n| # MEDIUM|          |       |            |              |           |                 |               |\\n|        | 1,200    | 600   | -          | -            | -         | -               | -             |\\n| SPEC.  | n: 11\u221225 |       |            |              |           |                 |               |\\n| # HARD |          |       |            |              |           |                 |               |\\n|        | 680      | 400   | 200        | 200          | 200       | 210             | 200           |\\n| SPEC.  | n: 26\u221235 | n: 11\u221220 | n: 17\u221233 | n: 9\u221215     |           |                 |               |\\n| # Total | S/I     |       |            |              |           |                 |               |\\n|        | 2,232    | 1,150 | 530        | 380          | 350       | 510             | 350           |\\n| #Q/A   |          |       |            |              |           |                 |               |\\n|        | 6,696    | 3,450 | 1,590      | 1,140        | 1,050     | 1,530           | 1,050         |\\n| #Len Q |          |       |            |              |           |                 |               |\\n|        | 45.0     | 39.0  | 52.0       | 60.0         | 61.0      | 54.0            | 62.0          |\\n| #Len A |          |       |            |              |           |                 |               |\\n|        | 162.26   | 63.94 | 194.06     | 95.10        | 141.47    | 126.01          | 101.03        |\\n\\nthat maximizes the number of edges. Like Topological Sort, we use an external program to evaluate the solution.\\n\\n- Hamilton Path: In an undirected graph, a Hamilton path is a path that visits every node exactly once. Hence, this task asks the LMM to generate a Hamilton path given an undirected graph.\\n\\n- Graph Neural Networks: This setting of this task is updating the node embedding of an undirected graph with the sum of all the neighbors' embeddings. Each node in the graph has a two-dimension node embedding.\\n\\nThe detailed data statistics of VisionGraph are presented in Table 1. VisionGraph consists of 5,902 problems in total, where the easy Connectivity, Cycle, and Topological Sort tasks include three difficulty levels, and others only contain easy/hard levels.\\n\\n3. Experimental Setup\\n\\n3.1. Comparing Models\\n\\nWe test widely used powerful commercial LMMs (GPT-4V, Gemini, and Qwen-Plus/Max) and open-sourced LMMs: MiniGPT-4 (Zhu et al., 2023) extends the Q-Former architecture to enhance multimodal interactions. It leverages the shallow transformer approach to align visual features from a frozen visual encoder with the language model, thereby enabling robust multimodal comprehension and generation.\\n\\nInstructBLIP (Wang et al., 2022) introduces instruction-aware visual features by incorporating instructions directly into the Q-Former architecture (Li et al., 2023a). It allows the model to dynamically adapt its multimodal understanding based on explicit instructions provided alongside visual inputs.\\n\\nLLaVA (Liu et al., 2023) takes a distinct approach by utilizing a linear layer to map fine-grained visual features from a frozen vision encoder into the embedding space of the pre-trained LLM.\\n\\nQwen-VL (Bai et al., 2023) is a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images.\\n\\nSPHINX (Lin et al., 2023) is a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings.\\n\\nInternLM-XComposer (Zhang et al., 2023b) is a vision-language large model that enables advanced image-text comprehension and composition. For closed-source LMMs, GPT-4V (OpenAI, 2023) from OpenAI is recognized as the most powerful MLLMs to date, surpassing a host of Vicuna-based models, e.g., MiniGPT-4, InstructBLIP, and LLaVA. Besides, Gemini (Team et al., 2023), released by Google, has emerged as a formidable challenger to GPT-4V, exhibiting significant multi-modal capabilities over different benchmarks.\\n\\n3.2. Evaluation Metric\\n\\nThe graph theory problem evaluation will be based on three distinct sub-questions, each with its specific criteria. The first (node recognition) assesses accuracy in counting graph nodes by comparing the answer to a standard solution. The second sub-question (edge recognition) involves representing graph edges for four types of graphs using tuples: two-element tuples for undirected/directed unweighted graphs; and three-element tuples including edge weights for undirected/directed weighted graphs. The evaluation of the answer to this question encompasses two key metrics:\\n\\n- Correct Rate: Quantifies the proportion of correctly identified tuples in the response against the standard solution.\\n- Error Rate: Quantifies the proportion of incorrectly identified tuples relative to the total tuples in the response.\\n\\nIn our study, the third question about multimodal graph theory problems encompasses a range of graph problems that necessitate binary (yes/no) or descriptive responses.\"}"}
{"id": "gjoUXwuZdy", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. The prompts for GPT-4V and Gemini-pro under zero- or few-shots settings. Few-shot samples are randomly selected from the easy/medium sub-sets of the training set. For each sub-question, we design various output demands to gain the corresponding answer format, which is given in Table 5 of the Appendix.\\n\\nCOT\\n\\nSpecifically, for Connectivity and Cycle problems, we assess responses using two tiers of accuracy metrics: a basic metric for correct binary answers, and a more stringent metric requiring verifiable paths for affirmative responses (yes), as applied to powerful GPT-4V and Gemini models. In Topological Sort tasks, the accuracy of each step in the solution sequence is critical, where the solution is verified by the procedure. For Shortest Path problems, our evaluation criteria include verifying both the existence and the total weight of the proposed shortest path via the Python tool. In Maximum Flow problems, the focus is on accurately calculating the maximal flow value. For Bipartite Graph Matching, correct responses must identify the maximum number of matches and provide an error-free assignment. Hamilton Path challenges require each step of the proposed path to be precise and consistent with the graph's structure. Lastly, GNN tasks demand correct post-convolution node embeddings, in line with the specified graph convolution operation. Overall, we set the corresponding evaluation approach for different problems.\\n\\n3.3. Implementation Details\\n\\nOur model\u2019s training was conducted in two distinct phases. The initial phase covered 5 epochs, using a batch size of 16 and leveraging the AdamW (Kingma & Ba, 2014) optimizer with a learning rate of 1e-4. Encountering a performance bottleneck after 2 epochs, we adapted the dataset by introducing a VQA task focused on incorporating fine-grained edge information. This adjustment marked the inception of the second training phase, building upon the initial training phase. The subsequent training employed modified hyperparameters, including a revised data path and an adjusted learning rate of 3e-5, with the total epochs set to 3. Throughout both phases, batch sizes, optimizer settings, and the utilization of gradient checkpoint and lazy preprocessing remained consistent. During inference, the temperature parameter is assigned a default value of 0.2, and the beam size is configured to 1.\\n\\nFigure 3. It illustrates two scenarios in augmented graph understanding data: 1) Overall Edge Recognition, focusing on identifying and interpreting the connections between nodes; 2) Edge-relevant VQA, which addresses questions specifically related to the visual aspects and significance of the graph\u2019s edges and nodes.\\n\\n4. Comparative Analysis of LMMs\\n\\nIn this section, we compare various supervised finetuning LMMs, Gemini, and GPT-4V on graphical structures understanding and multimodal graph theory problem-solving, especially for the effect of supervised fine-tuning approaches.\\n\\n4.1. Graphical Structures Understanding Ability\\n\\nCompared to powerful GPT-4V and Gemini, open-source LMMs such as LLava and InstructBLIP, have also achieved impressive visual understanding capability on open-world images. By analyzing the performance of GPT-4V, Gemini, and other LMMs, we can observe the overall performance and exposed problems of current LMMs on graph theory problems. Table 2 shows the prompting methods for LMMs. Table 3 displays the results for node and edge recognition in...\"}"}
{"id": "gjoUXwuZdy", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3. Overall results in the VisionGraph benchmark.\\n\\n| Model              | Task Types | Connect Cycle Topo. Sort | Shortest Path | Max. Flow | Bipartite Graph | Hamilton Path |\\n|--------------------|------------|--------------------------|---------------|-----------|-----------------|---------------|\\n|                    |            | GNNs                     |               |           |                 |               |\\n|                    |            | Node Recognition         |               |           |                 |               |\\n| MiniGPT-4 \u2663 (Vicuna-7b) | 19.14      | 12.04                    | 42.96         | 42.19     | 32.76           | 8.33          | 60.34         | 53.85         |\\n| BLIP-2 \u2663 (FlanT5-xxl) | 37.74      | 52.88                    | 47.41         | 81.25     | 67.24           | 22.62         | 62.07         | 61.54         |\\n| InstructBLIP \u2663 (FlanT5-xl) | 36.12     | 47.64                    | 46.67         | 75.00     | 56.90           | 36.90         | 53.45         | 74.36         |\\n| InstructBLIP \u2663 (FlanT5-xxl) | 35.31     | 52.88                    | 61.48         | 85.94     | 77.59           | 17.86         | 65.52         | 61.54         |\\n| Sphinx \u2663          | 61.99      | 98.95                    | 94.07         | 100.00    | 91.38           | 55.95         | 100.00        | 97.44         |\\n| Internlm \u2663        | 67.92      | 100.00                   | 97.78         | 100.00    | 98.25           | 77.38         | 100.00        | 100.00        |\\n| Llava-v1.5-7b \u2663   | 64.15      | 96.86                    | 92.59         | 100.00    | 93.10           | 13.10         | 100.00        | 94.87         |\\n| Llava-v1.5-13b \u2663  | 62.26      | 97.91                    | 91.11         | 100.00    | 96.55           | 11.90         | 100.00        | 97.44         |\\n| Qwen-Plus (0-shot) | 2.96       | 0.00                     | 0.00          | 0.00      | 5.17            | 0.00          | 0.00          | 56.41         |\\n| Qwen-max (0-shot) | 29.11      | 31.94                    | 30.37         | 12.50     | 3.45            | 14.29         | 29.31         | 46.15         |\\n| Gemini (0-shot)   | 40.97      | 42.93                    | 47.41         | 67.19     | 72.41           | 10.71         | 65.52         | 35.90         |\\n| GPT-4V (0-shot)   | 46.49      | 81.15                    | 81.48         | 89.06     | 58.62           | 20.24         | 100.00        | 97.44         |\\n\\nEdge Recognition (Correct \u2191/Error \u2193)\\n\\n| Model              | Task Types | Connect Cycle Topo. Sort | Shortest Path | Max. Flow | Bipartite Graph | Hamilton Path |\\n|--------------------|------------|--------------------------|---------------|-----------|-----------------|---------------|\\n|                    |            | GNNs                     |               |           |                 |               |\\n|                    |            | Node Recognition         |               |           |                 |               |\\n| MiniGPT-4 \u2663 (Vicuna-7b) | 11.78/31.78 | 0.68/1.59                | 12.54/58.89   | 4.78/87.20 | 0.61/61.15      | 14.45/47.53  |\\n| BLIP-2 \u2663 (FlanT5-xxl) | 12.49/84.03 | 15.11/84.69              | 0.08/2.14     | 1.75/96.84 | 0.00/0.00       | 9.92/75.89   |\\n| Sphinx \u2663          | 44.76/66.69 | 22.13/79.69              | 37.84/73.07   | 39.88/70.62 | 20.68/86.57      | 83.93/53.51  |\\n| Internlm \u2663        | 53.08/35.01 | 40.78/60.05              | 55.70/50.85   | 57.82/45.02 | 23.45/80.27      | 71.21/42.34  |\\n| InstructBLIP \u2663 (FlanT5-xl) | 17.24/87.62 | 26.02/88.06              | 0.00/0.00     | 5.70/93.93 | 0.00/0.00       | 12.72/83.13  |\\n| InstructBLIP \u2663 (FlanT5-xxl) | 16.34/81.50 | 16.04/85.54              | 0.00/0.00     | 3.58/98.31 | 0.00/0.00       | 13.26/76.86  |\\n| Llava-v1.5-7b \u2663   | 46.81/58.13 | 23.23/77.63              | 36.56/72.97   | 38.76/66.47 | 9.80/91.56       | 63.10/54.70  |\\n| Llava-v1.5-13b \u2663  | 51.18/53.41 | 22.60/76.91              | 38.80/70.26   | 41.93/63.50 | 9.89/91.72       | 67.88/54.21  |\\n\\nVisionGraph. Open-source LMMs initially exhibited inferior zero-shot performances, prompting us to fine-tune them using the VisionGraph training set. This fine-tuning led to improved results, particularly notable in the Llava-v1.5-7b/13b models, which outperformed GPT-4V in both node and edge recognition tasks. However, GPT-4V still excelled over Gemini in node recognition and demonstrated a lower error rate in edge recognition, suggesting superior spatial understanding than Gemini. Additionally, GPT-4V showed improved edge recognition performance when moving from a zero-shot to a four-shot setting. Regarding edge recognition, all LMMs exhibit a notably high error rate, and the error rate outperforms the right rate. These findings suggest that while LMMs benefit from continued training on multimodal graph problems, enhancing their spatial perception, there remains significant potential for further improvement. This may be primarily due to an inferior spatial perception ability, which will significantly impact the accuracy of next-step visual reasoning.\\n\\n4.2. Effects of Supervised Fine-tuning Approaches\\n\\nAfter evaluating the visual recognition ability, we further analyze the graph reasoning accuracy of LMMs and the effects of supervised fine-tuning approaches. The experimental results are shown in Table 3. First, we introduce 200k edge recognition and edge-relevant VQA data as shown in Figure 3 to enhance the graph understanding capability. They are constructed by generating the graph according to random setting nodes, edges, and questions. We observe that introducing more graph understanding will enhance the\"}"}
{"id": "gjoUXwuZdy", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multimodal Graph Theory Problems with Large Multimodal Models\\n\\nFigure 4. Overview of the prompting approach for GPT-4V (DPR).\\n\\nWhile comparing accuracy on specific graph theory problems, we find that the data augmentation method significantly improves the performance of the model in terms of Cycle (Llava-v1.5-13b: 60.38 vs. 52.83) and Connectivity (Llava-v1.5-13b: 53.93 vs. 47.12). Hence, to improve the multimodal graph reasoning capability of LMMs, we need to introduce more low-level perception data. In addition, from Table 3, we observe that the few-shot setting also improves the graph perception and reasoning accuracy of GPT-4V, which also indicates that introducing more data to improve LMMs is effective. This may be attributed to the long-tail distribution of training data in LMMs.\\n\\n5. Improving Multi-step Reasoning Capability of LMMs\\n\\nThis section mainly introduces and evaluates the Multimodal Graph Agent approach: Description Program Reasoning (DPR), designed for multimodal graph theory challenges.\\n\\n5.1. Description Program Reasoning\\n\\nTo improve the performance of GPT-4V on multimodal graph theory problems, inspired by LLMs-powered Agents (Wang et al., 2023b; Qin et al., 2024; Yao et al., 2023), we devise a multimodal graph problem-solving approach: Description-Program-Reasoning (DPR), which is a natural language and code interleaved reasoning chain. Specifically, we first use the Llava-7b augmented by the graph understanding data to produce high-quality graph explanations, which are fed into GPT-4V to enhance the graphical structure understanding. As the process shown in Figure 4, we prompt GPT-4V to answer the specific graph problems based on the visual graph and its descriptions:\\n\\n\u2022 Create the Adjacency Matrix of the graph according to the visual graph and its description generated by small models. We require the adjacency matrix to be given in the form of triples, i.e., (node1, node2, weight) for directed graphs and (node1, node2) for undirected graphs.\\n\u2022 Select the appropriate graph theory algorithm and generate the corresponding codes. This step aims to generate codes used for performing multi-step reasoning on the visual graph.\\n\u2022 Perform multi-step reasoning based on the Adjacency Matrix and generated codes. We can also use an external Python interpreter to perform multi-step reasoning, enhancing the overall performance.\\n\u2022 Give the final answer in the demanding format.\\n\\nHence, GPT-4V with DPR is a comprehensive multi-modal Agent that integrates complex task decomposition, small model enhancement, code generation, and tool invocation.\\n\\nWe illustrate the proposed DPR in Figure 4 and 5. In the following, we employ the DPR prompting and evaluate it on three representative tasks in the VisionGraph benchmark with varying difficulties.\\n\\n5.2. Results and Analysis\\n\\nAblation Study. Table 4 shows all LMMs' performances on Connectivity, Cycle, and Shortest Path with varying complexity. Since Llava can not follow instructions (shown in\"}"}
{"id": "gjoUXwuZdy", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4. Model performance on three common graph theory problems in VisionGraph.\\n\\n| Task Types     | Connectivity | Cycle | Shortest Path |\\n|----------------|--------------|-------|---------------|\\n| Easy           | 60.71        | 36.00 | 0.00          |\\n| Medium         | 53.57        | 51.40 | 0.00          |\\n| Hard           | 52.94        | 59.32 | 0.00          |\\n| Avg.           | 54.45        | 51.83 | 0.00          |\\n\\n- **MiniGPT-4 (Vicuna-7b)**\\n  - Easy: 60.71\\n  - Medium: 53.57\\n  - Hard: 52.94\\n  - Avg: 54.45\\n\\n- **BLIP-2 (FlanT5-xxl)**\\n  - Easy: 37.50\\n  - Medium: 43.37\\n  - Hard: 56.30\\n  - Avg: 46.63\\n\\n- **InstructBLIP (FlanT5-xl)**\\n  - Easy: 46.43\\n  - Medium: 46.43\\n  - Hard: 53.78\\n  - Avg: 48.79\\n\\n- **Sphinx**\\n  - Easy: 39.29\\n  - Medium: 45.41\\n  - Hard: 52.10\\n  - Avg: 46.63\\n\\n- **Sphinx w/ DPR**\\n  - Easy: 67.86\\n  - Medium: 59.69\\n  - Hard: 52.94\\n  - Avg: 58.76\\n\\n- **Internlm**\\n  - Easy: 78.57\\n  - Medium: 66.33\\n  - Hard: 52.10\\n  - Avg: 52.94\\n\\n- **Internlm w/ DPR**\\n  - Easy: 89.29\\n  - Medium: 72.96\\n  - Hard: 56.30\\n  - Avg: 70.08\\n\\n- **Llava-v1.5-7b**\\n  - Easy: 64.29\\n  - Medium: 50.00\\n  - Hard: 53.78\\n  - Avg: 53.27\\n\\n- **Llava-v1.5-13b**\\n  - Easy: 71.43\\n  - Medium: 49.49\\n  - Hard: 49.58\\n  - Avg: 52.83\\n\\n- **Gemini (0-shot)**\\n  - Easy: 69.64\\n  - Medium: 56.63\\n  - Hard: 47.06\\n  - Avg: 55.52\\n\\n- **Gemini (DPR)**\\n  - Easy: 66.07\\n  - Medium: 52.04\\n  - Hard: 36.97\\n  - Avg: 49.32\\n\\n- **Qwen-plus**\\n  - Easy: 62.50\\n  - Medium: 56.63\\n  - Hard: 47.06\\n  - Avg: 54.45\\n\\n- **Qwen-plus w/ DPR**\\n  - Easy: 57.14\\n  - Medium: 46.43\\n  - Hard: 35.29\\n  - Avg: 44.47\\n\\n- **Qwen-max**\\n  - Easy: 62.50\\n  - Medium: 56.63\\n  - Hard: 46.22\\n  - Avg: 54.18\\n\\n- **Qwen-max w/ DPR**\\n  - Easy: 60.71\\n  - Medium: 51.02\\n  - Hard: 27.73\\n  - Avg: 45.01\\n\\n- **GPT-4V (0-shot)**\\n  - Easy: 69.64\\n  - Medium: 42.86\\n  - Hard: 17.65\\n  - Avg: 38.81\\n\\n- **GPT-4V (2-shot)**\\n  - Easy: 67.86\\n  - Medium: 56.12\\n  - Hard: 47.06\\n  - Avg: 54.98\\n\\n- **GPT-4V (0-COT)**\\n  - Easy: 64.29\\n  - Medium: 34.69\\n  - Hard: 7.56\\n  - Avg: 30.45\\n\\n- **GPT-4V (2-COT)**\\n  - Easy: 67.86\\n  - Medium: 56.63\\n  - Hard: 45.38\\n  - Avg: 54.71\\n\\n- **GPT-4V (DPR)**\\n  - Easy: 92.86\\n  - Medium: 58.67\\n  - Hard: 36.97\\n  - Avg: 56.87\\n\\n- **w/ Python**\\n  - Easy: 92.86\\n  - Medium: 61.73\\n  - Hard: 51.26\\n  - Avg: 63.07\\n\\nFigure 4) to generate the detailed reasoning process, we apply DPR to Llava by adding the results of edge recognition to the conversation history so that its graph description can be attention when answering the graph theory question. The whole process could be simplified as \u201cDescription-Reasoning\u201d. We observe that DPR significantly improves the multi-step reasoning capabilities of LMMs and performs better on more complex Shortest Paths. For GPT-4V, DPR equipped with the Python tool exceeds its 2-shot performance: gain by 8%, 10%, and 14% on Connectivity, Cycle, and Shortest Path, respectively. It indicates DPR shows its superiority in complex multi-step graph reasoning. However, we also observe that GPT-4V and Gemini gain a low performance on hard graph theory problems (nodes > 20 or nodes > 11 on Shortest Path). Especially on the Shortest Path, many LMMs are not capable of solving this task, such as 0.0 for Llava-v1.5-7b/13b and Gemini. Overall, we demonstrate that:\\n\\n1) The natural language and code interactive reasoning chain enhances complex multi-modal reasoning capabilities of LMMs.\\n2) The limited graph perception inherent in LMMs results in their poor performance in multi-step reasoning on the visual graph;\\n3) GPT-4V exhibits superior multi-step graph reasoning abilities compared to Gemini.\\n\\nCase Study. We report a representative case in Figure 6 to analyze the performance of LMMs. Gemini Pro and GPT-4V made the correct assessment \u201cThere is a path between node 3 and node 1.\u201d More various cases (Five cases shown in Figure 6.\"}"}
{"id": "gjoUXwuZdy", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Give the shortest path from node 4 to node 0.\\n\\n**LLaVA-7B (0-shot):** The shortest path from node 4 to node 0 is **4-5-0** with a total weight of **5**.\\n\\n**Final Answer:** The shortest path from node 4 to node 0 is 4-5-0 with a total weight of 5.\"}"}
