{"id": "bitterwolf23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C. Methods\\n\\nHere we give an overview over the evaluated OOD detection methods. For clarity, we denote vectors in bold and lowercase letters and matrices in bold and uppercase letters. We write neural networks as functions $n$, which are parametrized by weights $\\\\theta$, take an input sample $x$ and produce an output vector $o$ of size $C$, where $C$ is typically the number of classes in a classification task (1000 in the case of IN-1K). We refer to $o$ as the logits of $x$, which can be transformed to a probability vector $p$ (also of size $C$) via the softmax function:\\n\\n$$p_i = \\\\frac{\\\\exp(o_i)}{\\\\sum_c \\\\exp(o_c)}.$$ \\n\\nThe network $n$ can be decomposed into a feature extractor $h$ and the networks last layer $g$:\\n\\n$$o = n(x) = g(h(x)),$$\\n\\nwhere $g$ is a fully connected, linear layer, i.e. $g(h) = W^T h + b$ with weight $W$ and bias $b$. We refer to $h = h(x)$ as the features or the embeddings of $x$ w.r.t. the network $n$. As presented in Section 4, for each sample $x$, a method returns an OOD-score $s = f(x)$, a scalar value which is supposed to be larger for ID data and smaller for OOD data. Methods accessing $h(x)$ directly in order to compute the OOD-score are referred to as feature-based methods, in contrast to methods that derive their OOD-score from the logits (even though obviously the logits implicitly also depend on these features). In the following, we will describe how each method computes the score $s$ for a test input $x$.\\n\\n**MSP** (Hendrycks & Gimpel, 2017): The most popular OOD-detection baseline uses the confidence, i.e. the max softmax probability of a models probability output vector:\\n\\n$$s = \\\\max_c (p_c).$$\\n\\n**Max-Logit** (Hendrycks et al., 2022): Similar to MSP, Max-Logit returns the largest entry of the logit-vector $o$, i.e.\\n\\n$$s = \\\\max_c (o_c).$$\\n\\n**Energy** (Liu et al., 2020): The Energy based OOD detection method uses the denominator of the softmax-function as OOD-score:\\n\\n$$s = \\\\log C \\\\sum_c \\\\exp(o_c).$$\\n\\n**KL-Matching** (Hendrycks et al., 2022): KL-Matching computes a mean probability vector $d_c$ for each of the $C$ classes. For a test input, the KL-distances of all $d_c$ vectors to its probability vector $p$ are computed, and the OOD-score is the negative of the smallest of those distances:\\n\\n$$s = -\\\\min_c \\\\text{KL}[p||d_c].$$\\n\\nIn the original paper by (Hendrycks et al., 2022), the average for $d_c$ is computed over an additional validation set. Since none of the other methods leverages extra data and we are interested in fair comparison, we deploy KL-Matching like in (Wang et al., 2022a; Yang et al., 2022), where the average is computed over the train set.\\n\\n**KNN** (Sun et al., 2022): KNN is a non-parametric method that computes distances in the feature-space. Specifically, the feature vector of a test input is normalized to $z = h / ||h||_2$ and the pairwise distances $r_i(z) = ||z - z_i||_2$ to the normalized features $Z = \\\\{z_1, \\\\ldots, z_N\\\\}$ of all samples of the training set are computed. The distances $r_i(z)$ are then sorted according to their magnitude and the $K$th smallest distance, denoted $r_K(z)$ is used as negative OOD-score:\\n\\n$$s = -r_K(z).$$\\n\\nLike suggested in (Sun et al., 2022), we use $K = 1000$.\\n\\n**Mahalanobis distance** (Lee et al., 2018): This popular method fits a class-conditional Gaussian with shared covariance matrix to the train set, i.e. computes\\n\\n$$\\\\hat{\\\\mu}_c = \\\\frac{1}{N_c} \\\\sum_{i:y_i = c} h_i,$$\\n\\n$$\\\\hat{\\\\Sigma} = \\\\frac{1}{N} \\\\sum_{c} \\\\sum_{i:y_i = c} (h_i - \\\\hat{\\\\mu}_c)(h_i - \\\\hat{\\\\mu}_c)^T.$$\"}"}
{"id": "bitterwolf23a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $N_c$ is the number of train samples in class $c$ and $N$ is the total number of train samples. The OOD-score of a test sample is then the Mahalanobis distance induced by $\\\\hat{\\\\Sigma}$ between its feature $h$ and the closest class mean:\\n\\n$$s = -\\\\min_c \\\\left( h - \\\\hat{\\\\mu}_c \\\\right) \\\\hat{\\\\Sigma}^{-1} \\\\left( h - \\\\hat{\\\\mu}_c \\\\right)^T$$\\n\\nRelative Mahalanobis distance (Ren et al., 2021): A modification of the Mahalanobis distance method, thought to improve near-OOD detection, is to additionally fit a global Gaussian distribution to the train set without taking class-information into account:\\n\\n$$\\\\hat{\\\\mu}_{\\\\text{global}} = \\\\frac{1}{N} \\\\sum_i h_i,$$\\n$$\\\\hat{\\\\Sigma}_{\\\\text{global}} = \\\\frac{1}{N} \\\\sum_i \\\\left( h_i - \\\\hat{\\\\mu}_{\\\\text{global}} \\\\right) \\\\left( h_i - \\\\hat{\\\\mu}_{\\\\text{global}} \\\\right)^T$$\\n\\nThe OOD-score is then defined as the difference between the original Mahalanobis distance and the Mahalanobis distance w.r.t. the global Gaussian distribution:\\n\\n$$s = -\\\\min_c \\\\left( \\\\left( h - \\\\hat{\\\\mu}_c \\\\right) \\\\hat{\\\\Sigma}^{-1} \\\\left( h - \\\\hat{\\\\mu}_c \\\\right)^T - \\\\left( h - \\\\hat{\\\\mu}_{\\\\text{global}} \\\\right) \\\\hat{\\\\Sigma}_{\\\\text{global}}^{-1} \\\\left( h - \\\\hat{\\\\mu}_{\\\\text{global}} \\\\right)^T \\\\right)$$\\n\\nReAct (Sun et al., 2021): The authors propose to perform a truncation of the feature vector, $\\\\bar{h} = \\\\min(h, r)$, where the $\\\\min$ operation is to be understood element-wise and $r$ is the truncation threshold. The truncated features can then be converted to so-called rectified logits via $\\\\bar{o} = g(\\\\bar{h}) = W^T \\\\bar{h} + b$. While the rectified logits can now be used with a variety of existing detection methods, we follow (Sun et al., 2021) and use the rectified Energy as OOD-score:\\n\\n$$s = \\\\log C \\\\sum_c \\\\exp (\\\\bar{o}_c)$$\\n\\nAs suggested in (Wang et al., 2022a), we set the threshold $r$ such that $1\\\\%$ of the activations from the train set would be truncated.\\n\\nVirtual Logit Matching (Wang et al., 2022a): The idea behind ViM is that meaningful features are thought to lie in a low-dimensional manifold, called the principal space $P$, whereas features from OOD-samples should also lie in $P^\\\\perp$, the space orthogonal to $P$. $P$ is the $D$-dimensional subspace spanned by the eigenvectors with the largest $D$ eigenvalues of the matrix $F^T F$, where $F$ is the matrix of all train features offsetted by $u = -W^T + b$ (+ denotes the Moore-Penrose inverse). A sample with feature vector $h$ is then also offset to $\\\\tilde{h} = h - u$ and can be decomposed into $\\\\tilde{h} = \\\\tilde{h}_P + \\\\tilde{h}_{P^\\\\perp}$, and $\\\\tilde{h}_{P^\\\\perp}$ is referred to as the Residual of $h$. ViM leverages the Residual and converts it to a virtual logit $o_0 = \\\\alpha \\\\left\\\\| \\\\tilde{h}_{P^\\\\perp} \\\\right\\\\|_2$, where $\\\\alpha = \\\\sum_{i=1}^N \\\\max_{c} o_{ci} \\\\sum_{i=1}^N \\\\left\\\\| h_{P^\\\\perp}^i \\\\right\\\\|_2$ is designed to match the scale of the virtual logit to the scale of the real train logits. The virtual logit is then appended to the original logits of the test sample, i.e. to $o$, and a new probability vector is computed via the softmax function. The probability corresponding to the virtual logit is then the final OOD-score:\\n\\n$$s = -\\\\exp(o_0) \\\\sum_{c=1}^C \\\\exp(o_c) + \\\\exp(o_0)$$\\n\\nLike suggested in (Wang et al., 2022a), we use $D = 1000$ if the dimensionality of the feature space $d$ is $d \\\\geq 2048$, $D = 512$ if $2048 \\\\geq d \\\\geq 768$, and $D = d/2$ rounded to integers otherwise.\\n\\nCosine (Tack et al., 2020; Galil et al., 2023): This method computes the maximum cosine-similarity between the features of a test-sample and embedding vectors $\\\\tilde{u}_c$ (sometimes also called concept-vector):\\n\\n$$s = \\\\max_c \\\\frac{\\\\tilde{u}_c^T h}{\\\\left\\\\| \\\\tilde{u}_c^T h \\\\right\\\\|_2}$$\\n\\nFor zero-shot CLIP, $\\\\tilde{u}_c$ can be obtained by creating text-embeddings from the ImageNet class names. Encoding 'A photo of ...' yields an embedding from the corresponding class. For classifiers, we use the class-wise train means $\\\\hat{\\\\mu}_c$, that are also used for Mahalanobis distance.\"}"}
{"id": "bitterwolf23a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nMCM/RCos (Ming et al., 2022; Techapanurak et al., 2020): Maximum-Concept-Matching was recently introduced as a zero-shot OOD detection method for CLIP and applies additional softmax-scaling to the cosine-similarities of the Cosine method, potentially with a temperature scaling (which we omit, following (Ming et al., 2022)). Again, we extend this method to work with conventional classifiers by using the class-means $\\\\hat{\\\\mu}_c$ like they are used for Mahalanobis distance as embedding/concept vectors. We then refer to it as relative cosine (short: MCM/RCos or just RCos) in order to distinguish it from CLIPs zero-shot method.\\n\\nD. Definitions of OOD detection metrics\\n\\nThe performance of OOD detectors is commonly reported in terms of the false positive rate at a fixed true positive rate $Q$, denoted as $\\\\text{FPR@TPR}_Q$, short $\\\\text{FPR}$. This means that the detector is interpreted as making the decision to accept an unknown input $x$ if $S(x) \\\\geq \\\\tau$, for a threshold $\\\\tau$ that is chosen such that $Q\\\\%$ of ID inputs are accepted, and rejecting the input as OOD if $S(x) < \\\\tau$. The $\\\\text{FPR@TPR}_Q$ counts the fraction of falsely accepted OOD inputs under this decision scheme.\\n\\nThis means the lower the $\\\\text{FPR@TPR}_Q$, the better the OOD detection performance. In the OOD detection literature, the most commonly used value for $Q$ is $95\\\\%$, which we too use throughout this paper. We also report results in terms of the mean area under the receiver-operator characteristic curve, short $\\\\text{AUROC}$ in Table 4. It represents the probability that an ID input receives a higher score (equal scores counted half) than an OOD input when both are drawn randomly from their respective evaluation datasets (Bitterwolf et al., 2022). Like for the $\\\\text{FPR}$, the mean $\\\\text{AUROC}$ corresponds to first uniformly drawing an OOD class and then drawing a sample from that class.\\n\\nE. Illustrative examples from the cleaning process\\n\\n\u2717 (brain coral)\\n\u2717 (coral reef)\\n\u2713 \u2713 (bluestriped grunt)\\n\u2717 (spiderweb)\\n\u2717 (spiderweb)\\n\u2713 \u2713 (cat-faced spider)\\n\u2717 (plane)\\n\u2717 (pole)\\n\u2713 \u2713 (sky)\\n\u2717 (plate)\\n\u2717 (strawberry)\\n\u2713 \u2713 (waffles)\\n\\nFigure 9. Cleaning the OOD classes. Top: Samples that were excluded due to overlap with ID classes. Bottom: Samples from the same OOD class that were included in the cleaned datasets.\"}"}
{"id": "bitterwolf23a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Detailed Information for Each OOD Class\\n\\n| OOD class name            | shortname | # samples | source dataset | ImageNet-21K overlap |\\n|---------------------------|-----------|-----------|----------------|----------------------|\\n| AF A (cyanobacterium)     | AF A      | 46        |                |                      |\\n| bagpipe                   | Bagp      | 97        | imagenet-21k   | bagpipe              |\\n| bluestriped grunt         | BSGrunt   | 96        |                | grunt                |\\n| cable                     | Cabl      | 88        | scraped        | cable television     |\\n| California pitcher plant  | CPitch    | 100       |                | pitcher plant        |\\n| California slender salamander | CSSala | 100       |                | slender salamander    |\\n| California two-spot octopus | C2SOct | 100       |                | octopus              |\\n| caracal                   | Caracal   | 100       | iNat. Download | caracal              |\\n| cat-faced spider          | CatFSp    | 100       |                | unclear/very broad class |\\n| Central American tapir    | CAT apir  | 100       |                | tapir                |\\n| chicken quesadilla        | CQuesa    | 100       |                | creme brulee         |\\n| common cuttlefish         | CCuttle    | 100       |                | cuttlefish           |\\n| creme brulee              | CBrulee   | 99        | FOOD -101      | creme brulee         |\\n| cupcakes                  | CCake     | 80        | FOOD -101      | -                     |\\n| donuts                    | Donuts    | 100       | FOOD -101      | doughnut             |\\n| door                      | Door      | 100       | MyNursingHome  | interior door        |\\n| dreamfish                 | Dreamf    | 100       |                | sea bream            |\\n| dune thistle              | DThist    | 100       |                | creme brulee         |\\n| dusky flathead (fish)     | DFlath    | 100       |                | flathead             |\\n| E. micromeris (cactus)    | EMicro    | 100       |                | -                    |\\n| Eastern leaf-footed bug   | ELFBug    | 100       |                | leaf-footed bug      |\\n| European paper wasp       | EPWasp    | 100       |                | paper wasp           |\\n| false killer whale        | FalseKW  | 67        |                | unclear/very broad class |\\n| field road                | FieldRd   | 96        | P L A C E S    | byway                |\\n| fire extinguisher         | FireEx    | 106       | MyNursingHome  | fire extinguisher    |\\n| fireworks                 | FireW     | 100       | scraped        | -                    |\\n| forest path               | ForPth    | 100       | P L A C E S    | unclear/very broad class |\\n| Franciscan wallflower     | Franci    | 100       |                | wallflower           |\\n| French fries              | Fries     | 100       | FOOD -101      | french fries         |\\n| Gal\u00e1pagos fur seal        | GFurS     | 91        |                | arcella              |\\n| giant cuttlefish          | GCuttle    | 99        |                | cuttlefish           |\\n| glass of milk             | GlMilk    | 89        | scraped        | milk                  |\\n| gramophone                | Gramo     | 56        | scraped        | gramophone           |\\n| high heels                | HHeels    | 99        | scraped        | -                    |\\n| Hindu temple              | HinTp     | 51        | scraped        | unclear/very broad class |\\n| Horse Hoof clam           | HHClam    | 31        |                | seashell             |\\n| Indo-Pacific bottlenose dolphin | IPBNDol | 100       |                | dolphin              |\\n| long-tailed silverfish    | LTSilF    | 100       |                | silverfish           |\\n| Lumholtz's tree-kangaroo  | LTRoo     | 100       |                | tree wallaby         |\\n| M. wesenbergii (cyanobacterium) | MWeisen | 33        |                | microorganism        |\\n| marbled newt              | MNewt     | 100       |                | newt                 |\\n| mbira                     | Mbira     | 67        | scraped        | -                    |\\n| Mexican lime cactus       | MLCact    | 100       |                | barrel cactus        |\\n| Pampas deer               | PDeer     | 82        |                | buck                 |\\n| pyramid                   | Pyra      | 100       | caltech-101    | Cheops                |\\n| redbreast sunfish         | RBSunf    | 100       |                | sunfish              |\\n| rosybells (flowering plant) | Rosyb  | 100       |                | -                    |\\n| ruby octopus              | RubyOct   | 100       |                | octopus              |\\n| scissors                  | Sciss     | 100       | caltech-101    | scissors              |\\n| shuttlecock               | ShCo      | 67        | scraped        | shuttlecock           |\\n| silver-haired bat         | SilverHB  | 99        |                | bat                  |\\n| skipper caterpillar       | SCaterp   | 100       | iNat. Download | caterpillar           |\\n| sky                       | Sky       | 68        | P L A C E S    | sky                  |\\n| southern calamari         | SCalam    | 99        |                | squid                |\\n| spaghetti bolognese       | SBolo     | 67        | FOOD -101      | spaghetti             |\\n| stapler                   | Stapl     | 100       | caltech-101    | stapler               |\\n| sweet pea                 | SwPea     | 100       |                | unclear/very broad class |\\n| two-toed amphiuma (salamander) | 2TAmph | 176       |                | amphiuma             |\\n| waffles                   | Waffle    | 61        | FOOD -101      | -                    |\\n| walker                    | Walker    | 99        | MyNursingHome  | walker               |\\n| water dispenser (jugless) | WDisp     | 100       | MyNursingHome  | water cooler         |\\n| Windsor chair             | WiChair   | 71        | caltech-101    | Windsor chair         |\\n| yellow trumpets           | YTrump    | 100       |                | yellow trumpet        |\\n| '\u00c5ohelo' '\u00c5o (flowering plant) | 'Ao   | 100       |                | -                    |\"}"}
{"id": "bitterwolf23a", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nFigure 12. Samples of each class of the NINCO dataset (3/3).\"}"}
{"id": "bitterwolf23a", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nuniform noise\\nGaussian noise\\nRademacher noise\\nIN pixel permutations\\nblack\\nwhite\\nshades of grey\\nmonochrome\\ntricolour\\nprimary tricolour\\nhorizontal stripes\\nvertical stripes\\nsmooth noise\\nsmooth noise+\\nsmooth color\\nsmooth IN pixel permutations\\nblobs\\n\\nFigure 13. Samples of each OOD unit-test.\"}"}
{"id": "bitterwolf23a", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | Version | Energy | # Fails | Max |\\n|---------------|---------|--------|---------|-----|\\n| ViT-B-384-l2b | 12k     | 87.2   | 0       | 5.2 |\\n| Swinv2-B-256  | 21k     | 86.3   | 2       | 14.2|\\n| ViT-B-384-oai | 12k     | 87.0   | 3       | 14.5|\\n| Deit3-B-224   | 21k     | 85.7   | 1       | 36.2|\\n| clip-ViT-B-224|         | 66.6   | 0       | 100.0|\\n| clip-ViT-L-336|         | 74.3   | 0       | 100.0|\\n\\nFPR of pretrained transformers for unit-tests. The ViTs pretrained only model acc. method # fails max\\n\\n| Metric         | # Fails | Max  |\\n|----------------|---------|------|\\n| Relative Mahalanobis | 10       | 100.0|\\n| Relative Mahalanobis | 9        | 100.0|\\n| Relative Mahalanobis | 4        | 51.5 |\\n| Relative Mahalanobis | 4        | 39.5 |\\n| Relative Mahalanobis | 0        | 9.8  |\\n| Relative Mahalanobis | 0        | 0.2  |\\n| Energy+React    | 1        | 12.2 |\\n| Energy+React    | 0        | 9.8  |\\n| Energy+React    | 3        | 15.0 |\\n| KL-Matching     | 0        | 4.8  |\\n| KL-Matching     | 1        | 12.8 |\\n| KL-Matching     | 8        | 62.0 |\\n| Mahalanobis     | 11       | 100.0|\\n| Mahalanobis     | 7        | 100.0|\\n| MCM/RCos        | 0        | 0.2  |\\n| MCM/RCos        | 3        | 19.0 |\\n| MCM/RCos        | 0        | 0.8  |\\n| MCM/RCos        | 0        | 0.0  |\\n| MaxLogit        | 1        | 11.5 |\\n| MaxLogit        | 0        | 4.5  |\\n| MaxLogit        | 0        | 0.0  |\\n| mcm-clip        | 0        | 5.0  |\\n| mcm-clip        | 1        | 17.0 |\\n| cosine          | 0        | 1.8  |\\n| cosine          | 0        | 1.0  |\\n| cosine          | 0        | 1.5  |\\n| cosine          | 0        | 0.0  |\\n| cosine          | 0        | 0.0  |\\n| MSP             | 1        | 18.0 |\\n| MSP             | 1        | 12.8 |\\n| MSP             | 0        | 7.5  |\\n| MSP             | 5        | 48.5 |\\n| MSP             | 4        | 28.0 |\\n| MSP             | 0        | 7.0  |\\n| MSP             | 0        | 0.0  |\\n| ViM             | 6        | 100.0|\\n| ViM             | 7        | 100.0|\\n| ViM             | 2        | 100.0|\\n| ViM             | 1        | 25.5 |\\n| knn             | 0        | 2.2  |\\n| knn             | 2        | 18.8 |\\n| knn             | 2        | 20.2 |\\n| knn             | 0        | 3.5  |\\n| knn             | 0        | 0.8  |\\n| kNN             | 0        | 3.5  |\\n| kNN             | 0        | 0.8  |\\n| kNN             | 2        | 18.8 |\\n| kNN             | 2        | 20.2 |\\n| kNN             | 0        | 3.5  |\\n| kNN             | 0        | 0.8  |\\n\\nIn or Out? Fixing ImageNet OOD Detection Evaluation.\"}"}
{"id": "bitterwolf23a", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Table 11. In or Out? Fixing ImageNet OOD Detection Evaluation |\\n|-------------------------------------------------------------|\\n| **FPR of transformers without pretraining for OOD unit-tests.** |\\n| | | 6.2 | 100 | 1.0 | 4.5 | 0.0 | 100 | 13 | 100 |\\n| | | 5.2 | 6.2 | 6.5 | 1.0 | 0.0 | 18 | 1.2 | 2.5 |\\n| | | 2.0 | 5.2 | 0.5 | 0.8 | 41 | 33 | 0.0 | 0.2 |\\n| | | 8.0 | 1.0 | 0.0 | 9.8 | 3.2 | 8.0 | 1.0 | 0.0 |\\n| | | 100 | 0.0 | 0.0 | 0.0 | 100 | 100 | 51 | 51 |\\n| | | 28 | 0.0 | 0.0 | 16 | 0.0 | 0.0 | 0.0 | 1.5 |\\n| | | 0.0 | 0.0 | 0.5 | 0.0 | 2.0 | 0.0 | 0.0 | 1.2 |\\n| | | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.5 | 0.0 | 0.0 |\\n| | | 9.8 | 3.2 | 8.0 | 1.0 | 0.0 | 9.8 | 3.2 | 8.0 |\\n| | | 82 | 0.0 | 0.0 | 0.8 | 0.8 | 0.0 | 0.0 | 4.5 |\\n| | | 94 | 0.2 | 0.0 | 0.2 | 0.0 | 3.5 | 0.0 | 0.0 |\\n| | | 100 | 100 | 28 | 0.0 | 0.0 | 100 | 100 | 28 |\\n| | | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 14 | 0.0 | 0.0 | 0.0 | 1.0 | 0.8 | 17 | 1.0 |\\n| | | 0.0 | 0.5 | 0.0 | 2.8 | 0.0 | 11 | 72 | 0.0 |\\n| | | 49 | 0.0 | 25 | 100 | 0.0 | 14 | 0.0 | 1.5 |\\n| | | 14 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 |\\n| | | 12 | 70 | 1.0 | 11 | 20 | 40 | 100 | 0.0 | 9.0 |\\n| | | 74 | 13 | 16 | 2.0 | 5.2 | 0.5 | 0.8 | 27 | 13 |\\n| | | 13 | 11 | 8.5 | 49 | 100 | 0.0 | 1.0 | 2.8 | 47 | 10 |\\n| | | 22 | 23 | 10 | 13 | 2.2 | 3.0 | 0.0 | 8.8 | 0.5 | 2.0 |\\n| | | 100 | 0.0 | 100 | 100 | 20 | 100 | 6.5 | 6.0 | 0.0 | 10 |\\n| | | 12 | 8 | 8.8 | 0.5 | 0.8 | 0.0 | 5.5 | 0.0 | 0.5 | 2.0 |\\n| | | 8 | 7 | 5.0 | 0.0 | 0.5 | 0.0 | 4.0 | 4.0 | 0.8 | 1.0 |\\n| | | 47 | 10 | 6.2 | 10 | 6.5 | 100 | 12 | 100 | 100|\\n| | | 23 | 22 | 74 | 0.0 | 0.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 99 | 0.2 | 7.2 | 1.0 | 0.0 | 0.0 | 0.8 | 8.8 | 0.0 | 2.2 |\\n| | | 49 | 0.0 | 25 | 100 | 0.0 | 0.0 | 2.5 | 0.0 | 16 |\\n| | | 31 | 0.0 | 41 | 1.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 15 | 0.0 | 1.8 | 5.0 | 0.2 | 0.0 | 5.2 | 0.2 | 0.0 | 0.0 |\\n| | | 8 | 100 | 0.0 | 100 | 0.0 | 100 | 100 | 51 | 51 |\\n| | | 3 | 13 | 8 | 5 | 25 | 20 | 40 | 100 | 0.0 | 9.0 |\\n| | | 100 | 0.0 | 100 | 100 | 28 | 0.0 | 0.2 | 1.2 | 8.8 | 0.0 |\\n| | | 11 | 17 | 72 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 49 | 0.0 | 25 | 100 | 0.0 | 0.0 | 2.5 | 0.0 | 16 | 31 |\\n| | | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 |\\n| | | 22 | 23 | 10 | 13 | 2.2 | 3.0 | 0.0 | 8.8 | 0.5 | 2.0 |\\n| | | 100 | 0.0 | 100 | 100 | 20 | 100 | 6.5 | 6.0 | 0.0 | 10 |\\n| | | 12 | 8 | 8.8 | 0.5 | 0.8 | 0.0 | 5.5 | 0.0 | 0.5 | 2.0 |\\n| | | 100 | 100 | 20 | 100 | 100 | 0.2 | 9.2 | 8.2 | 0.0 | 9.0 |\\n| | | 0.0 | 1.8 | 80 | 1.0 | 0.0 | 0.2 | 1.0 | 0.0 | 8.0 | 0.0 | 0.5 |\\n| | | 3.2 | 2.8 | 0.2 | 3.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 14 | 10 | 18 | 5.0 | 3.2 | 1.0 | 0.0 | 6.2 | 1.8 | 8.0 | 0.8 |\\n| | | 5.5 | 3.8 | 3.2 | 9.5 | 0.2 | 0.5 | 2.2 | 5.5 | 2.0 | 5.8 | 0.0 |\\n| | | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 0.0 | 0.0 | 0.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 4.0 | 4.0 | 6.8 | 0.8 | 1.0 | 7.2 | 0.5 | 57 | 0.0 | 0.5 |\\n| | | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 2.0 | 4.0 | 0.0 | 1.0 |\\n| | | 8.0 | 0.0 | 0.5 | 0.0 | 3.2 | 2.8 | 0.2 | 3.5 | 0.0 | 0.0 | 0.0 |\\n| | | 14 | 10 | 18 | 5.0 | 3.2 | 1.0 | 0.0 | 6.2 | 1.8 | 8.0 | 0.8 |\\n| | | 23 | 22 | 74 | 0.0 | 0.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 8 100 | 100 | 0.0 | 100 | 0.0 | 100 | 100 | 51 | 51 |\\n| | | 3 | 13 | 8 | 5 | 25 | 20 | 40 | 100 | 0.0 | 9.0 |\\n| | | 100 | 0.0 | 100 | 100 | 28 | 0.0 | 0.2 | 1.2 | 8.8 | 0.0 |\\n| | | 0.0 | 0.5 | 1.8 | 80 | 1.0 | 0.0 | 0.2 | 1.0 | 0.0 | 8.0 | 0.0 |\\n| | | 3.2 | 2.8 | 0.2 | 3.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 14 | 10 | 18 | 5.0 | 3.2 | 1.0 | 0.0 | 6.2 | 1.8 | 8.0 | 0.8 |\\n| | | 23 | 22 | 74 | 0.0 | 0.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 8 100 | 100 | 0.0 | 100 | 0.0 | 100 | 100 | 51 | 51 |\\n| | | 3 | 13 | 8 | 5 | 25 | 20 | 40 | 100 | 0.0 | 9.0 |\\n| | | 100 | 0.0 | 100 | 100 | 28 | 0.0 | 0.2 | 1.2 | 8.8 | 0.0 |\\n| | | 0.0 | 0.5 | 1.8 | 80 | 1.0 | 0.0 | 0.2 | 1.0 | 0.0 | 8.0 | 0.0 |\\n| | | 3.2 | 2.8 | 0.2 | 3.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 14 | 10 | 18 | 5.0 | 3.2 | 1.0 | 0.0 | 6.2 | 1.8 | 8.0 | 0.8 |\\n| | | 23 | 22 | 74 | 0.0 | 0.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 8 100 | 100 | 0.0 | 100 | 0.0 | 100 | 100 | 51 | 51 |\\n| | | 3 | 13 | 8 | 5 | 25 | 20 | 40 | 100 | 0.0 | 9.0 |\\n| | | 100 | 0.0 | 100 | 100 | 28 | 0.0 | 0.2 | 1.2 | 8.8 | 0.0 |\\n| | | 0.0 | 0.5 | 1.8 | 80 | 1.0 | 0.0 | 0.2 | 1.0 | 0.0 | 8.0 | 0.0 |\\n| | | 3.2 | 2.8 | 0.2 | 3.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| | | 14 | 10 | 18 | 5.0 | 3.2 | 1.0 | 0.0 | 6.2 | 1.8 | 8.0 | 0.8 |\"}"}
{"id": "bitterwolf23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nGramo\\nRosyb\\nYTrump\\nSwPea\\nCPitch\\nMLCact\\nFranci\\nFireEx\\nRBSunf\\nai\\nEMicro\\nBSGrunt\\nDFlath\\nELFBug\\nSCaterp\\nForPth\\nFries\\nMWesen\\nSilverHB\\nGCuttle\\nCAT apir\\nSCalam\\nBagp\\nPyra\\nShCo\\nLTSilF\\nDThist\\nCCuttle\\nSciss\\nStapl\\nFireW\\nCBr\u00fbl\u00e9e\\nWiChair\\nEPWasp\\nCabl\\nWDisp\\nRubyOct\\nC2SOct\\nMbira\\nFieldRd\\nDreamf\\nIPBNDol\\nSky\\nCCake\\nCSSala\\nWalker\\nPDeer\\nCaracal\\nFalseKW\\nWaffle\\nDoor\\nAFA\\nHHClam\\nHinTp\\nSBolo\\n2TAmph\\nCQuesa\\nLTRoo\\nHHeels\\nMNewt\\nGlMilk\\nDonuts\\nCatFSp\\nGFurS\\n\\nFigure 7.\\n\\nFPR of a pretrained ViT-B and pretrained ConvNext-B for all classes of NINCO.\\n\\n91% FPR). From a human perspective, those classes are arguably hard to detect. We note, however, that it is possible to tell them apart, as a ViT IN-21K-classifier e.g. identifies the Gal\u00e1pagos fur seal as a fur seal (IN-21K class) in 92% of samples and misclassifies only 6% of them as a sea lion.\\n\\nThe networks however also fail for classes more obvious to humans: donut (84% FPR ViT, confused with bagel), spaghetti bolognese (69% FPR, carbonara) and chicken quesadilla (73% FPR, burrito) also confuse both models.\\n\\n4.2. Results on the OOD unit-tests\\n\\nAuditing OOD detectors on the OOD unit-tests, we find that surprisingly many combinations of models and OOD detection methods struggle to distinguish supposedly easy inputs from ID-data. While results for all models and methods can be found in Appendix I, we provide some illustrative unit-test results in Table 2 for a ViT pretrained on IN-21k and a ConvNext both with and without IN-21K pretraining. In general, most methods fail fewer unit tests when applied to pretrained models, however there are still many severely flawed combinations, often involving methods that would otherwise shine based on their detection of natural OOD data discussed above: especially the feature-based methods ViM, Maha and RMaha reveal weaknesses, each failing multiple unit-tests on at least 21 of 26 models. Many tested OOD detectors are vulnerable to black, white and grey, which is concerning as encountering inputs of this kind could occur in many real-world applications due to camera malfunction or occlusion.\\n\\nTable 2. Some detectors fail OOD unit-tests: FPR for a ViT and a ConvNext (with and without pretraining) on selected unit-tests.\\n\\nFPR larger than 10% count as failed and are thus marked red.\\n\\n| Method | MSP | VIT21k | VIT1k | VIT21k | VIT1k | VIT21k | VIT1k | VIT21k | VIT1k | VIT21k | VIT1k | VIT21k | VIT1k | VIT21k | VIT1k |\\n|--------|-----|--------|-------|--------|-------|--------|-------|--------|-------|--------|-------|--------|-------|--------|-------|\\n| MSP    | 0.0 | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   |\\n| ViM    | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |\\n| Maha   | 0.0 | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   |\\n| Cos    | 0.0 | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   |\\n| Cnv1k  | 0.0 | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   |\\n| Cnv21k | 0.0 | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   | 0.0    | 0.0   |\\n\\nEspecially for methods relying on feature representations (like ViM and Maha) the OOD unit-tests reveal difficulties.\\n\\n5. Conclusions\\n\\nWe introduce with NINCO a novel, ID-contamination-free and challenging OOD test-dataset for IN-1K with fine-grained class-resolution. We find that many OOD detectors work better than previously thought, when their recorded number of undetected OOD inputs is not inflated by ID contaminations. However, most detection methods cannot reliably be applied with arbitrary classifier models, as even OOD unit-tests are failed by many combinations. We are hopeful for NINCO and the cleaned test OOD subsets to facilitate the more precise development of reliable OOD detectors which do not try to avoid presumed failures which are actually correct decisions.\\n\\nAcknowledgements\\n\\nWe thank Vaclav Voracek for helpful discussions and suggesting the cdf plots. We acknowledge support from the German Federal Ministry of Education and Research (BMBF) through the T\u00c9ubingen AI Center (FKZ: 01IS18039A) and from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany\u2019s Excellence Strategy (EXC number 2064/1, Project number 390727645), as well as from the Carl Zeiss Foundation in the project \u00aaCertification and Foundations of Safe Machine Learning Systems in Healthcare\u00ba. We also thank the European Laboratory for Learning and Intelligent Systems (ELLIS) for supporting Maximilian M\u00fcller.\\n\\nReferences\\n\\nBendale, A. and Boult, T. Towards open set deep networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. URL https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.173.\"}"}
{"id": "bitterwolf23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nBitterwolf, J., Meinke, A., and Hein, M. Certifiably adversarially robust detection of out-of-distribution data. In NeurIPS, 2020.\\n\\nBitterwolf, J., Meinke, A., Augustin, M., and Hein, M. Breaking down out-of-distribution detection: Many methods based on ood training data estimate a combination of the same core quantities. In ICML, 2022.\\n\\nBossard, L., Guillaumin, M., and Van Gool, L. Food-101 \u00b1 mining discriminative components with random forests. In Fleet, D., Pajdla, T., Schiele, B., and Tuytelaars, T. (eds.), Computer Vision \u00b1 ECCV 2014, pp. 446\u00b1461, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10599-4.\\n\\nCimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A. Describing textures in the wild. In CVPR, 2014.\\n\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.\\n\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.\\n\\nFellbaum, C. Wordnet: An electronic lexical database. Cambridge, MA: MIT Press, 1998.\\n\\nFort, S., Ren, J., and Lakshminarayanan, B. Exploring the limits of out-of-distribution detection. In NeurIPS, 2021. URL https://openreview.net/forum?id=j5NrN8ffXC.\\n\\nGalil, I., Dabbah, M., and El-Yaniv, R. A framework for benchmarking class-out-of-distribution detection and its application to imagenet. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Iuubb9W6Jtk.\\n\\nHein, M., Andriushchenko, M., and Bitterwolf, J. Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem. In CVPR, 2019.\\n\\nHendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In ICLR, 2017. URL https://openreview.net/forum?id=Hkg4TI9xl.\\n\\nHendrycks, D., Mazeika, M., and Dietterich, T. Deep anomaly detection with outlier exposure. In ICLR, 2019. https://github.com/hendrycks/outlier-exposure.\\n\\nHendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples. In CVPR, 2021.\\n\\nHendrycks, D., Basart, S., Mazeika, M., Zou, A., Kwon, J., Mostajabi, M., Steinhardt, J., and Song, D. Scaling out-of-distribution detection for real-world settings. In ICML, 2022.\\n\\nHuang, R. and Li, Y. MOS: Towards scaling out-of-distribution detection for large semantic space. In CVPR, 2021.\\n\\nIsmail, A., Ahmad, S. A., Che Soh, A., Hassan, M. K., and Harith, H. H. Mynursinghome: A fully-labelled image dataset for indoor object classification. Data in Brief, 2020. doi: https://doi.org/10.1016/j.dib.2020.106268. URL https://www.sciencedirect.com/science/article/pii/S2352340920311628.\\n\\nKolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Young, J., Gelly, S., and Houlsby, N. Big transfer (bit): General visual representation learning. In ECCV, 2020. URL https://doi.org/10.1007/978-3-030-58558-7_29.\\n\\nKoner, R., Sinhamahapatra, P., Roscher, K., G\u00a8unnemann, S., and Tresp, V. Oodformer: Out-of-distribution detection transformer. arXiv preprint arXiv:2107.08976, 2021.\\n\\nKrasin, I., Duerig, T., Alldrin, N., Ferrari, V., Abu-El-Haija, S., Kuznetsova, A., Rom, H., Uijlings, J., Popov, S., Kamali, S., Malloci, M., Pont-Tuset, J., Veit, A., Belongie, S., Gomes, V., Gupta, A., Sun, C., Chechik, G., Cai, D., Feng, Z., Narayanan, D., and Murphy, K. Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from storage.googleapis.com/openimages/web/index.html, 2017.\\n\\nKrizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.\\n\\nLee, K., Lee, K., Lee, H., and Shin, J. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In NeurIPS, 2018.\\n\\nLi, F.-F., Andreeto, M., Ranzato, M., and Perona, P. Caltech 101, 2022.\\n\\nLiu, W., Wang, X., Owens, J., and Li, Y. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems, 2020.\\n\\nMeinke, A., Bitterwolf, J., and Hein, M. Provably adversarially robust detection of out-of-distribution data (almost) for free. NeurIPS, 2022.\"}"}
{"id": "bitterwolf23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nMing, Y., Cai, Z., Gu, J., Sun, Y., Li, W., and Li, Y. Delving into out-of-distribution detection with vision-language representations. In NeurIPS, 2022. URL https://openreview.net/forum?id=KnCS9390Va.\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In ICML, 2021.\\n\\nRen, J., Fort, S., Liu, J., Roy, A. G., Padhy, S., and Lakshminarayanan, B. A simple fix to mahalanobis distance for improving near-ood detection, 2021. URL https://arxiv.org/abs/2106.09022.\\n\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015.\\n\\nSalehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., and Sabokrou, M. A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges. arXiv preprint arXiv:2110.14051, 2021.\\n\\nShankar, V., Roelofs, R., Mania, H., Fang, A., Recht, B., and Schmidt, L. Evaluating machine accuracy on imagenet. In NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future, 2021. URL https://openreview.net/forum?id=Q3R088EFtng.\\n\\nSong, Y., Sebe, N., and Wang, W. Rankfeat: Rank-1 feature removal for out-of-distribution detection. In NeurIPS, 2022. URL https://openreview.net/forum?id=-deKNiSOXLG.\\n\\nSteiner, A. P., Kolesnikov, A., Zhai, X., Wightman, R., Uszkoreit, J., and Beyer, L. How to train your vit? data, augmentation, and regularization in vision transformers. TMLR, 2022. URL https://openreview.net/forum?id=4nPswr1KcP.\\n\\nSun, Y., Guo, C., and Li, Y. React: Out-of-distribution detection with rectified activations. In NeurIPS, 2021.\\n\\nSun, Y., Ming, Y., Zhu, X., and Li, Y. Out-of-distribution detection with deep nearest neighbors. ICML, 2022.\\n\\nTack, J., Mo, S., Jeong, J., and Shin, J. Csi: Novelty detection via contrastive learning on distributionally shifted instances. In NeurIPS, 2020.\\n\\nTechapanurak, E., Suganuma, M., and Okatani, T. Hyperparameter-free out-of-distribution detection using cosine similarity. In Proceedings of the Asian Conference on Computer Vision, 2020.\\n\\nTouvron, H., Cord, M., and Jegou, H. Deit iii: Revenge of the vit. ECCV, 2022.\\n\\nVan Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S. The inaturalist species classification and detection dataset. In CVPR, 2018.\\n\\nVasudevan, V., Caine, B., Gontijo-Lopes, R., Fridovich-Keil, S., and Roelofs, R. When does dough become a bagel? analyzing the remaining mistakes on imagenet. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=mowt1WNhTC7.\\n\\nVaze, S., Han, K., Vedaldi, A., and Zisserman, A. Open-set recognition: a good closed-set classifier is all you need? In International Conference on Learning Representations, 2022.\\n\\nWang, H., Liu, W., Bocchieri, A., and Li, Y. Can multi-label classification networks know what they don't know? NeurIPS, 2021.\\n\\nWang, H., Li, Z., Feng, L., and Zhang, W. Vim: Out-of-distribution with virtual-logit matching. In CVPR, 2022a.\\n\\nWang, H., Zhang, A., Zhu, Y., Zheng, S., Li, M., Smola, A., and Wang, Z. Partial and asymmetric contrastive learning for out-of-distribution detection in long-tailed recognition. In ICML 2022, 2022b.\\n\\nWen, X., Zhao, B., and Qi, X. A simple parametric classification baseline for generalized category discovery. ArXiv, abs/2211.11727, 2022.\\n\\nWightman, R. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019.\\n\\nXia, G. and Bouganis, C.-S. On the usefulness of deep ensemble diversity for out-of-distribution detection. arXiv preprint arXiv:2207.07517, 2022.\\n\\nXie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training with noisy student improves imagenet classification. arXiv preprint arXiv:1911.04252, 2019.\\n\\nYang, J., Wang, P., Zou, D., Zhou, Z., Ding, K., Peng, W., Wang, H., Chen, G., Li, B., Sun, Y., et al. Openood: Benchmarking generalized out-of-distribution detection. arXiv preprint arXiv:2210.07242, 2022.\\n\\nZhou, B., Lapedriza, A., Khosla, A., Oliva, A., and Torralba, A. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452\u00b11464, 2017.\"}"}
{"id": "bitterwolf23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A detailed overview over the results on the NINCO benchmark is presented in Table 3, where we show the mean FPR for all models and methods across the dataset's OOD classes. Tables 4-6 show AUROC, AUPR-S and AUPR-E with the same conclusions. The best method per model is marked bold, and the difference to the MSP-baseline is shown in green where a model outperforms the MSP-baseline and in red if it performs worse than MSP. It is clearly visible that there is no one-fits-all method. Instead, different models synergize with different methods. Overall, the two ViT models pretrained only on IN-21K in combination with Mahalanobis distance outperform other models and methods by a clear margin. This is in line with the observations of previous works (Koner et al., 2021; Fort et al., 2021; Galil et al., 2023), which also found the ViTs to perform exceptionally well. In terms of MSP, the ViTs are not better than e.g. the ConvNext, indicating that their improved OOD detection capabilities stem from a favourably structured feature-space. It is further interesting to see that for models without pretraining, out of all methods only Relative Mahalanobis and the cosine-based methods improve over the MSP-baseline fairly consistently. Apart from KL-Matching and KNN, most methods improve over the MSP-baseline for most pretrained models and the CLIP-methods Cosine and RCos perform comparably well, yielding their best results with models pretrained both on CLIP and IN-12k. Since CLIP models are trained with cosine-similarity, it is likely that the structure of the feature space after finetuning remains favorable to cosine-based methods, while it might harm the performance of other feature-based methods like Mahalanobis compared to models pretrained only on IN-21k.\\n\\nIt has been remarked (Hendrycks et al., 2022) that the advantage of models pretrained with IN-21K in the OOD detection task CIFAR-10 vs. CIFAR-100 (Krizhevsky & Hinton, 2009) might partially be explained by the CIFAR-100 classes not truly being unseen at train time, as they have a large overlap with IN-21K classes. We checked each NINCO class for overlap with the 21,824 classes of IN-21K with the help of a ViT classifier for IN-21K, see Table 9. This allows us to test whether the pretrained models have a larger advantage over purely IN-1k-trained models when trying to detect those classes with overlap compared to the classes without overlap. In Appendix K, notice no substantially different changes between the models with and without pretraining. We remark, however, that even for several models without pretraining, the subselections of classes show quite different results.\"}"}
{"id": "bitterwolf23a", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 17.\\n\\n| Model          | RMaha | KL-M | MaxL | Maha | RCos | KNN | MSP | E+R | ViM | Cos |\\n|----------------|-------|------|------|------|------|-----|-----|-----|-----|-----|\\n| Swinv2-B-256   |       |      |      |      |      |     |     |     |     |     |\\n| Deit3-B-224    |       |      |      |      |      |     |     |     |     |     |\\n| Deit3-B-384    |       |      |      |      |      |     |     |     |     |     |\\n| ViT-B-384      |       |      |      |      |      |     |     |     |     |     |\\n\\nComparing the cleaned and original datasets in terms of FPR. The best method per model and dataset is marked bold.\"}"}
{"id": "bitterwolf23a", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | In-Out | These | Those |\\n|----------------|--------|-------|-------|\\n| Acc.           | 63.4   | 51.0  | 70.6  |\\n| 51.0           | 70.6   | 88.5  | 47.3  |\\n| 47.3           | 65.8   | 26.9  | 78.4  |\\n| 26.9           | 78.4   | 11.8  | 93.1  |\\n| 11.8           | 93.1   | 82.3  | 88.2  |\\n| 82.3           | 88.2   | 82.6  | 66.0  |\\n| 82.6           | 66.0   | 32.0  | 77.8  |\\n| 66.0           | 32.0   | 27    | 95.8  |\\n| 32.0           | 95.8   | 71.5  | 91.5  |\\n| 95.8           | 91.5   | 78.0  | 88.9  |\\n| 91.5           | 78.0   | 64.4  | 80.2  |\\n| 78.0           | 80.2   | 75.8  | 83.1  |\\n| 80.2           | 75.8   | 88.0  | CO-f  |\\n| 75.8           | 88.0   | 76.5  | 70.0  |\\n| 88.0           | 76.5   | 70.0  | 81.2  |\\n| 76.5           | 70.0   | 61.3  | 67.2  |\\n| 70.0           | 61.3   | 67.2  | 72.2  |\\n| 61.3           | 67.2   | 63.0  | 68    |\\n| 67.2           | 63.0   | 68    | 79.5  |\\n| 63.0           | 68    | 45.3  | 97.8  |\\n| 68             | 97.8   | 76.7  | 53.3  |\\n| 97.8           | 76.7   | 57.0  | 58.4  |\\n| 76.7           | 57.0   | 58.4  | 60.3  |\\n| 57.0           | 58.4   | 60.3  | 29.0  |\\n| 58.4           | 60.3   | 29.0  | 86.8  |\\n| 60.3           | 29.0   | 86.8  | 88.7  |\\n| 29.0           | 86.8   | 88.7  | 75.6  |\\n| 86.8           | 88.7   | 75.6  | 21.2  |\\n| 88.7           | 75.6   | 21.2  | 74.2  |\\n| 75.6           | 21.2   | 74.2  | 60.1  |\\n| 21.2           | 74.2   | 60.1  | 69.8  |\\n| 74.2           | 60.1   | 69.8  | 50.0  |\\n| 60.1           | 69.8   | 50.0  | 89.8  |\\n| 69.8           | 50.0   | 89.8  | 75.5  |\\n| 50.0           | 89.8   | 75.5  | 41.1  |\\n| 89.8           | 75.5   | 41.1  | 44.2  |\\n| 75.5           | 41.1   | 44.2  | 71.5  |\\n| 41.1           | 44.2   | 71.5  | 54.8  |\\n| 44.2           | 71.5   | 54.8  | 79.1  |\\n| 71.5           | 54.8   | 79.1  | 51.0  |\\n| 54.8           | 79.1   | 51.0  | 93.2  |\\n| 79.1           | 51.0   | 93.2  | 48.6  |\\n| 51.0           | 93.2   | 48.6  | 79.7  |\\n| 93.2           | 48.6   | 79.7  | 36    |\\n| 48.6           | 79.7   | 36    | 34.8  |\\n| 79.7           | 36    | 34.8  | 76.6  |\\n| 36             | 34.8   | 76.6  | 58.1  |\\n| 34.8           | 76.6   | 58.1  | 56.2  |\\n| 76.6           | 58.1   | 56.2  | 59.5  |\\n| 58.1           | 56.2   | 59.5  | 83.2  |\\n| 56.2           | 59.5   | 83.2  | 87.5  |\\n| 59.5           | 83.2   | 87.5  | 59.3  |\\n| 83.2           | 87.5   | 59.3  | 58.3  |\\n| 87.5           | 59.3   | 58.3  | 77.5  |\\n| 59.3           | 58.3   | 77.5  | 51.3  |\\n| 58.3           | 77.5   | 51.3  | 92.1  |\\n| 77.5           | 51.3   | 92.1  | 92.7  |\\n| 51.3           | 92.1   | 92.7  | 89.2  |\\n| 92.1           | 92.7   | 89.2  | 66.1  |\\n| 92.7           | 89.2   | 66.1  | 84.8  |\\n| 89.2           | 66.1   | 84.8  | 59.0  |\\n| 66.1           | 84.8   | 59.0  | 64.6  |\\n| 84.8           | 59.0   | 64.6  | 78.8  |\\n| 59.0           | 64.6   | 78.8  | 83.5  |\\n| 64.6           | 78.8   | 83.5  | 82.3  |\\n| 78.8           | 83.5   | 82.3  | 53.6  |\\n| 83.5           | 82.3   | 53.6  | 63  |\\n| 82.3           | 53.6   | 63   | 85.0  |\\n| 53.6           | 63   | 85.0  | 63  |\\n| 63             | 85.0   | 63   | 83.9  |\\n| 85.0           | 63   | 83.9  | 84.4  |\\n| 63             | 83.9  | 84.4  | 86.0  |\\n| 83.9           | 84.4  | 86.0  | 73.3  |\\n| 84.4           | 86.0  | 73.3  | 64.9  |\\n| 86.0           | 73.3  | 64.9  | 81.6  |\\n| 73.3           | 64.9  | 81.6  | 54.3  |\\n| 64.9           | 81.6  | 54.3  | 60.5  |\\n| 81.6           | 54.3  | 60.5  | 36.5  |\\n| 54.3           | 60.5  | 36.5  | 10.5  |\\n| 60.5           | 36.5  | 10.5  | 74.0  |\\n| 36.5           | 10.5  | 74.0  | 58.8  |\\n| 10.5           | 74.0  | 58.8  | 81.5  |\\n| 74.0           | 58.8  | 81.5  | 45.2  |\\n| 58.8           | 81.5  | 45.2  | 64.2  |\\n| 81.5           | 45.2  | 64.2  | 64.5  |\\n| 45.2           | 64.2  | 64.5  | 86.4  |\\n| 64.2           | 64.5  | 86.4  | 42  |\\n| 64.5           | 86.4  | 42  |\\n| 86.4           | 42  |\\n| 42             | 82  |\\n| 82             | 48.4  |\\n| 48.4           | 77.4  |\\n| 77.4           | 29.5  |\\n| 29.5           | 59.8  |\\n| 59.8           | 39.1  |\\n| 39.1           | 71.4  |\\n| 71.4           | 44.1  |\\n| 44.1           | 35.8  |\\n| 35.8           | 48.6  |\\n| 48.6           | 66.2  |\\n| 66.2           | 73.0  |\\n| 73.0           | 41.3  |\\n| 41.3           | 65.8  |\\n| 65.8           | 83.9  |\\n| 83.9           | 46.1  |\\n| 46.1           | 43.8  |\\n| 43.8           | 60.9  |\\n| 60.9           | 80.6  |\\n| 80.6           | 36.3  |\\n| 36.3           | 74.2  |\\n| 74.2           | 84.2  |\\n| 84.2           | 63.1  |\\n| 63.1           | 49.0  |\\n| 49.0           | 55.6  |\\n| 55.6           | 70.2  |\\n| 70.2           | 68.9  |\\n| 68.9           | 85.6  |\\n| 85.6           | 67  |\\n| 67  |\\n| 67             | 39.2  |\\n| 39.2           | 65.7  |\\n| 65.7           | 88.4  |\\n| 88.4           | 56.8  |\\n| 56.8           | 54.9  |\\n| 54.9           | 79.0  |\\n| 79.0           | 60.3  |\\n| 60.3           | 73.8  |\\n| 73.8           | 79.0  |\\n| 79.0           | 70.6  |\\n| 70.6           | 81.5  |\\n| 81.5           | 77.1  |\\n| 77.1           | 94.5  |\\n| 94.5           | 86.0  |\\n| 86.0           | 26.0  |\\n| 26.0           | 83.0  |\\n| 83.0           | 58.2  |\\n| 58.2           | 50  |\\n| 50            | 77.0  |\\n| 77.0           | 81.2  |\\n| 81.2           | 47.8  |\\n| 47.8           | 92.5  |\\n| 92.5           | 22  |\\n| 22            | 48.0  |\\n| 48.0           | 64.4  |\\n| 64.4           | 44.2  |\\n| 44.2           | 79.3  |\\n| 79.3           | 27.7  |\\n| 27.7           | 39.1  |\\n| 39.1           | 79.8  |\\n| 79.8           | 58.9  |\\n| 58.9           | 65  |\\n| 65            | 23  |\\n| 23            | 81.8  |\\n| 81.8           | 75.5  |\\n| 75.5           | 70.8  |\\n| 70.8           | 53.3  |\\n| 53.3           | 87.2  |\\n| 87.2           | 84.8  |\\n| 84.8           | 73.6  |\\n| 73.6           | 79.0  |\\n| 79.0           | 33.7  |\\n| 33.7           | 49.5  |\\n| 49.5           | 70.2  |\\n| 70.2           | 83.9  |\\n| 83.9           | 74.2  |\\n| 74.2           | 76.8  |\\n| 76.8           | 74.5  |\\n| 74.5           | 55  |\\n| 55            | 99.2  |\\n| 99.2           | 28.5  |\\n| 28.5           | 62.3  |\\n| 62.3           | 49.2  |\\n| 49.2           | 3  |\\n| 3            | 55.8  |\\n| 55.8           | 69.2  |\\n| 69.2           | 73.4  |\\n| 73.4           | 97.0  |\\n| 97.0           | 72.0  |\\n| 72.0           | 57.0  |\\n| 57.0           | 31.5  |\\n| 31.5           | 49.8  |\\n| 49.8           | 72.6"}
{"id": "bitterwolf23a", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Since the classes of NINCO can be distinguished by whether they belong to an IN-21k class or not, we present results on both of these groups here. We note that they should be taken with care, since the groups differ both in size (9 vs. 55 classes) and difficulty of the individual classes. Most models and methods perform better on the classes with IN-21k overlap, and ViT+Maha is the best OOD-detector in both cases. While RMaha and (Relative) Cosine yield the most consistent improvements over MSP in both cases, ViM performs comparably better on the classes without overlap. Pretraining only on IN-21k yields the best OOD-detectors in both cases.\\n\\nTable 1. Mean FPR for classes without 21k overlap.\\n\\n| Model                      | Inacc. | MSP | MaxL | Ener | KL-M | Maha | RMaha | ViM | E+R | KNN | Cos | MCM/RCos |\\n|----------------------------|--------|-----|------|------|------|------|-------|-----|-----|-----|-----|-----------|\\n| none                       |        |     |      |      |      |      |       |     |     |     |     |           |\\n| ViT-B-384                  | 56.5    | 41.8 | \u221215  | 39.6 | \u221217  | 51.7 | \u221225   | 31.7 | \u221225  | 36.9 | \u221216  | 67.3      |\\n| ViT-B-224                  | 64.8    | 50.6 | \u221214  | 48.3 | \u221217  | 60.2 | \u221221   | 34.1 | \u221231  | 43.7 | \u221221  | 42.2      |\\n| Swinv2-B-256               | 66.3    | 58.7 | \u22128   | 59.1 | \u22127   | 62.0 | \u22124    | 40.1 | \u221226  | 42.7 | \u221224  | 34.3      |\\n| Deit3-B-384                | 72.9    | 71.1 | \u22122   | 73.3 | +0   | 68.6 | \u22124    | 43.0 | \u221230  | 43.6 | \u221229  | 64.4      |\\n| Deit3-B-224                | 75.1    | 72.8 | \u22122   | 72.6 | \u22123   | 69.5 | \u22126    | 47.7 | \u221227  | 48.7 | \u221226  | 67.5      |\\n| CnvNxt-B                   | 61.4    | 60.0 | \u22121   | 67.0 | +6   | 57.6 | \u22124    | 31.0 | \u221230  | 37.4 | \u221224  | 27.5      |\\n| CnvNxt-T                   | 62.9    | 57.2 | \u22126   | 54.4 | \u22129   | 61.6 | \u22121    | 34.7 | \u221228  | 42.2 | \u221221  | 30.6      |\\n| BiT-m                      | 69.7    | 62.2 | \u22127   | 63.9 | \u22126   | 62.6 | \u22127    | 40.9 | \u221229  | 42.1 | \u221228  | 31.5      |\\n| EffNetv2-M                 | 55.9    | 51.8 | \u22124   | 56.3 | +0   | 55.7 | \u22120    | 48.6 | \u22127   | 46.9 | \u22129   | 53.3      |\\n| EffNetb7                   | 69.0    | 70.5 | +2   | 81.3 | +12  | 62.5 | \u22127    | 55.5 | \u221214  | 50.4 | \u221219  | 50.4      |\\n| EffNet-B0                  | 75.0    | 75.9 | +1   | 84.0 | +9   | 68.7 | \u22128    | 55.5 | \u221214  | 50.4 | \u221219  | 58.7      |\\n| ResNet50                   | 76.0    | 76.6 | +1   | 77.5 | +1   | 69.0 | \u22121    | 77.0 | +1   | 94.8 | +19  | 64.0      |\\n| JFT                        | 86.8    | 74.8 |     | 87.2 | +12  | 72.0 | +1    | 85.2 | +14  | 65.8 | \u22126   | 85.2      |\\n| clip-JFT                    | 87.2    | 64.8 | \u22127   | 67.5 | \u22124   | 66.5 | \u22125    | 83.7 | +12  | 72.0 | +1   | 85.2      |\\n\\nTable 19. Mean FPR for classes without 21k overlap.\"}"}
{"id": "bitterwolf23a", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model       | Acc. | MaxL | Ener | KL-M | Maha | RMaha | ViM | E+R | KNN | MCM/RCos |\\n|------------|------|------|------|------|------|-------|-----|-----|-----|----------|\\n| **21k**    |      |      |      |      |      |       |     |     |     |          |\\n| ViT-B-384  | 86.0 |      |      |      |      |       |     |     |     |          |\\n| iT-B-384   | 84.5 |      |      |      |      |       |     |     |     |          |\\n| iT-B-224   | 86.3 |      |      |      |      |       |     |     |     |          |\\n| Swinv2-B-256 | 86.7 |      |      |      |      |       |     |     |     |          |\\n| Deit3-B-384 | 85.7 |      |      |      |      |       |     |     |     |          |\\n| Deit3-B-224 | 85.1 |      |      |      |      |       |     |     |     |          |\\n| CnvNxt-B   | 84.1 |      |      |      |      |       |     |     |     |          |\\n| CnvNxt-T   | 82.3 |      |      |      |      |       |     |     |     |          |\\n| BiT-m      | 85.6 |      |      |      |      |       |     |     |     |          |\\n| EffNetv2-M | 84.9 |      |      |      |      |       |     |     |     |          |\\n| EffNetb7   | 85.1 |      |      |      |      |       |     |     |     |          |\\n| ResNet50   | 80.4 |      |      |      |      |       |     |     |     |          |\\n| JFT        |      |      |      |      |      |       |     |     |     |          |\\n\\n**Table 20.** Mean FPR for classes with 21k overlap.\"}"}
{"id": "bitterwolf23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Out-of-distribution (OOD) detection is the problem of identifying inputs which are unrelated to the in-distribution task. The OOD detection performance when the in-distribution (ID) is ImageNet-1K is commonly being tested on a small range of test OOD datasets. We find that most of the currently used test OOD datasets, including datasets from the open set recognition (OSR) literature, have severe issues: in some cases more than 50% of the dataset contains objects belonging to one of the ID classes. These erroneous samples heavily distort the evaluation of OOD detectors. As a solution, we introduce with NINCO a novel test OOD dataset, each sample checked to be ID free, which with its fine-grained range of OOD classes allows for a detailed analysis of an OOD detector's strengths and failure modes, particularly when paired with a number of synthetic 'OOD unit-tests'. We provide detailed evaluations across a large set of architectures and OOD detection methods on NINCO and the unit-tests, revealing new insights about model weaknesses and the effects of pretraining on OOD detection performance. We provide code and data at https://github.com/j-cb/NINCO.\\n\\n1. Introduction\\n\\nWhile deep learning based models have shown impressive performance on many real world tasks, they often exhibit unforeseen behaviour when confronted with unknown situations like receiving an input that is not related to the task it has been trained on. Such samples are regarded as out-of-distribution (OOD) and deep neural network classifiers are known to make very confident predictions that those belong to one of the in-distribution (ID) classes (Hendrycks & Gimpel, 2017; Hein et al., 2019). This unwanted behaviour is a serious obstacle when applying classifiers in real world applications. The purpose of OOD detectors is to reject OOD inputs, which depending on the application can mean requesting human intervention, steering towards a safe state, or simply abstaining from making a prediction, while at the same time letting ID inputs pass through.\\n\\nCurrent OOD detection evaluations in image classification rely on the assumption that there is no ID class present in an OOD test image, not even in the background. We follow this definition and consider an input to be out-of-distribution (OOD) if it does not contain any of the in-distribution classes. However, we show that this assumption is not fulfilled for most of the current test OOD datasets for ImageNet-1K (IN-1K) of Russakovsky et al. (2015). The closely related task of open set recognition (OSR), which simultaneously demands detection of OOD data and high classification accuracy on the ID data, is evaluated on OOD datasets which have the same requirements as in OOD detection. We also examine the test OOD datasets that have been used in the OSR literature for IN-1K and find similar issues there. We demonstrate that occurrences of objects from ID classes in test OOD datasets are often correctly recognized by state-of-the-art OOD detectors, but as an unwarranted consequence held against them as mistakes in OOD detection evaluations (false 'false positive'). Even in cases where current models struggle to identify ID content, e.g. if ID objects are partially occluded or in the background, OOD datasets containing ID objects are not future proof: when evaluating on them, one would not realize if a future model correctly predicts the class of a visible ID object.\\n\\nThe erroneous occurrences of ID objects in existing OOD datasets can be characterized into two failure modes, which we illustrate in Figure 1 and define as follows.\\n\\nCategorical ID contaminations show objects from ID classes which already are classes in a base dataset from which the test OOD dataset has been built. Their label coincides with an ID class or semantically designates a subset of an ID class, e.g. the class hayfield from the P L A C E S dataset and the IN-1k class hay.\\n\\nIncidental ID contaminations on the other hand occur in images which are supposed to belong to an OOD category but which contain an ID object. The object can be in the background or an aspect of the specific instance of the shown main object, e.g. the IN-1k class plane in an image of the OOD category sky. We show that ID contaminations cause overfitting to the base dataset from which the OOD dataset is drawn, and we demonstrate for NINCO that no model shows OOD performance worse than 5% OOD accuracy on the base dataset. Under these assumptions, it follows that OOD detectors should be evaluated on datasets where the proportions of ID contaminations are equal in both OOD and ID cases. This can be achieved if the test OOD dataset contains no contaminations at all, or by either removing all ID contaminations from an OOD dataset or by ensuring a balanced proportion of ID contaminations in the base datasets.\\n\\nWe plan to start a series of workshops at major machine learning conferences where datasets will be made available for OOD evaluation, including NINCO. Following a similar strategy to OSR (Chen et al., 2017), we provide a website for all participating datasets, allowing the community to evaluate and compare OOD detectors in a standardized setting.\\n\\nWe provide detailed evaluations across a large set of architectures and OOD detection methods on NINCO and the unit-tests, revealing new insights about model weaknesses and the effects of pretraining on OOD detection performance. We provide code and data at https://github.com/j-cb/NINCO.\"}"}
{"id": "bitterwolf23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\n**Figure 1.** Contamination of OOD test sets with ID samples (ImageNet).\\n\\n- **Blue:** ImageNet-1K class found in the image.\\n- **Brown:** Label of the image in the original source dataset.\\n\\nTop: Samples from classes of the OOD dataset that by class meaning categorically overlap with ImageNet-1K classes.\\n\\nBottom: Labels alone do not reveal that the images are ID, but incidental ID objects can be found.\\n\\nStrongly impact the conclusions which can be drawn from evaluating OOD detection methods by (1) systematically underestimating the true OOD detection performance and (2) unrightfully punishing stronger OOD detectors.\\n\\nProbing the true performance of OOD detectors for IN-1K requires a range of OOD classes that are challenging, diverse, and most importantly actually OOD. Compiling a test OOD dataset is indeed a challenging task, as the 1000 classes of IN-1K cover a fair portion of the images found in general image datasets. In this paper we introduce the NINCO (No ImageNet Class Objects) dataset which contains 5,879 images that we individually checked not to contain any ID object from the classes in IN-1K. These images are ordered into 64 OOD classes, which facilitates a specific analysis of the failure modes of an OOD detector. Additionally, we provide a dataset of `\u00baOOD unit-tests\u00ba`, synthetic images which do not resemble real world photos, but are designed to test specific weaknesses that might have impact in real-world applications (e.g. due to a camera failure). We find that surprisingly many OOD detectors struggle to detect these supposedly easy unit-tests, in particular methods that work well on natural test data.\\n\\nWe provide a detailed OOD detection evaluation on NINCO for a range of eleven OOD detection methods across a large number of architectures and training schemes. Surprisingly, it turns out to be difficult for many OOD detectors to improve consistently over the baseline of Maximum Softmax Probability (MSP). While we confirm the observation that pretraining on larger datasets generally helps OOD detectors and particularly methods explicitly using pre-logit feature-information, we find that the type of pretraining has a strong impact.\\n\\n2. Existing test OOD datasets for ImageNet-1K\\n\\nFirst, we give an overview of the datasets that have been used to evaluate OOD detection performance for IN-1K as ID. In the following we use **blue** for the name of an ImageNet class and **brown** for the category name in the source dataset used for the generation of the test OOD dataset.\\n\\nINATURALIS T OOD PLANTS is a subset of 10,000 images curated by Huang & Li (2021) from 110 OOD plant species of iNat2017 (Van Horn et al., 2018) which is sourced from the iNaturalist project. It is frequently used as test OOD dataset (Xia & Bouganis, 2022; Ming et al., 2022).\"}"}
{"id": "bitterwolf23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nTable 1. Percentage of ID samples in commonly used test OOD datasets found by visual inspection of 400 random samples per dataset. Unclear samples are ignored (which are at most 6.7% (for P L A C E S ) of the 400 samples).\\n\\n| Dataset      | ID samples | OOD samples |\\n|--------------|------------|-------------|\\n| P L A C E S  | 59.5%      |             |\\n| S P E C I E S | 57.0%      |             |\\n| I M A G E N E T-20 | 20.2%   |             |\\n| T E X T U R E S | 25.6%      |             |\\n| INAT.P L A N T S | 2.5%      |             |\\n| T E X T U R E S | 43%        |              |\\n| P L A C E S  | 59.5%      |             |\\n| S P E C I E S | 57.0%      |             |\\n| I M A G E N E T-20 | 20.2%   |             |\\n| T E X T U R E S | 25.6%      |             |\\n| INAT.P L A N T S | 2.5%      |             |\\n| T E X T U R E S | 43%        |              |\\n\\nP L A C E S is a subset of Places365 (Zhou et al., 2017) curated by Huang & Li (2021) as \u201c50 categories . . . that are not present in IN-1K\u201d. It is used as test OOD dataset in (Huang & Li, 2021; Sun et al., 2021; Ming et al., 2022). The dataset contains 9,822 images from 50 environment classes. We find that several of these classes are either subsets of ID classes, e.g. hayfield (hay), cornfield (corn), lagoon (seashore and lakeshore), or contain mostly ID objects, e.g. underwater (coral reef and scuba diver), ocean (seashore).\\n\\nT E X T U R E S (Cimpoi et al., 2014) contains 5,640 images of various objects that show one of 47 patterns. It is used as test OOD dataset in (Huang & Li, 2021; Sun et al., 2021; Wang et al., 2021; Xia & Bouganis, 2022; Ming et al., 2022) and others. Wang et al. (2022a) address the issue of overlap with IN-1K and remove four categorically ID textures (bubbly (bubble), honeycombed (honeycomb), cobwebbed (spider web), spiralled (spiral)). We find that even their version (denoted as T E X T U R E S 43) contains about 20% ID images.\\n\\nS P E C I E S was proposed in (Hendrycks et al., 2022) as OOD dataset for IN-21K (Deng et al., 2009) and should thus also be OOD for the IN-1K subset. Sourced from iNaturalist, it consists of 700,000 images from 1,316 species which were selected for not being in IN-21K. They sort the species into 10 superclasses. The largest superclass Fungi largely coincides with the IN-1K class mushroom, and also many of the remaining species are ID. Papers evaluating on S P E C I E S for IN-1K OOD detection include (Salehi et al., 2021; Yang et al., 2022; Song et al., 2022).\\n\\nI M A G E N E T-O (Hendrycks et al., 2021) contains 2,000 images from IN-21K, excluding its subset IN-1K. To make the dataset challenging it was composed from images where a ResNet-50 classifier for a subset of 200 IN-1K classes attains high confidence. The samples being OOD relies on the assumption that IN-21K without IN-1K is OOD for IN-1K. However, this assumption does not hold, due to a significant overlap between ImageNet classes from IN-1K and IN-21K, e.g. analytical balance/scale and pickle/cucumber, and insufficient filtering for incidental ID objects.\\n\\nO P E N I M A G E -O (Wang et al., 2022a) consists of 17,632 images from the OpenImage-v3 (Krasin et al., 2017) test set which their human labellers categorize as OOD. It is also used in Yang et al. (2022).\\n\\nConcerningly, several test OOD datasets for IN-1K that are in use by the community contain a substantial fraction of samples that show ID objects. Figure 1 shows some examples.\"}"}
{"id": "bitterwolf23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nThe categorical ID failure mode illustrated in the top part is the inclusion of samples from explicitly ID classes of the source dataset from which the OOD dataset has been built. For instance, the class *hayfield* from the *PLACES*-dataset overlaps with the IN-1K class *hay*. However, also in principally innocuous classes (bottom part), many incidental ID samples can still be found. Here, the occurring failure modes are numerous: some ID objects happen to be in the background, some are a prominent part of the depicted scene, and some happen to realize both the original class and the ID class. For instance, the class *table knife* contains samples which also show a *plate*, and the class *striped* from the *TEXTURES*-dataset often shows the stripes of a *zebra*.\\n\\nIn order to quantify the severity of ID objects in test OOD datasets, we manually check for ID objects in 400 random samples from each of the most commonly used datasets. For fair treatment, unclear and ambiguous samples, which we would exclude from NINCO introduced below, are ignored in this survey. The results in Table 1 show that for many of these common OOD detection benchmarks, a substantial fraction of samples is actually ID: For both the *PLACES* and *SPECIES* datasets, it is more than 50%. Only *INATURAL OOD LANDS* (2.5% of samples ID) and *OPEN IMAGERY*-O (4.9% ID) contain comparably few ID images.\\n\\n2.2. Effect of ID contamination on OOD evaluation\\n\\nIn Figure 2, we show how OOD detection evaluation with incidental ID samples can unrightfully punish strong OOD-detectors: A better model can correctly recognize ID objects with high confidence even if they are in the background of the image, leading to a false \u00aafalse positive\u00ba in the evaluation, while a weaker model not recognizing the ID object and providing a low-confidence prediction is \u00aarewarded\u00ba with a false \u00aatrue negative\u00ba. For example, the strong Vision-Transformer (ViT) (Dosovitskiy et al., 2021) identifies the pole besides an otherwise empty desert road, and thus has high confidence on the image where the weaker ResNet-50 does not recognize any ID class with high confidence. Similarly, in the second example, the ViT is punished with a false \u00aafalse positive\u00ba for recognizing (above the detection threshold) the *oranges* in the background while ignoring the unknown flying fox (truly OOD), whereas the ResNet-50 even does predict a wrong ID class, namely *squirrel monkey*, but does so with low confidence (below the detection threshold), and is thus rewarded with a false \u00aatrue negative\u00ba.\\n\\nWe quantify the effect of ID contaminations on evaluation results in customary OOD datasets in Figure 3 for the MSP baseline and the Mahalanobis OOD detection method (Lee et al., 2018). For the test OOD datasets which showed a large portion of ID samples in Table 1, we report the FPR at 95% TPR obtained with a ViT when evaluating on the original 400 samples and our cleaned subsample of it not containing any more ID objects (detailed results for a range of models and methods can be found in Appendix J). We find that ID contaminations strongly impact the conclusions which can be drawn from evaluating OOD detection methods on those datasets. Most clearly, both methods perform substantially better after removing the images with ID objects from the OOD datasets, in some cases reducing the FPR by more than 50%. This is unsurprising: If a significant fraction of the dataset is actually ID, this fraction should not be detected as OOD by a well-performing method. Hence, evaluating OOD detection performance with partially ID data leads to a systematic overestimation of the true FPR of the OOD detection method and disadvantages better models as discussed above. Additionally, we observe that the differences between OOD detectors become more pronounced. In Figure 3 it can be seen that for each dataset, the FPR for the Mahalanobis OOD detector decreases more than for the MSP-baseline. The effect is particularly strong for *SPECIES* (25.6% gain of MSP vs. 33.2% gain of Mahalanobis) and *PLACES* (19.6% gain vs. 26.3% gain), which are the two datasets we found to contain most ID samples. We further emphasize that due to the presence of large fractions of ID samples in most common benchmarks, even the performance of a perfect detector would saturate significantly above 0% FPR. For example with *SPECIES*, we find that for a strong current detector already more than 85% of the \u00b4false positives\u00b4 contain ID objects.\\n\\n3. A new OOD test set for ImageNet-1K\\n\\nAs discussed in Sec. 1, an OOD input for IN-1K is an image that does not contain an object from one (or several) of the 1 000 IN-1K classes. These ImageNet classes are based on individual WordNet (Fellbaum, 1998) synsets, each consisting of one or more keywords that are synonymous in some context. During the ImageNet creation process (Deng et al., 2009), images were first collected from the web by using variations of each keyword of a respective class and then verified by humans to fit its synset\u2019s definition.\"}"}
{"id": "bitterwolf23a", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We provide 400 samples for each of 17 OOD unit-tests, mirroring the sizes and file formats of random ImageNet samples. Their reproducible definitions are given as follows:\\n\\n\u2022 uniform noise (Hendrycks & Gimpel, 2017): Each RGB colour channel of each pixel is independently sampled uniformly between 0.0 and 1.0.\\n\\n\u2022 Gaussian noise (Hendrycks & Gimpel, 2017): For each image, first \\\\( \\\\sigma \\\\) is chosen randomly between (0.05, 0.075, 0.1, 0.15, 0.2, 0.3). Then each RGB colour channel of each pixel is independently sampled from \\\\( \\\\mathcal{N}(0.5, \\\\sigma) \\\\).\\n\\n\u2022 Rademacher noise (Hendrycks et al., 2019): Then each RGB colour channel of each pixel is independently set to 0.0 or 1.0 with 50% probability.\\n\\n\u2022 IN pixel permutations (Hein et al., 2019): We choose a random IN-1K validation image and randomly shuffle its pixels (no remixing of colours).\\n\\n\u2022 black: All colour channels are set to 0.0.\\n\\n\u2022 white: All colour channels are set to 1.0.\\n\\n\u2022 shades of grey: All colour channels are set to the same value, sampled uniformly between 0.0 or 1.0.\\n\\n\u2022 monochrome: All pixels are set to a uniformly random RGB-colour (sampled uniformly from [0, 1]).\\n\\n\u2022 tricolour: The image is split into three stripes of equal size, vertically or horizontally with probability 50%. Each stripe is set to an independent uniformly random RGB-colour.\\n\\n\u2022 primary tricolour: The image is split into three stripes of equal size, vertically or horizontally with probability 50%. Each stripe is set to a colour where each RGB-channel value is chosen randomly as either 0.0 or 1.0.\\n\\n\u2022 horizontal stripes: The image is split into a random number chosen between (4, 5, 7, 10, 15, 20) of horizontal stripes of equal size. Each stripe is set to an independent uniformly random RGB-colour.\\n\\n\u2022 vertical stripes: The image is split into a random number chosen between (4, 5, 7, 10, 15, 20) of vertical stripes of equal size. Each stripe is set to an independent uniformly random RGB-colour.\\n\\n\u2022 smooth noise (Hein et al., 2019; Bitterwolf et al., 2020; Meinke et al., 2022): For each image, first \\\\( \\\\sigma \\\\) is chosen randomly between (10, 15, 25, 40, 60, 85). A uniform noise image is sampled. Then we apply a Gaussian filter with a size of \\\\( \\\\sigma \\\\) pixels. Finally, the pixel values are scaled linearly such that the minimum brightness over all channels and pixels is 0.0 and the maximum is 1.0.\\n\\n\u2022 smooth noise+: For each image, first \\\\( \\\\sigma \\\\) is chosen randomly between (10, 15, 25, 40, 60, 85). A uniform noise image is sampled. Then we apply a Gaussian filter with a size of \\\\( \\\\sigma \\\\) pixels. Finally, each RGB channel is scaled linearly such that its minimum brightness over all pixels is 0.0 and the maximum is 1.0.\\n\\n\u2022 smooth color: For each image, first \\\\( \\\\sigma \\\\) is chosen randomly between (10, 15, 25, 40, 60, 85), \\\\( \\\\delta \\\\) uniformly between 0.1 and 0.3, and a uniformly random RGB-colour \\\\( c \\\\). A uniform noise image is sampled. Then we apply a Gaussian filter with a size of \\\\( \\\\sigma \\\\) pixels. Finally, each RGB channel is scaled linearly such that \\\\( c - \\\\delta \\\\) is the 2.5th quantile of its values and \\\\( c + \\\\delta \\\\) the 97.5th.\"}"}
{"id": "bitterwolf23a", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\n- smooth IN pixel permutations (Hein et al., 2019): For each image, first $\\\\sigma$ is chosen randomly between (1, 1.5, 2, 3, 4, 6, 8). An IN pixel permutations image is sampled. Then we apply a Gaussian filter with a size of $\\\\sigma$ pixels.\\n\\n- blobs (Hendrycks et al., 2019): For each image, first $\\\\sigma$ is chosen randomly between (1.5, 2, 2.5, 3, 3.5, 4). Each RBG colour channel of each pixel is independently set to 1.0 with 70% probability or 0.0 with 30%. Then we apply a Gaussian filter with a size of $\\\\sigma$ pixels. Finally, all channel values below 0.75 are set to 0.0. Where necessary, the resulting channel values are clipped to [0, 1].\\n\\nWe show samples of each unit-test in the following Appendix H in Figure 13.\"}"}
{"id": "bitterwolf23a", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nAF\\n\\nbagpipe  bluestriped grunt  cable\\n\\nCalifornia pitcher plant  California slender salamander  California two-spot octopus  caracal  cat-faced spider  Central American tapir  chicken quesadilla  common cuttlefish  cr\u00e8me br\u00fbl\u00e9e  cupcakes  donuts  door  dreamfish  dune thistle  dusky flathead  E. micromeris  Eastern leaf-footed bug  European paper wasp  false killer whale  field road\\n\\nFigure 10. Samples of each class of the NINCO dataset (1/3).\"}"}
{"id": "bitterwolf23a", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nFigure 11. Samples of each class of the NINCO dataset (2/3).\"}"}
{"id": "bitterwolf23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4.\\n\\nand in green if it improves. Bold values mark the best-performing method per model.\\n\\n| Model | Pre Acc. | Mean AUROC | Mean FPR |\\n|-------|----------|------------|----------|\\n| clip-ViT-B-384-l2b | 86.6 | 81.1 | 73.5 |\\n| JFT | 86.8 | 83.6 | 82.5 |\\n| clip-ViT-L-336 | 72.5 | | |\\n| clip-ViT-B-224 | 74.0 | | |\\n| clip-ViT-B-384 | 81.4 | 84.2 | |\\n| clip-ViT-B-384-oai | 87.2 | 85.8 | |\\n| clip-ViT-B-224 | 74.9 | | |\\n| clip-ViT-B-384 | 86.7 | 81.1 | 77.7 |\\n| clip-ViT-B-224 | 83.8 | 81.0 | 78.8 |\\n| clip-ViT-B-384 | 86.3 | 86.3 | 87.0 |\\n| clip-ViT-B-224 | 84.1 | 85.1 | 86.0 |\\n| clip-ViT-B-384 | 84.3 | 82.7 | 80.2 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 84.6 | 81.1 | 76.2 |\\n| clip-ViT-B-224 | 84.5 | 85.1 | 86.0 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | 89.5 |\\n| clip-ViT-B-224 | 85.9 | 85.9 | 87.9 |\\n| clip-ViT-B-384 | 84.1 | 82.3 | 83.6 |\\n| clip-ViT-B-224 | 86.3 | 87.9 | 87.6 |\\n| clip-ViT-B-384 | 88.0 | 88.0 | "}
{"id": "bitterwolf23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":false,\"rotation_correction\":90,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                  | Pre acc. | None | 21k | clip | z. shot | +12k | clip | clip | clip | clip | JFT 86.8 EffNetb7-ns 52.7 |\\n|-----------------------|----------|------|-----|------|---------|------|------|------|------|------|--------------------------|\\n| V iM E+R              | 51.5     | 52.4 | 52.8| 51.9 | 94.2    | 90.8 | 51.9 | 50.9 | 48.7 | 37.5 | 61.0 61.4                   |\\n| MCM/RCos              | 47.0     | 48.4 | 49.1| 49.6 | 49.0    | 48.4 | 49.6 | 49.6 | 48.4 | 47.1 | 47.1 49.6                   |\\n| RMaha                  | 50.1     | 50.1 | 50.1| 49.7 | 49.1    | 48.5 | 49.1 | 49.1 | 48.5 | 47.1 | 50.1 50.1                   |\\n| Ener KL-M              | 48.4     | 48.4 | 48.4| 48.4 | 48.4    | 48.4 | 48.4 | 48.4 | 48.4 | 47.1 | 48.4 48.4                   |\\n| MSP                    | 48.4     | 48.4 | 48.4| 48.4 | 48.4    | 48.4 | 48.4 | 48.4 | 48.4 | 47.1 | 48.4 48.4                   |\\n| Higher is better. The difference to MSP is shown in red if a method performs worse, in green if it improves. Bold values mark the best-performing method per model. |\"}"}
{"id": "bitterwolf23a", "page_num": 15, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "bitterwolf23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model   | Pretraining | Top-1 Acc. | Parameters | Name          |\\n|---------|-------------|------------|------------|---------------|\\n| ViT-B-384-l2b-12k | laion2b + IN-12k | 87.2      | 87M         | vit base patch16 clip 384.laion2b ft in12k in1k |\\n| ViT-B-384-oai-12k | openai + IN-12k | 87.0      | 87M         | vit base patch16 clip 384.openai ft in12k in1k |\\n| ViT-B-384-l2b | laion2b | 86.6      | 87M         | vit base patch16 clip 384.laion2b ft in1k |\\n| ViT-B-384-oai | openai | 86.2      | 87M         | vit base patch16 clip 384.openai ft in1k |\\n| ViT-B-384-21k | IN-21k | 86.0      | 87M         | vit base clip 384 |\\n| ViT-B-224-21k | IN-21k | 84.5      | 87M         | vit base clip 224 |\\n| Swinv2-B-256-21k | IN-21k | 86.3      | 88M         | swinv2 base window12to16 192to256 22kft1k |\\n| Deit3-B-384-21k | IN-21k | 86.7      | 87M         | deit3 base patch16 384 in21ft1k |\\n| Deit3-B-224-21k | IN-21k | 85.7      | 87M         | deit3 base patch16 224 in21ft1k |\\n| CnvNxt-B-21k | IN-21k | 86.3      | 89M         | convnext base in22ft1k |\\n| CnvNxt-T-21k | IN-21k | 84.1      | 29M         | convnext tiny 384 in22ft1k |\\n| BiT-m | IN-21k | 82.3      | 45M         | resnetv2 101x1 bitm |\\n| EffNetv2-M-21k | IN-21k | 85.6      | 54M         | tf efficientnetv2 m in21ft1k |\\n| EffNetb7-ns JFT - noisy student | IN-21k | 86.8      | 66M         | tf efficientnet b7 ns |\\n| ViT-B-384-augreg | | 81.1      | 87M         | vit base patch16 384 augreg in1k |\\n| Swinv2-B-256 | | 84.6      | 88M         | swinv2 base window16 256 |\\n| Deit3-B-384 | | 85.1      | 87M         | deit3 base patch16 384 |\\n| Deit3-B-224 | | 83.8      | 87M         | deit3 base patch16 224 |\\n| XCiT-M-224 | | 82.6      | 84M         | xcit medium 24 p16 224 |\\n| XCiT-M-224-dist | | 84.3      | 84M         | xcit medium 24 p16 224 dist |\\n| CnvNxt-B | | 84.4      | 89M         | convnext base |\\n| BiT-s | | 78.0      | 45M         | resnetv2 101x1 bitm |\\n| EffNetv2-M | | 85.0      | 54M         | tf efficientnetv2 m |\\n| EffNetb7 | | 84.9      | 66M         | tf efficientnet b7 |\\n| EffNet-B0 | | 77.7      | 5M          | efficientnet b0 |\\n| ResNet50 | | 80.4      | 26M         | resnet50 |\\n| CLIP-ViT-B16 openai | | 66.6      | 150M        | |\\n| CLIP-ViT-B16 openai | | 74.2      | 428M        | |\"}"}
{"id": "bitterwolf23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nFigure 4. Difficult OOD classes in NINCO: Examples of images from some of NINCO\u2019s most difficult (see Table 7) OOD classes (first row) and from the ImageNet-1K class (second row) which the OOD class is most frequently confused for.\\n\\nSourcing OOD test samples for ImageNet-1K from ImageNet-21K (or its subsets) based on class-labels has been leading to highly contaminated datasets (5 of the datasets in Table 1 are sourced from ImageNet-21K and all contain between 20% and 53% ID samples and show significant categorical contamination). This is partly due to the class-structure of those datasets: Both ImageNet-1K and ImageNet-21K contain leaf and internal nodes of the WordNet-tree as classes. While the internal nodes of ImageNet-1K are not ancestors to other ImageNet-1K classes, ImageNet-21K internal nodes can be ancestors to ImageNet-1K nodes, and vice versa. Moreover, there are ambiguous class-definitions in WordNet, like e.g. police dog, which is not parent or child of another dog class, but mostly shows a German Shepherd, or an alley cat showing one of the many cat classes without being parent or child to other cat classes. Besides, there is significant incidental contamination even for nominally disjoint classes. Since the automation of filtering for challenging OOD data would require a strong detector that already solves the problems that the dataset is meant to pose, we conclude that it is impossible to construct a clean and challenging OOD dataset without manually checking the OOD samples for ID contamination.\\n\\nIn reality, many ImageNet samples fit one but not necessarily all keywords of their class label. This means that to make sure that OOD detectors are treated fairly, OOD test samples cannot fall into the definition of any keyword of any IN-1K class. For example, photos of the Sumatran orangutan cannot be considered OOD, since they could be included in the IN-1K class (orangutan, orang, orangutang, Pongo pygmaeus), even though Pongo pygmaeus only refers to the Bornean orangutan. To determine what counts as an ID object, we follow the WordNet glosses as well as dictionary definitions of keywords and source dataset class labels. For difficult cases, we consult additional sources like Wikipedia. For example, the species northern elephant seal does not fall into the ID class sea lion, among other biological criteria distinguished by the fact that the former do not have ears while the latter do. An image of an OOD dataset can furthermore not incidentally contain ID objects, to avoid cases as in Figure 1 (bottom) and Figure 2.\\n\\n3.1. NINCO dataset construction\\n\\nFor each OOD class of our new NINCO dataset, we start by choosing a base class which consists of all samples from a named class of an existing or newly scraped dataset. The majority of the NINCO base classes are sourced from SPECIES (Hendrycks et al., 2022), which provides images scraped from iNaturalist. For each base class, we carefully decide, based on WordNet glosses, iNaturalist taxonomy details and Wikipedia, whether it can be included according to the non-permissive interpretation described at the beginning of Section 3. The choice of base classes is not random, since there is no way to randomly sample from the set of concepts that might occur at test time. Rather, we aim for a variety of classes that are challenging, diverse and, most importantly, not actually categorically ID to begin with. Then for each base class, we individually inspect each image for ID objects. To help remembering the 1000 ID classes, we display the 5 top ID classes of a ViT\u2019s prediction on each image. If an ID object is at least partially visible, the corresponding sample is removed. In cases where it is ambiguous whether we see an ID object in the image, the sample is not included in the cleaned dataset. As the iNaturalist data (including the SPECIES dataset) has been curated by experts and can be considered very reliable, we generally trust in the main object belonging to the species it is labelled as. For base classes chosen from the other sources, we consider ourselves competent to verify whether a label is correct. In addition to samples showing ID objects, we also remove images where no object from the OOD class is visible, e.g. we exclude pictures of animal traces or remains which frequently appear in iNaturalist. While for most existing datasets, the cleaning has been outsourced to external services like Amazon Mechanical Turk.\"}"}
{"id": "bitterwolf23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nBy researching all OOD classes and visually inspecting all their samples ourselves, we as authors of NINCO were able to do more in-depth research for each ambiguous case and obtain more coherent decisions, which we are positive leads to a higher quality dataset. Such high data quality is crucial for in-depth evaluations (V asudevan et al., 2022; Shankar et al., 2021), as only being completely in-distribution free allows understanding a detector's individual mistakes.\\n\\nThe NINCO (No ImageNet Objects) dataset consists of 64 OOD classes with a total of 5,879 samples. The base classes which we cleaned to obtain NINCO were sourced from SPECIES (35 classes) (Hendrycks et al., 2022), P LACES (3 classes) (Zhou et al., 2017), which both are discussed in Section 2, as well as from the FOOD-101 dataset (7 classes) (Bossard et al., 2014), CA LT ECH-101 (4 classes) (Li et al., 2022), MYNURS HOME (4 classes) (Ismail et al., 2020), ImageNet-21k (1 class) and newly scraped from iNaturalist.org (2 classes) or other websites like Flickr (8 classes). Details for all NINCO OOD classes are given in Appendix F. We show samples from all NINCO classes in Figures 10 and 11 in Appendix H. In addition to NINCO, we also provide the 2715 OOD images obtained from cleaning 400 samples of eleven test OOD datasets as discussed in Section 2.2. In order to notice ID contaminations potentially biasing the drawn conclusions, we recommend to also evaluate on these cleaned versions when evaluating on those original benchmarks.\\n\\n3.2. OOD unit-tests\\n\\nFollowing common practice (e.g. Hendrycks et al. (2022)), we argue that evaluating an OOD detector on a range of simple, synthetic classes besides the variably challenging natural image classes of an OOD dataset can give additional insights about its OOD detection weaknesses. Example images and reproducibility details for all 17 pre-existing and newly proposed OOD unit-tests are included in Appendices G and H. Since these OOD unit-tests do not represent a diverse distribution of photos, but different modes of synthetically generated image inputs which any good OOD detector should be expected to detect, we don't include them in summary metrics or distribution plots. Instead, we suggest to count an OOD unit test as failed if a method has an FPR above a user-defined threshold, which we suggest setting at 10%, and to report the number of failed OOD unit-tests (which should be 0 for a strong OOD detector) alongside the aggregate results on a test OOD dataset like NINCO. For each OOD unit-test, we provide a set of 400 samples in typical ImageNet format, by mirroring the sizes and file formats of random ImageNet samples. While some OOD unit-tests may appear redundant at first sight, we find that they provide important information as some detectors e.g. mostly pass the monochrome test but completely fail on black, which reveals a specific weakness that is very realistic to be encountered in practice.\\n\\n3.3. OOD detectors and how to evaluate them\\n\\nAn OOD detector for inputs from the domain $X$ of possible input images is represented by a score function $S: X \\\\rightarrow \\\\mathbb{R} \\\\cup \\\\{\\\\pm \\\\infty\\\\}$ which is generally supposed to be larger on ID inputs than on OOD inputs. One example is the Maximum Softmax Probability (MSP) or confidence $S_{MSP}(x) = \\\\max_{k=1}^{K} p_k(x)$ of a classifier with output probabilities $p$ for $K$ ID classes. The MSP is the standard baseline OOD detection method (Hendrycks & Gimpel, 2017), since it is intuitively expected to be low on OOD compared to ID inputs. Observing that standard classifiers are frequently overconfident on OOD inputs, OOD detection research aims at finding detectors that improve on this baseline. In Appendix C, we give an overview of a range of OOD detection methods which have been proposed for IN-1K as ID. An OOD detector is usually obtained by combining such an OOD detection method with a concrete classifier model. We analyze OOD detectors in terms of the fraction of falsely accepted OOD inputs at a true positive rate of 95%, short FPR. Detailed definitions can be found in Appendix D. Different OOD classes (and similarly also different test OOD datasets) represent different probabilistic distributions of inputs that a detector is tested against. An important arising question is how the collective of individual performance measurements can be interpreted and whether they can be aggregated into one number that can be used to make an informed decision on which OOD detector works best. Certainly, the notion of \u2018best\u2019 may notably vary depending on the application and situation and we often cannot hope to model a \u2018true\u2019 out-distribution, or even be sure that it meaningfully exists. An aggregate number which gives a good overview of an OOD detector's performance on the class based NINCO dataset is the mean FPR of the individual FPR values for each of the 64 OOD classes of NINCO. However, for many applications it is not possible to model the potential OOD inputs that might be encountered at test time with a fixed probability distribution. Thus a single aggregate number cannot tell the full story, and may hide outliers in the FPR values. For one, some errors might be less acceptable than others, e.g. a FPR of 20.0% might be very bad for monochrome inputs, but would lose much significance when subsumed into a mean. For OOD unit tests, where OOD detectors can be expected to be very robust, we therefore propose regarding pass-fail statistics instead of mean FPR. Also, an evaluator might want to be informed about the concrete failure modes of the model, e.g. all OOD classes with a particular high FPR. An OOD detector showing consistent improvements on most of the OOD classes (instead of only in terms of the mean) can be seen as strong.\"}"}
{"id": "bitterwolf23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5. Cumulative distribution of the % of NINCO-classes for which an FPR at least as low as a given x-value is achieved. The area over this curve corresponds to the mean FPR. The further in the top left corner, the better. The best methods explicitly access pre-logit features (Left): Different OOD detection methods with a ViT-B pretrained on IN-21k (mean FPR in parentheses, pre-logit feature-accessing methods are solid, others dashed). Not all pretraining helps (Right): RMaha applied to ViT-B with different training variants (MCM for CLIP zero-shot is dashed). Only the top model does not fail OOD unit-tests.\\n\\nEvidence for the method yielding actual improvement, as opposed to the detector overfitting to a limited scope of test OOD data, which Wang et al. (2022a) describe as a form of hackability. Due to these considerations, and with the OOD data being organized into OOD classes as in NINCO, we suggest evaluations of OOD detectors to always provide the distribution of results over OOD classes and additionally to make the individual results available, such that the reader can make an informed comparison based on which types of OOD inputs are most relevant to them.\\n\\n4. Evaluation results for OOD Detectors\\n\\nWe evaluate a range of IN-1K models obtained from the public timm-library (Wightman, 2019) and state-of-the-art OOD-detection methods on NINCO. We focus on transformer architectures and convolutional networks, both with and without pretraining. While most pretrained models were initially trained on IN-21K, we also include an EfficientNet trained via noisy student (Xie et al., 2019) on the JFT-300M dataset, and four ViTs with CLIP-pretraining (Radford et al., 2021) and subsequent fine-tuning, as well as a zero-shot CLIP model. A detailed description of all models can be found in Appendix B. We investigate the following commonly used OOD detection methods, which can be grouped into two categories: Max-Softmax (MSP) (Hendrycks & Gimpel, 2017), Max-Logit (Hendrycks et al., 2022), Energy (Liu et al., 2020) and KL-Matching (Hendrycks et al., 2022) derive an OOD-score exclusively from logit outputs, whereas Mahalanobis distance (Maha) (Lee et al., 2018), Virtual Logit Matching (ViM) (Wang et al., 2022a), ReAct (Sun et al., 2021), Relative Mahalanobis distance (RMaha) (Ren et al., 2021), and K-Nearest-Neighbours (KNN) (Sun et al., 2022) also leverage explicit information from the features of the DNN's penultimate (pre-logit) layer. For the zero-shot evaluation of CLIP, we use Maximum-Concept-Matching (MCM) (Ming et al., 2022) and Cosine-similarity (Cos) (Galil et al., 2023) to class-specific text-embeddings.\\n\\n4.1. Results on NINCO\\n\\nComparison of OOD detection Methods. In Figure 5 (left), we illustrate the performance of a single ViT when combined with a range of OOD-methods. Overall, most feature-based methods, like Maha, RMaha and ViM, outperform the MSP-baseline by a clear margin. Notably, MaxLogit and Energy, which do not explicitly access the pre-logit features, are also able to strongly improve over MSP, while KL-Matching performs roughly on par, and KNN much worse. We observe that while Maha, RMaha and ViM improve over MSP in all FPR ranges, this is different for e.g. MaxLogit: For large FPR, it is similar to MSP, indicating that the method brings no advantage over MSP for hard test classes, and its improved mean performance is mainly due to lower FPR for the easier OOD classes. When regarding the mean FPR values of all method-model-combinations shown in Table 3 in Appendix A, we observe that while Maha in combination with a (pretrained) ViT is the single best OOD-detector, this method often performs worse when combined with other models. RMaha, however, yields good results with all models, and is together with (Relative) Cosine the only method which can fairly consistently improve over the MSP baseline in terms of mean FPR. For most models, it is either the best-performing method, or close to the best-performing method, which is somewhat surprising, given its relatively poor performance on the unit-tests. We further...\"}"}
{"id": "bitterwolf23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In or Out? Fixing ImageNet OOD Detection Evaluation\\n\\nFigure 6. IN-21K pretraining boosts feature-based OOD detectors on NINCO: Mean FPR vs. accuracy for MSP and each model's best detector, which (except for the noisy-student model) always explicitly accesses the pre-logit features. OOD detection strongly improves when using models pretrained on IN-21K. Additional CLIP-pretraining or on JFT can yield higher accuracy, but OOD detection need not be better than with IN-21K pretraining.\\n\\nNote that for all models (except the noisy-student model), the best-performing method always explicitly accesses the pre-logit features, and that in contrast to e.g. KNN, Energy and ReAct, even the adapted methods based on feature space cosine similarity Cos and MCM/RCos fairly consistently improve over the MSP-baseline. Each OOD dataset representing a different out-distribution that can be relevant for certain applications, we find that results vary on the cleaned subsets of eleven previous benchmarks which we evaluate in Appendix J, while the overall conclusions on the methods and models resemble those on NINCO.\\n\\nPretraining matters. In Figure 6, we plot the mean FPR on NINCO over the accuracy for all investigated models for both the MSP-baseline (left) and the best-performing OOD detector per model (right). For MSP, the mean FPR decreases roughly linearly with accuracy. Since most pretrained models (blue) have higher accuracy, they typically also show better OOD-detection performance, but also between models of similar accuracy, the pretrained ones achieve better mean FPR. For the best-performing OOD detector, improvements can be observed for models both with and without pretraining. Notably, the linear relation between FPR and accuracy disappears, and all purely 1K models (green) perform roughly on one level. In comparison, the gains for the majority of models pretrained on IN-21K (blue) are larger. In particular ViT and BiT benefit strongly from leveraging their respective best method, which as discussed above is always feature-based. In other words, pretraining helps in two ways: First, it leads to higher ID-performance (accuracy), which benefits methods like the MSP-baseline. Second, it creates better feature-embeddings for this task, which lead to improvements beyond the accuracy-MSP correlation. This is most clearly visible for the pretrained BiT-m, which has comparably low accuracy (82%) and hence no outstanding MSP-performance, but outperforms all 1k-models by a significant margin with features leveraging ViM. However, as we observe in Figure 5 (right), the benefit of pretraining depends strongly on the specific data and training method: With RMaha, the ViT with 'traditional' IN-21K pretraining from (Steiner et al., 2022) clearly outperforms models with the distillation-based training of DeiT3 (Touvron et al., 2022), CLIP-pretraining or even CLIP with interjected IN-12K training. The zero-shot methods for CLIP, despite having shown promising results in (Galil et al., 2023) and (Ming et al., 2022) and performing well on the unit tests, are not competitive to IN-1k classifiers on NINCO. Regarding all methods, the five models trained with different pretraining strategies (EfficientNet-b7 with noisy student and four ViTs with CLIP-pretraining (Radford et al., 2021) and subsequent fine-tuning) show some of the highest accuracies in our survey, yet, their OOD-detection performance is surprisingly poor. Overall, we see strong indication that the precise type of pretraining has a large impact on whether it produces a feature space that is beneficial for feature-based methods. In Appendix K we investigate whether IN-21K-pretraining particularly benefits detection of OOD classes that overlap with IN-21K classes, but we notice no substantially different changes between the model with and without pretraining.\\n\\nAnalysis of failure cases. In Figure 7 we plot the individual FPR for each OOD class of NINCO for the combination ViT+Maha, the overall best OOD detector in terms of mean FPR, and contrast it with ConvNext+Maha, which also shows good mean FPR. Performance varies widely between OOD classes, with both models severely struggling for some classes. Where the ViT shows large FPR, the ConvNext rarely performs better, while it also fails to detect certain classes like the long-tailed silverfish where the ViT does well. We illustrate samples from hard classes in Figure 4. Both models struggle to detect the Gal\u00e1pagos fur seal (98% FPR for the ViT), often confused with the IN-1K class sea lion, and the cat-faced spider (confused with barn spider).\"}"}
{"id": "bitterwolf23a", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "bitterwolf23a", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"In T able 14 we show the FPR values averaged across the cleaned subsampled datasets on which T able 1 in the main paper is based. Detailed results on the individual datasets are shown in T ables 15-18. There, we show results on the uncleaned full (-f) and cleaned subsampled (-c) datasets: P lace (Pl), S pecies (Spc), ImageNet-O (IN), Texture (txt) & Texture 43, O pen ImageNet-O (OpO), INatulist Ood Plants (iNat), ImageNet-1K-Ood (IN1K), 360 Species Classes (OS), S emantic Shift Benchmark Easy (SBe) & Hard (SBh), and COOD (CO).\\n\\nSince Texture and INatulist are fairly easy test OOD datasets, the FPR values of most models in T able 14 are lower than on NINCO. In general, the results allow similar conclusions: Feature-based methods outperform methods not explicitly accessing pre-logit feature-information, yet still fail for some models, and pretraining only on IN-21k yields the best OOD-detectors. Again, Cosine and MCM/RCos improve fairly consistently over MSP, and are in some cases even the best-performing method.\\n\\n| Model            | MSP | MaxL | Ener | KL | MMaha | RMaha | ViM | E+R | KNN | Cos | MCM/RCos |\\n|------------------|-----|------|------|----|-------|-------|-----|-----|-----|-----|-----------|\\n| 21k ViT-B-384    | 86.0| 39.7 | 27.0 | -  | 13    | 25.7 | -   | 14  | 38.4| 22.4| 27.5      |\\n| 21k ViT-B-224    | 84.5| 43.3 | 30.8 | -  | 13    | 29.3 | -   | 14  | 42.7| 23.8| 32.6      |\\n| Swinv2-B-256     | 86.3| 41.9 | 32.3 | -  | 10    | 31.5 | -   | 10  | 46.4| 47.4| 40.4      |\\n| Deit3-B-384      | 86.7| 53.4 | 45.4 | -  | 8     | 46.4 | -   | 7   | 52.5| 40.8| 43.1      |\\n| Deit3-B-224      | 86.3| 55.1 | 46.9 | -  | 8     | 47.2 | -   | 8   | 56.1| 47.5| 42.0      |\\n| CnvNxt-B         | 86.3| 38.6 | 32.9 | -  | 6     | 35.3 | -   | 3   | 43.6| 36.3| 37.0      |\\n| CnvNxt-T         | 84.1| 44.1 | 37.6 | -  | 7     | 35.7 | -   | 8   | 50.7| 36.2| 34.0      |\\n| BiT-m            | 82.3| 59.9 | 52.0 | -  | 8     | 52.6 | -   | 7   | 55.3| 30.9| 32.7      |\\n| EffNetv2-M       | 84.9| 43.4 | 42.5 | -  | 1     | 49.7 | +6  | 43.7| 41.1| 36.3| 37.0      |\\n| JFT              |     | 81.1| 63.5 | 59.4| -  | 58.8 | -   | 5    | 59.6| 49.1| 55.4      |\\n| Swinv2-B-256     | 86.2| 63.5| 63.0 | -  | 1     | 68.6 | +5  | 60.9| 54.4| 49.4| 52.1      |\\n| Deit3-B-384      | 85.6| 43.4 | 42.5 | -  | 1     | 49.7 | +6  | 43.7| 41.1| 36.3| 37.0      |\\n| EffNetv2-M       | 84.9| 59.0 | 59.4 | -  | 0     | 70.5 | +12 | 56.8| 50.2| 50.6| 57.3      |\\n| EffNetb7         | 84.9| 60.1 | 63.4 | +3  | 6   | 75.7 | +16 | 56.0| 52.7| 44.4| 47.0      |\\n| EffNet-B0        | 77.7| 69.3 | 69.9 | +1  | 1     | 77.3 | +8  | 68.2| 65.7| 65.7| 63.8      |\\n| ResNet50         | 80.4| 68.3 | 70.0 | +2  | 4     | 81.0 | +13 | 75.9| 77.0| 77.0| 97.6      |\\n| JFT EffNetb7     | 86.8| 53.8 | 49.9 | -  | 4     | 62.5 | +9  | 52.7| 46.5| 47.0| 46.5      |\\n| clip EffNetb7    | 87.2| 53.8 | 49.9 | -  | 4     | 62.5 | +9  | 52.7| 46.5| 47.0| 46.5      |\"}"}
{"id": "bitterwolf23a", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method   | fpr  |\\n|---------|------|\\n| RMaha   | 87.0 |\\n| KL-M    | 9    |\\n| MaxL    | 9    |\\n| Maha    | 1    |\\n| RCos    | 9    |\\n| KNN     | 2    |\\n| MSP     | 5    |\\n| E+R     | 2    |\\n| ViM     | 9    |\\n| Cos     | 4    |\\n| ViM     | 9    |\\n| ViM     | 1    |\\n| ViM     | 9    |\\n| ViM     | 5    |\\n| ViM     | 2    |\\n| ViM     | 9    |\\n| ViM     | 5    |\\n| ViM     | 2    |\\n| ViM     | 9    |\\n| ViM     | 5    |\\n| ViM     | 2    |\\n| ViM     | 9    |\\n| ViM     | 5    |\\n| ViM     | 2    |\\n| ViM     | 9    |\\n| ViM     | 5    |\\n| ViM     | 2    |\\n| ViM     | 9    |\\n| ViM     | 5    |\\n| ViM     | 2    |\\n| ViM     | 9    |\\n| ViM     | 5    |\\n| ViM     | 2    |\\n| ViM     | 9    |\\n| ViM     | 5    |\\n| ViM     | 2    |\\n| ViM     | 9    |\\n| ViM     | 5    |\\n| ViM     | 2    |\\n\\nComparing the cleaned and original datasets in terms of FPR. The best method per model and dataset is marked bold.\"}"}
{"id": "bitterwolf23a", "page_num": 32, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
