{"id": "3MW8GKNyzI", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Develop a snowman game similar to the Hangman game but draws a snowman figure standing on the snow instead of a hangman. It also downloads (or uses library) the complete English dictionary to select words for three levels of difficulty. Think step-by-step, take a deep breath and choose a higher quality code for each step.\\n\\nSure, here's a step-by-step guide on how to create a Snowman game similar to Hangman. We'll use Python for this example.\\n\\nStep 1: Import necessary libraries\\n```python\\nimport random\\nimport urllib.request\\n```\\n\\nStep 2: Download the English dictionary\\n```python\\nurl = \\\"https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\\\"\\nurllib.request.urlretrieve(url, 'words.txt')\\n```\\n\\nStep 3: Load the words from the dictionary and categorize them into three levels of difficulty\\n```python\\nwith open('words.txt') as word_file:\\n    english_words = word_file.read().split()\\n    easy_words = [word for word in english_words if 4 <= len(word) <= 6]\\n    medium_words = [word for word in english_words if 7 <= len(word) <= 9]\\n    hard_words = [word for word in english_words if len(word) >= 10]\\n```\\n\\nStep 4: Create a function to randomly select a word based on the difficulty level\\n```python\\ndef get_random_word(difficulty):\\n    if difficulty == 'easy':\\n        word = random.choice(easy_words)\\n    elif difficulty == 'medium':\\n        word = random.choice(medium_words)\\n    else:\\n        word = random.choice(hard_words)\\n    return word.upper()\\n```\"}"}
{"id": "3MW8GKNyzI", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\n```python\\ndef print_snowman(guesses):\\nsnowman = [' _____ ', ' ( ) ', ' ( ( ) ) ', '(___(___)_)', '( ) ', ' ( ) ', ' ( ) ', ' ( ) ', ' ( ) ', ' _______ ']\\nfor i in range(guesses):\\n    print(snowman[i])\\n```\\n\\nStep 6: Create the main function for the game\\n\\n```python\\ndef play_snowman():\\ndifficulty = input(\\\"Choose difficulty (easy, medium, hard): \\\")\\nword = get_random_word(difficulty)\\nguessed_letters = []\\nguesses = 0\\nprint('The word contains', len(word), 'letters.')\\nwhile guesses < 10:\\n    guess = input(\\\"Guess a letter: \\\").upper()\\n    if guess in guessed_letters:\\n        print(\\\"You already guessed that letter.\\\")\\n    elif guess in word:\\n        guessed_letters.append(guess)\\n        print(\\\"Good job! The word contains that letter.\\\")\\n    else:\\n        guesses += 1\\n        print_snowman(guesses)\\n        print(\\\"Sorry, the word does not contain that letter.\\\")\\n    print(' '.join(letter if letter in guessed_letters else '_' for letter in word))\\n    if set(guessed_letters) == set(word):\\n        print(\\\"Congratulations! You've successfully guessed the word.\\\")\\n        break\\n    else:\\n        print(\\\"Sorry, you didn't guess the word. The word was\\\", word)\\n```\\n\\nStep 7: Run the game\\n\\n```python\\nplay_snowman()\\n```\\n\\nThis is a simple version of the game. You can enhance it by adding more features like scoring, time limit, etc.\\n\\n20\"}"}
{"id": "3MW8GKNyzI", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sure! Here's a step-by-step guide to developing a snowman game similar to Hangman, using the complete English dictionary for three levels of difficulty.\\n\\nStep 1: Setting up the project\\n* Create a new Python project directory and initialize it with Git.\\n* Install the necessary libraries:\\n  + 'numpy' for generating random numbers\\n  + 'nltk' for accessing the English dictionary\\n  + 'pygame' for creating the game graphics\\n  + 'pydub' for playing sound effects (optional)\\n* Download the English dictionary dataset (e.g., from Kaggle) and store it in a file named 'dictionary.txt'.\\n\\nStep 2: Loading the dictionary and generating words\\n* Use the 'nltk' library to read the dictionary file and store the words in a list.\\n* Split the list into three parts: easy, medium, and hard words, based on their length and frequency of use.\\n* Shuffle the lists to randomize the order of the words.\\n\\nStep 3: Creating the game graphics\\n* Use 'pygame' to create a window for the game, with a white background to represent the snow.\\n* Draw a snowman figure on the background, using black rectangles for the body, hat, and scarf.\\n* Add buttons for the player to submit their guesses.\\n\\nStep 4: Implementing the game logic\\n* Create a function to generate a random word from the appropriate difficulty level and display it on the screen with missing letters (e.g., _ _ _ _ _ _).\\n* Create a function to check if a guessed letter appears in the word, and update the display accordingly (e.g., _ _ _ X _ _).\\n* Create a function to handle incorrect guesses, such as displaying a warning message or playing a sound effect.\\n* Create a function to win the game, such as displaying a congratulations message or playing a victory sound effect.\\n\\nStep 5: Adding sound effects (optional)\\n* Use 'pydub' to play sound effects for various actions in the game, such as guessing a letter correctly or incorrectly.\\n* Consider using a library like 'pygame_mixer' to simplify the process of playing sound effects in pygame.\"}"}
{"id": "3MW8GKNyzI", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nStep 6: Testing and refining the game\\n\\n* Test the game thoroughly to ensure that it works as expected, with no bugs or errors.\\n* Refine the game mechanics and graphics as needed, based on player feedback or personal preference.\\n* Consider adding additional features, such as keeping track of the player's score or implementing a time limit for each guess.\\n\\nStep 7: Packaging and distributing the game\\n\\n* Package the game as a standalone executable file using a tool like PyInstaller.\\n* Distribute the game to friends, family, or online communities, and gather feedback for future improvements.\\n\\nThis is a high-level overview of the steps involved in creating a snowman game similar to Hangman. Of course, the details will depend on your specific implementation and the tools you choose to use. Good luck with your project!\\n\\n---\\n\\nGPT-4-0613 Wins\\n\\nProgram Output:\\n\\nBelow is the terminal output of the snowman game programmed by GPT-4-0613, demonstrating its correctness. It is clear that GPT-4-0613's response is more helpful and relevant to the user's request.\\n\\n```\\n>>> python snowman.py\\nChoose difficulty (easy, medium, hard): easy\\nThe word contains 6 letters.\\nGood job! The word contains that letter.\\n_ A _ _ _ _\\nGuess a letter: e\\nGood job! The word contains that letter.\\n_ A _ _ _ E\\nGuess a letter: d\\n_____ ( )\\nSorry, the word does not contain that letter.\\n_ A _ _ _ E\\nGuess a letter: g\\n_____ ( ) ( ( ) )\\n22\\n```\"}"}
{"id": "3MW8GKNyzI", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nUnique Social Gaming Branding\\nYouTube Content Strategy\\nSimulated Unfiltered AIs\\nAGI Research and Progress Prediction\\nTrade Reporting and UAT Coordination\\nOperations & Fleet Management\\nEducational Assessment Strategies\\nMeaning of Life Inquiry\\nPhilosophical Texts & Concepts\\nBiblical Theology & Doctrines\\nMedical Case Summaries\\nUnderstanding Depression Varieties\\nGreetings and Personal Inquiries\\nRelationship Communication Issues\\nCats and Dog Breeds\\nJessica's Resilient Romance\\nMedieval Fantasy Quest Narrative\\nCyberpunk Sci-Fi Narratives\\nMovie Reviews and Discussions\\nAI Image Prompt Crafting\\nAnime Evolution & Relationships\\nIsrael-Hamas Conflict Analysis\\nAntisemitism and Historical Inaccuracies\\nUS Presidents Inquiry\\nGlobal Geography & Demographics\\nSimplified Quantum Mechanics Explained\\nAdvanced Mathematical Concepts\\nChemical Reactions and Calorimetry\\nRegression Analysis Essentials\\nMath Order of Operations\\nSports Rankings and Analysis\\nFed Inflation Rate Decisions\\nAutomotive Insights and Comparisons\\nPC Components & Upgrades\\nDetermining Today's Day Puzzles\\nTrip Itinerary Planning\\nWeight Comparison Bricks Feathers\\nBody Recomposition Weekly Workout\\nCooking and Recipes\\nHorticulture & Plant Care\\nSocietal Ideologies & Welfare\\nApple Count After Eating\\nMusic Discovery & Recommendations\\nGuitar Chords and Music Theory\\nChess Gameplay and Strategies\\nRiddles Creation and Solutions\\nPoetry Writing & Styles\\nOriginal Joke Requests\\nJapanese Language Translation\\nWord Play and Phonetics\\nGerman Language Practice\\nHexadecimal Message Decryption\\nASCII Art Creations\\nLinux vs Windows Comparison\\nLinux Terminal Command Simulation\\nRust Programming Essentials\\nSQL Database Table Queries\\nWeb Development Essentials\\nAWS Terraform Deployment & Services\\nPandas DataFrame Operations\\nPyTorch Neural Networks\\nTransformer Attention Mechanisms\\nLarge Language Models Analysis\\nHierarchical Clustering\\n\\nFigure 12. Top-64 clusters visualized in hierarchy. x-axis represents the cosine similarity distance. y-axis shows the topic title per cluster summarized by gpt-4-turbo.\"}"}
{"id": "3MW8GKNyzI", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Confidence Interval Simulation Study\\n\\nWe conduct a simulation study to evaluate the bootstrap confidence intervals versus the sandwich estimator. To a large extent, both intervals are the same\u2014indeed, their intervals are often identical to the naked eye. Nonetheless, in our experiments, there are some differences. First, in Figure 13, we conduct a replay study using the same 213576 data points mentioned in the main text. We also do a suite of experiments in simulation using the same beta generating process as in the main text, with $\\\\gamma = 2$. The result is shown in Figure 14; results are similar across many choices of the parameter $\\\\gamma$ and the model strength, which indicates that both intervals will have good coverage and width in the practical conditions we would expose them to.\\n\\nB. The Nonparametric Bradley-Terry Model\\n\\nNonparametric Bradley-Terry. We next consider a nonparametric extension of the Bradley-Terry (BT) model (Bradley & Terry, 1952) to the case where the ranking is not necessarily transitive. Let $G(m)$ denote the set of all paths to the model $m$, i.e., $G(m) = \\\\{ g \\\\in B : g \\\\neq g_i \\\\neq g_j, \\\\forall i \\\\neq j, g_M = m \\\\}$ (11), where $B = A \\\\cup \\\\{(a_2, a_1) : a \\\\in A\\\\}$. Each element of $G(m)$ is a chain of model pairings that leads to $m$; for example, if $m = 5$ and $M = 6$, one element of $G(m)$ is $((1, 2), (2, 4), (4, 3), (3, 6), (6, 5))$. Our score function is given by the average...\"}"}
{"id": "3MW8GKNyzI", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\npath-sum of the log odds of the second model winning, over the entirety of $G(m)$:\\n\\n$$s(\\\\theta)m = \\\\sum_{g \\\\in G(m)} \\\\log \\\\theta((1, g_1, 1)) - \\\\theta((1, g_1, 1)) + \\\\sum_{a \\\\in g} \\\\log \\\\theta(a)$$\\n\\nwhere $\\\\theta(a) = \\\\theta(a_1) \\\\mathbb{1}\\\\{a \\\\in A\\\\} + (1 - \\\\theta((a_2, a_1))) \\\\mathbb{1}\\\\{a \\\\notin A\\\\}$, with the convention that $\\\\theta((m, m)) = 1/2$ for all $m$.\\n\\nNote that for any $g \\\\in G(m)$ where $a \\\\in g$ and $m \\\\notin a$, we also have some $g' \\\\in G(m)$ such that $(a_2, a_1) \\\\in g'$. Meanwhile, if $a \\\\in g$ and $m \\\\in a$, then $a = (m', m)$ for some $m'$. Thus, we can compute\\n\\n$$s(\\\\theta)m = \\\\sum_{a \\\\in A} \\\\mathbb{1}\\\\{m / \\\\in a\\\\} \\\\log \\\\theta(a) - \\\\theta(a) + \\\\log \\\\theta((m', m'))$$\\n\\nHow is the BT score related to the original Bradley-Terry model? In the original Bradley-Terry model, $H_t \\\\in \\\\{0, 1\\\\}$, and the probability of model $m$ beating model $m'$ is assumed to be given by\\n\\n$$\\\\theta((m', m)) = e^{\\\\xi_m} e^{\\\\xi_m'} + e^{\\\\xi_m'}$$\\n\\nfor some unknown parameters $\\\\xi_1, \\\\ldots, \\\\xi_M$\u2014the Bradley-Terry coefficients. The basic goal of the Bradley-Terry model is to estimate these parameters from the observed outcomes. In our setting, however, we use the outcomes to get a CLT on $\\\\theta$, and then can immediately recover the coefficients. Taking without loss of generality $\\\\xi_1 = 0$, we have that\\n\\n$$\\\\log \\\\theta((1, m')) - \\\\theta((1, m')) + \\\\log \\\\theta((m', m)) - \\\\theta((m', m)) = \\\\log \\\\theta((1, m')) \\\\theta((m', 1)) + \\\\log \\\\theta((m', m)) \\\\theta((m, m'))$$\\n\\nThus, all the sums over paths in (12) are equal to\\n\\n$$\\\\xi_m' + \\\\xi_m - \\\\xi_{g_1, 1}$$\\n\\nTaking without loss of generality $\\\\xi_1 = 0$, we have that\\n\\n$$\\\\log \\\\theta((1, m')) - \\\\theta((1, m')) + \\\\log \\\\theta((m', m)) - \\\\theta((m', m)) = \\\\log e^{\\\\xi_m} (e^{\\\\xi_m'} + 1) e^{\\\\xi_m'} + 1 + \\\\log e^{\\\\xi_m} (e^{\\\\xi_m'} + e^{\\\\xi_m}) e^{\\\\xi_m'} (e^{\\\\xi_m'} + e^{\\\\xi_m})$$\\n\\nThus, if the parametric BT model is well-specified, the nonparametric version will exactly recover the Bradley-Terry coefficients. However, our nonparametric analogue of the BT model has major advantages over the original: it will retain statistical validity even if $H_t$ is not binary, if the win rate is non-transitive, and if the logistic model assumed by the BT model is misspecified. In practice, the nonparametric BT coefficient can be easily computed by (16).\"}"}
{"id": "3MW8GKNyzI", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nC. Valid P-Value\\n\\nConsider the p-value $p_i = 1 |H_A'| + 1 \\\\left( 1 + X_{h \\\\in H_A'} \\\\{ h \\\\geq H_i' \\\\} \\\\right)$.\\n\\n(25)\\n\\nWe will prove that this p-value is valid, i.e., that $P(p_i \\\\leq t) \\\\leq t$, under the null hypothesis that the vector $H_i' = (H_t, A_t = A_i')$ is exchangeable, where $\\\\parallel$ denotes the concatenation operator. First, notice that we can equivalently write $p_i = 1 |H_i'| X_{h \\\\in H_i'} \\\\{ h \\\\geq H_i' \\\\}$.\\n\\n(26)\\n\\nWe also have that $P(p_i \\\\leq t) \\\\leq P(p_i \\\\leq k |H_i'|)$, where $k = \\\\lfloor t |H_i'| \\\\rfloor / |H_i'|$. Then, since the data points are exchangeable, we have that $H_i'$ is uniformly distributed among the ranks of $H_i'$, so $P(p_i \\\\leq k |H_i'|) \\\\leq k |H_i'| \\\\leq t$, completing the proof.\"}"}
{"id": "3MW8GKNyzI", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An example user prompt from the cluster \\\"Python Game Programming Challenge\\\". This prompt distinctly separates GPT-4-0613 performance from Llama-2-70b-chat. Due to limited space, the code from each assistant's response is omitted.\\n\\nExample 3: An example user prompt from the cluster \\\"Movie Recommendations & Rating\\\". Both assistants retrieve the correct movie information and gave an almost identical explanation. Therefore, this prompt cannot distinguish GPT-4-0613's performance from Llama-2-70b-chat.\"}"}
{"id": "3MW8GKNyzI", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Example 4:\\n\\nAn example user prompt from the cluster \\\"Travel Itinerary Planning\\\". Both assistant provided an equally helpful itinerary. Therefore, this prompt cannot distinguish GPT-4-0613's performance from Llama-2-70b-chat.\\n\\n**User Prompt**\\n\\nCan you make a list of things I could do with my girlfriend in Lille, France?\\n\\n**GPT-4-0613's Answer**\\n\\nAbsolutely, here are some suggestions for activities you and your girlfriend can enjoy in Lille, France:\\n\\n1. Visit the Palais des Beaux-Arts: This is one of France's largest museums and it's right in the heart of Lille. It houses collections of European paintings, a medieval and Renaissance collection, ceramics, French sculptures and more.\\n2. Explore the Vieux Lille: Stroll through the charming old town with its Flemish influenced architecture, chic boutiques, and delightful eateries.\\n3. Visit the Lille Cathedral (Basilique-cathedrale Notre-Dame-de-la-Treille): This beautiful cathedral is a mix of modern and traditional architecture.\\n4. Enjoy a picnic at the Citadel Park: This is a great place to relax, have a picnic, and enjoy the outdoors.\\n5. Visit the LaM (Lille Metropole Musee d'art moderne, d'art contemporain et d'art brut): This museum is home to a unique collection of modern and contemporary art.\\n6. Take a boat tour on the Deule River: This is a romantic way to see the city and its surroundings.\\n7. Visit the Maison Natale de Charles de Gaulle: This is the birthplace of Charles de Gaulle and it's now a museum dedicated to his life and work.\\n8. Explore the Marche de Wazemmes: This is one of the largest markets in France and it's a great place to buy local products.\\n9. Enjoy a meal at a local restaurant: Lille is known for its delicious cuisine, so make sure to try some local dishes.\\n10. Visit the Lille Zoo: This is a small but well-kept zoo that houses about 70 different species.\\n11. Take a romantic walk through the Jardin des Plantes de Lille: This botanical garden is a peaceful place to enjoy nature.\\n12. Visit the Musee de l'Hospice Comtesse: This museum is located in a former hospital and it's home to a collection of art and artifacts from the Middle Ages.\\n13. Enjoy a performance at the Lille Opera: Check the schedule and book tickets for a romantic night out.\\n14. Take a day trip to the nearby city of Bruges: This charming Belgian city is just a short train ride away.\\n15. Visit the Le Palais Rihour: This palace houses the Lille Tourist Office and it's also home to the Salle du Conclave, which has a beautiful Gothic fireplace.\\n\\nRemember to check the opening times and any COVID-19 restrictions before you visit. Enjoy your trip!\\n\\n**Llama-2-70b-chat's Answer**\\n\\nSure! Lille is a beautiful city in northern France that offers a variety of activities for couples to enjoy. Here are some ideas for things you could do:\\n\\n1. Visit the Palais des Beaux-Arts: This is one of France's largest museums and it's right in the heart of Lille. It houses collections of European paintings, a medieval and Renaissance collection, ceramics, French sculptures and more.\\n2. Explore the Vieux Lille: Stroll through the charming old town with its Flemish influenced architecture, chic boutiques, and delightful eateries.\\n3. Visit the Lille Cathedral (Basilique-cathedrale Notre-Dame-de-la-Treille): This beautiful cathedral is a mix of modern and traditional architecture.\\n4. Enjoy a picnic at the Citadel Park: This is a great place to relax, have a picnic, and enjoy the outdoors.\\n5. Visit the LaM (Lille Metropole Musee d'art moderne, d'art contemporain et d'art brut): This museum is home to a unique collection of modern and contemporary art.\\n6. Take a boat tour on the Deule River: This is a romantic way to see the city and its surroundings.\\n7. Visit the Maison Natale de Charles de Gaulle: This is the birthplace of Charles de Gaulle and it's now a museum dedicated to his life and work.\\n8. Explore the Marche de Wazemmes: This is one of the largest markets in France and it's a great place to buy local products.\\n9. Enjoy a meal at a local restaurant: Lille is known for its delicious cuisine, so make sure to try some local dishes.\\n10. Visit the Lille Zoo: This is a small but well-kept zoo that houses about 70 different species.\\n11. Take a romantic walk through the Jardin des Plantes de Lille: This botanical garden is a peaceful place to enjoy nature.\\n12. Visit the Musee de l'Hospice Comtesse: This museum is located in a former hospital and it's home to a collection of art and artifacts from the Middle Ages.\\n13. Enjoy a performance at the Lille Opera: Check the schedule and book tickets for a romantic night out.\\n14. Take a day trip to the nearby city of Bruges: This charming Belgian city is just a short train ride away.\\n15. Visit the Le Palais Rihour: This palace houses the Lille Tourist Office and it's also home to the Salle du Conclave, which has a beautiful Gothic fireplace.\\n\\nRemember to check the opening times and any COVID-19 restrictions before you visit. Enjoy your trip!\"}"}
{"id": "3MW8GKNyzI", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"do with your girlfriend in Lille:\\n\\n1. Explore the city's charming streets and historic landmarks, such as the Grand Place, the Vieille Bourse, and the Cathedral of Lille.\\n2. Visit one of the many museums, such as the Palais des Beaux-Arts de Lille, the Musee d'Art Moderne, d'Art Contemporain et d'Art Brut, or the Musee de l'Hospice Comtesse.\\n3. Take a stroll along the canal and enjoy the views of the city from the waterfront.\\n4. Visit the famous Lille Christmas market, which takes place every year in December and features festive stalls, food, and drinks.\\n5. Go shopping at the city's many boutiques and galleries, or at the large shopping centers such as the Euralille or the Lille Europe.\\n6. Enjoy a romantic dinner at one of Lille's many excellent restaurants, such as the Michelin-starred Le Comptoir du Relais or the cozy Le Bistrot d'Alsace.\\n7. Take a day trip to the nearby city of Bruges, Belgium, which is known for its picturesque canals and medieval architecture.\\n8. Go to a show or concert at the Lille Opera or the Le Zenith concert hall.\\n9. Visit the nearby town of Roubaix, which is famous for its textile industry and its beautiful Art Deco architecture.\\n10. Take a walk in the beautiful Parc de la Citadelle, which offers a peaceful escape from the city and features a pond, a botanical garden, and a zoo.\\n11. Go on a bike ride or a hike in the surrounding countryside, such as the Parc Naturel Regional Nord-Pas de Calais.\\n12. Visit the famous Lille flea market, which takes place every Sunday and offers a wide range of antiques, vintage items, and second-hand goods.\\n13. Take a cooking class together and learn how to make some of the region's delicious specialties, such as carbonnade flamande (beef stew) or tarte au maroilles (cheese tart).\\n14. Go on a wine tasting tour and discover the region's excellent wines, such as the Chablis or the Champagne.\\n15. Take a hot air balloon ride over the city and enjoy a bird's-eye view of Lille's beautiful architecture and surroundings.\\n\\nThese are just a few ideas to get you started. Lille is a vibrant and romantic city that offers many opportunities for couples to have a great time together. Bon voyage!\"}"}
{"id": "3MW8GKNyzI", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nD.2. Arena Bench Prompt\\n\\nExample 1: A prompt from Arena Bench requiring a very high level of complex problem-solving skill and reasoning while adhering to real-world application.\\n\\nCreate a flutter app for habit tracking that tracks daily habits for a user. The user should be able to create multiple daily tasks that he wants to track. And he should be able to group the habits into a set of groups. Once the user completes a task, he is rewarded a number of points per task. The app should have a page that summarizes the total score per group. And the aggregate score of all the groups of habits. This flutter app needs to be compilable for both Android and iOS.\\n\\nExample 2: A prompt from Arena Bench requiring a very high level of complex problem-solving skill and reasoning while adhering to real-world application.\\n\\nI want to set up a remote raspberry pi zero, powered by a solar panel with simple wiring. I want to power a small 2W pump, a simple electret microphone, a custom python script running on the raspberry pi that is used to classify audio detected by the microphone. What components will I need to optimize for cost and minimize any electrical work (e.g. soldering)? What size solar panel will I need to power this whole system?\"}"}
{"id": "3MW8GKNyzI", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. The platform is publicly available at https://chat.lmsys.org.\\n\\n1. Introduction\\n\\nRecent advancements in large language models (LLMs) have significantly expanded their capabilities beyond traditional natural language processing boundaries, addressing a broad array of general tasks (OpenAI, 2023; Gemini et al., 2023; Touvron et al., 2023). These developments underscore the potential of LLMs but also have raised concerns with respect to performance evaluation. Current benchmarks often fail to capture the nuanced and diverse aspects of these models, particularly in assessing their alignment with human preferences in real-world, open-ended tasks.\\n\\nTo assess the performance of LLMs, the research community has introduced a variety of benchmarks. These benchmarks can be categorized based on two factors: the source of questions (either static or live) and the evaluation metric (either ground truth or human preference). According to these factors, benchmarks can be classified into four categories, as shown in Figure 1. While a range of benchmarks is beneficial, the most prevalent current method for evaluating LLMs remains a static, ground-truth-based evaluation, partly because such evaluations are inexpensive and reproducible. However, these static, ground-truth-based benchmarks exhibit several limitations. Firstly, the questions within these benchmarks are not open-ended, hindering the ability to capture the flexible and interactive use found in real-world settings (Zheng et al., 2023b). Secondly, the test sets in these benchmarks are static, meaning they can become contaminated over time, which undermines the reliability of the evaluation results (Yang et al., 2023). Furthermore, for many complex tasks, establishing a definitive ground truth is not only challenging but sometimes unattainable. Consequently, current benchmarks fail to adequately address the needs of state-of-the-art LLMs, particularly in evaluating user preferences. Thus, there is an urgent necessity for an open, live evaluation platform based on human preference that can more accurately mirror real-world usage.\\n\\nCreating such a benchmark platform entails significant challenges. It requires the collection of live, fresh, and diverse\"}"}
{"id": "3MW8GKNyzI", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nuser questions to accurately represent real-world scenarios. Additionally, developing scalable, incremental, and efficient ranking systems is essential for evaluating a large number of models. Moreover, ensuring the quality of human evaluations is crucial given the noisy nature of human preferences.\\n\\nTo this end, we introduce Chatbot Arena, a benchmarking platform for LLMs that features anonymous, randomized battles in a crowdsourced setting. Chatbot Arena is a free website open to all users. On this website, a user can ask a question and get answers from two anonymous LLMs. Afterward, the user casts a vote for the model that delivers the preferred response, with the models' identities revealed only after voting. This crowdsourced method effectively gathers a diverse array of fresh user prompts, accurately reflecting real-world LLM applications. Armed with this data, we employ a suite of powerful statistical techniques, ranging from the statistical model of Bradley & Terry (1952) to the E-values of Vovk & Wang (2021), to estimate the ranking over models as reliably and sample-efficiently as possible.\\n\\nWith these tools in hand, we have designed efficient sampling algorithms specifically to select model pairs in a way that accelerates the convergence of rankings while retaining statistical validity.\\n\\nWe conduct a thorough analysis of the collected data to ensure the credibility of our platform. We demonstrate that the user-generated questions are sufficiently diverse to encompass a wide range of LLM use cases and are sufficiently challenging to differentiate between models. Furthermore, we confirm that the crowd-sourced votes are highly consistent with expert evaluations.\\n\\nWe have been running our system since Apr 2023 and have received over 240K votes from about 90K users in over 100 different languages as of Jan 2024. To encourage user engagement, we have made over 50 state-of-the-art models available for free. We also collaborate with leading model developers such as OpenAI, Google, Anthropic, Mistral, Hugging Face, and various universities, incorporating their latest models into our platform. We keep the community engaged by routinely updating the leaderboard, publishing analytical blogs, releasing datasets, and sharing information via tweets. Because of its unique and significant value, our leaderboard has emerged as one of the most referenced in the LLM field and has become a benchmark for the industry.\\n\\nWe commit to making our data and code available, ensuring that this platform is open-source and open-accessible.\\n\\nWe make the following contributions:\\n\\n- We build the first large-scale crowd-sourced live LLM evaluation platform with over 1M user visits.\\n- We conduct an in-depth analysis of the collected data, including prompt diversity, quality, vote quality, and insights on human feedback.\\n- We will publicly release a human preference dataset with over 100K pairwise votes collected from Chatbot Arena.\\n- We design an efficient sampling algorithm that actively chooses which model pairs to show, such that our sample efficiency improves, sometimes to a large degree.\\n\\n2. Related Work\\n\\nLLM Benchmarks.\\n\\nWe briefly review the common LLM benchmarks, following the classification presented in Figure 1. The most prevalent benchmarks are static, ground-truth-based ones, typically in the form of multiple-choice questions or question-answering tasks with predefined answers and test cases. These benchmarks encompass a range of topics including language understanding, mathematics, coding, and logical reasoning. Prominent examples in this category are MMLU (Hendrycks et al., 2020), Hel-laSwag (Zellers et al., 2019), GSM-8K (Cobbe et al., 2021), BigBench (Srivastava et al., 2023), AGIEval (Zhong et al., 2023), and HumanEval (Chen et al., 2021). Benchmarks focusing on safety, such as ToxicChat (Lin et al., 2023), and comprehensive suites like HELM (Liang et al., 2022), also exist. In addition to closed-ended questions, benchmarks can include open-ended questions that are evaluated by human judgment, which can be rated by experts or crowd workers such as Amazon Mechanical Turk (Karpinska et al., 2021; Geng et al., 2023; Wang et al., 2023). The recent trend includes utilizing GPT-4 for approximating human judgment (Chiang & Lee, 2023), with notable instances being MT-Bench (Zheng et al., 2023b) and AlpacaEval (Li et al., 2023). In addition to static benchmarks, live benchmarks that include fresh questions are also available. These questions can be obtained from annual exams or weekly online contests such as Codeforces (Li et al., 2022; Huang et al., 2023). They can also be sourced from human interaction. Some studies have explored using live human interaction for reinforcement learning from human preference (Bai et al., 2022; Ouyang et al., 2022; Touvron et al., 2023). However, these studies are typically limited to specific organizations.\\n\\nIn this paper, we introduce Chatbot Arena, the first open, large-scale, and crowdsourced benchmark platform that utilizes live human interaction.\\n\\nRisks of Static Benchmarks.\\n\\nStatic benchmarks have certain issues, including contamination, saturation, overfitting, and a lack of human alignment (Yang et al., 2023; Oren et al., 2023). DynaBench (Kiela et al., 2021) identifies these challenges and recommends the use of a live benchmark.\"}"}
{"id": "3MW8GKNyzI", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nFigure 4. Model's performance between Arena Bench and MT-Bench, showing an increased gap between open and proprietary models. Both uses GPT-4 as judge.\\n\\nThen asked experts to label their preference per comparison. The experts were given the prompts and answers blindly, and asked to carefully fact-check model's answer with external resources like search engine. Manually labeling each data point took on average 3-5 minutes. For reference, we also use GPT-4 as a judge for pairwise comparisons. The agreement rate between crowd-users, experts, and GPT-4-judge are presented in Table 3. The corresponding win-rate are shown in Table 4.\\n\\nTo summarize, we observe high agreement rates (72% to 83%) between Arena crowd-user and experts in both setup. Note that agreement rates between two experts are around similar levels (79.4% and 89.8%). As for the 10%-20% disagreement between experts, it is mostly due to some user prompts don't have a ground truth answer. Depending on the preference of the evaluator, sometimes both answers can be argued as being better than the other one, such as the examples in Appendix D.4. The gap between crowd-vs-expert agreement rate and expert-vs-expert agreement rate (5%-10%) is mostly attributed to crowd user making mistakes or overlooking factual errors in model's response. Overall, the agreement rates presented in Table 3 validate the decent quality of crowd-sourced votes in Chatbot Arena.\\n\\n7. Experiments\\n\\n7.1. Ranking system\\n\\nComputing the rank on real data. In this section, we report results from our experiments on approximate ranking. For this experiment, we ran a replay of \\\\( T = 213,576 \\\\) historical votes from our online platform and calculate the BT coefficients using our earlier-described estimation algorithm.\\n\\nThe laborers are graduate students at UC Berkeley.\\n\\nTable 3. Pairwise agreement rate between crowd-user, gpt-4 judge, and experts on pairwise battles. The top part of the table is between GPT-4-Turbo and Llama-2-13b-chat. The bottom is between GPT-4-Turbo and GPT-3.5-Turbo-0613.\\n\\n|                     | Llama-2-13b | Expert 1 | Expert 2 | GPT-4 | Crowd | Expert 1 | Expert 2 | GPT-4 |\\n|---------------------|-------------|----------|----------|-------|-------|----------|----------|-------|\\n| Expert 1            | -           | 89.8%    | 81.0%    |       |       | -        | -        |       |\\n| Expert 2            | -           | -        | 78.5%    |       |       | -        | -        |       |\\n\\nTable 4. GPT-4-Turbo's win-rate across crowd-user, gpt-4 judge, and experts on pairwise battles against Llama-2-13b and GPT-3.5-Turbo-0613.\\n\\n|                     | Llama-2-13b | Expert 1 | Expert 2 | GPT-4 | Crowd | Expert 1 | Expert 2 | GPT-4 |\\n|---------------------|-------------|----------|----------|-------|-------|----------|----------|-------|\\n| Expert 1            | 81.2%       | 89.4%    | 86.9%    | 78.8% |       |          |          |       |\\n| Expert 2            | 76.3%       | 82.5%    | 89.4%    | 79.4% |       |          |          |       |\\n\\nWe evaluate the coverage of the intervals. A natural follow-up question is whether or not the intervals are doing their job correctly: whether they cover the true BT coefficients with probability at least (and almost exactly) \\\\( 1 - \\\\alpha \\\\). Of course, this cannot be evaluated on real data, so we run a simulation.\\n\\nA vector of BT coefficients is drawn, with each coordinate sampled i.i.d. from a distribution \\\\( \\\\text{beta}(1/\\\\gamma, 1/\\\\gamma) \\\\); we take \\\\( \\\\gamma = 2 \\\\) in Figure 6 (and we vary \\\\( \\\\gamma \\\\) in Appendix A). Given these coefficients, a dataset is synthesized, and the coverage and average width are computed for each of 20 trials. The results can be seen in Figure 6 for the uncorrected intervals. The coverage of the intervals behaves as expected, centering around \\\\( 1 - \\\\alpha \\\\), regardless of the number of models. Meanwhile, the more models are included, the larger the intervals become.\\n\\nEvaluating the active sampling rule. Next, we discuss the evaluation of our active sampling rule as Equation (9) for win matrix estimation. We evaluate this sampling rule by taking the best fit BT coefficients to our 213,576 point sized holdout set, and then sampling from that distribution using our active sampling algorithm. The results are displayed in Figure 7. It is hard to tell by looking at plots, but the improvement is substantial: To estimate \\\\( \\\\theta^* \\\\) to a precision of 0.2, random needs 6,800 samples and adaptive needs 4,400 samples; meanwhile to estimate the score to a precision of 0.3, random needs 17,200 samples and adaptive needs 16,400 samples. Thus, the random baseline requires 54% and 5% more data to achieve the same level of precision.\"}"}
{"id": "3MW8GKNyzI", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\n| Model Name                        | Version   |\\n|----------------------------------|-----------|\\n| zephyr-7b-beta                   | (#19-37)  |\\n| wizardlm-13b                     | (#19-37)  |\\n| llama2-70b-steerlm-chat          | (#19-37)  |\\n| solar-10.7b-instruct-v1.0        | (#18-36)  |\\n| dolphin-2.2.1-mistral-7b         | (#17-37)  |\\n| pplx-70b-online                  | (#17-31)  |\\n| gpt-3.5-turbo-1106               | (#15-29)  |\\n| openchat-3.5                     | (#14-29)  |\\n| llama-2-70b-chat                 | (#14-28)  |\\n| openhermes-2.5-mistral-7b        | (#12-28)  |\\n| vicuna-33b                       | (#7-25)   |\\n| starling-lm-7b-alpha             | (#7-26)   |\\n| gpt-3.5-turbo-0314               | (#7-22)   |\\n| wizardlm-70b                     | (#7-22)   |\\n| tulu-2-dpo-70b                   | (#6-21)   |\\n| yi-34b-chat                      | (#6-19)   |\\n| claude-instant-1                 | (#6-19)   |\\n| gemini-pro                       | (#6-18)   |\\n| mixtral-8x7b-instruct-v0.1       | (#4-18)   |\\n| gemini-pro-dev-api               | (#3-18)   |\\n| claude-2.1                       | (#3-14)   |\\n| claude-1                         | (#3-8)    |\\n| mistral-medium                   | (#3-8)    |\\n| gpt-4-0613                       | (#3-7)    |\\n| gpt-4-0314                       | (#2)      |\\n| gpt-4-turbo                      | (#1)      |\\n\\nFigure 5. Intervals for the BT coefficients with and without multiplicity correction. The multiplicity correction, in this case a chi-square CLT interval, is technically required for the purpose of calculating the ranking, because it ensures all scores are simultaneously contained in their intervals (and the ranking is a function of all the scores). However, it induces extra conservatism, so we report both intervals.\\n\\nFigure 6. Intervals for the BT coefficients as a function of the number of samples and the number of models $M$.\\n\\nFigure 7. Interval widths on the win matrix (upper figure) and on the BT coefficients (lower figure) as a function of the number of samples, for random sampling and also adaptive sampling. Improvements from adaptive sampling can be seen in both cases, although they are more subtle on the scale of the score.\\n\\nTable 5. Confusion matrix of different $\\\\alpha$. \\\"Pred.\\\" means predicted. Positive means anomalous and negative means normal.\\n\\n| $\\\\alpha$ | Pred. Positive | Pred. Negative |\\n|----------|----------------|----------------|\\n| $\\\\alpha = 0.1$ | 13/14          | 12/36          |\\n| $\\\\alpha = 0.3$ | 21/29          | 4/21           |\\n\\nrespectively. One can see from the plots in Figure 7 that these results are not cherry-picked: the sample-efficiency of our method is better at all values on the horizontal axis.\\n\\n7.2. Anomalous Users Detection\\n\\nWe evaluate the outlier detection method in Section 5.1. We construct the evaluation set by manually identifying 25 anomalous users whose inputs are highly repetitive or meaningless (e.g., asking \\\"hi\\\" for 100 times or inputting garbled texts). We randomly sample 25 normal users with at least 50 votes, and inspect their input prompts to ensure no abnormal behaviors. As mentioned in Section 5.1, per user we compute five $M_j$ and identify the user as anomalous if $M_j \\\\geq \\\\chi^2_{j, 1 - \\\\alpha/5}$. We present results of two different $\\\\alpha$ (i.e., the significance level) in Table 5. We find the detection method effective (e.g., reaching 90% true positive and 60-70% true negative rate). We inspect the false negative errors and find those are from users do not always behave abnormally, making them harder to detect.\\n\\n8. Discussion\\n\\nLimitations. Although our user base is extensive, we anticipate that it will primarily consist of LLM hobbyists and...\"}"}
{"id": "3MW8GKNyzI", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nresearchers who are eager to experiment with and evaluate the latest LLMs. This inclination may result in a biased distribution of users. Additionally, despite the wide array of topics encompassed by the prompts discussed in previous sections, the data predominantly comes from our online chat interface. This source might not accurately reflect the real-world usage of LLMs in production environments or specialized domains, potentially leading to a skewed prompt distribution. Moreover, our study concentrates on assessing the helpfulness of LLMs but overlooks their safety aspects. We recognize the possibility and necessity of a parallel mechanism to evaluate the safety of these models.\\n\\nFuture Directions.\\nIn our future work, we plan to develop comprehensive topic leaderboards and establish a dedicated section for multimodal and agent-based LLMs in more dynamic, gamified settings, catering to more complex tasks. We also believe our approach to detecting harmful users could be improved and made more formally rigorous by using the theory of nonnegative supermartingales and E-values (Howard et al., 2020; Waudby-Smith & Ramdas, 2020; Ov\u0161k & Wang, 2021; Ramdas et al., 2023); this would deal with the dependence, but the variants we tried did not perform well in terms of power.\\n\\n9. Conclusion\\nIn this paper, we present Chatbot Arena, an open platform for evaluating LLMs through crowdsourced, pairwise human preferences. We conduct an in-depth analysis of the crowdsourced user prompts and preference votes to validate the diversity and quality. We develop an efficient model sampling and ranking algorithm. Our dataset including 100K pairwise preference votes will be released for future research.\\n\\nAcknowledgments\\nThis project is partly supported by sponsorship from Kaggle, MBZUAI, a16z, Together AI, Anyscale, and HuggingFace. This project is also partly supported by Accenture, AMD, Google, IBM, Intel, Microsoft, Samsung SDS, SAP, Uber, and VMware. Lianmin Zheng is supported by a Meta Ph.D. Fellowship. The authors would like to thank Siyuan Zhuang for insightful discussion and Tijana Zrni\u0107 for helpful feedback on the manuscript.\\n\\nImpact Statement\\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.\"}"}
{"id": "3MW8GKNyzI", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "3MW8GKNyzI", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nLin, Z., Wang, Z., Tong, Y., Wang, Y., Guo, Y., Wang, Y., and Shang, J. ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 4694\u20134702, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.311. URL https://aclanthology.org/2023.findings-emnlp.311.\\n\\nLiu, T.-Y. et al. Learning to rank for information retrieval. Foundations and Trends\u00ae in Information Retrieval, 3(3):225\u2013331, 2009.\\n\\nMcInnes, L., Healy, J., and Melville, J. Umap: Uniform manifold approximation and projection for dimension reduction, 2020.\\n\\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\n\\nOren, Y., Meister, N., Chatterji, N., Ladhak, F., and Hashimoto, T. B. Proving test set contamination in black box language models. arXiv preprint arXiv:2310.17623, 2023.\\n\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022.\\n\\nRamdas, A., Gr\u00fcnwald, P., Ovovk, V., and Shafer, G. Game-theoretic statistics and safe anytime-valid inference. Statistical Science, 38(4):576\u2013601, 2023.\\n\\nRao, P. V. and Kupper, L. L. Ties in paired-comparison experiments: A generalization of the bradley-terry model. Journal of the American Statistical Association, 62(317):194\u2013204, 1967. doi: 10.1080/01621459.1967.10482901.\\n\\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023.\\n\\nSz\u00f6r\u00e9nyi, B., Busa-Fekete, R., Paul, A., and H\u00fcllermeier, E. Online rank elicitation for plackett-luce: A dueling bandits approach. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/7eacb532570ff6858afd2723755ff790-Paper.pdf.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n\\nOvovk, V. and Wang, R. E-values: Calibration, combination and applications. The Annals of Statistics, 49(3):1736\u20131754, 2021.\\n\\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13484\u201313508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL https://aclanthology.org/2023.acl-long.754.\\n\\nWaudby-Smith, I. and Ramdas, A. Estimating means of bounded random variables by betting. arXiv preprint arXiv:2010.09686, 2020.\\n\\nWhite, H. Maximum likelihood estimation of misspecified models. Econometrica: Journal of the Econometric Society, pp. 1\u201325, 1982.\\n\\nYang, S., Chiang, W.-L., Zheng, L., Gonzalez, J. E., and Stoica, I. Rethinking benchmark and contamination for language models with repurposed samples. arXiv preprint arXiv:2311.04850, 2023.\\n\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791\u20134800, 2019.\\n\\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Li, Z., Lin, Z., Xing, E. P., Gonzalez, J. E., and Stoica, I. Lmsys-chat-1m: A large-scale real-world lmp conversation dataset, 2023a.\\n\\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023b. URL https://openreview.net/forum?id=uccHPGDlao.\"}"}
{"id": "3MW8GKNyzI", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nZhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.\\n\\nZhu, B., Frick, E., Wu, T., Zhu, H., and Jiao, J. Starling-7b: Improving LLM helpfulness & harmlessness with rlaif, November 2023.\"}"}
{"id": "3MW8GKNyzI", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nFigure 8. Screenshot of Chatbot Arena.\\n\\nFigure 9. The number of votes over time.\"}"}
{"id": "3MW8GKNyzI", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nFigure 10. The number of votes per model.\\n\\nFigure 11. Similarity matrix of top-64 topic clusters.\"}"}
{"id": "3MW8GKNyzI", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nD.3. Arena Bench System Prompt\\n\\nThe novel evaluation procedure is as follow: we prompt GPT-4-Turbo with the system prompt displayed below alongside a user prompt, a reference answer, and 2 assistant's answers. For reference answer, we present the user prompt with 3 assistants' answers, GPT-4-Turbo, GPT-4-0314, and Claude-1, to GPT-4-Turbo and ask GPT-4-Turbo to generate an answer to the prompt. To ensure consistent pairwise judgment, we set up GPT-3.5-Turbo-0301 as the baseline answer for all models to be compared against. To avoid positional bias, we conduct two judgments per prompt: the first judgment presents the baseline answer as Assistant A while the second judgment presents the baseline answer as Assistant B. In total, we conduct 700 pairwise comparisons between each model against GPT-3.5-Turbo-0301 across 350 user prompts to calculate a win-rate against the baseline. Then we project the win-rate on a scale from 0 to 10 by assigning wins with a score of 10, ties with a score of 5, and losses with a score of 0. Further, we assign a significant win or loss as 3 wins or 3 losses, respectively, and keeping the other verdicts as a single win, loss, or tie. Finally, we calculate the final score by averaging across the wins, losses, and ties.\\n\\n<|System Prompt|>\\n\\nPlease act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt displayed below. Your job is to evaluate which assistant's answer is better.\\n\\nWhen evaluating the assistants' answers, compare both assistants' answers. You must identify and correct any mistakes or inaccurate information. Then consider if the assistant's answers are helpful, relevant, and concise. Helpful means the answer correctly responds to the prompt or follows the instructions. Note when user prompt has any ambiguity or more than one interpretation, it is more helpful and appropriate to ask for clarifications or more information from the user than providing an answer based on assumptions. Relevant means all parts of the response closely connect or are appropriate to what is being asked. Concise means the response is clear and not verbose or excessive. Then consider the creativity and novelty of the assistant's answers when needed. Finally, identify any missing important information in the assistants' answers that would be beneficial to include when responding to the user prompt.\\n\\nAfter providing your explanation, you must output only one of the following choices as your final verdict with a label:\\n\\n1. Assistant A is significantly better: \\n2. Assistant A is slightly better: \\n3. Tie, relatively the same: \\n4. Assistant B is slightly better: \\n5. Assistant B is significantly better:\\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201cMy final verdict is tie: \\n\\nExample output: \u201d"}
{"id": "3MW8GKNyzI", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nD.4. Arena Battles with No Ground Truth\\n\\nExample 1: An example of an Arena Battle between GPT-4-Turbo and Llama-2-13B-chat, where depending on different preferences, either answer can be argued as better than the other. While both answers are correct, GPT-4-Turbo\u2019s answer is concise and appropriate to the format of multiple choice question. On the other hand, Llama-2-13B-chat provides helpful explanation for the multiple choice answer.\\n\\n<|User Prompt|>\\nMultip\\\\choice Question\\n\\nAll plants have both a haploid and a diploid form. As the plant changes from one form to another, this is called:\\n\\n- Alternative life cycle\\n- Succession\\n- An alternation of generations\\n\\n<|GPT-4-Turbo\u2019s Answer|>\\nan alternation of generations\\n\\n<|Llama-2-13B-chat\u2019s Answer|>\\nThe correct answer is: an alternation of generations.\\n\\nPlants, like other organisms, have different life cycle stages, and they can exist in two different forms: haploid and diploid. The haploid form is a single set of chromosomes, while the diploid form is two sets of chromosomes. As the plant changes from one form to another, this is called an alternation of generations. This process involves the alternation of haploid and diploid phases, and it is a fundamental aspect of the life cycle of plants.\\n\\nAlternative life cycle and succession are not correct descriptions of this process, as they refer to different concepts. Alternative life cycle refers to the different stages of life an organism may go through, such as larva, pupa, and adult in insects, while succession refers to the process of one species replacing another in a particular environment or niche.\\n\\nExample 2: An example of an Arena Battle between GPT-4-Turbo and Llama-2-13B-chat, where depending on different preferences, either answer can be argued as better than the other.\\n\\n<|User Prompt|>\\nWhat are critical elements of a Product Mindset for startup founders?\\n\\n<|GPT-4-Turbo\u2019s Answer|>\\nAdopting a Product Mindset is crucial for startup founders as it helps them focus on creating products that deliver value to customers while achieving business objectives. Here are some critical elements of a Product Mindset for startup founders:\\n\\n1. Customer Centricity:\\n   - Understand and empathize with your customers\u2019 needs, problems, and motivations.\"}"}
{"id": "3MW8GKNyzI", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\n- Continuously gather and incorporate customer feedback into the product development process.\\n- Prioritize features and improvements that directly address customer pain points and enhance their experience.\\n\\n2. Value Proposition:\\n- Clearly define the unique value your product offers to the market.\\n- Align your product's features and capabilities with its value proposition to ensure it meets the expectations of your target audience.\\n\\n3. Problem-Solving Focus:\\n- Identify and focus on solving real problems for users rather than just creating features for the sake of innovation.\\n- Validate assumptions and hypotheses through experimentation and user testing.\\n\\n4. Iterative Development:\\n- Embrace an agile development approach, where you build, measure, and learn in iterative cycles.\\n- Be open to pivoting or making significant changes based on feedback and market response.\\n\\n5. Data-Driven Decision Making:\\n- Use data to inform decisions about product development, marketing, and growth strategies.\\n- Establish key performance indicators (KPIs) to measure the success of the product and track progress over time.\\n\\nBy internalizing these elements, startup founders can develop a strong Product Mindset that guides their decisions and strategies, ultimately leading to a product that customers love and that contributes to the startup's success.\\n\\nA product mindset is a way of thinking and approaching product development that is critical for startup founders. Here are some essential elements of a product mindset for startup founders:\\n\\n1. Customer-centricity: A product mindset starts with understanding the customer's needs, desires, and pain points. Founders must be obsessed with their customers and be willing to put in the effort to understand their needs.\\n\\n2. Iteration and experimentation: A product mindset is all about experimentation and iteration. Founders must be willing to try new things, test hypotheses, and iterate on their product based on customer feedback.\\n\\n3. Focus on value: A product mindset is focused on delivering value to the customer. Founders must be able to identify and prioritize the most important features and functionality that deliver the most value to their customers.\"}"}
{"id": "3MW8GKNyzI", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\n4. Collaboration: A product mindset requires collaboration between cross-functional teams, including engineering, design, and product management. Founders must be able to work effectively with these teams to develop a product that meets customer needs.\\n\\n5. Data-driven decision making: A product mindset is data-driven, and founders must be comfortable using data to make informed decisions about their product. This includes analyzing customer feedback, usage data, and other metrics to inform product development.\\n\\nBy embracing these elements of a product mindset, startup founders can develop a product that meets customer needs, delivers value, and sets their company up for long-term success.\"}"}
{"id": "3MW8GKNyzI", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference that incorporates a human-in-the-loop approach for classical NLP benchmarks. Our system adopts a similar spirit. However, our focus is on chatting with LLMs, and we implement this on a significantly larger user scale.\\n\\nRanking System. Ranking systems have been a well-studied topic in statistics. Related topics include probability models (Hunter, 2004; Rao & Kupper, 1967), rank elicitation (Sz\u00f6r\u00e9nyi et al., 2015; Busa-Fekete et al., 2014a;b), and online experiment design (Chernoff, 1992; Karimi et al., 2021). The Elo rating system has also been used for LLMs (Bai et al., 2022; Boubdir et al., 2023). Contributing to this literature, we introduce techniques for accelerating ranking convergence and detecting abnormalities, specifically applied to large-scale, real-world settings of LLMs.\\n\\nHuman Preference Dataset. Owing to the significance of human preferences, several datasets and analyses exist that incorporate human preferences. These include OpenAssistant (K\u00f6pf et al., 2023), HH-RLHF (Bai et al., 2022), LMSYS-Chat-1M (Zheng et al., 2023a), and synthetic approximations of human preferences like UltraFeedback (Cui et al., 2023) and Nectar (Zhu et al., 2023). Our prior data release, LMSYS-Chat-1M (Zheng et al., 2023a), is similarly collected via crowdsourcing. However, LMSYS-Chat-1M comprises solely conversations and lacks human preference data, rendering it unsuitable for direct use in ranking studies. This paper focuses on the analysis of preference data for ranking purposes.\\n\\n3. Human Preference Data Collection\\n\\nIn this section, we discuss our interface design to collect human preferences and present summary statistics.\\n\\n3.1. Interface\\n\\nChatbot Arena crowd-sources feedback from users for model evaluation. Our goal is to design an ease-of-use interface to reduce friction for users to contribute data. Since we collect feedback from many users, it is difficult to set a consistent grading rubric across different people. Hence, we adopt a pairwise comparison mechanism where users only need to compare two model responses and vote for the better one, instead of requiring users to provide an absolute score.\\n\\nIn each battle, two anonymous models are sampled. To encourage data diversity, we do not preset any input prompt on the website. Users are free to input any prompt to the two models. We believe this creates incentives for user engagement, particularly given that we offer a free service. It also helps us collect a diverse set of inputs representing real-world usage. After models provide their answers, user compare them side-by-side and vote for the preferred answer. If a user cannot choose in the first turn, the user can continue chatting until identifying a winner. For those who are unsure, we also present two buttons, \\\"tie\\\" or \\\"both are bad.\\\" Figure 8 shows a screenshot of our interface. Before using our service, users are required to accept terms of use, which gives us their consent to release the data publicly.\\n\\n3.2. Data Statistics\\n\\nWe began collecting data in April 2023. As of Jan 2024, we have received around 240K votes from over 90K users. Our data involves more than 50 models, including both proprietary models like GPT-4, Claude, and Gemini, as well as open models such as LLaMA and Mistral. These conversations cover more than 100 languages, with 77% being in English, 5% in Chinese, and the remaining languages, such as Russian, German, Spanish, French, and Japanese, each representing less than 2% of the total. Each data point includes multi-turn conversations between the user and two LLMs, and a vote to indicate which model the user prefers. We summarize statistics in Table 1 along with other existing human preference datasets.\\n\\nFigure 10 in the Appendix shows the vote count per model. On average, 8K votes are collected for each model. In Figure 2, we select a set of representative models and present their win rate and the number of battles. Note that we employ non-uniform sampling to concentrate votes on model pairs that have similar performance due to higher uncertainty. This helps us reduce the number of votes required to reach stable results. We later develop an adaptive sampling method and demonstrate its effectiveness against random sampling. See Section 5 for further analysis.\\n\\nTo ensure anonymity, we use keywords to filter out conversations containing model identity such as model name (e.g., GPT, Claude) or companies (e.g., OpenAI, Anthropic). To avoid misuse, we adopt OpenAI moderation API to flag conversations that contain unsafe content. The flagged user requests account for 3% of the total requests. Figure 9 in the Appendix shows the number of valid user votes over time, where we get 1-2K votes per day in recent months and spikes as we introduce new models or leaderboard updates.\\n\\n4. From Pairwise Comparisons to Rankings\\n\\nOur data consists of pairwise comparisons\u2014but how can we use these comparisons to recover a ranking over all models? This is a well-studied topic in the literature on learning to rank (Liu et al., 2009), and we present our perspective here. We let $A = \\\\{ (m, m') : m < m' \\\\text{ and } m, m' \\\\in [M] \\\\}$ denote our comparative data set. We consider a sequential setting, where at time $t \\\\in \\\\mathbb{N}$, we serve the human a pair of models $A_t \\\\in A$ (which we pick), and in turn we observe the human's response $H_t \\\\in [0, 1]$. As an example, we might have that $A_t = (1, 2)$ and $H_t = 1, 2$.\"}"}
{"id": "3MW8GKNyzI", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The best model has rank 1, they will both get assigned rank 1. In particular, we have the rank of model $m$.\\n\\nA standard score function in this setting $P(\\\\text{A} > \\\\text{B}) = 1 + \\\\frac{a}{\\\\xi}$. Finding the win matrix is a relatively straightforward mean-estimation problem; we will provide details in Section 5.\\n\\nFormally, consider a score distribution over a form distribution over $[0,1]$. $s \\\\in \\\\{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1\\\\}$. Without loss of generality, we take $P(\\\\text{A} > \\\\text{B}) = 1 + \\\\frac{a}{\\\\xi}$. Our goal is to estimate the probability the human prefers model $A$ to $B$.\\n\\nFigure 2. Win-rate (left) and battle count (right) between a subset of models in Chatbot Arena.\\n\\nTable 1. Statistics of human preference datasets, including Anthropic HH (Bai et al., 2022), OpenAssistant Conversations (K\u00f6pf et al., 2023), and Chatbot Arena (as of 2024/1/21). The tokens are counted by Llama2's tokenizer. \u201cConv\u201d = Conversation. \u201cLang\u201d = Language.\\n\\n| Model            | Per Sample | Per Prompt | Per Response |\\n|------------------|------------|------------|--------------|\\n| llama-2-13b-chat | 66,497     | -          | 13,500       |\\n| llama-2-7b-chat  | 243,329    | 50         | 90,051       |\\n| llama-2-70b-chat | 338,704    | -          | 143          |\\n| OpenAssistant    | 214.2      | 35         | 36.9         |\\n| Anthropic HH     | 18.9       | 1         | 2.3          |\\n\\n$H_0 : \\\\theta = 0 \\\\iff P(\\\\text{A} > \\\\text{B}) = \\\\frac{1}{2}$ is the null hypothesis of the classical Bradley-Terry model. Without parametric assumptions $\\\\theta$. Although the BT model technically assumes a parametric form for the model win rates, the seminal results of Huber et al. (1967), White (1982) show that maximum likelihood estimators are still asymptotically normal even when these assumptions do not hold, so long as the so-called \u201csandwich\u201d estimator is invariant to addition in $\\\\xi$. $\\\\mathbf{\\\\xi} = \\\\mathbf{\\\\theta}$. $\\\\mathbf{\\\\xi}$ is the vector of Bradley-Terry (BT) coefficients (Bradley & Terry, 1952). In the Bradley-Terry model, $\\\\ell_p$ is the binary cross-entropy:\\n\\n$$ \\\\ell_p = -h_p + \\\\log(1 + \\\\frac{a}{\\\\xi}) = \\\\arg \\\\min \\\\ell_p $$\\n\\nwhere $h_p$ is the binary cross-entropy:\\n\\n$$ h_p = \\\\log(\\\\frac{1}{2}) $$\\n\\n$P(\\\\text{A} > \\\\text{B}) = 1 + \\\\frac{a}{\\\\xi}$.\"}"}
{"id": "3MW8GKNyzI", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\nOur online interface have reported different ranking scores, such as the Elo score (Elo, 1967) instead of the BT coefficients. We made this change because the BT coefficients are better for the purpose of statistical estimation.\\n\\n5. Efficient Approximate Ranking\\n\\nIn Section 4 we described how to calculate the win matrix, score, and rank. Now we describe our estimation procedures.\\n\\nWin matrix estimation. Estimation of the win matrix is relatively straightforward. Define $X_t(a) = \\\\frac{1}{\\\\sum_{a_t} h_t \\\\mathbb{1}\\\\{A_t = a\\\\}}$, where $p_t(a)$ is the probability of sampling pair $a$ at time $t$, and $X_t$ as the according vector. Then the estimator is $\\\\hat{\\\\theta}_T = \\\\frac{1}{T} \\\\sum_{t=1}^{T} X_t$.\\n\\n(4)\\n\\nNote that $E[X_t(a)] = \\\\theta^*_a$ for all $t$, and thus $\\\\hat{\\\\theta}_T$ is an unbiased estimator of $\\\\theta^*_a$. We will furthermore estimate the covariance matrix as $\\\\hat{\\\\Sigma}_T = \\\\frac{1}{T} \\\\sum_{t=1}^{T} (X_t - \\\\hat{\\\\theta}_T)(X_t - \\\\hat{\\\\theta}_T)^\\\\top$.\\n\\n(5)\\n\\nUnder the appropriate regularity conditions, we have that $\\\\sqrt{T} \\\\hat{\\\\Sigma}^{-\\\\frac{1}{2}}(\\\\hat{\\\\theta} - \\\\theta^*_a) \\\\rightarrow N(0, I_d)$,\\n\\n(6)\\n\\nand we construct confidence intervals accordingly. For an understanding of the appropriate regularity conditions, see Durrett (2019), Theorem 8.2.8, where condition (ii) is trivially satisfied so long as $p_t(a) > \\\\epsilon > 0$, and condition (i) is implied by the almost-sure convergence of $p_t(a)$ to a limiting distribution $p(a)$.\\n\\nEstimating the BT scores. To estimate the BT coefficients, mirroring (3), we perform (reweighted) maximum likelihood estimation on our data points:\\n\\n$s(\\\\hat{P}) = \\\\text{argmin} \\\\sum_{t=1}^{T} \\\\frac{1}{p(A_t)} \\\\log \\\\left( \\\\frac{1}{1 + e^{\\\\xi_{A_t}}} - \\\\frac{e^{\\\\xi_{A_t}}}{1 + e^{\\\\xi_{A_t}}} \\\\right)$.\\n\\n(7)\\n\\nwhere $A_t \\\\sim P$. We perform the inverse weighting by $p(A_t)$ because this allows us to target a score with a uniform distribution over $A$.\\n\\nTo compute confidence intervals on the BT coefficients, we employ two strategies: (1) the pivot bootstrap (DiCiccio & Efron, 1996), and (2) the \\\"sandwich\\\" robust standard errors outlined in Huber et al. (1967) (see also Freedman (2006) for an outline of the necessary technical assumptions). Ultimately, based on the results of a simulation study described in Appendix A, we choose to deploy the sandwich intervals due to their smaller size in large samples.\\n\\nApproximate rankings. Finally, we report an approximate ranking for each model that accounts for the uncertainty in the estimation of the score. Given an $M$-dimensional confidence set $C$ satisfying $P(s(P) \\\\in C) \\\\geq 1 - \\\\alpha$,\\n\\n(8)\\n\\nwe extract an approximate ranking $R_m = 1 + \\\\sum_{m'} \\\\mathbb{1}\\\\{m' \\\\in [M] \\\\cap \\\\inf_{m' \\\\in C} m' > \\\\sup_{m' \\\\in C} m'\\\\}$. The uniform validity of $C$ directly implies that $P(\\\\exists m: R_m > \\\\text{rank}(P)) \\\\leq \\\\alpha$\u2014i.e., with high probability, no model's performance is understated. A guarantee on the other side\u2014that no model's performance is overstated\u2014is possible by interchanging the inf and sup.\\n\\nTo get the uniform confidence set, we construct the chi-squared interval implied by the central limit theorem using the sandwich estimate of the variance. In other words, we construct the interval $\\\\{\\\\xi: T \\\\hat{V}^{-\\\\frac{1}{2}}(\\\\hat{\\\\xi} - \\\\xi) \\\\leq \\\\chi^2_{1 - \\\\alpha, M - 1}\\\\}$, where $\\\\hat{\\\\xi}$ is our MLE of the BT coefficients and $\\\\hat{V}$ is the sandwich variance of the logistic regression.\\n\\nActive sampling rule. Our sampling rule was to choose the model pair $a \\\\in A$ proportionally to the reduction in confidence interval size by sampling that pair:\\n\\n$p_t(a) \\\\propto s(\\\\hat{\\\\Sigma}_t,a,a)|\\\\{t: A_t = a\\\\} - s(\\\\hat{\\\\Sigma}_t,a,a)|\\\\{t: A_t = a\\\\}| + 1$.\\n\\n(9)\\n\\n5.1. Detecting Anomalous Users\\n\\nOn a different note, we take a first step towards identifying anomalous IP addresses in our dataset. In a dataset of $U$ unique IPs, we let $IP = \\\\{1, \\\\ldots, U\\\\}$ be the set of all IP addresses. Consider a \\\"test\\\" user, outside this database, who gives ratings $H'_1, \\\\ldots, H'_n$ when presented actions $A'_1, \\\\ldots, A'_n$. The idea of our procedure is to compare the distribution of ratings for the new user to the historical distribution of ratings for a given action. We let $H_a = \\\\{H_t: A_t = a\\\\}$ and every time a user submits a vote, we calculate the following number:\\n\\n$p_i = \\\\frac{1}{|H_{A'_i}| + 1} \\\\left( 1 + \\\\sum_{h \\\\in H_{A'_i}} \\\\mathbb{1}\\\\{h \\\\geq H'_i\\\\} \\\\right)$.\\n\\n(10)\\n\\nUnder the null hypothesis that $H_{A'_i}$ is exchangeable with $H'_i$, $p_i$ is a valid p-value (see Appendix C for a proof). Furthermore, the dependence of these p-values asymptotically is negligible.\\n\\nWith this p-value in hand, we can test against this null hypothesis sequentially by using Fisher's combination test (Fisher, 1928) along with a variant of the Bonferroni correction. In particular, for each user, after their $j$th vote, we compute $M_j = -2 \\\\sum_{i=1}^{j} \\\\log(p_i)$. At 5 randomly chosen values of $j$ between 1 and 100, we identify a user as...\"}"}
{"id": "3MW8GKNyzI", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\\n\\n\\\\[ \\\\text{anomalous if } M_j \\\\geq \\\\chi_2^2 \\\\frac{1}{1 - \\\\alpha/5} \\\\]\\n\\nThe times are randomly chosen, as to avoid anomalous users strategizing to hack this p-value. Despite the heuristic application of this procedure, it seems to work well in our small-scale tests reported in Table 5.\\n\\n6. Data Analysis\\n\\nTo examine whether Arena's crowdsourced data reflects real-world use cases, we conduct topic modeling on the user prompts. We show how effective are these prompts in distinguishing models. Lastly, we validate the vote quality by relabeling data with experts.\\n\\n6.1. Topic Modeling on User Prompts\\n\\nTo study the prompt diversity, we build a topic modeling pipeline with BERTopic\\\\(^3\\\\) (Grootendorst, 2022). We start with transforming user prompts into representation vectors using OpenAI's text embedding model (text-embedding-3-small). To mitigate the curse of dimensionality for data clustering, we employ UMAP (Uniform Manifold Approximation and Projection) (McInnes et al., 2020) to reduce the embedding dimension from 1,536 to 5. We then use the hierarchical density-based clustering algorithm, HDBSCAN, to identify topic clusters with minimum cluster size 32. Finally, to obtain topic labels, we sample 10 prompts from each topic cluster and feed into GPT-4-Turbo for topic summarization.\\n\\nThe pipeline identifies 600 clusters covering a wide range of topics including poetry writing, coding, math, and medical queries. We present the top-16 topic clusters in Figure 3.\\n\\nWe observe that the largest cluster only accounts for 1% of the entire set and the rest quickly drop to <0.5%, and the similarity between clusters is small, showing a long-tail and diverse distribution. Due to space limit, we present the similarity matrix and cluster hierarchy of top-64 clusters in Figure 11 and 12 in Appendix.\\n\\n6.2. Can Arena Prompts Distinguish Models?\\n\\nNext, we study how effective are these topic clusters in distinguishing models strengths. Constructing challenging prompts has become increasingly difficult due to LLMs' fast growing capabilities. For example, open models such as Llama-2-70b-chat can likely answer inquiries about movie or travel recommendation as good as GPT-4, but not in other domains such as reasoning or coding. To demonstrate, we sample 30 prompts from seven topic clusters and compare the performance of Llama-2-70b-chat and GPT-4.\\n\\nTo control variables, we factor out user votes and consider LLM-as-judge (Zheng et al., 2023b) to evaluate model response. Results are shown in Table 2, where we see GPT-4 has significantly higher win-rate (up to 97%) in clusters that require coding and reasoning skills. On the other hand, for clusters with less problem-solving tasks, GPT-4 win-rate drops to below 60%. We show examples in Appendix D.1.\\n\\nThis result shows models may exhibit varying strengths in different areas, but also highlights some of the topic clusters in Chatbot Arena are effective in differentiate models.\\n\\n6.3. Validating Vote Quality\\n\\nTo assess the quality of crowdsourced votes, we randomly selected 160 battles between GPT-4-Turbo and Llama-2-13B, as well as GPT-4-Turbo and GPT-3.5-Turbo-0613. We\\n\\n\\\\[ 0.3 \\\\quad 0.4 \\\\quad 0.5 \\\\quad 0.6 \\\\quad 0.7 \\\\quad 0.8 \\\\quad 0.9 \\\\quad 1 \\\\]\\n\\nSimilarity\\n\\nFigure 3. Similarity matrix of top-16 topic clusters. The number followed by the topic label represents the cluster size in percentage. Note that similarity is computed by cluster's centroid embeddings, hence diagonals are always one.\\n\\nTable 2. GPT-4-0613's win-rate against Llama-2-70b-chat on 30 sample prompts from various topic clusters. We use GPT-4-turbo as judge to evaluate model responses in pairwise comparison.\\n\\n| Topic Cluster                      | Win-rate | Size   |\\n|-----------------------------------|----------|--------|\\n| Python Game Programming Challenge | 96.7%    | 0.2%   |\\n| C/C++ Process Multi-Threading     | 86.7%    | 0.3%   |\\n| SQL Query Database Assistance     | 73.3%    | 0.2%   |\\n| Poetry Writing Prompts            | 66.7%    | 1.1%   |\\n| Python Coding Basics              | 65.0%    | 0.2%   |\\n| Linguistic Analysis & Wordplay    | 58.3%    | 0.7%   |\\n| Travel Itinerary Planning         | 58.3%    | 0.4%   |\\n| Movie Recommendations & Ratings   | 53.3%    | 0.2%   |\\n\\nThis result shows models may exhibit varying strengths in different areas, but also highlights some of the topic clusters in Chatbot Arena are effective in differentiate models.\\n\\nBuilding Challenging Benchmark.\\n\\nTo further demonstrate the prompt quality, we show it is possible to construct a challenging benchmark with crowd-sourced user prompts. To ensure both topic coverage and quality, we first run the topic modeling pipeline and follow a similar procedure in Zheng et al. (2023a) to select challenging questions sampled from each topic cluster. Examples prompts and evaluation procedures can be found in the Appendix D.2 and Appendix D.3, respectively. We observe the selected prompts are highly effective in differentiating models. In Figure 4, we compare Arena bench against a widely used LLM benchmark, MT-Bench (Zheng et al., 2023b). We can see that Arena Bench effectively reveals a significant gap in performance between proprietary and the strongest open models.\\n\\n6.3. Validating Vote Quality\\n\\nTo assess the quality of crowdsourced votes, we randomly selected 160 battles between GPT-4-Turbo and Llama-2-13B, as well as GPT-4-Turbo and GPT-3.5-Turbo-0613. We\\n\\n\\\\[ 0.3 \\\\quad 0.4 \\\\quad 0.5 \\\\quad 0.6 \\\\quad 0.7 \\\\quad 0.8 \\\\quad 0.9 \\\\quad 1 \\\\]\\n\\nSimilarity\\n\\nFigure 3. Similarity matrix of top-16 topic clusters. The number followed by the topic label represents the cluster size in percentage. Note that similarity is computed by cluster's centroid embeddings, hence diagonals are always one.\\n\\nTable 2. GPT-4-0613's win-rate against Llama-2-70b-chat on 30 sample prompts from various topic clusters. We use GPT-4-turbo as judge to evaluate model responses in pairwise comparison.\\n\\n| Topic Cluster                      | Win-rate | Size   |\\n|-----------------------------------|----------|--------|\\n| Python Game Programming Challenge | 96.7%    | 0.2%   |\\n| C/C++ Process Multi-Threading     | 86.7%    | 0.3%   |\\n| SQL Query Database Assistance     | 73.3%    | 0.2%   |\\n| Poetry Writing Prompts            | 66.7%    | 1.1%   |\\n| Python Coding Basics              | 65.0%    | 0.2%   |\\n| Linguistic Analysis & Wordplay    | 58.3%    | 0.7%   |\\n| Travel Itinerary Planning         | 58.3%    | 0.4%   |\\n| Movie Recommendations & Ratings   | 53.3%    | 0.2%   |\\n\\nThis result shows models may exhibit varying strengths in different areas, but also highlights some of the topic clusters in Chatbot Arena are effective in differentiate models.\\n\\nBuilding Challenging Benchmark.\\n\\nTo further demonstrate the prompt quality, we show it is possible to construct a challenging benchmark with crowd-sourced user prompts. To ensure both topic coverage and quality, we first run the topic modeling pipeline and follow a similar procedure in Zheng et al. (2023a) to select challenging questions sampled from each topic cluster. Examples prompts and evaluation procedures can be found in the Appendix D.2 and Appendix D.3, respectively. We observe the selected prompts are highly effective in differentiating models. In Figure 4, we compare Arena bench against a widely used LLM benchmark, MT-Bench (Zheng et al., 2023b). We can see that Arena Bench effectively reveals a significant gap in performance between proprietary and the strongest open models.\"}"}
