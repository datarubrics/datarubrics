{"id": "lai23b", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Detailed statistics of DS-1000.\\n\\n| Library            | Count | Avg. Problem Words | Avg. Lines of Code Context | Avg. Lines of Code Solution |\\n|--------------------|-------|--------------------|---------------------------|-----------------------------|\\n| Pandas             | 291   | 184.8              | 9.0                       | 5.4                         |\\n| NumPy              | 220   | 137.5              | 8.3                       | 2.5                         |\\n| Matplotlib         | 155   | 21.1               | 6.9                       | 3.0                         |\\n| Scikit-learn       | 115   | 147.3              | 11.0                      | 3.3                         |\\n| SciPy              | 106   |                    |                           |                             |\\n| TensorFlow         | 45    |                    |                           |                             |\\n| PyTorch            | 68    |                    |                           |                             |\\n\\n| Origin            | 100   | 140.0              | 8.9                       | 3.6                         |\\n| Surface Perturbation | 24   |                    |                           |                             |\\n| Semantic Perturbation | 88   |                    |                           |                             |\\n| Difficult Rewrite | 79    |                    |                           |                             |\\n\\nMore than in 36% of the cases, the model still predicted the original answer of the problem after the semantic perturbation, implying that the model is solving the original problems by memorizing their corresponding solutions. Therefore, we could significantly overestimate model performance if we test them on problems directly taken from the web. (See Appendix B for more details)\\n\\nTherefore, to proactively prevent memorization, we applied the above two perturbations to DS-1000 problems. Perturbation is a labor-intensive process. Even for a simple perturbation from min to max, our annotators needed to edit all mentions of min, smallest, minimum to make the problem coherent, and updated the code context, reference solution, and our evaluation metric accordingly.\\n\\nFinally, to make DS-1000 more challenging, we additionally introduced several semantic perturbations that increase the difficulty on purpose (\u201cDifficult Rewrite\u201d in Table 1).\\n\\n2.5. Quality Assurance\\n\\nTo ensure the quality of our benchmark, each problem, reference solution, and automatic multi-criteria evaluation were reviewed by at least three expert annotators familiar with the library. Additionally, we \u201cred teamed\u201d our automatic evaluation by requiring it to reject all programs known to be incorrect, e.g., solutions to semantically perturbed problems (see Figure 2). After the quality review, we also quantitatively measured the evaluation quality by examining whether our multi-criteria automatic metric can reject incorrect Codex-002 predictions (more details in Section 3).\\n\\n3. Dataset Statistics\\n\\nWe provide detailed dataset statistics in Table 3. DS-1000 contains 1000 problems originating from 451 unique StackOverflow problems. To defend against potential memorization, more than half of the DS-1000 problems are modified from the original StackOverflow problems (Section 2.4); they include 152 surface perturbations, 235 semantic perturbations, and 162 difficult rewrites.\\n\\nDS-1000 has carefully designed testing methods, checking both execution semantics and surface-form constraints. For each problem, there are 1.6 test cases (manually annotated corner test cases) on average, and 19.4% of them are accompanied by surface-form constraints. The average of problem words in DS-1000 is 140. On average, the reference solution contains 3.6 lines of code. Table 3 shows the library breakdown statistics: Most libraries have a similar distribution except Matplotlib because we adopted a different problem format due to its multimodal nature.\\n\\nTable 4 compares DS-1000 to other datasets. Notably, the average number of words per problem in DS-1000 is much larger than other data science related datasets (e.g., DSP, Chandel et al. 2022 and CoNaLa, Yin et al. 2018). More importantly, the problems in DS-1000 represent more diverse and naturalistic intent and context formats that cannot be seen in any other datasets. Unlike generic Python code generation benchmarks (MBPP, Austin et al. 2021 and HumanEval, Chen et al. 2021a), we note that data science code generation benchmarks have fewer test cases since the annotators need to define program inputs with complex objects such as square matrices, classifiers, or dataframes rather than simple primitives, such as floats or lists. Nevertheless, as we will show next, even a few test cases suffice for DS-1000.\\n\\nWe evaluate our multi-criteria automatic metric by checking whether it can reject incorrect solutions. We randomly sampled 10 problems from each library and sampled 40 predictions from Codex-002 for each problem (2800 problem-code examples in total).\\n\\nWe run our automatic metric on all the sample predictions, review the predictions manually, 3 We use a higher temperature of 0.7 compared with 0.2 in Section 4.2 to get more diverse predictions.\"}"}
{"id": "lai23b", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\\n\\nTable 4: Comparison of DS-1000 to other benchmarks. The first three benchmarks target general Python usage and the next three involve data science code generation. DS-1000 adapts realistic problems from StackOverflow and checks both execution semantics and surface-form constraints.\\n\\n| Dataset     | Problems Evaluation | Avg. Test Cases | Avg. P Words | Avg. Lines of Code | Solution Data Source |\\n|-------------|---------------------|-----------------|--------------|--------------------|----------------------|\\n| HumanEval   | 164                 | 7.7             | 23.0         | 6.3                | Hand-Written         |\\n| MBPP        | 974                 | 3.0             | 15.7         | 6.7                | Hand-Written         |\\n| APPS        | 10000               | 13.2            | 293.2        | 18.0               | Competitions         |\\n| JuICe       | 1981                | Exact Match + BLEU | 57.2 | 3.3      | Notebooks            |\\n| DSP         | 1119                | 2.1             | 71.9         | 4.5                | Notebooks            |\\n| CoNaLa      | 2879                | BLEU -         | 13.8         | 1.1                | StackOverflow        |\\n| DS-1000     | 1000               | + Surface-Form Constraints | 1.6 | 140.0   | StackOverflow        |\\n\\ncalculate how often they disagree, and report the following four quantities:\\n\\n\u2022 Sample Level False Discovery Rate: among all predicted samples that pass our automatic evaluation, 1.8% of them are incorrect according to our annotator.\\n\u2022 Sample Level False Omission Rate: among all predicted samples that do not pass our automatic evaluation, 0.5% of them are correct according to our annotator.\\n\u2022 Problem Level False Positive Percentage: among all problems, 5.7% of the problems contain at least one incorrect sample prediction that passes our automatic metric.\\n\u2022 Problem Level False Negative Percentage: among all problems, 5.7% (it happens to be the same as the above) problems contain at least one correct sample prediction that fails to pass our automatic metric.\\n\\nGenerally, problem-level measures are especially stringent since they require correctly judging all predictions among the 40 sample predictions. While an apple-to-apple comparison with other datasets is not possible due to the difference in the underlying model and benchmark construction method (as a point of reference, Li et al. (2022) find the problem Level False Positive Percentage to be 60% on APPS (Hendrycks et al., 2021)), these measures reflect that DS-1000 is reliable.\\n\\n4. Benchmarking State-of-the-Art Models\\n\\nWe used DS-1000 to benchmark five pre-trained code models from three different families. The best model Codex-002 achieves 43.3% accuracy, indicating room for improvement. We also show the results on the perturbed and unperturbed examples in Section 4.4.\\n\\n4.1. Prompt Format\\n\\nWe provide an official prompt format in DS-1000 because it significantly impacts the performance of pre-trained language models (Zhao et al., 2021). Figure 1 shows an example: each prompt starts with a natural language description and then provides a code context; the code context uses HTML-like markers to indicate the location of missing code that a model needs to fill in and provides both left and the right context to the missing code pieces.\\n\\nWe decide to use infilling as our official format because the right context is important to specify the behavior of the program predictions (e.g., the variable name for the result). More broadly, given that 1) infilling is an important functionality for real-world programming and 2) there is a growing trend in pre-training with the right context (Aghajanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022; Tay et al., 2022), we expect more future pre-trained models to perform infilling.\\n\\nOn the other hand, given that many current language models trained on code are not yet capable of infilling, we also provide an official prompt that transfers the right context information into the left context (Figure 2). Nevertheless, despite our best effort to design the prompts for left-to-right models, they still lag behind models with infilling capabilities (Section 4.3). We conjecture that infilling models are inherently more effective at utilizing the right context information. Finally, we only have Completion format for Matplotlib problems because Matplotlib provides global access to the current figure so the right context is not necessary.\\n\\nFrom now on, we refer to the infilling prompt format as Insertion format and the left-context-only format as Completion format.\"}"}
{"id": "lai23b", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right Completion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the average accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models and there is substantial room for improvement even for the best Codex-002 model.\\n\\n| Format       | Model         | Overall |\\n|--------------|---------------|---------|\\n| **Left-to-right** |               |         |\\n| Completion   | Codex-002     | 26.5    |\\n|              | Codex-001     | 9.4     |\\n|              | Codex-Cushman | 7.9     |\\n|              | Codex-6B      | 1.9     |\\n|              | InCoder-6B    | 3.1     |\\n| Insertion    | Codex-002     | 30.1    |\\n|              | InCoder-6B    | 2.9     |\\n\\n4.2. Experimental Setup\\n\\nModels. We experiment with three families of pre-trained models: Codex, InCoder (Fried et al., 2022), and CodeGen (Nijkamp et al., 2022). For the Codex models, we experiment with codex-davinci-002 (Codex-002), codex-davinci-001 (Codex-001), and codex-cushman-001 (Codex-Cushman). For InCoder and CodeGen, we experiment with the 6B parameters models. Among them, Codex and CodeGen models are trained to predict the right context while InCoder models are trained for both left-to-right generation and infilling. In addition, Codex-002 also supports infilling, although the exact model training details are not disclosed.\\n\\nImplementation Details. We generate 40 samples for each DS-1000 problem with temperature set to 0.2, top-p cutoff set to 0.95, and max generation length set to 1024. We set the stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION END\u201d. These samples are used in the unbiased estimator of pass@1. For DS-1000, evaluating generated codes does not require special computational resources like GPUs.\\n\\n4.3. Main Results\\n\\nTable 5 displays the pass@1 accuracy on DS-1000. We find that DS-1000 can differentiate models with different capabilities. The best model Codex-002 achieves a nontrivial but far-from-perfect average accuracy of 43.3%, indicating substantial room for improvement. In contrast, other models like CodeGen-6B or InCoder-6B have much worse overall performance, with accuracy lower than 5% on some libraries. Qualitatively, these smaller models often cannot correctly follow the prompt instruction, generating additional comments instead of the required code. Future ablation is needed to understand the underlying cause for this performance gap, which could be the difference in model size, lack of instruction tuning, or the difference in pre-training data.\\n\\nIn addition, we observe that model accuracy varies across different libraries. This speaks to the importance of a holistic evaluation of multiple data science libraries because performance in a specific library may not directly generalize to other libraries.\\n\\nMoreover, we find that Insertion format often leads to better performance. The same Codex-002 model has a 4.1% average accuracy improvement when used with Insertion format than used with Completion format. This shows the importance of the infilling capability for data science code completion.\\n\\n4.4. Results by Perturbation\\n\\nIn Section 2.4, we demonstrated the risk of memorizing the solutions on the numpy-100 problem set; do we observe the same effect on DS-1000? To investigate this, we applied surface perturbations (i.e., the problem changes but the reference solution does not change) and semantic perturbations (the reference solution will change) to the problems in DS-1000.\\n\\nTable 6 shows the results. The performance of Codex-002 drops after perturbation (3.4% on surface perturbations and 9.0% on semantic perturbations) but the drop is much less severe than what we observed on numpy-100. This indirectly suggests that Codex-002 might have memorized the solution for some StackOverflow problems, but the effect is less severe because they have not been repeated as often as numpy-100 on the internet. Still, we believe problem perturbation to be a useful strategy to defend against memorization.\\n\\n5 The performance of Codex-002 drops after perturbation (3.4% on surface perturbations and 9.0% on semantic perturbations) but the drop is much less severe than what we observed on numpy-100. This indirectly suggests that Codex-002 might have memorized the solution for some StackOverflow problems, but the effect is less severe because they have not been repeated as often as numpy-100 on the internet. Still, we believe problem perturbation to be a useful strategy to defend against memorization.\\n\\nNote that the results are not comparable to Table 5 since for each kind of perturbation, we only selected a subset of problems to perturb.\"}"}
{"id": "lai23b", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the perturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also cause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100.\\n\\n|                  | Origin | Surface | Semantic | Difficult |\\n|------------------|--------|---------|----------|-----------|\\n| Overall          | 49.8   | \u22123.4    | 58.0     | 38.2      |\\n| Origin           | 37.3   | 53.2    | 36.8     | 49.8      |\\n| NumPy            | 61.2   | \u22125.4    | 40.3     | 47.2      |\\n| Scikit-learn     | 52.6   | \u22122.8    | 56.7     | 45.8      |\\n| SciPy            | 33.0   | \u22120.9    | 40.3     | 38.2      |\\n| TensorFlow       | 64.9   | \u221235.0   | 71.3     | 66.0      |\\n| PyTorch          | 64.8   | \u22123.4    | 65.1     | 64.8      |\\n\\nWe give a preliminary error analysis in Appendix C.\\n\\n5. Related Work\\n\\nNatural Language to Code. Research on translating natural language to executable forms dates back several decades. The models have become increasingly capable of producing complex and general programs while requiring fewer human annotations. Zelle & Mooney (1996) and Zettlemoyer & Collins (2007) translate natural language queries to domain-specific database queries. Liang et al. (2013) and Berant et al. (2013) parse natural language into first-order logic to answer generic knowledge-based questions. Yu et al. (2018); Scholak et al. (2021) translate natural language problems to general SQL programs and develop models that can generalize across domains. While all the works above still need to train their models on the task they evaluate, recently Li et al. (2022); Chen et al. (2021a) show that generative models pre-trained on code can produce Python snippets to tackle competitive programming challenges, without any additional human annotations. Many other recent works corroborated this finding (Nijkamp et al., 2022; Fried et al., 2022; Xu et al., 2022; Black et al., 2022), and additional techniques at inference time further improve the performance (Poesia et al., 2022; Shi et al., 2022).\\n\\nCode Generation Benchmarks. As models become increasingly capable, researchers start to build increasingly difficult and general code generation benchmarks. While Zelle & Mooney (1996) focused only on domain-specific languages, Yu et al. (2018) builds a Text-to-SQL benchmark that evaluates the capability to write broad-domain SQL programs. Yin et al. (2018) evaluates the capability to write short but general Python snippets, while more recent papers Hendrycks et al. (2021); Li et al. (2022) evaluate models' capability to solve competitive programming problems in Python. If code generation models continue to improve, we expect future researchers to focus on more complex tasks. At the same time, however, it becomes more difficult to build reliable benchmarks aligned with real-world applications. Programs are most useful when they are executed; therefore, we need to evaluate their execution semantics, and the best general method so far is still to ask experts to manually write test cases. Consequently, most benchmarks with test cases focus on competition/interview/programming challenges (Hendrycks et al., 2021; Li et al., 2022), because these are the only applications where a lot of test cases are already available. Therefore, most recent papers that evaluate on real-world programs have to rely on unreliable surface-form metrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022), or similarity scores (Zhou et al., 2023) calculated by code representations (Wang et al., 2022). This streetlight effect might incentivize the community to work on problems that are easy to evaluate but not useful in practice. In response to this challenge, our paper manually implements a reliable metric for naturally occurring problems. Future works can consider using models to help humans write useful tests (Tufano et al., 2020), or formally verify the correctness of a predicted solution (Chu et al., 2017).\\n\\n6. Conclusion\\n\\nWe propose DS-1000, a benchmark for generating code for data analysis. Our benchmark 1) contains realistic problems, 2) implements reliable automatic metrics, and 3) proactively defends against memorization strategies. We hope DS-1000 can track the progress of this research area and facilitate fair comparisons between models, and our methods to construct it can inspire other areas where the task is complicated and the ground truth is challenging to evaluate.\"}"}
{"id": "lai23b", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) \u2013 across all Codex-002-predicted solutions that our evaluation accepts, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API uses or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.\\n\\n1. Introduction\\n\\nData science is important in many areas (Romero & Ventura, 2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but requires programming proficiency in specialized libraries, thus posing substantial barriers to lay users. Fortunately, these barriers could potentially be reduced by pre-trained code generation models: for example, Codex (Chen et al., 2021a) can complete small Python snippets with non-trivial accuracy and AlphaCode (Li et al., 2022) can tackle difficult competitive programming problems. We anticipate that these barriers will diminish if the community can make solid progress in applying these models to data science problems. However, we currently lack a benchmark that 1) focuses on everyday data science applications, 2) includes naturalistic intents and contexts, and 3) has a reliable execution-based evaluation metric. Most of the existing datasets with reliable test cases (Hendrycks et al., 2021; Chen et al., 2021a) focus on competition or interview-style programming problems; they measure algorithmic understanding but do not target real-world usage. Also, as represented by e.g., user problems on StackOverflow, users' data science coding problems usually have diverse contexts including their incorrect code, error messages, and input-output examples, which cannot be found in most prior data science relevant code generation benchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chandel et al., 2022; Chen et al., 2021a). Moreover, most of these benchmarks solely rely on surface-form metrics such as BLEU or CodeBLEU (Yin et al., 2018; Agashe et al., 2019; Chen et al., 2021b). These metrics diverge from the programmer's intent, increasingly so as model capability improves (Zhong et al., 2020). To our knowledge, no existing benchmarks contain both naturally occurring problems with diverse contexts and reliable evaluation metrics.\\n\\nTo fill this gap, we introduce DS-1000, a benchmark with a thousand problems covering seven widely-used Python data science libraries: NumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, and Matplotlib. We highlight three core features of DS-1000: 1) it contains realistic problems with diverse contexts, 2) it implements reliable multi-criteria execution-based evaluation metrics, and 3) it proactively defends against memorization. We outline how we achieved each of them below.\\n\\nFirst, we collected naturally occurring problems from StackOverflow, manually scored their representativeness and usefulness, and curated a subset of them to create our benchmark. While inputs in existing code generation datasets are either highly structured (problems or code context) or restricted in scope, our natural problems are diverse in content and format. For example, users might search for more efficient code implementations (Figure 1), provide incorrect code with an error message and ask for bug fixes (Figure 13), inquire about specific API usage (Figure 14), or ask for code improvements (Figure 15).\"}"}
{"id": "lai23b", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here is a sample dataframe:\\n\\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\\n\\nI'd like to add inverses of each existing column to the dataframe and name them based on existing column names with a prefix, e.g. inv_A is an inverse of column A and so on.\\n\\nThe resulting dataframe should look like so:\\n\\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\\n\\nObviously there are redundant methods like doing this in a loop, but there should exist much more pythonic ways of doing it \u2026\"}"}
{"id": "lai23b", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\\n\\n```\\ninv_df = df.join(df.apply(\\n    lambda x: 1/x).add_prefix(\\n    \\\"inv_\\\"))\\n```\\n\\n```\\ndf = pd.DataFrame(\\n    {\\n        \\\"A\\\": [1, 2, 3],\\n        \\\"B\\\": [4, 5, 6],\\n    })\\n```\\n\\n### BEGIN SOLUTION\\n### A known WRONG SOLUTION\\n```\\nresult = df.join(df.apply(math.e ** x).add_prefix('exp_'))\\n```\\n\\n### END SOLUTION\\n\\n```\\nprint(result)\\n```\\n\\nI'd like to apply the exponential function to each existing column\u2026 The resulting dataframe should look like so:\\n\\n```\\ndf = pd.DataFrame(\\n    {\\n        \\\"A\\\": [1, 2, 3],\\n        \\\"B\\\": [4, 5, 6],\\n        \\\"exp_A\\\": [e^1, e^2, e^3],\\n        \\\"exp_B\\\": [e^4, e^5, e^6],\\n    })\\n```\\n\\nTest cases\u2026\\n\\n```\\nimport pandas as pd\\ndf = pd.DataFrame(\\n    {\\n        \\\"A\\\": [1, 2, 3],\\n        \\\"B\\\": [4, 5, 6],\\n    })\\n```\\n\\n### BEGIN SOLUTION\\n### [insert]\\n### END SOLUTION\\n\\n```\\nprint(result)\\n```\\n\\n2.1. Problem Selection\\n\\nSourcing Popular StackOverflow Problems. To obtain natural and high-quality problems, we scraped data from StackOverflow under each library tag (e.g., \\\"NumPy\\\"). To select popular problems, we first removed duplicates and selected problems with at least 1 vote, 1000 views, that had an accepted answer. Next, we ranked problems based on votes and views and calibrated these statistics based on the time a problem was created since older problems naturally have more views and votes. We refer readers to Appendix A.1 for more details. Among the filtered problems, we randomly sampled an initial pool containing 4500 problems (1000 for NumPy, Pandas, and Matplotlib, 500 for Scikit-learn and SciPy, 250 for TensorFlow, and 250 for PyTorch).\\n\\nFiltering Suitable Problems. To select problems from the above pool for our benchmark, our annotators scored each problem according to the following rubric: whether a problem a) contains input-output examples in the problem, b) is difficult to predict the solution for models according to the annotators' judgment, c) is practically useful, d) has a clear description, and e) is feasible to evaluate the solution. We aggregated these scores, reranked the candidate problems, and incorporated the top-ranked ones to create DS-1000. We ended up with 451 unique StackOverflow problems. More than half of the original StackOverflow problems were filtered out because they ask for an explanation for an algorithm or general content (see Appendix A.1).\\n\\nControlling Library Version. Data science libraries are continuously evolving. As a result, the semantics of the problem is determined not only by the language description but also by the software environment (e.g., library version). For example, the same code snippet, `tf.math.reciprocal(A)`, is only valid in the newer version of TensorFlow. We fixed the evaluation environment to include the latest versions of libraries that can be installed with Python 3.7.10 and present the detailed documentation in Appendix A.1.\\n\\n2.2. Rewriting Problems and Reference Solutions\\n\\nCreating Executable Context. To implement an execution-based evaluation for each natural language problem, we needed to write an executable context. We first added package imports and defined the variables described in the problem. For example, in Figure 2, we imported the Pandas package and created the dataframe described in the problem as part of the context. Second, we needed to specify the desired behavior of the target program to be predicted. For example, in Figure 2, a code generation model can infer from the context that the resulting dataframe should be named as `result`, rather than `output`.\\n\\nRewriting Matplotlib Problems. Many Matplotlib problems on StackOverflow clarify their problems with example figures, which, however, cannot be encoded by current pre-trained code models. Therefore, we rewrote the StackOverflow problems in symbols (i.e., code and text) and adopted a different format from other libraries (see Figure 15).\\n\\nCollecting Reference Solutions. Finally, we obtained the reference solution for each problem from multiple high-vote replies, edited all reference solutions to be executable given the context we provided, and fixed errors whenever we noticed them (e.g., Figure 11). Even though we did not use the reference solution in DS-1000 for evaluation, we provided them in DS-1000 to facilitate future research.\\n\\n3\"}"}
{"id": "lai23b", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution, while \u201cSemantic\u201d perturbations do.\\n\\n| Perturbation Categories | Example                                                                 |\\n|------------------------|-------------------------------------------------------------------------|\\n| Surface                | Convert to completing function                                           |\\n|                        | Figure 16, change format of code context                                 |\\n|                        | Paraphrase the description of the problem                               |\\n|                        | Figure 17, express the same problem in different words                  |\\n|                        | Change the example input and output                                      |\\n|                        | Figure 18, replace this example with a longer one                        |\\n| Semantic               | Replace keywords with analogy words                                      |\\n|                        | Figure 19, replace \u201cinv\u201d with \u201cexp\u201d                                      |\\n|                        | Change the required index                                                |\\n|                        | Figure 20, need the specified rows and columns                           |\\n|                        | Reverse the order of the list, string or dataframe                       |\\n|                        | Figure 21, reverse the needed string                                     |\\n|                        | Change the type of the required result                                   |\\n|                        | Figure 22, change the DataFrame to a Series                              |\\n\\nDifficult Rewrite\\n\\nCombining several surface and semantic perturbations\\n\\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\\n\\nDigging more perturbations that increase the difficulty\\n\\nFigure 24, hypothesis testing\\n\\n2.3. Implementing Multi-Criteria Evaluations\\n\\nOur automatic evaluation is multi-criteria, checking both functional correctness and surface-form constraints.\\n\\nFunctional Correctness.\\n\\nTo evaluate functional correctness, we constructed test cases by converting the input-output examples provided in the StackOverflow problem; then the expert annotators manually wrote additional test cases to improve the evaluation. To evaluate a predicted program, we execute it on the test inputs and compare the outputs with the ground truth.\\n\\nHowever, checking the exact equivalence of outputs can inadvertently reject correct programs. Many problems involve floating point arithmetic, and many return values are acceptable since they are close to the ground truth answer, but they are not exactly equal. Some problems require random outputs, e.g., generating 100 samples from a distribution, and even executing the reference solution twice can lead to different results. Many problems do not fully specify all the parameters, e.g., the color scheme for the output figure in the Matplotlib library, or the hyper-parameters of a learning algorithm in Scikit-learn; therefore, programs with different parameters can satisfy the requirement, returning values that are different. In all these cases, we relied on the best judgment of our expert annotators to implement the metric for each problem, which sometimes involves complicated techniques, such as using statistical tests to handle randomness. See more examples in Appendix A.2.\\n\\nSurface-Form Constraints.\\n\\nFunctional correctness alone is insufficient. For example, vectorized operations can be expanded using for-loops, which, however, are inefficient and do not meet the requirement of the problem. Therefore, we introduced additional surface-form constraints that require the presence/absence of specific APIs for keywords. Notably, such a check is different from the standard surface-form metrics such as CodeBLEU (Ren et al., 2020), which requires the whole model prediction to be uniformly similar to a reference solution; instead, DS-1000 precisely targets small but important parts of surface form.\\n\\n2.4. Perturbation to Defend Against Memorization\\n\\nMany models are pre-trained on web text and hence memorize its content (Elangovan et al., 2021; Carlini et al., 2021); therefore, they might answer our problems correctly by simply recalling the solutions seen during pre-training if they were trained on StackOverflow or derivative sites. We demonstrate this effect on numpy-100, a problem set of 100 NumPy problems with solutions that are copied several thousand times on GitHub. When prompted to answer a selected subset of 20 problems, Codex-002 achieves 72.5% pass@1 accuracy.\\n\\nHowever, if the model truly knows how to solve those problems, it should be able to solve similar problems at the same level of difficulty. This motivates us to perturb the problems in two ways: surface perturbations and semantic perturbations. For surface perturbations, we paraphrased the problem or modified the code context in the problem, but the reference solution should stay the same after the perturbation; for example, changing from \u201cCreate a 5x5 matrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic perturbations, we changed the semantics of the reference solution without changing its difficulty; for example, asking for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide more detailed categories in Table 1. In all of these cases, the difficulty of the problem does not change for humans.\\n\\nTable 2: The performance of Codex-002 on numpy-100.\\n\\n| Origin | Surface | Semantic | Avg. Perturbation |\\n|--------|---------|----------|-------------------|\\n|        | 72.5    | 50.8     | 23.6              |\\n\\nWe manually applied these perturbations to numpy-100 and show the result on Table 2. Although the difficulty level remains the same to human users, the performance of Codex-002 drops to 40.6% after perturbation (50.8% on surface perturbations and 23.6% on semantic perturbations). Further...\"}"}
{"id": "lai23b", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for their helpful feedback on this work.\\n\\nReferences\\n\\nAgashe, R., Iyer, S., and Zettlemoyer, L. JuICe: A large scale distantly supervised dataset for open domain context-based code generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5436\u20135446, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1546. URL https://aclanthology.org/D19-1546.\\n\\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V ., Xu, H., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis, M., et al. Cm3: A causal masked multimodal model of the internet. arXiv preprint arXiv:2201.07520, 2022.\\n\\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n\\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey, C., Tworek, J., and Chen, M. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022.\\n\\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1533\u20131544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1160.\\n\\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L., Tow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An open-source autoregressive language model. In Proceedings of BigScience Episode #5 \u2013 Workshop on Challenges & Perspectives in Creating Large Language Models, pp. 95\u2013136, virtual+Dublin, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9.\\n\\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A., Abnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J., Arumugam, M., Asnicar, F., et al. Reproducible, interactive, scalable and extensible microbiome data science using qiime 2. Nature biotechnology, 37(8):852\u2013857, 2019.\\n\\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T. B., Song, D., Erlingsson, \u00da., Oprea, A., and Raffel, C. Extracting training data from large language models. In Bailey, M. and Greenstadt, R. (eds.), 30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021, pp. 2633\u20132650. USENIX Association, 2021. URL https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting.\\n\\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N. Training and evaluating a jupyter notebook data science assistant. arXiv preprint arXiv:2201.12901, 2022.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021a.\\n\\nChen, X., Gong, L., Cheung, A., and Song, D. PlotCoder: Hierarchical decoding for synthesizing visualization code in programmatic context. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 2169\u20132181, Online, August 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.169. URL https://aclanthology.org/2021.acl-long.169.\\n\\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An automated prover for SQL. In 8th Biennial Conference on Innovative Data Systems Research, CIDR 2017, Chaminade, CA, USA, January 8-11, 2017, Online Proceedings. www.cidrdb.org, 2017. URL http://cidrdb.org/cidr2017/papers/p51-chu-cidr17.pdf.\\n\\nElangovan, A., He, J., and Verspoor, K. Memorization vs. generalization : Quantifying data leakage in NLP performance evaluation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 1325\u20131335, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.113. URL https://aclanthology.org/2021.eacl-main.113.\\n\\nFaghmous, J. H. and Kumar, V . A big data guide to understanding climate change: The case for theory-guided data science. Big data, 2(3):155\u2014163, September 2014. ISSN 2167-6461. doi: 10.1089/big.2014.0026. URL https://europepmc.org/articles/PMC4174912.\"}"}
{"id": "lai23b", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\\n\\nM. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999, 2022.\\n\\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and Steinhardt, J. Measuring coding challenge competence with apps. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Curran, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/c24cd76e1ce41366a4bbe8a49b02a028-Paper-round2.pdf.\\n\\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Lago, A. D., Hubert, T., Choy, P., de Masson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang, P.-S., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J., Mankowitz, D. J., Robinson, E. S., Kohli, P., de Freitas, N., Kavukcuoglu, K., and Vinyals, O. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097, 2022. doi: 10.1126/science.abq1158. URL https://www.science.org/doi/abs/10.1126/science.abq1158.\\n\\nLiang, P., Jordan, M. I., and Klein, D. Learning dependency-based compositional semantics. Computational Linguistics, 39(2):389\u2013446, June 2013. doi: 10.1162/COLI_a_00127. URL https://aclanthology.org/J13-2005.\\n\\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., and Xiong, C. A conversational paradigm for program synthesis. CoRR, abs/2203.13474, 2022.\\n\\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek, C., and Gulwani, S. Synchromesh: Reliable code generation from pre-trained language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=KmtVD97J43e.\\n\\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Blanco, A., and Ma, S. Codebleu: a method for automatic evaluation of code synthesis. CoRR, abs/2009.10297, 2020. URL https://arxiv.org/abs/2009.10297.\\n\\nRomero, C. and Ventura, S. Data mining in education. Wiley Int. Rev. Data Min. and Knowl. Disc., 3(1):12\u201327, jan 2013. ISSN 1942-4787. doi: 10.1002/widm.1075. URL https://doi.org/10.1002/widm.1075.\\n\\nScholak, T., Schucher, N., and Bahdanau, D. PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 9895\u20139901, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.779. URL https://aclanthology.org/2021.emnlp-main.779.\\n\\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and Wang, S. I. Natural language to code translation with execution. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3533\u20133546, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.231.\\n\\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D., Schuster, T., Zheng, H. S., Houlsby, N., and Metzler, D. Unifying language learning paradigms. CoRR, abs/2205.05131, 2022. doi: 10.48550/arXiv.2205.05131. URL https://doi.org/10.48550/arXiv.2205.05131.\\n\\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and Sundaresan, N. Unit test case generation with transformers and focal context. arXiv preprint arXiv:2009.05617, 2020.\\n\\nWang, X., Wu, Q., Zhang, H., Lyu, C., Jiang, X., Zheng, Z., Lyu, L., and Hu, S. Heloc: Hierarchical contrastive learning of source code representation. In Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension, ICPC \u201922, pp. 354\u2013365, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392983. doi: 10.1145/3524610.3527896. URL https://doi.org/10.1145/3524610.3527896.\\n\\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, MAPS 2022, pp. 1\u201310, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392730. doi: 10.1145/3520312.3534862. URL https://doi.org/10.1145/3520312.3534862.\\n\\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig, G. Learning to mine aligned code and natural language pairs from stack overflow. In Proceedings of the 15th International Conference on Mining Software Repositories, MSR \u201918, pp. 476\u2013486, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450357166. doi: 10.1145/3196398.3196408. URL https://doi.org/10.1145/3196398.3196408.\\n\\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z., Ma, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev, D. Spider: A large-scale human-labeled dataset.\"}"}
{"id": "lai23b", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3911\u20133921, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.org/D18-1425.\\n\\nZelle, M. and Mooney, R. J. Learning to parse database queries using inductive logic programming. In Association for the Advancement of Artificial Intelligence (AAAI), pp. 1050\u20131055, 1996.\\n\\nZettlemoyer, L. and Collins, M. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pp. 678\u2013687, Prague, Czech Republic, June 2007. Association for Computational Linguistics. URL https://aclanthology.org/D07-1071.\\n\\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S. Calibrate before use: Improving few-shot performance of language models. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12697\u201312706. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/zhao21c.html.\\n\\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for text-to-SQL with distilled test suites. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 396\u2013411, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL https://aclanthology.org/2020.emnlp-main.29.\\n\\nZhou, S., Alon, U., Agarwal, S., and Neubig, G. Codebertscore: Evaluating code generation with pretrained models of code. CoRR, abs/2302.05527, 2023. doi: 10.48550/arXiv.2302.05527. URL https://doi.org/10.48550/arXiv.2302.05527.\"}"}
{"id": "lai23b", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendices\\n\\nA. Details on Data Collection\\n\\nA.1. Problem Selection\\n\\nSourcing Popular StackOverflow Problems.\\n\\nWe leverage StackOverflow to collect representative data science code generation problems on each library. To select popular problems, we first removed duplicates and selected problems with at least 1 vote, 1000 views, and accepted answers. After this initial filtering, we obtain 15881 NumPy problems, 26248 Pandas problems, 1965 PyTorch problems, 8258 TensorFlow problems, 4141 SciPy problems, and 4499 Scikit-learn problems. Next, we performed a stratified sampling on problems from each year to further subsample the problems from Pandas and TensorFlow. We designed a threshold for each year's problems differently because older problems naturally have higher votes. Table 8 displays the criteria we used to filter each year's problem on Pandas and TensorFlow.\\n\\nFiltering Suitable Problems.\\n\\nFrom the initial pool of popular problems, our annotators selected problems that are suitable for building DS-1000. Besides the considerations mentioned in Section 2, we discuss those problems that are not selected here. In general, we consider a problem to be unsuitable if our multi-criteria evaluation is not applicable (untestable problems). For example, we left StackOverflow problems involving hardware problems (See Figure 29), software errors (See Figure 30), concrete execution time analysis, etc. out of DS-1000. See Figure 31 for a concrete example where the problem asks for a natural language explanation of a method in TensorFlow. We leave incorporating more unsuitable StackOverflow problems for future work.\\n\\nControlling Library Version.\\n\\nTable 7 details the software versions that we build DS-1000 with.\\n\\n| Package Version         |\\n|-------------------------|\\n| Seaborn 0.11.2          |\\n| Matplotlib 3.5.2        |\\n| NumPy 1.21.6            |\\n| Pandas 1.3.5            |\\n| Scikit-learn 1.0.2      |\\n| SciPy 1.7.3             |\\n| TensorFlow 2.10.0       |\\n| PyTorch 1.12.1          |\\n\\nA.2. Example Problems\\n\\nHere we present an example problem from each of the seven libraries in DS-1000 to illustrate the challenges we encountered in creating DS-1000.\\n\\nFigure 9 shows a NumPy problem asking how to generate samples that suit log-uniform distribution. Since the result varies with different solutions and different settings, it's unreasonable to test the equivalence. Instead, we apply the Kolmogorov-Smirnov test that judges whether two groups of samples suit the identical or rather similar population.\\n\\nFigure 10 gives a SciPy problem that describes some trouble with the number of stored elements in a sparse matrix and asks for a solution without repetitive type conversion. Since our self-made assertion that checks the equivalence of two matrices cannot distinguish the difference between stored numbers, we need a special design for this problem. For functional correctness, we check the type of $b$, match the elements, and check the number of non-zero elements ($nnz$), which is the core of the problem. For surface-form constraints, we reject the use of $\\\\text{.toarray()}$, $\\\\text{.A}$, $\\\\text{.todense()}$, and $\\\\text{.array()}$, which might attempt to transform a sparse matrix into a dense one.\\n\\nFigure 11 shows a Pandas problem. We found that the solution with the highest votes ignores the requirement \u201cbut does not exactly match it\u201d in the description of the problem, and thus we had to fix the bug in our reference solution. Besides, we enhanced the test case to check the point.\\n\\nFigure 12 shows a TensorFlow problem. Since there is no built-in testing function defined in TensorFlow 2.10.0, we had to design it ourselves.\\n\\nFigure 13 demonstrates a PyTorch problem. Here we use $\\\\text{load_data()}$ to hide the input and let the models learn from the description. The correct solution is not a regular type conversion, as indicated in the error message.\\n\\nFigure 14 shows a Scikit-learn problem. It requires applying the preprocessing method defined in Scikit-learn to a Pandas dataframe, and it tests whether the models learn Scikit-learn, Pandas, and their interaction well. Actually, these data science libraries are not independent of others, and this problem exemplifies the interactions.\\n\\nFigure 15 shows a Matplotlib problem. Here the original problem on StackOverflow contains an example figure, which cannot be processed by current code models. We rewrite the original problem into a standalone problem, that is, \u201cPlot $y$ over $x$ and show blue dashed grid lines\u201d. The automatic evaluation comes in two parts. First, it compares the image produced by the generated program with the image produced by the reference program. If two images match exactly, then the generated program is considered correct. Otherwise, the automatic evaluation examines the problem.\"}"}
{"id": "lai23b", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\\n\\n| Year   | 2011 | 2012 | 2013 | 2014 | 2015 | 2016 | 2017 | 2018 | 2019 | 2020 | 2021 | 2022 |\\n|--------|------|------|------|------|------|------|------|------|------|------|------|------|\\n| Pandas vote | 50   | 50   | 14   | 14   | 14   | 4    | 4    | 4    | 2    | 2    | 1    | 1    |\\n| view    | 5k   | 5k   | 5k   | 5k   | 5k   | 1k   | 1k   | 1k   | 1.1k | 1.1k | 1k   | 1k   |\\n| problems | 2    | 8    | 467  | 494  | 554  | 2139 | 2483 | 1894 | 1985 | 809  | 225  | 8    |\\n| TensorFlow vote | -    | -    | -    | -    | -    | 10   | 5    | 4    | 2    | 2    | 1    | 1    |\\n| view    | -    | -    | -    | -    | -    | 3k   | 2k   | 1k   | 1.6k | 1.2k | 1.3k | 1k   |\\n| problems | -    | -    | -    | -    | -    | 100  | 632  | 1136 | 1167 | 1004 | 776  | 185  |\\n\\nMatplotlib axis object and asserts the conditions relevant to the problem specification. In this example, the assertions are testing the existence of grid lines and the color of the grid lines.\\n\\nA.3. Problem Perturbation\\n\\nHere, we give an example for each type of perturbation, as shown in Table 1. We highlight the changes we made through perturbations.\\n\\nFigure 16, Figure 17 and Figure 18 give examples of surface perturbations, showing code context perturbation, paraphrasing, and changes in example respectively. The original task hasn't changed.\\n\\nFigure 19 shows how we replace keywords with analogy words in a Pandas problem. The perturbed problem asks for applying an exponential function to column A and B.\\n\\nThe problem in Figure 20 concentrates on changing the required index. Here we specify the target index on which to operate using ordinal numbers. Figure 21 gives an example of reversing the order. The desired output string is reversed (from \\\"abc,def,ghi,jkl\\\" to \\\"jkl,ghi,def,abc\\\"). We expect the models to capture the information and handle the perturbation.\\n\\nFigure 22 shows an example of changing the type of the required result. Here we change the type from pd.DataFrame to pd.Series.\\n\\nFigure 23 and Figure 24 demonstrate how we get difficult rewrites. The example in Figure 23 replaces \\\"highest\\\" with \\\"lowest\\\" and changes the shape of the desired output (from $n \\\\times 1$ to $1 \\\\times n$). The example in Figure 24, on the other hand, focuses on digging more perturbations that could increase the difficulty. The models should not only learn how to use a two-sample KS test but also learn how to interpret the result of the KS test.\\n\\nA.4. Prompt Format\\n\\nAs we've mentioned in Section 4.1, we also provide a prompt of Completion format. Here are two examples (Figure 25 and Figure 26) showing that we have to translate the code in the right context into natural language instructions as complementary information.\\n\\nB. Details of Experiments on numpy-100\\n\\nnumpy-100 is a collection of 100 NumPy exercises from NumPy mailing list, StackOverflow, and NumPy documentation, which has been forked over 4.7k times on GitHub.\\n\\nAs shown in Figure 3, in the numpy-100 problem set, each problem is given a short, one-sentence description with no code context, followed by a reference solution.\\n\\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\\n\\n```python\\nprint(np.unravel_index(99, (6,7,8)))\\n```\"}"}
{"id": "lai23b", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem:\\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\\n\\n```python\\nimport numpy as np\\ndef f():\\n    [insert]\\n    return result\\n```\\n\\nFigure 6: A `numpy-100` example of surface perturbation.\\n\\nWe changed the code context.\\n\\nAt last, we equipped each problem and its perturbation with one test case and an automatic evaluation. Then we tested the performance of Codex-002 on them. We sampled 20 problems from `numpy-100` and generated 10 samples for each problem with the temperature set to 0.7, and top-p cutoff set to 0.95.\\n\\nC. Error Analysis\\n\\nWe provide a preliminary error analysis by showing an example model error in Figure 8 and provide additional examples in Figure 27 and 28. In this example, the problem asks for removing adjacent duplicated non-zero values in a given array, which cannot be satisfied by a single NumPy operation. The reference implements this problem by creating a binary array representing the selection and performing two operations to meet the problem requirement. However, we see Codex-002 fails on the composite request and attempts to answer the problem with a single method, `np.unique`, pointed out as incorrect in the problem already. This example error demonstrates the challenges in DS-1000 problems.\\n\\nProblem:\\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\\n\\n```python\\nimport numpy as np\\n[insert]\\nprint(result)\\n```\"}"}
{"id": "lai23b", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem:\\nGiven a numpy array, I wish to remove the adjacent (before removing) duplicate non-zero value and all the zero value. For instance, for an array like that:\\n\\n\\\\[0,0,1,1,1,2,2,0,1,3,3,3\\\\],\\n\\nI'd like to transform it to:\\n\\n\\\\[1,2,1,3\\\\].\\n\\nDo you know how to do it?\\n\\nI just know `np.unique(arr)` but it would remove all the duplicate value and keep the zero value. Thank you in advance!\\n\\n```python\\n# a: 1-d np.array as input\\nselection = np.ones(len(a), dtype=bool)\\nselection[1:] = a[1:] != a[:-1]\\nselection &= a != 0\\nresult = a[selection]\\n```\\n\\nWrong Solution\\n# Just mimic mentioned wrong solution\\nresult = np.unique(a)\"}"}
{"id": "lai23b", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Problem:**\\n\\nI could not find a built-in function in Python to generate a log uniform distribution given a min and max value (the R equivalent is here), something like: `loguni[n, min, max, base]` that returns `n` log uniformly distributed in the range `min` and `max`.\\n\\nThe closest I found though was `numpy.random.uniform`. That is, given range of `x`, I want to get samples of given size (`n`) that suit log-uniform distribution.\\n\\nAny help would be appreciated!\\n\\n**A:**\\n\\n```python\\nimport numpy as np\\nmin = 1\\nmax = np.e\\nn = 10000\\n\\n# generated by Reference solution\\nprint(result)\\n```\\n\\n---\\n\\n**Surface-form constraints**\\n\\nFor and while should not appear in Syntax Tree\\n\\n---\\n\\n**Problem:**\\n\\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, these elements shouldn't be stored once removed.\\n\\nScipy provides a method to set diagonal elements values:\\n\\n```python\\n...[omit for brevity]\\n```\\n\\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\\n\\n```python\\n...[omit for brevity]\\n```\\n\\n```python\\ngenerate\\nb.setdiag(0)\\nprint(b.toarray())\\n#<2x2 sparse matrix of type '<type 'numpy.float64'>'\\n#with 4 stored elements in Compressed Sparse Row format>\\n#array([[ 0.,  1.],\\n#       [ 1.,  0.]])\\n```\\n\\nThrough a dense array, we have of course:\\n\\n```python\\ngenerate\\ncsr_matrix(b.toarray())\\n#<2x2 sparse matrix of type '<type 'numpy.float64'>'\\n#with 2 stored elements in Compressed Sparse Row format>\\n```\\n\\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any workaround else than going from sparse to dense to sparse again?\\n\\n**A:**\\n\\n```python\\nfrom scipy import sparse\\nimport numpy as np\\na = np.ones((2, 2))\\nb = sparse.csr_matrix(a)\\nb.setdiag(0)\\nprint(b)\\n```\\n\\n---\\n\\n**Reference Solution**\\n\\n```python\\nb.setdiag(0)\\nb.eliminate_zeros()\\n```\"}"}
{"id": "lai23b", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I am using Pandas to get a dataframe like this:\\n\\n| name | a | b | c |\\n|------|---|---|---|\\n| Aaron | 3 | 5 | 7 |\\n| Aaron | 3 | 6 | 9 |\\n| Aaron | 3 | 6 | 10 |\\n| Brave | 4 | 6 | 0 |\\n| Brave | 3 | 6 | 1 |\\n\\nI want to replace each name with a unique ID so output looks like:\\n\\n| name | a | b | c |\\n|------|---|---|---|\\n| 1    | 3 | 5 | 7 |\\n| 1    | 3 | 6 | 9 |\\n| 1    | 3 | 6 | 10 |\\n| 2    | 4 | 6 | 0 |\\n| 2    | 3 | 6 | 1 |\\n\\nHow can I do that?\\n\\nReference Solution\\n\\n```python\\n# df: pd.DataFrame as input\\nresult = df.replace(df['name'].unique(), range(1, len(df['name'].unique()) + 1))\\n```\\n\\nWrong Solution\\n\\n```python\\n# create a column named \\\"ID\\\"\\ndf['ID'] = df.groupby(['name']).ngroup()\\nresult = df\\n```\\n\\nProblem:\\nI'm using tensorfl\\now 2.10.0.\\nI have a list of bytes and I want to convert it to a list of strings:\\n\\n```python\\nx = [b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\\n     b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\\n     b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\\n     b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\\n     b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\\n```\\n\\nHow can I get the string result list in Tensorflow?\\n\\nReference Solution\\n\\n```python\\n# x: list of bytes as input\\nresult = [tf.compat.as_str_any(a) for a in x]\\n```\\n\\nWrong Solution\\n\\n```python\\n# Not using method in Tensorflow\\nresult = [item.decode('utf-8') for item in x]\\n```\\n\\nProblem:\\nI'm using tensorflow 2.10.0.\\nI have a list of bytes and I want to convert it to a list of strings:\\n\\n```python\\nx = [b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\\n     b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\\n     b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\\n     b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\\n     b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\\n```\\n\\nHow can I get the string result list in Tensorflow?\\n\\nWrong SoluHon\\n\\n```python\\n# Not using method in Tensorflow\\nresult = [item.decode('utf-8') for item in x]\\n```\\n\\nFigure 27: An example wrong solution that misunderstands the requirements and modifies on the wrong column.\\n\\nFigure 28: An example wrong solution that uses a common function instead of a function of TensorFlow.\"}"}
{"id": "lai23b", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\\n\\nFigure 29: An example untestable problem involving hardware problems.\\n\\nFigure 30: An example untestable problem involving software errors.\"}"}
{"id": "lai23b", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 31: An example untestable problem involving explanations.\"}"}
{"id": "lai23b", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How might I do this?\\n\\nI wish to create a mask of 1s and 0s whose number of 0s corresponds to the entries to this:\\n\\n\\\\[\\n\\\\begin{bmatrix}\\n1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\\\n1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 \\\\\\\\\\n1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\\\\\\n1 & 1 & 1 & 0 & 0 & 0 & 0 & 0\\n\\\\end{bmatrix}\\n\\\\]\\n\\nI'm using tensorflow.\\n\\n**Problem:**\\n\\nI have a dataframe with column names, and I want to find the one containing a certain string, but does not exactly match it. I'm searching for 'spike' in column names like 'spike-2', 'hey spike', 'spiked-in' (the 'spike' part is always continuous).\\n\\nJust iterate over DataFrame.columns, now this is an example in which you will end up with a list of column names that match:\\n\\n\\\\[\\n\\\\text{spike_cols} = \\\\{\\\\text{col for col in df.columns if 'spike' in col}\\\\}\\n\\\\]\\n\\nWe need to write reference solutions by ourselves because high-vote replies do not exactly match it.\\n\\n**Figure 11:** An example problem of Pandas.\\n\\n**Figure 12:** An example problem of TensorFlow.\\n\\nWe implemented a well-designed test function for tensor comparison. We can use the `tf.reduce_min` function to find the minimum element in a tensor.\\n\\n```python\\n# generated by Reference solution\\nlengths = tf.constant([4, 3, 5, 2])\\nlengths_transposed = tf.expand_dims(lengths, 1)\\nrange = tf.range(0, 8, 1)\\nrange_row = tf.expand_dims(range, 0)\\nmask = tf.equal(lengths_transposed, range_row)\\nresult = tf.reduce_min(mask)\\n```\\n\\nJust iterate over DataFrame.columns, now this is an example in which you will end up with a list of column names that match:\\n\\n```python\\nspike_cols = [col for col in df.columns if 'spike' in col]\\n```\\n\\nYou can use the `df['name']` as `df[spike_cols]` or `df[spike_cols]`.\\n\\n**Test code**\\n\\nWe can use `df[df.columns.drop(spike_cols)]` or `df.columns.drop(spike_cols)`.\\n\\n**Test case 1**\\n\\n```python\\nspike_cols = [col for col in df.columns if 'spike' in col]  \\nresult = df.columns.drop(spike_cols)\\n```\\n\\n**Test case 2**\\n\\n```python\\nresult = df[df.columns.drop(spike_cols)]\\n```\\n\\n**Highest-vote Solution**\\n\\n```python\\ndef find_column(df, regex):\\n    spike_cols = [col for col in df.columns if any(s in col for s in ['spike', 'foo', 'bar'])]\\n    return df.filter(regex='(spike)|(foo)|(bar)').columns\\n\\ndf = pd.DataFrame(data)\\nresult = find_column(df, regex='(spike)|(foo)|(bar)')\\nprint(result)\\n```\\n\\nLet's denote the columns that contain the string 'spike' by `spike_cols`. Then, we can use the following code to find the column names that match:\\n\\n```python\\nspike_cols = [col for col in df.columns if 'spike' in col]\\n```\\n\\nand use `df[spike_cols]` or `df.columns[spike_cols]`.\\n\\nWe can also access the column later with `df['name']` as `df[spike_cols]` or `df[spike_cols]`.\\n\\nWe can use the `df[df.columns.drop(spike_cols)]` or `df.columns.drop(spike_cols)`.\\n\\n**Best-practice Solution**\\n\\n```python\\ndef find_column(df, regex):\\n    spike_cols = [col for col in df.columns if any(s in col for s in ['spike', 'foo', 'bar'])]\\n    return df.filter(regex='(spike)|(foo)|(bar)').columns\\n\\ndf = pd.DataFrame(data)\\nresult = find_column(df, regex='(spike)|(foo)|(bar)')\\nprint(result)\\n```\\n\\nJust iterate over DataFrame.columns, now this is an example in which you will end up with a list of column names that match:\\n\\n\\\\[\\n\\\\text{spike_cols} = \\\\{\\\\text{col for col in df.columns if 'spike' in col}\\\\}\\n\\\\]\\n\\nWe need to write reference solutions by ourselves because high-vote replies do not exactly match it.\\n\\n**Figure 11:** An example problem of Pandas.\\n\\n**Figure 12:** An example problem of TensorFlow.\\n\\nWe implemented a well-designed test function for tensor comparison. We can use the `tf.reduce_min` function to find the minimum element in a tensor.\\n\\n```python\\n# generated by Reference solution\\nlengths = tf.constant([4, 3, 5, 2])\\nlengths_transposed = tf.expand_dims(lengths, 1)\\nrange = tf.range(0, 8, 1)\\nrange_row = tf.expand_dims(range, 0)\\nmask = tf.equal(lengths_transposed, range_row)\\nresult = tf.reduce_min(mask)\\n```\\n\\nJust iterate over DataFrame.columns, now this is an example in which you will end up with a list of column names that match:\\n\\n\\\\[\\n\\\\text{spike_cols} = \\\\{\\\\text{col for col in df.columns if 'spike' in col}\\\\}\\n\\\\]\\n\\nYou can use the `df['name']` as `df[spike_cols]` or `df[spike_cols]`.\\n\\n**Test code**\\n\\nWe can use `df[df.columns.drop(spike_cols)]` or `df.columns.drop(spike_cols)`.\\n\\n**Test case 1**\\n\\n```python\\nspike_cols = [col for col in df.columns if 'spike' in col]  \\nresult = df.columns.drop(spike_cols)\\n```\\n\\n**Test case 2**\\n\\n```python\\nresult = df[df.columns.drop(spike_cols)]\\n```\\n\\n**Highest-vote Solution**\\n\\n```python\\ndef find_column(df, regex):\\n    spike_cols = [col for col in df.columns if any(s in col for s in ['spike', 'foo', 'bar'])]\\n    return df.filter(regex='(spike)|(foo)|(bar)').columns\\n\\ndf = pd.DataFrame(data)\\nresult = find_column(df, regex='(spike)|(foo)|(bar)')\\nprint(result)\\n```\"}"}
{"id": "lai23b", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Automa\\\\textsc{1}c Evalua\\\\textsc{1}on Test code\\n\\n```python\\ntorch.testing.assert_close(tensor_of_tensors, ans, check_dtype=False)\\n```\\n\\nTest case 1\\n\\n```python\\ntorch.random.manual_seed(42)\\nlist_of_tensors = [torch.randn(3), torch.randn(3), torch.randn(3)]\\nans =\\n```\\n\\nProblem: I have this code:\\n\\n```python\\nimport torch\\nlist_of_tensors = [torch.randn(3), torch.randn(3), torch.randn(3)]\\ntensor_of_tensors = torch.tensor(list_of_tensors)\\n```\\n\\nI am getting the error:\\n\\n```\\nValueError: only one element tensors can be converted to Python scalars\\n```\\n\\nHow can I convert the list of tensors to a tensor of tensors in PyTorch?\\n\\nA:\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nlist_of_tensors = load_data()\\n```\\n\\nBEGIN SOLUTION\\n\\n```python\\n[insert]\\n```\\n\\nEND SOLUTION\\n\\n```python\\nprint(tensor_of_tensors)\\n```\\n\\nReference Solution\\ntensor_of_tensors = torch.stack((list_of_tensors)) \\nAutoma\\\\textsc{1}c Evalua\\\\textsc{1}on\\nTest code \\n... \\n```\\n\\nBEGIN SOLUTION \\n```\\n[insert]\\n```\\n\\nEND SOLUTION \\n```\\n\\nReference Solu\\\\textsc{1}on\\ndf_out = pd.DataFrame(preprocessing.scale(data),   \\n                 index=data.index, ... load_data() \\n```\\n\\nBEGIN SOLUTION \\n```\\n[insert] \\n```\\n\\nEND SOLUTION \\n```\\n\\nprint(df_out)\\n\\nReference Solu\\\\textsc{1}on\\ndf_out = pd.DataFrame(preprocessing.scale(data), ...\\n```\\n\\nBEGIN SOLUTION \\n```\\n[insert] \\n```\\n\\nEND SOLUTION \\n```\\n\\nprint(df_out)\\n\\nProblem: I'm using the excellent \\\\texttt{read_csv()} func on from pandas, which gives:\\n\\n```\\nIn [31]: data = pandas.read_csv(\\\"lala.csv\\\", delimiter=\\\",\\n```\\n\\n```\\nIn [32]: data\\nOut[32]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 12083 entries, 0 to 12082\\nColumns: 569 entries, REGIONC to SCALEKER\\ndtypes: float64(51), int64(518)\\n```\\n\\nbut when i apply a func on from scikit-learn i lose the informa\\\\textsc{1}on:\\n\\n```python\\nfrom sklearn import preprocessing\\npreprocessing.scale(data)\\ngives numpy array.\\n```\\n\\nIs there a way to apply preprocessing.scale to DataFrames without losing the informa\\\\textsc{1}on(index, columns)?\\n\\nA:\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn import preprocessing\\ndata = load_data()\\n```\\n\\nBEGIN SOLUTION\\n\\n```python\\n[insert]\\n```\\n\\nEND SOLUTION\\n\\n```python\\nprint(df_out)\\n```\\n\\nReference Solu\\\\textsc{1}on\\n\\ndf_out = pd.DataFrame(preprocessing.scale(data),   \\n                 index=data.index, ... load_data() \\n```\\n\\nBEGIN SOLUTION \\n```\\n[insert] \\n```\\n\\nEND SOLUTION \\n```\\n\\nprint(df_out)\\n\\nReference Solu\\\\textsc{1}on\\n\\ndf_out = pd.DataFrame(preprocessing.scale(data), ...\\n```\\n\\nBEGIN SOLUTION \\n```\\n[insert] \\n```\\n\\nEND SOLUTION \\n```\\n\\nprint(df_out)\\n\\nFigure 13: An example problem of \\\\textsc{PyTorch}, with failed attempt and error message given in the description.\\n\\nFigure 14: An example problem of \\\\textsc{Scikit-learn}, requiring applying Scikit-learn preprocessing method to Pandas dataframe.\"}"}
{"id": "lai23b", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 15: An example problem of Matplotlib. Matplotlib original problems often contain example figures which cannot be processed by current code models. We rewrite original problems into standalone problems in the form of comments.\\n\\n```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nx = np.arange(10)\\ny = np.arange(10)\\n# Plot y over x and show blue dashed grid lines\\n# SOLUTION START\\nplt.plot(y, x)\\nplt.grid(color=\\\"blue\\\", linestyle=\\\"dashed\\\")\\n```\\n\\nFigure 16: An example problem of surface perturbation. We expect the model to complete the function (on the right).\"}"}
{"id": "lai23b", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem: How do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to a Pandas DataFrame?\\n\\n```python\\nfrom sklearn.datasets import load_iris\\nimport pandas as pd\\n\\ndata = load_iris()\\nprint(type(data))\\n\\ndata1 = pd.\\n```\\n\\nProblem: Can you give me any suggestion that transforms a sklearn Bunch object (from sklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\\n\\n```python\\nfrom sklearn.datasets import load_iris\\nimport pandas as pd\\n\\ndata = load_iris()\\nprint(type(data))\\n\\ndata1 = pd.\\n```\\n\\nMay be you can give me a Pandas method?\\n\\nProblem: How to convert a numpy array of dtype=object to torch Tensor?\\n\\n```python\\narray([\\n    array([0.5, 1.0, 2.0], dtype=float16),\\n    array([4.0, 6.0, 8.0], dtype=float16),\\n    array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\\n    array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\\n    array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\\n], dtype=object)\\n```\"}"}
{"id": "lai23b", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem: Sample dataframe:\\n\\n```\\n| A | B |\\n|---|---|\\n| 1 | 4 |\\n| 2 | 5 |\\n| 3 | 6 |\\n```\\n\\nI'd like to add inverses of each existing column to the dataframe and name them based on existing column names with a prefix, e.g. inv_A is an inverse of column A and so on. The resulting dataframe should look like so:\\n\\n```\\n| A | B   | inv_A | inv_B |\\n|---|-----|-------|-------|\\n| 1 | 4   | 1/1   | 1/4   |\\n| 2 | 5   | 1/2   | 1/5   |\\n| 3 | 6   | 1/3   | 1/6   |\\n```\\n\\nProblem: Sample dataframe:\\n\\n```\\n| A | B |\\n|---|---|\\n| 1 | 4 |\\n| 2 | 5 |\\n| 3 | 6 |\\n```\\n\\nI'd like to add exponentials of each existing column to the dataframe and name them based on existing column names with a prefix, e.g. exp_A is an exponential of column A and so on. The resulting dataframe should look like so:\\n\\n```\\n| A   | B   | exp_A | exp_B |\\n|-----|-----|-------|-------|\\n| 1   | 4   | $e^1$ | $e^4$ |\\n| 2   | 5   | $e^2$ | $e^5$ |\\n| 3   | 6   | $e^3$ | $e^6$ |\\n```\\n\\nNotice that e is the natural constant.\\n\\nProblem: I have a 2D array `a` to represent a many-many mapping:\\n\\n```\\n| 0 | 3 | 1 | 3 |\\n|---|---|---|---|\\n| 3 | 0 | 0 | 0 |\\n| 1 | 0 | 0 | 0 |\\n| 3 | 0 | 0 | 0 |\\n```\\n\\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular index (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\\n\\nProblem: I have a 2D array `a` to represent a many-many mapping:\\n\\n```\\n| 0 | 3 | 1 | 3 |\\n|---|---|---|---|\\n| 3 | 0 | 0 | 0 |\\n| 1 | 0 | 0 | 0 |\\n| 3 | 0 | 0 | 0 |\\n```\\n\\nWhat is the quickest way to 'zero' out the second row and the first column?\\n\\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential.\u201d\\n\\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\"}"}
{"id": "lai23b", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem:\\nI have the following dataframe:\\n\\n| text |\\n|------|\\n| abc  |\\n| def  |\\n| ghi  |\\n| jkl  |\\n\\nHow can I merge these rows into a dataframe with a single row like the following one?\\n\\n| text |\\n|------|\\n| abc, def, ghi, jkl |\\n\\nProblem:\\nI have the following dataframe:\\n\\n| text |\\n|------|\\n| abc  |\\n| def  |\\n| ghi  |\\n| jkl  |\\n\\nHow can I merge these rows into a dataframe with a single row like the following one?\\n\\n| text |\\n|------|\\n| jkl, ghi, def, abc |\\n\\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed.\\n\\nProblem:\\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values where the value (always a float \\\\(-1 \\\\leq x \\\\leq 1\\\\)) is above 0.3.\\n\\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all columns in. Is there a best practice on this?\\n\\ndesired Series:\\n\\n|     |     |     |\\n|-----|-----|-----|\\n| 0   | 3   | 0.373153 |\\n| 1   | 3   | 0.419219 |\\n|     | 4   | 0.356149 |\\n| 3   | 4   | 0.389972 |\\n\\ndtype: float64\\n\\nFigure 22: An example problem of semantic perturbation. The type of the desired result has been changed but the content still keeps the same.\"}"}
{"id": "lai23b", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem: I have a logistic regression model using PyTorch, where my input is high-dimensional and my output must be a scalar - 0, 1 or 2. I'm using a linear layer combined with a softmax layer to return a n x 3 tensor, where each column represents the probability of the input falling in one of the three classes (0, 1 or 2).\\n\\nHowever, I must return a n x 1 tensor, so I need to somehow pick the highest probability for each input and create a tensor indicating which class had the highest probability. How can I achieve this using PyTorch?\\n\\nTo illustrate, my Softmax outputs this:\\n\\n\\\\[\\n\\\\begin{bmatrix}\\n0.2, 0.1, 0.7 \\\\\\\\\\n0.6, 0.2, 0.2 \\\\\\\\\\n0.1, 0.8, 0.1 \\\\\\\\\\n\\\\end{bmatrix}\\n\\\\]\\n\\nAnd I must return this:\\n\\n\\\\[\\n\\\\begin{bmatrix}\\n2 \\\\\\\\\\n0 \\\\\\\\\\n1 \\\\\\\\\\n\\\\end{bmatrix}\\n\\\\]\\n\\nProblem: \u2026\\n\\nHowever, I must return a 1 x n tensor, and I want to somehow pick the lowest probability for each input and create a tensor indicating which class had the lowest probability. How can I achieve this using PyTorch?\\n\\nTo illustrate, my Softmax outputs this:\\n\\n\\\\[\\n\\\\begin{bmatrix}\\n0.2, 0.1, 0.7 \\\\\\\\\\n0.6, 0.3, 0.1 \\\\\\\\\\n0.15, 0.8, 0.05 \\\\\\\\\\n\\\\end{bmatrix}\\n\\\\]\\n\\nAnd I must return this:\\n\\n\\\\[\\n\\\\begin{bmatrix}\\n1, 2, 2 \\\\\\\\\\n\\\\end{bmatrix}\\n\\\\]\\n\\nWhich has the type torch.LongTensor.\\n\\nProblem: I can't figure out how to do a Two-sample KS test in Scipy. \u2026\\n\\ntest_stat = kstest(x, 'norm')\\n#>>> test_stat\\n#(0.021080234718821145, 0.76584491300591395)\\n\\nWhich means that at p-value of 0.76 we can not reject the null hypothesis that the two distributions are identical.\\n\\nHowever, I want to compare two distributions and see if I can reject the null hypothesis that they are identical.\\n\\n\u2026\\n\\nI tried the naive:\\n\\ntest_stat = kstest(x, z)\\nand got the following error:\\n\\nTypeError: 'numpy.ndarray' object is not callable\\n\\nIs there a way to do a two-sample KS test in Python? If so, how should I do it?\\n\\nThank You in Advance\\n\\nProblem: \u2026\\n\\nIs there a way to do a two-sample KS test in Python, then test whether I can reject the null hypothesis that the two distributions are identical (result=True means able to reject, and the vice versa) based on alpha? If so, how should I do it?\\n\\nThank You in Advance\\n\\nOrigin Problem: I can't figure out how to do a Two-sample KS test in Scipy. \u2026\\ntest_stat = \u2026 means able to reject, and the vice versa) based on alpha? If so, how should I do it? Thank You in Advance\"}"}
{"id": "lai23b", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem:\\n\\nSample dataframe:\\n\\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\\n\\nI'd like to add inverses of each existing column to the dataframe and name them based on existing column names with a prefix, e.g. inv_A is an inverse of column A and so on.\\n\\nA:\\n\\n```python\\nimport pandas as pd\\n\\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\\n```\\n\\nBEGIN SOLUTION\\n\\n```python\\nresult = ...\\n```\\n\\nEND SOLUTION\\n\\nFigure 25: Completion prompt corresponding to Figure 1.\\n\\nA:\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.ensemble import BaggingClassifier\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\nX_train, y_train = load_data()\\nassert type(X_train) == np.ndarray\\nassert type(y_train) == np.ndarray\\n\\nX_test = X_train\\n\\nparam_grid = {\\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\\n}\\n\\ndt = DecisionTreeClassifier(max_depth=1)\\n\\nbc = BaggingClassifier(dt, n_estimators=20, max_samples=0.5, max_features=0.5)\\n\\nbc = bc.fit(X_train, y_train)\\n```\\n\\nBEGIN SOLUTION\\n\\n```python\\nproba = bc.predict_proba(X_test)\\nprint(proba)\\n```\\n\\nEND SOLUTION\\n\\nProblem:\\n\\nSay that I want to train BaggingClassifier that uses DecisionTreeClassifier:\\n\\ndt = DecisionTreeClassifier(max_depth = 1)\\nbc = BaggingClassifier(dt, n_estimators = 20, max_samples = 0.5, max_features = 0.5)\\nbc = bc.fit(X_train, y_train)\\n\\nI would like to use GridSearchCV to find the best parameters for both BaggingClassifier and DecisionTreeClassifier (e.g. max_depth from DecisionTreeClassifier and max_samples from BaggingClassifier), what is the syntax for this? Besides, you can just use the default arguments of GridSearchCV.\"}"}
