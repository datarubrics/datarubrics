{"id": "bq1JEgioLr", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CI S\\nENCH: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models\\n\\nskills, particularly the code conversion skills. This issue becomes particularly prominent when using the Wolfram Language, with 46.9% error rate in code conversion skill. Despite providing grammar specifications in system prompts and a few examples as demonstrations, most attempts of code conversion result in syntax errors. In Wolfram Language, the error mainly comes from the violation of variable rules (for instance, Wolfram Language reserves certain letters such as \\\\texttt{E} as protected symbols and disallows underscores in variable names) or incorrect usage of certain functions. This observation suggests a potential improvement for LLM when using Wolfram Language.\\n\\nD. Problem Solving Abilities of Current LLMs\\n\\nD.1. Assessment of the Evaluation Protocol\\n\\nIn order to assess the effectiveness of our evaluation protocol's classification, we enlisted the assistance of two annotators to determine whether the errors identified by the model verifier were accurate or not. Through the annotation of 151 samples across different settings, we observed that 123 of them were correctly classified, resulting in an accuracy rate of 81.45%.\\n\\nTwo human annotators participate in the process. Decisions on the final abilities are determined by annotators, aided by assistants. By going through errors, these two annotators develop ten abilities and then employ a Language Learning Model (LLM) as a third evaluator to suggest additional abilities. They then compare and refine their findings based on this input. Ultimately, the final outcomes are determined by the annotators. After LLM annotate the error reasons, we conduct human-check by sampling 151 examples across all settings to make sure the annotations make sense. We make this human-AI cooperated analysis pipeline to reduce the cost of human post-analysis, while incorporate human checking to make sure the correctness of LLM decision and try to reduce the risk that reviewer mentioned. Though not perfect, we believe it can be another type of analyzing framework for future study of LLM problem-solving.\\n\\nD.2. Examples\\n\\nIn the context of each specific capability, we present several exemplary errors accompanied by their corresponding classifications and explanations derived from the GPT model. Referencing Figure S9, the ChatGPT solution employing the Chain-of-Thought method corresponds to error category 4, \u201cCausal Reasoning\u201d. The model explanation posits that \u201cThe error reason category is 4. Causal Reasoning. The model solution uses the relativistic momentum formula to calculate the momentum of the electron, which is not necessary for this problem since the electron is traveling at only 1.00% of the speed of light. The relativistic momentum formula is only needed when the velocity of the particle is a significant fraction of the speed of light. Therefore, using this formula in this problem leads to an incorrect value for the momentum and consequently an incorrect value for the de Broglie wavelength.\u201d Meanwhile, the ChatGPT solution that doesn't utilize the Chain-of-Thought method is categorized under error case 10, \u201cCalculation Skills\u201d. This example is further highlighted as an illustrative showcase for explanations of the Chain-of-Thought method in Section 5. More examples resulting in different error types are shown in Figures S10 to S19.\"}"}
{"id": "bq1JEgioLr", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem\\nCalculate the de Broglie wavelength of an electron traveling at 1.00% of the speed of light.\\n\\nCorrect Solution\\nThe mass of an electron is $9.11 \\\\times 10^{-31}$ kg. One percent of the speed of light is $v = (0.0100)c = 2.998 \\\\times 10^8$ m/s.\\n\\nThe momentum of the electron is given by $p = mv = 9.11 \\\\times 10^{-31} \\\\times 2.998 \\\\times 10^8 = 2.73 \\\\times 10^{-24}$ kg\u00b7m/s.\\n\\nThe de Broglie wavelength of this electron is $\\\\lambda = \\\\frac{h}{p} = 6.626 \\\\times 10^{-34} \\\\text{ J} \\\\cdot \\\\text{s} / 2.73 \\\\times 10^{-24} \\\\text{ kg} \\\\cdot \\\\text{m} / \\\\text{s} = 3.43 \\\\times 10^{-10}$ m = 343 pm.\\n\\nThis wavelength is of atomic dimensions.\\n\\nFinal Answer: 343 pm\\n\\nChatGPT Solution with Chain-of-Thought Method\\nLet's start by recalling the de Broglie wavelength formula:\\n\\n$\\\\lambda = \\\\frac{h}{p}$\\n\\nwhere $\\\\lambda$ is the de Broglie wavelength, $h$ is Planck's constant, and $p$ is the momentum of the particle. To calculate the momentum of the electron, we can use the relativistic momentum formula:\\n\\n$p = mv^2/c^2$\\n\\nwhere $m$ is the mass of the electron, $v$ is its velocity, and $c$ is the speed of light. Plugging in the given values, we get:\\n\\n$p = (9.11 \\\\times 10^{-31} \\\\text{ kg})(0.01 \\\\text{c})^2 = 8.19 \\\\times 10^{-23}$ kg\u00b7m/s.\\n\\nNow we can plug this momentum into the de Broglie wavelength formula:\\n\\n$\\\\lambda = \\\\frac{h}{p} = 6.626 \\\\times 10^{-34} \\\\text{ J} \\\\cdot \\\\text{s} / 8.19 \\\\times 10^{-23} \\\\text{ kg} \\\\cdot \\\\text{m} / \\\\text{s} = 8.09 \\\\times 10^{-10}$ m = 809 pm.\\n\\nTherefore, the de Broglie wavelength of the electron is 809 pm.\\n\\nFinal Answer: 809 pm\\n\\nChatGPT Solution without Chain-of-Thought Method\\nThe de Broglie wavelength of a particle is given by the equation:\\n\\n$\\\\lambda = \\\\frac{h}{p}$\\n\\nwhere $h$ is the Planck constant and $p$ is the momentum of the particle. The momentum of an electron is given by:\\n\\n$p = mv$\\n\\nwhere $m$ is the mass of the electron and $v$ is its velocity. We are given that the electron is traveling at 1.00% of the speed of light, which is $v = 0.01c$ where $c$ is the speed of light. The mass of an electron is $m = 9.11 \\\\times 10^{-31}$ kg.\\n\\nPutting it all together, we have:\\n\\n$\\\\lambda = \\\\frac{h}{mv}$\\n\\nSubstituting the values of $h$, $m$, and $c$, we get:\\n\\n$\\\\lambda = 6.626 \\\\times 10^{-34} \\\\text{ J} \\\\cdot \\\\text{s} / (9.11 \\\\times 10^{-31} \\\\text{ kg} \\\\cdot 0.01 \\\\cdot 2.998 \\\\times 10^8 \\\\text{ m/s}) = 3.31 \\\\times 10^{-12}$ m = 331 pm.\\n\\nTherefore, the de Broglie wavelength of the electron is 331 pm.\\n\\nFinal Answer: 331 pm\\n\\nFigure S9. An example problem, inaccurately solved by ChatGPT with and without the Chain of Thought method, is provided to highlight different types of errors. In the example, the mistaken step is highlighted in red.\"}"}
{"id": "bq1JEgioLr", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem\\nFind the length of the cardioid $r = 1 + \\\\sin \\\\theta$.\\n\\nCorrect Solution\\nThe cardioid's full length is given by the parameter interval $0 \\\\leq \\\\theta \\\\leq 2\\\\pi$, so\\n\\n$$L = \\\\int_{0}^{2\\\\pi} \\\\sqrt{r^2 + \\\\left(\\\\frac{dr}{d\\\\theta}\\\\right)^2} \\\\, d\\\\theta = \\\\int_{0}^{2\\\\pi} \\\\sqrt{(1 + \\\\sin \\\\theta)^2 + \\\\cos^2 \\\\theta} \\\\, d\\\\theta$$\\n\\nWe could evaluate this integral by multiplying and dividing the integrand by $\\\\sqrt{2 - 2 \\\\sin \\\\theta}$, or we could use a computer algebra system. In any event, we find that the length of the cardioid is $L = 8$.\\n\\nFinal Answer: $8$\\n\\nModel Solution\\nTo find the length of the cardioid, we use the formula for arc length in polar coordinates:\\n\\n$$L = \\\\int_{\\\\alpha}^{\\\\beta} \\\\sqrt{r^2 + \\\\left(\\\\frac{dr}{d\\\\theta}\\\\right)^2} \\\\, d\\\\theta$$,\\n\\nwhere $\\\\alpha$ and $\\\\beta$ are the angles that correspond to one full loop of the cardioid (i.e. $\\\\alpha = 0$ and $\\\\beta = 2\\\\pi$). First, we find $\\\\frac{dr}{d\\\\theta}$:\\n\\n$$\\\\frac{dr}{d\\\\theta} = \\\\frac{d}{d\\\\theta}(1 + \\\\sin \\\\theta) = \\\\cos \\\\theta.$$  \\n\\nThen, we plug in $r = 1 + \\\\sin \\\\theta$ and $\\\\frac{dr}{d\\\\theta} = \\\\cos \\\\theta$ into the formula for arc length:\\n\\n$$L = \\\\int_{0}^{2\\\\pi} \\\\sqrt{(1 + \\\\sin \\\\theta)^2 + \\\\cos^2 \\\\theta} \\\\, d\\\\theta = \\\\int_{0}^{2\\\\pi} \\\\sqrt{2 + 2 \\\\sin \\\\theta + \\\\sin^2 \\\\theta + \\\\cos^2 \\\\theta} \\\\, d\\\\theta = \\\\int_{0}^{2\\\\pi} \\\\sqrt{2 + 2 \\\\sin \\\\theta + \\\\sin^2 \\\\theta + \\\\cos^2 \\\\theta} \\\\, d\\\\theta = \\\\int_{0}^{2\\\\pi} \\\\sqrt{2 + 2 \\\\sin \\\\theta} \\\\, d\\\\theta = 4 \\\\int_{0}^{\\\\pi} \\\\sqrt{2 + 2 \\\\sin \\\\theta} \\\\, d\\\\theta$$\\n\\n(due to symmetry)\\n\\n$$= 8 \\\\int_{0}^{\\\\pi/2} \\\\sqrt{2 + 2 \\\\sin \\\\theta} \\\\, d\\\\theta$$\\n\\n(using the half-angle formula)\\n\\n$$= 16 \\\\int_{0}^{\\\\pi/2} \\\\cos \\\\theta \\\\, d\\\\theta = 32 \\\\sin \\\\theta \\\\bigg|_{0}^{\\\\pi/2} = 32.$$\\n\\nTherefore, the length of the cardioid is $32$, and the unit of the answer is whatever unit was used for $r$ (e.g. meters, centimeters, etc.).\\n\\nFinal Answer: $32$\\n\\nError Reason\\nThe model solution made an error in the calculation of the integral $\\\\int_{0}^{2\\\\pi} \\\\sqrt{2 + 2 \\\\sin \\\\theta} \\\\, d\\\\theta$. The correct solution correctly simplified this integral to $4 \\\\int_{0}^{\\\\pi} \\\\sqrt{2 + 2 \\\\sin \\\\theta} \\\\, d\\\\theta$ due to the symmetry of the integrand, and then used a substitution to evaluate the integral. The model solution, on the other hand, incorrectly evaluated this integral as $8 \\\\pi$.\\n\\nThis error falls under the category of 10. Calculation Skills.\"}"}
{"id": "bq1JEgioLr", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem Use Simpson's Rule with \\\\( n = 10 \\\\) to approximate \\n\\\\[\\n\\\\int_{1}^{2} \\\\frac{1}{x} \\\\, dx\\n\\\\]\\n.\\n\\nCorrect Solution\\n\\nPutting \\\\( f(x) = \\\\frac{1}{x}, n = 10 \\\\), and \\\\( \\\\Delta x = \\\\frac{1}{10} \\\\) in Simpson's Rule, we obtain\\n\\n\\\\[\\n\\\\int_{1}^{2} \\\\frac{1}{x} \\\\, dx \\\\approx \\\\frac{1}{3} \\\\left[ f(1) + 4f(1.1) + 2f(1.2) + 4f(1.3) + \\\\cdots + 2f(1.8) + 4f(1.9) + f(2) \\\\right] = 0.693150\\n\\\\]\\n\\nFinal Answer: 0.693150\\n\\nModel Solution\\n\\ntherefore not provided as this problem requires visual representations and cannot be solved through text alone.\\n\\nError Reason\\n\\nThe error reason category is 3. Spatial Perception. This problem requires visual representations to find the bonding and antibonding H\u00fcckel molecular orbitals for ethene. The model's solution is incomplete as it only provides equations and does not provide any visual representation.\"}"}
{"id": "bq1JEgioLr", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem: What is the mean speed, $\\\\bar{c}$, of $N$ molecules in air at 25\u00b0C?\\n\\nCorrect Solution:\\nThe integral required is\\n$$\\\\bar{c} = \\\\frac{4\\\\pi M^2}{\\\\pi RT} \\\\int_0^{\\\\infty} v^3 e^{-\\\\frac{Mv^2}{2RT}} dv = \\\\frac{4\\\\pi M^2}{\\\\pi RT} \\\\times \\\\frac{1}{2^2 RT M^2} = \\\\frac{8RT}{\\\\pi M^{1/2}}$$\\nwhere we have used the standard result from tables of integrals (or software) that\\n$$\\\\int_0^{\\\\infty} x^3 e^{-ax^2} dx = \\\\frac{1}{2a^2}$$\\nSubstitution of the data then gives\\n$$\\\\bar{c} = \\\\frac{8RT}{\\\\pi M^{1/2}} \\\\approx 475 \\\\text{ m s}^{-1}$$\\nwhere we have used $1 \\\\text{ J} = 1 \\\\text{ kg m}^2 \\\\text{ s}^{-2}$.\\n\\nFinal Answer: 475 m s$^{-1}$\\n\\nModel Solution:\\nTo calculate this, we can use the formula for the root-mean-square (rms) speed of gas molecules:\\n$$c_{\\\\text{rms}} = \\\\sqrt{\\\\frac{3kT}{m}}$$\\nwhere $k$ is the Boltzmann constant, $T$ is the temperature in Kelvin, and $m$ is the mass of the molecule. For $N_2$, $m = 28 \\\\text{ amu}$.\\n\\nPlugging in the values, we get:\\n$$c_{\\\\text{rms}} = \\\\sqrt{\\\\frac{3 \\\\times 1.38 \\\\times 10^{-23} \\\\text{ J/K} \\\\times (25 + 273) \\\\text{ K}}{28 \\\\times 1.66 \\\\times 10^{-27} \\\\text{ kg/amu}}} \\\\approx 515 \\\\text{ m s}^{-1}$$\\nNote that we converted the temperature from Celsius to Kelvin and the mass from atomic mass units (amu) to kilograms.\\n\\nFinal Answer: 515 m s$^{-1}$\\n\\nError Reason: The error reason category is 7. Scientific Literacy. The model solution uses the formula for the root-mean-square (rms) speed of gas molecules, which is only valid for ideal gases. However, the given problem does not specify that the gas is ideal, and in fact, air is not an ideal gas. The correct solution uses a more general formula that applies to all gases, including non-ideal gases like air. Therefore, the model's solution is not applicable to the given problem.\\n\\nProblem: What is the number of possible four-letter code words, selecting from the 26 letters in the alphabet?\\n\\nCorrect Solution:\\nThe number of possible four-letter code words, selecting from the 26 letters in the alphabet, in which all four letters are different is\\n$$26 \\\\text{P}_4 = (26)(25)(24)(23) = \\\\frac{26!}{22!} = 358,800$$\\n\\nFinal Answer: 358,800\\n\\nModel Solution:\\nThe number of possible four-letter code words, selecting from the 26 letters in the alphabet is\\n$$26^4 = 456,976$$\\n\\nFinal Answer: 456,976\\n\\nError Reason: The model's error reason category is 9. Logical Reasoning. The model did not consider the condition that all four letters should be different. The correct solution considers this condition and uses the permutation formula to calculate the number of possible four-letter code words.\"}"}
{"id": "bq1JEgioLr", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem: Find the bonding and antibonding H\u00fcckel molecular orbitals for ethene.\\n\\nCorrect Solution:\\nThe equations for \\\\( c_1 \\\\) and \\\\( c_2 \\\\) associated are\\n\\\\[\\n\\\\begin{align*}\\n    c_1 (\\\\alpha - E) + c_2 \\\\beta &= 0 \\\\\\\\\\n    c_1 \\\\beta + c_2 (\\\\alpha - E) &= 0\\n\\\\end{align*}\\n\\\\]\\nFor \\\\( E = \\\\alpha + \\\\beta \\\\), either equation yields \\\\( c_1 = c_2 \\\\). Thus,\\n\\\\[\\n\\\\psi_b = c_1 (2p_z^1 + 2p_z^2)\\n\\\\]\\nThe value of \\\\( c_1 \\\\) can be found by requiring that the wave function be normalized. The normalization condition on \\\\( \\\\psi_\\\\pi \\\\) gives\\n\\\\[\\nc_1^2 (1 + 2S + 1) = 1\\n\\\\]\\nUsing the H\u00fcckel assumption that \\\\( S = 0 \\\\), we find that \\\\( c_1 = 1/\\\\sqrt{2} \\\\). Substituting \\\\( E = \\\\alpha - \\\\beta \\\\) into either of the equations for \\\\( c_1 \\\\) and \\\\( c_2 \\\\) yields \\\\( c_1 = -c_2 \\\\), or\\n\\\\[\\n\\\\psi_a = c_1 (2p_z^1 - 2p_z^2)\\n\\\\]\\nThe normalization condition gives\\n\\\\[\\nc_1^2 (1 - 2S + 1) = 1,\\n\\\\]\\nor \\\\( c_1 = 1/\\\\sqrt{2} \\\\).\\nFinal Answer: 0.70710678\\n\\nModel Solution:\\nTwo bonding and two antibonding H\u00fcckel molecular orbitals for ethene. The unit of the answer is not applicable as it is a qualitative description of the molecular orbitals.\\nFinal Answer: None\\nError Reason:\\nThe error reason category for the model's solution is 6. Abstract Reasoning. The model's solution is incorrect because it did not consider the H\u00fcckel molecular orbital theory, which predicts that there are only two \\\\( \\\\pi \\\\) molecular orbitals for ethene: one bonding and one antibonding. This is due to the fact that ethene has only two \\\\( \\\\pi \\\\) electrons, which can occupy the two \\\\( \\\\pi \\\\) molecular orbitals. Therefore, the model's solution of two bonding and two antibonding molecular orbitals is not possible for ethene.\\n\\nProblem: A living room has floor dimensions of 3.5 m and 4.2 m and a height of 2.4 m. What does the air in the room weigh when the air pressure is 1.0 atm?\\n\\nCorrect Solution:\\nThe air's weight is equal to \\\\( mg \\\\), where \\\\( m \\\\) is its mass. Mass \\\\( m \\\\) is related to the air density \\\\( \\\\rho \\\\) and the air volume \\\\( V \\\\) (\\\\( \\\\rho = \\\\frac{m}{V} \\\\)).\\nCalculation: Putting the two ideas together and taking the density of air at 1.0 atm, we find\\n\\\\[\\nmg = (\\\\rho V) g = 1.21 \\\\text{ kg/m}^3 (3.5 \\\\text{ m} \\\\times 4.2 \\\\text{ m} \\\\times 2.4 \\\\text{ m}) = 9.8 \\\\text{ m^3} = 418 \\\\text{ N}\\n\\\\]\\nFinal Answer: 418 N\\n\\nModel Solution:\\nTo find the weight of the air in the room, we need to first calculate the volume of the room:\\n\\\\( V = lwh = 3.5 \\\\text{ m} \\\\times 4.2 \\\\text{ m} \\\\times 2.4 \\\\text{ m} = 35.28 \\\\text{ m}^3 \\\\). Next, we can use the ideal gas law to find the mass of the air in the room:\\n\\\\[\\nPV = nRT\\n\\\\]\\nwhere \\\\( P \\\\) is the pressure, \\\\( V \\\\) is the volume, \\\\( n \\\\) is the number of moles of gas, \\\\( R \\\\) is the gas constant, and \\\\( T \\\\) is the temperature. Assuming room temperature of 20\u00b0 C (293 K), we can solve for \\\\( n \\\\):\\n\\\\[\\nn = \\\\frac{PV}{RT} = \\\\frac{(1.0 \\\\text{ atm})(35.28 \\\\text{ m}^3)}{(0.0821 \\\\text{ L atm/mol K})(293 \\\\text{ K})} = 1.610 \\\\text{ mol}\\n\\\\]\\nFinally, we can find the weight of the air using the molar mass of air (approximately 28.97 g/mol):\\n\\\\[\\nW = mg = nm\\\\rho g = (1.610 \\\\text{ mol})(0.02897 \\\\text{ kg/mol})(9.81 \\\\text{ m/s}^2) = 416.400 \\\\text{ N}\\n\\\\]\\nFinal Answer: 101640 N\\nError Reason:\\nThe error in the model's solution is that they used the ideal gas law to find the mass of the air in the room, which assumes that the air behaves like an ideal gas. However, the ideal gas law is not applicable in this case because the problem does not provide information about the temperature of the air. The correct approach is to use the density of air at 1.0 atm and the volume of the room to find the mass of the air, as shown in the correct solution. Therefore, the reason for the error in the model's solution is 2. Identification of Assumptions.\"}"}
{"id": "bq1JEgioLr", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem\\nFrom an ordinary deck of playing cards, cards are to be drawn successively at random and without replacement. What is the probability that the third spade appears on the sixth draw?\\n\\nCorrect Solution\\nLet $A$ be the event of two spades in the first five cards drawn, and let $B$ be the event of a spade on the sixth draw. Thus, the probability that we wish to compute is $P(A \\\\cap B)$.\\n\\nIt is reasonable to take $P(A) = \\\\frac{13 \\\\cdot 2 \\\\cdot 39 \\\\cdot 3 \\\\cdot 52}{52 \\\\cdot 51} = 0.274$ and $P(B | A) = 11 \\\\cdot 47 = 0.234$.\\n\\nThe desired probability, $P(A \\\\cap B)$, is the product of those numbers:\\n\\n$$P(A \\\\cap B) = (0.274)(0.234) = 0.064$$\\n\\nFinal Answer: 0.064\\n\\nModel Solution\\nThe probability that the third spade appears on the sixth draw is $\\\\binom{13}{2} \\\\cdot \\\\binom{39}{4} \\\\cdot \\\\binom{52}{6}^{-1} = 78 \\\\cdot 741 / 20,358,520 = 0.258$.\\n\\nFinal Answer: 0.258\\n\\nError Reason\\nThe model's solution is incorrect because it calculates the probability of getting two spades in the first five cards and any spade on the sixth draw, instead of specifically calculating the probability of the third spade appearing on the sixth draw given that two spades have already appeared in the first five draws. Therefore, the error reason category is 4. Causal Reasoning\\n\\nFigure S17.\\nAn example problem is inaccurately solved by error reason 4. Causal Reasoning. The mistaken step is highlighted in red.\\n\\nProblem\\nGas cylinders of $\\\\text{CO}_2$ are sold in terms of weight of $\\\\text{CO}_2$. A cylinder contains 50 lb (22.7 kg) of $\\\\text{CO}_2$. How long can this cylinder be used in an experiment that requires flowing $\\\\text{CO}_2$ at 293 K ($\\\\eta = 146 \\\\mu \\\\text{P}$) through a 1.00-m-long tube (diameter = 0.75 mm) with an input pressure of 1.05 atm and output pressure of 1.00 atm? The flow is measured at the tube output\\n\\nCorrect Solution\\nThe gas flow rate $\\\\Delta V/\\\\Delta t$ is\\n\\n$$\\\\Delta V/\\\\Delta t = \\\\frac{\\\\pi r^2}{16 \\\\eta LP_0} \\\\left( P_2^2 - P_1^2 \\\\right) = 2.76 \\\\times 10^{-6} \\\\text{ m}^3 / \\\\text{s}$$\\n\\nConverting the $\\\\text{CO}_2$ contained in the cylinder to the volume occupied at 298 K and 1 atm pressure, we get\\n\\n$$n_{\\\\text{CO}_2} = 22.7 \\\\text{ kg} / 100.044 \\\\text{ kg mol}^{-1} = 516 \\\\text{ mol}, V = nRT/P = 1 \\\\times 10^4 \\\\text{ L} = 12 \\\\text{ m}^3$$\\n\\nGiven the effective volume of $\\\\text{CO}_2$ contained in the cylinder, the duration over which the cylinder can be used is $12 \\\\text{ m}^3 / 2.76 \\\\times 10^{-6} \\\\text{ m}^3 / \\\\text{s} = 4.49 \\\\times 10^6 \\\\text{ s}$. This time corresponds to roughly 52 days.\\n\\nFinal Answer: 52 days\\n\\nModel Solution\\nThe volume of $\\\\text{CO}_2$ in the cylinder is given by $V = m/\\\\rho = 22.7 \\\\text{ kg} / 1.98 \\\\text{ kg m}^{-3} = 11.5 \\\\text{ m}^3$.\\n\\nThe volume of $\\\\text{CO}_2$ that flows through the tube is given by\\n\\n$$V = \\\\pi r^2 L = \\\\pi \\\\times 0.75 \\\\times 10^{-3} \\\\times 1 \\\\text{ m} = 1.77 \\\\times 10^{-7} \\\\text{ m}^3$$\\n\\nThe time required to flow $V$ volume of $\\\\text{CO}_2$ is given by\\n\\n$$t = V/Q = V P_2/Q m P_0 = 2.08 \\\\times 10^6 \\\\text{ s}$$\\n\\nThe number of seconds in a year is $3.15 \\\\times 10^7 \\\\text{ s}$. Therefore, the cylinder can be used for $2.08 \\\\times 10^6 \\\\text{ s} / 3.15 \\\\times 10^7 \\\\text{ s yr}^{-1} = 0.066 \\\\text{ yr} = 24.1 \\\\text{ days}$.\\n\\nFinal Answer: 24.1 days\\n\\nError Reason\\nThe model's solution is incorrect because it calculates the time required to flow a volume of $\\\\text{CO}_2$ through the tube, but it does not take into account the total volume of $\\\\text{CO}_2$ in the cylinder. The correct solution calculates the total volume of $\\\\text{CO}_2$ in the cylinder and divides it by the flow rate to determine the duration over which the cylinder can be used. Therefore, the error reason category is 5. Problem Deduction Skills\\n\\nFigure S18.\\nAn example problem is inaccurately solved by error reason 5. Problem Deduction Skills. The mistaken step is highlighted in red.\"}"}
{"id": "bq1JEgioLr", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem: An astronaut whose height $h$ is 1.70 m floats \\\"feet down\\\" in an orbiting space shuttle at distance $r = 6.77 \\\\times 10^6$ m away from the center of Earth. What is the difference between the gravitational acceleration at her feet and at her head?\\n\\nCorrect Solution:\\nWe can approximate Earth as a uniform sphere of mass $M_E$. Then, the gravitational acceleration at any distance $r$ from the center of Earth is:\\n\\n$$a_g = \\\\frac{GM_E}{r^2}$$\\n\\nWe might simply apply this equation twice, first with $r = 6.77 \\\\times 10^6$ m for the location of the feet and then with $r = 6.77 \\\\times 10^6$ m + 1.70 m for the location of the head. However, a calculator may give us the same value for $a_g$ twice, and thus a difference of zero, because $h$ is so much smaller than $r$. Here's a more promising approach:\\n\\nBecause we have a differential change $dr$ in $r$ between the astronaut's feet and head, we should differentiate the equation with respect to $r$.\\n\\nCalculations: The differentiation gives us:\\n\\n$$da_g = -\\\\frac{2GM_E}{r^3} \\\\, dr$$\\n\\nwhere $da_g$ is the differential change in the gravitational acceleration due to the differential change $dr$ in $r$.\\n\\nFor the astronaut, $dr = h$ and $r = 6.77 \\\\times 10^6$ m. Substituting data into the equation, we find:\\n\\n$$da_g = -\\\\frac{2 \\\\times 6.67 \\\\times 10^{-11} \\\\times 5.98 \\\\times 10^{24}}{(6.77 \\\\times 10^6)^3} \\\\times (1.70 \\\\text{ m}) = -4.37 \\\\times 10^{-6} \\\\text{ m/s}^2$$\\n\\nFinal Answer: $-4.37 \\\\times 10^{-6} \\\\text{ m/s}^2$\\n\\nModel Solution:\\nWe can use the equation for the gravitational acceleration due to a point mass, given by Eq ($g = \\\\frac{GM}{r^2}$), to find the gravitational acceleration at the astronaut's feet and head. Because the astronaut is in free fall, the difference between these two accelerations is the tidal acceleration, given by Eq ($\\\\Delta g = 2\\\\frac{GMh}{r^3}$).\\n\\nCalculations: The gravitational acceleration at the astronaut's feet is:\\n\\n$$g_{feet} = \\\\frac{GM}{r^2} = \\\\frac{6.67 \\\\times 10^{-11} \\\\times 5.98 \\\\times 10^{24}}{(6.77 \\\\times 10^6)^2} = 9.78 \\\\text{ m/s}^2$$\\n\\nThe gravitational acceleration at the astronaut's head is:\\n\\n$$g_{head} = \\\\frac{GM}{(r+h)^2} = \\\\frac{6.67 \\\\times 10^{-11} \\\\times 5.98 \\\\times 10^{24}}{(6.77 \\\\times 10^6 + 1.70)^2} = 9.77 \\\\text{ m/s}^2$$\\n\\nThe difference between these two accelerations is:\\n\\n$$\\\\Delta g = g_{feet} - g_{head} = 9.78 \\\\text{ m/s}^2 - 9.77 \\\\text{ m/s}^2 = 0.01 \\\\text{ m/s}^2 = 1.0 \\\\times 10^{-5} \\\\text{ m/s}^2$$\\n\\nFinal Answer: $1.0 \\\\times 10^{-5} \\\\text{ m/s}^2$\\n\\nError Reason: The model's solution is incorrect because it uses the equation for the gravitational acceleration due to a point mass, which is not applicable in this case since the Earth is not a point mass. The correct equation to use is the one given in the correct solution, which takes into account the fact that the Earth is a uniform sphere of mass. Therefore, the error reason category is 1. Logical Decomposition and Analysis Skills.\"}"}
{"id": "bq1JEgioLr", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To reduce the likelihood of correct answers being merely guessed from candidates, we choose to mainly include questions with more challenging, free-response answers, rather than multiple-choice questions in previous works (Chen et al., 2023b; Lu et al., 2021a; 2022). In order to facilitate standardized and automated evaluation, we focus on answers that only contain single numerical numbers to avoid ambiguity for the textbook dataset. Further, we convert the answer to floating-point numbers rounded to three decimal places. For example, the answer $\\\\sqrt{2} \\\\pi$ will be converted to the decimal representation of 0.450. We also treat scientific notation as a unit to avoid overflow issues. For example, if the answer is $2.2 \\\\times 10^{-31}$ m, we take 2.2 as the final answer and $10^{-31}$ m as the unit.\\n\\nData preprocessing. We collect each problem from the original textbooks in PDF documents and manually process them into LaTeX documents using an OCR tool Mathpix. The data is manually collected by human annotators using a web-based annotation tool (Lu et al., 2021a), whose user interface is shown in Appendix A.3. All problems are carefully verified by human annotators to ensure that LaTeX documents can be compiled without any syntax errors. For reference, we also provide the original numbers in textbooks. For every problem, we provide the answer in two forms: the numerical value and the corresponding LaTeX expression with mathematical notations retained (e.g., 0.450 and $\\\\sqrt{2} \\\\pi$); the unit of each answer is saved as a separate attribute. The detailed step-by-step solutions are also provided in LaTeX. For problems having multiple answers, we either keep only the first subproblem and discard the remaining subproblems or convert each subproblem into a separate problem.\\n\\n4. Experiments\\nThis section presents the experiments to assess the capabilities of LLMs in scientific problem-solving. We first describe our experimental setup. Subsequently, we evaluate unimodal LLMs on the textbook dataset. Following this, we include additional experiments on the multimodal subset and the closed exam subset, as well as comparisons with other numerical computational tools.\\n\\n4.1. Experiment Setup\\nWe evaluate the textbook dataset on seven unimodal LLMs, which include four proprietary models: Claude2 (claude2) (Anthropic., 2023), GPT-3.5-Turbo (gpt-3.5-turbo) (OpenAI., 2022), GPT-4 (gpt-4), GPT-4-Turbo (gpt-4-turbo) (OpenAI., 2023), along with three open-source models: LLaMA-2-7B (llama-2-7b-chat), LLaMA-2-70B (llama-2-70b-chat) (Touvron et al., 2023b), and Mistral-7B (mistral-7b-instruct) (Jiang et al., 2023).\\n\\nWe consider two prompting strategies, including the Chain-of-Thought (CoT) prompting and prompting to use external tools.\\n\\n- Zero-shot and few-shot learning.\\n  In the zero-shot learning setting, models are not provided with any prior examples, which evaluates their inherent problem-solving capabilities with background knowledge and reasoning abilities. In the few-shot setting, a few examples are given to the models before the test example. This aims to assess their capability to learn new information from the demonstrations and incorporate it into their problem-solving processes.\\n\\n- Prompting-based approaches.\\n  For our experiments, all settings begin with a system prompt that describes the types and categories of questions. Additionally, we utilize a CoT prompting strategy in zero- and few-shot settings.\\n\\n- Tool-augmented approaches.\\n  Given that LLMs are limited in acquiring exact knowledge and performing precise calculations, some recent approaches, such as PAL (Gao et al., 2022) and PoT (Chen et al., 2023a) explore utilizing external tools such as the Python interpreter for program synthesis to enhance the capabilities of solving complex reasoning tasks. In line with these approaches and acknowledging the limitations of LLMs in performing precise calculations, we also include a setting that prompts the model to convert its solution steps in natural language into Python code, aiming to achieve more accurate results for certain computation steps. This tool-augmented approach can only be tested in the few-shot learning setting. We manually construct Python programs that produce the correct answer.\\n\\nImplementation details.\\nWe set temperature to zero for all models to reduce the randomness of the predictions. Few-shot examples, including solutions, are randomly selected from problems within each textbook. When external tools are used, we add a code snippet that translates the solution into specific programming languages in all few-shot examples. The code snippets are verified by human annotators that will produce the correct output. In terms of evaluation metrics, we compare the model outputs with the correct answers, allowing a relative tolerance of 5%. In particular to the exam dataset, the model solutions are graded using the rubrics provided by the instructors. Readers may refer to Appendix C for all prompts and the implementation details for utilizing external tools.\\n\\n4.2. Results and Analysis\\nWe report the model performance in terms of accuracy score for each textbook and an average score over all problems. The results of all LLMs in various settings on the textbook and the exam dataset are summarized in Tables 3 and S2 respectively. We have the following observations.\"}"}
{"id": "bq1JEgioLr", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3. Experimental results in terms of accuracy (%) on the textbook dataset. The best performing score is highlighted in bold and second-best is underlined. The average score is weighted by the number of problems in each textbook.\\n\\n| Model            | Chemistry | Physics | Math | Avg.         |\\n|------------------|-----------|---------|------|--------------|\\n| Zero-Shot Learning | 0.00      | 0.00    | 0.00 | 1.37         |\\n| LLaMA-2-7B       | 0.00      | 1.87    | 0.00 | 0.00         |\\n| Mistral-7B       | 9.35      | 5.13    | 8.82 | 5.48         |\\n| Claude2          | 15.00     | 12.83   | 14.71| 12.33        |\\n| GPT-3.5-Turbo    | 4.67      | 20.51   | 8.82 | 10.96        |\\n| GPT-4            | 45.79     | 28.21   | 26.47| 23.29        |\\n| GPT-4-Turbo      | 57.01     | 41.03   | 35.29| 24.66        |\\n| Zero-Shot Learning + CoT Prompting | 0.00 | 2.56 | 0.00 | 0.00 |\\n| LLaMA-2-7B       | 0.93      | 2.56    | 0.00 | 0.00         |\\n| Mistral-7B       | 6.54      | 10.26   | 2.94 | 2.74         |\\n| Claude2          | 20.56     | 15.38   | 8.82 | 8.23         |\\n| GPT-3.5-Turbo    | 6.54      | 23.08   | 2.94 | 10.20        |\\n| GPT-4            | 28.04     | 43.59   | 14.71| 20.41        |\\n| GPT-4-Turbo      | 60.75     | 35.90   | 29.41| 28.57        |\\n| Few-Shot Learning + CoT Prompting | 1.87 | 5.13 | 2.94 | 5.48 |\\n| LLaMA-2-7B       | 13.10     | 12.83   | 14.71| 12.33        |\\n| Mistral-7B       | 6.54      | 10.26   | 2.94 | 2.74         |\\n| Claude2          | 20.56     | 15.38   | 8.82 | 8.23         |\\n| GPT-3.5-Turbo    | 8.41      | 20.51   | 8.82 | 10.96        |\\n| GPT-4            | 41.12     | 33.33   | 17.65| 16.33        |\\n| GPT-4-Turbo      | 59.81     | 35.90   | 26.47| 18.37        |\\n| Few-Shot Learning + Python | 0.93 | 2.56 | 0.00 | 0.00 |\\n| LLaMA-2-7B       | 0.93      | 7.69    | 2.94 | 0.00         |\\n| Mistral-7B       | 4.67      | 0.00    | 5.88 | 2.04         |\\n| Claude2          | 6.54      | 12.82   | 14.71| 17.81        |\\n| GPT-3.5-Turbo    | 13.08     | 33.33   | 8.82 | 16.33        |\\n| GPT-4            | 57.01     | 38.46   | 44.12| 34.69        |\\n| GPT-4-Turbo      | 32.71     | 33.33   | 17.65| 26.53        |\\n\\nObservation 1. SICIBENCH is complex enough to differentiate among LLMs. Our results show that open-source models such as LLaMA-2 and Mistral are consistently outperformed by their proprietary counterparts across all settings within the textbook dataset. Notably, GPT-4 and GPT-4-Turbo lead in performance by a significant margin. For example, GPT-4-Turbo outperforms Mistral-7B by 34.76% in the zero-shot setting. Additionally, within both LLaMA and GPT series, we observe a clear correlation between increased model capacity (i.e., larger parameter sizes) and improved performance. Therefore, the complexity of SICIBENCH is able to differentiate the performance among different LLMs.\\n\\nObservation 2. SICIBENCH highlights varied efficacy of prompting strategies across LLMs. Our findings suggest that the effectiveness of employing prompting strategies or external computational tools varies significantly among different LLMs. As shown in the table, LLaMA-2-70B shows a marked improvement in the few-shot setting over the zero-shot setting, increasing from 2.41% to 8.40%. Similarly, the performance of GPT-4 is significantly improved when incorporating external tools, with an increase from 30.36% to 43.22%. Meanwhile, the up-to-date model GPT-4-Turbo exhibits superior performance in zero-shot learning settings. However, despite its advanced capabilities demonstrated by its outstanding zero-shot learning performance, it falls short compared to GPT-4 in few-shot learning when leveraging Python for numerical computation. This suggests a potential reduction in its program understanding capabilities. In summary, such findings illustrate SICIBENCH can reveal the nuanced differences in the ability of LLMs to utilize prompting strategies and external tools effectively.\"}"}
{"id": "bq1JEgioLr", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Performance of LLMs on the multimodal subset. GPT-4 models are augmented with image captions and OCR text.\\n\\n4.3. Additional Experiments\\nEvaluation on the multimodal subset. We evaluate two categories of models on problems with visual contexts: (1) GPT-4 (OpenAI., 2023) augmented with image captions from Multimodal Bard (Google, 2023) and OCR texts from EasyOCR (JaidedAI, 2022) and (2) open-source Large Multimodal Models (LMMs): InternLM-XComposer2-VL (Dong et al., 2024), Qwen-VL-Plus (Bai et al., 2023), SPHINX-MoE (Lin et al., 2023), and LLaV A-LLaMA-2-13B (Liu et al., 2023a). For GPT-4, we explore two prompting strategies: Chain-of-Thought (CoT) (Wei et al., 2022) and Program-of-Thoughts (PoT) (Chen et al., 2023a). The results presented in Figure 2 reveal that proprietary models augmented with image captions and OCR-detected text, significantly outperform their open-source counterparts. GPT-4 (PoT) that combines programming capabilities achieves an accuracy of 13.8%, markedly higher than 7.4% obtained by the best open model LLaV A-LLaMA-2-13B. This demonstrates the substantial potential for LLMs to effectively utilize visual contexts in scientific problem solving.\\n\\nEvaluation on the exam subset. To mirror real-world testing conditions with no few-shot examples provided, we evaluate GPT-3.5, GPT-4, Claude, LLaMA-2-7B, and LLaMA-2-70B on the closed exam dataset under zero-shot and zero-shot CoT settings. The experiment results summarized in Table S2 indicate a notable performance advantage of GPT-4, which achieves an averaged score of 57.54%. However, we note that their performance remains significantly lower than human benchmarking. For instance, in the Data Mining course, GPT-4 scores 64.44% and 42.67% in the midterm and final exams, lower than the average student scores of 80.18% and 72.71%, respectively, as reported by the course instructor. The results once again underline the challenging nature of our dataset.\\n\\nComparison with other scientific computing tools. We further utilize another famous scientific computing library Wolfram Language as the external tool and conduct experiments using GPT-3.5, Claude, LLaMA-2-7B, and LLaMA-2-70B. The experiment results reported in Figure S7 show that utilizing Wolfram Language does not help few-shot learning and even results in a deteriorated performance, with a decrease of 6.70% compared to the CoT prompting for Claude2, and a decrease of 6.17% for LLaMA-2-70B. A plausible explanation is the introduction of syntax errors when translating solution steps into the Wolfram Language, which could be a potential direction for improvement. For a detailed error analysis, readers are directed to Appendix C.3.\\n\\n5. Error Analysis of Prompting Strategies\\nConsidering the substantial advancements of current LLMs, an in-depth analysis of the particular skills that are either enhanced or limited under certain settings becomes imperative. Previous works have relied on human labor to annotate error reasons into different categories, which is both expensive and time-consuming (Zhong et al., 2023). In this section, we present an evaluation protocol that automates the classification of error reasons into deficient skills. This time-efficient approach enables large-scale analyses in future research. In order to quantify the impact of each setting on scientific problem-solving, we first define an essential skill set that is required by solving scientific problems. Then, an LLM verifier is employed to automatically classify each incorrectly solved problem based on the absence of a specific skill from the essential skill set. This approach generates error profiles, showcasing a direct comparison of different strategies. This evaluation protocol is summarized in Figure 3.\\n\\nFirstly, we analyze the incorrect solutions made by GPT-3.5 for problems that provide detailed solutions. We hire two college students, who are highly familiar with the problems in our datasets, to annotate the source of the error for each problem, indicating the specific line where the model makes a mistake and why. From 112 such error annotations and with the assistance of GPT-4, we distill these errors into ten essential skills that GPT-3.5 might lack:\\n\\n- Logical decomposition and analysis skills. This ability involves decomposing the problem into smaller, manageable parts, and understanding the relationships between these parts.\\n- Assumption identification. This skill involves the ability to recognize relevant and necessary assumptions in the problem.\\n- Spatial perception. This is important for understanding problems in areas such as Physics and Chemistry, where models need to visualize molecules, forces, fields, etc.\\n- Causal reasoning. This is the ability to understand cause and effect relationships.\\n- Problem deduction skills. This pertains to the ability to infer and deduce potential solutions or underlying principles from the given information in a problem.\"}"}
{"id": "bq1JEgioLr", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3. Pipeline of the evaluation protocol. The evaluation protocol involves analyzing both LLMs and reference (correct) solutions with the assistance of human annotators to identify error reasons. These reasons are then summarized into ten essential scientific problem-solving skills in which LLM may face challenges. Subsequently, a LLM verifier is employed to automatically attribute each incorrectly answered problem to a lack of a specific skill. The resulting error profiles enable the interpretation of the improved skills by certain prompting strategies and the direct comparison of various strategies.\\n\\nFigure 4. Error profiles of GPT-3.5 on the textbook dataset under four settings, which reveal the distribution of their deficiencies in ten essential problem-solving abilities.\\n\\n- **Abstract reasoning.** This skill involves the ability to understand complex concepts that cannot be perceived physically, and to recognize patterns or relationships beyond concrete examples.\\n- **Scientific literacy.** This skill involves a comprehensive understanding of key scientific principles, terminology, and methodologies across a range of disciplines.\\n- **Code conversion skills.** This involves the ability to accurately translate solution steps into different programming languages, like Python or Wolfram Language.\\n- **Logical reasoning.** This is the ability to make a reasoned argument and to identify fallacies or inconsistencies in an argument or set of data.\\n- **Calculation skills.** This involves the ability to accurately carry out mathematical operations and computations.\\n\\nAfter identifying this essential skill set, we assess the performance of the LLMs under different settings to discern the specific problem-solving skills they lack. Given the high cost of human annotations required to attribute the cause of incorrect solutions to specific skill deficiencies, we propose a novel self-critique protocol: we design a specific prompt that outlines these abilities, and employ another LLM to serve as a classifier and determine whether a specific error results from the lack of a particular problem-solving skill. Finally, we ask human annotators to scrutinize the classification results, which results in approximately 20% of incorrectly classified skills being discarded. To be specific, we utilize a GPT-3.5 model as the verifier to determine the reason behind each error and pinpoint the missing skill. The details regarding the specific prompts used are provided in Appendix C.1. This verification process is conducted for four settings, with results represented in bar charts (Figure 4). Additional examples of the evaluation protocol are elaborated in Appendix D.\\n\\nOur findings suggest that there is a lack of a universally effective setting: each configuration only enhances some specific abilities and occasionally even hurts other skills that the original model possesses. First, CoT prompting significantly improves calculation skills in the zero-shot scenario, with 13.6% error rates caused by calculation ability, considerably lower than the 29.0% error rate of the vanilla zero-shot baseline. However, CoT shows limitations in improving other skills, with 32.2% and 25.4% error rates in casual ability and logical decomposition ability in the zero-shot CoT setting, respectively, compared to 18.3% and 18.3% in the zero-shot setting. This contradicts previous claims about universal skill enhancement through zero-shot CoT and carefully-designed few-shot CoT prompts (Wei et al., 2022). An example in Figure S9 shows that the zero-shot learning setting without CoT has generated the correct formula but fails in the calculation steps. In this case, CoT prompting is even unable to use the correct formula as it misinterprets the specific conditions (non-necessity) in the problem. Second, the use of external tools significantly reduces calculation errors compared to the few-shot Cot setting, with a notable decrease from 14.5% to 6.2%. However, the use of external tools can weaken other skills, particularly the code conversion skills, i.e., generating the correct programs for the solution. Third, few-shot learning does not universally improve scientific problem-solving skills, as...\"}"}
{"id": "bq1JEgioLr", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem\\n\\nAt time $t = 0$ a tank contains $Q_0$ lb of salt dissolved in 100 gal of water; see Figure 2.3.1. Assume that water containing $\\\\frac{1}{4}$ lb of salt/gal is entering the tank at a rate of $r$ gal/min and that the well-stirred mixture is draining from the tank at the same rate. Set up the initial value problem that describes this flow process. By finding the amount of salt $Q(t)$ in the tank at any time, and the limiting amount $Q_L$ that is present after a very long time, if $r = 3$ and $Q_0 = 2Q_L$, find the time $T$ after which the salt level is within 2% of $Q_L$.\\n\\nWe assume that salt is neither created nor destroyed in the tank. Therefore variations in the amount of salt are due solely to the flows in and out of the tank. More precisely, the rate of change of salt in the tank, $\\\\frac{dQ}{dt}$, is equal to the rate at which salt is flowing in minus the rate at which it is flowing out. In symbols,\\n\\n$$\\\\frac{dQ}{dt} = \\\\text{rate in} - \\\\text{rate out}$$\\n\\nThe rate at which salt enters the tank is the concentration $\\\\frac{1}{4}$ lb/gal times the flow rate $r$ gal/min, or $\\\\left(\\\\frac{r}{4}\\\\right)$ lb/min. To find the rate at which salt leaves the tank, we need to multiply the concentration of salt in the tank by the rate of outflow, $r$ gal/min. Since the rates of flow in and out are equal, the volume of water in the tank remains constant at 100 gal, and since the mixture is \u201cwell-stirred,\u201d the concentration throughout the tank is the same, namely, $\\\\left(\\\\frac{Q(t)}{100}\\\\right)$ lb/gal. Therefore the rate at which salt leaves the tank is $\\\\left(\\\\frac{rQ(t)}{100}\\\\right)$ lb/min. Thus the differential equation governing this process is\\n\\n$$\\\\frac{dQ}{dt} = \\\\frac{r}{4} - \\\\frac{rQ}{100}$$\\n\\nThe initial condition is $Q(0) = Q_0$.\\n\\nUpon thinking about the problem physically, we might anticipate that eventually the mixture originally in the tank will be essentially replaced by the mixture flowing in, whose concentration is $\\\\frac{1}{4}$ lb/gal. Consequently, we might expect that ultimately the amount of salt in the tank would be very close to 25 lb. We can also find the limiting amount $Q_L = 25$ by setting $\\\\frac{dQ}{dt}$ equal to zero and solving the resulting algebraic equation for $Q_L$. Rewriting it in the standard form for a linear equation, we have\\n\\n$$\\\\frac{dQ}{dt} + \\\\frac{rQ}{100} = \\\\frac{r}{4}$$\\n\\nThus the integrating factor is $e^{rt/100}$ and the general solution is\\n\\n$$Q(t) = 25 + ce^{-rt/100}$$\\n\\nwhere $c$ is an arbitrary constant. To satisfy the initial condition, we must choose $c = Q_0 - 25$. Therefore the solution of the initial value problem is\\n\\n$$Q(t) = 25 + (Q_0 - 25)e^{-rt/100}$$\\n\\nFrom above equations, you can see that $Q(t) \\\\rightarrow 25$ (lb) as $t \\\\rightarrow \\\\infty$, so the limiting value $Q_L$ is 25, confirming our physical intuition. Further, $Q(t)$ approaches the limit more rapidly as $r$ increases. In interpreting the solution, note that the second term on the right side is the portion of the original salt that remains at time $t$, while the first term gives the amount of salt in the tank due to the action of the flow processes. Now suppose that $r = 3$ and $Q_0 = 2Q_L = 50$; then\\n\\n$$Q(t) = 25 + 25e^{-0.03t}$$\\n\\nSince 2% of 25 is 0.5, we wish to find the time $T$ at which $Q(t)$ has the value 25.5. Substituting $t = T$ and $Q = 25.5$ and solving for $T$, we obtain\\n\\n$$T = \\\\frac{\\\\ln 50}{0.03} \\\\approx 130.40766848$$\\n\\nFinal Answer: $T \\\\approx 130.40766848$ (min).\"}"}
{"id": "bq1JEgioLr", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CI: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models\\n\\nC. Experimental Details\\nC.1. Prompts\\n\\nThe APIs of ChatGPT and GPT-4 have three message parameters: SYSTEM, USER, and ASSISTANT. The SYSTEM parameter represents the system prompt, which provides context and instructions to the model. The USER parameter is the training prompt or input provided by the user, and the ASSISTANT parameter contains the output of the model or the response. All system prompts and training prompts used in our experiments are provided below.\\n\\nSystem Prompt for Zero-Shot, Few-Shot, and Chain-of-Thought settings.\\nPlease provide a clear and step-by-step solution for a scientific problem in the categories of Chemistry, Physics, or Mathematics. The problem will specify the unit of measurement, which should not be included in the answer. Express the final answer as a decimal number with three digits after the decimal point. Conclude the answer by stating \\\"The answer is therefore \\\\boxed{[ANSWER]}\\\".\\n\\nSystem Prompt for Few-Shot Learning + Python.\\nPlease provide a clear and step-by-step solution for a scientific problem in the categories of Chemistry, Physics, or Mathematics. The problem will specify the unit of measurement. Please translate the solution steps into Python code and encase the Python code within triple backticks for clarity.\\n\\nSystem Prompt for Few-Shot Learning + Wolfram Language.\\nPlease provide a clear and step-by-step solution for a scientific problem in the categories of Chemistry, Physics, or Mathematics. The problem will specify the unit of measurement. Please translate the solution steps into Wolfram code and encase the Wolfram Language code within triple backticks for clarity.\"}"}
{"id": "bq1JEgioLr", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SCIBENCH: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models\\n\\nSystem Prompt for Evaluation Protocol.\\n\\nExamine the given problem, the correct solution, and the model's solution. Identify the reason for the error in the model's solution based on the following 10 categories:\\n\\n1. Logical Decomposition and Analysis Skills: This ability involves decomposing the problem into smaller, manageable parts, and understanding the relationships between these parts.\\n2. Identification of Assumptions: This skill involves the AI's ability to recognize relevant and necessary assumptions in the problem.\\n3. Spatial Perception: This is important for understanding problems in areas such as physics and chemistry, where you need to visualize molecules, forces, fields, etc.\\n4. Causal Reasoning: This is the ability to understand cause and effect relationships.\\n5. Problem Deduction Skills: This pertains to the ability to infer and deduce potential solutions or underlying principles from the given information in a problem.\\n6. Abstract Reasoning: This skill involves the ability to understand complex concepts that can't be perceived physically, and to recognize patterns or relationships beyond concrete examples.\\n7. Scientific Literacy: This skill involves a comprehensive understanding of key scientific principles, terminology, and methodologies across a range of disciplines.\\n8. Code Conversion Skills: This denotes the ability to accurately translate solution steps into different programming languages, like Python or Wolfram, without syntax errors.\\n9. Logical Reasoning: This is the ability to make a reasoned argument and to identify fallacies or inconsistencies in an argument or set of data.\\n10. Calculation Skills: This involves the ability to accurately carry out mathematical operations and computations.\\n\\nConclude your final error reason category number within $\\\\boxed{}$.\\n\\nTraining Prompt for Zero-Shot Chain-of-Thought.\\n\\nStage 1:\\nInput: \\\\[\\\\text{Input-Question}\\\\] Let's think step by step.\\nOutput: \\\\[\\\\text{Explanation}\\\\]\\n\\nStage 2:\\nInput: \\\\[\\\\text{Input-Question}\\\\] Let's think step by step. \\\\[\\\\text{Explanation}\\\\]. Therefore, the answer is:\\nOutput: \\\\[\\\\text{Answer}\\\\]\\n\\nTraining Prompt for Few-Shot Chain-of-Thought.\\n\\nInput:\\nProblem 1: \\\\[\\\\text{Question 1}\\\\] Explanation for Problem 1: \\\\[\\\\text{Explanation 1}\\\\]. The answer is $\\\\boxed{\\\\text{Answer 1}}$.\\nProblem 2: \\\\[\\\\text{Question 2}\\\\] Explanation for Problem 2: \\\\[\\\\text{Explanation 2}\\\\]. The answer is $\\\\boxed{\\\\text{Answer 2}}$.\\n...\\nProblem n: \\\\[\\\\text{Question n}\\\\] Explanation for Problem n: \\\\[\\\\text{Explanation n}\\\\]. The answer is $\\\\boxed{\\\\text{Answer n}}$.\\nProblem n+1: \\\\[\\\\text{Question n+1}\\\\]\\nOutput: Explanation for Problem n+1: \\\\[\\\\text{Explanation}\\\\]. The answer is $\\\\boxed{\\\\text{Answer}}$. \\n\\n19\"}"}
{"id": "bq1JEgioLr", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SIBENCH: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models\\n\\nTraining Prompt for Few-Shot Python or Wolfram Language.\\n\\nInput:\\nProblem 1: [Question 1]\\nExplanation for Problem 1: [Explanation 1].\\nPython/Wolfram language for Problem 1:\\n```\\n[Python/Wolfram code 1]\\n```\\n\\nProblem 2: [Question 2]\\nExplanation for Problem 2: [Explanation 2].\\nPython/Wolfram language for Problem 2:\\n```\\n[Python/Wolfram code 2]\\n```\\n\\n...\\n\\nProblem n: [Question n]\\nExplanation for Problem n: [Explanation n].\\nPython/Wolfram language for Problem n:\\n```\\n[Python/Wolfram code n]\\n```\\n\\nProblem n+1: [Question n+1]\\n\\nOutput: Explanation for Problem n+1: [explanation].\\nPython/Wolfram language for Problem n+1:\\n```\\n[Python/Wolfram code n+1]\\n```\\n\\nTraining Prompt for Evaluation Protocol.\\n\\nInput: The question is [input-question]. The correct solution is [Correct-Solution]. The model solution is [Model-Solution].\\n\\nOutput: [Error Type]\\n\\nTraining Prompt for Evaluation Protocol in Python or Wolfram Language.\\n\\nInput: The question is [input-question]. The correct solution is [Correct-Solution]. The model solution is [Model-Solution]. The translated program generates the answer as [Program Generated Answer], which is treated as model's output answer.\\n\\nOutput: [Error Type]\\n\\nC.2. Implementation Details\\nAll model output is extracted using boxed{} notation. To prevent any missed extractions, we supplement this process with a manual check. For both Python and Wolfram settings, we extract the programming language with the triple backtick ```, subsequently executing it within the corresponding language. The entirety of our code can be accessed via this repository.\\n\\nC.3. Additional Experiment on Wolfram Language\\nThe experiment results and error analysis for using Wolfram Language as external tools are presented in Figure S7 and Figure S8, compared with using CoT and Python Language. We observe that the use of external tools can weaken other\\n\\n(a) CoT Prompting\\n\\n0\\n5\\n10\\n15\\n20\\nAverage Score (%)\\n3.6\\n8.4\\n15.3\\n12.0\\n\\n(b) Python\\n\\n0\\n5\\n10\\n15\\n20\\n1.2\\n5.1\\n14.9\\n19.9\\n\\n(c) Wolfram Language\\n\\n0\\n5\\n10\\n15\\n20\\n0.0\\n2.2\\n3.8\\n7.9\\n\\nLLaMA-2-7B\\nLLaMA-2-70B\\nClaude2\\nGPT-3.5\\n\\nFigure S7. Comparison between few-shot learning with external tools.\"}"}
{"id": "bq1JEgioLr", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CI: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models\\n\\nlike the Laplace Transform, linear systems, numerical methods, and Fourier series. It culminates with a deep dive into nonlinear equations, partial differential equations, and boundary value problems.\\n\\n\u2022 PROBABILITY AND STATISTICAL INFERENCE (Hogg et al., 1977) (stat) covers probability and statistics, including fundamental concepts, discrete and continuous distributions, bivariate distributions, functions of random variables, and estimation techniques.\\n\\n\u2022 CALCULUS: EARLY TRANSCENDENTALS (Stewart et al., 2012) (calculus) begins with diagnostic tests in foundational topics, and explores functions from multiple perspectives. It comprehensively covers calculus concepts from limits to three-dimensional analytic geometry, incorporating applications in various fields.\\n\\nA.2. Textbook Examples\\n\\nThe textbook examples are provided in Figure S1. The examples from the multimodal subset are provided in Figures S2 to S5.\\n\\nProblem (fund)\\n\\nTwo charged particles are fixed to an x-axis: Particle 1 of charge $q_1 = 2.1 \\\\times 10^{-8}$ C is at position $x = 20$ cm and particle 2 of charge $q_2 = -4.00q_1$ is at position $x = 70$ cm. At what coordinate on the axis (other than at infinity) is the net electric field produced by the two particles equal to zero?\\n\\nAnswer: $-30$ cm\\n\\nProblem (thermo)\\n\\n$N_2O_3$ dissociates according to the equilibrium $N_2O_3(g) \\\\rightleftharpoons NO_2(g) + NO(g)$. At 298 K and one bar pressure, the degree of dissociation defined as the ratio of moles of $NO_2(g)$ or $NO(g)$ to the moles of the reactant assuming no dissociation occurs is $3.5 \\\\times 10^{-3}$. Calculate $\\\\Delta G^\\\\circ_R$ for this reaction.\\n\\nAnswer: $28$ kJ mol$^{-1}$\\n\\nProblem (class)\\n\\nHalley's comet, which passed around the sun early in 1986, moves in a highly elliptical orbit with an eccentricity of 0.967 and a period of 76 years. Calculate its minimum distances from the Sun.\\n\\nAnswer: $8.8 \\\\times 10^{10}$ m\\n\\nProblem (quan)\\n\\nA one-particle, one-dimensional system has $\\\\Psi = a^{-1/2}e^{-|x|}/a$ at $t = 0$, where $a = 1.0000$ nm. At $t = 0$, the particle's position is measured. Find the probability that the measured value is between $x = 0$ and $x = 2$ nm.\\n\\nAnswer: 0.4908\\n\\nProblem (chemmc)\\n\\nOne of the most powerful modern techniques for studying structure is neutron diffraction. This technique involves generating a collimated beam of neutrons at a particular temperature from a high-energy neutron source and is accomplished at several accelerator facilities around the world. If the speed of a neutron is given by $v_n = (3k_BT/m)^{1/2}$, where $m$ is the mass of a neutron, then what temperature is needed so that the neutrons have a de Broglie wavelength of $50$ pm?\\n\\nAnswer: $2500$ K\\n\\nProblem (atkins)\\n\\nThe change in molar internal energy when $CaCO_3$ (s) as calcite converts to another form, aragonite, is $+0.21$ kJ mol$^{-1}$. Calculate the difference between the molar enthalpy and internal energy changes when the pressure is 1.0 bar given that the densities of the polymorphs are $2.71$ g cm$^{-3}$ and $2.93$ g cm$^{-3}$, respectively.\\n\\nAnswer: $-0.28$ Pa m$^3$ mol$^{-1}$\\n\\nProblem (matter)\\n\\nIn an industrial process, nitrogen is heated to $500$ K at a constant volume of $1.000$ m$^3$. The gas enters the container at $300$ K and $100$ atm. The mass of the gas is $92.4$ kg. Use the van der Waals equation to determine the approximate pressure of the gas at its working temperature of $500$ K. For nitrogen, $a = 1.39dm^6atm mol^{-2}$, $b = 0.0391dm^3 mol^{-1}$.\\n\\nAnswer: $140$ atm\\n\\nProblem (calc)\\n\\nA planning engineer for a new alum plant must present some estimates to his company regarding the capacity of a silo designed to contain bauxite ore until it is processed into alum. The ore resembles pink talcum powder and is poured from a conveyor at the top of the silo. The silo is a cylinder $100$ ft high with a radius of $200$ ft. The conveyor carries ore at a rate of $60,000 \\\\pi$ ft$^3$/h and the ore maintains a conical shape whose radius is 1.5 times its height. If, at a certain time $t$, the pile is $60$ ft high, how long will it take for the pile to reach the top of the silo?\\n\\nAnswer: $9.8$ h\\n\\nProblem (stat)\\n\\nIn a study concerning a new treatment of a certain disease, two groups of 25 participants in each were followed for five years. Those in one group took the old treatment and those in the other took the new treatment. The theoretical dropout rate for an individual was $50\\\\%$ in both groups over that 5-year period. Let $X$ be the number that dropped out in the first group and $Y$ the number in the second group. Assuming independence where needed, give the sum that equals the probability that $Y \\\\geq X + 2$. HINT: What is the distribution of $Y - X + 25$?\\n\\nAnswer: 0.3359\\n\\nProblem (diff)\\n\\nNewton's law of cooling states that the temperature of an object changes at a rate proportional to the difference between its temperature and that of its surroundings. Suppose that the temperature of a cup of coffee obeys Newton's law of cooling. If the coffee has a temperature of $200^\\\\circ F$ when freshly poured, and $1$ min later has cooled to $190^\\\\circ F$ in a room at $70^\\\\circ F$, determine when the coffee reaches a temperature of $150^\\\\circ F$.\\n\\nAnswer: $6.07$ min\\n\\nFigure S1. Textbook examples with acronym highlighted in brown.\"}"}
{"id": "bq1JEgioLr", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem\\n\\nThe region $R$ enclosed by the curves $y = x$ and $y = x^2$ is rotated about the $x$-axis. Find the volume of the resulting solid.\\n\\n**Correct Solution**\\n\\nThe curves $y = x$ and $y = x^2$ intersect at the points $(0, 0)$ and $(1, 1)$. The region between them, the solid of rotation, and a cross-section perpendicular to the $x$-axis are shown in the Figure. A cross-section in the plane $\\\\pi$ has the shape of a washer (an annular ring) with inner radius $x^2$ and outer radius $x$, so we find the cross-sectional area by subtracting the area of the inner circle from the area of the outer circle:\\n\\n$$A(x) = \\\\pi x^2 - \\\\pi x^2 = \\\\pi x^2 - x^4$$\\n\\nTherefore we have\\n\\n$$V = \\\\int_0^1 A(x) \\\\, dx = \\\\int_0^1 \\\\pi x^2 - x^4 \\\\, dx = \\\\pi \\\\left[ \\\\frac{x^3}{3} - \\\\frac{x^5}{5} \\\\right]_0^1 = \\\\frac{2}{15} \\\\pi$$\\n\\n**Final Answer:** $\\\\frac{2}{15} \\\\pi$\\n\\nFigure S2. The example from the textbook *Calculus: Early Transcendentals*.\\n\\nA.3. UI Design of the Labeling Tool\\n\\nWe employed a team of seven individuals to gather data from textbooks using an annotation tool. Each individual was responsible for one to two books, encompassing approximately 100 examples. The user interface of the annotation tool is depicted in Figure S6. For subsequent verification, we preserved images of problems and their corresponding answers. To ensure clarity in future references, we have maintained the original sequence of problems as they appear in the textbooks.\\n\\nB. The Exam Dataset\\n\\nThe exam dataset is drawn from the following sources:\\n\\n- *Introduction to Data Mining* provides an introductory survey of data mining, which involves the automatic discovery of patterns, associations, changes, and anomalies in large databases. It explores various application areas of data mining, including bioinformatics, e-commerce, environmental studies, financial markets, multimedia data processing, network monitoring, and social service analysis.\\n\\n- *Fundamentals of Artificial Intelligence* provides an introduction to the core problem-solving and knowledge representation paradigms in artificial intelligence. It covers Lisp programming with regular assignments, as well as topics such as search methods, planning techniques, knowledge structures, natural language processing, expert systems, vision, and parallel architectures.\\n\\n- *Differential Equations* covers various topics in differential equations, including first-order and second-order linear equations with constant coefficients, power series solutions, and linear systems. Students will explore the principles and applications of these mathematical concepts.\\n\\nA detailed statistics of the exam dataset is summarized in Table S1. The experiment results of exam dataset are provided in Table S2.\"}"}
{"id": "bq1JEgioLr", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Problem A 2.00 kg particle moves along an x-axis in one-dimensional motion while a conservative force along that axis acts on it. The potential energy $U(x)$ associated with the force is plotted in the Figure. That is, if the particle were placed at any position between $x = 0$ and $x = 7.00 \\\\text{ m}$, it would have the plotted value of $U$. At $x = 6.5 \\\\text{ m}$, the particle has velocity $\\\\vec{v}_0 = (-4.00 \\\\text{ m/s})\\\\hat{i}$. From the Figure, determine the particle's speed at $x_1 = 4.5 \\\\text{ m}$.\\n\\n**Image Correct Solution**\\n\\nThe particle's kinetic energy is given by $K = \\\\frac{1}{2}mv^2$. Because only a conservative force acts on the particle, the mechanical energy $E_{\\\\text{mec}} = K + U$ is conserved as the particle moves. Therefore, on a plot of $U(x)$, the kinetic energy is equal to the difference between $E_{\\\\text{mec}}$ and $U$.\\n\\n**Calculations:** At $x = 6.5 \\\\text{ m}$, the particle has kinetic energy $K_0 = \\\\frac{1}{2}mv_0^2 = \\\\frac{1}{2}(2.00 \\\\text{ kg})(4.00 \\\\text{ m/s})^2 = 16.0 \\\\text{ J}$. (S1)\\n\\nBecause the potential energy there is $U = 0$, the mechanical energy is $E_{\\\\text{mec}} = K_0 + U_0 = 16.0 \\\\text{ J} + 0 = 16.0 \\\\text{ J}$. This value for $E_{\\\\text{mec}}$ is plotted as a horizontal line in the Figure. From that figure we see that at $x = 4.5 \\\\text{ m}$, the potential energy is $U_1 = 7.0 \\\\text{ J}$. The kinetic energy $K_1$ is the difference between $E_{\\\\text{mec}}$ and $U_1$: $K_1 = E_{\\\\text{mec}} - U_1 = 16.0 \\\\text{ J} - 7.0 \\\\text{ J} = 9.0 \\\\text{ J}$. (S3)\\n\\nBecause $K_1 = \\\\frac{1}{2}mv_1^2$, we find $v_1 = 3.0 \\\\text{ m/s}$.\\n\\n**Final Answer:** 3.0 m/s\\n\\n---\\n\\nTable S1. Statistics of the close exam dataset. We report the number of problem instances in each exam and the ratio of problems in the exam that include detailed solutions. We further report the ratio of problems in different formats, including free-response, multiple-choice, and true-false. For reference, the number in parentheses denotes the grading points assigned to the problems.\\n\\n| Data Mining | Machine Learning | Differential Equations |\\n|-------------|------------------|------------------------|\\n| **Midterm** | **Final**        | **Midterm**            | **Final**           |\\n| # Problems  |                  |                        |                      |\\n| 25 (90)     | 24 (75)          | 12 (56)                | 16 (75)            |\\n| % Solutions |                  |                        |                      |\\n| 56.0% (58)  | 16.7% (19)       | 100.0% (56)            | 31.2% (26)         |\\n| % Free-response |              |                        |                      |\\n| 40.0% (46)  | 33.3% (29)       | 66.7% (38)             | 81.3% (62)        |\\n| % Multiple-choice |               |                        |                      |\\n| 28.0% (28)  | 29.2% (28)       | 33.3% (18)             | 18.7% (13)        |\\n| % True-false |                  |                        |                      |\\n| 32.0% (16)  | 37.5% (18)       | 0.0% (0)               | 0.0% (0)          |\\n\\nTable S2. Experimental results in terms of total scores under zero-shot learning on the exam dataset. The best performing score is highlighted in **bold** and second-best is underlined.\\n\\n| Model  | Setting       | Data Mining | Machine Learning | Differential Equations |\\n|--------|---------------|-------------|------------------|------------------------|\\n| **Midterm** | **Final** | **Midterm** | **Final** | **Midterm** | **Final** | **Midterm** | **Final** |\\n| LLMa-2-7B Zero | 24 / 90 | 14 / 75 | 6 / 56 | 6/ 75 | 5 / 100 | 0 / 100 | 0 / 95 |\\n| Zero + CoT | 18 / 90 | 14 / 75 | 2 / 56 | 10 / 75 | 10 / 100 | 0 / 100 | 10 / 95 |\\n| LLMa-2-70B Zero | 23 / 90 | 18 / 75 | 18 / 56 | 12 / 75 | 20 / 100 | 5 / 100 | 0 / 95 |\\n| Zero + CoT | 31 / 90 | 18 / 75 | 10 / 56 | 11/ 75 | 35 / 100 | 10 / 100 | 0 / 95 |\\n| Claude2 Zero | 37 / 90 | 26 / 75 | 28 / 56 | 35 / 75 | 35 / 100 | 30 / 100 | 20 / 95 |\\n| Zero + CoT | 33 / 90 | 38 / 75 | 22 / 56 | 41 / 75 | 25 / 100 | 15 / 100 | 20 / 95 |\\n| GPT-3.5 Zero | 44 / 90 | 39 / 75 | 16 / 56 | 32 / 75 | 0 / 100 | 45 / 100 | 15 / 95 |\\n| Zero + CoT | 38 / 90 | 33 / 75 | 32 / 56 | 37 / 75 | 28 / 100 | 30 / 100 | 10 / 95 |\\n| GPT-4 Zero | 56 / 90 | 44 / 75 | 30 / 56 | 37 / 75 | 25 / 100 | 80 / 100 | 25 / 95 |\\n| Zero + CoT | 58 / 90 | 32 / 75 | 40 / 56 | 35 / 75 | 50 / 100 | 70 / 100 | 15 / 95 |\"}"}
{"id": "bq1JEgioLr", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem\\n\\nIf the particles in a system all move together, the com moves with them\u2014no trouble there. But what happens when they move in different directions with different accelerations? Here is an example. The three particles in the Figure are initially at rest. Each experiences an external force due to bodies outside the three-particle system. The directions are indicated, and the magnitudes are $F_1 = 6.0 \\\\text{ N}$, $F_2 = 12 \\\\text{ N}$, and $F_3 = 14 \\\\text{ N}$. What is the acceleration of the center of mass of the system?\\n\\nCorrect Solution\\n\\nThe position of the center of mass is marked by a dot in the figure. We can treat the center of mass as if it were a real particle, with a mass equal to the system's total mass $M = 16 \\\\text{ kg}$. We can also treat the three external forces as if they act at the center of mass (Figure b). We can now apply Newton\u2019s second law $\\\\vec{F}_{\\\\text{net}} = m \\\\vec{a}$ to the center of mass, writing $\\\\vec{F}_{\\\\text{net}} = M \\\\vec{a}_{\\\\text{com}}$,\\n\\n(S4) $\\\\vec{F}_1 + \\\\vec{F}_2 + \\\\vec{F}_3 = M \\\\vec{a}_{\\\\text{com}}$,\\n\\n(S5) $\\\\vec{a}_{\\\\text{com}} = \\\\frac{\\\\vec{F}_1 + \\\\vec{F}_2 + \\\\vec{F}_3}{M}$.\\n\\nThe equation tells us that the acceleration $\\\\vec{a}_{\\\\text{com}}$ of the center of mass is in the same direction as the net external force $\\\\vec{F}_{\\\\text{net}}$ on the system (Figure b). Because the particles are initially at rest, the center of mass must also be at rest. As the center of mass then begins to accelerate, it must move off in the common direction of $\\\\vec{a}_{\\\\text{com}}$ and $\\\\vec{F}_{\\\\text{net}}$. We can evaluate the right side of Eq. S6 directly on a vector-capable calculator, or we can rewrite Eq. S6 in component form, find the components of $\\\\vec{a}_{\\\\text{com}}$, and then find $\\\\vec{a}_{\\\\text{com}}$.\\n\\nAlong the $x$ axis, we have $a_{\\\\text{com},x} = \\\\frac{F_1 x + F_2 x + F_3 x}{M} = \\\\frac{-6.0 \\\\text{ N} + (12 \\\\text{ N}) \\\\cos 45^\\\\circ + 14 \\\\text{ N}}{16 \\\\text{ kg}} = 0.103 \\\\text{ m}^2/\\\\text{s}^2$.\\n\\n(S7)\\n\\nAlong the $y$ axis, we have $a_{\\\\text{com},y} = \\\\frac{F_1 y + F_2 y + F_3 y}{M} = 0 + (12 \\\\text{ N}) \\\\sin 45^\\\\circ = 0.530 \\\\text{ m}^2/\\\\text{s}^2$.\\n\\n(S8)\\n\\nFrom these components, we find that $\\\\vec{a}_{\\\\text{com}}$ has the magnitude $a_{\\\\text{com}} = \\\\sqrt{(a_{\\\\text{com},x})^2 + (a_{\\\\text{com},y})^2} = 1.16 \\\\text{ m}^2/\\\\text{s}^2$.\\n\\n(S9)\\n\\nFinal Answer: $1.16 \\\\text{ m}^2/\\\\text{s}^2$.\\n\\nFigure S4. The example from the textbook Fundamentals of Physics.\"}"}
{"id": "bq1JEgioLr", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SCE: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models\\n\\nindicated in the comparison between zero-shot and few-shot CoT settings. The improvement in one skill is offset by the shortcomings in others: although the few-shot CoT setting results in a reduction of 12.8% in errors related to causal reasoning, it also leads to an increase in errors associated with other skills, such as logical decomposition.\\n\\n6. Conclusion\\n\\nThis paper presents SCEL, a college-level benchmark that includes scientific problems from Mathematics, Physics, and Chemistry, as well as exam questions in Computer Science and Mathematics. Our comprehensive evaluation includes a diverse array of Large Language Models (LLMs), spanning both open-source and proprietary models, including unimodal as well as multimodal settings, and employing a variety of prompting strategies. The evaluation protocol we employ serves as a framework for evaluating advanced problem-solving skills of LLMs in scientific domains. The findings of this study highlight that while large language models (LLMs) exhibit impressive performance on introductory mathematical benchmarks, their mastery of problem-solving ability remains weak. These findings underscore the limitations of current LLMs in achieving satisfactory performance, even with the assistance of various tools. We envision that the SCEL benchmark dataset and evaluation protocol presented in this paper could lay a foundation for future research and enable advancements in understanding and enhancing problem-solving capabilities of LLMs.\\n\\nReproducibility Statement\\n\\nTo foster reproducible research, we include all dataset processing and experiment details of SCEL. We detail data processing in Section 3 and provide the UI design of data collection in Appendix A.3. We include all experiment details with LLM prompts in Appendix C. Finally, we make our dataset and code publicly available at this repository.\\n\\nEthical Statement\\n\\nThe questions of SCEL are sourced from science textbooks and exams. We conduct a manual examination of our dataset to ensure the absence of potential sensitive background or ethical concerns. The inclusion of exam questions has been authorized by the instructors of the respective courses.\\n\\nThe purpose of the textbook dataset is solely for academic use. Its collection adheres to the Fair Use Law in the US, where only a certain number of questions from each textbook are selected, ensuring that only a small portion of the textbook is utilized.\\n\\nImpact Statement\\n\\nThe introduction of SCEL represents a significant advancement in the evaluation of Large Language Models (LLMs) for scientific problem-solving tasks. By focusing on collegiate-level problems in mathematics, chemistry, and physics, SCEL addresses a critical gap in existing benchmarks, which have primarily focused on high-school subjects and basic algebraic operations. This development underscores the necessity of developing specialized benchmarks that challenge LLMs with higher complexity problems, thereby pushing the boundaries of the capabilities of LLMs in academic and research settings.\\n\\nWhile the current scope of SCEL encompasses a select group of scientific disciplines, the potential for future extensions is vast. Incorporating additional subjects such as biology, computer science, and engineering could provide a more comprehensive understanding of LLM capabilities across a broader spectrum of scientific knowledge. Moreover, extending the benchmark to social sciences, humanities, and other human-centric domains would be equally beneficial, as these areas often involve nuanced reasoning and interpretation of complex social dynamics and ethical considerations, posing unique challenges that could further enhance the versatility and applicability of LLMs.\\n\\nAcknowledgements\\n\\nThis work was supported by the National Science Foundation (NSF) under Grant Nos. 1829071, 1937599, 2106859, 2119643, 2202693, 2303037, and 2312501; the National Institutes of Health (NIH) under Grant No. U54HG012517; the Defense Advanced Research Projects Agency (DARPA) under Grant No. HR00112490370; NASA; SRC JUMP 2.0 Center; Amazon Research Awards; and Snapchat Gifts.\\n\\nReferences\\n\\nAnthropic. Claude2. https://www.anthropic.com/index/claude-2, 2023. 5\\nArora, D., Singh, H. G., et al. Have llms advanced enough? a challenging problem solving benchmark for large language models. arXiv preprint arXiv:2305.15074, 2023. 1, 3\\nAtkins, P., Atkins, P. W., and de Paula, J. Atkins' physical chemistry. Oxford university press, 2014a. 4, 12\\nAtkins, P., De Paula, J., and Friedman, R. Physical chemistry: quanta, matter, and change. Oxford University Press, USA, 2014b. 2, 4, 12\\nBai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 7\\n\\n9\"}"}
{"id": "bq1JEgioLr", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CI: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models\\n\\nBoyce, W. E., DiPrima, R. C., and Meade, D. B. Elementary differential equations and boundary value problems. John Wiley & Sons, 2021. 4, 12\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. 1\\n\\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y., T., Li, Y., Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. 4\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 1\\n\\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research (TMLR), 2023a. 1, 5, 7\\n\\nChen, W., Yin, M., Ku, M., Lu, P., Wan, E., Ma, X., Xu, J., Xia, T., and Wang, X. Theoremqa: A theorem-driven question answering dataset. arXiv preprint arXiv:2305.12524, 2023b. 3, 5\\n\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 1, 3, 4\\n\\nDong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., Zhang, W., Li, Y., Yan, H., Gao, Y., Zhang, X., Li, W., Li, J., Chen, K., He, C., Zhang, X., Qiao, Y., Lin, D., and Wang, J. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. 7\\n\\nEngel, T. and Reid, P. J. Thermodynamics, statistical thermodynamics, and kinetics. Prentice Hall Upper saddle River, 2010. 4, 12\\n\\nFrieder, S., Pinchetti, L., Griffiths, R.-R., Salvatori, T., Lukasiewicz, T., Petersen, P. C., Chevalier, A., and Berner, J. Mathematical capabilities of chatgpt. arXiv preprint arXiv:2301.13867, 2023. 4\\n\\nFu, Y., Ou, L., Chen, M., Wan, Y., Peng, H., and Khot, T. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. arXiv preprint arXiv:2305.17306, 2023. 3\\n\\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. PAL: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022. 1, 5\\n\\nGao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., Li, H., and Qiao, Y. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. 1\\n\\nGhazal, A., Rabl, T., Hu, M., Raab, F., Poess, M., Crolotte, A., and Jacobsen, H.-A. Bigbench: Towards an industry standard benchmark for big data analytics. In Proceedings of the 2013 ACM SIGMOD international conference on Management of data, pp. 1197\u20131208, 2013. 3\\n\\nGoogle. Bard. https://bard.google.com, 2023. 7\\n\\nGuo, T., Guo, K., Liang, Z., Guo, Z., Chawla, N. V., Wiest, O., Zhang, X., et al. What indeed can gpt models do in chemistry? a comprehensive benchmark on eight tasks. arXiv preprint arXiv:2305.18365, 2023. 3\\n\\nHalliday, D., Resnick, R., and Walker, J. Fundamentals of physics. John Wiley & Sons, 2013. 4, 12\\n\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 1, 3\\n\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 1, 3, 4\\n\\nHogg, R. V., Tanis, E. A., and Zimmerman, D. L. Probability and statistical inference, volume 993. Macmillan New York, 1977. 4, 13\\n\\nHuang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. 1\\n\\nJaidedAI. Easyocr: Ready-to-use ocr. https://github.com/JaidedAI/EasyOCR, 2022. 7\\n\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 5\\n\\nKabir, S., Udo-Imeh, D. N., Kou, B., and Zhang, T. Who answers it better? an in-depth analysis of chatgpt and stack overflow answers to software engineering questions. arXiv preprint arXiv:2308.02312, 2023. 4\\n\\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. 1\\n\\nLevine, I. N., Busch, D. H., and Shull, H. Quantum chemistry, volume 6. Pearson Prentice Hall Upper Saddle River, NJ, 2009. 4, 12\\n\\nLin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. 7\\n\\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. In NeurIPS, 2023a. 7\\n\\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023b. 1\\n\\nLiu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., and Zhang, Y. Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv preprint arXiv:2304.03439, 2023c. 4\"}"}
{"id": "bq1JEgioLr", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SCI-BENCH: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models\\n\\nLu, P., Gong, R., Jiang, S., Qiu, L., Huang, S., Liang, X., and Zhu, S.-C. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021), 2021a.\\n\\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y., Zhang, W., Yu, Z., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021b.\\n\\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022.\\n\\nLu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023a.\\n\\nLu, P., Peng, B., Cheng, H., Galley, M., Chang, K.-W., Wu, Y., N., Zhu, S.-C., and Gao, J. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023b.\\n\\nLu, P., Qiu, L., Chang, K.-W., Wu, Y., N., Zhu, S.-C., Rajpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In International Conference on Learning Representations (ICLR), 2023c.\\n\\nLu, P., Qiu, L., Yu, W., Welleck, S., and Chang, K.-W. A survey of deep learning for mathematical reasoning. In The 61st Annual Meeting of the Association for Computational Linguistics (ACL), 2023d.\\n\\nMcQuarrie, D. A. Quantum chemistry. University Science Books, 2008.\\n\\nMishra, S., Finlayson, M., Lu, P., Tang, L., Welleck, S., Baral, C., Rajpurohit, T., Tafjord, O., Sabharwal, A., Clark, P., et al. Lila: A unified benchmark for mathematical reasoning. In The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.\\n\\nOpenAI. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/chatgpt/. 2022.\\n\\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\n\\nSchick, T., Dwivedi-Yu, J., Dess\u00ec, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\\n\\nStewart, J., Watson, S., and Clegg, D. Calculus: Early transcendentals, 8th. Edition, Brooks/Cole, Cengae learning, 2012.\\n\\nSun, L., Han, Y., Zhao, Z., Ma, D., Shen, Z., Chen, B., Chen, L., and Yu, K. Scieval: A multi-level large language model evaluation benchmark for scientific research. arXiv preprint arXiv:2308.13149, 2023.\\n\\nSuzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n\\nTaylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.\\n\\nThornton, S. T. and Marion, J. B. Classical dynamics of particles and systems. Cengage Learning, 2021.\\n\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\\n\\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\\n\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\\n\\nWelleck, S., Liu, J., Bras, R. L., Hajishirzi, H., Choi, Y., and Cho, K. Naturalproofs: Mathematical theorem proving in natural language. arXiv preprint arXiv:2104.01112, 2021.\\n\\nZhang, D., Hu, Z., Zhoubian, S., Du, Z., Yang, K., Wang, Z., Yue, Y., Dong, Y., and Tang, J. Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. arXiv preprint arXiv:2401.07950, 2024.\\n\\nZhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023a.\\n\\nZhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023b.\\n\\nZhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.\\n\\nZhou, D., Sch\u00e4rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.\"}"}
{"id": "bq1JEgioLr", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A The Textbook Dataset\\n\\nA.1 Textbook Sources\\n\\n\u2022 PHYSICAL CHEMISTRY (ATKINS ET AL., 2014) (atkins) provides an exploration of equilibrium, structure, and reactions, integrating contemporary techniques like nanoscience, spectroscopy, and computational chemistry.\\n\\n\u2022 QUANTUM CHEMISTRY (McQuarrie, 2008) (chemmc) meticulously covers Quantum Mechanics, from foundational principles like blackbody radiation and Heisenberg's Uncertainty Principle to complex topics such as Schr\u00f6dinger equation, quantum mechanical operators, and the application of quantum mechanics in chemical bonding.\\n\\n\u2022 QUANTUM CHEMISTRY (Livine ET AL., 2009) (quan) explores quantum chemistry, providing a detailed understanding of the Schr\u00f6dinger equation, particle behavior in various scenarios, quantum mechanics operators, and other foundational quantum principles. It delves into specific applications like the electronic structure of diatomic and polyatomic molecules, variation methods, perturbation theory, electron spin and its implications in quantum mechanics, as well as various computational methods for molecular quantum mechanics.\\n\\n\u2022 PHYSICAL CHEMISTRY, QUANTUM, MATTER, AND CHANGE (ATKINS ET AL., 2014) (matter) combines physics and mathematics, beginning with basics like differentiation and integration, advancing through quantum mechanics and atomic structure, then exploring thermodynamics, molecular motion, and chemical kinetics. Each section is supplemented with mathematical concepts such as differential equations, vectors, and probability theory.\\n\\n\u2022 CLASSICAL DYNAMICS OF PARTICLE AND SYSTEMS (Hornton & Marion, 2021) (class) initiates with an exploration of fundamental mathematical concepts, discussing scalars, vectors, matrix operations, coordinate transformations, differentiation, and integration of vectors, using these constructs to illustrate concepts like velocity, acceleration, and angular velocity. It then transitions into the realm of Newtonian mechanics, detailing Newton's laws, frames of reference, and the equation of motion for a single particle.\\n\\n\u2022 THERMODYNAMICS, STATISTICAL THERMODYNAMICS, AND KINETICS (Engel & Reid, 2010) (thermo) navigates through thermodynamics' principles, from fundamental concepts to complex laws, further discussing real and ideal gases, solutions, electrochemical cells, and statistical thermodynamics. It concludes with an examination of the kinetic theory of gases, transport phenomena, and chemical kinetics.\\n\\n\u2022 FUNDAMENTALS OF PHYSICS (Halliday ET AL., 2013) (fund) covers undergraduate physics topics, ranging from fundamental concepts like motion and energy to more advanced areas such as quantum physics and nuclear physics.\\n\\n\u2022 ELEMENTARY DIFFERENTIAL EQUATIONS AND BOUNDARY VALUE PROBLEMS (Boye ET AL., 2021) (diff) provides a detailed exploration of differential equations, progressing from basic mathematical models to advanced topics...\"}"}
{"id": "bq1JEgioLr", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"S\\nCI\\nB\\nENCH: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models\\n\\nXiaoxuan Wang\\nZiniu Hu\\nPan Lu\\nYanqiao Zhu\\nJieyu Zhang\\nSatyen Subramaniam\\nArjun R. Loomba\\nShichang Zhang\\nYizhou Sun\\nWei Wang\\n\\nProject Homepage: https://scibench-ucla.github.io\\n\\nAbstract\\nMost existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite S\\nCI\\nB\\nENCH for LLMs. S\\nCI\\nB\\nENCH contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that S\\nCI\\nB\\nENCH will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.\\n\\n1. Introduction\\nRecent advancements in Large Language Models (LLMs) have dramatically expanded the boundaries of artificial intelligence (Brown et al., 2020; Gao et al., 2023; Liu et al., 2023b; OpenAI., 2022; Touvron et al., 2023a; Zhang et al., 2023a;b). They have demonstrated outstanding performance in many mathematical reasoning tasks that are typically considered challenging even for well-educated individuals (Chen et al., 2021; 2023a; Gao et al., 2022; Kojima et al., 2022; Wei et al., 2022). Notably, GPT-4 achieves a remarkable score of 163 out of 170 on the GRE Quantitative Exam, placing it at the 80th percentile ranking (OpenAI., 2023). While the remarkable improvements in these benchmark performances might suggest that LLMs are capable of performing scientific reasoning tasks, we argue that this assertion might be overly optimistic due to the inherent limitations of current benchmarks. Firstly, many existing benchmarks such as ScienceQA (Lu et al., 2022) and GSM8K (Cobbe et al., 2021) only contain problems grounded in grade-level subjects. Although other benchmarks like MATH (Hendrycks et al., 2021) introduce high-school level questions, they primarily focus on math problems. Secondly, recent works like MMLU (Hendrycks et al., 2020), AGIEval (Zhong et al., 2023), and JEEBench (Arora et al., 2023), despite introducing challenging problems that span a wide range of disciplines, only require basic computations\u2014addition, subtraction, multiplication, and exponentiation\u2014which do not adequately assess the depth of reasoning abilities of LLMs for solving scientific problems. Lastly, most of these benchmarks only include textual problems, which omit problems that incorporate visual elements such as figures or diagrams.\\n\\nIn parallel to benchmark developments, many studies propose various prompting strategies aimed at enhancing the reasoning abilities of LLMs in scientific problem solving. A notable example is the Chain-of-Thought (CoT) approach, which instructs LLMs to generate detailed, step-by-step solutions that prompt deeper problem thinking (Huang et al., 2022; Wang et al., 2022; Wei et al., 2022; Zhou et al., 2022). Additionally, other strategies propose to enable LLMs to utilize external tools (Lu et al., 2023b; Schick et al., 2023) that improve the numerical computation capability. However, even these strategic approaches, each with its specific strengths, struggle to fully address complex scientific problems.\"}"}
{"id": "bq1JEgioLr", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem (Using the Planck distribution) Compare the energy output of a black-body radiator (such as an incandescent lamp) at two different wavelengths by calculating the ratio of the energy output at 450 nm (blue light) to that at 700 nm (red light) at 298 K.\\n\\nFinal Answer:\\n\\n\\\\[\\n\\\\text{Final Answer:} \\\\quad \\\\text{therefore} \\\\quad \\\\text{Answer With Insert the data and evaluate this ratio.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Figure 1.} \\\\quad \\\\text{As shown in Figure 1, LLMs with CoT prompts accurately analyze the skills required for such complex tasks.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of college-level scientific problems, including ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Distinct from existing benchmarks, all of the problems are derived from real-world scenarios, necessitating a fine-grained analysis. To align our evaluation with real-world scenarios, we provide a separate, closed dataset that contains a carefully curated dataset of college-level scientific problems, encompassing ten essential skills requisite for successful scientific problem solving, necessitating a fine-grained analysis.}\\n\\\\]\\n\\n\\\\[\\n\\\\text{To mitigate these deficiencies, in this paper, we present a novel college-level} \\\\quad \\\\text{Sci Bench 5 e also includes a carefully trained dataset of"}
{"id": "bq1JEgioLr", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison of **SciBench** with other benchmarks. \u201cAlgebra\u201d refers to high-school level arithmetic computations; \u201cCalculus\u201d involves using integrals and differentials; \u201cStatistics\u201d focuses on applying statistical and probability concepts like bivariate distributions.\\n\\n| Benchmark         | Subject | Calculation | Visual Contexts | Detailed Solutions | Free Response |\\n|-------------------|---------|-------------|-----------------|--------------------|---------------|\\n| ScienceQA         | Math    | \u2713           | \u2713               | \u2713                  | \u2713             |\\n| IconQA            | Math    | \u2713           | \u2713               | \u2713                  | \u2713             |\\n| TabMWP            | Math    | \u2713           | \u2713               | \u2713                  | \u2713             |\\n| GSM8K             | Math    | \u2713           | \u2713               | \u2713                  | \u2713             |\\n| MATH              | Math    | \u2713           | \u2713               | \u2713                  | \u2713             |\\n| LILA              | Math    | \u2713           | \u2713               | \u2713                  | \u2713             |\\n| MMLU              | Math    | \u2713           | \u2713               | \u2713                  | \u2713             |\\n| TheoryQA          | Math    | \u2713           | \u2713               | \u2713                  | \u2713             |\\n| AGIEval           | Math    | \u2713           | \u2713               | \u2713                  | \u2713             |\\n| SciEval           | Math    | \u2713           | \u2713               | \u2713                  | \u2713             |\\n| JEEBench          | Math    | \u2713           | \u2713               | \u2713                  | \u2713             |\\n\\nproblem-solving. These skills include proficiency in domain knowledge, mathematical reasoning, numerical calculation abilities, and comprehension of common sense concepts. Subsequently, we employ an LLM-empowered self-critic approach to automatically classify the lacking skills in the solutions made by the benchmarked LLMs under each experiment configuration. Our analysis finds that (1) although CoT significantly improves the calculation ability, it is less effective in other aspects; (2) prompts with the use of external tools could potentially compromise other fundamental skills; (3) few-shot learning does not universally improve scientific problem-solving skills.\\n\\n2. Related Work\\n\\nRecently, many benchmarks have been proposed to assess the scientific problem-solving skills of LLMs, particularly in mathematical domains (Chen et al., 2023b; Fu et al., 2023; Guo et al., 2023; Hendrycks et al., 2020; Lu et al., 2023c;d; Mishra et al., 2022; Welleck et al., 2021; Zhong et al., 2023). Notable works include GSM8K (Cobbe et al., 2021) including 8.5K grade school math word problems; LILA (Mishra et al., 2022) which extends 20 datasets with task instructions and Python solutions; MATH (Hendrycks et al., 2021), a challenging collection of 12.5K math problems from math competitions; TheoryQA (Chen et al., 2023b), focusing on theorem applications on problem solving; and MathVista (Lu et al., 2023a), which evaluates the mathematical reasoning ability of LLMs in visual contexts. To provide a more holistic evaluation, recent studies have expanded their scope to multiple disciplines: ScienceQA (Lu et al., 2022) introduces a multimodal question-answering dataset with accompanying lecture notes and explanatory annotations. Taylor et al. (2022) provide a set of scientific tasks, including LaTeX equation conversions, domain knowledge probes, citation prediction, and chemical question answering. BIG-Bench (Ghazal et al., 2013) offers a large-scale general-purpose test suite that requires 204 multiple-choice or exact-match tasks, and its extension BIG-Bench Hard (Suzgun et al., 2022) poses challenging CoT prompts. SciEval (Sun et al., 2023) includes a mix of objective and subjective questions across multiple scientific fields to assess understanding, application, and research capabilities. JEEBench (Arora et al., 2023) incorporates pre-engineering-level scientific problems derived from college entrance exams. AGIEval (Zhong et al., 2023) evaluates LLMs on human-centric standardized exams, such as college entrance exams and lawyer qualification tests. Despite their extensive coverage across diverse disciplines, these datasets exhibit certain limitations. Sourced from lower educational level subjects, the majority of them focus on basic arithmetic operations rather than advanced mathematical computations. Furthermore, most of these benchmarks are confined to textual-only problems, omitting problems with visual elements such as graphs or diagrams. These drawbacks result in an incomplete assessment of the analytical and problem-solving skills required to tackle complex scientific problems. In contrast, **SciBench** focuses on college-level scientific problems across a broad spectrum of disciplines including Mathematics, Physics, and Chemistry. It emphasizes on a deep understanding of diverse scientific concepts, challenging LLMs to not only grasp these principles but also to efficiently retrieve and apply relevant knowledge. Furthermore, it demands sophisticated numerical computation skills, including the execution of advanced mathematical operations such as calculus and differential equations, as well as the application of advanced statistical and probability theories. Additionally, we include multimodal problems that necessitate the interpretation and integration of both textual and visual information. A detailed comparison of **SciBench** with some representative works is summarized in Table 1.\"}"}
{"id": "bq1JEgioLr", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Summary of the textbook dataset. We report the number of total problems, percentage with detailed solutions, and percentage with visual elements in columns four to six respectively.\\n\\n| Subject       | Title                                                                 | Acronym | # Problems | % Solutions | % Visual |\\n|---------------|-----------------------------------------------------------------------|---------|------------|-------------|----------|\\n| Physics       | Fundamentals of Physics (Halliday et al., 2013)                        | fund    | 142        | 9.2%        | 43.0%    |\\n|               | Statistical Thermodynamics (Engel & Reid, 2010)                        | thermo  | 83         | 20.5%       | 0.0%     |\\n|               | Classical Dynamics of Particles and Systems (Thornton & Marion, 2021) | class   | 66         | 12.1%       | 4.5%     |\\n| Chemistry     | Quantum Chemistry (Levine et al., 2009)                               | quan    | 41         | 19.5%       | 0.0%     |\\n|               | Quantum Chemistry (McQuarrie, 2008)                                   | chemmc  | 47         | 19.1%       | 0.0%     |\\n|               | Physical Chemistry (Atkins et al., 2014a)                             | atkins  | 122        | 13.9%       | 0.8%     |\\n|               | Physical Chemistry, Quanta, Matter, and Change (Atkins et al., 2014b) | matter  | 59         | 16.9%       | 3.4%     |\\n| Math          | Calculus: Early Transcendentals (Stewart et al., 2012)                | calc    | 161        | 19.3%       | 67.7%    |\\n|               | Probability and Statistical Inference (Hogg et al., 1977)              | stat    | 93         | 21.5%       | 1.1%     |\\n|               | Elementary Differential Equations and Boundary Value Problems (Boyce et al., 2021) | diff    | 55         | 9.1%        | 0.0%     |\\n\\nWhile the aforementioned datasets focus on evaluating LLMs' performance on scientific problem solving tasks, another line of research aims to analyze the diverse capabilities of LLMs more comprehensively. Liu et al. (2023c) assess the reading abilities of LLMs using multiple-choice questions. Frieder et al. (2023) focus on evaluating the mathematical capabilities of LLMs, including those at the college level, but with topics such as functional analysis or topology that differ from those in SCIBENCH, such as differential equations and calculus. Bubeck et al. (2023) explore the comprehensive abilities of GPT-4, but only use up to high-school level mathematical problems such as those in GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). Zhang et al. (2024) develop SciGLM, a scientific language model for collegiate-level problem reasoning, and evaluate its performance across multiple scientific datasets. Kabir et al. (2023) conduct a detailed manual analysis for LLMs. They also provide human-annotated qualitative analysis to assess the capabilities of the models. However, relying on human labor for direct solution analysis can be costly. Our evaluation protocol, based on predefined fundamental problem solving skills, enables automated classification of deficient skills for each incorrectly answered question. This approach enables an affordable, large-scale qualitative analysis of model solutions.\\n\\n3. The SCIBENCH Dataset\\n\\nTo evaluate the capabilities and analyze the limitations of Large Language Models (LLMs) to solve scientific computing problems, we collect a new dataset consisting of college-level textbooks and course exams in a variety of domains. This section details the dataset construction process.\\n\\nData selection criteria. Our dataset aims to improve the previous benchmarks by including more challenging problems. Specifically, the selected dataset should fulfill the following requirements:\\n\\n- **Inclusion of college-level problems.** The chosen problems demand a solid understanding of domain-specific knowledge, adept calculation skills, and the ability to perform complex numerical computations.\\n- **Inclusion of detailed solutions.** To facilitate a thorough analysis of the limitations of LLMs, detailed solutions should be provided as well, which could facilitate a finer-grained examination of the capacity of LLMs to handle complex problem-solving tasks.\\n- **Inclusion of visual elements.** In the real world, many scientific problems require the interpretation and integration of both textual and visual information. The included problems should thus contain visual elements (such as figures) in the contexts.\\n- **Inaccessibility in text formats.** To ensure an unbiased evaluation, questions should not be readily accessible online and cannot be easily extracted or transformed into text. This aims to mitigate any potential information leakage from the exposure of LLMs to pre-existing online question banks, such as those found in standardized tests like the SAT exams.\\n- **Assessment of advanced problem-solving capabilities.** The problems to benchmark should not be confined to basic arithmetic operations like addition and multiplication. Rather, they should enable evaluating the capability of LLMs in performing advanced computations such as calculus and differential equations.\\n\\nAccordingly, to construct the dataset, we select ten textbooks from three scientific fields Physics, Chemistry, and Mathematics that have been extensively used in college courses. We summarize the statistics of this textbook dataset in Table 2 and we use acronyms to refer to each textbook throughout the paper for brevity. Furthermore, in order to simulate real-world evaluation, we compile a closed set of exam questions from college courses from Computer Science and Math departments, including Data Mining, Machine Learning, and Differential Equations. This subset is less likely to be in LLM training data, making it an effective tool for LLM evaluation. Detailed statistics of these exam problems are summarized in Table S1. We refer readers to Appendix A for details on these textbooks and exams.\"}"}
