{"id": "UZlMXUGI6e", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nWeijia Zhang\\nChenlong Yin\\nHao Liu\\nXiaofang Zhou\\nHui Xiong\\n\\nAbstract\\nForecasting of Irregular Multivariate Time Series (IMTS) is critical for numerous areas, such as healthcare, biomechanics, climate science, and astronomy. Despite existing research addressing irregularities in time series through ordinary differential equations, the challenge of modeling correlations between asynchronous IMTS remains underexplored. To bridge this gap, this study proposes Transformable Patching Graph Neural Networks (T-PATCHGNN), which transforms each univariate irregular time series into a series of transformable patches encompassing a varying number of observations with uniform temporal resolution. It seamlessly facilitates local semantics capture and inter-time series correlation modeling while avoiding sequence length explosion in aligned IMTS. Building on the aligned patching outcomes, we then present time-adaptive graph neural networks to model dynamic inter-time series correlation based on a series of learned time-varying adaptive graphs. We demonstrate the remarkable superiority of T-PATCHGNN on a comprehensive IMTS forecasting benchmark we build, which contains four real-world scientific datasets covering healthcare, biomechanics and climate science, and seventeen competitive baselines adapted from relevant research fields.\\n\\n1. Introduction\\n\\nWhile the forecasting of Multivariate Time Series (MTS) has been extensively investigated, most research focuses on regularly sampled and fully observed MTS (Lim & Zohren, 2021). The forecasting challenges associated with Irregular Multivariate Time Series (IMTS), characterized by their irregular sampling intervals and missing data, have received significantly less attention. Indeed, IMTS are prevalent across a wide range of subject areas, such as healthcare, biomechanics, climate science, astronomy, and finance (Rubanova et al., 2019; De Brouwer et al., 2019; Yao et al., 2018; Vio et al., 2013; Engle & Russell, 1998; Zhang et al., 2021a). Accurate forecasting of IMTS serves as the foundation to support various significant activities from making informed decisions to planning with foresight. Unlike regular MTS, the modeling and analysis for IMTS is more challenging due to the inherent irregularity within the series and asynchrony between them (Horn et al., 2020). As illustrated in Figure 1(a), given a set of historical IMTS observations and forecasting queries, the IMTS forecasting problem aims to accurately predict the values in correspondence to these queries. Although a few proactive efforts have been made for IMTS forecasting (Rubanova et al., 2019; De Brouwer et al., 2019; Bilo\u02c7s et al., 2021; Schirmer et al., 2022), these works mainly focus on handling irregularity within the time series based on neural Ordinary Differential Equations (ODEs) (Chen et al., 2018), failing to explicitly consider the crucial correlations between multiple series. Moreover, calculating ODE solvers is computationally expensive due to the numerical integration process, leading to poor efficiency in both training and inference stages (Bilo\u02c7s et al., 2021; Shukla & Marlin, 2020).\\n\\nIt is a non-trivial task for accurate IMTS forecasting, which faces three major challenges. (1) The first challenge is the irregularity in intra-time series dependency modeling. The varying time intervals between adjacent observations disrupt the consistent flow of time series data, making it difficult for classical time series forecasting models (Lim & Zohren, 2021) to accurately capture the underlying temporal dynamics and dependencies (Rubanova et al., 2019; Che et al., 2018). (2) The second challenge is the asynchrony in inter-time series correlation modeling. While there are always considerable correlations between time series of different variables, the observations among IMTS can be significantly misaligned at time due to irregular sampling or missing data. (3) The third challenge is the lack of efficient methods for handling the irregularity and asynchrony simultaneously.\"}"}
{"id": "UZlMXUGI6e", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nAcknowledgements\\nThis work was partially supported by the National Key Research and Development Program of China (No.2023YFF0725001), National Natural Science Foundation of China (Grant No.92370204, No.62102110), Guangzhou-HKUST(GZ) Joint Funding Program (Grant No.2023A03J0008), Education Bureau of Guangzhou Municipality, Guangdong Science and Technology Department, and CCF-Baidu Open Fund.\\n\\nImpact Statement\\nThis paper details efforts to advance time series analysis and its applications across various scientific domains. While our research may lead to many societal impacts, we do not find it necessary to single out any particular consequences for emphasis here.\\n\\nReferences\\nA. Johnson, T. Pollard, L. S. H. L. L.-W. M. F. M. G. B. M. P. S. L. A. C. and Mark, R. G. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1\u20139, 2016.\\nBaytas, I. M., Xiao, C., Zhang, X., Wang, F., Jain, A. K., and Zhou, J. Patient subtyping via time-aware lstm networks. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 65\u201374, 2017.\\nBilo\u02c7s, M., Sommer, J., Rangapuram, S. S., Januschowski, T., and G\u00a8unnemann, S. Neural flows: Efficient alternative to neural odes. Advances in neural information processing systems, 34:21325\u201321337, 2021.\\nCao, D., Wang, Y ., Duan, J., Zhang, C., Zhu, X., Huang, C., Tong, Y ., Xu, B., Bai, J., Tong, J., et al. Spectral tem- poral graph neural network for multivariate time-series forecasting. Advances in neural information processing systems, 33:17766\u201317778, 2020.\\nChai, T. and Draxler, R. R. Root mean square error (rmse) or mean absolute error (mae)?\u2013arguments against avoiding rmse in the literature. Geoscientific model development, pp. 1247\u20131250, 2014.\\nChe, Z., Purushotham, S., Cho, K., Sontag, D., and Liu, Y . Recurrent neural networks for multivariate time series with missing values. Scientific reports, 8(1):6085, 2018.\\nChen, R. T., Rubanova, Y ., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.\\nChen, X., Li, X., Liu, B., and Li, Z. Biased temporal convolution graph network for time series forecasting with missing values. In International Conference on Learning Representations, 2024.\\nCini, A., Marisca, I., and Alippi, C. Filling the g aps: Mul-tivariate time series imputation by graph neural networks. In International Conference on Learning Representations, 2022.\\nDe Brouwer, E., Simm, J., Arany, A., and Moreau, Y . Gru-ode-bayes: Continuous modeling of sporadically-observed time series. Advances in neural information processing systems, 32, 2019.\\nEngle, R. F. and Russell, J. R. Autoregressive conditional duration: a new model for irregularly spaced transaction data. Econometrica, pp. 1127\u20131162, 1998.\\nFan, W., Wang, P., Wang, D., Wang, D., Zhou, Y ., and Fu, Y . Dish-ts: a general paradigm for alleviating distribution shift in time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 7522\u20137529, 2023.\\nHorn, M., Moor, M., Bock, C., Rieck, B., and Borgwardt, K. Set functions for time series. In International Conference on Machine Learning, pp. 4353\u20134363. PMLR, 2020.\\nHuang, Q., Shen, L., Zhang, R., Ding, S., Wang, B., Zhou, Z., and Wang, Y . Crossgnn: Confronting noisy multi-variate time series via cross interaction refinement. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\nIkaro Silva, George Moody, D. S. L. C. and Mark, R. Predicting in-hospital mortality of icu patients: The physionet computing in cardiology challenge 2012. Computing in cardiology, 39:245\u2013248, 2012.\\nKipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2016.\\nLi, Y ., Yu, R., Shahabi, C., and Liu, Y . Diffusion con-volutional recurrent neural network: Data-driven traffic forecasting. In International Conference on Learning Re-presentations, 2018.\\nLi, Z., Li, S., and Yan, X. Time series as images: Vision transformer for irregularly sampled time series. arXiv preprint arXiv:2303.12799, 2023.\\nLim, B. and Zohren, S. Time-series forecasting with deep learning: a survey. Philosophical Transactions of the Royal Society A, 379(2194):20200209, 2021.\"}"}
{"id": "UZlMXUGI6e", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nLiu, F., Liu, H., and Jiang, W. Practical adversarial attacks on spatiotemporal traffic forecasting models. Advances in Neural Information Processing Systems, 35:19035\u201319047, 2022.\\n\\nMarisca, I., Cini, A., and Alippi, C. Learning to reconstruct missing data from spatiotemporal graphs with sparse observations. Advances in Neural Information Processing Systems, 35:32069\u201332082, 2022.\\n\\nMenne, M., W. J. C. and V ose, R. Long-term daily climate records from stations across the contiguous united states.\\n\\nNie, Y ., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2022.\\n\\nObrist, P. A., Gaebelein, C. J., Teller, E. S., Langer, A. W., Grignolo, A., Light, K. C., and McCubbin, J. A. The relationship among heart rate, carotid dp/dt, and blood pressure in humans as a function of the type of stress. Psychophysiology, pp. 102\u2013115, 1978.\\n\\nRubanova, Y ., Chen, R. T., and Duvenaud, D. K. Latent ordinary differential equations for irregularly-sampled time series. Advances in neural information processing systems, 32, 2019.\\n\\nSchirmer, M., Eltayeb, M., Lessmann, S., and Rudolph, M. Modeling irregular time series with continuous recurrent units. In International Conference on Machine Learning, pp. 19388\u201319405. PMLR, 2022.\\n\\nShukla, S. N. and Marlin, B. Interpolation-prediction networks for irregularly sampled time series. In International Conference on Learning Representations, 2018.\\n\\nShukla, S. N. and Marlin, B. Multi-time attention networks for irregularly sampled time series. In International Conference on Learning Representations, 2021.\\n\\nShukla, S. N. and Marlin, B. M. A survey on principles, models and methods for learning from irregularly sampled time series. arXiv preprint arXiv:2012.00168, 2020.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nVio, R., Diaz-Trigo, M., and Andreani, P. Irregular time series in astronomy and the use of the lomb\u2013scargle periodogram. Astronomy and Computing, 1:5\u201316, 2013.\\n\\nWu, H., Hu, T., Liu, Y ., Zhou, H., Wang, J., and Long, M. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The Eleventh International Conference on Learning Representations, 2022.\\n\\nWu, Z., Pan, S., Long, G., Jiang, J., and Zhang, C. Graph wavenet for deep spatial-temporal graph modeling. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pp. 1907\u20131913, 2019.\\n\\nWu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Philip, S. Y . A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4\u201324, 2020a.\\n\\nWu, Z., Pan, S., Long, G., Jiang, J., Chang, X., and Zhang, C. Connecting the dots: Multivariate time series forecasting with graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 753\u2013763, 2020b.\\n\\nYao, Z.-J., Bi, J., and Chen, Y .-X. Applying deep learning to individual and community health monitoring data: A survey. Machine Intelligence Research, 15:643\u2013655, 2018.\\n\\nYi, K., Zhang, Q., Fan, W., He, H., Hu, L., Wang, P., An, N., Cao, L., and Niu, Z. Fouriergnn: Rethinking multivariate time series forecasting from a pure graph perspective. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n\\nYu, B., Yin, H., and Zhu, Z. Spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pp. 3634\u20133640, 2018.\\n\\nZeng, A., Chen, M., Zhang, L., and Xu, Q. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp. 11121\u201311128, 2023.\\n\\nZhang, J., Zheng, S., Cao, W., Bian, J., and Li, J. Warp-former: A multi-scale modeling approach for irregular clinical time series. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 3273\u20133285, 2023a.\\n\\nZhang, W., Liu, H., Zha, L., Zhu, H., Liu, J., Dou, D., and Xiong, H. Mugrep: A multi-task hierarchical graph representation learning framework for real estate appraisal. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 3937\u20133947, 2021a.\\n\\nZhang, W., Zhang, L., Han, J., Liu, H., Zhou, J., Mei, Y ., and Xiong, H. Irregular traffic time series forecasting based on asynchronous spatio-temporal graph convolutional network. arXiv preprint arXiv:2308.16818, 2023b.\"}"}
{"id": "UZlMXUGI6e", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nZhang, X., Zeman, M., Tsiligkaridis, T., and Zitnik, M.\\n\\nGraph-guided network for irregularly sampled multivariate time series. In International Conference on Learning Representations, 2021b.\\n\\nZhang, Y. and Yan, J. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In The Eleventh International Conference on Learning Representations, 2022.\\n\\nZhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun, M. Graph neural networks: A review of methods and applications. AI open, 1:57\u201381, 2020.\"}"}
{"id": "UZlMXUGI6e", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nTable 4: Performance of varying observation and forecast horizons.\\n\\n| Algorithm       | History=3h, Forecast=45h | History=12h, Forecast=36h | History=36h, Forecast=12h | History=45h, Forecast=3h |\\n|-----------------|---------------------------|----------------------------|----------------------------|---------------------------|\\n| MSE $\\\\times 10^{-3}$ |                            |                            |                            |                            |\\n| MAE $\\\\times 10^{-2}$ |                            |                            |                            |                            |\\n| DLinear         | 51.82                      | 17.13                      | 43.56                      | 15.73                     |\\n| TimesNet        | 57.30                      | 10.70                      | 24.95                      | 7.62                      |\\n| PatchTST        | 42.18                      | 13.67                      | 18.56                      | 7.80                      |\\n| Crossformer     | 9.48                       | 5.96                       | 8.57                       | 5.70                      |\\n| Graph Wavenet   | 9.43                       | 5.86                       | 7.23                       | 4.71                      |\\n| MTGNN           | 9.83                       | 5.95                       | 7.48                       | 5.01                      |\\n| StemGNN         | 8.70                       | 5.37                       | 7.46                       | 6.69                      |\\n| CrossGNN        | 10.44                      | 6.56                       | 7.97                       | 5.47                      |\\n| FourierGNN      | 9.59                       | 5.61                       | 7.95                       | 4.99                      |\\n| GRU-D           | 8.18                       |                            |                            |                            |\\n| SeFT            | 9.78                       |                            |                            |                            |\\n| RainDrop        | 10.47                      |                            |                            |                            |\\n| Warpformer      |                            |                            |                            |                            |\\n| mTAND           |                            |                            |                            |                            |\\n| Latent-ODE      |                            |                            |                            |                            |\\n| CRU             |                            |                            |                            |                            |\\n| Neural Flow     |                            |                            |                            |                            |\\n| T-PATCH GNN     |                            |                            |                            |                            |\\n\\nA. Additional Experiment\\n\\nA.1. Varying Observation and Forecast Horizons\\n\\nTable 4 presents the model\u2019s performance on longer- (forecast next 36 / 45 hours using historical 12 / 3 hours) and shorter-horizon (forecast next 12 / 3 hours using historical 36 / 45 hours) forecasting on PhysioNet. We can observe T-PATCH GNN achieves the consistently best performance for different forecasting horizons. Moreover, our model showcases larger superiority over baselines to process longer historical windows (e.g., 24h, 36h, and 45h), which is probably attributed to the transformable patching to facilitate long-range time series dependencies modeling. Furthermore, it shows that the performance of these algorithms tends to be closer when the historical observed window becomes very short (e.g., 3h). This may be because a shorter historical window contains less semantics and is thus easier to capture by different models. While most models perform better when forecasting horizon window reduction, the ODE-based models yet perform worse when using a longer history to predict shorter horizons. This may be because the too-long sequence and less labeled data degrade the performance of this type of method.\\n\\nA.2. Model Scalability with Increased Variables\\n\\nTo further analyze the impact of pre-alignment representation on scalability, we created a synthetic dataset designed to flexibly test the influence of increased variables and report the average training time per epoch and inference time per instance. In this test, we generated 1,000 IMTS instances with multiple variables. Each variable comprises an average of 10 observations, randomly distributed at different timestamps (in seconds) within a day. To conserve memory and enable testing with larger variables, we set the batch size to 1 and restricted the hidden dimension to 2. As illustrated in Table 5, we observe that the sequence length explosion problem deteriorates as the number of variables increases. This leads to pronounced scalability issues affecting both computational efficiency and memory usage for the pre-alignment methods, i.e., GRU-D, Warpformer and mTAND. However, our T-PATCH GNN with transformable patching effectively mitigates this problem, maintaining high training and inference efficiency despite the increased variables.\"}"}
{"id": "UZlMXUGI6e", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nThe number of layers and heads for the Transformer is 2. The learning rate is $1 \\\\times 10^{-3}$. We use the official implementation at https://github.com/mims-harvard/Raindrop.\\n\\nWarpformer (Zhang et al., 2023a) is a Transformer-based model that adopts a tailored input representation, explicitly encapsulating both the within-series irregularities and inter-series variations. It further incorporates a warping module to flexibly synchronize irregular time series at a predefined scale, along with a custom-designed attention module for advanced representation learning. We use the following setting in our experiment: The number of warp is 0-0.2-1. The number of the heads is 1 and layers is 2. The learning rate is $1 \\\\times 10^{-3}$. We use the official implementation at https://github.com/imJiawen/Warpformer.\\n\\nA.6.3. Models for Irregular Time Series Interpolation and Forecasting\\n\\nmTAND (Shukla & Marlin, 2021) is an IMTS interpolation model that can be easily applied to forecasting tasks by only replacing the queries for interpolation with forecasting. It learns embeddings for numerical values corresponding to continuous time steps and generates fixed-length representations for variable-length sequential data using an attention mechanism. We use the following setting in our experiment: The encoder and the decoder is mTAND-rnn, the k-iwae is 5, the std is 0.01, the number of the ref-points is 64. The learning rate is $1 \\\\times 10^{-3}$. We use the official implementation at https://github.com/reml-lab/mTAN.\\n\\nLatent-ODE (Rubanova et al., 2019) is an ODE-based model that improves RNNs with continuous-time hidden state dynamics specified by neural ODEs. We use the following setting in our experiment: The number of the rec-layers and gen-layers is 3 for PhysioNet and USHCN and 1 for MIMIC and Human Activity. The learning rate is $1 \\\\times 10^{-3}$. We use the official implementation at https://github.com/YuliaRubanova/latent-ode.\\n\\nCRU (Schirmer et al., 2022) integrates the Kalman Filter with an encoder-decoder architecture to facilitate updates of the latent states in ODEs. We use the following setting in our experiment: The scaling factor of timestamps for numerical stability is 0.2 for PhysioNet and MIMIC and 0.3 for USHCN and Human Activity. The variance activation function in encoder is square and the variance activation function in decoder is exp. The activation function for transition net is relu. The number of bias is 15 for USHCN and Human Activity and 20 for PhysioNet and MIMIC. The bandwidth is 3 for USHCN and Human Activity and 10 for PhysioNet and MIMIC. The learning rate is $1 \\\\times 10^{-3}$. We use the official implementation at https://github.com/boschresearch/Continuous-Recurrent-Units.\\n\\nNeural Flow (Bilo\u0161 et al., 2021) models the solution curves of ODEs through neural networks. We use the following setting in our experiment: The number of the flow-layers is 4 for MIMIC and 2 for other datasets. The number of the hidden-layers is 2 for MIMIC and 3 for other datasets. The rec-dims is 20 for MIMIC and 40 for other datasets. The flow model is GRU for MIMIC and coupling for other datasets. The time-net is TimeTanh for MIMIC and TimeLinear for other datasets. The time-hidden-dim is 8. The activation is ReLU for MIMIC and Tanh for other datasets. The learning rate is $1 \\\\times 10^{-3}$. The decay of the learning rate is 0.33 for MIMIC and 0.5 for other datasets. We use the official implementation at https://github.com/mbilos/neural-flows-experiments.\"}"}
{"id": "UZlMXUGI6e", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3. Parameter Sensitivity\\n\\nFigure 5 displays the effect of different intra- and inter-time series modeling blocks. We observe stacking multiple $K$ blocks always has the potential to achieve better performance. However, it also costs more expensive computational overheads. Therefore, we choose $K = 1$ for the major experiments.\\n\\nFigure 6 shows the effect of dimensions $D_t$ & $D_g$ of variable&time embeddings. It indicates relatively small sizes (e.g., 10 or 15) usually perform better. A too-large size may lead to performance collapse due to the potential data sparsity issues for some variables to learn semantic embedding.\\n\\nFigure 7 reports the effect of different hidden dimension $D$. We find setting the hidden dimension to 32 for smaller datasets (e.g., Human Activity and USHCN) and 64 for larger datasets (e.g., PhysioNet) would be a good choice. However, this is not absolute. While MIMIC is a large-scale dataset, its best setting is 32 due to a notable sparsity in its measurements. The choice of hidden dimension should comprehensively consider the number of training data and the sparsity of measurements.\"}"}
{"id": "UZlMXUGI6e", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\n(a) Patch 1 (0-7h)\\n(b) Patch 2 (8-15h)\\n(c) Patch 3 (16-23h)\\n\\nFigure 8: Adjacent matrices of time-adaptive graphs learned from PhysioNet.\\n\\n(a) Patch 1 (Jan, Feb)\\n(b) Patch 2 (Mar, Apr)\\n(c) Patch 3 (May, Jun)\\n(d) Patch 4 (Jul, Aug)\\n(e) Patch 5 (Sep, Oct)\\n(f) Patch 6 (Nov, Dec)\\n(g) Patch 7 (Jan, Feb)\\n(h) Patch 8 (Mar, Apr)\\n(i) Patch 9 (May, Jun)\\n(j) Patch 10 (Jul, Aug)\\n(k) Patch 11 (Sep, Oct)\\n(l) Patch 12 (Nov, Dec)\\n\\nFigure 9: Adjacent matrices of time-adaptive graphs learned from USHCN.\\n\\nA.4. Visualization on Learned Adaptive Graphs\\n\\nFigure 8 and Figure 9 provide visualizations on the learned adjacent matrices of adaptive graph structures to analyze how they work in different contexts. Overall, we find the learned adjacent matrices are usually sparse, which implies our model attempts to learn the real correlations from data instead of simply aggregating these variables. Moreover, we observe remarked and insightful time-varying correlations learned from dynamic contexts (e.g., USHCN), further underscoring the necessity of learning time-adaptive graph structures.\\n\\nFor PhysioNet, as illustrated in Figure 8, we observe our model can learn insightful correlations between different indicator variables of patients. For example, the adjacent matrix indicates that heart rate (HR) and respiratory rate (RespRate) are highly correlated because they usually simultaneously increase during physical activity or stress to meet the body's higher demand for oxygen. A high correlation is also displayed between RespRate and body temperature (Temp) as they usually increase together in many situations like when the body is fighting an infection. In addition, some underlying and more complex correlations may be automatically discovered from data through the graph structure learning process. For instance, it indicates that there is a high correlation between blood urea nitrogen (BUN) and Lactate levels. Typically, BUN levels reflect renal function. Impaired renal function can lead to reduced clearance of both urea and lactate, and thus cause lactate to accumulate.\\n\\nThe cases from USHCN are illustrated in Figure 9, where the learned graph structures exhibit pronounced seasonal variation.\"}"}
{"id": "UZlMXUGI6e", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nFor instance, in winter (Patch 1 (Jan, Feb)), snowfall (SNOW) markedly influences maximum temperature forecasts (TMAX). This effect gradually wanes from winter to summer, correlating with the reduction or absence of snowfall. As seasons cycle from summer back to winter, this influence progressively strengthens. A similar trend is also showcased between TMAX and snow depth (SNWD). Furthermore, we discover that these correlations exhibit cyclical changes on an annual scale. This highlights our model's ability to learn the temporal dynamics of variable correlations within data.\\n\\nA.5. Description on Datasets\\n\\nPhysioNet contains 12,000 IMTS corresponding to different patients, where each consists of a total of 41 clinical signal variables irregularly collected during the initial 48 hours following the patient\u2019s admission to the ICU. For each IMTS, we use the first 24 hours as the observed data to predict the queried values in the next 24 hours.\\n\\nMIMIC is a widely accessible clinical database that houses electronic health records of patients in critical care. Following the pre-processing provided by (Bilo\u02c7s et al., 2021), we obtain 23,457 patients\u2019 IMTS collected from the first 48 hours after the patient\u2019s admission, and each with 96 variables. Similar to the PhysioNet, we utilize the initial 24-hour period as the observed data to forecast the target values for the subsequent 24-hour time frame.\\n\\nHuman Activity comprises 12 variables consisting of irregularly measured 3D positional records of four different sensors worn in the human left ankle, right ankle, belt, and chest. The dataset is gathered from five individuals executing a diverse range of activities such as walking, sitting, lying down, standing, and others. To better align with the requirements of realistic forecasting scenarios, we chunk the original time series to obtain a total of 5,400 IMTS, each of which contains 4,000 milliseconds span, and we leverage the first 3,000 milliseconds as the observed data to predict the positional value of sensors in the next 1,000 milliseconds.\\n\\nGiven that data missing is a frequent occurrence in climate research possibly due to sensor malfunctions, measurement errors, or data acquisition issues, we follow the previous works (De Brouwer et al., 2019; Schirmer et al., 2022) and have chosen the USHCN as one of our evaluation datasets. USHCN encompasses daily measurements for 5 climate variables spanning over 150 years collected by widespread meteorological stations throughout the United States. We follow the pre-processing adopted by (De Brouwer et al., 2019) to obtain 1,114 stations and four-year observational periods between 1996 and 2000. To meet realistic forecasting requirements, we chunk the data to acquire a total of 26,736 IMTS, each of which uses the previous 24 months\u2019 climate data to forecast the following month\u2019s climate conditions.\\n\\nA.6. Baseline Details\\n\\nWe incorporate seventeen relevant baselines for a fair comparison, covering the SOTA models from MTS forecasting, and IMTS classification, interpolation, and forecasting. We carefully search the key hyper-parameters of these models around their recommended setups. For a fair comparison across all models, we standardize the hidden dimensions to 64 for PhysioNet and MIMIC, and 32 for Human Activity and USHCN. We select a batch size of 192 for USHCN and 32 for the other datasets. The Adam optimizer is used for training, with early stopping implemented if there is no reduction in validation loss after 10 epochs.\\n\\nA.6.1. Models for MTS Forecasting\\n\\nFor MTS forecasting models, we input sequences after canonical pre-alignment and incorporate the observed time, mask information, and forecasting queries as additional features into these models.\\n\\nDLinear (Zeng et al., 2023) decomposes time series into trend series and remainder series, subsequently employing two single-layer linear networks to model each of these sequences to accomplish the forecasting task. We use the following setting in our experiment: The window size of moving average is 25. The learning rate is $1 \\\\times 10^{-4}$.\\n\\nTimesNet (Wu et al., 2022) disassembles complex sequential changes into different periods through a modular structure, and achieves unified modeling of both inter-period and intra-period representation by transforming the original one-dimensional time series into a two-dimensional space to capture cross-time dependency for forecasting. We use the following setting in...\"}"}
{"id": "UZlMXUGI6e", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nOur experiment: The number of top-k for period is 5. The number of the encoder layers is 2. The learning rate is $1 \\\\times 10^{-4}$.\\n\\nPatchTST (Nie et al., 2022) is a Transformer-based model using Patch and Channel Independence to capture cross-time dependency for forecasting. We use the following setting in our experiment: The number of multi-heads is 2. The length of the patch is 16. The length of the stride is 8. The number of the encoder layers is 1. The learning rate is $1 \\\\times 10^{-4}$.\\n\\nCrossformer (Zhang & Yan, 2022) is a Transformer-based model using Cross-Time Attention and Cross-Dimension Attention to capture cross-dimension dependency and cross-time dependency for forecasting. We use the following setting in our experiment: The length of the segment is 12. The size of the window is 2. The number of the encoder layers is 1 for the Human Activity and 2 for other datasets. The number of the multi-heads is 3 for the Human Activity and 8 for other datasets. The learning rate is $1 \\\\times 10^{-3}$.\\n\\nWe use the implementation provided by https://github.com/thuml/TimesNet to reproduce the above four baseline models.\\n\\nGraphWavenet (Wu et al., 2019) leverages the self-adaptive adjacency matrix and diffusion convolution to capture the cross-dimension dependency and uses gated mechanism and dilated casual convolution to capture the cross-time dependency for forecasting. We use the following setting in our experiment: The dilation exponential is 3 for MIMIC and 2 for other datasets. The size of the kernel is 5 for PhysioNet and Human Activity, 7 for USHCN and 9 for MIMIC. The number of the blocks is 2 for Human Activity and 3 for other datasets. The number of convolution layers is 3 for MIMIC and 4 for other datasets. The learning rate is $1 \\\\times 10^{-3}$. We use the official implementation at https://github.com/nnzhan/Graph-WaveNet.\\n\\nMTGNN (Wu et al., 2020b) integrates graph convolutional networks and temporal convolutional networks to capture cross-dimensional relationships and cross-temporal dependencies in a direct and explicit manner. We use the following setting in our experiment: The dilation exponential is 2 for PhysioNet and Human Activity, 3 for USHCN and 4 for MIMIC. The size of the kernel is 7. The number of the convolution layers is 4 for MIMIC and USHCN, 5 for PhysioNet and Human Activity. The size of the subgraph is 5 for USHCN, 12 for Human Activity and 20 for PhysioNet and MIMIC. The learning rate is $1 \\\\times 10^{-3}$. We use the official implementation at https://github.com/nnzhan/MTGNN.\\n\\nStemGNN (Cao et al., 2020) transfers the spatiotemporal domain to the frequency domain through discrete Fourier transform and graph Fourier transform while capturing spatiotemporal dependencies in the frequency domain. We use the following setting in our experiment: The number of layers is 5 and the learning rate is $1 \\\\times 10^{-4}$. We use the official implementation at https://github.com/microsoft/StemGNN.\\n\\nCrossGNN (Huang et al., 2023) uses adaptive multi-scale identifier to construct multi-scale time series with different noise levels, subsequently utilize cross-scale GNN to capture the cross-time dependency and cross-variable GNN to capture the cross-dimension dependency for forecasting. We use the following setting in our experiment: The dimension of the scale vector and variable vector is 10. The scale number is 4. The number of cross-scale neighbors is 10. The learning rate is $1 \\\\times 10^{-3}$. We use the official implementation at https://github.com/hqh0728/CrossGNN.\\n\\nFourierGNN (Yi et al., 2023) initially constructs a hypervariate graph and transforms features into the Fourier space. Subsequently, it stacks Fourier graph operators in the Fourier domain and finally maps the convolved results back to the original feature space for forecasting. We use the following setting in our experiment: The number of frequency is 1. The scale is 0.02. The hidden size factor is 1. The sparsity threshold is 0.01. The learning rate is $1 \\\\times 10^{-3}$. We use the official implementation at https://github.com/aikunyi/FourierGNN.\\n\\nA.6.2. Models for Irregular Time Series Classification\\n\\nTo adapt these classification baseline models for forecasting tasks, we substitute their classification output layer with a MLP-based forecasting output layer that is the same as us.\\n\\nGRU-D (Che et al., 2018) is a GRU-based model using time decay and missing data imputation strategies to handle irregularly sampled time series. We set learning rate to $1 \\\\times 10^{-3}$ and follows the implementation at https://github.com/zhiyongc/GRU-D.\\n\\nSEFT (Horn et al., 2020) converts the time series into a set encoding, then using set functions to model them. We use the following setting in our experiment: The number of layers is 2. The learning rate is $1 \\\\times 10^{-3}$. We use the implementation provided by https://github.com/mims-harvard/Raindrop.\\n\\nRainDrop (Zhang et al., 2021b) employs neural message passing and temporal self-attention to model the dependencies among sensors, considering cross-sample shared relationships between sensors and adaptively estimates unaligned observations based on neighboring measurements. We use the following setting in our experiment: The dimension of the...\"}"}
{"id": "UZlMXUGI6e", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nFigure 1: (a) Irregular multivariate time series forecasting problem, where $v_1$, $v_2$, and $v_3$ represent three different variables. (b) Canonical pre-alignment representation causes the average sequence length to increase from 5 to 15, an explosive growth proportional to the variable count. This asynchrony complicates direct comparisons and correlations at specific time points and potentially obscures or distorts the actual relationships between the time series, resulting in a significant challenge to model inter-time series correlations (Zhang et al., 2021b). (3) The last challenge is the sequence length explosion with the increase of variables. As shown in Figure 1(b), to facilitate IMTS modeling, current studies typically represent IMTS in a time-aligned format which involves extending each univariate irregular time series to a uniform length corresponding to the count of all unique timestamps among IMTS observations (Che et al., 2018). However, such a canonical pre-alignment representation may lead to the sequence length explosively growing proportional to the addition of variables, which raises severe scalability concerns on both computation and memory overhead when encountering a large number of variables. To this end, we propose a Transformable Patching Graph Neural Networks (T-PATCH GNN) approach for IMTS forecasting.\\n\\nT-PATCH GNN initially transforms each univariate irregular time series into a series of transformable patches, which vary in observation count but maintain a unified time horizon resolution. This process for IMTS offers three major advantages: (1) The independent patching process for each univariate irregular time series bypasses the canonical pre-alignment representation for IMTS, eliminating the risk of sequence length explosion in the representation of IMTS with large-scale variables; (2) local semantics of irregular time series can be better captured by putting each individual observation into patches with richer context (Nie et al., 2022); (3) after transformable patching, the IMTS is naturally aligned in a consistent patch-level temporal resolution. It addresses the asynchrony problem, seamlessly facilitating subsequent inter-time series correlation modeling.\\n\\nAlong this line, a transformable time-aware convolution network is introduced to encode each transformable patch into a latent embedding, which subsequently serves as input tokens to a Transformer for intra-time series dependency modeling. Furthermore, we present time-adaptive graph neural networks to model the inter-time series correlation. To explicitly represent the dynamic correlations between IMTS, we learn a series of time-varying adaptive graphs constructed based on both the learnable inherent variable embedding and dynamic patch embedding, and consequently, these graphs keep the same temporal resolution as transformable patches. Then, graph neural networks are applied to these learned graphs to model patch-level dynamic correlations between IMTS. Finally, a Multi-Layer Perception (MLP) output layer is employed to generate predicted results in terms of forecasting queries based on the obtained comprehensive latent representation of IMTS.\\n\\nOur major contributions are summarized as follows:\\n\\n\u2022 We propose a new transformable patching method to transform each univariate irregular time series of IMTS into a series of variable-length yet time-aligned patches. This tactfully bypasses the canonical pre-alignment representation for IMTS while aligning IMTS in a consistent temporal resolution. It prevents the sequence length of aligned IMTS from explosively growing proportional to the increasing variables, and meanwhile, seamlessly facilitates local semantics capture and inter-time series correlation modeling for IMTS.\\n\\n\u2022 Based on the transformable patching outcomes, we propose time-adaptive graph neural networks to model the dynamic inter-time series correlation within IMTS.\\n\\n\u2022 We build a benchmark for IMTS forecasting evaluation. Seventeen state-of-the-art baseline models from various relevant research fields, i.e., IMTS forecasting, interpolation, classification, and MTS forecasting, are taken for a fair comparison on four public scientific IMTS datasets, which cover areas of healthcare, biomechanics, and climate science. Extensive experiments demonstrate remarkable superiority of T-PATCH GNN.\\n\\n2. Related Works\\n2.1. Irregular Multivariate Time Series Forecasting\\n\\nExisting efforts on IMTS primarily focus on classification tasks (Che et al., 2018; Shukla & Marlin, 2021; Zhang et al., 2021b; 2023a; Horn et al., 2020; Shukla & Marlin, 2018; Li et al., 2023; Baytas et al., 2017). Only a few proactive studies (Rubanova et al., 2019; De Brouwer et al., 2019; Bilo\u0161 et al., 2021; Schirmer et al., 2022) have made efforts...\"}"}
{"id": "UZlMXUGI6e", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nSpecifically, these works primarily rely on neural ODEs (Chen et al., 2018) and focus on handling the continuous dynamics and irregularity within the time series. For instance, Latent-ODE (Rubanova et al., 2019) enables Recurrent Neural Networks (RNNs) to have continuous-time hidden state dynamics specified by neural ODEs. GRU-ODE-Bayes (De Brouwer et al., 2019) incorporates neural ODEs to develop a continuous-time Gated Recurrent Unit (GRU) and introduces a Bayesian update network to process the sparse observations. CRU (Schirmer et al., 2022) handles irregular intervals between observations by evolving the hidden state based on a linear stochastic differential equation and the continuous-discrete Kalman filter. However, calculating ODE solvers is known to be low-efficient due to the expensive numerical integration computation. To address this, Neural Flows (Bilo\u02c7s et al., 2021) models the solution curves of ODEs through neural networks to mitigate the expensive numerical solvers in neural ODEs.\\n\\nWhile these works have made big efforts to handle the irregularity within irregular time series, it is still underexplored to effectively model the inter-time series correlations within asynchronous IMTS.\\n\\n2.2. Irregular Multivariate Time Series Representation\\n\\nTo represent IMTS in a time-aligned manner and facilitate the subsequent modeling, existing works predominantly adopt a pre-alignment representation method (Che et al., 2018; Shukla & Marlin, 2021; Zhang et al., 2021b; 2023a; Baytas et al., 2017; Rubanova et al., 2019; De Brouwer et al., 2019; Bilo\u02c7s et al., 2021; Schirmer et al., 2022). It involves extending all univariate series in IMTS to a consistent sequence length that equals the number of all unique timestamps in IMTS and indicating the missing values with mask terms (Che et al., 2018). However, with the number of variables increasing, such a representation method may suffer from the sequence length explosion problem, which is detailed in Section 3.2, raising severe scalability concerns on both computation and memory overhead.\\n\\nBeyond the pre-alignment representation, Horn et al. (2020) introduce a more scalable representation method by regarding observations of IMTS as a set of tuples comprising of time, value, and variable indicator, and then these tuples are summarized for the IMTS classification. However, this representation method may not be suitable for the forecasting task that requires each variable to be more meticulously and distinctively analyzed.\\n\\n2.3. Graph Neural Networks for Multivariate Time Series\\n\\nGraph Neural Networks (GNNs) are introduced to MTS for their powerful capability to model complicated correlations between variables (Li et al., 2018; Yu et al., 2018; Wu et al., 2019; 2020b; Huang et al., 2023; Yi et al., 2023; Cao et al., 2020; Liu et al., 2022). DCRNN (Li et al., 2018) and STGCN (Yu et al., 2018) apply GNNs to the pre-defined graph structures, which may be difficult to obtain in some domains. Therefore, some studies (Wu et al., 2019; 2020b; Huang et al., 2023; Yi et al., 2023; Cao et al., 2020) propose to learn graph structures from data, enabling automatic modeling of variables' topological relationships. However, when it comes to IMTS, the observations can be notably misaligned at times, raising challenges for the inter-time series correlation modeling. Raindrop (Zhang et al., 2021b) addresses it by propagating the asynchronous observations at all the timestamps when an observation appears at an arbitrary variable, which involves the IMTS pre-alignment and may suffer from the sequence length explosion problem.\\n\\nAnother line of works associated with us applies GNNs for modeling regular MTS with missing data (Cini et al., 2022; Marisca et al., 2022; Chen et al., 2024), which usually necessitate aligning the missing MTS at times like the aforementioned pre-alignment representation and focus on handling the data missing issues. However, our work emphasizes bypassing the canonical pre-alignment representation to address both the irregularity and asynchrony challenges within IMTS modeling.\\n\\n3. Preliminary\\n\\n3.1. Problem Definition\\n\\nDefinition 1 (Irregular Multivariate Time Series). An IMTS can be represented as:\\n\\n\\\\[\\nO = \\\\{ o_{n}^{i} : L_{n} \\\\}_{n=1}^{N},\\n\\\\]\\n\\nwhere there are \\\\( N \\\\) variables, the \\\\( n \\\\)-th variable contains \\\\( L_{n} \\\\) observations, and the \\\\( i \\\\)-th observation of \\\\( n \\\\)-th variable is composed of the recorded time \\\\( t_{n}^{i} \\\\) and value \\\\( x_{n}^{i} \\\\).\\n\\nDefinition 2 (Forecasting Query). A forecasting query is represented as \\\\( q_{n}^{j} \\\\), denoting \\\\( j \\\\)-th query on \\\\( n \\\\)-th variable to predict its corresponding value at a future time \\\\( q_{n}^{j} \\\\).\\n\\nProblem 1 (Irregular Multivariate Time Series Forecasting). Given historical IMTS observations \\\\( O = \\\\{ (t_{n}^{i}, x_{n}^{i}) : L_{n} \\\\}_{n=1}^{N} \\\\), and a set of IMTS forecasting queries \\\\( Q = \\\\{ q_{n}^{j} : Q_{n} \\\\}_{n=1}^{N} \\\\), the problem is to accurately forecast recorded values \\\\( \\\\hat{X} = \\\\{ \\\\hat{x}_{n}^{j} : Q_{n} \\\\}_{n=1}^{N} \\\\) in correspondence to the forecasting queries:\\n\\n\\\\[\\nF(O, Q) \\\\rightarrow \\\\hat{X},\\n\\\\]\\n\\nwhere \\\\( F(\\\\cdot) \\\\) denotes the forecasting model we aim to learn.\\n\\n3.2. Canonical Pre-Alignment Representation for IMTS\\n\\nTo facilitate IMTS modeling, a pre-alignment representation method (Che et al., 2018) has been widely adopted as the standard in current studies (Che et al., 2018; Shukla & Marlin, 2021; Zhang et al., 2021b; 2023a; Rubanova et al., 2019; De Brouwer et al., 2019; Bilo\u02c7s et al., 2021; Schirmer et al., 2022). It involves extending all univariate series in IMTS to a consistent sequence length that equals the number of all unique timestamps in IMTS and indicating the missing values with mask terms (Che et al., 2018). However, with the number of variables increasing, such a representation method may suffer from the sequence length explosion problem, which is detailed in Section 3.2, raising severe scalability concerns on both computation and memory overhead.\"}"}
{"id": "UZlMXUGI6e", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nDe Brouwer et al., 2019; Bilo\u02c7s et al., 2021; Schirmer et al., 2022). In this method, an IMTS is represented by three matrices \\\\((T, X, M)\\\\).\\n\\n\\\\(T = \\\\{t_l\\\\}_{l=1}^{L} = \\\\bigcup_{n=1}^{N} \\\\{t_{ni}\\\\}_{ni=1}^{L_n} \\\\in \\\\mathbb{R}^{L \\\\times N}\\\\)\\ndenotes the chronological unique timestamps of all observations within \\\\(O\\\\).\\n\\n\\\\(X = [\\\\tilde{x}_{nl}]_{n=1}^{N} \\\\bigcup_{l=1}^{L} \\\\in \\\\mathbb{R}^{L \\\\times N}\\\\) are variable's values corresponding to the timestamps, where \\\\(\\\\tilde{x}_{nl} = x_{ni}\\\\) if the value of \\\\(n\\\\)-th variable is observed at time \\\\(t_l\\\\), otherwise \\\\(\\\\tilde{x}_{nl}\\\\) would be filled 'NA'.\\n\\n\\\\(M = [m_{nl}]_{n=1}^{N} \\\\bigcup_{l=1}^{L} \\\\in \\\\mathbb{R}^{L \\\\times N}\\\\) represents a masking matrix, where \\\\(m_{nl} = 1\\\\) if \\\\(\\\\tilde{x}_{nl}\\\\) is observed at time \\\\(t_l\\\\), otherwise zero.\\n\\nWe can observe that the sequence length \\\\(L\\\\) depends on the number of unique timestamps among \\\\(O\\\\). Let \\\\(L_{\\\\text{avg}} = \\\\frac{1}{N} \\\\sum_{n=1}^{N} L_n\\\\) and \\\\(L_{\\\\text{max}} = \\\\max_{n=1}^{N} L_n\\\\) respectively denote the averaged and maximal number of observations for \\\\(N\\\\) variables in an IMTS, then the sequence length \\\\(L\\\\) after pre-aligned representation theoretically falls into:\\n\\n\\\\[ L_{\\\\text{max}} \\\\leq \\\\bigcup_{n=1}^{N} \\\\{t_{ni}\\\\}_{ni=1}^{L_n} \\\\leq N \\\\times L_{\\\\text{avg}}, \\\\quad (2) \\\\]\\n\\nwhich could be explosively growing proportional to the number of variables, thereby posing significant scalability concerns when dealing with large-scale variables.\\n\\n4. Methodology\\nThe overview of T-PATCH GNN is illustrated in Figure 2. In subsequent sections, we sequentially introduce the technical details of irregular time series patching, intra- and inter-time series modeling, and the IMTS forecasting process.\\n\\n4.1. Irregular Time Series Patching\\nIn this section, as a unified patching operation is applied to all univariate irregular time series, we take the \\\\(n\\\\)-th variable for illustration and omit the superscript \\\\(n\\\\) for simplicity in the presentation.\\n\\n4.1.1. T\uff32\uff21\uff2e\uff33\uff26\u041e\u0420\uff2d\uff21\uff22\uff2c\uff25 PATCH\uff29\uff2e\uff27\\nTime series patching has been demonstrated effective in MTS forecasting tasks due to its benefits in capturing local semantic information, reducing computation and memory usage, and modeling longer-range historical observations (Nie et al., 2022). The standard time series patching segments regular time series into a series of subseries-level patches, each of which consists of a fixed number of consecutive observations. However, in the context of IMTS, this approach will lead to patches spanning across diverse time horizons due to the varying time intervals between observations. For instance, a patch composed of five sequential observations might span merely a few minutes for densely sampled scenarios and could cover several days in cases of sparse sampling. This variability in the patch's temporal resolution can even exacerbate the inherent irregularity and asynchrony characteristics in IMTS modeling.\\n\\nTo address this problem, we propose to divide each univariate irregular time series \\\\(o_1: L\\\\) as a series of transformable patches \\\\([o_l: r_p]\\\\) \\\\(P_p=1\\\\) with variable-length consecutive observations, where \\\\(P\\\\) is the number of resulting patches, and \\\\(l_1 = 1, r_P = L\\\\). Each transformable patch spans a patch window size \\\\(s\\\\) with a unified time horizon (e.g., 2 hours) to guarantee a consistent temporal resolution across time and variables. The division can be overlapped or disjoint between two consecutive transformable patches. Along this line, the resulting patches of IMTS are aligned in a consistent time horizon resolution. As each univariate irregular time series is patched independently, this bypasses the canonical pre-alignment process on IMTS, preventing sequence length explosion from the increasing variable count.\\n\\n4.1.2. PATCH ENCODING\\nAfter transforming each univariate irregular time series into a series of transformable patches, we encode each patch into a latent embedding to capture the local semantics within time series.\\n\\nContinuous time embedding. To model the time information in IMTS, we first adopt a continuous time embedding (Shukla & Marlin, 2021) to encode the continuous time of observations:\\n\\n\\\\[\\n\u03d5(t)[d] = \\\\begin{cases} \\n\u03c9_0 \\\\cdot t + \u03b1_0, & \\\\text{if } d = 0 \\\\\\\\\\n\\\\sin (\u03c9_d \\\\cdot t + \u03b1_d), & \\\\text{if } 0 < d < D \\n\\\\end{cases}\\n\\\\]\\n\\nwhere the \\\\(\u03c9_d\\\\) and \\\\(\u03b1_d\\\\) are learnable parameters and \\\\(D\\\\) is embedding's dimension. The linear term captures non-periodic patterns that evolve over time and the periodic terms capture periodicity among time series data, where \\\\(\u03c9_d\\\\) and \\\\(\u03b1_d\\\\) represent the frequency and phase of the sine function.\\n\\nBy incorporating continuous time embedding via concatenation, we derive observations in the patch:\\n\\n\\\\[\\nz_{lp:rp} = [z_i]_{i=1}^{rp} [\u03d5(t_i) \u2225 x_i]_{i=1}^{rp}.\\n\\\\]\\n\\nTransformable time-aware convolution. As each transformable patch is essentially a sub-irregular time series, we introduce the Transformable Time-aware Convolution Network (TTCN) (Zhang et al., 2023b) to capture the semantics within it. TTCN employs a meta-filter to derive the time-aware convolution filter, featuring adaptively generated parameters and transformable filter size that matches the input sequence's length, formulated as:\\n\\n\\\\[\\nf_d = \\\\exp(F_d(\u03d5(t)) P_L p j=1 \\\\exp(F_d(\u03d5(t)) # L p i=1\\n\\\\]\\n\\nwhere \\\\(L_p\\\\) is the sequence length of patch \\\\(o_{lp:rp}\\\\), \\\\(f_d \\\\in \\\\mathbb{R}^{L_p \\\\times D_{in}}\\\\) is the derived filter for \\\\(d\\\\)-th feature map, \\\\(D_{in}\\\\) is dimension of inputs, and \\\\(F_d\\\\) denotes the meta-filter that can be...\"}"}
{"id": "UZlMXUGI6e", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nFigure 2: Overview of T-PATCH GNN, which initially divides each univariate irregular time series into a series of transformable patches with varying number of consecutive observations but maintains a unified time horizon resolution. Then the patching outcomes can be seamlessly modeled by Transformer and time-adaptive GNNs, which incorporate the time-varying adaptive graph structure learning (GSL), to realize an effective intra- and inter-time series modeling for IMTS.\\n\\n\u2295g represents a gated adding operation.\\n\\nBy normalizing the derived filter parameters along the temporal dimension, TTCN ensures consistent scaling of the convolution results for sequences with varying lengths.\\n\\nWith $D-1$ filters derived based on Eq. (5), we attain the latent patch embedding $h_{cp} \\\\in \\\\mathbb{R}^{D-1}$ through the following temporal convolution:\\n\\n$$h_{cp} = \\\\sum_{i=1}^{L_p} X_i = f_d \\\\left[ i \\\\right] \\\\mathbf{z}_l \\\\mathbf{r}_p \\\\left[ i \\\\right], \\\\quad \\\\mathbf{d}=1.$$\\n\\nTTCN is applicable to encode transformable patches as it offers flexibility to adapt to variable-length sequences through transformable filters, customs parameterization for varying time intervals in irregular time series, and the ability to model arbitrarily long sequences without additional learnable filter parameters.\\n\\nConsidering that some patches may have no observations in the cases of sparse time series or high time horizon resolution, we additionally incorporate a patch masking term into the patch embedding:\\n\\n$$h_p = [h_{cp} \\\\| m_p], \\\\quad (7)$$\\n\\nwhere $m_p$ equals one if the patch has observations, otherwise zero, and we have $h_{1:P} = [h_p]_{p=1} \\\\in \\\\mathbb{R}^{P \\\\times D}$.\\n\\n4.2. Intra- and Inter-Time Series Modeling\\n\\nThis section elaborates on how applying transformable patching to irregular time series can seamlessly facilitate both intra- and inter-time series modeling.\\n\\n4.2.1. Transformer to Model Sequential Patches\\n\\nWith the patches encoded, they can be utilized as input tokens in a Transformer (Vaswani et al., 2017) to model the dependencies within the irregular time series. The position encodings $\\\\text{PE}_{1:P} \\\\in \\\\mathbb{R}^{P \\\\times D}$ are added to indicate the temporal order of patches:\\n\\n$$x_{tf,n1:P} = h_{n1:P} + \\\\text{PE}_{1:P}.$$  \\n\\nAfter that, the multi-head attention is applied by transforming them into query matrices $q_n = x_{tf,n1:P} W_Q h, key$ matrices $k_n = x_{tf,n1:P} W_K h$ and value matrices $v_n = x_{tf,n1:P} W_V h$, where $W_Q h, W_K h, W_V h \\\\in \\\\mathbb{R}^{D \\\\times (D/H)}$ are learnable parameters, and $H$ is the number of heads. A scaled dot-product attention is adopted to obtain the outputs of intra-time series modeling:\\n\\n$$h_{tf,n1:P} = \\\\|_{h=1} \\\\text{Softmax} \\\\left[ q_n k_n T \\\\right] h \\\\in \\\\mathbb{R}^{P \\\\times D}.$$  \\n\\n4.2.2. Time-Varying Adaptive Graph Structure Learning\\n\\nTime series of different variables often exhibit substantial correlations. Insights from other variables can be highly...\"}"}
{"id": "UZlMXUGI6e", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\nFor instance, there is a significant correlation between a patient's heart rate and blood pressure that changes in one can be indicative of changes in the other, reflecting the body's cardiovascular status (Obrist et al., 1978). However, observations within IMTS can be notably misaligned at times, raising obstacles for the inter-time series correlation modeling. Existing work (Zhang et al., 2021b) addresses this by propagating the asynchronous observations at all the timestamps when an observation appears at an arbitrary variable, which also involves the IMTS pre-alignment and may suffer from the sequence length explosion problem. Fortunately, the asynchrony problem among IMTS can be seamlessly addressed after applying transformable patching to IMTS. Each variable has a consistent number of patches that are aligned to a uniform time horizon resolution. Along this line, we present time-adaptive graph neural networks to model inter-time series correlation within IMTS.\\n\\nTo shed light on the dynamic correlations underlying IMTS, we propose to learn a series of time-varying adaptive graphs, which keep the same temporal resolution as the patches. Specifically, inspired by studies (Wu et al., 2019; 2020b), we first maintain two embedding dictionaries with learnable parameters for all variables $E_{s1}, E_{s2} \\\\in \\\\mathbb{R}^{N \\\\times D_g}$. This learns to capture the inherent characteristics of variables. While the above variable embedding can be updated during training, they will be static in the inference and remain invariable across all the periods in time series. However, the correlations between variables can dynamically change along with time (Zhang et al., 2021b). To address this, we incorporate patch embedding $H_{tp} = \\\\{h_{tp,n}\\\\}_{n=1}^{N} \\\\in \\\\mathbb{R}^{N \\\\times D}$, which implies the time-varying semantics of time series at the patch-level temporal resolution, into the static variable embedding through a gated adding operation:\\n\\n$$E_{p,k} = E_{s_k} + g_{p,k} \\\\ast E_{dp,k},$$\\n\\nwhere $g_{p,k} = \\\\text{ReLU}(\\\\text{tanh}(H_{tp} \\\\parallel E_{s_k})W_{g,k})$, $k = \\\\{1, 2\\\\}$.\\n\\nIn this way, we obtain the time-varying adaptive graph structure for each patch's time horizon to explicitly characterize the dynamic correlations underlying IMTS:\\n\\n$$A_p = \\\\text{Softmax}(\\\\text{ReLU}(E_{p,1}E_{p,2}))$$\\n\\nBased on the learned graph structures, we introduce GNNs (Kipf & Welling, 2016; Wu et al., 2020a; Zhou et al., 2020) to model the dynamic inter-time series correlations at a patch-level resolution:\\n\\n$$H_p = \\\\text{ReLU}(M_{X_m=0}(A_p)H_{tp}W_{gnn}! \\\\in \\\\mathbb{R}^{N \\\\times D}),$$\\n\\nwhere $M$ is the number of layers for GNNs, and $W_{gnn} \\\\in \\\\mathbb{R}^{D \\\\times D_g}$ are learnable parameters at $m$-th layer.\\n\\nIn practical usage, we can flexibly stack multiple intra- and inter-time series modeling blocks to effectively address diverse IMTS modeling scenarios.\\n\\nSubsequently, a flattened layer with a linear head is used to obtain the final latent representation for each variable:\\n\\n$$H = \\\\text{Flatten}(H_p)p_{p=1}W_f \\\\in \\\\mathbb{R}^{N \\\\times D_o},$$\\n\\nwhere $W_f \\\\in \\\\mathbb{R}^{P \\\\times D \\\\times D_o}$ are learnable parameters.\\n\\nGiven $H_n \\\\in H$ for $n$-th variable and a set of forecasting queries $\\\\{[q_{n,j}]_{Q_n,j=1}^{Q_n}\\\\}_{N_n=1}$, an MLP projection layer is used to generate the predicted results for these queries:\\n\\n$$\\\\hat{x}_{n,j} = \\\\text{MLP}(H_n \\\\parallel \\\\phi(q_{n,j}))$$\\n\\nThe model is trained by minimizing the Mean Squared Error (MSE) loss between the prediction and the ground truth:\\n\\n$$L = \\\\frac{1}{N} \\\\sum_{n=1}^{N} \\\\sum_{j=1}^{Q_n} (\\\\hat{x}_{n,j} - x_{n,j})^2.$$\"}"}
{"id": "UZlMXUGI6e", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"5. Experiments\\n\\n5.1. Experimental Setup\\n\\n5.1.1. Datasets\\n\\n| Algorithm   | PhysioNet | MIMIC | Human Activity | USHCN |\\n|-------------|-----------|-------|----------------|-------|\\n| DLinear     | 89\u00b12      | 82\u00b12  | 79\u00b12           | 81\u00b12  |\\n| Latent-ODE  | 93\u00b12      | 97\u00b12  | 90\u00b12           | 88\u00b12  |\\n| FourGNN     | 96\u00b12      | 91\u00b12  | 85\u00b12           | 89\u00b12  |\\n| StemGNN     | 98\u00b12      | 92\u00b12  | 93\u00b12           | 90\u00b12  |\\n| GraphWavenet| 97\u00b12      | 96\u00b12  | 99\u00b12           | 97\u00b12  |\\n| Warpformer  | 98\u00b12      | 96\u00b12  | 99\u00b12           | 97\u00b12  |\\n| SeFT        | 95\u00b12      | 92\u00b12  | 95\u00b12           | 92\u00b12  |\\n| mTAND       | 94\u00b12      | 92\u00b12  | 95\u00b12           | 92\u00b12  |\\n| GRU-D       | 99\u00b12      | 96\u00b12  | 98\u00b12           | 97\u00b12  |\\n\\n5.1.2. I\\n\\n\\n\\n5.1.3. Evaluation\\n\\n| Algorithm   | MSE       | MAE       |\\n|-------------|-----------|-----------|\\n| DLinear     | 85\u00b12      | 85\u00b12      |\\n| Latent-ODE  | 96\u00b12      | 96\u00b12      |\\n| FourGNN     | 97\u00b12      | 97\u00b12      |\\n| StemGNN     | 97\u00b12      | 97\u00b12      |\\n| GraphWavenet| 98\u00b12      | 98\u00b12      |\\n| Warpformer  | 98\u00b12      | 98\u00b12      |\\n| SeFT        | 98\u00b12      | 98\u00b12      |\\n| mTAND       | 98\u00b12      | 98\u00b12      |\\n| GRU-D       | 98\u00b12      | 98\u00b12      |\\n\\n5.2. Results\\n\\nThe results show that the proposed method achieves the best performance across all datasets, with a significant reduction in both MSE and MAE compared to the baseline models. The model is able to generalize well to unseen data and is robust to noise in the input sequences. The results are validated through extensive experimentation on multiple datasets, demonstrating the model's applicability in real-world scenarios.\"}"}
{"id": "UZlMXUGI6e", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We evaluate the performance of \\\\( T_{\\\\text{w/o PE}} \\\\) when constructing adaptive graph; (5) where the results are provided in Appendix Section A.1. (6) \\\\( w/o \\\\) Transformer\\n\\nTable 2 shows the results of model ablation. As can be seen, Table 1 reports the models' forecasting performance evaluating with standard time series patching (Nie et al., 2022); (2) \\\\( \\\\text{rp Patch} \\\\) represents the patch embedding when constructing adaptive graph; (3) \\\\( \\\\text{w/o Patch} \\\\) removes the transformable patching and adopts the canonical pre-alignment representation; (4) \\\\( \\\\text{w/o VE} \\\\) removes the variable embedding in Eq. (9) as can be seen, compared to the complete model. From these results, we observe removing any component can lead to a performance descent, and even exacerbate the inherent irregularity and asynchrony in some datasets like PhysioNet. It verifies our claims that the standard patching faces troubles with the variability in the patch's temporal resolution, which may increase explosively proportional to the number of variables. In extreme cases, the sequence length can indeed facilitate the subsequent intra- and inter-time series modeling for IMTS. However, directly using standard time series patching (Nie et al., 2022); (1) \\\\( \\\\text{Complete} \\\\) does not achieve satisfactory performance, probably because they fail to effectively model inter-time series correlations to enhance forecasting performance. We also test these models' performance on longer and shorter forecasting windows, which proves that patching irregular time series can even lead to worse performance. Moreover, the existing IMTS forecasting models do not attain consistently competitive performance across all datasets, which indicates the variables' correlational signals forecasting tasks (PhysioNet and MIMIC). This characteristic is more important than its dynamic patterns to characterize their interrelation without effectively identifying discrepancy between these signals, it is difficult to accurately make sense because there is a remarkable semantic disagreement in these tasks can usually dynamically vary along with climate forecasting, which indicates the variables' correlation in these tasks can usually dynamically vary along with climate forecasting, which indicates the variables' correlation.\\n\\n### Ablation Study\\n\\n5.3. Ablation Study\\n\\nTable 3 showcases the extent of the sequence length explosion. Aligned lengths represent the sequence length after canonical pre-alignment representation which effectively circumvents this issue by processing the original number of observations, particularly when dealing with a larger number of observations. In extreme cases, the sequence length can expand more than 20-fold from the original number issue following canonical pre-alignment representation.\\n\\n| Description | Max aligned length | Avg aligned length | Avg # observations | # Variable |\\n|-------------|--------------------|--------------------|--------------------|------------|\\n| Max amplification | 98 \u00b1 10 | 83 \u00b1 83 | 97 \u00b1 88 | 88 \u00b1 72 |\\n| Avg amplification | 53 \u00b1 0 \u00b1 53 \u00b1 0 | 27 \u00b1 0 \u00b1 27 \u00b1 0 | 0 \u00b1 0 \u00b1 0 \u00b1 0 | 0 \u00b1 0 \u00b1 0 \u00b1 0 |\\n| # Variable | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |"}
{"id": "UZlMXUGI6e", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach\\n\\n(a) Average training time per epoch\\n(b) Average inference time per instance\\n\\nFigure 3: Efficiency comparison of training and inference.\\n\\nTo further study the benefits of transformable patching on the model's efficiency, we present the average training time per epoch and average inference time per IMTS instance on MIMIC in Figure 3. We can observe $T$-PATCH-GNN outperforms all models that employ canonical pre-alignment representation in terms of efficiency during both training and inference phases. Furthermore, when compared to current predominant ODE-based IMTS forecasting models, $T$-PATCH-GNN even achieves at least 65 times faster training speeds and 15 times quicker inference speeds. More analytical testing on models' scalability with increased variables is provided in Appendix A.2.\\n\\n5.5. Effect of Patch Size\\n\\nFigure 4 depicts the effect of different patch window sizes on various datasets. We can observe the impact of patch size on performance varies across datasets from different areas. Specifically, for PhysioNet and MIMIC, performance remains relatively stable with smaller patch sizes and peaks when the patch size reaches 8 hours. This could be attributed to the sparse nature of many physiological signals, where a timespan shorter than four hours may not encompass sufficient observations to capture local patterns effectively within sub-series. However, as patch size increases beyond this point, we observe a decline in model performance. An excessively large patch size results in a reduced patch-level temporal resolution, adversely affecting the detailed intra- and inter-time series analysis. When it comes to Human Activity and USHCN, a relatively small patch size would be preferred. As the IMTS from these areas usually exhibit highly dynamic patterns, a relatively small patch size can enable finer-grained modeling of dynamics within IMTS.\\n\\nFrom another perspective, the optimal patch size can be selected by comprehensively considering the forecasting and observed window sizes. Long-range forecasting and observation usually involve a larger patch size to better capture the trend semantics within patches and long-range dependencies across time series (e.g., PhysioNet and MIMIC), whereas short-range forecasting (e.g., Human Activity and USHCN) are more recommended to choose a relatively smaller patch size for finer-grained resolution modeling. The sensitivity analysis on more hyper-parameters is provided in Appendix A.3.\\n\\n6. Conclusion\\n\\nThis paper presented a Transformable Patching Graph Neural Networks approach, $T$-PATCH-GNN, to address the IMTS forecasting problem. $T$-PATCH-GNN achieved the alignment between asynchronous IMTS by transforming each univariate irregular time series into a series of transformable patches with varying observation counts but maintaining unified time horizon resolution. This transformation enabled the capture of local semantics within IMTS and seamlessly facilitated intra- and inter-time series modeling without a canonical pre-alignment representation process, preventing the aligned sequence length from explosively growing proportional to the increasing variables. Building on transformable patching, we then presented the time-adaptive graph neural networks to model dynamic inter-time series correlations based on a series of learned time-varying adaptive graphs. We demonstrated the remarkable superiority of $T$-PATCH-GNN on a comprehensive IMTS forecasting benchmark we build.\"}"}
