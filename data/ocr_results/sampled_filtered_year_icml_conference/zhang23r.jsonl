{"id": "zhang23r", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Efficient attentions with supported attention patterns. The complexity denotes the time complexity in the training phase and is based on sequence length $n$, omitting independent factors for simplicity. ($\\\\ast$) denotes the attentions that perform differently as the original paper claims. We clarify the reasons in Appendix A.\\n\\n| Method                   | Attention Name | Complexity | Noncausal Self | Causal Self | Noncausal Cross | Causal Cross |\\n|--------------------------|----------------|------------|----------------|-------------|----------------|--------------|\\n| Sparsity                 |                | $O(n)$     | %              | %           | %              | %            |\\n| Kernel                   |                | $O(n)$     | %              | %           | %              | %            |\\n| LARA                     |                | $O(n)$     | %              | %           | %              | %            |\\n| cosFormer                |                | $O(n)$     | %              | %           | %              | %            |\\n| Performer                |                | $O(n)$     | %              | %           | %              | %            |\\n| Low-rank factorization   |                | $O(n)$     | %              | %           | %              | %            |\\n| Prototype                | ABC            | $O(n)$     | %              | %           | %              | %            |\\n| ProbSparse               |                | $O(n \\\\log n)$ | %              | %           | %              | %            |\\n| Hybrid                   | LongShort      | $O(n)$     | %              | %           | %              | %            |\\n| State space              | S4D            | $O(n \\\\log n)$ | %              | %           | %              | %            |\\n| IO optimization          | FlashAttention | $O(n^2)$   | %              | %           | %              | %            |\"}"}
{"id": "zhang23r", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Experimental results on CAB with (a) noncausal self, (b) causal self, (c) noncausal cross, and (d) causal cross attention pattern.\\n\\n| Method       | TTS       | Sum       | MLM       | CI       |\\n|--------------|-----------|-----------|-----------|----------|\\n| Transformer-TTS | 3.475 1.974 | 4.095 2.198 | 34.61 6.35 | 31.66    |\\n| Transformer   | 3.475 1.974 | 4.095 2.198 | 34.61 6.35 | 31.66    |\\n| GPT-2        | 4.095 2.198 | 34.61 6.35 | 31.66    |          |\\n| MCD          | 4.095 2.198 | 34.61 6.35 | 31.66    |          |\\n| MSD          | 4.095 2.198 | 34.61 6.35 | 31.66    |          |\\n| R-1          | 4.095 2.198 | 34.61 6.35 | 31.66    |          |\\n| R-2          | 4.095 2.198 | 34.61 6.35 | 31.66    |          |\\n| R-L          | 4.095 2.198 | 34.61 6.35 | 31.66    |          |\\n| PPL          | 4.095 2.198 | 34.61 6.35 | 31.66    |          |\\n| vanilla      | 3.475 1.974 | 4.095 2.198 | 34.61 6.35 | 31.66    |\\n| ABC          | 5.780 2.631 | 32.22 5.55  | 29.53    | 0.956    |\\n| Performer    | 6.635 3.053 | 27.22 3.88  | 25.21    | -1.014   |\\n\\nOn the dimension of efficient attention, we find that local attention is a strongly competitive efficient model on most real-world tasks, which agrees with the previous studies (Xiong et al., 2022), although it is utterly defeated in LRA. It achieves the highest score of 0.978 under the noncausal self pattern and ranks second under the causal self pattern. In contrast, S4D, which achieves impressive results on LRA, appears less inspiring than expected on Sum and MLM tasks under the noncausal self attention pattern.\\n\\n#### Efficiency Analysis\\n\\nWe conduct a simulation efficiency experiment under noncausal self attention pattern. The experiment is performed on a single A100 GPU, where attention mechanisms are fed a set of dummy sequences with lengths of \\\\{256, 512, 1024, 2048, 4096, 8192\\\\}. Table 2 shows the time and memory consumption against sequence length by each attentions. We can find that the advantage of efficient attention gradually expands as the sequence length increases. Moreover, kernel-based and low-rank attentions are more efficient on time and space. Further, we report efficiency length to measure the utility of efficient attentions. The efficiency length is defined as the intersection point of computational time and memory curves by efficient models and vanilla attention. The efficiency length represents the minimum length that a sub-quadratic efficient model surpasses vanilla attention in efficiency. Specifically, we perform regression on vanilla and efficient attentions by their theoretical complexity. Then we calculate the intersection point.\"}"}
{"id": "zhang23r", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\\n\\n...tion of the regression line and the quadratic regression curve to get an efficiency length. Figure 2c shows that efficient attentions gain superiority over vanilla attention once the sequence contains thousands of tokens. For noncausal self attention, ABC and Performer have the shortest efficiency lengths on running time and memory usage, respectively. The details of efficiency lengths and experiment settings can be found in Appendix G.\\n\\n4.2. Discussions\\n\\nPerformance Consistency Across Attention Patterns\\n\\nWe investigate whether efficient attentions perform consistently across different attention patterns. To achieve this, we compute Pearson correlation (Benesty et al., 2009) over CI between each pair of attention patterns against the same arrangement of efficient attentions for intra-benchmark comparison. Note that we also include CI computed from LRA for inter-benchmark comparison between CAB\u2019s four attention patterns and LRA. In Figure 3a, CAB\u2019s NS attention pattern is highly correlated with LRA, reflecting that efficient NS attentions developed on LRA do contribute to a wide range of real-world applications. In contrast, cross attention patterns share non-positive correlations with self attention patterns, indicating their inconsistent performance on self and cross attention.\\n\\nOn the causality dimension, the performances of causal and noncausal models are highly related but not strictly consistent. We also include task-to-task correlation analysis in Appendix I.\\n\\nBenefit of Attention\\n\\nSelf attention is designed to capture contextual features while cross attention integrates non-homologous information. But how much do these features benefit the model? We conduct an ablation study without the attention mechanism. In a preliminary experiment, we find that causal and cross patterns are indispensable to the backbone models as expected, while noncausal self attention only has a limited contribution. Therefore, we focus on discussing the attentive functionality of noncausal self attention. Figure 3b depicts the CI of backbone models with and without attention on tasks. Detailed CI can be found in Appendix H. Results show that attention mechanism improves performance on most tasks. Embarrassingly, after removing the attention mechanism, the PoinTr and Informer achieve improvement on PCC and LSTF tasks, respectively. A possible reason is that there are alternative contextualizing structures such as the convolutional neural network in the backbone models and the contribution of the attention mechanism is weakened. Besides, we also notice that the current efficient attention family has already surpassed vanilla attention in modeling noncausal self relationships on most tasks.\\n\\nInterpolation and Extrapolation on Long-Context Language Modeling\\n\\nLanguage models with efficient attentions are more likely to face unexpected long context that exceeds predefined lengths, resulting in an extrapolation problem. It is critical for efficient attentions to scale to longer sequences while preserving the performance on short ones. Therefore, we conduct an experiment on the language modeling task, where we train the model with the input of 8,192 tokens and calculate the perplexity of contexts with 256-16, 384 tokens during the test. Figure 3c shows the results on both of interpolation and extrapolation. On one hand, as the size of context grows to 8,192, language models achieve decreasing perplexity. It means that longer contexts indeed improve language modeling, and efficient attentions do well in interpolation as expected. On the other hand, for sequences with more than 8,192 tokens, the perplexities of all these efficient attention-equipped language models become higher. Notably, local attention and LongShort with local features have only a slight drop in performance in extrapolation, while ABC with only global features fails to extrapolate to longer sequences. Although S4D performs well as causal attention, it has a large perplexity with sequence input longer than 8,192, indicating that S4D has relatively poor extrapolation ability.\\n\\n5. Related Work\\n\\nLong Sequence Modeling\\n\\nMost efficient attention architectures are designed for long sequence modeling. Previous works evaluate their proposed attention modules separately on Language Model (Choromanski et al., 2020; Qin et al., 2021; Peng et al., 2022a), Masked Language Model (Qin et al., 2021; Peng et al., 2022a), Image Classification (Choromanski et al., 2020; Zheng et al., 2022), and Machine Translation (Peng et al., 2022a; 2021). Without unified criteria for evaluating attention modules, researchers find it hard to distinguish between them and select an appropriate attention module for their research. Later, Tay et al. (2020b) proposed Long Range Arena (LRA), a benchmark that contains 5 different classification tasks across language and image domains. However, their benchmark only considers the noncausal self attention pattern and contains most synthetic tasks which are far from real-world applications. Besides, different attention modules perform nearly equally on LRA (Xiong et al., 2022), which causes LRA a limited benchmark. Shaham et al. (2022) proposed SCROLLS for long language sequences but ignores long sequence modeling tasks in other fields.\"}"}
{"id": "zhang23r", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\\n\\n| Sequence Length | vanilla | Performer | LARA | Nystr\u00f6mformer | LongShort | ProbSparse | ABC | S4D | cosFormer | local |\\n|-----------------|--------|-----------|------|---------------|-----------|------------|-----|-----|-----------|-------|\\n| 256             |        |           |      |               |           |            |     |     |           |       |\\n| 512             |        |           |      |               |           |            |     |     |           |       |\\n| 1024            |        |           |      |               |           |            |     |     |           |       |\\n| 2048            |        |           |      |               |           |            |     |     |           |       |\\n| 4096            |        |           |      |               |           |            |     |     |           |       |\\n| 8192            |        |           |      |               |           |            |     |     |           |       |\\n\\nRunning Time (Relative)\\n\\n| Memory Usage (Relative) | vanilla | Performer | LARA | Nystr\u00f6mformer | LongShort | ProbSparse | ABC | S4D | cosFormer | local |\\n|------------------------|--------|-----------|------|---------------|-----------|------------|-----|-----|-----------|-------|\\n| 256                    |        |           |      |               |           |            |     |     |           |       |\\n| 512                    |        |           |      |               |           |            |     |     |           |       |\\n| 1024                   |        |           |      |               |           |            |     |     |           |       |\\n| 2048                   |        |           |      |               |           |            |     |     |           |       |\\n| 4096                   |        |           |      |               |           |            |     |     |           |       |\\n| 8192                   |        |           |      |               |           |            |     |     |           |       |\\n\\nEfficiency Length\\n\\nFigure 2. Empirical running time (a) and memory cost (b) with sequence length. Relative measurements to vanilla attention are reported.\\n\\nFigure 3. (a) Pairwise attention pattern correlation where NS, CS, NC, and CC signify noncausal self, causal self, noncausal cross, and causal cross attention respectively; (b) Ablation study of removing attention. The gray part is the score achieved by the existing efficient attention family, where we select the most well-performed efficient attention for each task; (c) the performance of efficient attentions based on different context lengths in the LM task during the test phase. S4D fails on the context with 16,384 tokens.\\n\\n6. Conclusion\\n\\nIn this paper, we propose to evaluate efficient attentions under a more fine-grained attention taxonomy with four distinguishable attention patterns, each of which reflects different attentive functionality. Under the taxonomy, we present Comprehensive Attention Benchmark (CAB), consisting of seven real-world tasks and eight backbone networks. We conduct exhaustive experiments to benchmark nine typical efficient attentions on CAB. The empirical results show that existing efficient attentions make significant progress in noncausal self attention, but are still struggling to catch up with vanilla attention in conditionality and causality modeling. The efficiency analysis shows that although existing efficient attentions enjoy lower computational complexity, most of them only start to achieve significant efficiency gain when applied to sequences with more than one thousand tokens, especially in the causal attention scenario. Moreover, we find that attention mechanisms, in the noncausal self case, are sometimes not helpful or even harmful to neural models that already contain feature mixing. Our study sheds light on long-context language modeling and shows that efficient attentions such as local attention and LongShort...\"}"}
{"id": "zhang23r", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer\u2019s efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods\u2019 capability on long-range modeling is the Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures. We conduct exhaustive experiments to benchmark the performances of nine widely-used efficient attention architectures designed with different philosophies on CAB.\\n\\nExtensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.\"}"}
{"id": "zhang23r", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\\n\\ncausal cross, representing the distinguishable attentive functionality to sequence modeling (\u00a72). With that in mind, we then collect seven real-world tasks from diverse fields of computer vision, natural language processing, speech processing, and time series forecasting (\u00a73). Among these tasks, CAB includes rich backbone architectures to evaluate the attention mechanisms, testing their performances and generalization abilities. Given four attention patterns defined by the attention taxonomy, we advocate a pattern-wise comparison between attention mechanisms, evaluating their attentive functionality respectively.\\n\\nWe conduct exhaustive experiments on CAB, assessing nine widely-used efficient attentions designed with different philosophies (\u00a74). The experimental results reveal several insights into designing efficient attention architectures. First, we show that existing efficient transformers claiming comparable or even superior performances to vanilla attention, often achieve less competitive results in the causal cross scenario, indicating efficient attentions' modeling capability cannot always generalize across different attention patterns. Second, by quantifying the efficiency of attentions in long sequence contexts using efficiency length (i.e., the minimum length for a sub-quadratic efficient model to surpass vanilla attention in efficiency), we disclose the underlying inefficiency problem of existing efficient attention methods in modeling relatively short sequences. Third, we investigate interpolation and extrapolation on a long-context language model, and find that it is promising for efficient attention, such as local attention and Long-Short Transformer, to scale to long-context language modeling.\\n\\nWe hope CAB and all the related codes will be released at https://github.com/Shark-NLP/CAB.\\n\\n2. Attention Taxonomy\\n\\nAttention methods are designed to capture token-to-token dependency within and across sequences. Let \\\\( Y = \\\\{y_1, y_2, \\\\ldots, y_n\\\\} \\\\) and \\\\( Y \\\\in \\\\mathbb{R}^{n \\\\times d} \\\\) be a target sequence and its feature matrix, and \\\\( X = \\\\{x_1, x_2, \\\\ldots, x_m\\\\} \\\\) and \\\\( X \\\\in \\\\mathbb{R}^{m \\\\times d} \\\\) be a source sequence and its feature matrix, where \\\\( n \\\\) and \\\\( m \\\\) are target and source sequence lengths respectively, and \\\\( d \\\\) denotes dimensionality. Note that \\\\( X \\\\) could be the same as \\\\( Y \\\\) in the case of self attention. The target and source feature matrices are transformed to query-key-value feature matrices as \\\\( Q = Y W_Q, K = X W_K, V = X W_V \\\\) with learnt parametric matrices \\\\( W_Q, W_K, W_V \\\\). Vanilla transformer (Vaswani et al., 2017) learns to integrate key-value feature matrices \\\\( K \\\\in \\\\mathbb{R}^{m \\\\times d}, V \\\\in \\\\mathbb{R}^{m \\\\times d} \\\\) into a query one \\\\( Q = \\\\{q_1, q_2, \\\\ldots, q_n\\\\} \\\\in \\\\mathbb{R}^{n \\\\times d} \\\\) with token-to-token attention:\\n\\n\\\\[\\n\\\\text{Attn}(Q, K, V) = \\\\text{softmax}(QK^\\\\top / \\\\sqrt{d})\\n\\\\]\\n\\nin which we omit the multihead notation without loss of generality. Vanilla attention in Eq. 1 advocates a token-to-token alignment matrix \\\\( QK^\\\\top \\\\), which results in quadratic complexity \\\\( O(nm) \\\\) of computational time and memory usage.\\n\\nIt is observed that \\\\( \\\\text{Attn}(\\\\cdot) \\\\) in Eq. 1 acts as an integration model, projecting query features \\\\( Q \\\\) by integrating key-value features without changing \\\\( Q \\\\)'s shape. Beyond the vanilla token-to-token attention mechanism, we extend Eq. 1 to a more general form of attention family as:\\n\\n\\\\[\\n\\\\text{Attn}(Q, K, V) = f(Q; K, V) : \\\\mathbb{R}^{n \\\\times d} \\\\rightarrow \\\\mathbb{R}^{n \\\\times d}\\n\\\\]\\n\\nwhere \\\\( \\\\text{Attn}_i(Q, K, V) = f_i(q_i; Q, K, V) \\\\), \\\\( i = 1, \\\\ldots, n \\\\) (2)\\n\\nwhere \\\\( Q \\\\) is treated as the main variable and \\\\( K, V \\\\) are conditional ones. Comparing with Eq. 1, Eq. 2 covers efficient attention methods without explicitly modeling the token-to-token alignment. In usage, attentions are modified to fit generative models with underlying structures, such as conditional models and causal models, to achieve specific attentive functionality.\\n\\nIn this section, we put forward a fine-grained attention taxonomy, considering the conditionality and causality of attentions. We show their challenges and potential impact on real-world applications in modeling conditionality and causality. Under the taxonomy, we present four attention patterns with different attentive functionality as shown in Figure 1.\\n\\nSelf Attention & Cross Attention\\n\\nSelf attention (Lin et al., 2017; Vaswani et al., 2017; Dosovitskiy et al., 2020) learns contextual features within a sequence whereas cross attention (Bahdanau et al., 2015) learns to integrate features across sequences. Both self attention and cross attention are highly effective in sequence learning, but designing cross attention is more challenging for efficient attentions without explicit token-to-token alignment. On the aspect of computation, \\\\( Q \\\\in \\\\mathbb{R}^{n \\\\times d} \\\\) and \\\\( K, V \\\\in \\\\mathbb{R}^{m \\\\times d} \\\\) are of different shapes and are impossible to perform elementwise operations along length dimension, such as cross-covariance used in XCiT (Ali et al., 2021). Besides, different from self attention, cross attention lacks implicit alignment between \\\\( Q \\\\) and \\\\( K, V \\\\), such as neighborhood information. Cross attention learns to integrate conditional features \\\\( K, V \\\\) into the main features \\\\( Q \\\\), and has been successfully applied to real-world applications, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), data-to-text generation (Du\u02c7sek et al., 2020; Shen et al., 2020) and knowledge enhanced model (Jin et al., 2020; Liu et al., 2021a).\"}"}
{"id": "zhang23r", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\\n\\nFigure 1 illustrates the computation diagrams of these four patterns. These attention patterns construct typical sequence autoregressively, neural models (Vaswani et al., 2017; Li et al., 2019a; Brown et al., 2020) usually adopt a causal graph $G(V, E)$ with vertices $V = \\\\{y_i \\\\in Y\\\\}$ and edges $E = \\\\{y_i \\\\rightarrow y_j | i \\\\in [1, \\\\ldots, n], j \\\\in [1, \\\\ldots, n], i \\\\leq j\\\\}$.\\n\\n1 When generating a sequence $Y = \\\\{y_1, y_2, \\\\ldots, y_n\\\\}$ autoregressively, neural models (Vaswani et al., 2017; Li et al., 2019a; Brown et al., 2020) usually adopt a causal graph $G(V, E)$ with vertices $V = \\\\{y_i \\\\in Y\\\\}$ and edges $E = \\\\{y_i \\\\rightarrow y_j | i \\\\in [1, \\\\ldots, n], j \\\\in [1, \\\\ldots, n], i \\\\leq j\\\\}$.\\n\\nNoncausal Attention & Causal Attention While modeling a sequence, noncausal models have a holistic view of the whole sequence whereas causal ones only access to previous tokens up to now. This restriction forbids efficient attention like Nystr\u00f6mformer (Xiong et al., 2021), which must be conditioned on the whole target sequence to achieve linearity, to be applied in causal models. Furthermore, when training a causal model, efficient attentions such as RFA (Peng et al., 2021) and ABC (Peng et al., 2022a) have to provide different sets of features from $Y$ for each query token. This mechanism in these attentions, therefore, leads to multiplied memory consumption compared to their noncausal counterparts. We provide a detailed analysis of the increasing computation costs in Appendix B. In real-world applications, causal attentions bring up strong generation capability and is widely adopted in text generation (Bahdanau et al., 2015; Vaswani et al., 2017; Zhang et al., 2020), text-to-speech synthesis (Li et al., 2019a) and language modeling (Radford et al., 2019; Brown et al., 2020; Raffel et al., 2020; Chen et al., 2021b).\\n\\nAttention Patterns As discussed above, conditionality and causality are two orthogonal aspects of modeling sequences. Thus we obtain four attention patterns via Cartesian product as $\\\\{\\\\text{noncausal}, \\\\text{causal}\\\\} \\\\times \\\\{\\\\text{self}, \\\\text{cross}\\\\}$: noncausal self (NS), causal self (CS), noncausal cross (NC), and causal cross (CC). Following the general form of attentions in Eq. 2, we formulate the four attention patterns as below:\\n\\n$\\\\text{NS}_i(Q, K, V) \\\\equiv f_i(q_i; Q, K, V), \\\\text{s.t. } Y = X(3)$\\n\\n$\\\\text{CS}_i(Q, K, V) \\\\equiv f_i(q_i; Q \\\\leq i, K \\\\leq i, V \\\\leq i), \\\\text{s.t. } Y = X(4)$\\n\\n$\\\\text{NC}_i(Q, K, V) \\\\equiv f_i(q_i; Q, K, V)(5)$\\n\\n$\\\\text{CC}_i(Q, K, V) \\\\equiv f_i(q_i; Q \\\\leq i, K, V)(6)$\\n\\nIn real-world applications, causal attentions bring up strong generation capability and is widely adopted in text generation (Bahdanau et al., 2015; Vaswani et al., 2017; Zhang et al., 2020), text-to-speech synthesis (Li et al., 2019a) and language modeling (Radford et al., 2019; Brown et al., 2020; Raffel et al., 2020; Chen et al., 2021b).\"}"}
{"id": "zhang23r", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Task statistics of datasets, data lengths, evaluation metrics, backbone neural networks and required attention patterns. For encoder-decoder models, both source and target lengths are listed.\\n\\n| Task          | Dataset          | Length | Metric       | Model       | Noncausal | Causal |\\n|---------------|------------------|--------|--------------|-------------|-----------|--------|\\n| TTS (Li et al., 2019a) | LJSpeech        | 559    | MCD, MSD    | Transformer-TTS | % | % |\\n| Summarization (Sum) | Multi-News      | 4,096/400 | ROUGE       | Transformer | % | % |\\n| Long Sequence Time-series Forecasting (LSTF) | Electricity Transformer Temperature (ETT) | 336/720 | MSE, MAE    | Informer | % | % |\\n|               | Electricity Consuming Load (ECL) | 336/960 |              |            |          |        |\\n|               | Weather         | 720/720 |              |            |          |        |\\n| Point Cloud Completion (PCC) | PCN dataset (Griffiths & Boehm, 2019) | 2,048  | CD, F-Score | PoinTr | % | % |\\n| Language Modeling (LM) | PG-19 dataset (Rae et al., 2019) | 8,192  | PPL         | GPT-2      | % | % |\\n| Masked Language Modeling (MLM) | RoBERTa-base (Liu et al., 2019) | 2,048  |            |            | % | % |\\n| Super-Resolution (SR) | FFHQ dataset (Karras et al., 2019) | 16,384 | PSNR, SSIM  | SR3 (Saharia et al., 2022) | % | % |\\n\\nTTS (Li et al., 2019a) as backbone networks. We adopt Mean Cepstral Distortion (MCD) and Mel Spectral Distortion (MSD) for objective evaluation.\\n\\nSummarization (Sum) This task is one that makes a comprehensive summary of multiple input documents without losing important information. We consider multi-document summarization task and use Multi-News datasets (Fabbri et al., 2019) for long sequence modeling. To make the task more challenging, we set the maximum source text and target summary lengths to be 4,096 and 400, respectively.\\n\\nTransformer (Vaswani et al., 2017) implemented on PARAGEN (Feng et al., 2022) serves the evaluating backbone and ROUGE (R-N) (Lin, 2004) is used to evaluate each attention.\\n\\nLong Sequence Time-series Forecasting (LSTF) Long sequence time-series forecasting is to predict long-term future behavior based on past states. This task evaluates models on three datasets, including Electricity Transformer Temperature (ETT), Electricity Consuming Load (ECL), and Weather (Zhou et al., 2021a). Following (Zhou et al., 2021a), we use Informer as backbone and conduct univariate and multivariate evaluations. We average their Mean Square Error (MSE) and Mean Absolute Error (MAE) to obtain the final scores.\\n\\nPoint Cloud Completion (PCC) This task is to complete a frame of point cloud data when providing partial points. CAB adopts PCN dataset (Griffiths & Boehm, 2019) which inputs 2,048 3D point coordinates and requires models to output the complete point cloud data comprising 14,336 points. PoinTr (Yu et al., 2021) is adopted here serving as evaluating backbone. Chamfer Distance (CD) (Huang et al., 2020) and F-Score (Tatarchenko et al., 2019) are selected to evaluate each attention. For clearer comparison, CD metrics are enlarged by 1,000 times.\\n\\nLanguage Modeling (LM) The language modeling task requires the language model to predict the next text based on the previous information. In this task, we consider a long-context language modeling where the context length is prolonged to 8,192. We use PG-19 dataset (Rae et al., 2019) and conduct evaluations based on the backbone model GPT-2 (Radford et al., 2019) implemented on FAIRSEQ (Ott et al., 2019). Perplexity (PPL) is used to serve as the evaluation metric.\\n\\nMasked Language Modeling (MLM) Different from the language modeling task, masked language modeling (Devlin et al., 2019) uses full context to predict masked tokens. Roberta-base (Liu et al., 2019) implemented on FAIRSEQ is adopted as the evaluation backbone. We use the same dataset and evaluation metric as LM task while the context length is set to 2,048.\\n\\nSuper-Resolution (SR) In this task, we aims to convert low-resolution (16 \u00d7 16) face images into high-resolution (128 \u00d7 128) images. Following Saharia et al. (2022), we train the backbone model SR3 on Flickr-Faces-HQ (FFHQ) dataset (Karras et al., 2019) and conduct evaluation on CelebA-HQ dataset (Karras et al., 2018). Peak Signal-to-Noise Ratio (PSNR) and Structural SIMilarity (SSIM) (Wang et al., 2004) are selected to evaluate.\"}"}
{"id": "zhang23r", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\\n\\nN. Calculation Method of Efficiency Length\\n\\nVanilla attention is of quadratic complexity while most efficient attention models are sublinear, thus we use $y = ax^2 + bx + c$ to denote the inference time/memory costs $y$ of vanilla attention with respect to sequence length $x$. Compared to this, linear attention can be modeled as $y = ex + f$ where $x$ and $y$ denote sequence length and inference time/memory costs respectively. The calculation method of efficiency length is as follows:\\n\\n1. We first collect different results of $y$ given different $x$ lengths, ranging from $[256, 512, 1024, 2048, 4096, 8192]$.\\n2. Then we use python scipy library to fit the curve to these data points and obtain $a$, $b$, $c$ and $e$, $f$ for each attention.\\n3. Finally, we compute the intersection points of $y = ax^2 + bx + c$ and $y = ex + f$. This may produce two intersection points $[x_1, x_2]$. We select the larger one for consideration of long sequence modeling.\\n4. This intersection point $\\\\text{max}(x_1, x_2)$ is the efficiency length.\\n\\nO. Extending CAB with new tasks and models\\n\\nWith a new dataset on a long sequence modeling task, we choose typical or state-of-the-art backbone architectures on this task. Then we investigate the backbone models on which patterns of attention are required. Finally, a new attention model with certain attention functionality is placed in the backbone architecture. For an example of machine translation task, its backbone Transformer (Vaswani et al., 2017) requires noncausal self attention (encoder attention), causal self attention (decoder attention) and casual cross attention (encoder-decoder attention). Thus Transformer is used to evaluate efficient attention models on these three patterns, by directly replacing PyTorch's multi-head attention.\"}"}
{"id": "zhang23r", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Transformer are promising for extrapolation problems in long-context language modeling.\\n\\nAcknowledgements\\nWe would like to thank the anonymous reviewers for their valuable suggestions that greatly helped improve this work. This work is partially supported by the Shanghai Committee of Science and Technology (Grant No. 21DZ1100100) and the joint research scheme of the National Natural Science Foundation of China (NSFC) and the Research Grants Council (RGC) under grant number N\\n\\nReferences\\nAinslie, J., Ontanon, S., Alberti, C., Cvicek, V ., Fisher, Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., and Yang, L. ETC: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 268\u2013284, Online, November 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.19. URL https://aclanthology.org/2020.emnlp-main.19.\\n\\nAinslie, J., Ontanon, S., Alberti, C., Cvicek, V ., Fisher, Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., and Yang, L. ETC: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 268\u2013284, Online, November 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.19. URL https://aclanthology.org/2020.emnlp-main.19.\\n\\nAli, A., Touvron, H., Caron, M., Bojanowski, P., Douze, M., Joulin, A., Laptev, I., Neverova, N., Synnaeve, G., Verbeek, J., et al. Xcit: Cross-covariance image transformers. Advances in neural information processing systems, 34:20014\u201320027, 2021.\\n\\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine translation by jointly learning to align and translate. In ICLR, 2015. URL http://arxiv.org/abs/1409.0473.\\n\\nBeltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.\\n\\nBenesty, J., Chen, J., Huang, Y ., and Cohen, I. Pearson correlation coefficient. In Noise reduction in speech processing, pp. 1\u20134. Springer, 2009.\\n\\nBrandes, N., Ofer, D., Peleg, Y ., Rappoport, N., and Linial, M. Proteinbert: A universal deep-learning model of protein sequence and function. Bioinformatics, 38(8):2102\u20132110, 2022.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\\n\\nChen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and R\u00b4e, C. Scatterbrain: Unifying sparse and low-rank attention. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021a. URL https://openreview.net/forum?id=SehIKudiIo1.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021b.\\n\\nChen, Y ., Zeng, Q., Ji, H., and Yang, Y . Skyformer: Remodel self-attention with gaussian kernel and nystr\u00f6m method. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021c. URL https://openreview.net/forum?id=pZCYG7gjkKz.\\n\\nChen, Y ., Zeng, Q., Ji, H., and Yang, Y . Skyformer: Remodel self-attention with gaussian kernel and nystr\u00f6m method. In Ranzato, M., Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 2122\u20132135. Curran Associates, Inc., 2021d. URL https://proceedings.neurips.cc/paper/2021/file/10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf.\\n\\nChen, Z., Gong, M., Ge, L., and Du, B. Compressed self-attention for deep metric learning with low-rank approximation. In Bessiere, C. (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 2058\u20132064. International Joint Conferences on Artificial Intelligence Organization, 2020. doi: 10.24963/ijcai.2020/285. URL https://ijcai.org/2020/285.\"}"}
{"id": "zhang23r", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\\n\\nMain track.\\n\\nChild, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.\\n\\nChoromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020.\\n\\nChoromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH.\\n\\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., and Salakhutdinov, R. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978\u20132988, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285.\\n\\nDai, Z., Lai, G., Yang, Y., and Le, Q. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 4271\u20134282. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/2cd2915e69546904e4e5d4a2ac9e1652-Paper.pdf.\\n\\nDao, T., Fu, D., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with IO-awareness. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 16344\u201316359. Curran Associates, Inc., 2022a. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf.\\n\\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022b.\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\\n\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.\\n\\nDv\u011bsek, O., Novikova, J., and Rieser, V. Evaluating the state-of-the-art of end-to-end natural language generation: The e2e nlg challenge. Computer Speech & Language, 59:123\u2013156, 2020.\\n\\nFabbri, A., Li, I., She, T., Li, S., and Radev, D. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1074\u20131084, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1102. URL https://aclanthology.org/P19-1102.\\n\\nFeng, J., Zhou, Y., Zhang, J., Qian, X., Wu, L., Zhang, Z., Liu, Y., Wang, M., Li, L., and Zhou, H. Paragen: A parallel generation toolkit. arXiv preprint arXiv:2210.03405, 2022.\\n\\nGriffiths, D. and Boehm, J. A review on deep learning techniques for 3D sensed data classification. Remote Sensing, 11(12):1499, 2019.\\n\\nGu, A., Dao, T., Ermon, S., Rudra, A., and R\u00e9, C. Hippo: Recurrent memory with optimal polynomial projections. Advances in Neural Information Processing Systems, 33:1474\u20131487, 2020.\\n\\nGu, A., Goel, K., and R\u00e9, C. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum?id=uYLFoz1vlAC.\\n\\nGu, A., Gupta, A., Goel, K., and R\u00e9, C. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022b.\\n\\nGu, J., Bradbury, J., Xiong, C., Li, V. O., and Socher, R. Non-autoregressive neural machine translation. In International Conference on Learning Representations, 2018.\"}"}
{"id": "zhang23r", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "zhang23r", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "zhang23r", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "zhang23r", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\\n\\nShen, X., Chang, E., Su, H., Niu, C., and Klakow, D. Neural data-to-text generation via jointly learning the segmentation and correspondence. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7155\u20137165, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.641. URL https://aclanthology.org/2020.acl-main.641.\\n\\nShen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 3531\u20133539, 2021.\\n\\nTatarchenko, M., Richter, S. R., Ranftl, R., Li, Z., Koltun, V., and Brox, T. What do single-view 3d reconstruction networks learn? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3405\u20133414, 2019.\\n\\nTay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C. Sparse sinkhorn attention. In Proceedings of the 37th International Conference on Machine Learning, ICML'20. JMLR.org, 2020a.\\n\\nTay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020b.\\n\\nTay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A survey. ACM Computing Surveys (CSUR), 2020c.\\n\\nTay, Y., Bahri, D., Metzler, D., Juan, D.-C., Zhao, Z., and Zheng, C. Synthesizer: Rethinking self-attention for transformer models. In International conference on machine learning, pp. 10183\u201310192. PMLR, 2021.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nVyas, A., Katharopoulos, A., and Fleuret, F. Fast transformers with clustered attention. Advances in Neural Information Processing Systems, 33:21665\u201321674, 2020.\\n\\nWang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.\\n\\nWang, S., Zhou, L., Gan, Z., Chen, Y.-C., Fang, Y., Sun, S., Cheng, Y., and Liu, J. Cluster-former: Clustering-based sparse transformer for question answering. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 3958\u20133968, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.346. URL https://aclanthology.org/2021.findings-acl.346.\\n\\nWang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013612, 2004.\\n\\nWu, H., Wu, J., Xu, J., Wang, J., and Long, M. Flowformer: Linearizing transformers with conservation flows. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 24226\u201324242. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/wu22m.html.\\n\\nXiong, W., Oguz, B., Gupta, A., Chen, X., Liskovich, D., Levy, O., Yih, S., and Mehdad, Y. Simple local attention remains competitive for long-context tasks. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1975\u20131986, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.144. URL https://aclanthology.org/2022.naacl-main.144.\\n\\nXiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nystr\u00f6mformer: A Nystr\u00f6m-based algorithm for approximating self-attention. Proceedings of the AAAI Conference on Artificial Intelligence, 35(16):14138\u201314148, May 2021. doi: 10.1609/aaai.v35i16.17664. URL https://ojs.aaai.org/index.php/AAAI/article/view/17664.\\n\\nYing, C., Ke, G., He, D., and Liu, T.-Y. Lazy-former: Self attention with lazy update. arXiv preprint arXiv:2102.12702, 2021.\\n\\nYu, X., Rao, Y., Wang, Z., Liu, Z., Lu, J., and Zhou, J. Pointr: Diverse point cloud completion with geometry-aware transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 12498\u201312507, October 2021.\\n\\nZaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283\u201317297, 2020.\"}"}
{"id": "zhang23r", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zeng, Z., Xiong, Y., Ravi, S., Acharya, S., Fung, G. M., and Singh, V. You only sample (almost) once: Linear cost self-attention via bernoulli sampling. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12321\u201312332. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/zeng21a.html.\\n\\nZhang, B., Xiong, D., and Su, J. Accelerating neural transformer via an average attention network. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1789\u20131798, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1166. URL https://aclanthology.org/P18-1166.\\n\\nZhang, H., Gong, Y., Shen, Y., Li, W., Lv, J., Duan, N., and Chen, W. Poolingformer: Long document modeling with pooling attention. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12437\u201312446. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/zhang21h.html.\\n\\nZhang, J., Zhao, Y., Saleh, M., and Liu, P. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pp. 11328\u201311339. PMLR, 2020.\\n\\nZheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 27011\u201327041. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/zheng22b.html.\\n\\nZhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W. Informer: Beyond efficient transformer for long sequence time-series forecasting. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12):11106\u201311115, May 2021a. URL https://ojs.aaai.org/index.php/AAAI/article/view/17325.\\n\\nZhu, C., Ping, W., Xiao, C., Shoeybi, M., Goldstein, T., Anandkumar, A., and Catanzaro, B. Long-short transformer: Efficient transformers for language and vision. Advances in Neural Information Processing Systems, 34:17723\u201317736, 2021. doi: 10.1609/aaai.v35i12.17325. URL https://ojs.aaai.org/index.php/AAAI/article/view/17325.\"}"}
{"id": "zhang23r", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Efficient Attentions with Performance Differences\\n\\ncosFormer requires a \\\\( \\\\cos \\\\)-reweighting mechanism to behave like vanilla softmax attention. However, its reweighting mechanism needs the maximum length between the source and target sequences in Eq. 16. Under causal self pattern, this operation requires different \\\\( L \\\\) values for each token of \\\\( Q \\\\) and \\\\( K \\\\), which is ignored in its released codebase repository \\\\(^5\\\\). Therefore, the released code cannot model causality.\\n\\nPerformer uses random feature matrices to project \\\\( Q \\\\) and \\\\( K \\\\). However, before transforming \\\\( Q \\\\), \\\\( K \\\\) with the softmax kernel, its released codebase \\\\(^6\\\\) uses the golden target sequence length to compute random feature maps under causal self pattern. However, this operation brings inconsistency between training and inference, because the golden target length is unavailable during inference. Thus, we cannot use the released code to perform causal self tasks.\\n\\nB. Increasing Computation Costs for Causal Efficient Attention\\n\\nIn this section, we use an efficient attention example RFA (Peng et al., 2021) to illustrate enforcing causality might bring an increase in computation for efficient attention. In RFA, the noncausal version models the query \\\\( q_t \\\\) as:\\n\\n\\\\[\\nRFA(q_t, \\\\{k_i\\\\}, \\\\{v_i\\\\}) = \\\\phi(q_t)^\\\\top P_i \\\\phi(k_i) \\\\otimes v_i \\\\phi(q_t) P_j \\\\phi(k_j) \\\\tag{7}\\n\\\\]\\n\\n\\\\[\\n= \\\\phi(q_t)^\\\\top S_t \\\\phi(q_t) \\\\cdot z_t \\\\tag{8}\\n\\\\]\\n\\nwhere \\\\( S_t \\\\in \\\\mathbb{R}^{2D \\\\times d} \\\\) and \\\\( z_t \\\\in \\\\mathbb{R}^d \\\\) and \\\\( 2D \\\\) is the dimensionality of kernel \\\\( \\\\phi \\\\). In its causal version, the hidden state \\\\( S_t \\\\) and \\\\( z_t \\\\) are recurrently modified to maintain the causality:\\n\\n\\\\[\\nS_t = S_{t-1} + \\\\phi(k_t) \\\\otimes v_t z_t \\\\tag{9}\\n\\\\]\\n\\nWe need to store \\\\( S_t \\\\) and \\\\( z_t \\\\) explicitly during training, which contains the historical information specific for each query \\\\( q_t \\\\).\\n\\nAs a result, these recurrent operations add additional \\\\( O(ndD) \\\\) time and memory consumption.\\n\\nC. Compositional Index\\n\\nWe define \\\\( s_{kj}^{i} \\\\) as the score of \\\\( i \\\\)-th attention model on \\\\( k \\\\)-th metric in the \\\\( j \\\\)-th task, where \\\\( k = 1, \\\\ldots, K \\\\) and \\\\( j = 1, \\\\ldots, M \\\\). To obtain Compositional index (CI), we first normalize \\\\( s_{kj}^{i} \\\\). For the metric whose value is higher indicating high performance, \\\\( s_{kj}^{i} \\\\) is normalized to \\\\( s_{kj}^{i} = s_{kj}^{i} - \\\\mu_{kj} \\\\sigma_{kj} \\\\), where \\\\( \\\\mu_{kj} \\\\) and \\\\( \\\\sigma_{kj} \\\\) denote the mean and standard deviation of the models' scores on this metric. Otherwise, we normalize \\\\( s_{kj}^{i} \\\\) by \\\\( s_{kj}^{i} = \\\\mu_{kj} - s_{kj}^{i} \\\\sigma_{kj} \\\\) for the metric whose value is lower indicating high performance.\\n\\nThen we average the normalized score of each metric to obtain a task score\\n\\n\\\\[\\nCI_{ij} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} s_{kj}^{i}\\n\\\\]\\n\\nwhich is further converted to the final score\\n\\n\\\\[\\nCI_i = \\\\frac{1}{M} \\\\sum_{j=1}^{M} CI_{ij}\\n\\\\]\\n\\nD. Hyperparameters\\n\\nHyperparameters for all tasks are shown in Table 4. We also report the hyperparameters of efficient attentions in Table 5.\\n\\nE. Task Score for Comparison\\n\\nWe show the compositional index of each task in Table 6. For a new efficient attention, it can use our provided means and variances to obtain the normalized score. We calculate the means and standard deviations based on vanilla, local, cosFormer, LongShort, LARA, Performer, Nystr\u00f6mformer, ProbSparse, ABC, and S4D. The means and standard deviations of metrics are shown in the Table 7, Table 8, Table 9, and Table 10.\\n\\n---\\n\\n5 https://github.com/OpenNLPLab/cosFormer/blob/main/cosformer.py#L106\\n6 https://github.com/google-research/google-research/blob/master/performer/fast_attention/tensorflow/fast_attention.py#L190\"}"}
{"id": "zhang23r", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Hyperparameters for the tasks in CAB\\n\\n| Task   | TTS  | LSTF | PCC  | Sumn | LM  | MLM | SR   |\\n|--------|------|------|------|------|-----|-----|------|\\n| Data   | LJSpeech | ETT | ECL | Weather | PCN | Multi-News | PG-19 | FFHQ | CelebA-HQ |\\n| Batch Size | 48 | 32 | 48 | 64 | 16 | 256 | 4 |\\n| Number of Steps | 20K | 6 (epochs) | 300 (epochs) | 50K | 125K | 125K | 1M |\\n| Warmup Steps | 4K | - | 4K | 1K | 10K | 5K | - |\\n| Peak Learning Rate | 5e-4 | 1e-4 | 5e-4 | 5e-4 | 5e-4 | 3e-4 | 1e-4 |\\n| Scheduler | Inverse Sqrt | Exponential Decay | LamdaLR | Inverse Sqrt | Inverse Sqrt | Polynomial Decay | Linear |\\n| Optimizer | AdamW | AdamW | AdamW | AdamW | AdamW | AdamW | AdamW |\\n| | (0.9, 0.98) | (0.9, 0.999) | (0.9, 0.98) | (0.9, 0.98) | (0.9, 0.999) | (0.9, 0.98) | (0.9, 0.999) |\\n| Clip Norm | 5.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\\n| Attention Dropout | 0.1 | 0.05 | 0.0 | 0.1 | 0.1 | 0.1 | 0.2 |\\n| Weight Decay | 0.01 | 0.0 | 5e-4 | 0.01 | 0.01 | 0.01 | 0.0 |\\n| Tokens per Batch | - | - | - | 2 | 17 | 2 | 19 |\\n| Iteration | - | 5 | 3 | 3 | - | - | - |\\n\\nF. Detailed Description of Efficient Architectures\\n\\nWe here only present the noncausal self forms of these efficient architectures. In the subsequent discussion, we denote \\\\( n \\\\) and \\\\( m \\\\) as the target and source sequence lengths, respectively. Also, we omit the factor \\\\( \\\\frac{1}{\\\\sqrt{d}} \\\\) for simplicity.\\n\\nLocal Attention\\n\\nLocal attention (Luong et al., 2015a) intuitively forces each query token to attend to its neighborhood tokens within a fixed window size \\\\( w \\\\) to reduce the computation costs. In the full attention map, the neighborhood tokens contribute a large proportion of the attention score for the querying token (Qin et al., 2021). Therefore, local attention intuitively selects \\\\( \\\\frac{w}{2} \\\\) (for causal self attention pattern) tokens around a querying token to serve as \\\\( K_w \\\\) and \\\\( V_w \\\\). Its formulation is as follows:\\n\\n\\\\[\\n\\\\text{Local}(q_i, K_w, V_w) = \\\\text{softmax}(q_i K_w^\\\\top) V_w\\n\\\\]\\n\\nwhere \\\\( K_w, V_w \\\\in \\\\mathbb{R}^{w \\\\times d} \\\\) denote the neighborhood tokens within \\\\( w \\\\) size around the query token \\\\( q_i \\\\) in \\\\( K \\\\) and \\\\( V \\\\). The complexity is of linear complexity \\\\( O(nwd) \\\\) when \\\\( w \\\\) is predefined and independent of sequence length \\\\( n \\\\). Due to the locality inductive bias, local attention is competent only on causal self and noncausal self attention patterns.\\n\\nNystr\u00f6mformer\\n\\nNystr\u00f6mformer (Xiong et al., 2021) adopts the Nystr\u00f6m method to approximate the softmax attention matrix. It selects \\\\( r \\\\) landmarks for \\\\( Q \\\\) and \\\\( K \\\\) with Segment-means (Shen et al., 2018) and produces \\\\( e_Q \\\\) and \\\\( e_K \\\\). Using iterative Moore-Penrose pseudoinverse (Razavi et al., 2014), Nystr\u00f6mformer decomposes the softmax attention matrix into three sub-softmax matrices as follows:\\n\\n\\\\[\\n\\\\text{Nystr\u00f6mformer}(Q, K, V) = \\\\text{softmax}(Q e_K^\\\\top)(\\\\text{softmax}(e_Q e_K^\\\\top)) + \\\\text{softmax}(e_Q K^\\\\top) V\\n\\\\]\\n\\nwhere \\\\( (\\\\text{softmax}(e_Q e_K^\\\\top)) + \\\\) is the Moore-Penrose pseudoinverse of \\\\( \\\\text{softmax}(e_Q e_K^\\\\top) \\\\). The complexity Moore-Penrose pseudoinverse is linear with respect to \\\\( n \\\\) when its iteration time is constant. Therefore, the overall computation cost is \\\\( O(nrd) \\\\) when \\\\( r \\\\) is predefined. Because it needs to select the same number of landmarks for \\\\( Q \\\\) and \\\\( K \\\\) and requires the full \\\\( Q \\\\) representation, it is only adaptable in the noncausal self attention pattern.\\n\\nPerformer\\n\\nPerformer (Choromanski et al., 2020) is another linear transformer using random feature map \\\\( \\\\phi(\\\\cdot) : \\\\mathbb{R}^d \\\\rightarrow \\\\mathbb{R}^{r+} \\\\) (for some \\\\( r > 0 \\\\)) as a kernel function to reduce computational complexity. Formally, the random feature map is defined to approximate the exponential kernel as\\n\\n\\\\[\\n\\\\exp(q_i^\\\\top k_j) = \\\\mathbb{E}_{q}(\\\\phi(q_i)^\\\\top \\\\phi(k_j))\\n\\\\]\\n\\nwhere \\\\( \\\\phi(x) = \\\\exp(W_r x - \\\\frac{1}{2} \\\\|x\\\\|_2^2 r^{-1}) \\\\in \\\\mathbb{R}^r \\\\) and all elements of \\\\( W_r \\\\) are independently drawn from a standard Gaussian \\\\( q(w) = \\\\mathcal{N}(w; 0, 1) \\\\). Given the defined random feature map, the whole softmax attention for each query can then be...\"}"}
{"id": "zhang23r", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Hyperparameters for various efficient attentions under noncausal self attention pattern (a), causal self attention pattern (b), noncausal cross attention pattern (c), and causal cross attention pattern (d). (*) We find that the performance of LongShort with num\\\\textsubscript{landmarks} = 64 is significantly lower than its performance with num\\\\textsubscript{landmarks} = 16 on Sum task under the noncausal self attention pattern. So we finally use LongShort with num\\\\textsubscript{landmarks} = 16.\\n\\n| Method         | Hyperparameters     | TTS Sum | LM     | ABC    | LARA   | ProbSparse | Performer | LongShort | Nystr\u00f6mformer | S4D   | cosFormer |\\n|----------------|---------------------|---------|--------|--------|--------|------------|-----------|-----------|----------------|-------|-----------|\\n|                | wsize | 15 15 15 15 | 15 15 15 15 | 15 15 15 15 | 15 15 15 15 |\\n| ABC            | num\\\\textsubscript{landmarks} | *64 64 16 16 | 64 64 16 16 | 64 64 16 16 | *64 64 16 16 |\\n| LARA           | num\\\\textsubscript{landmarks} | 64 64 16 16 | 64 64 16 16 | *64 64 16 16 | 64 64 16 16 |\\n| ProbSparse     | factor | 5 5 5 5 | 5 5 5 5 | 5 5 5 5 | 5 5 5 5 |\\n| Performer      | approx\\\\textsubscript{attn} dim | 64 64 16 16 | 64 64 16 16 | 64 64 16 16 | *64 64 16 16 |\\n| LongShort      | wsize | 15 15 15 15 | 15 15 15 15 | 15 15 15 15 | *15 15 15 15 |\\n|                | num\\\\textsubscript{landmarks} | *64 16 16 16 | 64 16 16 16 | *64 16 16 16 | 64 16 16 16 |\\n| Nystr\u00f6mformer  | conv\\\\textsubscript{kernel} size | - - - - | - - - - | - - - - | - - - - |\\n| S4D            | d\\\\textsubscript{state} | 64 64 16 16 | 64 64 16 16 | 64 64 16 16 | 64 64 16 16 |\\n| cosFormer      | - - | - - | - - | - - | - - |\\n\\nThanks to the kernel approximation, Performer achieves \\\\(O(nrd)\\\\) complexity in both noncausal self and noncausal cross attention patterns; however, for causal self and causal cross attentions, Performer has to store the cumulative summation for both \\\\(P_{mj} \\\\sum \\\\phi(k_j) v^\\\\top_j\\\\) and \\\\(P_{mj}' \\\\sum \\\\phi(k'_{j}) v^\\\\top_j\\\\), which requires a dedicated CUDA operator to accelerate the computation.\\n\\nLARA (Zheng et al., 2022) further extends the random feature approximation from estimating individual exponential kernels to directly estimating the whole softmax function. In particular, there exists a distribution \\\\(p\\\\) that instantiates the random feature (Zheng et al., 2022) such as:\\n\\n\\\\[\\nm \\\\sum \\\\phi(q^\\\\top_i k_j) v_j^\\\\top P_{mj} \\\\sum \\\\phi(k_j) v_j^\\\\top \\\\approx \\\\phi(q_i^\\\\top) \\\\sum \\\\phi(k_j) v_j^\\\\top P_{mj}' \\\\sum \\\\phi(k'_{j}) v_j^\\\\top = \\\\text{Performer}(q_i, K, V) \\\\tag{13}\\n\\\\]\\n\\nBy viewing softmax attention as an expectation over a potentially complicated distribution \\\\(p\\\\), Zheng et al. (2022) noted that Performer can be understood as performing self-normalized importance sampling to approximate this expectation, with standard Gaussian as the proposal distribution. LARA generalizes this view by adopting multiple adaptive proposal distributions to further improve the approximation fidelity. It is shown that LARA takes the following form:\\n\\n\\\\[\\n\\\\text{LARA}(q_n, K, V) = \\\\phi(q_n^\\\\top) A_n \\\\sum \\\\phi(k_j) v_j^\\\\top \\\\phi(q_n^\\\\top) A_n \\\\sum \\\\phi(k'_{j}) v_j^\\\\top,\\n\\\\tag{15}\\n\\\\]\\n\\nwhere \\\\(A_n\\\\) is a query-specific diagonal matrix that ensures the computation overall still yields a valid self-normalized importance sampling estimation; and the elements of sampled \\\\(W_r\\\\) in \\\\(\\\\phi(x) = \\\\exp(W_r x - \\\\frac{1}{2} \\\\|x\\\\|^2)\\\\) are allowed to be drawn from different distributions beyond standard Gaussian.\"}"}
{"id": "zhang23r", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6. Compositional index.\\n\\n| Method        | NS    | CS  | NC  | CC  |\\n|---------------|-------|-----|-----|-----|\\n| TTS Sum SR MLM| -0.301| -0.246| -0.077| 0.527 |\\n| TTS Sum LM    | -0.047| 0.784 | -   | 0.449 |\\n| PCC LSTF      | 1.047 | 0.867 |     |     |\\n| vanilla local  | 0.362 | 2.617| 0.421| 0.515 |\\n| ABC           | 0.043 | -0.678| -2.525| 0.414 |\\n| LARA          | -0.639| -0.522| 0.554| 0.479 |\\n| cosFormer     | 0.516 | -0.190| 1.042| 0.498 |\\n| ProbSparse    | 0.705 | -0.246| -0.697| -2.408 |\\n| Performer     | -0.282| -0.088| 0.439| -1.211 |\\n| LongShort     | 0.572 | -0.322| 0.298| 0.528 |\\n| Nystr\u00f6mformer | -2.011| -0.321| 0.088| 0.481 |\\n| S4D           | 1.035 | 0.457 | 0.176| 1.030 |\\n\\nTable 7. The means and standard deviations for noncausal self attention pattern evaluation.\\n\\n| Method       | TTS  | Sum | SR  | MLM |\\n|--------------|------|-----|-----|-----|\\n| FastSpeech   | 3.425| 1.974|     |     |\\n| Transformer-TTS|     |     |     |     |\\n| Transformer-SR3|     |     |     |     |\\n| RoBERTa      |     |     |     |     |\\n\\nFollowing the same method defined in Eq. 13, cosFormer (Qin et al., 2021) adopts another kernel function $\\\\text{ReLU}(\\\\cdot)$ instead of random feature map. Due to the importance of the softmax operator that can ensure the non-negativeness of value weights and stabilize the training process, cosFormer designed a new $\\\\cos$-based reweighting mechanism similar to softmax. Denote $q'_{i} = \\\\text{ReLU}(q_{i})$ and $k'_{j} = \\\\text{ReLU}(k_{j})$, the re-weight formula is expressed as follows:\\n\\n$$s(q_{i}, k_{j}) = q_{i}^\\\\top k_{j} \\\\cos(\\\\pi \\\\frac{i-j}{L})$$\\n\\nwhere $L = \\\\max(n, m)$. Using this formula and Ptolemy's theorem, the final expression of cosFormer is as follows:\\n\\n$$\\\\text{cosFormer}(q_{i}, K, V) = \\\\sum_{j=1}^{m} q_{i} \\\\cos((k_{j} \\\\cos)_{i} \\\\top v_{j}) + \\\\sum_{j=1}^{m} q_{i} \\\\sin((k_{j} \\\\sin)_{i} \\\\top v_{j})$$\\n\\nwhere $q_{i} \\\\cos = q_{i} \\\\cos(\\\\pi \\\\frac{i}{2L})$, $q_{i} \\\\sin = q_{i} \\\\sin(\\\\pi \\\\frac{i}{2L})$, $k_{j} \\\\cos = k_{j} \\\\cos(\\\\pi \\\\frac{j}{2L})$, $k_{j} \\\\sin = k_{j} \\\\sin(\\\\pi \\\\frac{j}{2L})$. Similar to Performer, it can efficiently fulfill noncausal self and noncausal cross attentions with linear complexity $O(n^2d)$, but its causal attention relies on a specialized CUDA operator to achieve high efficiency.\\n\\nLongShort Transformer (Zhu et al., 2021) models the long-range and short-term dependency of a sequence with two different methods discussed above. For long-range modeling, it utilizes the low-rank property of the softmax function to convert key and value to a more compact representation using a learnable matrix $W \\\\in \\\\mathbb{R}^{d \\\\times r}$:\\n\\n$$K = P^\\\\top K \\\\in \\\\mathbb{R}^{r \\\\times d}$$\\n\\nand\\n\\n$$V = P^\\\\top V \\\\in \\\\mathbb{R}^{r \\\\times d}$$\\n\\nwhere $P = \\\\text{softmax}(K W)$ and $r$ is the compressed dimension. For short-term modeling, it borrows local attention to output a token representation fused with neighborhood features via segment-wise local attention. Let $l$ be the segment length and $s_{i} = \\\\lfloor \\\\frac{i}{l} \\\\rfloor$ be the segment id for $i$-th token, segment-wise local attention produces $K_{w} = K(s_{i} - 1)l + l_{2}:(s_{i} + 1)l + l_{2}$ and $V_{w} = V(s_{i} - 1)l + l_{2}:(s_{i} + 1)l + l_{2}$. Combined with these two methods, the output of LongShort Transformer is formalized as follows:\\n\\n$$\\\\text{LongShort}(q_{i}, K, V) = \\\\text{softmax}(q_{i}[K_{w}; K])[V_{w}; V]$$\\n\\nwhere $[\\\\cdot; \\\\cdot]$ is a concatenation operator at the first dimension. The complexity $O(nrd + nwd)$ is also linear w.r.t. the target length $n$. Due to its introduction of local attention, LongShort Transformer can only complete tasks under causal self and noncausal self attention patterns.\"}"}
{"id": "zhang23r", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\\n\\nTable 8. The means and standard deviations for causal self attention pattern evaluation.\\n\\n| Method     | TTS | Sum | LM |\\n|------------|-----|-----|----|\\n| Transformer-TTS | 4.083 | 2.200 | 32.96 |\\n| Transformer  |     |     |    |\\n| GPT-2       |     |     |    |\\n| MCD         | 0.045 | 0.013 | 1.927 |\\n| MSD         |     |     |    |\\n| R-1         | 22.26 |     |    |\\n| R-2         |     |     |    |\\n| R-L         |     |     |    |\\n| PPL         |     |     |    |\\n\\nTable 9. The means and standard deviations for noncausal cross attention pattern evaluation.\\n\\n| Method     | PCC | LSTF | PoinTr | Informer | ETT | Weather | ECL |\\n|------------|-----|------|--------|----------|-----|---------|-----|\\n| CD-\u21131      | 7.702 | 0.247 | 0.762  | 1.189    | 0.806 | 0.476   | 0.620   |\\n| CD-\u21132      |     |      |       |          |     |         |     |\\n| F-score    |     |      |       |          |     |         |     |\\n| MSE        |     |      |       |          |     |         |     |\\n| MAE        |     |      |       |          |     |         |     |\\n| MSE        |     |      |       |          |     |         |     |\\n| MAE        |     |      |       |          |     |         |     |\\n| MSE        |     |      |       |          |     |         |     |\\n| MAE        |     |      |       |          |     |         |     |\\n| \u00b5          | 0.647 | 0.033 | 0.039  | 0.056    | 0.021 | 0.010   | 0.189   |\\n| \u03c3          |     |      |       |          |     |         |     |\\n\\nProbSparse Attention\\nProbSparse (Zhou et al., 2021a) calculates attention scores for few \\\\((Q, K)\\\\) dot-product pairs based on sparsity of self attention scores. Specifically, ProbSparse selects some \u201cimportant\u201d queries whose attention distributions on keys are non-uniform. Then we only need to calculate the attention scores between the \u201cimportant\u201d queries and all keys and output the weighted sum of \\\\(V\\\\). For other trivial queries, ProbSparse takes the mean of \\\\(V\\\\) as outputs.\\n\\nTo distinguish the \u201cimportant\u201d queries, ProbSparse first samples some keys \\\\(K\\\\), and then measures the sparsity of query \\\\(q_i\\\\) by \\\\(\\\\bar{M}_{q_i, K} = \\\\max_j (q_i \\\\bar{k}^\\\\top_j \\\\sqrt{d}) - 1\\\\) as follows:\\n\\n\\\\[\\n\\\\bar{M}_{q_i, K} = \\\\max_j (q_i \\\\bar{k}^\\\\top_j \\\\sqrt{d}) - 1 \\\\tag{19}\\n\\\\]\\n\\nwhere \\\\(L_K\\\\) denotes the number of keys. Finally, the Top-\\\\(u\\\\) queries with large \\\\(\\\\bar{M}_{q_i, K}\\\\) are the \u201cimportant\u201d queries.\\n\\nABC\\nIn this paper, we use ABC to denote ABC MLP (Peng et al., 2022a). ABC MLP bounds the amount of memory slot for \\\\(K\\\\) and \\\\(V\\\\) to be \\\\(r < m\\\\), independent of both \\\\(n\\\\) and \\\\(m\\\\). ABC MLP uses a learnable matrix to fill each memory slot with reweighted key and value representations:\\n\\n\\\\[\\ne_K = \\\\text{softmax}(W_K \\\\phi_K^\\\\top) K, \\\\quad e_V = \\\\text{softmax}(W_V \\\\phi_V^\\\\top) V\\n\\\\]\\n\\nwhere \\\\(W_K \\\\phi, W_V \\\\phi \\\\in \\\\mathbb{R}^{r \\\\times d}\\\\) is a learnable matrix representing the bounded memory. Leveraging the bounded representation \\\\(e_K\\\\) and \\\\(e_V\\\\), ABC MLP executes following softmax attention:\\n\\n\\\\[\\n\\\\text{ABC MLP}((q_i, K, V)) = \\\\text{softmax}(q_i e_K^\\\\top e_V) \\\\tag{20}\\n\\\\]\\n\\nABC MLP is again of linear complexity \\\\(O(nrd)\\\\).\\n\\nS4D\\nS4D (Gu et al., 2022b) is different from the above attention-based methods, as it relies on State Space Matrix called \u201cHiPPO matrix\u201d (Gu et al., 2020) which enables the whole model to be viewed as a convolutional model. It does not require the introduction of \\\\(Q, K, V\\\\) as it takes the whole modeling process as a linear time-invariant (LTI) system \\\\(u(t) \\\\rightarrow y(t)\\\\) where \\\\(u(t)\\\\) is the input sequence. To parameterize the system, S4D designs a Convolution Kernel \\\\(K(t) \\\\in \\\\mathbb{R}^{n \\\\times d}\\\\) and introduces an inner state dimension \\\\(r\\\\) during parameterization of \\\\(K(t)\\\\):\\n\\n\\\\[\\nK(t) = CE_t AB, \\\\quad y(t) = (K \\\\ast u)(t) \\\\tag{20}\\n\\\\]\"}"}
{"id": "zhang23r", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\\n\\nTable 10. The means and standard deviations for causal cross attention pattern evaluation.\\n\\n| Method       | TTS | Sum  | Transformer-TTS | Transformer | MCD | MSD |\\n|--------------|-----|------|-----------------|-------------|-----|-----|\\n| \u00b5            | 5.503 | 2.627 | 31.35           | 5.26        | 28.80 |\\n| \u03c3            | 1.292 | 0.427 | 3.771           | 1.260       | 3.286 |\\n\\nwhere $A$, $B$, $C$ are all initialized with certain values and trained with the whole model. During implementation, S4D processes $u$ and $K$ in the frequency domain and transforms back the output to the time domain:\\n\\n$$K(\\\\omega) = \\\\text{fft}(K(t))$$  \\\\hspace{1cm} (21)\\n\\n$$U(\\\\omega) = \\\\text{fft}(u(t))$$  \\\\hspace{1cm} (22)\\n\\n$$S4D(u) = \\\\text{ifft}(K(\\\\omega)U(\\\\omega))$$  \\\\hspace{1cm} (23)\\n\\nTherefore, S4D has the complexity of $O(nrd)$. FlashAttention (Dao et al., 2022b) transformers are bottlenecked by the memory when handling long sequences. It leverages the tiling method to reduce the number of memory IO between GPU high bandwidth memory and on-chip SRAM, which lowers down the IO complexity. Because it uses the same computational method as vanilla attention, its complexity is still $O(n^2)$.\\n\\nG. Efficiency Length\\n\\nWe show efficiency lengths under noncausal self attention pattern in Table 11, where we set hyperparameters as $\\\\text{emb} \\\\text{dim} = 512$, $\\\\text{head} = 8$, $\\\\text{batch \\\\ size} = 4$, $\\\\text{wsize} = 16$, $\\\\text{num \\\\ landmarks} = 16$, $\\\\text{approx \\\\ attn \\\\ dim} = 16$, $\\\\text{dstate} = 16$, $\\\\text{conv \\\\ kernel \\\\ size} = 5$. We use the adaptive factor for ProbSparse introduced in Skyformer (Chen et al., 2021d) repository.\\n\\nTable 11. The efficiency lengths of attention architectures compared to vanilla attention under noncausal self attention pattern.\\n\\n| Method     | Running Time | Memory Usage | Efficiency Length | Efficiency Length |\\n|------------|--------------|--------------|-------------------|-------------------|\\n| Performer  | 2,361        | 0.977        | 37                | 0.999             |\\n| LARA       | 2,541        | 0.975        | 68                | 0.999             |\\n| ABC        | 1,877        | 0.964        | 70                | 0.999             |\\n| Nystr\u00f6mformer | 3,045   | 0.987        | 94                | 0.999             |\\n| cosFormer  | 2,539        | 0.990        | 164               | 0.999             |\\n| ProbSparse | 3,450        | 0.989        | 281               | 0.994             |\\n| LongShort  | 5,652        | 0.998        | 342               | 0.999             |\\n| S4D        | 6,011        | 0.996        | 949               | 0.999             |\\n| local      | 4,195        | 0.996        | 1,169             | 0.999             |\\n\\nH. Benefit of Attention\\n\\nWe report the CI of backbone models with and without attention in Table 12.\"}"}
{"id": "zhang23r", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12. The CI of backbone models with and without attention on tasks. \u2018-\u2019 denotes that the method fails on the task.\\n\\n| Method          | TTS  | Sum | PCC | SR  | LSTF | MLM |\\n|-----------------|------|-----|-----|-----|------|-----|\\n| efficient attention | 1.034| 0.715| 0.720| 1.041| 0.591| 0.528|\\n| vanilla attention    | -0.300| -0.246| 0.908| -0.076| -0.102| 0.527|\\n| w/o attention      | -2.155| -1.817| 0.940| -0.058| 0.027| \u2013 |\\n\\nFigure 4. Task-to-task correlation. Except for the causal cross attention pattern, tasks within the other three patterns scarcely correlate positively with each other. This suggests that there does hardly exist a single task that can comprehensively test the capability of some attention module under an attention pattern.\"}"}
{"id": "zhang23r", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We show the supported patterns of existing efficient models in Table 13. The patterns are checked based on design philosophy rather than their implementation. We do not assess them by their submodules because the submodules among efficient attentions overlapped with each other severely.\\n\\n| Model                          | NS   | CS  | NC  | CC  |\\n|-------------------------------|------|-----|-----|-----|\\n| Transformer-XL (Dai et al., 2019) | %    | %   |     |     |\\n| Sinkhorn Attention (Tay et al., 2020a) | %    |     | %   |     |\\n| Funnel Transformer (Dai et al., 2020) | %    |     |     | %   |\\n| Synthesizer (Tay et al., 2021) | %    | %   | %   |     |\\n| FlashAttention (Dao et al., 2022b) | %    | %   | %   | %   |\\n| Axial Transformer (Ho et al., 2019) | %    | %   |     | %   |\\n| Sparse Transformer (Child et al., 2019) | %    | %   | %   | %   |\\n| Routing Transformer (Roy et al., 2021) | %    | %   | %   | %   |\\n| LogSparse Transformer (Li et al., 2019b) | %    | %   | %   | %   |\\n| Reformer (Kitaev et al., 2020) | %    | %   | %   | %   |\\n| ProbSparse/Informer (Zhou et al., 2021b) | %    | %   | %   | %   |\\n| Cluster-Former (Wang et al., 2021) | %    | %   | %   | %   |\\n| FNet (Lee-Thorp et al., 2022) | %    | %   | %   | %   |\\n| S4 (Gu et al., 2022a) | %    | %   | %   | %   |\\n| S4D (Gupta et al., 2022) | %    | %   | %   | %   |\\n| local Attention (Luong et al., 2015b) | %    | %   | %   | %   |\\n| Global Attention (Luong et al., 2015b) | %    | %   | %   | %   |\\n| Set Transformer (Lee et al., 2019a) | %    | %   | %   | %   |\\n| Compressive Transformer (Rae et al., 2020) | %    | %   | %   | %   |\\n| Longformer (Beltagy et al., 2020) | %    | %   | %   | %   |\\n| ETC (Ainslie et al., 2020b) | %    | %   | %   | %   |\\n| Linformer (Wang et al., 2020) | %    | %   | %   | %   |\\n| Linear Transformer (Katharopoulos et al., 2020a) | %    | %   | %   | %   |\\n| Big Bird (Zaheer et al., 2020) | %    | %   | %   | %   |\\n| Performer (Choromanski et al., 2021) | %    | %   | %   | %   |\\n| RFA (Peng et al., 2021) | %    | %   | %   | %   |\\n| Long-Short Transformer (Zhu et al., 2021) | %    | %   | %   | %   |\\n| Nystr\u00f6mformer (Xiong et al., 2021) | %    | %   | %   | %   |\\n| Poolingformer (Zhang et al., 2021) | %    | %   | %   | %   |\\n| Luna (Ma et al., 2021) | %    | %   | %   | %   |\\n| DPFP (Schlag et al., 2021) | %    | %   | %   | %   |\\n| Efficient Attention (Shen et al., 2021) | %    | %   | %   | %   |\\n| YOSO (Zeng et al., 2021) | %    | %   | %   | %   |\\n| XCiT (Ali et al., 2021) | %    | %   | %   | %   |\\n| SOFT (Lu et al., 2021) | %    | %   | %   | %   |\\n| Skyformer (Chen et al., 2021c) | %    | %   | %   | %   |\\n| Scatterbrain (Chen et al., 2021a) | %    | %   | %   | %   |\\n| cosFormer (Qin et al., 2022) | %    | %   | %   | %   |\\n| ABC (Peng et al., 2022b) | %    | %   | %   | %   |\\n| SAC (Li et al., 2020) | %    | %   | %   | %   |\\n| FLASH (Hua et al., 2022) | %    | %   | %   | %   |\\n| LARA (Zheng et al., 2022) | %    | %   | %   | %   |\\n| Flowformer (Wu et al., 2022) | %    | %   | %   | %   |\"}"}
{"id": "zhang23r", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 14. Details of Architectural Hyperparameters. \\\"Tr\\\" denotes Transformer and \\\"FS2\\\" denotes FastSpeech2 for brevity.\\n\\n| Backbone | FS2 | Tr-TTS | Tr | PoinTr | Informer | GPT-2 | RoBERTa | SR3ETT | ECL | WTH |\\n|----------|-----|--------|----|--------|----------|-------|---------|--------|-----|-----|\\n| # attention heads | 2 | 8 | 8 | 6 | 8 | 8 | 8 | 12 | 12 | 1 |\\n| Hidden size | 256 | 512 | 512 | 768 | 512 | 512 | 512 | 768 | 768 | 64,128,512 |\\n| Hidden size in FFN | 1024 | 2048 | 2048 | 3072 | 2048 | 2048 | 2048 | 3072 | 3072 | - |\\n| # encoder layers | 8 | 6 | 6 | 6 | 2 | 3 | 3 | - | 12 | - |\\n| # decoder layers | - | 6 | 6 | 6 | 1 | 2 | 2 | 12 | - | - |\\n\\n### Table 15. Training cost of tasks.\\n\\n| Task | TTS | FastSpeech2 | TTS Transformer-TTS | Sum | PoinTr | LSTF | ETT | LSTF | ECL | WTH |\\n|------|-----|-------------|---------------------|-----|--------|------|-----|------|-----|-----|\\n| GPU hour | 24 | 96 | 96 | 576 | 1.5 | 2 | 1.5 | 384 | 880 | 36 |\\n\\n### Table 16. Model parameters on noncausal self pattern. \\\"FS2\\\" denotes FastSpeech2 and \\\"Tr\\\" denotes Transformer for brevity.\\n\\n| Method | FS2 | Tr-TTS | Tr | SR3 | RoBERTa |\\n|--------|-----|--------|----|-----|---------|\\n| Vanilla | 41.23 | 54.40 | 123.70 | 99.55 | 124.54 |\\n| local | 41.23 | 54.40 | 123.70 | 99.55 | 124.54 |\\n| cosFormer | 41.23 | 54.40 | 123.70 | 99.55 | 124.54 |\\n| LongShort | 41.50 | 55.20 | 124.11 | 99.65 | 126.34 |\\n| LARA | 41.23 | 54.40 | 123.70 | 99.55 | 124.54 |\\n| Performer | 41.23 | 54.40 | 123.70 | 99.55 | 124.54 |\\n| Nystr\u00f6mformer | 41.23 | 54.40 | 123.70 | 99.55 | 124.54 |\\n| ProbSparse | 41.23 | 54.40 | 123.70 | 99.55 | 124.54 |\\n| ABC | 41.36 | 54.50 | 123.75 | 99.72 | 124.63 |\\n| S4D | 40.44 | 55.20 | 131.59 | 93.62 | 154.08 |\\n\\n### Table 17. Model parameters on causal self pattern.\\n\\n| Method | Transformer-TTS | Transformer | GPT-2 |\\n|--------|-----------------|-------------|-------|\\n| vanilla | 54.40 | 123.70 | 122.32 |\\n| S4D | 55.20 | 131.59 | 155.41 |\\n| LongShort | 57.57 | 126.88 | 136.61 |\\n| local | 54.40 | 123.70 | 122.32 |\\n| ABC | 54.50 | 123.75 | 122.42 |\\n\\n### Table 18. Model parameters on causal cross pattern.\\n\\n| Method | Transformer-TTS | Transformer |\\n|--------|-----------------|-------------|\\n| vanilla | 54.40 | 123.70 |\\n| ABC | 54.50 | 123.75 |\\n| Performer | 54.40 | 123.70 |\\n\\n### Table 19. Model parameters on noncausal cross pattern.\\n\\n| Method | PoinTr | Informer-ETT | Informer-ECL | Informer-WTH |\\n|--------|--------|--------------|--------------|--------------|\\n| vanilla | 52.83 | 11.33 | 20.60 | 19.49 |\\n| ABC | 52.90 | 11.33 | 20.60 | 19.50 |\\n| Performer | 52.84 | 11.33 | 20.60 | 19.49 |\\n| cosFormer | 52.84 | 11.33 | 20.60 | 19.49 |\"}"}
