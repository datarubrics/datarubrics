{"id": "Ffpg52swvg", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWe present CRUXEval (Code Reasoning, Understanding, and Execution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a general recipe for generating our execution benchmark by sampling from a model, which can be used for more challenging versions of the benchmark if needed. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval show no improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction. When it comes to reasoning about code, GPT-4 has a huge edge over other models but still fails consistently on some surprisingly simple Python programs.\\n\\n1. Introduction\\n\\nIn recent months, software engineering and programming have become increasingly mainstream domains for language models (LMs) as they attempt to conquer a potpourri of tasks including code completion, program repair, debugging, test case generation, and code optimization (see Zan et al. (2023) and Fan et al. (2023) for surveys). Recent models including Code Llama (Roziere et al., 2023), GPT-3.5 (Brown et al., 2020; Ouyang et al., 2022), and GPT-4 (OpenAI, 2023) have shown promise in code-related tasks and are being used to develop tools to help programmers.\\n\\nListing 1. Sample problems\\n\\nThe primary evaluation for code LMs is via benchmarks such as HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which test the ability to generate short code snippets from natural language specifications. While HumanEval and MBPP capture code generation abilities on simple and fundamental tasks, there is an absence of benchmarks capturing other fundamental dimensions of code LMs such as code understanding and execution.\\n\\nMotivated by this, we contribute a new benchmark, CRUXEval (Code Reasoning, Understanding, and Execution Evaluation) with two tasks: 1) output prediction, CRUXEval-O to measure code execution following and 2) input prediction, CRUXEval-I to measure code reasoning and understanding. Two samples in CRUXEval are shown in Listing 1 (modified for readability). CRUXEval examines the abilities of code LMs to reason about the execution behaviour of simple Python programs. While LMs shouldn't be expected to replace an interpreter on arbitrarily complex problems, we ensure the samples in our benchmark are simple (maximum 13 lines, no complex arithmetic) and solvable by a university-level CS graduate without needing more memory than the output (in our opinion). CRUXEval provides a useful and important probe for better understanding the capabilities of code LMs, as following a few simple steps of code execution should be a basic requirement for these models. The ability to reason about the execution behaviour of any code is a necessary requirement for these models.\"}"}
{"id": "Ffpg52swvg", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\n```python\\ndef f(text, char):\\n    if char in text:\\n        pref, char, suff = text.rpartition(char)\\n        suff = suff[:-len(char)] + char + suff[len(char):]\\n    return suff + pref\\n    return text\\n```\\n\\n```python\\ndef f(text):\\n    ls = list(text)\\n    for i in range(1, len(ls) - 1):\\n        ls.insert(i, '+\\n    return ''.join(ls).center((len(ls) - 1) * 2)\\n```\\n\\n```python\\ndef f(names, num):\\n    queue = names\\n    while len(queue) > 1:\\n        for _ in range(num):\\n            queue.append(queue.pop(0))\\n        queue.pop(0)\\n    return queue.pop()\\n```\\n\\n```python\\ndef f(text, position, value):\\n    length = len(text)\\n    index = position % (length + 1)\\n    if position < 0 or index < 0:\\n        index = length // 2\\n    new_text = list(text)\\n    new_text.insert(index, value)\\n    return ''.join(new_text)\\n```\\n\\n```python\\ndef f(array, elem):\\n    34\\n```\"}"}
{"id": "Ffpg52swvg", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\n```python\\narray.reverse()\\ntry:\\n    while elem in array:\\n        array.remove(elem)\\nfinally:\\n    array.reverse()\\nreturn array\\n```\\n\\n```python\\ndef f(nums):\\n    count = len(nums)\\n    for i in range(-count + 1, 0):\\n        nums.append(nums[i])\\n    return nums\\n```\\n\\n```python\\ndef f(nums):\\n    i1 = nums.index(swap1)\\n    i2 = nums.index(swap2)\\n    nums[i1], nums[i2], nums[i1 + 1], nums[i2 + 1] = nums[i2], nums[i1], nums[i2 + 1], nums[i1 + 1]\\n    return nums\\n```\\n\\nE.2. Direct Prediction Prompts\\n\\nIn Listings 9, 10, 11, 12, and 13, we include the prompts we use for our evaluation. We use a few-shot prompt for all models other than GPT models. For many models, we observed that using the zero-shot prompt leads to a generation that are not in an easily parsable format, and including the few-shot examples led to predictable formatting. For fairness, we also measured the performance of several few-shot prompts on the GPT models for a randomly sampled subset of the benchmark (instead of the full benchmark for cost reasons). However, we observed a decrease in performance compared to the zero-shot prompts for both input prediction and output prediction. Therefore, we decided to use the zero-shot prompt for GPT models and report numbers using that prompt. In addition, we use a separate output prediction prompt for Phind because the prompt in Listing 11 often led to explanation text before completing the assert.\\n\\nListing 9.\\nInput Prediction (non-GPT)\\nYou will be given a function f and an output in the form f(??) == output. Find any input such that executing f on the input leads to the given output. There may be multiple answers, but you should only output one. Think step by step before arriving at an answer. Finally, surround the answer, with no additional words, with [ANSWER] and [/ANSWER] tags. Express your answer as a function call that when executed will give the output.\\n\\n```python\\ndef f(my_list):\\n    count = 0\\n    for i in my_list:\\n        35\\n```\"}"}
{"id": "Ffpg52swvg", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\n```python\\nif len(i) % 2 == 0:\\n    count += 1\\nreturn count\\n```\\n\\n```python\\nassert f(??) == 3\\n```\"}"}
{"id": "Ffpg52swvg", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Listing 14. CoT input prediction prompt (Code Llama)\\n\\nYou will be given a function f and an output in the form f(??) == output. Your task is to find any input such that executing f on the input leads to the given output. There may be multiple answers, but only output one.\\n\\nFirst, think step by step. Then, surround the answer with [ANSWER] and [/ANSWER] tags. Express your answer as a function call that when executed will give the output.\\n\\n```python\\ndef f(x):\\n    return x + 1\\n```\\n\\n```python\\nassert f(??) == 17\\n```\\n\\nTo find an input such that executing f on the input leads to the given output, we can work backwards from the given assertion. We know that f(??) == 17.\\n\\nSince the function f(x) returns x + 1, for f(??) to be equal to 17, the value of ?? should be 16.\\n\\nTherefore, the function call that will give the output as 17 is:\\n\\n```\\n[ANSWER]f(16)\\n```\\n\\nListing 15. CoT input prediction prompt (GPT)\\n\\nYou will be given a function f and an output in the form f(??) == output. Your task is to find any input such that executing f on the input leads to the given output. There may be multiple answers, but only output one.\\n\\nFirst, think step by step. Then, surround the answer with [ANSWER] and [/ANSWER] tags. Express your answer as a function call that when executed will give the output.\\n\\n```python\\nassert f(??) == {output}\\n```\\n\\nListing 16. CoT output prediction prompt (Code Llama)\\n\\nYou are given a function and an input. Complete the assertion with the output of executing the function on the input. First, reason step by step before arriving at an answer. Then, surround the answer as an assertion with [ANSWER] and [/ANSWER] tags.\\n\\n```python\\ndef f(s):\\n    return s + \"a\"\\n```\\n\\n```python\\nassert f(\"hi\") == ??\\n```\\nThe function f takes a string s as input and returns the concatenation of s with the string \\\"a\\\".\\n\\nTo determine the output of executing the function f on the input \\\"hi\\\", we need to concatenate \\\"hi\\\" with \\\"a\\\".\\n\\nTherefore, the output of executing the function f on the input \\\"hi\\\" is \\\"hia\\\".\\n\\n```\\n[ANSWER]assert f(\\\"hi\\\") == \\\"hia\\\"\\n```\\n\\nListing 17. CoT output prediction prompt (GPT)\\n\\nWhat should the output of this code be so that the assertion is correct? Reason step by step before arriving at an answer. Finally, surround the answer, with no additional words, with [ANSWER] and [/ANSWER] tags.\\n\\n```python\\n37\\n```\"}"}
{"id": "Ffpg52swvg", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nD. Evaluation Results\\n\\nD.1. Main Results\\n\\nTable 2 shows the pass@1 and pass@5 results of all evaluated models on CRUXEVAL, and Fig. 16 shows them in box-plot form.\\n\\n| Model Size | Input Prediction | Output Prediction |\\n|------------|------------------|-------------------|\\n| Pass@1     | Pass@5           | Pass@1            | Pass@5           |\\n| CodeLlama 7B | 36.6%            | 36.4%             |\\n| CodeLlama 13B | 39.0%            | 38.4%             |\\n| CodeLlama 34B | 46.5%            | 41.1%             |\\n| CodeLlama Python 7B | 36.3%            | 36.4%             |\\n| CodeLlama Python 13B | 40.5%            | 37.8%             |\\n| CodeLlama Python 34B | 41.5%            | 40.7%             |\\n| StarCoder-Base 7B | 30.0%            | 31.1%             |\\n| StarCoder-Base 15.5B | 31.6%            | 33.3%             |\\n| WizardCoder 13B | 39.2%            | 37.9%             |\\n| WizardCoder 34B | 42.8%            | 41.2%             |\\n| Phi-1 1.3B | 13.9%            |\\n| Phi-1.5 1.3B | 24.1%            |\\n| Phind v2 34B | 47.9%            |\\n| Deepseek Coder-Base 6.7B | 41.1%            |\\n| Deepseek Coder-Base 33B | 46.6%            |\\n| Deepseek Coder-Instruct 6.7B | 36.6%            |\\n| Deepseek Coder-Instruct 33B | 47.4%            |\\n| Mistral 7B | 36.0%            |\\n| GPT-3.5 | 49.2%            |\\n| GPT-4 | 67.1%            |\\n\\nD.2. Additional Results on Impact of CoT\\n\\nTable 3 shows the results of including CoT on Code Llama 13B, 34B, GPT-3.5, and GPT-4.\\n\\nSample-wide improvements from CoT: In Fig. 17, we show a histogram of how much CoT improves the pass@1 score of each sample (negative values means that CoT decreased the accuracy). We observe that CoT leads to little improvement for the majority of samples, this effect is partly due to samples already having high pass@1 scores. As evidenced by Fig. 17d, we see that CoT is much more effective for GPT-4 output prediction compared to both GPT-4 input prediction and other models. For the other models, however, we observe a large proportion of samples for which CoT actually decreases the pass@1.\"}"}
{"id": "Ffpg52swvg", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 16. Main Results with confidence intervals compared to Codellama 34B.\\n\\nFigure 17. Histogram of Score Differences between CoT and Original (T = 0.2).\\n\"}"}
{"id": "Ffpg52swvg", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3. Impact of CoT on CRUXE\\n\\n| Model    | CoT Input | Prediction  | Output Prediction |\\n|----------|-----------|-------------|-------------------|\\n|          | Pass@1    | Pass@5      | Pass@1            | Pass@5          |\\n| Code Llama 13B |\u2717          | 39.0%       | 38.4%             | 58.2%           |\\n| Code Llama 13B | \u2713         | 39.1%       | 39.3%             | 55.2%           |\\n| Code Llama 34B |\u2717          | 46.5%       | 41.1%             | 64.7%           |\\n| Code Llama 34B | \u2713         | 50.4%       | 46.0%             | 68.3%           |\\n| GPT-3.5   |\u2717          | 49.2%       | 50.0%             | 66.5%           |\\n| GPT-3.5   | \u2713         | 49.1%       | 53.3%             | 76.3%           |\\n| GPT-4     |\u2717          | 67.1%       | 63.4%             | 76.8%           |\\n| GPT-4     | \u2713         | 74.8%       | 81.9%             | 88.4%           |\\n\\nIn Fig. 18, we show a more granular perspective of Fig. 7, which again highlights that CoT often decreases the pass@1 score of many samples. Again, we observe a stark difference between the impact of CoT on GPT-4 and other models.\\n\\nQualitative example of CoT harming performance: Finally, we show one example of input prediction and one example of output prediction where GPT-4 succeeds without CoT and fails with CoT.\\n\\n```python\\n# Output Prediction\\ndef f(phone_number):\\n    while phone_number.find('77777') != -1:\\n        phone_number = phone_number.replace('77777', 'seg', 1)\\n    return phone_number\\n\\nassert f('7747777722') == '774seg22'\\n\\n# GPT-4 CoT says that '77777' is not in '7747777722', returning '7747777722'\\n```\"}"}
{"id": "Ffpg52swvg", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\n# Input Prediction\\n\\n```python\\ndef f(mylist):\\n    revl = mylist[:]\\n    revl.reverse()\\n    mylist.sort(reverse=True)\\n    return mylist == revl\\n```\\n\\nassert f([5, 8]) == True\\n\\n> GPT-4 CoT correctly says that \u201cwe need to provide a list that remains the same when sorted in descending order and when reversed,\u201d but then says the list should already be sorted in descending order, returning `f([5, 4, 3, 2, 1])`.\\n\\nCorrelations between failures of different models\\n\\nCorrelations between failures of different models: Fig. 19 shows $P(Y|X=0)/P(Y)$, the accuracy of model $Y$ given that model $X$ fails completely relative to the original accuracy of model $Y$. Although what is hard for a better model tend to be hard for worse models on average, worse models succeeded on some examples where the better models fail completely, showing idiosyncrasies in failures even for the best GPT-4 CoT model.\\n\\nD.3. Results on Diversity of Generations\\n\\nDiversity of generations across models: Next, we analyze the diversity of generated inputs and outputs across various models (without regard to correctness). In Fig. 20, we plot the mean and median number of unique answers generated across samples of CRUXEval for a selection of evaluated models. First, we observe the drastic increase in diversity between using $T=0.2$ and $T=0.8$. Second, by comparing Fig. 20a with Fig. 20b, we note that input prediction generally has a larger diversity of generations than output prediction. This may be due to the fact that output prediction only has one correct answer, but input prediction may have multiple correct answers. Third, we observe that at the same temperatures, CodeLlama models have the highest diversity, while distilled models like Phind and WizardCoder have a lower diversity.\"}"}
{"id": "Ffpg52swvg", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"F. Qualitative Analysis\\n\\nIn this section, we see some examples of interesting successes and failures of the best performing model, GPT-4, with and without CoT. GPT-4 is relatively sensitive to its prompt, and slight tweaks in the prompt may lead correct examples to fail or incorrect examples to succeed. However, we believe that these examples are nevertheless interesting and reveal insights into the operating modes of GPT-4. Note that some of these examples may not be in the benchmark and were taken from a larger set of generated examples.\\n\\nF.1. Output Prediction without CoT\\n\\nF.1.1. GPT-4 Successes without CoT, Output Prediction\\n\\nEven without CoT, we found that GPT-4 achieves impressively high pass@1 scores on output prediction. We highlight a few GPT-4 successes below that we found impressive, suggesting that GPT-4 has the capability to perform somewhat complex reasoning and code execution.\\n\\n```python\\ndef f(text):\\n    if ',' in text:\\n        before, _, after = text.partition(',')\\n        return after + ' ' + before\\n    return ',' + text.partition(' ')[-1] + ' 0'\\n\\nassert f('244, 105, -90') == ' 105, -90 244'\\n# GPT-3.5 output: '-90 244'\\n# CodeLlama 34B output: '244, 105, -90 0'\\n```\\n\\n```python\\ndef f(text):\\n    text = text.lower()\\n    count = 0\\n    for char in text:\\n        if char.isalpha():\\n            count += 1\\n    return count\\n\\nassert f(\"The computer factory\") == 18\\n# GPT-3.5 output: 3\\n# CodeLlama 34B output: 16\\n```\\n\\n```python\\ndef f(text):\\n    d = {}\\n    updated = []\\n    for ch in text:\\n        if ch in d:\\n            d[ch] += 1\\n        else:\\n            d[ch] = 1\\n    while len(d) != 0:\\n        el = d.popitem()\\n        for i in range(el[1]):\\n            updated.append(el[0])\\n    return ''.join(updated)\\n\\nassert f('pdrq') == 'qrdp'\\n# GPT-3.5 output: 'pdrq'\\n# CodeLlama 34B output: 'qprd'\\n```\\n\\n```python\\ndef f(a, b):\\n    b.reverse()\\n    c = b.copy()\\n    b.extend(a.copy())\\n    b.extend(c)\\n    return b\\n\\nassert f([5, 2, 3], [4, 9, 3, 1]) == [1, 3, 9, 4, 5, 2, 3, 1, 3, 9, 4]\\n# GPT-3.5 output: [1, 3, 9, 4, 5, 2, 3]\\n# CodeLlama 34B output: [4, 9, 3, 1, 5, 2, 3, 4, 9, 3, 1]\\n```\\n\\n```python\\ndef f(s):\\n    ret = ';'.join(sorted([c for c in s if c.isalnum()]))\\n    return ret\\n\\nassert f('%*\u02c68938a(6\u02c6' * 3) == '3;3;3;6;6;6;8;8;8;8;8;8;9;9;9;a;a;a'\\n# GPT-3.5 and CodeLlama 34B both do not terminate after 500 tokens\\n```\"}"}
{"id": "Ffpg52swvg", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def f(nums, a, b):\\n    new_nums = []\\n    for n in nums:\\n        if n < a or n > b:\\n            new_nums.append(n)\\n    new_nums.sort()\\n    new_nums.extend(nums)\\n    return new_nums\\n\\nassert f([25, 44, 24, 22, 38, 5, 35, 15], 20, 44) == [5, 15, 25, 44, 24, 22, 38, 5, 35, 15]\\n\\n# GPT-3.5 output: [5, 15, 22, 24, 25, 35, 38, 44, 25, 44, 24, 22, 38, 5, 35, 15]\\n# CodeLlama 34B output: [5, 15, 22, 24, 25, 35, 38, 44, 25, 44, 24, 22, 38, 5, 35, 15]\\n\\nWe still find a set of relatively simple failures on output prediction, which we expect would be relatively simple without CoT.\\n\\ndef f(nums):\\n    nums.reverse()\\n    return \"\".join(map(str, nums))\\n\\nassert f([-1, 9, 3, 1, -2]) == '-2139-1'\\n# GPT-4 output: \"-2, 1, 3, 9, -1\"\\n\\ndef f(nums, num):\\n    for i in nums:\\n        if nums[i] == num:\\n            return num\\n    return 'Not Found'\\n\\nassert f({'elad': 186, 'colton': 162, '12': 5}, 5) == '5'\\n# GPT-4 output: 'Not found'\\n\\ndef f(text):\\n    dups = list(text)\\n    dups.append(dups[0])\\n    return ''.join(dups)\\n\\nassert f('u') == 'uu'\\n# GPT-4 output: 'u'\\n\\ndef f(match, fill, n):\\n    return fill[:n] + match\\n\\nassert f('9', '8', 2) == '89'\\n# GPT-4 output: '889'\\n\\ndef f(string, prefix):\\n    if string.startswith(prefix):\\n        return string.removeprefix(prefix)\\n    return string\\n\\nassert f(\"Vipra\", \"via\") == 'Vipra'\\n# GPT-4 output: \"\"\\n\\nSimilarly, we highlight examples from input prediction.\\n\\nF.2.1. GPT-4 Successes without CoT, Input Prediction\\n\\ndef f(l, elems):\\n    l.reverse()\\n    l.extend(elems)\\n    l.extend(l)\\n    l.reverse()\\n    l.reverse()\\n    del l[(len(l)-1):]\\n    return l\\n\\nassert f([], [-1, 2, 7, 2, 8]) == [-1, 2, 7, 2, 8, -1, 2, 7, 2]\"}"}
{"id": "Ffpg52swvg", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def f(text, position):\\n    length = len(text)\\n    index = position % length\\n    if position < 0 or index < 0:\\n        index = length // 2\\n    new_text = list(text)\\n    new_text.pop(index)\\n    return ''.join(new_text)\\n\\nassert f('voxnzcuo', 7) == 'voxnzcu'\\n\\ndef f(data, num):\\n    new_dict = {}\\n    temp = list(data.items())\\n    for i in range(len(temp) - 1, num - 1, -1):\\n        new_dict[temp[i]] = None\\n    return temp[num:] + list(new_dict.items())\\n\\nassert f({2: 10, 3: 1}, 0) == [(2, 10), (3, 1), ((3, 1), None), ((2, 10), None)]\\n\\ndef f(nums):\\n    nums = [y for y in nums if y > 0]\\n    if len(nums) <= 3:\\n        return nums\\n    nums.reverse()\\n    half = len(nums)//2\\n    return nums[:half] + [0]*5 + nums[half:]\\n\\nassert f([10, -1, 3, 2, -5, -6, 2, 6]) == [6, 2, 0, 0, 0, 0, 0, 2, 3, 10]\\n\\ndef f(text):\\n    upper = 0\\n    lower = 0\\n    for i in range(len(text)):\\n        if text[i].islower():\\n            lower+=1\\n        else:\\n            upper+=1\\n    if lower > upper:\\n        return text.swapcase()\\n    return text\\n\\nassert f('iAv,sual6eaParntEnthuhsisth,{kasae}') == 'IaV,SUAL6EApARNTeNTHUHSISTH,{KASAE}'\\n\\ndef f(name):\\n    return [name[0], name[1][::-1][0]]\\n\\nassert f(??) == ['m', 'a']\\n\\ndef f(nums):\\n    if 0 in nums:\\n        nums.pop(nums.index(0))\\n        nums.insert(0, 0)\\n    if 1 in nums:\\n        nums.pop(nums.index(1))\\n        nums.append(1)\\n    return nums\\n\\nassert f(??) == [0, 0, 1, 0, 1]\"}"}
{"id": "Ffpg52swvg", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def f(text, speaker):\\n    while text.startswith(speaker):\\n        text = text[len(speaker):]\\n    return text\\nassert f('Do you know who the other was? [NEGMENDS]', '') == 'Do you know who the other was? [NEGMENDS]'\\n\\ndef f(text):\\n    if text.isdecimal():\\n        return text\\n    else:\\n        s = ''.join(sorted(text))\\n        if s.isdecimal():\\n            return 'true'\\n        else:\\n            return 'false'\\nassert f('133476815') == '133476815'\\n\\ndef f(length, text):\\n    if len(text) == length:\\n        return text[::-1]\\n    return False\\nassert f(5, \\\"Hello\\\") == False\\n\\nF.3. Output Prediction with CoT\\nF.3.1. GPT-4 Successes with CoT, Output Prediction\\nWe see that when equipped with CoT, GPT-4 has some abilities to simulate an execution process, similar to what was seen in (Nye et al., 2021).\\n\\nIn this example, we see both the ability to reason about the flow of the code and the ability to count uppercase letters.\\n\\n```python\\ndef f(string):\\n    upper = 0\\n    for c in string:\\n        if c.isupper():\\n            upper += 1\\n    return upper * (2,1)[upper % 2]\\nassert f('PoIOarTvpoead') == 8\\n```\\n\\nGPT-4: The function f(string) counts the number of uppercase letters in the input string. If the number of uppercase letters is even, it multiplies this count by 2. If the number of uppercase letters is odd, it multiplies this count by 1.\\n\\nIn the string 'PoIOarTvpoead', there are 4 uppercase letters: 'P', 'I', 'O', and 'T'. Since 4 is an even number, the function will return 4 * 2 = 8.\\n\\nSo, the correct output of the function for the input 'PoIOarTvpoead' is 8.\\n\\n[ANSWER] 8 [/ANSWER]\\n\\nIn the next two examples, we see the ability of GPT-4 with CoT to do relatively complex string manipulation tasks.\\n\\n```python\\ndef f(text):\\n    replaced = list(text)\\n    while ' ' in replaced: replaced.remove(' ')\\n    while '@' in replaced: replaced.remove('@')\\n    while '&' in replaced: replaced.remove('&')\\n    return ''.join(replaced)\\nassert f('&C C#BiOt &[(15-5%#-u&1@t') == 'CC#BiOt[(15-5%#-u1t'\\n```\\n\\nGPT-4: The function 'f(text)' takes a string as input and returns a new string where all spaces (' '), '@' symbols, and '&' symbols have been removed.\\n\\nLet's apply this function to the string '&C C#BiOt &[(15-5%#-u&1@t':\\n\\n1. Remove all spaces: '&CC#BiOt&[(15-5%#-u&1@t'\\n2. Remove '@' symbols: 'C#BiOt&[(15-5%#-u&1t'\\n3. Remove '&' symbols: 'C#BiOt[(15-5%#-u1t'\\n\\n[ANSWER] 'C#BiOt[(15-5%#-u1t' [/ANSWER]\"}"}
{"id": "Ffpg52swvg", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Fig. 11, we plot the output prediction pass@1 scores against the input prediction pass@1 scores for each sample, observing little to no correlation between the difficulty of the two.\\n\\nMethod-level statistics: In Fig. 12, we show the number of samples containing each method in `str`, `list`, and `dict`. Even though we distilled the same number of samples using each `str` function and about twice as many for each `list` and `dict` functions, we observe that the resulting distribution is highly non-uniform. This is due to a few reasons. First, about 30% of the time, CODO sometimes fails to follow the instruction of including the library method in the resulting function. Second, some functions naturally lead to more operations that are removed by the filter. Third, common functions such as `str/list.index` or `list.append` are used in methods they are not prompted in.\"}"}
{"id": "Ffpg52swvg", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Next, we investigate trends of which methods are easier/harder for code LMs. For each method in the list, `str` and `dict` libraries listed in Appendix B.1 with at least 5 samples, we calculate the average input prediction and output prediction score of benchmark samples containing that method. We show the 7 easiest and hardest methods for Code Llama 34B (Fig. 13), WizardCoder 34B (Fig. 14), and GPT-4 (Fig. 15). Some of the hardest methods, such as `str.rsplit`, `str.maketrans`, and `str.rfind`, seem to be more obscure. We hypothesize that they may be underrepresented in pretraining corpora, explaining models' worse performance. While the distilled datasets of models like Phi, Phind, and WizardCoder are not yet available to the public, we speculate that they may include of fewer instances of these underrepresented functions and that distilling more obscure methods may help the model better learn their syntax and semantics.\"}"}
{"id": "Ffpg52swvg", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method               | WizardCoder 34B (Input) | GPT-4 (Input) | Code Llama 34B (Input) | WizardCoder 34B (Output) | GPT-4 (Output) | Code Llama 34B (Output) |\\n|----------------------|--------------------------|---------------|------------------------|--------------------------|----------------|-------------------------|\\n| str.isnumeric        | 11                       | 11            | 11                     | 11                       | 11            | 11                      |\\n| str.isascii          | 9                        | 9             | 9                      | 9                        | 9             | 9                       |\\n| list.sort            | 20                       | 20            | 20                     | 20                       | 20            | 20                      |\\n| str.isalnum          | 8                        | 8             | 8                      | 8                        | 8             | 8                       |\\n| str.isalpha          | 9                        | 9             | 9                      | 9                        | 9             | 9                       |\\n| str.istitle          | 6                        | 6             | 6                      | 6                        | 6             | 6                       |\\n| dict.keys            | 11                       | 11            | 11                     | 11                       | 11            | 11                      |\\n| dict.popitem         | 16                       | 16            | 16                     | 16                       | 16            | 16                      |\\n| list.sort            | 20                       | 20            | 20                     | 20                       | 20            | 20                      |\\n| dict.get             | 13                       | 13            | 13                     | 13                       | 13            | 13                      |\\n| str.rpartition       | 12                       | 12            | 12                     | 12                       | 12            | 12                      |\\n| str.rfind            | 11                       | 11            | 11                     | 11                       | 11            | 11                      |\\n| str.swapcase         | 9                        | 9             | 9                      | 9                        | 9             | 9                       |\\n| str.removeprefix     | 12                       | 12            | 12                     | 12                       | 12            | 12                      |\\n| str.expandtabs       | 6                        | 6             | 6                      | 6                        | 6             | 6                       |\\n| str.maketrans        | 9                        | 9             | 9                      | 9                        | 9             | 9                       |\\n| str.translate        | 9                        | 9             | 9                      | 9                        | 9             | 9                       |\\n| str.swapcase         | 9                        | 9             | 9                      | 9                        | 9             | 9                       |\\n| str.rsplit           | 8                        | 8             | 8                      | 8                        | 8             | 8                       |\\n| str.center           | 8                        | 8             | 8                      | 8                        | 8             | 8                       |\\n| str.title            | 7                        | 7             | 7                      | 7                        | 7             | 7                       |\\n| dict/list.clear      | 22                       | 22            | 22                     | 22                       | 22            | 22                      |\\n| dict.values          | 9                        | 9             | 9                      | 9                        | 9             | 9                       |\\n| list.insert          | 35                       | 35            | 35                     | 35                       | 35            | 35                      |\\n| str.format           | 10                       | 10            | 10                     | 10                       | 10            | 10                      |\\n| str.splitlines       | 11                       | 11            | 11                     | 11                       | 11            | 11                      |\\n| str.maketrans        | 9                        | 9             | 9                      | 9                        | 9             | 9                       |\\n| str.translate        | 9                        | 9             | 9                      | 9                        | 9             | 9                       |\\n| str.swapcase         | 9                        | 9             | 9                      | 9                        | 9             | 9                       |\\n\\n**Figure 14.** Easiest and hardest methods for WizardCoder 34B input and output prediction, by pass@1 score (T = 0.2).\\n\\n**Figure 15.** Easiest and hardest methods for Code Llama 34B input and output prediction, by pass@1 score (T = 0.2).\"}"}
{"id": "Ffpg52swvg", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"For evaluation, we used the gpt-3.5-turbo and gpt-4 models on October 26, 2023. Note that this is before the OpenAI developer day release of GPT-4-Turbo. The corresponding HuggingFace model URLs for the rest of the evaluated models are listed in Table 1.\\n\\n| Model Name          | HuggingFace URL                                      |\\n|---------------------|------------------------------------------------------|\\n| Mistral (7B)        | https://huggingface.co/mistralai/Mistral-7B-v0.1     |\\n| Phi-1 (1.3B)        | https://huggingface.co/microsoft/phi-1               |\\n| Phi-1.5 (1.3B)      | https://huggingface.co/microsoft/phi-1_5             |\\n| DeepSeek Instruct (6.7B) | https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct |\\n| DeepSeek Instruct (33B) | https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct |\\n| DeepSeek Base (6.7B) | https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base |\\n| DeepSeek Base (33B)  | https://huggingface.co/deepseek-ai/deepseek-coder-33b-base |\\n| StarCoderBase (15.5B) | https://huggingface.co/bigcode/starcoderbase         |\\n| StarCoderBase (7B)  | https://huggingface.co/bigcode/starcoderbase-7b      |\\n| WizardCoder (13B)   | https://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0 |\\n| WizardCoder (34B)   | https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0 |\\n| Phind (34B)         | https://huggingface.co/Phind/Phind-CodeLlama-34B-v2  |\\n| CodeLlama 7B        | https://huggingface.co/codellama/CodeLlama-7b-hf     |\\n| CodeLlama (13B)     | https://huggingface.co/codellama/CodeLlama-13b-hf    |\\n| CodeLlama (34B)     | https://huggingface.co/codellama/CodeLlama-34b-hf    |\\n| CodeLlama Python (7B) | https://huggingface.co/codellama/CodeLlama-7b-Python-hf |\\n| CodeLlama Python (13B) | https://huggingface.co/codellama/CodeLlama-13b-Python-hf |\\n| CodeLlama Python (34B) | https://huggingface.co/codellama/CodeLlama-34b-Python-hf |\"}"}
{"id": "Ffpg52swvg", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Limitations and Future Work\\n\\nCorrelations between various code tasks: While our benchmark serves as an interesting lens to analyze code LMs, one might object that output prediction can simply be done with a Python interpreter and that input prediction abilities can be greatly enhanced by equipping a LM with an interpreter, like in GPT-4 Code Interpreter mode. While this is true, we believe that a good code LM still ought to have good code understanding and execution capabilities, similar to that of a strong programmer. We see that base models have a reasonably strong correlation between HumanEval, input prediction, and output prediction score. An interesting future direction is to more deeply investigate the correlations between performance on various code-related tasks such as code completion, execution, bug-finding, and code summarization.\\n\\nDistilling future execution benchmarks: Our benchmark only measures the input and output prediction accuracy of relatively simple and self-contained Python functions distilled from a single model (Code Llama 34B). It would also be interesting to measure these capabilities on longer and more difficult code snippets, open-domain code samples, or code in other programming languages. As our distillation technique is relatively general, we welcome others to create their own benchmarks measuring the execution of code snippets from other distributions.\\n\\nVariation due to prompt and temperature: The accuracy of a model on our benchmark may be very sensitive to the prompt and task format (Mizrahi et al., 2023). We try our best to address this by using prompts that are similar as possible across models (see Appendix E.2 and E.3) but understand that some prompts may improve the performance of certain models while decrease the performance on others. There are also countless prompting techniques (see (Liu et al., 2023d) for a comprehensive survey) that can be tried to improve the performance. We also run all our experiments with $T = 0$.2 and $T = 0.8$ due to budget constraints, but different temperatures will lead to different performance for all models. One must always be cautious and critical when using benchmarks to compare models. For example, for input prediction, while Phind v2's 47.9% pass@1 may seem to beat CodeLlama's 46.5%, the standard deviations of both models with respect to the 800 samples selected turns out to be around 1.5%, so this conclusion cannot be made.\\n\\nInformation loss due to pass@1: While the average pass@k metric is common in the code generation literature, it compresses a large amount of information into one number. While we suggest reporting pass@1 and pass@5 for our benchmark, we comment that pass@k is only one perspective of measuring execution ability. We try to shed more light on behaviour by including a bit more analysis throughout this work, but encourage the development of different evaluation and analysis techniques.\\n\\nFine-tuning: In our first fine-tuning experiment, we only check for exact string match when decontaminating the fine-tuning set, so there may still be semantic duplication or similar programs with small modifications, which may lead to a higher performance than if those examples were removed. In this work, we only consider the most direct and straightforward fine-tuning scheme. We believe there is room for improvement via more sophisticated techniques, such as using process supervision (Uesato et al., 2022), fine-tuning on correct CoT generations, or fine-tuning on snippets of code while including the program state after each step. Seeing that models like Phi, WizardCoder, and Phind outperformed Code Llama on HumanEval but not CRUXEval inspires the need for a deeper investigation of the utility of finetuning on distilled data from a more powerful model. Lastly, it remains a curiosity whether fine-tuning on execution information can help code generation abilities.\\n\\nJointly improving code generation and code execution: As we discovered, distilled models like Phi, Phind, and WizardCoder that are fine-tuned on code generation do not improve significantly on CRUXEval compared to their base models. It is unknown whether the opposite is true: does improved fine-tuning on code execution lead to better code generation abilities? It would also be interesting to explore techniques that can lead to improved performance on both code generation and code execution simultaneously.\\n\\nUnderstanding reasoning from the lens of code: As future work, we believe that our benchmark serves as a good starting point towards understanding the code reasoning abilities of LM. Many further execution evaluations may be possible, such as testing execution of recursive functions, execution from a natural language description and an input, or execution of a composition of two functions. We find that output prediction serves as a good testbed for understanding CoT failures, because each step clearly corresponds to an operation with a ground truth, so reasoning failures can be pinpointed. We observed many examples of CoT failures due to simple mistakes that the model seems to have knowledge about (see Appendix F.3.2 for examples), and it should be possible to analyze and characterize this behaviour more systematically.\\n\\nSelf-repair: Lately, self-repair has been used to improve the reasoning and programming abilities of LLMs (Chen et al., 2023; Olausson et al., 2023b; Madaan et al., 2023b; Peng et al., 2023; Zhang et al., 2023c; Tyen et al., 2023). From our\"}"}
{"id": "Ffpg52swvg", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"qualitative analysis, we find that when using CoT, many output prediction failures are recitation errors of information the model may already understand. Therefore, we believe that these mistakes may be easier to repair than when the correct reasoning path is not found in the first place, and that CRUXEval may be a simpler task to better understand model repair capabilities.\"}"}
{"id": "Ffpg52swvg", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide more details about the process of constructing our benchmark. A sample prompt for generating functions and test cases is shown in Listing 6. The prompt is constructed by including two few-shot examples, one containing a specified `str` function and one containing a specified `list` function. The full list of specified functions is given in B.1, and the full list of few-shot examples chosen from is given in B.2. We learned that having random-looking inputs instead of common words and phrases in the few-shot prompts significantly increased the difficulty of the benchmark.\\n\\nListing 6. Sample prompt for generating functions and test cases\\n\\nYou will be given a function name between `[TASK]` and `[/TASK]` tags. Following the examples given, write a Python function that makes use of the given function and 5 test inputs for that function.\\n\\n```\\n[TASK]\\nstr.center\\n[/TASK]\\n\\n```python\\ndef f(text):\\n    ls = list(text)\\n    for i in range(1, len(ls) - 1):\\n        ls.insert(i, '+\\n    return ''.join(ls).center((len(ls) - 1) * 2)\\n```\\n\\n```\\n[TEST]\\nassert f('lynel') == ??\\nassert f('nzoh') == ??\\nassert f('u') == ??\\nassert f('anfsoixz') == ??\\nassert f('xzd') == ??\\n[/TEST]\\n\\n```\\n\\n```\\n[TASK]\\nlist.append\\n```\\n\\n```python\\ndef f(nums):\\n    count = len(nums)\\n    for i in range(-count+1, 0):\\n        nums.append(nums[i])\\n    return nums\\n```\\n\\n```\\n[TEST]\\nassert f([2, 6, 1, 3, 1]) == ??\\nassert f([7, 1, 2, 6, 0, 2]) == ??\\nassert f([4, 3, 2, 1, 2, -1, 4, 2]) == ??\\nassert f([0, 6, 2, -1, -2]) == ??\\nassert f([-6, -2, 1, -3, 0, 1]) == ??\\n[/TEST]\\n```\\n\\n```\\n[TASK]\\nstr.zfill\\n```\\n\\n```python\\n```\\nB.1. Functions used in prompt\\n\\nFor each of `str`, `list`, and `dict`, we use all the non-dunder methods under that class. The resulting list of methods is as follows:\\n\\n- `str`: capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, format_map, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, maketrans, partition, removeprefix, removesuffix, replace, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill\\n\\n- `list`: append, clear, copy, count, extend, index, insert, pop, remove, reverse, sort\\n\\n- `dict`: clear, copy, fromkeys, get, items, keys, pop, popitem, setdefault, update, values\\n\\nMotivated by seeing a GPT-4 failure of treating the `\u02c6` symbol as an exponential rather than an xor, we also attempted using all the non-dunder methods from `operator`. However, we found that the majority of the functions obtained were either\"}"}
{"id": "Ffpg52swvg", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nToo simple and uninteresting, or too computational, since many of the methods under operator are bit-manipulation or calculational operations. Therefore, we excluded it from our final benchmark.\\n\\nB.2. Few-shot Examples\\n\\nWe use 10 handwritten few-shot examples, 5 using str functions and 5 using list functions. For each prompt, we include two few-shot examples, one string few-shot example and one list few-shot example, for a total of 25 different combinations of few-shot prompts. We generate programs and inputs using CodelLama with temperature $T = 1$.\\n\\nOne interesting observation is that for a fixed pair of few-shot examples, there seems to be a limit to the number of diverse functions that can be generated: after about 60000 generations, only about 5000 of them were unique. Using all 25 combinations of few-shot prompts helps to overcome this duplication bottleneck.\\n\\nThe full set of few-shot examples can be found in Listing 8.\\n\\nB.3. Dataset Statistics\\n\\nIn Fig. 9, we show the distribution of character count and line count of the 800 samples in our benchmark.\\n\\nIn Fig. 10, we show the distribution of the \\\"step count\\\" of programs (with one outlier of 3175 steps excluded). Here, steps roughly correspond to Python bytecode operations during execution. The precise definition can be understood by checking the \\\"numsteps\\\" variable in our code here.\"}"}
{"id": "Ffpg52swvg", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Remove all '@' symbols: 'CC#BiOt[(15-5%-u&1t'\\n3. Remove all '&' symbols: 'CC#BiOt[(15-5%-u1t'\\n\\nSo, the output of the function for the given string should be 'CC#BiOt[(15-5%-u1t'.\\n\\n[ANSWER] 'CC#BiOt[(15-5%-u1t' [/ANSWER]\\n\\n```python\\ndef f(widths):\\n    reversed_names = []\\n    for skill in sorted(widths, key=widths.get, reverse=True):\\n        reversed_names.append(\"{0}={1}\".format(widths[skill], skill))\\n    return '&'.join(reversed_names)\\n```\\n\\nGPT-4: The function f takes a dictionary as an argument. The dictionary keys are skill names and the values are their corresponding widths. The function sorts the dictionary based on the values in descending order. Then, it creates a list of strings where each string is a combination of the value and the key separated by an equal sign. Finally, it joins all the strings in the list with an ampersand (&) and returns the resulting string.\\n\\nLet's break down the steps:\\n1. The dictionary {'craftsmanship':2, 'upholstery':1, 'tailoring':3} is sorted based on the values in descending order, resulting in the list ['tailoring', 'craftsmanship', 'upholstery'].\\n2. For each skill in the list, a string is created with the format \"{value}={key}\". The resulting list is ['3=tailoring', '2=craftsmanship', '1=upholstery'].\\n3. The strings in the list are joined with an ampersand (&), resulting in the string '3=tailoring&2=craftsmanship&1=upholstery'.\\n\\n[ANSWER] '3=tailoring&2=craftsmanship&1=upholstery' [/ANSWER]\\n\\nIn the next two examples, we see the ability to simulate and reason about for/while loops.\\n\\n```python\\ndef f(nums):\\n    count = 0\\n    for i in range(len(nums)):\\n        if len(nums) == 0:\\n            break\\n        if count % 2 == 0:\\n            nums.pop()\\n        else:\\n            nums.pop(0)\\n        count += 1\\n    return nums\\n```\\n\\nGPT-4: The function 'f(nums)' takes a list of numbers as input. It initializes a counter 'count' to 0. Then it enters a loop that runs for the length of the list. In each iteration, it checks if the length of the list is 0. If it is, it breaks the loop. This condition will never be true because the loop runs for the initial length of the list and not the current length of the list. Then it checks if the counter is even. If it is, it removes the last element from the list. If it's not, it removes the first element from the list. After each iteration, it increments the counter by 1.\\n\\nSo, for the input list [3, 2, 0, 0, 2, 3], the function will perform the following operations:\\n- Remove the last element (3) because the counter is 0 (even). The list becomes [3, 2, 0, 0, 2].\\n- Remove the first element (3) because the counter is 1 (odd). The list becomes [2, 0, 0, 2].\\n- Remove the last element (2) because the counter is 2 (even). The list becomes [2, 0, 0].\\n- Remove the first element (2) because the counter is 3 (odd). The list becomes [0, 0].\\n- Remove the last element (0) because the counter is 4 (even). The list becomes [0].\\n- Remove the first element (0) because the counter is 5 (odd). The list becomes [ ].\\n\\nSo, the function should return an empty list for the input [3, 2, 0, 0, 2, 3].\\n\\n[ANSWER] [ ] [/ANSWER]\\n\\n```python\\ndef f(number):\\n    return 42\\n```\\n\\n42\"}"}
{"id": "Ffpg52swvg", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nmultiplier = 2\\npower = 1\\n\\nwhile number > 0:\\n    remainder = number % multiplier\\n    number //= multiplier\\n    if remainder == 0:\\n        power += 1\\n    if number == 1:\\n        return f'{2 ** power}-free'\\n\\nassert f(8) == '16-free'\\n\\n# GPT-4: The function f takes a number as input and checks if it is divisible by 2. If it is, it increments the\\n\u2192\\npower by 1 and continues to divide the number by 2 until the number is no longer divisible by 2. The\\n\u2192\\nfunction then returns a string that says the number is \\\"2 to the power of (power + 1)-free\\\".\\n\\nIn the case of the number 8, the function will divide 8 by 2 three times before it is no longer divisible by 2. This\\n\u2192\\nmeans the power will be incremented three times, from 1 to 4. Therefore, the function will return the\\n\u2192\\nstring \\\"2 to the power of 4-free\\\", or \\\"16-free\\\".\\n\\nSo, the output of the code should be \\\"16-free\\\" for the assertion to be correct.\\n\\n[ANSWER] \\\"16-free\\\"\\n\\nF.3.2. GPT-4 FAILURES WITH\\n\\nNow, we highlight some failure cases of GPT-4 output prediction with CoT. One interesting failure mode is that the CoT\\nmakes a mistake on information that GPT-4 seems to know when prompted independently. For example, in the first example\\nbelow, the CoT insists that 6173 is less than 1000. However, when prompted independently, GPT-4 knows this is not true. We\\nprovide a few examples of these paired mistakes with proof of knowledge below. The outputs are abbreviated for brevity\\nand to highlight the failures.\\n\\nThe first example shows a failure in number comparison.\\n\\nPrompt: What is the output of the code?\\n\\ndef f(num):\\n    if 0 < num < 1000 and num != 6174:\\n        return 'Half Life'\\n    return 'Not found'\\n\\nassert f(6173) == ??\\n\\nGPT-4: ... In the case of 'f(6173)', the number 6173 is greater than 0, less than 1000, and not equal to 6174, so\\n\u2192\\nthe function will return 'Half Life'...\\n\\nPrompt: If num = 6173, is it true that 0 < num < 1000 and num != 6174?\\n\\nGPT-4: No, the statement is not true. The variable num is assigned the value 6173, so the condition 0 < num < 1000\\n\u2192\\nis false because 6173 is not less than 1000. The second condition, num != 6174, is true because 6173 is\\n\u2192\\nindeed not equal to 6174. However, since the first condition is false, the overall statement is false.\\n\\nThis example is showing a semantic misunderstanding of str.islower.\\n\\nPrompt: What is the output of the code?\\n\\ndef f(text):\\n    if not text.islower():\\n        temp_text = ''\\n        for i in range(len(text)):\\n            if text[i].islower():\\n                temp_text += text[i]\\n        return temp_text\\n    return text\\n\\nassert f(\\\"816a\\\") == ??\\n\\nGPT-4: ... The function 'f' takes a string 'text' as input and checks if the string is not all lowercase using the '\\n\u2192\\nislower()' method ...\\n\\n...In the given assertion, the input string is \\\"816a\\\". This string is not all lowercase because it contains digits\\n\u2192\\n, which are neither lowercase nor uppercase. Therefore, the function will go into the loop and check each\\n\u2192\\ncharacter ...\\n\\nPrompt: What is \\\"816a\\\".islower()?\"}"}
{"id": "Ffpg52swvg", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nGPT-4: \\n\\nThe string \\\"816a\\\" contains digits and a lowercase alphabetic character 'a'. Since the only alphabetic character in the string is lowercase, the 'islower()' method would return 'True' when called on this string.\\n\\nThis example shows a failure with sorting \\\"h e l l o\\\", which seems like it could be a tokenization artifact.\\n\\nPrompt: What is the output of this code?\\n\\ndef f(str):\\n    return str and ''.join(sorted(str))\\n\\nassert f(\\\"h e l l o\\\") == ??\\n\\nGPT-4: \\n\\n1. The input string \\\"\\\"h e l l o\\\" is truthy, so the condition 'str' is 'True'.\\n2. The 'sorted' function is called on the string, which results in the list [' ', ' ', ' ', 'e', 'h', 'l', 'l', 'o'].\\n\\nPrompt: In Python, what is sorted(\\\"h e l l o\\\")?\\n\\nGPT-4: \\n\\nHere's what happens:\\n\\n```python\\nresult = sorted(\\\"h e l l o\\\")\\n```\\n\\nThe 'result' will be a list that contains the characters of the string, including spaces, sorted in ascending order based on their Unicode code points. The space character has a lower code point than any of the letters, so it will appear first in the sorted list. The result will look like this:\\n\\n```python\\n[' ', ' ', ' ', ' ', 'e', 'h', 'l', 'l', 'o']\\n```\\n\\nThis example shows a failure with the `str.strip` method.\\n\\nPrompt: What is the output of this code?\\n\\ndef f(text, strip_chars):\\n    return text[::-1].strip(strip_chars)[::-1]\\n\\nassert f('tcmfsmj', 'cfj') == ??\\n\\nGPT-4: The function 'f' takes two arguments: 'text' and 'strip_chars'. It reverses the 'text' string, strips characters from both ends that are found in 'strip_chars', and then reverses the string back to its original order.\\n\\nLet's break down what happens step by step with the input 'f('tcmfsmj', 'cfj')':\\n\\n1. The string 'tcmfsmj' is reversed, resulting in 'jmsfmct'.\\n2. The 'strip' method is called with 'cfj' as the argument, which removes any leading or trailing characters found in 'cfj' from the reversed string 'jmsfmct'.\\n3. The leading 'j' and the trailing 'c' and 't' are stripped from the string, resulting in 'msfm'.\\n4. The stripped string 'msfm' is then reversed again, resulting in 'mfsm'.\\n\\nTherefore, the assertion should be:\\n\\n```python\\nassert f('tcmfsmj', 'cfj') == 'mfsm'\\n```\\n\\nThe output of the code, if the assertion passes, is nothing since assertions do not produce output if the condition is true.\\n\\nPrompt: In Python, what is \\\"jmsfmct\\\".strip(\\\"cfj\\\")?\\n\\nGPT-4: \\n\\nHere's what happens step by step:\\n\\n- The first character 'j' is in the set of characters to remove ('cfj'), so it is removed.\\n- The next character 'm' is not in the set of characters to remove, so the stripping stops at the beginning of the string.\\n- Looking at the end of the string, the last character 't' is not in the set of characters to remove, so the stripping stops at the end of the string without removing anything.\\n\\nThe resulting string after the 'strip' operation will be '\\\"msfmct\\\".'\"}"}
{"id": "Ffpg52swvg", "page_num": 45, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nIn this example, we observe a list indexing error.\\n\\nPrompt: What is the output of this code?\\n```python\\ndef f(nums):\\n    count = len(nums)\\n    for i in range(-count+1, 0):\\n        nums.extend([nums[i], nums[i]])\\n    return nums\\n```\\n\\nGPT-4: \\n- Initial list: `[0, 6, 2, -1, -2]`\\n- `i = -4`: Extend with two copies of `nums[-4]` which is `6`. New list: `[0, 6, 2, -1, -2, 6, 6]`\\n- `i = -3`: Extend with two copies of `nums[-3]` which is `2`. New list: `[0, 6, 2, -1, -2, 6, 6, 2, 2]`\\n...\\n\\nPrompt: In Python, what is the value at index -3 of `[0, 6, 2, -1, -2, 6, 6]`?\\n\\nGPT-4: In Python, negative indices count from the end of the list, with -1 being the last element. To find the value at index -3, we count three places from the end of the list.\\n\\nGiven the list:\\n```\\n[0, 6, 2, -1, -2, 6, 6]\\n```\\nThe indices of the elements are as follows:\\n```\\n0 1 2 3 4 5 6 (positive indices)\\n-7 -6 -5 -4 -3 -2 -1 (negative indices)\\n```\\nSo, the value at index -3 is `-2`.\\n\\nIn this example, we see a string indexing error.\\n\\nPrompt: In Python, what is the output of this code?\\n```python\\ndef f(text):\\n    for i in range(-len(text), 0):\\n        if text[i].islower():\\n            return text[:i] + text[i].swapcase() + text[-i:]\\n    return text\\n```\\n\\nGPT-4: \\n- When `i = -3`, `text[i]` is `''o''`, which is lowercase. So the function will return a new string according to the rule above...\\n- `text[-i:]` is `''o5''` (the substring from the character after the current character to the end of the string).\\n\\nTherefore, the function will return `''BOo5''`...\\n\\nPrompt: In Python, if text = \\\"Boo5\\\" and i = -3, what is text[-i:]?\\n\\nGPT-4: ...Given 'text = \\\"Boo5\\\"' and 'i = -3', let's calculate '-i':\\n```\\n'i = -(-3) = 3'\\n```\\nSo, 'text[-i:]' is equivalent to 'text[3:]', which means we start from the 3rd index (keeping in mind that indexing starts at 0) and take all characters from that position to the end of the string.\\nThe character at index 3 in the string \\\"Boo5\\\" is '5'. Therefore, 'text[-i:]' will return '5'.\\n\\nF.4. Input Prediction with CoT\\n\\nF.4.1. GPT-4 Successes with Output Prediction\\n\\nThe following examples highlights code reasoning abilities.\\n\\n```python\\ndef f(text, size):\\n    counter = len(text)\\n```\\n\\n45\"}"}
{"id": "Ffpg52swvg", "page_num": 50, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the following example, it is very easy to solve the task by choosing source and target to make target in source false, going directly to the last line. However, GPT-4 tries to go down the more complicated path.\\n\\n```python\\ndef f(source, target):\\n    if target in source:\\n        prefix, suff = source.split(target)\\n        prefix = prefix[:len(prefix) // 2] + target + prefix[len(prefix) // 2:]\\n    return prefix + suff\\n    return source\\nassert f('abcjava', 'python') == 'abcjava'\\n```\\n\\nIn the following example, an important insight is that the last line of the function is a \\\".....\\\".join(), but that \\\".....\\\" does not occur in the output. This confuses the model.\\n\\n```python\\ndef f(letters):\\n    letters_only = letters.strip(\\\"., !?*\\\")\\n    return \\\".....\\\".join(letters_only.split(\\\" \\\"))\\nassert f(??) == 'h,e,l,l,o,wo,r,ld'\\n```\\n\\nF.5. Simple and Isolated GPT-4 Failures\\n\\nSome of the failures from both input prediction and output prediction can be isolated to capabilities on smaller subtasks. Here, we highlight some interesting subtasks, as well as GPT-4's response to these subtasks.\\n\\n**Prompt:** What is a string containing 'a' three times, 'b' three times, 'c' twice, 'd' three times, and 'z' twice?\\n\\n**GPT-4:** A string containing the specified number of each letter can be constructed in many different ways, as the order of the characters is not specified. Here is one example of such a string: 'aaabbbccdzz'\\n\\n**Prompt:** In Python, what is \\\" BaB \\\".rfind(\\\" B \\\")?\\n\\n**GPT-4:** In Python, the 'rfind()' method of a string object returns the highest index of the substring if found in the given string. If the substring is not found, it returns -1.\\n\\nThe string '\\\" BaB \\\"' has a space, followed by a capital 'B', followed by another space as the substring we are looking for with 'rfind(\\\" B \\\")'.\\n\\nLet's break down the string '\\\" BaB \\\"':\\n- Index 0: space (' ')\\n- Index 1: 'B'\\n- Index 2: 'a'\\n- Index 3: 'B'\\n- Index 4: space (' ')\\n\\nWe are looking for the substring '\\\" B \\\"' (space, 'B', space). This substring appears only once in the string, starting at index 3 and ending at index 5 (if we consider the space after the last 'B').\\n\\nTherefore, '\\\" BaB \\\".rfind(\\\" B \\\")' will return 3, because that is the highest index at which the substring '\\\" B \\\"' starts.\\n\\n**Prompt:** In Python, if I have a string s = 'iabnm\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc', what is s[1::2]?\\n\\n**GPT-4:** In Python, the slicing operation 's[start:stop:step]' allows you to extract a substring from a string 's' by specifying a starting index 'start', a stopping index 'stop', and a step 'step'. If 'start' is omitted, it defaults to the beginning of the string (index 0). If 'stop' is omitted, it defaults to the end of the string. If 'step' is omitted, it defaults to 1.\\n\\nIn your example, 's[1::2]' means that you want to start at index 1 of the string 's', go until the end of the string (since no stop index is provided), and select every second character (because the step is 2).\\n\\nGiven the string 's = 'iabnm\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc', let's break down what 's[1::2]' will return:\\n- Start at index 1: 'a'\\n- Every second character from index 1: 'abnm\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc' -> 'an\u02dc'\\n\\nSo, 's[1::2]' will return the string ''an\u02dc'.'\\n\\n**Prompt:** In Python, what is \\\"+.join([*, '+', 'n', 'z', 'o', 'h'])?\\n\\n```python\\n50\\n```\"}"}
{"id": "Ffpg52swvg", "page_num": 51, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nIn Python, the 'join' method is used to concatenate a sequence of strings with a specified separator. In the example you provided, `'+'.join(['*', '+', 'n', 'z', 'o', 'h'])`, the 'join' method is called on the string '\"+', which is the separator, and it concatenates the elements of the list ['*', '+', 'n', 'z', 'o', 'h]' with a plus sign between each element.\\n\\nThe result of this operation would be a single string where each element from the list is separated by a plus sign:\\n\\n```python\\n'*+*+n+z+o+h'\\n```\\n\\nEach element in the list is a string, and they are joined together in the order they appear in the list, with the '+' character inserted between each pair of elements.\\n\\nPrompt: In Python, if text = \"!123Leap and the net will appear\" and res = 123, what is text[len(str(res)):]?\\n\\nGPT-4: In the given Python code snippet, you have a string 'text' and an integer 'res'.\\n\\n```python\\ntext = \"!123Leap and the net will appear\\nres = 123\\n```\\n\\nYou want to know what 'text[len(str(res)):]' evaluates to. Let's break it down step by step:\\n\\n1. 'str(res)' converts the integer 'res' to a string, which is '\"123\"'.\\n2. 'len(str(res))' calculates the length of the string '\"123\"', which is '3'.\\n3. 'text[len(str(res)):]' slices the string 'text' starting from the index equal to the length of the string representation of 'res' (which is '3') to the end of the string.\\n\\nSo, 'text[len(str(res)):]' will return the substring of 'text' starting from the 4th character (since indexing in Python starts at 0) to the end of the string.\\n\\nThe 4th character in the string 'text' is the character ''L'' (the first three characters are '\"!12\"'), so the result will be:\\n\\n```python\\n\\\"Leap and the net will appear\\\"\\n```\\n\\nThis is the substring of 'text' starting from the 4th character to the end.\\n\\nPrompt: In Python, what is \\\"pomodoro\\\".replace(\\\"or\\\", \\\"pomodoro\\\")?\\n\\nGPT-4: In Python, the 'replace' method is used to replace occurrences of a substring within a string with another substring. When you call '\\\"pomodoro\\\".replace(\\\"or\\\", \\\"pomodoro\\\")', you are asking Python to replace every occurrence of the substring '\\\"or\\\"' in the string '\\\"pomodoro\\\"' with the substring '\\\"pomodoro\\\"'.\\n\\nHere's what happens step by step:\\n\\n1. Python finds the first occurrence of '\\\"or\\\"' in '\\\"pomodoro\\\"', which is at the end of the string.\\n2. Python replaces this occurrence with '\\\"pomodoro\\\"', resulting in the string '\\\"pomodopomodoro\\\"'.\\n\\nThere is only one occurrence of '\\\"or\\\"' in the original string, so the final result of the 'replace' method in this case is '\\\"pomodopomodoro\\\"'.\\n\\nF.6. Failing Gems\\n\\nNext, we identify a collection of examples that we find GPT-4 often fails on, even with CoT. Some of these examples overlap with examples shown above, but others are new. Overall, we believe this collection of examples provides an interesting lens to understand the behaviour of GPT-4 on input and output prediction.\\n\\nF.6.1. Failing, Output Prediction\\n\\ndef f(nums):\\n    for i in range(-len(nums), 0):\\n        nums.insert(-i, nums[i])\\n    return nums\\n\\nassert f([-6, -2, 1, -3, 0, 1]) == [-6, -6, -2, 1, 1, 1, -3, 0, 0, 1, 1, -6]\"}"}
{"id": "Ffpg52swvg", "page_num": 52, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"if text[i].islower():\\n    temp_text += text[i]\\nreturn temp_text\\nreturn text\\n\\nassert f(\\\"816a\\\") == '816a'\\ndef f(list, separator):\\n    text = separator.join(list)\\n    return separator.join(reversed(text))\\n\\nassert f(['is', 'it', 'top'], '@') == 'p@o@t@@@t@i@@@s@i'\\ndef f(text, res):\\n    for c in \u2018*' \u2018\u201d\u2019:\\n        text = text.replace(c, \u2018!' + str(res))\\n    if text.startswith('!'):\\n        text = text[len(str(res)):]  \\n    return text\\n\\nassert f('\"Leap and the net will appear', 123) == '3Leap and the net will appear'\\ndef f(num):\\n    if 0 < num < 1000 and num != 6174:\\n        return \u2018Half Life\u2019\\n    return \u2018Not found\u2019\\n\\nassert f(6173) == \u2018Not found\u2019\\ndef f(date):\\n    return date[6:] + date[4:6] + date[0:4]\\n\\nassert f(\\\"08-10-2009\\\") == '20090-08-1'\\ndef f(text, suffix):\\n    if suffix and suffix[-1] in text:\\n        return f(text.rstrip(suffix[-1]), suffix[:-1])\\n    else:\\n        return text\\n\\nassert f('rpyttc', 'cyt') == 'rpytt'\\ndef f(s, x):\\n    count = 0\\n    for i, c in enumerate(s):\\n        if x in s[i:] and x not in s[:i]:\\n            count += 1\\n    return count\\n\\nassert f('fvyijrtwrjrsasgt', 'g') == 15\\n\\ndef f(text):\\n    segments = text.split()\\n    for i in range(len(segments)):\\n        segments[i] = segments[i][0].upper() + segments[i][1:-1] + segments[i][-1].upper()\\n    return \u2018 '.join(segments)\\n\\nassert f(\\\"hey !\\\") == 'HeY !!'\\ndef f(pattern, items):\\n    result = []\\n    for text in items:\\n        pos = text.rfind(pattern)\\n        if pos >= 0:\\n            result.append(pos)\\n    return result\\n\\nassert f(\\\" B \\\", [' bBb ', ' BaB ', ' bB', ' bBbB ', ' bbb']) == []\\ndef f(str):\\n    return str and ''.join(sorted(str))\\n\\nassert f(\\\"h e l l o\\\") == ' ehllo'\\ndef f(t):\\n    return t.replace('or', t.center(len(t), 'o'))\\n\\nassert f(\\\"pomodoro\\\") == 'pomodpomodoroo'\\n\\nF.6.2. FAILING\\n\\ndef f(dimension):\\n    dinline = str(dimension)[1:].zfill(2)\\n    return dinline[0] * int(dinline[1])\\n\\nassert f(??) == 'kkkkk'\\ndef f(text):\\n    for elem in text:\\n        if elem.isupper():\\n            52\"}"}
{"id": "Ffpg52swvg", "page_num": 53, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"try:\\n    text.remove(elem)\\nexcept ValueError:\\n    pass\\n\\nreturn text\\n\\nassert f('??') == ''\\n\\ndef f(text):\\n    ls = list(text)\\n    for i in range(0, len(ls)):\\n        if ls[i] != '+':\\n            ls.insert(i, '+')\\n            ls.insert(i, '*')\\n            break\\n    return '+'.join(ls)\\n\\nassert f('nzoh') == '*+++n+z+o+h'\\n\\ndef f(text):\\n    new_text = list(text)\\n    dict = {}\\n    for char in new_text:\\n        dict[char] = new_text.count(char)\\n    return dict\\n\\nassert f('aaabbbccdddzz') == {'a': 3, 'b': 3, 'c': 2, 'd': 3, 'z': 2}\\n\\ndef f(text):\\n    odd = ''\\n    even = ''\\n    for i, c in enumerate(text):\\n        if i % 2 == 0:\\n            even += c\\n        else:\\n            odd += c\\n    return even + odd.lower()\\n\\nassert f('Mammoth') == 'Mmohamt'\\n\\ndef f(nums, i):\\n    nums.append(nums.pop(i % len(nums)))\\n    nums[i % len(nums)] = '?'\\n    split = nums[:::2]\\n    split.remove('?\\n    return split + nums[1::2]\\n\\nassert f([4, 2, 4, 2], 0) == [2, 4, 4]\\n\\ndef f(prefix, s):\\n    return str.removeprefix(prefix, s)\\n\\nassert f('hymi', 'hymifulhxhzpnyihyf') == 'hymi'\\n\\ndef f(text):\\n    if ',' in text:\\n        before, _, after = text.partition(',')\\n        return after + ' ' + before\\n    return ',' + text.partition(' ')[-1] + ' 0'\\n\\nassert f('244, 105, -90') == ' 105, -90 244'\\n\\ndef f(s):\\n    return '{}{}{}'.format(s[3:], s[2], s[5:8])\\n\\nassert f('jbucwc') == 'cwcuc'\\n\\ndef f(nums):\\n    for i in range(len(nums)):\\n        nums.insert(i, nums[i]**2)\\n    return nums\\n\\nassert f([1, 2, 4]) == [1, 1, 1, 1, 2, 4]\\n\\ndef f(c, text):\\n    t = c\\n    for c in reversed(text):\\n        t = c + t*2\\n        t = c + t\\n    return t + text\\n\\nassert f(';?', 'i') == 'ii;?;?i'\\n\\ndef f(nums, location, item):\\n    if len(nums) >= location and 0 <= location:\\n        return nums.insert(location, item)\\n    return nums\\n\\nassert f([1, 2, 3, 4, 5, 6], -5, -5) == [1, 2, 3, 4, 5, 6]\\n\\ndef f(text):\\n    return max(text.find(ch) for ch in 'aeiou')\"}"}
{"id": "Ffpg52swvg", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 20. Number of distinct generations of various models (out of 10) at $T = 0.2$ and $T = 0.8$.\\n\\nCoT increases the diversity of generations: In Fig. 21, for each model, we plot the average number of distinct generations across all samples, where different chains of thought with the same input or output prediction are considered identical. We see again the trend of input prediction generations being more diverse than output prediction generations. Interestingly, we observe that using CoT increases the diversity at both temperatures.\\n\\nFigure 21. Number of distinct generations of various models (normalized to be out of 10) at $T = 0.2$ and $T = 0.8$ with and without CoT. We observe that CoT increases the diversity of generations.\\n\\nFunctional diversity via distribution of pass@1 scores: In Fig. 22, for each model, we plot the percentage of samples where the pass@1 score ($T = 0.2$) is between 0.1 and 0.9, exclusive, indicating that the sample is neither \\\"too easy\\\" nor \\\"too hard\\\" for the model. This is a measure of functional diversity because models with more diversity are likely to generate both correct and incorrect samples, leading to more intermediate pass@1 scores. We make a few observations relatively consistent with our prior observations. First, the percentages are relatively low across the board, indicating that at a temperature of $T = 0.2$, models are generally producing a majority of correct or a majority of incorrect outputs. Second, distilled models have a much lower functional diversity than base models, for example comparing Phind 34B to CodeLlama 34B or DeepSeek Instruct 33B to DeepSeek Base 33B. Third, CoT greatly increases the functional diversity of models, which is very evident when looking at GPT-3.5 and GPT-4.\"}"}
{"id": "Ffpg52swvg", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Models               | % of Samples with Pass@1 in (0.1, 0.9), Input | % of Samples with Pass@1 in (0.1, 0.9), Output |\\n|----------------------|----------------------------------------------|------------------------------------------------|\\n| GPT-3.5 + CoT        | 50%                                          | 50%                                            |\\n| CodeLlama 13B + CoT  | 45%                                          | 45%                                            |\\n| CodeLlama 34B + CoT  | 40%                                          | 40%                                            |\\n| GPT-4 + CoT          | 35%                                          | 35%                                            |\\n| Code Llama 7B        | 30%                                          | 30%                                            |\\n| DeepSeek Base 7B     | 25%                                          | 25%                                            |\\n| CodeLlama Py 13B     | 20%                                          | 20%                                            |\\n| CodeLlama Py 34B     | 15%                                          | 15%                                            |\\n| CodeLlama 34B        | 10%                                          | 10%                                            |\\n| CodeLlama 13B        | 5%                                           | 5%                                             |\\n| DeepSeek Base 33B    | 0%                                           | 0%                                             |\\n| CodeLlama Py 7B      | 0%                                           | 0%                                             |\\n| DeepSeek Inst 33B    | 0%                                           | 0%                                             |\\n| StarCoder 15.5B      | 0%                                           | 0%                                             |\\n| Mistral 7B           | 0%                                           | 0%                                             |\\n| Phi-1.5 1.3B         | 0%                                           | 0%                                             |\\n| StarCoder 7B         | 0%                                           | 0%                                             |\\n| Phind 34B            | 0%                                           | 0%                                             |\\n| DeepSeek Inst 7B     | 0%                                           | 0%                                             |\\n\\nFigure 22. Percentage of samples where pass@1 score is in (0.1, 0.9), exclusive.\\n\\nD.4. Difficulty of Benchmark Samples\\n\\nDistribution of sample difficulties: In Fig. 23, we show the average pass@1 score across all models for $T = 0.2$ in order to get a sense of the difficulty distribution of CRUXEval.\\n\\nIn Fig. 24, we show the pass@1 distributions of a few of the best-performing models at $T = 0.8$. Compared to the overall distribution, the distribution appears to be more bimodal. The output prediction distribution is more bimodal than the input prediction distribution, perhaps reflecting the differences in the tasks themselves. We also see the familiar trend of CoT increasing the number of samples with intermediate scores (in this case between 0.25 and 0.75).\\n\\nFully solved and unsolved samples: In Figs. 25 and 26, we examine a different metric, the percentage of examples with pass@1 score equal to 0 and 1, respectively, at $T = 0.8$. In a sense, this metric captures the ability of models to solve problems. It is also related to diversity, as with a higher diversity, the likelihood of solving the problem may increase. A few observations arise from looking at this metric. Fig. 25 shows the percentage of samples that are completely unsolved by each model, i.e. with 0 pass@1. We analyze this metric for $T = 0.8$, because it leads to more diversity, which would improve this metric. First, when considering non-CoT modes, while GPT-3.5 and GPT-4 (red) are the two best-performing models at pass@1, they perform considerably worse at this metric than models such as Code Llama 34B and DeepSeek Base 33B. Second, instruction-tuned and distilled models (DeepSeek Instruct, Phind, WizardCoder) perform worse than their base counterparts, suggesting that their diversity may have been stifled from adherence to their instruction tuning datasets. Third, we observe that for the two Code Llama models,\"}"}
{"id": "Ffpg52swvg", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Figure 24. Pass@1 Distributions of Selected Models\\n\\nCoT actually makes this metric worse, but for GPT models, CoT makes it better. For GPT models, we hypothesize that this may be due to the increased diversity of CoT.\\n\\n### Figure 25. Percentage of samples unsolved, where pass@1 is 0 ($T = 0.8$)\\n\\nIn contrast, Fig. 26 shows the percentage of samples that models get fully correct, i.e. with a perfect pass@1. We analyze this metric for $T = 0.2$, as it would lead to more consistency, improving this metric. First, we see that GPT-4 excels, achieving over 60% for both input and output prediction. Second, when comparing base models with instruction tuned models, we see a trend matching the one before: since instruction tuned models are more consistent, they score better on this metric. Third, for output prediction, even though GPT-4 + CoT generally increases diversity, we see that consistency is not sacrificed!\"}"}
{"id": "Ffpg52swvg", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nFigure 26. Percentage of samples fully solved, where pass@1 score is 1 (T = 0.2).\\n\\nD.5. Impact of Anonymizing Functions\\n\\nAs a small ablation to understand the effect of variable names on execution ability, we also test CodeLlama 7B, 13B, and 34B on an anonymized version of a subset of the benchmark, where variable names are replaced with x1, x2, ... identifiers. An example of an anonymized function is shown in Listing 7. We use the same few-shot prompt without anonymization and report both pass@1 (T = 0.2) and pass@5 (T = 0.8) results on the anonymized benchmark with N = 10 samples. The results are shown in Table 4. This strengthens the case against memorization affects.\\n\\nListing 7. Sample of benchmark and anonymized version\\n\\nOriginal:\\n\\n```python\\ndef f(s):\\n    nums = ''.join(filter(lambda c:c.isdecimal(), s))\\n    if nums == '': return 'none'\\n    m = max([int(num) for num in nums.split(',')])\\n    return str(m)\\nassert f('01,001') == '1001'\\n```\\n\\nAnonymized:\\n\\n```python\\ndef f(x0):\\n    x1 = ''.join(filter(lambda x2: x2.isdecimal(), x0))\\n    if x1 == '':\\n        return 'none'\\n    x3 = max([int(x4) for x4 in x1.split(',')])\\n    return str(x3)\\nassert f('01,001') == '1001'\\n```\\n\\nTable 4. Impact of Anonymization on CRUXEVAL\\n\\n| Model     | Anonymized | Input Prediction | Output Prediction |\\n|-----------|------------|------------------|-------------------|\\n|           | Pass@1     | Pass@5           | Pass@1            | Pass@5            |\\n| CodeLlama 7B | \u2713          | 36.6%            | 36.4%             | 36.6%            |\\n| CodeLlama 13B | \u2713         | 39.0%            | 38.3%             | 39.0%            |\\n| CodeLlama 34B | \u2713        | 46.5%            | 41.1%             | 46.5%            |\\n\\n|           | Pass@5     | Pass@5           | Pass@1            | Pass@5            |\\n| CodeLlama 7B | \u2713          | 48.0%            | 43.5%             | 48.0%            |\\n| CodeLlama 13B | \u2713         | 50.2%            | 44.7%             | 50.2%            |\\n| CodeLlama 34B | \u2713        | 57.4%            | 47.5%             | 57.4%            |\\n\\n|       | \u2206          | +0.9%            | +5.3%             | -2.4%            |\\n| CodeLlama 7B |       | +1.0%            | +5.6%             | -2.2%            |\\n| CodeLlama 13B |       | +1.5%            | +6.4%             | -2.0%            |\\n| CodeLlama 34B |       | +1.5%            | +6.4%             | -2.0%            |\"}"}
{"id": "Ffpg52swvg", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.6. Impact of Data-Generating Model\\n\\nIn the early phases of this work, we were concerned that using Code Llama 34B to generate the benchmark would give the model an unfair advantage. Therefore, we checked the performance of a few models when generating data with Code Llama 13B, GPT-3.5, and GPT-4. The results are shown in Table 5.\\n\\nThese samples were generated using a different prompt and a much more relaxed filter, so the raw scores differ from those in the main text. Across all datasets, we see that the relative ordering of Code Llama 13B, Code Llama 34B, and GPT-3.5 are preserved. We also observed that generating data with GPT-3.5 led to a significantly easier benchmark. After looking at a few samples manually, we believe this is because the resulting inputs are much more predictable and guessable, such as \\\\texttt{f(\\\"abcde\\\")} rather than \\\\texttt{f(\\\"mai2!\\\")}. Including few-shot examples with random inputs did not improve this issue, and we believe this is an artifact of instruction tuning. We believe that together with the anonymization results in Appendix D.5, these results provide some evidence that evaluating a model on its own generated data does not seem to provide it a significant advantage.\\n\\n| Data Model | Evaluation Model | Input Pass@1 | Output Pass@1 |\\n|------------|------------------|--------------|--------------|\\n| CL 13B     | CL 13B           | 28.1%        | 28.4%        |\\n| CL 13B     | CL 34B           | 33.8%        | 29.2%        |\\n| CL 34B     | CL 13B           | 25.1%        | 24.3%        |\\n| CL 34B     | CL 34B           | 29.9%        | 25.4%        |\\n| CL 34B     | GPT-3.5          | 40.5%        | 36.6%        |\\n| GPT-3.5    | CL 13B           | 42.3%        | 49.7%        |\\n| GPT-3.5    | CL 34B           | 52.1%        | 50.7%        |\\n| GPT-3.5    | GPT-3.5          | 67.1%        | 67.2%        |\\n| GPT-4      | CL 13B           | 28.1%        | 42.4%        |\\n| GPT-4      | CL 34B           | 37.0%        | 44.6%        |\\n\\nD.7. Fine-tuning\\n\\nWe discover three interesting insights from fine-tuning. In the main text, we only discuss insight 3. As a refresher, we fine-tuned CODELLAMA34B on 138889 samples of Python functions distilled with the procedure outlined in Sec. 3, without filtering. For the output prediction task, the model was fine-tuned on assertions of the form \\\\texttt{assert f(input) == output}, and for the input prediction task, the model was fine-tuned on assertions of the form \\\\texttt{assert output == f(input)}.\\n\\n1. Direct fine-tuning leads to modest performance improvements: In the first setup, we analyze a stronger decontamination setup than that in the main text. Specifically, we remove samples that match functions used in the benchmark, even if the input-output pairs are different. In Fig. 27, we show the train and test accuracy of the model during the fine-tuning process. For ease of evaluation, the train accuracy is reported on a random subset of 500 samples from the fine-tuning set. The reported test accuracy is on a superset of CRUXEval.\\n\\n   First, we observe that fine-tuning is able to significantly increase performance on both input and output prediction tasks. Second, we observe that while the training accuracy is steadily increasing and the model is able to overfit the training set, the testing accuracy plateaus relatively quickly. This suggesting that simple fine-tuning may not be enough to achieve near-perfect scores on CRUXEval. Third, we observe that it is easier to overfit the training set on the output prediction benchmark than on the input prediction benchmark. We hypothesize this may be partially due to the fact that \\\\texttt{assert output == f(input)} is a less natural format for assertions and partially due to the fact that input prediction requires a more sophisticated level of reasoning compared to output prediction.\"}"}
{"id": "Ffpg52swvg", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 27.\\nTrain accuracy (500 random samples) and test accuracy (superset of CRUXEval) while finetuning. For both tasks, there is improvement, and the model steadily fits the training set while plateauing on the testing set.\\n\\n2. The format of fine-tuning data greatly impacts its effectiveness: We also discovered that it is important that the finetuning assertions be formatted in the same way as when evaluating the model at test time. As evidence of this, we fine-tune CLOLLAMA34B with two different sets of assertions, one on \\\\( \\\\text{assert output} == f(\\\\text{input}) \\\\) and the other on \\\\( \\\\text{assert } f(\\\\text{input}) == \\\\text{output} \\\\) assertions. We compare the accuracy of the two finetuned models on both input and output prediction in Fig. 28. We observe that when the format of the fine-tuning data and the testing data are different, the model even has difficulty overfitting the training set, showing that it may not have fully learned the equivalence of the two formats and the meaning of the == operator. This is perhaps another example of the \\\"reversal curse\\\" of LLMs (Berglund et al., 2023). The corresponding testing accuracy also plateaued at a lower accuracy when the format was misaligned. For example, in Fig. 28a, comparing the light green line with the light blue line shows almost a 10% difference in testing accuracy for input prediction when trained on a misaligned format. That being said, fine-tuning still improved performance relative to the base model, even with a mismatched format, showing that the fine-tuning with a mismatched format did still instill some information into the model.\\n\\nFigure 28.\\nAligning the fine-tuning data format with the evaluation data format is very important for benchmark performance.\\n\\n3. Including benchmark programs still cannot improve test accuracy beyond 70%: Finally, we explore the upper limits of fine-tuning on functions and assertions via a \\\"cheating\\\" setup. We curate a small set of 7259 samples consisting only of programs in the benchmark but with different input-output pairs. We finetune on a mixture of 50% of the original finetuning set and 50% of this new set, showing the training and testing accuracy over time in Fig. 29. Despite finetuning on programs very similar to the benchmark, we still observe a plateauing effect in the test accuracy, suggesting that our execution tasks may be too difficult to learn from this simple fine-tuning scheme. Therefore, we suggest a few more fine-tuning ideas for improving our benchmark in Sec. A.\"}"}
{"id": "Ffpg52swvg", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 29. Finetuning 50% on the original finetuning set and 50% on \\\"cheating\\\" data.\"}"}
{"id": "Ffpg52swvg", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we list all the prompts we use throughout the paper. Other than ensuring that generations could be parsed properly, all prompts were not optimized towards any particular models.\\n\\n### E.1. Benchmark Generation Few-Shot Prompts\\n\\n#### Listing 8.\\n\\n**string_1**\\n\\n```\\n[TASK]\\nstr.split\\n[/TASK]\\n\\n[PYTHON]\\ndef f(text):\\n    words = text.split()\\n    result = []\\n    for i in range(len(words)):\\n        if i % 2 == 0:\\n            result.append(words[i][::-1])\\n        else:\\n            result.append(words[i].upper())\\n    return ' '.join(result)\\n[/PYTHON]\\n\\n[TEST]\\nassert f(\\\"am7 fiDfd n\\\") == ??\\nassert f(\\\"bnasadl\\\") == ??\\nassert f(\\\"a j c n x X k\\\") == ??\\nassert f(\\\"98 bask2 asoijdf9\\\") == ??\\nassert f(\\\"\\\") == ??\\n[/TEST]\\n```\\n\\n**string_2**\\n\\n```\\n[TASK]\\nstr.capitalize\\n[/TASK]\\n\\n[PYTHON]\\ndef f(text):\\n    a = []\\n    words = text.split(' ')\\n    for i in range(len(words)):\\n        if words[i][0].isdigit():\\n            return 'no'\\n        if i%2 == 0:\\n            a.append(words[i].capitalize())\\n        else:\\n            a.append(words[i])\\n    return ' '.join(a)\\n[/PYTHON]\\n\\n[TEST]\\nassert f(\\\"20xk flkawhf\\\") == ??\\nassert f(\\\"lkw hj sfaibw fi 9\\\") == ??\\nassert f(\\\"abbot 2929 mbpu\\\") == ??\\nassert f(\\\"rotor zisxrs fh29nx\\\") == ??\\nassert f(\\\"pxk 5 bxD 9\\\") == ??\\n[/TEST]\\n```\\n\\n**string_3**\\n\\n```\\n[TASK]\\nstr.rindex\\n[/TASK]\\n\\n[PYTHON]\\ndef f(text, char):\\n    index = text.rindex(char)\\n    result = list(text)\\n    while index > 0:\\n        result[index] = result[index-1]\\n        result[index-1] = char\\n        index -= 2\\n    return ''.join(result)\\n[/PYTHON]\\n\\n[TEST]\\nassert f('mnjs krupa', 'u') == ??\\nassert f('kqwomn0xj', 'm') == ??\\nassert f('qpfi jzm', 'j') == ??\\nassert f('102x0zoq', '0') == ??\\nassert f('nzu ei,') == ??\\n[/TEST]\\n```\\n\\n**string_4**\\n\\n```\\n[TASK]\\nstr.rpartition\\n```\"}"}
{"id": "Ffpg52swvg", "page_num": 54, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"assert f(\\\"qsqgijwmmhbchoj\\\") == 13\"}"}
{"id": "Ffpg52swvg", "page_num": 46, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nfor i in range(size-int(size%2)):\\n    text = ' '+text+' '\\n    counter += 2\\n    if counter >= size:\\n        return text\\n\\nassert f(??) == ' 7 '\\n\\nGPT-4: The function f takes two arguments: a text and a size. It adds spaces to the beginning and end of the text until the length of the text is greater than or equal to the size. The function stops adding spaces when the length of the text is greater than or equal to the size.\\n\\nIn this case, the output is ' 7 ', which is a string of length 11. The text in the middle is '7', which is of length 1. The function adds 5 spaces to the beginning and end of the text to reach a total length of 11.\\n\\nSo, the input to the function should be '7' and 11.\\n\\n[ANSWER] f('7', 11) [/ANSWER]\\n\\ndef f(text):\\n    uppercase_index = text.find('A')\\n    if uppercase_index >= 0:\\n        return text[:uppercase_index] + text[text.find('a') + 1 :]\\n    else:\\n        return ''.join(sorted(text))\\n\\nassert f(??) == ' DEGHIVjkptx'\\n\\nGPT-4: The function f takes a string as input. It first checks if there is an uppercase 'A' in the string. If there is, it removes the first occurrence of lowercase 'a' from the string and returns the string. If there is no uppercase 'A', it sorts the string in alphabetical order and returns it.\\n\\nIn this case, the output is ' DEGHIVjkptx', which is a sorted string. This means that the input string did not contain an uppercase 'A', and the function sorted the string.\\n\\nTo get this output, we can input a string that contains the same characters but in a different order. For example, the string 'xtpkjVIGHED ' contains the same characters and when sorted by the function, it will give the output ' DEGHIVjkptx'.\\n\\nSo, the answer is 'xtpkjVIGHED '.\\n\\n[ANSWER] 'xtpkjVIGHED ' [/ANSWER]\\n\\nThe next example shows some ability in hexadecimal conversion. This may be partially due to memorization and data contamination effects, but this exact hex string was not found after a cursory Google search.\\n\\ndef f(text):\\n    hex_string = text.encode('utf-16')\\n    result = \"0x\"\\n    for c in hex_string:\\n        result += \"%.2X\" % c\\n    return result\\n\\nassert f(??) == '0xFFFE66006E007700200052006F006D0065002100'\\n\\nGPT-4: The function f takes a string as input, encodes it in utf-16 format, and then converts each character into a hexadecimal representation. The output '0xFFFE66006E007700200052006F006D0065002100' is the hexadecimal representation of a string in utf-16 format.\\n\\nTo find the input, we need to convert the hexadecimal representation back to a string. We can do this by first removing the '0x' prefix and then converting each pair of hexadecimal digits back to a character.\\n\\nThe hexadecimal string 'FFFE66006E007700200052006F006D0065002100' corresponds to the utf-16 encoded string 'nw Rome!'.\\n\\nSo, the input to the function f that will give the output '0xFFFE66006E007700200052006F006D0065002100' is 'nw Rome!'.\\n\\n[ANSWER] f('nw Rome!') [/ANSWER]\\n\\nJust like we see in output prediction, GPT-4 is able to reason about non-trivial string manipulation code.\\n\\ndef f(text):\\n    text = text.split(',')\\n    text.pop(0)\\n    text.insert(0, text.pop(text.index('T')))\"}"}
{"id": "Ffpg52swvg", "page_num": 47, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nreturn 'T' + ',' + ','.join(text)\\n\\nassert f(??) == 'T,T,Sspp,G ,.tB,Vxk,Cct'\\n\\nGPT-4: The function f takes a string as input, splits it into a list of substrings divided by commas, removes the first element of the list, finds the index of the substring 'T', removes it from its current position and inserts it at the beginning of the list. Finally, it joins the list into a string separated by commas and adds 'T' at the beginning of the string.\\n\\nTo get the output 'T,T,Sspp,G ,.tB,Vxk,Cct', we need to have 'T' as the second element in the list after splitting the input string by commas. The rest of the elements can be in any order. So, one possible input could be 'any,T,Sspp,G ,.tB,Vxk,Cct'. Let's check this:\\n\\nf('any,T,Sspp,G ,.tB,Vxk,Cct') will split the input into ['any', 'T', 'Sspp', 'G ', '.tB', 'Vxk', 'Cct'], remove the first element to get ['T', 'Sspp', 'G ', '.tB', 'Vxk', 'Cct'], find the index of 'T' (which is 0), remove it from the list and insert it at the beginning to get the same list ['T', 'Sspp', 'G ', '.tB', 'Vxk', 'Cct'], join the list into a string 'T,Sspp,G ,.tB,Vxk,Cct' and add 'T' at the beginning to get 'T,T,Sspp,G ,.tB,Cct', which is the desired output.\\n\\nSo, the answer is 'any,T,Sspp,G ,.tB,Vxk,Cct'.\\n\\nIn the following example, GPT-4 shows simple code analysis abilities, understanding that the first case will produce an error.\\n\\ndef f(nums):\\n    count = len(nums)\\n    if count == 0:\\n        nums = [0] * int(nums.pop())\\n    elif count % 2 == 0:\\n        nums.clear()\\n    else:\\n        del nums[:count//2:]\\n    return nums\\n\\nassert f(??) == []\\n\\nGPT-4: The function f takes a list of numbers as input. If the length of the list is 0, it tries to pop an element from the list, which will cause an error because the list is empty. If the length of the list is even, it clears the list. If the length of the list is odd, it deletes the first half of the list.\\n\\nTo get an empty list as output, we can input an even-length list. For example, we can input a list with two elements: [1, 2]. So, the answer is [1, 2].\\n\\nF.4.2. GPT-4 FAILURES WITH CODING\\nAs with output prediction, also notice some relatively simple failures of CoT. One observation is that GPT-4 often tries to check the incorrect input against the assertion and says that it passes, even when it does not. We believe this is not necessarily just due to the lack of execution ability, but a prior that the solution it found is more likely to be correct. In these first three examples, the model goes directly to a wrong answer without much step-by-step reasoning.\\n\\ndef f(text):\\n    odd = ''\\n    even = ''\\n    for i, c in enumerate(text):\\n        if i % 2 == 0:\\n            even += c\\n        else:\\n            odd += c\\n    return even + odd.lower()\\n\\nassert f(??) == 'Mmohamt'\\n\\nGPT-4: The function f takes a string as input and separates the characters at even and odd indices. It then concatenates the characters at even indices with the characters at odd indices converted to lowercase.\\n\\nTo get the output 'Mmohamt', we can see that the even indexed characters are 'M', 'o', 'a', 't' and the odd indexed characters are 'm', 'h', 'm'.\\n\\n47\"}"}
{"id": "Ffpg52swvg", "page_num": 48, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nSo, the input string can be \\\\('Mmohamt'\\\\).\\n\\n\\\\[\\\\text{ANSWER}\\\\]\\n\\\\[f('Mmohamt')\\\\]\\n\\\\[\\\\text{ANSWER}\\\\]\\n\\n\\\\footnotesize{def f(text):\\n    string = ''\\n    for char in text:\\n        string += char + char.lower()\\n    return string}\\n\\n\\\\footnotesize{assert f(??) == 'llaallaakk'}\\n\\nGPT-4: The function \\\\(f\\\\) takes a string as input and for each character in the string, it appends the character itself \\\\(\\\\rightarrow\\\\) and its lowercase version to a new string. The function then returns this new string.\\n\\nGiven the output \\\\('llaallaakk'\\\\), we can see that each character is repeated twice, once in its original form and once \\\\(\\\\rightarrow\\\\). This means that the input string must have been \\\\('LAK'\\\\).\\n\\n\\\\[\\\\text{ANSWER}\\\\]\\n\\\\[\\\\text{\\\\textbf{LAK}}\\\\]\\n\\\\[\\\\text{\\\\text{ANSWER}}\\\\]\\n\\n\\\\footnotesize{def f(nums, i):\\n    nums.append(nums.pop(i % len(nums)))\\n    nums[i % len(nums)] = '?'\\n    split = nums[::2]\\n    split.remove('?')\\n    return split + nums[1::2]}\\n\\n\\\\footnotesize{assert f(??) == [2, 4, 4]}\\n\\nGPT-4: The function \\\\(f\\\\) takes a list of numbers and an index as input. It pops the element at the index (modulo the \\\\(\\\\rightarrow\\\\) length of the list) and appends it to the end of the list. Then it replaces the element at the same index \\\\(\\\\rightarrow\\\\), with a '?'. It then splits the list into two lists: one with elements at even indices and one with elements \\\\(\\\\rightarrow\\\\), at odd indices. It removes the '?' from the first list and returns the concatenation of the two lists.\\n\\nTo get the output \\\\([2, 4, 4]\\\\), we need to find a list and an index such that after popping the element at the index and appending it to the end, and replacing the element at the index with '?', the list split into even and odd indexed elements (with '?' removed) gives \\\\([2, 4, 4]\\\\).\\n\\nOne possible input could be the list \\\\([2, 4, 4, 4]\\\\) and the index 1. After popping the element at index 1 and appending it to the end, the list becomes \\\\([2, 4, 4, 4]\\\\). Replacing the element at index 1 with '?' gives \\\\([2, ?, 4, 4]\\\\). Splitting into even and odd indexed elements gives \\\\([2, 4]\\\\) and \\\\([?, 4]\\\\). Removing '?' from the first list gives \\\\([2, 4]\\\\), and concatenating the two lists gives \\\\([2, 4, 4]\\\\).\\n\\nSo, the input \\\\(f([2, 4, 4, 4], 1)\\\\) will give the output \\\\([2, 4, 4]\\\\).\\n\\nThe following example is a bit tricky, because the naming of the variables does not match the semantics of str.removeprefix.\\n\\n\\\\footnotesize{def f(prefix, s):\\n    return str.removeprefix(prefix, s)}\\n\\n\\\\footnotesize{assert f(??) == 'hymi'}\\n\\nGPT-4: First, we need to understand what the function does. The function \\\\(f\\\\) takes two arguments: a prefix and a \\\\(\\\\rightarrow\\\\) string \\\\(s\\\\). It then removes the prefix from the string \\\\(s\\\\) and returns the result.\\n\\nGiven that the output is \\\\('hymi'\\\\), we can infer that the string \\\\(s\\\\) must have ended with \\\\('hymi'\\\\) after the prefix was \\\\(\\\\rightarrow\\\\). Therefore, we can choose any prefix and append \\\\('hymi'\\\\) to it to form the string \\\\(s\\\\).\\n\\nFor example, if we choose the prefix to be \\\\('hello'\\\\), then the string \\\\(s\\\\) would be \\\\('hellohymi'\\\\). When the prefix \\\\('hello'\\\\) is removed from \\\\('hellohymi'\\\\), the result is \\\\('hymi'\\\\), which is the desired output.\\n\\nSo, the input to the function \\\\(f\\\\) that would give the output \\\\('hymi'\\\\) is \\\\(('hello', 'hellohymi')\\\\).\\n\\n\\\\[\\\\text{ANSWER}\\\\]\\n\\\\[('hello', 'hellohymi')\\\\]\\n\\\\[\\\\text{ANSWER}\\\\]\\n\\nThe following example shows a case where GPT-4 \\\"checks\\\" the forward execution incorrectly.\\n\\n\\\\footnotesize{def f(text):\\n    count = len(text)\\n    for i in range(-count+1, 0):\\n        text = text + text[i]\\n}\\n\\n\\\\footnotesize{48}\"}"}
{"id": "Ffpg52swvg", "page_num": 49, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GPT-4: The function $f$ takes a string as input and appends the characters from the end of the string to itself. The number of characters appended is equal to the length of the original string minus one.\\n\\nTo get the output 'wlace Alc l ', we need to find a string that when appended with its last $|\\\\text{string}| - 1$ characters, gives 'wlace Alc l '.\\n\\nBy observing the output, we can see that the last 3 characters are spaces. This means that the last 3 characters of the original string must also be spaces.\\n\\nThe first character of the output is 'w', which is the first character of the original string. The second character of the output is 'l', which is the last character of the original string.\\n\\nFrom this, we can infer that the original string is 'w l '.\\n\\nLet's check this:\\n\\n$$f(\\\\text{'w l '}) = \\\\text{'w l '} + \\\\text{'l'} = \\\\text{'}wlace Alc l \\\\{'}$$\\n\\nSo, the input that gives the output 'wlace Alc l ' when passed to the function $f$ is 'w l '.\\n\\nSo, the answer is:\\n\\n[ANSWER] 'w l ' [/ANSWER]\\n\\nF.4.3. GPT-4 FAILURES WITH EXAMPLES REQUIRING INSIGHTS\\n\\nUnlike output prediction, where stepping through the code line by line suffices, some input prediction examples require insights and observations that can be difficult for models like GPT-4 to make. These examples are a bit more challenging, and passing them can require making these insights. Here, we highlight some of these examples, corresponding relevant insights, and GPT-4's failure modes.\\n\\nIn the following example, $\\\\text{new nums}$ consists of a sorted portion and an unsorted portion. One insight is that the sorted portion must be a subarray of the unsorted portion. GPT-4 always takes $[5, 15, 25, 44]$ to be the sorted portion and $[24, 22, 38, 5, 35, 15]$ to be the unsorted portion, which cannot be the case because 44 (from the sorted portion) is not in the unsorted portion.\\n\\n```python\\ndef f(nums, a, b):\\n    new_nums = []\\n    for n in nums:\\n        if n < a or n > b:\\n            new_nums.append(n)\\n    new_nums.sort()\\n    new_nums.extend(nums)\\n    return new_nums\\n```\\n\\nassert f(??) == [5, 15, 25, 44, 24, 22, 38, 5, 35, 15]\\n\\nIn the following example, the simplest solution is to bypass the while loop completely, but the model does not find it. However, the model chooses an output like \\\"baec\\\" because it back-translates the \\\"a\\\" to \\\"i\\\" using the translation table. Unfortunately, it does not take into account that other characters also get translated in the translation table.\\n\\n```python\\ndef f(input_string):\\n    table = str.maketrans('aioe', 'ioua')\\n    while 'a' in input_string or 'A' in input_string:\\n        input_string = input_string.translate(table)\\n    return input_string\\n```\\n\\nassert f(??) == 'biec'\\n\\nIn the following example, one must notice that $x$ cannot end with a number and $y$ cannot start with a number. Reasoning about $x+y = \\\\text{nisou79-85233}$, GPT-4 incorrectly deduces that the $y$ portion must be 85233, while in reality, it must be 79-85233 because $x$ cannot end with a digit.\\n\\n```python\\ndef f(urn):\\n    pieces = urn.split('--')\\n    x = pieces[0].rstrip('0123456789')\\n    y = pieces[1].lstrip('0123456789')\\n    return x+y\\n```\\n\\nassert f(??) == 'nisou79-85233'\"}"}
{"id": "Ffpg52swvg", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nThe behavior of code also paves the way to tackling more difficult tasks such as code repair with execution feedback and code summarization.\\n\\nAt a high level, our benchmark is constructed as follows. First, we use CodeLLAMA34B to generate a large set of functions and inputs. The outputs are generated by executing the functions on the inputs. Second, we filter the set so that our benchmark only consists of short problems with low computation and memory requirements, problems which a good human programmer should be able to do without extra memory in a minute or so. Third, we randomly select 800 samples passing the filter, ensuring the benchmark is both small enough to easily run but large enough to reliably see performance differences among various models. While it is difficult to come up with example where the strongest models like GPT-4 fail consistently, we observe that models actually fail often on random yet reasonable generated programs. We also highlight that as models improve, this generate-and-filter approach can be used to create future benchmarks that are more difficult and test different aspects of program execution.\\n\\nThe best model, GPT-4, achieves a pass@1 of 67% on CRUXEval-I and 63% on CRUXEval-O. In contrast, the best open-source models only achieve 47% on CRUXEval-I and 44% on CRUXEval-O, failing over half the time at simple execution prediction and code reasoning despite being trained on 100G of Python code and 1T of code data. We also observe that for base models, stronger HumanEval performance correlates with stronger CRUXEval performance. However, this trend breaks down for models distilled on GPT-4 like data such as WizardCoder, Phind, and Phi, which have impressively high HumanEval scores but no better than CodeLlama on CRUXEval.\\n\\nWe also observe that CoT and fine-tuning on input-output assertions are effective techniques for improving performance on CRUXEval, but are far from enough to ace it. Overall, our benchmark reveals that the gap between GPT-4 and open source models reflects GPT-4's stronger ability to reason about the behavior of code. As existing benchmarks like HumanEval and MBPP are insufficient for measuring code understanding and execution ability, capturing it through our benchmark is critical to make progress towards closing the gap between open models and GPT-4. Finally, we discover that despite its impressive abilities, GPT-4 consistently fails to understand the execution behavior of some surprisingly simple Python programs.\\n\\n2. Related Work\\n\\nLMs for Code Generation: There have been many efforts training LMs to generate code. Base models include Codex (Chen et al., 2021), CodeGeeX (Zheng et al., 2023), SantaCoder (Allal et al., 2023), PolyCoder (Xu et al., 2022), InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), StarCoder (Li et al., 2023a), DeepSeek-Coder (AI, 2023), and Code Llama (Roziere et al., 2023). Later, some of these models were fine-tuned on instruction-like data distilled from GPT-3.5 and GPT-4, resulting in models like Phind (Royzen et al., 2023), WizardCoder (Luo et al., 2023), and Phi-1/Phi-1.5 (Li et al., 2023b; Gunasekar et al., 2023). We evaluate the performance of a selection of these models on CRUXEval.\\n\\nBenchmarks for Evaluating Code LMs: There are various benchmarks serving to evaluate different aspects of these code LMs. We survey a handful here and refer readers to the survey (Zhang et al., 2023h) for more. HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) evaluate Python code generation on relatively simple functions. HumanEval+ (Liu et al., 2023c) augments HumanEval with better test cases after discovering many passing solutions are incorrect. ReCode (Wang et al., 2022a) is a variant of HumanEval with perturbed function names and docstrings. HumanEval-X (Zheng et al., 2023), MultiPLe (Cassano et al., 2022), and MBXP (Athiwaratkun et al., 2022) are extensions of HumanEval and MBPP with a focus on including programming languages outside of Python. APPS (Hendrycks et al., 2021), CodeContests (Li et al., 2022), and LeetCode-Hard (Shinn et al., 2023) evaluate code generation on more difficult, interview or competition style problems. There are also benchmarks to evaluate code generation in data science applications, such as DS-1000 (Lai et al., 2023), ARCADE (Yin et al., 2022), NumpyEval (Zhang et al., 2023b), and PandasEval (Jain et al., 2022). Going one step further, some benchmarks also measure ability to use API's or perform more general software engineering tasks, such as JuICe (Agashe et al., 2019), APIBench (Patil et al., 2023), RepoBench (Liu et al., 2023e), ODEX (Wang et al., 2022b), SWE-Bench (Jimenez et al., 2023), GoogleCodeRepo (Shrivastava et al., 2023), RepoEval (Zhang et al., 2023a), and Cocomic-Data (Ding et al., 2022).\\n\\nFinally, there are a variety of benchmarks for other tasks, such as code translation (Roziere et al., 2020; Zhu et al., 2022; Ahmad et al., 2021), test case generation (Tufano et al., 2022; Watson et al., 2020), code search (Husain et al., 2019), type prediction (Mir et al., 2022; Wei et al., 2023; Malik et al., 2019), commit message generation (Liu et al., 2020), code summarization (LeClair et al., 2019; Iyer et al., 2016; Barone & Sennrich, 2017; Hasan et al., 2021; Alon et al., 2018), code security (Liguori et al., 2022; Pearce et al., 2022; Tony et al., 2023), program repair (Jiang et al., 2023b; Xia et al., 2022; Tufano et al., 2019; Haque et al., 2022; Jin et al., 2023; Gupta et al., 2017; Berabi et al., 2021), performance optimization (Garg et al., 2022; Madaan et al., 2023a), and so on.\"}"}
{"id": "Ffpg52swvg", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To our knowledge, our CRUXEval is the first publicly available benchmark to measure the execution ability of code LMs. While some prior work has measured the output prediction ability of code LMs, we leverage our CRUXEval to perform a more thorough investigation of these capabilities. Our CRUXEval is the first to measure the ability of code LMs to perform input prediction.\\n\\nLeveraging Test Cases and Code Execution: Another line of work uses test cases and code execution information to improve code generation. Some examples include Speculyzer (Key et al., 2022), CodeT (Chen et al., 2022),CodeGen-Test (Zhong et al., 2022), Coder-Reviewer reranking (Zhang et al., 2023g), MBR-EXEC (Shi et al., 2022), TCoT (Tian & Chen, 2023), Algo (Zhang et al., 2023d), Pangu-Coder2 (Shen et al., 2023), LEVER (Ni et al., 2023), and Self-Play (Haluptzok et al., 2022). The idea of these works is to generate many programs and many test cases and select which programs and test cases seem correct based on the execution results. Other works use RL-style execution feedback to improve code generation, including CodeRL (Le et al., 2022), Reflexion (Shinn et al., 2023), and PG-TD (Zhang et al., 2023e). (Chen et al., 2023; Olausson et al., 2023b; Madaan et al., 2023b; Peng et al., 2023; Zhang et al., 2023c) investigate self-repair, using error messages as feedback for models to improve.\\n\\nMost relevant to our work, a handful of works examine and improve the execution ability of code LMs. (Austin et al., 2021), Scratchpad (Nye et al., 2021), and CodeExecutor (Liu et al., 2023a) train code LMs on execution information. Inspired by these works, we briefly touch on two primitive ways to improve performance on our benchmark, chain-of-thought and fine-tuning. Moving forward, we believe our CRUXEval could serve as a useful reference point as more techniques are designed to improve code execution abilities.\\n\\nFailure modes of LM Reasoning: Another dream of the community is to better understand the failure modes of LMs on reasoning tasks. (Bubeck et al., 2023; Liu et al., 2023b; Arkoudas, 2023; Zhang et al., 2022; Dziri et al., 2023; Olausson et al., 2023a; Lee et al., 2023; Zhang et al., 2023f) all investigate and point out various failure modes of LMs on a wide variety of reasoning tasks. Other examples of reasoning failures include 1) understanding negation (Hoseini et al., 2021), 2) ignoring irrelevant context (Shi et al., 2023), 3) operating under counterfactual situations such as 1-indexed Python or base-9 addition (Wu et al., 2023), and 4) generating Python code after identifier swaps like print, len = len, print (Miceli-Barone et al., 2023). Taking a more theoretical perspective, (Dziri et al., 2023; Zhou et al., 2023; Merrill & Sabharwal, 2023; Giannou et al., 2023) characterize the types of reasoning tasks transformers can and cannot be expected to carry out. (Merrill et al., 2021) argues that it is not possible to learn meaning from ungrounded form with context dependence and assuming that syntax is independent of semantics. In this work, we use CRUXEval to empirically examine failures in code execution / reasoning.\\n\\n3. Benchmark Construction\\nCRUXEval consists of 800 distinct functions, each with an input-output pair such that executing the function on the input deterministically produces the output. Using these functions and input-output pairs, we derive two benchmark tasks. In the output prediction task, the goal is to predict the output of executing the function on its associated input. In the input prediction task, the goal is to find any input such that executing the function on that input produces the output. For both tasks, we use an execution-based correctness metric. For input prediction, a generated input passes if \\\\texttt{assert f(generated input) == output} passes, and for output prediction, a generated output passes if \\\\texttt{assert f(input) == generated output} passes. A few statistics about the samples of CRUXEval can be found in Appendix B.3.\\n\\n3.1. Generating Candidates\\nWe use CODELLAMA to generate all the candidate functions and inputs of CRUXEval. To do so, we prompt it with the name of a function in the Python standard library such as \\\\texttt{str.zfill} and ask it to generate a Python function that makes use of the library function in addition to 5 test inputs. We provide two varying few-shot examples in our prompt for improved diversity of generations (see Appendix B.2 for more details). A sample prompt is shown in Listing 6.\\n\\nWe use a total of 69 different functions from the standard library: 47 from the \\\\texttt{str}, 11 from the \\\\texttt{dict}, and 11 from the \\\\texttt{list} (see Appendix B.1 for the full list of functions). Overall, we generate a total of 102000 functions (46\\\\% \\\\texttt{str}, 27\\\\% \\\\texttt{dict}, 27\\\\% \\\\texttt{list}) and 489306 input-output pairs.\\n\\n3.2. Filtering Candidates\\nNext, we filter the generated candidates to ensure that the samples in the dataset are reasonable and of high quality. In order to avoid forcing the model to perform tasks such as arithmetic calculation, we design filters so that the benchmark only consists of samples that are solveable by a human without extra memory.\\n\\nConcretely, we filter based on the following criteria.\\n\\n- **Compile time:** all arguments of the function must be used in the function, length of code is between 75 and 300 characters, no syntax errors, proper assertion \\\\texttt{assert f(input) == output}.\\n\\n---\\n\\n3\"}"}
{"id": "Ffpg52swvg", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\n- Runtime: no floating point operations, true division, exp, other integer operations must have at least one argument \\\\( \\\\leq 3 \\\\), string and list operations must have at least one argument with length \\\\( \\\\leq 3 \\\\), finish running in 2 seconds, no uncaught exceptions.\\n\\n- Best effort to remove other undesirable code: function cannot have any imports (such as os, sys), must be deterministic (no random, set ordering), and cannot have side effects such as input, builtins, etc.\\n\\n3.3. Data size and measuring noise\\n\\nThe success of HumanEval (164 examples) shows that evaluation benchmarks can be small, and having faster/cheaper evaluation is an overlooked advantage. We first overgenerate a larger set (\\\\( \\\\sim 1700 \\\\)) of samples and then pick a minimal data size that leads to sufficiently small noise.\\n\\nOn the overgenerated set, C\\\\_ODE\\\\_L\\\\_LAMA\\\\_34B outperforms C\\\\_ODE\\\\_L\\\\_LAMA\\\\_13B (as expected), and we would like to retain this property with high confidence in our smaller dataset. To do this, we took bootstrap samples of size \\\\( N \\\\) out of \\\\( \\\\sim 1700 \\\\) samples to measure the probability that the performance would be reversed, shown in Fig. 1. 800 examples are enough to test that C\\\\_ODE\\\\_L\\\\_LAMA\\\\_34B > C\\\\_ODE\\\\_L\\\\_LAMA\\\\_13B, C\\\\_ODE\\\\_L\\\\_LAMA > C\\\\_ODE\\\\_L\\\\_LAMA\\\\_34B, and as well as between D\\\\_EEPSEEK\\\\_33B > C\\\\_ODE\\\\_L\\\\_LAMA\\\\_34B output.\\n\\nWe measure two types of noise: 1) sampling which data points to include in the benchmark, and 2) sampling candidates from models for each data point (temperature > 0).\\n\\nOf these, 1) dominates 2). For 1) since model A does not always outperform model B on all data points even if model A > model B in aggregate, the measured performance depends on which data points are included. We measure both noise types on each model individually and type 1) noise on pairs of models using the bootstrap. Fortunately, we do not see major differences in noise level between models and the most important factor is just the size of the dataset. Type 1) noise is generally around 1.5% whereas type 2) noise is around 0.2% at \\\\( N = 800 \\\\). Type 1) noise usually becomes smaller on pairs of models due to correlation, yielding statistically significant results at the \\\\( \\\\alpha = 0.025 \\\\) (one-sided) for many model pairs.\\n\\n4. Evaluation\\n\\nWe evaluate a selection of models on CRUXE\\\\_VAL: StarCoder (Base 7B, 15.5B) (Li et al., 2023a), Mistral (7B) (Jiang et al., 2023a), WizardCoder (13B, 34B) (Luo et al., 2023), Phi-1 (Gunasekar et al., 2023) and Phi-1.5 (Li et al., 2023b) (1.3B), Phind v2 (Royzen et al., 2023) (34B), CodeLlama (Roziere et al., 2023) (Base and Python 7B, 13B, 34B), DeepSeek Coder (Base and Instruct 6.7B, 33B), GPT-200, GPT-400, GPT-800, GPT-1600, GPT-2000.\\n\\nIn Fig. 1, we plot reported HumanEval scores (we did not reproduce them ourselves) against scores on CRUXE\\\\_VAL. 4\"}"}
{"id": "Ffpg52swvg", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Indeed, we spot some interesting outliers: when comparing the distilled models WizardCoder 34B and Phind 34B to Codellama 34B, we see that the distilled models score over 20% more than Code Llama on HumanEval but do not show this drastic improvement when evaluated on either input or output prediction. In addition, the Phi-1 model outperforms most of the bigger models on HumanEval, but performs among the worst of all our evaluated models on CRUXEval. Overall, this suggests that models optimized for the HumanEval task by distilling data from GPT-3.5 and GPT-4 (WizardCoder, Phind, Phi) may not have learned other code reasoning capabilities along the way. On the other hand, for models such as StarCoder, Mistral, CodeLlama, and DeepSeek-Base, we still see a positive trend between HumanEval score and CRUXEval score, suggesting that code generation and execution/understanding abilities are correlated.\\n\\n**Relationship between input prediction and output prediction:**\\n\\nIn Fig. 4 (top), we compare the input prediction $\\\\text{HumanEval@1}$ with the output prediction $\\\\text{HumanEval@1}$. Conceptually, the two tasks seem relatively different: output prediction is directly testing code execution ability, while input prediction requires a higher-level understanding of the code's functionality. However, we discover that there is a strong correlation between their performance. This suggests the hypothesis that performance on relatively distinct coding-related tasks may be closely correlated. In addition, we see a relatively clear impact of scaling the model size on our two tasks.\\n\\n**Confusion matrix/error correlation for different models.**\\n\\nFig. 5 shows the pairwise correlation of pass@1 scores for each pair of models. The correlation is chosen based on its highest signal among cosine distance, Spearman and Kendall. The middle section of \u201copen\u201d-ish models (StarCoder, Code Llama, DeepSeek, etc.) are strongly correlated with each other. Strong correlations are seen between sizes of the same model, between models of the same size, and between instruct and base (P\\\\text{HIND} 34B, W\\\\text{IZARD} 34B vs. C\\\\text{ODE} 34B). CoT results also tend to have strong...\"}"}
{"id": "Ffpg52swvg", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nFan, A., Gokkaya, B., Harman, M., Lyubarskiy, M., Sen-gupta, S., Yoo, S., and Zhang, J. M. Large language models for software engineering: Survey and open problems. arXiv preprint arXiv:2310.03533, 2023.\\n\\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., tau Yih, W., Zettlemoyer, L., and Lewis, M. Incoder: A generative model for code infilling and synthesis. preprint arXiv:2204.05999, 2022.\\n\\nGarg, S., Moghaddam, R. Z., Clement, C. B., Sundaresan, N., and Wu, C. Deepperf: A deep learning-based approach for improving software performance. arXiv preprint arXiv:2206.13619, 2022.\\n\\nGiannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped transformers as programmable computers. arXiv preprint arXiv:2301.13196, 2023.\\n\\nGudibande, A., Wallace, E., Snell, C., Geng, X., Liu, H., Abbeel, P., Levine, S., and Song, D. The false promise of imitating proprietary llsms. arXiv preprint arXiv:2305.15717, 2023.\\n\\nGunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Del Giorno, A., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.\\n\\nGupta, R., Pal, S., Kanade, A., and Shevade, S. Deepfix: Fixing common c language errors by deep learning. In Proceedings of the aaai conference on artificial intelligence, volume 31, 2017.\\n\\nHaluptzok, P., Bowers, M., and Kalai, A. T. Language models can teach themselves to program better. arXiv preprint arXiv:2207.14502, 2022.\\n\\nHaque, M. M. A., Ahmad, W. U., Lourentzou, I., and Brown, C. Fixeval: Execution-based evaluation of program fixes for competitive programming problems. 2022.\\n\\nHasan, M., Muttaqueen, T., Ishtiaq, A. A., Mehrab, K. S., Haque, M. M. A., Hasan, T., Ahmad, W. U., Iqbal, A., and Shahriyar, R. Codesc: A large code-description parallel dataset. arXiv preprint arXiv:2105.14220, 2021.\\n\\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.\\n\\nHosseini, A., Reddy, S., Bahdanau, D., Hjelm, R. D., Sor-doni, A., and Courville, A. Understanding by understanding not: Modeling negation in language models. arXiv preprint arXiv:2105.03519, 2021.\\n\\nHusain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019.\\n\\nIyer, S., Konstas, I., Cheung, A., and Zettlemoyer, L. Summarizing source code using a neural attention model. In 54th Annual Meeting of the Association for Computational Linguistics 2016, pp. 2073\u20132083. Association for Computational Linguistics, 2016.\\n\\nJain, N., Vaidyanath, S., Iyer, A., Natarajan, N., Parthasarathy, S., Rajamani, S., and Sharma, R. Jig-saw: Large language models meet program synthesis. In Proceedings of the 44th International Conference on Software Engineering, pp. 1219\u20131231, 2022.\\n\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023a.\\n\\nJiang, N., Liu, K., Lutellier, T., and Tan, L. Impact of code language models on automated program repair. arXiv preprint arXiv:2302.05020, 2023b.\\n\\nJimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.\\n\\nJin, M., Shahriar, S., Tufano, M., Shi, X., Lu, S., Sundaresan, N., and Svyatkovskiy, A. Inferfix: End-to-end program repair with llms. arXiv preprint arXiv:2303.07263, 2023.\\n\\nKey, D., Li, W.-D., and Ellis, K. I speak, you verify: Toward trustworthy neural program synthesis. arXiv preprint arXiv:2210.00848, 2022.\\n\\nLai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, W.-t., Fried, D., Wang, S., and Yu, T. Ds-1000: A natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319\u201318345. PMLR, 2023.\\n\\nLe, H., Wang, Y., Gotmare, A. D., Savarese, S., and Hoi, S. C. H. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314\u201321328, 2022.\\n\\nLeClair, A., Jiang, S., and McMillan, C. A neural model for generating natural language summaries of program subroutines. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), pp. 795\u2013806. IEEE, 2019.\"}"}
{"id": "Ffpg52swvg", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nLee, N., Sreenivasan, K., Lee, J. D., Lee, K., and Papailiopoulos, D. Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023.\\n\\nLi, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023a.\\n\\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097, 2022.\\n\\nLi, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023b.\\n\\nLiguori, P., Al-Hossami, E., Cotroneo, D., Natella, R., Cukic, B., and Shaikh, S. Can we generate shellcodes via natural language? an empirical study. Automated Software Engineering, 29(1):30, 2022.\\n\\nLiu, C., Lu, S., Chen, W., Jiang, D., Svyatkovskiy, A., Fu, S., Sundaresan, N., and Duan, N. Code execution with pre-trained language models. arXiv preprint arXiv:2305.05383, 2023a.\\n\\nLiu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., and Zhang, Y. Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv preprint arXiv:2304.03439, 2023b.\\n\\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. arXiv preprint arXiv:2305.01210, 2023c.\\n\\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023d.\\n\\nLiu, S., Gao, C., Chen, S., Nie, L. Y., and Liu, Y. Atom: Commit message generation based on abstract syntax tree and hybrid ranking. IEEE Transactions on Software Engineering, 48(5):1800\u20131817, 2020.\\n\\nLiu, T., Xu, C., and McAuley, J. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091, 2023e.\\n\\nLuo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023.\\n\\nMadaan, A., Shypula, A., Alon, U., Hashemi, M., Raniganathan, P., Yang, Y., Neubig, G., and Yazdanbakhsh, A. Learning performance-improving code edits. arXiv preprint arXiv:2302.07867, 2023a.\\n\\nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023b.\\n\\nMalik, R. S., Patra, J., and Pradel, M. Nl2type: inferring javascript function types from natural language information. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), pp. 304\u2013315. IEEE, 2019.\\n\\nMerrill, W. and Sabharwal, A. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023.\\n\\nMerrill, W. C., Goldberg, Y., Schwartz, R., and Smith, N. A. Provable limitations of acquiring meaning from ungrounded form: What will future language models understand? Transactions of the Association for Computational Linguistics, 9:1047\u20131060, 2021.\\n\\nMiceli-Barone, A. V., Barez, F., Konstas, I., and Cohen, S. B. The larger they are, the harder they fail: Language models do not recognize identifier swaps in python. arXiv preprint arXiv:2305.15507, 2023.\\n\\nMir, A. M., Latora \u02c7skinas, E., Proksch, S., and Gousios, G. Type4py: Practical deep similarity learning-based type inference for python. In Proceedings of the 44th International Conference on Software Engineering, pp. 2241\u20132252, 2022.\\n\\nMizrahi, M., Kaplan, G., Malkin, D., Dror, R., Shahaf, D., and Stanovsky, G. State of what art? a call for multi-prompt llm evaluation. arXiv preprint arXiv:2401.00595, 2023.\\n\\nNi, A., Iyer, S., Radev, D., Stoyanov, V., Yih, W.-t., Wang, S., and Lin, X. V. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 26106\u201326128. PMLR, 2023.\\n\\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., and Xiong, C. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations, 2022.\\n\\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.\"}"}
{"id": "Ffpg52swvg", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nOlausson, T. X., Gu, A., Lipkin, B., Zhang, C. E., Solar-Lezama, A., Tenenbaum, J. B., and Levy, R. Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. arXiv preprint arXiv:2310.15164, 2023a.\\n\\nOlausson, T. X., Inala, J. P., Wang, C., Gao, J., and Solar-Lezama, A. Demystifying gpt self-repair for code generation. arXiv preprint arXiv:2306.09896, 2023b.\\n\\nOpenAI, R. Gpt-4 technical report. arxiv 2303.08774, 2023.\\n\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\\n\\nPatil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.\\n\\nPearce, H., Ahmad, B., Tan, B., Dolan-Gavitt, B., and Karri, R. Asleep at the keyboard? assessing the security of github copilot\u2019s code contributions. In 2022 IEEE Symposium on Security and Privacy (SP), pp. 754\u2013768. IEEE, 2022.\\n\\nPeng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L., Yu, Z., Chen, W., and Gao, J. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.\\n\\nRoyzen, M., Wei, J., and Coleman, R. Phind, 2023. URL https://www.phind.com.\\n\\nRoziere, B., Lachaux, M.-A., Chanussot, L., and Lample, G. Unsupervised translation of programming languages. Advances in Neural Information Processing Systems, 33:20601\u201320611, 2020.\\n\\nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\\n\\nShen, B., Zhang, J., Chen, T., Zan, D., Geng, B., Fu, A., Zeng, M., Yu, A., Ji, J., Zhao, J., et al. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv preprint arXiv:2307.14936, 2023.\\n\\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and Wang, S. I. Natural language to code translation with execution. arXiv preprint arXiv:2204.11454, 2022.\\n\\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Sch\u00e4rli, N., and Zhou, D. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pp. 31210\u201331227. PMLR, 2023.\\n\\nShinn, N., Labash, B., and Gopinath, A. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\\n\\nShrivastava, D., Larochelle, H., and Tarlow, D. Repository-level prompt generation for large language models of code. In International Conference on Machine Learning, pp. 31693\u201331715. PMLR, 2023.\\n\\nTian, Z. and Chen, J. Test-case-driven programming understanding in large language models for better code generation. arXiv preprint arXiv:2309.16120, 2023.\\n\\nTony, C., Mutas, M., Ferreyra, N. E. D., and Scandariato, R. Llmseceval: A dataset of natural language prompts for security evaluations. arXiv preprint arXiv:2303.09384, 2023.\\n\\nTufano, M., Watson, C., Bavota, G., Penta, M. D., White, M., and Poshyvanyk, D. An empirical study on learning bug-fixing patches in the wild via neural machine translation. ACM Transactions on Software Engineering and Methodology (TOSEM), 28(4):1\u201329, 2019.\\n\\nTufano, M., Deng, S. K., Sundaresan, N., and Svyatkovskiy, A. Methods2test: A dataset of focal methods mapped to test cases. In Proceedings of the 19th International Conference on Mining Software Repositories, pp. 299\u2013303, 2022.\\n\\nTyen, G., Mansoor, H., Chen, P., Mak, T., and C\u02d8arbune, V. Llms cannot find reasoning errors, but can correct them! arXiv preprint arXiv:2311.08516, 2023.\\n\\nUesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\\n\\nWang, S., Li, Z., Qian, H., Yang, C., Wang, Z., Shang, M., Kumar, V., Tan, S., Ray, B., Bhatia, P., et al. Recode: Robustness evaluation of code generation models. arXiv preprint arXiv:2212.10264, 2022a.\\n\\nWang, S. I., Gu, A., Liu, J., Wei, Y., Jain, N., Lai, Y., Sootla, S., Press, O., Rozi\u00e8re, B., and Synnaeve, G. Eval-Arena: noise and errors by leaderboard showdowns. https://github.com/crux-eval/eval-arena, 2024.\\n\\nWang, Z., Zhou, S., Fried, D., and Neubig, G. Execution-based evaluation for open-domain code generation. arXiv preprint arXiv:2212.10481, 2022b.\"}"}
{"id": "Ffpg52swvg", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nWatson, C., Tufano, M., Moran, K., Bavota, G., and Poshyvanyk, D. On learning meaningful assert statements for unit test cases. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pp. 1398\u20131409, 2020.\\n\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.\\n\\nWei, J., Durrett, G., and Dillig, I. Typet5: Seq2seq type inference using static analysis. arXiv preprint arXiv:2303.09564, 2023.\\n\\nWu, Z., Qiu, L., Ross, A., Aky\u00fcr\u00fck, E., Chen, B., Wang, B., Kim, N., Andreas, J., and Kim, Y. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv preprint arXiv:2307.02477, 2023.\\n\\nXia, C. S., Wei, Y., and Zhang, L. Practical program repair in the era of large pre-trained language models. arXiv preprint arXiv:2210.14179, 2022.\\n\\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pp. 1\u201310, 2022.\\n\\nYin, P., Li, W.-D., Xiao, K., Rao, A., Wen, Y., Shi, K., Howland, J., Bailey, P., Catasta, M., Michalewski, H., et al. Natural language to code generation in interactive data science notebooks. arXiv preprint arXiv:2212.09248, 2022.\\n\\nZan, D., Chen, B., Zhang, F., Lu, D., Wu, B., Guan, B., Yongji, W., and Lou, J.-G. Large language models meet nl2code: A survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 7443\u20137464, 2023.\\n\\nZhang, F., Chen, B., Zhang, Y., Liu, J., Zan, D., Mao, Y., Lou, J.-G., and Chen, W. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023a.\\n\\nZhang, H., Li, L. H., Meng, T., Chang, K.-W., and Broeck, G. V. d. On the paradox of learning to reason from data. arXiv preprint arXiv:2205.11502, 2022.\\n\\nZhang, K., Li, G., Li, J., Li, Z., and Jin, Z. Toolcoder: Teach code generation models to use APIs with search tools. arXiv preprint arXiv:2305.04032, 2023b.\\n\\nZhang, K., Wang, D., Xia, J., Wang, W. Y., and Li, L. Algo: Synthesizing algorithmic programs with generated oracle verifiers. arXiv preprint arXiv:2305.14591, 2023d.\\n\\nZhang, S., Chen, Z., Shen, Y., Ding, M., Tenenbaum, J. B., and Gan, C. Planning with large language models for code generation. arXiv preprint arXiv:2303.05510, 2023e.\\n\\nZhang, S. D., Tigges, C., Biderman, S., Raginsky, M., and Ringer, T. Can transformers learn to solve problems recursively? arXiv preprint arXiv:2305.14699, 2023f.\\n\\nZhang, T., Yu, T., Hashimoto, T., Lewis, M., Yih, W.-t., Fried, D., and Wang, S. Coder reviewer reranking for code generation. In International Conference on Machine Learning, pp. 41832\u201341846. PMLR, 2023g.\\n\\nZhang, Z., Chen, C., Liu, B., Liao, C., Gong, Z., Yu, H., Li, J., and Wang, R. A survey on language models for code. 2023h.\\n\\nZheng, Q., Xia, X., Zou, X., Dong, Y., Wang, S., Xue, Y., Wang, Z., Shen, L., Wang, A., Li, Y., et al. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. arXiv preprint arXiv:2303.17568, 2023.\\n\\nZhong, M., Liu, G., Li, H., Kuang, J., Zeng, J., and Wang, M. Codegen-test: An automatic code generation model integrating program test information. arXiv preprint arXiv:2202.07612, 2022.\\n\\nZhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O., Susskind, J., Bengio, S., and Nakkiran, P. What algorithms can transformers learn? a study in length generalization. arXiv preprint arXiv:2310.16028, 2023.\\n\\nZhu, M., Jain, A., Suresh, K., Ravindran, R., Tipirneni, S., and Reddy, C. K. Xlcost: A benchmark dataset for cross-lingual code intelligence. arXiv preprint arXiv:2206.08474, 2022.\"}"}
{"id": "Ffpg52swvg", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Correlation between Input and Output Prediction Scores, with CoT (bot) and without CoT (top). Correlations with other CoT results, even GPT-4 vs Llama 13B. For the output task, DEESEEK forms a small sub-cluster of especially strong associations.\\n\\n5.1. Chain of Thought Prompting\\nNext, we evaluate how the popular chain-of-thought (CoT) prompting method (Wei et al., 2022) affects the performance of Code Llama, GPT-3.5, and GPT-4 models on CRUX-EVAL. The full prompts can be found in Appendix E.3. All results are reported using $N = 10$ samples other than CodeLlama 13B and 34B without CoT, which are reported with $N = 100$ samples. As before, pass@1 is reported with $T = 0$ and pass@5 with $T = 0.8$. Additional results can be found in Appendix D.2.\\n\\nImpact of CoT: We begin by focusing our attention on the pass@1 scores of models with and without CoT. In Fig. 4, we plot the input and output prediction scores of each model with and without CoT. First, GPT-4 benefits significantly more than other models. Second, output prediction boosts $\\phi_1$. In Fig. 5, we plot the correlation between predictions on output prediction are generally larger than input prediction. In fact, CoT does not seem to improve Code Llama 13B and GPT-3.5 performance on input prediction. This is intuitive, as input prediction involves a more difficult reasoning task, while output prediction only requires executing the program step by step. We defer raw numbers to the Appendix in Table 3.\\n\\nCoT widens the gap between pass@5 and pass@1 scores: In Fig. 6, we plot the pass@5 scores against the pass@1 scores for all models. For models without CoT (shown in blue), there is a positive correlation between pass@1 and pass@5 scores. For models with CoT (shown in orange), we see an increase in the gap between pass@5 and pass@1 scores. We believe this phenomenon may be due to the additional diversity induced by CoT, which we analyze in detail in Appendix D.3.\\n\\nPredictions of CoT vs. Base Model: In Fig. 7, we show a confusion matrix over samples to better understand the correlations between direct output predictions and CoT predictions. For CodeLlama 13B (not shown), 34B, and GPT-3.5, we observe a large number of samples where direct prediction succeeds but CoT fails. However, with GPT-4, we observe that there are relatively few samples where this is the case.\\n\\n5.2. Fine-tuning Experiments\\nNext, we do a preliminary analysis to understand the effect of simple fine-tuning schemes on CRUX-EVAL performance. We fine-tuned CODELLAMA 34B on nearly 140K samples of Python functions distilled with the procedure outlined in Sec. 3, without filtering. We perform weak decontamination, only removing samples where both the function and input-output pairs match samples in the benchmark.\"}"}
{"id": "Ffpg52swvg", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In particular, we finetune on a mixture of 50% samples where the function is not in the benchmark and 50% samples where the function is in the benchmark but input-output pairs are not, a very liberal setup. The training and test accuracy over time for output prediction is shown in Fig. 8. Despite finetuning on programs very similar to the benchmark, we still observe a plateauing effect in the test accuracy, suggesting that our execution tasks may be too difficult to learn from this simple fine-tuning scheme. We defer a few other insights from fine-tuning to Appendix D.7 and suggest a few fine-tuning ideas for improving our benchmark in Appendix A.\\n\\n6. Qualitative Analysis\\n\\nAll models except GPT-4 have over 50% failure rate, showing they cannot do simple executions. In this section, we focus on GPT-4 with CoT and verify that the remaining 20% failures are due to the model, are consistent and are indeed on simple programs. We refer the reader to Appendix F for further examples of the failures highlighted in this section as well as some impressive successes.\\n\\nFailures of GPT-4 CoT.\\n\\nGPT-4 CoT scored 0/10 on 54 output prediction tasks and 65 input prediction tasks. On <= 0.5 > 0.5\\n\\nConfusion Matrix (Input)\\n\\n| <= 0.5 | > 0.5 |\\n|--------|-------|\\n| Code Llama 34B Score | 102 | 291 |\\n| Code Llama 34B + CoT Score | 330 | 77 |\\n\\nConfusion Matrix (Output)\\n\\n| <= 0.5 | > 0.5 |\\n|--------|-------|\\n| GPT-3.5 Score | 106 | 263 |\\n| GPT-3.5 + CoT Score | 309 | 122 |\\n\\nConfusion Matrix (Input)\\n\\n| <= 0.5 | > 0.5 |\\n|--------|-------|\\n| GPT-4 Score | 120 | 491 |\\n| GPT-4 + CoT Score | 147 | 42 |\\n\\nConfusion Matrix (Output)\\n\\n| <= 0.5 | > 0.5 |\\n|--------|-------|\\n| GPT-4 Score | 166 | 490 |\\n| GPT-4 + CoT Score | 130 | 14 |\\n\\nFigure 7. Confusion Matrix of Direct vs. CoT Prediction\\n\\nFigure 8. CRUXEval Performance after Fine-Tuning\"}"}
{"id": "Ffpg52swvg", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def f(string, sep):\\n    cnt = string.count(sep)\\n    return ((string + sep) * cnt)[::-1]\\n\\nassert f('caabcfcabfc', 'ab') == 'bacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaacbacfbacfcbaa\\n\\nListing 2. GPT-4 failures on simple programs\\n\\ndef f(prefix, s):\\n    return str.removeprefix(prefix, s)\\n\\nassert f('hymi', 'hymifulhxhzpnyihyf') == 'hymi'\\n\\n# GPT4+CoT preds:\\n# f('p', 'phymi')\\n# 'fulhxhzpnyihyf'\\n\\nListing 3. GPT-4 CoT failures where solutions requires counting\\nto 30\\n\\ndef f(L):\\n    N = len(L)\\n    for k in range(1, N // 2 + 1):\\n        i = k - 1\\n        j = N - k\\n        while i < j:\\n            # swap elements:\\n            L[i], L[j] = L[j], L[i]\\n            # update i, j:\\n            i += 1\\n            j -= 1\\n    return L\\n\\nassert f([16, 14, 12, 7, 9, 11]) == [11, 14, 7, 12, 9, 16]\\n\\n# GPT4+CoT preds:\\n# f([16, 9, 12, 7, 14, 11])\\n# f([16, 9, 7, 12, 14, 11])\\n# [11, 9, 7, 12, 14, 16]\\n# [11, 9, 7, 12, 14, 16]\\n\\nListing 4. GPT-4 CoT failure, cannot easily tell the answer\\n\\nwithout running the loops\\n\\ndef f(text, suffix):\\n    if suffix == '':\\n        suffix = None\\n    return text.endswith(suffix)\\n\\nassert f('uMeGndkGh', 'kG') == ??\\n\\n# GPT-4 CoT: True, should be False\\n\\ndef f(num):\\n    if 0 < num < 1000 and num != 6174:\\n        return 'Half Life'\\n    return 'Not found'\\n\\nassert f(6173) == ??\\n\\n# GPT-4 CoT: 'Half Life', should be 'Not found'\\n\\nListing 5. GPT-4 CoT output failures\\n\\nListing 5 shows GPT-4 CoT failures for output prediction\\n\\nonly. In Listing 5 (bot), the model fails because it concludes\\n\\nthat 6173 is not less than 1000. Small perturbations like\\n\\nchanging to 0 < num and num < 1000\\n\\nor changing the strings also failed. Both problems only have 2 possible\\n\\nanswers and other models sometimes get them correctly\\n\\nwhereas GPT-4 CoT is consistently wrong. We manually\\ntested scratchpad (Nye et al., 2021) style prompts, which\\n\\nfailed in the same way as regular CoT (Appendix F.3).\\n\\nFailures of GPT-4, Input Prediction: Below are two sim-\\nple failures on input prediction where GPT-4 failed for con-\\ncise and simple Python programs even with CoT.\\n\\ndef f(text, repl):\\n    trans = str.maketrans(text.lower(), repl.lower())\\n    return text.translate(trans)\\n\\nassert f(??) == 'lwwer case'\\n\\n# GPT4 CoT: 'lower case', 'ow'\\n# could be 'lower case', 'lwwer case'\\n\\ndef f(text):\\n    string = ''\\n    for char in text:\\n        string += char + char.lower()\\n    return string\\n\\nassert f(??) == 'llaallaakk'\\n\\n# GPT-4 CoT: 'LAK'\\n# should be 'lalak'\\n\\nOther GPT-4 Failures: We conclude with a set of four rel-\\natively simple string manipulation tasks that we discovered\\n\\nthat GPT-4 fails on. We suspect the errors could be partially\\n\\ndue to tokenization. For more examples, see Appendix F.5.\\n\\n- In Python, what is \\\" BaB \\\".rfind(\\\" B \\\")?\\n- In Python, if I have a string s = 'iabnm\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc\u02dc',\\n\\n  what is s[1::2]?\\n- In Python, what is \\\"+\".join(['*', '+', 'n', 'z', 'o',\\n\\n  'h'])?\\n- What is \\\"pomodoro\\\".replace(\\\"or\\\", \\\"pomodoro\\\")?\\n\\n7. Conclusion\\nWe propose CRUXEval, a new benchmark consisting of\\nsimple Python functions to evaluate the input and output\\nprediction abilities of code LMs. First, we propose a three-\\npart recipe to distill our benchmark consisting of large-scale\\ndistillation, filtering, and data size selection via a statistical\"}"}
{"id": "Ffpg52swvg", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution\\n\\nnoise analysis (Sec. 3). Second, we conduct a qualitative analysis by evaluating 20 models on our benchmark (Sec. 4). Our analysis leads to insights regarding the correlation between HumanEval and our benchmark, the correlation between input and output prediction, differences between various code LMs, and the diversity of different models.\\n\\nThird, we explore how CoT (Sec. 5.1) and fine-tuning (Sec. 5.2) can improve performance. Fourth, we provide a qualitative analysis of GPT-4\u2019s successes and failures (Sec. 6 and Appendix F). Overall, we believe that CRUXEval provides a complementary perspective to classical code LM evaluation such as HumanEval and MBPP and encourage creators of future code LMs to try out our benchmark!\\n\\nImpact Statement\\n\\nThis paper presents work whose goal is to advance the field of code generation. One vision for the societal impact of our work is that people will be more careful and thoughtful about the way they evaluate code LMs, which are getting more and more widespread.\\n\\nReferences\\n\\nAgashe, R., Iyer, S., and Zettlemoyer, L. Juice: A large scale distantly supervised dataset for open domain context-based code generation. arXiv preprint arXiv:1910.02216, 2019.\\n\\nAhmad, W. U., Tushar, M. G. R., Chakraborty, S., and Chang, K.-W. Avatar: A parallel corpus for java-python program translation. arXiv preprint arXiv:2108.11590, 2021.\\n\\nAI, D. Deepseek coder: Let the code write it-self. https://github.com/deepseek-ai/DeepSeek-Coder, 2023.\\n\\nAllal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C. M., Muennighoff, N., Mishra, M., Gu, A., Dey, M., et al. Santacoder: don't reach for the stars! arXiv preprint arXiv:2301.03988, 2023.\\n\\nAlon, U., Brody, S., Levy, O., and Yahav, E. code2seq: Generating sequences from structured representations of code. arXiv preprint arXiv:1808.01400, 2018.\\n\\nArkoudas, K. Gpt-4 can't reason. arXiv preprint arXiv:2308.03762, 2023.\\n\\nAthiwaratkun, B., Gouda, S. K., Wang, Z., Li, X., Tian, Y., Tan, M., Ahmad, W. U., Wang, S., Sun, Q., Shang, M., et al. Multi-lingual evaluation of code generation models. arXiv preprint arXiv:2210.14868, 2022.\\n\\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n\\nBarone, A. V. M. and Sennrich, R. A parallel corpus of python functions and documentation strings for automated code documentation and code generation. arXiv preprint arXiv:1707.02275, 2017.\\n\\nBerabi, B., He, J., Raychev, V., and Vechev, M. Tfix: Learning to fix coding errors with a text-to-text transformer. In International Conference on Machine Learning, pp. 780\u2013791. PMLR, 2021.\\n\\nBerglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, A. C., Korbak, T., and Evans, O. The reversal curse: Llms trained on \u201ca is b\u201d fail to learn \u201cb is a\u201d. arXiv preprint arXiv:2309.12288, 2023.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877\u20131901, 2020.\\n\\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\\n\\nCassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps-Costin, L., Pinckney, D., Yee, M.-H., Zi, Y., Anderson, C. J., Feldman, M. Q., et al. Multipl-e: A scalable and extensible approach to benchmarking neural code generation. arXiv preprint arXiv:2208.08227, 2022.\\n\\nChen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.-G., and Chen, W. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n\\nChen, X., Lin, M., Sch\u00e4rli, N., and Zhou, D. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.\\n\\nDing, Y., Wang, Z., Ahmad, W. U., Ramanathan, M. K., Nalapati, R., Bhatia, P., Roth, D., and Xiang, B. Cocomic: Code completion by jointly modeling in-file and cross-file context. arXiv preprint arXiv:2212.10007, 2022.\\n\\nDziri, N., Lu, X., Sclar, M., Li, X. L., Jian, L., Lin, B. Y., West, P., Bhagavatula, C., Bras, R. L., Hwang, J. D., et al. Faith and fate: Limits of transformers on compositionality. arXiv preprint arXiv:2305.18654, 2023.\"}"}
