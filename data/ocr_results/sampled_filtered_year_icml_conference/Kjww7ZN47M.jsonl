{"id": "Kjww7ZN47M", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3. CollegeMath: Extraction from textbooks\\n\\nTo construct the CollegeMath dataset, we made use of the GPT-3.5-Turbo API to parse and extract questions and answers from raw, segmented LaTeX exercises and their corresponding solutions.\\n\\nA.3.1. Extracting Questions from Exercises\\n\\nThe primary goal was to convert raw, potentially unstructured questions from math textbooks into well-formatted LaTeX-formatted questions. Below is the prompt template we utilized for this extraction process:\\n\\nI want you to act as a Math Parser. Your task is to convert raw messy questions from a math textbook into well-structured LaTeX-formatted questions.\\n\\nPlease ensure to retain the original question numbers.\\n\\nIf needed, prepend the original instructions to the parsed questions to make them more comprehensible.\\n\\nIf needed, skip the broken questions.\\n\\n#Raw Questions#:\\n\\n```latex\\n<insert demo>\\n```\\n\\n#Well-structured LaTeX-formatted Questions#:\\n\\nA.3.2. Extracting Answers from Solutions\\n\\nSimilarly, for answers, our aim was to transform raw, messy answers from textbooks into clear, LaTeX-formatted answers. Here's the template for this task:\\n\\nI want you to act as a Math Parser. Your task is to convert raw messy answers from a math textbook into well-structured LaTeX-formatted answers.\\n\\nPlease ensure to retain the original answer numbers.\\n\\nIf needed, skip the broken answers.\\n\\n#Raw Answers#:\\n\\n```latex\\n<insert demo>\\n```\\n\\n#Well-structured LaTeX-formatted Answers#:\\n\\nBy employing the aforementioned prompt templates, we were able to extract a comprehensive set of questions and answers, thereby forming the foundation of the CollegeMath dataset.\\n\\nA.4. MathScale: Concrete Examples\\n\\nA.4.1. Extracted Topics\\n\\nA set of 30 topics, randomly chosen, is listed below to illustrate the variety:\\n\\n- Arithmetic operations\\n- Word problem solving\\n- Mathematics\\n- Money and finance\\n- Problem-solving strategies\\n- Arithmetic\\n- Multiplication\\n- Proportions\\n- Basic arithmetic operations\\n- Conversion of units\\n- Measurement and weight\\n- Multiplication and addition\\n- Budgeting\\n- Basic arithmetic\\n- Wages and overtime\\n- Calculating earnings\"}"}
{"id": "Kjww7ZN47M", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Similarly, we provide a list of 30 knowledge points, chosen at random, to demonstrate the depth and breadth of content:\\n\\n- Random selection of marbles\\n- Definition and properties of dot product\\n- Manipulation of complex numbers\\n- Calculation of time required to complete a task\\n- How to apply the concept of a seven-day cycle\\n- Distinct numbers\\n- Expectation of a function of a random variable\\n- Ability to calculate total time\\n- Combinations of numbers\\n- Calculation of weekly income\\n- Relative motion\\n- Understanding the relationship between centimeters and kilometers\\n- Diagonalizing a matrix\\n- Proportional relationships between two quantities\\n- Ergodic Markov chain\\n- Addition of values\\n- Counting the number of cars\\n- Converting fractions to whole numbers\\n- Identifying relationships between different variables\\n- Ability to set up and solve a proportion equation\\n- Addition and subtraction of matrices\\n- Using logarithms to solve exponential equations\\n- Probability of rolling a specific number on a six-sided die\\n- Divisibility of polynomials\\n- Application of multiplication to calculate total revenue\\n- Identifying the highest and lowest scores\\n- Ability to calculate percentages.\\n- Geometric interpretation of dot product\\n- Dividing complex numbers\\n- Understanding weight units\\n\\nA.4.3. A TRAINING EXAMPLE OF MATHSCALE QA\\n\\nWe provide a concrete training example from MathScaleQA:\\n\\n---------------------gpt-3.5 generated question wrapped in alpaca---------------------\\n\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGiven the matrix\\n\\\\[\\nA = \\\\begin{pmatrix}\\n1 & 2 & 3 \\\\\\\\\\n2 & 5 & 2 \\\\\\\\\\n3 & 1 & 4\\n\\\\end{pmatrix}\\n\\\\]\\nfind the rank of \\\\( A \\\\).\\n\\n### Response:\\n---------------------gpt-3.5 generated answer---------------------\\n\\nTo find the rank of a matrix, we need to find the largest number of linearly independent rows (or columns). We can use the row echelon form to help us determine this.\\n\\nFirst, we perform the following row operations to obtain the row echelon form:\\n1. Subtract 2 times the first row from the second row.\\n2. Subtract 3 times the first row from the third row.\\n\\nWe obtain the matrix:\\n\\\\[\\n\\\\begin{pmatrix}\\n1 & 2 & 3 \\\\\\\\\\n0 & -1 & -4 \\\\\\\\\\n0 & 0 & -15\\n\\\\end{pmatrix}\\n\\\\]\\n\\nNext, we perform the following row operations:\\n1. Add 5 times the second row to the third row.\\n\\nWe obtain the matrix:\\n\\\\[\\n\\\\begin{pmatrix}\\n1 & 2 & 3 \\\\\\\\\\n0 & 1 & 5 \\\\\\\\\\n0 & 0 & 0\\n\\\\end{pmatrix}\\n\\\\]\\n\\nNow, our matrix is in row echelon form, and we can see that there are 3 non-zero rows, which means there are 3 linearly independent rows. Therefore, the rank of \\\\( A \\\\) is 3.\\n\\nThe answer is 3.\\n\\nA.5. Why Choose GPT-3.5?\\n\\nWe are not trying to draw the conclusion that if we replace GPT-3.5 with GPT-4 in all steps of MathScale, we will get worse results. On the contrary, we strongly believe that upgrading GPT-3.5 to GPT-4 for the whole MathScale pipeline, the results are highly likely to be better (with potentially better topic/KP extraction and better question generation). We use GPT-3.5 in\"}"}
{"id": "Kjww7ZN47M", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"our pipeline because 1) it is cheaper and therefore friendly to scaling experiments; 2) it is easier to compare with previous work (e.g., MetaMath, WizardMath), which are also based on GPT-3.5.\\n\\nWhy GPT-4 based solutions lead to slightly worse results?\\n\\n\u2022 the capability gap between LLaMA-2-7b and GPT-4 is larger than that between LLaMA-2-7b and GPT-3.5 (see Sec 5.3)\\n\u2022 we observed that the GPT-4 based solutions are 15% longer than GPT-3.5 based solutions on average, which might be harder to learn.\\n\\nA.6. Comparison under the Same Training Size\\n\\n| Models Train | Size | GSM8K | MATH | College | TAL | Math | Math23k | Ape210k | Gaokao | Bench | Math | AGIE | Gaokao | Math | AGIE | SAT | Math | Micro | Average | Macro | Average |\\n|--------------|------|-------|------|--------|-----|------|---------|--------|--------|-------|------|------|-------|------|------|-----|------|-------|---------|-------|---------|\\n| WizardMath-7B | 96.5K | 52.8 | 10.3 | 6.8 | 14.0 | 32.5 | 19.2 | 5.9 | 6.1 | 22.5 | 16.3 | 18.9 |\\n| MathScale-7b | 96.5K | 51.3 | 16.8 | 12.2 | 25.5 | 44.0 | 28.1 | 9.0 | 5.6 | 35.2 | 22.7 | 25.2 |\\n| MetaMath-7b | 395K | 66.2 | 20.6 | 9.4 | 22.5 | 44.0 | 29.9 | 5.9 | 5.1 | 36.2 | 24.7 | 26.6 |\\n| MathScale-7b | 300K | 59.5 | 21.8 | 14.9 | 27.9 | 49.4 | 32.7 | 10.0 | 7.6 | 45.0 | 27.1 | 29.8 |\\n\\nTable 9. Comparison with WizardMath and MetaMath under the Same Training Size on MWP Bench.\\n\\nWe show our final results with 2M examples in Table 5 to demonstrate how far our method can go by scaling up. For comparison with other models with the same number of examples, please refer to Figure 3. The last two figures show our method always obtain a better results compared to MetaMath and WizardMath on macro/micro average precisions with a comparable sized training examples. The detailed results are shown the Table 9. Note that we do not have results of using 395K examples for MathScale-7b. Therefore, we use the results of 300k instead to compare with MetaMath-7b. Even if we used fewer training examples, we still obtain better results on 8 out of 9 datasets.\\n\\nA.7. Evaluation on Individual Topics\\n\\nWe examine the subset performances on MATH and CollegeMath, as shown in Table 10 and Table 11, respectively. It is evident that MathScale consistently delivers exceptional results across diverse topics. For CollegeMath, despite the MWP Bench training set's seed questions only encompassing algebra, precalculus, and calculus, MathScale demonstrates robust performance in OOD's test sets including vector calculus, probability, and linear algebra. However, an area of challenge is differential equations, where all models show limited success.\\n\\nWe further provide an example of MathScale Mistral answering a question from MATH test set:\\n\\n---------------------a MATH test question wrapped in alpaca---------------------\\n\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\nInstruction:\\n\\nSuppose that I have 6 different books, 2 of which are math books. In how many ways can I stack my 6 books on a shelf if I do not want the math books to be next to each other?\\n\\nResponse:\\n\\n---------------------MathScale-Mistral generated answer---------------------\\n\\nLet's first find the total number of ways to stack the 6 books on the shelf, which is simply 6!\\n\\nNow, let's count the number of ways in which the math books are next to each other. We can treat the two math books as a single block, which can be stacked in 5! ways. However, there are 2! ways to arrange the math books within the block, so the total number of ways in which the math books are next to each other is 2! \\\\cdot 5!.\\n\\nTherefore, the number of ways in which the math books are not next to each other is 6! - 2! \\\\cdot 5! = 480.\\n\\nThe answer is 480.\"}"}
{"id": "Kjww7ZN47M", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                | CollegeMath | Algebra | Precalculus | Calculus | Vector | Differential Equation |\\n|----------------------|-------------|---------|-------------|----------|--------|-----------------------|\\n| closed-source models | GPT-4       | 41.1    | 21.2        | 20.6     | 29.0   | 11.5                  |\\n|                      | GPT-3.5     | 37.7    | 16.6        | 17.8     | 32.7   | 10.0                  |\\n| open-source models fine-tuned on LLaMA-2 13B | WizardMath | 12.0    | 7.4         | 8.2      | 14.5   | 2.8                   |\\n|                      | MAmmoTH-CoT | 11.2    | 4.2         | 7.0      | 8.1    | 2.8                   |\\n|                      | GAIR-Abel   | 15.3    | 6.0         | 5.0      | 3.6    | 2.1                   |\\n|                      | MetaMath    | 19.4    | 9.8         | 5.6      | 8.1    | 1.4                   |\\n|                      | MathScale   | 35.0    | 17.8        | 15.8     | 24.5   | 7.9                   |\\n| open-source models fine-tuned on LLaMA-2 7B | WizardMath | 9.7     | 5.2         | 10.2     | 11.8   | 1.4                   |\\n|                      | MAmmoTH-CoT | 9.5     | 4.8         | 7.0      | 10.0   | 2.1                   |\\n|                      | GAIR-Abel   | 12.0    | 4.2         | 5.2      | 6.3    | 3.5                   |\\n|                      | MetaMath    | 19.1    | 6.8         | 4.4      | 5.4    | 2.8                   |\\n|                      | MathScale   | 34.2    | 19.6        | 18.8     | 27.2   | 7.9                   |\\n| open-source models fine-tuned on Mistral 7B | WizardMath | 29.3    | 14.0        | 11.4     | 16.3   | 5.0                   |\\n|                      | MetaMath Mistral | 28.1 | 12.2        | 11.2     | 21.8   | 7.1                   |\\n|                      | MathScale Mistral | 37.1 | 18.0        | 19.4     | 27.2   | 8.6                   |\\n\\nTable 11. Performance metrics across various topics on CollegeMath. Within each section, the highest performing results are highlighted in bold font.\"}"}
{"id": "Kjww7ZN47M", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MathScale: Scaling Instruction Tuning for Mathematical Reasoning\\n\\nZhengyang Tang\\n\\nXingxing Zhang\\n\\nBenyou Wang\\n\\nFuru Wei\\n\\nAbstract\\n\\nLarge language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., GPT-3.5). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then builds a concept graph, which is subsequently used to generate new math questions.\\n\\nMathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct MWPBENCH, a benchmark of Math Word Problems, which is a collection of 9 datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on MWPBENCH, MathScale7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.8% in micro average accuracy and 43.6% in macro average accuracy, respectively.\\n\\n1. Introduction\\n\\nLarge language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate, potentially due to the inherent necessity for multi-step complex reasoning in mathematical problem-solving. Instruction Tuning (Wei et al., 2021) is an effective approach to unlock certain capabilities in LLMs. Unfortunately, this approach is constrained by the limited size of the currently available datasets on mathematical reasoning. For example, the most popular math datasets, GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b), each only contains around 7.5K training examples.\\n\\nAn effective method to tackle this challenge is to augment existing high-quality math datasets using frontier LLMs such as GPT-3.5 and GPT-4. For instance, WizardMath (Luo et al., 2023) introduces an array of operations for GPT-3.5 to generate math questions with increased complexity. MetaMath (Yu et al., 2023) bootstraps questions in GSM8K and MATH through answer augmentation, question rephrasing, self-verification and FOBAR questions. The newly generated examples by these methods exhibit substantial similarity to the original examples contained within the training set, which limits their power in generating large scale math datasets.\\n\\nWe therefore propose a conceptually simple and scalable method MathScale, which is less dependent on original training examples. Specifically, we first prompt GPT-3.5 to extract high level concepts (i.e., topics and knowledge points) from existing seed math questions. In this step, we convert concrete math questions to extractions and the dependency to original questions is largely removed. Given these extractions, we then build a concept graph, which is used to estimate the connections between different concepts. Finally, we can instruct GPT-3.5 to generate new math questions based on randomly sampled concepts from the graph. Intuitively, we can generate significantly more examples using different combinations of concepts than using augmentation-based methods, since the resulting number of new examples is bounded by the number of augmentation operations.\\n\\nMathScale also bears resemblance to the cognitive mechanisms underlying the process of mathematical learning in humans (Tall, 2013). Tall (2013) argues that the learning process of human involves two distinct steps called concept compression and connection forging. Concept compression mirrors the process of high level concept extraction, while connection forging is similar to our approach.\"}"}
{"id": "Kjww7ZN47M", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MathScale: Scaling Instruction Tuning for Mathematical Reasoning\\n\\nConcept graph construction. Mathematical capability evaluation is another issue arising from the lack of high-quality mathematical datasets. Recently, most LLMs employ GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) for evaluation. However, GSM8K focuses on elementary-level problems, while MATH offers competition-level challenges. There is a clear gap between the two kinds of capabilities measured. Therefore, we introduce MWPBENCH, a comprehensive and unified benchmark to measure mathematical reasoning capabilities. MWPBENCH is composed of 9 different math word problem datasets (including GSM8K and MATH) and it covers math word problems from elementary school to college level with different difficulty levels. Moreover, MWPBENCH standardizes evaluations across all datasets with a unified protocol, promoting consistent and fair model comparisons.\\n\\nMathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on MWPBENCH, MathScale-7B achieves 35.2% in micro average accuracy and 38.2% in macro accuracy, outperforming its best peers of equivalent size by 42.8% and 43.6%, respectively.\\n\\n2. MWPBENCH Evaluation Framework\\n\\n2.1. MWPBENCH Existing Datasets\\n\\nOur first endeavor is to collate established datasets, including GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), TAL-SCQ (TAL, 2023), Math23k (Wang et al., 2017), Ape210k (Zhao et al., 2020), GaokaoBench-Math (Zhang et al., 2023), and AGIEval (Zhong et al., 2023) series (see Table 1). Types of problems of these datasets are different. For example, most datasets contain math word problems, while TAL-SCQ comprises multi-choice questions. Intuitively, multi-choice questions are simpler because LLMs only need to figure out which choice leads to a higher probability. Therefore, we convert all multi-choice questions to math word problems (detailed in Appendix A.1). Secondly, some of the datasets (e.g., Math23k, Ape210k) are not in English and we translate them to English to expand existing math datasets (detailed in Appendix A.2). Note that we translated part of their training sets and full test sets into English. We also noticed another dataset, Lila (Mishra et al., 2022), which includes math question with code solutions as well as code questions from MBPP (Austin et al., 2021) and APPs (Hendrycks et al., 2021a), focusing differently from ours.\\n\\nCollegeMath\\n\\nExisting datasets does not cover college-level mathematics which requires diverse skills such as analytical thinking, logical reasoning, and quantitative analysis. We therefore propose CollegeMath to bridge this gap. We curated a collection of nine college mathematics textbooks, each addressing a distinct topic (see Table 2 for more details). These textbooks encompass seven critical mathematical disciplines: algebra, pre-calculus, calculus, vector calculus, probability, linear algebra, and differential equations. These textbooks are originally in PDF format and we convert them to text format using the Mathpix API, where equations are transformed to LaTeX format. Once converted a textbook to text format, we are ready to extract exercises and their solutions. For each book, we first manually segment the book into chapter and identify pages with exercises and their solutions. Then we extract questions in exercises and their associated short answers (see more details of our prompts in Appendix A.3). In total, this dataset contains 1281 examples for training and 2818 examples for test.\\n\\n2.2. Unified Evaluation Protocol\\n\\nOne of the challenges in benchmarking LLMs for mathematical reasoning is the inconsistency across evaluation metrics and protocols used in different work (Touvron et al., 2023; Luo et al., 2023; Yue et al., 2023). MWPBENCH aims to evaluate the mathematical reasoning abilities of instruction tuned LLMs using a unified evaluation protocol. We employ zero-shot setting for evaluation and use the accuracy metric. The reason behind that is we believe fine-tuned LLMs should be able to answer questions directly without demonstrations, while in few-shot setting the final results may change with different set of demonstrations. For prompt template, we choose the Alpaca template (Taori et al., 2023) as default, which is the most widely used for instruction tuning (Taori et al., 2023; Luo et al., 2023; Yu et al., 2023). However, we support customized template just in case that LLMs are trained with a different instruction template (e.g., OpenAI ChatGPT template). For decoding, we choose greedy decoding to eliminate randomness in comparisons, selecting the top-1 completion as the solution. To further standardize the evaluation, we carefully implemented the answer extraction and verification processes (with high precision fuzzy match).\"}"}
{"id": "Kjww7ZN47M", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Statistics in MathScale\\n\\n| Dataset Level | Difficulty | Question Type | Language | #Train | #Test |\\n|---------------|------------|---------------|----------|--------|-------|\\n| GSM8K         | Elementary | Easy          | Word     | 7473   | 1319  |\\n| MATH Competition | ExHard    | Word         | En       | 7498   | 5000  |\\n| TAL-SCQ       | K12 Math   | Medium        | MC       | 2638   | 1496  |\\n| Math23k       | Elementary | Easy          | Word     | 1000   | 949   |\\n| Ape210k       | Elementary | Easy          | Word     | 967    | 4874  |\\n| GaokaoBench-Math | High School | Hard       | MC   | Zh     | 0     | 508   |\\n| AGIEval-Gaokao-Math | High School | Hard       | MC   | Zh     | 0     | 404   |\\n| AGIEval-SAT-Math | High School | Hard       | MC   | En     | 0     | 102   |\\n| CollegeMath   | College    | ExHard        | Word     | 1281   | 2818  |\\n\\n**Total**\\n\\n- #Train: 20857\\n- #Test: 17470\\n\\n---\\n\\n### Details of permissively licensed books we use to construct the CollegeMath dataset\\n\\n| Topic | Book Title | License | #Train | #Test |\\n|-------|------------|---------|--------|-------|\\n| Algebra | Beginning and Intermediate Algebra (Wallace, 2010) | CC BY 3.0 | 1171   | 1000  |\\n| Precalculus | PRECALCULUS (Stitz & Zeager, 2013) | CC | 80     | 500   |\\n| Calculus | Calculus (Guichard, 2009) | CC BY-NC-SA 3.0 | 30     | 500   |\\n| Vector Calculus | CORRAL's VECTOR CALCULUS (Corral, 2008) | GFDL | 0     | 110   |\\n| Probability | Introduction to Probability (Grinstead & Snell, 2006) | GFDL | 0     | 38    |\\n| Probability | Probability and Statistics: The Science of Uncertainty (Evans & Rosenthal, 2004) | Custom | 200   | 101   |\\n| Linear Algebra | Matrix Theory and LINEAR ALGEBRA (Selinger, 2018) | CC BY | 0     | 123   |\\n| Linear Algebra | A First Course in LINEAR ALGEBRA (Kuttler & Farah, 2017) | CC BY | 0     | 137   |\\n| Differential Equations | ELEMENTARY DIFFERENTIAL EQUATIONS (Trench, 2001) | CC BY-NC-SA 3.0 | 0     | 309   |\\n\\n---\\n\\n#### MathScale: Scaling Instruction Tuning for Mathematical Reasoning\\n\\nWe present details of MathScale in this section. MathScale aims to generate large scale Mathematical Reasoning dataset by prompting ChatGPT and it contains four steps.\\n\\n### 3.1. Concept Extraction\\n\\nAs shown in Figure 1, MathScale takes seed math questions as input and we use the training set of MathWPBENCH (around 20K math questions). In the first step, we extract high level concepts (i.e., topics and knowledge points) from these seed questions with prompt engineering of GPT-3.5. We aim to extract meta information needed to solve a particular math question. We believe \u201ctopics\u201d and \u201cknowledge points\u201d are important meta information for questions. A \u201ctopic\u201d refers to the mathematical subject name or the topic name of math book chapter such as \u201cMoney and finance\u201d and \u201cArithmetic operations\u201d. While \u201cknowledge points\u201d refers to more fine grained math concepts (e.g., theorems, skills) in problem solving. Typical examples are \u201cDefinition and properties of dot product\u201d or \u201cConverting fractions to whole numbers\u201d. We instruct GPT-3.5 to act as a Math teacher and extract 1 or 2 topics and 1 to 5 knowledge points from a given seed question (see the prompt template in Table 3).\\n\\nTo ensure the diversity of the extracted topics and knowledge points, we use the training set of MathWPBENCH, which includes questions from different sources. We also remove topics and knowledge points that appear only one time to reduce noise. In total, we extracted around 2K topics and 8K knowledge points. The above process mirrors the concept compression described in (Tall, 2013).\\n\\n### 3.2. Concept Graph Construction\\n\\nGiven the topics and knowledge points extracted from the previous step, we move on to construct a concept graph $C$, whose nodes are the extracted topics $T = \\\\{t_1, t_2, \\\\ldots, t_{|T|}\\\\}$ and knowledge points (KPs) $K = \\\\{k_1, k_2, \\\\ldots, k_{|K|}\\\\}$. As shown in Figure 2, we have three types of edges in this graph (i.e., topic to topic edge, topic to KP edge and KP to KP edge), which results to three sub-graphs (topic graph, topic-KP graph, KP graph). When...\"}"}
{"id": "Kjww7ZN47M", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MathScale: Scaling Instruction Tuning for Mathematical Reasoning\\n\\nFigure 1. Overview of MathScale. MathScale starts from seed math questions and there are three steps in this pipeline (i.e., concept extract, concept graph construction and mathematical reasoning data generation). After these three steps, we obtain the MathScaleQA dataset, which is subsequently used to train open LLMs. Finally, we obtain MathScale models.\\n\\na topic (or KP) \\\\( u \\\\) is co-occurred with another topic (or KP) \\\\( v \\\\), we build an edge between them and the edge weight is related to their co-occurrence statistics. Define co-occurrence as \\\\( u \\\\) and \\\\( v \\\\) have been extracted from the seed question. Formally, let \\\\( E \\\\) denote edges in \\\\( C \\\\) and \\\\( f_{co}(u, v) \\\\) is the edge weight between \\\\( u \\\\) and \\\\( v \\\\).\\n\\nIntuitively, two KPs (or topics) are more likely to be reasonable composition when they have been frequently used to solve the same seed questions. Let \\\\( w_{uv} \\\\) denote the raw co-occurrence count between node \\\\( u \\\\) and node \\\\( v \\\\). The adjusted weight \\\\( f_{co}(u, v) \\\\) is defined as follows:\\n\\n\\\\[\\nf_{co}(u, v) = \\\\log(w_{uv} + \\\\epsilon)\\n\\\\]\\n\\nwhere \\\\( \\\\epsilon \\\\) is a small constant introduced to maintain non-zero counts and prevent computational issues.\\n\\nConcept Composition\\n\\nGiven the graph \\\\( C \\\\), we are ready to sample topics and KPs from it and the sampled topics and KPs are subsequently used to generate new math questions. We use a graph random walk algorithm to create concept compositions. We start from a uniformly random sampling from the \\\\( |T| \\\\) topics we have extracted. Note that in implementation, we simply enumerate all extracted topics for multiple epochs. In the second step, we do a random walk for one to two steps in the topic sub-graph to search for related topics. The probability distribution for the graph random walk is not uniform and defined as follows:\\n\\n\\\\[\\np_{uv} = \\\\frac{\\\\exp(f_{co}(u, v))}{\\\\sum_{v' \\\\in N(u)} \\\\exp(f_{co}(u, v'))}\\n\\\\]\\n\\nwhere \\\\( N(u) \\\\) denotes the set of nodes adjacent to \\\\( u \\\\) in the topic sub-graph.\\n\\nIn the third step, we continue to randomly walk in the hybrid topic-KP graph for a single step with the probability distribution calculated as in Equation (2) on the topic-KP graph. So that we now have one sampled KP.\\n\\nIn the last step, we continue to expand to more KPs by randomly walking on the KP graph for zero to four steps again with the probability distribution computed as in Equation (2) on KP graph. We finally obtained a set of sampled topics \\\\( \\\\hat{T} \\\\) and KPs \\\\( \\\\hat{K} \\\\).\\n\\nThe whole process above is an analogy of the connection forging described in (Tall, 2013).\\n\\n3.3. Mathematical Reasoning Data Generation\\n\\nAct as a Math Teacher and create a new question and its solution based on the provided topics and knowledge points. Ensure that the created questions:\\n\\n1. Adhere to the provided topics.\\n2. Necessitate the combined use of the associated knowledge points.\\n\\nTable 4. Prompt template for Mathematical Reasoning Data Generation.\"}"}
{"id": "Kjww7ZN47M", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Running Examples of the concept graph construction process in the MathScale pipeline.\\n\\nWith the novel compositions of topics $\\\\hat{T}$ and KPs $\\\\hat{K}$ at hand, we query GPT-3.5 to generate corresponding question-answer pairs. Inspired by how math teachers design questions from existing exercises, we opt to include few-shot examples to guide GPT-3.5 in question formulation. These examples are chosen from the seed questions, based on the Jaccard distance of their knowledge points set. We ask GPT-3.5 to adhere to $\\\\hat{T}$ and encourage combine use of KPs $\\\\hat{K}$. We present the template for prompts in Table 4.\\n\\nFurthermore, we apply a decontamination process, where all math questions in the test set of MWPBENCH are removed.\\n\\n3.4. Validation\\n\\nWe observe that sometimes in the newly generated QA pairs, the solution is incorrect. We therefore also tried to add an additional validation process as follows. We first instruction GPT-4 to generate a reference solution for the question and then ask GPT-4 again to validate the GPT-4 solution against the solution generated in the previous step. We assume GPT-4 is more accurate than GPT-3.5. If GPT-4 believe the original solution is incorrect, we replace it with the new GPT-4 solution. Small scale experiments (Table 7) show the step does not improve the results. Perhaps because essentially we are trying to distill GPT-3.5 using open source LLMs. Although some solutions are incorrect, they are still help open source LLMs to learn the model distributions of GPT-3.5. Therefore, in our final pipeline, we remove this validation step.\\n\\n4. Experiments\\n\\n4.1. Implementation\\n\\nData Generation\\n\\nIn concept extraction (Section 3.1), we use the MWPBENCH training set, comprising around 20K questions, as the seed questions for our MathScale pipeline and we employ GPT-3.5-Turbo-0613 for the extraction. In total, we obtain 2,018 topics and 8,892 knowledge points. We then construct graphs to establish relationships among these concepts (Section 3.2). The edge weight in the graph is smoothed using Equation (1) and we set $\\\\varepsilon = 1e^{-5}$. In the concept composition process, treating the iteration through all topic nodes as one epoch, we repeat this process for approximately 1K epochs, resulting 2 million unique concept compositions. Then we instruct GPT-3.5-Turbo-0613 to create 2 million question-answer pairs with these compositions. We also decontaminate the generated datasets by excluding all math questions in the test set of MWPBENCH. To leverage the precious high quality math reasoning data, we additionally combine the generated data with the training set of MWPBENCH. We call the resulting dataset MathScaleQA.\\n\\nThe validation step (Section 3.4) is excluded from the final pipeline, because we find that the validation step does not improve results (see details in Section 5.3). We provide concrete examples of concept extraction and MathScaleQA in Appendix A.4.\\n\\nModel Training\\n\\nThe questions in MathScaleQA are formatted using the Alpaca prompt (Taori et al., 2023) as follows.\"}"}
{"id": "Kjww7ZN47M", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Below is the instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n\\n```\\nquestion\\n```\\n\\n### Response:\\n\\nOur training pipeline is adapted from the open-instruct (Wang et al., 2023) toolkit. We utilize the LLaMA-2 7B and 13B models (Touvron et al., 2023) as well as the Mistral 7B model (Jiang et al., 2023) as our backbone models. We use a batch size of 128 and train on the MathScaleQA dataset for 3 epochs using a learning rate of 2e-5. We call the resulting models MathScale-7B, MathScale-13B and MathScale-Mistral-7B. We leave exploration of the LLaMA-2 70B model in future work.\\n\\n4.2. Models in Comparison\\n\\nClose-Source Models\\n\\nWe include the most capable GPT models developed by OpenAI, which are the light-weighted GPT-3.5-Turbo-0613 and the powerful GPT-4-0314. These models are known to be good at mathematical reasoning and serves as the upper bounds.\\n\\nOpen-Source Models:\\n\\nWe also compare our model against open-source math models. Specially, we compare with WizardMath (Luo et al., 2023), GAIR-Abel (Chern et al., 2023), MetaMath (Yu et al., 2023), and MAmmoTH (Yue et al., 2023). WizardMath (Luo et al., 2023) is based on evol-instruct (Xu et al., 2023) and reinforcement learning. MetaMath (Yu et al., 2023) is trained on a dataset by augmenting GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) using answer or question side paraphrasing. The dataset used to train MAmmoTH (Yue et al., 2023) comprises a collection of 13 existing math datasets with GPT-4 CoT (Wei et al., 2022) and/or PoT (Gao et al., 2023; Chen et al., 2022) annotations. We evaluate all models using CoT natural language style math solutions. We noticed that some of the models (e.g., GPT-4 and MAmmoTH) can produce code solution of math problems in addition to natural language solutions. For fair comparison, we refrain from comparing using code-interpreter style solutions, because all models above can produce code-interpreter style solutions if the solutions in their training data are replace by GPT annotated code solutions. Also note that WizardMath v1.1 is a Mistral based math model and we do not know how its training data are constructed (the authors did not release any detail of the training data of WizardMath v1.1). We evaluate all models on MWPBENCH, which contains 9 datasets on mathematical reasoning. We report accuracies of the 9 datasets as well as their micro-average and macro-average. We prompt all models using the Alpaca template (see Section 4.1). (Luo et al., 2023) recommended an improved prompt for during inference (i.e., adding Let's think step by step after the standard Alpaca template). However, we observe mixed results on MWPBENCH for some models in comparison. For example, we observe improved results on GSM8K, but decreased results on MATH. We therefore do not use this optimization for all models in comparison.\\n\\n4.3. Main Results\\n\\nAs shown in Table 5, MathScale obtains best micro average and macro average scores on MWPBENCH compared to other models based on LLaMA-2 7B, LLaMA-2 13B or Mistral 7B. Specifically, On average, MathScale-7B achieves a 35.2% (micro) and 38.2% (macro) accuracy across MWPBENCH, surpassing its best counterparts of equivalent size by 42.8% and 43.6%, respectively. The trends are similar for MathScale-13B and MathScale-Mistral. This also confirms the effectiveness of our MathScaleQA dataset regardless of the backbone model. Note that in GaokaoBench-Math, AGIEval-Gaokao-MATH, and AGIEval-SAT-MATH, there is no training set. Even on these out-of-domain test sets, MathScale-7B wildly outperforms other open-source models in comparison. When compared to frontier LLMs, MathScale-Mistral demonstrates performance parity in both micro and macro averages relative to GPT-3.5-Turbo (see the first block in Table 5). We further compare with WizardMath and MetaMath under the same training size (see Figure 3 and detailed results in Appendix A.6), and that MathScale-7B consistently achieves better results on most datasets as well as both macro and micro accuracies. Results of different subsets on MATH and CollegeMath are in Appendix A.7.\\n\\n5. Analysis and Discussions\\n\\n5.1. Scaling Property of MathScale\\n\\nAs described in Section 3, given a fixed set of math concepts, iterating over concept graphs allows us to generate different compositions of mathematical concepts, thereby synthesizing large amount of new math data. We use LLaMA-2 7B as our base model to study the scaling property of MathScale. When scaling the size of the MathScaleQA dataset, we observe a nearly logarithmic growth in the performance of the MathScale-7b model across all datasets within MWPBENCH, as depicted in Figure 3. We draw the scaling curve up to two million examples (size of the full MathScaleQA). We also compare MathScale against WizardMath and MetaMath at their respective training sizes. MathScale outperforms both models across all datasets (except for GSM8K) when using an equivalent amount of training data. Given the scaling curves in Figure 3, we anticipate that the performance of MathScale may continue to improve with even more synthetic training examples. Due to resource constraints, we...\"}"}
{"id": "Kjww7ZN47M", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Performance metrics on MWPBench. All evaluations were conducted utilizing the driver provided by MWPBench, ensuring a consistent and fair comparison. Within each section, the highest results are highlighted in bold font. \\\"AGIE\\\" stands for AGIEval.\\n\\n5.2. Ablation on Concept Extraction\\n\\nIn the concept extraction process (Section 3.1), we use all the 20K seed questions. We attempt to answer the following two questions.\\n\\n1) Does the number of seed questions matter? 2) Does the number of extracted concepts matter?\\n\\nWe control the size of resulting training examples to 25K for fast experimentation. In all experiments, we use the LLaMA-2 7B model as our backbone model.\\n\\nNumber of Seed Questions\\n\\nTo assess the influence of seed questions, we firstly randomly remove 50% of the seed questions from the MWPBench training set (i.e., we use only 10K seed questions). The results are shown in Table 6. We observe the macro average on MWPBench drops by 2.9%. Further, when we limit the data source of seed questions exclusively to the training sets of GSM8K and MATH, there is a performance decrease of 3.5%. These results above indicate that incorporating a larger and more diverse set of seed questions is beneficial.\\n\\nNumber of Math Concepts\\n\\nAdditionally, we examine the impact of extracted math concepts. As shown in Table 6, by removing half of the topics or knowledge points, we observe a notable decrease in the macro average on the MWPBench. Particularly, removing knowledge points lead to a greater decrease in performance (i.e., -8.6% with 50% knowledge points v.s. -2.3% with 50% of topics). This highlights the essential role that knowledge points play in enhancing the effectiveness of MathScale.\\n\\nTable 6. Ablation studies of concept extraction with a control training size of 25K on MWPBench.\"}"}
{"id": "Kjww7ZN47M", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Performance on MathScaleQA using different sizes of training dataset.\\n\\n4.1.2. In this section, we design a controlled experiment on 5K generated data from MathScaleQA and again using LLaMA-2 7B as our base model.\\n\\nGPT-4 vs. GPT-3.5 Accuracy\\n\\nWe manually annotate 100 randomly chosen generated data points and generate answers with GPT-3.5-Turbo and GPT-4. GPT-4 demonstrates an impressive accuracy of 87%, significantly outperforming the accuracy of 69% by GPT-3.5-Turbo. Therefore, we used GPT-4 to generate reference solutions and validate our synthetic solutions, replacing any incorrect solutions with the GPT-4 reference solutions.\\n\\nResults\\n\\nWithin the 5K examples, 26% of the solutions are identified as incorrect by GPT-4 and are replaced. We have another two settings with either all GPT-3.5 solutions and GPT-4 solutions. The results are shown in Table 7 and we observe that using original GPT-3.5-Turbo solutions lead to a similar result as using the validation step. This observation is counter-intuitive. Maybe because training on synthetic data generated from GPT-3.5 is essential distillation. Even if some solutions are incorrect, they may still help to the open-source LLMs (e.g., LLaMA-2 or Mistral) to mimic the distributions of GPT-3.5. We also notice that in neural machine translation distillation, the step of validating incorrect translations is also ignored (Kim & Rush, 2016). Considering the high cost of GPT-4 and similar results obtained, we opt to omit the validation and correction step from the final MathScale pipeline.\\n\\n5.4. Performance on a Fresh Math Dataset\\n\\nWhile MathScaleQA generated by GPT-3.5 is rigorously decontaminated to prevent overlap with the MWPBench test set, there may still be a small chance that some of the test sets have been leaked to GPT-3.5-Turbo or contained in the training data of LLaMA-2. Because GPT-3.5-Turbo uses human-annotated queries submitted by users through their APIs. These queries may include test sets such as GSM8K. The training set of LLaMA-2 is not released and we are not sure if some examples in test sets of MWPBench are included or not.\\n\\n4 We are not trying to draw the conclusion that if we replace GPT-3.5 with GPT-4 in all steps of MathScale (see more details in Appendix A.5).\\n\\n5 https://openai.com/research/instruction-following\"}"}
{"id": "Kjww7ZN47M", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MathScale: Scaling Instruction Tuning for Mathematical Reasoning\\n\\nMethods Micro Average Macro Average\\n\\n100% GPT-3.5 Solutions\\n10.6 11.5\\n\\n74% GPT-3.5 Solutions and 26% GPT-4 Corrected Solutions\\n10.2 11.1\\n\\n100% GPT-4 Solutions\\n9.8 10.9\\n\\nTable 7. Ablation studies of validation step with a control training size of 5K on MWPBENCH.\\n\\nTo address this issue, we manually curate a new dataset comprising the latest 30 math questions from latest Gaokao Math exam, held in June for China National Higher Education Entrance Examination. We term this dataset, Fresh-GaokaoMath-2023, which we believe Fresh-GaokaoMath-2023 is not likely to be included in the training data of LLaMA-2 or GPT-3.5-Turbo. Because LLaMA-2 and GPT-3.5-Turbo are released before Fresh-GaokaoMath-2023 is created.\\n\\nWe compare our LLaMA-2 7B based model MathScale-7B against two other LLaMA-2 7B based models (i.e., WizardMath-7B and MetaMath-7B) as well as GPT-3.5-Turbo and GPT-4. Results are in Table 8.\\n\\nMathScale consistently surpasses WizardMath and MetaMath, which aligns with the main results shown in Table 5. It demonstrates the robustness and adaptability of MathScale in handling fresh math questions.\\n\\nTable 8. Performance metrics on Fresh-GaokaoMath-2023.\\n\\n| Model                | Fresh-GaokaoMath-2023 |\\n|----------------------|------------------------|\\n| GPT-4                | 43.3                   |\\n| GPT-3.5-Turbo        | 40.0                   |\\n| WizardMath-7B        | 13.3                   |\\n| MetaMath-7B          | 16.6                   |\\n| MathScale-7B         | 30.0                   |\\n\\n6. Related Work\\n\\nChatGPT-based Instruction Tuning\\n\\nA pivotal aspect driving advancements in math instruction tuning is the use of ChatGPT for data synthesis. For instance, WizardMath (Luo et al., 2023) introduced reinforced evol-instruct which integrates five operations: adding constraints, deepening, concretizing, increasing reasoning steps, and complicating input, thereby facilitating comprehensive evolution. Similarly, MetaMath (Yu et al., 2023) employs a bootstrapping strategy for questions, incorporating answer augmentation, rephrasing, self-verification, and FOBAR. While these methods are effective, the breadth space is inherently confined to manually designed operations. Our approach seeks to enable ChatGPT to emulate cognitive processes in human mathematical learning, thus overcoming the limitations faced by previous methodologies.\\n\\nTool-Integration Instruction Tuning\\n\\nRecent studies have also explored integrating tools into ChatGPT-based instruction tuning for mathematics. ToRA (Gou et al., 2023) combines natural language reasoning with program-based tool usage to synthesize trajectory data. Each trajectory iteratively concatenates reasoning, programming, and program outputs until the final answer is reached. Our current focus is solely on natural language reasoning. While tool integration within the MathScale pipeline is an intriguing prospect, we reserve its exploration for future research.\\n\\n7. Conclusions\\n\\nWe propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs. We also construct MWPBENCH, a comprehensive benchmark of Math Word Problems covering K-12, college, and competition level math problems. Evaluated on MWPBENCH, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.8% in micro average accuracy and 43.6% in macro average accuracy, respectively.\\n\\nAcknowledgements\\n\\nThis work was supported by the Shenzhen Science and Technology Program (JCYJ20220818103001002), Shenzhen Doctoral Startup Funding (RCBS20221008093330065), Tianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC) (12326608), Shenzhen Key Laboratory of Cross-Modal Cognitive Computing (grant number ZDSYS20230626091302006), and Shenzhen Stability Science Program 2023, Shenzhen Key Lab of Multimodal Cognitive Computing.\\n\\nImpact Statement\\n\\nThis paper seeks to advance mathematical reasoning by introducing a scalable method for generating high-quality synthetic data with large language models, along with new evaluation benchmarks to foster consistent and fair model comparisons in academia. While our efforts center on assessing mathematical capabilities, it's crucial to note that the models may exhibit biases not examined in our study. Addressing these biases and ensuring the models' alignment with societal values is essential, highlighting the need for comprehensive evaluations that encompass both technical performance and ethical considerations.\\n\\n9\"}"}
{"id": "Kjww7ZN47M", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n\\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.\\n\\nChern, E., Zou, H., Li, X., Hu, J., Feng, K., Li, J., and Liu, P. Generative ai for math: Abel. https://github.com/GAIR-NLP/abel, 2023.\\n\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n\\nCorral, M. CORRAL'S VECTOR CALCULUS. 2008.\\n\\nEvans, M. J. and Rosenthal, J. S. Probability and statistics: The science of uncertainty. Macmillan, 2004.\\n\\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. PAL: Program-aided language models. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 10764\u201310799. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/gao23f.html.\\n\\nGou, Z., Shao, Z., Gong, Y., Yang, Y., Huang, M., Duan, N., Chen, W., et al. Tora: A tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023.\\n\\nGrinstead, C. M. and Snell, J. L. Grinstead and Snell's introduction to probability. Chance Project, 2006.\\n\\nGuichard, D. Calculus. 2009.\\n\\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021a.\\n\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b.\\n\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\\n\\nKim, Y. and Rush, A. M. Sequence-level knowledge distillation. In Su, J., Duh, K., and Carreras, X. (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1317\u20131327, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1139. URL https://aclanthology.org/D16-1139.\\n\\nKuttler, K. and Farah, I. A First Course in Linear Algebra, 2017A version (Lyryx). Lyryx, 2017.\\n\\nLangley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207\u20131216, Stanford, CA, 2000. Morgan Kaufmann.\\n\\nLuo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen, S., and Zhang, D. Wizard-math: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.\\n\\nMishra, S., Finlayson, M., Lu, P., Tang, L., Welleck, S., Baral, C., Rajpurohit, T., Tafjord, O., Sabharwal, A., Clark, P., et al. Lila: A unified benchmark for mathematical reasoning. arXiv preprint arXiv:2210.17517, 2022.\\n\\nSelinger, P. Matrix theory and linear algebra, 2018. URL https://www.mathstat.dal.ca/\u02dcselinger/linear-algebra/. An introduction to linear algebra for first or second year university students. Licensed under Creative Commons CC BY 4.0 License. Last updated on October 26, 2018.\\n\\nStitz, C. and Zeager, J. Precalculus. Stitz Zeager Open Source Mathematics, 2013.\\n\\nTAL. Tal-scq5k, 2023. URL https://github.com/math-eval/TAL-SCQ5K. GitHub repository.\\n\\nTall, D. How humans learn to think mathematically: Exploring the three worlds of mathematics. Cambridge University Press, 2013.\\n\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\"}"}
{"id": "Kjww7ZN47M", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MathScale: Scaling Instruction Tuning for Mathematical Reasoning\\n\\nTrench, W. F. \\nElementary Differential Equations\\nBrooks/Cole Thomson Learning, San Antonio, Texas, USA, 2001.\\nURL http://ramanujan.math.trinity.edu/wtrench/texts/TRENCH_DIFF_EQNS_I.PDF. Free Edition 1.01 (December 2013).\\n\\nWallace, T.  \\nBeginning and intermediate algebra\\n2010.\\n\\nWang, Y., Liu, X., and Shi, S. Deep neural solver for math word problems. In Proceedings of the 2017 conference on empirical methods in natural language processing, pp. 845\u2013854, 2017.\\n\\nWang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K. R., Wadden, D., MacMillan, K., Smith, N. A., Beltagy, I., et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023.\\n\\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\\n\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.\\n\\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.\\n\\nYu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrapping your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.\\n\\nYue, X., Qu, X., Zhang, G., Fu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.\\n\\nZhang, X., Li, C., Zong, Y., Ying, Z., He, L., and Qiu, X. Evaluating the performance of large language models on gaokao benchmark. 2023.\\n\\nZhao, W., Shang, M., Liu, Y., Wang, L., and Liu, J. Ape210k: A large-scale and template-rich dataset of math word problems. arXiv preprint arXiv:2009.11506, 2020.\\n\\nZhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.\"}"}
{"id": "Kjww7ZN47M", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Appendix\\n\\nA.1. MathScale: Transform Non-Word Problems into Word Problems\\n\\nFor datasets like TAL-SCQ (TAL, 2023), GaokaoBench-Math (Zhang et al., 2023), and AGIEval (Zhong et al., 2023), the problems are presented in a multiple-choice format. To eliminate the influence of the problem type and concentrate on the intrinsic ability of LLMs to address mathematical problems, we converted these non-word problems into word problems.\\n\\nA.1.1. Filtering Questions\\n\\nInitially, we identified and filtered out questions that rely heavily on the multiple-choice format. This filtering was done using specific keywords and phrases that are indicative of multiple-choice questions.\\n\\n```python\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\ndef is_bad_question(question):\\n    question = question.lower()\\n    keywords = [\\n        '?',\\n        'which of the following',\\n        'which one',\\n        'which is',\\n        'the following',\\n        'which statement'\\n    ]\\n    for keyword in keywords:\\n        if keyword in question:\\n            print(f'Filtered question: {question}')\\n            return True\\n    return False\\n```\\n\\nListing 1. Filtering questions\\n\\nA.1.2. Creating Question-Answer Pairs\\n\\nAfter filtering out the aforementioned questions, the remaining questions were paired with their corresponding correct answer choices. This transformation resulted in a format where each problem is presented as a word problem followed by its solution.\\n\\nA.2. MathScale: Translation of Non-English Problems to English\\n\\nFor several datasets, namely Math23k (Wang et al., 2017), Ape210k (Zhao et al., 2020), GaokaoBench-Math (Zhang et al., 2023), and AGIEval-Gaokao (Zhong et al., 2023), the problems are originally presented in Chinese. To ensure uniformity and mitigate the effects of multilingual representations, we translated these Chinese problems into English. The translation was facilitated by the GPT-3.5-Turbo API. Due to parsing errors encountered during the post-processing, a few examples were excluded. The prompt template employed for the translation request is provided below:\\n\\nI want you to act as a Math Translator. Your task is to translate Chinese math questions into English math questions. Make sure to keep the original question numbers. Make sure to keep the math formula in Latex format. The translations should be clear, accurate, and easily understandable for students who are native English speakers.\\n\\n# Chinese Math Questions #:\\n<insert chinese questions>\\n\\n# English Math Questions #:\\n\\n12\"}"}
