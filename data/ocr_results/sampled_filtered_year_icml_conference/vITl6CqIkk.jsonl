{"id": "vITl6CqIkk", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Additional Implementational Details\\n\\nParameter Setting\\nFor the experiments, we use two A100 80G GPUs to sample images from the pre-trained models Stable Diffusion v1.5 (Rombach et al., 2022). Our SA-HOI is built upon it, with all the available pre-trained weights from its publicly available repository. For CFG, we adopt guidance scale of 7.5, the text prompt is \\\"A photo of a person verbing a/an object\\\" for HOI category <verb, object>, and the negative prompt is set as \\\"\\\". We adopt DDIMScheduler (von Platen et al., 2022) with 50 steps for the denoising process, and all the generated images are with size 512 \u00d7 512. For pose detection, we apply pose detectors from RTMPose toolbox (Chen et al., 2019). For the object segmentation mask, we adopt Mask-RCNN (Abdulla, 2017) as the segmentation model. For each HOI category, we utilize templated description \\\"A photo of a human verbing a/an object\\\" as a text prompt for category <human, verb, object>, and sample 50 images with different seeds.\\n\\nCombination of pose and interaction boundary guidance with CFG\\nTo incorporate our tailored guidance scheme for HOI into text-2-image models like Stable Diffusion (Rombach et al., 2022), we need to combine our guidance with CFG (Ho & Salimans, 2022). In practice, we formulate the final guided noise as\\n\\n\\\\[\\n\\\\tilde{\\\\epsilon}(x_t) = \\\\epsilon_{\\\\theta}(x_t, c) + (1 + s_c)(\\\\epsilon_{\\\\theta}(x_t, c) - \\\\epsilon_{\\\\theta}(x_t)) + (1 + s_p)(\\\\epsilon_{\\\\theta}(x_t, M_{\\\\text{pose}}) - \\\\epsilon_{\\\\theta}(x_t)) + (1 + s_i)(\\\\epsilon_{\\\\theta}(x_t, M_{\\\\text{inter}}) - \\\\epsilon_{\\\\theta}(x_t))\\n\\\\]\\n\\nwhere \\\\(s_c\\\\), \\\\(s_p\\\\), \\\\(s_i\\\\) are guidance scale for CFG, pose guidance and interaction boundary guidance. \\\\(c\\\\) denotes text prompts describing HOI category, and \\\\(M_{\\\\text{pose}}\\\\) and \\\\(M_{\\\\text{inter}}\\\\) are masks for corresponding guidance. We simply adopt identical value for \\\\(s_p\\\\) and \\\\(s_i\\\\) in practice.\\n\\nB. Additional Related Work\\n\\n3D Human Object Interaction Generation\\nSeveral attempts (Xu et al., 2023; Diller & Dai, 2023; Peng et al., 2023) exist for 3D Human Object Interaction Generation, which aims to generate motion sequence for HOI instance. 3D HOI generation differs from 2D HOI generation from following perspectives: (1) their generation target is sparse keypoint information including coordinates and velocity, while image generation requires dense pixel-wise generation. (2) their main focus lies on the temporal modeling over motion sequence, while we focus on the realness and fidelity of images. (3) applicable HOI categories for existing 3D HOI generation approaches are quite limited, while 2D HOI generation could expand to most common HOI categories. In summary, there are significant differences between 2D and 3D HOI generation in various aspects. Therefore, we did not compare them in subsequent experiments.\\n\\nC. Qualitative results\\n\\nVisualization comparison of different methods\\nWe provide several qualitative comparisons for our methods and other model, which is shown in Figure 4. Comparing with other three recent methods, our approach showcases enhancements in human pose, object appearance, and HOI semantic expression simultaneously, resulting in improved overall image quality.\\n\\nD. Human Object Interaction Image Generation Benchmark\\n\\nD.1. Exampler dataset images\\nSome exampler images of our dataset are shown in Figure 5. These images not only exhibit a diverse range of poses and realistic spatial distances between human object pairs, but also effectively convey the semantic information associated with their HOI categories, which makes them well-suited for the evaluation of image generation quality.\\n\\nD.2. HOI category List\\nWe list detailed HOI categories for our H-A, H-O, H-H scenarios as follow,\\n\\nH-A: <chase, bird>, <feed, bird>, <pet, bird>, <release, bird>, <watch, bird>, <feed, cow>, <herd, cow>, <hold, cow>, <hug, cow>, <kiss, cow>, <lasso, cow>, <milk, cow>, <pet, cow>, <ride, cow>, <walk, cow>, <carry, dog>, <dry, dog>, <feed, dog>, <groom, dog>, <hold, dog>, <hose, dog>, <hug, dog>, <inspect, dog>, <kiss, dog>, <pet, dog>, <run, dog>, <scratch, dog>, <straddle, dog>, <train, dog>, <walk, dog>, <wash, dog>\"}"}
{"id": "vITl6CqIkk", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "vITl6CqIkk", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Qualitative comparisons among Stable Diffusion v1.5 (Rombach et al., 2022), Diffusion HPC (Weng et al., 2023) and SAG (Hong et al., 2023) and our method.\\n\\n(a) Text prompt: A photo of a person holding a suitcase.\\n(b) Text prompt: A photo of a person feeding a cow.\\n(c) Text prompt: A photo of a person kissing a cat.\"}"}
{"id": "vITl6CqIkk", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Exemplar images of our dataset. Images within each HOI category show diverse poses and coherent spatial correlations between human and object, and they all adhere to their HOI categories semantics tightly.\"}"}
{"id": "vITl6CqIkk", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparison on tailored metrics of HOI Generation with other methods. Each split under the metrics denotes the evaluation for corresponding region pose, \u201cJOINT\u201d denotes combined pose of human and animal under H-A scenario or combined pose of two humans under H-H scenario.\\n\\n| Model | BODY HAND | BODY HAND | ANIMAL JOINT |\\n|-------|-----------|-----------|--------------|\\n| SD    | A@1       | A@5       |              |\\n|       | 69.54     | 27.83     | 15.71        |\\n| D     | 71.35     | 27.93     | 14.33        |\\n| HPC   | 74.58     | 28.17     | 15.82        |\\n| Ours  | 73.88     | 29.01     | 12.70        |\\n\\n| SD    | H-O       |              |\\n|-------|-----------|--------------|\\n|       | A@1       | A@5       |\\n|       | 67.48     | 28.86     |\\n| D     | 67.92     | 27.53     |\\n| HPC   | 67.92     | 27.53     |\\n| Ours  | 68.69     | 29.65     |\\n\\n| SD    | H-H       |              |\\n|-------|-----------|--------------|\\n|       | A@1       | A@5       |\\n|       | 70.67     | 23.57     |\\n| D     | 73.40     | 25.50     |\\n| HPC   | 76.99     | 27.01     |\\n| Ours  | 75.31     | 26.77     |\\n\\nindicating our approach preserves rich HOI semantic information and more accurate expression for it. R-Acc are all improved compared to SD under all scenarios and we reach the best accuracy among all methods. Both in detection and retrieval manner, SA-HOI harvests better fidelity towards HOI semantics, indicating that our model attains better semantic consistency between text prompts and images.\\n\\nSubjective Evaluation:\\nWe carry out a user study for a subjective evaluation. We invited 26 participants to rate the quality of the provided images through a questionnaire. Ten textual prompts describing different HOI categories were randomly selected, and images were generated accordingly using different models. Additionally, realistic images from our benchmark were included for each prompt to enhance credibility. Participants were instructed to rate images on a scale from 1 to 5, representing bad quality to perfect quality. Evaluation criteria included (1) Human Pose Realism, which assesses the credibility of the length, number, and angles of human limbs. (2) Object Appearance: Evaluating the appearance of objects in the generated image. (3) Interaction Semantics: Judging the semantic relevance between the generated images and the given textual prompts. (4) Overall Quality: Rating the overall quality of the generated image. With a total of 5.2k responses, insights into the performance comparison among the different models were obtained. Table 3 shows the performance, from which we can conclude: (1) our approach outperforms other methods in subjective evaluation across all metrics; (2) there exists a positive correlation between our proposed evaluation metric HOIF and human preference, suggesting its reliability. Qualitative comparisons are shown in Figure. 4 in Appendix.\\n\\nTable 3: Subjective evaluation of generation results.\\n\\n| Model | Human | Object | Interaction | Overall |\\n|-------|-------|--------|-------------|---------|\\n| SD    | 2.48  | 3.24   | 3.67        | 2.98    |\\n| DHPC  | 3.01  | 2.27   | 2.97        | 2.15    |\\n| SAG   | 3.08  | 3.37   | 3.35        | 3.04    |\\n| Ours  | 3.31  | 3.66   | 4.03        | 3.54    |\\n| GT    | 4.35  | 4.34   | 4.09        | 4.25    |\\n\\n6.3. Ablations and Analysis\\nIn this section, we investigate how the performance of the proposed method is affected by different model settings. We study mainly in three aspects: contribution of network components, the formation of pose guidance and masking parameter sensitivity analysis.\\n\\nContribution of Network Components:\\nWe conduct a detailed ablation study by examining the effectiveness of each proposed component in our network structure, and the result are presented in Table 4, from which we can observe: (1) separately introducing Pose and Interaction boundary guidance (line 3-4) along with iterative refinement outperform Gaussian Blurring (line 2) and SD (line 1) under all HOI metrics (such as 3.61% body PCS for pose guidance) and FID (separately minimizes 3.87% and 3.14%), demonstrating that our guidance schemes are well-suited for HOI targeted refinement. (2) without iterative refinement based on quality assessment criteria (line 5), we outperform SD by only adopting pose and interaction boundary guidance, indicating their effectiveness. (3) incorporating iterative refinement (line 6) harvests continual enhancement over only one-step refinement (line 5) under all HOI metrics (such\"}"}
{"id": "vITl6CqIkk", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Experiments on contribution of network components. \\\"P\\\", \\\"IB\\\" and \\\"R\\\" denotes Pose guidance, interaction boundary guidance and iterative refinement. \\\"SD\\\" represents Stable Diffusion, \\\"GB\\\" denotes Gaussian Blurring. Experiments are conducted under H-A scenario.\\n\\n| MODEL | FID \u2193 | PCS(%) \u2191 | HODD(10^-2) \u2193 | PPD(10^-2) \u2193 | HOIF(%) \u2191 |\\n|-------|-------|-----------|----------------|---------------|-----------|\\n| BODY | 58.84 | 69.54 | 27.83 | 15.71 | 18.82 |\\n| HAND | 30.06 | 38.06 | 83.38 | 8.14 | 12.22 |\\n| ANIMAL | 8.02 | 12.15 | 9.11 | 12.22 | 9.24 |\\n| JOINT | 30.15 | 37.34 | 82.88 | 7.45 | 7.63 |\\n\\nFormation of pose guidance:\\nAs shown in Table 5, our body pose guidance (line 2) surpasses SD (line 1) with higher PCS and lower PPD. To test the generalization ability of our pose guidance, we implement more pose guidance beyond human body. Considering hands hold crucial part for various HOI categories, we adopt hand pose guidance, which follows the same process in Algorithm 1, except the pose attention is generated from hand pose detector rather than body pose detector. We also adopt animal pose guidance to improve animal generation. Experiments are conducted on H-A scenario. From results in Table 5, we observe: (1) separately applying pose guidance on hand part (line 3) and animal part (line 4) contributes to not only higher PCS (1.88% on hand), but also minimizes PDD for corresponding regions (1.50% on hand and 2.35% on animal). (2) By incorporating different guidance schemes, we harvest better performance than single pose guidance (line 5-7). This underscores the robust extensibility of our approach, allowing flexible adjustment of applied guidance types and combinations according to the specific HOI categories.\\n\\nTable 5: Experiments on formation of pose guidance.\\n\\n| MODEL | PCS(%) | PPD(10^-2) | HOIF(%) |\\n|-------|--------|-----------|--------|\\n| HAND | 27.83 | 12.22 | 9.24 | 7.63 |\\n| ANIMAL | 29.71 | 10.72 | 8.35 | 7.44 |\\n| JOINT | 28.05 | 13.11 | 6.89 | 6.83 |\\n| B+H | 31.50 | 10.56 | 7.67 | 6.97 |\\n| B+A | 29.67 | 11.14 | 6.73 | 6.40 |\\n| B+A+H | 31.37 | 10.71 | 6.67 | 6.42 |\\n\\nConclusion\\nWe present a method SA-HOI that utilizes pose quality and interaction boundary region information as guidance to generate high-quality human object interaction (HOI) images. We further introduce a HOI generation benchmark including a dataset and multiple tailored metrics for comprehensive quality evaluation. Experiments show the effectiveness of our method. With the method and benchmark, our work contributes novel insights to the HOI image generation field.\"}"}
{"id": "vITl6CqIkk", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Impact Statement\\n\\nHuman Object Interaction Image Generation carries significantly broader impact potential from both positive and negative sides. By generating realistic images depicting human-object interactions, our method contributes to diverse applications like immersive virtual environment creation. However, it also raises ethical concerns related to privacy and misinformation, which must be carefully considered and addressed to ensure responsible deployment and minimize potential negative consequences.\\n\\nAcknowledgements\\n\\nThis work was supported by the grants from National Natural Science Foundation of China (62372014, 61925201, 62132001).\\n\\nReferences\\n\\nAbdulla, W. Mask r-cnn for object detection and instance segmentation on keras and tensorflow. https://github.com/matterport/Mask_RCNN, 2017.\\n\\nChao, Y.-W., Wang, Z., He, Y., Wang, J., and Deng, J. HICO: A benchmark for recognizing human-object interactions in images. In Proceedings of the IEEE International Conference on Computer Vision, 2015.\\n\\nChen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., Zhang, Z., Cheng, D., Zhu, C., Cheng, T., Zhao, Q., Li, B., Lu, X., Zhu, R., Wu, Y., Dai, J., Wang, J., Shi, J., Ouyang, W., Loy, C. C., and Lin, D. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\\n\\nDhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021.\\n\\nDiller, C. and Dai, A. Cg-hoi: Contact-guided 3d human-object interaction generation, 2023.\\n\\nGal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.\\n\\nGao, C., Liu, S., Zhu, D., LIU, Q., Cao, J., He, H., He, R., and Yan, S. Interactgan: Learning to generate human-object interaction. Proceedings of the 28th ACM International Conference on Multimedia, 2020. URL https://api.semanticscholar.org/CorpusID:222278643.\\n\\nHo, J. and Salimans, T. Classifier-free diffusion guidance, 2022.\\n\\nHong, S., Lee, G., Jang, W., and Kim, S. Improving sample quality of diffusion models using self-attention guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7462\u20137471, 2023.\\n\\nHuang, K., Sun, K., Xie, E., Li, Z., and Liu, X. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation, 2023a.\\n\\nHuang, Z., Wu, T., Jiang, Y., Chan, K. C., and Liu, Z. Re-Version: Diffusion-based relation inversion from images. arXiv preprint arXiv:2303.13495, 2023b.\\n\\nHuggingFace, 2022. URL https://huggingface.co/CompVis/stable-diffusion-v1-4.\\n\\nJu, X., Zeng, A., Zhao, C., Wang, J., Zhang, L., and Xu, Q. Humansd: A native skeleton-guided diffusion model for human image generation, 2023.\\n\\nKim, G., Kwon, T., and Ye, J. C. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2426\u20132435, June 2022.\\n\\nLei, T., Caba, F., Chen, Q., Ji, H., Peng, Y., and Liu, Y. Efficient adaptive human-object interaction detection with concept-guided memory. 2023.\\n\\nLv, Z., Li, X., Li, X., Li, F., Lin, T., He, D., and Zuo, W. Learning semantic person image generation by region-adaptive normalization, 2021.\\n\\nMa, L., Jia, X., Sun, Q., Schiele, B., Tuytelaars, T., and Gool, L. V. Pose guided person image generation, 2018.\\n\\nMen, Y., Mao, Y., Jiang, Y., Ma, W.-Y., and Lian, Z. Control-lable person image synthesis with attribute-decomposed gan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5084\u20135093, 2020.\\n\\nMokady, R., Hertz, A., Aberman, K., Pritch, Y., and Cohen-Or, D. Null-text inversion for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.\\n\\nPavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A. A. A., Tzionas, D., and Black, M. J. Expressive body capture: 3d hands, face, and body from a single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nPeng, X., Xie, Y., Wu, Z., Jampani, V., Sun, D., and Jiang, H. Hoi-diff: Text-driven synthesis of 3d human-object interactions using diffusion models, 2023.\"}"}
{"id": "vITl6CqIkk", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Semantic-Aware Human Object Interaction Image Generation\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR, 2021.\\n\\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv e-prints, art. arXiv:2204.06125, April 2022. doi: 10.48550/arXiv.2204.06125.\\n\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10684\u201310695, June 2022.\\n\\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\\n\\nSong, Y. and Ermon, S. Improved techniques for training score-based generative models. arXiv preprint arXiv:2006.09011, 2020.\\n\\nvon Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lambert, N., Rasul, K., Davaadorj, M., and Wolf, T. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers, 2022.\\n\\nWeng, Z., Bravo-S\u00e1nchez, L., and Yeung-Levy, S. Diffusion-hpc: Synthetic data generation for human mesh recovery in challenging domains, 2023.\\n\\nXu, S., Li, Z., Wang, Y.-X., and Gui, L.-Y. Interdiff: Generating 3d human-object interactions with physics-informed diffusion. In ICCV, 2023.\\n\\nZhang, L. and Agrawala, M. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023.\\n\\nZhang, M., Wang, X., Decardi-Nelson, B., Bo, S., Zhang, A., Liu, J., Tao, S., Cheng, J., Liu, X., Yu, D., Poon, M., and Garg, A. Smpl: Simulated industrial manufacturing and process control learning environments, 2023.\"}"}
{"id": "vITl6CqIkk", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nRecent text-to-image generative models have demonstrated remarkable abilities in generating realistic images. Despite their great success, these models struggle to generate high-fidelity images with prompts oriented toward human-object interaction (HOI). The difficulty in HOI generation arises from two aspects. Firstly, the complexity and diversity of human poses challenge plausible human generation. Furthermore, untrustworthy generation of interaction boundary regions may lead to deficiency in HOI semantics. To tackle the problems, we propose a Semantic-Aware HOI generation framework SA-HOI. It utilizes human pose quality and interaction boundary region information as guidance for denoising process, thereby encouraging refinement in these regions to produce more reasonable HOI images. Based on it, we establish an iterative inversion and image refinement pipeline to continually enhance generation quality. Further, we introduce a comprehensive benchmark for HOI generation, which comprises a dataset involving diverse and fine-grained HOI categories, along with multiple custom-tailored evaluation metrics for HOI generation. Experiments demonstrate that our method significantly improves generation quality under both HOI-specific and conventional image evaluation metrics. The code is available at https://github.com/XZPKU/SA-HOI.git.\"}"}
{"id": "vITl6CqIkk", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Semantic-Aware Human Object Interaction Image Generation\\n\\n(2) Semantic complexity and Variability in Interactions: merely possessing a plausible human pose falls short of meeting the semantic demands inherent in HOI, the quality of interaction boundary region also plays a crucial role. As shown in Figure 1(b) first row, the third generated image from the Stable diffusion model lacks the portrayal of the human wrapping their arms around the cow at the interaction boundary, compromising the expression of \\\"human hugging a cow\\\". Similarly, the fourth generated image fails to express the \\\"hold\\\" semantic as hand and fork is not contacted.\\n\\nTo address the challenges above, we present a Semantic-Aware HOI (SA-HOI ) generation framework as shown in Figure 2. Firstly, we use stable diffusion model to generate an initial image. We employ a human pose detector trained on a large dataset of real human poses, to assess the quality of each generated body part. A lower confidence score for a specific keypoint from the pose detector indicates a deviation from typical human poses, signaling a potential issue with the quality of the generated body part (deformed body parts). Additionally, we identify interaction boundary regions by assessing the proximity of human body parts to the contour of the object with the help of segmentation tool. Secondly, we utilize a blurring technique on the region containing deformed body parts and the interaction boundary, which intentionally removes fine-grained information from the corresponding area. Then the remaining information within blurred content is subsequently utilized to guide image refinement. Specifically, in the reverse process of diffusion models, we leverage pose and interaction boundary aware attention maps to enhance the overall quality and minimize artifacts around the highlighted body parts or interaction boundary area. Thirdly, we introduce an Iterative inversion and Image Refinement pipeline, allowing the model to continually improve itself based on the refined image from the last iteration. This process leads to a gradual enhancement of generation quality, all achieved without the need for additional training. Some generated samples from our approach is presented in Figure 1(b) second row.\\n\\nTo the best of our knowledge, we are the first work concentrating on HOI image generation from pure text descriptions. We introduce a comprehensive benchmark for HOI generation, consisting of 150 prompts covering human-object, human-animal, and human-human interactions. The benchmark includes diverse yet fine-grained scenarios, such as \\\"holding\\\" 49 different objects and 16 ways of interacting with a \\\"horse\\\". We further propose specific evaluation metrics tailored for HOI image generation. These metrics comprehensively assess the quality of generated HOI images in terms of authenticity, plausibility, and fidelity. They reflect the quality of the generated body pose, HOI spatial configuration, and the degree of semantic consistency with the provided text.\\n\\nIn summary, our contributions are threefold. (1) We introduce a semantic-aware method to enhance overall quality and reduce artifacts for HOI generation. Equipped with an iterative inversion and image refinement pipeline, our model can continually enhance itself in a step-by-step manner. (2) We propose the first HOI generation benchmark covering human-object, human-animal and human-human interaction, along with evaluation metrics that are specifically designed for HOI generation. (3) Extensive experiments demonstrate our method outperforms existing diffusion-based methods under both HOI-specific and conventional evaluation metrics for image generation.\\n\\n2. Related Work\\n\\nSampling guidance for diffusion models. Multiple guidance schemes have been proposed for diffusion models recently. Classifier guidance (CG) (Dhariwal & Nichol, 2021) is proposed to use a classifier to guide the reverse process toward specific class distribution. (Ho & Salimans, 2022) proposes classifier-free guidance (CFG) as an alternate strategy for CG. DiffusionCLIP (Kim et al., 2022) expands text-to-image generation with CLIP guidance. SAG (Hong et al., 2023) further points out that self-attention maps within diffusion model can be adopted as guidance messages for reverse process, by utilizing which model can generate more high-quality images. However, self-attention maps mainly concentrate on high-frequency part within image, lacking targeted guidance toward human pose and interaction boundary region. Instead, our method explicitly concentrates on human pose and interaction boundary region, enhancing HOI image quality by refining these areas.\\n\\nPosed-guided Human Image Generation.\\n\\nPose-guided human image generation (HIG) has been well explored (Men et al., 2020; Lv et al., 2021; Ma et al., 2018), which aims to generate images with source image's appearance and desired pose condition. The development of ControlNet (Zhang & Agrawala, 2023) further leads to more approaches (Ju et al., 2023) focusing on the accuracy and diversity of pose control. (Weng et al., 2023) proposes to utilize SMPL (Zhang et al., 2023) model to provide plausible human pose prior to refine images. However, pose-guided HIG only targets for the rationality of human pose, while HOI generation has further requirements on interaction expression and fidelity to HOI semantics.\\n\\nCustomized Image generation for diffusion models.\\n\\nCustomized image generation emerges as personalized applications of diffusion-based models. ControlNet (Zhang & Agrawala, 2023) introduces additional controls to generate images with customized signals like depth and skeleton information. Textual Inversion (Gal et al., 2022) generates images for specific unique concepts by optimizing corresponding concept embedding with a few exampler images.\"}"}
{"id": "vITl6CqIkk", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compared with them, HOI generation not only concerns instance-level generation for humans and objects, but also requires semantic-level interaction generation, which has been seldom explored before. ReVersion (Huang et al., 2023b) concentrates on relation modelling, targeting for generating relation-customized images by optimizing relation embedding in inversion manner. However, it can only apply to 10 specific relations and need extra training for each relation, which is not efficient. Instead, our method not only requires no additional training, but also can be applied to 150 HOI categories. T2I-CompBench (Huang et al., 2023a) proposes a comprehensive benchmark for the compositional image generation and evaluation, primarily focusing on attribute binding, spatial relationships, and complex object compositions, but the adopted evaluation metrics lack comprehensive justification for HOI image quality, thus not ideally suitable for HOI image generation task. Another attempt InteractGAN (Gao et al., 2020) targets HOI image generation with a different task formulation, which requires human images, object images, as well as action categories as inputs, and generates images with consistent identity for the input human and object by adjusting the human pose. Compared with it, our HOI image generation task relies solely on textual prompts describing HOI categories and can accommodate 150 HOI categories, embracing greater diversity and superior scalability.\\n\\n3. Preliminaries\\n\\nDenoising Diffusion Probabilistic Models\\n\\nDenoising Diffusion Probabilistic Model (DDPM) recovers an image from Gaussian distribution noise through an iterative denoising process. Formally, given an image $x_0$ along with variance schedule $\\\\beta_t$ at a timestep $t \\\\in \\\\{T, T-1, \\\\ldots, 1\\\\}$, we can obtain $x_t$ through forward process, which is pre-defined as a Markovian process. For a trained diffusion model parameterized as $\\\\epsilon_\\\\theta(x_t, t)$ and $\\\\Sigma_\\\\theta(x_t, t)$, we define its reverse process as following:\\n\\n$$\\\\Sigma_\\\\theta(x_t, t) = \\\\beta_t = \\\\alpha^{2t}.$$ \\n\\nFor a given $x_T \\\\sim N(0, I)$, DDPM iteratively sample $x_{T-1}, x_{T-2}, \\\\ldots, x_0$ by computing:\\n\\n$$x_{t-1} = \\\\frac{1}{\\\\sqrt{\\\\alpha_t}} (x_t - \\\\beta_t \\\\sqrt{1 - \\\\bar{\\\\alpha}_t} \\\\epsilon_\\\\theta(x_t, t)) + \\\\sigma_t z (1)$$\\n\\nwhere $\\\\alpha_t = 1 - \\\\beta_t$, $\\\\bar{\\\\alpha}_t = \\\\sum_{i=0}^{t} \\\\alpha_i$, $z \\\\sim N(0, I)$ and $\\\\epsilon_\\\\theta$ is network parameterized by $\\\\theta$. By applying reparametrization trick, we can obtain $\\\\hat{x}_0$, an intermediate reconstruction of $x_0$ at a timestep $t$, using the following equation:\\n\\n$$\\\\hat{x}_0 = \\\\frac{1}{\\\\sqrt{\\\\bar{\\\\alpha}_t}} (x_t - \\\\sqrt{1 - \\\\bar{\\\\alpha}_t} \\\\epsilon_\\\\theta(x_t, t))$$\\n\\nGeneralized Guidance of Diffusion models\\n\\nGuidance schemes for diffusion models can be generalized as follows: the input for a diffusion model at timestep $t$ is generalized condition $h_t$ and perturbed sample $x_t$ that lacks $h_t$, and guidance can be formulated through the utilization of an imaginary regressor, $p_{im}(h_t|x_t)$, which aims to predict $h_t$ from $x_t$, whose formulation is:\\n\\n$$\\\\tilde{\\\\epsilon}_{x_t,h_t} = \\\\epsilon_\\\\theta(x_t, h_t) - s\\\\sigma_t \\\\nabla x_t \\\\log p_{im}(h_t|x_t) (3)$$\\n\\nwhere $s$ is the guidance scale. By the calculation of the gradient of $p_{im}$, samples generated under guidance are expected to be more suitable with that information stored in $h_t$. Further, with Bayes' rule, the gradient can be further formulated as:\\n\\n$$\\\\nabla x_t \\\\log p_{im}(c|X_t) = -\\\\frac{1}{\\\\sigma_t} (\\\\epsilon_\\\\theta(x_t, h_t) - \\\\epsilon_\\\\theta(x_t)) (4)$$\\n\\nthen we acquire the final expression of $\\\\tilde{\\\\epsilon}_{x_t,h_t}$ as follows:\\n\\n$$\\\\tilde{\\\\epsilon}_{x_t,h_t} = \\\\epsilon_\\\\theta(x_t) + (1 + s)(\\\\epsilon_\\\\theta(x_t, h_t) - \\\\epsilon_\\\\theta(x_t)) (5)$$\\n\\nBy storing different information inside $h_t$, we can provide corresponding guidance during the generation process. Specifically, applying Gaussian kernel $G_{\\\\sigma}$ convolution over $x_t$, i.e., $\\\\tilde{x}_t = G_{\\\\sigma} * x_t$, can be viewed as one simplified guidance scheme, as $h_t = x_t - \\\\tilde{x}_t$ and $x_t = x_t$ in Equation 5, which is proven (Hong et al., 2023) to effectively guide diffusion more appropriate to the salient information stored in $h_t$ with a moderate $\\\\sigma$ in Gaussian kernel, thus harvesting more high-quality generation.\\n\\n4. Method\\n\\n4.1. Overview\\n\\nThe overall framework of SA-HOI is shown in Figure 2. It consists of two designs: Pose and Interaction Boundary Guidance (PIBG) and Iterative Inversion and Refinement (IIR). In PIBG, for given HOI prompt $t_0$ and noise $n_0$, we first utilize Stable Diffusion to generate $I_0$ as the initial image. To measure the pose quality of $I_0$, we adopt a pose detector to acquire human body joint positions $\\\\{P_{\\\\text{pose}} \\\\}_{i=1}^{N_{\\\\text{pose}}}$ and corresponding confidence score $\\\\{S_{\\\\text{pose}} \\\\}_{i=1}^{N_{\\\\text{pose}}}$, where $N_{\\\\text{pose}}$ is visible joint number for human in $I_0$. Considering that a low confidence score signals potentially deformed generation for specific body parts, we utilize $S_{\\\\text{pose}}$ and $P_{\\\\text{pose}}$ to construct pose mask $M_{\\\\text{pose}}$, which highlights low-quality pose regions. Considering the spatial context of interactions, we identify interaction boundary regions by assessing the proximity of human body parts to the contour of the object with the help of segmentation tool. These identified regions are highlighted in the interaction boundary mask $M_{\\\\text{inter}}$ to enhance the semantic expression in the vicinity of interaction boundary areas. For each denoising step, to guide the model focusing on low pose quality or interaction boundary regions and minimizing artifacts in them, we adopt Gaussian Blurring on these regions by utilizing $M_{\\\\text{pose}}$ and $M_{\\\\text{inter}}$ as region constraints, and subsequently...\"}"}
{"id": "vITl6CqIkk", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Semantic-Aware Human Object Interaction Image Generation\\n\\n1. Pose and Interaction Boundary Guidance\\n\\nLow-quality Pose Mask $M_{\\\\text{pose}}$ and interaction boundary mask $M_{\\\\text{inter}}$ are adopted to refine low pose quality part and interaction boundary region of original image $I_0$ in each denoising step as detailed in Algorithm 1. IIR gradually enhances generation quality based on inversion model $N$ and PIBG as shown in Algorithm 2. Our benchmark includes a Dataset of realistic images covering human-object, human-animal, and human-human interactions, as well as comprehensive HOI Evaluation Metrics reflecting authenticity, plausibility and fidelity of generated HOI images.\\n\\n4.2. Pose Guidance for HOI Generation\\n\\nTo deal with the complex and diverse human structure across various scenarios, we propose to integrate pose guidance into the process of HOI generation. The pseudo code of our pose and interaction boundary guided sampling is shown in Algorithm 1. In each denoising step, we first acquire predicted noise $\\\\epsilon_t$ and intermediate reconstruction $\\\\hat{x}_0$ (line 5-6) following conventional design in Stable Diffusion. Then we apply Gaussian Blurring $G$ on $\\\\hat{x}_0$ to get degraded latent feature $\\\\tilde{x}_0$ and $\\\\tilde{x}_t$ (line 7-8). To apply the derivation presented in Equation 5, we incorporate pose quality information within $h_t$ by utilizing the pose detection results $P_{\\\\text{pose}}$ and $S_{\\\\text{pose}}$.\\n\\nPose Attention and Mask Generation $P_{\\\\text{pose}}$ and $S_{\\\\text{pose}}$ are utilized to generate $A_{\\\\text{pose}}$ and $M_{\\\\text{pose}}$, which aims to highlight low pose quality regions and guide model to diminish deformed generation in these regions. For each joint $P_{\\\\text{pose}}_i = (x_i, y_i)$, the score $S_{\\\\text{pose}}_i$ represents the reliability of corresponding joint generation, where a lower score denotes a higher possibility of low-quality generation. To guide the model to refine the low-quality areas, we highlight the regions with low pose scores by calculating $G_i(x, y) = \\\\frac{1}{S_{\\\\text{pose}}_i^2} \\\\exp(-\\\\frac{(x-x_i)^2 + (y-y_i)^2}{2\\\\sigma^2})$ (6) where $G_i \\\\in \\\\mathbb{R}^{H \\\\times W}$, $x, y$ are pixel-wise coordinates of image, $H, W$ are image size and $\\\\sigma$ is the deviation of Gaussian distribution. $G_i$ represents the attention centered on the $i$th joint, where a lower $S_{\\\\text{pose}}_i$ leads to increased emphasis on...\"}"}
{"id": "vITl6CqIkk", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"with high attention, indicating areas that require refinement, with potential generation problems aforementioned.\\n\\nTo dynamically determine the ratio of refined areas as the generation progresses, for a given human-object pair, we detect the joint pose quality areas, but also mitigate unwarranted effects on unrelevant areas, i.e., poor pose quality areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nusing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two factors: (1) \\\\( \\\\text{pose} \\\\) to activate low pose quality areas, and (2) \\\\( \\\\theta \\\\) to activate low pose quality areas, but also mitigate unwarranted effects on irrelevant areas. Then degraded pose quality regions, we sample\\n\\n\\\\[ \\\\text{pose} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nUsing the pose blurring process. This allows us to selectively modify only the regions of interest in the image. Following Equation 5, we transform pose data into a mask using a threshold.\\n\\n\\\\[ \\\\text{mask} = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nFurther, to preserve the majority of the generated content and modulate the changes in the image content, we can form the ultimate attention map \\\\( A \\\\) across all joints, which accommodates varying sample content and modulates the refinement process.\\n\\n\\\\[ A = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nThe mask \\\\( M \\\\) is obtained by downsampling:\\n\\n\\\\[ M = \\\\text{DownSample}(\\\\text{pose}) \\\\]\\n\\nWe then consider two"}
{"id": "vITl6CqIkk", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2\\nIterative Image Refinement Pipeline\\n\\n1: Input: Threshold $\\\\theta$ to stop iterative refinement. Origin image $I_0$ and score $S_0$ evaluated by $Q$. Largest iterate rounds $K$.\\n\\n$I_{\\\\text{refine}} = \\\\{\\\\}$, $S_{\\\\text{refine}} = \\\\{\\\\}$ to store refined images and scores.\\n\\n2: Output: refined HOI image $I$ in $I_{\\\\text{refine}}$ with highest score in $S_{\\\\text{refine}}$.\\n\\n3: for $k = 0, 1, 2, \\\\ldots, K-1$ do\\n\\n4: $n_k, t_k \\\\leftarrow N(I_k)$ \\\\{Acquire noise and text embedding for $I_k$\\\\}\\n\\n5: $I_{k+1} \\\\leftarrow \\\\text{SA-HOI}(n_k, t_k)$ \\\\{Refine image\\\\}\\n\\n6: $S_{k+1} \\\\leftarrow Q(I_{k+1})$ \\\\{Assess Quality of $I_{k+1}$\\\\}\\n\\n7: $I_{\\\\text{refine}} \\\\leftarrow I_{\\\\text{refine}} + I_{k+1}$ \\\\{Store $I_{k+1}$\\\\}\\n\\n8: $S_{\\\\text{refine}} \\\\leftarrow S_{\\\\text{refine}} + S_{k+1}$\\n\\n9: if $S_{k+1} - S_k < \\\\theta$ then \\\\{No significant change\\\\}\\n\\n10: break \\\\{Finish Iterative Refinement\\\\}\\n\\n11: end if\\n\\n12: end for\\n\\n---\\n\\n5. HOI Generation Benchmark\\n\\n5.1. Dataset\\n\\nOur dataset consists of 150 HOI categories, covering human-object, human-animal, and human-human interaction scenarios for comprehensive evaluation. The categories are all collected from public HOI detection data-set HICO-DET (Chao et al., 2015).\\n\\n**H-A:** $\\\\langle \\\\text{human, verb, animal} \\\\rangle$, where all 91 HOI categories concerning animal in HICO-DET are included.\\n\\n**H-O:** $\\\\langle \\\\text{human, hold, object} \\\\rangle$. We meticulously select 49 HOI categories with \u201chold\u201d as verb, ensuring physical contact between human and object.\\n\\n**H-H:** $\\\\langle \\\\text{human, verb, human} \\\\rangle$, which includes all 10 human-human interaction categories in HICO-DET. We select real images from HICO-DET for these HOI categories as reference images, with each category randomly sampling up to most 200 images. To exclude the influence of multiple HOI co-existing in one image, we crop the corresponding HOI union part as real image samples. To sum up, we collect 5k images to serve as our HOI generation dataset, allowing us to estimate the distribution of real images. Example images of our dataset are shown in Figure. 5 in Appendix.\\n\\n5.2. Evaluation Metrics\\n\\nTo better assess the generated HOI image quality, we tailored several metrics for HOI generation, enabling comprehensive evaluation of generated images from the perspectives of plausibility, authenticity, fidelity.\\n\\n**Authenticity**\\n\\n**Pose Distribution Distance (PDD):** Humans typically adopt similar poses for the same HOI category, thereby conforming to corresponding pose distribution. Consequently, we measure the distribution distance between real images and generated images to gauge the authenticity of the generated human poses. Formally, given $N_{\\\\text{real}}$ real images and $N_{\\\\text{syn}}$ generated images, we detect joints coordinates $\\\\{J_i = (X_i, Y_i)\\\\}_{K_i=1}^K$ for human in each image. We normalize $J_i$ by the width $W$ and height $H$ of human bounding box to eliminate the influence of human size, and turn $\\\\{J_i\\\\}_{K_i=1}^K$ into $\\\\{\\\\hat{J}_i\\\\}_{K_i=1}^K$ by transforming relative reference system with the human hip joint as the origin, with the aim to better capture the pose information. Then we utilize KL-Divergence to measure the pose distribution distance between real and generated pose utilizing $\\\\hat{J}_i$. Given $P = \\\\{\\\\hat{J}_{\\\\text{syn}}_j\\\\}_{N_{\\\\text{syn}}=1}^N$ and $Q = \\\\{\\\\hat{J}_{\\\\text{real}}_j\\\\}_{N_{\\\\text{real}}=1}^N$, we calculate the distribution distance as $\\\\text{PDD}(P, Q) = \\\\sum_{i=1}^{N_{\\\\text{real}}} P(i) \\\\log \\\\frac{P(i)}{Q(i)}$, and average over all HOI categories to serve as PDD, with a lower PDD suggests a more precise matching to real poses.\\n\\n**Human-Object Distance Distribution (HODD):** Similarly, human-object pairs involved in the same HOI category exhibit a consistent spatial configuration pattern, approximated by the distribution of distances between humans and objects. We utilize human joints and object outer contours to calculate their distances. For a given human object pair, we detect the joint coordinates of the person $\\\\{C_i\\\\}_{K_i=1}^K$ and obtain the outer contour points $\\\\{O_{ot}\\\\}_{N_{ot}=1}^N$ of the object through a segmentation model, where $K$ is joint number for human and $N_{ot}$ is number for representing object outer contour. The exact distance between the person and the object is then represented by computing the closest distance between $C_i$ and $O_i$, which is formulated as $\\\\text{DIS}(C, O) = \\\\min_{i,j} ||C_i - O_j||^2$. Then we measure the distribution distance between generated and real images with KL-Divergence following the same procedure as PDD.\\n\\n**Plausibility**\\n\\n**Pose Confidence Score (PCS):** The confidence score of pose\\n\\n6\"}"}
{"id": "vITl6CqIkk", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"showcases the plausibility and rationality of the detected human joints. Formally, for a detected person with $K$ body joints along with confidence scores $\\\\{S_i\\\\}_{i=1}^K$ within image $I$, the pose confidence for $I$ is $\\\\prod_{i=1}^K S_i$, and we average pose confidence across all generated images to serve as PCS. Given that the human pose detector is trained on an extensive dataset of real human poses, a higher Pose Confidence Score (PCS) indicates increased plausibility in the human generation. Conversely, a low score suggests a departure from typical real human poses used in detector training, signaling a potential issue with the quality of the generated body part, termed as deformed body parts.\\n\\n**Fidelity**\\n\\n**Human Object Interaction Fidelity (HOIF):** To measure the semantic fidelity between images and HOI categories, we introduce HOI detector as oracle model to examine the alignment of images. An off-the-shelf HOI detector $g(\\\\cdot)$ (Lei et al., 2023) trained on HICO-DET (Chao et al., 2015) is adopted to detect possible HOI triplets in images. Since no available ground truth bounding boxes can be treated as annotation, we simplify the metric as the confidence score on the target HOI category detection. Given the image $I$, we detect all possible HOI triplets for given target HOI category $c$ and a threshold $\\\\theta$ for activate HOI examination, $S_c = g(I, c, \\\\theta)$, where $S_c$ is the confidence score for the detected HOI. We average over all generated images as HOIF, with higher HOIF signifying better semantic consistency between images and HOI text prompts.\\n\\n**R-accuracy:** We measure the retrieval accuracy of images to their HOI categories to measure fidelity. Formally, we utilize CLIP (Radford et al., 2021) to encode image embedding $E_i$ and HOI categories text embedding $E_t$, and calculate the accuracy of images retrieved to their own HOI categories within $E_t$. Higher accuracy is positively correlated with higher semantic consistency between text prompts and images.\\n\\n### 6. Experiment\\n\\n#### 6.1. Implementation Details\\n\\nWe employ the Stable Diffusion v1.5 (Rombach et al., 2022) as the base text-to-image model and apply our SA-HOI on it. Hyperparameters $\\\\theta$, $\\\\delta$, $\\\\phi_0$, $\\\\alpha$, $T$ are set as 0.01, 1, 1, 0.6 and 4. For evaluation, we adopt general image evaluation metrics, including FID, KID, and our tailored metrics for HOI generation as introduced in Sec. 5.2. We also include a subjective evaluation for more comprehensive evaluation of our method. We utilize our iterative image refinement pipeline to generate HOI images for evaluation. More details can be found in Appendix.\\n\\n#### Table 1: Comparison on general text-to-image metrics with other methods.\\n\\n| MODEL          | SCENARIO | FID  | KID(10^-2) |\\n|----------------|----------|------|-------------|\\n| TABLE DIFFUSION| H-A      | 58.84| 2.022       |\\n| SAG            | H-A      | 56.91| 2.019       |\\n| TABLE DIFFUSION| HPC      | 59.42| 2.043       |\\n| OURS           | H-A      | 54.55| 1.812       |\\n| TABLE DIFFUSION| H-O      | 78.28| 2.896       |\\n| SAG            | H-O      | 75.99| 2.845       |\\n| TABLE DIFFUSION| HPC      | 77.92| 3.012       |\\n| OURS           | H-O      | 74.70| 2.784       |\\n| TABLE DIFFUSION| H-H      | 137.66| 3.5792     |\\n| SAG            | H-H      | 138.06| 3.6213     |\\n| TABLE DIFFUSION| HPC      | 139.43| 3.9578     |\\n| OURS           | H-H      | 134.72| 3.3218     |\\n\\n#### 6.2. Comparison with other methods\\n\\n**General Metrics:** We compare our methods with Stable Diffusion (Rombach et al., 2022), SAG (Hong et al., 2023) and Diffusion HPC (Weng et al., 2023) in Table 1. We observe: (1) Our method has lower FID and KID than Stable Diffusion under each scenario (H-A, H-O, H-H), which signifies by rectifying pose and interaction boundary regions, our method stably harvests quality improvement by statistical significance. (2) We still harvest lower FID and KID compared with SAG under each scenario, indicating our tailored guidance provides more accurate and suitable information for the refinement of HOI images than self-attention maps, thus gaining further improvement. (3) We outperform Diffusion HPC by 4.87%/ 3.22%/ 4.71% on FID (line 3/ 7/11), as well as KID. This showcases rectifying interaction boundary regions further improves overall image quality.\\n\\n**HOI metrics:** The results for HOI-specific metrics are shown in Table 2, from which we can observe (1) Plausibility: Credit to our pose guidance refining on low-quality poses, both PCS for body and hand joints are improved under all scenarios compared to SD, indicating SA-HOI enhances the credibility for joint poses. Also, our PCS is comparable with Diffusion HPC (line11-12), who utilize extra SMPL (Pavlakos et al., 2019) model to provide human pose prior, indicating our plausibility in human pose generation. (2) Authenticity: Our method surpasses other comparison approaches on all splits in terms of HODD and PDD metrics. This suggests that both the distribution of body/hand/animal pose and the distribution of inter-distance (spatial configuration pattern) in our generated images closely resemble realistic distributions. (3) Fidelity: Thanks to the interaction boundary guidance to improve generation quality on interaction boundary regions, we achieve the highest HOIF.\"}"}
