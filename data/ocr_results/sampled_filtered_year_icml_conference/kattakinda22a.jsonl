{"id": "kattakinda22a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\nPriyatham Kattakinda, Soheil Feizi\\n\\nAbstract\\nStandard training datasets for deep learning often do not contain objects in uncommon and rare settings (e.g., \\\"a plane on water\\\", \\\"a car in snowy weather\\\"). This can cause models trained on these datasets to incorrectly predict objects that are typical for the context in the image, rather than identifying the objects that are actually present. In this paper, we introduce FOCUS (Familiar Objects in Common and Uncommon Settings), a dataset for stress-testing the generalization power of deep image classifiers. By leveraging the power of modern search engines, we deliberately gather data containing objects in common and uncommon settings; in a wide range of locations, weather conditions, and time of day. We present a detailed analysis of the performance of various popular image classifiers on our dataset and demonstrate a clear drop in accuracy when classifying images in uncommon settings. We also show that finetuning a model on our dataset drastically improves its ability to focus on the object of interest leading to better generalization. Lastly, we leverage FOCUS to machine annotate additional visual attributes for the entirety of ImageNet. We believe that our dataset will aid researchers in understanding the inability of deep models to generalize well to uncommon settings and drive future work on improving their distributional robustness.\\n\\n1. Introduction\\nSince the remarkable success of AlexNet (Krizhevsky et al., 2012) in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2015), deep learning models have been used in a variety of applications ranging from robotics and self-driving cars to stock trading and computational biology. Undoubtedly, large scale datasets such as ImageNet (Deng et al., 2009) deserve much of the credit for the success of deep learning. These datasets typically consist of natural images of objects in some environment. For our purposes, the environment in an image includes all the contextual information surrounding the object in the image. Evidently, objects do not occur independently of their environments. In other words, objects are more likely to be found in some environments than in others (we call these common settings). For example, ships are often on water; cars are usually on streets; birds are usually on trees, etc. Search engines are more likely to return images with objects in their common settings when queried for an object alone, i.e., without any additional qualifiers (e.g., just \\\"deer\\\" or \\\"frog\\\"). As a result, objects in uncommon settings are often missing in many of the popular datasets in use today. Therefore, a classifier's performance on these datasets is not indicative of how well it does in novel environments.\\n\\nTo address this issue, we introduce a new dataset containing images both in common and uncommon settings called FOCUS (Familiar Objects in Common and Uncommon Settings). Our key idea is that modern search engines often return many relevant results even for qualified queries of objects in uncommon settings. For example, searching \\\"bird indoors\\\" still returns a few relevant images even though this is an uncommon setting. Building on this idea, we collect images of objects in various common and uncommon environments explicitly. FOCUS has around 21K images of ten objects along with annotations for different aspects of the environment in the images including a wide range of locations, weather conditions, and time of day. Depending on the class, we further annotate these environmental settings as common or uncommon.\\n\\nUsing FOCUS, we assess the performance of some popular deep learning models with high accuracy on ImageNet, on uncommon settings. We observe that all of these popular models show significant drop in accuracy when tested on objects in uncommon settings. Next, we finetune these models on FOCUS and show clear evidence for substantial improvement in generalization. We also use FOCUS to train classifiers that can detect various visual attributes we consider in FOCUS and use these classifiers to add additional annotations to ImageNet. To the best of our knowledge, FOCUS is the first large-scale dataset of natural images...\"}"}
{"id": "kattakinda22a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\n(a) deer at night\\n(b) plane in snowy weather\\n(c) ship indoors\\n(d) bird at night\\n(e) car in rain\\n(f) frog in snow\\n\\nFigure 1: Some uncommon images in the FOCUS dataset. The images in the first column depict uncommon time of day, those in the second column depict uncommon weather, and the ones in the third column depict uncommon locations.\\n\\nwith explicit environmental annotations such as locations, weather conditions, and time of day for both common and uncommon settings. We believe richly annotated datasets such as FOCUS can pave the way to develop models that not only have high accuracy in common settings but are reliable in rare and uncommon settings as well. Our dataset and code for evaluating models on FOCUS are available at https://github.com/priyathamkat/focus.\\n\\n2. Related Work\\n\\nDeep neural networks are well known to rely on spurious features or \u201cshortcuts\u201d for image classification and object detection (Geirhos et al., 2020). Beery et al. (2018) demonstrate this phenomenon in the case of detection and classification of animals in uncommon locations; cows are improperly detected or incorrectly classified in beaches. Sagawa et al. (2020) show a similar issue in regard to classification of waterbirds and the color of human hair in CelebA (Liu et al., 2015). Rosenfeld et al. (2018) observe that object detectors may not always detect objects artificially placed into an image. Further, these objects can have a non-local impact, causing the detector to not detect other objects in the image.\\n\\nIn light of the above, test accuracy on datasets like CIFAR (Krizhevsky et al., 2014), ImageNet (Deng et al., 2009), etc., though crucial, is insufficient to measure the efficacy of deep neural networks. Many datasets have been proposed in the literature for testing the out-of-distribution effectiveness of classification models. Hendrycks et al. (2021a) propose three datasets, namely, ImageNet-Renditions, DeepFashion Remixed, StreetView StoreFronts that are designed to test the generalization ability of classifiers to unseen rendition styles, camera view points, and geography, respectively. Barbu et al. (2019) propose ObjectNet, a dataset that provides a far richer variation in the rotation, viewpoint and backgrounds of many object classes in ImageNet. They observe that models trained on ImageNet are 40-45% less accurate on ObjectNet. Realistic corruptions such as blur, noise, etc., can occur in the real world, for instance, due to camera shake, low light, etc. Thus, training on high-quality clean images may cause classifiers to perform poorly on corrupted images. Hendrycks & Dietterich (2019) propose ImageNet-C, a dataset of 15 types of artificially corrupted images to systematically study robustness of deep learning models against (synthetic) corruptions. Hendrycks et al. (2021a) propose Real Blurry Images, a dataset of 1000 real world blurry images. ImageNet-A (Hendrycks et al., 2021b) is a dataset of natural, adversarial images which yield drastically low performance on classifiers trained on ImageNet.\\n\\nIn parallel to OOD datasets, many works have been proposed to explicitly find the spurious dependencies on context and/or circumvent them. Singla et al. (2021) propose a method for identifying the visual attributes that cause classification failures using the features of an adversarially robust model. Xiao et al. (2020) propose the Backgrounds\"}"}
{"id": "kattakinda22a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\nChallenge to evaluate how robust models are to (synthetic) changes in backgrounds. Sauer & Geiger (2021) propose a generative framework which allows choosing the color, texture and background of a generated image independently. Using this, they generate counterfactual images and show that training on these images improves out-of-distribution robustness.\\n\\nBDD100K (Yu et al., 2020) is a large scale video dataset for driving that covers a wide range of locations, environments and weather conditions. Wong et al. (2021) show that training sparse linear models with deep features as inputs results in improved debuggability of neural networks. In a similar vein, 3DB (Leclerc et al., 2021) uses photorealistic simulations to test and debug computer vision models. Beery et al. (2018) introduce a dataset of camera trap images. Since the traps are fixed, the backgrounds in these images are also more or less fixed and hence this dataset provides an ideal testing ground for classification/detection in uncommon contexts.\\n\\nA large body of influential work proposes models that explicitly use context to improve image classification or object detection performance. Galleguillos et al. (2008) use a conditional random field to incorporate spatial and semantic context. Mottaghi et al. (2014) propose a deformable parts based model that uses both local and global context to improve object detection at various scales. Bell et al. (2016) present a novel architecture called the Inside-Outside Net which uses skip pooling to extract context inside the region of interest while the information from outside the ROI is extracted through spatial recurrent neural networks. Choi et al. (2012) build an explicit support context model to capture inter-object physical relationships and use it to detect out-of-context objects. Lastly, Divvala et al. (2009) is an extensive survey on the role of various types of context in object detection.\\n\\n3. FOCUS: A Dataset with Common and Uncommon Settings\\n\\n3.1. Building FOCUS\\n\\nWe use the time of day, weather and the locations depicted in an image to characterize environment in it. Modern search engines are capable of returning relevant results even for uncommon qualified queries of objects (e.g., \\\"frog indoors\\\"). We leverage this to collect images of objects in various common and uncommon environments explicitly. Concretely, we query the Microsoft Bing Image Search API with statements of the form <object> <preposition> <attribute> (e.g., \\\"ship on grass\\\"). This ensures that we collect a great variety of uncommon images. We also use synonyms of the object categories and the attributes to increase the number of samples we collect. Note that we only query for images which have a license that permits sharing allowing us to release the FOCUS dataset for the research community.\\n\\nWe collect a total of around 37K images using the above procedure. But the search results are not always accurate; a significant fraction of them do not have the relevant object or if they do, the object is not in the environment mentioned in the search query. In addition, because we query images based on only one attribute, we do not have any information about the other attributes in the images. For instance, we do not know the time of day or the locations in a search result for \\\"car in rain.\\n\\nWe conduct an Amazon Mechanical Turk study both to improve the accuracy of annotations inferred from the search queries and to collect missing annotations. Images are shown to workers in a series of Human Intelligence Tasks (HITs), and they are asked to annotate the image with the appropriate choice for the different attributes. See the appendix B for more details about the design of our HITs.\\n\\n3.2. The Data in FOCUS\\n\\nThe FOCUS dataset is a collection of around 21K images, each annotated with the time of day, the weather condition and the locations in the image. Concretely, our dataset is as follows:\\n\\n\\\\[ \\\\{(x_i, y_i, t_i, w_i, l_i)\\\\} \\\\]\\n\\nwhere\\n\\n- \\\\(x_i\\\\) is the image\\n- \\\\(y_i\\\\) is the object label\\n- \\\\(t_i\\\\) is the time of day\\n- \\\\(w_i\\\\) is the weather\\n- \\\\(l_i\\\\) are the locations\\n\\nThe rationale behind our choices for different attributes is as follows:\\n\\n1. Object Label: We choose to work with the same 10 object classes in CIFAR-10 (Krizhevsky et al., 2014). These classes cover 226 (including various birds, animals and vehicles) of the 1000 classes in ImageNet.\\n2. Time of day: Most images in standard datasets are captured during the day, when the objects are well lit. In contrast, nighttime images often lack a lot of details and are corrupted by high levels of noise.\\n3. Weather: Our choices of weather are fairly comprehensive and include the raining, snowing and foggy.\"}"}
{"id": "kattakinda22a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"**FOCUS: Familiar Objects in Common and Uncommon Settings**\\n\\nTable 1: A frequency breakdown of the various categories and attributes in the FOCUS dataset. Uncommon settings are highlighted in orange.\\n\\n| Location       | Truck | Car  | Plane | Ship | Cat  | Dog  | Horse | Deer | Frog | Bird |\\n|----------------|-------|------|-------|------|------|------|-------|------|------|------|\\n| **Time of Day**|       |      |       |      |      |      |       |      |      |      |\\n| Day            | 1036  | 2232 | 1498  | 1573 | 1809 | 2415 | 2989  | 1622 | 931  | 2298 |\\n| Night          | 50    | 241  | 128   | 159  | 72   | 56   | 60    | 51   | 180  | 45   |\\n| None           | 29    | 212  | 109   | 29   | 941  | 503  | 66    | 34   | 323  | 124  |\\n| **Weather**    |       |      |       |      |      |      |       |      |      |      |\\n| Cloudy         | 139   | 301  | 259   | 324  | 50   | 151  | 186   | 95   | 17   | 171  |\\n| Foggy          | 27    | 103  | 56    | 109  | 7    | 46   | 105   | 78   | 1    | 43   |\\n| Partly Cloudy  | 145   | 310  | 305   | 309  | 111  | 170  | 284   | 175  | 44   | 151  |\\n| Raining        | 10    | 92   | 19    | 7    | 11   | 7    | 14    | 2    | 3    | 17   |\\n| Snowing        | 9     | 39   | 4     | 3    | 23   | 39   | 36    | 44   | 0    | 32   |\\n| Sunny          | 538   | 956  | 694   | 687  | 639  | 1083 | 957   | 742  | 423  | 1184 |\\n| None           | 247   | 884  | 398   | 322  | 1981 | 1478 | 533   | 571  | 946  | 869  |\\n| **Locations**  |       |      |       |      |      |      |       |      |      |      |\\n| Forest         | 172   | 378  | 178   | 79   | 123  | 247  | 586   | 916  | 142  | 400  |\\n| Grass          | 326   | 651  | 391   | 105  | 517  | 804  | 1142  | 1255 | 274  | 541  |\\n| Indoors        | 31    | 259  | 125   | 5    | 1344 | 790  | 93    | 10   | 39   | 40   |\\n| Rocks          | 43    | 115  | 55    | 71   | 137  | 109  | 92    | 95   | 239  | 183  |\\n| Sand           | 295   | 526  | 239   | 187  | 143  | 414  | 662   | 214  | 152  | 296  |\\n| Street         | 708   | 1635 | 389   | 103  | 405  | 372  | 417   | 103  | 29   | 130  |\\n| Snow           | 88    | 255  | 110   | 140  | 127  | 298  | 206   | 290  | 7    | 176  |\\n| Water          | 46    | 211  | 296   | 1606 | 93   | 403  | 296   | 122  | 355  | 581  |\\n| None           | 56    | 96   | 526   | 51   | 372  | 279  | 146   | 49   | 446  | 699  |\\n\\n3.3. Common vs. Uncommon Settings\\n\\nWe consider two sources of uncommon (object, environment) pairs:\\n\\n1. The pair is uncommon in the real world (e.g., \\\"ship on grass\\\"). On the Internet, searching for \\\"ship\\\" alone is extremely unlikely to return any images of a ship on grass in the top results. In other words, the rarity of a pair in the real world is reflected in the dataset.\\n\\n2. The pair is uncommon due to the choice of labels and queries used to construct a particular benchmark. For example, consider the \\\"plane\\\" class. ImageNet has two labels corresponding to an airplane: \\\"warplane, military plane\\\", \\\"airliner\\\". Neither of these are planes that are usually found on water making \\\"plane in water\\\" an uncommon, if not a non-existent pair in ImageNet. Seaplanes, however, are not that uncommon in the real world.\\n\\nWe declare an (object, attribute) uncommon if the number of samples corresponding to that pair is low (case 1 above). Additionally, we also declare the \\\"(plane, water)\\\" pair as uncommon (case 2 above). Our final choices for uncommon settings are highlighted in orange, in table 1. Figure 1 shows some uncommon images from our dataset.\\n\\nWe believe that assigning \\\"night\\\" as uncommon Time of Day and \\\"snowing\\\", \\\"raining\\\", \\\"foggy\\\" as uncommon Weather is reasonable. However, categorizing locations into common and uncommon settings for various objects is more subjective. In order to further justify our assignments, we use Gram matrices to identify out-of-distribution samples (Sastry & Oore, 2020). We first compute the Gram matrix deviations for images in the validation set of ImageNet. Next, we identify the threshold that achieves a 95% true positive rate (TPR) for these images. Finally, we compute the same deviations for images in FOCUS and note the fraction (table 2) of images with deviations above the threshold. For each class (row), we see that the locations with higher percentages are, generally speaking, tagged as uncommon (highlighted in orange).\"}"}
{"id": "kattakinda22a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\n4.4. Expanding ImageNet with FOCUS\\n\\nAlthough FOCUS has rich sample annotations, its relatively small size prohibits training deep classifiers on it from the scratch. In this section, we show that FOCUS can be effectively used to provide rich annotations for other large-scale and standard image datasets such as ImageNet. To do this, we train three classifiers on FOCUS, one for predicting each of the contextual attributes considered in FOCUS. Specifically, we use a pretrained ResNet50 model as backbone and attach a linear head for $n$-way classification ($n = 3, 7, 9$ for time of day, weather, and location, respectively). To estimate the efficacy of these classifiers on ImageNet, we conducted a separate AMT survey where we ask AMT workers to annotate the contextual attributes for 5,000 randomly chosen images from ImageNet. We find that these classifiers are reasonably accurate: the predictions of the classifier agree with that of the humans 74%, 71%, and 75% of the time, for each of the attributes, respectively.\\n\\nFinally, we use the attribute classifiers to provide rich annotations of time, weather and location for all ImageNet samples. Figure 5 shows the statistics of various attributes in the train split of ImageNet. Interestingly, we observe that some contexts such as inclement weather and images of objects in the dark are severely underrepresented. Given the wide use of Imagenet in the machine learning community, we believe that such richly annotated version of it can facilitate large-scale future works to improve model generalization. To that end, we will release these annotations on ImageNet, along with the FOCUS dataset upon the acceptance of this paper.\\n\\n5. Conclusion\\n\\nIn this work, we introduced FOCUS, a dataset that contains images both in common and uncommon settings. FOCUS has around 21K samples annotated by their environmental attributes such as locations, weather, and time of day. Using FOCUS, we evaluated the performance of several popular ImageNet classifiers. These models clearly have reduced performance when classifying images in uncommon settings. We showed that finetuning on our dataset alleviates this issue. We also used FOCUS to machine annotate ImageNet with the attributes mentioned above. We believe that richly annotated datasets such as FOCUS open new directions for the development of deep models that are reliable both in common and uncommon settings.\\n\\nAcknowledgements\\n\\nThe authors would like to thank Yogesh Balaji and Mazda Moayeri for their helpful comments and suggestions.\\n\\nThis project was supported in part by NSF CAREER AWARD 1942230, a grant from NIST 60NANB20D134, HR00112090132, ONR grant 13370299 and AWS Machine Learning Research Award.\\n\\nReferences\\n\\nAndrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in Neural Information Processing Systems, 32:9453\u20139463, 2019.\\n\\nSara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European conference on computer vision (ECCV), pp. 456\u2013473, 2018.\\n\\nSean Bell, C. Lawrence Zitnick, Kavita Bala, and Ross Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\\n\\nMyung Jin Choi, Antonio Torralba, and Alan S. Willsky. Context models and out-of-context objects. Pattern Recogn. Lett., 33(7):853\u2013862, may 2012. ISSN 0167-8655. doi: 10.1016/j.patrec.2011.12.004. URL https://doi.org/10.1016/j.patrec.2011.12.004.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255. Ieee, 2009.\\n\\nSantosh K. Divvala, Derek Hoiem, James H. Hays, Alexei A. Efros, and Martial Hebert. An empirical study of context in object detection. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1271\u20131278, 2009. doi: 10.1109/CVPR.2009.5206532.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.\\n\\nCarolina Galleguillos, Andrew Rabinovich, and Serge Belongie. Object categorization using co-occurrence, location and appearance. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1\u20138, 2008. doi: 10.1109/CVPR.2008.4587799.\"}"}
{"id": "kattakinda22a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. *Nature Machine Intelligence*, 2(11):665\u2013673, 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 770\u2013778, 2016.\\n\\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. *Proceedings of the International Conference on Learning Representations*, 2019.\\n\\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadvath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. *ICCV*, 2021a.\\n\\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 15262\u201315271, June 2021b.\\n\\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 1314\u20131324, 2019.\\n\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In *Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1*, NIPS'12, pp. 1097\u20131105, Red Hook, NY, USA, 2012. Curran Associates Inc.\\n\\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset. [online: http://www.cs.toronto.edu/kriz/cifar.html], 55(5), 2014.\\n\\nGuillaume Leclerc, Hadi Salman, Andrew Ilyas, Sai Vemprala, Logan Engstrom, Vibhav Vineet, Kai Xiao, Pengchuan Zhang, Shibani Santurkar, Greg Yang, Ashish Kapoor, and Aleksander Madry. 3db: A framework for debugging computer vision models. In *Arxiv preprint arXiv:2106.03805*, 2021.\\n\\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In *Proceedings of International Conference on Computer Vision (ICCV)*, December 2015.\\n\\nRoozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2014.\\n\\nAude Oliva and Antonio Torralba. The role of context in object recognition. *Trends in cognitive sciences*, 11(12):520\u2013527, 2007.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), *Advances in Neural Information Processing Systems 32*, pp. 8024\u20138035. Curran Associates, Inc., 2019.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. *arXiv preprint arXiv:2103.00020*, 2021.\\n\\nAmir Rosenfeld, Richard Zemel, and John K Tsotsos. The elephant in the room. *arXiv preprint arXiv:1808.03305*, 2018.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. *International Journal of Computer Vision*, 115:211\u2013252, 2015.\\n\\nShiori Sagawa*, Pang Wei Koh*, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks. In *International Conference on Learning Representations*, 2020. URL https://openreview.net/forum?id=ryxGuJrFvS.\\n\\nChandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with Gram matrices. In Hal Daum\u00e9 III and Aarti Singh (eds.), *Proceedings of the 37th International Conference on Machine Learning*, volume 119 of *Proceedings of Machine Learning Research*, pp. 8491\u20138501. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/sastry20a.html.\"}"}
{"id": "kattakinda22a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. BDD100K: A diverse driving dataset for heterogeneous multitask learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2636\u20132645, 2020.\\n\\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\"}"}
{"id": "kattakinda22a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The images for our dataset were collected using queries formed as a concatenation of an object label and one of the phrases from below:\\n\\n**Attribute Phrases used in queries**\\n- raining\\n- in rain\\n- foggy\\n- in fog\\n- snow\\n- on snow\\n- sand\\n- in a desert, on sand\\n- forest\\n- in forest\\n- water\\n- on water\\n- night\\n- at night\\n- grass\\n- on grass\\n- street\\n- on a street, on a road\\n\\nIn addition, we use some class specific queries:\\n- ship on ice\\n- ship on a dock\\n- dog on a couch\\n- dog on a bed\\n- dog on the floor\\n- cat on a couch\\n- cat on a bed\\n- cat on the floor\\n- horse in a stable\\n- car in a garage\\n- truck in a garage\\n- plane in a hangar.\\n\\nFigure 6 shows some more uncommon images in FOCUS.\"}"}
{"id": "kattakinda22a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\n(a) bird at night\\n(b) car in forest\\n(c) cat at night in forest\\n(d) plane at night indoors\\n(e) plane in water\\n(f) deer in fog\\n(g) car on sand\\n(h) dog in snow\\n(i) horse in snow\\n(j) car in water\\n(k) horse in water\\n(l) ship in fog\\n(m) bird indoors\\n(n) cat in snowy weather & water\\n(o) car in snow\\n\\nFigure 6: Some uncommon images in the FOCUS dataset.\"}"}
{"id": "kattakinda22a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Human Intelligence Tasks (HITs)\\n\\nTo gather high quality annotations, we first vet the workers through a qualification process; workers are shown a series of 10 images (in the UI shown in figure 7) from our dataset for which the ground truth is known (these images were annotated by us manually). We qualify workers who have done well on this qualification test. Worker's annotations were checked manually in this stage instead of using a strict threshold as there is an element of subjectivity to the annotations. In the second stage, our HITs have 25 images each; 23 of which are unannotated and 2 are from the subset that were annotated by us. We use these 2 images as a way to track workers' annotation accuracy. Each image is annotated by two workers and we pick annotations of the worker who has the higher annotation accuracy on the 2 \\\"check\\\" images in that HIT. Workers received a base pay of $0.67 per HIT (25 images) which takes an average of around 5 minutes to annotate. A bonus of $30 was paid for completion of 100 HITs. This payment structure has created an incentive for workers to annotate a large number of images.\"}"}
{"id": "kattakinda22a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\nFigure 7: Our UI for annotating images in FOCUS.\"}"}
{"id": "kattakinda22a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\nC. Accuracy as a function of class and attributes\\n\\nThis section shows the accuracy of different models for each class and attribute. Uncommon attributes are highlighted in shades of orange, while common attributes are highlighted in shades of blue. The last row (except the bottom right-most value) shows the overall accuracy for the corresponding attribute. Additionally, the last column is the generalization gap with respect to the corresponding attribute. The first 10 values in the last column are class specific generalization gaps, while the last (i.e., bottom rightmost) value is the aggregate generalization gap defined in Equation 1 (and reported in table 4).\\n\\nAn image can have a combination of common and uncom-\\nmon attributes (e.g., \u201ccat on street at night\u201d \u2014 uncommon time but common location). Such images may appear in a \u201cblue\u201d cell in one of the tables while in a different table they may appear in \u201corange\u201d. This explains why for some classes the models seem to do better at \u201cnight\u201d than in day.\\n\\nNote that majority of the uncommon weather conditions: (\u201craining\u201d, \u201csnowing\u201d, \u201cfoggy\u201d) occur during the day and they are potentially decreasing the classification accuracy so much that it outweighs the drop due to \u201cnight\u201d and leads to this apparently paradoxical result.\\n\\n| Class     | Attribute | Accuracy | Generalization Gap |\\n|-----------|-----------|----------|-------------------|\\n| Class 1   | Attribute 1 | 0.85     | 0.05              |\\n| Class 2   | Attribute 2 | 0.90     | 0.02              |\\n| Class 3   | Attribute 3 | 0.80     | 0.03              |\\n| Class 4   | Attribute 4 | 0.88     | 0.04              |\\n| Class 5   | Attribute 5 | 0.92     | 0.01              |\\n| Class 6   | Attribute 6 | 0.86     | 0.06              |\\n| Class 7   | Attribute 7 | 0.91     | 0.03              |\\n| Class 8   | Attribute 8 | 0.89     | 0.04              |\\n| Class 9   | Attribute 9 | 0.93     | 0.02              |\\n| Class 10  | Attribute 10| 0.90     | 0.03              |\\n| Aggregate |           | 0.88     | 0.05              |\"}"}
{"id": "kattakinda22a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\n![Table](image)\\n\\nFigure 8: Accuracy of ResNet50 for all combinations of classes and attributes.\"}"}
{"id": "kattakinda22a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\n(a) Category vs Time of Day\\n\\n- Total: 0.59, 0.46, 0.58, 0.53, 0.74, 0.87\\n\\n(b) Category vs Weather\\n\\n- Total: 0.62, 0.65, 0.59, 0.48, 0.52, 0.45\\n\\n(c) Category vs Location\\n\\n- Total: 0.51, 0.62, 0.45, 0.72, 0.42, 0.47\\n\\nFigure 9: Accuracy of Wide-ResNet50-2 for all combinations of classes and attributes.\"}"}
{"id": "kattakinda22a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Familiar Objects in Common and Uncommon Settings\\n\\n#### (a) Category vs Time of Day\\n\\n| Category | Day | Night |\\n|----------|-----|-------|\\n| truck    | 0.57| 0.48  |\\n| car      | 0.60| 0.55  |\\n| plane    | 0.55| 0.60  |\\n| ship     | 0.70| 0.87  |\\n| cat      | 0.86| 0.93  |\\n| dog      | 0.93| 0.65  |\\n| horse    | 0.65| 0.52  |\\n| deer     | 0.65| 0.71  |\\n| frog     | 0.67| 0.88  |\\n| bird     | 0.16| 0.87  |\\n\\n#### (b) Category vs Weather\\n\\n| Category | Cloudy | Foggy | Partly Cloudy | Raining | Snowing | Sunny |\\n|----------|--------|-------|---------------|---------|---------|-------|\\n| truck    | 0.58   | 0.46  | 0.57          | 0.63    | 0.53    | 0.48  |\\n| car      | 0.57   | 0.53  | 0.48          | 0.62    | 0.49    | 0.57  |\\n| plane    | 0.61   | 0.60  | 0.62          | 0.56    | 0.50    | 0.57  |\\n| ship     | 0.56   | 0.50  | 0.59          | 0.51    | 0.52    | 0.65  |\\n| cat      | 0.47   | 0.64  | 0.67          | 0.34    | 0.38    | 0.44  |\\n| dog      | 0.64   | 0.77  | 0.90          | 0.92    | 0.92    | 0.92  |\\n| horse    | 0.77   | 0.90  | 0.92          | 0.87    | 0.84    | 0.84  |\\n| deer     | 0.87   | 0.83  | 0.84          | 0.87    | 0.83    | 0.93  |\\n| frog     | 0.84   | 0.84  | 0.84          | 0.87    | 0.83    | 0.93  |\\n| bird     | 0.93   | 0.77  | 0.90          | 0.92    | 0.92    | 0.92  |\\n\\n#### (c) Category vs Location\\n\\n| Category | Forest | Grass | Indoors | Rocks | Sand | Street | Snow | Water |\\n|----------|--------|-------|---------|-------|------|--------|------|-------|\\n| truck    | 0.52   | 0.59  | 0.42    | 0.71  | 0.46 | 0.52   | 0.54 | 0.49  |\\n| car      | 0.59   | 0.52  | 0.54    | 0.49  | 0.52 | 0.54   | 0.54 | 0.49  |\\n| plane    | 0.56   | 0.70  | 0.56    | 0.70  | 0.78 | 0.62   | 0.78 | 0.62  |\\n| ship     | 0.62   | 0.89  | 0.85    | 0.93  | 0.85 | 0.84   | 0.87 | 0.84  |\\n| cat      | 0.44   | 0.46  | 0.36    | 0.45  | 0.55 | 0.55   | 0.55 | 0.45  |\\n| dog      | 0.45   | 0.55  | 0.55    | 0.46  | 0.54 | 0.54   | 0.54 | 0.46  |\\n| horse    | 0.45   | 0.30  | 0.33    | 0.43  | 0.27 | 0.57   | 0.52 | 0.52  |\\n| deer     | 0.44   | 0.30  | 0.33    | 0.43  | 0.27 | 0.57   | 0.52 | 0.52  |\\n| frog     | 0.47   | 0.64  | 0.66    | 0.55  | 0.42 | 0.64   | 0.64 | 0.55  |\\n| bird     | 0.71   | 0.80  | 0.76    | 0.85  | 0.85 | 0.93   | 0.93 | 0.93  |\\n\\nFigure 10: Accuracy of EfficientNet-b4 for all combinations of classes and attributes.\"}"}
{"id": "kattakinda22a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Familiar Objects in Common and Uncommon Settings\\n\\n| Category   | Daytime Accuracy | Nighttime Accuracy |\\n|------------|------------------|--------------------|\\n| truck      | 0.54             | 0.47               |\\n| car        | 0.60             | 0.54               |\\n| plane      | 0.72             | 0.86               |\\n| ship       | 0.41             | 0.53               |\\n| cat        | 0.82             | 0.91               |\\n| dog        | 0.48             | 0.50               |\\n| horse      | 0.60             | 0.64               |\\n| deer       | 0.83             | 0.20               |\\n| frog       | 0.66             | 0.60               |\\n\\n### Category vs Time of Day\\n\\n| Weather      | Daytime Accuracy | Nighttime Accuracy |\\n|--------------|------------------|--------------------|\\n| cloudy       | 0.60             | 0.57               |\\n| partly cloudy| 0.53             | 0.42               |\\n| raining      | 0.59             | 0.59               |\\n| snowing      | 0.53             | 0.52               |\\n| sunny        | 0.77             | 0.86               |\\n\\n### Category vs Location\\n\\n| Location     | Daytime Accuracy | Nighttime Accuracy |\\n|--------------|------------------|--------------------|\\n| forest       | 0.53             | 0.56               |\\n| grass        | 0.41             | 0.67               |\\n| indoors      | 0.45             | 0.45               |\\n| rocks        | 0.79             | 0.81               |\\n| sand         | 0.82             | 0.85               |\\n| street       | 0.72             | 0.72               |\\n| snow         | 0.85             | 0.86               |\\n| water        | 0.92             | 0.92               |\\n\\nFigure 11: Accuracy of EfficientNet-b7 for all combinations of classes and attributes.\"}"}
{"id": "kattakinda22a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\nTable 2: Uncommon locations (highlighted in orange) in FOCUS are OOD with respect to ImageNet. Higher percentages (see text for details) indicate that the corresponding images are out-of-distribution.\\n\\n| Location       | Percentages |\\n|----------------|-------------|\\n| Forest         | 70.9        |\\n| Grass Indoors  | 71.3        |\\n| Rocks          | 70.7        |\\n| Sand           | 78.8        |\\n| Street         | 70.7        |\\n| Snow           | 66.4        |\\n| Water          | 73.8        |\\n| Truck          | 71.4        |\\n| Car            | 69.4        |\\n| Plane          | 88.2        |\\n| Ship           | 62.7        |\\n| Cat            | 78.7        |\\n| Dog            | 28.8        |\\n| Horse          | 86.9        |\\n| Deer           | 81.1        |\\n| Frog           | 76.2        |\\n| Bird           | 30.6        |\\n\\nWe acknowledge that this is by no means the one true way. We facilitate other studies opting to do it in other ways by providing all annotations for the collected samples in our dataset, FOCUS.\\n\\n3.4. Why FOCUS?\\n\\nConsider a dataset for image classification, that is constructed by collecting samples for each object class without controlling for the environment the object is in. If the images are collected from the internet, using a search engine, then the distribution of images in the dataset reflects that of the search results. Barring any bias in the search engine, it is reasonable to expect that the images are an accurate representation of the real world. This means that for a given object, most, if not all, images will depict the object in an environment that is typical or natural for the object to be in (e.g., \\\"ship in water\\\"). Furthermore, unless the number of samples is exceptionally large, the dataset may not contain some objects in some specific environments (e.g., \\\"plane in water\\\").\\n\\nA deep learning model trained on such a dataset will exploit any correlation between objects and their environments in the images since context is a shortcut for identifying the object, especially when the correlation is strong. This can be troublesome when the object of interest is an atypical environment; a model that relies too much on context may fail to identify the object in the absence of context cues it has seen during training. This does not mean that all usage of context is \\\"spurious\\\" and undoubtedly, even humans depend on various types of context (namely semantic, spatial, pose, etc.,) (Oliva & Torralba, 2007), especially when the object is occluded or lacks discernible details. However, our argument is that humans' use of context is nuanced; we do not look at the surroundings when we can tell what an object is simply by looking at the object alone. On the contrary, today's deep learning models are indiscriminate in their use of context and therein lies the problem.\\n\\nLastly, we believe that FOCUS complements existing datasets such as ObjectNet (Barbu et al., 2019) by controlling for a different set of contexts, namely, illumination, weather, and location. In addition, in order to exercise a rich variation in the control variables, ObjectNet only includes objects that are easily manipulated by humans. However, our procedure for building FOCUS (Section 3.1) can be used with any object class.\\n\\n4. Evaluating Deep Models on FOCUS\\n\\nIt is crucial to ensure that machine learning models are reliable even in rare and uncommon settings especially when they are deployed in safety-critical applications. As an example, consider a self-driving car that infers various attributes about its surroundings using deep learning models. The deployed models may be accurate in 99.9% of cases that occur in common settings (e.g., pedestrian crossing on a sunny day). However, given the vast complexity of the real world, uncommon and corner cases, although rare, are still possible (e.g., a heavily snow covered car cutting in). If a model is unreliable in those uncommon settings, it could make a grave error resulting in loss of life and/or property.\\n\\nIn spirit of the aforementioned, we stress test the generalization power of various deep learning models to uncommon settings using the FOCUS dataset. Specifically, we are considering models that are trained using images close to the mode of the (object, environment) distribution (i.e., common images) and evaluating them on images that fall more on the tail of the (object, environment) distribution (i.e., uncommon images).\\n\\n4.1. Experimental Setup\\n\\nModel architectures.\\n\\nWe select some of the most popular deep learning models that have high test accuracy on ImageNet, namely ResNet50 (He et al., 2016), WideResNet50-2 (Zagoruyko & Komodakis, 2016), MobileNet-v3-large (Howard et al., 2019), EfficientNet-b4 (Tan & Le, 2019), EfficientNet-b7 (Tan & Le, 2019), CLIP (Radford et al., 2021), ViT-B/16 (Dosovitskiy et al., 2021), and ResNeXt-50 (32x4d) (Xie et al., 2017). For the first three models, we use the pretrained weights provided by PyTorch (Paszke et al., 2019). We obtain the weights for both variations of EfficientNet from https://github.com/lukemelas/EfficientNet-PyTorch. We use the authors' implementation for CLIP (from https://github.com/openai/CLIP). Following Radford et al. (2021), the text inputs to CLIP are of the form \\\"a photo of <class>\\\", one for each of the 1000 classes in...\"}"}
{"id": "kattakinda22a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\nResNet50  Wide-ResNet50-2  MobileNet-v3-large  EfficientNet-b4  EfficientNet-b7  CLIP  ViT-B/16  ResNeXt-50 (32x4d)\\n\\nFigure 2: Top-1 classification accuracy for different models on the FOCUS dataset as a function of the partitions.\\n\\nEvaluation metrics. Since the models we evaluate are pretrained on ImageNet, they output 1,000 probabilities (recall, ImageNet has 1,000 classes). On the other hand, our dataset has only 10 object categories. We resolve this apparent mismatch by first constructing a mapping (denoted by $M$) between the 1000 labels in ImageNet and those in our dataset. Concretely, a label $l_I$ in ImageNet is assigned to a label $l_F$ in our dataset if $l_F$ is a semantic superclass of $l_I$. For example, all the different dog breeds in ImageNet are mapped to the dog label in FOCUS. Decidedly, some labels in ImageNet do not have any corresponding labels in our dataset (e.g., \\\"analog clock\\\", \\\"carton\\\" etc.). As our dataset has no images from these labels, we declare a misclassification whenever the network predicts a label that is not in the domain of $M$. We say a prediction is correct if the ImageNet label with the highest logit was assigned to the ground truth label in FOCUS. That is, for a sample $(x_i, y_i, t_i, w_i, l_i)$ from the FOCUS dataset, let $g(x_i)$ be the ImageNet label predicted by a trained network. Then, we have:\\n\\n$$\\\\text{correct prediction} \\\\iff f(x_i) := M(g(x_i)) = y_i.$$  \\n\\nTo facilitate the evaluation of the effect of uncommon attributes, we first partition the dataset based on the number of uncommon attributes in images: $P_i$ is a subset of FOCUS samples with $i$ uncommon attributes for $i = 0, 1, 2, 3$. Note that $P_0$ denote common samples while $\\\\bigcup_{i \\\\geq 1} P_i$ denotes uncommon samples that have at least one uncommon attribute. We further subdivide $P_i$ into $P_{A_i}$, $A_i \\\\subseteq \\\\{t, w, l\\\\}$, $|A_i| = i$ where the attributes in $A_i$ are uncommon (for instance, $P_{(w,l)}$ is the set of all images with two uncommon attributes: weather and location). Table 3 shows the sizes of the different partitions in our dataset.\\n\\nWe then evaluate the classification accuracy of different models in different partitions (referred to as $\\\\text{Acc}(P_i)$). In an attempt to measure the effect of a single attribute on the accuracy of a model $f$, we define the following generalization gap with respect to an attribute $a$:\\n\\n$$G_a = |\\\\{x_i \\\\in C(a) | f(x_i) = y_i\\\\}| - |\\\\{x_i \\\\in UC(a) | f(x_i) = y_i\\\\}|,$$\\n\\nwhere $C(a)$ and $UC(a)$ are the subsets of images in which attribute $a$ is common and uncommon, respectively. Succinctly, $G_a$ is the difference in the classification accuracy between images with a common choice for $a$ and those with an uncommon choice for the same. The larger the $G_a$, the worse the generalization performance of the model on uncommon choices for $a$.\\n\\n4.2. Results\\n\\nFigure 2 shows the accuracy of different model architectures on different partitions of the FOCUS dataset. We observe that for all the models, the accuracy falls as the number of uncommon attributes increases. Note that both the EfficientNet models have the highest accuracy in all three subsets, with EfficientNet-b7 performing the best among all the models. We postulate that this is because these models have larger input size: 380 for EfficientNet-b4 and 600 for EfficientNet-b7 (see Tan & Le (2019) for details). However, CLIP has the lowest (best) generalization gap between common and uncommon choices for $a$.\\n\\nWe have not included $P_3$ in this analysis as it has very few samples (only 35).\"}"}
{"id": "kattakinda22a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 3: Sizes of different partitions in FOCUS.\\n\\n| Partition | Size |\\n|-----------|------|\\n| $P_0$     | 14,144 |\\n| $P_1$     | 5,974  |\\n| $P_2$     | 674    |\\n| $P_3$     | 21     |\\n\\n- $P_i$ is the set of images with uncommon attributes and $P_A$, $A \\\\subseteq \\\\{t, w, l\\\\}$ is the set of images where the attributes in $A$ are uncommon. Note that $P_0$ constitutes common images.\\n\\n### Table 4: Generalization gap (as in Equation 1) per attribute for various models. The best gap on each attribute is in boldface.\\n\\n| Model               | $G_t$ | $G_w$ | $G_l$ |\\n|---------------------|-------|-------|-------|\\n| ResNet50            | 9%    | 23%   | 20%   |\\n| Wide-ResNet50-2     | 11%   | 22%   | 19%   |\\n| MobileNet-v3-large  | 12%   | 20%   | 19%   |\\n| EfficientNet-b4     | 7%    | 19%   | 18%   |\\n| EfficientNet-b7     | 11%   | 18%   | 16%   |\\n| CLIP                | 0%    | 17%   | 15%   |\\n| ViT-B/16            | 11%   | 17%   | 19%   |\\n| ResNeXt-50 (32x4d)  | 8%    | 18%   | 18%   |\\n\\nThe mon and uncommon images (i.e., $A(P_0) - A(\\\\bigcup_i \\\\geq 1 P_i)$) at 21.6% and ResNet has the highest gap (i.e., the worst generalization) at 29.8%.\\n\\n### Table 4 shows the generalization gap (as in Equation 1) per attribute for various models. We see that all the gaps are positive, clearly indicating poor generalization ability to uncommon settings. Note that $G_t$ is smaller than both $G_w$ and $G_l$ for all the models. So, these models are not hurt as much by uncommon time (i.e., \u201cnight\u201d) as they are by uncommon weather or location. Additionally, we see that CLIP has the best generalization in uncommon time of day, weather (tied with ViT-B/16), and location.\\n\\n#### 4.3. Finetuning\\n\\nIn an attempt to improve classification accuracy in uncommon settings, we finetune the classifiers (except CLIP) we previously tested in subsection 4.2, on the FOCUS dataset. We do not fine-tune CLIP on FOCUS because it is a zero-shot classifier and unlike the other models, it does not have a final fully connected layer that can be fine-tuned. We start by randomly splitting the dataset into train and test sets, which are 70% and 30% of the dataset in size, respectively. We use SGD with a learning rate of 1e-4 to update the last layer (fully-connected layer) of each model for 10 epochs of the train split. Figure 3 shows the gains in top-1 classification accuracy for each of the model on different partitions of the test set. All models perform better on $P_1$ and $P_2$ after $P_0$.\\n\\n![Figure 3](image)\\n\\nFigure 3: Gains in accuracy of classification of objects on uncommon settings after finetuning on FOCUS.\\n\\nFinetuning, with ViT-B/16 having a substantial gain of 40% and 56% respectively. This shows that finetuning on FOCUS improves the generalization of classification models and allows them to identify objects in atypical contexts.\\n\\nFigure 4 provides some insight into the effects of finetuning a model on FOCUS. The figure compares the localization maps of a base ResNet50 model that was pretrained on imagenet with those of the finetuned model. Each row illustrates an image from the test split of FOCUS that is misclassified by the base model, but is classified correctly by the finetuned model. It also includes the Grad-CAM (Selvaraju et al., 2017) localization maps of both the models on that image. In each case, it is evident that the base model is attending heavily to the wrong part of the image. As a result, it incorrectly predicts a class that is closely associated with the background: (a) In figure 4a, the model is relying on the presence of water to predict \u2018seashore\u2019 even though it\u2019s in the middle of an ocean (b) In figure 4b, bowl and the handles on the drawer cause the model to predict a dish-washer (c) Lastly, in figure 4c, the model only looks at the wet, brown fur in water to predict a \u2018brown bear\u2019. On the other hand, the finetuned model focuses more accurately on the correct object alone, allowing it to identify these objects...\"}"}
{"id": "kattakinda22a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FOCUS: Familiar Objects in Common and Uncommon Settings\\n\\nInput Image Localization before finetuning Localization after finetuning\\n\\n(a) Plane on Water. Base model attends to the water and predicts 'seashore', while the Finetuned model correctly predicts 'warplane'.\\n\\n(b) Dog Indoors. Base model predicts 'dishwasher'. Finetuned model, which localizes on the dog correctly, predicts 'Labrador Retriever'.\\n\\n(c) Cat in Water. Base model incorrectly predicts 'brown bear'. Note the lack of attention on the cat's head. Finetuned model has a more accurate localization map and predicts 'tabby cat'.\\n\\nFigure 4: Comparison of localization maps of a base ResNet50 model (pretrained on ImageNet) before and after finetuning on FOCUS. In each sub-figure, the first image is the input uncommon image. The second and third images, respectively, show the Grad-CAM (Selvaraju et al., 2017) and the Guided Grad-CAM of the model before finetuning. Finally, the fourth and fifth images show the same for the model after finetuning. All Grad-CAM and Guided Grad-CAM images use the predicted class as their target. After finetuning on FOCUS, the model learns to focus more precisely on the object of interest which leads to substantial increase in classification accuracy (see table 3).\"}"}
{"id": "kattakinda22a", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### (a) Category vs Time of Day\\n\\n|       | day  | night |\\n|-------|------|-------|\\n| truck | 0.60 | 0.41  |\\n| car   | 0.52 | 0.63  |\\n| plane | 0.63 | 0.60  |\\n| ship  | 0.87 | 0.00  |\\n| cat   | 0.82 | 0.91  |\\n| dog   | 0.88 | 0.74  |\\n| horse | 0.74 | 0.53  |\\n| deer  | 0.61 | 0.61  |\\n| frog  | 0.84 | 0.00  |\\n| bird  | 0.00 | 0.44  |\\n\\n### (b) Category vs Weather\\n\\n|       | cloudy | foggy | partly cloudy | raining | snowing | sunny |\\n|-------|--------|-------|---------------|---------|---------|-------|\\n| truck | 0.71   | 0.62  | 0.62          | 0.38    | 0.45    | 0.42  |\\n| car   | 0.49   | 0.50  | 0.55          | 0.67    | 0.61    | 0.67  |\\n| plane | 0.42   | 0.49  | 0.61          | 0.83    | 0.84    | 0.90  |\\n| ship  | 0.44   | 0.42  | 0.50          | 0.71    | 0.73    | 0.81  |\\n| cat   | 0.71   | 0.73  | 0.81          | 0.88    | 0.87    | 0.88  |\\n| dog   | 0.88   | 0.87  | 0.88          | 0.88    | 0.87    | 0.88  |\\n| horse | 0.88   | 0.87  | 0.88          | 0.88    | 0.87    | 0.88  |\\n| deer  | 0.84   | 0.87  | 0.88          | 0.88    | 0.87    | 0.88  |\\n| frog  | 0.58   | 0.53  | 0.58          | 0.58    | 0.53    | 0.58  |\\n| bird  | 0.58   | 0.53  | 0.58          | 0.58    | 0.53    | 0.58  |\\n\\n### (c) Category vs Location\\n\\n|       | forest | grass | indoors | rocks | sand | street | snow | water |\\n|-------|--------|-------|---------|-------|------|--------|------|-------|\\n| truck | 0.56   | 0.63  | 0.42    | 0.66  | 0.34 | 0.43   | 0.47 | 0.42  |\\n| car   | 0.55   | 0.42  | 0.26    | 0.39  | 0.53 | 0.43   | 0.17 | 0.58  |\\n| plane | 0.34   | 0.66  | 0.42    | 0.41  | 0.36 | 0.27   | 0.58 | 0.51  |\\n| ship  | 0.58   | 0.51  | 0.20    | 0.45  | 0.39 | 0.68   | 0.66 | 0.66  |\\n| cat   | 0.37   | 0.42  | 0.44    | 0.50  | 0.51 | 0.75   | 0.79 | 0.73  |\\n| dog   | 0.75   | 0.79  | 0.73    | 0.37  | 0.42 | 0.44   | 0.50 | 0.51  |\\n| horse | 0.72   | 0.79  | 0.57    | 0.37  | 0.42 | 0.44   | 0.50 | 0.51  |\\n| deer  | 0.75   | 0.83  | 0.92    | 0.72  | 0.79 | 0.57   | 0.37 | 0.42  |\\n| frog  | 0.72   | 0.79  | 0.57    | 0.37  | 0.42 | 0.44   | 0.50 | 0.51  |\\n| bird  | 0.75   | 0.83  | 0.92    | 0.72  | 0.79 | 0.57   | 0.37 | 0.42  |\\n\\nFigure 12: Accuracy of CLIP for all combinations of classes and attributes.\"}"}
{"id": "kattakinda22a", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Familiar Objects in Common and Uncommon Settings\\n\\n#### (a) Category vs Time of Day\\n\\n| Category | Day | Night |\\n|----------|-----|-------|\\n| truck    | 0.70| 0.48  |\\n| car      | 0.65| 0.48  |\\n| plane    | 0.59| 0.57  |\\n| ship     | 0.57| 0.57  |\\n| cat      | 0.79| 0.85  |\\n| dog      | 0.85| 0.85  |\\n| horse    | 0.50| 0.50  |\\n| deer     | 0.58| 0.58  |\\n| frog     | 0.92| 0.92  |\\n| bird     | 0.70| 0.70  |\\n\\n#### (b) Category vs Weather\\n\\n| Category | Cloudy | Foggy | Partly Cloudy | Raining | Snowing | Sunny |\\n|----------|--------|-------|---------------|---------|---------|-------|\\n| truck    | 0.71   | 0.65  | 0.69          | 0.46    | 0.50    | 0.49  |\\n| car      | 0.60   | 0.62  | 0.62          | 0.62    | 0.62    | 0.62  |\\n| plane    | 0.61   | 0.61  | 0.62          | 0.62    | 0.62    | 0.62  |\\n| ship     | 0.64   | 0.73  | 0.79          | 0.82    | 0.79    | 0.87  |\\n| cat      | 0.64   | 0.73  | 0.79          | 0.82    | 0.79    | 0.87  |\\n| dog      | 0.82   | 0.79  | 0.87          | 0.44    | 0.44    | 0.54  |\\n| horse    | 0.51   | 0.56  | 0.65          | 0.51    | 0.56    | 0.65  |\\n| deer     | 0.56   | 0.71  | 0.33          | 0.53    | 0.71    | 0.33  |\\n| frog     | 0.45   | 0.21  | 0.53          | 0.45    | 0.21    | 0.53  |\\n| bird     | 1.00   | 0.14  | 1.00          | 1.00    | 0.14    | 1.00  |\\n\\n#### (c) Category vs Location\\n\\n| Category    | Forest | Grass | Indoors | Rocks | Sand | Street | Snow | Water |\\n|-------------|--------|-------|---------|-------|------|--------|------|-------|\\n| truck       | 0.60   | 0.70  | 0.52    | 0.70  | 0.47 | 0.50   | 0.48 | 0.48  |\\n| car         | 0.47   | 0.50  | 0.48    | 0.48  | 0.52 | 0.63   | 0.72 | 0.85  |\\n| plane       | 0.53   | 0.57  | 0.41    | 0.52  | 0.63 | 0.74   | 0.85 | 0.87  |\\n| ship        | 0.72   | 0.85  | 0.87    | 0.82  | 0.88 | 0.88   | 0.88 | 0.89  |\\n| cat         | 0.91   | 0.92  | 0.86    | 0.82  | 0.84 | 0.82   | 0.84 | 0.84  |\\n| dog         | 0.45   | 0.60  | 0.34    | 0.43  | 0.44 | 0.29   | 0.31 | 0.31  |\\n| horse       | 0.40   | 0.67  | 0.47    | 0.49  | 0.47 | 0.28   | 0.30 | 0.30  |\\n| deer        | 0.42   | 0.36  | 0.40    | 0.41  | 0.25 | 0.51   | 0.56 | 0.56  |\\n| frog        | 0.63   | 0.71  | 0.63    | 0.61  | 0.49 | 0.67   | 0.74 | 0.94  |\\n| bird        | 0.67   | 0.74  | 0.94    | 0.67  | 0.74 | 0.94   | 0.67 | 0.74  |\\n\\nFigure 13: Accuracy of ViT-B/16 for all combinations of classes and attributes.\"}"}
{"id": "kattakinda22a", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Familiar Objects in Common and Uncommon Settings\\n\\n#### (a) Category vs Time of Day\\n\\n| Object | Day | Night |\\n|--------|-----|-------|\\n| Truck  | 0.62 | 0.47  |\\n| Car    | 0.62 | 0.53  |\\n| Plane  | 0.53 | 0.79  |\\n| Ship   | 0.89 | 0.39  |\\n| Cat    | 0.89 | 0.49  |\\n| Dog    | 0.90 | 0.96  |\\n| Horse  | 0.95 | 0.70  |\\n| Deer   | 0.70 | 0.50  |\\n| Frog   | 0.69 | 0.64  |\\n| Bird   | 0.64 | 0.39  |\\n\\n#### (b) Category vs Weather\\n\\n| Object       | Cloudy | Foggy | Partly Cloudy | Raining | Snowing | Sunny |\\n|--------------|--------|-------|---------------|---------|---------|-------|\\n| Truck        | 0.71   | 0.63  | 0.64          | 0.45    | 0.68    | 0.64  |\\n| Car          | 0.63   | 0.62  | 0.64          | 0.62    | 0.64    | 0.64  |\\n| Plane        | 0.55   | 0.48  | 0.56          | 0.48    | 0.56    | 0.56  |\\n| Ship         | 0.66   | 0.74  | 0.81          | 0.83    | 0.90    | 0.90  |\\n| Cat          | 0.87   | 0.83  | 0.90          | 0.83    | 0.90    | 0.90  |\\n| Dog          | 0.33   | 0.32  | 0.44          | 0.32    | 0.44    | 0.44  |\\n| Horse        | 0.35   | 0.46  | 0.55          | 0.46    | 0.55    | 0.55  |\\n| Deer         | 0.30   | 0.14  | 0.53          | 0.14    | 0.53    | 0.53  |\\n| Frog         | 0.37   | 1.00  | 0.18          | 1.00    | 0.18    | 1.00  |\\n| Bird         | 0.30   | 0.14  | 0.53          | 0.14    | 0.53    | 0.53  |\\n\\n#### (c) Category vs Location\\n\\n| Object       | Forest  | Grass   | Indoors | Rocks  | Sand   | Street | Snow   | Water  |\\n|--------------|---------|---------|---------|--------|--------|--------|--------|--------|\\n| Truck        | 0.59    | 0.67    | 0.45    | 0.68   | 0.42   | 0.48   | 0.54   | 0.80   |\\n| Car          | 0.42    | 0.48    | 0.52    | 0.44   | 0.53   | 0.49   | 0.54   | 0.80   |\\n| Plane        | 0.53    | 0.69    | 0.53    | 0.49   | 0.33   | 0.44   | 0.36   | 0.67   |\\n| Ship         | 0.67    | 0.53    | 0.53    | 0.49   | 0.33   | 0.44   | 0.36   | 0.67   |\\n| Cat          | 0.54    | 0.80    | 0.78    | 0.70   | 0.88   | 0.78   | 0.70   | 0.88   |\\n| Dog          | 0.88    | 0.93    | 0.90    | 0.84   | 0.92   | 0.93   | 0.90   | 0.84   |\\n| Horse        | 0.38    | 0.42    | 0.29    | 0.43   | 0.53   | 0.49   | 0.33   | 0.44   |\\n| Deer         | 0.43    | 0.29    | 0.16    | 0.33   | 0.26   | 0.48   | 0.25   | 0.48   |\\n| Frog         | 0.62    | 0.69    | 0.57    | 0.62   | 0.69   | 0.57   | 0.62   | 0.69   |\\n| Bird         | 0.62    | 0.69    | 0.57    | 0.62   | 0.69   | 0.57   | 0.62   | 0.69   |\\n\\n---\\n\\nFigure 14: Accuracy of ResNext-50 (32x4d) for all combinations of classes and attributes.\"}"}
