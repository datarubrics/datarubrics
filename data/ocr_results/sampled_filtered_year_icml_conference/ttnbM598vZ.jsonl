{"id": "ttnbM598vZ", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\nthat node features are derived from particle labels (either OC or LC), the feature distribution remains largely unchanged despite variations in the OC to LC ratio. Consequently, feature shifts are minimal under PU conditions. Consequently, the baselines like StruRW and SpecReg show some benefits over others in regularizing and adjusting graph structure to handle structure shift. Specifically, SpecReg shows enhanced benefits during the transition from low PU to high PU, possibly due to its regularization of spectral smoothness, which mitigates edge perturbations beneficially under CSS conditions. Despite these improvements in the pileup dataset, SpecReg does not perform as well in other datasets characterized by CSS, such as MAG. This may be attributed to the fact that spectral regularization is more effective in scenarios with a limited variety of node connections, akin to the binary cases in the pileup dataset. However, it appears less capable of managing more complex shifts in neighborhood distribution involving multiple classes, as seen in datasets like MAG or Arxiv.\\n\\nConversely, StruRW achieves comparable performances to PA-BOTH in scenarios transitioning from high PU to low PU, predominantly influenced by LS. This effectiveness is likely due to the fact that their edge weights incorporate $w$, which includes $\\\\alpha$ that implicitly contains the node label ratio. While our analysis suggests that using $w$ directly is not a principled approach for addressing CSS and LS, it proves beneficial in scenarios where LS significantly affects outcomes, providing a better calibration compared to approaches that do not address LS, like PA-CSS. However, while StruRW holds an advantage over PA-CSS, its performance still lags behind PA-BOTH, which offers a more systematic solution for both CSS and LS.\\n\\nArxiv\\n\\nResults from the Arxiv datasets align well with expectations and the shift measures detailed in Table 10. Notably, CSS is most pronounced when the source graph includes papers published before 2007, with experimental results showing the most substantial improvements under these conditions. In the scenario where papers from 2016-2018 are used for testing, both PA-CSS and PA-BOTH outperform the baselines significantly, yet PA-LS emerges as the superior variant. This aligns with the LS metrics reported, which indicate a significant LS in this context. A similar pattern is observed when training on papers from before 2011 and testing on those from 2016-2018, with PA-LS achieving the best results.\\n\\nFor the target comprising papers from 2014-2016, our model continues to outperform baselines, albeit with a narrower margin compared to other datasets. In this case, not only does our method perform comparably, but all baselines also show similar performance levels, suggesting limited potential for improvements in this dataset. Furthermore, insights from synthetic experiments reveal that a CSS metric value around 0.16 does not lead to substantial performance degradation, which accounts for the moderate improvements over baselines in scenarios other than those using the source graph with pre-2007 papers.\\n\\nIn our evaluation of baseline performances, we note that StruRW emerges as the superior baseline, effectively handling CSS. In contrast, IWDAN tends to underperform relative to other baselines, which we attribute primarily to inaccuracies and instability in its label weight estimation. Designed for computer vision tasks where accuracy is typically high, IWDAN lacks mechanisms for regularization and robustness in its estimation processes, leading to its underperformance in our experiments involving tasks with a total of 40 classes. Meanwhile, the performance of other baselines is comparable to the ERM training.\\n\\nDBLP/ACM\\n\\nThe generalization results between the DBLP and ACM datasets offer insights into the comparative effects of feature shift versus structure shift. As discussed in the main text, baselines focused on feature alignment tend to perform well in this dataset, suggesting that this dataset is predominantly influenced by feature shifts rather than structural shifts and that feature alignment can address the shift effectively. This trend also leads to non-graph methods performing comparably to, or even better than, graph-based methods due to the dominance of feature shifts.\\n\\nIn response to these observations, we integrated adversarial training into our method to align feature shifts and investigated whether additional benefits could be derived from mitigating structure shifts. Our analysis of the experimental results, in conjunction with the shift measures detailed in Table 10, reveals a significant LS between these two datasets. Specifically, we note that the ACM graph exhibits a more imbalanced label distribution compared to the DBLP graph. This finding aligns with the experimental outcomes, where PA-LS emerges as the most effective model and IWDAN as the best baseline when training on ACM and testing on DBLP. Both methods are adept at handling LS, supporting our earlier assertion that LS plays a crucial role when transitioning from an imbalanced dataset to a more balanced one. Conversely, in the transition from DBLP to ACM, where LS has a lesser impact, PA-BOTH proves to be the most effective.\"}"}
{"id": "ttnbM598vZ", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof. \\\\( P_T(\\\\hat{Y}_u = i) = X_i' \\\\in Y \\\\)\\n\\n\\\\[ P_T(\\\\hat{Y}_u = i | Y_u = i') P_T(Y_u = i') (a) = X_i' \\\\in Y \\\\]\\n\\n\\\\[ P_S(\\\\hat{Y}_u = i | Y_u = i') P_T(Y_u = i') \\\\]\\n\\n\\\\[ P_S(Y_u = i') P_T(Y_u = i') \\\\]\\n\\n\\\\hline\\n\\n\\\\text{Algorithm Details}\\n\\n\\\\text{C.1. Details in optimization for } \\\\gamma\\n\\n\\\\text{C.1.1. Empirical estimation of } \\\\Sigma \\\\text{ and } \\\\nu \\\\text{ in matrix form}\\n\\nFor the least square problem that solves for \\\\( w \\\\Sigma w = \\\\nu \\\\) where \\\\( \\\\Sigma \\\\in \\\\mathbb{R}^{|Y|^2 \\\\times |Y|^2} \\\\), \\\\( w \\\\in \\\\mathbb{R}^{|Y|^2 \\\\times 1} \\\\), \\\\( \\\\nu \\\\in \\\\mathbb{R}^{|Y|^2 \\\\times 1} \\\\),\\n\\n\\\\[ \\\\hat{\\\\Sigma} = \\\\frac{1}{|E_S|} E_S \\\\in \\\\mathbb{R}^{|Y|^2 \\\\times |E_S|} \\\\]\\n\\nwhere each column represents the joint distribution of the classes prediction associated with the starting and ending node of each edge in the source graph.\\n\\n\\\\[ E_S \\\\in \\\\mathbb{R}^{|Y|^2 \\\\times |E_S|}, \\\\quad \\\\forall \\\\text{edge } uv \\\\in E_S. \\\\]\\n\\nAnd each entry \\\\( E_{ij,uv} = [g(h(L_u))_i \\\\otimes g(h(L_v))_j], \\\\quad \\\\forall i, j \\\\in Y. \\\\)\\n\\n\\\\[ M_S \\\\in \\\\mathbb{R}^{|E_S| \\\\times |Y|^2} \\\\]\\n\\nencodes the ground truth of the starting and ending node of an edge, as \\\\( E_{uv,y} = 1 \\\\) for each edge \\\\( uv \\\\in E_S \\\\).\\n\\n\\\\[ \\\\hat{\\\\nu} = \\\\frac{1}{|E_T|} E_T \\\\in \\\\mathbb{R}^{|Y|^2 \\\\times |E_T|}, \\\\quad \\\\forall \\\\text{edge } uv \\\\in E_T. \\\\]\\n\\nAnd each entry \\\\( E_{ij,uv} = [g(h(L_u))_i \\\\otimes g(h(L_v))_j], \\\\quad \\\\forall i, j \\\\in Y. \\\\)\\n\\n\\\\[ 1 \\\\in \\\\mathbb{R}^{|E_T| \\\\times 1} \\\\]\\n\\nis the all one vector.\\n\\n\\\\text{C.1.2. Calculate for } \\\\alpha \\\\text{ in matrix form}\\n\\nTo finally solve for the ratio weight \\\\( \\\\gamma \\\\), we need the value \\\\( \\\\alpha \\\\).\\n\\n\\\\[ \\\\alpha_i = P_T(y_u = i | A_{uv} = 1) P_S(y_u = i | A_{uv} = 1) = P_j P_T(y_u = i, y_v = j | A_{uv} = 1) P_j P_S(y_u = i, y_v = j | A_{uv} = 1) P_S(y_u = i | A_{uv} = 1) P_S(y_u = i | A_{uv} = 1) \\\\]\\n\\n\\\\[ P_j P_T(y_u = i, y_v = j | A_{uv} = 1) P_S(y_u = i, y_v = j | A_{uv} = 1) P_S(y_u = i | A_{uv} = 1) \\\\]\"}"}
{"id": "ttnbM598vZ", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\nIn matrix form, we construct $K \\\\in \\\\mathbb{R}^{\\\\left|Y\\\\right| \\\\times \\\\left|Y\\\\right| \\\\times 2}$, where $K_{i,j} = P_{S}(y_u = i, y_v = j | A_{uv} = 1) P_{S}(y_u = i | A_{uv} = 1)$, $\\\\forall i, j \\\\in \\\\left|Y\\\\right|$. Note that $K_{i,i'} = 0$ for $i' \\\\neq i$, $\\\\forall j \\\\in \\\\left|Y\\\\right|$.\\n\\n$\\\\alpha = Kw$.\\n\\nC.2. Details in optimization for $\\\\beta$\\n\\nFor the least square problem that solves for $\\\\beta$ where $C \\\\in \\\\mathbb{R}^{\\\\left|Y\\\\right| \\\\times \\\\left|Y\\\\right|}$, $\\\\beta \\\\in \\\\mathbb{R}^{\\\\left|Y\\\\right| \\\\times 1}$, $\\\\mu \\\\in \\\\mathbb{R}^{\\\\left|Y\\\\right| \\\\times 1}$\\n\\nEmpirically, we estimate the value of $\\\\hat{C}$ and $\\\\hat{\\\\mu}$ in matrix form as following:\\n\\n$\\\\hat{C} = \\\\frac{1}{\\\\left|V_S\\\\right|} D_S L_S D_S \\\\in \\\\mathbb{R}^{\\\\left|Y\\\\right| \\\\times \\\\left|V_S\\\\right|}$, where each column represents the distribution of the class prediction of each node in the source graph.\\n\\n$D_S: u, u = g(h(L_S) u), \\\\forall u \\\\in V_S$. And each entry $D_S_{i,u} = g(h(L_S) u)_i, \\\\forall i \\\\in Y$.\\n\\n$L_S \\\\in \\\\mathbb{R}^{\\\\left|V_S\\\\right| \\\\times \\\\left|Y\\\\right|}$ that encodes the ground truth class of each node, as $L_S_{u,y} = 1$ for each node $u \\\\in V_S$.\\n\\n$\\\\hat{\\\\mu} = \\\\frac{1}{\\\\left|V_T\\\\right|} D_T \\\\mu_{1} \\\\in \\\\mathbb{R}^{\\\\left|V_T\\\\right| \\\\times 1}$ Similarly, $D_T \\\\in \\\\mathbb{R}^{\\\\left|Y\\\\right| \\\\times \\\\left|V_T\\\\right|}$, where each column represents the distribution of the class prediction of each node in the target graph.\\n\\n$D_T: u, u = g(h(L_T) u), \\\\forall u \\\\in V_T$. And each entry $D_T_{i,u} = g(h(L_T) u)_i, \\\\forall i \\\\in Y$.\\n\\n$\\\\mu_1 \\\\in \\\\mathbb{R}^{\\\\left|V_T\\\\right| \\\\times 1}$ is the all one vector.\\n\\nD. More Related Works\\n\\nOther node-level DA works\\n\\nOther domain invariant learning-based methods, like Shen et al. (2020b) proposed to align the class-conditioned representations with conditional MMD distance by using pseudo-label predictions for the target domain, Zhang et al. (2021) aimed to use separate networks to capture the domain-specific features in addition to a shared encoder for adversarial training and further Pang et al. (2023) transformed the node features into spectral domain through Fourier transform for alignment. Other approaches like Cai et al. (2021) disentangled semantic, domain, and noise variables and used semantic variables that are better aligned with target graphs for prediction. Liu et al. (2024) explored the role of GNN propagation layers and linear transformation layers, thus proposing to use a shared transformation layer with more propagation layers on the target graph instead of a shared encoder.\\n\\nNode-level OOD works\\n\\nIn addition to GDA, many works target the out-of-distribution (OOD) generalization without access to unlabeled target data. For the node classification task, EERM (Wu et al., 2022) and LoRe-CIA (Wang et al., 2023) both extended the idea of invariant learning to node-level tasks, where EERM minimized the variance over representations across different environments and LoRe-CIA enforced the cross-environment Intra-class Alignment of node representations to remove their reliance on spurious features. Wang et al. (2021) extended mixup to the node representation under node and graph classification tasks.\\n\\nGraph-level DA and OOD works\\n\\nThe shifts and methods in graph-level problems are significantly different from those for node-level tasks. The shifts in graph-level tasks can be modeled as IID by considering individual graphs and often satisfy the covariate shift assumption, which makes some previous IID works applicable. Under the availability of target graphs, there are several graph-level GDA works like (Yin et al., 2023; 2022), where the former utilized contrastive learning to align the graph representations with similar semantics and the latter employed graph augmentation to match the target graphs under adversarial training. Regarding the scenarios in which we do not have access to the target graphs, it becomes the graph OOD problem. A dominant line of work in graph-level OOD is based on invariant learning originating from causality to identify a subgraph that remains invariant across graphs under distribution shifts. Among these works, Wu et al. (2021); Chen et al. (2022); Li et al. (2022a); Yang et al. (2022); Chen et al. (2023); Gui et al. (2023); Fan et al. (2022; 2023) aimed to find the invariant subgraph, and Miao et al. (2022); Yu et al. (2020) used graph information bottleneck. Furthermore, another line of work is based on causal discovery and graph matching to identify a subgraph that remains invariant across graphs under distribution shifts.\"}"}
{"id": "ttnbM598vZ", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\nworks adopted graph augmentation strategies, like (Sui et al., 2023; Jin et al., 2022) and some mixup-based methods (Han et al., 2022; Ling et al., 2023; Jia et al., 2023). Moreover, some works focused on handling the size shift (Yehudai et al., 2021; Bevilacqua et al., 2021; Chuang & Jegelka, 2022).\\n\\nE. Experiments details\\nE.1. Dataset Details\\n\\nDataset Statistics\\nHere we report the number of nodes, number of edges, feature dimension, and the number of labels for each dataset. The Arxiv-year means the graph with papers till that year. The edges are all undirected edges, which are counted twice in the edge list.\\n\\n| Dataset | # NODES | # EDGES | NODE FEATURE DIMENSION | # LABELS |\\n|---------|---------|---------|-------------------------|---------|\\n| ACM DBLP | 7410 | 11135 | 7537 | 6 |\\n| ACM RXIV -2007 | 5578 | 7341 | 7537 | 6 |\\n| ACM RXIV -2009 | 4980 | 5849 | 128 | 40 |\\n| ACM RXIV -2016 | 9410 | 13179 | 128 | 40 |\\n| ACM RXIV -2018 | 69499 | 232419 | 128 | 40 |\\n| ACM RXIV -2018 | 120740 | 615415 | 128 | 40 |\\n\\nTable 6. MAG dataset statistics\\n\\n| Country | # NODES | # EDGES | NODE FEATURE DIMENSION | # LABELS |\\n|---------|---------|---------|-------------------------|---------|\\n| US | 132558 | 697450 | 128 | 20 |\\n| CN | 101952 | 285561 | 128 | 20 |\\n| DE | 43032 | 126683 | 128 | 20 |\\n| JP | 37498 | 90944 | 128 | 20 |\\n| RU | 32833 | 67994 | 128 | 20 |\\n| FR | 29262 | 78222 | 128 | 20 |\\n\\nTable 7. Pileup dataset statistics\\n\\n| GG | # NODES | # EDGES | NODE FEATURE DIMENSION | # LABELS |\\n|----|---------|---------|-------------------------|---------|\\n| -10 | 18611 | 53725 | 28 | 2 |\\n| -30 | 17242 | 42769 | 28 | 2 |\\n| -50 | 41390 | 173392 | 28 | 2 |\\n| -140 | 38929 | 150026 | 28 | 2 |\\n| GG | 60054 | 341930 | 28 | 2 |\\n| GG | 154750 | 2081229 | 28 | 2 |\\n\\nDBLP and ACM are two paper citation networks obtained from DBLP and ACM, originally from (Tang et al., 2008) and processed by (Wu et al., 2020). We use the processed version. Nodes are papers and undirected edges represent citations between papers. The goal is to predict the 6 research topics of a paper: \u201cDatabase\u201d, \u201cData mining\u201d, \u201cArtificial intelligent\u201d, \u201cComputer vision\u201d, \u201cInformation Security\u201d and \u201cHigh Performance Computing\u201d.\\n\\nArxiv introduced in (Hu et al., 2020) is another citation network of Computer Science (CS) Arxiv papers to predict 40 classes on different subject areas. The feature vector is a 128-dimensional word2vec vector with the average embedding of the paper\u2019s title and abstract. Originally it is a directed graph with directed citations between papers, we convert it into an undirected graph.\\n\\nE.1.1. MORE DETAILS\\n\\nMAG DATASETS\\nMAG is a subset of the Microsoft Academic Graph (MAG) as detailed in (Hu et al., 2020; Wang et al., 2020), originally containing entities as papers, authors, institutions, and fields of study. There are four types of directed relations in the original graph connecting two types of entities: an author \u201cis affiliated with\u201d an institution, an author \u201cwrites\u201d a paper, a paper \u201ccites\u201d a paper, and a paper \u201chas a topic of\u201d a field of study. The node feature for a paper is the word2vec vector with 128 dimensions. The task is to predict the publication venue of papers, which in total has 349 classes. We curate the graph\"}"}
{"id": "ttnbM598vZ", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\nto include only paper nodes and convert directed citation links to undirected edges. Papers are split into separate graphs based on the country of the institution the corresponding author is affiliated with. Then, we detail the process of generating a separate \u201cpaper-cites-paper\u201d homogeneous graph for each country from the original ogbn-mag dataset.\\n\\nDetermine the country of origin for each paper.\\n\\nThe rule of determining the country of the paper is based on the country of the institute the corresponding author is affiliated with. Since the original ogbn-mag dataset does not indicate the information of the corresponding author, we retrieve the metadata of the papers via OpenAlex. Specifically, there is a boolean variable on OpenAlex boolean indicating whether an author is the corresponding author for each paper. Then, we further locate the institution this corresponding author is affiliated with and retrieve that institution\u2019s country to use as the country code for the paper. All these operations can be done through OpenAlex. However, not all papers include this corresponding author information on OpenAlex. Regarding the papers that miss this information, we determine the country of this paper through a majority vote based on the institution country of all authors in this paper. Namely, we first identify all authors recorded in the original dataset via the \u201cauthor\u2014writes\u2014paper\u201d relation and acquire the institute information for these authors through the relation of \u201cauthor\u2014is affiliated with\u2014institution\u201d. Then, with the country information retrieved from OpenAlex for these institutions, we do a majority vote to determine the final country code for the paper.\\n\\nGenerate country-specific graphs.\\n\\nBased on the country information obtained above, we generate a separate citation graph for a given country \\\\( C \\\\). It will contain all papers that have a country code of \\\\( C \\\\) and the edges indicating the citation relationships within these papers. The edge index set \\\\( E \\\\) is initialized as \\\\( \\\\emptyset \\\\). For each citation pair \\\\((v_i, v_j)\\\\) in the original \u201cpaper-cites-paper\u201d graph, it is added to \\\\( E \\\\) iff. both \\\\( v_i \\\\) and \\\\( v_j \\\\) have the same country affiliation \\\\( C \\\\). We then obtain the node set \\\\( V \\\\) based on all unique nodes appearing in \\\\( E \\\\). In the scope of this work, we only focus on the top 19 publication venues with the most papers for classification and combine the rest of the classes into a single dummy class.\\n\\nE.1.2. MORE DETAILS FOR HEP DATASETS\\n\\nInitially, there are multiple graphs with each graph representing a collision event in the large hadron collider (LHC). Here, we collate the graphs together to form a single large graph. We use 100 graphs in each domain to create the single source and target graph respectively. In the source graph, the nodes in 60 graphs are used for training, 20 are used for validation and 20 are used for testing. In the target graph, the nodes in 20 graphs are used for validation and 80 are used for testing. The particles can be divided into charged and neutral particles, where the labels of the charged particles are known by the detector. Therefore, the classifications are only done on the neutral particles. The node features contain the particle\u2019s position in \\\\( \\\\eta \\\\) axis, \\\\( p_t \\\\) as energy, the pdgID one hot encoding to indicate the type of particle, and the label of the particle (label for changed, unknown for neutral) to help with classification as neighborhood information.\\n\\nPileup (PU) levels indicate the number of other collisions in the background event, it is closely related to the label distribution of LC and OC. For instance, a high PU graph will have mostly OC particles and few LC particles. Also, it will cause significant CSS as the distribution of particles easily influences the connections between them. The physical processes correspond to different types of signal decay of the particles, which mainly causes some slight feature shifts and nearly no LS or CSS under the same PU level.\\n\\nE.2. Detailed experimental setting\\n\\nModel architecture\\n\\nThe backbone model is GraphSAGE with mean pooling having 3 GNN layers and 2 MLP layers for classification. The hidden dimension for GNN is 300 for Arxiv and MAG, 50 for Pileup, 128 for the DBLP/ACM dataset and 20 for synthetic datasets. The classifier dimension 300 for Arxiv and MAG, 50 for Pileup, 40 for DBLP/ACM dataset and 20 for synthetic datasets. If there is adversarial training with a domain classifier for some baselines, it has 3 layers and the hidden dimension is the same as the GNN dimension. All experiments are repeated three times.\\n\\nHardware\\n\\nAll experiments are run on NVIDIA RTX A6000 with 48G memory and Quadro RTX 6000 with 24G memory. Specifically, for the UDAGCN baselines, we try with the 48G memory GPU but still out of memory.\\n\\nSynthetic Datasets\\n\\nThe synthetic dataset is generated under the contextual stochastic block model (CSBM), where there are in total of 6000 nodes and 3 classes. We vary the edge connection probability matrix and the node label distribution in different settings. The node features are generated from a Gaussian distribution where \\\\( P_0 = \\\\mathcal{N}(\\\\begin{bmatrix} 1 & 0 \\\\\\\\ 0 & 1 \\\\end{bmatrix}, \\\\sigma^2 I) \\\\), \\\\( P_1 = \\\\mathcal{N}(\\\\begin{bmatrix} 0 & 1 \\\\\\\\ 1 & 0 \\\\end{bmatrix}, \\\\sigma^2 I) \\\\) and \\\\( P_2 = \\\\mathcal{N}(\\\\begin{bmatrix} 0 & 0 \\\\\\\\ 0 & 1 \\\\end{bmatrix}, \\\\sigma^2 I) \\\\), \\\\( \\\\sigma = 0.3 \\\\). and the distribution is the same for the source and target graph in all settings. We\\n2 This is an alternative way considering the Microsoft Academic website and underlying APIs have been retired on Dec. 31, 2021.\"}"}
{"id": "ttnbM598vZ", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use GraphSAGE (Hamilton et al., 2017) as backbones for the PU conditions, baselines with graph structure regularisation, and importance of tailored solutions for GDA like Pair-Align. Due to the suboptimality of performing only conditional feature alignment yet ignoring the structure, highlighting the particular cases of each scenario. With joint shifts in CSS and LS, Pair-Align methods perform the best and IWDAN is the second best. This matches our expectations that PU condition shifts in physical processes, Pair-Align methods still excels in transitioning from low PU to high PU, while PA-CSS and LS in non-graph tasks.\\n\\nHyperparameter Study\\nOur hyperparameter tuning is used for validation and the rest 80 percent are held out for testing. We select the best model based on the target validation and metric scores and report its scores on the target testing nodes.\\n\\nEvaluation and Metric\\nand synthetic datasets. For the MAG datasets, we evaluate the top 19 classes as we group the remaining classes as a dummy class. The Pileup dataset uses the binary f1 score, and the same model architecture for all baselines.\\n\\nTable 1. The Pileup dataset uses the binary f1 score, and the top 19 classes as we group the remaining classes as a dummy class. The Pileup dataset uses the binary f1 score.\"}"}
{"id": "ttnbM598vZ", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Ablation Study (from high PU to low PU in HEP datasets). But when this work also curates a new, by far the largest dataset MAG (IIDAI), indicating this dataset contains mostly feature shifts. While the best in most cases. PA-CSS contributes more compared to PA-LS when CSS dominates (MAG datasets, Arxiv, and pre-2007, testing on 2016-2018 as shown in Table 10), PA-LS alone offers slight improvements to PA-LS when CSS dominates (MAG datasets, Arxiv, and pre-2007, testing on 2016-2018 as shown in Table 10), PA-LS achieves the best performance.\\n\\nWe greatly thank Yongbin Feng for discussing relevant HEP (IIDAI). The work of HZ was supported in part by the Defense Advanced Research Projects Agency (DARPA) under Cooperative Agreement Number: HR00112320012 and a research grant from the IBM-Illinois Discovery Accelerator Institute (OMAINS). Supported by NSF award PHY-2117997 and IIS-2239565.\\n\\nAcknowledgement\\n\\nThere are many potential societal specific highlights here. For example, addressing structure shift has limited benefits in this dataset. Importantly, GDA methods and the non-graph methods, suggesting that lines perform similarly with no significant gap between the best model and baseline respectively.\\n\\nIndicate the best model and baseline.\"}"}
{"id": "ttnbM598vZ", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nAzizzadenesheli, K., Liu, A., Yang, F., and Anandkumar, A. Regularized learning for domain adaptation under label shifts. *International Conference on Learning Representations*, 2018.\\n\\nBertolini, D., Harris, P., Low, M., and Tran, N. Pileup per particle identification. *Journal of High Energy Physics*, 2014.\\n\\nBevilacqua, B., Zhou, Y., and Ribeiro, B. Size-invariant graph representations for graph classification extrapolations. *International Conference on Machine Learning*, 2021.\\n\\nCai, R., Wu, F., Li, Z., Wei, P., Yi, L., and Zhang, K. Graph domain adaptation: A generative view. *arXiv preprint arXiv:2106.07482*, 2021.\\n\\nChen, Y., Zhang, Y., Bian, Y., Yang, H., Kaili, M., Xie, B., Liu, T., Han, B., and Cheng, J. Learning causally invariant representations for out-of-distribution generalization on graphs. *Advances in Neural Information Processing Systems*, 2022.\\n\\nChen, Y., Bian, Y., Zhou, K., Xie, B., Han, B., and Cheng, J. Does invariant graph learning via environment augmentation learn invariance? *Advances in Neural Information Processing Systems*, 2023.\\n\\nChuang, C.-Y. and Jegelka, S. Tree mover\u2019s distance: Bridging graph metrics and stability of graph neural networks. *Advances in Neural Information Processing Systems*, 2022.\\n\\nCicek, S. and Soatto, S. Unsupervised domain adaptation via regularized conditional alignment. *Proceedings of the IEEE/CVF international conference on computer vision*, 2019.\\n\\nDai, Q., Wu, X.-M., Xiao, J., Shen, X., and Wang, D. Graph transfer learning via adversarial domain adaptation with graph convolution. *IEEE Transactions on Knowledge and Data Engineering*, 2022.\\n\\nDeshpande, Y., Sen, S., Montanari, A., and Mossel, E. Contextual stochastic block models. *Advances in Neural Information Processing Systems*, 31, 2018.\\n\\nDing, M., Kong, K., Chen, J., Kirchenbauer, J., Goldblum, M., Wipf, D., Huang, F., and Goldstein, T. A closer look at distribution shifts and out-of-distribution generalization on graphs. *NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications*, 2021.\\n\\nDou, Y., Liu, Z., Sun, L., Deng, Y., Peng, H., and Yu, P. S. Enhancing graph neural network-based fraud detectors against camouflaged fraudsters. *Proceedings of the 29th ACM international conference on information & knowledge management*, 2020.\\n\\nFan, S., Wang, X., Mo, Y., Shi, C., and Tang, J. Debiasing graph neural networks via learning disentangled causal substructure. *Advances in Neural Information Processing Systems*, 2022.\\n\\nFan, S., Wang, X., Shi, C., Cui, P., and Wang, B. Generalizing graph neural networks on out-of-distribution graphs. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2023.\\n\\nGanin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V. Domain-adversarial training of neural networks. *The journal of machine learning research*, 2016.\\n\\nGong, M., Zhang, K., Liu, T., Tao, D., Glymour, C., and Sch\u00f6lkopf, B. Domain adaptation with conditional transferable components. *International Conference on Machine Learning*, 2016.\\n\\nGui, S., Liu, M., Li, X., Luo, Y., and Ji, S. Joint learning of label and environment causal independence for graph out-of-distribution generalization. *Advances in Neural Information Processing Systems*, 2023.\\n\\nHamilton, W., Ying, Z., and Leskovec, J. Inductive representation learning on large graphs. *Advances in Neural Information Processing Systems*, 2017.\\n\\nHan, X., Jiang, Z., Liu, N., and Hu, X. G-mixup: Graph data augmentation for graph classification. *International Conference on Machine Learning*, 2022.\\n\\nHighfield, R. Large hadron collider: Thirteen ways to change the world. *The Daily Telegraph. London*. Retrieved, 2008.\\n\\nHu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. *Advances in Neural Information Processing Systems*, 2020.\\n\\nJackson, M. O. et al. Social and economic networks, volume 3. *Princeton university press Princeton*, 2008.\\n\\nJi, Y., Zhang, L., Wu, J., Wu, B., Li, L., Huang, L.-K., Xu, T., Rong, Y., Ren, J., Xue, D., et al. Drugood: Out-of-distribution dataset curator and benchmark for ai-aided drug discovery\u2013a focus on affinity prediction problems with noise annotations. *Proceedings of the AAAI Conference on Artificial Intelligence*, 2023.\\n\\nJia, T., Li, H., Yang, C., Tao, T., and Shi, C. Graph invariant learning with subgraph co-mixup for out-of-distribution generalization. *arXiv preprint arXiv:2312.10988*, 2023.\"}"}
{"id": "ttnbM598vZ", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\nJin, W., Zhao, T., Ding, J., Liu, Y., Tang, J., and Shah, N. Empowering graph representation learning with test-time graph transformation. International Conference on Learning Representations, 2022.\\n\\nKipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. International Conference on Learning Representations, 2016.\\n\\nKoh, P. W., Sagawa, S., Marklund, H., Xie, S. M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R. L., Gao, I., et al. Wilds: A benchmark of in-the-wild distribution shifts. International Conference on Machine Learning, 2021.\\n\\nKomiske, P. T., Metodiev, E. M., Nachman, B., and Schwartz, M. D. Pileup mitigation with machine learning (pumml). Journal of High Energy Physics, 2017.\\n\\nLi, H., Zhang, Z., Wang, X., and Zhu, W. Learning invariant graph representations for out-of-distribution generalization. Advances in Neural Information Processing Systems, 2022a.\\n\\nLi, T., Liu, S., Feng, Y., Paspalaki, G., Tran, N., Liu, M., and Li, P. Semi-supervised graph neural networks for pileup noise removal. The European Physics Journal C, 2022b.\\n\\nLiao, P., Zhao, H., Xu, K., Jaakkola, T., Gordon, G. J., Jegelka, S., and Salakhutdinov, R. Information obfuscation of graph neural networks. International Conference on Machine Learning, 2021.\\n\\nLing, H., Jiang, Z., Liu, M., Ji, S., and Zou, N. Graph mixup with soft alignments. International Conference on Machine Learning, 2023.\\n\\nLipton, Z., Wang, Y.-X., and Smola, A. Detecting and correcting for label shift with black box predictors. International Conference on Machine Learning, 2018.\\n\\nLiu, M., Fang, Z., Zhang, Z., Gu, M., Zhou, S., Wang, X., and Bu, J. R. Rethinking propagation for unsupervised graph domain adaptation. arXiv preprint arXiv:2402.05660, 2024.\\n\\nLiu, S., Li, T., Feng, Y., Tran, N., Zhao, H., Qiu, Q., and Li, P. Structural re-weighting improves graph domain adaptation. International Conference on Machine Learning, 2023.\\n\\nLiu, X., Guo, Z., Li, S., Xing, F., You, J., Kuo, C.-C. J., El Fakhri, G., and Woo, J. Adversarial unsupervised domain adaptation with conditional and label shift: Infer, align and iterate. Proceedings of the IEEE/CVF international conference on computer vision, 2021.\\n\\nLong, M., Cao, Y., Wang, J., and Jordan, M. Learning transferable features with deep adaptation networks. International Conference on Machine Learning, 2015.\\n\\nLong, M., Cao, Z., Wang, J., and Jordan, M. I. Conditional adversarial domain adaptation. Advances in Neural Information Processing Systems, 2018.\\n\\nMiao, S., Liu, M., and Li, P. Interpretable and generalizable graph learning via stochastic attention mechanism. International Conference on Machine Learning, 2022.\\n\\nPang, J., Wang, Z., Tang, J., Xiao, M., and Yin, N. Sanga: Spectral augmentation for graph domain adaptation. Proceedings of the 31st ACM International Conference on Multimedia, 2023.\\n\\nPeters, J., Janzing, D., and Sch\u00f6lkopf, B. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017.\\n\\nRojas-Carulla, M., Sch\u00f6lkopf, B., Turner, R., and Peters, J. Invariant models for causal transfer learning. The Journal of Machine Learning Research, 2018.\\n\\nShen, X., Dai, Q., Chung, F.-l., Lu, W., and Choi, K.-S. Adversarial deep network embedding for cross-network node classification. Proceedings of the AAAI conference on artificial intelligence, 2020a.\\n\\nShen, X., Dai, Q., Mao, S., Chung, F.-l., and Choi, K.-S. Network together: Node classification via cross-network deep network embedding. IEEE Transactions on Neural Networks and Learning Systems, 2020b.\\n\\nShervashidze, N., Schweitzer, P., Van Leeuwen, E. J., Mehlhorn, K., and Borgwardt, K. M. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011.\\n\\nShlomi, J., Battaglia, P., and Vlimant, J.-R. Graph neural networks in particle physics. Machine Learning: Science and Technology, 2020.\\n\\nSui, Y., Wu, Q., Wu, J., Cui, Q., Li, L., Zhou, J., Wang, X., and He, X. Unleashing the power of graph data augmentation on covariate distribution shift. Advances in Neural Information Processing Systems, 2023.\\n\\nSzklarczyk, D., Gable, A. L., Lyon, D., Junge, A., Wyder, S., Huerta-Cepas, J., Simonovic, M., Doncheva, N. T., Morris, J. H., Bork, P., et al. String v11: protein\u2013protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets. Nucleic acids research, 2019.\\n\\nTachet des Combes, R., Zhao, H., Wang, Y.-X., and Gordon, G. J. Domain adaptation with conditional distribution.\"}"}
{"id": "ttnbM598vZ", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\ndenote the format of edge connection probability matrix as $B = \\\\begin{pmatrix} p & q & q \\\\\\\\ q & p & q \\\\\\\\ q & q & p \\\\end{pmatrix}$, where $p$ is the intra-class edge probability and $q$ is the inter-class edge probability.\\n\\n- The source graph has $P_Y = [1/3, 1/3, 1/3]$ and $p = 0.02$, $q = 0.005$.\\n- For setting 1 and 2 with the shift in only class ratio, they have the same $P_Y$, and setting 1 has $p = 0.015$, $q = 0.0075$ and setting 2 has $p = 0.01$, $q = 0.01$.\\n- For setting 3 and 4 with the shift in only cardinality, they have the same $P_Y$, and setting 3 has $p = 0.02/2$, $q = 0.005/2$ and setting 4 has $p = 0.02/4$, $q = 0.005/4$.\\n- For setting 5 and 6 with the shift in both class ratio and cardinality, they have the same $P_Y$, and setting 5 has $p = 0.015/2$, $q = 0.0075/2$ and setting 6 has $p = 0.01/2$, $q = 0.005/2$.\\n- For setting 7 and 8 with shifts in both CSS and label shift, they have the same edge connection probability as $p = 0.015/2$, $q = 0.0075/2$ but different label distributions. Setting 7 has $P_Y = [0.5, 0.25, 0.25]$ and setting 8 has $P_Y = [0.1, 0.3, 0.6]$.\\n\\nRegarding the experiments studying the shift in pileup levels, the pair with PU10 and PU30 is from signal qq. The other two pairs with PU10 and PU50, PU30 and PU140 are from signal gg. The experiments that study the shift in physical processes are from the same PU level 10. Compared to the Pileup datasets used in the StruRW paper (Liu et al., 2023), we investigate the physical process shift with datasets from signal qq and signal gg instead of signal gg and signal $\\\\nu\\\\nu$ ($\\\\nu\\\\nu$). Also, we conduct more experiments to study the pileup shifts under the same physical process being signal qq (PU10 vs. PU30) or signal gg (PU10 vs. PU50 and PU30 vs. PU140). In addition, the StruRW paper treats each event as a single graph. They train the algorithm using multiple training graphs and adopt the edge weights as the average from each graph. In this paper, we collate the graphs for all events together for training and weight estimations.\\n\\nThe graph is formed based on the ending year, meaning that the graph contains all nodes till the specified ending year. For instance, for the experiments where the source papers ended in 2007, the source graph contains all nodes and edges associated with papers that were published no later than 2007. Then, if the target years are from 2014 to 2016, then the entire target graph contains all papers published till 2016, but we only evaluate on the papers published from 2014 to 2016.\\n\\nSince we observe that this dataset presents additional feature shift, so we additionally add adversarial layers to align the node representations. Basically, it is the combination of Pair-Align with label-weighted adversarial feature alignment, and the hyperparameters with additional adversarial layers are the same with DANN and will be detailed below. Also, note that to systematically control the label shift degree in this relatively small graph ($< 10000$ nodes), the split of nodes for training/validation/testing is done regarding each class of nodes. This is slightly different from the data in previous papers using this dataset, so the results may not be directly comparable.\\n\\nE.3. Hyperparameter tuning\\n\\nHyperparameter tuning involves adjusting $\\\\delta$ for edge probability regularization in $\\\\gamma$ calculation and $\\\\lambda$ for L2 regularization in the least square optimizations for $w$ and $\\\\beta$. Selecting $\\\\delta$ correlates to the degree of structure shift and $\\\\lambda$ is chosen based on the number of labels and classification performance. In datasets like Arxiv and MAG, where classification is challenging and labels are numerous, leading to ill-conditioned or rank-deficient confusion matrices, a larger $\\\\lambda$ is required. For simpler tasks with fewer classes, like synthetic and low PU datasets, a lower $\\\\lambda$ suffices. $\\\\delta$ should be small for larger CSS (MAG and Pileup) and large with smaller CSS (Arxiv and physical process shift in Pileup) to counteract the spurious $\\\\gamma$ value that may caused by variance in edge formation. Below is the detailed range of hyperparameters.\\n\\nThe learning rate is 0.003 and the number of epochs is 400 for all experiments. The hyperparameters are tuned mainly for the robustness control, as the $\\\\delta$ in regularizing edges and $\\\\lambda$ in L2 regularization for optimization of $w$ and $\\\\beta$. Here, for all datasets, $\\\\lambda_\\\\beta$ for $\\\\beta$ is chosen from $\\\\{0.005, 0.01, 0.1, 1.5\\\\}$ to reweight the ERM loss to handle the LS. Additionally, we also consider reweighting the ERM loss by source label distribution together. Specifically, we found it useful in the case with imbalanced training label distribution, like both directions in DBLP/ACM datasets, transitioning from high PU to low PU, and the Arxiv training with papers pre-2007 and pre-2009. In other cases, we do not reweight the ERM loss by source label distribution.\"}"}
{"id": "ttnbM598vZ", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We design two metrics to measure the degree of structure shift in terms of CSS and LS. Specifically, we calculate the total variation distance of this conditional neighborhood node label distribution of each class as:\\n\\n$$\\\\left\\\\|P \\\\in Y_i \\\\right\\\\|_{TV}$$\\n\\nE.4. Baseline Tuning\\n\\n\u2022 For the Arxiv dataset, regarding the settings with training data till 2007, we use the StruRW-ERM baseline and we tune the coefficient to update the label weight calculated after each epoch as:\\n\\n$$\\\\left\\\\{q_{smooth} \\\\right\\\\}$$\\n\\nfrom their published code.\\n\\n\u2022 For StruRW, we use the StruRW-ERM baseline and we tune the coefficient to update the label weight calculated after each epoch as:\\n\\n$$\\\\left\\\\{q_{smooth} \\\\right\\\\}$$\\n\\nfrom their published code.\\n\\n\u2022 For IWDAN, we tune three hyperparameters, the same two parameters as the coefficient before the domain alignment loss and the max value of the rate added during the gradient reversal layer.\\n\\n\u2022 For UDAGCN, we also tune the two hyperparameters from DANN as the coefficient before the domain alignment loss and max value of the rate added during the gradient reversal layer.\\n\\n\u2022 For SpecReg, we totally tune for 5 hyperparameters and we follow the original hyperparameters for the dataset Arxiv and.\\n\\n\u2022 For the synthetic datasets, the.\\n\\n\u2022 For the MAG dataset, the.\\n\\n\u2022 For the DBLP/ACM dataset, the.\\n\\n\u2022 For the Pileup dataset, regarding the settings with pileup shift,\"}"}
{"id": "ttnbM598vZ", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\nThen, we take a weighted average of the TV distance for each class based on the label distribution of end nodes conditioned on an edge $P(U|\\\\text{e}uv \\\\in E_U)$ since classes that appear more often as a center node in the neighborhood may affect more in the structure shift. The CSS-src in the table indicates the weighted average by $P(S(Y_u|\\\\text{e}uv \\\\in E_S))$ and CSS-tgt in the table indicates the weighted average by $P(T(Y_u|\\\\text{e}uv \\\\in E_T))$, and CSS-both is the average of CSS-src and CSS-tgt.\\n\\nThe metric of LS is calculated as the total variation distance between the source and target label distribution as:\\n\\n$$TV(P_S(Y), P_T(Y)) = \\\\frac{1}{2} \\\\sum_{i \\\\in Y} |P_S(Y = i) - P_T(Y = i)|$$\\n\\nThe shift metrics for each dataset are shown in the following tables.\\n\\n### Table 8. MAG dataset shift metrics\\n\\n| Source | Target | CSS-src | CSS-tgt | CSS-both | LS |\\n|--------|--------|---------|---------|----------|----|\\n| US \u2192 CN | US \u2192 DE | US \u2192 JP | US \u2192 RU | US \u2192 FR | CN \u2192 US | CN \u2192 DE | CN \u2192 JP | CN \u2192 RU | CN \u2192 FR | CSS-src: 0.1639, 0.2299, 0.1322, 0.3532, 0.2530, 0.2062, 0.1775, 0.1487, 0.2120, 0.1540 | CSS-tgt: 0.2062, 0.2217, 0.1438, 0.2866, 0.2854, 0.1639, 0.2311, 0.1323, 0.2027, 0.2661 | CSS-both: 0.1850, 0.2258, 0.1380, 0.3199, 0.2692, 0.1850, 0.2043, 0.1405, 0.2073, 0.2100 | LS: 0.2734, 0.1498, 0.1699, 0.3856, 0.1706, 0.2734, 0.2691, 0.1522, 0.2453, 0.2256 |\\n\\n### Table 9. HEP pileup dataset shift metrics\\n\\n| Source | Target | CSS-src | CSS-tgt | CSS-both | LS |\\n|--------|--------|---------|---------|----------|----|\\n| DPU10 \u2192 30 | DPU30 \u2192 10 | DPU10 \u2192 50 | DPU50 \u2192 10 | DPU30 \u2192 140 | DPU140 \u2192 30 | CSS-src: 0.1941, 0.1567, 0.2910, 0.2111, 0.1871, 0.1307, 0.0232, 0.0222 | CSS-tgt: 0.1567, 0.1941, 0.2111, 0.2910, 0.1871, 0.1307, 0.0222, 0.0232 | CSS-both: 0.1754, 0.1754, 0.2510, 0.2510, 0.1589, 0.1589, 0.0227, 0.0227 | LS: 0.2258, 0.2258, 0.3175, 0.3175, 0.1590, 0.1590, 0.0348, 0.0348 |\\n\\n### Table 10. Real dataset shift metrics\\n\\n| Source | Target | CSS-src | CSS-tgt | CSS-both | LS |\\n|--------|--------|---------|---------|----------|----|\\n| DBLP 1950-2007 | ACM 1950-2009 | DBLP 2014-2016 | ACM 2016-2018 | DBLP 2014-2016 | ACM 2016-2018 | CSS-src: 0.2070, 0.2651, 0.1531, 0.2010, 0.1023, 0.1443, 0.1400, 0.2241 | CSS-tgt: 0.2404, 0.3060, 0.2043, 0.2737, 0.1504, 0.2301, 0.2241, 0.1400 | CSS-both: 0.2237, 0.2844, 0.1787, 0.2374, 0.1263, 0.1872, 0.1820, 0.1820 | LS: 0.2938, 0.4396, 0.2990, 0.4552, 0.2853, 0.4438, 0.3435, 0.3435 |\\n\\n### Table 11. Synthetic CSBM dataset shift metrics\\n\\n| Source | Target | CSS (ONLY CLASS RATIO SHIFT) | CSS (ONLY DEGREE SHIFT) | CSS (SHIFT IN BOTH) | CSS + LS |\\n|--------|--------|-------------------------------|--------------------------|---------------------|---------|\\n| CSS-src | CSS-tgt | CSS-both | CSS + LS |\\n| CSS-src: 0.1655, 0.3322, 0.0042, 0.0053, 0.1673, 0.3308, 0.1777, 0.2939 | CSS-tgt: 0.1655, 0.3322, 0.0042, 0.0053, 0.1673, 0.3308, 0.1215, 0.1840 | CSS-both: 0.1655, 0.3322, 0.0042, 0.0053, 0.1673, 0.3308, 0.1496, 0.2389 | CSS + LS: 0.1650, 0.2667 |\\n\\n### E.6. More results analysis\\n\\nIn this section, we will discuss more regarding our experimental results and provide some explanations of our Pair-Align performance and comparison over the baselines.\\n\\n**Synthetic Data**\\n\\nAs discussed in the main text, our major conclusion is that our Pair-Align is practical for handling alignment by focusing only on the conditional neighborhood node label distribution to address class ratio shifts. Although Pair-Align's performance is not the best among the baselines when there is a shift in node degree, we argue that in practice, ERM training alone is adequate under node degree shifts, especially when the graph size is large. Here, the graph size is only 6000\u2014a\"}"}
{"id": "ttnbM598vZ", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\nIt should be perfect when the graph size is larger. Also, in the second setting with a degree shift, the degree ratio shift of 4 is relatively large, but the accuracy remains at 96%. We expect that the decay should be negligible when the graph size is larger, often at least 10 times larger than 6000.\\n\\nRegarding performance gains in addressing structure shifts, we observe that PA-CSS demonstrates significant improvements, particularly in the second case of each scenario with larger degree shifts. Among the baselines, StruRW consistently outperforms others in different CSS scenarios, except in node degree shifts. This is expected since StruRW is specifically designed to handle CSS. Plus, in the synthetic CSBM data used here, the instability commonly associated with using hard pseudo-labels does not significantly affect performance due to easy classification task. However, compared to our Pair-Align methods, StruRW still shows limited performance even with only CSS shifts. When both CSS and LS shifts occur, IWDAN emerges as the best baseline, as its algorithm addresses both conditional shifts and LS in non-graph problems effectively. In synthetic datasets, shifts are less complex than in real-world graph-structured data, allowing IWDAN to lead to empirical improvements. Our PA-BOTH outperforms all in scenarios involving CSS and LS shifts. By comparing PA-CSS and PA-LS, we found that when both CSS and LS occur, the impact of CSS often dominates, making PA-CSS more effective than PA-LS. However, this observation is based on our source graph\u2019s balanced label distribution and does not hold in the HEP pileup dataset when moving from highly imbalanced data (high PU conditions) to more balanced data (low PU conditions), which we will discuss later in relation to the Pileup dataset.\\n\\nAnother advantage of using synthetic dataset results is that they help us understand the experimental results on real datasets better. For example, by combining the shift statistics from Table 11 with the experimental results, we see that a CSS metric value around 0.16 does not significantly impact the performance, thus not clearly demonstrating the effectiveness of Pair-Align. However, Pair-Align methods show substantial benefits under larger shifts, with metric values around 0.3. MAG\\n\\nOverall, our Pair-Align methods demonstrated significant advantages over the majority of baseline approaches, including the top-performing baseline, StruRW. When considering the relative improvement to ERM performance (as well as the performance of other baselines, except StruRW), there is an average relative benefit of over 45% when training on the US graph and nearly 100% when training on the CN graph. This substantial improvement corroborates our discussion regarding the existing gap, where current methods fall short in effectively addressing structure shifts. As detailed in the main text, our PA-CSS methods not only surpass StruRW in performance but also yield additional benefits from handling LS, as the LS degree indicated in Table 8. We believe the primary advantages stem from our principled approach to addressing CSS with $\\\\gamma$, which remains unbiased by LS, and the enhanced robustness afforded by using soft label predictions and regularized least square estimations. This also elucidates the shortcomings of IWDAN, a non-graph method for addressing conditional shift and LS, which underperforms under the MAG dataset conditions as discussed in the main text.\\n\\nWe next explore the relationship between performance improvements and the degree of structure shift. The experimental results align closely with the CSS measurements shown in Table 8. For example, the transitions from US to JP and CN to JP involve a smaller degree of CSS compared to other scenarios, resulting in relatively modest improvements. Similarly, generalizations between the US and CN also show fewer benefits. Conversely, the impact of LS is less evident in the outcomes associated with PA-LS, as this approach alone yields only marginal improvements. However, when we evaluate the additional gains from LS mitigation provided by PA-BOTH in comparison to PA-CSS, scenarios with larger LS (such as US $\\\\rightarrow$ CN, CN $\\\\rightarrow$ US, US $\\\\rightarrow$ RU, and CN $\\\\rightarrow$ DE) demonstrate more substantial benefits.\\n\\nPileup Mitigation\\n\\nThe most crucial discussions concerning HEP pileup datasets are detailed in the main text, particularly focusing on the distinct impacts of CSS and LS in transitions from high PU conditions to low PU conditions, and vice versa. This underscores that while the two directions have identical measures of LS, the direction of generalization is crucial. From a training perspective, it is clear that a model trained on a highly imbalanced dataset may neglect nodes in minor classes, leading to worse performance on more balanced datasets. To improve generalization, it is essential to adjust the classification loss to increase the model\u2019s focus on these minor nodes during training. This explains why PA-CSS alone does not yield benefits in scenarios transitioning from high to low PU, and why PA-LS becomes necessary. Conversely, when transitioning from low to high PU, PA-CSS suffices to address CSS, as LS has a minimal effect on performance in this direction.\\n\\nWe then review baseline performance under the shift in pileup (PU) conditions. As noted in the main text, methods primarily addressing feature shifts, such as DANN, UDAGCN, and IWDAN, underperform, underscoring that PU conditions predominantly affect graph structure rather than node features. This observation aligns with the physical interpretation of PU shifts described in the dataset details in E.1.2. PU shift correlates with changes in the number of other collisions (OC) during collision events, directly influencing the OC ratio and the pattern of node connections, as illustrated in Fig 1. Given...\"}"}
{"id": "ttnbM598vZ", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\nAdvances in Neural Information Processing Systems, 2020.\\nTang, J., Zhang, J., Yao, L., Li, J., Zhang, L., and Su, Z.\\n\\nArnetminer: extraction and mining of academic social networks.\\nProceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 2008.\\n\\nTzeng, E., Hoffman, J., Saenko, K., and Darrell, T.\\nAdversarial discriminative domain adaptation.\\nProceedings of the IEEE conference on computer vision and pattern recognition, 2017.\\n\\nVeli\u010dkovi\u0107, P., Cucurull, G., Casanova, A., Romero, A., Li\u00f2, P., and Bengio, Y.\\nGraph attention networks.\\nInternational Conference on Learning Representations, 2018.\\n\\nWang, D., Lin, J., Cui, P., Jia, Q., Wang, Z., Fang, Y., Yu, Q., Zhou, J., Yang, S., and Qi, Y.\\nA semi-supervised graph attentive network for financial fraud detection.\\nIEEE International Conference on Data Mining, 2019.\\n\\nWang, K., Shen, Z., Huang, C., Wu, C.-H., Dong, Y., and Kanakia, A.\\nMicrosoft academic graph: When experts are not enough.\\nQuantitative Science Studies, 2020.\\n\\nWang, Q., Wang, Y., and Ying, X.\\nImproved invariant learning for node-level out-of-distribution generalization on graphs.\\nSubmitted to The Twelfth International Conference on Learning Representations, 2023.\\n\\nWang, Y., Wang, W., Liang, Y., Cai, Y., and Hooi, B.\\nMixup for node and graph classification.\\nProceedings of the Web Conference, 2021.\\n\\nWei, R., Yin, H., Jia, J., Benson, A. R., and Li, P.\\nUnderstanding non-linearity in graph neural networks from the bayesian-inference perspective.\\nAdvances in Neural Information Processing Systems, 2022.\\n\\nWu, J., He, J., and Ainsworth, E.\\nNon-iid transfer learning on graphs.\\nProceedings of the AAAI Conference on Artificial Intelligence, 2023.\\n\\nWu, M., Pan, S., Zhou, C., Chang, X., and Zhu, X.\\nUnsupervised domain adaptive graph convolutional networks.\\nProceedings of The Web Conference, 2020.\\n\\nWu, Q., Zhang, H., Yan, J., and Wipf, D.\\nHandling distribution shifts on graphs: An invariance perspective.\\nInternational Conference on Learning Representations, 2022.\\n\\nWu, Y., Wang, X., Zhang, A., He, X., and Chua, T.-S.\\nDiscovering invariant rationales for graph neural networks.\\nInternational Conference on Learning Representations, 2021.\\n\\nXu, K., Hu, W., Leskovec, J., and Jegelka, S.\\nHow powerful are graph neural networks?\\nInternational Conference on Learning Representations, 2018.\\n\\nYang, N., Zeng, K., Wu, Q., Jia, X., and Yan, J.\\nLearning substructure invariance for out-of-distribution molecular representations.\\nAdvances in Neural Information Processing Systems, 2022.\\n\\nYehudai, G., Fetaya, E., Meirom, E., Chechik, G., and Maron, H.\\nFrom local structures to size generalization in graph neural networks.\\nInternational Conference on Machine Learning, 2021.\\n\\nYin, N., Shen, L., Li, B., Wang, M., Luo, X., Chen, C., Luo, Z., and Hua, X.-S.\\nDeal: An unsupervised domain adaptive framework for graph-level classification.\\nProceedings of the 30th ACM International Conference on Multimedia, 2022.\\n\\nYin, N., Shen, L., Wang, M., Lan, L., Ma, Z., Chen, C., Hua, X.-S., and Luo, X.\\nCoco: A coupled contrastive framework for unsupervised domain adaptive graph classification.\\nInternational Conference on Machine Learning, 2023.\\n\\nYou, Y., Chen, T., Wang, Z., and Shen, Y.\\nGraph domain adaptation via theory-grounded spectral regularization.\\nInternational Conference on Learning Representations, 2023.\\n\\nYu, J., Xu, T., Rong, Y., Bian, Y., Huang, J., and He, R.\\nGraph information bottleneck for subgraph recognition.\\nInternational Conference on Learning Representations, 2020.\\n\\nZellinger, W., Grubinger, T., Lughofer, E., Natschl\u00f6r, T., and Saminger-Platz, S.\\nCentral moment discrepancy (cmd) for domain-invariant representation learning.\\nInternational Conference on Learning Representations, 2016.\\n\\nZhang, K., Sch\u00f6lkopf, B., Muandet, K., and Wang, Z.\\nDomain adaptation under target and conditional shift.\\nInternational Conference on Machine Learning, 2013.\\n\\nZhang, X., Du, Y., Xie, R., and Wang, C.\\nAdversarial separation network for cross-network node classification.\\nProceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021.\\n\\nZhang, Y., Song, G., Du, L., Yang, S., and Jin, Y.\\nDane: Domain adaptive network embedding.\\nIJCAI International Joint Conference on Artificial Intelligence, 2019.\\n\\nZhao, H., Zhang, S., Wu, G., Moura, J. M., Costeira, J. P., and Gordon, G. J.\\nAdversarial multiple source domain adaptation.\\nAdvances in Neural Information Processing Systems, 2018.\"}"}
{"id": "ttnbM598vZ", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\nZhao, H., Des Combes, R. T., Zhang, K., and Gordon, G. On learning invariant representations for domain adaptation. International Conference on Machine Learning, 2019.\\n\\nZhu, Q., Ponomareva, N., Han, J., and Perozzi, B. Shift-robust gnns: Overcoming the limitations of localized graph training data. Advances in Neural Information Processing Systems, 2021.\\n\\nZhu, Q., Zhang, C., Park, C., Yang, C., and Han, J. Shift-robust node classification via graph adversarial clustering. arXiv preprint arXiv:2203.15802, 2022.\\n\\nZhu, Q., Jiao, Y., Ponomareva, N., Han, J., and Perozzi, B. Explaining and adapting graph conditional shift. arXiv preprint arXiv:2306.03256, 2023.\"}"}
{"id": "ttnbM598vZ", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With the goal to achieve (b), here we suppose that the observed messages $h(a)$ is based on the assumption that node attributes and edges are conditionally independent of others given the node labels.\\n\\nThe Contextual Stochastic Block Model (CSBM) is a framework combining the stochastic block model with node features. For random graph generation, a CSBM with nodes belonging to $k$ classes is defined by parameters $P_{m}$, messages. For simplicity, we assume that $P$ independently sampled from $P_{m}$.\\n\\nIf some of them are identical, we modify the coefficient $u$ if there exists a transformation that modifies the neighborhood of node $t \\\\in N$. Since $P$ in a graph generated from CSBM, the probability that an edge exists between a node $i, v \\\\in V$ is $P_{ij}$. (Edge Conditional Independence) Given node labels $P$, (Conditional Alignment in the previous layer $P$) = $P_{ij}$. Since $P$ is symmetric, i.e., $P_{ij} = P_{ji}$.\\n\\nTheorem A.1 (Contextual Stochastic Block Model)\\n\\nA. Some Definitions\\n\\n\u2022 (Pairwise Alignment Improves Graph Domain Adaptation)\\n\\n\u2022 (Deshpande et al., 2018)\\n\\n\u2200 $t \\\\in Y$: (c) is based on the assumption that $t \\\\in Y$.\\n\\nGiven the following assumptions $h$:\\n\\n1. $t \\\\in Y$: (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that given $t \\\\in Y$, it suffices to achieve by making the input distribution equal $P(t)$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. (c) is based on the assumption that $t \\\\in Y$. ("}
{"id": "ttnbM598vZ", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Then, $P$ structures given node labels, then $\\\\nu$ can induce $\\\\nu$. And (b) is achieved since $P$ can be guaranteed.\\n\\n**Remark**\\n\\nIteratively, we can achieve $\\\\nu$. Then, $P$ of others given the node labels. Based on Eq. (9), this means it suffices to let $\\\\nu$. Under the assumption that given $\\\\nu$, since the source and target graphs undergo the same set of functions. Based on Eq. (9), this means it suffices to let $\\\\nu$. Therefore, $\\\\nu$ holds for all $\\\\nu$ such that $\\\\nu$ is independently sampled from $\\\\nu$. Is satisfied, and node representations are conditionally independent of each other given the node labels, then $P$ is proved above that when using a multiset transformation to align two distributions, this can be guaranteed.\\n\\n**B.2. Proof for Lemma 3.8**\\n\\nPairwise Alignment Improves Graph Domain Adaptation\\n\\n**B.3. Proof for Lemma 3.11**\\n\\n$$S(\\\\hat{=} u) = Y_{\\\\nu}$$\\n\\n$$S(\\\\hat{=} u | k) = Y$$\\n\\n$$\\\\mu$$\\n\\n$$S(\\\\hat{=} u | k) = Y$$\\n\\n$$\\\\mu$$\\n\\n$$S(\\\\hat{=} u | k) = Y$$\\n\\n$$\\\\mu$$\\n\\n$$S(\\\\hat{=} u | k) = Y$$\\n\\n$$\\\\mu$$\\n\\n$$S(\\\\hat{=} u | k) = Y$$\\n\\n$$\\\\mu$$\"}"}
{"id": "ttnbM598vZ", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\nShikun Liu\\nDeyu Zou\\nHan Zhao\\nPan Li\\n\\nAbstract\\nGraph-based methods, pivotal for label inference over interconnected objects in many real-world applications, often encounter generalization challenges, if the graph used for model training differs significantly from the graph used for testing. This work delves into Graph Domain Adaptation (GDA) to address the unique complexities of distribution shifts over graph data, where interconnected data points experience shifts in features, labels, and in particular, connecting patterns. We propose a novel, theoretically principled method, Pairwise Alignment (Pair-Align) to counter graph structure shift by mitigating conditional structure shift (CSS) and label shift (LS). Pair-Align uses edge weights to recalibrate the influence among neighboring nodes to handle CSS and adjusts the classification loss with label weights to handle LS. Our method demonstrates superior performance in real-world applications, including node classification with region shift in social networks, and the pileup mitigation task in particle colliding experiments. For the first application, we also curate the largest dataset by far for GDA studies. Our method shows strong performance in synthetic and other existing benchmark datasets.\\n\\n1. Introduction\\nGraph-based methods are commonly used to enhance label inference for interconnected objects by utilizing their connection patterns in many real-world applications (Jackson et al., 2008; Szklarczyk et al., 2019; Shlomi et al., 2020). Nonetheless, these methods often encounter generalization challenges, as the objects that lack labels and require inference may originate from domains that differ significantly from those with abundant labeled data, thereby exhibiting distinct interconnecting patterns. For instance, in fraud detection within financial networks, label acquisition may be constrained to specific network regions due to varying international legal frameworks and diverse data collection periods (Wang et al., 2019; Dou et al., 2020). Another example is particle filtering for Large Hadron Collider (LHC) experiments (Highfield, 2008), where reliance on simulation-derived labeled data poses a challenge. These simulations may not accurately capture the nuances of real-world experimental conditions, potentially leading to discrepancies in label inference performance when applied to actual experiment scenarios (Li et al., 2022b; Komiske et al., 2017).\\n\\nGraph Neural Networks (GNNs) have recently demonstrated remarkable effectiveness in utilizing object interconnections for label inference tasks (Kipf & Welling, 2016; Hamilton et al., 2017; Veli\u010dkovi\u0107 et al., 2018). However, their effectiveness is often hampered by the vulnerability to variations in data distribution (Ji et al., 2023; Ding et al., 2021; Koh et al., 2021). This has sparked significant interest in developing GNNs capable of generalization from one domain (source domain $S$) to another, potentially different domain (target domain $T$). This field of study, known as graph domain adaptation (GDA), is gaining increasing attention.\\n\\nGDA distinguishes itself from the traditional domain adaptation setting, primarily because the data points in GDA are interlinked rather than independent. This non-IID nature of graph data renders traditional domain adaptation techniques suboptimal when applied to graphs. The distribution shifts in features, labels, and connecting patterns between objects may significantly impact the adaptation/generalization accuracy. Despite the recent progress made in GDA (Wu et al., 2020; You et al., 2023; Zhu et al., 2021; Liu et al., 2023), current solutions still struggle to tackle the various shifts prevalent in real-world graph data. We provide a detailed discussion of the limitations of existing GDA methods in Section 2.2.\\n\\nThis work conducts a systematic study of the distinct challenges present in GDA and proposes a novel method, named Pairwise Alignment (Pair-Align) to tackle graph structure shift for node prediction tasks. Combined with feature alignment...\"}"}
{"id": "ttnbM598vZ", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\nCitation Dataset\\n\\nStructure Shift\\n\\nLabel Shift (LS)\\n\\nHEP: OC vs. LC portion is larger in compared to Citation:...\\n\\nConditional Structure Shift (CSS)\\n\\nCitation:...\\n\\nHEP Dataset\\n\\nparticles to infer (unknow)\\n\\nFigure 1. We illustrate structure shifts in real-world datasets: a) The HEP dataset in pileup mitigation tasks (Bertolini et al., 2014) has a shift in PU levels (change in the number of other collisions (OC) around the leading collision (LC) for proton-proton collision events), where \\\\( G_S \\\\) is in PU30 and \\\\( G_T \\\\) is in PU10; Here, in the green circles, the center nodes in grey are the particles whose labels are to be inferred. They have different ground-truth labels but the same neighborhood that includes one OC and one LC particle. b) The citation MAG dataset shifts in regions, where the source graph contains papers in the US and the target graph contains papers in German. More statistics on graph distribution shift from real-world examples can be found in Appendix E.5.\\n\\nment methods offered by traditional non-graph DA tech-\\nniques (Ganin et al., 2016; Tachet des Combes et al., 2020), Pair-Align can in principle address a wide range of distribu-\\ntion shifts in graph data.\\n\\nOur analysis begins with examining a graph with its adja-\\ncency matrix \\\\( A \\\\) and node labels \\\\( Y \\\\). We observe that graph\\nstructure shift (\\\\( P_S(A, Y) \\\\neq P_T(A, Y) \\\\)) typically mani-\\nfests as either conditional structure shift (CSS) or label shift (LS), or a combination of both. CSS refers to the change in\\nneighboring connections among nodes within the same class (\\\\( P_S(A | Y) \\\\neq P_T(A | Y) \\\\)) whereas LS denotes changes in\\nthe class distribution of nodes (\\\\( P_S(Y) \\\\neq P_T(Y) \\\\)). These\\nshifts are illustrated in Fig. 1 via examples in HEP and so-\\ncial networks, and are justified by statistics from several\\nreal-world applications.\\n\\nIn light of the two types of shifts, the Pair-Align method\\naims to estimate and subsequently mitigate the distribu-\\ntion shift in the neighboring nodes' representations for any\\ngiven node class \\\\( c \\\\). To achieve this, Pair-Align employs a\\nbootstrapping technique to recalibrate the influence of neigh-\\nboring nodes in the message aggregation phase of GNNs.\\nThis strategic reweighting is key to effectively countering\\nCSS. Concurrently, Pair-Align calculates label weights to\\nalleviate disparities in the label distribution between source\\nand target domains (addressing LS) by adjusting the classi-\\nfication loss. Pair-Align is depicted in Figure 2.\\n\\nTo demonstrate the effectiveness of our pipeline, we curate\\nthe regional MAG data that partitions large citation networks\\naccording to the regions where papers got published (Hu\\net al., 2020; Wang et al., 2020) to simulate the region shift.\\nTo the best of our knowledge, this is the largest dataset (of\\n\\\\( \\\\approx 380k \\\\) nodes, 1.35M edges) to study GDA with data retrieved\\nfrom the real-world database. We also include other graph\\ndata with shifts, like the pileup mitigation task studied in Liu\\net al. (2023). Our method shows strong performance in these\\ntwo applications. Moreover, our method also outperforms\\nbaselines significantly in synthetic datasets and other real-\\nworld benchmark datasets.\\n\\n2. Preliminaries and Related Works\\n\\n2.1. Notation and The Problem Setup\\n\\nWe use capital letters, e.g., \\\\( Y \\\\) to denote scalar random vari-\\nables, and lower-case letters, e.g., \\\\( y \\\\) to denote their realiza-\\ntions. The bold counterparts are used for their vector-valued\\ncorrespondences, e.g., \\\\( Y, y \\\\), and the calligraphic letters,\\ne.g. \\\\( Y \\\\), are for the value spaces. We always use capital let-\\nters to denote matrices. Let \\\\( P \\\\) denote a distribution, whose\\nsubscript \\\\( U \\\\in \\\\{ S, T \\\\} \\\\) indicates the domain it depicts, e.g.\\n\\\\( P_S(Y) \\\\). The probability of a realization, e.g. \\\\( Y = y \\\\), can\\nthen be denoted as \\\\( P_S(Y = y) \\\\).\\n\\nGraph Neural Networks (GNNs). We use \\\\( G = (V, E, x) \\\\)\\nto denote a graph with the node set \\\\( V \\\\), the edge set \\\\( E \\\\) and\\nnode features \\\\( x = [ \\\\cdots x_u \\\\cdots ]_{u \\\\in V} \\\\). We focus on undi-\\nrected graphs where the graph structure can also be rep-\\nresented as a symmetric adjacency matrix \\\\( A \\\\) where the\\nentries \\\\( A_{uv} = A_{vu} = 1 \\\\) when nodes \\\\( u, v \\\\) form an edge and\\notherwise 0. GNNs take \\\\( A \\\\) and \\\\( x \\\\) as input and output node\\nrepresentations \\\\( \\\\{ h_u, \\\\forall u \\\\in V \\\\} \\\\). The standard GNNs (Hamil-\\nton et al., 2017) has a message-passing procedure. Specif-\\nically, with \\\\( h^{(1)}_u = x_u \\\\), for each node \\\\( v \\\\) and each layer\\n\\\\( k \\\\in [L] := \\\\{1, \\\\ldots, L\\\\} \\\\),\\n\\\\( h^{(k+1)}_u = UPT(h^{(k)}_u, AGG(\\\\{ \\\\{h^{(k)}_v : v \\\\in N_u\\\\} \\\\})) \\\\),\\nwhere \\\\( N_v \\\\) denotes the set of neighbors of node \\\\( v \\\\) and \\\\( \\\\{\\\\{\\\\cdot\\\\}\\\\} \\\\)\\ndenotes a multiset. The AGG function aggregates messages\\nfrom the neighbors, and the UPT function updates the node\\nrepresentations. The last-layer node representation\\n\\\\( h^{(L)}_u \\\\) is\\nused to predict the label \\\\( y_u \\\\in Y \\\\) in node classification tasks.\\n\\nDomain Adaptation (DA). In DA, each domain \\\\( U \\\\in \\\\{ S, T \\\\} \\\\) has its own joint feature and label distribution\\n\\\\( P_U(X, Y) \\\\). In the unsupervised setting, we have access\\nto labeled source data \\\\( \\\\{(x_i, y_i)\\\\}_{i=1}^N \\\\) and unlabeled target\\ndata \\\\( \\\\{(x_i)\\\\}_{i=1}^M \\\\) IID sampled from the source and target do-\\nmain respectively. The model comprises a feature encoder\\n\\\\( \\\\phi : X \\\\rightarrow H \\\\) and a classifier \\\\( g : H \\\\rightarrow Y \\\\), with classification\\nerror in domain \\\\( U \\\\) denoted as \\\\( \\\\varepsilon_U(g \\\\circ \\\\phi) = P_U(g(\\\\phi(X)) \\\\neq Y) \\\\).\\nThe objective is to train the model with available data to\"}"}
{"id": "ttnbM598vZ", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\n1. Weight edges by \\n2. Training\\nGNN encoder\\nand Classifier \\n3. Optimize \\n   (message from\\nclass 0 node) \\n3. Optimize \\n   Weight the\\nclassification loss\\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\n   \\"}
{"id": "ttnbM598vZ", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We first define shifts in graphs as feature shift and structure shift. Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To remove the effect of CSS under GNN, the objective of solving structure shift and propose our pairwise alignment algorithm that handles both CSS and LS. To address various distribution shifts for GDA in principle, we first decouple the potential distribution shifts in graph (CSS) and the Label Shift (LS). Then, we analyze the objective of solving structure shift and propose our pairwise"}
{"id": "ttnbM598vZ", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1) the distribution of the degree/cardinality of the neighbors without the adjusted edge weights for the source and target, where\\n\\nAligning the distributions of the node degrees\\n\\nTherefore, our method focuses on aligning the distribution\\n\\nThis theorem reveals that it suffices to align two distributions with mean pooling (Xu et al., 2018).\\n\\nAssume \\\\( \\\\gamma \\\\) and then apply GNN encoding as\\n\\nEmotionally, we estimate\\n\\nLemma 3.8. Assume \\\\( \\\\gamma \\\\), we define\\n\\nLemma 3.2), Lemma 3.8 shows that\\n\\n\\\\( \\\\nu \\\\) stands for the joint distribution of the predicted types of edges and the true types of edges, and\\n\\nnode representations are conditionally independent of\\n\\nIf \\\\( \\\\nu \\\\), Lemma 3.8 shows that\\n\\n\\\\( \\\\nu \\\\) is the label distribution of the end node conditioned on an edge, while the latter is the label distribution of nodes with-\\n\\nis the label distribution of the end node conditioned on an edge, while the latter is the label distribution of nodes with-\\n\\nAccording to Definition 3.7, we estimate\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\\n\\n\\\\( \\\\nu \\\\) is satisfied. Given\"}"}
{"id": "ttnbM598vZ", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use $\\\\beta$ can be symmetric, so we may add an extra constraint $w$ as edge weights on $\\\\gamma$ we begin by employing an estimated $w$ with the obtained $\\\\gamma$, where the constraints guarantee a valid target edge type.\\n\\nThe key insight is similar to the estimation of $\\\\gamma$.\\n\\n### 3.3. Addressing Label Shift\\n\\nCSS, our algorithm completely addresses the structure shift. Combined with the previous module that uses $\\\\gamma$ to solve for the source domain, then\\n\\nEach other given the node labels, then $\\\\mu$ and node representations are conditionally independent of $\\\\nu$.\\n\\nLemma 3.11. If $\\\\mu$ is estimated by solving a linear system $P_\\\\gamma = \\\\gamma w$ with the constraints to guarantee a valid target label distribution can be solved with a least square problem with the constraints $\\\\gamma w$ can estimate $\\\\mu$ and $\\\\nu$.\\n\\nInspired by the techniques in Lipton et al. (2018); Azizshaheli et al. (2018), we estimate the ratio between the source and target label distribution by aligning the node-representations that further pass through the classifier for the source domain, and (5), respectively. Specifically, Eq. (4) and (5) can be addressed to CSS.\\n\\nIn summary, handling CSS is an iterative process where $\\\\hat{w}$ progressively enhances the conditional alignment, thus improve the estimation of $\\\\gamma$ and $\\\\nu$.\\n\\n### 3.4. Algorithm Overview\\n\\nThe algorithm is shown in Alg. 1. At the start of each epoch, $\\\\hat{w}$ get soft labels $\\\\hat{Y}$ for $\\\\hat{S}$, revise the estimated $\\\\gamma$ and $\\\\nu$.\\n\\nNow, we are able to put everything together. The entire algorithm is shown in Alg. 1. At the start of each epoch, update the estimation of $\\\\gamma$ and $\\\\nu$.\\n\\nThe source graph $\\\\hat{G}$ is revised as $\\\\hat{G}_t$, $\\\\hat{G}$ is a $\\\\min_{n_e = n} \\\\beta$-weighted cross-entropy loss, and $\\\\hat{G}_{tgt}$ is the target graph.\\n\\nLet $\\\\hat{G}$ the estimated $\\\\gamma$ is obtained via solving a linear system $P_\\\\gamma = \\\\gamma w$ with the constraints to guarantee a valid target label distribution. Finally, we calculate $\\\\hat{Y}_{tgt} = \\\\hat{Y}_t$ as the weights of the source and target classes. This may lead to ill-conditioned $\\\\hat{X}$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\nTypically, node classification tends to have imperfect accuracy and results in similar prediction probabilities across classes. This may lead to ill-conditioned $\\\\hat{X}$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\nLet $\\\\hat{G}$ the estimated $\\\\gamma$ is obtained via solving a linear system $P_\\\\gamma = \\\\gamma w$ with the constraints to guarantee a valid target label distribution. Finally, we calculate $\\\\hat{Y}_{tgt} = \\\\hat{Y}_t$ as the weights of the source and target classes. This may lead to ill-conditioned $\\\\hat{X}$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4) and (5) can be used to improve robustness of the estimation, we incorporate L2 regularization into the least square optimization for $\\\\mu$.\\n\\n$\\\\hat{X}$ is satisfied, $\\\\hat{X}$ and $\\\\hat{Y}$, respectively. Specifically, Eq. (4)"}
{"id": "ttnbM598vZ", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pairwise Alignment Improves Graph Domain Adaptation\\n\\nto be important in the early training stage and can guide a better weight estimation in the later stage.\\n\\nWe also introduce a regularization strategy to improve the robustness of $\\\\gamma$. This is to deal with the variance in edge formation that may affect $P_U(Y_v|Y_u, v \\\\in N_u)$ in $\\\\gamma$ calculation.\\n\\nTake a specific example to demonstrate the idea of regularizing $\\\\gamma$. Suppose node labels are binary and suppose we count the numbers of edges of different types in the source graph and obtain\\n\\n$\\\\hat{P}_S(Y_u=0, Y_v=0|e_{uv} \\\\in E_S) = 0.001$ and $\\\\hat{P}_S(Y_u=0, Y_v=1|e_{uv} \\\\in E_S) = 0.0005$.\\n\\nThen without any regularization, based on the estimated edge-type distributions, we obtain\\n\\n$\\\\hat{P}_S(Y_v=0|Y_u=0, v \\\\in N_u) = 2/3$ and $\\\\hat{P}_S(Y_v=0|Y_u=0, v \\\\in N_u) = 1/3$.\\n\\nHowever, the estimation $\\\\hat{P}_S(Y_v=i, Y_u=j|e_{uv} \\\\in E_S)$ may be inaccurate when its value is close to 0. Because in this case, the number of edges of the corresponding type $(i, j)$ is too small in the graph. These edges may be formed based on randomness. Conversely, larger observed values like $\\\\hat{P}_S(Y_u=0, Y_v=0|e_{uv} \\\\in E_S) = 0.2$ and $\\\\hat{P}_S(Y_u=0, Y_v=1|e_{uv} \\\\in E_S) = 0.1$ are often more reliable. To address the issue, we may introduce a regularization term $\\\\delta$ when using $w$ to compute $\\\\gamma$.\\n\\n$\\\\hat{P}_S(Y_u=i, Y_v=j|e_{uv} \\\\in E_S) + \\\\delta \\\\hat{P}_S(Y_u=i, Y_v=j|e_{uv} \\\\in E_S) + \\\\delta = [w]_{ij}$\\n\\nand replace $w$ with $w'$ when computing $\\\\gamma$.\\n\\n3.6. Comparison to StruRW (Liu et al., 2023)\\n\\nThe edge weights estimation in StruRW and Pair-Align differ in two major points. First, StruRW computes edge weights as the ratio of the source and target edge connection probabilities. This by definition, if using our notations, corresponds to $w$ instead of $\\\\gamma$ and ignores the effect of $\\\\alpha$. However, Thm 3.3 shows that using $\\\\gamma$ is the key to reduce CSS. Second, even for the estimation of $w$, StruRW suffers from inaccurate estimation. In our notation, StruRW simply assumes that $P_S(\\\\hat{Y}=i|Y=i) = 1, \\\\forall i \\\\in Y$, i.e., perfect training in the source domain and uses hard pseudo-labels in the target domain to estimate $w$. In contrast, our optimization to obtain $w$ is more stable. Moreover, StruRW ignores the effect of LS entirely. From this perspective, StruRW can be understood as a special case of Pair-Align under the assumption of no LS and perfect prediction in the target graph. Furthermore, our work is the first to rigorously formulate the idea of conditional alignment in graphs.\\n\\n4. Experiments\\n\\nWe evaluate three variants of Pair-Align to understand how its different components deal with the distribution shift on synthetic datasets and 5 real-world datasets. These variants include PA-CSS with only $\\\\gamma$ as source graph edge weights to address CSS, PA-LS with only $\\\\beta$ as label weights to address LS, and PA-BOTH that combines both. We next briefly introduce datasets and settings while leaving more details in Appendix E.\\n\\n4.1. Datasets and Experimental Settings\\n\\nSynthetic Data. CSBMs (see the definition in Appendix A) are used to generate the source and target graphs with three node classes. We explore four scenarios in structure shift without feature shift, where the first three explore CSS with shifts in the conditional neighboring node's label distribution (class ratio), shifts in the conditional node's degree distribution (degree), and shifts in both. Considering these three types of shift is inspired by the argument in Thm 3.3. The fourth setting examines CSS and LS jointly. In addition, we consider two degrees of shift under each scenario with the left column being the smaller shift as shown in Table 3. The detailed configurations of the CSBM regarding edge probabilities and node features are in Appendix E.2.\\n\\nMAG. We extract paper nodes and their citation links from the original MAG (Hu et al., 2020; Wang et al., 2020). Papers are split into separate graphs based on their countries of publication determined by their corresponding authors. The task is to classify the publication venue of the papers. Our experiments study generation across the top 6 countries with the most number of papers (in total 377k nodes, 1.35M edges). We train models on the graphs from US/China and test them on the graphs from the rest countries.\\n\\nPileup Mitigation (Liu et al., 2023) is a dataset of a de-noising task in HEP named pileup mitigation (Bertolini et al., 2014). Proton-proton collisions produce particles with leading collisions (LC) and nearby bunch crossings as other collisions (OC). The task is to identify whether a particle is from LC or OC. Nodes are particles and particles are connected if they are close in the $\\\\eta$-$\\\\phi$ space. We study two distribution shifts: the shift of pile-up (PU) conditions (mostly structure shift), where PU$_k$ indicates the averaged number of other collisions in the beam is $k$, and the shift in the data generating process (primarily feature shift).\\n\\nArxiv (Hu et al., 2020) is a citation network of Arxiv papers to classify papers' subject areas. We study the shift in time by using papers published in earlier periods to train and test on papers published later. Specifically, we train on papers published from 1950 to 2007/ 2009/ 2011 and test on paper published between 2014 to 2016 and 2016 to 2018.\\n\\nDBLP and ACM (Tang et al., 2008; Wu et al., 2020) are two paper citation networks obtained from DBLP and ACM. Nodes are papers and edges represent citations between papers. The goal is to predict the research topic of a paper. We train the GNN on one network and test it on the other.\\n\\nBaselines. DANN (Ganin et al., 2016) and IWDAN (Tang et al., 2018) are baseline methods for domain adaptation.\"}"}
