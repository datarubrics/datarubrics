{"id": "garg23a", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm          | Source w/o aug (RW) | Source w/o aug | Source (w aug) (RW) | NoisyStudent (RS+RW) | CDANN (RS+RW) | FixMatch (RS+RW) | IW-CDANN | DANN (RS+RW) | BN-adapt | TENT | SENTRY |\\n|--------------------|---------------------|---------------|--------------------|----------------------|---------------------|----------------|----------|----------------|----------|------|--------|\\n|                   | Avg. Accuracy       |               |                    |                      |                     |                |          |                |          |      |        |\\n|                   | 50                  | 55            | 60                 | 65                   | 70                  | 75             | 80       |                |          |      |        |\\n| Relative Accuracy  |                     |               |                    |                      |                     |                |          |                |          |      |        |\\n|                   | None                | 10.0          | 3.0                | 1.0                  | 0.5                 |                |          |                |          |      |        |\\n| Dirichlet Shift   | \u221210^2               | \u221210^1         | \u221210^0              | 0                    | 100                 | 10^1           | 10^2     |                |          |      |        |\\n|                   |                     |               |                    |                      |                     |                |          |                |          |      |        |\\n|                   |                     |               |                    |                      |                     |                |          |                |          |      |        |\\n|                   |                     |               |                    |                      |                     |                |          |                |          |      |        |\\n|                   |                     |               |                    |                      |                     |                |          |                |          |      |        |\\n|                   |                     |               |                    |                      |                     |                |          |                |          |      |        |\"}"}
{"id": "garg23a", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm       | Source (w/o aug) | Source (w aug) | Source (adv) | Source (adv) (RW) | SENTRY | CORAL | CORAL (RS+RW) | Source (w/o aug) (RW) | Source (w aug) (RW) | BN-adapt | BN-adapt (RS+RW) | TENT | TENT (RS+RW) | DANN | DANN (RS+RW) | CDANN | CDANN (RS+RW) | FixMatch | FixMatch (RS+RW) | NoisyStudent | NoisyStudent (RS+RW) | IW-Correlation | IW-CDANN | IW-DANN |\\n|-----------------|-----------------|----------------|-------------|------------------|--------|-------|---------------|------------------------|---------------------|-----------|------------------|------|--------------|------|--------------|-------|---------------|----------|------------------|------------|-----------------|-----------------|---------|---------|\\n|                 | Avg. Accuracy   | Relative Accuracy |             |                  |        |       |               |                        |                     |           |                  |      |              |      |              |        |               |          |                 |            |                 |                 |         |         |\\n|                 |                 |                 |             |                  |        |       |               |                        |                     |           |                  |      |              |      |              |        |               |          |                 |            |                 |                 |         |         |\\n|                 |                 |                 |             |                  |        |       |               |                        |                     |           |                  |      |              |      |              |        |               |          |                 |            |                 |                 |         |         |\\n|                 |                 |                 |             |                  |        |       |               |                        |                     |           |                  |      |              |      |              |        |               |          |                 |            |                 |                 |         |         |\\n|                 |                 |                 |             |                  |        |       |               |                        |                     |           |                  |      |              |      |              |        |               |          |                 |            |                 |                 |         |         |\\n|                 |                 |                 |             |                  |        |       |               |                        |                     |           |                  |      |              |      |              |        |               |          |                 |            |                 |                 |         |         |\"}"}
{"id": "garg23a", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 25. Nonliving 26. Relative performance and accuracy plots for different DA algorithms across various shift pairs in Nonliving26.\"}"}
{"id": "garg23a", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 26. DomainNet. Relative performance and accuracy plots for different DA algorithms across various shift pairs in DomainNet.\"}"}
{"id": "garg23a", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use four domains (clipart, painting, real, sketch) from the DomainNet dataset (Peng et al., 2019). We use the real domain as the source and the other domains as targets.\\n\\nWe use three domains (train, val, and test) from the Visda dataset (Peng et al., 2018). While the 'train' domain contains synthetic renditions of the objects, the 'val' and 'test' domains contain real-world images. To avoid confusing domain names with their roles as splits, we rename them as 'synthetic', 'Real-1', and 'Real-2'. We use the synthetic (original train set) as the source domain and use the other domains as targets.\\n\\nWe use data from the wilds benchmark which includes three domains: train, OOD val, and OOD test, for toxicity detection with domains corresponding to different demographic subpopulations. The dataset has subpopulation shift across different demographic groups as the dataset in each domain is collected from a different partition of online articles.\\n\\nWe also use data from the Retiring Adults (Ding et al., 2021) where we consider the ACSIncome prediction task with various domains representing different states and time-periods. We randomly select three states and consider the dataset due to shifting time across those states. Details about the precise time-periods and states are in Table 1.\\n\\nWe use data from the Mimic Readmission (Johnson et al., 2020; PhysioBank, 2000) where the task is to predict readmission risk with various domains representing data from different time-periods. Details about the precise time-periods are in Table 1.\\n\\nWe provide scripts to setup these datasets with a single command in our code. To investigate the performance of different methods under the stricter label shift setting, we also include a hold-out partition of the source domain in the set of target domains. For these distribution shift pairs where source and target domains are i.i.d. partitions, we obtain the stricter label shift problem. We summarize the information about the source and target domains in Table 1.\\n\\nTrain-test splits\\nWe partition each source and target dataset into 80% and 20% i.i.d. splits. We use 80% splits for training and 20% splits for evaluation (or validation). We throw away labels for the 80% target split and only use labels in the 20% target split for final evaluation. The rationale behind splitting the target data is to use a completely unseen batch of data for evaluation. This avoids evaluating on examples where a model potentially could have overfit.\\n\\nOver-fitting to unlabeled examples during evaluation. In practice, if the aim is to make predictions on all the target data (i.e., transduction), we can simply use the (full) target set for training and evaluation.\\n\\nE. Illustration of Our Proposed Meta-algorithm\\nDA Algorithm\\nSample Class\\nbalanced\\nPredict with $f$\\n\\nDA Algorithm\\nSample Class\\nbalanced\\nPredict with $f$\\n\\nTarget Marginal Estimation\\n$f$\\n\\nRe-weight classifier\\n$f$\\n\\nFigure 9.\\n(left) Illustration of RS method at every iteration.\\n(right) Illustration of post-hoc reweighting of the classifier with RW method.\\n\\nF. Methods to estimate target marginal under the stricter label shift assumption\\nIn this section, we describe the methods proposed to estimate the target label marginal under the stricter label shift assumption. Recall that under the label shift assumption, $p_s$ $p_y$ $q$ can differ from $p_t$ $p_y$ $q$ but the class conditional stays the same, i.e., $p_t \\\\ p_x \\\\ | \\\\ y \\\\ q = p_s \\\\ p_x \\\\ | \\\\ y \\\\ q$. We focus our discussion on recent methods that leverage off-the-shelf classifiers to yield consistent estimates under mild assumptions (Lipton et al., 2018; Azizzadenesheli et al., 2019; Alexandari et al., 2021; Garg et al., 2020). For simplicity, we assume we possess labeled source data $t$, $x_1$, $y_1$, $q$, $p_2$, $y_2$ $q$, ..., $p_n$, $y_n$ $q$ and unlabeled target data $x_n$, $x_n`, x_n`, ..., $x_n`$.\"}"}
{"id": "garg23a", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\nTo estimate the shift, RLLS solves the following optimization problem to estimate the importance weights $w_t$:\\n\\n$$\\\\arg\\\\min_w \\\\mathbb{P}_{\\\\mathbf{w}} RLLS \\\\quad \\\\mathcal{P} \\\\quad \\\\mathcal{C} \\\\quad \\\\mathcal{f}_w \\\\quad \\\\mathcal{\\\\mu}_f \\\\quad \\\\lambda_{RLLS} || \\\\mathcal{w} ||_1^2.$$ (1)\\n\\nwhere $W_t$ is the same constrained set defined above. We can again estimate the target label marginal by simply multiplying the estimated importance weights with the source label marginal.\\n\\nG. Theoretical Definition for Relaxed Label Shift\\n\\nDomain adaptation problems are, in general, ill-posed (Ben-David et al., 2010b). Several attempts have been made to investigate additional assumptions that render the problem well-posed. One such example includes the label-shift setting, where $p_{x|y}$ does not change but that $p_y$ can. Under label shift, two challenges arise: (i) estimate the target label marginal $p_{t,y}$; and (ii) train a classifier $f$ to maximize the performance on the target domain. However, these assumptions are typically, to some degree, violated in practice. This paper aims to relax this assumption and focuses on relaxed label shift setting. In particular, we assume that the label distribution can shift from source to target arbitrarily but that $p_{x|y}$ varies between source and target in some comparatively restrictive way (e.g., shifts arising naturally in the real world like ImageNet (Russakovsky et al., 2015) to ImageNetV2 (Recht et al., 2019)).\\n\\nMathematically, we assume a divergence-based restriction on $p_{x|y}$, i.e., for some small $\\\\epsilon > 0$ and distributional distance $D$, we have\\n\\n$$\\\\max_y D(p_{t,x|y}, p_{t,x|y}) \\\\leq \\\\epsilon$$\\n\\nbut allowing an arbitrary shift in the label marginal $p_y$. Previous works have defined these constraints in different ways (Wu et al., 2019; Tachet des Combes et al., 2020; Kumar et al., 2020).\"}"}
{"id": "garg23a", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In particular, we can use Wasserstein-infinity distance to define our constraint. First, we define Wasserstein given probability measures $p$, $q$ on $\\\\mathbb{X}$:\\n\\n$$W_\\\\infty(p, q) = \\\\inf_{f: \\\\mathbb{R}^d \\\\to \\\\mathbb{R}^d} \\\\|f \\\\# p - q\\\\|_\\\\infty,$$\\n\\nwhere $\\\\#$ denotes the push forward of a measure, i.e., for every set $S \\\\subseteq \\\\mathbb{R}^d$, $p \\\\|_S q = p \\\\| f^{-1}(S) q$. Intuitively, $W_\\\\infty$ moves points from the distribution $p$ to $q$ by distance at most $\\\\epsilon$ to match the distributions. Hence, our $D_\\\\epsilon$ is\\n\\n$$\\\\max_y W_\\\\infty(p \\\\|_y s, p \\\\|_y t) \\\\leq \\\\epsilon.$$\\n\\nSimilarly, we can define our distribution constraint in KL or TV distances. We can define our constraint in a representation space $\\\\mathbb{Z}$ obtained by projection inputs $x \\\\in \\\\mathbb{X}$ with a function $h: \\\\mathbb{X} \\\\to \\\\mathbb{Z}$. Intuitively, we want to define the distribution distance with some $h$ that captures all the required information for predicting the label of interest but satisfies a small distributional divergence in the projected space. However, in practice, it's hard to empirically verify these distribution distances for small enough $\\\\epsilon$ with finite samples. Moreover, we lack a rigorous characterization of the sense in which those shifts arise in popular DA benchmarks, and since, the focus of our work is on the empirical evaluation with real-world datasets, we leave a formal investigation for future work.\\n\\nH. Target Marginal Estimation and its Effect on Accuracy\\n\\nH.1. Vision Datasets\"}"}
{"id": "garg23a", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\nH.2. Tabular Datasets\\n\\nDirichlet Shift (\u03b1)\\n\\nEstimation Error\\n\\nTabular Datasets\\n\\nSource\\n\\nDANN (None)\\n\\nDANN (RS)\\n\\nNone 10.0 3.0 1.0 0.5\\n\\nDirichlet Shift (\u03b1)\\n\\n0.0\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\n1.2\\n\\nEstimation Error\\n\\n(a) Target label marginal estimation ($\\\\ell_1$) error with RLLS and classifiers obtained with different DA methods\\n\\nRelative Accuracy\\n\\nTabular Datasets\\n\\nSource (RW)\\n\\nDANN (RW)\\n\\nDANN (RS + RW)\\n\\nNone 10.0 3.0 1.0 0.5\\n\\nDirichlet Shift (\u03b1)\\n\\n\u2212102\\n\\n\u2212101\\n\\n\u2212100\\n\\n0\\n\\n100\\n\\n101\\n\\n102\\n\\nRelative Accuracy\\n\\n(b) Relative performance of DA methods when paired with RW corrections\\n\\nFigure 11.\\n\\nFor tabular datasets, RLLS with classifiers obtained with DA methods improves over RLLS with a source-only classifier for severe target label marginal shifts. Correspondingly for severe target label marginal shifts, we see improved performance with post-hoc RW correction applied to classifiers trained with DA methods as compared to when applied to source-only models.\\n\\nH.3. NLP Datasets\\n\\nDirichlet Shift (\u03b1)\\n\\nEstimation Error\\n\\nLanguage Datasets\\n\\nSource\\n\\nDANN (None)\\n\\nDANN (RS)\\n\\nNone 10.0 3.0 1.0 0.5\\n\\nDirichlet Shift (\u03b1)\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\n0.25\\n\\n0.30\\n\\nEstimation Error\\n\\nLanguage Datasets\\n\\nSource\\n\\nCDANN (None)\\n\\nCDANN (RS)\\n\\nNone 10.0 3.0 1.0 0.5\\n\\nDirichlet Shift (\u03b1)\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\n0.25\\n\\n0.30\\n\\nEstimation Error\\n\\nLanguage Datasets\\n\\nSource\\n\\nPseudoLabel (None)\\n\\nPseudoLabel (RS)\\n\\nNone 10.0 3.0 1.0 0.5\\n\\nDirichlet Shift (\u03b1)\\n\\n0.00\\n\\n0.05\\n\\n0.10\\n\\n0.15\\n\\n0.20\\n\\n0.25\\n\\n0.30\\n\\nEstimation Error\\n\\nLanguage Datasets\\n\\nSource\\n\\nPseudoLabel (None)\\n\\nPseudoLabel (RS)\\n\\nNone 10.0 3.0 1.0 0.5\\n\\nDirichlet Shift (\u03b1)\\n\\n\u2212102\\n\\n\u2212101\\n\\n\u2212100\\n\\n0\\n\\n100\\n\\n101\\n\\n102\\n\\nRelative Accuracy\\n\\nLanguage Datasets\\n\\nSource (RW)\\n\\nDANN (RW)\\n\\nDANN (RS + RW)\\n\\nNone 10.0 3.0 1.0 0.5\\n\\nDirichlet Shift (\u03b1)\\n\\n\u2212102\\n\\n\u2212101\\n\\n\u2212100\\n\\n0\\n\\n100\\n\\n101\\n\\n102\\n\\nRelative Accuracy\\n\\nLanguage Datasets\\n\\nSource (RW)\\n\\nCDANN (RW)\\n\\nCDANN (RS + RW)\\n\\nNone 10.0 3.0 1.0 0.5\\n\\nDirichlet Shift (\u03b1)\\n\\n\u2212102\\n\\n\u2212101\\n\\n\u2212100\\n\\n0\\n\\n100\\n\\n101\\n\\n102\\n\\nRelative Accuracy\\n\\nLanguage Datasets\\n\\nSource (RW)\\n\\nPseudoLabel (RW)\\n\\nPseudoLabel (RS + RW)\\n\\nNone 10.0 3.0 1.0 0.5\\n\\nDirichlet Shift (\u03b1)\\n\\n\u2212102\\n\\n\u2212101\\n\\n\u2212100\\n\\n0\\n\\n100\\n\\n101\\n\\n102\\n\\nRelative Accuracy\\n\\nLanguage Datasets\\n\\nSource (RW)\\n\\nPseudoLabel (RW)\\n\\nPseudoLabel (RS + RW)\\n\\nFigure 12.\\n\\nFor NLP datasets, RLLS with source-only classifiers performs better than RLLS with classifiers obtained with DA methods. Correspondingly, we see improved performance with post-hoc RW correction applied to source-only models over classifiers trained with DA methods.\"}"}
{"id": "garg23a", "page_num": 49, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 6.\\n\\n| Dataset     | Epoch | Batch Size | \\\\(\\\\ell^2\\\\) Regularization | Learning Rate | Details |\\n|-------------|-------|------------|-----------------------------|--------------|---------|\\n| CIFAR10     | 50    | 200        |                             | 0.0001       |         |\\n| CIFAR100    | 50    | 200        |                             | 0.01         |         |\\n| Camelyon    | 10    | 96         |                             | 0.01         |         |\\n| FMoW        | 30    | 64         |                             | 0.0001       |         |\\n| Entity13    | 40    | 256        | 5e-5 (chosen from \\\\(10^{-5}, 10^{-4}, 10^{-3}\\\\)) | 0.2          |         |\\n| Entity30    | 40    | 256        | 5e-5 (chosen from \\\\(10^{-5}, 10^{-4}, 10^{-3}\\\\)) | 0.2          |         |\\n| Living17    | 40    | 256        | 5e-5 (chosen from \\\\(10^{-5}, 10^{-4}, 10^{-3}\\\\)) | 0.2          |         |\\n| Nonliving26 | 40    | 256        | 0 5e-5 (chosen from \\\\(10^{-5}, 10^{-4}, 10^{-3}\\\\)) | 0.2          |         |\\n| Officehome  | 50    | 96         | 0.0001 (chosen from \\\\(10^{-5}, 10^{-4}, 10^{-3}\\\\)) | 0.01         |         |\\n| DomainNet   | 15    | 96         | 0.0001 (chosen from \\\\(10^{-5}, 10^{-4}, 10^{-3}\\\\)) | 0.01         |         |\\n| Visda       | 10    | 96         | 0.0001 (chosen from \\\\(10^{-5}, 10^{-4}, 10^{-3}\\\\)) | 0.01         |         |\\n| Civilcomments | 5     | 32         | 0.01 (chosen from \\\\(10^{-5}, 10^{-4}, 10^{-3}\\\\)) | 2e-5         |         |\\n| Retiring Adults | 50   | 200        | 0.0001 (chosen from \\\\(10^{-5}, 10^{-4}, 10^{-3}\\\\)) | 0.01         |         |\\n| Mimic Readmissions | 100 | 128       | 0.0 (chosen from \\\\(10^{-5}, 10^{-4}, 10^{-3}\\\\)) | 5e-4         |         |\\n\\n**Recent works (Deng & Zheng, 2021; Guillory et al., 2021; Chen et al., 2021; Jiang et al., 2021; Baek et al., 2022; Garg et al., 2022b)** have proposed numerous heuristics to predict classifier performance under distribution shift. Analyzing the usefulness of these heuristics for hyperparameter selection is an interesting avenue for future work.\\n\\n### M.3. Compute Infrastructure\\n\\nOur experiments were performed across a combination of Nvidia T4, A6000, P100 and V100 GPUs. Overall, to run the entire RLSBENCH suite on a T4 GPU machine with 8 CPU cores we would approximately need 70k GPU hours of compute.\\n\\n### M.4. Data Augmentation\\n\\nIn our experiments, we leverage data augmentation techniques that encourage robustness to some variations between domains for vision datasets. For weak augmentation, we leverage random horizontal flips and random crops of pre-defined size. For strong augmentation, we apply the following transformations sequentially: random horizontal flips, random crops of pre-defined size, augmentation with Cutout (DeVries & Taylor, 2017), and RandAugment (Cubuk et al., 2020). For the exact implementation of RandAugment, we directly use the implementation of Sohn et al. (2020). The pool of operations includes: autocontrast, brightness, color jitter, contrast, equalize, posterize, rotation, sharpness, horizontal and vertical shearing, solarize, and horizontal and vertical translations. We apply \\\\(N = 2\\\\) random operations for all experiments.\\n\\n### N. Comparison with SENTRY on officehome dataset with different hyperparameters\\n\\nIn this section, we shed more light on the discrepancy observed between SENTRY results reported in the original paper (Prabhu et al., 2021) and our implementation. We note that for the main experiments on Officehome dataset, we used a batch size of 96 for all methods including SENTRY. However, SENTRY reported results with a batch size of 16 in their work. Hence, we re-run the SENTRY algorithm with a batch size of 16. To investigate the impact of the decreased batch size, we make a comparison with FixMatch (the best algorithm on Officehome in our runs) by re-running it with the decreased batch size.\"}"}
{"id": "garg23a", "page_num": 50, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Table 7 we report results on individual shift pairs in officehome. We observe that SENTRY improves over FixMatch for the default minor shift in the label distribution in the officehome dataset. However, as the shift severity increases we observe that SENTRY performance degrades. Overall, we observe that RS-FixMatch performs similar or superior to SENTRY on 3 out of 4 shift pairs in officehome.\\n\\nAlgorithm Alpha = None Alpha = 10.0 Alpha = 3.0 Alpha = 1.0 Alpha = 0.5 Avg\\nFixMatch 92.5 95.2 98.0 100.0 100.0 97.1\\nRS-FixMatch 92.5 96.4 98.0 100.0 100.0 97.4\\nSENTRY 93.0 94.0 98.0 83.3 87.5 91.2\\n\\n(a) Product to Product (in-distribution)\\nAlgorithm Alpha = None Alpha = 10.0 Alpha = 3.0 Alpha = 1.0 Alpha = 0.5 Avg\\nFixMatch 71.4 71.5 70.7 73.1 75.5 72.4\\nRS-FixMatch 74.7 74.0 72.1 73.1 70.4 72.9\\nSENTRY 78.1 78.0 75.1 71.7 65.3 73.6\\n\\n(b) Product to Real\\nAlgorithm Alpha = None Alpha = 10.0 Alpha = 3.0 Alpha = 1.0 Alpha = 0.5 Avg\\nFixMatch 41.5 44.0 44.2 48.4 39.4 43.5\\nRS-FixMatch 45.5 44.8 43.6 50.0 37.4 44.2\\nSENTRY 45.8 46.5 41.4 40.3 27.3 40.3\\n\\n(c) Product to ClipArt\\nAlgorithm Alpha = None Alpha = 10.0 Alpha = 3.0 Alpha = 1.0 Alpha = 0.5 Avg\\nFixMatch 54.4 51.3 54.7 57.3 55.9 54.7\\nRS-FixMatch 57.2 53.6 55.9 57.3 58.8 56.6\\nSENTRY 63.7 62.0 62.1 65.3 55.9 61.8\\n\\n(d) Product to Art\\nTable 7. Officehome results with batch size 16 instead of 96 used throughout our experiments.\\n\\nMore generally, across our runs, we also observed model training with SENTRY to be unstable. Investigating further, we observe that the maximization objective to enforce consistency cause instabilities. This behavior is specifically prevalent for experiments where we don't use the underlying model with pre-trained weights (for example, in BREEDs datasets).\"}"}
{"id": "garg23a", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 31. Mimic Readmissions. Relative performance and accuracy plots for different DA algorithms across various shift pairs in Mimic Readmissions.\"}"}
{"id": "garg23a", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset          | Source | DANN | IW-DANN | CDANN | IW-CDANN | PseudoLabel |\\n|------------------|--------|------|---------|-------|----------|-------------|\\n| Civilcomments    |        | 86.85| 86.62   | 86.95 | 86.91    | 87.16       |\\n|                  |        | 86.8 | 89.1    | 86.6  | 88.8     | 87.4        |\\n|                  |        | 86.8 | 86.9    | 89.0  | 86.9     | 88.9        |\\n|                  |        | 86.9 | 88.6    |       |          |             |\\n\\n**Table 2.** Results with different DA methods on NLP datasets aggregated across target label marginal shifts.\\n\\n| Dataset          | Source | DANN | IW-DANN | CDANN | IW-CDANN | PseudoLabel |\\n|------------------|--------|------|---------|-------|----------|-------------|\\n| Retiring Adult   |        | 77.44| 77.17   | 77.35 | 78.15    | 78.44       |\\n| Mimic Readmission|       | 57.57| 56.36   | 56.48 | 56.67    | 56.71       |\\n|                  |        | 57.6 | 59.0    | 56.4  | 55.1     | 57.3        |\\n|                  |        | 57.6 | 57.7    | 57.7  | 57.9     |             |\\n\\n**Table 3.** Results with different DA methods on tabular datasets aggregated across target label marginal shifts.\\n\\n| Dataset            | Source (wo aug) | Source (w aug) |\\n|--------------------|-----------------|----------------|\\n| BN-adapt TENT      | DANN            | IW-DANN        |\\n| Fix-Match          | Noisy-Student   | Sentry         |\\n\\n**Table 4.** Results with different DA methods on vision datasets aggregated across target label marginal shifts. While no single DA method performs consistently across different datasets, FixMatch seems to provide the highest aggregate improvement over a source-only classifier.\"}"}
{"id": "garg23a", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 5.\\n\\nResults with DA methods paired with re-sampling (RS) and re-weighting (RW) correction (with RLLS estimate) aggregated across target label marginal shifts for vision datasets.\\n\\nRS and RW seem to help for all datasets and they both together significantly improve aggregate performance over no correction for all DA methods.\\n\\n| Dataset       | RLSbench | Domain Adaptation | TENT | NoisyStudent |\\n|---------------|----------|-------------------|------|--------------|\\n|               |          | Source            | BN-adapt CDANN FixMatch | None RW None RW RS RS+ |\\n|               | RW None  | RW None            | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\n|               | RW       | RW                | RW   | RW RS RS+    |\\"}
{"id": "garg23a", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we summarize deep DA methods compared in our RLS BENCH testbed. We also discuss how each method combines with our meta-algorithm to handle shift in class proportion.\\n\\nL.1. Source only training\\n\\nWe consider empirical risk minimization on the labeled source data as a baseline. Since this simply ignores the unlabeled target data, we call this as source only training. As mentioned in the main paper, we perform source only training with and without data augmentations. Formally, we minimize the following ERM loss:\\n\\n$$L_{\\\\text{source only}} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\ell(p_f(p_T x_i), y_i),$$\\n\\n(4)\\n\\nwhere $T$ is the stochastic data augmentation operation for vision datasets and $\\\\ell$ is a loss function. For NLP and tabular datasets, $T$ is the identity function. Throughout the paper, we use cross-entropy loss minimization. Unless specified otherwise, we use strong augmentations as the data augmentation technique for vision datasets. For NLP and tabular datasets, we do not use any data augmentation.\\n\\nAs mentioned in the main paper, we do not include re-sampling results with a source only model as it is trained only on source data and we observed no differences with just balancing the source data (as for most datasets source is already balanced) in our experiments. After obtaining a classifier $f$, we can first estimate the target label marginal and then adjust the classifier with post-hoc re-weighting with importance ratios $w_{t, p} y_{Q'}$.\\n\\n**Adversarial training of a source only model**\\n\\nAlong with standard training of a source only model with data augmentation, we experiment with adversarially robust models (Madry et al., 2017). To train adversarially robust models, we replace the standard ERM objective with a robust risk minimization objective:\\n\\n$$L_{\\\\text{source only (adv)}} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\ell(p_R(p_T x_i), y_i),$$\\n\\n(5)\\n\\nwhere $R_{p,q}$ performs the adversarial augmentation. In our paper, we use targeted Projected Gradient Descent (PGD) attacks with $\\\\ell_2$ perturbation model.\\n\\nL.2. Domain-adversarial training methods\\n\\nDomain-adversarial training methods aim to learn domain invariant feature representations. These methods aimed at practical problems with non-overlapping support and are motivated by theoretical results showing that the gap between in- and out-of-distribution performance depends on some measure of divergence between the source and target distributions (Ben-David et al., 2010a; Ganin et al., 2016). While simultaneously minimizing the source error, these methods align the representations between source and target distribution. To perform alignment, these methods penalize divergence between feature representations across domains, encouraging the model to produce feature representations that are similar across domain.\\n\\nBefore describing these methods, we first define some notation. Consider a model $f = \\\\tilde{g} \\\\circ \\\\tilde{h}$, where $h: X \\\\to \\\\mathbb{R}^d$ is the featurizer that maps the inputs to some $d$ dimensional feature space, and the head $g: \\\\mathbb{R}^d \\\\to \\\\Delta_k$ maps the features to the prediction space. Following Sagawa et al. (2021), with all of our domain invariant methods, we use strong augmentations with source and target data for vision datasets. For NLP and tabular datasets, we do not use any data augmentation.\\n\\n**DANN**\\n\\nDANN was proposed in Ganin et al. (2016). DANN approximates the divergence between feature representations of source and target domain by leveraging a domain discriminator classifier. Domain discriminator $f_d$ aims to discriminate between source and target domains. Given a batch of inputs from source and target, this deep network $f_d$ classifies whether the examples are from the source data or target data. In particular, the following loss function is used:\\n\\n$$L_{\\\\text{domain disc.}} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\ell(p_{f_d(p_h T x_i)}, y_i),$$\\n\\n(6)\"}"}
{"id": "garg23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\nB.2. NLP Datasets\\n\\nRelative Accuracy\\n\\n(a) Performance of DA methods relative to source-only training with increasing target label marginal shift\\n\\nFigure 6. Performance of different DA methods relative to a source-only model across all distribution shift pairs in NLP datasets grouped by shift severity in label marginal. For each distribution shift pair and DA method, we plot the relative accuracy of the model trained with that DA method by subtracting the accuracy of the source-only model. Hence, the black dotted line at 0 captures the performance of the source-only model. Smaller the Dirichlet shift parameter, the more severe is the shift in target class proportion.\\n\\n(a) Performance of DANN and IW-DANN methods degrades with increasing severity of target label marginal shift often falling below the performance of a source-only classifier (except for Noisy Student). Performance of PsuedoLabel, CDANN, and IW-CDANN show less susceptibility to increasing severity in target marginal shift.\\n\\n(b) RS and RW (in our meta-algorithm) together significantly improve aggregate performance over no correction for all DA methods. While RS consistently helps (over no correction) across different label marginal shift severities, RW hurts slightly for BN-adapt, TENT, and NoisyStudent when shift severity is small. However, for severe shifts ($\\\\alpha_{Pt}$ $\\\\geq 0.5$), RW significantly improves performance for all the methods. Detailed results with all methods on individual datasets in App. J.\\n\\nC. Comparison between IW-CDANN, IW-DANN, and SENTRY with Existing DA methods paired with our Meta-Algorithm\\n\\nFig. 7 shows the relevant comparison.\\n\\nNote. On Officehome dataset, we observe a slight discrepancy between SENTRY results with our runs and numbers originally reported in the paper (Prabhu et al., 2021). We find that this is due to differences in batch size used in original work versus in our runs (which we kept the same for all the algorithms). In App. N, we report SENTRY results with the updated batch size. With the new batch size, we reconcile SENTRY results but also observe a significant improvement in FixMatch results. We refer reader to App. N for a more detailed discussion.\\n\\nD. Dataset Details\\n\\nIn this section, we provide additional details about the datasets used in our benchmark study.\"}"}
{"id": "garg23a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. Comparison of existing DA methods paired with our RS and RW correction and DA methods specifically proposed for relaxed label shift problems. Across vision and tabular datasets, we observe the susceptibility of IW-DAN, IW-CDAN, and SENTRY with increasing severity of target label marginal shifts. In particular, for severe target label marginal shifts, the performance of IW-DAN, IW-CDAN, and SENTRY often falls below that of the source-only model. However, existing DA techniques when paired with RS + RW correction significantly improve over the source-only model. For NLP, datasets we observe similar behavior but with relatively less intensity.\\n\\n\u2022 CIFAR10\\nWe use the original CIFAR10 dataset (Krizhevsky & Hinton, 2009) as the source dataset. For target domains, we consider (i) synthetic shifts (CIFAR10-C) due to common corruptions (Hendrycks & Dietterich, 2019); and (ii) natural distribution shift, i.e., CIFAR10v2 (Recht et al., 2018; Torralba et al., 2008) due to differences in data collection strategy. We randomly sample 3 set of CIFAR-10-C datasets. Overall, we obtain 5 datasets (i.e., CIFAR10v1, CIFAR10v2, CIFAR10C-Frost (severity 4), CIFAR10C-Pixelate (severity 5), CIFAR10-C Saturate (severity 5)).\\n\\n\u2022 CIFAR100\\nSimilar to CIFAR10, we use the original CIFAR100 set as the source dataset. For target domains we consider synthetic shifts (CIFAR100-C) due to common corruptions. We sample 4 CIFAR100-C datasets, overall obtaining 5 domains (i.e., CIFAR100, CIFAR100C-Fog (severity 4), CIFAR100C-Motion Blur (severity 2), CIFAR100C-Contrast (severity 4), CIFAR100C-spatter (severity 2)).\\n\\n\u2022 FMoW\\nIn order to consider distribution shifts faced in the wild, we consider FMoW-WILDs (Koh et al., 2021; Christie et al., 2018) from WILDS benchmark, which contains satellite images taken in different geographical regions and at different times. We use the original train as source and OOD val and OOD test splits as target domains as they are collected over different time-period. Overall, we obtain 3 different domains.\\n\\n\u2022 Camelyon17\\nSimilar to FMoW, we consider tumor identification dataset from the wilds benchmark (Bandi et al., 2018). We use the default train as source and OOD val and OOD test splits as target domains as they are collected across different hospitals. Overall, we obtain 3 different domains.\"}"}
{"id": "garg23a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\nDataset\\n- CIFAR10\\n- CIFAR100\\n- Camelyon\\n- Entity13\\n- Entity30\\n- Living17\\n- Nonliving26\\n- FMoW\\n- Officehome\\n- Domainnet\\n- Visda\\n\\nCifar10v1\\nCifar10v2\\n\\nYears 2002-'13\\nYear 2013-'16\\nYear 2016-'18\\n\\nProduct\\n- RealWorld\\n- ClipArt\\n- Art\\n- Real\\n- ClipArt\\n- Sketch\\n- Painting\\n- Rendering\\n- Real\\n- 1\\n- Real\\n- 2\\n\\nv1 (disjoint sub.)\\nv2 (disjoint sub.)\\n\\nFigure 8. Examples from all the domains in each vision dataset.\"}"}
{"id": "garg23a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Details of the datasets considered in our RLSBENCH.\\n\\n- **RLSbench: Domain Adaptation Under Relaxed Label Shift**\\n\\n**Dataset Source** | **Target**\\n--- | ---\\nCIFAR10 | CIFAR10v1, CIFAR10v2, CIFAR10C-Frost (severity 4), CIFAR10C-Pixelate (severity 5), CIFAR10-C Saturate (severity 5)\\nCIFAR100 | CIFAR100, CIFAR100C-Fog (severity 4), CIFAR100C-Motion Blur (severity 2), CIFAR100C-Contrast (severity 4), CIFAR100C-spatter (severity 2)\\nCamelyon | Camelyon (Hospital 1\u20133), Camelyon (Hospital 4), Camelyon (Hospital 5)\\nFMoW | FMoW (2002\u2013'13), FMoW (2013\u2013'16), FMoW (2016\u2013'18)\\nEntity13 | Entity13 (ImageNetv1 sub-population 1), Entity13 (ImageNetv1 sub-population 2), Entity13 (ImageNetv2 sub-population 1), Entity13 (ImageNetv2 sub-population 2)\\nEntity30 | Entity30 (ImageNetv1 sub-population 1), Entity30 (ImageNetv1 sub-population 2), Entity30 (ImageNetv2 sub-population 1), Entity30 (ImageNetv2 sub-population 2)\\nLiving17 | Living17 (ImageNetv1 sub-population 1), Living17 (ImageNetv1 sub-population 2), Living17 (ImageNetv2 sub-population 1), Living17 (ImageNetv2 sub-population 2)\\nNonliving26 | Nonliving26 (ImageNetv1 sub-population 1), Nonliving26 (ImageNetv1 sub-population 2), Nonliving26 (ImageNetv2 sub-population 1), Nonliving26 (ImageNetv2 sub-population 2)\\nOfficehome | Product, Art, ClipArt, Real\\nDomainNet | Real, Painting, Sketch, ClipArt\\nVisda | Synthetic, Real-1 (originally referred to as val), Real-2 (originally referred to as test)\\nCivilcomments | Train (all formed by disjoint partitions of online articles)\\nMimic Readmissions | Mimic Readmissions (year: 2008), Mimic Readmissions (year: 2009), Mimic Readmissions (year: 2010), Mimic Readmissions (year: 2011), Mimic Readmissions (year: 2012), Mimic Readmissions (year: 2013)\\nRetiring Adults | Retiring Adults (year: 2014; states: ['MD', 'NJ', 'MA']), Retiring Adults (year: 2015; states: ['MD', 'NJ', 'MA']), Retiring Adults (year: 2016; states: ['MD', 'NJ', 'MA']), Retiring Adults (year: 2017; states: ['MD', 'NJ', 'MA']), Retiring Adults (year: 2018; states: ['MD', 'NJ', 'MA'])\\n\\n- **RLSbench: BREEDs Benchmark**\\n\\nWe also consider BREEDs benchmark (Santurkar et al., 2021) in our setup to assess robustness to subpopulation shifts. BREEDs leverage class hierarchy in ImageNet to re-purpose original classes to be the subpopulations and defines a classification task on superclasses. We consider distribution shift due to subpopulation shift which is induced by directly making the subpopulations present in the training and test distributions disjoint. BREEDs benchmark contains 4 datasets Entity-13, Entity-30, Living-17, and Non-living-26, each focusing on different subtrees and levels in the hierarchy. We also consider natural shifts due to differences in the data collection process of ImageNet (Russakovsky et al., 2015), e.g., ImageNetv2 (Recht et al., 2019) and a combination of both. Overall, for each of the 4 BREEDs datasets (i.e., Entity-13, Entity-30, Living-17, and Non-living-26), we obtain four different domains. We refer to them as follows: BREEDsv1 sub-population 1 (sampled from ImageNetv1), BREEDsv1 sub-population 2 (sampled from ImageNetv1), BREEDsv2 sub-population 1 (sampled from ImageNetv2), BREEDsv2 sub-population 2 (sampled from ImageNetv2). For each BREEDs dataset, we use BREEDsv1 sub-population A as source and the other three as target domains.\"}"}
{"id": "garg23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\nPhysioToolkit PhysioBank. Physionet: components of a new research resource for complex physiologic signals. *Circulation*, 101(23):e215\u2013e220, 2000.\\n\\nViraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 8558\u20138567, 2021.\\n\\nJoaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. *Dataset shift in machine learning*. Mit Press, 2008.\\n\\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classifiers generalize to cifar-10? *arXiv preprint arXiv:1806.00451*, 2018.\\n\\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In *International Conference on Machine Learning*, pp. 5389\u20135400. PMLR, 2019.\\n\\nManley Roberts, Pranav Mani, Saurabh Garg, and Zachary Lipton. Unsupervised learning under latent label shift. In *Advances in Neural Information Processing Systems* (NeurIPS), 2022.\\n\\nElan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. Domain-adjusted regression or: Erm may already learn features sufficient for out-of-distribution generalization. *arXiv preprint arXiv:2202.06856*, 2022.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. *International journal of computer vision*, 115(3):211\u2013252, 2015.\\n\\nMarco Saerens, Patrice Latinne, and Christine Decaestecker. *Adjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure*. *Neural Computation*, 2002.\\n\\nShiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne David, Ian Stavness, Wei Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey Levine, Chelsea Finn, and Percy Liang. Extending the wilds benchmark for unsupervised adaptation. In *NeurIPS Workshop on Distribution Shifts*, 2021.\\n\\nKuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, 2018.\\n\\nKuniaki Saito, Donghyun Kim, Piotr Teterwak, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Tune it the right way: Unsupervised validation of domain adaptation via soft neighborhood density. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 9184\u20139193, 2021.\\n\\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. *ArXiv*, abs/1910.01108, 2019.\\n\\nShibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation shift. In *International Conference on Learning Representations (ICLR)*, 2021.\\n\\nSteffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. *arXiv preprint arXiv:2006.16971*, 2020.\\n\\nBernhard Sch\u00f6lkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij. On Causal and Anticausal Learning. In *International Conference on Machine Learning (ICML)*, 2012.\\n\\nHidetoshi Shimodaira. Improving Predictive Inference Under Covariate Shift by Weighting the Log-Likelihood Function. *Journal of Statistical Planning and Inference*, 2000.\\n\\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. *Advances in Neural Information Processing Systems*, 33, 2020.\\n\\nBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In *European conference on computer vision*. Springer, 2016.\\n\\nBaochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In *Proceedings of the AAAI Conference on Artificial Intelligence*, 2016.\\n\\nBaochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. In *Domain Adaptation in Computer Vision Applications*. Springer, 2017.\\n\\nRemi Tachet, Han Zhao, Yu-Xiang Wang, and Geoff Gordon. Domain Adaptation with Conditional Distribution Matching and Generalized Label Shift. *arXiv:2003.04475 [cs, stat]*, December 2020. URL http://arxiv.org/abs/2003.04475. arXiv: 2003.04475.\"}"}
{"id": "garg23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\nRemi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoffrey J Gordon. Domain adaptation with conditional distribution matching and generalized label shift. Advances in Neural Information Processing Systems, 33, 2020.\\n\\nShuhuan Tan, Xingchao Peng, and Kate Saenko. Class-imbalanced domain adaptation: An empirical odyssey. In European Conference on Computer Vision, pp. 585\u2013602. Springer, 2020.\\n\\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 33:18583\u201318599, 2020.\\n\\nAntonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pp. 1521\u20131528. IEEE, 2011.\\n\\nVladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks, 10(5):988\u2013999, 1999.\\n\\nHemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5018\u20135027, 2017.\\n\\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c.\\n\\nChen Wei, Kihyuk Sohn, Clayton Mellina, Alan Yuille, and Fan Yang. Crest: A class-rebalancing self-training framework for imbalanced semi-supervised learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10857\u201310866, 2021.\\n\\nFlorian Wenzel, Andrea Dittadi, Peter Vincent Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow, David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, et al. Assaying out-of-distribution generalization in transfer learning. arXiv preprint arXiv:2207.09239, 2022.\\n\\nOlivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre Alvise-Rebuffi, Ira Ktena, Taylan Cemgil, et al. A fine-grained analysis on distribution shift. arXiv preprint arXiv:2110.11328, 2021.\\n\\nGarrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 2020.\\n\\nYifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. Domain adaptation with asymmetrically-relaxed distribution alignment. In International Conference on Machine Learning (ICML), 2019.\\n\\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10687\u201310698, 2020a.\\n\\nXinpeng Xie, Jiawei Chen, Yuexiang Li, Linlin Shen, Kai Ma, and Yefeng Zheng. Self-supervised cyclegan for object-preserving image-to-image domain adaptation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XX 16, pp. 498\u2013513. Springer, 2020b.\\n\\nDa Xu, Yuting Ye, and Chuanwei Ruan. Understanding the role of importance weighting for deep learning. arXiv preprint arXiv:2103.15209, 2021.\\n\\nHongliang Yan, Yukang Ding, Peihua Li, Qilong Wang, Yong Xu, and Wangmeng Zuo. Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2272\u20132281, 2017.\\n\\nHuaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei Koh, and Chelsea Finn. Wild-time: A benchmark of in-the-wild distribution shift over time. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=F9ENmZABB0.\\n\\nBianca Zadrozny. Learning and Evaluating Classifiers Under Sample Selection Bias. In International Conference on Machine Learning (ICML), 2004.\\n\\nJingzhao Zhang, Aditya Menon, Andreas Veit, Srinadh Bhojanapalli, Sanjiv Kumar, and Suvrit Sra. Coping with label shift via distributionally robust optimisation. In International Conference on Learning Representations (ICLR), 2021.\\n\\nKun Zhang, Bernhard Sch\u00f6lkopf, Krikamol Muandet, and Zhikun Wang. Domain Adaptation Under Target and Conditional Shift. In International Conference on Machine Learning (ICML), 2013.\\n\\nRichard Zhang. Making convolutional networks shift-invariant again. In ICML, 2019.\"}"}
{"id": "garg23a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Weichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. Collaborative and adversarial network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2018.\\n\\nYuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In International Conference on Machine Learning. PMLR, 2019.\\n\\nHan Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant representations for domain adaptation. In International Conference on Machine Learning, pp. 7523\u20137532. PMLR, 2019.\\n\\nYang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In Proceedings of the European conference on computer vision (ECCV), pp. 289\u2013305, 2018.\"}"}
{"id": "garg23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"Appendix\\n\\nA. Description of Plots\\n\\nFor each plot in Fig. 2, we obtain all the distribution shift pairs with a specific alpha (i.e., the value on the x-axis). Then for each distribution shift pair (with a specific alpha value), we obtain relative performance by subtracting the performance of a source-only model trained on the source dataset of that distribution shift pair from the performance of the model trained on that distribution shift pair with the DA algorithm of interest. Thus for each alpha and each DA method, we obtain relative performance values. We draw the box plot and the mean of these relative performance values.\\n\\nFor (similar-looking) plots, we use the same technique throughout the paper. The only thing that changes is the group of points over which aggregation is performed.\\n\\nB. Tabular and NLP Results Omitted from the Main Paper\\n\\n**B.1. Tabular Datasets**\\n\\n| Dirichlet Shift (alpha) | Relative Accuracy |\\n|-------------------------|------------------|\\n| None                    | 10.0             |\\n| 10                      | 3.0              |\\n| 1.0                     | 1.0              |\\n| 0.5                     | 0.5              |\\n\\n**Domain alignment methods**\\n\\n| Source only | DANN | IW-DANN | CDANN | IW-CDANN |\\n|-------------|------|---------|-------|----------|\\n| None        | 10.0 | 10.0    | 3.0   | 3.0      |\\n| 10.0        | 10.0 | 10.0    | 3.0   | 3.0      |\\n| 3.0         | 3.0  | 3.0     | 3.0   | 3.0      |\\n| 1.0         | 1.0  | 1.0     | 1.0   | 1.0      |\\n| 0.5         | 0.5  | 0.5     | 0.5   | 0.5      |\\n\\n**Self-training methods**\\n\\n| PseudoLabel | Relative Accuracy |\\n|-------------|------------------|\\n| None        | 10.0             |\\n| 10.0        | 10.0             |\\n| 3.0         | 3.0              |\\n| 1.0         | 1.0              |\\n| 0.5         | 0.5              |\\n\\nFigure 5. Performance of different DA methods relative to a source-only model across all distribution shift pairs in tabular datasets grouped by shift severity in label marginal. For each distribution shift pair and DA method, we plot the relative accuracy of the model trained with that DA method by subtracting the accuracy of the source-only model. Hence, the black dotted line at 0 captures the performance of the source-only model. Smaller the Dirichlet shift parameter, the more severe is the shift in target class proportion.\\n\\n(a) Shifts with \\\\( \\\\alpha_t \\\\) have little to no impact on different DA methods whereas the performance of all DA methods degrades when \\\\( \\\\alpha_{Pt} \\\\), often falling below the performance of a source-only classifier.\\n\\n(b) RS and RW (in our meta-algorithm) together significantly improve aggregate performance over no correction for all DA methods. While RS consistently helps (over no correction) across different label marginal shift severities, RW hurts slightly when shift severity is small. However, for severe shifts (\\\\( \\\\alpha_{Pt} \\\\)) RW significantly improves performance for all the methods. Results with all methods on individual datasets in App. J.\"}"}
{"id": "garg23a", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm     | Source (w/o aug) (RW) | Source (w aug) (RW) | Source (w aug) | Source (adv) (RW) |\\n|--------------|-----------------------|---------------------|----------------|-------------------|\\n| CORAL        |                       |                     | 0.5            |                   |\\n| CORAL (RS+RW)|                       |                     | 0.1            |                   |\\n| NoisyStudent |                       |                     | 0.0            |                   |\\n| BN-adapt     |                       |                     | 0.0            |                   |\\n| TENT         |                       |                     | 0.0            |                   |\\n| DANN (RS+RW) |                       |                     | 0.0            |                   |\\n| FixMatch     |                       |                     | 0.0            |                   |\\n| IW-DANN      |                       |                     | 0.0            |                   |\\n| IW-CDANN     |                       |                     | 0.0            |                   |\\n| CDANN        |                       |                     | 0.0            |                   |\\n\\nFigure 27. Officehome. Relative performance and accuracy plots for different DA algorithms across various shift pairs in Officehome.\"}"}
{"id": "garg23a", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm            | Source (w aug) | Source (w/o aug) | NoisyStudent | BN-adapt | TENT | DANN | IW-DANN | FixMatch | CDANN | IW-CDANN | NoisyStudent | SENTRY |\\n|----------------------|----------------|------------------|--------------|----------|------|------|---------|----------|-------|----------|--------------|--------|\\n|                      | RW             | RW               | RS+RW        | RS+RW    | RW   | RW   | RW      | RW       | RW    | RW       | RW           | RW     |\\n| Avg. Accuracy        |                |                  |              |          |      |      |         |          |       |          |              |        |\\n| visda                |                |                  |              |          |      |      |         |          |       |          |              |        |\\n| Relative Accuracy    |                |                  |              |          |      |      |         |          |       |          |              |        |\\n| Dirichlet Shift (alpha) |          |                  |              |          |      |      |         |          |       |          |              |        |\\n| None                | 10.0           | 3.0              | 1.0          | 0.5      |      |      |         |          |       |          |              |        |\\n| 10^{-2}             |                |                  |              |          |      |      |         |          |       |          |              |        |\\n| 10^{-1}             |                |                  |              |          |      |      |         |          |       |          |              |        |\\n| 10^{-0}             |                |                  |              |          |      |      |         |          |       |          |              |        |\\n| 10^{0}              |                |                  |              |          |      |      |         |          |       |          |              |        |\\n| 10^{1}              |                |                  |              |          |      |      |         |          |       |          |              |        |\\n| 10^{2}              |                |                  |              |          |      |      |         |          |       |          |              |        |\\n\\nFigure 28. Visda. Relative performance and accuracy plots for different DA algorithms across various shift pairs in Visda.\"}"}
{"id": "garg23a", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 29. Civilcomments. Relative performance and accuracy plots for different DA algorithms across various shift pairs in Civilcomments.\"}"}
{"id": "garg23a", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method               | Source (w/o aug) | Dirichlet Shift (alpha) | relative Accuracy |\\n|----------------------|------------------|-------------------------|-------------------|\\n| DANN                 | 74               | 10.0                    | 10.0              |\\n| IW-DANN              | 76               | 3.0                     | 3.0               |\\n| Source (w/o aug) (RW)| 78               | 1.0                     | 1.0               |\\n| CDANN                | 80               | 0.5                     | 0.5               |\\n| PseudoLabel          | 82               |                         |                   |\\n| IW-CDANN             | 84               |                         |                   |\\n| DANN (RS+RW)         |                  |                         |                   |\\n| Source (w/o aug) (RW)|                  |                         |                   |\\n| CDANN (RS+RW)        |                  |                         |                   |\\n| PseudoLabel (RS+RW)  |                  |                         |                   |\\n\\nFigure 30. Retiring Adults. Relative performance and accuracy plots for different DA algorithms across various shift pairs in Retiring Adults.\"}"}
{"id": "garg23a", "page_num": 45, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\nwhere $x_1, x_2, \\\\ldots, x_n$ are $n$ source examples and $x_{n+1}, \\\\ldots, x_m$ are $m$ target examples. Overall, the following loss function is used to optimize models with DANN:\\n\\n$$L_{\\\\text{DANN}} = \\\\cdots$$\\n\\n$L_{\\\\text{DANN}}$ is maximized with respect to the domain discriminator classifier and $L_{\\\\text{DANN}}$ minimized with respect to the underlying featurize and the source classifier. This is achieved by gradient reversal layer in practice. To train, three networks, we use three different learning rates $\\\\eta_f, \\\\eta_g, \\\\eta_f$.\\n\\nWe adapted our DANN implementation from Sagawa et al. (2021) and Transfer learning library (Jiang et al., 2022).\\n\\nCDANN\\n\\nConditional Domain adversarial neural network is a variant of DANN (Long et al., 2018). Here the domain discriminator is conditioned on the classifier $g$\u2019s prediction. In particular, instead of training the domain discriminator on the representation output of $h$, these methods operate on the outer product between the feature presentation $h$ at an input $x$ and the classifier\u2019s probabilistic prediction $f(g(h))$ (i.e., $h^T x$). Thus instead of training the domain discriminator classifier $f_d$ on the $d$-dimensional input space, they train it on $d$-dimensional space. In particular, the following loss function is used:\\n\\n$$L_{\\\\text{CDAN domain disc.}} = \\\\cdots$$\\n\\nWe adapted our implementation for CDANN from Transfer learning library (Jiang et al., 2022).\\n\\nTo adapt DANN and CDANN to our meta algorithm, at each epoch we can perform rebalancing of source and target data as in Step 1 and 4 of Algorithm 1. After obtaining the classifier $f$, we can use this classifier to first obtain an estimate of the target label marginal and then perform re-weighting adjustment with the obtained estimate.\\n\\nIW-DANN and IW-CDANN\\n\\nTachet et al. (2020) proposed training with importance re-weighting correction with DANN and CDANN objectives to accommodate for the shift in the target label proportion. In particular, at every epoch of training they first estimate the importance ratio $p_w$ (with BBSE on training source and training target data) and then re-weight the domain discriminator objective and ERM objective. In particular, the domain discriminator loss for IW-DANN can be written as:\\n\\n$$L_{\\\\text{p-w domain disc.}} = \\\\cdots$$\\n\\nwhere we multiply the source loss with importance weights. Similarly, we can re-write the source only training objective with importance re-weighting as follows:\\n\\n$$L_{\\\\text{p-w source only}} = \\\\cdots$$\\n\\nOverall, the following objective is used to optimize models with IW-DANN:\\n\\n$$L_{\\\\text{IW-DANN}} = \\\\cdots$$\\n\\nwhere the importance weights are updated after every epoch with classifier obtained in previous step. Similarly, with using importance re-weights with the CDANN objective, we obtain IW-CDANN objective.\\n\\nIn population, IW-CDANN and IW-DANN correction matches the correction with our meta-algorithm for DANN and CDANN. However, the behavior this importance re-weighting correction can be different from our meta-algorithm for over-parameterized models with finite data (Byrd & Lipton, 2019). Recent empirical and theoretical findings have highlighted that importance re-weighting have minor to no effect on overparameterized models when trained for several epochs (Byrd &})"}
{"id": "garg23a", "page_num": 46, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift (Lipton, 2019; Xu et al., 2021). On the other hand, with finite samples, re-sampling (when class labels are available) has shown different and promising empirical behavior (An et al., 2020; Idrissi et al., 2022). This may highlight the differences in the behavior of IW-CDANN (or IW-DANN) with our meta algorithm on CDANN (or DANN).\\n\\nWe refer to the implementation provided by the authors (Tachet et al., 2020).\\n\\nL.3. Self-training methods\\n\\nSelf-training methods leverage unlabeled data by 'pseudo-labeling' unlabeled examples with the classifier's own predictions and training on them as if they were labeled examples. Recent self-training methods also often make use of consistency regularization, for example, encouraging the model to make similar predictions on augmented versions of unlabeled example.\\n\\nIn our work, we experiment with the following methods:\\n\\nPseudoLabel (Lee et al., 2013) proposed PseudoLabel that leverages unlabeled examples with classifier's own prediction. This algorithm dynamically generates psuedolabels and overfits on them in each batch. In particular, while pseudolabels are generated on unlabeled examples, the loss is computed with respect to the same label. PseudoLabel only overfits to the assigned label if the confidence of the prediction is greater than some threshold $\\\\tau$.\\n\\nRefer to $T$ as the data-augmentation technique (i.e., identity for NLP and tabular datasets and strong augmentation for vision datasets). Then, PseudoLabel uses the following loss function:\\n\\n$$L_{\\\\text{PseudoLabel}}(p, q) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\ell_{pf}(p_T x_i, y_i) - \\\\lambda t m \\\\sum_{i=1}^{n} \\\\ell_{pf}(p_T x_i, r_{y_i} q') \\\\max_{y \\\\neq p_T x_i} y_{pf}(y_{p_T x_i}) - \\\\tau \\\\eta,$$\\n\\nwhere $r_{y_i} = \\\\arg \\\\max_{y \\\\neq p_T x_i} y_{pf}(y_{p_T x_i})$. PseudoLabel increases $\\\\lambda$ between labeled and unlabeled losses over epochs, initially placing 0 weight on unlabeled loss and then linearly increasing the unlabeled loss weight until it reaches the full value of hyperparameter $\\\\lambda$ at some threshold step. We fix the step at which $\\\\lambda_t$ reaches its maximum value $\\\\lambda$ be 40% of the total number of training steps, matching the implementation to (Sohn et al., 2020; Sagawa et al., 2021).\\n\\nFixMatch (Sohn et al., 2020) proposed FixMatch as a variant of the simpler Pseudo-label method (Lee et al., 2013). This algorithm dynamically generates psuedolabels and overfits on them in each batch. FixMatch employs consistency regularization on the unlabeled data. In particular, while pseudolabels are generated on a weakly augmented view of the unlabeled examples, the loss is computed with respect to predictions on a strongly augmented view. The intuition behind such an update is to encourage a model to make predictions on weakly augmented data consistent with the strongly augmented example. Moreover, FixMatch only overfits to the assigned labeled with weak augmentation if the confidence of the prediction with strong augmentation is greater than some threshold $\\\\tau$.\\n\\nRefer to $T_{\\\\text{weak}}$ as the weak-augmentation and $T_{\\\\text{strong}}$ as the strong-augmentation function. Then, FixMatch uses the following loss function:\\n\\n$$L_{\\\\text{FixMatch}}(p, q) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\ell_{pf}(p_T x_i, y_i) - \\\\lambda m \\\\sum_{i=1}^{n} \\\\ell_{pf}(p_T x_i, r_{y_i} q') \\\\max_{y \\\\neq p_T x_i} y_{pf}(y_{p_T x_i}) - \\\\tau \\\\eta,$$\\n\\nwhere $r_{y_i} = \\\\arg \\\\max_{y \\\\neq p_T x_i} y_{pf}(y_{p_T x_i})$. We adapted our implementation from Sagawa et al. (2021) which matches the implementation of Sohn et al. (2020) except for one detail. While Sohn et al. (2020) augments labeled examples with weak augmentation, Sagawa et al. (2021) proposed to strongly augment the labeled source examples.\\n\\nNoisyStudent (Xie et al., 2020b) proposed a different variant of Pseudo-labeling. Noisy Student generates pseudolabels, fixes them, and then trains the model (from scratch) until convergence before generating new pseudolabels. Contrast it with FixMatch and PseudoLabel which dynamically generate pseudolabels. The first set of pseudolabels are obtained by training an initial teacher model only on the source labeled data. Then in each iteration, randomly initialized models fit the labeled source data and pseudolabeled target data with pseudolabels assigned by the converged model in the previous iteration.\\n\\nNoisy student objective can be summarized as:\"}"}
{"id": "garg23a", "page_num": 47, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\nNoisyStudent\\n\\nwhere \\\\( r_y^i \\\\) is computed with the classifier obtained at step. Note that the randomly initialized model at each iteration uses a dropout of 0.5 in the penultimate layer. We adopted our implementation of NoisyStudent to Sagawa et al. (2021). To initialize the initial teacher model, we use the source-only model trained with strong augmentations without dropout.\\n\\nSENTRY\\n\\nPrabhu et al. (2021) proposed a different variant of pseudolabeling method. This method is aimed to tackle DA under relaxed label shift scenario. SENTRY incorporates a target instance based on its predictive consistency under a committee of strong image transformations. In particular, SENTRY makes \\\\( N \\\\) strong augmentations of an unlabeled target example and makes a prediction on those. If the majority of the committee matches the prediction on the sample example with weak-augmentation then entropy is minimized on that example, otherwise the entropy is maximized. Moreover, the authors employ an 'information-entropy' objective aimed to match the prediction at every example with the estimated target label marginal. Overall the SENTRY objective is defined as follows:\\n\\n\\\\[\\nL_{\\\\text{SENTRY}} = \\\\min \\\\sum_{i=1}^{n} \\\\left( \\\\sum_{j=1}^{k} \\\\frac{f_k p_y^j | x_i q}{\\\\log p_{r_t p_y^j | x_i q}} \\\\right) \\\\lambda_{\\\\text{unsup}}\\n\\\\]\\n\\n47\"}"}
{"id": "garg23a", "page_num": 48, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"M. Hyperparameter and Architecture Details\\n\\nM.1. Architecture and Pretraining Details\\n\\nFor all datasets, we used the same architecture across different algorithms:\\n\\n- CIFAR-10: Resnet-18 (He et al., 2016) pretrained on Imagenet\\n- CIFAR-100: Resnet-18 (He et al., 2016) pretrained on Imagenet\\n- Camelyon: Densenet-121 (Huang et al., 2017) not pretrained on Imagenet as per the suggestion made in (Koh et al., 2021)\\n- FMoW: Densenet-121 (Huang et al., 2017) pretrained on Imagenet\\n- BREEDs (Entity13, Entity30, Living17, Nonliving26): Resnet-18 (He et al., 2016) not pretrained on Imagenet as per the suggestion in (Santurkar et al., 2021). The main rationale is to avoid pre-training on the superset dataset where we are simulating sub-population shift.\\n- Officehome: Resnet-50 (He et al., 2016) pretrained on Imagenet\\n- Domainnet: Resnet-50 (He et al., 2016) pretrained on Imagenet\\n- Visda: Resnet-50 (He et al., 2016) pretrained on Imagenet\\n- Civilcomments: Pre-trained DistilBERT-base-uncased (Sanh et al., 2019)\\n- Retiring Adults: We use an MLP with 2 hidden layers and 100 hidden units in both of the hidden layers\\n- Mimic Readmissions: We use the transformer architecture described in Yao et al. (2022)\\n\\nExcept for Resnets on CIFAR datasets, we used the standard pytorch implementation (Gardner et al., 2018). For Resnet on cifar, we refer to the implementation here: https://github.com/kuangliu/pytorch-cifar. For all the architectures, whenever applicable, we add antialiasing (Zhang, 2019). We use the official library released with the paper. For imagenet-pretrained models with standard architectures, we use the publicly available models here: https://pytorch.org/vision/stable/models.html. For imagenet-pretrained models on the reduced input size images (e.g. CIFAR-10), we train a model on Imagenet on reduced input size from scratch. We include the model with our publicly available repository. For bert-based models, we use the publicly available models here: https://huggingface.co/docs/transformers.\\n\\nM.2. Hyperparameters\\n\\nFirst, we tune learning rate and $\\\\ell_2$ regularization parameter by fixing batch size for each dataset that correspond to maximum we can fit to 15GB GPU memory. We set the number of epochs for training as per the suggestions of the authors of respective benchmarks. Note that we define the number of epochs as a full pass over the labeled training source data. We summarize learning rate, batch size, number of epochs, and $\\\\ell_2$ regularization parameter used in our study in Table 6.\\n\\nFor each algorithm, we use the hyperparameters reported in the initial papers. For domain-adversarial methods (DANN and CDANN), we refer to the suggestions made in Transfer Learning Library (Jiang et al., 2022). We tabulate hyperparameters for each algorithm next:\\n\\n- DANN, CDANN, IW-CDANN and IW-DANN: As per Transfer Learning Library suggestion, we use a learning rate multiplier of $0.1$ for the featurizer when initializing with a pre-trained network and $1$ otherwise. We default to a penalty weight of $1.0$ for all datasets with pre-trained initialization.\\n- FixMatch: We use the lambda is 1.0 and use threshold $\\\\tau$ as 0.9.\"}"}
{"id": "garg23a", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 19. CIFAR100. Relative performance and accuracy plots for different DA algorithms across various shift pairs in CIFAR100.\"}"}
{"id": "garg23a", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm       | Source (w aug) | Source (w/o aug) | Source (w/o aug) (RW) | CORAL | Source (w aug) (RW) | CORAL (RS+RW) | Source (adv) | Source (adv) (RW) | NoisyStudent | NoisyStudent (RS+RW) | CDANN (RS+RW) | DANN (RS+RW) | TENT (RS+RW) | BN-adapt (RS+RW) | IW-DANN | FixMatch | FixMatch (RS+RW) | SENTRY |\\n|-----------------|----------------|------------------|-----------------------|-------|---------------------|----------------|-------------|-------------------|--------------|---------------------|----------------|-------------|-------------|------------------|---------|----------|-------------------|--------|\\n|                 | 70             | 75               | 80                    | 85    | 90                  | 95             | Avg. Accuracy | camelyon          | None        | 10.0                | 3.0            | 1.0         | 0.5         | Relative Accuracy |\\n| Dirichlet Shift | \u03b1 = \u221210^2      | \u221210^1            | \u221210^0                 | 0     | 10^0                | 10^1           | 10^2         |                   |              |                     |                |             |             |                   |\\n|                 | 10^2           | \u221210^1            | \u221210^0                 | 0     | 10^0                | 10^1           | 10^2         |                   |              |                     |                |             |             |                   |\\n|                 | 10^1           | \u221210^0            | 0                     | 10^0  | 10^1                | 10^2           | 10^2         |                   |              |                     |                |             |             |                   |\\n|                 | 10^0           | 0                | 10^0                  | 10^0  | 10^1                | 10^2           | 10^2         |                   |              |                     |                |             |             |                   |\\n\\nFigure 20. Camelyon. Relative performance and accuracy plots for different DA algorithms across various shift pairs in Camelyon.\"}"}
{"id": "garg23a", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"| Algorithm       | Source (adv) (RW) | Source (adv) | CDANN (RS+RW) | DANN | IW-DANN | CDANN | IW-CDANN | Source (w/o aug) (RW) | Source (w/o aug) | FixMatch | NoisyStudent | SENTRY |\\n|-----------------|-------------------|--------------|---------------|------|---------|-------|----------|-----------------------|-----------------|----------|--------------|--------|\\n|                 |                   |              |               |      |         |       |          |                       |                 |          |              |        |\\n| Avg. Accuracy   | 45                | 50           | 55            | 60   | 65      | 70    | 75       | 80                    |                 |          |              |        |\\n| SOURCE (w aug)  |                   |              |               |      |         |       |          |                       |                 |          |              |        |\\n|                  | Dirichlet Shift (alpha) |    |    |    |    |    |    |    |    |    |    |    |    |\\n|                  | \u221210^2             | \u221210^1        | \u221210^0         | 0    | 10^0    | 10^1  | 10^2     |                       |                 |          |              |        |\\n\\n*Figure 21. FMoW. Relative performance and accuracy plots for different DA algorithms across various shift pairs in FMoW.*\"}"}
{"id": "garg23a", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm | None | 10.0 | 3.0 | 1.0 | 0.5 |\\n|-----------|------|------|-----|-----|-----|\\n| CORAL     |      |      |     |     |     |\\n| CORAL (RS+RW) | | | | | |\\n| Source (adv) | | | | | |\\n| SENTRY    |      |      |     |     |     |\\n| Source (adv) (RW) | | | | | |\\n| Source (w/o aug) | | | | | |\\n| Source (w/o aug) (RW) | | | | | |\\n| BN-adapt  |      |      |     |     |     |\\n| TENT      |      |      |     |     |     |\\n| DANN      |      |      |     |     |     |\\n| IW-DANN   |      |      |     |     |     |\\n| IW-CDANN  |      |      |     |     |     |\\n| CDANN     |      |      |     |     |     |\\n| FixMatch  |      |      |     |     |     |\\n| NoisyStudent | | | | | |\\n| BN-adapt (RS+RW) | | | | | |\\n| TENT (RS+RW) | | | | | |\\n| DANN (RS+RW) | | | | | |\\n| Source (w aug) (RW) | | | | | |\\n| CDANN (RS+RW) | | | | | |\\n| NoisyStudent (RS+RW) | | | | | |\\n| FixMatch (RS+RW) | | | | | |\\n\\nFigure 22. Entity13. Relative performance and accuracy plots for different DA algorithms across various shift pairs in Entity13.\"}"}
{"id": "garg23a", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Target label marginal estimation ($\\\\ell_1$) error with RLLS and classifiers obtained with different DA methods.\\n\\n(b) Relative performance of DA methods when paired with RW corrections.\\n\\nFigure 10. Target label marginal estimation ($\\\\ell_1$) error and relative performance with RLLS and classifiers obtained with different DA methods. Across all shift severities (except for $\\\\alpha = 0.5$) in vision datasets, RLLS with classifiers obtained with DA methods improves over RLLS with a source-only classifier. Correspondingly, we see significantly improved performance with post-hoc RW correction applied to classifiers trained with DA methods as compared to when applied to source-only models.\"}"}
{"id": "garg23a", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13. Comparison of different target label marginal estimation methods. We plot estimation errors with different methods with the source-only classifier. For all modalities, we observe a trade-off between estimation error with the baseline method and RLLS (or MLLS) method with severity in target marginal shift.\\n\\nI. Results with Oracle Early Stopping Criterion\\n\\nIn this section, we report results with oracle early stopping criterion. On vision and tabular datasets, we observe differences in performance when using target performance versus source hold-out performance for model selection. This highlights a more nuanced behavior than the accuracy-on-the-line phenomena (Miller et al., 2021; Recht et al., 2019). We hope to study this contrasting behavior in more detail in future work.\\n\\nFigure 14. Average accuracy of different DA methods aggregated across all distribution pairs in each modality. We compare the performance with early stopping point obtained with source validation performance and target validation performance.\"}"}
{"id": "garg23a", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"### Figure 15.\\nAccuracy difference between using source and target performance as early stopping criteria for different DA methods aggregated across all distribution shift pairs in vision datasets.\\n\\nWe observe that as the shift severity increases (i.e., as $\\\\alpha$ decreases), the accuracy difference increases for all the methods.\\n\\n### Figure 16.\\nAccuracy difference between using source and target performance as early stopping criteria for different DA methods aggregated across all distribution shift pairs in language datasets.\\n\\nWe observe that as the shift severity increases (i.e., as $\\\\alpha$ decreases), the accuracy difference increases for all the methods without any correction. With RS and RW corrections, we observe that the accuracy difference remains relatively constant as the shift severity increases.\\n\\n### Figure 17.\\nAccuracy difference between using source and target performance as early stopping criteria for different DA methods aggregated across all distribution shift pairs in tabular datasets.\\n\\nWe observe that as the shift severity increases (i.e., as $\\\\alpha$ decreases), the accuracy difference increases for all the methods.\"}"}
{"id": "garg23a", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Results on Individual Datasets\\n\\n| DA Algorithm | Source (adv) (RW) | Source (adv) | CORAL | CORAL (RS+RW) | Source (w aug) | TENT | BN-adapt | Source (w aug) (RW) | Source (w/o aug) | DANN (RS+RW) | BN-adapt (RS+RW) | TENT (RS+RW) | FixMatch (RS+RW) | NoisyStudent (RS+RW) | SENTRY |\\n|--------------|------------------|--------------|-------|---------------|----------------|------|----------|---------------------|-----------------|--------------|------------------|--------------|------------------|---------------------|--------|\\n| **CIFAR10**  |                  |              |       |               |                 |      |          |                     |                 |              |                  |              |                  |                     |        |\\n| **Relative Accuracy** |                  |              |       |               |                 |      |          |                     |                 |              |                  |              |                  |                     |        |\\n| **None**     | 10.0             | 3.0          | 1.0   | 0.5           |                 |      |          |                     |                 |              |                  |              |                  |                     |        |\\n| **Dirichlet Shift (alpha)** |                  |              |       |               |                 |      |          |                     |                 |              |                  |              |                  |                     |        |\\n| \u221210^2        |                  |              |       |               |                 |      |          |                     |                 |              |                  |              |                  |                     |        |\\n| \u221210^1        |                  |              |       |               |                 |      |          |                     |                 |              |                  |              |                  |                     |        |\\n| \u221210^0        |                  |              |       |               |                 |      |          |                     |                 |              |                  |              |                  |                     |        |\\n| 0            |                  |              |       |               |                 |      |          |                     |                 |              |                  |              |                  |                     |        |\\n| 10^0         |                  |              |       |               |                 |      |          |                     |                 |              |                  |              |                  |                     |        |\\n| 10^1         |                  |              |       |               |                 |      |          |                     |                 |              |                  |              |                  |                     |        |\\n| 10^2         |                  |              |       |               |                 |      |          |                     |                 |              |                  |              |                  |                     |        |\\n\\nFigure 18. CIFAR10. Relative performance and accuracy plots for different DA algorithms across various shift pairs in CIFAR10.\"}"}
{"id": "garg23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nDespite the emergence of principled methods for domain adaptation under label shift, their sensitivity to shifts in class conditional distributions is precariously under explored. Meanwhile, popular deep domain adaptation heuristics tend to falter when faced with label proportions shifts. While several papers modify these heuristics in attempts to handle label proportions shifts, inconsistencies in evaluation standards, datasets, and baselines make it difficult to gauge the current best practices. In this paper, we introduce RLS-BENCH, a large-scale benchmark for relaxed label shift, consisting of \u0105500 distribution shift pairs spanning vision, tabular, and language modalities, with varying label proportions. Unlike existing benchmarks, which primarily focus on shifts in class-conditional \\\\( p_x|y \\\\), our benchmark also focuses on label marginal shifts. First, we assess 13 popular domain adaptation methods, demonstrating more widespread failures under label proportion shifts than were previously known. Next, we develop an effective two-step meta-algorithm that is compatible with most domain adaptation heuristics: (i) pseudo-balance the data at each epoch; and (ii) adjust the final classifier with target label distribution estimate. The meta-algorithm improves existing domain adaptation heuristics under large label proportion shifts, often by 2\u201310% accuracy points, while conferring minimal effect (\u01030.5%) when label proportions do not shift. We hope that these findings and the availability of RLS-BENCH will encourage researchers to rigorously evaluate proposed methods in relaxed label shift settings. Code is publicly available at https://github.com/acmi-lab/RLSbench.\"}"}
{"id": "garg23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\nDistribution Shift\\nTarget Domain\\nSource Domain\\nCamelyon\\nLiving17\\nCIFAR10\\nCivilComments\\n\\n$p_y$ shifts arbitrarily\\n\\nExisting Benchmarks\\nRLSBench (Ours)\\n\\nFigure 1. Domain adaptation under Relaxed Label Shift.\\n\\n(a) Overview of RLSbench setup: Unlike existing benchmarks for which the label marginal $p$ doesn't shift, in RLSbench, $p_y$ can shift arbitrarily. The class conditionals $p_x|y$ shift in seemingly natural ways following popular benchmarks. RLSbench draws on 14 multi-domain datasets spanning vision, NLP, and tabular modalities.\\n\\n(b) Key results: As the severity of target label proportion increases, the performance of existing popular DA methods degrades, often dropping below source-only classifiers. DA methods, when paired with our meta-algorithm, significantly improve over a source-only classifier.\\n\\nDriven research has produced a variety of heuristic methods (Ganin et al., 2016; Sohn et al., 2020; Wang et al., 2021; Li et al., 2016) that despite yielding gains in benchmark performance tend to break when $p_y$ shifts. While this vulnerability has previously been demonstrated for domain-adversarial methods (Wu et al., 2019; Zhao et al., 2019), we show that this problem is more widespread than previously known. Several recent papers attempt to address shift in label distribution compounded by natural variations in $p_x|y$ (Tan et al., 2020; Tachet des Combes et al., 2020; Prabhu et al., 2021). However, it can be hard to compare experimental results across papers, owing to discrepancies in how shifts in $p_y$ are simulated and the choice of evaluation metrics. Moreover, many methods violate the unsupervised contract by peeking at target validation performance during model selection and hyperparameter tuning (Wilson & Cook, 2020; Saito et al., 2021). In short, there is a paucity of comprehensive and fair comparisons between DA methods for settings with shifts in label distribution.\\n\\nIn this paper, we develop RLSBENCH, the first standardized test bed of relaxed label shift settings, where $p_y$ can shift arbitrarily and the class conditionals $p_x|y$ can shift in seemingly natural ways (following the popular DA benchmarks). While existing DA benchmarks typically focus on shifts in $p_x|y$, our benchmarks additionally focuses on shifts in label marginals $p_y$. We evaluate a collection of popular DA methods based on domain-invariant representation learning, self-training, and test-time adaptation across 14 multi-domain datasets spanning vision, Natural Language Processing (NLP), and tabular modalities. The different domains in each dataset present a different shift in $p_x|y$. Since these datasets exhibit minor to no shift in label marginal, we simulate shift in target label marginal via stratified sampling with varying severity. Overall, we obtain 560 different source and target distribution shift pairs and train $\\\\bar{n}$ models in our testbed.\\n\\nBased on our experiments on RLSBENCH, we make several findings. First, we observe that while popular DA methods often improve over a source-only classifier absent shift in target label distribution, their performance tends to degrade, dropping below source-only classifiers under severe shifts in target label marginal. Next, we develop a meta-algorithm with two simple corrections: (i) re-sampling the data to balance the source and pseudo-balance the target; (ii) re-weighting the final classifier using an estimate of the target label marginal. We observe that in these relaxed label shift settings, the performance of existing DA methods (e.g. CDANN, FixMatch, and BN-adapt), when paired with our meta-algorithm, significantly improves over a source-only classifier. On the other hand, existing methods specifically proposed for relaxed label shift (e.g., IW-CDANN and SENTRY), often fail to improve over a source-only classifier and significantly underperform when compared to existing DA methods paired with our meta-algorithm.\\n\\nOverall, RLSbench provides a comprehensive and standardized suite for label distributions shifts, bringing existing benchmarks one step closer to exhibit the sort of diversity that we should expect to encounter when deploying models in the wild. Our findings emphasize the effectiveness of a simple, previously overlooked baseline. We hope that the RLSBENCH and our meta-algorithm (that can be paired with any DA method) provide a framework for rigorous and reproducible future research in relaxed label shift scenarios.\"}"}
{"id": "garg23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\noutput space. Let $\\\\mathbb{P}_s, \\\\mathbb{P}_t : \\\\mathbb{X} \\\\rightarrow \\\\mathbb{Y}$ be the source and target distributions and let $p_s$ and $p_t$ denote the corresponding probability density (or mass) functions. Unlike the standard supervised setting, in unsupervised DA, we possess labeled source data $t_x^1, y^1_q, p_{x^2}, y^2_q, \\\\ldots, p_{x^n}, y^n_q$ and unlabeled target data $t_{x^n_q}^1, t_{x^n_q}^2, \\\\ldots, t_{x^n_q}^m_u$. With $f : \\\\mathbb{X} \\\\rightarrow \\\\Delta_k$ we denote a predictor function which predicts $\\\\arg \\\\max_y f_y p_{x^q}$ on an input $x$. For a vector $v$, we use $v_y$ to access the element at index $y$.\\n\\nIn the traditional label shift setting, one assumes that $p_{p_{x^q}|y^q}$ does not change but that $p_{p_{y^q}|q}$ can. Under label shift, two challenges arise: (i) estimate the target label marginal $p_t p_y^q$; and (ii) train a classifier $f$ to maximize the performance on target domain. This paper focuses on the relaxed label shift setting. In particular, we assume that the label distribution can shift from source to target arbitrarily but that $p_{p_{x^q}|y^q}$ varies between source and target in some comparatively restrictive way (e.g., shifts arising naturally in the real-world like ImageNet (Russakovsky et al., 2015) to ImageNetV2 (Recht et al., 2019)). Mathematically, we assume a divergence-based restriction on $p_{p_{x^q}|y^q}$. That is, for some small $\\\\epsilon > 0$ and distributional distance $D$, we have $\\\\max_y D(p_{p_{s(p_{x^q}|y^q)}, p_{p_{t(p_{x^q}|y^q)}}} \\\\leq \\\\epsilon$ and allow an arbitrary shift in the label marginal $p_{p_y^q}$. We discuss several precise instantiations in App. G. However, in practice, it's hard to empirically verify these distribution distances for small enough $\\\\epsilon$ with finite samples. Moreover, we lack a rigorous characterization of the sense in which those shifts arise in popular DA benchmarks, and since, the focus of our work is on the empirical evaluation with real-world datasets, we leave a formal investigation for future work.\\n\\nThe goal in DA is to adapt a predictor from a source distribution with labeled data to a target distribution from which we only observe unlabeled examples. While prior work addressing relaxed label shift has primarily focused on classifier performance, we also separately evaluate methods for estimating the target label marginal. This can be beneficial for two reasons. First, it can shed more light into how improving the estimates of target class proportion improves target performance. Second, understanding how the class proportions are changing can be of independent interest.\\n\\n2.1. Prior Work\\n\\nUnsupervised domain adaption\\n\\nTwo popular settings for which DA is well-posed include (i) covariate shift (Zhang et al., 2013; Zadrozny, 2004; Cortes et al., 2010; Cortes & Mohri, 2014; Gretton et al., 2009) where $p_{p_{x^q}}$ can change from source to target but $p_{p_{y^q}|x^q}$ remains invariant; and (ii) label shift (Saerens et al., 2002; Lipton et al., 2018; Azzizzadenesheli et al., 2019; Alexandari et al., 2021; Garg et al., 2020; Zhang et al., 2021; Roberts et al., 2022) where the label marginal $p_{p_y^q}$ can change but $p_{p_{x^q}|y^q}$ is shared across source and target. Principled methods with strong theoretical guarantees exists for adaptation under these settings when target distribution's support is a subset of the source support. Ben-David et al. (2010b;a); Mansour et al. (2009); Zhao et al. (2019); Wu et al. (2019); Johansson et al. (2019) present theoretical analysis when the assumption of contained co-variate support is violated. In another line of work, Elkan & Noto (2008); Bekker & Davis (2020); Garg et al. (2021; 2022a) extend the label shift setting to problems where previously unseen classes may appear in the target and $p_{p_{x|y}}$ remains invariant among seen classes. More recently, a massive literature has emerged exploring a benchmark-driven heuristic approach (Long et al., 2015; 2017; Sun & Saenko, 2016; Sun et al., 2017; Zhang et al., 2019; 2018; Ganin et al., 2016; Sohn et al., 2020). However, rigorous evaluation of DA methods is typically restricted to these carefully curated benchmark datasets where there is minor to no shift in label marginal from source to target.\\n\\nRelaxed Label Shift\\n\\nExploring the problem of shift in label marginal from source to target with natural variations in $p_{p_{x^q}|y^q}$, a few papers highlighted theoretical and empirical failures of DA methods based on domain-adversarial neural network training (Yan et al., 2017; Wu et al., 2019; Zhao et al., 2019; Johansson et al., 2019). Subsequently, several papers attempted to handle these problems in domain-adversarial training (Tachet et al., 2020; Prabhu et al., 2021; Liu et al., 2021; Tan et al., 2020; Manders et al., 2019). However, these methods often lack comparisons with other prominent DA methods and are evaluated under different datasets and model selection criteria. To this end, we perform a large scale rigorous comparison of popular representative DA methods in a standardized evaluation framework.\\n\\nDomain generalization\\n\\nIn domain generalization, the model is given access to data from multiple different domains and the goal is to generalize to a previously unseen domain at test time (Blanchard et al., 2011; Muandet et al., 2013). For a survey of different algorithms for domain generalization, we refer the reader to Gulrajani & Lopez-Paz (2020). A crucial distinction here is that unlike the domain generalization setting, in DA problems, we have access to unlabeled examples from the test domain.\\n\\nDistinction from previous distribution shift benchmark studies\\n\\nPrevious studies evaluating robustness under distribution shift predominantly focuses on transfer learning and domain generalization settings Wenzel et al. (2022); Gulrajani & Lopez-Paz (2020); Djolonga et al. (2021); Wiles et al. (2021); Koh et al. (2021). Taori et al. (2020); Hendrycks et al. (2021) studies the impact of robustness interventions (e.g. data augmentation techniques, adversarial training) on target (out of distribution) performance. Notably, Sagawa et al. (2021) focused on evaluating DA methods on WILDS-2.0. Our work is complementary to these studies, as we...\"}"}
{"id": "garg23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\nWe present the first extensive study of DA methods under shift in $p_y$ and natural variations in $p_x \\\\mid y$. \\n\\n3. RLSBENCH: A Benchmark for Relaxed Label Shift\\n\\nIn this section, we introduce RLSBENCH, a suite of datasets and DA algorithms that are at the core of our study. Motivated by correction methods for the (stricter) label shift setting (Saerens et al., 2002; Lipton et al., 2018) and learning under imbalanced datasets (Wei et al., 2021; Cao et al., 2019a), we also present a meta-algorithm with simple corrections compatible with almost any DA method.\\n\\n3.1. Datasets\\n\\nRLSBENCH builds on 14 multi-domain datasets for classification, including tasks across applications in object classification, satellite imagery, medicine, and toxicity detection. Across these datasets, we obtain a total of 56 different source and target pairs. More details about datasets are in App. D.\\n\\n(i) CIFAR-10 which includes the original CIFAR-10 (Krizhevsky & Hinton, 2009), CIFAR-10-C (Hendrycks & Dietterich, 2019) and CIFAR-10v2 (Recht et al., 2018); (ii) CIFAR-100 including the original dataset and CIFAR-100-C; (iii) all four BREEDs datasets (Santurkar et al., 2021), i.e., Entity13, Entity30, Nonliving26, Living17.\\n\\nBREEDs leverages class hierarchy in ImageNet (Rusakovsky et al., 2015) to repurpose original classes to be the subpopulations and define a classification task on superclasses. We consider subpopulation shift and natural shifts induced due to differences in the data collection process of ImageNet, i.e., ImageNetv2 (Recht et al., 2019) and a combination of both. (iv) OfficeHome (Venkateswara et al., 2017) which includes four domains: art, clipart, product, and real; (v) DomainNet (Peng et al., 2019) where we consider four domains: clipart, painting, real, sketch; (vi) Visda (Peng et al., 2018; 2017) which contains three domains: train, val and test; (vii) FMoW (Koh et al., 2021; Christie et al., 2018) from WILDS benchmark which includes three domains: train, OOD val, and OOD test\u2014with satellite images taken in different geographical regions and at different times; (viii) Camelyon (Bandi et al., 2018) from WILDS benchmark which includes three domains: train, OOD val, and OOD test, for tumor identification with domains corresponding to different hospitals; (ix) Civilcomments (Borkan et al., 2019) which includes three domains: train, OOD val, and OOD test, for toxicity detection with domains corresponding to different demographic subpopulations; (x) Retiring Adults (Ding et al., 2021) where we consider the ACSIncome prediction task with various domains representing different states and time-period; and (xi) Mimic Readmission (Johnson et al., 2020; PhysioBank, 2000) where the task is to predict readmission risk with various domains representing data from different time-period.\\n\\nSimulating a shift in target marginal\\n\\nThe above datasets present minor to no shift in label marginal. Hence, we simulate such a shift by altering the target label marginal and keeping the source target distribution fixed (to the original source label distribution). Note that, unlike some previous studies, we do not alter the source label marginal because, in practice, we may have an option to carefully curate the training distribution but might have little to no control over the target label marginal.\\n\\nFor each target dataset, we have the true labels which allow us to vary the target label distribution. In particular, we sample the target label marginal from a Dirichlet distribution with a parameter $\\\\alpha \\\\cdot \\\\beta$. Specifically, $p_{t, y} \\\\sim \\\\text{Dir}(\\\\beta)$ where $\\\\beta = \\\\alpha \\\\cdot p_{t, y}$ and $p_{t, y}$ is the original target label marginal. The Dirichlet parameter $\\\\alpha$ controls the severity of shift in target label marginal. Intuitively, as $\\\\alpha$ decreases, the severity of the shift increases. For completeness, we also include the target dataset with the original target label marginal. For ease of exposition, we denote the shifts as NOONE (no external shift) in the set of Dirichlet parameters, i.e. the limiting distribution as $\\\\alpha \\\\rightarrow \\\\infty$. After simulating the shift in the target label marginal (with two seeds for each $\\\\alpha$), we obtain 560 pairs of different source and target datasets.\\n\\n3.2. Domain Adaptation Methods\\n\\nWe implement the following algorithms (a more detailed description of each method is included in App. L):\\n\\nSource only\\n\\nAs a baseline, we include model trained with empirical risk minimization (Vapnik, 1999) with cross-entropy loss on the source domain. We include source only models trained with and without augmentations. We also include adversarial robust models trained on source data with augmentations (Source (adv)). In particular, we use models adversarially trained against $\\\\ell_2$-perturbations.\\n\\nDomain alignment methods\\n\\nThese methods employ domain-adversarial training schemes aimed to learn invariant representations across different domains (Ganin et al., 2016; Zhang et al., 2019; Tan et al., 2020). For our experiments, we include the following five methods: Domain Adversarial Neural Networks (DANN (Ganin et al., 2016)), Conditional DANN (CDANN (Long et al., 2018), Maximum Classifier Discrepancy (MCD (Saito et al., 2018)), Importance-reweighted DANN and CDANN (i.e. IW-DANN & IW-CDANN Tachet des Combes et al. (2020)).\\n\\nSelf-training methods\\n\\nThese methods \\\"pseudo-label\\\" unlabeled examples with the model's own predictions and then train on them as if they were labeled examples. For vision datasets, these methods often also use consistency regularization, which encourages the model to make consistent predictions.\"}"}
{"id": "garg23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Recall that we experiment with target label marginal estimation methods that leverage off-the-shelf classifiers to obtain an estimate. We observe that estimators leveraging DA classifiers tend to perform better than using source-only classifiers for tabular and vision datasets (Fig. 4). For NLP, we observe that DA classifier and source-only classifier have performance (with source-only often performing slightly better). Correspondingly, as one might expect, better estimation yields greater accuracy improvements when applying our RW correction. In particular, RW correction with DA methods improves over the source-only classifier for vision and tabular datasets and vice-versa for NLP datasets. (Fig. 4).\\n\\nEarly stopping criterion matters. We observe a consistent 2% and 8% accuracy difference on vision and tabular datasets respectively with all methods (Fig. 14). On NLP datasets, while the early stopping criteria have 2% accuracy difference when RW and RS corrections are not employed, the difference becomes negligible when these corrections are employed (Fig. 14). These results highlight that subsequent works should describe the early stopping criteria used within their evaluations.\\n\\nData augmentation helps. Corroborating findings from previous studies in other settings (Gulrajani & Lopez-Paz, 2020; Sagawa et al., 2021), we observe that data augmentation can improve the performance of a source-only model on vision datasets in relaxed label shift scenarios (refer to result on each dataset in App. J). Thus, whenever applicable, subsequent methods should use data augmentations.\\n\\n5. Conclusion\\n\\nOur work is the first large-scale study investigating methods under the relaxed label shift scenario. Relative to works operating strictly under the label shift assumption, RLSBENCH provides an opportunity for sensitivity analysis, allowing researchers to measure the robustness of their methods under various sorts of perturbations to the class-conditional distributions. Relative to the benchmark-driven deep domain adaptation literature, our work provides a comprehensive and standardized suite for evaluating under shifts in label distributions, bringing these benchmarks one step closer to exhibit the sort of diversity that we should expect to encounter when deploying models in the wild. On one hand, the consistent improvements observed from label shift adjustments are promising. At the same time, given the underspecified nature of the problem, practitioners must remain vigilant and take performance on any benchmark with a grain of salt, considering the various ways that it might (or might not) be representative of the sorts of situations that might arise in their application of interest.\\n\\nIn the future, we hope to extend RLSBENCH to datasets from real applications in consequential domains such as healthcare and self-driving, where label marginals and class conditionals can be expected to shift across locations and over time. We also hope to incorporate self-supervised methods that learn representations by training on a union of unlabeled data from source and target via proxy tasks like reconstruction (Gidaris et al., 2018; He et al., 2022) and contrastive learning (Caron et al., 2020; Chen et al., 2020). While re-weighting predictions using estimates of the target label distribution yields significant gains, the remaining gap between our results and oracle performance should motivate future work geared towards improved estimators. Also, we observe that the success of target label marginal estimation techniques depends on the nature of the shifts in $p_x|y$. Mathematically characterizing the behavior of label shift estimation techniques when the label shift assumption is violated would be an important contribution.\\n\\nReproducibility Statement\\n\\nOur code with all the results will be released on github. https://github.com/acmi-lab/RLSbench. We implement our RLSBENCH library in PyTorch (Paszke et al., 2017) and provide an infrastructure to run all the experiments to generate corresponding results. We have stored all models and logged all hyperparameters to facilitate reproducibility. In our appendices, we provide additional details on datasets and experiments. In App. D, we describe datasets and in App. M, we provide hyperparameter details.\\n\\nAcknowledgments\\n\\nWe thank Amrith Setlur, Pratyush Maini, and Aditi Ragunathan for providing feedback on an earlier draft of RLSbench. We also thank Xingjian Shi and Weisu Yin for their initial help with running the large-scale experiments. SG acknowledges Amazon Graduate Fellowship and JP Morgan AI Ph.D. Fellowship for their support.\\n\\nReferences\\n\\nAmr Alexandari, Anshul Kundaje, and Avanti Shrikumar. Adapting to label shift with bias-corrected calibration. In International Conference on Machine Learning (ICML), 2021.\\n\\nJing An, Lexing Ying, and Yuhua Zhu. Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients. arXiv preprint arXiv:2009.13447, 2020.\\n\\nKamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learning for domain adaptation under label shifts. In International Conference on Learning Representations (ICLR), 2019.\"}"}
{"id": "garg23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\nChristina Baek, Yiding Jiang, Aditi Raghunathan, and Zico Kolter. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. arXiv preprint arXiv:2206.13089, 2022.\\n\\nPeter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE Transactions on Medical Imaging, 2018.\\n\\nJessa Bekker and Jesse Davis. Learning from positive and unlabeled data: a survey. Machine Learning, 2020. URL https://doi.org/10.1007/s10994-020-05877-5.\\n\\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1-2), 2010a.\\n\\nShai Ben-David, Tyler Lu, Teresa Luu, and D\u2019avid P\u2019al. Impossibility Theorems for Domain Adaptation. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2010b.\\n\\nDavid Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain adaptation. arXiv preprint arXiv:2106.04732, 2021.\\n\\nGilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. Advances in neural information processing systems, 24, 2011.\\n\\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In Companion Proceedings of The 2019 World Wide Web Conference, 2019.\\n\\nMateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. Neural networks, 106:249\u2013259, 2018.\\n\\nJonathon Byrd and Zachary C Lipton. What is the effect of importance weighting in deep learning? In International Conference on Machine Learning (ICML), 2019.\\n\\nKaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. In Advances in Neural Information Processing Systems, volume 32, 2019a.\\n\\nZhangjie Cao, Kaichao You, Mingsheng Long, Jianmin Wang, and Qiang Yang. Learning to transfer examples for partial domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2985\u20132994, 2019b.\\n\\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912\u20139924, 2020.\\n\\nNitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority oversampling technique. Journal of artificial intelligence research, 16:321\u2013357, 2002.\\n\\nJiefeng Chen, Frederick Liu, Besim Avci, Xi Wu, Yingyu Liang, and Somesh Jha. Detecting errors and estimating accuracy on unlabeled data with self-training ensembles. Advances in Neural Information Processing Systems, 34:14980\u201314992, 2021.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597\u20131607. PMLR, 2020.\\n\\nGordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\\n\\nCorinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and algorithm for regression. Theoretical Computer Science, 519, 2014.\\n\\nCorinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning Bounds for Importance Weighting. In Advances in Neural Information Processing Systems (NIPS), 2010.\\n\\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 702\u2013703, 2020.\\n\\nWeijian Deng and Liang Zheng. Are labels always necessary for classifier accuracy evaluation? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15069\u201315078, 2021.\\n\\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.\\n\\nFrances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine 10\"}"}
{"id": "garg23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\nAdvances in Neural Information Processing Systems, 34:6478\u20136490, 2021.\\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D'Amour, Dan Moldovan, et al. On robustness and transferability of convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16458\u201316468, 2021.\\nCharles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data. In International Conference Knowledge Discovery and Data Mining (KDD), pp. 213\u2013220, 2008.\\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc \u00b8ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 2016.\\nJacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\\nSaurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary Lipton. A unified view of label shift estimation. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\nSaurabh Garg, Yifan Wu, Alex Smola, Sivaraman Balakrishnan, and Zachary Lipton. Mixture proportion estimation and PU learning: A modern approach. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\\nSaurabh Garg, Sivaraman Balakrishnan, and Zachary Lipton. Domain adaptation under open set label shift. In Advances in Neural Information Processing Systems (NeurIPS), 2022a.\\nSaurabh Garg, Sivaraman Balakrishnan, Zachary Lipton, Behnam Neyshabur, and Hanie Sedghi. Leveraging unlabeled data to predict out-of-distribution performance. In International Conference on Learning Representations (ICLR), 2022b.\\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.\\nArthur Gretton, Alexander J Smola, Jiayuan Huang, Marcel Schmittfull, Karsten M Borgwardt, and Bernhard Sch\u00a8olkopf. Covariate Shift by Kernel Mean Matching. Journal of Machine Learning Research (JMLR), 2009.\\nDevin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell, and Ludwig Schmidt. Predicting with confidence on unseen distributions. arXiv preprint arXiv:2107.03315, 2021.\\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Computer Vision and Pattern Recognition (CVPR), 2016.\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000\u201316009, 2022.\\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.\\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadhavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8340\u20138349, 2021.\\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700\u20134708, 2017.\\nBadr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves competitive worst-group-accuracy. In Conference on Causal Learning and Reasoning, pp. 336\u2013351. PMLR, 2022.\\nJunguang Jiang, Yang Shu, Jianmin Wang, and Mingsheng Long. Transferability in deep learning: A survey, 2022.\\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of sgd via disagreement. arXiv preprint arXiv:2106.13799, 2021.\\nFredrik D. Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in domain-invariant representations. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research. PMLR, 2019.\"}"}
{"id": "garg23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "garg23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\nMeta algorithm to handle label marginal shift\\n\\n1: \\\\( \\\\text{SampleClassBalanced}(p_X^S, Y^S) \\\\)\\n\\n2: for \\\\( t = 1 \\\\) to \\\\( T \\\\) do\\n\\n3: \\\\( \\\\hat{p}_Y^T \\\\leftarrow \\\\arg \\\\max_y f_y(p_X^T) \\\\)\\n\\n4: \\\\( \\\\text{SampleClassBalanced}(p_X^T, \\\\hat{p}_Y^T) \\\\)\\n\\n5: Run an epoch of \\\\( A \\\\) to update \\\\( f \\\\) on balanced source data \\\\( p_X^S, \\\\hat{Y}^S \\\\) and target data \\\\( p_X^T, \\\\hat{Y}^T \\\\)\\n\\n6: end for\\n\\n7: \\\\( \\\\hat{p}_p^T \\\\leftarrow \\\\text{EstimateLabelMarginal}(\\\\hat{f}_i, X^1_S, Y^1_S, X^1_T) \\\\)\\n\\n8: \\\\( f^j \\\\leftarrow p^p_y \\\\leftarrow \\\\sum_{k} f^k \\\\leftarrow \\\\sum_{k} f_y^k(p_X^T) \\\\) for all \\\\( j \\\\in Y \\\\)\\n\\nOutput\\nTarget label marginal \\\\( \\\\hat{p}_p^T \\\\) and classifier \\\\( f^j \\\\)\\n\\nsustainable predictions on augmented views of unlabeled exam-\\nptoms (Lee et al., 2013; Xie et al., 2020b; Berthelot et al., 2021). We include the following three algorithms: Fix-\\nMatch (Sohn et al., 2020), Noisy Student (Xie et al., 2020a), Selective Entropy Optimization via Committee Consis-\\ntency (SENTRY (Prabhu et al., 2021)). For NLP and tab-\\nular dataset, where we do not have strong augmentations\\ndefined, we consider PseudoLabel algorithm (Lee et al., 2013).\\n\\nTest-time adaptation methods\\nThese methods take a\\nsource model and adapt a few parameters (e.g. batch norm\\nparameters, etc.) on the unlabeled target data with an aim\\nto improve target performance. We include: CORAL (Sun\\net al., 2016) or Domain Adjusted Regression (DARE (Rosen-\\nfeld et al., 2022)), BatchNorm adaptation (BN-adapt (Li\\net al., 2016; Schneider et al., 2020)), Test entropy minimiza-\\ntion (TENT (Wang et al., 2021)).\\n\\n3.3. Meta algorithm to handle target label marginal shift\\nHere we discuss two simple general-purpose corrections\\nthat we implement in our framework. First, note that, as\\nthe severity of shift in the target label marginal increases,\\nthe performance of DA methods can falter as the training\\nis done over source and target datasets with different class\\nproportions. Indeed, failure of domain adversarial training\\nmethods (one category of deep DA methods) has been theo-\\nretically and empirically shown in the literature (Wu et al.,\\n2019; Zhao et al., 2019). In our experiments, we show that\\na failure due to a shift in label distribution is not limited to\\ndomain adversarial training methods, but is common with\\nall the popular DA methods (Sec. 4).\\n\\nRe-sampling\\nTo handle label imbalance in standard su-\\npervised learning, re-sampling the data to balance the class\\nmarginal is a known successful strategy (Chawla et al., 2002;\\nBuda et al., 2018; Cao et al., 2019b). In relaxed label shift,\\nwe seek to handle the imbalance in the target data (with re-\\nspect to the source label marginal), where we do not have\\naccess to true labels. We adopt an alternative strategy of\\nleveraging pseudolabels for target data to perform pseudo\\nclass-balanced re-sampling (Zou et al., 2018; Wei et al.,\\n2021). For relaxed label shift problems, (Prabhu et al.,\\n2021) employed this technique with their committee consis-\\ntency objective, SENTRY. However, they did not explore re-\\nsampling based correction for existing DA techniques. Since\\nthis technique can be used in conjunction with any DA meth-\\nods, we employ this re-sampling technique with existing\\nDA methods and find that re-sampling benefits all DA meth-\\nods, often improving over SENTRY in our testbed (Sec. 4).\\n\\nRe-weighting\\nWith re-sampling, we can hope to train the\\nclassifier \\\\( p_f \\\\) on a mixture of balanced source and balanced\\ntarget datasets in an ideal case. However, this still leaves\\nopen the problem of adapting the classifier \\\\( p_f \\\\) to the original\\ntarget label distribution which is not available. If we can es-\\ntimate the target label marginal, we can post-hoc adapt the\\nclassifier \\\\( p_f \\\\) with a simple re-weighting correction (Lipton\\net al., 2018; Alexandari et al., 2021). To estimate the tar-\\nget label marginal, we turn to techniques developed under\\nthe stricter label shift assumption (recall, the setting where\\n\\\\( p_x | y \\\\) remains domain invariant). These approaches lever-\\nage off-the-shelf classifiers to estimate target marginal and\\nprovide \\\\( \\\\mathcal{O}(\\\\frac{1}{n}) \\\\) convergence rates under the label shift\\ncondition with mild assumptions on the classifier (Lipton\\net al., 2018; Azizzadenesheli et al., 2019; Garg et al., 2020).\\n\\nWhile the relaxed label shift scenario violates the condi-\\ntions required for consistency of label shift estimation tech-\\nniques, we nonetheless employ these techniques and em-\\npirically evaluate efficacy of these methods in our testbed.\\nIn particular, to estimate the target label marginal, we ex-\\nperiment with: (i) RLLS (Azizzadenesheli et al., 2019); (ii)\\nMLLS (Alexandari et al., 2021); and (iii) baseline estimator\\nthat simply averages the prediction of a classifier \\\\( f \\\\) on un-\\nlabeled target data. We provide precise details about these\\nmethods in App. F. Since these methods leverage off-the-\\nshelf classifiers, classifiers obtained with any DA methods\\ncan be used in conjunction with these estimation methods.\\n\\nSummary\\nOverall, in Algorithm 1, we illustrate how to in-\\ncorporate the re-sampling and re-weighting correction with\\nexisting DA techniques. Fig. 9 in App. E illustrates the\\nmethod. Algorithm \\\\( A \\\\) can be any DA method and in Step\\n7, we can use any of the three methods listed above to esti-\\nmate the target label marginal. We instantiate Algorithm 1\\nA different strategy could be to re-sample target pseudolabel\\nmarginal to match source label marginal. For simplicity, we choose\\nto balance source label marginal and target pseudolabel marginal.\"}"}
{"id": "garg23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\nwith several algorithms from Sec. 3.2 in App. L. Intuitively, in an ideal scenario when the re-sampling step in our meta-algorithm perfectly corrects for label imbalance between source and target, we expect DA methods to adapt classifier \\\\( f \\\\) to \\\\( p|y \\\\). The re-weighting step in our meta-algorithm can then adapt the classifier \\\\( f \\\\) to the target label marginal \\\\( p_t \\\\). We emphasize that in our work, we do not claim to propose these corrections. But, to the best of our knowledge, our work is the first to combine these two corrections together and perform extensive experiments across diverse datasets.\\n\\n3.4. Other choices for realistic evaluation\\n\\nFor a fair evaluation and comparison across different datasets and DA algorithms, we re-implemented all the algorithms with consistent design choices whenever applicable. We also make several additional implementation choices, described below. We defer the additional details to App. M.\\n\\nModel selection criteria and hyperparameters\\n\\nGiven that we lack validation i.i.d data from the target distribution, model selection in DA problems can not follow the standard workflow used in supervised training. Prior works often omit details on how to choose hyperparameters leaving open a possibility of choosing hyperparameters using the test set which can provide a false and unreliable sense of improvement. Moreover, inconsistent hyperparameter selection strategies can complicate fair evaluations misassociating the improvements to the algorithm under study. In our work, we use source hold-out performance to pick the best hyperparameters. First, for \\\\( \\\\ell_2 \\\\) regularization and learning rate, we perform a sweep over random hyperparameters to maximize the performance of source only model on the hold-out source data. Then for each dataset, we keep these hyperparameters fixed across DA algorithms. For DA methods specific hyperparameters, we use the same hyperparameters across all the methods incorporating the suggestions made in corresponding papers. Within a run, we use hold out performance on the source to pick the early stopping point. In appendices, we report oracle performance by choosing the early stopping point with target accuracy.\\n\\nEvaluation criteria\\n\\nTo evaluate the target label marginal estimation, we report \\\\( \\\\ell_1 \\\\) error between the estimated label distribution and true label distribution. To evaluate the classifier performance on target data, we report performance of the (adapted) classifier on a hold-out partition of target data.\\n\\nArchitectural and pretraining details\\n\\nWe experiment with different architectures (e.g., DenseNet121, Resenet18, Resnet50, DistilBERT, MLP and Transformer). We experiment with randomly-initialized models and Imagenet, and DistillBert pre-trained models. Given a dataset, we use the same architecture across different DA algorithms.\\n\\nData augmentation\\n\\nData augmentation is a standard ingredient to train vision models which can approximate some of the variations between domains. Unless stated otherwise, we train all the vision datasets using the standard strong augmentation technique: random horizontal flips, random crops, augmentation with Cutout (DeVries & Taylor, 2017), and RandAugment (Cubuk et al., 2020). To understand help with data augmentations alone, we also experiment with source-only models trained without any data augmentation. For tabular and NLP datasets, we do not use any augmentations.\\n\\n4. Main Results\\n\\nWe present aggregated results on vision datasets in our testbed in Fig. 2. In App. B, we present aggregated results on NLP and tabular datasets. We include results on each dataset in App. J. Note that we do not include RS results with a source only model as it is trained only on source data and we observed no differences with just balancing the source data (as for most datasets source is already balanced) in our experiments. Unless specified otherwise, we use source validation performance as the early stopping criterion. Based on running our entire RLSBENCH suite, we distill our findings into the following takeaways.\\n\\nPopular deep DA methods without any correction fall.\\n\\nWhile DA methods often improve over a source-only classifier for cases when the target label marginal shift is absent or low, the performance of these methods (except Noisy Student) drops below the performance of a source-only classifier when the shift in target label marginal is severe (i.e., when \\\\( \\\\alpha \\\\leftarrow 0.5 \\\\) in Fig. 2a, 5a, and 6a). On the other hand, DA methods when paired with RS and RW correction, significantly improve over a source-only model even when the shift in target label marginal is severe (Fig. 2b, 5b, and 6b).\\n\\nRe-sampling to pseudobalance target often helps all DA methods across all modalities.\\n\\nWhen the shift in target label marginal is absent or very small (i.e., \\\\( \\\\alpha \\\\rightarrow \\\\text{ONE} \\\\), 10.0 in Fig. 2b, 5b, and 6b), we observe no (significant) differences in performance with re-sampling. However, as the shift severity in target label marginal increases (i.e., \\\\( \\\\alpha \\\\rightarrow 3.0 \\\\rightarrow 1.0 \\\\rightarrow 0.5 \\\\) in Fig. 2b, 5b, and 6b), we observe that re-sampling typically improves all DA methods in our testbed.\\n\\nBenefits of post-hoc re-weighting of the classifier depends on shift severity and the underlying DA algorithm. For domain alignment methods (i.e. DANN and CDANN) and self-training methods, in particular FixMatch and PseudoLabel, we observe that RW correction typically improves (over no correction) significantly when the target label marginal shift is severe (i.e., \\\\( \\\\alpha \\\\rightarrow 3.0 \\\\rightarrow 1.0 \\\\rightarrow 0.5 \\\\) in Fig. 2b, 5b, and 6b) and has no (significant) effect when the shift in target label marginal is absent or very small (i.e., \\\\( \\\\alpha \\\\rightarrow \\\\text{ONE} \\\\), 10.0 in Fig. 2b, 5b, and 6b). For BN-adapt, TENT, and NoisyS-\"}"}
{"id": "garg23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\n| Test-time training methods | Source only | BN-adapt | TENT | CORAL | DANN | IW-DANN | CDANN | IW-CDANN | FixMatch | NoisyStudent | SENTRY |\\n|---------------------------|-------------|---------|------|-------|------|---------|-------|----------|----------|-------------|--------|\\n| Relative Accuracy         | None        | 10.0    | 3.0  | 1.0   | 0.5  | 0.0     | 0.0   | 0.0      | 0.0      | 0.0         | 0.0    |\\n\\n### (a) Performance of DA methods relative to source-only training with increasing severity of target label marginal shift\\n\\n- Smaller the Dirichlet shift parameter, the more severe is the shift in target class proportion.\\n\\n### (b) Performance of DA methods relative to source-only training when paired with our meta-algorithm (RS and RW corrections)\\n\\n- RS and RW (in our meta-algorithm) together significantly improve aggregate performance over no correction for all DA methods.\\n- While RS consistently helps (over no correction) across different label marginal shift severities, RW hurts slightly for BN-adapt, TENT, and NoisyStudent when shift severity is small.\\n- However, for severe shifts, RW significantly improves performance for all the methods.\\n\\nParallel results on tabular and language datasets in App. B. Detailed results with all methods on individual datasets in App. J. A more detailed description of the plotting technique in App. A.\"}"}
{"id": "garg23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RLSbench: Domain Adaptation Under Relaxed Label Shift\\n\\nCORAL\\nCORAL (RS+RW)\\nSource (w/o aug) (RW)\\nSource (w/o aug)\\nSENTRY\\nSource (w aug)\\nSource (w aug) (RW)\\nBN-adapt\\nTENT\\nNoisyStudent (RS+RW)\\nNoisyStudent\\nIW-DANN\\nDANN\\nBN-adapt (RS+RW)\\nTENT (RS+RW)\\nCDANN\\nIW-CDANN\\nDANN (RS+RW)\\nCDANN (RS+RW)\\nFixMatch\\nFixMatch (RS+RW)\\n\\nFigure 3. Average accuracy of different DA methods aggregated across all distribution pairs in each modality. Parallel results with all methods on individual datasets in App. J.\\n\\nFigure 4. Target label marginal estimation ($\\\\ell_1$) error and accuracy with RLLS and classifiers obtained with different DA methods. (Left) Across all shift severities in vision datasets, RLLS with classifiers obtained with DA methods improves over RLLS with a source-only classifier. (Right) For tabular datasets, RLLS with classifiers obtained with DA methods improves over RLLS with a source-only classifier for severe target label marginal shifts. Plots for each DA method and all datasets are in App. H.\\n\\nShifts in our testbed), RW correction does no harm to performance for BN-adapt, TENT, and NoisyStudent even when the target label marginal shift is less severe or absent (refer to datasets in App. J).\\n\\nDA methods paired with our meta-algorithm often improve over source-only classifier but no one method consistently performs the best. First, we observe that our source-only numbers are better than previously published results. Similar to previous studies (Gulrajani & Lopez-Paz, 2020), this can be attributed to improved design choices (e.g. data augmentation, hyperparameters) which we make consistent across all methods. While there is no consistent method that does the best across datasets, overall, FixMatch with RS and RW (our meta-algorithm) performs the best for vision datasets. For NLP datasets, source-only with RW (our meta-algorithm) performs the best overall. For tabular datasets, CDANN with RS and RW (our meta-algorithm) performs the best overall (Fig. 3).\\n\\nExisting DA methods when paired with our meta-algorithm significantly outperform other DA methods specifically proposed for relaxed label shift. We observe that, with consistent experimental design across different methods, existing DA methods with RS and RW corrections often improve over previously proposed methods specifically aimed to tackle relaxed label shift, i.e., IW-CDANN, IW-DANN, and SENTRY (Fig. 7). For severe target label marginal shifts, the performance of IW-DANN, IW-CDANN, and SENTRY often falls below that of the source-only model. Moreover, while the importance weighting (i.e., IW-CDANN and IW-DANN) improves over CDANN and DANN resp. (Fig. 2a, 5a and 6a), RS and RW corrections significantly outweigh those improvements (Fig. 7).\\n\\nBN-adapt and TENT with our meta-algorithm are simple and strong baselines. For models with batch norm parameters, BN-adapt (and TENT) with RS and RW steps is a computationally efficient and strong baseline. We observe that while the performance of BN-adapt (and TENT) can drop substantially when the target label marginal shifts (i.e., $\\\\alpha_{P_t}$, $\\\\alpha_u$ in Fig. 2(a)), RS and RW correction improves the performance often improving BN-adapt (and TENT) over all other DA methods when the shift in target label marginal is extreme (i.e., $\\\\alpha_u$ in Fig. 2(b)).\\n\\nDA methods yield better target label marginal estimates, and hence larger accuracy improvements with\"}"}
