{"id": "notin22a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\nFigure 3. Combining autoregressive inference and retrieval inference. Predictions in Tranception are based on two complementary modes of inference: autoregressive predictions based on the context of previously generated tokens and predictions based on the empirical distribution of amino acid at each position in the retrieved set of homologous sequences.\\n\\n4. Inference-time retrieval\\n\\n4.1. Multiple sequence alignments\\n\\nMultiple sequence alignments retrieve neighboring proteins in sequence space and align them in the position coordinate system of the seed sequence. The overwhelming majority of fitness prediction models rely on MSAs as they inherently capture critical information about homology, phylogeny, and 3D structure of the corresponding protein family (Thompson et al., 1994; 1997). At a given position, the observed distribution of amino acids over sequences in the MSA recapitulates evolutionary constraints: the protein sequences that are part of the MSA are the variants that maintain fitness and that were not selected out by evolution.\\n\\n4.2. Two modes of inference\\n\\nWe propose to augment the autoregressive inference mode from Tranception with a second inference mode \u2013 retrieval inference (Fig. 3) \u2013 that is based on the empirical distribution of amino acids observed across sequences in the MSA. To that end, the first step consists of retrieving a MSA at inference time for a wild-type sequence from the protein family of interest. When focusing on amino acid substitutions, the retrieved set of homologous sequences is common to both the wild-type and the mutated sequences: we do a retrieval step once per family, and amortize the cost over all mutated sequences to be scored. When dealing with insertions and deletions, we tailor the retrieved MSA to each mutated sequence by deleting columns in the MSA corresponding to deleted positions and adding zero-filled columns in the MSA at inserted positions in the mutated protein. At inference time, the inserted columns or fully non-covered positions are ignored and the model solely relies on its autoregressive mode to make predictions at these positions. The second step is to compute the empirical distribution of amino acids for each aligned position based on pseudocounts (ignoring gaps) and Laplace smoothing (Jurafsky & Martin, 2008) (Appendix B.5). Since the distribution of sequences found in protein databases is biased by human sampling (certain organisms are more studied than others) and evolutionary sampling (groups of species that arose from large radiations will have over-represented proteins), we re-weigh sequences in the MSA using the scheme described in Hopf et al. (2017). Finally, we estimate the log likelihood \\\\( \\\\log \\\\ P(x) \\\\) for a protein sequence \\\\( x \\\\) by a weighted arithmetic average of the log likelihood \\\\( \\\\log \\\\ P_A(x) \\\\) from the autoregressive inference mode and the log likelihood \\\\( \\\\log \\\\ P_R(x) \\\\) obtained from the retrieval inference mode. This can be equivalently viewed as a weighted geometric average in probability space, and form a proper probability distribution up to a normalization constant:\\n\\n\\\\[\\n\\\\log \\\\ P(x) = \\\\frac{1}{C} [ (1 - \\\\alpha) \\\\log \\\\ P_A(x) + \\\\alpha \\\\log \\\\ P_R(x) ]\\n\\\\]\\n\\nThe normalization constant \\\\( C \\\\) cancels out when computing the log likelihood ratio as per equation 2, and thus we do not need to estimate it in practice. Using equation 1, we further obtain:\\n\\n\\\\[\\n\\\\log \\\\ P(x) \\\\propto \\\\sum_{i=1}^{X} [ (1 - \\\\alpha) \\\\log \\\\ P_A(x_i | x_{<i}) + \\\\alpha \\\\log \\\\ P_R(x_i) ]\\n\\\\]\\n\\nThe above scoring decomposition as the sum of position level scores is advantageous in practice as it allows us to ignore one of the two inference modes as needed (eg., ignoring retrieval inference at positions with insertions when scoring indels). Following the scoring procedure described in \u00a7 3.4, we first traverse the sequence in the canonical order from left to right and compute the sequence log probability as per equation 4 (in practice, all computations are performed in parallel across positions using masks in the self-attention layers to preserve autoregressiveness). We then perform the symmetric operation traversing the protein sequence from right to left, and finally average the two log likelihood ratios. Retrieving the MSA and computing the corresponding pseudocounts at inference is relatively cheap computationally. Since we only rely on aggregate statistics per position, our proposed approach is not very sensitive to the characteristics of the MSA for the protein family of interest (Fig. 4), nor to the hyperparameters chosen to retrieve that MSA. As Tranception is trained on a diverse set of non-aligned sequences it is not subject to the biases that may stem from solely training on proteins that can be aligned, and thus exhibits higher performance on difficult to align sequences such as disordered proteins (\u00a7 7).\\n\\n5. ProteinGym\\n\\nProteinGym is an extensive set of Deep Mutational Scanning (DMS) assays (Appendix A) curated to enable thorough...\"}"}
{"id": "notin22a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\nComparisons of various mutation effect predictors in different regimes. ProteinGym is comprised of two benchmarks: 1) a substitution benchmark which consists of the experimental characterisation of \u223c1.5M missense variants across 87 DMS assays 2) an indel benchmark that includes \u223c300k mutants across 7 DMS assays.\\n\\nThe relationship between protein fitness measured experimentally and as reflected by the distribution of sequences selected by evolution is complex. The fitness landscape of naturally occurring proteins is the result of an intricate set of overlapping constraints that these proteins are subjected to in an organism. Consequently, it is often challenging to identify a single molecular property that is both easy to measure experimentally and that reflects that complexity. To build this benchmark we therefore prioritized assays where both the experimentally-measured property for each mutated protein is expected to reflect the role of the protein in organism fitness as well as - where available - their quality measured via experimental replicates. The resulting set of DMS assays covers a wide range of functional properties (eg., thermostability, ligand binding, aggregation, viral replication, drug resistance), spans a diverse protein families (eg., kinases, ion channel proteins, g-protein coupled receptors, polymerases, transcription factors, tumor suppressors) and different taxa (eg., humans, other eukaryotes, prokaryotes, viruses).\\n\\nProteinGym is the largest and most diverse set of DMS experiments specifically targeted at the task of variant effect prediction. It contains more than twice the number of assays and variants present in the DeepSequence benchmark (Riesselman et al., 2018), which had been created for the same purpose (Table 1). While most of the curated DMS assays (in our benchmark or others) probe the effect of single amino-acid substitutions, our collection also includes several multiple amino-acid variants which are critical to assess the ability of models to extrapolate further away in sequence space from the naturally occurring proteins they are trained on. Lastly, as most mutation effect predictors are not able to quantify the effect of insertions and deletions, indels have been absent from the majority of prior benchmarks. We expand on the set of DMS available in Shin et al. (2021) and Dallago et al. (2021) to address this gap.\\n\\nThe relationship between protein function and organism fitness has been shown to often be non-linear (Boucher et al., 2016). As such, we use Spearman's rank correlation coefficient between model scores and the experimental measurements as the standard measure of model performance (Riesselman et al., 2018; Meier et al., 2021). In certain instances, the DMS measurements are characterized by a bimodal profile for which rank correlations are not well suited. To that end, we provide additional measures of model performance: the area under the ROC curve (AUC) and Matthews correlation coefficient (MCC) between model scores and the experimental measurements (Appendix E.1).\\n\\n6. Results\\n\\n6.1. Baselines\\n\\nWe compare the ability of various models to predict the effects of mutations across the DMS assays in ProteinGym. We focus on the main approaches described in \u00a7 2, including a number of protein-specific alignment-based methods \u2013 Site independent model and EVmutation (Hopf et al., 2017), DeepSequence (Riesselman et al., 2018), EVE (Frazer et al., 2021) \u2013 and large-scale protein language models trained across protein families that leverage alignments during training, such as the MSA Transformer (Rao et al., 2021) or that are alignment-free, such as ESM-1v (Meier et al., 2021). Although technically trained on subsets of unaligned sequences, Wavenet (Shin et al., 2021) models are protein-specific and use MSAs to extract their training data (as such, we group them with other alignment-based models). Tranception falls in the category of large-scale protein language models trained across families and, thanks to its two modes of inference, can be seen as a hybrid between ESM-1v and the MSA transformer. As in ESM-1v, it is trained on a large set of unaligned sequences, which makes the training procedure scalable and removes biases that would result from training on alignable proteins only. Similar to the MSA transformer, it leverages the information in a retrieved MSA to enhance fitness predictions. The critical difference is that Tranception is never trained on MSAs and is therefore less sensitive to their characteristics and limitations (Fig. 4). To allow fair comparisons across models we focus on single seed scoring only, but provide additional results with model ensembles in Appendix E.5.\\n\\n6.2. ProteinGym substitution benchmark\\n\\nWe compute the Spearman's rank correlation coefficient \u03c1, AUC and MCC between model scores and experimental measurements for all DMS assays in the ProteinGym substitution benchmark (Appendix E.2, Fig. 6), and draw very similar conclusions across metrics. Tranception with retrieval outperforms all other baselines on the overall benchmark, with markedly higher performance in the regime of low-depth MSAs (Table 2) and on multiple mutants (Table 3). When analyzing performance at the taxon level (Table 11), we observe consistently high performance from Tranception across categories, in particular on human proteins and other eukaryotes. This high performance on human proteins has immediate clinical applications, since Tranception outperforms EVE and directly extends to modelling the entire human proteome, while EVE models need to be trained for each new protein of interest and are available for only \u223c3k proteins at the time of writing. Without retrieval, Tranception...\"}"}
{"id": "notin22a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\n| Measure Category | DeepSequence | ProteinGym | Fold increase |\\n|------------------|--------------|------------|---------------|\\n| Number of assays by taxon | | | |\\n| Human | 9 | 33 | 3.7 |\\n| Other eukaryotes | 10 | 14 | 1.4 |\\n| Prokaryotes | 13 | 24 | 1.8 |\\n| Virus | 5 | 22 | 4.4 |\\n| All taxa | 37 | 93 | 2.5 |\\n\\n| Number of variants by type | | | |\\n| Single substitutions | 0.12M | 0.36M | 2.9 |\\n| Multiple substitutions | 0.55M | 1.26M | 2.3 |\\n| Indels | 0 | 0.27M | - |\\n| All variants | 0.67M | 1.89M | 2.8 |\\n\\nTable 1. Comparison of the ProteinGym and DeepSequence benchmarks. ProteinGym contains a substantially higher number of assays and variants compared to DeepSequence. It addresses notable gaps such as the limited number of viral DMS assays, limited multiple substitutions assays and absence of indels benchmark.\\n\\n| Model | Model | Spearman\u2019s rank correlation by MSA depth |\\n|-------|-------|----------------------------------------|\\n|       |       | Low | Medium | High | All | All |\\n| Alignment-based models | | | | | | | |\\n| Site indep | 0.428 | 0.403 | 0.350 | 0.397 | 0.725 |\\n| Wavenet | 0.319 | 0.398 | 0.469 | 0.398 | 0.725 |\\n| DeepSequence | 0.375 | 0.397 | 0.506 | 0.415 | 0.733 |\\n| EVmutation | 0.401 | 0.421 | 0.468 | 0.427 | 0.738 |\\n| EVE | 0.408 | 0.440 | 0.507 | 0.448 | 0.751 |\\n| Protein language models | | | | | | | |\\n| ESM-1v | 0.321 | 0.348 | 0.484 | 0.371 | 0.713 |\\n| MSA Transformer | 0.373 | 0.418 | 0.482 | 0.422 | 0.737 |\\n| Tranception (w/o retrieval) | 0.394 | 0.398 | 0.439 | 0.406 | 0.728 |\\n| Tranception (w/ retrieval) | 0.453 | 0.438 | 0.488 | 0.451 | 0.754 |\\n\\nTable 2. Average AUC and Spearman\u2019s rank correlation between model scores and experimental measurements by MSA depth on the ProteinGym substitution benchmark. Alignment depth for the various proteins is measured by the ratio of the effective number of sequences $N_{\\\\text{eff}}$ in the MSA, using the same weighting scheme as in (Hopf et al., 2017), by the length covered $L$:\\n\\n- Low: $N_{\\\\text{eff}}/L < 1$\\n- Medium: $1 < N_{\\\\text{eff}}/L < 100$\\n- High: $N_{\\\\text{eff}}/L > 100$\\n\\nTranception outperforms all other baselines overall, with the largest performance gaps observed on the low depth proteins. Wavenet outperforms ESM-1v, the only baseline which also does not leverage alignments for inference. The performance lift is particularly significant on proteins with shallow alignments, on multiple mutations and viral proteins.\\n\\n6.3. ProteinGym indel benchmark\\n\\nWe report the performance metrics (Spearman, AUC, MCC) on the ProteinGym indel benchmark (Table 4) and compare the performance of Tranception with Wavenet, the only baseline from the set described in \u00a7 6.1 that is able to quantify the effects of deletions or insertions. Other alignment-based models are constrained by the fixed coordinate system from the original MSA they have been trained on. ESM-1v and MSA transformer both rely on a scoring heuristic (Appendix D) that requires the mutated position to exist in the wild-type sequence. On the indel benchmark, Tranception outperforms Wavenet both with and without retrieval.\\n\\n7. Discussion\\n\\nGain of scope for proteins with shallow alignments. Tranception with retrieval outperforms other mutation effects predictors on the ProteinGym substitutions and indels benchmarks. While retrieval is key to achieve such results, our method only relies on high level statistics of the retrieved alignments. It is thus fairly robust to the various hyperparameters selected to generate these alignments \u2013 even when they result in relatively shallow alignments. When progressively removing sequences in the MSA based on their minimum similarity to the seed sequence (Fig. 4), we observe that Tranception has consistently high performance, unlike EVE or MSA Transformer. Given that one can obtain shallow MSAs for far more proteins than deep MSAs, our lightweight inference-time retrieval can be effectively leveraged to enhance mutation effect predictions for the vast majority of proteins. Disordered proteins are examples of...\"}"}
{"id": "notin22a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\n| Model Name         | Spearman\u2019s Rank Correlation |\\n|--------------------|------------------------------|\\n|                   | 1  | 2  | 3  | 4  | 5+ | All |\\n| Site indep         | 0.396 | 0.325 | 0.286 | 0.319 | 0.421 | 0.397 |\\n| Wavenet            | 0.394 | 0.344 | 0.329 | 0.281 | 0.396 | 0.398 |\\n| DeepSequence       | 0.415 | 0.394 | 0.372 | 0.304 | 0.418 | 0.415 |\\n| EVmutation         | 0.427 | 0.392 | 0.379 | 0.319 | 0.433 | 0.427 |\\n| EVe               | 0.448 | 0.392 | 0.375 | 0.334 | 0.420 | 0.448 |\\n| Protein language models |\\n| ESM-1v             | 0.372 | 0.291 | 0.190 | 0.160 | 0.245 | 0.371 |\\n| MSA Transformer    | 0.423 | 0.359 | 0.390 | 0.327 | 0.431 | 0.422 |\\n| Tranception (w/o retrieval) | 0.397 | 0.412 | 0.425 | 0.335 | 0.479 | 0.406 |\\n| Tranception (w/ retrieval) | 0.448 | 0.435 | 0.443 | 0.368 | 0.499 | 0.451 |\\n\\nTable 3. Average Spearman\u2019s rank correlation between model scores and experimental measurements by mutation depth.\\n\\nMutation depth is measured by the number of distinct substitutions compared with the wild-type sequence.\\n\\n| Model Name         | AUC  | Spearman\u2019s Rank Correlation |\\n|--------------------|------|------------------------------|\\n|                   |     | 1  | 2  | 3  | 4  | 5+ | All |\\n| Wavenet            | 0.412 | 0.724 |\\n| Tranception (w/o retrieval) | 0.430 | 0.740 |\\n| Tranception (w/ retrieval)  | 0.463 | 0.759 |\\n\\nTable 4. Average AUC and Spearman\u2019s rank correlation between model scores and experimental measurements on the ProteinGym indel benchmark.\\n\\nGain of coverage. A major benefit from Tranception is gain of coverage. For example, BRCA1, the gene encoding Breast cancer type 1 susceptibility protein, has 1863 amino acids making it challenging to model with alignment based methods. Trying to obtain an alignment for the full protein results in poor diversity. Not only alignment-based models (e.g., EVE) obtain relatively weak performance when trained on such alignments (Table 15), but they are unable to make predictions for all mutations impacting regions of the protein with insufficient coverage (see Appendix D for more details on these limitations). We know from previous work (Frazer et al., 2021) that it is possible to obtain higher quality alignments and model performance when dealing with the two RING and BRCT domains of BRCA1 separately. But this comes at the cost of not being able to make predictions for the majority of the protein (outside of these two domains), and ignore potential dependencies across domains. Tranception is not subject to these coverage limitations and is able to score all potential mutations of the protein. It obtains relatively high performance without retrieval, yet benefits from leveraging the retrieval inference mode based on full-protein alignment or domain-specific alignments, with slightly higher performance for the latter.\\n\\nExtrapolating far away in sequence space. Compared with other baselines, Tranception obtains much higher performance on multiple mutants, with the gap widening with mutation depth. While we sought to include as many as-assays with multiple mutants as possible when building ProteinGym, our benchmarks are still relatively biased towards single mutant assays: on the substitution benchmark, only 11 assays out of 87 included multiple mutants. Since the actual number of multiple mutants is exponentially higher than the number of single mutants for any protein family, it is likely that the reported gap between Tranception and other baselines would only increase on a benchmark with a higher proportion of multiple mutant assays. Performing well on multiple mutants has important ramifications in several applications, in particular in machine-learning guided protein design for which accurate extrapolation of sequence likelihood far away in sequence space is critical to uncover novel desirable candidate proteins.\\n\\n8. Conclusion. Models of the effects of genetic variations are emerging as powerful tools with diverse applications \u2013 from quantifying genetic predispositions to certain pathologies to designing novel proteins. Yet our understanding of how to build these models is still in its infancy. To make progress, we need methods that can scale and perform well across the whole protein universe, including regions that are hard to align or that are recent in evolutionary terms. The model we present...\"}"}
{"id": "notin22a", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\nFigure 5. Cross-entropy loss vs number of gradient steps. Left: Tranception S architecture trained on different UniRef datasets (UniRef50, UniRef90, UniRef100). Right: Small (S), Medium (M) or Large (L) Tranception architectures trained on UniRef100.\\n\\nTable 9. Comparison of unidirectional vs bidirectional scoring. Performance is measured via Spearman\u2019s rank correlation $\\\\rho$ between model scores and experimental measurements. Analysis is performed with Tranception L (with and without retrieval), and scores are reported on both the validation DMS set and the full DMS set. The bidirectional scoring is the average of log likelihood scores obtained by traversing the sequence in the canonical direction (left to right) and the reverse direction (right to left).\\n\\nAppendix B.4. We train with the AdamW optimizer (Loshchilov & Hutter, 2019), with a learning rate schedule annealed over the first 10k steps up to the maximum value ($3 \\\\times 10^{-4}$), and then linearly decreased until the end of training. Other training hyperparameters are summarized in Table 8. In terms of computing resources, small architectures are trained on 8 V100 GPUs for $\\\\sim$1 week, medium architectures with 32 V100 GPUs for $\\\\sim$1 week, and our largest model, Tranception L, is trained on 64 A100 GPUs for $\\\\sim$2 weeks.\\n\\nWe provide the training loss curves of the different architecture variants in Fig. 2 and Fig. 5. While trained beyond the optimal number of steps for transformer models used in NLP tasks (Kaplan et al., 2020), our larger networks still appear to be undertrained. We note that cross-entropy is not necessarily a good indicator of downstream performance when comparing datasets with very different characteristics. For instance, UniRef50 is by design less redundant than UniRef100, thus we expect the loss to be typically lower on the latter (as can be seen in Fig. 5). It so happens that, for the particular task we are interested in (fitness prediction), the more granular UniRef100 both leads to lower cross-entropy loss and higher downstream task performance (Table 6), hence we trained our larger models on that dataset.\\n\\nB.4. Scoring protein sequences\\n\\nMirrored sequences. While protein sequences are not strictly invariant to mirroring (i.e., the same sequence of amino acids assembled in the left to right order may not lead to the same 3D structure if assembled from right to left), prior autoregressive transformer architectures (e.g., (Madani et al., 2020)) have proposed to augment their training dataset by including all sequences and their reverse. In this work we further investigate the benefits of using each sequence and its reverse at inference when predicting fitness (\u00a7 3.4). To remove potential discrepancies between training and inference, we apply a data augmentation similar to that of Madani et al. (2020) at training time and randomly reverse a subset of sequences in each batch (each sequence in the batch is flipped with a probability 0.5). We find that averaging the log-likelihood ratios obtained by scoring each sequence and its mirror image leads to superior downstream task performance, with or without retrieval (Table 9).\"}"}
{"id": "notin22a", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\nScoring window\\n\\nWhen scoring protein sequences that are longer than the maximum context length of the model (1,024 amino acids in our model), we have the choice to either truncate the sequence or combine predictions from slided (overlapping or non-overlapping) windows of the sequence. We observe very little performance difference between the different approaches in terms of average Spearman's rank correlation with validation DMS assays measurements, and therefore leveraged the former for simplicity. For single mutations, the optimal scoring window is selected so as to maximize the context available around that mutation for prediction when scoring the sequence from left to right and right to left. When scoring multiple mutants, we applied a very similar approach, maximizing context around the barycenter of the various mutant positions.\\n\\nB.5. Retrieval\\n\\nAugmenting the autoregressive predictions at each position via retrieval inference only occurs at the positions covered (even partially) by the MSA \u2013 outside of these positions, we fully rely on autoregressive predictions. We compute the pseudocounts at each position of the alignment via weighted Laplace smoothing (Jurafsky & Martin, 2008), with a small smoothing parameter ($10^{-5}$). As discussed in \u00a7 4, sequence are weighted as per the procedure described in (Hopf et al., 2017) and we fully ignore gaps in the MSA when computing the pseudocounts.\\n\\nWe optimize the retrieval inference weight $\\\\alpha$ from equation 4 via linearly-spaced grid search between 0.0 (no retrieval) and 1.0 (full retrieval on covered positions, autoregressive only otherwise). As per the results in table 10, the optimal $\\\\alpha$ value on the validation DMS set is 0.6. Except for the analysis discussed in this section, whenever retrieval is used in this paper, it is with this 0.6 retrieval inference weight.\\n\\n| Retrieval inference weight $\\\\alpha$ | 0.0  | 0.1  | 0.2  | 0.3  | 0.4  | 0.5  | 0.6  | 0.7  | 0.8  | 0.9  | 1.0  |\\n|-----------------------------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\\n| Validation set                    | 0.399 | 0.412 | 0.424 | 0.435 | 0.444 | 0.450 | 0.452 | 0.449 | 0.443 | 0.431 | 0.397 |\\n| Full set                          | 0.401 | 0.410 | 0.419 | 0.428 | 0.435 | 0.441 | 0.445 | 0.446 | 0.442 | 0.432 | 0.404 |\\n\\nTable 10. Retrieval inference weight optimization. We perform a linearly-spaced grid search for $\\\\alpha$ on our validation DMS set and obtain an optimal rate of 0.6.\\n\\nC. Multiple Sequence Alignments\\n\\nFor every assay in ProteinGym, we build MSAs of the corresponding protein by performing five search iterations of the profile HMM homology search tool Jackhmmer (Eddy, 2011) on the UniRef100 database of non-redundant proteins (Suzek et al., 2014), downloaded on November 24 2021. We explore a range of 9 bit score thresholds, from 0.1 to 0.9. We subsequently select the alignment with the highest number of significant Evolutionary Couplings (ECs) (Hopf et al., 2014). To be consistent in our approach across all assays, we build a single MSA for each protein, and do not investigate domain-specific alignments where relevant. As noted in Appendix E.4, crafting domain-specific alignments, when these domains are known, may help further increase performance of the different models. Characteristics of the different MSAs (e.g., number of sequences, selected bit score) we obtained with the above process are provided in the ProteinGym reference file available in our GitHub repository.\\n\\nD. Baselines\\n\\nExcept for the enhancements we discuss in this section, we use the official codebases for all baselines included in \u00a7 6. For the Site independent and EVmutation models we use the EVcouplings library. For Wavenet, we use the code made available in the SeqDesign repository. All alignment-based models are trained on the MSAs we obtain via the process discussed in Appendix C, and the same MSAs are used at inference time with MSA transformer or for retrieval in Tranception.\\n\\nD.1. ESM-1v and MSA Transformer\\n\\nWe start from the official ESM codebase, and extend it as follows to support:\\n\\n- the scoring of multiple mutants \u2013 as discussed in Meier et al. (2021) this is achieved by independently summing the effects of each single mutations that comprise the multiple mutant;\"}"}
{"id": "notin22a", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\n- the scoring of sequences that are longer than the context size of the ESM-1v and MSA Transformer models. We leverage the same routine we developed for Tranception to select the optimal scoring window (see Appendix B.4);\\n\\n- the weighted sampling of sequences of an input MSA in the MSA Transformer. We compute sequence weights with the same procedure used for retrieval inference in Tranception, i.e., based on the procedure described in Hopf et al. (2017).\\n\\nWhen scoring with ESM-1v or MSA Transformer, we use the masked-marginals approach introduced in Meier et al. (2021). This scoring heuristic has two main limitations:\\n\\n1. As discussed above, scores for multiple mutants are computed as the sum of the effects from the individual single mutants comprising the multiple mutation, which leads to ignoring potentially important epistatic effects.\\n\\n2. Masked scoring implies a fixed frame of reference in which the mutated position exists in the original wild-type sequence. Consequently, these two models are unable to score indels with the masked-marginals heuristic.\\n\\nAs discussed in \u00a7 6, we compute scores reported in the main results tables are obtained with a single model seed for fair comparison across models, but we also report ensemble performance in Appendix E.5. We use the most recent checkpoints made publicly available for each model (resp. esm1v\\n\\n$t33\\n\\n650M\\n\\nUR90S\\n\\n1 and esm\\n\\nmsa1b\\n\\nt12\\n\\n100M\\n\\nUR50S\\ntime of writing). For MSA transformer, we follow (Meier et al., 2021) and first filter sequences in the input MSA with HHFilter (Steinegger et al., 2019) to ensure minimum coverage of 75% and maximum sequence identity of 90%, and then sample 384 sequences (with the weights discussed above).\\n\\nD.2. EVE and DeepSequence\\n\\nFor both DeepSequence and EVE, we use the Pytorch implementation available in the official EVE github repository (we use the optimal parameters for EVE as per Frazer et al. (2021), and as per Riesselman et al. (2018) for DeepSequence).\\n\\nEVE and DeepSequence models are trained on protein-specific MSAs. Sequences with more than 50% of gaps in the MSA are removed from training. More importantly, positions in the MSA with more than 30% of gaps in the MSA are removed, such that EVE and DeepSequence models do not provide scores for mutations impacting these particular positions. When the full protein is difficult to cover, this sometimes results in a full domain of the protein being dropped (see the BRCA1 protein example in Appendix E.4). For that reason, when reporting performance results in \u00a7 6 we compare all models on the exact same set of mutants, excluding the ones that are not scored by EVE and DeepSequence. We nonetheless provide a full performance comparison on the entire set of mutants in the files available in our GitHub repository for the models that provide scores for all mutants (see \u00a7 E).\\n\\nExcept for Wavenet, all alignment-based models are unable to score indels since they rely on the fixed coordinate system from the original MSA they have been trained on.\\n\\nE. Detailed performance analysis\\n\\nE.1. Performance reporting methodology\\n\\nWe report performance based on the 3 metrics described in \u00a7 6:\\n\\n- Spearman\u2019s rank correlation $\\\\rho$ between model scores and DMS experimental measurements. Since certain DMS assays are relatively difficult to model resulting in very low (and sometimes negative) $\\\\rho$ values, we report the signed $\\\\rho$ value instead of the absolute $\\\\rho$ values reported in prior works (Riesselman et al., 2018; Meier et al., 2021). To that end, we adjust the signs of measured phenotypes where needed, to ensure consistency in the directionality across assays. In ProteinGym, a higher DMS score is always associated with higher fitness;\\n\\n- AUC between model scores and DMS experimental measurements. This metric is particularly relevant when focusing on assays with a bimodal distribution of the measured phenotype. We binarize DMS measurements by setting the threshold manually when the assay is clearly bimodal and there is no ambiguity about the correct threshold value to select. We use a median cutoff in all other instances. We report the numerical value of the cutoff and the binarization method (median or manual) in the ProteinGym reference file (available in our GitHub repository);\\n\\n- Matthew\u2019s correlation coefficient (MCC) between model scores and DMS experimental measurements. This complements the analysis obtained with AUC. DMS measurements are binarized with the same thresholds as for AUC. Predictions are binarized by using their median value as threshold.\\n\\nIn order to standardize measured outcomes as much as possible across assays, we further preprocess the raw DMS measurements as follows:\"}"}
{"id": "notin22a", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\nModel Spearman correlation by taxa category\\n\\n| Taxon        | Human | Other | Eukaryote | Prokaryote | Virus |\\n|--------------|-------|-------|-----------|------------|-------|\\n| Site indep   | 0.398 | 0.446 | 0.350     | 0.410      | 0.397 |\\n| Wavenet      | 0.388 | 0.453 | 0.480     | 0.308      | 0.398 |\\n| Deepsequence | 0.391 | 0.482 | 0.487     | 0.350      | 0.415 |\\n| EVmutation   | 0.405 | 0.475 | 0.484     | 0.380      | 0.427 |\\n| EVE          | 0.411 | 0.485 | 0.497     | 0.435      | 0.448 |\\n\\nTable 1: Average Spearman's rank correlation $\\\\rho$ between model scores and experimental measurements by taxon.\\n\\n- **Silent mutations**: certain assays include nucleotide substitutions resulting in silent mutations. We remove these from our benchmark;\\n- **Duplicate mutations**: certain assays include duplicate mutants \u2013 either nucleotide substitutions resulting in mutant repeats in the experiments, or indels resulting in identical protein sequences. We remove duplicates by averaging all DMS measurements across duplicate mutants;\\n- **Missing measurements**: mutants with missing assay measurement are dropped.\\n\\nFinally, ProteinGym contains several DMS assays for the same protein, for a handful of proteins (e.g., we include 4 assays for BCL2 ECOLX, 4 assays for P53). While all different and important in their own right, these different assays have experimental measurements that are strongly correlated. Consequently, models that tend to do well for one of these assays relative to others, tend to perform well across all of the assays for the same protein. In order to remove the potential biases resulting from these correlated assays, we first aggregate all performance metrics at the Uniprot ID level and then compute the different average results in \u00a7 6 (i.e., when there are 4 DMS assays for the same protein, each of these assays carries a weight of 0.25 in the aggregate results).\\n\\n### E.2. Detailed results\\n\\nWe report below aggregated performance results by taxon for the different metrics described in Appendix E.1 on the ProteinGym substitution benchmark: Spearman's rank in Table 11, AUC in Table 12 and MCC in Table 13. Overall model ranking is the same across all performance metrics. The Spearman's rank correlation coefficients between model scores and DMS experimental measurements for Tranception, EVE, ESM-1v and MSA Transformer for each DMS assay in the substitution benchmark are shown in Fig. 6. Performance for the same metric for Tranception and Wavenet on the ProteinGym indel benchmark is provided in Table 14.\\n\\nDetailed performance tables for all models at the Uniprot ID level and at the DMS level for both the Protein substitutions and indels benchmarks are made available in the Tranception GitHub repository.\\n\\n### E.3. MSA Filtering analyses\\n\\nWe report additional DMS level results for the MSA filtering analysis described in \u00a7 7 in Fig. 7. We observe that the performance of Tranception is less sensitive to MSA depth compared to EVE or MSA Transformer.\\n\\n### E.4. Full protein Vs domain-specific models\\n\\nFor all DMS assays we focused on full protein alignments. It may be possible to further increase the performance (and the coverage) of models leveraging MSAs by tailoring the creation of the MSA to known protein domains. For instance, we can obtain more diverse alignments for the RING and BRCT domains of BRCA1, thereby increasing the predictive performance of alignment-based models trained specifically on these alignments (Table 15). This also helps improve predictions made by Tranception with retrieval.\"}"}
{"id": "notin22a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Staller, M. V., Holehouse, A. S., Swain-Lenz, D., Das, R. K., Pappu, R. V., and Cohen, B. A. A high-throughput mutational scan of an intrinsically disordered acidic transcriptional activation domain. Cell systems, 6(4):444\u2013455, 2018.\\n\\nStarita, L. M., Pruneda, J. N., Lo, R. S., Fowler, D. M., Kim, H. J., Hiatt, J. B., Shendure, J., Brzovic, P. S., Fields, S., and Klevit, R. E. Activity-enhancing mutations in an E3 ubiquitin ligase identified by high-throughput mutagenesis. Proceedings of the National Academy of Sciences, 110(14):E1263\u2013E1272, April 2013.\\n\\nStarr, T. N., Greaney, A. J., Hilton, S. K., Ellis, D., Crawford, K. H., Dingens, A. S., Navarro, M. J., Bowen, J. E., Tortorici, M. A., Walls, A. C., King, N. P., Veesler, D., and Bloom, J. D. Deep Mutational Scanning of SARS-CoV-2 Receptor Binding Domain Reveals Constraints on Folding and ACE2 Binding. Cell, 182(5):1295\u20131310.e20, September 2020.\\n\\nSteinegger, M. and S\u00f6ding, J. Clustering huge protein sequence sets in linear time. Nature Communications, 9, 2018.\\n\\nSteinegger, M., Meier, M., Mirdita, M., V\u00f6hringer, H., Haunsberger, S. J., and S\u00f6ding, J. Hh-suite3 for fast remote homology detection and deep protein annotation. BMC Bioinformatics, 20, 2019.\\n\\nStiffler, M., Hekstra, D., and Ranganathan, R. Evolvability as a Function of Purifying Selection in TEM-1\u03b2-Lactamase. Cell, 160(5):882\u2013892, February 2015.\\n\\nSuiter, C. C., Moriyama, T., Matreyek, K. A., Yang, W., Scaletti, E. R., Nishii, R., Yang, W., Hoshitsuki, K., Singh, M., Trehan, A., Parish, C., Smith, C., Li, L., Bhojwani, D., Yuen, L. Y. P., Li, C.-k., Li, C.-h., Yang, Y.-l., Walker, G. J., Goodhand, J. R., Kennedy, N. A., Klussmann, F. A., Bhatia, S., Relling, M. V., Kato, M., Hori, H., Bhatia, P., Ahmad, T., Yeoh, A. E. J., Stenmark, P., Fowler, D. M., and Yang, J. J. Massively parallel variant characterization identifies NUDT15 alleles associated with thiopurine toxicity. Proceedings of the National Academy of Sciences, 117(10):5394\u20135401, March 2020.\\n\\nSuzek, B. E., Wang, Y., Huang, H., McGarvey, P. B., Wu, C. H., and the UniProt Consortium. UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, 31(6):926\u2013932, 11 2014.\\n\\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions, 2014.\\n\\nThompson, J. D., Higgins, D. G., and Gibson, T. J. Clustal w: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice. Nucleic acids research, 22 22:4673\u201380, 1994.\\n\\nThompson, J. D., Gibson, T. J., Plewniak, F., Jeanmougin, F., and Higgins, D. G. The clustal x windows interface: flexible strategies for multiple sequence alignment aided by quality analysis tools. Nucleic acids research, 25 24:4876\u201382, 1997.\\n\\nThompson, S., Zhang, Y., Ingle, C., Reynolds, K. A., and Kortemme, T. Altered expression of a quality control protease in E. coli reshapes the in vivo mutational landscape of a model enzyme. eLife, 9:e53476, July 2020.\\n\\nToth-Petroczy, A., Palmedo, P., Ingraham, J., Hopf, T. A., Berger, B., Sander, C., and Marks, D. S. Structured states of disordered proteins from genomic sequences. Cell, 167(1):158\u2013170, 2016.\\n\\nTripathi, A., Gupta, K., Khare, S., Jain, P. C., Patel, S., Kumar, P., Pulianmackal, A. J., Aghera, N., and Varadarajan, R. Molecular Determinants of Mutant Phenotypes, Inferred from Saturation Mutagenesis Data. Molecular Biology and Evolution, 33(11):2960\u20132975, November 2016.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2017.\\n\\nWang, S., Yu, M., Guo, X., Wang, Z., Klinger, T., Zhang, W., Chang, S., Tesauro, G., Zhou, B., and Jiang, J. R3:\"}"}
{"id": "notin22a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval.\\n\\nReinforced ranker-reader for open-domain question answering. In AAAI, 2018.\\n\\nWeile, J., Sun, S., Cote, A. G., Knapp, J., Verby, M., Mellor, J. C., Wu, Y., Pons, C., Wong, C., Lieshout, N., Yang, F., Tasan, M., Tan, G., Yang, S., Fowler, D. M., Nussbaum, R., Bloom, J. D., Vidal, M., Hill, D. E., Aloy, P., and Roth, F. P. A framework for exhaustively mapping functional missense variants. Molecular Systems Biology, 13(12):957, December 2017. ISSN 1744-4292, 1744-4292. doi: 10.15252/msb.20177908. URL https://onlinelibrary.wiley.com/doi/10.15252/msb.20177908.\\n\\nWeinstein, E. N. and Marks, D. S. A structured observation distribution for generative biological sequence prediction and forecasting. bioRxiv, pp. 2020\u201307, 2021.\\n\\nWrenbeck, E. E., Azouz, L. R., and Whitehead, T. A. Single-mutation fitness landscapes for an enzyme on multiple substrates reveal specificity is globally encoded. Nature Communications, 8(1):15695, August 2017. ISSN 2041-1723. doi: 10.1038/ncomms15695. URL http://www.nature.com/articles/ncomms15695.\\n\\nWu, N. C., Young, A. P., Al-Mawsawi, L. Q., Olson, C. A., Feng, J., Qi, H., Chen, S.-H., Lu, I.-H., Lin, C.-Y., Chin, R. G., Luan, H. H., Nguyen, N., Nelson, S. F., Li, X., Wu, T.-T., and Sun, R. High-throughput profiling of influenza A virus hemagglutinin gene at single-nucleotide resolution. Scientific Reports, 4(1):4942, May 2014. ISSN 2045-2322. doi: 10.1038/srep04942. URL http://www.nature.com/articles/srep04942.\\n\\nWu, N. C., Olson, C. A., Du, Y., Le, S., Tran, K., Remenyi, R., Gong, D., Al-Mawsawi, L. Q., Qi, H., Wu, T.-T., and Sun, R. Functional Constraint Profiling of a Viral Protein Reveals Discordance of Evolutionary Conservation and Functionality. PLOS Genetics, 11(7):e1005310, July 2015. ISSN 1553-7404. doi: 10.1371/journal.pgen.1005310. URL https://dx.plos.org/10.1371/journal.pgen.1005310.\\n\\nYoung, H. J., Chan, M., Selvam, B., Szymanski, S. K., Shukla, D., and Procko, E. Deep Mutagenesis of a Transporter for Uptake of a Non-Native Substrate Identifies Conformationally Dynamic Regions. preprint, Biochemistry, April 2021. URL http://biorxiv.org/lookup/doi/10.1101/2021.04.19.440442.\"}"}
{"id": "notin22a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nA. Glossary\\n\\nMultiple Sequence Alignment (MSA): a set of homologous protein sequences that are aligned in the position coordinate system of the seed sequence. They have been shown to capture information about the homology, phylogeny, and structure of the corresponding protein family (Thompson et al., 1994; 1997). The depth of the MSA refers to the number of sequences it contains.\\n\\nDeep Mutational Scanning (DMS): a technique which combines saturation mutagenesis with high-throughput functional screening to obtain a thorough description of the fitness landscape of the corresponding proteins (Fowler & Fields, 2014).\\n\\nGain of scope: increase in the number of distinct protein families that can be modeled. Proteins that have been historically difficult to model are the ones with shallow MSA or that are difficult to align (e.g., disordered proteins).\\n\\nB. Tranception model architecture and training details\\n\\nB.1. Ablation studies\\n\\nTranception is an autoregressive transformer architecture designed to explicitly promote head specialization and extraction of contiguous protein subsequence patterns, building on ideas introduced in Primer (So et al., 2021) and Inception (Szegedy et al., 2014). We performed thorough ablations when developing Tranception and summarize the main variants tested in Table 5. Our largest transformer model, Tranception L, has 700M parameters and is trained on UniRef100 (Suzek et al., 2014). In early iterations we also experimented training our architecture on UniRef90 and UniRef50, clustered versions of UniRef100 at 90% and 50% similarity levels respectively, but observed superior performance from training on UniRef100 (see Appendix B.3 and Table 6).\\n\\n| Hyperparameter | GPT2 | S Primer | Tranception LS | Tranception S | Tranception M | Tranception L |\\n|----------------|------|----------|----------------|---------------|---------------|---------------|\\n| Parameters     | 85M  | 85M      | 85M            | 85M           | 300M          | 700M          |\\n| Attention heads| 12   | 12       | 12             | 12            | 16            | 20            |\\n| Layers         | 12   | 12       | 12             | 12            | 24            | 36            |\\n| Embedding size | 768  | 768      | 768            | 768           | 1024          | 1280          |\\n| Activation function | GELU | Squared | Squared        | Squared       | Squared       | Squared       |\\n| Position encoding | Learned | Learned | Learned        | Learned       | Grouped       | Grouped       |\\n\\nTable 5. Characteristics of different model variants used in ablations. All models had a max context length of 1024 tokens, and we use the default dropout value of 0.1 in all variants. Tranception LS differs from Tranception S by the use of learned position embeddings instead of Grouped ALiBi.\\n\\nTo decide between different architecture options while not overfitting these decisions to our benchmark, we selected a small yet representative subset of DMS assays in the ProteinGym substitution benchmark (10 out of 87 substitution DMS assays):\\n\\n- BLAT ECOLX (Jacquier et al., 2013)\\n- CALM1 HUMAN (Weile et al., 2017)\\n- CCDB ECOLI (Tripathi et al., 2016)\\n- DLG4 RAT (McLaughlin Jr et al., 2012)\\n- PA I34A1 (Wu et al., 2015)\\n- Q2N0S5 9HIV1 (Haddox et al., 2018)\\n- RL401 YEAST (Roscoe et al., 2013)\\n- SPG1 STRSG (Olson et al., 2014)\\n- SPIKE SARS2 (Starr et al., 2020)\\n- TPOR HUMAN (Bridgford et al., 2020)\\n\\nTogether, these 10 assays cover the different taxa (3 viral proteins, 4 human and other eukaryote proteins, 3 prokaryote proteins), mutation depths (3 low, 4 medium and 3 high as per the classification described in Table 2) and include one assay with multiple mutants (which matches the overall proportion of assays with multiple mutants within ProteinGym). Downstream performance of the different ablations on this validation set, and the overall substitution set are reported in Table 6.\"}"}
{"id": "notin22a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\n| Model variant | Training data | Position encoding | Spearman $\\\\rho$ validation set | Spearman $\\\\rho$ full set |\\n|---------------|---------------|-------------------|-------------------------------|--------------------------|\\n| GPT2          | S             | Learned embedding | 0.324                         | 0.320                    |\\n| Primer        | S             | Learned embedding | 0.314                         | 0.315                    |\\n| Tranception LS| Uniref100     | Learned embedding | 0.330                         | 0.333                    |\\n| Tranception S | Uniref100     | Grouped ALiBi     | 0.344                         | 0.335                    |\\n| Tranception LS| Uniref90      | Grouped ALiBi     | 0.264                         | 0.275                    |\\n| Tranception LS| Uniref50      | Grouped ALiBi     | 0.248                         | 0.247                    |\\n| Tranception M | Uniref100     | Grouped ALiBi     | 0.358                         | 0.376                    |\\n| Tranception L | Uniref100     | Grouped ALiBi     | 0.399                         | 0.404                    |\\n\\nTable 6. Performance of the different model variants in ablation studies. Performance is measured via Spearman\u2019s rank correlation $\\\\rho$ between model scores and experimental measurements, following the approach discussed in E.1. Retrieval inference is excluded from this analysis. Model selection is performed on the validation set described in Appendix B.1.\\n\\n| Metric                  | Value          |\\n|-------------------------|----------------|\\n| Number of sequences     | 249M           |\\n| Max sequence length     | 40,921         |\\n| 95th percentile of length | 939           |\\n| 75th percentile of length | 470           |\\n| Median sequence length  | 314            |\\n| 25th percentile of length | 198           |\\n| 5th percentile of length | 92            |\\n| Min sequence length     | 12             |\\n\\nTable 7. High level statistics of protein sequences in UniRef100 after preprocessing. About 98% of sequences in UniRef100 have length lower than 1,024.\\n\\n| Hyperparameter           | Value          |\\n|-------------------------|----------------|\\n| Training steps           | 150k           |\\n| Batch size              | 1,024          |\\n| Peak learning rate      | $3 \\\\times 10^{-4}$ |\\n| Weight decay            | $10^{-4}$      |\\n| Optimizer               | AdamW          |\\n\\nTable 8. Model training hyperparameters.\\n\\nB.2. Data processing\\nExcept for the two ablations focusing on UniRef50 and UniRef90, all models are trained on UniRef100. We perform very mild filtering steps of the data to remove fragments and low quality sequences, and preserve as much sequence diversity as possible. For each UniRef100 sequence cluster, we map the corresponding UniRef50 cluster which pools together sequences within 50% similarity from one another. We use 99% of the data (\u223c249 million sequences) for training and set aside 1% of the data for validation (\u223c2.5 million sequences). All singletons at the UniRef50 cluster level are removed (e.g., isolated fragments). We further exclude from the training and validation datasets all sequences that contained the infrequent Pyrrolysine (O) or Selenocysteine (U) amino acids, or with two or more consecutive indeterminate amino acids X to remove lower quality sequences. The remaining indeterminate amino acids (X, B, J, Z) are kept at train time and randomly imputed as follows: X is imputed to any of the 20 standard amino acids, B to either D (Aspartic acid) or N (Asparagine), J to either I (Isoleucine) or L (Leucine), Z to either E (Glutamic acid) or Q (Glutamine). All sequences with indeterminates are excluded from the validation set. Table 7 recapitulates key statistics of sequences in UniRef100 after applying these filtering criteria. The observed distribution of sequences guided our choice for the maximum context length of 1,024 tokens for our transformer models, as it allows to handle 98% of protein sequences in UniRef100 without truncation.\\n\\nB.3. Model training\\nAll model variants are trained for 150k steps, with a batch size of 1,024 sequences. During training, we reverse sequences at random and truncate sequences if longer than the maximum context size as per the scoring scheme details described in...\"}"}
{"id": "notin22a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\nFigure 4. Robustness to alignment depth. We measure the average performance (Spearman's correlation with DMS measurements) of Tranception, EVE and MSA Transformer as we filter out an increasing proportion of MSA sequences based on their similarity to the seed sequence. The left figure aggregates results across all 87 substitution assays in ProteinGym, the right figure focuses on the tumor protein P53 (Kotler et al. (2018) assay; other examples are provided in Appendix E.3). The performance of Tranception is robust to MSA depth, while that of EVE and MSA transformer drops significantly as diversity in the MSA is reduced. Rightmost points for Tranception and MSA transformer correspond to performance with no retrieval and with a single-sequence input MSA (the seed) respectively. On the right figure, the histogram reports the number of sequences in the MSA per similarity to the seed sequence groupings.\\n\\nThis work makes progress in this direction on five main aspects.\\n\\nFirstly, the combination of the Tranception model architecture together with retrieval at inference delivers state-of-the-art fitness prediction performance, with a significantly stronger ability to extrapolate to multiple mutants. Scaling up the size of our transformer (Hesslow et al., 2022), together with training on a larger and more diverse set of protein sequences (Mitchell et al., 2020; Steinegger & Soding, 2018) will likely improve our performance further.\\n\\nSecondly, our suggested retrieval at inference approach is fairly robust to alignment characteristics, and Tranception performs well with retrieval of just the nearest homologs. Our model can make use of deep alignments when they are available, and small, or no alignments when need be, resulting in both high-performance and broad scope. This is a significant advantage over alignment-based methods like EVE which require deep enough alignments to capture the complex relationships across residues in the protein sequence of interest.\\n\\nThirdly, unlike most existing mutation effect predictors, Tranception is able to handle insertions and deletions out-of-the-box, and outperforms prior baselines in that regime as well.\\n\\nFourthly, we find our approach to be robust across taxa and protein families, making it well suited to a broad range of tasks. This includes predicting the effect of mutations in viruses, a key component of forecasting outbreaks, and predicting disease causing variants in humans, of value to both diagnosis and preventative care.\\n\\nFinally, our autoregressive model is naturally suited to sequence generation and hence has great potential for protein design.\\n\\nAcknowledgements\\nWe thank Lood Van Niekerk and Aaron Kollasch for their help running the DeepSequence and Wavenet baselines on the ProteinGym benchmark, the broader OATML group and Marks Lab for helpful discussions when writing this manuscript, and the Google Cloud Platform team for research credits and TRC access when developing and training our models. P.N. is supported by GSK and the UK Engineering and Physical Sciences Research Council (EPSRC ICASE award no. 18000077). M.D., J.F. and J.M.H. are supported by the Chan Zuckerberg Initiative CZI2018-191853. A.G. is a Clarendon Scholar and Open Philanthropy AI Fellow. D.S.M. holds a Ben Barres Early Career Award by the Chan Zuckerberg Initiative as part of the Neurodegeneration Challenge Network, CZI2018-191853. Y.G. holds a Turing AI Fellowship (Phase 1) at the Alan Turing Institute, which is supported by EPSRC grant reference V030302/1.\"}"}
{"id": "notin22a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "notin22a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval and modifiers of MPL-dependent oncogenic transformation identified by deep mutational scanning.\\n\\nBlood, 135(4):287\u2013292, January 2020. ISSN 0006-4971, 1528-0020. doi: 10.1182/blood.2019002561. URL https://ashpublications.org/blood/article/135/4/287/381157/\\n\\nNovel-drivers-and-modifiers-of-MPL-dependent.\\n\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners, 2020.\\n\\nChan, Y. H., Venev, S. V., Zeldovich, K. B., and Matthews, C. R. Correlation of fitness landscapes from three orthologous TIM barrels originates from sequence and structure constraints. Nature Communications, 8(1):14614, April 2017. ISSN 2041-1723. doi: 10.1038/ncomms14614. URL http://www.nature.com/articles/ncomms14614.\\n\\nChen, J. Z., Fowler, D. M., and Tokuriki, N. Comprehensive exploration of the translocation, stability and substrate recognition requirements in VIM-2 lactamase. eLife, 9:e56707, June 2020. ISSN 2050-084X. doi: 10.7554/eLife.56707. URL https://elifesciences.org/articles/56707.\\n\\nChiasson, M. A., Rollins, N. J., Stephany, J. J., Sitko, K. A., Matreyek, K. A., Verby, M., Sun, S., Roth, F. P., DeSloover, D., Marks, D. S., Rettie, A. E., and Fowler, D. M. Multiplexed measurement of variant abundance and activity reveals VKOR topology, active site and human variant impact. eLife, 9:e58026, September 2020. ISSN 2050-084X. doi: 10.7554/eLife.58026. URL https://elifesciences.org/articles/58026.\\n\\nDallago, C., Mou, J., Johnston, K. E., Wittmann, B. J., Bhat-tacharya, N., Goldman, S., Madani, A., and Yang, K. K. Flip: Benchmark tasks in fitness landscape inference for proteins. 2021.\\n\\nDandage, R., Pandey, R., Jayaraj, G., Rai, M., Berger, D., and Chakraborty, K. Differential strengths of molecular determinants guide environment specific mutational fates. PLOS Genetics, 14(5):e1007419, May 2018. ISSN 1553-7404. doi: 10.1371/journal.pgen.1007419. URL https://dx.plos.org/10.1371/journal.pgen.1007419.\\n\\nDavidi, D., Shamshoum, M., Guo, Z., Bar-On, Y. M., Prywes, N., Oz, A., Jablonska, J., Flamholz, A., Wernick, D. G., Antonovsky, N., et al. Highly active rubiscos discovered by systematic interrogation of natural sequence diversity. The EMBO journal, 39(18):e104081, 2020.\\n\\nDeng, Z., Huang, W., Bakkalbasi, E., Brown, N. G., Adamski, C. J., Rice, K., Muzny, D., Gibbs, R. A., and Palzkill, T. Deep Sequencing of Systematic Combinatorial Libraries Reveals $\\\\beta$-Lactamase Sequence Constraints at High Resolution. Journal of Molecular Biology, 424(3-4):150\u2013167, December 2012. ISSN 00222836. doi: 10.1016/j.jmb.2012.09.014. URL https://linkinghub.elsevier.com/retrieve/pii/S0022283612007711.\\n\\nDoud, M. and Bloom, J. Accurate Measurement of the Effects of All Amino-Acid Mutations on Influenza Hemagglutinin. Viruses, 8(6):155, June 2016. ISSN 1999-4915. doi: 10.3390/v8060155. URL http://www.mdpi.com/1999-4915/8/6/155.\\n\\nDoud, M. B., Ashenberg, O., and Bloom, J. D. Site-Specific Amino Acid Preferences Are Mostly Conserved in Two Closely Related Protein Homologs. Molecular Biology and Evolution, 32(11):2944\u20132960, November 2015. ISSN 0737-4038, 1537-1719. doi: 10.1093/molbev/msv167. URL https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msv167.\\n\\nDuenas-Decamp, M., Jiang, L., Bolon, D., and Clapham, P. R. Saturation mutagenesis of the hiv-1 envelope cd4 binding loop reveals residues controlling distinct trimer conformations. PLoS pathogens, 12(11):e1005988, 2016.\\n\\nEddy, S. R. Accelerated profile hmm searches. PLoS computational biology, 7(10):e1002195, 2011.\\n\\nEdgar, R. C. Muscle: multiple sequence alignment with high accuracy and high throughput. Nucleic acids research, 32 5:1792\u20137, 2004.\\n\\nElnaggar, A., Heinzinger, M., Dallago, C., Rihawi, G., Wang, Y., Jones, L., Gibbs, T., Feher, T., Angerer, C., Steinegger, M., et al. Prottrans: towards cracking the language of life's code through self-supervised deep learning and high performance computing. arXiv preprint arXiv:2007.06225, 2020.\\n\\nFaure, A. J., Domingo, J., Schmiedel, J. M., Hidalgo-Carcedo, C., Diss, G., and Lehner, B. Mapping the energetic and allosteric landscapes of protein binding domains. Nature, 604(7904):175\u2013183, 2022.\\n\\nFernandes, J. D., Faust, T. B., Strauli, N. B., Smith, C., Crosby, D. C., Nakamura, R. L., Hernandez, R. D., and Frankel, A. D. Functional Segregation of Overlapping Genes in HIV. Cell, 167(7):1762\u20131773.e12, December 2016. ISSN 00928674. doi: 10.1016/j.cell.2016.\"}"}
{"id": "notin22a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "notin22a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "notin22a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\nLee, J. M., Huddleston, J., Doud, M. B., Hooper, K. A., Wu, N. C., Bedford, T., and Bloom, J. D. Deep mutational scanning of hemagglutinin helps predict evolutionary fates of human H3N2 influenza variants. *Proceedings of the National Academy of Sciences*, 115(35):E8276\u2013E8285, August 2018. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.1806133115.\\nURL http://www.pnas.org/lookup/doi/10.1073/pnas.1806133115.\\n\\nLee, K., Chang, M.-W., and Toutanova, K. Latent retrieval for weakly supervised open domain question answering. *ArXiv*, abs/1906.00300, 2019.\\n\\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Tau Yih, W., Rockt\u00e4schel, T., Riedel, S., and Kiela, D. Retrieval-augmented generation for knowledge-intensive NLP tasks. *ArXiv*, abs/2005.11401, 2020.\\n\\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In *ICLR*, 2019.\\n\\nMadani, A., McCann, B., Naik, N., Keskar, N. S., Anand, N., Eguchi, R. R., Huang, P.-S., and Socher, R. Progen: Language modeling for protein generation, 2020.\\n\\nManekar, S. C. and Sathe, S. R. A benchmark study of k-mer counting methods for high-throughput sequencing. *GigaScience*, 7(12):giy125, 2018.\\n\\nMatreyek, K. A., Starita, L. M., Stephany, J. J., Martin, B., Chiasson, M. A., Gray, V. E., Kircher, M., Khechaduri, A., Dines, J. N., Hause, R. J., Bhatia, S., Evans, W. E., Relling, M. V., Yang, W., Shendure, J., and Fowler, D. M. Multiplex assessment of protein variant abundance by massively parallel sequencing. *Nature Genetics*, 50(6):874\u2013882, June 2018. ISSN 1061-4036, 1546-1718. doi: 10.1038/s41588-018-0122-z.\\nURL http://www.nature.com/articles/s41588-018-0122-z.\\n\\nMatreyek, K. A., Stephany, J. J., Ahler, E., and Fowler, D. M. Integrating thousands of pten variant activity and abundance measurements reveals variant subgroups and new dominant negatives in cancers. *Genome medicine*, 13(1):1\u201317, 2021.\\n\\nMattenberger, F., Latorre, V., Tirosh, O., Stern, A., and Geller, R. Globally defining the effects of mutations in a picornavirus capsid. *eLife*, 10:e64256, January 2021. ISSN 2050-084X. doi: 10.7554/eLife.64256.\\nURL https://elifesciences.org/articles/64256.\\n\\nMavor, D., Barlow, K., Thompson, S., Barad, B. A., Bonny, A. R., Cario, C. L., Gaskins, G., Liu, Z., Deming, L., Axen, S. D., Caceres, E., Chen, W., Cuesta, A., Gate, R. E., Green, E. M., Hulce, K. R., Ji, W., Kenner, L. R., Mensa, B., Morinishi, L. S., Moss, S. M., Mravic, M., Muir, R. K., Niekamp, S., Nnadi, C. I., Palovcak, E., Poss, E. M., Ross, T. D., Salcedo, E. C., See, S. K., Subramaniam, M., Wong, A. W., Li, J., Thorn, K. S., Conchuir, S. O., Roscoe, B. P., Chow, E. D., DeRisi, J. L., Kortemme, T., Bolon, D. N., and Fraser, J. S. Determination of ubiquitin fitness landscapes under different chemical stresses in a classroom setting. *eLife*, 5:e15802, April 2016. ISSN 2050-084X. doi: 10.7554/eLife.15802.\\nURL https://elifesciences.org/articles/15802.\\n\\nMcLaughlin Jr, R. N., Poelwijk, F. J., Raman, A., Gosal, W. S., and Ranganathan, R. The spatial architecture of protein function and adaptation. *Nature*, 491(7422):138\u2013142, November 2012. ISSN 0028-0836, 1476-4687. doi: 10.1038/nature11500.\\nURL http://www.nature.com/articles/nature11500.\\n\\nMeier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., and Rives, A. Language models enable zero-shot prediction of the effects of mutations on protein function. *bioRxiv*, 2021. doi: 10.1101/2021.07.09.450648.\\nURL https://www.biorxiv.org/content/early/2021/07/10/2021.07.09.450648.\\n\\nMelamed, D., Young, D. L., Gamble, C. E., Miller, C. R., and Fields, S. Deep mutational scanning of an RRM domain of the *Saccharomyces cerevisiae* poly(A)-binding protein. *RNA*, 19(11):1537\u20131551, November 2013. ISSN 1355-8382, 1469-9001. doi: 10.1261/rna.040709.113.\\nURL http://rnajournal.cshlp.org/lookup/doi/10.1261/rna.040709.113.\\n\\nMelnikov, A., Rogov, P., Wang, L., Gnirke, A., and Mikkelsen, T. S. Comprehensive mutational scanning of a kinase in vivo reveals substrate-dependent fitness landscapes. *Nucleic Acids Research*, 42(14):e112\u2013e112, August 2014. ISSN 0305-1048, 1362-4962. doi: 10.1093/nar/gku511.\\nURL https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gku511.\\n\\nMighell, T. L., Evans-Dutson, S., and O\u2019Roak, B. J. A Saturation Mutagenesis Approach to Understanding PTEN Lipid Phosphatase Activity and Genotype-Phenotype Relationships. *The American Journal of Human Genetics*, 102(5):943\u2013955, May 2018. ISSN 0002-9297. doi: 10.1016/j.ajhg.2018.03.018.\\nURL https://linkinghub.elsevier.com/retrieve/pii/S0002929718301071.\\n\\nMishra, P., Flynn, J., Starr, T., and Bolon, D. Systematic Mutant Analyses Elucidate General and Client-Specific...\"}"}
{"id": "notin22a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval.\\n\\nAspects of Hsp90 Function. Cell Reports, 15(3):588\u2013598, April 2016. ISSN 2211-1247. doi: 10.1016/j.celrep.2016.03.046. URL https://linkinghub.elsevier.com/retrieve/pii/S2211124716303175.\\n\\nMitchell, A. L., Almeida, A., Beracochea, M., Boland, M. A., Burgin, J., Cochrane, G., Crusoe, M. R., Kale, V., Potter, S. C., Richardson, L. J., Sakharova, E. A., Scheremetjew, M., Korobeynikov, A. I., Shlemov, A., Kunyavskaya, O., Lapidus, A. L., and Finn, R. D. Mg-nify: the microbiome analysis resource in 2020. Nucleic Acids Research, 48:D570 \u2013 D578, 2020.\\n\\nNambiar, A., Heflin, M., Liu, S., Maslov, S., Hopkins, M., and Ritz, A. Transforming the language of life: transformer neural networks for protein prediction tasks. In Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics, pp. 1\u20138, 2020.\\n\\nNewberry, R. W., Arhar, T., Costello, J., Hartoularos, G. C., Maxwell, A. M., Naing, Z. Z. C., Pittman, M., Reddy, N. R., Schwarz, D. M. C., Wassarman, D. R., Wu, T. S., Barrero, D., Caggiano, C., Catching, A., Cavazos, T. B., Estes, L. S., Faust, B., Fink, E. A., Goldman, M. A., Gomez, Y. K., Gordon, M. G., Gunsalus, L. M., Hoppe, N., Jaime-Garza, M., Johnson, M. C., Jones, M. G., Kung, A. F., Lopez, K. E., Lumpe, J., Martyn, C., McCarthy, E. E., Miller-Vedam, L. E., Navarro, E. J., Palar, A., Pellegino, J., Saylor, W., Stephens, C. A., Strickland, J., Torosyan, H., Wankowicz, S. A., Wong, D. R., Wong, G., Redding, S., Chow, E. D., DeGrado, W. F., and Kampmann, M. Robust Sequence Determinants of \u03b1-Synuclein Toxicity in Yeast Implicate Membrane Binding. ACS Chemical Biology, 15(8):2137\u20132153, August 2020. ISSN 1554-8929, 1554-8937. doi: 10.1021/acschembio.0c00339. URL https://pubs.acs.org/doi/10.1021/acschembio.0c00339.\\n\\nNg, P. C. and Henikoff, S. Predicting deleterious amino acid substitutions. Genome research, 11(5):863\u2013874, 2001.\\n\\nNutschel, C., Fulton, A., Zimmermann, O., Schwaneberg, U., Jaeger, K.-E., and Gohlke, H. Systematically Scrutinizing the Impact of Substitution Sites on Thermostability and Detergent Tolerance for Bacillus subtilis Lipase A. Journal of Chemical Information and Modeling, 60(3):1568\u20131584, March 2020. ISSN 1549-9596, 1549-960X. doi: 10.1021/acs.jcim.9b00954. URL https://pubs.acs.org/doi/10.1021/acs.jcim.9b00954.\\n\\nOlson, C., Wu, N., and Sun, R. A Comprehen- sive Biophysical Description of Pairwise Epistasis throughout an Entire Protein Domain. Current Biology, 24(22):2643\u20132651, November 2014. ISSN 0960-9822. doi: 10.1016/j.cub.2014.09.072. URL https://linkinghub.elsevier.com/retrieve/pii/S0960982214012688.\\n\\nPokusaeva, V. O., Usmanova, D. R., Putintseva, E. V., Esipinar, L., Sarkisyan, K. S., Mishin, A. S., Bogatyreva, N. S., Ivankov, D. N., Akopyan, A. V., Avvakumov, S. Y., Povolotskaya, I. S., Filion, G. J., Carey, L. B., and Kondrashov, F. A. An experimental assay of the interactions of amino acids from orthologous sequences shaping a complex fitness landscape. PLOS Genetics, 15(4):e1008079, April 2019. ISSN 1553-7404. doi: 10.1371/journal.pgen.1008079. URL https://dx.plos.org/10.1371/journal.pgen.1008079.\\n\\nPress, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation, 2021.\\n\\nQi, H., Olson, C. A., Wu, N. C., Ke, R., Loverdo, C., Chu, V., Truong, S., Remenyi, R., Chen, Z., Du, Y., Su, S.-Y., Al-Mawsawi, L. Q., Wu, T.-T., Chen, S.-H., Lin, C.-Y., Zhong, W., Lloyd-Smith, J. O., and Sun, R. A Quan- titative High-Resolution Genetic Profile Rapidly Identifies Sequence Determinants of Hepatitis C Viral Fitness and Drug Sensitivity. PLoS Pathogens, 10(4):e1004064, April 2014. ISSN 1553-7374. doi: 10.1371/journal.ppat.1004064. URL https://dx.plos.org/10.1371/journal.ppat.1004064.\\n\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2018. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf.\\n\\nRadivojac, P., Obradovic, Z., Smith, D. K., Zhu, G., Vucetic, S., Brown, C. J., Lawson, J. D., and Dunker, A. K. Protein flexibility and intrinsic disorder. Protein Science, 13(1):71\u201380, 2004.\\n\\nRamensky, V., Bork, P., and Sunyaev, S. Human non-synonymous snps: server and survey. Nucleic acids research, 30(17):3894\u20133900, 2002.\\n\\nRao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen, X., Canny, J. F., Abbeel, P., and Song, Y. S. Evaluating protein transfer learning with TAPE. CoRR, abs/1906.08230, 2019. URL http://arxiv.org/abs/1906.08230.\\n\\nRao, R., Meier, J., Sercu, T., Ovchinnikov, S., and Rives, A. Transformer protein language models are unsupervised structure learners. In International Conference on Learning Representations, 2020.\"}"}
{"id": "notin22a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\nRao, R., Liu, J., Verkuil, R., Meier, J., Canny, J. F., Abbeel, P., Sercu, T., and Rives, A. Msa transformer. bioRxiv, 2021. doi: 10.1101/2021.02.12.430858. URL https://www.biorxiv.org/content/early/2021/02/13/2021.02.12.430858.\\n\\nRemmert, M., Biegert, A., Hauser, A., and S\u00f6ding, J. Hh-blits: lightning-fast iterative protein sequence searching by hmm-hmm alignment. Nature Methods, 9:173\u2013175, 2012.\\n\\nReva, B., Antipin, Y., and Sander, C. Predicting the functional impact of protein mutations: application to cancer genomics. Nucleic acids research, 39(17):e118\u2013e118, 2011.\\n\\nRiesselman, A. J., Ingraham, J. B., and Marks, D. S. Deep generative models of genetic variation capture the effects of mutations. Nature methods, 15(10):816\u2013822, 2018.\\n\\nRives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15), 2021.\\n\\nRobertson, S. E. and Zaragoza, H. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3:333\u2013389, 2009.\\n\\nRockah-Shmuel, L., T\u00f3th-Petr\u00f3czy, A., and Tawfik, D. S. Systematic Mapping of Protein Mutational Space by Prolonged Drift Reveals the Deleterious Effects of Seemingly Neutral Mutations. PLOS Computational Biology, 11(8):e1004421, August 2015. ISSN 1553-7358. doi: 10.1371/journal.pcbi.1004421. URL https://dx.plos.org/10.1371/journal.pcbi.1004421.\\n\\nRomero, P. A., Tran, T. M., and Abate, A. R. Dissecting enzyme function with microfluidic-based deep mutational scanning. Proceedings of the National Academy of Sciences, 112(23):7159\u20137164, June 2015. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.1422285112. URL http://www.pnas.org/lookup/doi/10.1073/pnas.1422285112.\\n\\nRoscoe, B. P. and Bolon, D. N. Systematic Exploration of Ubiquitin Sequence, E1 Activation Efficiency, and Experimental Fitness in Yeast. Journal of Molecular Biology, 426(15):2854\u20132870, July 2014. ISSN 0022-2836. doi: 10.1016/j.jmb.2014.05.019. URL https://linkinghub.elsevier.com/retrieve/pii/S0022283614002587.\\n\\nRoscoe, B. P., Thayer, K. M., Zeldovich, K. B., Fushman, D., and Bolon, D. N. Analyses of the Effects of All Ubiquitin Point Mutants on Yeast Growth Rate. Journal of Molecular Biology, 425(8):1363\u20131377, April 2013. ISSN 0022-2836. doi: 10.1016/j.jmb.2013.01.032. URL https://linkinghub.elsevier.com/retrieve/pii/S0022283613000636.\\n\\nRuss, W. P., Figliuzzi, M., Stocker, C., Barrat-Charlaix, P., Socolich, M., Kast, P., Hilvert, D., Monasson, R., Cocco, S., Weigt, M., et al. An evolution-based model for designing chorismate mutase enzymes. Science, 369(6502):440\u2013445, 2020.\\n\\nSarskan, K., Bolotin, D., Meer, M., Usmanova, D., Mishin, A., Sharonov, G., Ivankov, D., Bozhanova, N., Baranov, M., Soylemez, O., et al. Local fitness landscape of the green fluorescent protein. Nature, 533(7603):397\u2013401, 2016.\\n\\nSeuma, M., Faure, A., Badia, M., Lehner, B., and Bolognesi, B. The genetic landscape for amyloid beta fibril nucleation accurately discriminates familial Alzheimer's disease mutations. eLife, 10:e63364, February 2021. ISSN 2050-084X. doi: 10.7554/eLife.63364. URL https://elifesciences.org/articles/63364.\\n\\nShin, J.-E., Riesselman, A. J., Kollasch, A. W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A. C., and Marks, D. S. Protein design and variant prediction using autoregressive generative models. Nature communications, 12(1):1\u201311, 2021.\\n\\nSievers, F., Wilm, A., Dineen, D., Gibson, T. J., Karplus, K., Li, W., Lopez, R., McWilliam, H., Remmert, M., S\u00f6ding, J., Thompson, J. D., and Higgins, D. G. Fast, scalable generation of high-quality protein multiple sequence alignments using clustal omega. Molecular Systems Biology, 7:539 \u2013 539, 2011.\\n\\nSinai, S., Jain, N., Church, G. M., and Kelsic, E. D. Generative aav capsid diversification by latent interpolation. bioRxiv, 2021.\\n\\nSo, D. R., Ma\u0144ke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V. Primer: Searching for efficient transformers for language modeling, 2021.\\n\\nSoh, Y. S., Moncla, L. H., Eguia, R., Bedford, T., and Bloom, J. D. Comprehensive mapping of adaptation of the avian influenza polymerase protein PB2 to humans. eLife, 8:e45079, April 2019. ISSN 2050-084X. doi: 10.7554/eLife.45079. URL https://elifesciences.org/articles/45079.\\n\\nSourisseau, M., Lawrence, D. J. P., Schwarz, M. C., Storrs, C. H., Veit, E. C., Bloom, J. D., and Evans, M. J. Deep Mutational Scanning Comprehensively Maps How Zika Envelope Protein Mutations Affect Viral Growth and Antibody Escape. Journal of Virology, 93(23), 2019.\"}"}
{"id": "notin22a", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Model performance on the ProteinGym substitution benchmark. We report the DMS-level performance (measured by the Spearman's rank correlation $\\\\rho$ between model scores and experimental measurements) of Tranception with retrieval, ESM-1v (Meier et al., 2021), MSA Transformer (Rao et al., 2021) and EVE (Frazer et al., 2021) for each DMS in the ProteinGym substitution benchmark.\"}"}
{"id": "notin22a", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\n| Model                  | Human | Other | Eukaryote | Prokaryote | Virus | All  |\\n|------------------------|-------|-------|-----------|------------|-------|------|\\n| **Alignment-based**    |       |       |           |            |       |      |\\n| Site indep             | 0.732 | 0.768 | 0.696     | 0.720      | 0.725 |      |\\n| Wavenet                | 0.730 | 0.766 | 0.763     | 0.665      | 0.725 |      |\\n| Deepsequence           | 0.729 | 0.779 | 0.766     | 0.685      | 0.733 |      |\\n| EVmutation             | 0.735 | 0.775 | 0.760     | 0.702      | 0.738 |      |\\n| EVE                    | 0.742 | 0.782 | 0.769     | 0.732      | 0.751 |      |\\n| **Protein language**   |       |       |           |            |       |      |\\n| ESM-1v                 | 0.734 | 0.749 | 0.762     | 0.620      | 0.713 |      |\\n| MSA Transformer        | 0.723 | 0.784 | 0.768     | 0.702      | 0.737 |      |\\n| Tranception (w/o retrieval) | 0.716 | 0.762 | 0.745     | 0.709      | 0.728 |      |\\n| Tranception (w/ retrieval) | 0.749 | 0.795 | 0.766     | 0.727      | 0.754 |      |\\n\\nTable 12. Average AUC between model scores and experimental measurements by taxon.\\n\\n| Model                  | Human | Other | Eukaryote | Prokaryote | Virus | All  |\\n|------------------------|-------|-------|-----------|------------|-------|------|\\n| **Alignment-based**    |       |       |           |            |       |      |\\n| Site indep             | 0.323 | 0.344 | 0.279     | 0.311      | 0.312 |      |\\n| Wavenet                | 0.322 | 0.351 | 0.369     | 0.233      | 0.314 |      |\\n| Deepsequence           | 0.327 | 0.369 | 0.386     | 0.261      | 0.330 |      |\\n| EVmutation             | 0.333 | 0.364 | 0.370     | 0.288      | 0.334 |      |\\n| EVE                    | 0.338 | 0.366 | 0.387     | 0.333      | 0.352 |      |\\n| **Protein language**   |       |       |           |            |       |      |\\n| ESM-1v                 | 0.321 | 0.325 | 0.377     | 0.174      | 0.296 |      |\\n| MSA Transformer        | 0.311 | 0.388 | 0.389     | 0.290      | 0.334 |      |\\n| Tranception (w/o retrieval) | 0.302 | 0.349 | 0.350     | 0.298      | 0.319 |      |\\n| Tranception (w/ retrieval) | 0.348 | 0.390 | 0.379     | 0.330      | 0.356 |      |\\n\\nTable 13. Average Matthew's correlation coefficient (MCC) between model scores and experimental measurements by taxon.\\n\\nE.5. Model ensembling\\n\\nPrior work from Riesselman et al. (2018); Meier et al. (2021) noted that additional performance gains on the fitness prediction task may be achieved in practice by ensembling several independently trained versions of the same model (e.g., trained with different random seeds). We report the performance from several ensemble versions in Table 16. Additionally, we revisit how ensembling has been approached so far for fitness prediction \u2013 we propose that instead of ensembling several times the same model architecture, we may obtain substantially higher performance in practice by ensembling different yet complementary model architectures. We ensemble different pairs of models together and obtain the strongest results by combining Tranception with EVE (Table 17).\\n\\nF. ProteinGym curation\\n\\nTo build the ProteinGym benchmark, we initially collected a set of 137 deep mutational scanning assays. We then filtered out 43 of these assays based on the following criteria: Data had not been made publicly available (9), non-protein assays (UTR, tRNA, promoter, etc.; 7), synthetic proteins (3), insufficient number of measurements (3), outdated (i.e., a more recent improved assay on the same protein and same property was found; 4), majority of data hitting experimental floor (6), low dynamic range (6), assay not relevant to fitness prediction (5).\\n\\nThe final version of the ProteinGym consists of the experimental measurements of 94 deep mutational scanning experiments (87 substitutions assays and 7 indels assays) from the following 77 publications: (Adkar et al., 2012; Jones et al., 2020; Kozek et al., 2020; Firnberg et al., 2014; Jia et al., 2021; Chan et al., 2017; Wu et al., 2015; Mavor et al., 2016; Ahler et al., 2019; Newberry et al., 2020; Fernandes et al., 2016; Mighell et al., 2018; Roscoe & Bolon, 2014; Kotler et al., 2018; Rockah-Shmuel et al., 2015; Giacomelli et al., 2018; Melamed et al., 2013; Melnikov et al., 2014; Brenan et al., 2016; Suiter et al., 2020; Chen et al., 2020; Tripathi et al., 2016; Pokusaeva et al., 2019; Aakre et al., 2015; Haddox et al., 2016; Soh...\"}"}
{"id": "notin22a", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 14.\\n\\nAverage Spearman's rank correlation $\\\\rho$ between model scores and experimental measurements on the ProteinGym indel benchmark.\\n\\n| Domain | Tranception (w/o retrieval) | Tranception (retrieval full MSA) | Tranception (retrieval domain MSA) | EVE (full MSA) | EVE (domain MSA) |\\n|--------|-----------------------------|----------------------------------|------------------------------------|--------------|-----------------|\\n| RING   | 0.567                       | 0.588                            | 0.607                              | 0.320        | 0.573           |\\n| BRCT   | 0.354                       | 0.490                            | 0.504                              | N/A          | 0.593           |\\n\\n### Table 15.\\n\\nBRCA1 model performance summary by domain, as measured by Spearman's $\\\\rho$ between model scores and experimental measurements. When using a full-protein alignment, EVE is unable to score mutations in the BRCT domain due to insufficient coverage in that region, as per the limitations discussed in Appendix D.\\n\\n### Table 16.\\n\\nSingle-architecture ensemble analysis. Tranception without retrieval (single seed) achieves higher average performance than the ESM-1v ensemble. Tranception with retrieval (single seed) achieves higher performance than the MSA Transformer ensemble. The ensemble of 5 EVE models does marginally better than a single Tranception model with retrieval. Performance is measured via Spearman's rank correlation between model scores and DMS measurements.\\n\\n| Model pair ensembled | Spearman $\\\\rho$ |\\n|----------------------|-----------------|\\n| Tranception w/o retrieval only | 0.406 |\\n| Tranception + ESM-1v | 0.427 |\\n| Tranception + MSA Transformer | 0.449 |\\n| Tranception + EVE | 0.473 |\\n\\n### Table 17.\\n\\nPaired architecture ensemble analysis. In this analysis we always use Tranception without retrieval. We note however that ensembling Tranception with retrieval and EVE provides only marginally higher overall performance (0.475) suggesting that the retrieval inference may capture information analogous to that of alignment-based models like EVE, while the autoregressive inference provides different but complementary information.\"}"}
{"id": "notin22a", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Protein | Species | Authors | # sequences | Minimum % similarity |\\n|---------|---------|---------|-------------|---------------------|\\n| A4 HUMAN | Seuma 2021 |        | 1000        | 0.1                 |\\n| BRCA1 HUMAN | Findlay 2018 |    | 2000       | 0.2                 |\\n| CCDB ECOLI | Tripathi 2016 |   | 50000      | 0.0                 |\\n| KCNH2 HUMAN | Kozek 2020 |      | 20000      | 0.2                 |\\n| MSH2 HUMAN | Jia 2020 |        | 25000      | 0.0                 |\\n| NRAM I33A0 | Jiang 2016 standard |    | 20000      | 0.2                 |\\n| NUD15 HUMAN | Suiter 2020 |      | 50000      | 0.2                 |\\n| PA I34A1 | Wu 2015 |        | 2000       | 0.2                 |\\n| PABP YEAST | Melamed 2013 |    | 2500       | 0.2                 |\\n| PTEN HUMAN | Matreyek 2021 |    | 2500       | 0.2                 |\\n| Q2N0S5 9HIV1 | Haddox 2018 |   | 25000      | 0.2                 |\\n| R1AB SARS2 | Flynn 2022 growth | | 100000    | 0.2                 |\\n| TPMT HUMAN | Matreyek 2018 |      | 2500       | 0.2                 |\\n| UBE4B MOUSE | Starita 2013 |     | 1000       | 0.2                 |\\n| VKOR1 HUMAN | Chiasson 2020 abundance | | 5000       | 0.2                 |\\n\\nFigure 7. Robustness to alignment depth for several DMS assays in the ProteinGym substitution benchmark. We measure the Spearman's rank correlation between model score and experimental measurement for Tranception, EVE and MSA Transformer as we progressively exclude sequences in the corresponding MSAs based on their similarity to the sees sequence used to create the alignment.\"}"}
{"id": "notin22a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval\\n\\nPascal Notin\\nMafalda Dias\\nJonathan Frazer\\nJavier Marchena-Hurtado\\nAidan Gomez\\nDebora S. Marks\\nYarin Gal\\n\\nAbstract\\n\\nThe ability to accurately model the fitness landscape of protein sequences is critical to a wide range of applications, from quantifying the effects of human variants on disease likelihood, to predicting immune-escape mutations in viruses and designing novel biotherapeutic proteins. Deep generative models of protein sequences trained on multiple sequence alignments have been the most successful approaches so far to address these tasks. The performance of these methods is however contingent on the availability of sufficiently deep and diverse alignments for reliable training. Their potential scope is thus limited by the fact many protein families are hard, if not impossible, to align. Large language models trained on massive quantities of non-aligned protein sequences from diverse families address these problems and show potential to eventually bridge the performance gap. We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches. To enable more rigorous model testing across a broader range of protein families, we develop ProteinGym \u2013 an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks.\\n\\nEqual senior authorship\\n\\n1. Introduction\\n\\nUnsupervised models predicting the effects of mutations in protein sequences are emerging as central tools in drug design, pathogen forecasting, identification of disease causating variants and more. Several modeling approaches have been introduced in recent years, offering various trade-offs in terms of performance, diversity of proteins which can be modelled and types of sequence variation which can be scored. Current state-of-the-art methods for predicting the effect of single amino acid substitutions are trained on a multiple sequence alignment (MSA) for each protein sequence or domain of interest. In this context, MSAs serve two purposes. First, they act as a data acquisition tool by identifying sequences related to the target in a large protein database, in order to then train a model on a relevant set of sequences. Second, they align sequences by modelling insertions, deletions and substitutions, resulting in a coordinate system which enables the amino acid at a given position to be compared across the training set. While training models on protein-specific alignments has been shown to be an effective approach for thousands of proteins (Frazer et al., 2021) it nevertheless brings severe limitations. For instance, such models can not make predictions for sequences which are incompatible with the coordinate system of the MSA used in training (eg., insertions and deletions), thereby limiting scope. Additionally, a large fraction of the proteome corresponds to regions that can not be aligned such as so-called disordered regions \u2013 around half of all human proteins contain regions of at least 40 amino acids classified as disordered (Radivojac et al., 2004; Toth-Petroczy et al., 2016). Even when alignments are accessible, the protein function might be taxa specific, and the MSA algorithm may not retrieve a large enough set of homologous sequences for model training. Alignment-based models may be relatively sensitive to the characteristics of the MSAs they are trained on \u2013 including the choice of hyperparameters used to retrieve these MSAs. Lastly, there is a lack of information sharing across models that are independently trained on different data subsets. Recently, language models trained on large quantities of non-aligned (Meier et al., 2021) or aligned (Rao et al., 2021) protein sequences have made first steps towards addressing these issues. However, mod-\\n\\nProceedings of the 39th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).\"}"}
{"id": "notin22a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce Tranception, a novel autoregressive transformer architecture that promotes specialization across attention heads for enhanced protein modeling. We combine autoregressive predictions and homology from retrieved sequences at inference to reach state-of-the-art fitness prediction performance on both substitutions and indels.\\n\\nOur contributions are as follows:\\n\\n\u2022 We introduce Tranception, a novel autoregressive transformer architecture that promotes specialization across attention heads for enhanced protein modeling;\\n\\n\u2022 We combine autoregressive predictions and homology from retrieved sequences at inference to reach state-of-the-art fitness prediction performance on both substitutions and indels;\\n\\n\u2022 We curate an extensive set of multiplexed assays of variant effects \u2013 the ProteinGym benchmarks \u2013 substantially increasing both the number and diversity of assays compared to existing benchmarks.\\n\\n2. Background\\n\\n2.1. Mutation effect prediction with aligned sequences\\n\\nPredicting the effect of genetic variation using aligned protein sequences from diverse organisms, and in particular, predicting if a variant is likely to be disease-causing in humans, has a long history. While initial models focused on extracting position-specific information from alignments, subsequent work sought to capture more complex patterns. Hopf et al. (2017) proposed to model interactions between pairs of distinct positions with energy based models. Riesselman et al. (2018) later expanded on the concept with DeepSequence: Variational Autoencoders trained on protein-specific MSAs to learn a distribution of amino acid sequence which capture higher-order interactions. Focusing on predicting the pathogenicity of protein variants in human disease-related genes, EVE (Frazer et al., 2021) subsequently enhanced the DeepSequence architecture to reach higher fitness prediction performance.\\n\\n2.2. Modeling proteins without alignments\\n\\nWhile MSAs capture meaningful information about protein functions and structures, they also have certain limitations.\"}"}
{"id": "notin22a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\nNot all proteins are alignable and, if they are, the depth of the corresponding alignments may not be enough to train models sufficiently large to learn the complex interactions between residues. This has led to a stream of research investigating alternative modeling approaches that do not rely on aligned sequences. Shin et al. (2021); Weinstein & Marks (2021) developed models that could be trained on non-aligned sequences, although they still relied on MSA routines to recover protein-specific sets of homologous sequences to serve as training data. Alley et al. (2019); Heinzinger et al. (2019) were the first to introduce models trained across protein families, relying on LSTM architectures (Hochreiter & Schmidhuber, 1997). Building on advances in the Natural Language Processing literature to train larger-scale language models, Madani et al. (2020); Rives et al. (2021); Nambiar et al. (2020) proposed the use of transformer architectures to model protein sequences. Rao et al. (2020) introduced the MSA transformer, an architecture to learn a model of MSAs across thousands of protein families, while ESM-1v (Meier et al., 2021) and ProtTrans (Elnaggar et al., 2020) focused on learning patterns exclusively from non-aligned sequences from very large protein databases. Closely related models which combine unsupervised protein embedding with supervised data are seeing diverse applications, from supervised protein design tasks (Biswas et al., 2021), to task agnostic sequence representations (Bepler & Berger, 2019), to protein structure prediction (Jumper et al., 2021; Baek et al., 2021).\\n\\n2.3. Deep Mutational Scanning benchmarks\\n\\nUsing a large number of Deep Mutational Scanning experiments (DMS) or Multiplex Assays of Variant Effects (MAVEs) to assess the performance of protein models was first proposed in Hopf et al. (2017), with a benchmark of \u223c20 different assays. Riesselman et al. (2018) later doubled the size of this benchmark (\u223c40 assays). The list of DMS assays curated to benchmark fitness predictors has seen only modest updates thereafter. More recent benchmarks for protein modeling (Rao et al., 2019; Dallago et al., 2021) have introduced additional assays focused on assessing model performance across a diverse set of downstream tasks. We include all assays related to fitness prediction from these prior benchmarks when building ProteinGym.\\n\\n2.4. Retrieval\\n\\nRetrieval aims at identifying objects related to a target one in a reference database to improve the processing or modeling of that object. In the Natural Language Processing literature, retrieval has been leveraged for open domain question answering (Robertson & Zaragoza, 2009; Wang et al., 2018; Karpukhin et al., 2020) or to augment pretrained language models to find relevant information in massive datasets at inference. Grave et al. (2017) and Khandelwal et al. (2020) both extended language models with a k-NN retrieval over pretrained embeddings at test time for question answering, while ORQA (Lee et al., 2019) and REALM (Guu et al., 2020) architectures jointly trained both the 'retriever' and 'reader' models end-to-end. RAG (Lewis et al., 2020) then applied similar concepts to the broader task of generative language modelling. More recently, RETRO (Borgeaud et al., 2021) demonstrated the benefits of retrieval at the scale of trillions of tokens. The bioinformatics literature has also heavily contributed to retrieval systems, in particular through Multiple Sequence Alignments (Thompson et al., 1994; Edgar, 2004; Sievers et al., 2011; Remmert et al., 2012) who may be used to both retrieve and align homologous sequences.\\n\\n3. Tranception\\n\\nTranception is a novel autoregressive transformer architecture that was designed with two core principles in mind: 1) promoting specialization across attention heads 2) explicitly extracting patterns from contiguous subsequences.\\n\\n3.1. Tranception attention\\n\\nThe concept of 'k-mers' is well-established in biological sequence analysis: k-mers are contiguous subsequences of k elements (typically nucleotides or amino acids) which have proved to be critically useful abstractions in several applications such as de novo assembly, read correction, repeat detection, comparison of genomes, metagenomes (Manekar & Sathe, 2018). The majority of protein language models to date (\u00a7 2.2) have focused on extracting patterns (via sequence tokenization or attention mechanisms) at the amino acid level only. In this work we investigate the benefits from explicitly attending over contiguous subsequences of amino acid tokens via a novel attention mechanism \u2013 Tranception attention (Fig. 1) \u2013 which combines ideas from Primer (So et al., 2021) and Inception (Szegedy et al., 2014) networks. Similar to Primer, we leverage squared ReLU activations and depthwise convolutions after the different multi-head attention projections. Instead of using similar-sized kernels for each depthwise convolution, we split the attention heads at each layer in 4 groups and apply convolutions with different kernel sizes on each group, thereby combining information at different resolutions as in Inception. This incentivizes each attention head to specialize to pattern extractions at different k-mer sizes and leads to both more efficient training and downstream task performance compared with Primer and GPT2 (Radford et al., 2018) (Fig. 2 and Appendix B.1).\\n\\n3.2. Grouped ALiBi position encoding\\n\\nIn order to further promote specialization across attention heads and enhance predictions for protein sequences that\"}"}
{"id": "notin22a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval\\n\\nFigure 2. Training loss comparison across transformer architectures for protein modeling. We plot the training loss as a function of the number of gradient steps for GPT2 (Radford et al., 2018), Primer (So et al., 2021), Tranception with learned position embeddings and Tranception with grouped ALiBi. All models have similar number of parameters and only differ by their attention mechanism, non-linear activations and position encodings. Tranception converges faster and to a lower loss compared with other architectures. This translates into higher downstream task performance (Appendix B.1).\\n\\n3.3. Data processing and augmentations\\n\\nOur models are trained on UniRef (Suzek et al., 2014), a large scale protein sequence database. We perform thorough ablations when developing Tranception (Appendix B.1). Similar to Meier et al. (2021), we investigate the impact of training on protein sequences clustered at different levels of similarity. Unlike what was observed for masked-language model architectures, we find that keeping as much of the granularity available in the dataset is beneficial to downstream task performance. We therefore train our final model (700M parameters) on UniRef100 which, after preprocessing (Appendix B.2), leads to a training dataset of \\\\( \\\\sim 250 \\\\) million protein sequences. Our vocabulary is comprised of the standard 20 amino acids (Kessel & Ben-Tal, 2018). We find that averaging the predictions obtained by scoring each sequence and its reverse at inference time leads to higher downstream performance (Appendix B.4), and therefore apply sequence mirroring at random during training to teach our model to score sequences from both directions. Our model has a maximum context size of 1024 tokens, which is wider than the length of 98% of protein sequences in UniRef100 (Table 7). At train time, if a protein is longer than the maximum context size of the model (after accounting for the special start and end of sequence tokens), we extract a randomly-selected contiguous slice of width equal to that maximum context size. Indeterminate amino acids are imputed at random during training and inference.\\n\\n3.4. Scoring sequences for fitness prediction\\n\\nThe goal of fitness prediction is to assess the effects of mutations (e.g., amino acid substitutions, insertions or deletions) on the ability of the corresponding mutated protein sequence to perform its function. A common approach to estimating mutation effects is to quantify the likelihood ratio between the mutated sequence and a naturally occurring reference sequence for that protein family, referred to as the \u2018wild-type\u2019 sequence (Riesselman et al., 2018). More formally, we represent each protein \\\\( x \\\\) as a sequence of amino acids \\\\( (x_1, x_2, ..., x_l) \\\\). Our model is trained in a self-supervised fashion to predict the next token \\\\( x_i \\\\) in the sequence based on the context of the prior \\\\( i-1 \\\\) tokens, such that the probability of the full sequence factorizes as:\\n\\n\\\\[\\nP(x) = \\\\prod_{i=1}^{l} P(x_i | x_1, ..., x_{i-1}) = \\\\prod_{i=1}^{l} P(x_i | x_{<i})\\n\\\\] (1)\\n\\nThe fitness \\\\( F_x \\\\) of a given mutated protein \\\\( x_{mut} \\\\) is then measured via the log-likelihood ratio with the wild-type sequence \\\\( x_{wt} \\\\):\\n\\n\\\\[\\nF_x = \\\\log \\\\frac{P(x_{mut})}{P(x_{wt})}\\n\\\\] (2)\\n\\nWhen assessing fitness in sequences longer than the context length, we select the sequence slice providing the widest left and right context for the set of mutations considered (Appendix 3.4). At inference time and building on our data augmentations, we take the arithmetic average of the log-likelihood ratios obtained by scoring each sequence and its reverse.\"}"}
