{"id": "guo23j", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LongCoder: A Long-Range Pre-trained Language Model for Code Completion\\n\\npretraining data, making the evaluation less reliable. Additionally, models with larger-scale pretraining are even more likely to have seen the test data before. For example, OpenAI Codex is trained on all GitHub repositories, that undoubtedly, include most (if not all) test data in PY150, JavaCorpus, and LCC. As the code completion models have seen wider adoption in software development, future evaluation can be \\\"self-fulfilling\\\". For example, GitHub CoPilot is a popular commercial code completion tool powered by OpenAI Codex. A lot of code generated by Codex may have already been submitted to GitHub. This could give widely-used models like Codex an advantage if it is evaluated on a dataset with a data source of the latest GitHub repositories, as we could be evaluating Codex on its own input. To address these problems, we need the community to contribute new, clean, and high-quality datasets with code from private projects to support future research.\\n\\nFuture Work\\n\\nLongCoder opens up new research opportunities in code generation not only within a large file, but also across multiple files. For example, we could allow the model to look at other files in the project for even more accurate code completion. It could enable new applications including automatically extracting package requirements, generating build files, refactoring the project, etc.\\n\\nAcknowledgments\\n\\nJian Yin is the corresponding author. Daya Guo and Jian Yin are supported by the National Natural Science Foundation of China (U1911203, U2001211, U22B2060), Guangdong Basic and Applied Basic Research Foundation (2019B1515130001), Key-Area Research and Development Program of Guangdong Province (2020B0101100001). We would like to thank the anonymous reviewers and the meta-reviewer for their insightful comments.\\n\\nReferences\\n\\nAllamanis, M. The adverse effects of code duplication in machine learning models of code. In Onward!, pp. 143\u2013153. ACM, 2019.\\n\\nAllamanis, M. and Sutton, C. Mining source code repositories at massive scale using language modeling. In 2013 10th Working Conference on Mining Software Repositories (MSR), pp. 207\u2013216. IEEE, 2013.\\n\\nAllamanis, M. and Sutton, C. Mining idioms from source code. In SIGSOFT FSE, pp. 472\u2013483. ACM, 2014.\\n\\nBeltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.\\n\\nBielik, P., Raychev, V., and Vechev, M. T. PHOG: probabilistic model for code. In ICML, volume 48 of JMLR Workshop and Conference Proceedings, pp. 2933\u20132942. JMLR.org, 2016.\\n\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In NeurIPS, 2020.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n\\nChild, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.\\n\\nChoromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In ICLR. OpenReview.net, 2021.\\n\\nClement, C. B., Lu, S., Liu, X., Tufano, M., Drain, D., Duan, N., Sundaresan, N., and Svyatkovskiy, A. Long-range modeling of source code files with ewash: Extended window access by syntax hierarchy. In EMNLP, pp. 4713\u20134722. Association for Computational Linguistics, 2021.\\n\\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pp. 4171\u20134186. Association for Computational Linguistics, 2019.\\n\\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H. Unified language model pre-training for natural language understanding and generation. In NeurIPS, pp. 13042\u201313054, 2019.\\n\\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, W.-t., Zettlemoyer, L., and Lewis, M. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999, 2022.\\n\\nGuo, D., Lu, S., Duan, N., Wang, Y., Zhou, M., and Yin, J. Unixcoder: Unified cross-modal pre-training for code representation. In ACL, pp. 7212\u20137225. Association for Computational Linguistics, 2022.\\n\\nHellendoorn, V. J. and Devanbu, P. T. Are deep neural networks the best choice for modeling source code? In ESEC/SIGSOFT FSE, pp. 763\u2013773. ACM, 2017.\"}"}
{"id": "guo23j", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LongCoder: A Long-Range Pre-trained Language Model for Code Completion\\n\\nHindle, A., Barr, E. T., Gabel, M., Su, Z., and Devanbu, P. T.\\n\\nOn the naturalness of software. Commun. ACM, 59(5):122\u2013131, 2016.\\n\\nHusain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M.\\n\\nCodesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019.\\n\\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\\n\\nTransformers are rnns: Fast autoregressive transformers with linear attention. In ICML, volume 119 of Proceedings of Machine Learning Research, pp. 5156\u20135165. PMLR, 2020.\\n\\nKitaev, N., Kaiser, L., and Levskaya, A.\\n\\nReformer: The efficient transformer. In ICLR. OpenReview.net, 2020.\\n\\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al.\\n\\nCompetition-level code generation with alphacode. Science, 378(6624):1092\u20131097, 2022.\\n\\nLiu, F., Li, G., Zhao, Y., and Jin, Z.\\n\\nMulti-task learning based pre-trained language model for code completion. In ASE, pp. 473\u2013485. IEEE, 2020.\\n\\nLiu, T., Xu, C., and McAuley, J.\\n\\nRepobench: Benchmarking repository-level code auto-completion systems. arXiv preprint, 2023.\\n\\nLu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C. B., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K., Fu, S., and Liu, S.\\n\\nCodexglue: A machine learning benchmark dataset for code understanding and generation. In NeurIPS Datasets and Benchmarks, 2021.\\n\\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., and Xiong, C.\\n\\nA conversational paradigm for program synthesis. arXiv preprint arXiv:2203.13474, 2022.\\n\\nQin, Z., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B., Yan, J., Kong, L., and Zhong, Y.\\n\\nCosformer: Rethinking softmax in attention. In ICLR. OpenReview.net, 2022.\\n\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.\\n\\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n\\nRaychev, V., Bielik, P., and Vechev, M. T.\\n\\nProbabilistic model for code with decision trees. In OOPSLA, pp. 731\u2013747. ACM, 2016.\\n\\nSvyatkovskiy, A., Deng, S. K., Fu, S., and Sundaresan, N.\\n\\nIntellicode compose: code generation using transformer. In ESEC/SIGSOFT FSE, pp. 1433\u20131443. ACM, 2020.\\n\\nTay, Y., Dehghani, M., Bahri, D., and Metzler, D.\\n\\nEfficient transformers: A survey. ACM Computing Surveys, 55(6):1\u201328, 2022.\\n\\nTu, Z., Su, Z., and Devanbu, P. T.\\n\\nOn the localness of software. In SIGSOFT FSE, pp. 269\u2013280. ACM, 2014.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I.\\n\\nAttention is all you need. In NIPS, pp. 5998\u20136008, 2017.\\n\\nWang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H.\\n\\nLinformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.\\n\\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J.\\n\\nA systematic evaluation of large language models of code. In MAPS@PLDI, pp. 1\u201310. ACM, 2022.\\n\\nZaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Onta\u02dcn\u00b4on, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A.\\n\\nBig bird: Transformers for longer sequences. In NeurIPS, 2020.\"}"}
{"id": "guo23j", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens \u2014 bridge tokens and memory tokens \u2014 to improve performance and efficiency.\\n\\nBridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference.\\n\\n1. Introduction\\n\\nCode completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context. With the advancement of large language models, Transformer-based models (Vaswani et al., 2017) have demonstrated impressive results in code completion (Chen et al., 2021). However, the computational cost of these models grows quadratically with the length of input, making them less suitable for modeling long code context. On the other hand, modeling long code can potentially improve the accuracy of code completion and enable applications on a file and even project level.\\n\\nAn efficient model that can scale to such long input can be suitable for code completion that contains long context. In this paper, we propose a new pre-trained language model, named LongCoder, for long code modeling. As shown in Figure 1, LongCoder features a sparse attention mechanism that reduces the computational complexity (to linear). LongCoder exploits a sliding window mechanism for self-attention that attends only to local context. To allow LongCoder to maintain an understanding of the entire code file, we introduce bridge attention and global attention, with the corresponding two types of globally accessible tokens, bridge tokens and memory tokens.\\n\\nBridge attention aggregates the information of a code snippet and allows it to be accessed from a long distance. Bridge tokens are inserted throughout the input sequence and can attend to a fixed length of context. Memory tokens provide global attention to statements that include a package import, definitions of classes, functions, or structures. The scope of these statements is often global and invoked later, which means they have a longer impact than other statements, making them worth memorizing. By referring to these statements, the model can exploit long context while maintaining linear complexity.\\n\\nTo evaluate the effectiveness of LongCoder and encourage future research on Long Code Completion, we construct a new dataset called LCC by filtering code from GitHub based on length, with the goal of focusing on longer code examples. On average, the examples in LCC are $5 \\\\times$ longer than those in existing datasets (Lu et al., 2021). We benchmark several baselines, LongCoder and OpenAI Codex (Chen et al., 2021) on LCC. Our experimental results demonstrate that code completion can benefit from taking longer context into consideration, and our LongCoder achieves superior performance compared to existing models with comparable computational costs.\\n\\nOverall, our contributions are as follows:\\n\\n\u2022 We construct a new dataset (LCC) for code completion tasks that requires long code modeling to encourage more research in such scenarios.\\n\\n\u2022 We propose two types of sparse attention, motivated by observations on attention patterns of existing models and how human programmers write code.\"}"}
{"id": "guo23j", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LongCoder: A Long-Range Pre-trained Language Model for Code Completion\\n\\nOut-of-Window Context\\n\\nIn-Window Context\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass ConvBlock(nn.Module):\\n    \"\"\"\\n    A block of convolutional and pooling layers\\n    \"\"\"\\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\\n        super(ConvBlock, self).__init__()\\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\\n        self.pool = nn.MaxPool2d(kernel_size=2)\\n\\n    def forward(self, x):\\n        x = self.pool(F.relu(self.conv(x)))\\n        return x\\n\\nclass ConvNet(nn.Module):\\n    \"\"\"\\n    A convolutional neural network\\n    \"\"\"\\n    def __init__(self):\\n        super(ConvNet, self).__init__()\\n        self.conv_block1 = ConvBlock(3, 32, 3, 1, 1)  # first block\\n        self.conv_block2 = ConvBlock(32, 64, 3, 1, 1)  # second block\\n        self.flatten = nn.Flatten()\\n        self.fc1 = nn.Linear(in_features=64 * 8 * 8, out_features=128)\\n        self.fc2 = nn.Linear(in_features=128, out_features=10)\\n\\n    def forward(self, x):\\n        x = self.conv_block1(x)\\n        x = self.conv_block2(x)\\n        x = self.flatten(x)\\n        x = F.relu(self.fc1(x))\\n        x = self.fc2(x)\\n        return x\\n```\\n\\nFigure 1. (Left) An example of how LongCoder facilitates completion with longer context. The memory tokens save potentially useful information (including package imports, class and function definitions) for global access despite whether they are within the sliding window. The bridge tokens aggregate local information by attending to a fixed length of tokens. The information flow within the window is omitted for clarity. (Right) Attention patterns used in BigBird (Zaheer et al., 2020), Longformer (Beltagy et al., 2020) and LongCoder. Best viewed in color.\\n\\n\u2022 We train and release LongCoder, a sparse and efficient pre-trained Transformer model for long code modeling, which achieves superior performance on both long and regular code completion with comparable computational resources.\\n\\n2. Related Work\\n\\nCode Completion\\n\\nCode completion is an essential task that helps programmers improve their efficiency by suggesting and automatically completing code based on context and previous inputs. Prior works have explored the use of statistical learning for the code completion task, such as the use of n-gram techniques (Tu et al., 2014; Hindle et al., 2016) and probabilistic grammar-based methods (Allamanis & Sutton, 2014; Bielik et al., 2016; Raychev et al., 2016; Hellendoorn & Devanbu, 2017). With the success of pre-training in natural language processing (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020), decoder-only pre-trained models based on Transformer have been proposed to promote the development of code completion. Svyatkovskiy et al. (2020) and Lu et al. (2021) respectively propose GPT-C and CodeGPT, which are pre-trained by generating code from left to right in an auto-regressive manner on large amounts of code. Liu et al. (2020) and Guo et al. (2022) pre-train similar models CugLM and UniXcoder with multi-task learning by leveraging code structure for code completion. Codex (Chen et al., 2021), PolyCoder (Xu et al., 2022), CodeGen 2\\n\\n1 All the codes and data are available at https://github.com/microsoft/CodeBERT.\"}"}
{"id": "guo23j", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LongCoder: A Long-Range Pre-trained Language Model for Code Completion (Nijkamp et al., 2022), InCoder (Fried et al., 2022), and AlphaCode (Li et al., 2022) build large language models with billions of parameters and achieve impressive performance on code generation by training on a large-scale and high-quality code corpus. For these pre-trained models, it is impractical to simply expand the context window to model long-range sequences, due to computational complexity of the attention mechanism increasing quadratically with the input length. Therefore, Clement et al. (2021) propose to extract the most important code fragments and integrate them into a fixed-length context window. However, due to the constraint of fixed window length, some high-priority code, such as class and function definitions, may be omitted. Additionally, increasing the window length would also introduce additional computational overhead. Different from these works, LongCoder is a sparse Transformer that can take advantage of the entire file-level code context while maintaining comparable efficiency in terms of computational resources during inference.\\n\\nLong-Range Transformer Models\\n\\nThe original Transformer (Vaswani et al., 2017) is inefficient for modeling long sequences since its time and space complexity is $O(n^2)$, where $n$ is the length of the sequence. Prior studies focus on optimizing the complexity to enable processing of longer sequences. To name a few, Sparse Transformer (Child et al., 2019) reduces the quadratic complexity of standard self-attention by computing attention on sparse query-key pairs. Sparse Transformer uses a dilated sliding window to capture local context. Reformer (Kitaev et al., 2020) proposes locality sensitive hashing (LSH) attention to reduce the complexity and memory footprint. Longformer (Beltagy et al., 2020) uses dilated sliding windows to model longer sequences and adds global memory tokens to allow interaction with all tokens. Performer (Choromanski et al., 2021) generalizes attention calculation by introducing kernel functions. They then propose a random kernel function, namely orthogonal random features (ORF) to approximate the standard self-attention. Linformer (Wang et al., 2020) applies low-rank projection to the length dimension to reduce the complexity of self-attention. Linear Transformers (Katharopoulos et al., 2020) uses a kernel function that exploits the associativity property of matrix products to reduce complexity. BigBird (Zaheer et al., 2020) has an attention pattern comprised of random attention, window attention and global attention. CosFormer (Qin et al., 2022) proposes a linear operator and a cosine-based distance re-weighting mechanism as the substitute for softmax attention. We recommend Tay et al. (2022) as a more comprehensive survey on long-range efficient Transformer models. Different from these works, our LongCoder introduces code heuristics into the dynamic construction of global attention to imitate how human programmers code.\\n\\n3. Long Code Completion\\n\\nCode completion is a fundamental and important task for code models, which can help programmers improve their efficiency while coding. Previous public benchmarks primarily focused on completion with short code context. For instance, CodeXGLUE (Lu et al., 2021) offers two code completion datasets from PY150 (Raychev et al., 2016) in Python and Github Java Corpus Allamanis & Sutton (2013) in Java, and also builds two test datasets to evaluate next-line prediction. The average length of the code context in the two test datasets is 478 tokens and 365 tokens, respectively. However, according to our statistics, the average length of a Python source file on GitHub is 1,305 tokens. After tokenization, the average length becomes 2,090 tokens while 41%/24% of the files have a length longer than 1,024/2,048 tokens, which highlights the need for models that can handle longer code sequences in order to be more practical and useful in the real-world. Meanwhile, longer code sequences contain more complex structures and require models to consider more context and dependencies. This can be challenging for previously proposed code completion models that focus on short code and do not take into account the long context of the code. By evaluating models on longer code sequences, we can better understand their ability to handle more complex and realistic scenarios. Meanwhile, long code completion poses new challenges for efficiency of code models, as in vanilla Transformers (Vaswani et al., 2017), the computational resources grow quadratically with the input length.\\n\\nIn this paper, we introduce the Long Code Completion Benchmark (LCC), a new benchmark that focuses on code completion with long code context for three programming languages: Python, Java, and C#. Specifically, we construct our datasets from the github-code dataset, which contains a vast number of code files sourced from GitHub with an open-source license that permits research use. The steps to construct the datasets are as follows:\\n\\n\u2022 We first follow Allamanis (2019) to deduplicate examples with high similarity (Jacobi similarity $\\\\geq 0.9$) in order to eliminate forked files, and then remove code files that can't be parsed into an abstract syntax tree using a standard compiler tool called tree-sitter.\\n\\n\u2022 Since the benchmark primarily focuses on the code completion task with long code context, we remove code files whose length of code tokens after tokenization is shorter than 512. Additionally, we also eliminate excessively long code files with a length greater than 2,048 tokens.\\n\\n[https://huggingface.co/datasets/codeparrot/github-code](https://huggingface.co/datasets/codeparrot/github-code)\\n\\n[https://github.com/tree-sitter/tree-sitter](https://github.com/tree-sitter/tree-sitter)\"}"}
{"id": "guo23j", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LongCoder: A Long-Range Pre-trained Language Model for Code Completion\\n\\nTable 1. Data statistics of the code context length in LCC test set. 25%/50%/75% refer to the first/second/third quartile.\\n\\n| Language | Average | 25%   | 50%   | 75%   |\\n|----------|---------|-------|-------|-------|\\n| Python   | 1993.3  | 1056  | 1438  | 2211  |\\n| Java     | 1841.4  | 1058  | 1307  | 2003  |\\n| C#       | 1970.5  | 1023  | 1396  | 2143  |\\n\\nFor each programming language, we sample 100k examples for training, and 10k examples for development and 10k for testing. For each sample on development and test sets, we randomly sample an uncommented line of code not shorter than 3 tokens and ensure that there is sufficient context, i.e., a context larger than 512 code tokens. The data statistics of the context length in the LCC test sets are listed in Table 1.\\n\\nWe follow Lu et al. (2021) to evaluate the performance of the models in terms of Exact Match (EM) and Edit Similarity (Edit Sim) on a per-line basis (Svyatkovskiy et al., 2020).\\n\\n4. LongCoder\\n\\nLongCoder is an attempt to tackle the efficiency problem of modeling longer code. It applies sparse attention to reduce quadratic time and space complexity of self-attention to linear. There are three types of attention in LongCoder\u2014window attention, bridge attention, and global attention. Each type is motivated by observations on previous models and focuses on one important aspect in modeling long code. The three types of attention are illustrated in Figure 1 and we will describe them individually.\\n\\n4.1. Window Attention\\n\\nCode completion largely relies on local context while only a few instances of long-distance dependencies are present. For example, in Figure 1 (bottom left), generating assignment operators and parentheses only depend on the current statement, whereas to generate variables such as \\\\( x \\\\) and \\\\( \\\\text{conv} \\\\) block, the model needs to look at neighboring statements. Intuitively, we can exploit such locality to sparsify the attention to achieve better efficiency. We further verify this observation by counting the distribution of average attention scores between two tokens within different distances. As shown in Figure 2, a large portion of attention is concentrated within a narrow window. Notably, a fixed window of 256 covers more than 90% of the attention weights. This sparsity enables us to apply a sliding window attention mechanism (Beltagy et al., 2020; Zaheer et al., 2020).\\n\\n![Figure 2. Distribution of average attention scores between two tokens within different distances in CodeGPT (Lu et al., 2021).](image)\\n\\nFormally, given the linear projections \\\\( Q, K, V \\\\), the self-attention scores in Transformer are calculated as follows:\\n\\n\\\\[\\n\\\\text{Attention}(Q, K, V) = \\\\text{softmax}(QK^T\\\\sqrt{d_k} + M)V\\n\\\\]\\n\\nwhere \\\\( M \\\\) is a mask matrix (to be completed in Equation 5) to control the context a token can attend to when computing its contextual representation. If the \\\\( i \\\\)-th token is allowed to attend to the \\\\( j \\\\)-th token, then \\\\( M_{ij} = 0 \\\\), otherwise \\\\( -\\\\infty \\\\).\\n\\nFor window attention, the mask attention matrix \\\\( M_{\\\\text{window}} \\\\) is calculated as follows:\\n\\n\\\\[\\nM_{\\\\text{window}}_{ij} = \\\\begin{cases} \\n0 & \\\\text{if } |i - j| \\\\leq w \\\\\\\\\\n-\\\\infty & \\\\text{otherwise}\\n\\\\end{cases}\\n\\\\]\\n\\nwhere \\\\( w \\\\) is the window size. This window attention pattern reduces the complexity of the self-attention mechanism by limiting the receptive field size of each token to a small window of size \\\\( w \\\\) at each layer. The computation complexity of this pattern is \\\\( O(n \\\\times w) \\\\), which scales linearly with input sequence length \\\\( n \\\\). After applying \\\\( N \\\\) transformer layers of such sliding window attention, the receptive field size increases to \\\\( N \\\\times w \\\\) at the top layer. Since each token only attends to \\\\( w \\\\) tokens to its left rather than the entire preceding sequence, the model can achieve faster inference speed.\\n\\n4.2. Bridge Attention\\n\\nWindow attention is good at handling local dependencies and it also has a wide receptive field as discussed above. However, if a token needs to access tokens from a distance of \\\\( L \\\\) tokens away, it would require \\\\( \\\\lceil \\\\frac{L}{w} \\\\rceil \\\\) hops through window attention. This makes it challenging to access information from distant context as the attention score between them will be greatly reduced due to accumulation through multiplication. Thus, we introduce a new type of special...\"}"}
{"id": "guo23j", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Visualization of the attention matrix (partially shown for clarity) in CodeGPT (Lu et al., 2021). The attention matrix is averaged across all Transformer layers.\\n\\nSpecifically, we insert \\\\( m \\\\) bridge tokens every \\\\( \\\\lceil \\\\frac{n}{m} \\\\rceil \\\\) tokens and use a separate set of projections, \\\\( Q_b, K_b, V_b \\\\) to compute attention scores for the bridge attention. The bridge tokens do not involve next token prediction but they are used to aggregate information from the preceding \\\\( \\\\lceil \\\\frac{n}{m} \\\\rceil \\\\) tokens. The use of additional projections allows for the ability to model different types of attention. Finally, the mask matrix for bridge attention is calculated as follows:\\n\\n\\\\[\\nM_{\\\\text{bridge}}_{ij} = \\\\begin{cases} \\n0 & \\\\text{if } j \\\\in S_b \\\\text{ and } i \\\\geq j \\\\\\\\\\n0 & \\\\text{if } i \\\\in S_b \\\\text{ and } i - j \\\\leq \\\\lceil \\\\frac{n}{m} \\\\rceil \\\\\\\\\\n-\\\\infty & \\\\text{otherwise}\\n\\\\end{cases}\\n\\\\] (3)\\n\\nThe complexity of bridge attention is \\\\( O(m \\\\times n) \\\\approx O(n) \\\\) where \\\\( m \\\\ll n \\\\). Compared to stacked window attention, bridge attention allows each token to attend to any preceding token with at most 2 hops, which enables the model to effectively access long-range context.\\n\\n4.3. Global Attention\\n\\nIdentifiers with the global scope, for example, package imports, global functions, classes and their member functions (i.e., methods), can be called from any location within a file. For long code, a local sliding window cannot capture such information that should be globally accessible. This is especially outstanding for package imports, which are usually located at the beginning of a file. For example, in Figure 1, without knowing the user has imported \\\\texttt{torch.nn.functional} as a new identifier \\\\texttt{F}, the model cannot be sure whether to use \\\\texttt{F} or the full package name. Also, to call the forward function of \\\\texttt{conv block1} and \\\\texttt{conv block2}, the model needs access to the original definition of the class \\\\texttt{ConvBlock}, which also falls outside the current sliding window. Directly accessing these tokens is similar to how human programmers quickly refer to definitions in the code. This global effect can also be observed in the visualization of the attention matrix in CodeGPT (Lu et al., 2021). As shown in Figure 3, some tokens seem to have a global impact while others only matter locally.\\n\\nTherefore, in addition to bridge tokens, where the model automatically learns to aggregate globally useful information, we add another type of global token, namely memory tokens, to inject code heuristics to the attention. Specifically, we leverage the structure of code with tree-sitter to parse the code into an abstract syntax tree (AST). Then, we find all statements that include a package import, class or function definition and grant the line feeds (LF, \\\\texttt{\\\\n}) of those statements global access. We denote the set of positions of these line feeds as \\\\( G \\\\), where \\\\( k = |G| \\\\). The mask matrix \\\\( M_{\\\\text{global}} \\\\) of global attention is calculated as follows:\\n\\n\\\\[\\nM_{\\\\text{global}}_{ij} = \\\\begin{cases} \\n0 & \\\\text{if } j \\\\in G \\\\text{ and } i \\\\geq j \\\\\\\\\\n-\\\\infty & \\\\text{otherwise}\\n\\\\end{cases}\\n\\\\] (4)\\n\\nThe complexity of the global attention is \\\\( O(kn) \\\\approx O(n) \\\\), as we have \\\\( k \\\\ll n \\\\), where \\\\( n \\\\) is the length of the sequence.\\n\\nUnlike previous work (Clement et al., 2021) which extracts the most significant statements and encode them in a fixed-length context window, our global attention requires less memory and can reuse previously encoded hidden states. Finally, considering all three types of attention together, \\\\( M \\\\) in Equation 1 becomes:\\n\\n\\\\[\\nM = \\\\max(M_{\\\\text{window}}, M_{\\\\text{bridge}}, M_{\\\\text{global}})\\n\\\\] (5)\\n\\nwhere \\\\( \\\\max \\\\) is the element-wise maximum function.\\n\\n5. Experiments\\n\\n5.1. Experimental Settings\\n\\nBaselines\\n\\nWe evaluate LongCoder against several publicly available pre-trained code generation models, including GPT-2 (Radford et al., 2019), CodeGPT (Lu et al., 2021), and UniXcoder (Guo et al., 2022). GPT-2 is pre-trained on a text corpus and CodeGPT is pre-trained on the CodeSearchNet dataset (Husain et al., 2019) using next token prediction as the objective. UniXcoder based on UniLM (Dong et al., 2019) is pre-trained on a cross-modal dataset that includes code, text, and abstract syntax trees. Additionally, we also compare LongCoder with sparse Transformer models, such as... Both are common code styles in PyTorch.\"}"}
{"id": "guo23j", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LongCoder: A Long-Range Pre-trained Language Model for Code Completion\\n\\nTable 2. Experimental results on the Long Code Completion (LCC) dataset.\\n\\n| Model           | #Param. | Memory | Runtime | Python | Java | C# |\\n|-----------------|---------|--------|---------|--------|------|----|\\n| OpenAI Codex    | 12B     | -      | -       | 39.65  | 68.97| 43.15|\\n| Transformer     | 124M    | 191M   | 750ms   | 10.64  | 43.64| 15.32|\\n| GPT-2           | 124M    | 191M   | 750ms   | 11.20  | 42.62| 17.09|\\n| CodeGPT         | 124M    | 191M   | 750ms   | 12.24  | 43.81| 19.20|\\n| UniXcoder       | 126M    | 191M   | 750ms   | 16.55  | 50.22| 23.93|\\n| LongFormer      | 150M    | 381M   | 781ms   | 16.79  | 51.07| 24.80|\\n| BigBird         | 128M    | 205M   | 804ms   | 17.03  | 51.14| 25.19|\\n| LongCoder       | 150M    | 211M   | 812ms   | 17.88  | 55.07| 26.42|\\n| w/o pretrain    | 150M    | 211M   | 812ms   | 17.61  | 54.82| 25.96|\\n\\nTable 3. Data statistics about the context length of CodeXGLUE test dataset. 25%/50%/75% refer to the first/second/third quartile.\\n\\n| Language | Average | 25%  | 50%  | 75%  |\\n|----------|---------|------|------|------|\\n| Python   | 477.8   | 83   | 197  | 502  |\\n| Java     | 365.0   | 74   | 171  | 397  |\\n\\nas LongFormer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020). LongFormer uses a dilated sliding window to model long sequences in the generation task, while BigBird has an attention pattern that includes random, window, and global attention. In addition to these comparable baselines, we also report the performance of OpenAI Codex on LCC for reference. Note that Codex is 100\u00d7 larger than other models and is likely to have seen the test set of LCC in its pretraining thus is not directly comparable.\\n\\nBenchmarks\\n\\nWe evaluate the performance of LongCoder and the baselines on two benchmarks: LCC (introduced in Section 3), and the code completion task benchmark in CodeXGLUE (Lu et al., 2021). CodeXGLUE provides PY150 (Raychev et al., 2016) and JavaCorpus (Allamanis & Sutton, 2013) datasets in Python and Java for line-level code completion. The statistics for the context length of the CodeXGLUE test datasets are listed in Table 3. We can see that the context length of the input sequence is 5 times shorter than LCC, and only a small portion of the samples require modeling for long code sequences. The objective of evaluating the performance of sparse models on the CodeXGLUE dataset is to examine their effectiveness in scenarios where the code context is relatively short. Moreover, longer context can benefit applications including cross-file code completion (Liu et al., 2023). We test the performance of LongCoder on the cross-file-random (XF-R) setting of RepoBench (Liu et al., 2023). The task is to predict the next line of code based on a given in-file context, consisting of import statements and preceding lines before the target line, as well as a cross-file context, comprising snippets from other files in the code repository, parsed by import statements.\\n\\nEvaluation Metrics\\n\\nWe report the number of parameters, inference memory consumption, runtime, Exact Match (EM) and Edit Similarity (Edit Sim) of the baselines. The inference memory consumption and runtime per example are calculated using a beam search with beam size of 5 and maximum generation length of 64 on a single V100 GPU.\"}"}
{"id": "guo23j", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### 5.2 Training Details\\n\\nWe set the maximum length of code context to 512 and 4096 for non-sparse and sparse models, respectively. In order to make a fair comparison between sparse models and non-sparse models, we set the window size \\\\( w \\\\) to 512 so that both types of models maintain the same local context length during inference. Note that this setting is different from the original setting of RepoBench (Liu et al., 2023), thus the results are not directly comparable to those reported in Liu et al. (2023). For sparse models, we use the parameters of UniXcoder released by Guo et al. (2022) to initialize the models. For LongCoder, we set the maximum size of bridge tokens \\\\( n \\\\) and global tokens \\\\( k \\\\) as 16 and 64, respectively. To ensure fair comparison with other models, we pre-train LongCoder on the CodeSearchNet dataset using the same next token prediction objective and pre-training setting as baselines (Lu et al., 2021; Guo et al., 2022). During fine-tuning, we use the Adam optimizer with a batch size of 16 and a learning rate of 2e-4. We fine-tune the model for 10 epochs and perform early stopping on the development set. Note that although the maximum context sequence length is 4096, during inference, we only retain a cache of at most 592 tokens for past key and value hidden states to maintain efficiency in terms of computational resources.\\n\\n### 5.3 Experimental Results\\n\\nTable 2 illustrates the comparison results of LongCoder with other models on the LCC dataset. The results reveal that the sparse models (i.e., the last two groups) have superior performance compared to the non-sparse models (i.e., the second group) on both EM and Edit Sim metrics, and they also maintain a similar inference speed. LongFormer is initialized using the parameters of UniXcoder, with the sole difference being the use of a sliding window attention mechanism. This mechanism allows the model to maintain a consistent inference speed while having a larger receptive field, resulting in improved performance. This demonstrates the effectiveness of the sliding window attention mechanism in code completion tasks. Compared to other sparse models, LongCoder achieves an improvement of 0.8%\u20131.3% in Exact Match score and 4.0%\u20136.0% in Edit Similarity, which reveal the effectiveness of our proposed bridge and global attention mechanisms. Table 4 shows the result of LongCoder on CodeXGLUE code completion benchmarks. It can be observed that LongCoder achieves state-of-the-art performance, which illustrates its effectiveness in scenarios where the code context is short. As shown in Table 5, LongCoder has an even larger advantage compared to UniXcoder, indicating its potential in more complex scenarios.\\n\\n### 5.4 Ablation Study\\n\\nTo better understand the impact of different components on overall performance, we conduct an ablation study on LongCoder, and the results are shown in Table 6. We can see that the average score of Exact Match drops by approximately 1% when memory tokens are removed (w/o memory tokens), which demonstrates the importance of these tokens. On the other hand, when bridge tokens are removed (w/o bridge tokens), the average score drops by about 3% in terms of Edit Similarity. This is likely because bridge tokens assist LongCoder in understanding the semantics of the code context and generating more accurate patterns, while memory tokens enable it to access concrete identifiers with global scope, thus improving accuracy on libraries, classes, and functions invoked. Additionally, we observe that selecting one as a memory token every 64 tokens (equidistant memory tokens) results in worse performance than LongCoder, indicating that the advantage of the memory tokens is not solely due to increased context length. We also evaluate the performance of LongCoder by only using code context within the window size during inference to verify if the improvement is solely attributed to the use of long code context or whether other factors such as fine-tuning settings also contribute. By only keeping the last 512 tokens as code context (w/o out-of-window context), we can see that the performance is nearly the same as UniXcoder in Table 2, which shows the importance of modeling long code context.\\n\\n### 5.5 Case Study\\n\\nWe also conduct a case study to demonstrate the effectiveness of LongCoder, as shown in Figure 4. We provide two examples in the Python and Java programming languages.\"}"}
{"id": "guo23j", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The function sequentially calls the getter functions of the class and has already completed call...\\n\\n...Out-of-Window Context\\n\\nTwo LCC examples of Python code and predictions of different models. Codex-2048 refers to the object into an XML string.\\n\\n...another limitation of our work is the evaluation datasets. Many existing code datasets and LCC share the same source data.\\n\\n...Due to resource constraints, we are not able to train a large model that is comparable to Codex. Besides, to compare resources during inference.\\n\\n...efficiency in terms of computational dependencies. This not only improves performance but also leverages the structure of the code to analyze the scope of...\"}"}
