{"id": "shi23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\nMethod Example Prompt\\n\\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together?\\nA: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna have 5 + 7 = 12 apples together. The answer is 12.\\n\\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together?\\nA: Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples. Elsa and Anna have 5 + 7 = 12 apples together.\\n\\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together?\\nA: Let's break down this problem:\\n1. How many apples does Anna have?\\n2. How many apples do Elsa and Anna have together?\\n1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples.\\n2. Elsa and Anna have 5 + 7 = 12 apples together.\\n\\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together?\\nA: Let's break down this problem:\\n1. How many apples does Anna have?\\n2. How many apples do Elsa and Anna have together?\\n1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples.\\n2. Elsa and Anna have 5 + 7 = 12 apples together.\\n\\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together?\\nA: Let's solve the problem by a Python program:\\n\\n```python\\nElsa_apples = 5\\nAnna_apples = 2 + Elsa_apples\\nElsa_Anna_apples = Elsa_apples + Anna_apples\\nprint(Elsa_Anna_apples)\\n```\\n\\nTable 13. Prompts used for all investigated techniques, without instruction. The placeholder [Problem of Interest] is substituted for each problem at the test time.\"}"}
{"id": "shi23a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\n**Method Example Prompt**\\n\\n```\\nSolve grade school math problems. Feel free to ignore irrelevant information given in the questions.\\n```\\n\\n**Q:** Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together?\\n\\n**A:** Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna have 5 + 7 = 12 apples together. The answer is 12.\\n\\n**Q:** Let\u2019s break down this problem: 1. How many apples does Anna have? 2. How many apples do Elsa and Anna have together?\\n\\n1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples.\\n2. Elsa and Anna have 5 + 7 = 12 apples together.\\n\\n**Q:** Let\u2019s think step by step:\\n\\n**Q:** Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together?\\n\\n**A:** Let\u2019s solve the problem by a Python program:\\n\\n```python\\nElsa_apples = 5\\nAnna_apples = 2 + Elsa_apples\\nElsa_Anna_apples = Elsa_apples + Anna_apples\\nprint(Elsa_Anna_apples)\\n```\\n\\n**Q:** Let\u2019s solve the problem by a Python program:\\n\\n```python\\nElsa_apples = 5\\nAnna_apples = 2 + Elsa_apples\\nElsa_Anna_apples = Elsa_apples + Anna_apples\\nprint(Elsa_Anna_apples)\\n```\\n\\n**Table 14.** All prompts with instructions. The placeholder [Problem of Interest] is substituted for each problem at the test time.\"}"}
{"id": "shi23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\nA. GSM-IC Details\\n\\nEach of the 100 base problem require two to seven steps to solve (Figure 5).\\n\\nFigure 5. Base problem distribution of GSM-IC with respect to the number of reasoning steps in the ground truth problem solution.\\n\\nStarting from the base problems, we follow the protocols below to create GSM-IC (\u00a73.1).\\n\\n1. Irrelevant sentence template.\\n   (a) For in-topic sentences, we manually write templates within the topic that is close to the original problem description. We are particularly careful about the shareable stuff, for example, money is sometimes considered shareable between family members. In such cases, we make sure that the added do not change the amount of shareable stuff to ensure that the final standard answer is not affected.\\n   (b) For off-topic sentences, we use general templates (Table 9) for all problems unless some of them can be considered as in-topic sentences for some problems\u2014for example, the sentence \\\"The height of \\\\{role\\\\} is \\\\{number\\\\} feet.\\\" is considered as an in-topic sentence for problems about heights of people.\\n\\n   The shoe size of \\\\[ROLE\\\\] is \\\\[NUMBER\\\\].\\n   \\\\[ROLE\\\\] is \\\\[NUMBER\\\\] years old.\\n   The height of \\\\[ROLE\\\\] is \\\\[NUMBER\\\\] feet.\\n   \\\\[ROLE\\\\] bought \\\\[NUMBER\\\\] tomatoes from the grocery store.\\n   \\\\[ROLE\\\\] has read \\\\[NUMBER\\\\] books in the past year.\\n\\n   Table 9. Off-topic sentence templates for GSM-IC.\\n   (c) We make sure that all sentences derived by each template are grammatical English sentences.\\n   (d) We write four in-topic and choose four off-topic distractor sentence templates for each problem.\\n\\n2. Blank fillers: role names.\\n   (a) We randomly choose a role name $X$, and use $X's father$, $X's mother$, $X's brother$, $X's sister$ and $X's neighbor$ as the overlapped role names.\\n   (b) We choose from the name set $\\\\{Ada, David, Emma, Jack, John, Mary, Max, Tom\\\\}$ for non-overlapped role names.\\n   (c) We write five names that have overlap with the original character, and five names that do not have overlap for each problem.\\n\\n3. Blank fillers: numbers.\\n   (a) For in-range numbers, we randomly sample positive integers in the range of $[\\\\ell, r]$, where $\\\\ell$ and $r$ denote the smallest and the largest number that appear in the problem description and standard solution, respectively.\"}"}
{"id": "shi23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\nOriginal Problem\\nKim plants 80 cherry pits. 25% of them sprout and Kim sells 6 of the saplings. How many cherry saplings does she have left?\\n\\nAdded Sentence\\nKim's mother plants 20 more potatoes.\\n\\nAmbiguous Problem\\nKim plants 80 cherry pits. 25% of them sprout and Kim sells 6 of the saplings. Kim's mother plants 20 more potatoes. How many cherry saplings does she have left?\\n\\nFixed Problem\\nKim plants 80 cherry pits. 25% of them sprout and Kim sells 6 of the saplings. Kim's mother plants 20 more potatoes. How many cherry saplings does Kim have left?\\n\\nTable 10.\\nAn example that adding irrelevant contexts causes ambiguity: after adding the sentence, it is unclear whether she refers to Kim or Kim's mother. To ensure that the standard answer is unchanged, we modify the final question to make it clear and faithful to the original problem.\\n\\n(b) For out-of-range numbers, we choose from the range of $[2, +\\\\infty)$ or $[\\\\ell_{10}, 10r]$. For very few problems that $\\\\ell$ is relatively large (i.e., $\\\\ell > 10^5$) where we choose out-of-range numbers from the range of $[2, \\\\ell_{10}]$; for other problems we choose out-of-range numbers $n = a \\\\times 10^b$ from the range $[10^r, \\\\infty)$, where $a$ and $b$ are both non-negative integers.\\n\\n(c) We write four in-range numbers and four out-of-range numbers for each problem.\\n\\nFinally, if adding the irrelevant sentence causes ambiguity (e.g., Table 10), we fix the question to ensure that the standard solution to the generated problem remains the same as the base problem.\\n\\nB. Sample Predictions on GSM-IC\\nIn addition to the example outputs shown in Figure 3, we include more example problems and the predictions by different techniques (Tables 11 and 12).\\n\\nC. Full prompts in experiments\\nWe list the prompts for all experiments in Tables 13 and 14.\"}"}
{"id": "shi23a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\nOriginal GSM8K Problem\\n\\nJessica is six years older than Claire. In two years, Claire will be 20 years old. How old is Jessica now?\\n\\nModified Problem (w/ Irrelevant Sentence)\\n\\nJessica is six years older than Claire. In two years, Claire will be 20 years old. Twenty years ago, the age of Claire's father is 3 times of Jessica's age. How old is Jessica now?\\n\\n**Correct Answer:**\\n\\n- In two years, Claire will be 20 years old. So, Claire is 18 years old now. Jessica is six years older than Claire, so Jessica is 18 + 6 = 24 years old now. The answer is 24.\\n\\n**Incorrect Answer:**\\n\\n- Jessica is six years older than Claire, so Jessica is 20 - 6 = 14 years old. Twenty years ago, the age of Claire's father is 3 times of Jessica's age, so 20 years ago, the age of Claire's father is 3 * 14 = 42 years old. The answer is 14.\\n\\nLet's break down this problem: 1. How old is Claire now? 2. How old is Jessica now?\\n\\n1. In two years, Claire will be 20 years old. So Claire is 20 - 2 = 18 years old now.\\n\\n2. Jessica is six years older than Claire. So Jessica is 18 + 6 = 24 years old now.\\n\\nThe answer is 24. The answer is 2.\"}"}
{"id": "shi23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\n**Method**\\n\\n**Original GSM8K Problem**\\nLucy has $65 in the bank. She made a $15 deposit and then followed by a $4 withdrawal. What is Lucy\u2019s bank balance?\\n\\n**Modified Problem (w/ Irrelevant Sentence)**\\nLucy has $65 in the bank. She made a $15 deposit and then followed by a $4 withdrawal. Lucy\u2019s mother\u2019s monthly rent is $10. What is Lucy\u2019s bank balance?\\n\\n**C**  \\n\\n**O**  \\n\\n**T** Lucy has $65 in the bank. She made a $15 deposit, so she has $65 + $15 = $80. She then made a $4 withdrawal, so she has $80 - $4 = $76. The answer is $76.\\n\\n**L**  \\n\\n**T**  \\n\\n**M** Let\u2019s break down this problem: 1. How much money did Lucy deposit? 2. How much money did Lucy withdraw? 3. How much money does Lucy have in the bank?\\n\\n**C**  \\n\\n**O**  \\n\\n**T** Let\u2019s break down this problem: 1. How much money did Lucy deposit? 2. How much money did Lucy withdraw? 3. How much money does Lucy have in the bank?\\n\\n**L**  \\n\\n**T**  \\n\\n**M** 1. Lucy deposited $15. 1. Lucy deposited $15.  \\n\\n**L**  \\n\\n**T**  \\n\\n**M** 2. Lucy withdrew $4. 2. Lucy withdrew $4.  \\n\\n**L**  \\n\\n**T**  \\n\\n**M** 3. Lucy has $65 + $15 - $4 = $76 in the bank. 3. Lucy has $65 + $15 - $4 - $10 = $66 in the bank.  \\n\\nThe answer is $76. The answer is $66.\\n\\n**P**  \\n\\n**R**  \\n\\n**O**  \\n\\n**G**  \\n\\n**R**  \\n\\n**A**  \\n\\n**M**  \\n\\n**L** Lucy balance = 65  \\n\\n**L**  \\n\\n**T**  \\n\\n**M** Lucy balance = Lucy balance + 15  \\n\\n**L**  \\n\\n**T**  \\n\\n**M** Lucy balance = Lucy balance - 4  \\n\\n**L**  \\n\\n**T**  \\n\\n**M** Lucy bank = Lucy bank - 10\\n\\n**Table 12.** Output example #2 for different prompting techniques, on a GSM8K problem and a corresponding GSM-IC problem. While all techniques give correct answers to the original problem, they output the same incorrect result for the modified GSM-IC problem.\"}"}
{"id": "shi23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\n**Original Problem**\\n\\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together?\\n\\n**Problem with Irrelevant Context**\\n\\nQ: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together?\\n\\n**Solution**\\n\\nA: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna have 5 + 7 = 12 apples together. The answer is 12.\\n\\n**Program Solution**\\n\\nA: Let's solve the problem by a Python program:\\n\\n```python\\nElsa_apples = 5\\nAnna_apples = 2 + Elsa_apples\\nElsa_Anna_apples = Elsa_apples + Anna_apples\\nprint(Elsa_Anna_apples)\\n```\\n\\n**Problem of Interest**\\n\\nQ: Jeanne wants to ride the Ferris wheel, the roller coaster, and the bumper cars. The Ferris wheel costs 5 tickets, the roller coaster costs 4 tickets and the bumper cars cost 4 tickets. Jeanne has 5 tickets. Jeanne's neighbor rides 8 kilometers to the bus station every day. How many more tickets should Jeanne buy?\\n\\n**COT Solution**\\n\\nA: Anna has 2 more apples than Elsa, so Anna has 2 + 5 = 7 apples. Elsa and Anna have 5 + 7 = 12 apples together. The answer is 12.\\n\\n4.3. Instructed Prompting\\n\\nIn addition to presenting irrelevant information in the exemplars, we also investigate whether natural language instructions help language models ignore irrelevant context and become less distracted. Extending the line of work (Suzgun et al., 2022; Sanh et al., 2021; Ouyang et al., 2022) that includes a general task description before exemplars, we add the sentence \\\"Solve grade school math problems. Feel free to ignore irrelevant information given in the questions.\\\" before our exemplars in the prompt (Figure 2), which explicitly instructs the language model to ignore irrelevant information in the problem description.\\n\\n5. Experiments\\n\\nBeing mindful of the experiment costs, we uniformly sample 4,000 examples from the GSM-IC dataset (denoted by GSM-IC -4K) for evaluation and analysis purposes throughout this paper. Unless otherwise specified, we mainly use code-davinci-002 in our experiments, and we also evaluate text-davinci-003 which is a model trained with RLHF to better follow instructions (Ouyang et al., 2022). For experiments without self-consistency decoding, we use greedy decoding (i.e., temperature $\\\\tau = 0$); for self-consistency experiments that require multiple samples for a problem, we sample 20 responses with temperature $\\\\tau = 0$.\\n\\n5.1. Main Results on GSM-IC\\n\\nWe compare the performance of different prompting techniques on GSM-IC -4K (Table 3), in terms of both micro\"}"}
{"id": "shi23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\nMethod\\n\\n|               | Micro Accuracy | Macro Accuracy |\\n|---------------|----------------|----------------|\\n| 2 Steps       | 73.5           | 70.8           |\\n| Overall Norm  | 76.2           | 8.3            |\\n| Prompting Exemplar w/o Irrelevant Context, code-davinci-002 | 79.0 | 76.0 |\\n|               | 77.8           | 20.0           |\\n|               | 6.0            | 15.8           |\\n| 2 Steps       | 29.0           | 29.1           |\\n| Overall Norm  | 65.9           | 1.7            |\\n|               | 15.8           | 2.3            |\\n| Prompting Exemplar w/o Irrelevant Context, text-davinci-003 | 69.3 | 66.9 |\\n|               | 68.4           | 10.0           |\\n|               | 9.0            | 11.3           |\\n| 2 Steps       | 79.8           | 72.4           |\\n| Overall Norm  | 80.8           | 16.7           |\\n|               | 14.0           | 14.7           |\\n| Prompting Exemplar w/ Irrelevant Context, code-davinci-002 | 80.5 | 74.4 |\\n|               | 78.1           | 20.0           |\\n|               | 12.0           | 17.9           |\\n| 2 Steps       | 78.1           | 84.6           |\\n| Overall Norm  | 85.9           | 23.3           |\\n|               | 28.0           | 29.8           |\\n| Prompting Exemplar w/ Irrelevant Context, text-davinci-003 | 81.0 | 85.4 |\\n|               | 82.8           | 23.3           |\\n|               | 28.0           | 29.8           |\\n| 2 Steps       | 67.0           | 55.0           |\\n| Overall Norm  | 74.9           | 11.7           |\\n|               | 9.0            | 10.8           |\\n| Prompting Exemplar w/ Irrelevant Context, text-davinci-003 | 68.8 | 54.8 |\\n|               | 63.2           | 15.0           |\\n|               | 7.5            | 14.5           |\\n\\nTable 3. Micro and macro accuracies (\u00d7100) on the GSM-IC -4K dataset. SC denotes self-consistency. Norm is the overall accuracy normalized by the fraction of solved base problems (Table 2), which is a measure for robustness w.r.t. irrelevant information. For text-davinci-003, the base problem accuracy with COT is 80.0, and the base problem accuracy with LT is 81.0. The best numbers in each column for each section (i.e., whether using code-davinci-002 or text-davinci-003, whether using exemplar with irrelevant context or not, and whether using self-consistency or not) are in boldface.\\n\\nOverall, we observe significant performance drop for both models with all prompting techniques. The drop on macro accuracy is especially large, showing that fewer than 30% of the base problems are consistently solved after adding distractors. Comparing the results of two models, text-davinci-003 achieves better normalized micro accuracy than code-davinci-002, though its macro accuracy is mostly worse. In Figure 3, we present a GSM-IC -4K example where a single irrelevant sentence causes different types of errors in investigated prompting techniques. One common error type is wrongly using the number in the irrelevant sentence, as shown in the LT prediction and other examples in Appendix B. Even if the model does not directly use the irrelevant number for numerical calculation, the presence of the irrelevant sentence in the reasoning steps alone can still cause a wrong prediction, as shown in the COT prediction.\\n\\nLT is generally the most robust technique to irrelevant context. In terms of micro accuracy, LT outperforms all other prompting methods across models. Using code-davinci-002, LT achieves about double macro accuracy of COT. Interestingly, with text-davinci-003, despite that LT outperforms COT on the micro accuracy, its macro accuracy is lower. Specifically, text-davinci-003 is highly susceptible to irrelevant context with role overlap; e.g., such irrelevant sentences decrease the macro accuracy to 0 on problems with more than 2 reasoning steps. See Table 4 for the breakdown performance on different types of irrelevant context. Selecting exemplars with distractors mitigates the distractibility. For few-shot prompts, we find that using exemplars with distractors (i.e., including problems with irrelevant context) consistently outperforms using the original exemplars without distractors across prompting techniques. While prior work has shown that training and fine-tuning with different types of problems improves model robustness (Li et al., 2022), our results show that prompting with exemplars that demonstrate how to ignore irrelevant context also results in significant robustness improvement. In Table 5, we further show that using exemplars with distractors does not cause a performance drop on the original GSM8K dataset.\"}"}
{"id": "shi23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Maria buys a large bar of French soap that lasts her for 2 months. She spends $8.00 per bar of soap. Every 10 months, Maria's neighbor buys a new shampoo and moisturizer for Maria's neighbor. If Maria wants to stock up for the entire year, how much will she spend on soap?\\n\\nCorrect COT Output to the Original Problem\\nMaria spends $8.00 per bar of soap. She buys a bar of soap every 2 months. There are 12 months in a year. Maria will spend $8.00 \\\\times 12 \\\\div 2 = $48.00 on soap for the year.\\nThe answer is $48.00.\\n\\nOutput to the Modified Problem\\nMaria buys a large bar of French soap that lasts her for 2 months. She spends $8.00 per bar of soap. Every 10 months, Maria's neighbor buys a new shampoo and moisturizer for Maria's neighbor. If Maria wants to stock up for the entire year, she will spend $8.00 \\\\times 12 = $96.00 on soap.\\nThe answer is $96.00.\\n\\nLet's break down this problem: 1. How much does Maria spend on soap every 10 months? 2. How much does Maria spend on soap every year?\\n1. Maria spends $8.00 per bar of soap. She buys a new bar of soap every 10 months. So Maria spends $8.00 \\\\times 10 = $80.00 every 10 months.\\n2. Maria spends $80.00 every 10 months. So Maria spends $80.00 \\\\times 12 = $960.00 every year.\\nThe answer is $960.00.\\n\\nSoap_price = 8.00\\nSoap_per_year = 2 \\\\times 12\\nSoap_total = Soap_price \\\\times Soap_per_year\\nprint(Soap_total) \\nOutput: 192.0\"}"}
{"id": "shi23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\n| Method               | Exemplar                       | Micro Accuracy | Macro Accuracy |\\n|----------------------|--------------------------------|----------------|----------------|\\n|                      | In Off Yes No In Out            | 63.1 80.7 68.3 76.6 70.2 74.6 | 10.2 33.0 10.3 22.2 11.0 19.0 |\\n|                      |                                 | 70.8 83.4 77.0 78.2 77.2 77.8 | 23.5 45.0 25.8 35.4 27.0 29.0 |\\n|                      |                                 | 44.1 63.5 50.7 58.4 54.3 54.5 | 4.1 24.0 9.3 16.2 7.0 11.0 |\\n| Prompting Exemplar w/o Irrelevant Context (code-davinci-002) | COT | 63.3 72.9 68.7 68.1 67.2 69.6 | 16.3 36.0 17.5 20.2 19.0 22.0 |\\n|                      | LT M                           | 75.4 76.9 75.6 76.8 75.3 77.2 | 6.1 7.0 6.2 9.1 6.0 6.0 |\\n|                      | PROGRAM                        | 52.9 70.5 60.2 64.5 61.5 62.8 | 10.2 37.0 14.4 23.2 15.0 17.0 |\\n| Prompting Exemplar w/ Irrelevant Context (code-davinci-002) | COT | 70.2 82.7 73.6 80.2 76.1 77.7 | 18.4 43.0 21.6 32.3 22.0 26.0 |\\n|                      | LT M                           | 73.0 87.5 81.4 80.2 80.0 81.4 | 28.6 58.0 37.1 42.4 41.0 35.0 |\\n|                      | PROGRAM                        | 52.9 70.5 60.2 64.5 61.5 62.8 | 10.2 37.0 14.4 23.2 15.0 17.0 |\\n\\nTable 4. Breakdown accuracies (\u00d7100) w.r.t. the factors of the added irrelevant sentence. Lower accuracy indicates the model is more fragile to the corresponding type of irrelevant contexts. Note that the macro average accuracies are higher than the corresponding ones reported in Table 3, as we only include a subset of created problems (i.e., those corresponding to the appropriate factor) here to compute the metric. The best result in each column is in boldface.\\n\\nOriginal exemplars reaches comparable or even better performance than uninstructed prompting that uses exemplars with distractors for both COT and LTM. Note that adding the instruction \u201cSolve grade school math problems.\u201d alone does not significantly improve the performance, and it is the instruction \u201cFeel free to ignore irrelevant information given in the questions.\u201d that makes the difference. Similar to the instruction \u201cLet\u2019s think step by step.\u201d employed by 0-CoT, this shows that language models are\u2014to some extent\u2014able to follow natural language instructions in a way that dramatically changes their problem solving behavior, suggesting that such instructions may be useful for guiding the behavior of language models on more tasks.\\n\\nOn the original GSM8K development set (Cobbe et al., 2021; Zhou et al., 2022), we do not observe a drop in accuracy when using exemplars with irrelevant information, adding natural language instructions, or both (Table 5). The same holds for SV AMP (Patel et al., 2021), an arithmetic reasoning benchmark constructed by applying different types of variations to math problems from existing clean datasets, e.g., changing sentence structures, asking different questions with the same information, etc. This is impressive because the results on GSM-IC show that prompt exemplars with irrelevant information and instructed prompting both improve robustness. For the PROGRAM prompt, we find that using exemplars with distractors even increases performance on SV AMP.\\n\\n5.4. Complicated Prompts May Hurt the Robustness to Irrelevant Context\\n\\nWe compare our 1-exemplar COT prompt (Figure 2) to a 4-exemplar prompt (Appendix D of Zhou et al., 2022),\"}"}
{"id": "shi23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.\\n\\n1. Introduction\\n\\nPrompting large language models performs decently well in a variety of domains (Brown et al., 2020; Chowdhery et al., 2022, inter alia). However, for most of these evaluation benchmarks, all the information provided in the problem description is relevant to the problem solution, as the problems in exams. This is different from real-world situations, where problems usually come with several pieces of contextually related information, which may or may not be relevant to the problems that we want to solve. We have to identify what information is actually necessary during solving those problems. Studies in psychology have shown that irrelevant information may significantly decrease some children and even adults problem-solving accuracy (Hoyer et al., 1979; Pasolunghi et al., 1999; Marzocchi et al., 2002, inter alia).\\n\\nIn this work, we study the distractibility of large language models for various prompting techniques; i.e., how is large language model prompting affected by irrelevant context, and what strategies can be used to improve performance? To measure distractibility, we construct the GSM-IC dataset, a grade-school math problem dataset derived from GSM8K (Cobbe et al., 2021) and introduce two different metrics. In contrast to prior work that derives benchmark variations by substituting sentences of the base problems with variations (Patel et al., 2021; Kumar et al., 2021, inter alia), we keep the base problem description and add to it one irrelevant sentence, while making sure that it does not affect the solution of the problem (Table 1).\\n\\nWe use Codex (code-davinci-002) and GPT-3.5 (text-davinci-003) in the GPT3 model family to evaluate state-of-the-art prompting techniques on GSM-IC, including chain-of-thought prompting (COT; Wei et al., 2022), zero-shot chain-of-thought prompting (0-COT; Kojima et al., 2022), least-to-most-prompting (LTM; Zhou et al., 2022), and prompting with programs (PROGRAM; 2 http://openai.com/api/1).\"}"}
{"id": "shi23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\nWe find that their performance on GSM-IC greatly decreases compared to the original GSM8K (without irrelevant context). We then investigate several approaches to mitigate this weakness, including self-consistency (Wang et al., 2022c) and adding irrelevant information to the exemplars in the prompt. In addition to demonstrating how to handle irrelevant information via exemplars, we also investigate the usage of task-specific instructions (Wei et al., 2021; Sanh et al., 2021; Ouyang et al., 2022; Suzgun et al., 2022; Chung et al., 2022), where we prepend an instruction sentence \u201cfeel free to ignore irrelevant information in the problem description\u201d to the exemplars. We summarize our key findings below:\\n\\n1. All investigated prompting techniques are sensitive to irrelevant information in the problem description. In particular, among the original problems that can be solved by baseline prompts with greedy decoding, no more than 18% of them can be consistently solved for all types of irrelevant information, showing that the large language model is easily distracted and produces inconsistent predictions when adding a small amount of irrelevant information to the problem description.\\n\\n2. Self-consistency improves the performance of all prompting techniques on GSM-IC. In particular, the recall rate of the correct answer for GSM-IC is as high as 99.7% with 20 samples per problem, i.e., at least one of the 20 solutions result in the correct final answer, which means that using multiple samples allows the model to almost always retrieve the correct answer.\\n\\n3. Adding irrelevant information to the exemplars shown in the prompt consistently boosts the performance, and the same holds for adding an instruction to ignore irrelevant context. This suggests that language models are\u2014to some extent\u2014able to learn to ignore irrelevant information by following examples or instructions.\\n\\n4. We identify different factors of the irrelevant information that affect the model\u2019s sensitivity to irrelevant context. Our breakdown analysis shows that varying the numbers in the irrelevant information does not notably change the model performance, while the degree of lexical overlap with the original problem description matters.\\n\\nFiltering out irrelevant information is essential for handling real-world tasks. Our evaluation indicates that despite the strong performance on challenging reasoning problems, state-of-the-art language models still have fundamental weaknesses in context understanding and identifying the relevant information from the input. Our findings suggest that in order to gain a more holistic understanding of the reasoning capability of language models, future work should also consider the model sensitivity to irrelevant context, in addition to solving more challenging problems.\\n\\nRelated Work\\n\\nFew-shot prompting. Few-shot prompting (Brown et al., 2020; Chowdhery et al., 2022, inter alia) has been significantly boosted with various techniques, including generating intermediate steps (Ling et al., 2017; Cobbe et al., 2021; Nye et al., 2021; Wei et al., 2022; Suzgun et al., 2022; Shi et al., 2022b, inter alia), problem decomposition (Zhou et al., 2022; Drozdov et al., 2022; Dohan et al., 2022; Khot et al., 2022; Press et al., 2022, inter alia), generating programs (Austin et al., 2021; Chowdhery et al., 2022; Gao et al., 2022; Chen et al., 2022, inter alia), marginalizing intermediate steps that share the same result (Wang et al., 2022c; Shi et al., 2022a), and ensemble (Wang et al., 2022b; Drozdov et al., 2022). In addition, Kojima et al. (2022) demonstrate that appropriate hint in prompts also leads to decent performance, even without any exemplar. In this work, we examine these cutting-edge prompting techniques (Wei et al., 2022; Zhou et al., 2022; Kojima et al., 2022; Wang et al., 2022c) on our benchmark, and demonstrate that they are sensitive to irrelevant input context.\\n\\nNatural language benchmarks with input perturbations. There has been a long line of work on adding input perturbations for natural language tasks, including model-agnostic input transformations (Liang et al., 2022; Ravichander et al., 2022, inter alia) and adversarial example generation against individual models (Jia & Liang, 2017; Shi et al., 2018; Morris et al., 2020; Wang et al., 2021). In particular, prior work has constructed arithmetic reasoning benchmarks through paraphrasing or rewriting sentences in the base problems from clean datasets (Patel et al., 2021; Kumar et al., 2021). Meanwhile, Liang et al. (2022) evaluate various large language models under several metrics, including accuracy, robustness, fairness, etc. Specifically, the input transformations in their robustness evaluation include semantics-preserving and semantics-altering perturbations, such as injecting typos and modifying sentences to change the ground-truth classification labels. In contrast the above work where the meaning of problem descriptions may be changed with perturbations, we keep all sentences in the original problem description, and introduce an irrelevant sentence that is ensured not to affect the standard answer.\\n\\nNatural language benchmarks with irrelevant input context. Jia & Liang (2017) have shown that neural question answering systems are largely affected by adversarial distracting sentences, whereas follow up work (Khashabi et al., 2017; Ni et al., 2019) proposes learning strategies that mitigate the problem. Similar issues have been found for general-purpose pretrained language models, on the tasks of factual reasoning (Kassner & Sch\u00a8utze, 2020; Pandia & Ettinger, 2021; Misra et al., 2023; Li et al., 2022), code generation (Jones & Steinhardt, 2022), and syntactic generalization (Chaves & Richter, 2021). In particular, Li et al.\"}"}
{"id": "shi23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\n(2022) evaluated T5 (Raffel et al., 2020) and PaLM (Chowdhery et al., 2022) with few-shot prompts, and proposed knowledge-aware finetuning that finetunes the model on problems with counterfactual and irrelevant context, which strengthens the model robustness to noisy context. In our evaluation, we show that without training or finetuning, adding irrelevant context into demonstrations in the prompt also mitigates the distractibility of the underlying language model and significantly improves the model performance on our GSM-IC benchmark.\\n\\nThere exist some logical reasoning benchmarks that contain irrelevant content in task descriptions (Weston et al., 2015; Sinha et al., 2019; Clark et al., 2021; Han et al., 2022; Tafjord et al., 2020, inter alia). However, previous work largely focuses on designing models that require extra training, and prompting alone still hardly achieves the same level of performance as finetuned models for these tasks (Han et al., 2022; Creswell et al., 2022). In our work, we focus on arithmetic reasoning, where prompting techniques have achieved the state-of-the-art results, e.g., on GSM8K, while we show that adding a single irrelevant sentence into the problem description significantly degrades the performance.\\n\\nPrompting with noisy ground truth. A line of work studies the model performance with incorrect prompting ex-emplars, i.e., the example problems are paired with wrong answers (Min et al., 2022; Kim et al., 2022). In addition, prior work has investigated the model sensitivity to other parts of the prompt, such as instruction tuning with misleading and irrelevant instructions (Webson & Pavlick, 2021) and wrong reasoning steps in the examples (Madaan & Yazdanbakhsh, 2022; Wang et al., 2022a). In particular, Madaan & Yazdanbakhsh (2022) conclude that the correctness of numbers and equations in chain-of-thought prompts does not play a key role in model performance, but using wrong entities and removing either equations or text explanation in the reasoning steps drastically hamper the performance.\\n\\nDifferent from this line of work, we always include correct answers to example problems in the prompt, and ensure that the irrelevant context added to the problem description does not change the ground truth answer. We show that the model performance significantly drops when presented with irrelevant context in problem descriptions, and different distributions of numbers and entities in the irrelevant context also lead to different levels of performance degradation.\\n\\n3. The GSM-IC Dataset\\n\\nIn this section, we introduce the creation process of the GSM-IC dataset (\u00a73.1) and the evaluation metrics (\u00a73.2).\\n\\n3.1. Dataset Creation\\n\\nWe randomly choose 1,000 problems from the GSM8K training set as a development set. To construct our base dataset, we then choose 100 problems from this development set that can be correctly solved by at least one of the prompting techniques mentioned in this paper; that is, our base dataset is an \u201ceasy\u201d subset of GSM8K (Table 2). Each base problem requires two to seven reasoning steps. Among the 100 base problems, 60 of them can be solved with two reasoning steps. The full dataset statistics can be found in Appendix A.\\n\\nWe then generate the examples of our new dataset by adding to each base problem one sentence containing irrelevant information. We use a template-based method (Figure 1) to generate these sentences, which can be characterized by the following three factors:\\n\\n\u2022 Topic of the inserted sentence. We write templates for both in-topic and off-topic sentences. In-topic sentences are closely related to the topic of the original problem, whereas off-topic sentences are about a different topic.\\n\\n\u2022 Role name overlap. Most sentence templates contain some role name blanks, which can be filled with names that may or may not overlap with the role names that occur in the problem. For blank fillers that have overlap with original role names, we: (1) randomly pick a role name from the original problem description and (2) create the blank fillers with template such as A\u2019s father and A\u2019s sister.\\n\\nWe do not generate new examples or perform analysis on the test set to avoid potential tuning-on-test-set issues. The number of reasoning steps of a problem is given by the number of sentences in its standard answer (Cobbe et al., 2021).\\n\\nFigure 1. Illustration of the considered factors when creating the GSM-IC dataset. Best viewed in color.\"}"}
{"id": "shi23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\n\u2022 Range of numbers. Since we focus on arithmetic reasoning, most sentence templates also contain a number blank. We can choose to fill in the number blank with a number of similar or different magnitude to those in the original problem description. Concretely, for a number \\\\( a \\\\), if there exists a number \\\\( b \\\\) in the original problem description or solution such that \\\\( 1 \\\\leq a \\\\leq 10 \\\\), we consider \\\\( a \\\\) as an in-range number, and otherwise an out-of-range number.\\n\\nSince the standard answer to GSM8K problems are all positive integers, we only consider positive integers as the number blank fillers.\\n\\nWe manually verify that (1) all the generated sentences are acceptable in English and that (2) adding them does not affect the standard solution of the base problem. Because the above factors are orthogonal, we generate for each base example a set of derived examples with different factor combinations. The full GSM-IC benchmark consists of 58,052 examples. More details about the dataset creation process can be found in Appendix A.\\n\\n3.2. Evaluation Metrics\\n\\nFor a problem \\\\( p \\\\), we denote its standard solution by \\\\( s(p) \\\\), and the solution of method \\\\( M \\\\) by \\\\( M(p) \\\\). To evaluate the distractibility of \\\\( M \\\\), we consider the following two metrics:\\n\\n\u2022 Micro accuracy \\\\( \\\\text{Acc}^{\\\\text{micro}}(M; P) \\\\) is the average accuracy of method \\\\( M \\\\) over all the test problems \\\\( P \\\\).\\n\\n\\\\[\\n\\\\text{Acc}^{\\\\text{micro}}(M; P) = \\\\frac{\\\\sum_{p \\\\in P} 1[\\\\text{M}(p) = s(p)]}{|P|}\\n\\\\]\\n\\nThis means that the micro accuracy weights all the individual test problems equally.\\n\\n\u2022 Macro accuracy \\\\( \\\\text{Acc}^{\\\\text{macro}}(M; B) \\\\) is the average accuracy of method \\\\( M \\\\) over classes of test problems, where each class \\\\( P(b) \\\\) consists of the set of test examples derived from the base example \\\\( b \\\\in B \\\\). We define \\\\( M \\\\)'s prediction for a class \\\\( P(b) \\\\) to be correct if and only if \\\\( M \\\\)'s prediction for all problems in this class are correct.\\n\\n\\\\[\\n\\\\text{Acc}^{\\\\text{macro}}(M; B) = \\\\frac{\\\\sum_{b \\\\in B} 1[\\\\bigwedge_{p \\\\in P(b)} \\\\text{M}(p) = s(p)]}{|B|}\\n\\\\]\\n\\nThis means that the macro accuracy is the fraction of base problems that can be consistently solved no matter what irrelevant sentence is being added.\\n\\n\u2022 Normalized accuracy measures how a method is affected by the distractors, considering its accuracy on base problems. For a micro or macro accuracy \\\\( \\\\text{Acc}^{\\\\text{micro}}(M) \\\\) achieved by method \\\\( M \\\\), we calculate its corresponding normalized accuracy by\\n\\n\\\\[\\n\\\\text{norm}(\\\\text{Acc}^{\\\\text{micro}}(M); M) = \\\\frac{\\\\text{Acc}^{\\\\text{micro}}(M)}{\\\\text{Acc}^{\\\\text{micro}}(M)}\\n\\\\]\\n\\nwhere \\\\( \\\\text{norm}(M) \\\\) denotes the base problem accuracy of method \\\\( M \\\\) (Table 2).\\n\\n4. Investigated Solutions\\n\\nIn the following section, we review the investigated prompting techniques (\u00a74.1), present the formats of our prompts (\u00a74.2), and introduce instructed prompting (\u00a74.3).\\n\\n4.1. Base Techniques\\n\\nChain-of-thought prompting (COT; Wei et al., 2022) is a prompting technique that guides the language models to solve a problem in a step-by-step manner. By presenting exemplars that solve the corresponding problems with intermediate reasoning steps in the prompts, COT significantly improves the reasoning performance over direct answer prediction without such intermediate reasoning steps.\\n\\nZero-shot chain-of-thought prompting (0-COT; Kojima et al., 2022) is a variation of COT where the prompt does not contain any exemplar. Instead, the model is prompted directly with the problem of interest followed by the instruction \\\"Let's think step by step:\\\"\\n\\nLeast-to-most prompting (LTM; Zhou et al., 2022) teaches language models to (1) break down a problem into subproblems, and (2) solve those subproblems sequentially using COT. The final answer is that to the last subproblem.\\n\\nProgram prompts (PROGRAM; Chowdhery et al., 2022) represent the arithmetic reasoning process as a program. Following prior work on solving GSM8K problems with code (Chowdhery et al., 2022; Gao et al., 2022; Chen et al., 2022), we include a Python program as the problem solution in the prompt, and execute the generated Python code using an external Python interpreter to obtain the final answer.\\n\\nSelf-consistency (SC; Wang et al., 2022c; Shi et al., 2022a) may further boost the reasoning performance by marginalizing over intermediate reasoning steps that share the same final result. In practice, SC can be implemented by (1) sampling several solutions from the large language model and (2) taking the majority vote. Note that SC is orthogonal to above techniques, and can be combined with any of them.\\n\\n4.2. Prompt Design\\n\\nWe present some example prompts used in our experiments (Figure 2). For few-shot prompting techniques (i.e., COT, LTM and PROGRAM), the input prompt includes exemplar problems and their solutions before the problem of interest. In order to keep simplicity and avoid over-fitting in prompt engineering, we follow Zhou et al. (2022) on exemplar creation; that is, we only use one simple exemplar for our main experiments. This exemplar is either based on the [Original Problem] or the [Problem with Irrelevant Context], which\"}"}
{"id": "shi23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\nTable 6. Micro accuracies ($\\\\times 100$) on the GSM8K development set and GSM-IC-4K. # Prompting exemplars denotes the number of exemplars used in the prompt. The best number in each column is in boldface.\\n\\n| Method          | CO T 1 | 60.3 | 73.6 | 70.8 |\\n|-----------------|--------|------|------|------|\\n| CO T 2          | 66.3   | 78.0 | 69.4 |\\n| CO T 4 + NST    | 58.8   | 79.0 | 76.0 |\\n| CO T 7          | 66.5   | 79.2 | 70.6 |\\n\\nTable 7. Accuracies ($\\\\times 100$) on the football split of DROP (Dua et al., 2019) benchmark.\\n\\n| Method          | CO T 7 | 67.4 | 68.2 |\\n|-----------------|--------|------|------|\\n| CO T 7 + NST    | 68.9   | 69.9 |\\n| LT M 5          | 73.4   | 70.2 |\\n| LT M 5 + NST    | 74.4   | 72.8 |\\n\\n5.5. Extension to DROP\\n\\nIn addition to GSM-IC, we extend our evaluation to the DROP dataset (Dua et al., 2019), where the task is to answer a question according to a long passage that naturally contains irrelevant context. We show an example about football games in Table 8.\\n\\nTable 8. A DROP example about football games.\\n\\nWe use the CO T and LT M prompts in (Zhou et al., 2022) as the baselines, and we evaluate the prompt variants with the instruction \u201cSolve following questions. Feel free to ignore irrelevant information given in the questions.\u201d added before the exemplars. Note that by adding a problem reduction step in the exemplar solution, the least-to-most prompt implicitly leads the model to come up with relevant subproblems to solve the given problem. Again, we observe that the instruction consistently improves the performance of both CO T and LT M prompting (Table 7).\\n\\n6. Conclusion and Discussion\\n\\nIn this work, we introduce GSM-IC, a dataset that supports comprehensive study of the distractibility of large language models when performing arithmetic reasoning in presence of irrelevant contexts. We examine a variety of prompting techniques on GSM-IC, and demonstrate that they are all sensitive to the irrelevant information in the problems. Among the studied techniques, self-consistency (Wang et al., 2022c) leads to a substantial improvement in robustness to irrelevant context across the board, and presenting example problems with irrelevant context in the prompt also consistently improves the performance. Similarly, we find that simply adding an instruction to ignore irrelevant information brings notable performance gains on our benchmark.\\n\\nDespite the improvement achieved by these methods, the fundamental issue remains: a single piece of irrelevant information can distract the models and substantially degrade their performance, even on problems whose clean versions they correctly solve. We encourage researchers to also prioritize improving on this fundamental limitation when developing new training and prompting techniques. We leave further investigation on the distractibility for other tasks and different language models for future work.\\n\\nAcknowledgement\\n\\nWe would like to thank Dale Schuurmans, Olivier Bousquet and Jack Nikodem for helpful discussion and feedback.\"}"}
{"id": "shi23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\nReferences\\n\\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and Sutton, C. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. NeurIPS, 2020.\\n\\nChaves, R. P. and Richter, S. N. Look at that! BERT can be easily distracted from paying attention to morphosyntax. In Proceedings of the Society for Computation in Linguistics 2021, pp. 28\u201338, Online, February 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.scil-1.3.\\n\\nChen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.\\n\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\n\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\\n\\nClark, P., Tafjord, O., and Richardson, K. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 3882\u20133890, 2021.\\n\\nCobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. URL https://arxiv.org/pdf/2110.14168.\\n\\nCreswell, A., Shanahan, M., and Higgins, I. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.\\n\\nDohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-Dickstein, J., Murphy, K., and Sutton, C. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.\\n\\nDrozdov, A., Sch\u00e4rli, N., Aky\u00fcrek, E., Scales, N., Song, X., Chen, X., Bousquet, O., and Zhou, D. Compositional semantic parsing with large language models. arXiv preprint arXiv:2209.15003, 2022.\\n\\nDua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\\n\\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.\\n\\nHan, S., Schoelkopf, H., Zhao, Y., Qi, Z., Riddell, M., Benson, L., Sun, L., Zubova, E., Qiao, Y., Burtell, M., et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022.\\n\\nHoyer, W. J., Rebok, G. W., and Sved, S. M. Effects of varying irrelevant information on adult age differences in problem solving. Journal of gerontology, 34(4):553\u2013560, 1979.\\n\\nJia, R. and Liang, P. Adversarial examples for evaluating reading comprehension systems. arXiv preprint arXiv:1707.07328, 2017.\\n\\nJones, E. and Steinhardt, J. Capturing failures of large language models via human cognitive biases. arXiv preprint arXiv:2202.12299, 2022.\\n\\nKassner, N. and Sch\u00fctze, H. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7811\u20137818, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.698. URL https://aclanthology.org/2020.acl-main.698.\\n\\nKhashabi, D., Khot, T., Sabharwal, A., and Roth, D. Learning what is essential in questions. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pp. 80\u201389, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/K17-1010. URL https://aclanthology.org/K17-1010.\\n\\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.\"}"}
{"id": "shi23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\nKim, J., Kim, H. J., Cho, H., Jo, H., Lee, S.-W., Lee, S.-g., Yoo, K. M., and Kim, T.\\nGround-truth labels matter: A deeper look into input-label demonstrations.\\narXiv preprint arXiv:2205.12685, 2022.\\n\\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y.\\nLarge language models are zero-shot reasoners.\\narXiv preprint arXiv:2205.11916, 2022.\\n\\nKumar, V., Maheshwary, R., and Pudi, V.\\nAdversarial examples for evaluating math word problem solvers.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2705\u20132712, 2021.\\n\\nLi, D., Rawat, A. S., Zaheer, M., Wang, X., Lukasik, M., Veit, A., Yu, F., and Kumar, S.\\nLarge language models with controllable working memory.\\narXiv preprint arXiv:2211.05110, 2022.\\n\\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et al.\\nHolistic evaluation of language models.\\narXiv preprint arXiv:2211.09110, 2022.\\n\\nLing, W., Yogatama, D., Dyer, C., and Blunsom, P.\\nProgram induction by rationale generation: Learning to solve and explain algebraic word problems.\\nIn Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 158\u2013167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.\\n\\nMadaan, A. and Yazdanbakhsh, A.\\nText and patterns: For effective chain of thought, it takes two to tango.\\narXiv preprint arXiv:2209.07686, 2022.\\n\\nMarzocchi, G. M., Lucangeli, D., De Meo, T., Fini, F., and Cornoldi, C.\\nThe disturbing effect of irrelevant information on arithmetic problem solving in inattentive children.\\nDevelopmental neuropsychology, 21(1):73\u201392, 2002.\\n\\nMin, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L.\\nRethinking the role of demonstrations: What makes in-context learning work?\\narXiv preprint arXiv:2202.12837, 2022.\\n\\nMisra, K., Rayz, J., and Ettinger, A.\\nCOMPS: Conceptual minimal pair sentences for testing robust property knowledge and its inheritance in pre-trained language models.\\nIn Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 2023.\\n\\nMorris, J. X., Lifland, E., Yoo, J. Y., Grigsby, J., Jin, D., and Qi, Y.\\nTextattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp.\\narXiv preprint arXiv:2005.05909, 2020.\\n\\nNi, J., Zhu, C., Chen, W., and McAuley, J.\\nLearning to attend on essential terms: An enhanced retriever-reader model for open-domain question answering.\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 335\u2013344, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1030. URL https://aclanthology.org/N19-1030.\\n\\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and Odena, A.\\nShow your work: Scratchpads for intermediate computation with language models.\\narXiv preprint arXiv:2112.00114, 2021.\\n\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.\\nTraining language models to follow instructions with human feedback.\\narXiv preprint arXiv:2203.02155, 2022. URL https://arxiv.org/abs/2203.02155.\\n\\nPandia, L. and Ettinger, A.\\nSorting through the noise: Testing robustness of information processing in pre-trained language models.\\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1583\u20131596, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.119. URL https://aclanthology.org/2021.emnlp-main.119.\\n\\nPasolunghi, M. C., Cornoldi, C., and De Liberto, S.\\nWorking memory and intrusions of irrelevant information in a group of specific poor problem solvers.\\nMemory & Cognition, 27:779\u2013790, 1999.\\n\\nPatel, A., Bhattamishra, S., and Goyal, N.\\nAre nlp models really able to solve simple math word problems?\\nIn NAACL-HLT, 2021. URL https://aclanthology.org/2021.naacl-main.168.pdf.\\n\\nPress, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M.\\nMeasuring and narrowing the compositionality gap in language models.\\narXiv preprint arXiv:2210.03350, 2022.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J.\\nExploring the limits of transfer learning with a unified text-to-text transformer.\\nJournal of Machine Learning Research, 2020. URL https://jmlr.org/papers/v21/20-074.html.\"}"}
{"id": "shi23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models Can Be Easily Distracted by Irrelevant Context\\n\\nRavichander, A., Gardner, M., and Marasovi\u0107, A. Condaqa: A contrastive reading comprehension dataset for reasoning about negation. arXiv preprint arXiv:2211.00295, 2022.\\n\\nSanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\\n\\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and Wang, S. I. Natural language to code translation with execution. In EMNLP, 2022a.\\n\\nShi, F., Suzgun, M., Freitag, M., Wang, X., Srivats, S., Vosoughi, S., Chung, H. W., Tay, Y., Ruder, S., Zhou, D., Das, D., and Wei, J. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057, 2022b. URL https://arxiv.org/pdf/2210.03057.\\n\\nShi, H., Mao, J., Xiao, T., Jiang, Y., and Sun, J. Learning visually-grounded semantics from contrastive adversarial samples. In Proceedings of the 27th International Conference on Computational Linguistics, pp. 3715\u20133727, 2018.\\n\\nSinha, K., Sodhani, S., Dong, J., Pineau, J., and Hamilton, W. L. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4506\u20134515, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1458. URL https://aclanthology.org/D19-1458.\\n\\nSuzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n\\nTafjord, O., Mishra, B. D., and Clark, P. Proofwriter: Generating implications, proofs, and abductive statements over natural language. arXiv preprint arXiv:2012.13048, 2020.\\n\\nWang, B., Xu, C., Wang, S., Gan, Z., Cheng, Y., Gao, J., Awadallah, A. H., and Li, B. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. arXiv preprint arXiv:2111.02840, 2021.\\n\\nWang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L., and Sun, H. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022a.\\n\\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.\\n\\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022c.\\n\\nWebson, A. and Pavlick, E. Do prompt-based models really understand the meaning of their prompts? arXiv preprint arXiv:2109.01247, 2021.\\n\\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\\n\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In NeurIPS, 2022. URL https://openreview.net/pdf?id=_VjQlMeSB_J.\\n\\nWeston, J., Bordes, A., Chopra, S., Rush, A. M., Van Merri\u00ebnboer, B., Joulin, A., and Mikolov, T. Towards AI-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.\\n\\nZhou, D., Sch\u00e4rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. URL https://arxiv.org/pdf/2205.10625.\"}"}
