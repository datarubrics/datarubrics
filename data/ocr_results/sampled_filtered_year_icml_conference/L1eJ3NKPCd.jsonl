{"id": "L1eJ3NKPCd", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nepochs. As is common, we do not use a positional embedding, since the architecture is not permutation invariant.\\n\\nTable 1:\\n\\nHidden dimension | Layers |\\n-----------------|--------|\\n                |        |\\n256             | 512    |\\n4               |        |\\n\\n| 1 | 22.5 | 46.0 |\\n| 2 | 33.4 | 69.1 |\\n\\nTable 2:\\n\\nHidden dimension | Layers |\\n-----------------|--------|\\n                |        |\\n120             | 256    |\\n256             | 512    |\\n512             | 1024   |\\n\\n| 1 | 11.64 | 85.42 | 84.83 | 86.61 |\\n| 2 | 11.33 | 86.25 | 88.29 | 88.28 |\\n| 4 | 11.44 | 88.52 | 53.11 | 11.33 |\\n\\nTable 3:\\n\\nHidden layer dimension | Layers |\\n-----------------------|--------|\\n120                    | 256    |\\n256                    | 512    |\\n512                    | 1024   |\\n\\n| 1 | 16.2  | 36.2  | 99.9  | 99.9  |\\n| 2 | 60.3  | 99.3  | 99.9  | 99.8  |\\n| 4 | 18.7  | 100.0 | 100.0 | 9.9   |\\n\\nWe train an LSTM on 100 compositions of in-order compositions with a modified prompt format that does not require capturing long-range dependencies. While the accuracies improve significantly, the LSTM still fails to compositionally generalize.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nB.3. Attention Masks\\n\\nDetailed setup. We train a 1-layer Transformer on a composition of 50 random in-order compositions of 5 bijections in the step-by-step prompt format. We visualize the attention masks for a fixed sequence of task tokens, averaged over 1000 different data tokens in Fig. 7 (right). We found the attention masks to be identical across different choices of the task tokens. Each row corresponds to a causal attention mask for a single token and sums up to 1. At any given row, the attention is over two elements\u2014the task token and the intermediate output of the composition. The five contiguous blocks along the columns correspond to the five steps of composition. These preliminary results indicate that it is possible to build a complete mechanistic understanding of attention for compositional tasks (see also Sec. C).\\n\\nB.4. Probing the layers in Transformers of different sizes\\n\\nFigure 14: We use a linear probe to study the accuracy at different layers on Transformers of different sizes. Most architectures see an increasing in accuracy in the latter half of the Transformer. The increase in accuracy is more gradual for Transformers with more layers. The accuracy increases sharply after an attention layer across all architectures.\\n\\nIn this section, we consider an experimental setup that is identical to the linear probe experiments in Fig. 7. We compute the probe accuracies for Transformers with different number of layers in Fig. 14. Across all models, we observe that accuracy increases in the last few layers. Furthermore, we also observe a sharp increase in accuracy right after the MLPs in the last few layers of the transformer.\\n\\nWe saw in Fig. 7 (right) that the attention masks for a 1-layer model seem to select an input and a task token to operate on at every step of the composition. We hence believe that attention has a huge role in compositionality and propose the following hypothesis: The probe accuracy after some MLPs see a sharp increase in accuracy because the attention layers play a critical role in selecting the right inputs to pass to the MLP. Specifically, unlike the 1-layer model, we suspect functions are now distributed across the model layers instead of being localized in the first MLP layer. Consequently, similar to the 1-layer model, attention heads at different layers will infer if the relevant functions implemented in MLP layers in that block are part of the prompt; if so, they transfer the input data through said function.\\n\\nB.5. Another failure with the direct format with bijections\\n\\nIn Fig. 6 (Left) we show that Transformers do not learn to compose 5 bijections and only generalize to compositions in the training data. Fig. 15 augments this result and shows that a similar failure occurs even when we consider the composition of just two bijections. Hence the model may not compose some function in the direct prompt format and the step-by-step format with an autoregressive objective is far more amenable to compositions.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nFigure 15: Transformers fail to generalize to compositions of even 2 bijections, when trained with the direct prompt format. The curve depicts the accuracy over all 625 in-order compositions of two bijections (25 choices for each bijection) when trained on different subsets of in-order compositions. The model is trained with direct composition. Even if we train on 500 such compositions, the model fails to generalize to the remaining 125 compositions. This is additional evidence that the model is incapable of composing bijections through direct composition.\\n\\nB.6. Additional experiments with training data from random and base\\n\\nIn this section, we conduct a collection of analyses for a model trained on in-order compositions of 5 bijections in the step-by-step prompt format. We perform the following experiments: (1) change the number of random functions in the training data (Fig. 16); (2) compare how base and random generalize to other in-order compositions (Fig. 17); (3) limit the maximum number of compositions in the training data and evaluate compositional generalization (Fig. 18); (4) look at alternate evaluation metrics (Fig. 19); and (5) test if the compositions are systematic (Hupkes et al., 2020) (Fig. 20).\\n\\nFigure 16: Training with different numbers of random functions. We train on a different number of random functions ranging from 5-70 in steps of 5. These plots are the accuracies averaged over all in-order compositions of 5 bijections over the course of training.\\n\\nFigure 17: How do different training datasets generalize to compositions of many and few functions? This is a fine-grained version of Fig. 4a. Model trained on 50 random compositions generalizes poorly to compositions of small numbers of functions while a model trained on the base generalizes poorly to compositions of 4 or 5 functions.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 18: Limiting maximum number of compositions in the training data.\\nThe figure plots the accuracy on all in-order compositions against the number of training iterations. Each sub-plot considers compositions of size exactly 2, 3, 4, 5, respectively in the training data. The model is able to generalize to most in-order compositions only if the training data consists of compositions of size at least 3 (bottom-right).\\n\\nFigure 19: Evaluation metric.\\nWe consider 3 different metrics for evaluating the models. The left column considers the average accuracy when the model generates the correct composition. The choice of metric doesn't change qualitative trends. Each sub-plot considers compositions of only size 2, 3, 4, 5, respectively. In each plot, we vary the number of such functions that are present in the training data. One exception is when we train on compositions of size 2. In this case, the guided generation accuracy is high, but the free generation accuracy is not.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "L1eJ3NKPCd", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Experimental Details\\n\\nA.1. Training methodology\\n\\nWe use nanoGPT as the Transformer architecture in all our experiments. The core Transformer block is a LayerNorm, a causal attention block, followed by another layer-norm and a 2-layer multi-layer perceptron (MLP). The Transformer block has two residual connections.\\n\\nThe input tokens are converted to one-hot vectors before being passed through the model. We do not use dropout or biases in the LayerNorm layers. We use weight-tying, i.e., the input and the output embedding layers share weights. Finally, we make use of mixed-precision (bf16 in torch) to speed-up training.\\n\\nLoss and Optimizer\\n\\nModels are trained using an autoregressive objective to predict the next token using the cross-entropy loss. Specifically, assume a sequence of tokens of \\\\( t \\\\) tokens denoted by \\\\( x_1: t \\\\). Let \\\\( p_w(y|x_1:t) \\\\) denote the probability distribution over the next token as predicted by a model with weights \\\\( w \\\\). For a sequence \\\\( x_1:T \\\\) of length \\\\( T \\\\), the autoregressive objective is\\n\\n\\\\[\\nL(w) = -\\\\sum_{t=1}^{T-1} \\\\log p_w(y = x_{t+1} | x_1:t).\\n\\\\]\\n\\nTraining is performed for 100 epochs with a cosine-annealed scheduled with warmup. We use an initial learning rate of \\\\( 3 \\\\times 10^{-4} \\\\) annealed eventually to \\\\( 6 \\\\times 10^{-5} \\\\). We use AdamW as the optimizer (\\\\( \\\\beta_1 = 0.9 \\\\) and \\\\( \\\\beta_2 = 0.95 \\\\)) with a weight decay of \\\\( 10^{-3} \\\\) and a batch-size of 512. We also make use of gradient clipping with a magnitude of 1.\\n\\nA.2. Data generating process\\n\\nData and task tokens. Both data and task tokens are converted to one-hot vectors before being fed to the Transformer. The set of data tokens is denoted by \\\\( X_d \\\\) and the size of the vocabulary, \\\\( |X_d| \\\\), is 10 in all our experiments. The data tokens in the input \\\\( x_d \\\\in X_d \\\\) is a sequence of 6 tokens and is the input to the function composition. The 6 tokens are sampled uniformly at random from \\\\( X_d \\\\) with replacement.\\n\\nThere are two sets of functions considered in this work. The set of functions \\\\( F_b \\\\) (which we refer to as bijections) applies a lookup table in an element-wise fashion to each of the 6 tokens in \\\\( x_d \\\\). The set of functions in \\\\( F_p \\\\) permute the 6 tokens in \\\\( x_d \\\\). The family of functions in \\\\( F_b \\\\) and \\\\( F_p \\\\) are described in Fig. 10. Each function from \\\\( F_p \\\\) and \\\\( X_b \\\\) has its own task token in \\\\( X_F \\\\).\\n\\nThe input starts with a sequence of \\\\( L \\\\) task tokens \\\\( x_f \\\\in X_L \\\\). The number of compositions is generally \\\\( L = 5 \\\\), but in a few experiments like Figs. 15, 6 (Right), \\\\( L = 2 \\\\).\\n\\nSampling task tokens\\n\\nThe task tokens can be sampled such that they satisfy certain properties. For example, let us consider the composition of two functions\u2014one from the set \\\\( F_1 \\\\subset F_p \\\\) and another from \\\\( F_2 \\\\subset F_b \\\\) (which is the setting in Fig. 6 (Right)). We can restrict the training data to compositions from the set \\\\( F_2 \\\\circ F_1 \\\\) which are in-order compositions.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nAlternatively, we can also choose to include out-of-order compositions, which include compositions from $F_1 \\\\circ F_1$, $F_2 \\\\circ F_2$ and $F_1 \\\\circ F_2$. In Fig. 6 (Right), we restrict our training and evaluation to in-order compositions of functions and we observe that training on a subset of the elements from $F_2 \\\\circ F_1$ suffices to compositionally generalize all functions in the set.\\n\\nTwo other commonly used subsets of functions are base and random. Consider $F_1, F_2, \\\\ldots, F_5 \\\\subset F_b$. The set random considers $k$ functions from the set $F_5 \\\\circ F_4 \\\\circ \\\\cdots \\\\circ F_1$ which are drawn uniformly at random.\\n\\nbase is used to test if the compositionality is seen when the Transformer is trained on the individual functions from $F_i$ for all $i \\\\in [5]$. In the training data, all compositions have 4 of the 5 functions to be the identity function $I$, i.e. it considers compositions of the form $I \\\\circ I \\\\circ F_3 \\\\circ I \\\\circ I$ or $I \\\\circ F_4 \\\\circ \\\\cdots \\\\circ I$. There are a total of $1 + \\\\sum_{i=1}^{5} F_i$ such functions; the 1 is when all 5 functions in the composition are identity. The model is never trained on the composition of two or more functions, and at least compositions of 3 functions are necessary to generalize to all in-order compositions Fig. 18.\\n\\nGenerating a sequence of tokens: A sequence starts with a sequence of two task tokens $x_f = [x_F_1, x_F_2]$ followed by a sequence of data tokens $x_d$. The sequence can either be presented in: (i) The step-by-step format, where the intermediate outputs are also included in the sequence; e.g., the sequence in the step-by-step format would look like $[x_F_1, x_F_2, x_d, F_1(x_d), F_2(F_1(x_d))]$ (see Fig. 11a) or (ii) The direct format, which does not include the intermediate outputs of the composition in the sequence and an example of such a sequence is $[x_F_1, x_F_2, x_d, F_2(F_1(x))]$ (see Fig. 11b).\\n\\nThe step-by-step and direct formats are also discussed in Fig. 3. The training data consists of 100,000 sequences for all experiments in one of the two formats.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nWe total number of functions, whichever was lower) for each cell which can be identified by the displacement and number of compositions; we then compute the accuracy averaged over those functions to populate the cell. The accuracy of a completion is calculated by averaging the accuracy of the last six tokens. We see that qualitative trends do not change when we use different metrics Fig. 19.\\n\\nComputing linear probe accuracy\\nWe consider the outputs after every attention block and every MLP block (including the residual stream in both cases). We then pass these outputs through the final embedding layer and a Softmax layer to get predictions over the next token. We use these predictions to compute the accuracy at that layer. The accuracy is averaged over 1000 different input data tokens and for 200 different compositions of functions.\\n\\nB. Additional Experiments\\nB.1. Sweeping hyper-parameters of the Transformer\\nWe vary the number of layers in the Transformer and train on direct composition in a setup identical to Fig. 6 (Right).\\n\\nWe vary the number of layers, the number of attention heads, and the embedding dimension of the nanoGPT model in Fig. 13. We consider a setup identical to Fig. 4; all models are trained on 50 random in-order compositions of 5 bijections. We report accuracy averaged over all 3125 in-order compositions.\\n\\nWe make the following observations. (1) Most surprisingly, the accuracy reduces as the number of layers become huge for this compositional task; we expect that this is due to issues with optimization of a large depth model. (2) The accuracy does not change with the number of attention heads for a 1-layer Transformer. (3) The accuracy increases as we increase the embedding dimension and the model under fits the training data when the embedding dimension is too small.\\n\\nFigure 12: Transformers requires at least 2-3 layers for compositional generalization with the direct prompt format.\\n\\nWe see compositionality in Transformers even if we change the number of layers and attention heads. Compositionality is seen even in a 1-layer Transformer when trained with the step-by-step prompt format on 50 in-order compositions of bijections. However the ability to compose degrades as we increase the number of layers in the Transformer.\\n\\nB.2. LSTMs do not learn to compose\\nWe report results on autoregressively trained LSTMs using the direct prompt format from Table 1 and the step-by-step prompt format in Table 3. LSTMs fail to generalize outside of the training data while Transformers generalize compositionally in both these scenarios. This points to an inductive bias that helps Transformers trained with an autoregressive objective generalize. Specifically, our mechanistic evaluation in Sec. 4.4 shows this is likely attributable to the use of Attention. The LSTMs are trained using the same data using the autoregressive objective defined in Appendix A. We use the AdamW optimizer with learning rate equal to 3e-4 ($\\\\beta_1 = 0.9$ and $\\\\beta_2 = 0.95$), batch size of 512 and weight decay of 1e-4 for 150 epochs.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We denote the output of the first Transformer block by $Z_1$. Using the above, the output of the first attention layer added to the residual stream is\\n\\n$$Z_2 = \\\\text{Attn}(Z_1) + Z_1.$$ \\n\\nThe second block uses the output of the first Transformer block to compute the second step of the composition. We start similarly by computing the query and value matrices of the attention layer in the first Transformer block are $W_1, B_1, K_1, Q_1$ and $P_1 = W_2, B_2, K_2, Q_2$. Note that $Z_1$ and $Z_2$ are identical to $Z_1$ and $Z_2$.\\n\\nStep 2: Computing the output of the second block.\\n\\n$$P_2 = \\\\text{M}(Z_2, P_1, F) = P_1 \\\\odot F(Z_2),$$\\n\\nwhere $F$ is a non-linear function such as ReLU. This is done for each layer, i.e., $P_3, \\\\ldots, P_T$. The output of the second block is then given by $P_T$. \\n\\nCompositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\"}"}
{"id": "L1eJ3NKPCd", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Using the above, we can compute the output of the attention layer in the second Transformer block which evaluates to \\n\\\\[\\n\\\\text{Attn}_2(Z_{B_1}) + Z_{B_1} = (K_2 Z_{B_1}) (M \\\\odot Z^T_{B_1} Q_2 Z_{B_1}) + Z_{B_1} = (K_2 Z_{B_1}) (M \\\\odot P_T P_2) + Z_{B_1} =\\\\]\\n\\\\[\\n\\\\begin{bmatrix}\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\n1 & 0 & 0 & 0 \\\\\\\\\\n0 & 1 & 0 & 0 \\\\\\\\\\n0 & 0 & 1 & 1 \\\\\\\\\\n0 & 0 & 0 & 1 \\\\\\\\\\n\\\\end{bmatrix}\\n+ Z_{B_1} = 3 x F_1 x F_2 x F_3 0 0 0 0.\\n\\\\]\\n\\nThe attention layer uses sub-matrix \\\\(P_2\\\\) of the position encodings to copy the second task token to the data token. We repeat the calculations in Equation (0), with \\\\(W_{21}\\\\) and \\\\(W_{22}\\\\) which yields \\n\\\\[\\n\\\\text{Block}_2(\\\\text{Block}_1(Z)) = W_{22} \\\\text{ReLU}(W_{21}(\\\\text{Attn}_2(Z_{B_1}) + Z_{B_1})) + (\\\\text{Attn}_2(Z_{B_1}) + Z_{B_1}) = 4 x F_1 x F_2 x F_3 F_2 \\\\circ F_1 (x_d) + x F_3 p_1 p_2 p_3 p_4.\\n\\\\]\\n\\nStep 3: Computing the output of the final Transformer block. Unsurprisingly, the calculations for the last Transformer block are almost identical. The query matrix is \\\\(Z^T_{B_2} Q_3 Z_{B_2} = P_T P_3\\\\) and the value matrix is \\\\(K_3 Z_{B_2} = 1 3\\\\)\\n\\n\\\\[\\n\\\\begin{bmatrix}\\n\\\\end{bmatrix}\\n\\\\begin{bmatrix}\\n1 & 0 & 0 & 0 \\\\\\\\\\n0 & 1 & 0 & 0 \\\\\\\\\\n0 & 0 & 1 & 1 \\\\\\\\\\n0 & 0 & 0 & 1 \\\\\\\\\\n\\\\end{bmatrix}\\n+ Z_{B_2} = 4 x F_1 x F_2 x F_3 F_3 \\\\circ F_2 \\\\circ F_1 (x_d) + x F_3 p_1 p_2 p_3 p_4.\\n\\\\]\\n\\nPassing the output of \\\\(\\\\text{Attn}_2(Z_{B_2})\\\\) through the last MLP, yields the output of the Transformer, which is \\n\\\\[\\n\\\\text{Tr}(Z) = \\\\text{Block}_3(\\\\text{Block}_2(\\\\text{Block}_1(Z))) = W_{32} \\\\text{ReLU}(W_{32}(\\\\text{Attn}_3(Z_{B_2}) + Z_{B_2})) + (\\\\text{Attn}_3(Z_{B_2}) + Z_{B_2}) = 4 x F_1 x F_2 x F_3 F_3 \\\\circ F_2 \\\\circ F_1 (x_d) + x F_3 p_1 p_2 p_3 p_4.\\n\\\\]\\n\\nHence, the output of the Transformer is a composition of the three functions \\\\(F_1\\\\), \\\\(F_2\\\\) and \\\\(F_3\\\\) applied to token \\\\(x_d\\\\).\"}"}
{"id": "L1eJ3NKPCd", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The depth of the composition\\n\\nWe consider trained models from Fig. 4a and analyze the accuracy of each of the 20 functions (atomic capabilities) when averaged all instances in which it was used compositionally. We breakdown the results to see if certain functions are more accurate when used in compositions compared to others and find that models seem to learn all functions equally well.\\n\\nB.7. Token embeddings\\n\\nWe study the token embeddings of the Transformer models and observe that they are similar for models with different number of layers and attention heads (see Fig. 21). We notice a block diagonal structure that separates task tokens from the data tokens. We also observe another block diagonal structure within the task tokens which occurs when we train only on in-order compositions.\\n\\nWe plot the inner product between all pairs of word embeddings of the tokens. The task tokens are orthogonal to the set of input tokens. Different functions in the same level, i.e. $\\\\{F(l)^i\\\\}_{i=1}^N$ for a fixed $l$, form a block-diagonal in this matrix. We observe similar word embeddings in Transformers of different sizes.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nC. Analysis of Step-by-step and Direct Prompt Formats\\n\\nC.1. Transformers for the step-by-step prompt format\\n\\nWe prove that there exists Transformers that can compositionally generalize in the step-by-step prompt format. Such a constructive proof, similar to (V on Oswald et al., 2023; Ahn et al., 2023; Weiss et al., 2021; Li et al., 2023c), can be used to generate plausible mechanistic hypothesis by highlighting the role of the attention and MLP layers. While the universal approximation theorem suggests that any function can be represented by a wide enough multi-layer perceptron (MLP), the construction suggests that Transformers can represent the same function efficiently.\\n\\nDescription of the data.\\n\\nWe will operate with a simplified prompt format where a composition of three functions is to be applied to a single input token. The construction can be generalized to compositions of more functions or to multiple input tokens. The input prompt \\\\[ x F_1, x F_2, x F_3, x d \\\\] has three task tokens and a single data token, and the desired output for this prompt is \\\\[ F_1(x d), F_2 \u25e6 F_1(x d), F_3 \u25e6 F_2 \u25e6 F_1(x d) \\\\].\\n\\nThe position encodings \\\\[ P = p_1 p_2 \u00b7\u00b7\u00b7 p_6 \\\\] are learnable parameters and have dimension \\\\( d_p \\\\), i.e., \\\\( P \u2208 \\\\mathbb{R}^{d_p \u00d7 6} \\\\). The number of input tokens is \\\\( d_v \\\\) and the number of task tokens is \\\\( d_f \\\\). Both input tokens \\\\( x_d \\\\) and task tokens \\\\( x F_1 \\\\) are embedded as a one-hot vector in \\\\( \\\\mathbb{R}^{d_x} \\\\) where \\\\( d_x = d_v + d_f \\\\). The first \\\\( d_v \\\\) dimensions are used to embed the data tokens and the last \\\\( d_f \\\\) dimensions embed the task token. Henceforth, both \\\\( x_d \\\\) and \\\\( x F_1 \\\\) refer to the corresponding one-hot vectors in \\\\( \\\\mathbb{R}^{d_x} \\\\). For convenience, we define \\\\( d = d_x + d_p \\\\). Tying this back to to section 3, observe that \\\\( |X_d| = d_v \\\\) and \\\\( |X_f| = d_f \\\\). We denote the input to the model using \\\\( Z \\\\), which includes the token embedding and position encoding. Specifically, we have \\\\( Z = x F_1 x F_2 x F_3 x F_1(x d) P_1 P_2 P_3 P_4 P_5 P_6 \\\\), i.e., \\\\( Z \u2208 \\\\mathbb{R}^{d \u00d7 6} \\\\). We assume that the position encoding is concatenated to the token embedding as opposed to added to it.\\n\\nMatrix notation.\\n\\nWe use \\\\( 1_{d_v} \\\\) to denote a one-hot vector in the space \\\\( \\\\mathbb{R}^{d_v} \\\\), i.e., it excludes dimensions for the task token. On the other hand, \\\\( x_d \\\\) denotes a one-hot vector in \\\\( \\\\mathbb{R}^{d_x} \\\\). We use \\\\( I_{n \u00d7 n} \\\\) to denote an identity matrix of size \\\\( n \u00d7 n \\\\), \\\\( 1_{m \u00d7 n} \\\\) and \\\\( 0_{m \u00d7 n} \\\\) to denote matrices of 1s and 0s of size \\\\( m \u00d7 n \\\\), and \\\\( 1_n \\\\) and \\\\( 0_n \\\\) to denote matrices of size \\\\( n \u00d7 1 \\\\).\\n\\nDescription of the architecture.\\n\\nBefore describing the Transformer architecture, we first define the attention and MLP layers. We use a simplified parameterization of linear attention (Ahn et al., 2023) with weights \\\\( Q \\\\) and \\\\( K \\\\). The MLP contains two fully connected layers with a ReLU non-linearity parameterized by the weights \\\\( W_1 \\\\) and \\\\( W_2 \\\\). The attention and MLP layers are functions of \\\\( Z \u2208 \\\\mathbb{R}^{d \u00d7 6} \\\\) and are defined as:\\n\\n\\\\[\\n\\\\text{Attn}_{Q,K}(Z) = (KZ)(M\u2299Z^TQZ),\\n\\\\]\\n\\nand\\n\\n\\\\[\\n\\\\text{MLP}_{W_1,W_2}(Z) = W_2 \\\\text{ReLU}(W_1Z),\\n\\\\]\\n\\nwhere \\\\( Q, K \u2208 \\\\mathbb{R}^{d \u00d7 d}, W_1 \u2208 \\\\mathbb{R}^{d \u00d7 (d_f d_v)}, W_2 \u2208 \\\\mathbb{R}^{(d_f d_v) \u00d7 d}. \\\\)\\n\\nThe matrix \\\\( M \u2208 \\\\mathbb{R}^{6 \u00d7 6} \\\\) enforces causal attention and restricts the attention to inputs from previous time-steps, i.e.,\\n\\n\\\\[\\nM = \\\\begin{bmatrix}\\n1 & 1 & 1 & \\\\cdots & 1 \\\\\\\\\\n0 & 1 & 1 & \\\\cdots & 1 \\\\\\\\\\n\\\\vdots & \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\n0 & 0 & 0 & \\\\cdots & 1 \\\\\\\\\\n\\\\end{bmatrix}.\\n\\\\]\\n\\nWe consider a 1-layer Transformer with an attention layer followed by an MLP layer. We omit layer-norm to simplify the proofs. The function computed by the Transformer is \\\\( \\\\text{Tr}_{Q,K,W_1,W_2}(Z) = \\\\text{MLP}(\\\\text{Attn}(Z) + Z) + \\\\text{Attn}(Z) + Z \\\\).\\n\\nHenceforth, we omit the subscripts of \\\\( \\\\text{Attn}, \\\\text{MLP} \\\\) and \\\\( \\\\text{Tr} \\\\) for brevity. We include a residual connection after both the attention and MLP layers which mirrors a typical Transformer architecture (Vaswani et al., 2017).\\n\\nThe output of the Transformer is passed through an unembedding matrix \\\\( W_e \\\\) followed by a Softmax layer to obtain a probability distribution over the next token denoted by \\\\( P(Y|Z) = \\\\text{Softmax}(W_e \\\\text{Tr}(Z)) \\\\).\"}"}
{"id": "L1eJ3NKPCd", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Theorem C.1. There exists weights $P, Q, K, W_1, W_2$ and position encodings $P$ such that an Autoregressive Transformer can compositionally generalize to any prompt $[x_{F_1}, x_{F_2}, x_{F_3}, x_d]$. The values of the weights satisfy $P^T P = I_{3 \\\\times 3}$, $Q = 0_{d \\\\times d}$, $K = \\\\begin{bmatrix} 0_{d_v \\\\times d_v} & 0_{d_f \\\\times d_v} & \\\\cdots & 0_{d_p \\\\times d_v} \\\\\\\\ 0_{d_f \\\\times d_v} & I_{d_f \\\\times d_f} & \\\\cdots & 0_{d_p \\\\times d_f} \\\\\\\\ \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ 0_{d_p \\\\times d_v} & 0_{d_p \\\\times d_f} & \\\\cdots & I_{d_p \\\\times d_p} \\\\end{bmatrix}$, $W_1 = \\\\begin{bmatrix} 1^T x_1 \\\\\\\\ 1^T x_2 \\\\\\\\ 1^T x_v \\\\\\\\ \\\\vdots \\\\\\\\ 1^T x_d \\\\\\\\ 1^T x_1 \\\\\\\\ 1^T x_2 \\\\\\\\ 1^T x_v \\\\\\\\ \\\\vdots \\\\\\\\ 1^T x_d \\\\\\\\ 1^T d_v \\\\\\\\ \\\\vdots \\\\\\\\ 1^T d_v \\\\\\\\ 1^T d_p \\\\\\\\ \\\\vdots \\\\\\\\ 1^T d_p \\\\end{bmatrix}$.\\n\\nProof. See Appendix C.4.\\n\\nThe construction uses the attention layer to aggregate the task token and data token, i.e., attention selects the relevant task token. The query vector of the attention selects the right task using the position encoding. The first layer of the MLP projects the summation of the task and data tokens (present in orthogonal spaces) onto the Cartesian product of the set of task and data tokens. The second layer computes the function and acts similar to a lookup table (Geva et al., 2022).\\n\\nThe construction requires the output of the first fully-connected layer has size at least $d_f d_v$ in order to encode the task and input tokens. In our experiments, we set $d_v = 10$ and $d_f = 21$ and hence the number of hidden units must be at least $210$. In practice, we require at least 500 hidden units (see Fig. 22), which is not too far from our estimate. We conjecture that the additional hidden units are helpful for optimization.\\n\\nC.2. Transformers for the direct prompt format\\n\\nWe also prove the existence of a Transformer for compositions of bijections in the direct prompt format. Unlike the step-by-step format, the direct prompt format lacks a \\\"scratchpad\\\" (Nye et al., 2021) for the intermediates outputs of the composition. In our construction, we use $K = 3$ Transformer blocks to compute the composition of $K$ functions; the output of the $k$-th block is the result of the $k$-th step of the composition.\\n\\nDescription of the data. We consider the composition of 3 functions with an input prompt denoted by $[x_{F_1}, x_{F_2}, x_{F_3}, x_d]$. Unlike the step-by-step format, the output is just a single token $[F_3 \\\\circ F_2 \\\\circ F_1(x_d)]$. The position encodings are denoted by $P = [p_1, p_2, \\\\ldots, p_4]$ where $p_i = p^T i_1 p^T i_2 p^T i_3 \\\\cdots p^T i_d$ and $p_i \\\\in \\\\mathbb{R}^{d_p}$ and $p_{ij} \\\\in \\\\mathbb{R}^{d_p/3}$. The dimensions $d_x, d_v, d_f$ and $d_p$. \\n\\n![Figure 22: We see a sharp increase in accuracy as we increase the embedding dimension of the Transformer. The number of hidden units in the MLP of the Transformer is 4 times the size of the embedding dimension.](image-url)\"}"}
{"id": "L1eJ3NKPCd", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nWe use $\\\\bar{d}_p$ to replace $d_p^3$. The input to the model is $Z = \\\\begin{pmatrix} x_{F1} & x_{F2} & x_{F3} & x_{d_{p1}} & p_{11} & p_{12} & p_{13} & p_{14} & p_{21} & p_{22} & p_{23} & p_{24} & p_{31} & p_{32} & p_{33} & p_{34} \\\\end{pmatrix}$, where $Z \\\\in \\\\mathbb{R}^{d \\\\times 4}$.\\n\\nDescription of the architecture. Each Transformer block is defined similar to the step-by-step format, i.e.,\\n\\n$$\\\\text{Block}_i Q_i, K_i, W_{i1}, W_{i2}(Z) = \\\\text{MLP}_i(\\\\text{Attn}_i(Z) + Z) + (\\\\text{Attn}_i(Z) + Z),$$\\n\\nwhich we henceforth denote by $\\\\text{Block}_i(Z)$. Unlike the step-by-step format, the model is now composed of 3 blocks corresponding to the 3 steps of the compositional task the model is expected to solve, i.e., $\\\\text{Tr}(Z) = \\\\text{Block}_3(\\\\text{Block}_2(\\\\text{Block}_1(Z)))$.\\n\\nThis input is passed through a Softmax layer to predict a probability distribution over the next token, denoted by $P(Y|Z) = \\\\text{Softmax}(WeZ)$. \\n\\nTheorem C.2. There exist weights $P_i, Q_i, K_i, W_{i1}, W_{i2}$ for $i \\\\in [1, 3]$ and position encodings $P$ such that the a 3-layer Transformer can compositionally generalize to any prompt of the form $[x_{F1}, x_{F2}, x_{F3}, x_{d}]$. The values of the weights satisfy $Q_1 = \\\\begin{pmatrix} 0_{d \\\\times d} & 0_{d \\\\times \\\\bar{d}_p} & 0_{d \\\\times \\\\bar{d}_p} & 0_{d \\\\times \\\\bar{d}_p} \\\\end{pmatrix}$, $Q_2 = \\\\begin{pmatrix} 0_{d \\\\times d} & 0_{d \\\\times d} & 0_{d \\\\times \\\\bar{d}_p} & 0_{d \\\\times \\\\bar{d}_p} \\\\end{pmatrix}$, $Q_3 = \\\\begin{pmatrix} 0_{d \\\\times d} & 0_{d \\\\times d} & 0_{d \\\\times d} & 0_{d \\\\times \\\\bar{d}_p} \\\\end{pmatrix}$, $K_1 = \\\\begin{pmatrix} 0_{d \\\\times d} & 0_{d \\\\times d} & 0_{d \\\\times d} & 0_{d \\\\times \\\\bar{d}_p} \\\\end{pmatrix}$, $K_2 = K_1$, $K_3 = K_1$. $P_T_1 P_1 = \\\\begin{pmatrix} 1 0 0 1 \\\\ 0 1 0 0 \\\\ 0 0 1 0 \\\\ 1 0 0 1 \\\\end{pmatrix}$, $P_T_2 P_2 = \\\\begin{pmatrix} 1 0 0 0 \\\\ 0 1 0 1 \\\\ 0 0 1 0 \\\\ 0 1 0 1 \\\\end{pmatrix}$, $P_T_3 P_3 = \\\\begin{pmatrix} 1 0 0 0 \\\\ 0 1 0 0 \\\\ 0 0 1 1 \\\\ 0 0 1 1 \\\\end{pmatrix}$.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nFigure 8: Transformers trained on base and some random functions generalize first to a composition of more functions before it generalizes to a composition of few of them. Each line is the average accuracy over all compositions of \\\\( k \\\\) functions and each subplot is a Transformer trained on a different subset of functions. The base is trained on the individual functions and these Transformers learn to compose a smaller set of functions (more functions in composition are identity) before learning to compose many of them. The opposite is true when the model is trained on a random subset of 25 compositions of functions.\\n\\n4.5. Training dynamics\\n\\nWe would like to study how compositional capabilities evolve over the course of training (Okawa et al., 2023) and understand if Transformers learn functions \\\\( F_1 \\\\) and \\\\( F_2 \\\\) before learning compositions like \\\\( F_1 \\\\circ F_2 \\\\). In Fig. 8, we track the accuracy over the course of training to understand if compositions of fewer functions are learned before compositions of many functions. The setup for this figure is identical to Fig. 4a with the accuracy faceted by the number of function compositions. We find that the order in which functions are learned depends entirely on the training data. If the training data consists of base and very few in-order compositions, then a Transformer generalizes to fewer compositions (more identities) first before generalizing to compositions of multiple functions. On the other hand, if the model is trained on 25 random in-order compositions, then it is better at generalizing to more complex compositions of these functions; this trend is lost when we train on 50 random in-order compositions.\\n\\n5. Conclusion\\n\\nGiven several recent works focused on prediction or elicitation of capabilities in pretrained models, we ask whether the very motivation guiding these works is tractable: can we possibly characterize all capabilities of a model, specifically a Transformer, pretrained on a compositional data domain? To address this question, we proposed a synthetic, but well-defined, data domain and formalized the notion of a capability as representing a function defined over the domain. Breaking compositional generalization into two relevant scenarios (in-order vs. out-of-order), we showed that the compositional structure of the data forces a model to learn to compose at relatively minimal data diversity, which indicatively address our primary question: an appropriate prompt could make the model compose its capabilities, yielding an \u201cexplosion of capabilities\u201d. This can arguably make tractable analysis of capabilities in a pretrained model relatively difficult.\\n\\nImpact Statement\\n\\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.\\n\\nAcknowledgements\\n\\nRR thanks Kento Nishi, Gautam Reddy and Eric Bigelow for their discussions at the early stages of this project. RR thanks AWS AI, for their gift to Penn Engineering\u2019s ASSET Center for Trustworthy AI. ESL was partially supported by the National Science Foundation (IIS-2008151).\\n\\nAuthor Contributions\\n\\nESL and RR conceived the initial project direction and defined the problem setup with inputs from HT and MK. The experiments were led by RR with inputs from ESL, HT and MK. The writing of the introduction and related work was led by ESL with help from HT and RR. RR, ESL and HT extensively collaborated on the methods section. The results and appendix were led by RR. The expository figures were created by HT and RR. HT and RPD acted as advisors in the work.\\n\\nReferences\\n\\nAbid, A., Farooqi, M., and Zou, J. Persistent anti-muslim bias in large language models. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pp. 298\u2013306, 2021.\\n\\nAhn, K., Cheng, X., Daneshmand, H., and Sra, S. Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nAllen-Zhu, Z. and Li, Y. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316, 2023a.\\n\\nAllen-Zhu, Z. and Li, Y. Physics of language models: Part 3.2, knowledge manipulation. arXiv preprint arXiv:2309.14402, 2023b.\\n\\nAllen-Zhu, Z. and Li, Y. Physics of language models: Part 1, context-free grammar. arXiv preprint arXiv:2305.13673, 2023c.\\n\\nAndreas, J. Measuring compositionality in representation learning. arXiv preprint arXiv:1902.07181, 2019.\\n\\nArora, S. and Goyal, A. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\\n\\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n\\nBhattamishra, S., Ahuja, K., and Goyal, N. On the ability and limitations of transformers to recognize formal languages. arXiv preprint arXiv:2009.11264, 2020.\\n\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-lut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\\n\\nChan, A., Salganik, R., Markelius, A., Pang, C., Rajkumar, N., Krasheninnikov, D., Langosco, L., He, Z., Duan, Y., Carroll, M., et al. Harms from increasingly agentic algorithmic systems. arXiv preprint arXiv:2302.10329, 2023.\\n\\nChan, S., Santoro, A., Lampinen, A., Wang, J., Singh, A., Richemond, P., McClelland, J., and Hill, F. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878\u201318891, 2022.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\n\\nChughtai, B., Chan, L., and Nanda, N. A toy model of universality: Reverse engineering how networks learn group operations, may 2023. URL http://arxiv.org/abs/2302.3025, 2023.\\n\\nCsord\u00e1s, R., Irie, K., and Schmidhuber, J. The devil is in the detail: Simple tricks improve systematic generalization of transformers. arXiv preprint arXiv:2108.12284, 2021a.\\n\\nCsord\u00e1s, R., Irie, K., and Schmidhuber, J. The neural data router: Adaptive control flow in transformers improves systematic generalization. arXiv preprint arXiv:2110.07732, 2021b.\\n\\nCsord\u00e1s, R., Irie, K., and Schmidhuber, J. Ctl++: Evaluating generalization on never-seen compositional patterns of known functions, and compatibility of neural representations. arXiv preprint arXiv:2210.06350, 2022.\\n\\nFodor, J. A. The language of thought, volume 5. Harvard university press, 1975.\\n\\nFodor, J. A. and Lepore, E. The compositionality papers. Oxford University Press, 2002.\\n\\nFodor, J. A. and Pylyshyn, Z. W. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3\u201371, 1988.\\n\\nGanguli, D., Hernandez, D., Lovitt, L., Askell, A., Bai, Y., Chen, A., Conerly, T., Dassarma, N., Drain, D., Elhage, N., et al. Predictability and surprise in large generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pp. 1747\u20131764, 2022.\\n\\nGarg, S., Tsipras, D., Liang, P. S., and Valiant, G. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022.\\n\\nGarrido-Mu\u02dcnoz, I., Montejo-R\u00b4aez, A., Mart\u00b4\u0131nez-Santiago, F., and Ure\u02dcna-L\u00b4opez, L. A. A survey on bias in deep nlp. Applied Sciences, 11(7):3184, 2021.\\n\\nGehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462, 2020.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nGeva, M., Caciularu, A., Dar, G., Roit, P., Sadde, S., Shlain, M., Tamir, B., and Goldberg, Y.\\n\\nLm-debugger: An interactive tool for inspection and intervention in transformer-based language models. arXiv preprint arXiv:2204.12130, 2022.\\n\\nHendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.\\n\\nHenighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T. B., Dhariwal, P., Gray, S., et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.\\n\\nHernandez, D., Kaplan, J., Henighan, T., and McDannel, S. Scaling laws for transfer. arXiv preprint arXiv:2102.01293, 2021.\\n\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\\n\\nHonovich, O., Shaham, U., Bowman, S. R., and Levy, O. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782, 2022.\\n\\nHosseini, A., Vani, A., Bahdanau, D., Sordoni, A., and Courville, A. On the compositional generalization gap of in-context learning. arXiv preprint arXiv:2211.08473, 2022.\\n\\nHuang, P.-S., Zhang, H., Jiang, R., Stanforth, R., Welbl, J., Rae, J., Maini, V., Yogatama, D., and Kohli, P. Reducing sentiment bias in language models via counterfactual evaluation. arXiv preprint arXiv:1911.03064, 2019.\\n\\nHupkes, D., Singh, A., Korrel, K., Kruszewski, G., and Bruni, E. Learning compositionally through attentive guidance. arXiv preprint arXiv:1805.09657, 2018.\\n\\nHupkes, D., Dankers, V., Mul, M., and Bruni, E. Compositionality decomposed: How do neural networks generalise? Journal of Artificial Intelligence Research, 67:757\u2013795, 2020.\\n\\nJiang, L., Hwang, J. D., Bhagavatula, C., Le Bras, R., Liang, J., Dodge, J., Sakaguchi, K., Forbes, M., Borchardt, J., Gabriel, S., et al. Can machines learn morality? the delphi experiment. arXiv e-prints, pp. arXiv\u20132110, 2021.\\n\\nJones, A. L. Scaling scaling laws with board games. arXiv preprint arXiv:2104.03113, 2021.\\n\\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\\n\\nLake, B. and Baroni, M. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning, pp. 2873\u20132882. PMLR, 2018.\\n\\nLee, T., Yasunaga, M., Meng, C., Mai, Y., Park, J. S., Gupta, A., Zhang, Y., Narayanan, D., Teufel, H. B., Bellagente, M., et al. Holistic evaluation of text-to-image models. arXiv preprint arXiv:2311.04287, 2023.\\n\\nLepori, M. A., Serre, T., and Pavlick, E. Break it down: Evidence for structural compositionality in neural networks. arXiv preprint arXiv:2301.10884, 2023.\\n\\nLewis, M., Yu, Q., Merullo, J., and Pavlick, E. Does clip bind concepts? probing compositionality in large image models. arXiv preprint arXiv:2212.10537, 2022.\\n\\nLi, K., Hopkins, A. K., Bau, D., Vi\u00e9gas, F., Pfister, H., and Wattenberg, M. Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task, 2023a. Comment: ICLR 2023 oral (notable-top-5%): https://openreview.net/forum?id=DeG07TcZvT; code: https://github.com/likenneth/othello-world.\\n\\nLi, Y., Yosinski, J., Clune, J., Lipson, H., and Hopcroft, J. Convergent learning: Do different neural networks learn the same representations? arXiv preprint arXiv:1511.07543, 2015.\\n\\nLi, Y., Ildiz, M. E., Papailiopoulos, D., and Oymak, S. Transformers as algorithms: Generalization and implicit model selection in in-context learning. arXiv preprint arXiv:2301.07067, 2023b.\\n\\nLi, Y., Sreenivasan, K., Giannou, A., Papailiopoulos, D., and Oymak, S. Dissecting chain-of-thought: A study on compositional in-context learning of mlps. arXiv preprint arXiv:2305.18869, 2023c.\\n\\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\\n\\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.\\n\\nLi\u0161ka, A., Kruszewski, G., and Baroni, M. Memorize or generalize? searching for a compositional rnn in a haystack. arXiv preprint arXiv:1802.06467, 2018.\\n\\nLiu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022.\\n\\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "L1eJ3NKPCd", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nRahul Ramesh 1\u2020 Ekdeep Singh Lubana 2 3 4 Mikail Khona 5\u2020 Robert P. Dick 2 Hidenori Tanaka 3 4\\n\\nAbstract\\n\\nTransformers trained on huge text corpora exhibit a remarkable set of capabilities, e.g., performing basic arithmetic. Given the inherent compositional nature of language, one can expect the model to learn to compose these capabilities, potentially yielding a combinatorial explosion of what operations it can perform on an input. Motivated by the above, we train autoregressive Transformer models on a synthetic data-generating process that involves compositions of a set of well-defined monolithic capabilities. Through a series of extensive and systematic experiments on this data-generating process, we show that: (1) autoregressive Transformers can learn linear chains of compositions from small amounts of training data and generalize to exponentially or even combinatorially many functions; (2) generating intermediate outputs when composing functions is more effective for generalizing to new, unseen compositions than not generating any intermediate outputs; (3) biases in the order of the compositions in the training data result in Transformers that fail to compose some combinations of functions; and (4) the attention layers select which capability to apply while the feed-forward layers execute the selected capability. Code is available at https://github.com/rahul13ramesh/compositional_capabilities.\\n\\n1. Introduction\\n\\nLarge scale Transformers pretrained on huge text corpora have revolutionized machine learning in recent years (Radford et al., 2021). Due to an ever-increasing interest in adopting these models in our daily lives, evaluating and predicting their capabilities has become increasingly important (Bommasani et al., 2021; Ganguli et al., 2022; Shevlane et al., 2023; Rae et al., 2021; Hoffmann et al., 2022; Tay et al., 2022; Henighan et al., 2020; Hernandez et al., 2021; Sharma & Kaplan, 2020). Motivated by this, recent works have performed extensive empirical analyses to understand the possibilities and limitations of using these models in practical tasks of interest. For example, such works show large language models (LLMs) can generate coherent text completions based on a provided context, perform code generation and debugging, use online APIs and tools in an automated manner, and even solve multimodal problems such as image captioning (Wei et al., 2022a; Bubeck et al., 2023; Austin et al., 2021; Chen et al., 2021; Lee et al., 2023; Liang et al., 2022; Qin et al., 2023; Liu et al., 2023; Suzgun et al., 2022; Srivastava et al., 2022).\"}"}
{"id": "L1eJ3NKPCd", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While such benchmarking of pretrained models is extremely valuable, it often focuses on evaluating rather \u201cnarrow\u201d or \u201catomic\u201d capabilities; for example, the ability to identify whether a given passage of text is biased or toxic (Gehman et al., 2020; Liang et al., 2022). However, given the compositional nature of training data (such as language), a model could learn to compose its atomic capabilities and perform complex tasks that it was never explicitly trained for. This can lead to an underestimation of the capabilities of the model; vice versa, if the model does not learn to compose, we can be certain that benchmarking for atomic capabilities is sufficient to characterize the model.\\n\\nMotivated by the above, we analyze if a Transformer trained on a compositional data-generating process, without any special modifications to the usual training pipeline, can learn both relevant atomic capabilities and an ability to compose those capabilities. Bubeck et al. (2023) recently show that LLMs exhibit \u201csparks\u201d of such compositionality, e.g., generating text that merges content of varying styles or evaluate mathematical expressions through the application of a sequence of functions (Fig. 1). However, due to their black-box nature, it is unclear if an LLM actually learns to compose capabilities or merely memorizes relevant samples from its training data. Moreover, while interacting with an LLM, it can be difficult to guarantee that we are utilizing a prompt that will appropriately guide the model to use the capabilities we desire, let alone compose them.\\n\\nTo circumvent challenges faced with LLMs pretrained on real world data and focus on our specific motivation, \u201ccan an autoregressive Transformer trained on compositional data learn to compose its capabilities\u201d, we choose to limit the purview of this work to a well-defined synthetic domain. This is similar in spirit to recent works that utilize synthetic datasets generated using objects like first-order logic machines, context-free grammars, linear regressors, modular arithmetic, and even board games to establish and understand phenomenology of modern neural networks (Liu et al., 2022; Allen-Zhu & Li, 2023c;a;b; Garg et al., 2022; Li et al., 2023c; Saparov & He, 2022; Chan et al., 2022; Bhattamishra et al., 2020; Zhou et al., 2023; Nanda et al., 2023a;b; Li et al., 2023a; Lubana et al., 2023; Jones, 2021). The goal of such works, including ours, is to develop interpretable demonstrations and mechanistic hypotheses that enable a characterization of the target phenomenology in a controlled setting. Accordingly, we emphasize that we do not intend to develop novel protocols for improving Transformers\u2019 ability to compositionally generalize, but rather to demonstrate its existence and understand what drives it.\\n\\nOverall, we make the following contributions.\\n\\n\u2022 A minimal synthetic setup for characterizing Transformers\u2019 ability to compose. We propose a minimal setup involving linear chains of compositions of predefined functions $F$ (bijections and permutations) that operate on a string of arbitrary tokens (Section 3), which allows us to precisely study the ability of Transformers to compose functions. Motivated by instruction induction and tuning in LLMs (Honovich et al., 2022; Wei et al., 2021), we instantiate a notion of \u201ctask tokens\u201d which specify what functions are to be applied to the input string. This helps us avoid any ambiguity in task specification (Shah et al., 2022).\\n\\n\u2022 Transformers show explosion of capabilities. We characterize the ability of a Transformer trained on our proposed setup to compositionally generalize, i.e., to apply a composition of specific functions chosen from $F$, to an input string. We show that a Transformer, trained on very few compositions, can generalize to exponentially or even combinatorially many functions (Section 4.1)\u2014these functions are entirely \u201cout-of-distribution\u201d, i.e., the model never sees them in its training data and hence was not explicitly trained to learn them. Crucially, allowing the model to recursively process its intermediate outputs\u2014i.e., stepwise inference (Kojima et al., 2022; Wei et al., 2022b)\u2014significantly improves compositional generalization (Section 4.3 and appendix C.3).\\n\\n\u2022 Characterizing limitations and mechanisms of compositionality in a Transformer. We formalize a notion of \u201cdistance\u201d between the functions seen by the model during pretraining and the ones it is evaluated on, hence enabling a precise characterization of when the model struggles to compose (Section 4.2). As we show, the training data determines whether the Transformer generalizes to an exponential or combinatorial set of functions\u2014which we call in-order and and out-of-order generalization respectively. Furthermore, linear probing (Tenney et al., 2019; Li et al., 2023a), and an analysis of the attention maps suggests the following mechanism for solving our task: the attention layer selects the task token and the fully connected layers compute the function corresponding to it (Section 4.4). We also prove the existence of Transformers that can compositionally generalize to our task and analyze why stepwise inference helps with it (Appendix C). Our mechanistic analysis and theoretical construction align extremely well.\\n\\n2. Related Work\\n\\nCapabilities in a Transformer. Transformers pretrained on large-scale, web-crawled datasets have been shown to exhibit a slew of interesting capabilities, such as basic arithmetic, question answering, commonsense knowledge reasoning, stylistic transformation of a piece of text, and even multimodal reasoning (Radford et al., 2018; 2019; Brown et al., 2020; Bubeck et al., 2023; Wei et al., 2022a; 2021;\"}"}
{"id": "L1eJ3NKPCd", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nRae et al., 2021; Chowdhery et al., 2022; Austin et al., 2021; Chen et al., 2021; Bommasani et al., 2021). However, this generality can come at the cost of a model also learning capabilities that are undesirable (Bommasani et al., 2021; Tamkin et al., 2021; Chan et al., 2023), e.g., producing sensitive, biased, or toxic outputs (Weidinger et al., 2021; McGuffie & Newhouse, 2020; Garrido-Mu\u00f1oz et al., 2021; Lin et al., 2021; Jiang et al., 2021; Abid et al., 2021; Parrish et al., 2021; Xu et al., 2021; Huang et al., 2019; Sheng et al., 2019; Gehman et al., 2020; Xu et al., 2020; Tamkin et al., 2021). This has motivated several works focused on understanding capabilities of a pretrained model, including (i) predicting capabilities of a future model, e.g., via fitting power laws to data/model scaling results (Rae et al., 2021; Hoffmann et al., 2022; Hernandez et al., 2021; Sharma & Kaplan, 2020; Arora & Goyal, 2023) and (ii) eliciting capabilities of a given model, e.g., via identification of appropriate prompts or via step-wise inference protocols such as chain-of-thought, to understand what tasks a model can be reliably used for (Liang et al., 2022; Suzgun et al., 2022; Lee et al., 2023). However, we argue that measuring a language model's performance on benchmarks to identify the existence of a set of capabilities is bound to be insufficient for characterizing what tasks it can perform: given the compositional nature of data these models are trained on, it is possible that they learn to compose capabilities, hence learning to perform several more tasks than we explicitly train them on. In fact, with a related motivation, Yu et al. (2023) design a benchmark for evaluating a model's ability to combine its skills in a recent contemporary work.\\n\\nCompositionality in neural networks. The ability to compositionally reason has been touted as a cornerstone of human intelligence (Fodor & Lepore, 2002; Fodor & Pylyshyn, 1988; Fodor, 1975; Schulz et al., 2016). Accordingly, several works have studied the ability of a neural network to compositionally generalize, usually demonstrating a negative result, and correspondingly developing explicit strategies that help improve the model's ability to generalize (Li\u0161ka et al., 2018; Hupkes et al., 2018; Lake & Baroni, 2018; Csord\u00e1s et al., 2021b;a; 2022; Ontan\u00f3n et al., 2021; Lepori et al., 2023; Lewis et al., 2022; Yun et al., 2022; Okawa et al., 2023; Hosseini et al., 2022). Our work differs from prior literature in several ways. First, we do not intend to develop protocols for improving compositional generalization in a Transformer; instead, we show that Transformers can learn to compose its capabilities and perform tasks it was never explicitly trained on, with autoregressive training on tokens from a compositional data-generating process. To this end, we define a synthetic task that allows for perfect task specification and which avoids ambiguity from prompt misspecification. While similar to the compositional table lookup task used in prior work (Li\u0161ka et al., 2018; Csord\u00e1s et al., 2022), our task involves a much larger set of capabilities to train and test for (3125 or 4 million, depending on the setup, compared to 128 capabilities in prior work).\\n\\nSecond, we aim to understand the extent of compositional generalization in a Transformer trained on our proposed domain, i.e., what kind of compositions does the model fail to perform and when. We define a framework to precisely characterize these failures modes and use the popular linear probing protocol for understanding model internals to show the critical role of attention layers in enabling compositionality (Li et al., 2023a). Finally, we analyze the impact of step-wise inference protocols, wherein intermediate outputs generated by the model are recursively passed to it as inputs, and which has been used for solving several challenging benchmark tasks recently (Suzgun et al., 2022; Wei et al., 2022b).\\n\\nSimilar to our work, Li et al. (2023c) study step-wise inference in Transformers trained on synthetic data from a compositional data generating process. However, there are notable differences\u2014we show that Transformers compositionally generalize to combinatorially many new functions and carefully controlling the training data allows us to highlight the benefit of step-wise inference. Furthermore, Li et al. (2023b) study compositionality with prompts used for in-context learning (Garg et al., 2022), while our synthetic setup avoids ambiguity in specifying the compositions. Many other works that study whether Transformers can compositionally generalize (Csord\u00e1s et al., 2021a; Ontan\u00f3n et al., 2021), focus on compositionality within a single forward pass, i.e., the model is not allowed to recursively process its inputs. We find the use of intermediate outputs significantly simplifies the problem and, given its popularity in practical scenarios (Kojima et al., 2022; Wei et al., 2022b), our results serve as a demonstration that inference protocols that allow Transformers to recursively refine their outputs can lead to a wide range of capabilities, especially ones that we never explicitly train the model for. Finally, our work studies compositions of linear chains of functions such as $F_1 \\\\circ F_2 \\\\circ F_3$, but compositions can be defined over more complex structures like trees (Andreas, 2019) which include functions like $F_3(F_2(\\\\cdot), F_1(\\\\cdot))$. Murty et al. (2022) seek to characterize such tree-structured compositional derivations implemented by a Transformer, however, they only consider a single forward pass through the Transformer. In contrast to Murty et al. (2022) our goals are different, which is to understand properties of the training data that allow us to successfully compose functions.\\n\\n3. Formalizing capabilities and compositions\\n\\nAs noted by Hupkes et al. (2020), despite extensive work exploring compositionality in neural networks, the term is often used for several related concepts. To avoid ambiguity, we thus present a definition of a \u201ccompositional model\u201d that\"}"}
{"id": "L1eJ3NKPCd", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We define two ways to compose \\\\( L \\\\) and \\\\( M \\\\). Then, a model transforms the ability to compose. Let \\\\( F \\\\) denote the ordered set \\\\((1, 2, 3, 4)\\\\) (see Fig. 2). The notation partitions into equally sized subsets and tokens of functions that are allowed at the setup, we create a spurious correlation between a subset of functions with 1\u20133 digits in web-crawled data (Razeghi et al., 2022), which leads to an inability of the model to perform multiplication in higher order numbers. To model this in our setup, we create a spurious correlation between a subset of functions and the position of their identifiers in the input and output domain of a language model are generally the set defines a map between points from its input space to the same space. This is motivated by the fact that the input \\\\( x \\\\) functions are applied to. We refer to \\\\( M \\\\) data tokens \\\\( x \\\\) functions are allowed at the \\\\( F \\\\) setup, we create a spurious correlation between a subset of functions that a Transformer can implement by composing linear chains if, for any subset of functions \\\\( F \\\\) say a model \\\\( t \\\\) implements linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing \\\\( L \\\\) functions; however, this intends to understand the set of capabilities\u2014or the set of \\\\( F \\\\) functions are applied to. We refer to \\\\( K \\\\) as \\\\( d \\\\) and \\\\( k \\\\) possible function at position \\\\( l \\\\). Based on the above, we define two ways to compose \\\\( L \\\\) and \\\\( M \\\\). Then, a model transforms the ability to compose. Let \\\\( F \\\\) denote the ordered set \\\\((1, 2, 3, 4)\\\\) (see Fig. 2). The notation partitions into equally sized subsets and tokens of functions that are allowed at the setup, we create a spurious correlation between a subset of functions with 1\u20133 digits in web-crawled data (Razeghi et al., 2022), which leads to an inability of the model to perform multiplication in higher order numbers. To model this in our setup, we create a spurious correlation between a subset of functions and the position of their identifiers in the input and output domain of a language model are generally the set defines a map between points from its input space to the same space. This is motivated by the fact that the input \\\\( x \\\\) functions are applied to. We refer to \\\\( M \\\\) data tokens \\\\( x \\\\) functions are allowed at the \\\\( F \\\\) setup, we create a spurious correlation between a subset of functions that a Transformer can implement by composing linear chains if, for any subset of functions \\\\( F \\\\) say a model \\\\( t \\\\) implements linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work, we restrict our attention to the ability to define \\\\( K \\\\) functions\u2014that a Transformer can implement by composing linear chains of compositions which we formalize and correspondingly, describe them. In this work"}
{"id": "L1eJ3NKPCd", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We consider two formats for representing a sample with the final output of the function composition, while ways. They are followed by:\\n\\n(a). Direct prompting:\\n\\n(b). Step-by-step prompting:\\n\\nWe use nanoGPT (Appendix A), a Transformer with 12 layers with each Transformer block identical to the one in Vaswani et al. (2017). We use the same architecture across different data tokens (see Appendix A.2). The accuracy of a training, we evaluate whether the model possesses a capability corresponding to a set of composition of functions, by computing the accuracy of the model completion on 1000 random base functions. The set of functions contains task tokens corresponding to a random subset of four of the five positions are identity, totalling to overall 21 positions in the composition is not the identity function. For example, if we consider compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least four of the five positions are identity, totalling to overall 21 positions in the composition is not the identity function. For example, if we consider compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of functions where at least one of the four positions contains task tokens corresponding to a random subset of four of the five positions, the training set will contain compositions of"}
{"id": "L1eJ3NKPCd", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nvary the number of layers, attention heads, and embedding dimension in Appendix B.1.\\n\\n4.1. Combinatorial explosion and Exponential growth in capabilities\\n\\nDo Transformers only generalize to functions present in the training data or do they reflect compositional structure present in data? In Fig. 4, we train on data consisting of a small subset of in-order compositions of bijections $F_b$, in the step-by-step prompt format. We consider the composition of 5 functions in both Figs. 4a and 4b. Each position of the composition can be one of four choices, with the four choices at different positions being different in Fig. 4a and the same in Fig. 4b. In addition, any position can also be selected to be identity.\\n\\nWe find that Transformers can capture the compositional structure in data and generalize to exponential and combinatorial sets of functions in Figs. 4a and 4b, despite being trained on an extremely small subset of function compositions. For example, a Transformer trained on 30\u2013100 function compositions, generalizes to 3125 unseen compositions of these functions almost perfectly. In contrast, we note that LSTMs fail to compositionally generalize in this same setup (Appendix B.2), while Transformers with different numbers of layers and attention heads show compositional generalization (Appendix B.1). This indicates that the inductive bias of the architecture contributes to compositional generalization and any autoregressive model is not guaranteed to succeed.\\n\\nWe also observe that base\u2014which serves as a null model that only trains on the atomic capabilities (or functions)\u2014does not compositionally generalize. Overall, then, we note that compositional generalization occurs with the step-by-step prompt format, provided the right architecture and training data are used.\\n\\n4.2. In-order vs. Out-of-order generalization\\n\\nHow do biases in the training data influence a Transformer's ability to compose? Are Transformers capable of both in-order and out-of-order generalization or does it depend on the nature of training data? For the functions in Fig. 4a, the number of in-order compositions is $5^5 = 3125$ and the number of out-of-order compositions is a whopping $(21)^5 = 4084101$; essentially all of these functions are different from the ones seen in the training data. Like in Section 4.1, we only consider Transformers trained with the step-by-step prompt format on functions from the set of bijections $F_b$. In Fig. 5, we consider the training data to have functions from base, some in-order and some out-of-order compositions. We fail to see in-order or out-of-order generalization unless the data also includes in-order or out-of-order compositions respectively. However, a small number of in-order (10 of them) or out-of-order compositions (100 of them) in the training data results in in-order generalization or limited out-of-order generalization. All scenarios in Fig. 5 do not fully generalize to out-of-order compositions. This indicates that out-of-order compositions may require a lot more data compared to in-order compositions.\\n\\n4.3. Direct vs. step-by-step compositions\\n\\nBoth Sections 4.1 and 4.2 discuss experiments using the step-by-step prompt format, but do these results also hold for direct prompting? Fig. 6 (left) and Fig. 15 answer this in the negative. Specifically, in Fig. 6 (left), we consider a setup identical to Fig. 4a and train on a different number of random functions. Transformers fail to generalize to new in-order compositions with direct prompting when we consider compositions of bijections from $F_b$. We observe this failure even if we train on 2000 of the 3125 possible in-order compositions of functions, i.e., even if the data has high diversity. In contrast, in Fig. 4a, a mere 100 compositions in the step-by-step format suffices to generalize to all possible in-order compositions.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nFigure 5: The training data determines if a Transformer generalizes to an exponential (in-order generalization) or combinatorial (out-of-order generalization) number of functions. Each sub-plot uses a different subset of functions (from $F_b$) to generate the training data and we evaluate them on a combinatorial set of functions generated from 20+1 functions (one of them being identity). The x-axis varies the number of displacements and the y-axis varies the number of compositions\u2014equivalently the number of functions that are not identity. We make the following observations: (1) A Transformer trained on just 31 functions (top-middle) generalize to nearly exponentially many or $31^25$ compositions of functions. (2) All the above configurations do not generalize perfectly to the entire combinatorial set. They however partially generalize to nearly 4 million compositions of functions. The generalization is worse if we increase the number of compositions or displacements (see Fig. 2 for pictorial description of displacements).\\n\\nOn the other hand, we see in-order generalization if a Transformer is trained on a composition of a permutation function from $F_p$ and a bijection function from $F_b$.\\n\\nIn Fig. 6 (right), we train on compositions of two functions, where one position is one of 25 bijections, and the other is one of 25 permutations. We vary the number of compositions seen in the training data and find that 250 compositions in the training data are enough for the model to generalize to all 625 possible compositions of the two functions. We note that bijections and permutations operate on orthogonal features of the input: bijections operate on the value of the token while permutations operate on the position of the token. We speculate that this is important for compositional generalization in the direct prompt format.\\n\\nWhy is compositional generalization harder for direct prompts? (Appendix C.3)\\n\\nThe ability to run multiple forward passes through the model allows us tackle a richer class of problems (Merrill & Sabharwal, 2023). The step-by-step and direct prompt formats differ because the former only allows one forward pass. As a result, we expect for the direct prompt format to enable compositional generalization, it must compute the $L$ steps of the composition in the intermediate layers of the model within a single forward pass itself. For example, consider a model that computes the functions $F$ and $G$, and is able to compositionally generalize to function $G \\\\circ F$. Since $G \\\\circ F$ is computed using a single forward pass, $G$ must occur in a layer after $F$ (see also Fig. 11b). However, this model may not generalize to $F \\\\circ G$, since that will require $F$ to occur after $G$ in the model\u2019s layers. Hence, to compositionally generalize to both combinations of $F$ and $G$, a model may have to learn copies of $F$ and $G$ at multiple layers. This will likely require training data with large amounts of data diversity so that most combinations of functions are seen by the model during training itself.\\n\\nWe further formalize the intuition above in Appendix C. Specifically, in Appendix C.3, we argue that a model trained with the direct prompt format requires more compositions in the training data, by a factor of $O(L)$, compared to a model trained with the step-by-step format. In Theorem C.2, we prove that there exists an $L$-layer Transformer that can compositionally generalize with direct prompting. However, empirically, we find that even with the additional training data, the direct prompt format fails to generalize in Fig. 6 (left). This is because the existence of a solution need not guarantee that a Transformer trained with gradient descent converges to that particular minima. The weights can instead converge to a minima that only memorizes compositions.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards a mechanistic understanding\\n\\nIn this section, we try to uncover the underlying mechanism for compositional generalization exhibited by Transformers in our setup\u2014particularly for compositions of bijections in the step-by-step prompt format. Prior work on mechanistic interpretability often studies smaller neural networks to extract insights for larger networks (Nelson et al., 2021; Wang et al., 2022; Chughtai et al., 2023). The rationale relates to the universality hypothesis (Li et al., 2015; Olah et al., 2020), which states that networks of different scales are likely to learn similar functions when trained on the same data. In line with this direction, we attempt to understand a 1-layer Transformer trained on our data generating process.\\n\\nTo develop a hypothesis for our mechanistic evaluation, we first show in Appendix C.1 the existence of 1-layer Transformers that can compositionally generalize to a simplified version of our task via the step-by-step prompt format. In particular, our construction uses the attention layer to copy the relevant task token\u2014similar to an induction head (Ols-son et al., 2022)\u2014and the feed-forward layer to compute a single step of the function composition. The model is run \\\\( L \\\\) times serially, where each run computes one step of the function composition. The attention layer uses a position encoding as the key and query to determine which tokens to attend to and propagates the task token as the value.\\n\\nWe next evaluate if the theoretical construction, even though a simplification, lines up with empirical evaluations on the actual task. Specifically, we first use linear probing to understand which layers contribute to improvements in the accuracy and then visualize the attention maps to understand which tokens the model attends to.\\n\\nLinear probe accuracy.\\n\\nIn Fig. 7 (left), we use a linear probe to analyze the importance of attention layers and MLP layers in a 12-layer Transformer. Following Geva et al. (2022), we fix the parameters of probe to the last linear layer, i.e., the unembedding layer of the trained model. We use a Transformer trained on 100 random in-order compositions of 5 functions identical to the model in Fig. 4a. In Fig. 14 we show similar linear probe experiments on Transformers of different sizes and consistently find a sharp increase in accuracy right after an MLP layer, i.e., the accuracy rarely increases after an attention layer.\\n\\nVisualizing attention maps.\\n\\nAnalyzing the attention maps of a 12-layer Transformer for a discernible pattern can be difficult. We hence analyze the attention maps of a 1-layer Transformer trained for step-by-step prompts, which also exhibits in-order generalization. In Fig. 7 (right), we plot the attention map for a predefined composition of functions from the set \\\\( F_b \\\\). Keeping the task tokens to be fixed corresponding to the predefined composition, we sample 1000 data tokens and compute the attention map for the 1-layer model. The average of these maps is reported in the figure. We see that all data tokens attend to: (i) the task token that specifies the current function to be computed and (ii) the data token that the function is to be applied to.\\n\\nThe results above remarkably line up with our theoretical construction. For example, the attention maps in Fig. 7 always attend to the relevant task tokens and data token when computing the next step of the composition. The task and data tokens are all embedded in orthogonal spaces, similar to our construction, with the exception of 5 tokens which all correspond to the the identity function (see Appendix B.7). In parallel, the linear probe accuracy for a 1-layer Transformer in Fig. 14 shows no increase in accuracy after the attention layer (similar to results in Fig. 7), but a sharp increase in accuracy occurs after the MLP layers, indicating that the function is entirely computed in the MLP layers.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\n\\\\[\\nW_{11} = \\\\begin{bmatrix}\\n1 \\\\times d_1 & \\\\cdots & 1 \\\\times d_v \\\\\\\\\\n1 \\\\times d_2 & \\\\cdots & 1 \\\\times d_v \\\\\\\\\\n\\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\n1 \\\\times d_v & \\\\cdots & 1 \\\\times d_v\\n\\\\end{bmatrix}\\n\\\\]\\n\\n\\\\[\\nW_{21} = W_{22} = W_{23}, \\\\quad \\\\text{and} \\\\quad W_{31} = W_{32} = W_{33}.\\n\\\\]\\n\\nProof. See Appendix C.5.\\n\\nC.3. Difference between the direct and step-by-step prompt formats\\n\\nThe ability to run multiple forward passes through the Transformer allows us to tackle a richer class of problems (Merrill & Sabharwal, 2023). This ability differentiates the step-by-step and direct prompt formats. In the step-by-step prompt format, the Transformer makes \\\\(L\\\\) different forward passes, while the direct prompt format allows only one forward pass through the model to generate the output. This is also mirrored in our constructions in appendices C.1 and C.2\u2014a model for the step-by-step prompt format requires only 1 layer, while one for the direct prompt format uses \\\\(L = 3\\\\) layers to compensate for the lack of multiple forward passes. We expect that a Transformer for the direct prompt format cannot circumvent these computations and conjecture that our Transformer construction for the direct format (in appendix C.5) is efficient with respect to the number of layers.\\n\\nConjecture C.3. We conjecture that a Transformer with width of \\\\(\\\\text{poly}(|F|)\\\\), needs \\\\(O(L)\\\\) layers in the direct prompt format compared to the \\\\(O(1)\\\\) layers step-by-step format in order to compositionally generalize on our synthetic task. That is, a model must compute all \\\\(L\\\\) intermediate outputs of the composition across different layers of the Transformer. We expand on this further in the next subsection. We also note that as per the universal approximation theorem, it is certainly possible to construct a Transformer with 1-layer such that it generalizes for the direct prompt format; however, such a model must have its width to be exponential in \\\\(|F|\\\\) in order to store \\\\(|F|L\\\\) different functions in a single layer.\\n\\nC.3.1. How many training compositions does each prompt format need?\\n\\nTo further understand the difference between the two prompt formats, we will use a (highly simplified) model to reason about the number of function compositions in the training data that is required for perfect compositional generalization on our task. Let us consider a composition of \\\\(L\\\\) of functions from \\\\(F\\\\). We assume that the compositions in the training data \\\\(F_{\\\\text{train}} \\\\subset F\\\\) are sampled uniformly at random from the set of all compositions. For this analysis, we assume that the Transformer can perfectly identify which functions to compose\u2014which we ascribe to the attention layers\u2014and will focus entirely on capability acquisition which we hypothesize is carried out by the MLP layers. We assume that a Transformer for the step-by-step prompt format must learn a function (capability) only once, while a Transformer for the direct prompt format must learn the function \\\\(L\\\\) different times\u2014once for each layer of the Transformer.\\n\\nIf the function composition \\\\(F(l) \\\\in F_{\\\\text{train}}\\\\) occurs in the training data, we assume that the Transformer for the step-by-step format has learned all the capabilities \\\\(F(l)\\\\) for \\\\(i \\\\in [1, L]\\\\), while a Transformer for the direct prompt format can only learn capability \\\\(F(l)_i\\\\) at layer \\\\(i\\\\). These assumptions are informed by Theorems C.1 and C.2.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Detour into the coupon collector's problem. In order to learn all capabilities, the training data must contain each capability at least once. We note that this is the coupon collector's problem (Myers & Wilf, 2006): the collector seeks all distinct coupons and receives a coupon at every round drawn uniformly at random. The number of rounds corresponds to the number of function compositions in the training data and we would like to calculate the expected number of rounds required to learn all capabilities. It is a well-known result that the expected number of rounds to collect all $F$ coupons is $F H_{F}$ where $H_{F}$ is the Harmonic number; asymptotically this is $O(F \\\\log F)$. Furthermore, the probability that we complete a collection of size $f$, in $n$ rounds is $p(L, f) = \\\\frac{F!}{F^{L}} \\\\frac{F-1}{F-1}$, where $F^{-1}K^{-1}$ is the Stirling number of the second kind.\\n\\nIn the step-by-step prompt format, we observe $L$ capabilities (or coupons) with every composition. All capabilities are learned if we observe each of them in at least one training sample. The expected number of training compositions $N$ required to learn all capabilities is $O(F \\\\log F)$ (see Xu & Tang (2011)). On the other hand, the direct prompt format can be treated as $L$ independent coupon collector problems and must observe each capability once for each of the $L$ layers.\\n\\nThe expected number of rounds to learn all capabilities is the expected value of the maximum number of rounds for $L$ independent coupon collector problems. If we apply Chebyshev's inequality, we get $P(N \\\\geq F H_{F} + c \\\\log F) \\\\leq \\\\frac{\\\\pi^{2}}{6}c^{2} \\\\log 2$. Hence, the maximum value of $L$ different runs is $O(F \\\\log F)$ as $n \\\\to \\\\infty$, or in other words, the expected number of rounds to learn all capabilities is $O(F \\\\log F)$. The expected number of training compositions differs by a factor of $L$ between the two prompt formats, which tallies with the observation that a Transformer is expected to learn the same set of capabilities $L$ different times in the direct format.\\n\\nIn practice, we find that Transformers for the direct format can sometimes fail to compositionally generalize, even with a large number of compositions in the training data (Section 4.3). We hypothesize that this is attributable to the optimization landscape, i.e., gradient descent is unable to find weights that compositionally generalize and instead prefers weights that memorize compositions of functions present in the training data. In the direct prompt, gradient descent must recover the individual capabilities from a set of compositions of bijections and this is a computationally hard problem since it is similar to finding the minimal generating set of a group (its time complexity is linear in the size of the group which is $O(FL)$).\\n\\n### C.4. Proof of Theorem C.1\\n\\nStep 1: Computing the attention layer. The attention layer copies the task tokens onto the relevant data token similar to an induction head (Olsson et al., 2022). We first compute the query and value matrices of the attention:\\n\\n$$Z^T Q Z = \\\\begin{bmatrix} x^T_{F1} p^T_1 & x^T_{F2} p^T_2 & x^T_{F3} p^T_3 & x^T_d p^T_4 & F_1(x_d) p^T_5 & F_2 \\\\circ F_1(x_d) p^T_6 & \\\\cdots & \\\\cdots & \\\\cdots & \\\\cdots & \\\\cdots \\\\end{bmatrix} = \\\\begin{bmatrix} 0 & p^T_1 & 0 & p^T_2 & 0 & p^T_3 & 0 & p^T_4 & 0 & p^T_5 & 0 & p^T_6 \\\\end{bmatrix} = P^T P$$\\n\\nOur construction considers a $P$ such that $p_i = p_i + 4$ for all $i \\\\in [1, 3]$ and $p_i \\\\cdot p_j = 0$ for all $j \\\\in [1, 3]$ and $j \\\\neq i$. The mask $M$ converts $P^T P$ into an upper triangular matrix, and zeroes out all entries in the lower triangle of the matrix.\"}"}
{"id": "L1eJ3NKPCd", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The attention layer computes \\\\( \\\\text{Attn}(Z) = (KZ)(M \\\\odot Z^TQZ) \\\\)\\n\\n\\\\[\\n= (KZ)(P \\\\odot P^T)\\n\\\\]\\n\\n\\\\[\\n= \\\\begin{bmatrix}\\n0 & d_v \\\\times d_v & 0 \\\\\\\\\\n0 & d_f \\\\times d_v & 0 \\\\\\\\\\n0 & d_p \\\\times d_p & 0 \\\\\\\\\\n0 & d_p \\\\times d_p & 0 \\\\\\\\\\n0 & d_v \\\\times d_v & 0 \\\\\\\\\\n0 & d_f \\\\times d_v & 0 \\\\\\\\\\n0 & d_f \\\\times d_v & 0 \\\\\\\\\\n0 & d_p \\\\times d_p & 0 \\\\\\\\\\n0 & d_p \\\\times d_p & 0 \\\\\\\\\\nI_d_f \\\\times I_d_f & 0 & 0 \\\\\\\\\\nI_d_f \\\\times I_d_f & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\nI_d_f \\\\times I_d_f & 0 & 0 \\\\\\\\\\nI_d_f \\\\times I_d_f & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\nI_d_f \\\\times I_d_f & 0 & 0 \\\\\\\\\\nI_d_f \\\\times I_d_f & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\nI_d_f \\\\times I_d_f & 0 & 0 \\\\\\\\\\nI_d_f \\\\times I_d_f & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 \\"}
{"id": "L1eJ3NKPCd", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks\\n\\nReLU(W\u2081(Attn(Z) + Z)) = ReLU(Attn(Z) + Z)\u1d40W\u2081\u1d40\\n\\nIncluding the final weight matrix W\u2082, we get\\n\\nW\u2082ReLU(W\u2081(Attn(Z) + Z)) = W\u2082ReLU(Attn(Z) + Z)\\n\\nHence, the output of the Transformer is\\n\\nTr(Z) = MLP(Attn(Z) + Z) + (Attn(Z) + Z)\\n\\nIf we set We = I_d \u00d7 d, then WeTr(Z) evaluates to\\n\\nW\u2081F\u2081(x_d)F\u2082 \u25e6 F\u2081(x_d)F\u2083 \u25e6 F\u2082 \u25e6 F\u2081(x_d)\\n\\nwhich will assign high probabilities to the desired token when passed through a Softmax layer. Hence, a Transformer prompted with [x\u2081, x\u2082, x\u2083, x_d] will auto-regressively generate [F\u2081(x_d), F\u2082 \u25e6 F\u2081(x_d), F\u2083 \u25e6 F\u2082 \u25e6 F\u2081(x_d)] for any combination of data and task tokens.\\n\\nC.5. Proof of Theorem C.2\\n\\nThe details of construction are similar to Appendix C.4.\\n\\nStep 1: Computing the output of the first block.\\n\\nThe first Transformer block computes the first step of the composition. The attention layer in particular, copies the relevant task token to the data token. The value and query matrices of the...\"}"}
