{"id": "marro23a", "page_num": 57, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 31. $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 C Adversarial. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\\n\\nFigure 32. $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 C ReLU. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "marro23a", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\n\u2022 If $e(x)\\\\times(x^\\\\ast) = 1$ and $e(y)\\\\times(x^\\\\ast) = 0$, set $x^{\\\\prime\\\\prime\\\\ast}_i$ equal to a value in $1 - \\\\gamma, \\\\frac{3}{4}$;\\n\\n\u2022 If $e(x)\\\\times(x^\\\\ast) = 1$ and $e(y)\\\\times(x^\\\\ast) = 1$, set $x^{\\\\prime\\\\prime\\\\ast}_i$ equal to a value in $\\\\frac{3}{4}, 1$.\\n\\nBy doing so, we have obtained a $x^{\\\\prime\\\\prime\\\\ast}$ such that $x^{\\\\prime\\\\prime\\\\ast} \\\\in B_\\\\infty(x^\\\\ast, \\\\gamma)$ and $e(y)\\\\times(x^{\\\\prime\\\\prime\\\\ast}) = \\\\hat{y}^\\\\ast$.\\n\\nSince:\\n\\n\u2022 $e(x)\\\\times(x^{\\\\prime\\\\prime}) = e(x)\\\\times(x^\\\\ast)$ for all $x^{\\\\prime\\\\prime}$;\\n\\n\u2022 $h(x^{\\\\prime\\\\prime}) = 0$ for all $x^{\\\\prime\\\\prime}$;\\n\\n\u2022 $h(x^{\\\\prime\\\\prime}) = 1$ iff $R(e(x)\\\\times(x^{\\\\prime\\\\prime})), e(y)\\\\times(x^{\\\\prime\\\\prime})$ is true;\\n\\n$R(e(x)\\\\times(x^\\\\ast)), e(y)\\\\times(x^{\\\\prime\\\\prime})$ is false for all choices of $\\\\hat{y}^\\\\ast$. In other words, $\\\\hat{x}^\\\\ast$ is a solution to Equation (32) and thus $z \\\\in co\\\\Pi_23SAT$.\\n\\nSince:\\n\\n\u2022 $q(z)$ can be computed in polynomial time;\\n\\n\u2022 $z \\\\in co\\\\Pi_23SAT \\\\Rightarrow q(z) \\\\in CCA_\\\\infty$;\\n\\n\u2022 $q(z) \\\\in CCA_\\\\infty \\\\Rightarrow z \\\\in co\\\\Pi_23SAT$;\\n\\nwe can conclude that $co\\\\Pi_23SAT \\\\leq CCA_\\\\infty$.\\n\\nF.3. Proof of Corollary 4.4\\n\\nThe proof of $CCA_p \\\\in \\\\Sigma_2P$ is the same as the one for Theorem 4.3. For the hardness proof, we follow a more involved approach compared to those for Corollaries 3.2 and 3.8.\\n\\nFirst, let $\\\\varepsilon_{\\\\rho}\\\\rho_{p,n}$ be the value of epsilon such that $\\\\rho_{p,n} \\\\varepsilon_{\\\\rho}\\\\rho_{p,n} = \\\\frac{1}{2}$. In other words, $B_p(x(s), \\\\varepsilon_{\\\\rho}\\\\rho_{p,n})$ is an $L_p$ ball that contains $[0, 1]^n$, while the intersection of the corresponding $L_p$ sphere and $[0, 1]^n$ is the set $\\\\{0, 1\\\\}^n$ (for $p < \\\\infty$).\\n\\nLet $inv^{\\\\prime}T(x)$ be defined as follows:\\n\\n$$inv^{\\\\prime}T(x) = or_{i=1,\\\\ldots,n} or_{eq x_i, \\\\frac{1}{2}}, leq_{x_i, 0}, geq_{x_i, 1}.$$\\n\\nLet $inv^{\\\\prime}F(x)$ be defined as follows:\\n\\n$$inv^{\\\\prime}F(x) = or_{i=1,\\\\ldots,n} or_{eq x_i, \\\\frac{3}{4}}, eq_{x_i, \\\\frac{1}{4}}.$$\\n\\nWe define $h^{\\\\prime}$ as follows:\\n\\n$$h^{\\\\prime}_0(x) = not(h^{\\\\prime}_1(x)).$$\\n\\nwith $h^{\\\\prime}_1(x) = or_{(inv^{\\\\prime}T(x), and (not((inv^{\\\\prime}F(x)))}, g(x)}$.\\n\\nNote that:\\n\\n\u2022 If $x_i \\\\in (-\\\\infty, 0] \\\\cup \\\\{\\\\frac{1}{2}\\\\} \\\\cup [1, \\\\infty)$ for some $i$, then the top class is 1;\\n\\n\u2022 Otherwise, if $x$ is not a valid encoding, the top class is 0;\\n\\n\u2022 Otherwise, the top class is 1 if $R(e(x)\\\\times(x)), e(y)\\\\times(x)$ is true and 0 otherwise.\\n\\nFinally, let $\\\\frac{1}{8} < \\\\gamma^{\\\\prime} < \\\\frac{1}{4}$. Our query is thus:\\n\\n$$q(z) = \\\\langle x(s), \\\\gamma^{\\\\prime}, \\\\frac{1}{2}, C_u, h^{\\\\prime} \\\\rangle.$$\"}"}
{"id": "marro23a", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nProof of \\\\( z \\\\in \\\\text{co} \\\\Pi_{23}^{\\\\text{SAT}} = \\\\Rightarrow q(z) \\\\in \\\\text{CCA} \\\\)\\n\\nIf \\\\( z \\\\in \\\\text{co} \\\\Pi_{23}^{\\\\text{SAT}} \\\\), then \\\\( \\\\exists x^* \\\\). \\\\( \\\\forall \\\\hat{y}. \\\\neg R(x^*, \\\\hat{y}) \\\\). Let \\\\( x^{**} \\\\) be defined as follows:\\n\\n\\\\[\\nx^{**i} = \\\\begin{cases} \\n  1 & \\\\text{if } x^i = 0 \\\\\\\\\\n  3 & \\\\text{if } x^i = 1 \\n\\\\end{cases}\\n\\\\]\\n\\n(57)\\n\\nNote that:\\n\\n- \\\\( x^{**} \\\\in B_p(x(s)) \\\\), \\\\( \\\\varepsilon \\\\rho_{p,n} \\\\);\\n- \\\\( e(x(x^{**})) = x(x^*) \\\\);\\n- \\\\( f(x^{**}) = 0 \\\\), since \\\\( x^{**} \\\\in \\\\{\\\\frac{1}{4},\\\\frac{3}{4}\\\\} \\\\);\\n- Since \\\\( \\\\gamma' < \\\\frac{1}{4} \\\\), there is no \\\\( i \\\\) such that \\\\( \\\\exists x'' \\\\in B_p(x^{**}, \\\\gamma') \\\\).\\n- For all \\\\( x'' \\\\in B_p(x^{**}, \\\\gamma') \\\\):\\n  - If \\\\( x'' \\\\) is not a valid encoding (i.e. \\\\( x''i \\\\in \\\\{\\\\frac{1}{4},\\\\frac{3}{4}\\\\} \\\\) for some \\\\( i \\\\)), then \\\\( h'(x'') = 0 \\\\);\\n  - Otherwise, \\\\( h'(x'') = 1 \\\\) iff \\\\( R(e(x(x^{**})), e(y(x''))) \\\\) is true.\\n\\nTherefore, since \\\\( \\\\forall \\\\hat{y}. \\\\neg R(x^*, \\\\hat{y}) \\\\), we know that \\\\( \\\\forall x'' \\\\in B_p(x^{**}, \\\\gamma') \\\\). \\\\( f(x'') = 0 \\\\). In other words, \\\\( x^{**} \\\\) is a solution to Equation (7).\\n\\nProof of \\\\( q(z) \\\\in \\\\text{CCA} \\\\Rightarrow z \\\\in \\\\text{co} \\\\Pi_{23}^{\\\\text{SAT}} \\\\)\\n\\nIf \\\\( q(z) \\\\in \\\\text{CCA} \\\\), then we know that \\\\( \\\\exists x^* \\\\in B_p(x(s)), \\\\varepsilon \\\\rho_{p,n} \\\\).\\n\\n\\\\[\\nh'(x^*) \\\\neq h(x(s)) \\\\land \\\\forall x'' \\\\in B_p(x^*, \\\\gamma'). h'(x'') = h'(x^*) \\\\]\\n\\n(58)\\n\\nFrom this, due to the fact that \\\\( \\\\gamma' > \\\\frac{1}{8} \\\\) and that \\\\( p > 0 \\\\), we can conclude that for all \\\\( \\\\hat{y} \\\\), there exists a \\\\( x'' \\\\in B_p(x^*, \\\\gamma') \\\\) such that:\\n\\n- For all \\\\( \\\\hat{y} \\\\), there exists a \\\\( x'' \\\\in B_p(x^*, \\\\gamma') \\\\) such that \\\\( e(x(x'')) = e(x(x^*)) \\\\).\\n- Since \\\\( \\\\gamma' > \\\\frac{1}{8} \\\\) and that \\\\( p > 0 \\\\), we can conclude that for all \\\\( \\\\hat{y} \\\\), there exists a \\\\( x'' \\\\in B_p(x^*, \\\\gamma') \\\\) such that:\\n  - For all \\\\( \\\\hat{y} \\\\), there exists a \\\\( x'' \\\\in B_p(x^*, \\\\gamma') \\\\) such that \\\\( e(x(x'')) = e(x(x^*)) \\\\).\\n  - Therefore, since \\\\( \\\\forall \\\\hat{y}. \\\\neg R(x^*, \\\\hat{y}) \\\\), we know that \\\\( \\\\forall x'' \\\\in B_p(x^{**}, \\\\gamma') \\\\). \\\\( f(x'') = 0 \\\\). In other words, \\\\( x^{**} \\\\) is a solution to Equation (7).\"}"}
{"id": "marro23a", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In other words, for all $\\\\hat{y}$ there exists a corresponding $x'' \\\\in B_p(\\\\mathbf{x}^*, \\\\gamma')$ such that $e(y)(x'') = \\\\hat{y}$.\\n\\nTherefore, since $h'(x'') = 1$ iff $R(e(x)(x''), e(y)(x''))$ is true and since $\\\\forall x'' \\\\in B_p(\\\\mathbf{x}^*, \\\\gamma')$. $h'(x'') = 0$, we can conclude that $\\\\forall \\\\hat{y}$. $\\\\neg R(e(x)(x^*), \\\\hat{y})$. In other words, $z \\\\in co\\\\text{-}\\\\Pi_{23}^{\\\\text{SAT}}$.\\n\\nF.4. Proof of Corollary 4.5\\n\\nSimilarly to the proof of Corollary 3.4, it follows from the fact that ReLU classifiers are polynomial-time classifiers (w.r.t. the size of the tuple).\"}"}
{"id": "marro23a", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we outline the similarities between transductive approaches to robust learning (taking the work of Chen et al. (2021) as an example) and CA: the former fixes the input and adapts the model at inference time, while the latter fixes the model and solves an optimization problem in the input space. In particular, the approach by Chen et al. involves adapting the model at test time to a set $U$ of user-provided (and potentially adversarially corrupted) inputs. For the sake of clarity, we rewrite the adaptation task as follows:\\n\\n$$\\n\\\\arg \\\\min_\\\\theta L_d(f_\\\\theta, U)\\n$$\\n\\n(61)\\n\\nwhere $L_d$ is an unsupervised adaptation loss, and define $\\\\Gamma(U) = f_\\\\theta^*$, where $\\\\theta^*$ is the solution to Equation (61). Attacking this technique thus involves solving the following constrained optimization problem (adapted from Equation 6 of the original paper):\\n\\n$$\\n\\\\arg \\\\max_{U'} L_a(f_{\\\\theta^*}, U')\\n\\\\text{s.t.} \\\\theta^* = \\\\arg \\\\min_\\\\theta L_d(f_\\\\theta, U')\\n$$\\n\\n(62)\\n\\nwhere $L_a$ is the loss for the adversarial objective. Chen then provides an alternative formulation (Equation 8 of the original paper) that is more tractable from a formal point of view; however, our adapted equation is a good starting point for our comparison. In particular, as in our work, attacking the approach by Chen et al. requires solving a problem that involves nested optimization, and therefore the same \u201ccore\u201d of complexity. With this formulation, the connections between Chen et al. and CA become clear:\\n\\n- Both approaches use an optimization problem at inference time that is parameterized over the input (thus avoiding the second informal asymmetry mentioned in Section 3.5);\\n- Attacks against both approaches lead to nested optimization problems.\\n\\nWe therefore conjecture that it should be possible to extend the result from our Theorem 4.3 to the approach by Chen et al. However, there are some differences between the work of Chen et al. and ours that will likely need to be addressed in a formal proof:\\n\\n- The former is designed with transductive learning in mind, while the latter is intended for \u201cregular\u201d classification (i.e. where the model is fixed);\\n- The former is meant to be used with arbitrarily large sets of inputs, while the latter only deals with one input at the time;\\n- The former uses two different losses ($L_d$ and $L_a$), which can potentially make theoretical analyses more complex;\\n- There are several possible ways to adapt a model to a given $U$, and a proof would likely have to consider a sufficiently \u201cinteresting\u201d subset of such techniques.\\n\\nWe hope that our theoretical findings will encourage research into such areas.\"}"}
{"id": "marro23a", "page_num": 53, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 23. Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 C ReLU. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\\n\\nFigure 24. \\\\( R^2 \\\\) of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 A Standard. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "marro23a", "page_num": 54, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 25. $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\\n\\nFigure 26. $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "marro23a", "page_num": 55, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 27. $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 B Standard. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\\n\\nFigure 28. $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 B Adversarial. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "marro23a", "page_num": 56, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 29. $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 B ReLU. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\\n\\nFigure 30. $R^2$ of linear model for the heuristic adversarial distances given the exact decision boundary distances for MNIST & CIFAR10 C Standard. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. The x axis is logarithmic.\"}"}
{"id": "marro23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nSamuele Marro\\nMichele Lombardi\\n\\nAbstract\\nIn the context of adversarial robustness, we make three strongly related contributions. First, we prove that while attacking ReLU classifiers is \\\\( \\\\text{NP} \\\\)-hard, ensuring their robustness at training time is \\\\( \\\\Sigma_2^P \\\\)-hard (even on a single example). This asymmetry provides a rationale for the fact that robust classifications approaches are frequently fooled in the literature. Second, we show that inference-time robustness certificates are not affected by this asymmetry, by introducing a proof-of-concept approach named Counter-Attack (CA). Indeed, CA displays a reversed asymmetry: running the defense is \\\\( \\\\text{NP} \\\\)-hard, while attacking it is \\\\( \\\\Sigma_2^P \\\\)-hard.\\n\\nFinally, motivated by our previous result, we argue that adversarial attacks can be used in the context of robustness certification, and provide an empirical evaluation of their effectiveness. As a byproduct of this process, we also release UG100, a benchmark dataset for adversarial attacks.\\n\\n1. Introduction\\nAdversarial attacks, i.e. algorithms designed to fool machine learning models, represent a significant threat to the applicability of such models in real-world contexts (Brown et al., 2017; Brendel et al., 2019; Wu et al., 2020). Despite years of research effort, countermeasures (i.e. \u201cdefenses\u201d) to adversarial attacks are frequently fooled by applying small tweaks to existing techniques (Carlini & Wagner, 2016; 2017a; He et al., 2017; Hosseini et al., 2019; Tramer et al., 2020; Croce et al., 2022). We argue that this pattern is due to differences between the fundamental mathematical problems that defenses and attacks need to tackle, and we investigate this topic by providing three contributions.\\n\\nFirst, we prove a set of theoretical results about the complexity of attack and training-time defense problems, including the fact that attacking a ReLU classifier is \\\\( \\\\text{NP} \\\\)-hard in the general case, while finding a parameter set that makes a ReLU classifier robust on even a single input is \\\\( \\\\Sigma_2^P \\\\)-hard. To the best of our knowledge, this is the first complexity bound for general ReLU classifiers, and the main contribution of this work. We also provide more general bounds for non-polynomial classifiers, and show in particular that an \\\\( \\\\mathcal{A} \\\\)-time classifier can be attacked in \\\\( \\\\text{NP} \\\\)-time. Instead of using a PAC-like formalization, we rely on a worst-case semantic of robustness. This approach results in a formalization that is both more easier to deal with and independent of data distribution assumptions, while still providing a rationale for difficulties in training robust classifiers that are well-known in the related literature. Our proofs also lay the groundwork for identifying tractable classes of defenses.\\n\\nSecond, we prove by means of an example that inference-time defenses can sidestep the asymmetry. Our witness is a proof-of-concept approach, referred to as Counter-Attack (CA), that evaluates robustness on the fly for a specific input (w.r.t. to a maximum distance \\\\( \\\\varepsilon \\\\)) by running an adversarial attack. Properties enjoyed by this technique are likely to extend to other inference-time defense methods, if they are based on similar principles. Notably, when built over an exact attack, generating a certificate is \\\\( \\\\text{NP} \\\\)-hard in the worst case, \\\\( \\\\varepsilon \\\\)-bounded attacks are impossible, and attacking using perturbations of magnitude \\\\( \\\\varepsilon' > \\\\varepsilon \\\\) is \\\\( \\\\Sigma_2^P \\\\)-hard. On the other hand, using a non-exact attack results in partial guarantees (no false positives for heuristic attacks, no false negatives for bounding techniques).\\n\\nFinally, since our results emphasize the connection between verification and attack problems, we provide an empirical investigation of the use of heuristic attacks for verification. We found heuristic attacks to be high-quality approximators for exact decision boundary distances: a pool of seven heuristic attacks provided an accurate (average over-estimate between 2.04% and 4.65%) and predictable (average \\\\( R^2 > 0.99 \\\\)) approximation of the true optimum for small-scale Neural Networks trained on the MNIST and CIFAR10 datasets. We release our benchmarks and adversarial examples (both exact and heuristic) in a new dataset, named UG100.\\n\\nOverall, we hope our contributions can support future research by highlighting potential structural challenges, point-1 All our code, models, and data are available under MIT license at https://github.com/samuelemarro/counter-attack.\"}"}
{"id": "marro23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nout key sources of complexity, inspiring research on\\nheuristics and tractable classes, and suggesting alternative\\nperspectives on how to build robust classifiers.\\n\\n2. Background and Formalization\\nIn this section, we introduce key definitions (adapted from\\nDreossi et al. (2019)) that we will use to frame our results.\\nOur aim is to capture the key traits shared by most of the\\nliterature on adversarial attacks, so as to identify properties\\nthat are valid under broad assumptions.\\n\\nAdversarial Attacks and Robustness\\nWe start by defining the concept of an adversarial example,\\nwhich intuitively represents a modification of a legitimate input\\nthat is so limited as to be inconsequential for a human observer,\\nbut sufficient to mislead a target model. Formally, let\\n\\\\( f : X \\\\to \\\\{1, \\\\ldots, N\\\\} \\\\) be a discrete classifier.\\n\\nLet \\\\( B_p(x, \\\\varepsilon) = \\\\{x' \\\\in X | \\\\|x - x'\\\\|_p \\\\leq \\\\varepsilon\\\\} \\\\) be a\\n\\\\( L_p \\\\) ball of radius \\\\( \\\\varepsilon \\\\) and center \\\\( x \\\\).\\n\\nThen we have:\\n\\n**Definition 2.1** (Adversarial Example).\\nGiven an input \\\\( x \\\\), a threshold \\\\( \\\\varepsilon \\\\), and a\\n\\\\( L_p \\\\) norm, an adversarial example is an input \\\\( x' \\\\in B_p(x, \\\\varepsilon) \\\\) such that\\n\\\\( f(x') \\\\in C(x) \\\\), where \\\\( C(x) \\\\subseteq \\\\{1, \\\\ldots, N\\\\} \\\\setminus \\\\{f(x)\\\\} \\\\).\\n\\nThis definition is a simplification compared to human per-\\nception, but it is adequate for a sufficiently small \\\\( \\\\varepsilon \\\\), and it is\\nadopted in most of the relevant literature. An\\nadversarial attack can then be viewed as an optimization procedure\\nthat attempts to find an adversarial example. We define an adver-\\nsarial attack for a classifier \\\\( f \\\\) as a function \\\\( a_{f,p} : X \\\\to X \\\\) that solves\\nthe following optimization problem:\\n\\n\\\\[\\n\\\\arg \\\\min_{x'} \\\\{\\\\|x' - x\\\\|_p | f(x') \\\\in C(x)\\\\}\\n\\\\]\\n\\n(1)\\nThe attack is considered successful if the returned solu-\\ntion \\\\( x' = a_{f,p}(x) \\\\) also satisfies \\\\( \\\\|x' - x\\\\|_p \\\\leq \\\\varepsilon \\\\).\\nWe say that an attack is\\nexact if it solves Equation (1) to optimal-\\nity (or, in the case of its decision variant, if it succeeds\\nif and only if a solution exists); otherwise, we say that\\nthe attack is\\nheuristic. An attack is said to be\\ntargeted if \\\\( C(x) = C_{t,y}(x) = \\\\{y'\\\\} \\\\) with\\n\\\\( y' \\\\neq f(x) \\\\); it is instead\\nuntargeted if \\\\( C_{u}(x) = \\\\{1, \\\\ldots, N\\\\} \\\\setminus \\\\{f(x)\\\\} \\\\).\\nWe define the\\ndecision boundary distance \\\\( d^*_p(x) \\\\) of a given input\\n\\\\( x \\\\) as the\\nminimum \\\\( L_p \\\\) distance between\\n\\\\( x \\\\) and another input \\\\( x' \\\\) such\\nthat \\\\( f(x) \\\\neq f(x') \\\\). This is also the value of\\n\\\\( \\\\|a_{f,p}(x) - x\\\\|_p \\\\) for an exact, untargeted, attack.\\n\\nIntuitively, a classifier is\\nrobust w.r.t. an example\\niff \\\\( x \\\\) cannot be successfully attacked. Formally:\\n\\n**Definition 2.2** ((\\\\( \\\\varepsilon \\\\), \\\\( p \\\\))-Local Robustness).\\nA discrete clas-\\nsifier \\\\( f \\\\) is \\\\((\\\\varepsilon, p)\\\\)-locally robust w.r.t. an example\\n\\\\( x \\\\in X \\\\) iff\\n\\\\( \\\\forall x' \\\\in B_p(x, \\\\varepsilon) \\\\) we have\\n\\\\( f(x') = f(x) \\\\).\\n\\nWe use the term \u201cnorm\u201d for \\\\( 0 < p < 1 \\\\) even if in such cases\\nthe \\\\( L_p \\\\) function is not subadditive.\\n\\nUnder this definition, finding a parameter set \\\\( \\\\theta \\\\) that makes a\\nclassifier \\\\( f_\\\\theta \\\\) robust on\\n\\\\( x_0 \\\\) can be seen as solving the follow-\\ning constraint satisfaction problem:\\n\\n\\\\[\\n\\\\text{find } \\\\theta \\\\text{ s.t. } \\\\forall x' \\\\in B_p(x_0, \\\\varepsilon). f_\\\\theta(x') = f_\\\\theta(x_0)\\n\\\\]\\n\\n(2)\\nwhich usually features an additional constraint on the min-\\nimum clean accuracy of the model (although we make no\\nassumptions on this front). Note that classifiers are usually\\nexpected to be robust on more than one point. However, we\\nwill show that the computational asymmetry exists even if\\nwe require robustness on a single point.\\n\\nA common optimization reformulation of Equation (2),\\nwhich enforces robustness\\nand\\naccuracy, is the nested op-\\ntimization problem used for adversarial training in Madry\\net al. (2018). Specifically, if we have a single ground truth\\ndata point \\\\( \\\\langle x_0, y \\\\rangle \\\\), the optimization problem is:\\n\\n\\\\[\\n\\\\arg \\\\min_{\\\\theta} \\\\max_{x' \\\\in B_p(x_0, \\\\varepsilon)} L(\\\\theta, x', y)\\n\\\\]\\n\\n(3)\\nwhere\\n\\\\( L \\\\) is a proxy for\\n\\\\( f_\\\\theta(x') = y \\\\) (e.g. the cross entropy\\nloss between\\n\\\\( f_\\\\theta(x') \\\\) and\\n\\\\( y \\\\)). The link between\\n\\\\( \\\\exists\\\\forall \\\\) queries\\n(such as that in Equation (2) and nested optimization prob-\\nlems (such as that in Equation (3)) underlies the intuition of\\nseveral of our theoretical results (see Section 3.1).\\n\\nReLU Networks and FSFP Spaces\\nAdditionally, our\\nresults rely on definitions of ReLU networks and FSFP\\nspaces.\\n\\n**Definition 2.3** (ReLU network).\\nA ReLU network is a\\ncomposition of sum, multiplication by a constant, and\\nReLU activation, where\\nReLU:\\n\\\\( \\\\mathbb{R} \\\\to \\\\mathbb{R}^+ \\\\) is defined as\\nReLU\\\\( (x) = \\\\max(x, 0) \\\\).\\n\\nNote that any hardness result for ReLU classifiers also ex-\\ntends to general classifiers.\\n\\nFixed-Size Fixed-Precision (FSFP) spaces, on the other\\nhand, capture two common assumptions about real-world\\ninput spaces: all inputs can be represented with the same\\nnumber of bits and there exists a positive minorant of the\\ndistance between inputs.\\n\\n**Definition 2.4** (Fixed-Size Fixed-Precision space).\\nGiven\\na real\\n\\\\( p > 0 \\\\), a space\\n\\\\( X \\\\subseteq \\\\mathbb{R}^n \\\\) is FSFP if there exists a\\n\\\\( \\\\nu \\\\in \\\\mathbb{N} \\\\) such that\\n\\\\( \\\\forall x \\\\). \\\\( |r(x')| \\\\leq \\\\nu \\\\) (where\\n\\\\( |r(x')| \\\\) is the size of\\nthe representation of\\n\\\\( x \\\\)) and there exists a\\n\\\\( \\\\mu \\\\in \\\\mathbb{R} \\\\) such that\\n\\\\( \\\\mu > 0 \\\\) and\\n\\\\( \\\\forall x, x' \\\\in X. (\\\\|x' - x\\\\|_p < \\\\mu = \\\\Rightarrow x = x') \\\\).\\n\\nExamples of FSFP spaces include most image encodings,\\nas well as 32-bit and 64-bit IEE754 tensors. Examples of\\nnon-FSFP spaces include the set of all rational numbers in\\nan interval. Similarly to ReLU networks, hardness results\\nfor FSFP spaces also apply to more general spaces.\"}"}
{"id": "marro23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nSeveral of our theoretical results concern complexity classes in the Polynomial Hierarchy such as $\\\\Sigma P^2$. $\\\\Sigma P^2$ is the class of problems that can be solved in $NP$ time if we have an oracle that solves an $NP$-time problem in $O(1)$. $\\\\Sigma P^2$-hard problems include finding a strong Nash equilibrium (Gottlob et al., 2011) and $co\\\\Pi^2_3SAT$ (Stockmeyer, 1976). A notable conjecture is the Polynomial Hierarchy conjecture (Stockmeyer, 1976), a generalization of the $P \\\\neq NP$ conjecture which states that the Polynomial Hierarchy does not collapse (i.e. $P \\\\subsetneq NP \\\\subsetneq \\\\Sigma P^2 \\\\subsetneq \\\\Sigma P^3 \\\\ldots$).\\n\\nIn other words, under broad assumptions, we cannot solve a $\\\\Sigma P^2$-hard problem efficiently even if we can solve $NP$-hard problems in constant time.\\n\\n3. An Asymmetrical Setting\\n\\nIn this section, we prove the existence of a structural asymmetry between the computational classes of attack and training-time defense problems (barring the collapse of the Polynomial Hierarchy) by studying their decision versions.\\n\\nWhile the asymmetry is worst-case in nature, it holds under broad assumptions and provides an explanation for why attacks seem to outperform defenses in practice.\\n\\n3.1. Intuition\\n\\nThe intuition behind our theorems consists in three main observations:\\n\\n\u2022 ReLU networks, due to their expressive power, are capable of computing input-output relations that are at least as complex as Boolean formulae;\\n\u2022 Attacking usually requires solving an optimization problem, whose decision variant (finding any adversarial example) can be expressed as an $\\\\exists$ query;\\n\u2022 Training a robust classifier, on the other hand, usually requires solving a nested optimization problem, whose decision variant (finding any robust parameter set) can be expressed as an $\\\\exists\\\\forall$ query.\\n\\nFrom these considerations, we show that solving 3SAT can be reduced to attacking the ReLU classifier that computes the corresponding Boolean formula, and thus that attacking a ReLU classifier is $NP$-hard (Theorem 3.1).\\n\\nWe then prove that, given a 3CNF formula $z(x, y)$, it is possible to build a ReLU classifier $f_x(y)$ (where $x$ are parameters and $y$ are inputs) that computes the same formula. We use this result to prove that $co\\\\Pi^2_3SAT$ (a subclass of $TQBF$ that is known to be $\\\\Sigma P^2$-hard) can be reduced to finding a parameter set that makes $f$ robust, which means that the latter is $\\\\Sigma P^2$-hard (Theorem 3.7).\\n\\nNote that hardness results for decision problems trivially extend to their corresponding optimization variants.\\n\\nNote that our approach (which is common in proofs by reduction) allows us to study the worst-case complexity of both tasks without making assumptions on the training distribution or the specifics of the learning algorithm. Studying the average-case complexity of such tasks would of course be of great importance, however: 1) such an approach would require to introduce assumptions about the training distribution; and 2) despite the recent advancements in fields such as PAC learning, average case proof in this setting are still very difficult to obtain except in very specific cases (see Section 3.4). We hope that our theoretical contributions will allow future researchers to extend our work to average-case results.\\n\\nIn short, while our theorems rely on specific instances of ReLU classifiers, they capture very general phenomena: ReLU networks can learn functions that are at least as complex as Boolean formulae, and robust training requires solving a nested optimization problem. The proofs thus provide an intuition on the formal mechanisms that underly the computational asymmetries, while at the same time outlining directions for studying tractable classes (since both 3SAT and $TQBF$ are extensively studied in the literature).\\n\\n3.2. Preliminaries\\n\\nWe begin by extending the work of Katz et al. (2017), who showed that proving linear properties of ReLU networks is $NP$-complete. Specifically, we prove that the theorem holds even in the special case of adversarial attacks:\\n\\nTheorem 3.1 (Untargeted $L_\\\\infty$ attacks against ReLU classifiers are $NP$-complete).\\n\\nLet $U_{AT}$ be the set of all tuples $\\\\langle x, \\\\epsilon, f \\\\rangle$ such that:\\n\\n$\\\\exists x' \\\\in B(x, \\\\epsilon). f(x') \\\\neq f(x) \\\\tag{4}$\\n\\nwhere $x \\\\in X$, $X$ is a FSFP space and $f$ is a ReLU classifier.\\n\\nThen $U_{AT}$ is $NP$-complete.\\n\\nCorollary 3.2. For every $0 < p \\\\leq \\\\infty$, $U_{AT}^p$ is $NP$-complete.\\n\\nCorollary 3.3. Targeted $L_p$ attacks (for $0 < p \\\\leq \\\\infty$) against ReLU classifiers are $NP$-complete.\\n\\nCorollary 3.4. Theorem 3.1 holds even if we consider the more general set of polynomial-time classifiers w.r.t. the size of the tuple.\\n\\nA consequence of Theorem 3.1 is that the complementary task of attacking, i.e. proving that no adversarial example exists (which is equivalent to proving that the classifier is locally robust on an input), is $coNP$-complete.\\n\\nThe proofs of all our theorems and corollaries can be found in the appendices.\"}"}
{"id": "marro23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nWe then provide a more general upper bound that holds for classifiers in any complexity class:\\n\\n**Theorem 3.5**\\n\\n(Untargeted $L_p$ attacks against $A$-time classifiers are in $NP^A$.)\\n\\nLet $A$ be a complexity class, let $f$ be a classifier, let $Z_f = \\\\{\u27e8x, y\u27e9 | y = f(x), x \u2208 X\\\\}$ and let $U_{AT}^p(f) = \\\\{\u27e8x, \u03b5, g\u27e9 \u2208 U_{AT}^p\u2032 | g = f\\\\}$, where $U_{AT}^p$ is the same as $U_{AT}^p\u2032$ but without the ReLU classifier restriction. If $Z_f \u2208 A$, then for every $0 < p \u2264 \u221e$, $U_{AT}^p(f) \u2208 NP^A$.\\n\\n**Corollary 3.6.**\\n\\nFor every $0 < p \u2264 \u221e$, if $Z_f \u2208 \\\\Sigma^P_n$, then $U_{AT}^p(f) \u2208 \\\\Sigma^P_{n+1}$.\\n\\nAs a consequence, if $Z_f \u2208 P$, then $U_{AT}^p(f) \u2208 NP$.\\n\\nInformally, Theorem 3.1 establishes that, under broad assumptions, evaluating and attacking a general classifier are in complexity classes that are strongly conjectured to be distinct, with the attack problem being the harder one. Note that, in some special cases, one can obtain polynomial-time classifiers with polynomial-time attacks by placing additional restrictions on the input distribution and/or the structure of the classifier. Refer to Section 3.4 for an overview of such approaches.\\n\\n### 3.3. Complexity of Robust Training\\n\\nWe then proceed to prove our main result, i.e. that finding a robust parameter set, as formalized by our semantic, is in a distinct complexity class compared to the attack problem.\\n\\n**Theorem 3.7**\\n\\n(Finding a set of parameters that make a ReLU network $(\u03b5, p)$-locally robust on an input is $\\\\Sigma^P_2$-complete.)\\n\\nLet $P_{L-ROB}^p$ be the set of tuples $\u27e8x, \u03b5, f_\u03b8, v\u27e9$ such that:\\n\\n\\\\[ \\\\exists \u03b8\u2032. (v_f(\u03b8\u2032) = 1 \u21d2 \u2200 x\u2032 \u2208 B_p(x, \u03b5). f_\u03b8\u2032(x\u2032) = f_\u03b8(x\u2032)) \\\\] (5)\\n\\nwhere $x \u2208 X$, $X$ is a FSFP space and $v_f$ is a polynomial-time function that is 1 iff the input is a valid parameter set for $f$. Then $P_{L-ROB}^\u221e$ is $\\\\Sigma^P_2$-complete.\\n\\n**Corollary 3.8.**\\n\\n$P_{L-ROB}^p$ is $\\\\Sigma^P_2$-complete for all $0 < p \u2264 \u221e$.\\n\\n**Corollary 3.9.**\\n\\nTheorem 3.7 holds even if, instead of ReLU classifiers, we consider the more general set of polynomial-time classifiers w.r.t. the size of the tuple.\\n\\nThe $\\\\Sigma^P_2$ complexity class includes $NP$ and is conjectured to be strictly harder (as part of the Polynomial Hierarchy conjecture). In other words, if the Polynomial Hierarchy conjecture holds, robustly training a general ReLU classifier is strictly harder than attacking it. Note that our results hold in the worst-case, meaning there can be specific circumstances under which guaranteed robustness could be achieved with reasonable effort. However, in research fields where similar asymmetries are found, they tend to translate into practically meaningful difficulty gaps: for example, Quantified Boolean Formula problems (which are $\\\\Sigma^P_2$-complete) are in practice much harder to solve than pure SAT problems (which are $NP$-complete).\\n\\nWe conjecture this is also the case for our result, as it mirrors the key elements in the SAT/TQBF analogy. First, generic classifiers can learn (and are known to learn) complex input-output mappings with many local optima. Second, while attacks rely on existential quantification (finding an example), achieving robustness requires addressing a universally quantified problem (since we need to guarantee the same prediction on all neighboring points).\\n\\n### 3.4. Relevance of the Result and Related Work\\n\\nIn this section we discuss the significance of our results, both on the theoretical and the practical side.\\n\\n**Theoretical Relevance**\\n\\nAs we mentioned, results about polynomial-time attack and/or robustness certificates are available, but under restrictive assumptions. For example, Mahloujifar & Mahmoody (2019) showed that there exist exact polynomial-time attacks against classifiers trained on product distributions. Similarly, Awasthi et al. (2019) showed that for degree-2 polynomial threshold functions there exists a polynomial-time algorithm that either proves that the model is robust or finds an adversarial example. Other complexity lower bounds also exist, but again they apply under specific conditions. Degwekar et al. (2019), extending the work of Bubeck et al. (2018) and Bubeck et al. (2019), showed that there exist certain cryptography-inspired classification tasks such that learning a classifier with a robust accuracy of 99% is as hard as solving the Learning Parity with Noise problem (which is $NP$-hard).\\n\\nOn the other hand, Song et al. (2021) showed that learning a single periodic neuron over noisy isotropic Gaussian distributions in polynomial time would imply that the Shortest Vector Problem (conjectured to be $NP$-hard) can be solved in polynomial time.\\n\\nFinally, Garg et al. (2020) provided an average-case complexity analysis, by introducing assumptions on the data-generation process. In particular, by requiring attackers to provide a valid cryptographic signature for inputs, it is possible to prevent attacks with limited computational resources from fooling the model in polynomial time.\\n\\nCompared to the above results, both Theorem 3.1 and Theorem 3.7 apply to a wider class of models. In fact, to the best of our knowledge, Theorem 3.7 is the first robust training complexity bound for general ReLU classifiers.\\n\\n**Empirical Relevance**\\n\\nTheorems 3.1 and 3.7 imply that training-time defenses can be strictly (and significantly) harder than attacks. This result is consistent with a recurring...\"}"}
{"id": "marro23a", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 11. Expected vs true quantile for MNIST balanced with 5-fold cross validation.\\n\\n| Architecture | Training | Expected Quantile | True Quantile    |\\n|--------------|----------|-------------------|-----------------|\\n|              |          | 1.00%             | 1.30\u00b10.79%      |\\n|              |          | 50.00%            | 49.98\u00b13.10%     |\\n|              |          | 99.00%            | 93.99\u00b12.59%     |\\n| Standard     | Adversarial | 1.00%             | 0.97\u00b10.40%      |\\n|              |          | 50.00%            | 50.12\u00b11.14%     |\\n|              |          | 99.00%            | 90.44\u00b11.90%     |\\n| ReLU         |          | 1.00%             | 1.02\u00b10.31%      |\\n|              |          | 50.00%            | 50.02\u00b11.05%     |\\n|              |          | 99.00%            | 95.10\u00b12.82%     |\\n| Standard     | Adversarial | 1.00%             | 1.17\u00b10.97%      |\\n|              |          | 50.00%            | 50.17\u00b14.54%     |\\n|              |          | 99.00%            | 98.69\u00b10.59%     |\\n| ReLU         |          | 1.00%             | 1.04\u00b10.49%      |\\n|              |          | 50.00%            | 50.34\u00b12.49%     |\\n|              |          | 99.00%            | 98.73\u00b10.53%     |\"}"}
{"id": "marro23a", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"# Computational Asymmetries in Robust Classification\\n\\n## Table 12.\\n\\n| Architecture | Training | Expected Quantile | True Quantile |\\n|--------------|----------|-------------------|---------------|\\n| **A**        | Standard | 1.00% 1.09\u00b10.86%  | 50.00% 50.09\u00b11.84% |\\n|              | Adversarial | 1.00% 1.05\u00b10.23% | 50.00% 49.86\u00b13.59% |\\n|              | ReLU      | 1.00% 0.97\u00b10.41%  | 50.00% 49.93\u00b13.42% |\\n| **B**        | Standard | 1.00% 0.98\u00b10.18%  | 50.00% 49.91\u00b11.18% |\\n|              | Adversarial | 1.00% 0.91\u00b10.48% | 50.00% 50.00\u00b13.58% |\\n|              | ReLU      | 1.00% 1.10\u00b10.72%  | 50.00% 49.98\u00b12.21% |\\n| **C**        | Standard | 1.00% 0.93\u00b10.60%  | 50.00% 50.00\u00b11.86% |\\n|              | Adversarial | 1.00% 1.09\u00b10.17% | 50.00% 50.14\u00b12.63% |\\n|              | ReLU      | 1.00% 1.01\u00b10.62%  | 50.00% 50.02\u00b12.09% |\"}"}
{"id": "marro23a", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 13. Expected vs true quantile for CIFAR10 balanced with 5-fold cross validation.\\n\\n| Architecture | Training | Expected Quantile | True Quantile |\\n|--------------|----------|-------------------|---------------|\\n|              | 1.00%    | 0.95\u00b10.61%        | 1.00\u00b10.23%    |\\n|              | 50.00%   | 50.32\u00b12.38%       | 50.23\u00b12.65%   |\\n|              | 99.00%   | 98.87\u00b10.59%       | 98.81\u00b10.96%   |\\n| Standard     | 1.00%    | 1.07\u00b10.46%        | 1.13\u00b10.57%    |\\n|              | 50.00%   | 49.91\u00b12.78%       | 50.18\u00b12.05%   |\\n|              | 99.00%   | 98.93\u00b10.73%       | 98.82\u00b10.71%   |\\n| Adversarial  | 1.00%    | 1.07\u00b10.46%        | 1.09\u00b10.26%    |\\n|              | 50.00%   | 49.96\u00b12.72%       | 49.96\u00b11.60%   |\\n|              | 99.00%   | 98.86\u00b10.32%       | 97.93\u00b10.63%   |\\n| ReLU         | 1.00%    | 4.14\u00b15.32%        | 1.23\u00b10.38%    |\\n|              | 50.00%   | 50.37\u00b11.02%       | 50.11\u00b10.38%   |\\n|              | 99.00%   | 94.62\u00b12.87%       | 98.77\u00b10.51%   |\"}"}
{"id": "marro23a", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4. $F_1$ scores in relation to $\\\\epsilon$ for MNIST B for each considered percentile. For ease of visualization, we set the graph cutoff at $F_1 = 0.85$. We also mark $8/255$ (a common choice for $\\\\epsilon$) with a dotted line.\\n\\nFigure 5. $F_1$ scores in relation to $\\\\epsilon$ for MNIST C for each considered percentile. For ease of visualization, we set the graph cutoff at $F_1 = 0.85$. We also mark $8/255$ (a common choice for $\\\\epsilon$) with a dotted line.\"}"}
{"id": "marro23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Given a set $z = \\\\{z_{1,1}, \\\\ldots, z_{1,3}, \\\\ldots, z_{n,1}, z_{n,3}\\\\}$ of Boolean atoms (i.e. $z_{i,j}(x) = x_k$ or $\\\\neg x_k$ for a certain $k$) defined on an $n$-long Boolean vector $x$, $\\\\text{cnf}_3(z)$ returns the following Boolean function:\\n\\n$$ cnf'_3(x) = \\\\bigwedge_{i=1, \\\\ldots, n} \\\\bigvee_{j=1, \\\\ldots, 3} z_{i,j}(x) $$\\n\\nWe refer to $z$ as a 3CNF formula.\\n\\nSince $cnf'_3$ only uses negation, conjunction and disjunction, it can be implemented using respectively $\\\\neg$, $\\\\wedge$, and $\\\\vee$.\\n\\nNote that, given $z$, we can build $cnf'_3$ in polynomial time w.r.t. the size of $z$.\\n\\nComparison Functions\\n\\nWe can use $\\\\text{step}_0$, $\\\\text{step}_1$ and $\\\\neg$ to obtain comparison functions as follows:\\n\\n$$ \\\\text{geq}(x, k) = \\\\text{step}_1(x - k) $$\\n$$ \\\\text{gt}(x, k) = \\\\text{step}_0(x, k) $$\\n$$ \\\\text{leq}(x, k) = \\\\neg \\\\text{gt}(x, k) $$\\n$$ \\\\text{lt}(x, k) = \\\\neg \\\\text{geq}(x, k) $$\\n$$ \\\\text{eq}(x, k) = \\\\text{and}(\\\\text{geq}(x, k), \\\\text{leq}(x, k)) $$\\n\\nMoreover, we define $\\\\text{open}: \\\\mathbb{R}^3 \\\\rightarrow \\\\{0, 1\\\\}$ as follows:\\n\\n$$ \\\\text{open}(x, a, b) = \\\\text{and}(\\\\text{gt}(x, a), \\\\text{lt}(x, b)) $$\\n\\nB. Proof of Theorem 3.1\\n\\nB.1. $U$-ATT $\\\\in$ NP\\n\\nTo prove that $U$-ATT $\\\\in$ NP, we show that there exists a polynomial certificate for $U$-ATT that can be checked in polynomial time. The certificate is the value of $x'$, which will have a representation of the same size as $x$ (due to the FSFP space assumption) and can be checked by verifying:\\n\\n1. $\\\\|x - x'\\\\|_\\\\infty \\\\leq \\\\epsilon$, which can be checked in linear time;\\n2. $f_{\\\\theta}(x') \\\\neq f(x)$, which can be checked in polynomial time.\\n\\nB.2. $U$-ATT $\\\\in$ NP-Hard\\n\\nWe will prove that $U$-ATT $\\\\in$ NP-Hard by showing that 3SAT $\\\\leq$ U-ATT.$\\\\infty$.\\n\\nGiven a set of 3CNF clauses $z = \\\\{z_{1,1}, z_{1,2}, z_{1,3}\\\\}, \\\\ldots, \\\\{z_{m,1}, z_{m,2}, z_{m,3}\\\\}$ defined on $n$ Boolean variables $x_1, \\\\ldots, x_n$, we construct the following query $q(z)$ for $U$-ATT.$\\\\infty$:\\n\\n$$ q(z) = \\\\langle x(s), 1^2, f \\\\rangle $$\\n\\nwhere $x(s) = 1^2, \\\\ldots, 1^2$ is a vector with $n$ elements. Verifying $q(z) \\\\in U$-ATT.$\\\\infty$ is equivalent to checking:\\n\\n$$ \\\\exists x' \\\\in B^\\\\infty x(s), 1^2 \\\\cdot f(x') \\\\neq f(x(s)) $$\\n\\nNote that $x \\\\in B^\\\\infty x(s), 1^2$ is equivalent to $x \\\\in [0, 1]^n$.\\n\\nTruth Values\\n\\nWe will encode the truth values of $\\\\hat{x}$ as follows:\\n\\n$$ x'_i \\\\in 0, 1^2 \\\\Leftrightarrow \\\\hat{x}_i = 0 $$\\n$$ x'_i \\\\in 1^2, 1 \\\\Leftrightarrow \\\\hat{x}_i = 1 $$\\n\\nWe can obtain the truth value of a scalar variable by using $\\\\text{isT}(x_i) = \\\\text{gt}(x_i, 1^2)$. Let $\\\\text{bin}(x) = \\\\text{or}(\\\\text{isT}(x_1), \\\\ldots, \\\\text{isT}(x_n))$. \\n\\n13\"}"}
{"id": "marro23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Definition of $f$\\n\\nWe define $f$ as follows:\\n\\n\\\\[\\n f_1(x) = \\\\text{not} (\\\\text{isx}(s)) \\\\quad \\\\text{cnf}^{'}_3(\\\\text{bin}(x)) \\\\tag{27}\\n\\\\]\\n\\n\\\\[\\n f_0(x) = \\\\text{not} (f_1(x)) \\\\tag{28}\\n\\\\]\\n\\nwhere $\\\\text{cnf}^{'}_3 = \\\\text{cnf}_3(z)$ and $\\\\text{isx}(s)$ is defined as follows:\\n\\n\\\\[\\n \\\\text{isx}(s)(x) = \\\\text{and} eq_{x_1,1_2, \\\\ldots, eq_{x_n,1_2} } \\\\tag{29}\\n\\\\]\\n\\nNote that $f$ is designed such that $f(x(s)) = 0$, while for $x' \\\\neq x(s)$, $f(x') = 1$ iff the formula $z$ is true for the variable assignment $\\\\text{bin}(x')$.\\n\\nLemma B.1.\\n\\nLet $z \\\\in 3SAT \\\\Rightarrow q(z) \\\\in U_{\\\\infty}$\\n\\nProof. Let $z \\\\in 3SAT$. Therefore $\\\\exists x^* \\\\in \\\\{0, 1\\\\}^n$ such that $\\\\text{cnf}_3(z)(x^*) = 1$. Since $\\\\text{bin}(x^*) = x^*$ and $x^* \\\\neq x(s)$, $f(x^*) = 1$, which means that it is a valid solution for Equation (24). From this we can conclude that $q(z) \\\\in U_{\\\\infty}$.\\n\\nLemma B.2.\\n\\n$q(z) \\\\in U_{\\\\infty} = \\\\Rightarrow z \\\\in 3SAT$\\n\\nProof. Since $q(z) \\\\in U_{\\\\infty}$, $\\\\exists x^* \\\\in [0, 1]^n \\\\setminus x(s)$ that is a solution to Equation (24) (i.e. $f(x^*) = 1$). Then $\\\\text{cnf}^{'}_3(\\\\text{bin}(x^*)) = 1$, which means that there exists a $\\\\hat{x}$ (i.e. $\\\\text{bin}(x^*)$) such that $\\\\text{cnf}^{'}_3(\\\\hat{x}) = 1$. From this we can conclude that $z \\\\in 3SAT$.\\n\\nSince:\\n\\n- $q(z)$ can be computed in polynomial time;\\n- $z \\\\in 3SAT = \\\\Rightarrow q(z) \\\\in U_{\\\\infty};$\\n- $q(z) \\\\in U_{\\\\infty} = \\\\Rightarrow z \\\\in 3SAT$.\\n\\nwe can conclude that $3SAT \\\\leq U_{\\\\infty}$. \\n\\nB.3. Proof of Corollary 3.2\\n\\nB.3.1. $U_{\\\\infty} \\\\in \\\\text{NP}$\\n\\nThe proof is identical to the one for $U_{\\\\infty}$.\\n\\nB.3.2. $U_{\\\\infty} \\\\text{IS} \\\\text{NP-HARD}$\\n\\nThe proof that $q(z) \\\\in U_{\\\\infty} = \\\\Rightarrow z \\\\in 3SAT$ is very similar to the one for $U_{\\\\infty}$. Since $q(z) \\\\in U_{\\\\infty}$, we know that $\\\\exists x^* \\\\in B_p(x(s), \\\\epsilon) \\\\setminus x(s)$. $f(x^*) = 1$, which means that there exists a $\\\\hat{x}$ (i.e. $\\\\text{bin}(x^*)$) such that $\\\\text{cnf}^{'}_3(\\\\hat{x}) = 1$. From this we can conclude that $z \\\\in 3SAT$. \\n\\nThe proof that $z \\\\in 3SAT = \\\\Rightarrow q(z) \\\\in U_{\\\\infty}$ is slightly different, due to the fact that since $x^* \\\\neq x(s)$ we need to use a different input to prove that $\\\\exists x' \\\\in B_p(x(s))$. $f(x') = 1$.\\n\\nLet $0 < p < \\\\infty$. Given a positive integer $n$ and a real $0 < p < \\\\infty$, let $\\\\rho_{p,n}(r)$ be a positive minorant of the $L_\\\\infty$ norm of a vector on the $L_p$ sphere of radius $r$. For example, for $n = 2$, $p = 2$ and $r = 1$, any positive value less than or equal to $\\\\sqrt{2}^2$ is suitable. Note that, for $0 < p < \\\\infty$ and $n, r > 0$, $\\\\rho_{p,n}(r) < r$. \\n\\nLet $z \\\\in 3SAT$. Therefore $\\\\exists x^* \\\\in \\\\{0, 1\\\\}^n$ such that $\\\\text{cnf}_3(z)(x^*) = 1$. Let $x^{**}$ be defined as:\\n\\n\\\\[\\n x^{**}_i = \\\\begin{cases} \\n 1, & x_i = 0 \\\\frac{1}{2} - \\\\rho_{p,n}(\\\\frac{1}{2}) \\\\\\\\\\n 0, & x_i = 1 \\\\frac{1}{2} + \\\\rho_{p,n}(\\\\frac{1}{2})\\n \\\\end{cases} \\\\tag{30}\\n\\\\]\"}"}
{"id": "marro23a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"By construction, $x^{**} \\\\in B_p$. Additionally, $\\\\text{bin}(x^{**}) = x^*$, and since we know that $z$ is true for the variable assignment $x^*$, we can conclude that $f(x^{**}) = 1$, which means that $x^{**}$ is a valid solution for Equation (24). From this we can conclude that $q(z) \\\\in U_{\\\\text{ATT}}^p$.\\n\\nB.4. Proof of Corollary 3.3\\nThe proof is identical to the proof of Theorem 3.1 (for $p = \\\\infty$) and Corollary 3.2 (for $0 < p < \\\\infty$), with the exception of requiring $f(x') = 1$.\\n\\nB.5. Proof of Corollary 3.4\\nThe proof that attacking a polynomial-time classifier is in $\\\\text{NP}$ is the same as that for Theorem 3.1. Attacking a polynomial-time classifier is $\\\\text{NP}$-hard due to the fact that the ReLU networks defined in the proof of Theorem 3.1 are polynomial-time classifiers. Since attacking a general polynomial-time classifier is a generalization of attacking a ReLU polynomial-time classifier, the problem is $\\\\text{NP}$-hard.\\n\\nC. Proof of Theorem 3.5\\nProving that $U_{\\\\text{ATT}}^p(f) \\\\in \\\\text{NP}$ means proving that it can be solved in polynomial time by a non-deterministic Turing machine with an oracle that can solve a problem in $A$. Since $Z_f \\\\in A$, we can do so by picking a non-deterministic Turing machine with access to an oracle that solves $Z_f$. We then generate non-deterministically the adversarial example and return the output of the oracle. Due to the FSFP assumption, we know that the size of this input is the same as the size of the starting point, which means that it can be generated non-deterministically in polynomial time. Therefore, $U_{\\\\text{ATT}}^p(f) \\\\in \\\\text{NP}$.\\n\\nC.1. Proof of Corollary 3.6\\nFollows directly from Theorem 3.5 and the definition of $\\\\Sigma_p^n$.\\n\\nD. Proof of Theorem 3.7\\nD.1. Preliminaries\\n$\\\\Pi_2^3\\\\text{SAT}$ is the set of all $z$ such that:\\n$$\\\\forall \\\\hat{x} \\\\exists \\\\hat{y}. R(\\\\hat{x}, \\\\hat{y})$$\\nwhere $R(\\\\hat{x}, \\\\hat{y}) = \\\\text{cnf}_3(z)(\\\\hat{x}_1, \\\\ldots, \\\\hat{x}_n, \\\\hat{y}_1, \\\\ldots, \\\\hat{y}_n)$.\\n\\nStockmeyer (1976) showed that $\\\\Pi_2^3\\\\text{SAT}$ (also known as $\\\\forall\\\\exists^+_3\\\\text{SAT}$) is $\\\\Pi_2^p$-complete. Therefore, $\\\\text{co}\\\\Pi_2^3\\\\text{SAT}$, which is defined as the set of all $z$ such that:\\n$$\\\\exists \\\\hat{x} \\\\forall \\\\hat{y}. \\\\neg R(\\\\hat{x}, \\\\hat{y})$$\\nis $\\\\Sigma_2^p$-complete.\\n\\nD.2.\\n$\\\\text{PL-ROB}_{\\\\infty} \\\\in \\\\Sigma_2^p$ if there exists a problem $A \\\\in \\\\text{P}$ and a polynomial $q$ such that:\\n$$\\\\forall \\\\Gamma = \\\\langle x, \\\\varepsilon, f_\\\\theta, v_f \\\\rangle: \\\\Gamma \\\\in \\\\text{PL-ROB} \\\\iff \\\\exists y. |y| \\\\leq q(|\\\\Gamma|) \\\\land (\\\\forall z. (|z| \\\\leq q(|\\\\Gamma|) = \\\\implies \\\\langle \\\\Gamma, y, z \\\\rangle \\\\in A))$$\\nThis can be proven by setting $y = \\\\theta'$, $z = x'$, and $A$ as the set of triplets $\\\\langle \\\\Gamma, \\\\theta', x' \\\\rangle$ such that all of the following are true:\\n- $v_f(\\\\theta') = 1$;\\n- $\\\\|x - x'\\\\|_\\\\infty \\\\leq \\\\varepsilon$;\\n- $f_\\\\theta(x) = f_\\\\theta(x')$.\\nSince all properties can be checked in polynomial time, $A \\\\in \\\\text{P}$ and thus $\\\\text{PL-ROB}_{\\\\infty} \\\\in \\\\Sigma_2^p$. \\n\\n15\"}"}
{"id": "marro23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We will prove that $P_L$-\\\\textsc{Rob}$\\\\infty$ is $\\\\Sigma_2^P$-hard by showing that $\\\\text{co}\\\\Pi_3^P \\\\leq P_L$-\\\\textsc{Rob}$\\\\infty$.\\n\\nLet $n^{\\\\hat{x}}$ be the length of $\\\\hat{x}$ and let $n^{\\\\hat{y}}$ be the length of $\\\\hat{y}$.\\n\\nGiven a set $z$ of 3CNF clauses, we construct the following query $q(z)$ for $P_L$-\\\\textsc{Rob}$\\\\infty$:\\n\\n$$q(z) = \\\\langle x(s), 1_2, f_{\\\\theta}, v_f \\\\rangle$$\\n\\nwhere $x(s) = 1_2, \\\\ldots, 1_2$ is a vector with $n^{\\\\hat{y}}$ elements and $v_f(\\\\theta) = 1 \\\\iff \\\\theta \\\\in \\\\{0, 1\\\\}^{n^{\\\\hat{x}}}$.\\n\\nNote that $\\\\theta' \\\\in \\\\{0, 1\\\\}^{n^{\\\\hat{x}}}$ can be checked in polynomial time w.r.t. the size of the input.\\n\\nTruth Values\\n\\nWe will encode the truth values of $\\\\hat{x}$ as a set of binary parameters $\\\\theta'$, while we will encode the truth values of $\\\\hat{y}$ using $x'$ through the same technique mentioned in Appendix B.2.\\n\\nDefinition of $f_{\\\\theta}$\\n\\nWe define $f_{\\\\theta}$ as follows:\\n\\n- $f_{\\\\theta}, 1(x) = \\\\text{not isx}(s)(x)$, $\\\\text{cnf}_{\\\\theta}(x)$, where $\\\\text{cnf}_{\\\\theta}$ is defined over $\\\\theta$ and $\\\\text{bin}(x)$ using the same technique mentioned in Appendix B.2 and $\\\\text{isx}(s)(x) = \\\\text{and i=1,\\\\ldots,n}_{\\\\text{eq}}(x_i, 1_2)$;\\n\\n- $f_{\\\\theta}, 0(x) = \\\\text{not} (f_{\\\\theta}, 1(x))$.\\n\\nNote that $f_{\\\\theta}(x(s)) = 0$ for all choices of $\\\\theta$. Additionally, $f_{\\\\theta}$ is designed such that:\\n\\n$$\\\\forall x' \\\\in B_{\\\\infty}(x(s), \\\\varepsilon). f_{\\\\theta}(x') = 0 \\\\iff R(\\\\theta', \\\\text{bin}(x'))$$\\n\\nLemma D.1.\\n\\n$z \\\\in \\\\text{co}\\\\Pi_2^3$SAT $\\\\implies q(z) \\\\in P_L$-\\\\textsc{Rob}$\\\\infty$\\n\\nProof.\\n\\nSince $z \\\\in \\\\text{co}\\\\Pi_2^3$SAT, there exists a Boolean vector $x^*$ such that $\\\\forall \\\\hat{y}. \\\\neg R(x^*, \\\\hat{y})$.\\n\\nThen both of the following statements are true:\\n\\n- $v_f(x^*) = 1$, since $x^* \\\\in \\\\{0, 1\\\\}^{n^{\\\\hat{x}}}$;\\n\\n- $\\\\forall x' \\\\in B_{\\\\infty}(x(s), \\\\varepsilon). f_{\\\\theta}(x') = 0$, since $f_{\\\\theta}(x') = 1 \\\\iff R(\\\\theta', \\\\text{bin}(x'))$;\\n\\nTherefore, $x^*$ is a valid solution for Equation (5) and thus $q(z) \\\\in P_L$-\\\\textsc{Rob}$\\\\infty$.\\n\\nLemma D.2.\\n\\n$q(z) \\\\in P_L$-\\\\textsc{Rob}$\\\\infty$ $\\\\implies z \\\\in \\\\text{co}\\\\Pi_2^3$SAT\\n\\nProof.\\n\\nSince $q(z) \\\\in P_L$-\\\\textsc{Rob}$\\\\infty$, there exists a $\\\\theta^*$ such that:\\n\\n$$v_f(\\\\theta^*) = 1 \\\\land \\\\forall x' \\\\in B_{\\\\infty}(x(s), \\\\varepsilon). f_{\\\\theta^*}(x') = f_{\\\\theta^*}(x(s))$$\\n\\nNote that $\\\\theta^* \\\\in \\\\{0, 1\\\\}^{n^{\\\\hat{x}}}$, since $v_f(\\\\theta^*) = 1$. Moreover, $\\\\forall \\\\hat{y}. \\\\neg R(\\\\theta^*, \\\\hat{y})$, since $\\\\text{bin}(\\\\hat{y}) = \\\\hat{y}$ and $f_{\\\\theta^*}(\\\\hat{y}) = 1 \\\\iff R(\\\\theta^*, \\\\hat{y})$.\\n\\nTherefore, $\\\\theta^*$ is a valid solution for Equation (32), which implies that $z \\\\in \\\\text{co}\\\\Pi_2^3$SAT.\\n\\nSince:\\n\\n- $q(z)$ can be computed in polynomial time;\\n\\n- $z \\\\in \\\\text{co}\\\\Pi_2^3$SAT $\\\\implies q(z) \\\\in P_L$-\\\\textsc{Rob}$\\\\infty$;\\n\\n- $q(z) \\\\in P_L$-\\\\textsc{Rob}$\\\\infty$ $\\\\implies z \\\\in \\\\text{co}\\\\Pi_2^3$SAT,\\n\\nwe can conclude that $\\\\text{co}\\\\Pi_2^3$SAT $\\\\leq P_L$-\\\\textsc{Rob}$\\\\infty$. \\n\\n[16]\"}"}
{"id": "marro23a", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nH. Full Experimental Setup\\n\\nAll our code is written in Python + PyTorch (Paszke et al., 2019), with the exception of the MIPVerify interface, which is written in Julia. When possible, most experiments were run in parallel, in order to minimize execution times.\\n\\nModels\\n\\nAll models were trained using Adam (Kingma & Ba, 2014) and dataset augmentation. We performed a manual hyperparameter and architecture search to find a suitable compromise between accuracy and MIPVerify convergence. The process required approximately 4 months. When performing adversarial training, following (Madry et al., 2018) we used the final adversarial example found by the Projected Gradient Descent attack, instead of the closest. To maximize uniformity, we used for each configuration the same training and pruning hyperparameters (when applicable), which we report in Table 1.\\n\\nWe report the chosen architectures in Tables 2 and 3, while Table 4 outlines their accuracies and parameter counts.\\n\\nUG100\\n\\nThe first 250 samples of the test set of each dataset were used for hyperparameter tuning and were thus not considered in our analysis. For our G100 dataset, we sampled uniformly across each ground truth label and removed the examples for which MIPVerify crashed. Table 5 details the composition of the dataset by ground truth label.\\n\\nAttacks\\n\\nFor the Basic Iterative Method (BIM), the Fast Gradient Sign Method (FGSM) and the Projected Gradient Descent (PGD) attack, we used the implementations provided by the AdverTorch library (Ding et al., 2019). For the Brendel & Bethge (B&B) attack and the Deepfool (DF) attack, we used the implementations provided by the Foolbox Native library (Rauber et al., 2020). The Carlini & Wagner and the uniform noise attacks were instead implemented by the authors. We modified the attacks that did not return the closest adversarial example found (i.e. BIM, Carlini & Wagner, Deepfool, FGSM and PGD) to do so. For the attacks that accept $\\\\epsilon$ as a parameter (i.e. BIM, FGSM, PGD and uniform noise), for each example we first performed an initial search with a decaying value of $\\\\epsilon$, followed by a binary search. In order to pick the attack parameters, we first selected the strong set by performing an extensive manual search. The process took approximately 3 months. We then modified the strong set in order to obtain the balanced parameter set. We report the parameters of both sets (as well as the parameters of the binary and $\\\\epsilon$ decay searches) in Table 6.\\n\\nMIPVerify\\n\\nWe ran MIPVerify using the Julia library MIPVerify.jl and Gurobi (Gurobi Optimization, LLC, 2022). Since MIPVerify can be sped up by providing a distance upper bound, we used the same pool of adversarial examples utilized throughout the paper. For CIFAR10 we used the strong parameter set, while for MNIST we used the strong parameter set with some differences (reported in Table 7). Since numerical issues might cause the distance upper bound computed by the heuristic attacks to be slightly different from the one computed by MIPVerify, we ran a series of exploratory runs, each with a different correction factor (1.05, 1.25, 1.5, 2), and picked the first factor that caused MIPVerify to find a feasible (but not necessarily optimal) solution. If the solution was not optimal, we then performed a main run with a higher computational budget. We provide the parameters of MIPVerify in Table 8. We also report in Table 9 the percentage of tight bounds for each combination.\"}"}
{"id": "marro23a", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1. Training and pruning hyperparameters.\\n\\n| Parameter Name | Value |\\n|----------------|-------|\\n| **MNIST**     |       |\\n| **CIFAR10**   |       |\\n| **Common Hyperparameters** |       |\\n| Epochs        | 425   |\\n| Learning Rate | 1e-4  |\\n| Batch Size    | 32 128 |\\n| Adam $\\\\beta$  | (0.9, 0.999) |\\n| Flip %        | 50%   |\\n| Translation Ratio | 0.1   |\\n| Rotation (deg.) | 15\u00b0   |\\n| **Adversarial Hyperparameters (Adversarial and ReLU only)** |       |\\n| Attack PGD    |       |\\n| Attack #Iterations | 200   |\\n| Attack Learning Rate | 0.1   |\\n| Adversarial Ratio | 1   |\\n| $\\\\varepsilon$ | 0.05 2/255 |\\n| **ReLU Hyperparameters (ReLU only)** |       |\\n| L1 Regularization Coeff. | 2e-5 1e-5 |\\n| RS Loss Coeff. | 1.2e-4 1e-3 |\\n| Weight Pruning Threshold | 1e-3 |\\n| ReLU Pruning Threshold | 90% |\\n\\n### Table 2. MNIST Architectures.\\n\\n| Architecture | Input | Flatten | Linear (in = 784, out = 100) | ReLU | Linear (in = 100, out = 10) | Output |\\n|--------------|-------|---------|-----------------------------|------|-----------------------------|--------|\\n| MNIST A      |       |         |                             |      |                             |        |\\n| MNIST B      | Conv2D (in = 1, out = 4, 5x5 kernel, stride = 3, padding = 0) | ReLU | Flatten | Linear (in = 256, out = 10) | Output |\\n| MNIST C      | Conv2D (in = 1, out = 8, 5x5 kernel, stride = 4, padding = 0) | ReLU | Flatten | Linear (in = 288, out = 10) | Output |\"}"}
{"id": "marro23a", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. CIFAR10 architectures.\\n\\n(a) CIFAR10 A\\nInput\\nConv2D (in = 3, out = 8, 3x3 kernel, stride = 2, padding = 0)\\nReLU\\nFlatten\\nLinear (in = 1800, out = 10)\\nOutput\\n\\n(b) CIFAR10 B\\nInput\\nConv2D (in = 3, out = 20, 5x5 kernel, stride = 4, padding = 0)\\nReLU\\nFlatten\\nLinear (in = 980, out = 10)\\nOutput\\n\\n(c) CIFAR10 C\\nInput\\nConv2D (in = 3, out = 8, 5x5 kernel, stride = 4, padding = 0)\\nReLU\\nConv2D (in = 8, out = 8, 3x3 kernel, stride = 2, padding = 0)\\nReLU\\nFlatten\\nLinear (in = 72, out = 10)\\nOutput\\n\\nTable 4. Parameter counts and accuracies of trained models.\\n\\n| Architecture | Parameters | Training Accuracy |\\n|--------------|------------|-------------------|\\n| MNIST A      | 79510      | Standard 95.87%   |\\n|              |            | Adversarial 94.24%|\\n|              |            | ReLU 93.57%       |\\n| MNIST B      | 2674       | Standard 89.63%   |\\n|              |            | Adversarial 84.54%|\\n|              |            | ReLU 83.69%       |\\n| MNIST C      | 3098       | Standard 90.71%   |\\n|              |            | Adversarial 87.35%|\\n|              |            | ReLU 85.67%       |\\n| CIFAR10 A    | 18234      | Standard 53.98%   |\\n|              |            | Adversarial 50.77%|\\n|              |            | ReLU 32.85%       |\\n| CIFAR10 B    | 11330      | Standard 55.81%   |\\n|              |            | Adversarial 51.35%|\\n|              |            | ReLU 37.33%       |\\n| CIFAR10 C    | 1922       | Standard 47.85%   |\\n|              |            | Adversarial 45.19%|\\n|              |            | ReLU 32.27%       |\"}"}
{"id": "marro23a", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset    | Class      | Count | Percentage |\\n|------------|------------|-------|------------|\\n| MNIST      | 0          | 219   | 9.77%      |\\n| MNIST      | 1          | 228   | 10.17%     |\\n| MNIST      | 2          | 225   | 10.04%     |\\n| MNIST      | 3          | 225   | 10.04%     |\\n| MNIST      | 4          | 225   | 10.04%     |\\n| MNIST      | 5          | 220   | 9.82%      |\\n| MNIST      | 6          | 227   | 10.13%     |\\n| MNIST      | 7          | 221   | 9.86%      |\\n| MNIST      | 8          | 225   | 10.04%     |\\n| MNIST      | 9          | 226   | 10.08%     |\\n| CIFAR10    | Airplane   | 228   | 10.05%     |\\n| CIFAR10    | Automobile | 227   | 10.00%     |\\n| CIFAR10    | Bird       | 228   | 10.05%     |\\n| CIFAR10    | Cat        | 228   | 10.05%     |\\n| CIFAR10    | Deer       | 226   | 9.96%      |\\n| CIFAR10    | Dog        | 227   | 10.00%     |\\n| CIFAR10    | Frog       | 227   | 10.00%     |\\n| CIFAR10    | Horse      | 227   | 10.00%     |\\n| CIFAR10    | Ship       | 225   | 9.92%      |\\n| CIFAR10    | Truck      | 226   | 9.96%      |\"}"}
{"id": "marro23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.4. Proof of Corollary 3.8\\n\\nD.4.1. $P_{L ROB P}$\\n\\nThe proof is identical to the one for $P_{L ROB \u221e}$.\\n\\nD.4.2. $P_{L ROB p}$\\n\\nWe follow the same approach used in the proof for Corollary 3.2.\\n\\nProof of $q(z) \\\\in P_{L ROB p} =\u21d2 z \\\\in co \\\\\u03a0^2 3 SAT$\\n\\nIf $q(z) \\\\in P_{L ROB p}$, it means that $\u2203 \\\\theta^\u2217. v_f(\\\\theta^\u2217) = 1 =\u21d2 \u2200 x\u2032 \u2208 B_p x(s)$, $f(x\u2032) = 0$. Then $\u2200 \u02c6y$, there exists a corresponding input $y^{\u2217\u2217} \u2208 B_p x(s)$, defined as follows:\\n\\n$$y^{\u2217\u2217}_i = \\\\begin{cases} \\\\frac{1}{2} - \\\\rho_{p,n} & \\\\text{if } \u02c6y_i = 0 \\\\\\\\ \\\\frac{1}{2} + \\\\rho_{p,n} & \\\\text{if } \u02c6y_i = 1 \\\\end{cases} \\\\quad (37)$$\\n\\nsuch that $e(y)(y^{\u2217\u2217}) = \u02c6y$. Since $y^{\u2217\u2217} \u2208 B_p x(s)$, $cnf\u2032\u2032_3(\u03b8^\u2217, bin(y^{\u2217\u2217})) = 0$, which means that $R(\u03b8^\u2217, \u02c6y)$ is false. In other words, $\u2203 \\\\theta^\u2217. \u2200 \u02c6y. \u00acR(\u03b8^\u2217, \u02c6y)$, i.e. $z \\\\in co \\\\\u03a0^2 3 SAT$.\\n\\nProof of $z \\\\in co \\\\\u03a0^2 3 SAT =\u21d2 q(z) \\\\in P_{L ROB p}$\\n\\nThe proof is very similar to the corresponding one for Theorem 3.7.\\n\\nIf $z \\\\in co \\\\\u03a0^2 3 SAT$, then $\u2203 \u02c6x^\u2217. \u2200 \u02c6y. \u00acR(\u02c6x^\u2217, \u02c6y)$. Set $\u03b8^\u2217 = \u02c6x^\u2217$. We know that $f^\u03b8(x(s)) = 0$. We also know that $\u2200 x\u2032 \u2208 B_p x(s)$, $f^\u03b8(x\u2032) = 1 \u21d4 cnf\u2032\u2032_3(\u03b8^\u2217, x\u2032) = 0$. In other words, $\u2200 x\u2032 \u2208 B_p x(s)$, $f^\u03b8(x\u2032) = 1 \u21d4 R(\u03b8^\u2217, bin(x\u2032))$. Since $R(\u03b8^\u2217, \u02c6y)$ is false for all choices of \u02c6y, $\u2200 x\u2032 \u2208 B_p x(s)$, $f^\u03b8(x\u2032) = 0$. Given the fact that $f^\u03b8(x(s)) = 0$, we can conclude that $\u03b8^\u2217$ satisfies Equation (5).\\n\\nD.5. Proof of Corollary 3.9\\n\\nSimilarly to the proof of Corollary 3.4, it follows from the fact that ReLU classifiers are polynomial-time classifiers (w.r.t. the size of the tuple).\\n\\nE. Proof of Theorem 4.1\\n\\nThere are two cases:\\n\\n\u2022 $\u2200 x\u2032 \u2208 B_p (x, \u03b5). f(x\u2032) = f(x)$: then the attack fails because $f(x) \\\\notin C(x)$;\\n\\n\u2022 $\u2203 x\u2032 \u2208 B_p (x, \u03b5). f(x\u2032) \\\\neq f(x)$: then due to the symmetry of the $L_p$ norm $x \u2208 B_p (x\u2032, \u03b5)$. Since $f(x) \\\\neq f(x\u2032)$, $x$ is a valid adversarial example for $x\u2032$, which means that $f(x\u2032) = \u22c6$. Since $\u22c6 \\\\notin C(x)$, the attack fails.\\n\\nE.1. Proof of Corollary 4.2\\n\\nAssume that $\u2200 x. ||x||_r \u2265 \u03b7 ||x||_p$ and fix $x(s) \u2208 X$. Let $x\u2032 \u2208 B_r (x(s), \u03b7\u03b5)$ be an adversarial example. Then $||x\u2032 - x(s)||_r \u2264 \u03b7\u03b5$, and thus $\u03b7 ||x\u2032 - x(s)||_p \u2264 \u03b7\u03b5$. Dividing by $\u03b7$, we get $||x\u2032 - x(s)||_p \u2264 \u03b5$, which means that $x(s)$ is a valid adversarial example for $x\u2032$ and thus $x\u2032$ is rejected by $p$-CA.\\n\\nWe now proceed to find the values of $\u03b7$.\\n\\nE.1.1. $1 \u2264 r < p$\\n\\nWe will prove that $||x||_r \u2265 ||x||_p$.\\n\\nCase $p < \u221e$\\n\\nConsider $e = ||x||_p$. $e$ is such that $||e||_p = 1$ and for all $i$ we have $|e_i| \u2264 1$. Since $r < p$, for all $0 \u2264 t \u2264 1$ we have $|t|^p \u2264 |t|^r$. Therefore:\\n\\n$$||e||_r = \\\\sum_{i=1}^{n} |e_i|^r \\\\geq \\\\sum_{i=1}^{n} |e_i|^p \\\\left(\\\\frac{1}{r}\\\\right)^n = ||e||_p/r = 1 \\\\quad (38)$$\\n\\n17\"}"}
{"id": "marro23a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nThen, since \\\\(||e||_r \\\\geq 1\\\\):\\n\\n\\\\[ ||x||_r = ||x||_p \\\\]\\n\\nCase \\\\(p = \\\\infty\\\\)\\n\\nSince \\\\(||x||_r \\\\geq ||x||_p\\\\) for all \\\\(r < p\\\\) and since the expressions on both sides of the inequality are compositions of continuous functions, as \\\\(p \\\\to \\\\infty\\\\) we get \\\\(||x||_r = ||x||_\\\\infty\\\\).\\n\\nE.1.2. \\\\(r > p\\\\)\\n\\nWe will prove that \\\\(||x||_r \\\\geq n^{1/r - 1/p}||x||_p\\\\).\\n\\nCase \\\\(r < \\\\infty\\\\)\\n\\nH\u00f6lder\u2019s inequality states that, given \\\\(\\\\alpha, \\\\beta \\\\geq 1\\\\) such that \\\\(1/\\\\alpha + 1/\\\\beta = 1\\\\) and given \\\\(f\\\\) and \\\\(g\\\\), we have:\\n\\n\\\\[ ||fg||_1 \\\\leq ||f||_\\\\alpha ||g||_\\\\beta \\\\] (40)\\n\\nSetting \\\\(\\\\alpha = r/r - p, \\\\beta = r/p, f = (1, \\\\ldots, 1)\\\\) and \\\\(g = (x^p_1, \\\\ldots, x^p_n)\\\\), we know that:\\n\\n\u2022 \\\\(||fg||_1 = \\\\sum_{i=1}^n (1 \\\\cdot x^p_i) = ||x||_p^p\\\\);\\n\\n\u2022 \\\\(||f||_\\\\alpha = (\\\\sum_{i=1}^n 1)^{1/\\\\alpha} = n^{1/\\\\alpha}\\\\);\\n\\n\u2022 \\\\(||g||_\\\\beta = \\\\left( \\\\sum_{i=1}^n x^{pr/p}_i \\\\right)^{p/r} = ||x||^p_r\\\\).\\n\\nTherefore \\\\(||x||_p^p \\\\leq n^{1/\\\\alpha}||x||_r^p\\\\). Raising both sides to the power of \\\\(1/p\\\\), we get \\\\(||x||_r^p \\\\leq n^{1/p}\\\\alpha ||x||_r^p\\\\). Therefore:\\n\\n\\\\[ ||x||_r^p \\\\leq n^{(r/p) - (p/r)}||x||_r^p = n^{r - p/\\\\alpha}||x||_r^p \\\\] (41)\\n\\nDividing by \\\\(n^{r - p/\\\\alpha}||x||_r^p\\\\) we get:\\n\\n\\\\[ n^{r - p/\\\\alpha}||x||_r^p \\\\leq ||x||_r^p \\\\] (42)\\n\\nCase \\\\(r = \\\\infty\\\\)\\n\\nSince the expressions on both sides of the inequality are compositions of continuous functions, as \\\\(r \\\\to \\\\infty\\\\) we get \\\\(||x||_r = ||x||_\\\\infty\\\\).\\n\\nF. Proof of Theorem 4.3\\n\\nF.1. CCA \\\\(\\\\infty \\\\in \\\\Sigma P^2\\\\)\\n\\nCCA \\\\(\\\\infty \\\\in \\\\Sigma P^2\\\\) iff there exists a problem \\\\(A \\\\in P\\\\) and a polynomial \\\\(p\\\\) such that \\\\(\\\\forall \\\\Gamma = \\\\langle x, \\\\varepsilon, \\\\varepsilon', C, f \\\\rangle: \\\\Gamma \\\\in \\\\text{CCA}_\\\\infty \\\\iff \\\\exists y. ||y|| \\\\leq p(||\\\\Gamma||) \\\\land (\\\\forall z. (||z|| \\\\leq p(||\\\\Gamma||) \\\\implies \\\\langle \\\\Gamma, y, z \\\\rangle \\\\in A))\\\\) (43)\\n\\nThis can be proven by setting \\\\(y = x'\\\\), \\\\(z = x''\\\\) and \\\\(A\\\\) as the set of all triplets \\\\(\\\\langle \\\\Gamma, x', x'' \\\\rangle\\\\) such that all of the following are true:\\n\\n\u2022 \\\\(\\\\|x - x'\\\\|_\\\\infty \\\\leq \\\\varepsilon'\\\\)\\n\\n\u2022 \\\\(f(x') \\\\in C(x)\\\\)\\n\\n\u2022 \\\\(\\\\|x'' - x'\\\\|_\\\\infty \\\\leq \\\\varepsilon\\\\)\\n\\n\u2022 \\\\(f(x'') = f(x')\\\\)\\n\\nSince all properties can be checked in polynomial time, \\\\(A \\\\in P\\\\).\"}"}
{"id": "marro23a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nCCA^\u221e is \\\\Sigma^P_{2}-Hard\\n\\nWe will show that CCA^\u221e is \\\\Sigma^P_{2}-hard by proving that co\\\\Pi^P_{3} SAT \\\\leq CCA^\u221e.\\n\\nFirst, suppose that the length of \\\\hat{x} and \\\\hat{y} differ. In that case, we pad the shortest one with additional variables that will not be used.\\n\\nLet n be the maximum of the lengths of \\\\hat{x} and \\\\hat{y}.\\n\\nGiven a set z of 3CNF clauses, we construct the following query q(z) for CCA^\u221e:\\n\\nq(z) = \u27e8x(s), \u03b3, 1/2, C_u, h\u27e9\\n\\nwhere 1/4 < \u03b3 < 1/2 and x(s) = \\\\frac{1}{2}, \\\\ldots, \\\\frac{1}{2} is a vector with n elements. Verifying q(z) \u2208 CCA^\u221e is equivalent to checking:\\n\\n\u2203x' \u2208 B_x \\\\cup \\\\{0, 1\\\\}^n.\\n\\nh(x') \\\\neq h(x) \u2227 \u2200x'' \u2208 B_x \\\\cup \\\\{0, 1\\\\}^n.\\n\\nh(x'') = h(x')\\n\\nNote that x' \u2208 [0, 1]^n.\\n\\nTruth Values\\n\\nWe will encode the truth values of \\\\hat{x} and \\\\hat{y} as follows:\\n\\nx''_i \\\\in [0, \\\\frac{1}{4}] \u21d0\u21d2 \\\\hat{x}_i = 0 \u2227 \\\\hat{y}_i = 0\\n\\nx''_i \\\\in [\\\\frac{1}{4}, 1/2] \u21d0\u21d2 \\\\hat{x}_i = 0 \u2227 \\\\hat{y}_i = 1\\n\\nx''_i \\\\in [\\\\frac{1}{2}, 3/4] \u21d0\u21d2 \\\\hat{x}_i = 1 \u2227 \\\\hat{y}_i = 0\\n\\nx''_i \\\\in [3/4, 1] \u21d0\u21d2 \\\\hat{x}_i = 1 \u2227 \\\\hat{y}_i = 1\\n\\nLet e_{\\\\hat{x}}(x) = gt(x_i, 1/2). Let:\\n\\ne_{\\\\hat{y}}(x) = or\\\\ i = 1, ..., n\\\\ or\\\\ (x_i, 1/4, 1/2), open\\\\ (x_i, 3/4, 1)\\n\\nNote that e_{\\\\hat{x}}(x''_i) returns the truth value of \\\\hat{x}_i and e_{\\\\hat{y}}(x''_i) returns the truth value of \\\\hat{y}_i (as long as the input is within one of the ranges described in Equation (46)).\\n\\nInvalid Encodings\\n\\nAll the encodings other than the ones described in Equation (46) are not valid. We define inv as follows:\\n\\ninv_F(x) = or\\\\ i = 1, ..., n\\\\ or\\\\ (out(x_i), edge(x_i))\\n\\nwhere out(x_i) = or\\\\ (leq(x_i, 0), geq(x_i, 1)) and edge(x_i) = or\\\\ eq(x_i, 1/4), eq(x_i, 1/2), eq(x_i, 3/4)\\n\\nOn the other hand, we define inv_T as follows:\\n\\ninv_T(x) = eq(x_i, 1/2)\\n\\n19\"}"}
{"id": "marro23a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nDefinition of h\\n\\nLet g be a Boolean formula defined over e(x) and e(y) that returns the value of R (using the same technique as cnf\u20323).\\n\\nWe define h as a two-class classifier, where:\\n\\n$$h_1(x) = \\\\text{or} \\\\left( \\\\text{inv}T(x), \\\\neg (\\\\text{inv}F(x)) \\\\right), g(x)$$ (51)\\n\\nand $$h_0(x) = \\\\neg h_1(x)$$.\\n\\nNote that:\\n\\n- If $$x_i = 1$$ for some i, the top class is 1; therefore, $$h(x(s)) = 1$$;\\n- Otherwise, if x is not a valid encoding, the top class is 0;\\n- Otherwise, the top class is 1 if $$R(e(x), e(y))$$ is true and 0 otherwise.\\n\\nLemma F.1.\\n\\n$$z \\\\in \\\\text{co} \\\\Pi_3^2 \\\\Rightarrow q(z) \\\\in \\\\text{CCA}_\\\\infty$$\\n\\nProof.\\n\\nIf $$z \\\\in \\\\text{co} \\\\Pi_3^2$$, then there exists a Boolean vector $$x^*$$ such that $$\\\\forall \\\\hat{y}. \\\\neg R(x^*, \\\\hat{y})$$.\\n\\nWe now prove that setting $$x' = x^*$$ satisfies Equation (7). First, note that $$h(x^*) = 0$$, which satisfies $$h(x') \\\\neq h(x)$$. Then we need to verify that $$\\\\forall x'' \\\\in B_\\\\infty(x^*, \\\\gamma). h(x) = 0$$.\\n\\nFor every $$x'' \\\\in B_\\\\infty(x^*, \\\\gamma)$$, we know that $$x'' \\\\in [0, \\\\frac{1}{2} - \\\\gamma) \\\\cup (1 + \\\\gamma, 1]$$.\\n\\nThere are thus two cases:\\n\\n- $$x''$$ is not a valid encoding, i.e. $$x''_i \\\\leq 0 \\\\lor x''_i \\\\geq 1 \\\\lor x''_i \\\\in \\\\left[\\\\frac{1}{4}, \\\\frac{3}{4}\\\\right]$$ for some i. Then $$h(x'') = 0$$. Note that, since $$\\\\gamma < \\\\frac{1}{2}$$, $$\\\\frac{1}{2} \\\\notin [0, \\\\frac{1}{2} - \\\\gamma) \\\\cup (1 - \\\\gamma, 1 + \\\\gamma]$$, so it is not possible for $$x''$$ to be an invalid encoding that is classified as 1;\\n- $$x''$$ is a valid encoding. Then, since $$\\\\gamma < \\\\frac{1}{2}$$, $$e(x) = e(x^*)$$. Since $$h(x) = 1$$ iff $$R(e(x), e(y))$$ is true and since $$R(x^*, \\\\hat{y})$$ is false for all choices of $$\\\\hat{y}$$, $$h(x) = 0$$.\\n\\nTherefore, $$x^*$$ satisfies Equation (45) and thus $$q(z) \\\\in \\\\text{CCA}_\\\\infty$$.\\n\\nLemma F.2.\\n\\n$$q(z) \\\\in \\\\text{CCA}_\\\\infty = \\\\Rightarrow z \\\\in \\\\text{co} \\\\Pi_3^2$$\\n\\nProof.\\n\\nSince $$q(z) \\\\in \\\\text{CCA}_\\\\infty$$, there exists a $$x^* \\\\in B_\\\\infty$$ such that $$h(x^*) \\\\neq h(x)$$ and $$\\\\forall x'' \\\\in B_\\\\infty(x^*, \\\\gamma). h(x) = 0$$.\\n\\nWe will prove that $$e(x)$$ is a solution to $$\\\\text{co} \\\\Pi_3^2$$. Since $$h(x) = 1$$, $$h(x^*) = 0$$, which means that $$\\\\forall x'' \\\\in B_\\\\infty(x^*, \\\\gamma). h(x'') = 0$$.\\n\\nWe know that $$x^* \\\\in B_\\\\infty(x(s), \\\\frac{1}{2}) = [0, 1]_n$$. We first prove by contradiction that $$x^* \\\\in [0, \\\\frac{1}{2} - \\\\gamma) \\\\cup (1 + \\\\gamma, 1]$$.\\n\\nIf $$x^*_i \\\\in [\\\\frac{1}{2} - \\\\gamma, 1 + \\\\gamma]$$ for some i, then the vector $$x(w)$$ defined as follows:\\n\\n$$x(w)_j = \\\\begin{cases} \\\\frac{1}{2} & x^*_i = j \\\\\\\\ x^*_i & \\\\text{otherwise} \\\\end{cases}$$\\n\\nis such that $$x(w) \\\\in B_\\\\infty(x^*, \\\\gamma)$$ and $$h(x(w)) = 1$$ (since $$\\\\text{inv}T(x(w)) = 1$$). This contradicts the fact that $$\\\\forall x'' \\\\in B_\\\\infty(x^*, \\\\gamma). h(x'') = 0$$. Therefore, $$x^* \\\\in [0, \\\\frac{1}{2} - \\\\gamma) \\\\cup (1 + \\\\gamma, 1]$$. As a consequence, $$\\\\forall x'' \\\\in B_\\\\infty(x^*, \\\\gamma). e(x) = e(x^*)$$.\\n\\nWe now prove that $$\\\\forall \\\\hat{y}^* \\\\exists x''^* \\\\in B_\\\\infty(x^*, \\\\gamma)$$ such that $$e(y) = \\\\hat{y}^*$$. We can construct such $$x''^*$$ as follows. For every i:\\n\\n- If $$e(x) = 0$$ and $$e(y) = 0$$, set $$x''^*_i$$ equal to a value in $$[0, \\\\frac{1}{4})$$;\\n- If $$e(x) = 0$$ and $$e(y) = 1$$, set $$x''^*_i$$ equal to a value in $$[\\\\frac{1}{4}, \\\\gamma]$$;\"}"}
{"id": "marro23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\npattern in the literature where new defenses are routinely broken. For example, defensive distillation (Papernot et al., 2016) was broken by Carlini & Wagner (2016). Carlini also showed that several adversarial example detectors (Carlini & Wagner, 2017a), as well as model-based purifiers (Carlini & Wagner, 2017b) can be fooled. Similarly, He et al. (2017) showed that ensembles of weak defenses can be fooled, while the defense of Roth et al. (2019) was fooled by Hosseini et al. (2019). Finally, Tramer et al. (2020) and Croce et al. (2022) broke a variety of adaptive defenses.\\n\\nWhile our theorems formally hold only in the worst case, they rely at their core on two properties that can be expected to be practically relevant, and namely: 1) that NNs can learn response surfaces that are as complex as Boolean formulas, and 2) that robustness involves universal rather than existential quantification. For this reason, we think the asymmetry we identified can provide valuable insight into a large body of empirical work.\\n\\n3.5. Additional Sources of Asymmetry\\n\\nOn top of our identified structural difference, there are additional factors that may provide an advantage to the attacker, despite the fact that they lack a formal characterization at the moment of writing. We review them in this section, both as promising directions for future theoretical research, and since awareness of them can support efforts to build more robust defenses.\\n\\nFirst, the attacker can gather information about the target model, e.g. by using genuine queries (Papernot et al., 2017), while the defender does not have such an advantage. As a result, the defender often needs to either make assumptions about adversarial examples (Hendrycks & Gimpel, 2017; Roth et al., 2019) or train models to identify common properties (Feinman et al., 2017; Grosse et al., 2017). These assumptions can be exploited, such as in the case of Carlini & Wagner (2017a), who generated adversarial examples that did not have the expected properties.\\n\\nSecond, the attacker can focus on one input at the time, while the defender has to guarantee robustness on a large subset of the input space. This weakness can be exploited: for example, MagNet (Meng & Chen, 2017) relies on a model of the entire genuine distribution, which can sometimes be inaccurate. Carlini & Wagner (2017b) broke MagNet by searching for examples that were both classified differently and mistakenly considered genuine.\\n\\nFinally, defenses cannot significantly compromise the accuracy of a model. Adversarial training, for example, often reduces the clean accuracy of the model (Madry et al., 2018), leading to a trade-off between accuracy and robustness. All of these factors can, depending on the application context, exacerbate the effects of the structural asymmetry; for this reason, minimizing their impact represents another important research direction.\\n\\n4. Sidestepping the Asymmetry\\n\\nAn important aspect of our theoretical results is that they apply only to building robust classifiers at training time. This leaves open the possibility to sidestep the asymmetry by focusing on defenses that operate at inference time. Here, we prove that this indeed the case by means of an example, and characterize its properties since they can be expected to hold for other systems based on the same principles.\\n\\nOur witness is a proof-of-concept robustness checker, called Counter-Attack (CA), that relies on adversarial attacks to compute robustness certificates at inference time, w.r.t. to a maximum $p$-norm $\\\\varepsilon$. CA can compute certificates in $\\\\text{NP}$-time, and attacking it beyond its intended certification radius is $\\\\Sigma_2$-hard, proving that inference-time defenses can flip the attack-defense asymmetry. While an argument can be made that CA is usable as it is, our main aim is to pave the ground for future approaches with the same strengths, and hopefully having better scalability.\\n\\n4.1. Inference-Time Defenses can Flip the Asymmetry: the Case of Counter-Attack\\n\\nThe main idea in CA is to evaluate robustness on a case-by-case basis, flagging inputs as potentially unsafe if a robust answer cannot be provided. Specifically, given a norm-order $p$ and threshold $\\\\varepsilon$, CA operates as follows:\\n\\n- For a given input $x$, we determine if the model is $(\\\\varepsilon, p)$-locally robust by running an untargeted adversarial attack on $x$;\\n- If the attack succeeds, we flag the input.\\n\\nIn a practical usage scenario, flagged inputs would then be processed by a slower, but more robust, model (e.g. a human) or rejected; this behavior is similar to that of approaches for learning with rejection, but with a semantic tied to adversarial robustness.\\n\\nSimilarly, it is possible to draw comparisons between robust transductive learning (e.g. the work of Chen et al. (2021)) and CA. While the two techniques use different approaches, we believe that parts of our analysis might be adapted to study existing applications of transductive learning to robust classification. Refer to Appendix G for a more in-depth comparison.\\n\\nFinally, note that the flagging rate depends on the model\\n\\n---\\n\\n5 Note that the learning-with-rejection approach usually involves some form of confidence score; while the decision boundary distance might be seen as a sort of score, it does not have a probabilistic interpretation. Studying CA under this light represents a promising research direction.\"}"}
{"id": "marro23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nrobustness: a model that is locally robust on the whole input distribution would have a flagging rate of 0, while in the opposite case all inputs would be flagged. As a consequence, this form of inference-time defense is best thought of as a complement to training-time robustness approaches, designed to catch those cases that are hard to handle due to Theorem 3.7. A technique such as CA would indeed benefit from most advances in the field of adversarial robustness: training-time defenses for a better flagging rate, and attack algorithms for more effective and efficient certificates.\\n\\n4.2. Formal Properties\\n\\nThe formal properties of the CA approach depend on the kind of attack used to perform the robustness check. Specifically, when used with an exact attack, such as those from Carlini et al. (2017) and Tjeng et al. (2019), CA provides formal robustness guarantees for an arbitrary $p$ and $\\\\varepsilon$:\\n\\n**Theorem 4.1.** Let $0 < p \\\\leq \\\\infty$ and let $\\\\varepsilon > 0$. Let $f: \\\\mathbb{X} \\\\rightarrow \\\\{1, \\\\ldots, N\\\\}$ be a classifier and let $a$ be an exact attack. Let $f_{CA}^a: \\\\mathbb{X} \\\\rightarrow \\\\{1, \\\\ldots, N\\\\} \\\\cup \\\\{\\\\star\\\\}$ be defined as:\\n\\n$$f_{CA}^a(x) = \\\\begin{cases} f(x) & \\\\|a_{f,p}(x) - x\\\\|_p > \\\\varepsilon \\\\\\\\ \\\\star & \\\\text{otherwise} \\\\end{cases}$$\\n\\nThen $\\\\forall x \\\\in \\\\mathbb{X}$ an $L_p$ attack on $x$ with radius greater than or equal to $\\\\varepsilon$ and with $\\\\star \\\\not\\\\in C(x)$ fails. The notation $f_{CA}^a(x)$ refers to the classifier $f$ combined with CA, relying on attack $a$. The condition $\\\\star \\\\not\\\\in C(x)$ requires that the input generated by the attack should not be flagged by CA. Intuitively, CA guarantees robustness due to the fact that, if $x'$ is an adversarial example for an input $x$, $x$ is also an adversarial example for $x'$, which means that $x'$ will be flagged.\\n\\nDue to the properties of $L_p$ norms, CA also guarantees a degree of robustness against attacks with a different norm:\\n\\n**Corollary 4.2.** Let $1 \\\\leq p \\\\leq \\\\infty$ and let $\\\\varepsilon > 0$. Let $f$ be a classifier on inputs with $n$ elements that uses CA with norm $p$ and radius $\\\\varepsilon$. Then for all inputs and for all $1 \\\\leq r < p$, $L_r$ attacks of radius greater than or equal to $\\\\varepsilon$ and with $\\\\star \\\\not\\\\in C(x)$ will fail. Similarly, for all inputs and for all $r > p$, $L_r$ attacks of radius greater than or equal to $n^{1/r - 1/p} \\\\varepsilon$ and with $\\\\star \\\\not\\\\in C(x)$ will fail (treating $1, \\\\infty$ as 0).\\n\\nNote that since the only expensive step in CA consists in applying an adversarial attack to an input, the complexity is the same as that of a regular attack.\\n\\nAttacking with a Higher Radius\\n\\nIn addition to robustness guarantees for a chosen $\\\\varepsilon$, CA provides a form of computational robustness even beyond its intended radius. To prove this statement, we first formalize the task of attacking CA (referred to as Counter-CA, or CCA). This involves finding, given a starting point $x$, an input $x' \\\\in B_p(x, \\\\varepsilon')$ that is adversarial but not flagged by CA, i.e. such that $f(x') \\\\in C(x)$ $\\\\land \\\\forall x'' \\\\in B_p(x', \\\\varepsilon). f(x'') = f(x')$.\\n\\n**Theorem 4.3** (Attacking CA with a higher radius is $\\\\Sigma_2^P$-complete). Let $CCA_p$ be the set of all tuples $\\\\langle x, \\\\varepsilon, \\\\varepsilon', C, f \\\\rangle$ such that:\\n\\n$$\\\\exists x' \\\\in B_p(x, \\\\varepsilon'). (f(x') \\\\in C(x) \\\\land \\\\forall x'' \\\\in B_p(x', \\\\varepsilon). f(x'') = f(x'))$$\\n\\nwhere $x \\\\in \\\\mathbb{X}$, $\\\\mathbb{X}$ is a FSFP space, $\\\\varepsilon' > \\\\varepsilon$, $f(x) \\\\not\\\\in C(x)$ $f$ is a ReLU classifier and whether an output is in $C(x)$ for some $x^*$ can be decided in polynomial time. Then $CCA_\\\\infty$ is $\\\\Sigma_2^P$-complete.\\n\\n**Corollary 4.4.** $CCA_p$ is $\\\\Sigma_2^P$-complete for all $0 < p \\\\leq \\\\infty$.\\n\\n**Corollary 4.5.** Theorem 4.3 also holds if, instead of ReLU classifiers, we consider the more general set of polynomial-time classifiers w.r.t. the size of the tuple.\\n\\nIn other words, under our assumptions, fooling CA can be harder than running it, thus flipping the computational asymmetry. Corollary 3.6 also implies that it is impossible to obtain a better gap between running the model and attacking it, from a Polynomial Hierarchy point of view (e.g. a $P$-time model that is $\\\\Sigma_2^P$-hard to attack). Note that, due to the worst-case semantic of Theorem 4.3, fooling CA can be expected to be easy in practice when $\\\\varepsilon' \\\\gg \\\\varepsilon$: this is however a very extreme case, where the threshold might have been poorly chosen or the adversarial examples might be very different from genuine examples.\\n\\nPartial Robustness\\n\\nWhile using exact attacks with CA is necessary for the best formal behavior, the approach remains capable of providing partial guarantees when used with either heuristic or lower-bounding approaches. In particular, if a heuristic attack returns an example $x'$ with $\\\\|x - x'\\\\|_p \\\\leq \\\\varepsilon$, then $f$ is guaranteed to be locally non-robust on $x$. However, a heuristic attack failing to find an adversarial example does not guarantee that the model is locally robust.\\n\\nConversely, if we replace the attack with an optimization method capable of returning a lower bound $lb(x)$ on the decision boundary distance (e.g. a Mathematical Programming solver), we get the opposite result: if the method proves that $lb(x) > \\\\varepsilon$, then $f$ is locally robust on $x$, but $f$ might be robust even if the method fails to prove it.\\n\\nIn other words, with heuristic attacks false positives are impossible, while with lower-bound methods false negatives are impossible. Note that these two methods can be combined to improve scalability while retaining some formal guarantees.\"}"}
{"id": "marro23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nThese considerations provide further motivation for research in heuristic attacks, since every improvement in that field could lead to more reliable or faster robustness \u201ccertificates\u201d. Additionally, they emphasize the potential of bounding techniques (e.g. guaranteed approximation algorithms) as efficient certification tools. Finally, while we think that CA is an interesting technique per-se, we reiterate that the main appeal of the approach is to prove by means of an example that it is possible to circumvent the computational asymmetry we identified. We hope that future work will expand on this research direction, developing approaches that are both more efficient and with more formal guarantees.\\n\\n5. An Evaluation of Adversarial Attacks as Certification Tools\\n\\nCA highlights an interesting aspect of adversarial attacks: since attacking a classifier and certifying its local robustness are complementary tasks, adversarial attacks can be used to build inference-time certification techniques. This observation raises interest in evaluating existing (heuristic) attack algorithms in terms of their ability to serve as defenses (of which CA is just one of many possible applications). For example, in contexts where provable robustness is too resource-intensive, one could use sufficiently powerful heuristic attacks to determine with great accuracy if the model is locally robust (but without formal guarantees).\\n\\nFrom this point of view, it should be noted that checking robustness only requires evaluating the decision boundary distance, and not necessarily finding the adversarial example that is closest to an input \\\\( x \\\\), i.e. the optimal solution of Equation (1). As a consequence, an attack does not need to perform well to be usable as a defense, but just to come predictably close to the decision boundary. For example, an algorithm that consistently overestimates the decision boundary distance by a 10% factor would be as good as an exact attack for many practical purposes, since we could simply apply a correction to obtain an exact estimate. This kind of evaluation is natural when viewing the issue from the perspective of our CA method, but to the best of our knowledge it has never been observed in the literature.\\n\\nIn this section, we thus empirically evaluate the quality of heuristic attacks. Specifically, we test whether \\\\( \\\\| x - x_h \\\\|_p \\\\), where \\\\( x_h \\\\) is an adversarial example found by a heuristic attack, is predictably close to the true decision boundary distance \\\\( d^* \\\\). To the best of our knowledge, the only other work that performed a somewhat similar evaluation is Carlini et al. (2017), which evaluated the optimality of the Carlini & Wagner attack on 90 MNIST samples for a \\\\( \\\\sim 20k \\\\) parameter network.\\n\\nConsistently with Athalye et al. (2018) and Weng et al. (2018), we focus on the \\\\( L_\\\\infty \\\\) norm. Additionally, we focus on pools of heuristic attacks. The underlying rationale is that different adversarial attacks should be able to cover for their reciprocal blind spots, providing a more reliable estimate. Since this evaluation is empirical, it requires sampling from a chosen distribution, in our case specific classifiers and the MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky et al., 2009) datasets. This means that the results are not guaranteed for other distributions, or for other defended models: studying how adversarial attacks fare in these cases is an important topic for future work.\\n\\nExperimental Setup\\n\\nWe randomly selected \\\\( \\\\sim 2.3k \\\\) samples each from the test set of two datasets, MNIST and CIFAR10. We used three architectures per dataset (named A, B and C), each trained in three settings, namely standard training, PGD adversarial training (Madry et al., 2018) and PGD adversarial training with ReLU loss and pruning (Xiao et al., 2019) (from now on referred to as ReLU training), for a total of nine configurations per dataset.\\n\\nSince our analysis requires computing exact decision boundary distances, and size and depth both have a strong adverse impact on solver times, we used small and relatively shallow networks with parameters between \\\\( \\\\sim 2k \\\\) and \\\\( \\\\sim 80k \\\\). For this reason, the natural accuracy for standard training are significantly below the state of the art (89.63% - 95.87% on MNIST and 47.85% - 55.81% on CIFAR10). Adversarial training also had a negative effect on natural accuracies (84.54% - 94.24% on MNIST and 45.19% - 51.35% on CIFAR10), similarly to ReLU training (83.69% - 93.57% on MNIST and 32.27% - 37.33% on CIFAR10). Note that using reachability analysis tools for NNs, such as (Gehr et al., 2018), capable of providing upper bounds on the decision boundary in a reasonable time would not be sufficient for our goal: indeed both lower and upper bounds on the decision boundary distance could be arbitrarily far from \\\\( d^* \\\\), thus preventing us from drawing any firm conclusion.\\n\\nWe first ran a pool of heuristic attacks on each example, namely BIM (Kurakin et al., 2017), Brendel & Bethge (Brendel et al., 2019), Carlini & Wagner (Carlini & Wagner, 2017c), Deepfool (Moosavi-Dezfooli et al., 2016), Fast Gradient (Goodfellow et al., 2015) and PGD (Madry et al., 2018), in addition to simply adding uniform noise to the input. Our main choice of attack parameters (from now on referred to as the \u201cstrong\u201d parameter set) prioritizes finding adversarial examples at the expense of computational time. For each example, we considered the nearest feasible adversarial example found by any attack in the pool. We then ran the exact solver-based attack MIPVerify (Tjeng et al., 2019), which is able to find the nearest adversarial example to a given input. The entire process (including test runs) required \\\\( \\\\sim 45k \\\\) core-hours on an HPC cluster. Each node of the cluster has 384 GB of RAM and features two Intel CascadeLake 8260 CPUs, each with 24 cores and a clock...\"}"}
{"id": "marro23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nWe removed the examples for which MIPVerify crashed in at least one setting, obtaining 2241 examples for MNIST and 2269 for CIFAR10. We also excluded from our analysis all adversarial examples for which MIPVerify did not find optimal bounds (atol = 1e-5, rtol = 1e-10), which represent on average 11.95% of the examples for MNIST and 16.30% for CIFAR10. Additionally, we ran the same heuristic attacks with a faster parameter set (from now on referred to as the \u201cbalanced\u201d set) on a single machine with an AMD Ryzen 5 1600X six-core 3.6 GHz processor, 16 GBs of RAM and an NVIDIA GTX 1060 6 GB GPU. The process took approximately 8 hours. Refer to Appendix H for a more comprehensive overview of our experimental setup.\\n\\nDistance Approximation\\n\\nAcross all settings, the mean distance found by the strong attack pool is 4.09 \u00b12.02% higher for MNIST and 2.21 \u00b11.16% higher for CIFAR10 than the one found by MIPVerify. For 79.81 \u00b115.70% of the MNIST instances and 98.40 \u00b11.63% of the CIFAR10 ones, the absolute difference is less than 1/255, which is the minimum distance in 8-bit image formats. The balanced attack pool performs similarly, finding distances that are on average 4.65\u00b12.16% higher for MNIST and 2.04\u00b11.13% higher for CIFAR10. The difference is below 1/255 for 77.78\u00b116.08% of MNIST examples and 98.74 \u00b11.13% of CIFAR10 examples. We compare the distances found by the strong attack pool for MNIST A and CIFAR10 (using standard training) with the true decision bound distances in Figure 1. Refer to Appendix J for the full data.\\n\\nFor all datasets, architectures and training techniques there appears to be a strong, linear, correlation between the distance of the output of the heuristic attacks and the true decision boundary distance. We chose to measure this by training a linear regression model linking the two distances. For the strong parameter set, we find that the average R\u00b2 across all settings is 0.992\u00b10.004 for MNIST and 0.997\u00b10.003 for CIFAR10. The balanced parameter set performs similarly, achieving an R\u00b2 of 0.990\u00b10.006 for MNIST and 0.998\u00b10.002 for CIFAR10. From these results, we conjecture that increasing the computational budget of heuristic attacks does not necessarily improve predictability, although further tests would be needed to confirm such a claim. Note that such a linear model can also be used to correct decision boundary distance overestimates in the context of heuristic CA. Another (possibly more reliable) procedure would consist in using quantile fitting; results for this approach are reported in Appendix I.\\n\\nAttack Pool Ablation Study\\n\\nDue to the nontrivial computational requirements of running several attacks on the same input, we now study whether it is possible to drop some attacks from the pool without compromising its predictability. Specifically, we consider all possible pools of size n (with a success rate of 100%) and pick the one with the highest average R\u00b2 value over all architectures and training techniques. As shown in Figure 2, adding attacks does increase predictability, although with diminishing returns. For example, the pool composed of the Basic Iterative Method, the Brendel & Bethge Attack and the Carlini & Wagner attack achieves on its own a R\u00b2 value of 0.988\u00b10.004 for MNIST+strong, 0.986\u00b10.005 for MNIST+balanced, 0.935\u00b10.048 for CIFAR10+strong and 0.993\u00b10.003 for CIFAR10+balanced. Moreover, dropping both the Fast Gradient Sign Method and uniform noise leads to negligible (\u221d 0.001) absolute variations in the mean R\u00b2. These findings suggest that, as far as consistency is concerned, the choice of attacks represents a more important factor than the computational budget.\"}"}
{"id": "marro23a", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 10. Decision boundary distances found by the attack pools compared to those found by MIPVerify on MNIST B. The black line represents the theoretical optimum. Note that no samples are below the black line.\\n\\nFigure 11. Decision boundary distances found by the attack pools compared to those found by MIPVerify on MNIST C. The black line represents the theoretical optimum. Note that no samples are below the black line.\"}"}
{"id": "marro23a", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 12. Decision boundary distances found by the attack pools compared to those found by MIPVerify on CIFAR10 A. The black line represents the theoretical optimum. Note that no samples are below the black line.\\n\\nFigure 13. Decision boundary distances found by the attack pools compared to those found by MIPVerify on CIFAR10 B. The black line represents the theoretical optimum. Note that no samples are below the black line.\"}"}
{"id": "marro23a", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\n(a) CIFAR10 C Standard Strong\\n(b) CIFAR10 C Standard Balanced\\n(c) CIFAR10 C Adversarial Strong\\n(d) CIFAR10 C Adversarial Balanced\\n(e) CIFAR10 C ReLU Strong\\n(f) CIFAR10 C ReLU Balanced\\n\\nFigure 14. Decision boundary distances found by the attack pools compared to those found by MIPVerify on CIFAR10 C. The black line represents the theoretical optimum. Note that no samples are below the black line.\"}"}
{"id": "marro23a", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 18. Best pools of a given size by success rate and $R^2$ for MNIST strong.\\n\\n| n | Attacks               | Success Rate | Difference | $R^2$     |\\n|---|-----------------------|--------------|------------|-----------|\\n| 1 | PGD                   | 100.00\u00b10.00% | 10.98\u00b14.41%| 0.975\u00b10.010|\\n| 2 | C&W, PGD              | 100.00\u00b10.00% | 7.99\u00b13.31% | 0.986\u00b10.005|\\n| 3 | B&B, C&W, PGD         | 100.00\u00b10.00% | 4.71\u00b11.97% | 0.989\u00b10.004|\\n| 4 | B&B, C&W, DF, PGD     | 100.00\u00b10.00% | 4.36\u00b12.03% | 0.991\u00b10.005|\\n| 5 | No FGSM, Uniform      | 100.00\u00b10.00% | 4.09\u00b12.02% | 0.992\u00b10.005|\\n| 6 | No Uniform            | 100.00\u00b10.00% | 4.09\u00b12.02% | 0.992\u00b10.005|\\n| 7 | All                   | 100.00\u00b10.00% | 4.09\u00b12.02% | 0.992\u00b10.005|\\n\\n### Table 19. Best pools of a given size by success rate and $R^2$ for MNIST balanced.\\n\\n| n | Attacks               | Success Rate | Difference | $R^2$     |\\n|---|-----------------------|--------------|------------|-----------|\\n| 1 | BIM                   | 100.00\u00b10.00% | 11.72\u00b14.18%| 0.965\u00b10.010|\\n| 2 | BIM, B&B              | 100.00\u00b10.00% | 6.11\u00b12.28% | 0.980\u00b10.007|\\n| 3 | BIM, B&B, C&W         | 100.00\u00b10.00% | 5.29\u00b12.06% | 0.986\u00b10.005|\\n| 4 | BIM, B&B, C&W, DF     | 100.00\u00b10.00% | 4.85\u00b12.10% | 0.989\u00b10.005|\\n| 5 | No FGSM, Uniform      | 100.00\u00b10.00% | 4.65\u00b12.16% | 0.990\u00b10.006|\\n| 6 | No Uniform            | 100.00\u00b10.00% | 4.65\u00b12.16% | 0.990\u00b10.006|\\n| 7 | All                   | 100.00\u00b10.00% | 4.65\u00b12.16% | 0.990\u00b10.006|\\n\\n### Table 20. Best pools of a given size by success rate and $R^2$ for CIFAR10 strong.\\n\\n| n | Attacks               | Success Rate | Difference | $R^2$     |\\n|---|-----------------------|--------------|------------|-----------|\\n| 1 | DF                    | 100.00\u00b10.00% | 6.11\u00b13.49% | 0.989\u00b10.011|\\n| 2 | DF, PGD               | 100.00\u00b10.00% | 4.71\u00b12.37% | 0.995\u00b10.007|\\n| 3 | C&W, DF, PGD          | 100.00\u00b10.00% | 2.54\u00b11.30% | 0.996\u00b10.006|\\n| 4 | B&B, C&W, DF, PGD     | 100.00\u00b10.00% | 2.21\u00b11.16% | 0.997\u00b10.003|\\n| 5 | No FGSM, Uniform      | 100.00\u00b10.00% | 2.21\u00b11.16% | 0.997\u00b10.003|\\n| 6 | No Uniform            | 100.00\u00b10.00% | 2.21\u00b11.16% | 0.997\u00b10.003|\\n| 7 | All                   | 100.00\u00b10.00% | 2.21\u00b11.16% | 0.997\u00b10.003|\\n\\n### K. Ablation Study\\n\\nWe outline the best attack pools by size in Tables 18 to 21. Additionally, we report the performance of pools composed of individual attacks in Tables 22 to 25. Finally, we detail the performance of dropping a specific attack in Tables 26 to 29.\"}"}
{"id": "marro23a", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Attack Parameter Name | MNIST Parameters | CIFAR10 Parameters |\\n|-----------------------|------------------|-------------------|\\n| **BIM**               |                  |                   |\\n| Initial Search Factor | 0.75             | 0.75              |\\n| Initial Search Steps  | 30               | 30                |\\n| Binary Search Steps   | 20               | 20                |\\n| #Iterations           | 2^k 200          | 5^k 200           |\\n| Learning Rate         | 10^-3 10^-2 10^-5 10^-3 |\\n| **Brendel & Bethge**  |                  |                   |\\n| Initial Attack        | Blended Noise    |                   |\\n| Overshoot             | 1.1              |                   |\\n| LR Decay              | 0.75             |                   |\\n| LR Decay Every n Steps| 50               |                   |\\n| #Iterations           | 5^k 200          | 5^k 200           |\\n| Learning Rate         | 10^-3 10^-3 10^-5 10^-3 |\\n| Momentum              | 0.8              |                   |\\n| Initial Directions    | 1000             |                   |\\n| Init Steps            | 1000             |                   |\\n| **Carlini & Wagner**  |                  |                   |\\n| Minimum \u03c4             | 10^-5            |                   |\\n| Initial \u03c4             | 1                |                   |\\n| \u03c4 Factor               | 0.95 0.9 0.99 0.9 |                   |\\n| Initial Const         | 10^-5            |                   |\\n| Const Factor          | 2                |                   |\\n| Maximum Const         | 20               |                   |\\n| Reduce Const          | False            |                   |\\n| Warm Start            | True             |                   |\\n| Abort Early           | True             |                   |\\n| Learning Rate         | 10^-2 10^-2 10^-5 10^-4 |\\n| Max Iterations        | 1k 100           | 5k 100            |\\n| \u03c4 Check Every n Steps | 1                |                   |\\n| Const Check Every n Steps | 5          |                   |\\n| **Deepfool**          |                  |                   |\\n| #Iterations           | 5^k              |                   |\\n| Candidates            | 10               |                   |\\n| Overshoot             | 10^-5            |                   |\\n| **FGSM**              |                  |                   |\\n| Initial Search Factor | 0.75             |                   |\\n| Initial Search Steps  | 30               |                   |\\n| Binary Search Steps   | 20               |                   |\\n| #Iterations           | 2^k 200          | 5^k 200           |\\n| Learning Rate         | 10^-4 10^-3 10^-4 10^-3 |\\n| **PGD**               |                  |                   |\\n| Initial Search Factor | 0.75             |                   |\\n| Initial Search Steps  | 30               |                   |\\n| Binary Search Steps   | 20               |                   |\\n| #Iterations           | 5^k 200          | 5^k 200           |\\n| Learning Rate         | 10^-4 10^-3 10^-4 10^-3 |\\n| **Uniform Noise**     |                  |                   |\\n| Initial Search Factor | 0.75             |                   |\\n| Initial Search Steps  | 30               |                   |\\n| Binary Search Steps   | 20               |                   |\\n| Runs                  | 8^k 200          | 8^k 200           |\"}"}
{"id": "marro23a", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7. Parameter set used to initialize MIPVerify for MNIST. All other parameters are identical to the strong MNIST attack parameter set.\\n\\n| Attack Name | Parameter Name | Value |\\n|-------------|----------------|-------|\\n| BIM         | #Iterations    | 5k    |\\n|             | Learning Rate  | 1e-5  |\\n| Brendel & Bethge | Learning Rate | 1e-3  |\\n|             | Tau Factor     | 0.99  |\\n| Carlini & Wagner | Learning Rate | 1e-3  |\\n|             | #Iterations    | 5k    |\\n\\nTable 8. Parameters of MIPVerify.\\n\\n| Parameter Name | Value |\\n|----------------|-------|\\n| Exploration Main |       |\\n| Absolute Tolerance | 1e-5  |\\n| Relative Tolerance  | 1e-10 |\\n| Threads           | 1     |\\n| Timeout (s)       | 120, 7200 |\\n| Tightening Absolute Tolerance | 1e-4  |\\n| Tightening Relative Tolerance | 1e-10 |\\n| Tightening Timeout (s) | 20, 240 |\\n| Tightening Threads | 1     |\\n\\nTable 9. MIPVerify bound tightness statistics.\\n\\n| Architecture | Training % Tight |\\n|--------------|------------------|\\n| MNIST A      |                  |\\n| Standard     | 95.40%           |\\n| Adversarial  | 99.60%           |\\n| ReLU         | 82.46%           |\\n| MNIST B      |                  |\\n| Standard     | 74.61%           |\\n| Adversarial  | 85.68%           |\\n| ReLU         | 75.55%           |\\n| MNIST C      |                  |\\n| Standard     | 86.21%           |\\n| Adversarial  | 97.28%           |\\n| ReLU         | 95.63%           |\\n| CIFAR10 A    |                  |\\n| Standard     | 81.18%           |\\n| Adversarial  | 82.50%           |\\n| ReLU         | 92.73%           |\\n| CIFAR10 B    |                  |\\n| Standard     | 56.32%           |\\n| Adversarial  | 58.88%           |\\n| ReLU         | 81.67%           |\\n| CIFAR10 C    |                  |\\n| Standard     | 100.00%          |\\n| Adversarial  | 100.00%          |\\n| ReLU         | 100.00%          |\"}"}
{"id": "marro23a", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| \u03b5 | Mean F1 | 1% | 50% | 99% |\\n|---|---------|----|-----|-----|\\n| 0 | 0.8     |    |     |     |\\n| 0.85 | 0.9   |    |     |     |\\n| 0.9 | 0.95   |    |     |     |\\n| 1   | 1      |    |     |     |\\n\\n(a) MNIST A Standard Strong\\n\\n(b) MNIST A Standard Balanced\\n\\n(c) MNIST A Adversarial Strong\\n\\n(d) MNIST A Adversarial Balanced\\n\\n(e) MNIST A ReLU Strong\\n\\n(f) MNIST A ReLU Balanced\\n\\nFigure 3. F1 scores in relation to \u03b5 for MNIST A for each considered percentile. For ease of visualization, we set the graph cutoff at F1 = 0.8. We also mark 8/255 (a common choice for \u03b5) with a dotted line.\\n\\nI. Quantile-Based Calibration\\n\\nThe error correction model in CA can be empirically calibrated so as to control the chance of false positives (i.e. inputs wrongly reported as not robust) and false negatives (i.e. non-robust inputs reported as being robust).\\n\\nGiven the strong correlation that we observed between the distance of heuristic adversarial examples and the true decision boundary distance, using a linear model seems a reasonable choice. Under this assumption, the correction model depends only on the distance between the original example and the adversarial one, i.e. on \\\\( \\\\|x - a(x)\\\\| \\\\). This property allows us to rewrite the main check performed by CA as:\\n\\n\\\\[\\n| |x - a(x)|| - b(x) = \\\\alpha_1 | |x - a(x)|| + \\\\alpha_0 \\\\leq \\\\epsilon \\\\quad (63)\\n\\\\]\\n\\nwhere \\\\( a(x) \\\\) is the adversarial example found by the attack \\\\( a \\\\) for the input \\\\( x \\\\). The parameters \\\\( \\\\alpha_1, \\\\alpha_0 \\\\) can then be obtained via quantile regression (Koenker & Bassett Jr, 1978) by using the true decision boundary distance (i.e. \\\\( d^* \\\\)) as a target.\\n\\nThe approach provides a simple, interpretable mechanism to control how conservative the detection check should be: with a small quantile, CA will tend to underestimate the decision boundary distance, leading to fewer missed detections, but more false alarms; using a high quantile will lead to the opposite behavior.\\n\\nWe test this type of buffer using 5-fold cross-validation on each configuration. Specifically, we calibrate the model using 1%, 50%, and 99% as quantiles. Tables 10 to 13 provide a comparison between the expected quantile and the average true quantile of each configuration on the validation folds. Additionally, we plot in Figures 3 to 8 the mean F1 score in relation to the choice of \u03b5.\"}"}
{"id": "marro23a", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 10.\\nExpected vs true quantile for MNIST strong with 5-fold cross validation.\\n\\n| Architecture | Training | Expected Quantile | True Quantile   |\\n|--------------|----------|-------------------|-----------------|\\n|              |          | 1.00%             | 0.99\u00b11.02%      |\\n|              |          | 50.00%            | 49.93\u00b12.35%     |\\n|              |          | 99.00%            | 95.60\u00b13.77%     |\\n| Standard     |          | 1.00%             | 1.11\u00b10.53%      |\\n|              |          | 50.00%            | 50.25\u00b11.58%     |\\n|              |          | 99.00%            | 89.84\u00b16.42%     |\\n| Adversarial  |          | 1.00%             | 1.07\u00b10.48%      |\\n|              |          | 50.00%            | 49.80\u00b10.76%     |\\n|              |          | 99.00%            | 97.76\u00b10.71%     |\\n| ReLU         |          | 1.00%             | 1.11\u00b10.45%      |\\n|              |          | 50.00%            | 50.02\u00b11.72%     |\\n|              |          | 99.00%            | 91.95\u00b15.64%     |\\n| Standard     |          | 1.00%             | 1.07\u00b10.37%      |\\n|              |          | 50.00%            | 50.17\u00b11.64%     |\\n|              |          | 99.00%            | 98.73\u00b10.42%     |\\n| Adversarial  |          | 1.00%             | 1.05\u00b10.29%      |\\n|              |          | 50.00%            | 49.87\u00b13.58%     |\\n|              |          | 99.00%            | 99.00\u00b10.47%     |\\n| ReLU         |          | 1.00%             | 1.06\u00b10.67%      |\\n|              |          | 50.00%            | 50.02\u00b11.85%     |\\n|              |          | 99.00%            | 93.99\u00b13.51%     |\"}"}
{"id": "marro23a", "page_num": 49, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 15. Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 A Standard. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\\n\\nFigure 16. Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 A Adversarial. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "marro23a", "page_num": 50, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 17. Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 A ReLU. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\\n\\nFigure 18. Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 B Standard. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "marro23a", "page_num": 51, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 19. Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 B Adversarial. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\\n\\nFigure 20. Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 B ReLU. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "marro23a", "page_num": 52, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 21. Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 C Standard. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\\n\\nFigure 22. Mean difference between the distance of the closest adversarial examples and the exact decision boundary distance for MNIST & CIFAR10 C Adversarial. A dashed line means that the attack found adversarial examples (of any distance) for only some inputs, while the absence of a line means that the attack did not find any adversarial examples. The loosely and densely dotted black lines respectively represent the balanced and strong attack pools. Both axes are logarithmic.\"}"}
{"id": "marro23a", "page_num": 45, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 21.\\nBest pools of a given size by success rate and \\\\( R^2 \\\\) for CIFAR10 balanced.\\n\\n| n | Attacks          | Success Rate | Difference | \\\\( R^2 \\\\)  |\\n|---|------------------|--------------|------------|------------|\\n| 1 | DF               | 100.00\u00b10.00% | 6.11\u00b13.49% | 0.989\u00b10.011|\\n| 2 | B&B, DF          | 100.00\u00b10.00% | 2.52\u00b11.51% | 0.995\u00b10.004|\\n| 3 | BIM, B&B, DF     | 100.00\u00b10.00% | 2.21\u00b11.25% | 0.997\u00b10.002|\\n| 4 | BIM, B&B, C&W, DF| 100.00\u00b10.00% | 2.06\u00b11.16% | 0.998\u00b10.002|\\n| 5 | No FGSM, Uniform | 100.00\u00b10.00% | 2.04\u00b11.13% | 0.998\u00b10.002|\\n| 6 | No FGSM          | 100.00\u00b10.00% | 2.04\u00b11.13% | 0.998\u00b10.002|\\n| 7 | All              | 100.00\u00b10.00% | 2.04\u00b11.13% | 0.998\u00b10.002|\\n\\n### Table 22.\\nPerformance of individual attacks for MNIST strong.\\n\\n| Attack | Success Rate | Difference | \\\\( R^2 \\\\)  |\\n|--------|--------------|------------|------------|\\n| BIM    | 100.00\u00b10.00% | 10.90\u00b14.42%| 0.966\u00b10.012|\\n| B&B    | 99.99\u00b10.01%  | 18.50\u00b17.09%| 0.812\u00b10.044|\\n| C&W    | 100.00\u00b10.00% | 17.52\u00b12.74%| 0.910\u00b10.024|\\n| Deepfool| 100.00\u00b10.00% | 21.59\u00b17.73%| 0.923\u00b10.027|\\n| FGSM   | 99.72\u00b10.51%  | 44.43\u00b115.76%| 0.761\u00b10.132|\\n| PGD    | 100.00\u00b10.00% | 10.98\u00b14.41% | 0.975\u00b10.010|\\n| Uniform| 99.52\u00b10.91%  | 414.47\u00b1140.54%| 0.623\u00b10.138|\\n\\n### Table 23.\\nPerformance of individual attacks for MNIST balanced.\\n\\n| Attack | Success Rate | Difference | \\\\( R^2 \\\\)  |\\n|--------|--------------|------------|------------|\\n| BIM    | 100.00\u00b10.00% | 11.72\u00b14.18%| 0.965\u00b10.010|\\n| B&B    | 99.99\u00b10.03%  | 18.65\u00b17.29%| 0.812\u00b10.039|\\n| C&W    | 100.00\u00b10.00% | 22.55\u00b13.83%| 0.904\u00b10.025|\\n| Deepfool| 100.00\u00b10.00% | 21.59\u00b17.73%| 0.923\u00b10.027|\\n| FGSM   | 99.72\u00b10.51%  | 44.43\u00b115.76%| 0.761\u00b10.132|\\n| PGD    | 100.00\u00b10.00% | 16.23\u00b16.59% | 0.905\u00b10.070|\\n| Uniform| 98.66\u00b11.90%  | 521.61\u00b1181.40%| 0.484\u00b10.122|\\n\\n### Table 24.\\nPerformance of individual attacks for CIFAR10 strong.\\n\\n| Attack | Success Rate | Difference | \\\\( R^2 \\\\)  |\\n|--------|--------------|------------|------------|\\n| BIM    | 91.96\u00b17.40%  | 19.97\u00b15.95%| 0.934\u00b10.041|\\n| B&B    | 100.00\u00b10.00% | 508.66\u00b1196.37%| 0.174\u00b10.074|\\n| C&W    | 99.98\u00b10.02%  | 10.67\u00b13.64% | 0.926\u00b10.030|\\n| Deepfool| 100.00\u00b10.00% | 6.11\u00b13.49%  | 0.989\u00b10.011|\\n| FGSM   | 100.00\u00b10.00% | 31.80\u00b111.12%| 0.847\u00b10.123|\\n| PGD    | 100.00\u00b10.00% | 19.36\u00b15.99% | 0.952\u00b10.027|\\n| Uniform| 99.99\u00b10.02%  | 1206.79\u00b1277.68%| 0.910\u00b10.044|\"}"}
{"id": "marro23a", "page_num": 46, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 25. Performance of individual attacks for CIFAR10 balanced.\\n\\n| Attack   | Success Rate | Change Rate | Accuracy Rate | $\\rho^2$ |\\n|----------|--------------|-------------|---------------|---------|\\n| BIM      | 100.00\u00b10.00% | 19.23\u00b15.92% | 77.33\u00b115.89%  | 0.954\u00b10.025 |\\n| B&B      | 100.00\u00b10.00% | 50.64\u00b152.17%| 81.20\u00b110.68%  | 0.615\u00b10.349 |\\n| C&W      | 99.89\u00b10.09%  | 17.44\u00b14.01% | 84.82\u00b18.51%   | 0.923\u00b10.026 |\\n| Deepfool | 100.00\u00b10.00% | 6.11\u00b13.49%  | 95.06\u00b14.81%   | 0.989\u00b10.011 |\\n| FGSM     | 100.00\u00b10.00% | 31.80\u00b111.12%| 69.20\u00b117.72%  | 0.847\u00b10.123 |\\n| PGD      | 100.00\u00b10.00% | 20.18\u00b16.56% | 76.97\u00b116.07%  | 0.947\u00b10.031 |\\n| Uniform  | 99.85\u00b10.26%  | 1617.74\u00b1390.50% | 1.80\u00b10.67% | 0.853\u00b10.068 |\\n\\n### Table 26. Performance of pools without a specific attack for MNIST strong.\\n\\n| Attack   | Success Rate | Change Rate | Accuracy Rate | $\\rho^2$ |\\n|----------|--------------|-------------|---------------|---------|\\n| None     | 100.00\u00b10.00% | 4.09\u00b12.02%  | 79.81\u00b115.70%  | 0.992\u00b10.005 |\\n| BIM      | 100.00\u00b10.00% | 4.35\u00b12.03%  | 79.02\u00b115.62%  | 0.991\u00b10.005 |\\n| B&B      | 100.00\u00b10.00% | 6.76\u00b13.31%  | 64.46\u00b125.01%  | 0.990\u00b10.005 |\\n| C&W      | 100.00\u00b10.00% | 4.65\u00b12.20%  | 77.70\u00b116.02%  | 0.989\u00b10.006 |\\n| Deepfool | 100.00\u00b10.00% | 4.33\u00b11.97%  | 79.04\u00b115.75%  | 0.990\u00b10.004 |\\n| FGSM     | 100.00\u00b10.00% | 4.09\u00b12.02%  | 79.81\u00b115.70%  | 0.992\u00b10.005 |\\n| PGD      | 100.00\u00b10.00% | 4.26\u00b11.99%  | 79.36\u00b115.59%  | 0.991\u00b10.004 |\\n| Uniform  | 100.00\u00b10.00% | 4.09\u00b12.02%  | 79.81\u00b115.70%  | 0.992\u00b10.005 |\\n\\n### Table 27. Performance of pools without a specific attack for MNIST balanced.\\n\\n| Attack   | Success Rate | Change Rate | Accuracy Rate | $\\rho^2$ |\\n|----------|--------------|-------------|---------------|---------|\\n| None     | 100.00\u00b10.00% | 4.65\u00b12.16%  | 77.78\u00b116.08%  | 0.990\u00b10.006 |\\n| BIM      | 100.00\u00b10.00% | 5.13\u00b12.27%  | 76.14\u00b115.98%  | 0.988\u00b10.007 |\\n| B&B      | 100.00\u00b10.00% | 7.93\u00b13.69%  | 60.79\u00b125.99%  | 0.987\u00b10.006 |\\n| C&W      | 100.00\u00b10.00% | 4.93\u00b12.22%  | 77.05\u00b115.96%  | 0.988\u00b10.006 |\\n| Deepfool | 100.00\u00b10.00% | 5.03\u00b12.14%  | 76.34\u00b116.36%  | 0.988\u00b10.005 |\\n| FGSM     | 100.00\u00b10.00% | 4.65\u00b12.16%  | 77.78\u00b116.08%  | 0.990\u00b10.006 |\\n| PGD      | 100.00\u00b10.00% | 4.85\u00b12.10%  | 77.33\u00b115.85%  | 0.989\u00b10.005 |\\n| Uniform  | 100.00\u00b10.00% | 4.65\u00b12.16%  | 77.78\u00b116.08%  | 0.990\u00b10.006 |\\n\\n### Table 28. Performance of pools without a specific attack for CIFAR10 strong.\\n\\n| Attack   | Success Rate | Change Rate | Accuracy Rate | $\\rho^2$ |\\n|----------|--------------|-------------|---------------|---------|\\n| None     | 100.00\u00b10.00% | 2.21\u00b11.16%  | 98.40\u00b11.63%   | 0.997\u00b10.003 |\\n| BIM      | 100.00\u00b10.00% | 2.21\u00b11.16%  | 98.40\u00b11.63%   | 0.997\u00b10.003 |\\n| B&B      | 100.00\u00b10.00% | 2.54\u00b11.30%  | 98.17\u00b12.00%   | 0.996\u00b10.006 |\\n| C&W      | 100.00\u00b10.00% | 3.83\u00b12.06%  | 96.84\u00b13.12%   | 0.996\u00b10.004 |\\n| Deepfool | 100.00\u00b10.00% | 4.02\u00b11.19%  | 95.65\u00b13.10%   | 0.992\u00b10.005 |\\n| FGSM     | 100.00\u00b10.00% | 2.21\u00b11.16%  | 98.40\u00b11.63%   | 0.997\u00b10.003 |\\n| PGD      | 100.00\u00b10.00% | 2.50\u00b11.48%  | 98.11\u00b11.93%   | 0.995\u00b10.005 |\\n| Uniform  | 100.00\u00b10.00% | 2.21\u00b11.16%  | 98.40\u00b11.63%   | 0.997\u00b10.003 |\"}"}
{"id": "marro23a", "page_num": 47, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Attack          | Success Rate | Attack Success Rate Difference |\\n|----------------|--------------|--------------------------------|\\n| None           | 100.00\u00b10.00% | 2.04\u00b11.13%                     |\\n| BIM            | 100.00\u00b10.00% | 2.07\u00b11.15%                     |\\n| B&B            | 100.00\u00b10.00% | 4.08\u00b11.95%                     |\\n| C&W            | 100.00\u00b10.00% | 2.18\u00b11.22%                     |\\n| Deepfool       | 100.00\u00b10.00% | 4.00\u00b10.99%                     |\\n| FGSM           | 100.00\u00b10.00% | 2.04\u00b11.13%                     |\\n| PGD            | 100.00\u00b10.00% | 2.06\u00b11.16%                     |\\n| Uniform        | 100.00\u00b10.00% | 2.04\u00b11.13%                     |\"}"}
{"id": "marro23a", "page_num": 48, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 30. Parameters for the Fast-100, Fast-1k and Fast-10k sets.\\n\\n| Attack Parameter Name | MNIST | CIFAR10 |\\n|-----------------------|-------|---------|\\n|                       | 100   | 1k      | 10k     |\\n| BIM                   |       |         |         |\\n| Initial Search Factor | N/A   |         |         |\\n| Initial Search Steps  |       | N/A     |         |\\n| Binary Search Steps   | 10    | 20      | 20      |\\n| Starting $\\\\epsilon$   | 0.5   | 0.1     |         |\\n| #Iterations           | 10    | 50      | 500     |\\n| Learning Rate         | 0.1   | 0.01    | 1e-3    |\\n| Deepfool              |       |         |         |\\n| #Iterations           | 100   | 500     | 500     |\\n| Candidates            | 10    |         |         |\\n| Overshoot             | 0.1   | 1e-5    | 1e-5    |\\n| Loss Logits           |       |         |         |\\n| FGSM                  |       |         |         |\\n| Initial Search Factor | 0.75  | 0.5     |         |\\n| Initial Search Steps  | 30    | 10      |         |\\n| Binary Search Steps   | 20    |         |         |\\n| Starting $\\\\epsilon$   | 1     | 0.1     |         |\\n| #Iterations           | 10    | 50      | 500     |\\n| Learning Rate         | 0.1   | 0.01    | 1e-3    |\\n| Random Initialization  | True  |         |         |\\n| Uniform Noise         |       |         |         |\\n| Initial Search Factor | 0.75  | 0.75    | 0.75    |\\n| Initial Search Steps  | 30    | 30      | 30      |\\n| Binary Search Steps   | 20    | 20      | 20      |\\n| Starting $\\\\epsilon$   | 1     | 1       | 0.5     |\\n| Runs                  | 200   | 500     | 200     |\\n\\nL. Fast Parameter Set Tests\\n\\nWe list the chosen parameter sets for Fast-100, Fast-1k and Fast-10k in Table 30. We plot the difference between the distance of the closest adversarial examples and the true decision boundary distance in Figures 15 to 23, while we plot the $R^2$ values in Figures 24 to 32. We do not study the Brendel & Bethge and the Carlini & Wagner attacks due to the fact that the number of model calls varies depending on how many inputs are attacked at the same time. Note that, for attacks that do not have the a 100% success rate, the mean adversarial example distance can increase with the number of steps as new adversarial examples (for inputs for which there were previously no successful adversarial examples) are added.\"}"}
{"id": "marro23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nFigure 2. Best mean $R^2$ value in relation to the number of attacks in the pool.\\n\\ntant factor than the number of attacks in a pool. Refer to Appendix K for a more in-depth overview of how different attack selections affect consistency and accuracy.\\n\\nEfficient Attacks\\n\\nWe then explore if it is possible to increase the efficiency of attacks by optimizing for fast, rather than accurate, results. We pick three new parameter sets (namely Fast-100, Fast-1k and Fast-10k) designed to find the nearest adversarial examples within the respective number of calls to the model. We find that while Deepfool is not the strongest adversarial attack (see Appendix J), it provides adequate results in very few model calls. For details on these results see Appendix L.\\n\\nUG100 Dataset\\n\\nWe collect all the adversarial examples found by both MIPVerify and the heuristic attacks into a new dataset, which we name UG100. UG100 can be used to benchmark new adversarial attacks. Specifically, we can determine how strong an attack is by comparing it to both the theoretical optimum and heuristic attack pools. Another potential application involves studying factors that affect whether adversarial attacks perform sub-optimally.\\n\\n6. Conclusion\\n\\nIn this work, we provided three contribution in the context of adversarial robustness.\\n\\nFirst, we proved that attacking a ReLU classifier is $NP$-hard, while training a robust model of the same type is $\\\\Sigma^P_2$-hard. This result implies that defending is in the worst case harder than attacking; moreover, due to the broad applicability assumptions and the structure of its proof, it represents a reasonable explanation for the difficulty gap often encountered when building robust classifiers. The intuition behind our proofs can also help to pave the way for research into more tractable classes.\\n\\nSecond, we showed how inference-time techniques can sidestep the aforementioned computational asymmetry, by introducing a proof-of-concept defense called Counter Attack (CA). The central idea in CA is to check robustness by relying on adversarial attacks themselves: this strategy provides robustness guarantees, can invert the computational asymmetry, and may serve as the basis for devising more advanced inference-time defenses.\\n\\nFinally, motivated by the last observation, we provided an empirical evaluation of heuristic attacks in terms of their ability to consistently approximate the decision boundary distance. We found that state-of-the-art heuristic attacks are indeed very reliable approximators of the decision boundary distance, suggesting that even heuristic attacks might be used in defensive contexts.\\n\\nOur theoretical results highlight a structural challenge in adversarial ML, one that could be sidestepped through not only our CA approach, but potentially many more. Additionally, we showed that adversarial attacks can also play a role in asymmetry-free robustness, thus opening up new research directions on their defensive applications. We hope that our observations, combined with our formal analysis and our UG100 benchmark, can serve as the starting point for future research into these two important areas.\\n\\nAcknowledgements\\n\\nThe project leading to this application has received funding from the European Union\u2019s Horizon Europe research and innovation programme under grant agreement No. 101070149. We also acknowledge the CINECA award under the ISCRA initiative, for the availability of high performance computing resources and support. Finally, we thank Andrea Borghesi, Andrea Iacco and Rebecca Montanari for their advice and support.\"}"}
{"id": "marro23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nAthalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine Learning, pp. 274\u2013283. PMLR, 2018.\\n\\nAwasthi, P., Dutta, A., and Vijayaraghavan, A. On robustness to adversarial examples and polynomial optimization. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nBrendel, W., Rauber, J., K\u00fcmmerer, M., Ustyuzhaninov, I., and Bethge, M. Accurate, reliable and fast robustness evaluation. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nBrown, T. B., Man\u00e9, D., Roy, A., Abadi, M., and Gilmer, J. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.\\n\\nBubeck, S., Lee, Y. T., Price, E., and Razenshteyn, I. Adversarial examples from cryptographic pseudo-random generators. arXiv preprint arXiv:1811.06418, 2018.\\n\\nBubeck, S., Lee, Y. T., Price, E., and Razenshteyn, I. Adversarial examples from computational constraints. In International Conference on Machine Learning, pp. 831\u2013840. PMLR, 2019.\\n\\nCarlini, N. and Wagner, D. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016.\\n\\nCarlini, N. and Wagner, D. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 3\u201314, 2017a.\\n\\nCarlini, N. and Wagner, D. Magnet and \\\"Efficient defenses against adversarial attacks\\\" are not robust to adversarial examples. arXiv preprint arXiv:1711.08478, 2017b.\\n\\nCarlini, N. and Wagner, D. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 39\u201357. IEEE, 2017c.\\n\\nCarlini, N., Katz, G., Barrett, C., and Dill, D. L. Provably minimally-distorted adversarial examples. arXiv preprint arXiv:1709.10207, 2017.\\n\\nChen, J., Guo, Y., Wu, X., Li, T., Lao, Q., Liang, Y., and Jha, S. Towards adversarial robustness via transductive learning. arXiv preprint arXiv:2106.08387, 2021.\\n\\nCroce, F., Gowal, S., Brunner, T., Shelhamer, E., Hein, M., and Cemgil, T. Evaluating the adversarial robustness of adaptive test-time defenses. arXiv preprint arXiv:2202.13711, 2022.\\n\\nDegwekar, A., Nakkiran, P., and Vaikuntanathan, V. Computational limitations in robust classification and win-win results. In Conference on Learning Theory, pp. 994\u20131028. PMLR, 2019.\\n\\nDing, G. W., Wang, L., and Jin, X. AdverTorch v0.1: An adversarial robustness toolbox based on PyTorch. arXiv preprint arXiv:1902.07623, 2019.\\n\\nDreossi, T., Ghosh, S., Sangiovanni-Vincentelli, A., and Seshia, S. A. A formalization of robustness for deep neural networks. arXiv preprint arXiv:1903.10033, 2019.\\n\\nFeinman, R., Curtin, R. R., Shintre, S., and Gardner, A. B. Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.\\n\\nGarg, S., Jha, S., Mahloujifar, S., and Mohammad, M. Adversarially robust learning could leverage computational hardness. In Algorithmic Learning Theory, pp. 364\u2013385. PMLR, 2020.\\n\\nGehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., and Vechev, M. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In 2018 IEEE symposium on security and privacy (SP), pp. 3\u201318. IEEE, 2018.\\n\\nGoodfellow, I., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015. URL http://arxiv.org/abs/1412.6572.\\n\\nGottlob, G., Greco, G., and Scarcello, F. Pure Nash equilibria: Hard and easy games. arXiv e-prints, pp. arXiv\u20131109, 2011.\\n\\nGrosse, K., Manoharan, P., Papernot, N., Backes, M., and McDaniel, P. On the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017.\\n\\nGurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2022. URL https://www.gurobi.com.\\n\\nHe, W., Wei, J., Chen, X., Carlini, N., and Song, D. Adversarial example defenses: ensembles of weak defenses are not strong. In Proceedings of the 11th USENIX Conference on Offensive Technologies, pp. 15\u201315, 2017.\\n\\nHendrycks, D. and Gimpel, K. Early methods for detecting adversarial images. In International Conference on Learning Representations (Workshop Track), 2017.\\n\\nHosseini, H., Kannan, S., and Poovendran, R. Are odds really odd? Bypassing statistical detection of adversarial examples. arXiv preprint arXiv:1907.12138, 2019.\"}"}
{"id": "marro23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nKatz, G., Barrett, C., Dill, D., Julian, K., and Kochenderfer, M. Reluplex: An efficient smt solver for verifying deep neural networks. arXiv preprint arXiv:1702.01135, 2017.\\n\\nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nKoenker, R. and Bassett Jr, G. Regression quantiles. Econometrica: journal of the Econometric Society, pp. 33\u201350, 1978.\\n\\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Technical report, 2009.\\n\\nKurakin, A., Goodfellow, I., and Bengio, S. Adversarial machine learning at scale. 2017.\\n\\nLeCun, Y ., Cortes, C., and J.C. Burges, C. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.\\n\\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\\n\\nMahloujifar, S. and Mahmoody, M. Can adversarially robust learning leverage computational hardness? In Algorithmic Learning Theory, pp. 581\u2013609. PMLR, 2019.\\n\\nMeng, D. and Chen, H. MagNet: a two-pronged defense against adversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 135\u2013147, 2017.\\n\\nMoosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2574\u20132582, 2016.\\n\\nPapernot, N., McDaniel, P., Wu, X., Jha, S., and Swami, A. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (SP), pp. 582\u2013597. IEEE, 2016.\\n\\nPapernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., and Swami, A. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506\u2013519, 2017.\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani, A., Chilamkurthy, S., Raison, M., Tejani,"}
{"id": "marro23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computational Asymmetries in Robust Classification\\n\\nA. Proof Preliminaries\\n\\nA.1. Notation\\n\\nWe use $f_i$ to denote the $i$-th output of a network. We define $f$ as\\n\\n$$f(x) = \\\\arg\\\\max_i f_i(x)$$\\n\\n(8)\\n\\nfor situations where multiple outputs are equal to the maximum, we use the class with the lowest index.\\n\\nA.2. Arithmetic\\n\\nGiven two FSFP spaces $X$ and $X'$ with distance minorants $\\\\mu$ and $\\\\mu'$, we can compute new positive minorants after applying functions to the spaces as follows:\\n\\n- Sum of two vectors:\\n  $$\\\\mu + \\\\mu' = \\\\min(\\\\mu, \\\\mu')$$\\n\\n- Multiplication by a constant:\\n  $$\\\\alpha X = |\\\\alpha| \\\\mu$$\\n\\n- ReLU:\\n  $$\\\\mathrm{ReLU}(X) = \\\\mu$$\\n\\nSince it is possible to compute the distance minorant of a space transformed by any of these functions in polynomial time, it is also possible to compute the distance minorant of a space transformed by any composition of such functions in polynomial time.\\n\\nA.3. Functions\\n\\nWe now provide an overview of several functions that can be obtained by using linear combinations and ReLUs.\\n\\nmax\\n\\nCarlini et al. (2017) showed that we can implement the \\\\texttt{max} function using linear combinations and ReLUs as\\n\\n$$\\\\max(x, y) = \\\\mathrm{ReLU}(x - y) + y$$\\n\\n(9)\\n\\nWe can also obtain an $n$-ary version of \\\\texttt{max} by chaining multiple instances together.\\n\\nstep\\n\\nIf $X$ is a FSFP space, then the following scalar function:\\n\\n$$\\\\text{step}_0(x) = 1 - \\\\mu \\\\left(\\\\mathrm{ReLU}(x) - \\\\mathrm{ReLU}(x - \\\\mu)\\\\right)$$\\n\\n(10)\\n\\nis such that $\\\\forall i. \\\\forall x \\\\in X$, $\\\\text{step}_0(x_i)$ is 0 for $x_i \\\\leq 0$ and 1 for $x_i > 0$.\\n\\nSimilarly, let $\\\\text{step}_1$ be defined as follows:\\n\\n$$\\\\text{step}_1(x) = 1 - \\\\mu \\\\left(\\\\mathrm{ReLU}(x + \\\\mu) - \\\\mathrm{ReLU}(x)\\\\right)$$\\n\\n(11)\\n\\nNote that $\\\\forall i. \\\\forall x \\\\in X$, $\\\\text{step}_1(x_i)$ = 0 for $x_i < 0$ and 1 for $x_i \\\\geq 0$.\\n\\nBoolean Functions\\n\\nWe then define the Boolean functions\\n\\n- \\\\texttt{not}:\\n  $$\\\\{0, 1\\\\} \\\\rightarrow \\\\{0, 1\\\\}$$\\n\\n- \\\\texttt{and}:\\n  $$\\\\{0, 1\\\\}^2 \\\\rightarrow \\\\{0, 1\\\\}$$\\n\\n- \\\\texttt{or}:\\n  $$\\\\{0, 1\\\\}^2 \\\\rightarrow \\\\{0, 1\\\\}$$\\n\\n- \\\\texttt{if}:\\n  $$\\\\{0, 1\\\\}^3 \\\\rightarrow \\\\{0, 1\\\\}$$\\n\\nas follows:\\n\\n$$\\\\text{not}(x) = 1 - x$$\\n\\n(12)\\n\\nand\\n\\n$$\\\\text{or}(x, y) = \\\\text{step}_1(x + y - 2)$$\\n\\n(13)\\n\\nor\\n\\n$$\\\\text{or}(x, y) = \\\\text{step}_1(x + y)$$\\n\\n(14)\\n\\nif $$(a, b, c) = \\\\text{or}(a, b, c),$$\\n\\nand $$(a, c)$$\\n\\n(15)\\n\\nwhere $\\\\text{if}(a, b, c)$ returns $b$ if $a$ = 0 and $c$ otherwise.\\n\\nNote that we can obtain $n$-ary variants of \\\\texttt{and} and \\\\texttt{or} by chaining multiple instances together.\"}"}
{"id": "marro23a", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6. $F_1$ scores in relation to $\\\\epsilon$ for CIFAR10 A for each considered percentile. For ease of visualization, we set the graph cutoff at $F_1 = 0.8$. We also mark $8/255$ (a common choice for $\\\\epsilon$) with a dotted line.\\n\\nFigure 7. $F_1$ scores in relation to $\\\\epsilon$ for CIFAR10 B for each considered percentile. For ease of visualization, we set the graph cutoff at $F_1 = 0.8$. We also mark $8/255$ (a common choice for $\\\\epsilon$) with a dotted line.\"}"}
{"id": "marro23a", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8. $F_1$ scores in relation to $\\\\varepsilon$ for CIFAR10 C for each considered percentile. For ease of visualization, we set the graph cutoff at $F_1 = 0.8$. We also mark $8/255$ (a common choice for $\\\\varepsilon$) with a dotted line.\"}"}
{"id": "marro23a", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 14. Performance of the strong attack set on MNIST.\\n\\n| Architecture | Training Success Rate | Difference % Below 1/255 | $R^2$ |\\n|--------------|------------------------|---------------------------|------|\\n| MNIST A      | 100.00%                | 1.51%                     | 98.16% 0.996 |\\n| Adversarial  | 100.00%                | 2.48%                     | 81.43% 0.994 |\\n| ReLU         | 100.00%                | 2.14%                     | 84.33% 0.995 |\\n| MNIST B      | 100.00%                | 3.38%                     | 97.36% 0.995 |\\n| Adversarial  | 100.00%                | 4.34%                     | 75.09% 0.991 |\\n| ReLU         | 100.00%                | 4.80%                     | 68.02% 0.992 |\\n| MNIST C      | 100.00%                | 4.52%                     | 96.92% 0.996 |\\n| Adversarial  | 100.00%                | 8.76%                     | 48.78% 0.981 |\\n| ReLU         | 100.00%                | 4.84%                     | 68.24% 0.988 |\\n\\n### Table 15. Performance of the balanced attack set on MNIST.\\n\\n| Architecture | Training Success Rate | Difference % Below 1/255 | $R^2$ |\\n|--------------|------------------------|---------------------------|------|\\n| MNIST A      | 100.00%                | 1.68%                     | 97.94% 0.995 |\\n| Adversarial  | 100.00%                | 2.87%                     | 77.64% 0.993 |\\n| ReLU         | 100.00%                | 2.55%                     | 80.86% 0.993 |\\n| MNIST B      | 100.00%                | 4.09%                     | 96.55% 0.995 |\\n| Adversarial  | 100.00%                | 4.90%                     | 72.60% 0.988 |\\n| ReLU         | 100.00%                | 5.53%                     | 62.96% 0.989 |\\n| MNIST C      | 100.00%                | 5.43%                     | 96.04% 0.995 |\\n| Adversarial  | 100.00%                | 9.50%                     | 48.43% 0.977 |\\n| ReLU         | 100.00%                | 5.28%                     | 66.96% 0.986 |\"}"}
{"id": "marro23a", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 16. Performance of the strong attack set on CIFAR10.\\n\\n| Architecture | Training Success Rate | Difference % | Below 1/255 |\\n|--------------|-----------------------|--------------|-------------|\\n| CIFAR10 A    | Standard              | 100.00%      | 1.62%       | 100.00%     | 0.999       |\\n|              | Adversarial           | 100.00%      | 4.42%       | 95.88%      | 0.995       |\\n|              | ReLU                  | 100.00%      | 0.26%       | 100.00%     | 1.000       |\\n| CIFAR10 B    | Standard              | 100.00%      | 1.44%       | 100.00%     | 0.999       |\\n|              | Adversarial           | 100.00%      | 3.17%       | 97.69%      | 0.997       |\\n|              | ReLU                  | 100.00%      | 1.38%       | 98.81%      | 0.999       |\\n| CIFAR10 C    | Standard              | 100.00%      | 2.11%       | 100.00%     | 0.999       |\\n|              | Adversarial           | 100.00%      | 3.10%       | 97.14%      | 0.996       |\\n|              | ReLU                  | 100.00%      | 2.35%       | 96.12%      | 0.990       |\\n\\n### Table 17. Performance of the balanced attack set on CIFAR10.\\n\\n| Architecture | Training Success Rate | Difference % | Below 1/255 |\\n|--------------|-----------------------|--------------|-------------|\\n| CIFAR10 A    | Standard              | 100.00%      | 1.71%       | 100.00%     | 0.999       |\\n|              | Adversarial           | 100.00%      | 4.18%       | 96.57%      | 0.995       |\\n|              | ReLU                  | 100.00%      | 0.18%       | 100.00%     | 1.000       |\\n| CIFAR10 B    | Standard              | 100.00%      | 1.53%       | 100.00%     | 0.999       |\\n|              | Adversarial           | 100.00%      | 2.92%       | 98.46%      | 0.996       |\\n|              | ReLU                  | 100.00%      | 1.19%       | 98.94%      | 0.999       |\\n| CIFAR10 C    | Standard              | 100.00%      | 2.06%       | 100.00%     | 0.999       |\\n|              | Adversarial           | 100.00%      | 3.12%       | 97.28%      | 0.996       |\\n|              | ReLU                  | 100.00%      | 1.45%       | 97.44%      | 0.995       |\\n\\n### Figure 9.\\n\\nDecision boundary distances found by the attack pools compared to those found by MIPVerify on MNIST A. The black line represents the theoretical optimum. Note that no samples are below the black line.\"}"}
