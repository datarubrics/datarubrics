{"id": "moskovitz23a", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Domain (Constraint) | Algorithm  | Weighted Reward | Task Reward | Constraint Violation |\\n|---------------------|------------|-----------------|-------------|---------------------|\\n| Walker (Height)     | \u00b5-IMPALA   | 354 \u00b1 27.7      | 5.5 \u00b1 3.7   | 350 \u00b1 32.6          |\\n|                     | OGD-IMPALA | 487 \u00b1 3.2       | 5.5 \u00b1 3.8   | 510 \u00b1 15.0          |\\n|                     | \u00b5\u22c6-IMPALA  | 356 \u00b1 32.8      | 5.5 \u00b1 4.6   | 527 \u00b1 65.0          |\\n|                     | PID-IMPALA | 408 \u00b1 12.7      | 5.5 \u00b1 3.4   | 462 \u00b1 8.3           |\\n|                     | RNTR-IMPALA| 386 \u00b1 22.7      | 5.5 \u00b1 2.8   | 399 \u00b1 28.2          |\\n| Reacher (Velocity)  | \u00b5-IMPALA   | 436 \u00b1 31.7      | 5.5 \u00b1 6.8   | 450 \u00b1 68.4          |\\n|                     | OGD-IMPALA | 224 \u00b1 10.9      | 4.6 \u00b1 1.1   | 44 \u00b1 13.1           |\\n|                     | \u00b5\u22c6-IMPALA  | 893 \u00b1 8.5       | 5.5 \u00b1 1.1   | 913 \u00b1 18.0          |\\n|                     | PID-IMPALA | 428 \u00b1 25.6      | 5.5 \u00b1 7.2   | 439 \u00b1 72.0          |\\n|                     | RNTR-IMPALA| 841 \u00b1 102.9     | 5.5 \u00b1 1.1   | 850 \u00b1 103.0         |\\n| Humanoid (Height)   | \u00b5-DMPO    | \u2212254 \u00b1 23.5     | 5.5 \u00b1 8.0   | \u2212558 \u00b1 22.1         |\\n|                     | OGD-DMPO  | \u2212220 \u00b1 20.8     | 6.6 \u00b1 2.0   | \u2212601 \u00b1 20.7         |\\n|                     | \u00b5\u22c6-DMPO   | \u2212328 \u00b1 36.1     | 2.4 \u00b1 1.3   | \u2212359 \u00b1 33.7         |\\n|                     | PID-DMPO  | \u2212232 \u00b1 21.1     | 6.5 \u00b1 2.1   | \u2212602 \u00b1 4.2          |\\n|                     | ReLOAD-DMPO| \u2212137 \u00b1 55.7     | 7.6 \u00b1 2.0   | \u2212615 \u00b1 51.7         |\\n\\nTable 5. Oscillating control suite results for IMPALA-based agents, averaged over 8 random seeds. \u00b1 values indicate one standard error.\\n\\n| Hyperparameter Value | Value                      |\\n|----------------------|----------------------------|\\n| regularization weight | \u03b1_\u03a9 = 1e-2                 |\\n| value loss weight     | \u03b1_V = 0.25                 |\\n| policy loss weight    | \u03b1_\u03c0 = 1.0                  |\\n| discount factor       | \u03b3 = 0.99                   |\\n| RMSProp decay         | \u03f5 = 1e-4                   |\\n| initial step size     | 6e-4                       |\\n| final step size       | 6e-6                       |\\n| max gradient norm     | 0.2                        |\\n| inner loop steps      | 5                          |\\n| initial Bregman step size | 2.0                      |\\n| final Bregman step size | 0.5                        |\\n| initial \u00b5 learning rate | 1e-1                      |\\n| final \u00b5 learning rate  | 1e-3                       |\\n| network hidden units per layer | 256                |\\n| network depth         | 3                          |\\n| training duration     | 10e6                       |\\n\\nTable 7. Hyperparameter settings for IMPALA experiments.\"}"}
{"id": "moskovitz23a", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Hyperparameter           | Value                      |\\n|-------------------------|----------------------------|\\n| batch size              | 256                        |\\n| replay size             | 2e6                        |\\n| optimizer               | Adam                       |\\n| regularization weight   | 1e-2                       |\\n| value loss weight       | 0.25                       |\\n| discount factor         | $\\\\gamma = 0.99$           |\\n| initial step size       | 3e-4                       |\\n| final step size         | 1e-5                       |\\n| initial $\\\\mu$ learning rate | 1e-1              |\\n| final $\\\\mu$ learning rate | 1e-3               |\\n| network hidden units per layer | 256               |\\n| network depth           | 3                          |\\n| training duration       | 15e6                       |\\n\\nTable 8. Hyperparameter settings for DMPO experiments.\"}"}
{"id": "moskovitz23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The gradient descent-ascent updates at step $k$ are:\\n\\\\[ x_{k+1} = x_k - \\\\eta y_k \\\\]\\n\\\\[ y_{k+1} = y_k + \\\\eta x_k. \\\\]\\n\\nFor simplicity of notation we consider a fixed learning rate $\\\\eta$, but the same result is obtained for variable learning rates. This problem has a unique SP at $(x^\\\\star, y^\\\\star) = (0, 0)$. Consider the squared distance from the origin at step $k$,\\n\\\\[ \\\\Delta_k \\\\equiv x_k^2 + y_k^2. \\\\]\\nWe then have\\n\\\\[ \\\\Delta_{k+1} = (x_{k+1})^2 + (y_{k+1})^2 = (x_k - \\\\eta y_k)^2 + (y_k + \\\\eta x_k)^2 = \\\\Delta_k - 2\\\\eta x_k y_k + \\\\eta^2 y_k^2 + \\\\eta^2 x_k^2 = \\\\Delta_k + \\\\eta^2 \\\\Delta_k. \\\\]\\nTherefore, for any $\\\\eta > 0$, the distance from the SP grows with every step of gradient ascent-descent.\\n\\n**Lemma 3.2** (Being Singly-Optimistic is Not Enough).\\nThere exists a convex-concave objective $L$ for which no combination of one OMD and one MD player achieves LIC.\\n\\n**Proof.** Consider\\n\\\\[ \\\\min_x R \\\\max_y xy. \\\\]\\nThe OGD-GA (singly-optimistic) dynamics are given by\\n\\\\[ x_{k+1} = x_k - 2\\\\eta y_k + \\\\eta y_k - 1 y_k \\\\]\\n\\\\[ y_{k+1} = y_k + \\\\eta x_k. \\\\]\\n\\n**Stability analysis:** To facilitate analysis, we can rewrite this (discrete-time) dynamical system by introducing an extra variable $z$ which tracks $y$:\\n\\\\[ x_{k+1} = x_k - 2\\\\eta y_k + \\\\eta z_k \\\\]\\n\\\\[ y_{k+1} = y_k + \\\\eta x_k \\\\]\\n\\\\[ z_{k+1} = y_k. \\\\]\\nThe Jacobian is given by\\n\\\\[ J = \\\\begin{bmatrix} 1 - 2\\\\eta & \\\\eta & 0 \\\\\\\\ \\\\eta & 1 & 0 \\\\\\\\ 0 & 1 & 0 \\\\end{bmatrix}. \\\\]\\nThe eigenvalues $\\\\lambda$ are obtained by solving\\n\\\\[ \\\\det(J - \\\\lambda I) = -\\\\lambda^3 + 2\\\\lambda^2 - (2\\\\eta^2 + 1)\\\\lambda + \\\\eta^2 = 0. \\\\]\\n(18)\\nTwo of the three eigenvalues have modulus greater than one for all $\\\\eta > 0$, so $\\\\rho(J) > 1$ and the system is unstable.\\n\\n**D.2. Analysis of Mixed-Bregman OMD via Monotone Operators**\\n\\n**CMDPs as Monotone Inclusion Problems**\\nThe theory of monotone operators provides a general, powerful framework for analyzing the behavior of convex optimization problems (Ryu & Yin, 2022). While monotone operator theory's generality often facilitates relatively simple proofs of convergence, it can also make it more challenging to show rates of convergence. Nonetheless, as the main goal of our theoretical analysis is to simply demonstrate the fundamental soundness of our proposed approach, monotone operator theory is a useful choice. Specifically, we cast convex-concave SP problems as monotone inclusion problems.\\n\\n**Monotone inclusion problems** take the form\\n\\\\[ \\\\text{find } x \\\\in H \\\\text{ s.t. } 0 \\\\in F(x) \\\\]\\nwhere $F : H \\\\to H$ is a monotone operator and $H$ is a Hilbert space. A Hilbert space is a vector space upon which an inner product $\\\\langle \\\\cdot, \\\\cdot \\\\rangle$ is defined and for which the distance induced by this inner product is a complete metric space (one which contains every Cauchy sequence within it). One example of a vector space is the $d$-dimensional reals $\\\\mathbb{R}^d$, for which the inner product is the standard dot product and the induced metric is the Euclidean distance. A monotone operator is an operator for which the following inequality holds:\\n\\\\[ \\\\langle F(x) - F(y), x - y \\\\rangle \\\\geq 0. \\\\]\"}"}
{"id": "moskovitz23a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for all $x, y \\\\in H$. Often inclusion problems can be made simpler to solve if the operator $F$ can be split into two (or more) operators $F = A + B$, as $A$ and $B$ may be more tractable to use and evaluate separately. We therefore consider inclusion problems of the form\\n\\n$$\\\\text{find } x \\\\in H \\\\text{ s.t. } 0 \\\\in (A + B)(x),$$\\n\\nwhere $A: H \\\\Rightarrow H$ and $B: H \\\\rightarrow H$ are monotone operators. The notation $\\\\Rightarrow$ indicates $A$ is (in the general case) a set-valued operator, one which maps a point in $H$ to a (possibly empty) subset of $H$. A wide variety of problems can be expressed in this form, and in particular we consider the Lagrangian of the CMDP problem expressed as follows:\\n\\n$$\\\\min_{d\\\\pi \\\\in \\\\mathbb{R}^{|S||A|}} \\\\max_{\\\\mu \\\\in \\\\mathbb{R}^N} \\\\mathcal{I}_K(d\\\\pi) + L(d\\\\pi, \\\\mu) + \\\\mathcal{I}_{\\\\mathbb{R}^N} \\\\geq 0(\\\\mu),$$\\n\\nwhere $\\\\mathcal{I}_X(\\\\cdot)$ is the indicator function which equals 0 inside the set $X \\\\subseteq H$ and $\\\\infty$ outside of $X$. We can therefore express the CMDP problem in the form of Eq. (11) by noting that a SP must satisfy the first-order optimality condition:\\n\\n$$\\\\text{find } d\\\\pi, \\\\mu \\\\text{ s.t. } 0 \\\\in \\\\partial \\\\mathcal{I}_K(d\\\\pi) + \\\\partial \\\\mathcal{I}_{\\\\mathbb{R}^N} \\\\geq 0(\\\\mu) + \\\\nabla d\\\\pi L(d\\\\pi, \\\\mu) - \\\\nabla \\\\mu L(d\\\\pi, \\\\mu).$$\\n\\n(21)\\n\\nwhere we can note that $\\\\partial \\\\mathcal{I}_X = N_X$, where $N_X$ is the normal cone operator for $X$ (Ryu & Yin, 2022).\\n\\n(Optimistic) Mirror Descent as Fixed Point Iteration\\n\\nThe mirror descent update for the Bregman divergence $D_\\\\Omega(\\\\cdot; \\\\cdot)$ generated by the strictly convex, continuously differentiable function $\\\\Omega(\\\\cdot)$ and applied to a differentiable loss function $\\\\ell(\\\\cdot)$ is given by\\n\\n$$x_{k+1} = \\\\text{argmin } x \\\\in X \\\\langle \\\\nabla \\\\ell(x_k), x \\\\rangle + 1/\\\\eta_k D_\\\\Omega(x; x_k)$$\\n\\n$$= \\\\text{argmin } x \\\\in \\\\mathbb{R}^d \\\\langle \\\\nabla \\\\ell(x_k), x \\\\rangle + 1/\\\\eta_k D_\\\\Omega(x; x_k) + \\\\mathcal{I}_X(x).$$\\n\\n(22)\\n\\nThis is equivalent to solving the following inclusion problem:\\n\\n$$0 \\\\in \\\\nabla \\\\ell(x_k) + 1/\\\\eta_k (\\\\nabla \\\\Omega(x) - \\\\nabla \\\\Omega(x_k)) + N_X(x)$$\\n\\n$$\\\\iff \\\\nabla \\\\Omega(x_k) - \\\\eta_k \\\\nabla \\\\ell(x_k) \\\\in (\\\\nabla \\\\Omega + \\\\eta_k N_X)(x).$$\\n\\n(24)\\n\\n$$\\\\iff x \\\\in (\\\\nabla \\\\Omega + \\\\eta_k N_X) - 1/\\\\eta_k (\\\\nabla \\\\Omega(x_k) - \\\\eta_k \\\\nabla \\\\ell(x_k)) = \\\\text{Prt}_{\\\\Omega \\\\eta_k N_X}(\\\\nabla \\\\Omega - \\\\eta_k \\\\nabla \\\\ell).$$\\n\\n(25)\\n\\n$$\\\\text{Prt}_{\\\\Omega \\\\eta_k N_X} = (\\\\nabla \\\\Omega + \\\\eta_k N_X) - 1/\\\\eta_k (\\\\nabla \\\\Omega + \\\\eta_k N_X)$$\\n\\nis the proto-resolvent of $\\\\eta_k N_X$ relative to $\\\\Omega$ (Reich & Sabach, 2011). Thus, mirror descent can be seen as performing fixed point iteration (FPI) as follows:\\n\\n$$x_{k+1} = \\\\text{Prt}_{\\\\Omega \\\\eta_k N_X}(\\\\nabla \\\\Omega - \\\\eta_k \\\\nabla \\\\ell)(x_k).$$\\n\\n(27)\\n\\nFor optimistic mirror descent, we write the update as\\n\\n$$x_{k+1} = \\\\text{Prt}_{\\\\Omega \\\\eta_k N_X}(\\\\nabla \\\\Omega(x_k) - \\\\eta_k \\\\nabla \\\\ell(x_k) - 1/\\\\eta_k (\\\\nabla \\\\ell(x_k) - \\\\nabla \\\\ell(x_k - 1))).$$\\n\\n(28)\\n\\nMore generally, for SP problems like Eq. (13), we can think of $x_k = [d_k\\\\pi, \\\\mu_k]^T \\\\in K \\\\times \\\\mathbb{R}^{\\\\geq 0} = X$, where\\n\\n$$A = N_K N_{\\\\mathbb{R}^{\\\\geq 0}}, B = \\\\nabla d\\\\pi L(d\\\\pi, \\\\mu) - \\\\nabla \\\\mu L(d\\\\pi, \\\\mu), \\\\nabla \\\\Omega = \\\\nabla \\\\Omega \\\\pi \\\\nabla \\\\Omega \\\\mu.$$\\n\\n(29)\\n\\nAs a note, in the following analysis we'll frequently consider the Bregman divergence generated by $\\\\Omega$:\\n\\n$$D_\\\\Omega(x_1; x_2) = \\\\Omega(x_1) - \\\\Omega(x_2) - \\\\langle \\\\nabla \\\\Omega(x_2), x_1 - x_2 \\\\rangle.$$\\n\\n(30)\\n\\nIn the \u201cstacked\u201d/SP case, we can consider this to be the stacked divergence where $\\\\Omega = [\\\\Omega \\\\pi, \\\\Omega \\\\mu]^\\\\top$ and $\\\\nabla \\\\Omega$ is as above. We can then write the OMD update as\\n\\n$$x_{k+1} = \\\\text{Prt}_{\\\\Omega \\\\eta_k A}(\\\\nabla \\\\Omega(x_k) - \\\\eta_k B(x_k) - 1/\\\\eta_k (B(x_k) - B(x_k - 1))).$$\\n\\n(31)\\n\\nWhen $\\\\Omega \\\\pi(\\\\cdot) = \\\\Omega \\\\mu(\\\\cdot) = \\\\frac{1}{2} \\\\| \\\\cdot \\\\|_2^2$, this is equivalent to forward-reflected-backward splitting (Malitsky & Tam, 2018).\\n\\nD.3. Convergence Analysis\\n\\nBefore showing convergence, we require several preliminary lemmas. In the following, we assume that $A: H \\\\Rightarrow H$ is maximal monotone and $B: H \\\\rightarrow H$ is monotone and $L$-Lipschitz.\"}"}
{"id": "moskovitz23a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Then for all $k$ we get the desired bound.\\n\\nThen by the monotonicity of $F$, and suppose that $d^2$ is due to the Bregman 3-point identity. Then we can simply add and subtract both sides of\\n\\n$$D \\\\leq \\\\langle -\\\\eta, x \\\\rangle + \\\\langle v, x \\\\rangle - \\\\langle u, d \\\\rangle$$\\n\\nApply Lemma D.1 with $D$ and rearrange to get\\n\\n$$\\\\text{Theorem 3.6}$$\\n\\nProof. Let $F$ be maximal monotone, and let $A$ be given by Eq. (31) and let $L$ be monotone and $\\\\epsilon > 0$. Suppose that $d^2 \\\\in H$ is $\\\\epsilon$-Lipschitz, and let $d^2$ be arbitrary. Define $A = \\\\frac{1}{2} \\\\epsilon$ for some $\\\\epsilon > 0$, and let $d^2 \\\\in H$ as\\n\\n$$d^2 = \\\\frac{1}{2} \\\\epsilon,$$\"}"}
{"id": "moskovitz23a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Then, because $\\\\alpha > 1$ is a constant.\\n\\nCombining this with Eq. (32) gives\\n\\n$$\\\\lim_{k \\\\to \\\\infty} \\\\| x_k - \\\\bar{x} \\\\| = 0.$$ \\n\\nWe can telescope Lemma D.2 to get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $-\\\\text{strongly convex}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nLet $x_k$ be a sequential weak cluster point of the sequence $(x_k)$.\\n\\nWe know that $\\\\| x_k \\\\|^2 \\\\to 0$ as $k \\\\to \\\\infty$, so $x_k$ is bounded and hence there exists a subsequence $(x_{k_i})$ of $(x_k)$ which converges weakly to a point $x$ and, noting that $x_k - x_k = x_k - x$, we get\\n\\n$$\\\\| x_k - \\\\bar{x} \\\\| \\\\to 0.$$ \\n\\nThis result then immediately implies the following corollary when the learning rate is fixed.\\n\\nIn the setting of Theorem 3.6, if $\\\\alpha = \\\\frac{1}{L}$, then $\\\\eta$ is a constant.\\n\\nProof.\\n\\nLet $x_k$ be maximal monotone and let $(x_k)$ be bounded, where $x_k$ converges weakly to a point $\\\\bar{x}$.\\n\\nSo that\\n\\n$$\\\\lim_{k \\\\to \\\\infty} \\\\| x_k - \\\\bar{x} \\\\| = 0.$$ \\n\\nTherefore, we can see that $x_k$ converges weakly to a point contained in $\\\\Omega$. Then $\\\\Omega$ is a closed subset of $\\\\mathcal{H}$, and $\\\\Omega$ is demiclosed.\\n\\nTake the limit along a subsequence of\\n\\n$$\\\\lim_{k \\\\to \\\\infty} x_k = x.$$\\n\\nWe define the sequence $(x_k)$ according to Eq. (33) and, noting that $x_k - x_k = x_k - x$, we get\\n\\n$$\\\\lim_{k \\\\to \\\\infty} \\\\| x_k - \\\\bar{x} \\\\| = 0.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that our choice of $\\\\Omega$ is $\\\\text{strongly convex}$, so that\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly monotone}$, we have\\n\\n$$\\\\langle z, \\\\nabla \\\\sigma(x_k) \\\\rangle \\\\leq \\\\| \\\\sigma(x_k) \\\\|^2 + 1,$$\\n\\nwhich converges to\\n\\n$$\\\\| \\\\sigma(x_k) \\\\|^2 + 1.$$ \\n\\nBy subtracting $\\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2 = 4\\\\| x - \\\\bar{x} \\\\|^2 + 1$ from both sides and rearranging, we get\\n\\n$$D_k \\\\leq 4\\\\| x - \\\\bar{x} \\\\|^2 + 1,$$\\n\\nwhere $D_k = \\\\| x_k \\\\|^2 - 2 \\\\langle x - \\\\bar{x}, \\\\nabla \\\\sigma(x_k) \\\\rangle + \\\\| \\\\sigma(x_k) \\\\|^2$. Note that we do not lose generality here because if $B$ is $\\\\text{strongly"}
{"id": "moskovitz23a", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13. Learning curves for $\\\\mu$-DMPO and ReLOAD-DMPO on Walker, Walk with a velocity constraint (left), Quadruped, Walk with a torque constraint (center), and Humanoid, Walk with a height constraint (right). The horizontal dotted line indicates the value of the constraint. In each case, ReLOAD-DMPO significantly dampens oscillations compared to $\\\\mu$-DMPO.\\n\\nFigure 14. ReLOAD-DMPO outperforms $\\\\mu$-DMPO and matches the performance of MO-DMPO on the RWRL Suite. Here, we trained agents on the comparatively easy RWRL settings used by (Huang et al., 2022) over 8 random seeds. Error bars denote one standard error.\"}"}
{"id": "moskovitz23a", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2. Where lower values of the safety coefficient and threshold indicate more challenging tasks.\\n\\n| Domain | Algorithm | Safety-Coeff/Threshold | Weighted Reward |\\n|--------|-----------|-------------------------|-----------------|\\n| RWRL-Walker | 0.1 | 0.05/0.057 | 367 |\\n| ReLOAD | 0.05/0.097 | 0.05/0.077 | 315 |\\n| | 0.2/0.097 | 0.2/0.077 | 882 |\\n| | 0.3/0.097 | 0.3/0.077 | 930 |\\n| | 0.1/0.097 | 0.1/0.077 | 717 |\\n| | 0.3/0.097 | 0.2/0.077 | 979 |\\n| | 0.2/0.057 | 0.1/0.077 | 956 |\\n| | 0.1/0.097 | 0.1/0.057 | 942 |\\n| | 0.3/0.097 | 0.2/0.077 | 982 |\\n| | 0.2/0.057 | 0.1/0.097 | 652 |\\n| | 0.1/0.077 | 0.1/0.057 | 914 |\\n| | 0.3/0.077 | 0.2/0.077 | 980 |\\n| | 0.2/0.057 | 0.1/0.097 | 630 |\\n| | 0.1/0.077 | 0.1/0.057 | 914 |\\n| | 0.3/0.097 | 0.2/0.077 | 982 |\\n| | 0.2/0.057 | 0.1/0.097 | 597 |\"}"}
{"id": "moskovitz23a", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Domain | Algorithm | Safety-Coeff/Threshold | Weighted Reward | Mean performance for |\\n|--------|-----------|------------------------|----------------|---------------------|\\n| RWRL-Cartpole | ReLOAD 0.05/0.07 | 0.05/0.115 | 0.05/0.09 | 0.3/0.115 |\\n| | MetaL 0.05/0.07 | 0.05/0.115 | 0.05/0.09 | 0.3/0.115 |\\n\\nValues denote one standard error.\"}"}
{"id": "moskovitz23a", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4. Suite, where lower values of the safety coefficient and threshold indicate more challenging tasks.\\n\\n| Task Reward Constraint Violation | Quadruped | 0.1-D4PG | 0.05/0.545 | Domain Algorithm | Safety-Coeff/Threshold | Weighted Reward |\\n|---------------------------------|-----------|----------|------------|------------------|------------------------|-----------------|\\n|                                 | RWRL-Quadruped | 651 \u00b1 8 | 657 \u00b1 9 | MetaL | 0.05/0.645 | 0.05/0.745 |\\n|                                 | RC-D4PG | 0.05/0.645 | 0.05/0.745 |        |            |                |\\n|                                 | 997 \u00b1 2 | 988 \u00b1 3 | 998 \u00b1 4 | 999 \u00b1 5 | 999 \u00b1 6 | 999 \u00b1 7 |\\n|                                 | 0.1/0.545 | 0.3/0.545 | 0.2/0.545 | 0.3/0.745 | 0.2/0.745 | 0.2/0.645 |\\n|                                 | 997 \u00b1 0 | 998 \u00b1 1 | 999 \u00b1 2 | 0.999 \u00b1 3 | 0.999 \u00b1 4 | 0.999 \u00b1 5 |\\n|                                 | 0.1/0.745 | 0.1/0.645 | 0.1/0.545 | 0.3/0.745 | 0.2/0.745 | 0.2/0.645 |\\n|                                 | 996 \u00b1 4 | 999 \u00b1 5 | 999 \u00b1 5 | 999 \u00b1 6 | 999 \u00b1 7 | 999 \u00b1 8 |\\n|                                 | 0.999 \u00b1 0 | 0.999 \u00b1 1 | 0.999 \u00b1 2 | 0.999 \u00b1 3 | 0.999 \u00b1 4 | 0.999 \u00b1 5 |\\n|                                 | 0.999 \u00b1 0 | 0.999 \u00b1 1 | 0.999 \u00b1 2 | 0.999 \u00b1 3 | 0.999 \u00b1 4 | 0.999 \u00b1 5 |\\n|                                 | 0.999 \u00b1 0 | 0.999 \u00b1 1 | 0.999 \u00b1 2 | 0.999 \u00b1 3 | 0.999 \u00b1 4 | 0.999 \u00b1 5 |\\n\\nValues denote one standard error.\"}"}
{"id": "moskovitz23a", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 5\\n\\n\\\\textbf{ReLOAD-IMPALA}\\n\\n\\\\textbf{Loss:}\\n\\n\\\\begin{align*}\\nL(\\\\theta^k(i), \\\\eta^k, \\\\mu^k, \\\\pi_{\\\\theta^k}(\\\\cdot|s_t), \\\\pi_{\\\\theta^k-1}(\\\\cdot|s_t), v_{\\\\theta^k-1}(s_t), v_{\\\\theta^k}(s_t), \\\\log \\\\nu(at|s_t), \\\\rho_{\\\\theta^k-1}(s_t, a_t), \\\\mu^k-1, \\\\{\\\\tau_n\\\\}_{n=1}^N) = \\\\end{align*}\\n\\n1: Unroll trajectories:\\n\\\\begin{align*}\\n\\\\{\\\\pi_{\\\\theta^k}(i)(\\\\cdot|s_t), v_{\\\\theta^k}(i)(s_t), v_{\\\\theta^k-1}(s_t)} \\\\leftarrow \\\\text{Unroll}(\\\\{\\\\tau_n\\\\}, \\\\theta^k(i))\\n\\\\end{align*}\\n\\n2: Compute importance ratios wrt behavior policy $\\\\nu$:\\n\\\\begin{align*}\\n\\\\rho_{\\\\theta^k}(i)(s_t, a_t) = \\\\pi_{\\\\theta^k}(i)(a_t|s_t) \\\\nu(a_t|s_t)\\n\\\\end{align*}\\n\\n3: Compute rewards:\\n\\\\begin{align*}\\nr_{\\\\theta^k}(i) = \\\\text{ComputeRewards}(r(t), \\\\theta(i))\\n\\\\end{align*}\\n\\n4: Compute TD errors and advantages:\\n\\\\begin{align*}\\n\\\\delta_{\\\\theta^k}(i), A_{\\\\theta^k}(i) \\\\leftarrow \\\\text{V-Trace}(v_{\\\\theta^k}(i), r_{\\\\theta^k}(i), \\\\rho_{\\\\theta^k}(i)); \\\\delta_{\\\\theta^k-1}(i), A_{\\\\theta^k-1}(i) \\\\leftarrow \\\\text{V-Trace}(v_{\\\\theta^k-1}(i), r_{\\\\theta^k-1}(i), \\\\rho_{\\\\theta^k-1}(i))\\n\\\\end{align*}\\n\\n5: Weight cumulants:\\n\\\\begin{align*}\\nA_{\\\\mu^k}(i) \\\\leftarrow (1 - \\\\sigma(\\\\mu^k)) A_{\\\\theta^k}(i) - \\\\sigma(\\\\mu^k) A_{\\\\theta^k-1}(i); A_{\\\\mu^k-1}(i) \\\\leftarrow (1 - \\\\sigma(\\\\mu^k-1)) A_{\\\\theta^k-1}(i) - \\\\sigma(\\\\mu^k-1) A_{\\\\theta^k-1}(i)\\n\\\\end{align*}\\n\\n6: Compute optimistic advantages:\\n\\\\begin{align*}\\n\\\\tilde{A}_{\\\\mu^k}(i) \\\\leftarrow 2A_{\\\\mu^k}(i) - A_{\\\\mu^k-1}(i)\\n\\\\end{align*}\\n\\n7: Policy loss:\\n\\\\begin{align*}\\nL_{\\\\pi} = \\\\sum_t -\\\\rho_{\\\\theta^k}(i)(s_t, a_t) \\\\tilde{A}_{\\\\mu^k}(i)(s_t, a_t) \\\\log \\\\pi_{\\\\theta^k}(i)(a_t|s_t) + \\\\eta_k \\\\text{KL}[\\\\pi_{\\\\theta^k}(\\\\cdot|s_t), \\\\pi_{\\\\theta^k-1}(\\\\cdot|s_t)]\\n\\\\end{align*}\\n\\n8: Value loss:\\n\\\\begin{align*}\\nL_V = \\\\sum_t \\\\delta_{\\\\theta^k}(i)(s_t, a_t)^2 + \\\\delta_{\\\\theta^k-1}(i)(s_t, a_t)^2\\n\\\\end{align*}\\n\\n9: Regularization loss:\\n\\\\begin{align*}\\nL_{\\\\Omega} = \\\\sum_t \\\\text{KL}[\\\\pi_{\\\\theta^k}(\\\\cdot|s_t)||N(0, I)]\\n\\\\end{align*}\\n\\n10: Return $\\\\alpha_{\\\\pi} L_{\\\\pi} + \\\\alpha_V L_V + \\\\alpha_{\\\\Omega} L_{\\\\Omega}$\\n\\nreturns from across training and uses them to compute constraint violations.\\n\\nMetaL\\n\\nMeta-gradients for the Lagrange learning rate (MetaL; Calian et al., 2020) extends RC-D4PG by using meta-gradients (Xu et al., 2018) to meta-learn the learning rate for the Lagrange multiplier $\\\\eta$ with the goal of stabilizing optimization. Specifically, they define a set of inner losses for the actor, critic, and Lagrange multiplier corresponding to the standard components of Lagrangian optimization for CMDPs while also defining an outer loss $L_{\\\\text{outer}} = L_{\\\\text{critic}}(\\\\theta^{k+1} q(\\\\eta^k \\\\mu), \\\\mu^{k+1}(\\\\eta^k \\\\mu))$. That is, the outer loss is the standard critic loss evaluated using the updated critic parameters and Lagrange multipliers from the inner loop (which are themselves functions of the current Lagrange multiplier learning rate). The Lagrange multiplier learning rate is then updated using meta-gradients on this loss.\\n\\nG. Further Experimental Details\\n\\nPerformance Measures\\n\\nIn simple cases like the paradoxical CMDP, we know the optimal solution exactly and can compute, e.g., the $L_2$ distance between the iterates produced by the algorithm $(d_k \\\\pi, \\\\mu_k)$ and the SP $(d^\\\\star \\\\pi, \\\\mu^\\\\star)$. In more\"}"}
{"id": "moskovitz23a", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ReLOAD for Last-Iterate Convergence in Constrained MDPs\\n\\nAlgorithm 6 ReLOAD-MDPI\\n\\n1: \\\\text{Require: } CMDP \\\\ M, \\\\text{ step sizes } \\\\{\\\\eta_\\\\pi, \\\\eta_\\\\mu\\\\} > 0\\n\\n2: \\\\text{Initialize } \\\\pi_1, \\\\mu_1, \\\\pi_0, \\\\mu_0\\n\\n3: \\\\text{for } k = 1, \\\\ldots, K \\\\text{ do }\\n\\n4: \\\\quad \\\\{q_n\\\\}_{n=0}^N \\\\leftarrow \\\\text{PolicyEval}(M, \\\\pi_k)\\n\\n5: \\\\quad q_k^\\\\mu \\\\leftarrow q_k^0 - \\\\sum_{n=1}^N \\\\mu_k^n q_k^n \\\\text{ (mixed q-values)}\\n\\n6: \\\\quad v_k^1 = \\\\left[ q_k^1, \\\\pi_k, \\\\ldots, q_k^N, \\\\pi_k \\\\right]^\\\\top\\n\\n7: \\\\quad \\\\text{Update policy with OMWU: } \\\\pi_{k+1} = \\\\pi_k \\\\exp \\\\left( \\\\frac{2q_k^\\\\mu - q_k^0}{\\\\eta_\\\\pi} \\\\right)\\n\\n8: \\\\quad \\\\text{Update Lagrange multiplier with projected OGA: } \\\\mu_{k+1} = \\\\max \\\\left\\\\{ \\\\mu_k + \\\\eta_\\\\mu \\\\left( \\\\frac{2v_k^1 - v_k^0}{\\\\theta} \\\\right), 0 \\\\right\\\\}\\n\\n9: \\\\text{end for }\\n\\n10: \\\\text{return } \\\\pi_K, \\\\mu_K\\n\\nComplex settings such as the RWRL suite and control suite, previous work, e.g., Calian et al. (2020) and Huang et al. (2022) has used the penalized reward $R_{\\\\text{penalized}}$:\\n\\n$$R_{\\\\text{penalized}} = R^0 - \\\\sum_{n=1}^{N} \\\\max \\\\{ R_n - \\\\theta_n, 0 \\\\},$$\\n\\nwhich is the average task return $R^0$ minus the constraint overshoot \u2014 how much the average constraint returns violate their associated constraint thresholds. (No bonus is given for being further beneath the threshold than necessary.) However, this metric ignores the effect of the Lagrange multiplier, which in Lagrangian optimization acts on a coefficient on the constraint term, e.g.,\\n\\n$$L = v_0 + \\\\mu (v_1 - \\\\theta_1)$$\\n\\nfor a single constraint. We therefore prefer the weighted reward $R_{\\\\text{weighted}}$ given by\\n\\n$$R_{\\\\text{weighted}} = R^0 - \\\\sum_{n=1}^{N} \\\\mu^* n \\\\max \\\\{ R_n - \\\\theta_n, 0 \\\\},$$\\n\\nwhere $\\\\mu^* n$ is the optimal Lagrange multiplier. In our case, for large-scale environments we use a sigmoid to bound the Lagrange multiplier as described in Appendix F. Therefore, because the agent is maximizing the weighted value/advantage $(1 - \\\\sigma(\\\\mu)) A_0 + \\\\sigma(\\\\mu) A_1$, we divide by $1 - \\\\sigma(\\\\mu)$ to give\\n\\n$$R_{\\\\text{weighted}} = R^0 - \\\\sum_{n=1}^{N} \\\\hat{\\\\mu}^* n \\\\max \\\\{ R_n - \\\\theta_n, 0 \\\\}$$\\n\\nwhere $\\\\hat{\\\\mu}^* = \\\\sigma(\\\\mu)^* (1 - \\\\sigma(\\\\mu)^*)$.\\n\\nThis approach is also used by Stooke et al. (2020) and Zahavy et al. (2022). To estimate the optimal Lagrange multiplier(s), we ran $\\\\mu$-agents for 8 random seeds on each task and averaged the Lagrange multipliers, as they enjoy guarantees for average-iterate convergence.\\n\\nThe Paradoxical CMDP Tabular agents were trained for 500 episodes, with each episode lasting 10 time steps and with the agent starting in a uniformly-randomly chosen state. The discount factor $\\\\gamma$ was 0.9. $\\\\mu$-MDPI-Avg was obtained by maintaining a running average of the iterates of $\\\\mu$-MDPI. DRL agents ($\\\\mu$-MDPO and ReLOAD-MDPO) were trained for 30,000 episodes. The policy was parameterized as an MLP with a single hidden layer with 16 units. States were provided as a one-hot encoding. The agents were trained using RMSProp with an initial learning rate of 6e-4, which decayed linearly to 1e-4 over the course of training. There were 5 inner loop steps to approximate the MD update with a fixed MD step size of 0.25.\\n\\nCatch The Catch task was as described by Osband et al. (2019) with the addition of the constraint reward and threshold as described in Section 5. $\\\\mu$-MDPO and ReLOAD-MDPO agents were trained for 30,000 episodes with the same hyperparameter settings as for the Paradoxical CMDP, with the exception that the policy network had two hidden layers, each with 32 hidden units.\"}"}
{"id": "moskovitz23a", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Domain, Task | Constrained Quantity | Observation Dims\\n---|---|---\\nWalker | Walk Height | 24 0.5\\nWalker | Walk Velocity | 14-23 155\\nReacher | Easy Velocity | 4-5 1.02\\nQuadruped | Walk Torque | 54-77 610\\nHumanoid | Walk Height | 21 400\\n\\nTable 1. Experiment settings for oscillating control suite experiments. For multidimensional quantities, the constraint was placed on the $L^2$ norm.\\n\\nThe RWRL Suite\\n\\nHyperparameters and training regimes for 0.1-D4PG, RC-D4PG, and MetaL are as reported by Calian et al. (2020). For $\\\\mu$-DMPO and ReLOAD-DMPO, hyperparameters are given in Table 8. All task settings are as reported in Calian et al. (2020).\\n\\nOscillating Control Suite\\n\\nControl suite domains and accompanying thresholds are described in Table 1. OGD-IMPALA, DMPO is implemented by augmenting $\\\\mu$-IMPALA, DMPO with the optimistic gradient descent optimizer from the Optax library, whose code is available at: https://github.com/deepmind/optax/blob/master/optax/_src/alias.py. PID control is implemented as described in Stooke et al. (2020) with the same values of $K_P = 0.95$ and $K_D = 0.9$ as used in that work. For IMPALA, which does not normally have a trust region component to the policy update, RNTR-IMPALA implemented ReLOAD without a trust region (that is, only using optimistic advantages/value estimates).\\n\\nFurther Experimental Results\\n\\nFigure 10. Tabular PEG-based policy iteration in the paradoxical CMDP. PEG-PI uses projected optimistic gradient descent instead of MWU for the policy, while PEG-MDPI uses optimistic MWU for the policy update, which here produces the same updates as ReLOAD. Both variants converge in the last-iterate.\"}"}
{"id": "moskovitz23a", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11. Policy evaluation and ablations in the paradoxical CMDP.\\n\\n(a) To demonstrate the importance of accurate value estimation on performance, we plotted the $L^2$ distance of the final $(\\\\pi_K, \\\\mu_K)$ pair obtained by ReLOAD-MDPO from the SP of the paradoxical CMDP in Fig. 3a as a function of the average error in the value estimates over the course of training. We can see that performance gets dramatically worse as the value function error increases, and that adding standard techniques like batching and Huber loss rather than squared error loss improves value estimates sufficiently to allow for strong performance of the overall algorithm. (b) Here, we compare ReLOAD-MDPO against various ablations by measuring the $L^2$ distance from the SP of the paradoxical CMDP over the course of training. We can see that both $\\\\mu$-MDPO and ReLOAD without a trust region (RNTR-) fail to converge. However, performing OGD directly on the policy parameters, rather than via optimistic value estimates, performs nearly as well as ReLOAD. This is likely because the CMDP only has two states, so using the gradient computed from value estimates obtained from an old batch of data will likely be close to that obtained from the current batch of data, as there's more likely to be significant overlap. If we look to higher-dimensional tasks like control suite (Fig. 8), there is a much larger gap between the direct OGD approach and ReLOAD as the state-action pairs used for value estimation are more likely to differ from one minibatch to another.\\n\\nFigure 12. Learning curves for $\\\\mu$-IMPALA and ReLOAD-IMPALA on Walker, Walk with a height constraint (left) and Reacher, Easy with a velocity constraint (right). The horizontal dotted line indicates the value of the constraint. In each case, ReLOAD-IMPALA significantly dampens oscillations compared to $\\\\mu$-IMPALA.\"}"}
{"id": "moskovitz23a", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We'd like to rewrite the LHS of the inequality in terms of $m$.\\n\\nThen because $\\\\Omega$, we have\\n\\nConsider the LHS. Add and subtract $\\\\epsilon b$ to get\\n\\n$\\\\eta$ is the learning rate, strongly monotone while preserving the monotonicity of $B$.\\n\\nWe can write\\n\\nLet $2^{\\\\eta m}$, thereby making\\n\\n$D$.\\n\\nTheorem\\n\\nWe can write\\n\\n$A$. Observe that\\n\\nNow, set $k = \\\\frac{1}{\\\\epsilon}$, a constant\\n\\nReLOAD for Last-Iterate Convergence in Constrained MDPs\"}"}
{"id": "moskovitz23a", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"min_n 2 \\\\nabla \\\\Omega - 1 + 4 \\\\eta_m \\\\nabla \\\\Omega - \\\\varepsilon 2 - \\\\varepsilon 2 \\\\nabla \\\\Omega , 1 + \\\\varepsilon 2 o > 1 due to \\\\varepsilon \\\\leq 4 \\\\sigma - 2 \\\\nabla \\\\Omega + 8 \\\\eta_m \\\\nabla \\\\Omega + 2 . We then get\\n\\\\alpha a_{k+1} + b_{k+1} + \\\\varepsilon 2 \\\\| x_{k+1} - x_k \\\\| 2 \\\\leq a_k + b_k \\\\Rightarrow \\\\alpha (a_{k+1} + b_{k+1}) \\\\leq a_k + b_k ,\\nwhich, iterating, yields\\na_{k+1} + b_{k+1} \\\\leq 1 \\\\alpha (a_k + b_k) \\\\leq 1 \\\\alpha \\\\frac{1}{\\\\alpha k} (a_0 + b_0) . Therefore,\\nx_k \\\\rightarrow x with rate O(1/\\\\alpha k), and since x was arbitrarily chosen, it is unique.\\n\\nCorollary 4.1 (ReLOAD Convergence).\\nThe sequence ((d_k \\\\pi, \\\\mu_k)) generated by Eq. (14) converges in the last-iterate for \\\\eta \\\\in (0, 1/2).\\n\\nProof. To connect this problem to Theorem 3.6, we can take advantage of the fact that monotonicity is preserved by concatenation (Ryu & Yin, 2022) and set\\nx_k = [d_k \\\\pi, \\\\mu_k]_T \\\\in K \\\\times \\\\mathbb{R} \\\\geq 0 = X\\nsuch that\\nA = N K N R \\\\geq 0\\nB = \\\\nabla d \\\\pi L - \\\\nabla \\\\mu L \\\\nabla \\\\Omega = \\\\nabla \\\\Omega \\\\pi \\\\nabla \\\\Omega \\\\mu\\n(35)\\nWe can further confirm that the normal cone operator is maximal monotone (Ryu & Yin, 2022) and that \\\\nabla L is monotone and 1-Lipschitz, as the problem is bilinear. We then get the desired result by applying Corollary D.3.\"}"}
{"id": "moskovitz23a", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Note that this requires a value function estimate for each constraint reward in addition to the task reward. We then substituted the mixed advantages for the standard advantages in Eq. (36) to update the policy. To compute the Lagrange multiplier update, we computed the average constraint violation (gradient) on a minibatch of $L$ states obtained from rollouts of the current policy:\\n\\n$$\\\\Delta_k^n = \\\\frac{1}{L} \\\\sum_{\\\\ell=1}^L (v_{\\\\pi}(s_\\\\ell) - \\\\theta_n)$$\\n\\nand updated the Lagrange multiplier(s) as\\n\\n$$\\\\mu_{k+1}^n = \\\\max\\\\{\\\\mu_k^n + \\\\eta \\\\Delta_k^n, 0\\\\}$$\\n\\nfor $n = 1, 2, \\\\ldots, N$.\\n\\nTo add ReLOAD, we repeated the above steps but used the optimistic mixed advantages $\\\\tilde{A}_{\\\\pi k} = 2A_{\\\\pi k} - A_{\\\\pi k-1}$ and optimistic values to compute the constraint violations:\\n\\n$$\\\\tilde{v}_{\\\\theta k}^n(s_\\\\ell) = 2v_{\\\\theta k}^n(s_\\\\ell) - v_{\\\\theta k-1}^n(s_\\\\ell)$$\\n\\nfor $n = 1, 2, \\\\ldots, N$.\\n\\nTo compute the optimistic advantages and values, we maintain a copy of the previous parameters $\\\\theta_{k-1}$ to evaluate on the new trajectories collected by $\\\\pi_{\\\\theta_k}$. In practice, we also use entropy regularization on the policy, making our implementation of $\\\\mu$-MDPO equivalent to magnetic mirror-descent (MMD; Sokota et al., 2022).\\n\\n**IMPALA**\\n\\nThe Importance-Weighted Actor-Learner Architecture (IMPALA; Espeholt et al., 2018) is a distributed actor-critic agent which uses a set of actors to generate trajectories of experience which are then used by one or more learners to update the policy and value function parameters off-policy. To correct for the disjunction between the actor parameters and the learner parameters, updates are computed using an importance-weighting correction technique called V-trace. Unlike the other base agents with which we paired ReLOAD, IMPALA does not by default use a trust region approach for policy optimization. Therefore, our first modification was to add one via an inner loop in the style of MDPO. We then also added modifications for Lagrangian optimization and optimism as in MDPO. However, since we applied the IMPALA-based agents to large-scale tasks, optimization was inherently less stable, and so we used a sigmoid function to bound the Lagrange multiplier as done in Zahavy et al. (2021b); Stooke et al. (2020), leading to mixed advantages of the form:\\n\\n$$A_{\\\\theta k}^n(s, a) = \\\\sigma(\\\\mu_k^n) A_{\\\\theta 0}(s, a) + \\\\sigma(\\\\mu_k^n) A_{\\\\theta k}^n(s, a)$$\\n\\nThe overall procedure for the learner is summarized in Algorithm 3, Algorithm 4, and Algorithm 5. Code for V-trace can be found at this link: https://github.com/deepmind/rlax/blob/master/rlax/_src/vtrace.py.\\n\\n**Distributional MPO (DMPO)**\\n\\nMaximum a posteriori policy optimization (MPO; Abdolmaleki et al., 2018) is a deep policy iteration-style algorithm whose improvement step can be broken into two stages which are reminiscent of the \\\"E\\\" and \\\"M\\\" steps of the EM algorithm. In the E-step, a non-parametric variational \\\"policy\\\" $\\\\nu_k(a|s)$ is constructed using off-policy samples as follows:\\n\\n$$\\\\nu_k(a|s) \\\\propto \\\\pi_{\\\\theta_k}(a|s) \\\\exp q_{\\\\theta_k}(s, a) \\\\omega^\\\\star$$\\n\\nwhere $\\\\omega^\\\\star$ is the minimizer of the convex dual function\\n\\n$$g(\\\\omega) = \\\\omega \\\\epsilon + \\\\mathbb{E}_{s \\\\sim \\\\pi} \\\\mathbb{E}_{a \\\\sim \\\\pi_{\\\\theta_k}} \\\\exp \\\\left( \\\\frac{q_{\\\\theta_k}(s, a)}{\\\\omega} \\\\right)$$\\n\\nwhere $\\\\epsilon$ is a constant defining the radius of a KL trust region around the current policy. This (soft-)improved policy is then distilled into the parametric policy in an approximate \\\"M\\\" step using another KL trust region:\\n\\n$$\\\\theta_{k+1} = \\\\arg\\\\max_{\\\\theta} \\\\mathbb{E}_{d_\\\\nu} \\\\mathbb{E}_{\\\\nu_k} \\\\log \\\\pi_{\\\\theta}(a|s) \\\\ s.t. \\\\ KL[\\\\pi_{\\\\theta_k}(\\\\cdot|s)||\\\\pi_{\\\\theta}(\\\\cdot|s)] < \\\\epsilon.$$ \\n\\nIn distributional MPO (DMPO; Abdolmaleki et al., 2020), in order to provide more accurate value estimates, the standard critic is replaced with a distributional critic using a categorical representation of the return distribution (Bellemare et al., 2017). To add Lagrangian optimization to DMPO ($\\\\mu$-DMPO), we used the same strategy as in IMPALA to update both the Lagrange multipliers and to compute the mixed $q$-values $q_{\\\\mu_{\\\\theta_k}}(s, a)$ (rather than advantages in this case) to plug into Eq. (37). Similarly, for ReLOAD we computed the optimistic mixed $q$-values $\\\\tilde{q}_{\\\\mu_{\\\\theta_k}}(s, a) = q_{\\\\mu_{\\\\theta_k}}(s, a) - q_{\\\\mu_{\\\\theta_k-1}}(s, a)$. \\n\\n\"}"}
{"id": "moskovitz23a", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ReLOAD-IMPALA\\n\\nAlgorithm 4\\n\\n\\\\[ \\\\beta \\\\]\\n\\nwhere (RC-D4PG; Calian et al., 2020) augments D4PG for Lagrangian optimization in the same way as \\n\\n\\\\[ \\\\mu \\\\]\\n\\n\\\\( \\\\theta \\\\)\\n\\nvariational policy for each task and constraint reward\\n\\nconstraint value estimates obtained from samples from the most recent policy, RC-D4PG maintains a buffer of constraint augmentation their respective base algorithms, with the exception that rather than update the Lagrange multiplier by using\\n\\n\\\\[ \\\\nu \\\\]\\n\\ncritics using a categorical representation for the return distribution (Bellemare et al., 2017). Reward-constrained D4PG\\n\\ndistributional DDPG (D4PG; Barth-Maron et al., 2018) is a distributed algorithm which augments DDPG with distributional\\n\\n\\\\[ \\\\pi \\\\]\\n\\nalgorithm which uses a deterministic policy\\n\\nReward-Constrained D4PG (RC-D4PG)\\n\\nDeep deterministic policy gradients (DDPG; Lillicrap et al., 2015) is a DRL\\n\\nMulti-Objective MPO (MO-MPO)\\n\\nRather than scalarize the task and constraint rewards into a single, non-stationary\\n\\nMO-MPO then distills each of these improved policies into the parametric policy:\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\nUnroll previous parameters:\\n\\n\\\\[ \\\\pi \\\\]\\n\\nUnroll current parameters:\\n\\n\\\\[ \\\\pi \\\\]\\n\\nCompute previous policy importance weights:\\n\\n\\\\[ \\\\rho \\\\]\\n\\nGetFixedOutputs\\n\\nCompute behavior policy log-probs\\n\\nfor\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ 0 \\\\]\\n\\nend for\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\mu \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\nSet inner loop parameters\\n\\nUnroll previous parameters:\\n\\n\\\\[ \\\\pi \\\\]\\n\\nrequire: network parameters\\n\\n\\\\[ \\\\pi \\\\]\\n\\nrequire: network parameters\\n\\nUnroll current parameters:\\n\\n\\\\[ \\\\pi \\\\]\\n\\nCompute trust region step-size:\\n\\n\\\\[ \\\\eta \\\\]\\n\\n\\\\[ 1 \\\\]\\n\\n\\\\[ s \\\\]\\n\\n\\\\[ t \\\\]\\n\\n\\\\[ 0 \\\\]\\n\\n\\\\[ 1 \\\\]\\n\\nfor\\n\\n\\\\[ \\\\pi \\\\]\\n\\nUnroll\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\nUnroll\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\rho \\\\]\\n\\n\\\\[ \\\\log \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ \\\\"}
{"id": "moskovitz23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ReLOAD: Reinforcement Learning with Optimistic Ascent-Descent for Last-Iterate Convergence in Constrained MDPs\\n\\nTed Moskovitz\\nBrendan O'Donoghue\\nVivek Veeriah\\nSebastian Flennerhag\\nSatinder Singh\\nTom Zahavy\\n\\nAbstract\\nIn recent years, Reinforcement Learning (RL) has been applied to real-world problems with increasing success. Such applications often require to put constraints on the agent's behavior. Existing algorithms for constrained RL (CRL) rely on gradient descent-ascent, but this approach comes with a caveat. While these algorithms are guaranteed to converge on average, they do not guarantee last-iterate convergence, i.e., the current policy of the agent may never converge to the optimal solution. In practice, it is often observed that the policy alternates between satisfying the constraints and maximizing the reward, rarely accomplishing both objectives simultaneously. Here, we address this problem by introducing Reinforcement Learning with Optimistic Ascent-Descent (ReLOAD), a principled CRL method with guaranteed last-iterate convergence. We demonstrate its empirical effectiveness on a wide variety of CRL problems including discrete MDPs and continuous control. In the process we establish a benchmark of challenging CRL problems.\\n\\n1. Introduction\\nFrom navigating stratospheric balloons through the atmosphere to coordinating plasma control for nuclear fusion research, reinforcement learning (RL; Sutton & Barto, 2018) has proved increasingly effective at solving real-world problems (Bellemare et al., 2020; Degrave et al., 2022). In RL, an agent interacts with an environment over a series of time steps with the goal of maximizing its expected cumulative reward. When developing intelligent systems which must learn and act in the real world, it's often the case that constraints are placed on agents to ensure that certain safety or efficiency requirements are satisfied. Examples range from training a robot to run while avoiding placing too much torque on its joints, to video compression (Mandhane et al., 2022), to maximizing the efficiency of commercial cooling systems under stability constraints (Luo et al., 2022).\\n\\nA natural question, then, is how to solve such constrained tasks. In standard RL, we often look to the Reward Hypothesis, which postulates that all goals and purposes of an intelligent agent can be achieved by maximization of a scalar reward (Sutton, 2004). Szepesv\u00e1ri (2020) studied the implications of this hypothesis to CRL. Accordingly, constrained RL problems can be solved by integrating the constraints and task reward into a single, non-stationary reward signal (Altman, 1999). However, this approach carries a subtle but important challenge that can be easily overlooked: gradient-based optimization for such constrained problems only guarantees that the average of the agent's behavior over the course of training converges to an optimal solution, with no assurances for the final policy. Unfortunately, simple solutions like averaging model parameters are ineffective when the policy is implemented as a deep neural network. While this problem has been studied in the context of GANs (Daskalakis et al., 2018; Balduzzi et al., 2018), it has not been addressed within CRL, which brings additional complications. As an illustration, consider the problem of training an agent in the Walker domain in DeepMind Control Suite (Tassa et al., 2018) to walk subject to varying upper limits on its height. In Fig. 1a, learning looks stable due to averaging across multiple seeds, but in Fig. 1b, we can see dramatic oscillations over the course of a single training run, with the agent either simply walking normally or lying on the ground.\\n\\nIn this work, we address this issue, introducing Reinforcement Learning with Optimistic Ascent-Descent (ReLOAD), a principled CRL method for last-iterate convergence (LIC). Specifically, we make the following contributions:\\n\\n1. We analyze several existing methods for CRL and show that they fail to achieve LIC (Lemma 3.1, Lemma 3.2). We then prove LIC for a generalized form of optimistic mirror descent (Theorem 3.6).\\n\\n2. Building on these theoretical insights, we introduce ReLOAD (Section 4) and demonstrate empirically that it achieves LIC in both the tabular and function approximation settings (Section 5). Furthermore, ReLOAD improves the performance of strong baseline algorithms.\"}"}
{"id": "moskovitz23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ReLOAD for Last-Iterate Convergence in Constrained MDPs\\n\\nFigure 1. Standard gradient-based methods suffer from oscillations in constrained RL. (a) Training and agent to walk while keeping its height below different thresholds, learning looks stable when averaged across seeds. (b) Examining a single training run, we can see that learning oscillates dramatically, frequently leading to extreme behavior at the end of training (right).\\n\\nWe identify constraints within a range of control tasks which are especially challenging for traditional methods. We list constraint and task details in the hope that these problems can be reused as a benchmark for CRL.\\n\\n2. Reinforcement Learning with Constraints\\n\\nIn CRL, an agent not only seeks to maximize its cumulative reward, but must also obey constraints on its behavior. Typically, this problem is modeled as a Constrained Markov Decision Process (CMDP, Altman, 1999). An infinite horizon, discounted CMDP is a tuple $M_C = (S, A, r_0, \u03b3, \u03c1, \\\\{r_n\\\\}_{n=1}^N, \\\\{\u03b8_n\\\\}_{n=1}^N)$, where $S$ is the set of states, $A$ is the set of available actions, $P: S \u00d7 A \u2192 P(S)$ is the transition kernel, $r_0: S \u00d7 A \u2192 R$ is the reward function, $\u03b3 \u2208 [0, 1)$ is a discount factor, $\u03c1 \u2208 P(S)$ is the distribution over initial states, $\\\\{r_n\\\\}_{n=1}^N$ is the set of constraint rewards, and $\\\\{\u03b8_n\\\\}_{n=1}^N$ is the set of constraint thresholds. $P(\u00b7)$ denotes the set of distributions over a given space. At each time step, the agent samples an action from a stationary policy $\u03c0: S \u2192 P(A)$, which causes the environment to transition to a new state. This process induces a cumulative, discounted state-action occupancy measure (henceforth, simply \u201coccupancy measure\u201d) associated with the policy:\\n\\n$$d_\u03c0(s, a) \u225c (1 \u2212 \u03b3) \\\\sum_{t=0}^{\u221e} \u03b3^t \u03c0(a|s) P_\u03c0(s|s_t=s).$$\\n\\nThis probability measure lies within the following convex feasible set (a polytope in the discrete case):\\n\\n$$K \u225c \\\\{d_\u03c0 | d_\u03c0 \u2265 0, \\\\sum_a d_\u03c0(s, a) = (1 \u2212 \u03b3) \u03c1(s) + \u03b3 \\\\sum_{s\u2032,a\u2032} P_\u03c0(s\u2032|s) d_\u03c0(s\u2032, a\u2032)\\\\}.$$ \\n\\nThe agent\u2019s goal is to find a policy that maximizes its expected, cumulative, discounted reward while adhering to the designated constraints. This quantity is referred to as the policy\u2019s value $v_\u03c0$:\\n\\n$$v_\u03c0 \u225c \u27e8r_0, d_\u03c0\u27e9.$$\\n\\nMathematically, this can be formalized as a constrained optimization problem:\\n\\n$$\\\\text{minimize } v_\u03c0 - v_\u03c0^0 \\\\text{ subject to } v_\u03c0 \u2264 \u03b8_n, n = 1, . . . , N.$$\\n\\nOn inspection, we can see that Eq. (3) defines a linear program in $d_\u03c0$:\\n\\n$$\\\\text{minimize } d_\u03c0 \u2208 K \\\\text{ subject to } \u27e8r_0, d_\u03c0\u27e9 - \u27e8r_n, d_\u03c0\u27e9 \u2264 \u03b8_n, n = 1, . . . , N.$$\\n\\nTypically, CMDPs are solved via Lagrangian relaxation (Everett, 1963; Altman, 1999), which reframes the objective as a convex-concave min-max game:\\n\\n$$\\\\text{minimize } d_\u03c0 \u2208 K \\\\text{ maximize } \u00b5 \u2265 0 \\\\text{ subject to } \u2212\u27e8r_0, d_\u03c0\u27e9 + \\\\sum_n \u00b5_n (\u27e8r_n, d_\u03c0\u27e9 \u2212 \u03b8_n) \u225c L(d_\u03c0, \u00b5).$$\\n\\nWe provide a more comprehensive review of relevant work on CMDPs in Appendix A.\\n\\n2.1. The Scalarization Fallacy\\n\\nGiven Eq. (5), it would be tempting to simply solve the inner maximization problem over $\u00b5$ to find the optimal Lagrange multipliers $\u00b5^\u22c6$. One could then \u201cscalarize\u201d the task and constraint rewards into a single stationary reward function $r^\u22c6 = r_0 - \\\\sum_{n=1}^N \u00b5^\u22c6_n r_n$ and solve the resulting standard MDP. However, the solution to this MDP is not typically the solution to the CMDP\u2014one can easily define CMDPs for which the optimal policy must be stochastic (Altman, 1999; Szepesv\u00e1ri, 2020), but all MDPs admit deterministic optimal policies (Puterman, 2014). One notable exception occurs when one of the constraint thresholds $\u03b8_n$ is extreme (close to the highest or lowest possible $v_n$). In this case, one reward function $r_n$ will dominate the optimization and the solution will closely match the optimal policy for an MDP with reward $r_n$, as seen in Fig. 1 for $\u03b8 = 0.1$ and $\u03b8 = 0.0$. Such cases are discussed in detail in Appendix B. For non-\"}"}
{"id": "moskovitz23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alg within convex sets and the objective is bilinear, standard saddle-point Lagrange Optimization for Constrained MDPs\\n\\nAlgorithm 1\\n\\n1: Input: Lagrangian\\n2: for \\\\( k = 1, \\\\ldots, K \\\\) do\\n3: \\\\( d_{k+1} = \\\\text{Alg}^{\\\\pi_{k+1}}, \\\\mu_{k+1} \\\\)\\n4: \\\\( \\\\bar{L}_{k+1} = \\\\text{Alg}^{\\\\pi_{k+1}}, \\\\mu_{k+1} \\\\)\\n5: end for\\n\\n3: Optimization in Min-Max Games\\n\\nThe CMDP Langrangian min-max game belongs to a larger family of problems for which standard GD-style approaches fail to converge in the last-iterate: (in fact, any algorithm in the broad family of follow-the-leader approaches) fail to converge in the last-iterate: (Last-Iterate Convergence (LIC))\\n\\nDefinition 2.1 (Last-Iterate Convergence (LIC))\\n\\n...is the set of all equilibria \\\\( \\\\mathcal{E} \\\\) of a two-player convex-concave SP (SP), or equilibrium, of a two-player convex-concave SP of the Lagrangian (Freund & Schapire, 1997). Significantly, however, in general there is no guarantee regarding convergence with the following definitions:\\n\\nDefinition 2.2\\n\\n1: Input: Lagrangian\\n2: for \\\\( k = 1, \\\\ldots, K \\\\) do\\n3: \\\\( d_{k+1} = \\\\text{Alg}^{\\\\pi_{k+1}}, \\\\mu_{k+1} \\\\)\\n4: \\\\( \\\\bar{L}_{k+1} = \\\\text{Alg}^{\\\\pi_{k+1}}, \\\\mu_{k+1} \\\\)\\n5: end for\\n\\n2.2. Average- vs. Last-Iterate Convergence\\n\\nThis discrepancy can be easily overlooked, but it has significant practical implications, as seen in Fig. 1. How can we address this? For inspiration, we look beyond RL for the formulation of LIC for convex-concave games (Daskalakis & Panageas, 2018a).\\n\\nThere exist convex-concave SP problems for which averaged iterates converge to a Nash equilibrium rather than converging to it (Mertikopoulos et al., 2019). Specifically, GD approaches are instances of primal-dual mirror descent does not achieve LIC. Fortunately, there exists an approach called optimistic mirror descent which has been shown to achieve LIC, but may not converge in the last-iterate: (AIC, but may not converge in the last-iterate: (AIC)\\n\\nDefinition 2.3 (Average-Iterate Convergence (AIC))\\n\\nIn the same setting as Definition 2.1, we say that a sequence \\\\( \\\\{d_k\\\\} \\\\) converges with the following definitions:\\n\\n...are no-regret algorithms (e.g., gradient ascent/descent), the gradient estimate and a single projection into the constraint set at each step, making it particularly scalable to high-dimensional problems. For more discussion of this family of methods, see Appendix E, and for other approaches for minimax optimization.\\n\\nIn general, optimistic optimization attempts to use a \\\"hint\\\" about the next gradient to augment the current update. OMD uses the optimistic gradient obtained by doubling the current gradient and subtracting the previous one. For convenience, we'll denote the optimistic gradient of \\\\( L \\\\) at iteration \\\\( k \\\\) as\\n\\n...is the step-size at iteration \\\\( k \\\\). In other words, rather than use the gradient at iteration \\\\( k \\\\) to make a decision, we'll denote the optimistic gradient of \\\\( L \\\\) at iteration \\\\( k \\\\) as\\n\\n...corresponds to standard GD, as well as the negative entropy corresponding to multiplicative weights updating (MWU; Grigoriadis & Khachiyan, 1995). When \\\\( \\\\Omega(\\\\cdot) = \\\\|\\\\cdot\\\\|^2 \\\\) is the squared Euclidean distance\\n\\n\\\\[ \\\\Omega(u) = \\\\|u\\\\|^2 \\\\]\\n\\nis the corresponding to multiplicative weights updating (MWU; Grigoriadis & Khachiyan, 1995). When \\\\( \\\\Omega(\\\\cdot) = \\\\|\\\\cdot\\\\|^2 \\\\) is the squared Euclidean distance\\n\\n\\\\[ \\\\Omega(u) = \\\\|u\\\\|^2 \\\\]\\n\\nwhich induces \\\\( \\\\text{MD} \\\\), which for a generic SP problem\\n\\n\\\\[ \\\\text{MD} \\\\]\\n\\nis the formalized distinction between average- and last-iterate convergence\u2014there is no real convergence if\\n\\n\\\\[ \\\\bar{L} \\\\]\\n\\nfor brevity, \\\\( \\\\Omega(u) \\\\) is the squared Euclidean distance\\n\\n\\\\[ \\\\Omega(u) = \\\\|u\\\\|^2 \\\\]\\n\\nwhich induces \\\\( \\\\text{MD} \\\\), which for a generic SP problem\\n\\n\\\\[ \\\\text{MD} \\\\]\\n\\nis the formalized distinction between average- and last-iterate convergence\u2014there is no real convergence if\\n\\n\\\\[ \\\\bar{L} \\\\]\\n\\nfor brevity, \\\\( \\\\Omega(u) \\\\) is the squared Euclidean distance\\n\\n\\\\[ \\\\Omega(u) = \\\\|u\\\\|^2 \\\\]\\n\\nwhich induces \\\\( \\\\text{MD} \\\\), which for a generic SP problem\\n\\n\\\\[ \\\\text{MD} \\\\]\\n\\nis the formalized distinction between average- and last-iterate convergence\u2014there is no real convergence if\\n\\n\\\\[ \\\\bar{L} \\\\]\\n\\nfor brevity, \\\\( \\\\Omega(u) \\\\) is the squared Euclidean distance\\n\\n\\\\[ \\\\Omega(u) = \\\\|u\\\\|^2 \\\\]\\n\\nwhich induces \\\\( \\\\text{MD} \\\\), which for a generic SP problem\\n\\n\\\\[ \\\\text{MD} \\\\]\\n\\nis the formalized distinction between average- and last-iterate convergence\u2014there is no real convergence if\\n\\n\\\\[ \\\\bar{L} \\\\]\\n\\nfor brevity, \\\\( \\\\Omega(u) \\\\) is the squared Euclidean distance\\n\\n\\\\[ \\\\Omega(u) = \\\\|u\\\\|^2 \\\\]\\n\\nwhich induces \\\\( \\\\text{MD} \\\\), which for a generic SP problem\\n\\n\\\\[ \\\\text{MD} \\\\]\\n\\nis the formalized distinction between average- and last-iterate convergence\u2014there is no real convergence if\\n\\n\\\\[ \\\\bar{L} \\\\]\\n\\nfor brevity, \\\\( \\\\Omega(u) \\\\) is the squared Euclidean distance\\n\\n\\\\[ \\\\Omega(u) = \\\\|u\\\\|^2 \\\\]\\n\\nwhich induces \\\\( \\\\text{MD} \\\\), which for a generic SP problem\\n\\n\\\\[ \\\\text{MD} \\\\]\\n\\nis the formalized distinction between average- and last-iterate convergence\u2014there is no real convergence if\\n\\n\\\\[ \\\\bar{L} \\\\]\\n\\nfor brevity, \\\\( \\\\Omega(u) \\\\) is the squared Euclidean distance\\n\\n\\\\[ \\\\Omega(u) = \\\\|u\\\\|^2 \\\\]\\n\\nwhich induces \\\\( \\\\text{MD} \\\\), which for a generic SP problem\\n\\n\\\\[ \\\\text{MD} \\\\]\\n\\nis the formalized distinction between average- and last-iterate convergence\u2014there is no real convergence if\\n\\n\\\\[ \\\\bar{L} \\\\]\"}"}
{"id": "moskovitz23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There exists a convex-concave objective $L$.\\n\\nWe then need to show that OMD's guarantees carry over while both achieve AIC. While both achieve AIC, Fig. 2 shows that GDA's it-wards, while OGDA reaches the optimum. (b) The optimistic combination of one OMD and one MD player achieves LIC. (c) One optimistic gradient update $\\\\nabla L$ - $1$ to the standard forward-reflected-backward descent can be seen as performing fixed point iteration: $\\\\eta$.\\n\\nTo get an intuition for why OMD works, we can look to the optimistic mirror descent, we write the update as $\\\\nabla L$ - $k$ bends inwards. In contrast, we can see that the radius pointing towards the optimum. In contrast, we can see that $\\\\nabla L$ - $1$ to the standard forward-reflected-backward descent can be seen as performing fixed point iteration: $\\\\eta$.\\n\\nLet $\\\\eta$ be the indicator function which equals $0$ if $x \\\\leq 0$ and $\\\\infty$ otherwise. Then Eq. (6) can be written as $\\\\eta$.\\n\\nWe now generalize the convergence result of Malitsky & Tam (2018) to OMD for optimistic GDA (OGDA) to the simple bilinear problem $x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega X$. For example, $x \\\\in \\\\Omega X$, but in practice $x \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\Omega$. We'd like to show that MD achieves $\\\\Omega x$, but in practice $x \\\\in \\\\Omega$. Anticipating the application of OMD to RL, a natural question is whether using optimistic gradients\u2014particularly $\\\\eta \\\\in \\\\Omega x$, for which GDA can be shown to diverge while OGDA converges to the SP at $\\\\Omega x$. For example, $y \\\\in \\\\Omega x$, but in practice $y \\\\in \\\\"}
{"id": "moskovitz23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"portantly, the performance gap between ReLOAD and OGD is much greater than in Fig. 4, as the data distribution varies more significantly from batch to batch in high-dimensional problems. Fig. 9 depicts example training curves and final behaviors for $\\\\mu$-IMPALA and ReLOAD-IMPALA on Walker and Reacher, with curves for other domains in Appendix Fig. 13. We can see that ReLOAD significantly damps oscillations. For Walker (Fig. 9a), it produces an agent which moves forward with a modified, kneeling walk (panel (ii)), while $\\\\mu$-IMPALA typically either ends up lying down (panel (i))\u2014thus obeying the constraint but not performing the task\u2014or walking normally and ignoring the constraint (panel (iii)). We see a similar pattern with Reacher (Fig. 9b), with the ReLOAD agent moving quickly while keeping the tip of its arm in the rewarded area (panel (ii)), while $\\\\mu$-IMPALA either stops moving within the rewarded area (panel (i)) or maximizes velocity while swinging in a circle and ignoring the task (panel (iii)).\\n\\n6. Conclusion\\n\\nIn this work, we introduced ReLOAD, an RL framework for LIC in constrained MDPs. We examined the challenges in achieving LIC in this setting from a formal perspective, derived a convergence guarantee for a generalized form of OMD for whom a special case is the convex formulation of ReLOAD, and demonstrated ReLOAD's strong empirical performance on a range of challenging CMDPs. One shortcoming of the current analysis is a lack of theoretical understanding of non-convex ReLOAD, and in the future we'd like to experiment with more constraints. We believe that ReLOAD may offer insights into policy optimization with non-stationary rewards more generally, as well as the oscillations which are known to plague standard RL combined with function approximation (Young & Sutton, 2020; Gopalan & Thoppe, 2022), improving optimization within the primal-dual formulation of RL (Bas-Serrano et al., 2021), and extensions to convex MDPs (Zahavy et al., 2021b).\\n\\nAcknowledgements\\n\\nWork funded by DeepMind. The authors would like to thank Abbas Abdolmaleki, Steven Bohez, Edouard Leurent, Daniel Mankowitz, Dan Calian, Lior Shani, Yash Chandak, Chris Lu, Robert Lange, DJ Strouse, Jack Parker-Holder, Kate Baumli, Kevin Waugh, and other colleagues on the Discovery team and at DeepMind for helpful discussions and feedback over the course of this project.\\n\\nReferences\\n\\nAbbeel, P. and Ng, A. Y. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, pp. 1, 2004.\\n\\nAbdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M. Maximum a posteriori policy optimisation, 2018. URL https://arxiv.org/abs/1806.06920.\\n\\nAbdolmaleki, A., Huang, S., Hasenclever, L., Neunert, M., Song, F., Zambelli, M., Martins, M., Heess, N., Hadsell, R., and Riedmiller, M. A distributional view on multi-objective policy optimization. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 11\u201322. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/abdolmaleki20a.html.\\n\\nAbernethy, J., Lai, K. A., and Wibisono, A. Last-iterate convergence rates for min-max optimization: Convergence of hamiltonian gradient descent and consensus optimization. In Algorithmic Learning Theory, pp. 3\u201347. PMLR, 2021.\\n\\nAchiam, J., Held, D., Tamar, A., and Abbeel, P. Constrained policy optimization. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 22\u201331. PMLR, 06\u201311 Aug 2017. URL https://proceedings.mlr.press/v70/achiam17a.html.\\n\\nAgarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. The Journal of Machine Learning Research, 22(1):4431\u20134506, 2021.\\n\\nAltman, E. Constrained markov decision processes, 1999.\\n\\nAuer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48\u201377, 2002.\\n\\nBalduzzi, D., Racaniere, S., Martens, J., Foerster, J., Tuyls, K., and Graepel, T. The mechanics of n-player differentiable games. In International Conference on Machine Learning, pp. 354\u2013363. PMLR, 2018.\\n\\nBarth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB, D., Muldal, A., Heess, N., and Lillicrap, T. Distributed distributional deterministic policy gradients, 2018. URL https://arxiv.org/abs/1804.08617.\\n\\nBas-Serrano, J., Curi, S., Krause, A., and Neu, G. Logistic q-learning. In Banerjee, A. and Fukumizu, K. (eds.), Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, pp. 1198\u20131207. PMLR, 2021.\\n\\nZahavy, A., Curi, S., Krause, A., and Neu, G. Gradient and entropic gradient q-learning for constrained mdp's. In ICML 2021: Proceedings of the 38th International Conference on Machine Learning, pp. 2454\u20132463. PMLR, 2021.\\n\\nYoung, S., & Sutton, R. S. Stability and oscillations in reinforcement learning with function approximation. CoRR, abs/1912.09043. 2019.\\n\\nGopalan, S., & Thoppe, S. Constrained reinforcement learning with online convex optimization. In AISTATS 2019, 2019.\"}"}
{"id": "moskovitz23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "moskovitz23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dulac-Arnold, G., Levine, N., Mankowitz, D. J., Li, J., Paduraru, C., Gowal, S., and Hester, T. An empirical investigation of the challenges of real-world reinforcement learning, 2020. URL https://arxiv.org/abs/2003.11881.\\n\\nEfroni, Y., Mannor, S., and Pirotta, M. Exploration-exploitation in constrained mdps, 2020. URL https://arxiv.org/abs/2003.02189.\\n\\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures, 2018. URL http://arxiv.org/abs/1802.01561.\\n\\nEverett, H. Generalized lagrange multiplier method for solving problems of optimum allocation of resources. Oper. Res., 11(3):399\u2013417, jun 1963. URL https://doi.org/10.1287/opre.11.3.399.\\n\\nFlennerhag, S., Schroecker, Y., Zahavy, T., van Hasselt, H., Silver, D., and Singh, S. Bootstrapped meta-learning. arXiv preprint arXiv:2109.04504, 2021.\\n\\nFlennerhag, S., Zahavy, T., O'Donoghue, B., van Hasselt, H., Gyorgy, A., and Singh, S. Optimistic meta-gradients. arXiv preprint arXiv:2301.03236, 2023.\\n\\nFreund, Y. and Schapire, R. E. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119\u2013139, 1997. URL https://www.sciencedirect.com/science/article/pii/S002200009791504X.\\n\\nGeist, M., Scherrer, B., and Pietquin, O. A theory of regularized Markov decision processes. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2160\u20132169. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/geist19a.html.\\n\\nGidel, G., Berard, H., Vignoud, G., Vincent, P., and Lacoste-Julien, S. A variational inequality perspective on generative adversarial networks. In ICLR '19: Proceedings of the 2019 International Conference on Learning Representations, 2019.\\n\\nGopalan, A. and Thoppe, G. Approximate q-learning and sarsa (0) under the epsilon-greedy policy: a differential inclusion analysis. arXiv preprint arXiv:2205.13617, 2022.\\n\\nGrigoriadis, M. D. and Khachiyan, L. G. A sublinear-time randomized approximation algorithm for matrix games. Operations Research Letters, 18(2):53\u201358, 1995. URL https://www.sciencedirect.com/science/article/pii/0167637795000320.\\n\\nHsieh, Y.-G., Iutzeler, F., Malick, J., and Mertikopoulos, P. On the convergence of single-call stochastic extra-gradient methods, 2019. URL https://arxiv.org/abs/1908.08465.\\n\\nHuang, S., Abdolmaleki, A., Vezzani, G., Brakel, P., Mankowitz, D. J., Neunert, M., Bohez, S., Tassa, Y., Heess, N., Riedmiller, M., and Hadsell, R. A constrained multi-objective reinforcement learning framework. In Faust, A., Hsu, D., and Neumann, G. (eds.), Proceedings of the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning Research, pp. 883\u2013893. PMLR, 08\u201311 Nov 2022. URL https://proceedings.mlr.press/v164/huang22a.html.\\n\\nKorpelevich, G. M. The extragradient method for finding saddle points and other problems. In Ekonomika i Matematicheskie Metody, volume 12, pp. 747\u2013756, 1976.\\n\\nKumar, S., Kumar, A., Levine, S., and Finn, C. One solution is not all you need: Few-shot extrapolation via structured maxent rl. Advances in Neural Information Processing Systems, 33:8198\u20138210, 2020.\\n\\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning, 2015. URL https://arxiv.org/abs/1509.02971.\\n\\nLiu, T., Zhou, R., Kalathil, D., Kumar, P., and Tian, C. Learning policies with zero or bounded constraint violation for constrained mdps. In Thirty-fifth Conference on Neural Information Processing Systems, 2021.\\n\\nLuo, J., Paduraru, C., Voicu, O., Chervonyi, Y., Munns, S., Li, J., Qian, C., Dutta, P., Davis, J. Q., Wu, N., et al. Controlling commercial cooling systems using reinforcement learning. arXiv preprint arXiv:2211.07357, 2022.\\n\\nMalitsky, Y. and Tam, M. K. A forward-backward splitting method for monotone inclusions without cocoercivity, 2018. URL https://arxiv.org/abs/1808.04162.\\n\\nMandhane, A., Zhernov, A., Rauh, M., Gu, C., Wang, M., Xue, F., Shang, W., Pang, D., Claus, R., Chiang, C.-H., Chen, C., Han, J., Chen, A., Mankowitz, D. J., Broshear, J., Schrittwieser, J., Hubert, T., Vinyals, O., and Mann, T. Muzero with self-competition for rate control in vp9 video compression, 2022. URL https://arxiv.org/abs/2202.06626.\\n\\nMertikopoulos, P., Lecouat, B., Zenati, H., Foo, C.-S., Chandrasekhar, V., and Piliouras, G. Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg8jjC9KQ.\"}"}
{"id": "moskovitz23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ReLOAD for Last-Iterate Convergence in Constrained MDPs\\n\\nMoskovitz, T., Arbel, M., Huszar, F., and Gretton, A. Efficient Wasserstein natural gradients for reinforcement learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=OHgnfSrn2jv.\\n\\nPacchiano, A., Parker-Holder, J., and Pacchiano, A. Towards an understanding of default policies in multitask policy optimization. In International Conference on Artificial Intelligence and Statistics, pp. 10661\u201310686. PMLR, 2022.\\n\\nNemirovski, A. S. Prox-method with rate of convergence $o(1/t)$ for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229\u2013251, 2004.\\n\\nO'Donoghue, B. Variational Bayesian reinforcement learning with regret bounds. Advances in Neural Information Processing Systems, 34:28208\u201328221, 2021.\\n\\nO'Donoghue, B. and Lattimore, T. Variational Bayesian optimistic sampling. Advances in Neural Information Processing Systems, 34:12507\u201312519, 2021.\\n\\nO'Donoghue, B., Lattimore, T., and Osband, I. Stochastic matrix games with bandit feedback. arXiv preprint arXiv:2006.05145, 2020.\\n\\nOsband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E., Saraiva, A., McKinney, K., Lattimore, T., Szepesvari, C., Singh, S., Van Roy, B., Sutton, R., Silver, D., and Van Hasselt, H. Behaviour suite for reinforcement learning, 2019. URL https://arxiv.org/abs/1908.03568.\\n\\nPacchiano, A., Parker-Holder, J., Tang, Y., Choromanski, K., Choromanska, A., and Jordan, M. Learning to score behaviors for guided policy optimization. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7445\u20137454. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/pacchiano20a.html.\\n\\nPaternain, S., Chamon, L., Calvo-Fullana, M., and Ribeiro, A. Constrained reinforcement learning has zero duality gap. In Wallach, H., Larochelle, H., Beygelzimer, A., d\u2019Alch\u00b4e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/c1aeb6517a1c7f33514f7ff69047e74e-Paper.pdf.\\n\\nPerolat, J., Munos, R., Lespiau, J.-B., Omidshafiei, S., Rowland, M., Ortega, P., Burch, N., Anthony, T., Balduzzi, D., De Vylder, B., et al. From Poincar\u00e9 recurrence to convergence in imperfect information games: Finding equilibrium via regularization. In International Conference on Machine Learning, pp. 8525\u20138535. PMLR, 2021.\\n\\nPopov, L. D. A modification of the Arrow-Hurwicz method for search of saddle points. Mathematical Notes of the Academy of Sciences of the USSR, 28(5):845\u2013848, 1980.\\n\\nPuterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, 2014.\\n\\nReich, S. and Sabach, S. Existence and approximation of fixed points of Bregman firmly nonexpansive mappings in reflexive Banach spaces. In Springer Optimization and Its Applications, chapter Chapter 15, pp. 301\u2013316. Springer, 2011. URL https://EconPapers.repec.org/RePEc:spa:spochp:978-1-4419-9569-8_15.\\n\\nRyu, E. K. and Yin, W. Large-Scale Convex Optimization: Algorithms & Analyses via Monotone Operators. Cambridge University Press, 2022. doi: 10.1017/9781009160865.\\n\\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., Lillicrap, T., and Silver, D. Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. Nature, 588(7839):604\u2013609, 2020.\\n\\nSchulman, J., Levine, S., Abbeel, P., Jordan, M. I., and Moritz, P. Trust region policy optimization. In Bach, F. R. and Blei, D. M. (eds.), ICML, volume 37 of JMLR Workshop and Conference Proceedings, pp. 1889\u20131897. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/schulman15.html.\\n\\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n\\nShani, L., Efroni, Y., and Mannor, S. Adaptive trust region policy optimization: Global convergence and faster rates for regularized MDPs. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):5668\u20135675, 2023/01/16 2020.\\n\\nShani, L., Zahavy, T., and Mannor, S. Online apprenticeship learning. Proceedings of the AAAI Conference on Artificial Intelligence, 36(8):8240\u20138248, 2023/01/13 2022.\\n\\nSim\u02dcao, T. D., Jansen, N., and Spaan, M. T. AlwaysSafe: Reinforcement Learning Without Safety Constraint Violations During Training. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2021.\"}"}
{"id": "moskovitz23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sokota, S., D'Orazio, R., Kolter, J. Z., Loizou, N., Lanctot, M., Mitliagkas, I., Brown, N., and Kroer, C. A unified approach to reinforcement learning, quantal response equilibria, and two-player zero-sum games. arXiv preprint arXiv:2206.05825, 2022.\\n\\nStooke, A., Achiam, J., and Abbeel, P. Responsive safety in reinforcement learning by pid lagrangian methods, 2020. URL https://arxiv.org/abs/2007.03964.\\n\\nStrehl, A. L. and Littman, M. L. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309\u20131331, 2008.\\n\\nSutton, R. The reward hypothesis, 2004. URL http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html.\\n\\nSutton, R. S. and Barto, A. G. Reinforcement learning: An introduction. MIT press, 2018.\\n\\nSzepesv\u00e1ri, C. Constrained mdps and the reward hypothesis, Mar 2020. URL http://readingsml.blogspot.com/2020/03/constrained-mdps-and-reward-hypothesis.html.\\n\\nTassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T., and Riedmiller, M. Deepmind control suite, 2018. URL https://arxiv.org/abs/1801.00690.\\n\\nTessler, C., Mankowitz, D. J., and Mannor, S. Reward constrained policy optimization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SkfrvsA9FX.\\n\\nThomas, P. S., da Silva, B. C., Barto, A. G., and Brunskill, E. On ensuring that intelligent machines are well-behaved, 2017. URL https://arxiv.org/abs/1708.05448.\\n\\nTomar, M., Shani, L., Efroni, Y., and Ghavamzadeh, M. Mirror descent policy optimization, 2020. URL https://arxiv.org/abs/2005.09814.\\n\\nXu, Z., van Hasselt, H. P., and Silver, D. Meta-gradient reinforcement learning. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/2715518c875999308842e3455eda2fe3-Paper.pdf.\\n\\nYoung, K. and Sutton, R. S. Understanding the pathologies of approximate policy evaluation when combined with greedification in reinforcement learning, 2020. URL https://arxiv.org/abs/2010.15268.\\n\\nZahavy, T., Cohen, A., Kaplan, H., and Mansour, Y. Apprenticeship learning via frank-wolfe. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 6720\u20136728, 2020a.\\n\\nZahavy, T., Xu, Z., Veeriah, V., Hessel, M., Oh, J., van Hasselt, H. P., Silver, D., and Singh, S. A self-tuning actor-critic algorithm. Advances in neural information processing systems, 33:20913\u201320924, 2020b.\\n\\nZahavy, T., O'Donoghue, B., Barreto, A., Mnih, V., Flennerhag, S., and Singh, S. Discovering diverse nearly optimal policies with successor features. arXiv preprint arXiv:2106.00669, 2021a.\\n\\nZahavy, T., O'Donoghue, B., Desjardins, G., and Singh, S. Reward is enough for convex MDPs. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021b. URL https://openreview.net/forum?id=ELndVeVA-TR.\\n\\nZahavy, T., Schroecker, Y., Behbahani, F., Baumli, K., Flennerhag, S., Hou, S., and Singh, S. Discovering policies with domino: Diversity optimization maintaining near optimality, 2022. URL https://arxiv.org/abs/2205.13521.\"}"}
{"id": "moskovitz23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Additional Related Work\\n\\nBeyond that which is covered in the main text, there is a rich history of work on CMDPs. Borkar (2005) was the first to analyze an actor-critic approach to CMDPs, while Bhatnagar & Lakshmanan (2012) was the first to expand this approach to function approximation. Tessler et al. (2019); Achiam et al. (2017); Efroni et al. (2020); Bohez et al. (2019); Chow et al. (2018); Paternain et al. (2019) all focus on integrating constraints into sequential decision problems (Altman, 1999), commonly done, as noted, via Lagrangian relaxation (Tessler et al., 2019). Calian et al. (2020) argue for a soft-constraint approach to CRL, wherein the solution is not absolutely required to satisfy a particular constraint, but rather is penalized in proportion to its violation (Thomas et al., 2017; Dulac-Arnold et al., 2020). This philosophy lies in contrast to hard-constraint approaches, for which a solution is marked as invalid if there is any constraint violation (Dalal et al., 2018; Bura et al., 2022).\\n\\nIn zero-violation approaches, methods are initialized in the feasible region, and only updated in ways that are guaranteed not to leave the feasible region (Liu et al., 2021; Sim\u02dcao et al., 2021). In applications, soft constraints may be preferable when violations do not result in catastrophic system failure, but rather simply undesirable behavior (e.g., inefficiency). Calian et al. (2020) attempt to stabilize the learning process in CMDPs by using meta-gradients (Xu et al., 2018; Zahavy et al., 2020b) to adapt the learning rate of the Lagrange multiplier online. More recently, Bootstrapped meta gradients (Flennerhag et al., 2021) have been analyzed and shown to provide a form of optimism (Flennerhag et al., 2023); thus, it would be interesting to see if they can help to achieve LIC in CMDPs. Stooke et al. (2020) apply a principled approach to damping the dynamics of the Lagrange multiplier via PID control with the goal of reducing constraint overshoots over the course of training. However, neither of these two approaches guarantees LIC\u2014this may have a connection to Lemma 3.2, which shows that only one optimistic player (e.g., the Lagrange player) is in general not sufficient to guarantee LIC.\\n\\nEfroni et al. (2020) address the role of exploration in CMDPs, proving bounds for both the linear programming formulation and the primal-dual formulation of the problem, though they only consider standard gradients and do not focus on LIC vs. AIC.\\n\\nRather than formulate CRL as a CMDP, Huang et al. (2022) and Abdolmaleki et al. (2020) consider a multi-objective approach. That is, rather than integrate the constraints and the task reward into a single, non-stationary reward, they optimize the task reward and each constraint reward independently, and then search over a pareto front which balances among them. There are a number of recent applications of CRL which have achieved impressive practical successes. One example is the use of MuZero (Schrittwieser et al., 2020) for video compression by Mandhane et al. (2022). Specifically, the agent was trained on a constrained MDP to maximize video quality with a constraint on the allowed bit rate. Others involve using CRL for quality-diversity optimization where the agent is trying to find a set of diverse skill while all of the skills are required to satisfy a near-optimality constraint on the reward (Zahavy et al., 2021a; 2022; Kumar et al., 2020).\\n\\nBeyond extra-gradient methods (see Appendix C below), there are other approaches which aim to achieve LIC in min-max games. The symplectic gradient adjustment (Balduzzi et al., 2018) uses a signed additive term to the gradient to push the dynamics away from unstable equilibria and towards stable ones. Hamiltonian gradient descent updates the iterates in the direction of the (negative) gradient of the squared norm of the signed partial derivatives, and has been shown to achieve LIC in a variety of min-max games (Abernethy et al., 2021). However, when applied to CMDPs, minimizing the squared gradient with respect to the Lagrange multiplier(s) is equivalent to an apprenticeship learning problem (Abbeel & Ng, 2004; Zahavy et al., 2020a; Shani et al., 2022), which is itself a convex MDP representing a challenging optimization problem (Zahavy et al., 2021b). Perolat et al. (2021) instead augment the objective with an adaptive regularizer, solving the resulting convex/concave (but biased) problem exactly before iteratively refitting with progressively lesser regularization.\\n\\nOne aspect we have not touched on is the question of exploration. When the agent has uncertainty about the rewards or the constraints, how can it 'explore' sufficiently well in order to solve the CMDP? This would require information seeking behaviour to discover the rewards and constraints in the environment. A common heuristic for exploration is 'optimism in the face of uncertainty' (Dayan & Sejnowski, 1996; Strehl & Littman, 2008; O'Donoghue, 2021), however there is little work applying optimism to CMDPs. Some preliminary work in constrained bandits (O'Donoghue & Lattimore, 2021) or more generally in two-player zero sum matrix games (O'Donoghue et al., 2020) is encouraging, but the presented algorithms are not online and therefore questions like LIC are not applicable. Similarly, classic adversarial algorithms like EXP3 (Auer et al., 2002) typically only guarantee AIC. Clearly more work is required in this area.\\n\\nAs noted in the main text, many policy optimization methods in deep RL currently use a form of trust region to stabilize optimization. KL-based trust regions are discussed in detail by Agarwal et al. (2021); Geist et al. (2019); Moskovitz et al. (2022); Shani et al. (2020). While not explored further here, it would also be interesting to study policy optimization methods which use trust regions generated by other divergence measures or distances, such as the Wasserstein distance (Moskovitz et al., 2021; Pacchiano et al., 2020).\"}"}
{"id": "moskovitz23a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Extreme Constraints and Oscillations\\n\\nAs noted in the main text, so-called \u201cextreme\u201d constraints don\u2019t often induce oscillations in practice. Here, we present a simple, relatively informal argument to provide an intuition for why this is the case. Consider a simple CMDP with one constraint and where values lie in the range $[0, B]\\\\ (\\\\text{where } B < \\\\infty)$. (Note this bounding can be easily obtained for any reward function which is upper- and lower-bounded by adding the lower bound to rewards to make them non-negative.) The Lagrangian in this case is\\n\\n$$L(d\\\\pi, \\\\mu) = -\\\\langle r_0, d\\\\pi \\\\rangle + \\\\mu(\\\\langle r_1, d\\\\pi \\\\rangle - \\\\theta).$$\\n\\n(15)\\n\\nAn extreme constraint threshold $\\\\theta$, then, is one which is close to 0 or $B$, as those are the bounds on the value. Say that $\\\\theta = 0$.\\n\\nThen the Lagrangian reduces to\\n\\n$$L(d\\\\pi, \\\\mu) = -\\\\langle r_0, d\\\\pi \\\\rangle + \\\\mu\\\\langle r_1, d\\\\pi \\\\rangle,$$\\n\\n(16)\\n\\nwith the gradients being\\n\\n$$\\\\nabla_d\\\\pi L = \\\\mu r_1 - r_0$$\\n\\n$$\\\\nabla_\\\\mu L = \\\\langle r_1, d\\\\pi \\\\rangle = v_1.$$\\n\\nBecause $v_1 \\\\geq 0$, the Lagrange multiplier $\\\\mu$ will always be increasing, which, because of $\\\\nabla_d\\\\pi L$, means that the occupancy measure will increasingly be updated to align with the constraint reward $r_1$ and ignore the task reward $r_0$. Therefore, the constraint reward will dominate the optimization of $d\\\\pi$, and there is no \u201cback and forth\u201d between optimizing the constraint reward and the task reward. Similarly, if $\\\\theta = B$, $v_1 - B \\\\leq 0$, so $\\\\mu$ will be non-increasing, eventually dropping to 0, so that $d\\\\pi$ will only optimize the task reward $r_1$.\\n\\nThis also provides motivation for why problems with \u201cintermedaite\u201d constraints which induce oscillations are the most valuable/interesting CMDPs\u2014because extreme constraints cause one reward function to dominate, the problem essentially reduces to an MDP, and can reasonably be solved with standard RL methods.\\n\\nC. Extra-Gradient Methods\\n\\nThe minimax problems studied in the paper all fall within the broader class of variational inequality (VI) problems, which can be written as\\n\\n$$\\\\text{find } x^\\\\star \\\\in X \\\\subseteq \\\\mathbb{R}^d.$$ \\n\\nfor some single-valued operator $F: \\\\mathbb{R}^d \\\\rightarrow \\\\mathbb{R}^d$. For example, if $F = \\\\nabla L$ for some differentiable loss $L$, the solution to the VI problem is a critical point of $L$.\\n\\nThe extra-gradient algorithm (EG; Korpelevich, 1976) is as follows:\\n\\n$$x_{k+1} = \\\\Pi_X(x_k - \\\\eta_k F(x_k))$$\\n\\n$$x_{k+1} = \\\\Pi_X(x_k - \\\\eta_k F(x_{k+1})).$$\\n\\n(17)\\n\\nwhere $\\\\Pi_X(z) = \\\\min_{x \\\\in X} \\\\|x - z\\\\|$ is the projection operator onto $X$. The Bregman variant of EG is called the Mirror-Prox method (MP; Nemirovski, 2004). While EG achieves the optimal $O(1/k)$ convergence rate when $V$ is monotone and Lipschitz-continuous, it is difficult to scale, as each update requires two gradient computations and two projections into $X$. In addition to the optimistic gradient method, there are a number of so-called \u201csingle-call\u201d EG methods (one of which is the optimistic gradient):\\n\\n- Past EG (PEG; Chiang et al., 2012; Gidel et al., 2019; Popov, 1980):\\n\\n$$x_{k+1} = \\\\Pi_X(x_k - \\\\eta_k F(x_k - 1/2))$$\\n\\n$$x_{k+1} = \\\\Pi_X(x_k - \\\\eta_k F(x_{k+1})).$$\\n\\n(17)\\n\\n- Reflected Gradient (RG; Chambolle & Pock, 2011; Cui & Shanbhag, 2016):\\n\\n$$x_{k+1} = x_k - (x_k - 1 - x_k)$$\\n\\n$$x_{k+1} = \\\\Pi_X(x_k - \\\\eta_k F(x_{k+1})).$$\\n\\nBoth PEG and RG are very closely related to OGD, and are identical in the unconstrained case. As an additional note, the term \u201coptimistic gradient\u201d is frequently applied broadly within the literature, and is often used to refer to the EG method or any of its single-call variants. For a more comprehensive discussion and analysis of these methods, we refer the reader to Hsieh et al. (2019).\"}"}
{"id": "moskovitz23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2\\nPEG-MDPI\\n\\n1: Require: CMDP $M_C$, step sizes $\\\\{\\\\eta_\\\\pi, \\\\eta_\\\\mu\\\\} > 0$\\n\\n2: Initialize $\\\\pi_1, \\\\mu_1, \\\\pi_0, \\\\mu_0$\\n\\n3: for $k = 1, \\\\ldots, K$ do\\n\\n4: Half-step:\\n\\n$$\\\\pi_{k+1}/2 = \\\\pi_k \\\\exp q_\\\\mu_k - 1/2 \\\\pi_{k-1/2}/\\\\eta_\\\\pi D_{\\\\pi_k \\\\exp q_\\\\mu_k - 1/2 \\\\pi_{k-1/2}/\\\\eta_\\\\pi}, 1$$\\n\\n5: $\\\\mu_{k+1}/2 = \\\\max\\\\{\\\\mu_k - \\\\eta_\\\\mu (v_k - 1/2 1:N - \\\\theta), 0\\\\}$\\n\\n6: $q_{k+1}/2 \\\\mu_{k+1}/2 \\\\leftarrow -q_{k+1}/2 0 + P_{N n=1} \\\\mu_{k+1}/2 n q_{k+1}/2 n$ (mixed $q$-values)\\n\\n7: $v_{k+1}/2 1:N \\\\leftarrow [\\\\langle q_{k+1}/2 1, \\\\pi_{k+1}/2 \\\\rangle, \\\\ldots, \\\\langle q_{k+1}/2 N, \\\\pi_{k+1}/2 \\\\rangle]^\\\\top$\\n\\n8: Full-step:\\n\\n$$\\\\pi_{k+1} = \\\\pi_k \\\\exp q_\\\\mu_{k+1}/2 \\\\pi_{k+1}/2/\\\\eta_\\\\pi D_{\\\\pi_k \\\\exp q_\\\\mu_{k+1}/2 \\\\pi_{k+1}/2/\\\\eta_\\\\pi}, 1$$\\n\\n9: end for\\n\\n10: return $\\\\pi_K, \\\\mu_K$\\n\\nC.1. PEG Policy Iteration\\n\\nAs described in Section 4, policy evaluation in the tabular case is equivalent to gradient computation, and a trust region step equates to a projection step in OMD. While OMD only requires one of these each per update, PEG (Eq. (17)) requires two projections and one gradient call per update, which makes it less scalable than OMD. Nonetheless, we can derive a Bregman PEG-based optimization method for optimizing CMDPs which carries similar guarantees to ReLOAD.\\n\\nLet $\\\\ell(x) \\\\equiv L_x(x, y_k)$. Then we can write the PEG update as\\n\\n$$x_{k+1}/2 = \\\\arg\\\\min_{x \\\\in X} \\\\langle \\\\nabla \\\\ell(x_k - 1/2), x \\\\rangle + 1/\\\\eta_k D_\\\\Omega(x; x_k)$$\\n\\n$$x_{k+1} = \\\\arg\\\\min_{x \\\\in X} \\\\langle \\\\nabla \\\\ell(x_{k+1}/2), x \\\\rangle + 1/\\\\eta_k D_\\\\Omega(x; x_k).$$\\n\\nSetting $\\\\Omega_\\\\pi$ as the negative entropy and $\\\\Omega_\\\\mu$ as the squared Euclidean norm, we can derive the policy iteration-style algorithm in Algorithm 2. We verified its LIC on the paradoxical CMDP from Fig. 3, with results plotted in Appendix H.\\n\\nD. Theoretical Results\\n\\nD.1. Impossibility Results\\n\\nLemma 3.1 (Insufficiency of MD; (Daskalakis & Panageas, 2018a)). There exist convex-concave SP problems for which primal-dual mirror descent does not achieve LIC.\\n\\nProof. This proof is originally due to Daskalakis et al. (2018), but we repeat it here for completeness. Consider the problem\\n\\n$$\\\\min_{x \\\\in \\\\mathbb{R}} \\\\max_{y \\\\in \\\\mathbb{R}} xy.$$}"}
{"id": "moskovitz23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we can then express the CMDP problem in the form of a convex split monotone inclusion problem:\\n\\n\\\\[\\n\\\\begin{equation}\\n\\\\nabla \\\\pi_k \\\\in \\\\Omega \\\\subset H\\n\\\\end{equation}\\n\\\\]\\n\\nCorollary 4.1 (ReLOAD Convergence)\\n\\nTheorem 3.6, if \\\\( \\\\eta > 0 \\\\) or \\\\( \\\\Omega \\\\) is not the case:\\n\\n\\\\[\\n\\\\begin{equation}\\n\\\\nabla \\\\pi_k \\\\in \\\\Omega \\\\subset H\\n\\\\end{equation}\\n\\\\]\\n\\nThe sequence \\\\( \\\\{ \\\\pi_k \\\\} \\\\) converges at a rate policy evaluation is especially important, as estimating the stationary reward vector is not the case:\\n\\n\\\\[\\n\\\\begin{equation}\\n\\\\nabla \\\\pi_k \\\\in \\\\Omega \\\\subset H\\n\\\\end{equation}\\n\\\\]\\n\\nIn the following section, we'll show that this approach can be adapted to the RL setting to achieve LIC for CMDPs.\"}"}
{"id": "moskovitz23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2. ReLOAD with Function Approximation\\n\\nFor large state and action spaces, function approximation, i.e., deep RL (DRL), becomes preferable. To implement ReLOAD using DRL, we require the following ingredients:\\n\\n1. optimistic value estimates and\\n2. a trust region for the policy update.\\n\\nFortunately, both requirements are easy to satisfy for most state-of-the-art policy optimization methods.\\n\\nFor (1), a crucial factor is that with function approximation, rather than compute $q$-value estimates for every state-action pair in the environment, policy evaluation is performed over minibatches of sampled experience. This means that the agent can't simply store past gradients to compute optimistic values, as those gradients may have been obtained from the values of different $(s, a)$ pairs from those used to compute the current gradient. Instead, the agent must maintain a copy of the previous value network so that the optimistic $q$-values can be computed from the same samples:\\n\\n$$\\\\tilde{q}_k^{\\\\mu}(s, a) = 2q_k^{\\\\mu}(s, a) - q_{k-1}^{\\\\mu}(s, a).$$\\n\\nSimilarly, $\\\\mu$ must also be updated using value estimates from the same data. This is a complication which makes using OMD directly, as optimistic GAN methods do (Daskalakis et al., 2018), nontrivial for RL. As for (2), many high-performing policy optimization algorithms employ trust regions as a means of stabilizing learning and improving sample efficiency. Examples include TRPO (Schulman et al., 2015), MDPO (Tomar et al., 2020), and MPO (Abdolmaleki et al., 2018).\\n\\n5. Experiments\\n\\nNext, we study the LIC of ReLOAD empirically on a variety of CMDPs with discrete and continuous state and action spaces. We augmented ReLOAD with popular DRL algorithms including: MD Policy Iteration (MDPI; Geist et al., 2019), MD Policy Optimization (MDPO; Tomar et al., 2020), IMPALA (Espeholt et al., 2018), and distributional MPO (DMPO; Abdolmaleki et al., 2020).\\n\\nNotation:\\n\\nIn the following, when augmenting unconstrained methods to perform Lagrangian optimization, we prefix the method name with \\\"$\\\\mu$-\\\". For more detail on all algorithms, see Appendix F. To measure performance, we use the weighted reward: the task reward minus the multiplier-weighted constraint overshoot, given by\\n\\n$$r_0 - \\\\sum_{n=1}^{N} \\\\hat{\\\\mu}_n \\\\max \\\\{ r_n - \\\\theta_n, 0 \\\\},$$\\n\\nwhere $\\\\hat{\\\\mu}_n$ is the normalized optimal Lagrange multiplier (Stooke et al., 2020). $\\\\hat{\\\\mu}_n$ was calculated by averaging the Lagrange multipliers learned by the non-optimistic baseline agents (details in Appendix G). Experiments were repeated over 8 random seeds, and error bars denote one standard error. Additional plots and result tables can be found in Appendix H. Videos of trained agents can be found at https://tedmoskovitz.github.io/ReLOAD/.\\n\\nToy Example: A Paradoxical CMDP\\n\\nIn many real-world applications of CMDPs, constraints are introduced to ensure the system's integrity is maintained. For example, a robot may be trained to run as fast as possible but restrained so that it does not place sufficient torque on its joints to break them. To capture these conflicting goals in a simple setting, we tested tabular ReLOAD-MDPI (Algorithm 6) on the two-state CMDP in Fig. 3a. In this task, there is a single constraint reward which is equal to the primary reward $r_0 = r_1 = r_2$, so that $r = 1$ when the agent takes action $a_1$ placing it in $s_1$, and $r = 0$ for action $a_2$ which moves the agent to $s_2$. The constraint $\\\\theta = 1/2$ was chosen so that the agent may only choose $a_1$ at most half of the time. Plotting the value over the course of learning in Fig. 3b, we can see that ReLOAD converges, while $\\\\mu$-MDPI oscillates and fails to converge in the last-iterate. However, this approach does achieve AIC (\\\"$\\\\mu$-MDPI-Avg\\\"). To test performance with\"}"}
{"id": "moskovitz23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"function approximation, we then applied ReLOAD-MDPO and \\\\( \\\\mu \\\\)-MDPO to the same problem. As in the tabular case, ReLOAD significantly dampens oscillations compared to its non-optimistic counterpart (Fig. 3c). (However, some noise remains due to noise in the approximate policy evaluation.) Importantly, \\\\( \\\\mu \\\\)-MDPO-Avg does not converge in this setting, as it corresponds to averaging the parameters of a nonlinear network. Examining the optimization trajectories in Fig. 3d, we can see that ReLOAD-MDPO converges to the SP, while \\\\( \\\\mu \\\\)-MDPO gets \u201cstuck,\u201d circling but never reaching the optimum. In Fig. 4, we compare ReLOAD-MDPO against various ablations by measuring the \\\\( L_2 \\\\) distance from the SP over the course of training. Agents which use OGD directly on the policy parameters are prefixed by \u201cOGD-\u201d, and ReLOAD with no trust region is denoted by \u201cRNTR-\u201d. Both \\\\( \\\\mu \\\\)-MDPO and RNTR-MDPO fail to converge. However, performing OGD directly on the policy parameters, rather than via optimistic value estimates, performs nearly as well as ReLOAD. This is because the CMDP only has two states, so the gradient computed from the previous minibatch will be close to the current gradient. This difference becomes significant on large-scale problems.\\n\\nTo test ReLOAD\u2019s performance with \\\\( N > 1 \\\\) constraints, we generated random constraint rewards in a small CMDP analogous to the one depicted in Fig. 3 but with three states such that taking action \\\\( a_i \\\\) deterministically transitions the agent to state \\\\( s_i \\\\) for \\\\( i \\\\in \\\\{1, 2, 3\\\\} \\\\). Rewards for each state \\\\( r_n(s_i) \\\\) were randomly set between 0 and 1, and the resulting CMDP was tested for feasibility using the CVXPY package (Diamond & Boyd, 2016). We measured the performance of \\\\( \\\\mu \\\\)-MDPI and ReLOAD-MDPI for \\\\( N \\\\in \\\\{1, 2, 4, 8, 16, 32\\\\} \\\\), with each \\\\( N \\\\) repeated for 10 seeds. As discussed in Appendix B, it\u2019s less likely for random constraints to result in oscillations for lower \\\\( N \\\\), as it\u2019s easier for one reward to dominate the others. However, as \\\\( N \\\\) increases, it\u2019s more probable to have parity among the rewards, making the optimization process more prone to oscillations. Accordingly, the results (Fig. 5) indicate that while performance is comparable for low \\\\( N \\\\), ReLOAD is much better able to maintain performance as the number of constraints increases.\\n\\nWe then applied \\\\( \\\\mu \\\\)-MDPO and ReLOAD-MDPO to a constrained version of Bsuite\u2019s Catch (Osband et al., 2019). In the standard version of this task, the agent moves a paddle left or right to catch a falling ball. To convert this problem into a CMDP, we added a constraint reward \\\\( r_1 \\\\) which was 0.2 in the leftmost three columns of the environment and 0 elsewhere, with the constraint \\\\( \\\\theta_1 = 1 \\\\). To both obey the constraint and catch the ball, the agent could only effectively make one trip per episode to the left side of the arena. In Fig. 6a, ReLOAD successfully damps oscillations and achieves LIC, while \\\\( \\\\mu \\\\)-MDPO only achieves AIC. As an illustration of the consequences, we can see that when the Lagrange multiplier spikes upwards, indicating a constraint violation, the agent successfully catches the ball but lingers on the left side of the environment and thus violates the constraint (Fig. 6b, top). Conversely, a drop in the Lagrange multiplier indicates that the constraint is satisfied, but performance suffers\u2014the agent simply ignores balls falling on the left side of the arena (Fig. 6b, bottom). ReLOAD learns to stay to the right until the last moment, catching the ball while obeying the constraint (Fig. 6b, middle).\\n\\nThe Real-World RL Suite (RWRL; Dulac-Arnold et al., 2019) is a collection of DeepMind Control Suite tasks modified with constraints as well as a variety of other real-world challenges which has become a benchmark for applied RL agents (Dulac-Arnold et al., 2020; Huang et al., 2022; Calian et al., 2020; Brunke et al., 2022).\"}"}
{"id": "moskovitz23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ReLOAD for Last-Iterate Convergence in Constrained MDPs\\n\\nFigure 8. ReLOAD (dark blue) achieves the strongest performance on a variety of CMDPs which induce oscillations in control suite.\\n\\nFigure 9. ReLOAD significantly damps oscillations, resulting in agents which perform the desired task while obeying constraints.\\n\\nchallenging tasks: RWRL-Walker, RWRL-Cartpole, and RWRL-Quadruped across the same three constraint thresholds and four safety coefficients for each task used by Calian et al. (2020). As baselines, we applied MetaL, a tuned, fixed-Lagrange method (0.1-D4PG), and a primal-dual D4PG variant similar to $\\\\mu$-D4PG (RC-D4PG), all as in Calian et al. (2020). We also compared ReLOAD against $\\\\mu$-DMPO and multi-objective DMPO (Huang et al., 2022) on the easier safety coefficient settings used by Huang et al. (2022) (see Appendix F and Fig. 14 for further details). As we can see in Fig. 7, ReLOAD solves nearly all the CMDPs at least as well as the baselines and outperforms them for the most challenging safety coefficients. Interestingly, we found that the benchmark constraint thresholds introduced by Calian et al. (2020) were selected to be extreme so as to avoid oscillations. The RWRL suite therefore serves as a useful sanity check that ReLOAD performs strongly even without the threat of oscillations, but we would still like to test it on high-dimensional CMDPs which carry this threat.\\n\\nOscillating Control Suite. Finally, we identified tasks and constraint settings in DeepMind Control Suite which cause standard agents to oscillate. We trained ReLOAD on the following tasks: Walker, Walk with a constraint on the height of the agent, Reacher, Easy with a velocity constraint, Walker, Walk with a velocity constraint, Quadruped, Walk with a constraint on the torque applied to its joints, and Humanoid, Walk with a height constraint. We call this set of tasks and constraints, whose details can be found in Appendix G, the Oscillating Control Suite, and we believe it can serve as a challenging benchmark of CMDPs. To test its generality, we paired ReLOAD with both IMPALA and DMPO base agents, and compared it against the corresponding $\\\\mu$-agent, OGD, an agent which uses the fixed optimal Lagrange multiplier obtained by averaging the final iterates of the associated $\\\\mu$-across seeds ($\\\\mu^*$-), and PID control (PID-; Stooke et al., 2020). For IMPALA, we also tested RNTR. We found that in all cases, ReLOAD achieves higher average weighted reward at the end of training than the baselines (Fig. 8).\"}"}
