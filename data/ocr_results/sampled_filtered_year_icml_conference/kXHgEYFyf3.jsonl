{"id": "kXHgEYFyf3", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def read_next_bytes(fid, num_bytes, format_char_sequence, endian_character='<'):\\n...\\n\\ndef write_next_bytes(fid, data, format_char_sequence, endian_character='<'):\\n...\\n\\ndef write_images_binary(images, path_to_model_file):\\n    \\n    see: src/base/map.cc\\n    \\n    void Reconstruction::ReadImagesBinary(const std::string& path)\\n    void Reconstruction::WriteImagesBinary(const std::string& path)\\n    \\n    with open(path_to_model_file, 'wb') as fid:\\n        write_next_bytes(fid, len(images), 'Q')\\n        for _, img in images.items():\\n            write_next_bytes(fid, img.id, 'i')\\n            write_next_bytes(fid, img.qvec.tolist(), 'dddd')\\n            write_next_bytes(fid, img.tvec.tolist(), 'ddd')\\n            write_next_bytes(fid, img.camera_id, 'i')\\n            for char in img.name:\\n                write_next_bytes(fid, char.encode('utf-8'), 'c')\\n            write_next_bytes(fid, b'\\\\x00', 'c')\\n            write_next_bytes(fid, len(img.point3D_ids), 'Q')\\n            for xy, p3d_id in zip(img.xys, img.point3D_ids):\\n                write_next_bytes(fid, [*xy, p3d_id], 'ddq')\\n\\n    def read_points3d_binary(path_to_model_file):\\n    ...\\n\\n    def write_points3d_binary(points3D, path_to_model_file):\\n        \\n        see: src/base/map.cc\\n        \\n        void Reconstruction::ReadPoints3DBinary(const std::string& path)\\n        void Reconstruction::WritePoints3DBinary(const std::string& path)\\n        \\n        with open(path_to_model_file, 'wb') as fid:\\n            write_next_bytes(fid, len(points3D), 'Q')\\n            for _, pt in points3D.items():\\n                write_next_bytes(fid, pt.id, 'Q')\\n                write_next_bytes(fid, pt.xyz.tolist(), 'ddd')\\n                write_next_bytes(fid, pt.rgb.tolist(), 'BBB')\\n                write_next_bytes(fid, pt.error, 'd')\\n            track_length = pt.image_ids.shape[0]\\n            write_next_bytes(fid, track_length, 'Q')\\n            for image_id, point2D_id in zip(pt.image_ids, pt.point2D_idxs):\\n                write_next_bytes(fid, [image_id, point2D_id], 'ii')\\n\\n    def read_point3d_feature_binary(path_to_feature_file):\\n        \\n        point3d_features = {}\\n        with open(path_to_feature_file, 'rb') as file:\\n            num_points3d = struct.unpack('<Q', file.read(8))[0]\\n            dim_feature = struct.unpack('<Q', file.read(8))[0]\\n            for i in range(num_points3d):\\n                point3d_id = struct.unpack('<Q', file.read(8))[0]\\n                feature_num = struct.unpack('<I', file.read(4))[0]\"}"}
{"id": "kXHgEYFyf3", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def read_images_binary(path_to_model_file):\\n    \\n    \\\"\\\"\\\"Read camera images and their properties from a binary file.\\n    This function parses a binary file containing the properties of images used in 3D reconstruction\\n    and returns a dictionary of Image namedtuples, each containing the properties of an image.\\n    \\n    Args:\\n        path_to_model_file (str): The path to the binary model file to be read.\\n    \\n    Returns:\\n        dict: A dictionary where keys are image IDs (int) and values are Image namedtuples.\\n        Each Image namedtuple has the following fields:\\n        - id (int): The unique identifier of the image.\\n        - qvec (numpy.ndarray): A 4-element array representing the quaternion vector.\\n        - tvec (numpy.ndarray): A 3-element array representing the translation vector.\\n        - camera_id (int): The identifier of the camera used to capture the image.\\n        - name (str): The name of the image file.\\n        - xys (numpy.ndarray): An Nx2 array of [x, y] coordinates of 2D points in the image.\\n        - point3D_ids (numpy.ndarray): An N-element array of identifiers of 3D points corresponding to the 2D points.\\n    \\\"\\\"\\\"\\n    images = {}\\n    with open(path_to_model_file, 'rb') as fid:\\n        num_reg_images = read_next_bytes(fid, 8, 'Q')[0]\\n        for _ in range(num_reg_images):\\n            reg_image_id = read_next_bytes(fid, 4, 'i')[0]\\n            qvec = np.array(read_next_bytes(fid, 8 * 4, 'dddd'))\\n            tvec = np.array(read_next_bytes(fid, 8 * 3, 'ddd'))\\n            camera_id = read_next_bytes(fid, 4, 'i')[0]\\n            image_name = ''\\n            while True:\\n                char = read_next_bytes(fid, 1, 'c')[0]\\n                if char == b'\\x00':\\n                    break\\n                image_name += char.decode('utf-8')\\n            num_points2D = read_next_bytes(fid, 8, 'Q')[0]\\n            xys = np.zeros((num_points2D, 2), dtype=np.float32)\\n            point3D_ids = np.zeros(num_points2D, dtype=np.int64)\\n            for i in range(num_points2D):\\n                xys[i] = read_next_bytes(fid, 8 * 2, 'dd')\\n                point3D_ids[i] = read_next_bytes(fid, 8, 'Q')[0]\\n            images[reg_image_id] = Image(id=reg_image_id, qvec=qvec, tvec=tvec, camera_id=camera_id, name=image_name, xys=xys, point3D_ids=point3D_ids)\\n    return images\\n\\nListing 7. The full context for the read images binary function provides various functions in context implementing similar functionality in write binary images allows the model to copy relevant snippets for solving the problem.\"}"}
{"id": "kXHgEYFyf3", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"import torch\\n\\ndef _product_attr(vision, text, alter):\\n    \\n    Args:\\n    vision: N x D\\n    text: M x D\\n    alter: N x M, to replace results in some cases, see details in Returns\\n    \\n    Returns: N x M.\\n    For (n, m) element, set J_m = {j : text[m, j] == 1}.\\n    - if |J_m| > 0, it equals to (prod_{j in J_m} vision[n, j])**(1/|J_m|)\\n    - if |J_m| == 0, it equals to alter[n, m]\\n    \\n    vision = vision.unsqueeze(1)\\n    text = text.unsqueeze(0)\\n    num_attr = text.sum(-1)\\n    queried_attr = vision * text\\n    queried_attr = queried_attr.masked_fill(text == 0, 1)\\n    queried_attr = torch.float_power(queried_attr.prod(dim=2), 1 / torch.max(num_attr, torch.ones_like(num_attr))).float()\\n    no_attr_queries = num_attr.squeeze(0) == 0\\n    queried_attr[:, no_attr_queries] = alter[:, no_attr_queries]\\n    return queried_attr\\n\\ndef obj_with_attributes(input_embeddings, query_embeddings, n_obj, n_part, n_attr):\\n    \\n    Compute the similarity between object embeddings and query embeddings based on attributes.\\n    This function calculates the similarity score between each pair of object and query embeddings.\\n    The score is computed as the square root of the product of the object score and the geometric\\n    mean of the queried attributes, if any attributes are queried. If no attributes are queried,\\n    the object score is returned as is.\\n    \\n    vision = input_embeddings[:, :n_obj]\\n    text = query_embeddings[:, n_obj:n_obj + n_attr]\\n    alter = input_embeddings[:, n_obj + n_attr:]\\n    queried_attr = _product_attr(vision, text, alter)\\n    obj_score = (input_embeddings[:, :n_obj] * query_embeddings[:, :n_obj]).sum(dim=1, keepdim=True)\\n    scores = torch.sqrt(obj_score * queried_attr)\\n    return scores\\n\\nError\\nTraceback (most recent call last):\\n  File \"<string>\", line 17, in test_obj_with_attributes\\n  File \"/capture_args.py\", line 107, in wrapper\\n    output = func(*args, **kwargs)\\n                             ^^^^^^^^^^^^^^^^^^^^^^  \\n  File \"/tmp/tmptgi66m5s/paco_query_utils.py\", line 62, in obj_with_attributes\\n    queried_attr = _product_attr(vision, text, alter)\\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/tmp/tmptgi66m5s/paco_query_utils.py\", line 22, in _product_attr\\n    queried_attr = vision * text\\n                   ^~~~~~~~\\n  RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2\"}"}
{"id": "kXHgEYFyf3", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Length of retrieval. We compare how the performance of the models is impacted by the length (# tokens) of the retrieval context. Since we perform dependency-only-context retrieval, we only have the context required to understand the necessary functions for solving the problem instance. We find that the performance is not strongly correlated with the length of the retrieval context (Figure 11). This suggests that the choice of the retrieved context is a bigger factor than the length.\\n\\nCOT on R2E-Eval1. We use 0-shot and 2-shot COT to evaluate more enhanced code generation approaches. The following table describes performance.\\n\\n|        | Base | COT-0-shot | COT-2-shot |\\n|--------|------|------------|------------|\\n| GPT-3.5-TURBO | 48.9 | 45.8       | -          |\\n| GPT-4   | 33.2 | 33.0       | 28.8       |\\n\\nTable 7. Effect of COT on code generation on a subset of our R2E-Eval1 benchmark.\"}"}
{"id": "kXHgEYFyf3", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E. Experiments\\n\\nWe list down the list of models considered for code generation experiments here.\\n\\n| Model ID Link                                      |\\n|---------------------------------------------------|\\n| codellama/CodeLlama-34b-Python-hf                 |\\n| codellama/CodeLlama-13b-Python-hf                 |\\n| codellama/CodeLlama-7b-Python-hf                  |\\n| gpt-3.5-turbo-1106-16k OpenAI                     |\\n| gpt-4-1106                                         |\\n\\nE.1. Code Generation\\n\\nTo compute $P_{ASS}$, we generate $5$ completions for each problem instance using each model. We use nucleus sampling with $p = 0.95$ and $T = 0.2$. Below we list the prompts used (inspired from (Olausson et al., 2023))\\n\\nYou are a Python programming expert who is going to generate a Python function in a file using the function docstring. You will use the existing context of relevant files provided for implementation and ONLY return the completed function. Enclose the completed function in markdown code delimiters and do NOT return anything else.\\n\\n### Code Snippet\\n\\n```python\\n# Complete the function {function_name}. Only return the completed function enclosed in markdown code delimiters\\n```\"}"}
{"id": "kXHgEYFyf3", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E.2. Self Repair\\n\\nWe use GPT-4 and GPT-3.5-TURBO models for the self-repair task. We find problems from R2E where models fail to generate correct completion and we can extract the failing scenario (since some of the tests are dynamic, it is not always possible to extract the failing scenario). Additionally, since our GPT-4 has a context length of 8k, we additionally filter very long problems from the repair dataset. We then use the failing scenario as the prompt for the self-repair task.\"}"}
{"id": "kXHgEYFyf3", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"F. Code Generation Results\\n\\n```python\\n# nobuco/locate/link.py\\nimport inspect\\n\\ndef get_link(file=None, line=None):\\n    \"\"\" Print a link in PyCharm to a line in file.\\n    Defaults to line where this function was called. \"\"\"\\n    if file is None:\\n        file = inspect.stack()[1].filename\\n    if line is None:\\n        line = inspect.stack()[1].lineno\\n    string = f'File \"{file}\", line {max(line, 1)}'.replace(\"\\\\\", \"/\")\\n    return string\\n\\ndef get_link_to_obj(obj):\\n    obj_type = type(obj)\\n    if obj_type is property:\\n        obj = obj.fget\\n    if not hasattr(obj, '__module__') or not hasattr(obj, '__qualname__'):\\n        return None\\n    module = inspect.getmodule(obj)\\n    if module is None:\\n        return None\\n    if not hasattr(module, '__file__'):\\n        return None\\n    if obj_type is type:\\n        obj_source = obj.__qualname__\\n    elif obj_type is types.ModuleType:\\n        obj_source = obj.__name__\\n    else:\\n        obj_source = f'{obj.__module__}.{obj.__qualname__}\\n    return f'File \"{module.__file__}\", line {inspect.getsourcelines(obj)[1]}'.replace('\\\\', '/')\\n```\\n\\nListing 2. CodeLLaMa-34B repeats the code from its parent function and does not use the 'get_link' helper method.\"}"}
{"id": "kXHgEYFyf3", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def standardize_and_get_inchi(mol: Mol, options: str='', log_level=None, warnings_as_errors=False) -> str:\\n    \"\"\"Return InChI after standardising molecule and inferring stereo from coordinates.\\n    mol = deepcopy(mol)\\n    mol = assert_sanity(mol)\\n    mol = remove_isotopic_info(mol)\\n    has_pose = mol.GetNumConformers() > 0\\n    if has_pose:\\n        RemoveStereochemistry(mol)\\n    mol = RemoveHs(mol)\\n    try:\\n        mol = neutralize_atoms(mol)\\n    except AtomValenceException:\\n        logger.warning('Failed to neutralize molecule. Using uncharger. InChI check might fail.')\\n        mol = Uncharger().uncharge(mol)\\n    mol = add_stereo_hydrogens(mol)\\n    if has_pose:\\n        AssignStereochemistryFrom3D(mol, replaceExistingTags=True)\\n    with CaptureLogger():\\n        inchi = MolToInchi(mol, options=options, logLevel=log_level, treatWarningAsError=warnings_as_errors)\\n    return inchi\\n\\ndef check_identity(mol_pred: Mol, mol_true: Mol, inchi_options: str='') -> dict[str, Any]:\\n    \"\"\"Check if two molecules are identical in terms of their InChI representation.\\n    This function compares the InChI strings of two molecules after standardizing them and\\n    optionally applying InChI generation options. ...\\n    Args:\\n    ...\\n    Returns:\\n    ...\\n    Example:\\n    >>> ...\\n    \\n    mol_pred = deepcopy(mol_pred)\\n    mol_true = deepcopy(mol_true)\\n    mol_pred = assert_sanity(mol_pred)\\n    mol_true = assert_sanity(mol_true)\\n    mol_pred = remove_isotopic_info(mol_pred)\\n    mol_true = remove_isotopic_info(mol_true)\\n    has_pose_pred = mol_pred.GetNumConformers() > 0\\n    has_pose_true = mol_true.GetNumConformers() > 0\\n    if has_pose_pred:\\n        RemoveStereochemistry(mol_pred)\\n    if has_pose_true:\\n        RemoveStereochemistry(mol_true)\\n    mol_pred = RemoveHs(mol_pred)\\n    mol_true = RemoveHs(mol_true)\\n    try:\\n        mol_pred = neutralize_atoms(mol_pred)\\n    except AtomValenceException:\\n        logger.warning('Failed to neutralize molecule. Using uncharger. InChI check might fail.')\\n        mol_pred = Uncharger().uncharge(mol_pred)\\n    try:\\n        mol_true = neutralize_atoms(mol_true)\\n    except AtomValenceException:\\n        logger.warning('Failed to neutralize molecule. Using uncharger. InChI check might fail.')\\n        mol_true = Uncharger().uncharge(mol_true)\\n    mol_pred = add_stereo_hydrogens(mol_pred)\\n    mol_true = add_stereo_hydrogens(mol_true)\"}"}
{"id": "kXHgEYFyf3", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While Large Language Models' (LLM) coding capabilities have advanced rapidly, corresponding evaluation benchmarks on real-world programming setups are yet to catch up. Building a scalable and interactive testbed for evaluating general-purpose AI programming agents for real-world code has been challenging, particularly due to a lack of high-quality test suites available. In this paper, we present Repository to Environment (R\u00b2E), a framework that can turn any GitHub repository into a test environment to evaluate the performance of code-generating systems, both static and interactive. R\u00b2E is powered by a synergistic combination of program analysis and LLMs to construct equivalence test harnesses for any GitHub function. We instantiate our framework to build the first large-scale benchmark, R\u00b2E-Eval1, for building realistic environments for AI coding assistants. Our results demonstrate that even when SOTA models cannot generate correct solutions with advanced prompting techniques, they can effectively use environment feedback highlighting the need to move from static functional coding to interactive programming paradigm. We hope that our framework (and the instantiated benchmark) can motivate research directions by providing web-scale open-ended coding environments. R\u00b2E code is available at https://r2e.dev/.\\n\\n1. Introduction\\n\\nThe rapid improvement of LLMs' performance on code-related tasks has enabled the development of coding assistants deployed in the real world. However, evaluations on such real-world coding setups have not kept pace. Prior benchmarks (Chen et al., 2021; Wang et al., 2022b), used for evaluating coding capabilities of LLMs, only consist of short and isolated functional code completion problems. On the other hand, real-world software engineering requires more complex workflows involving integrating code with existing (large) codebases, using libraries, interacting with the interpreter, debugging errors, etc. In this work, to capture this interactive aspect (in contrast with single-shot code generation), we consider programming agents as AI systems that can similarly use interpreters and error feedback to improve their own outputs given a specification. As such, programming agents become more powerful, it urges the need to build real-world test environments to evaluate them.\\n\\nIn this work, we propose Repository to Environment (R\u00b2E), a scalable framework for turning any GitHub repository into a test environment to evaluate the performance of code generation systems on real-world scenarios (Section 3). We build on a key insight that test suites if synthesized for real-world code, can act as checks as well as orchestra tors for execution-guided programming environments. R\u00b2E takes a function (from GitHub), constructs an equivalence test harness \u2014 a scaffold consisting of test cases and a setup that establishes dependencies for executing the function. R\u00b2E further refines the docstring and uses the refined specification along with repository code and test harness as a problem instance for studying code generation. Figure 1 provides an end-to-end diagram of our approach.\\n\\nThese environments serve two evaluation purposes: First, a code generation system can be evaluated via the environment in these real-world scenarios. Secondly, even for an interactive programming agent, our environment can provide feedback to the agent using the interpreter (Figure 1 right). Notably, R\u00b2E framework is scalable and can be used to build web-scale open-domain coding datasets. Furthermore, R\u00b2E requires minimal human supervision and can be updated in a live manner for contamination-free evaluation.\\n\\nUsing this framework, we construct R\u00b2E-Eval1 (Section 4), the first large-scale benchmark of real-world coding problems consisting of natural-language docstrings, repository contexts, and equivalence test harnesses. Figure 2 shows an example of a function and corresponding synthesized...\"}"}
{"id": "kXHgEYFyf3", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Below, we list the prompt used for test harness generation.\\n\\nYou are a python programming expert who was hired to write tests for Python functions. You will be given a python function in a python file and you will write a complete test that covers the function and all the different corner cases.\\n\\nYou can assume a compiled reference implementation of the function is available, and hence do not need to predict the expected output of the function. That is, the test you write will use the reference implementation to generate the expected output.\\n\\nAlso, assume the function provided is correct and hence the test should focus on the behavior that is defined by the function ONLY.\\n\\nEnsure that the tests align with the function's expected input types, avoiding scenarios that the function is not designed to handle.\\n\\nCompletely avoid testing with invalid input types or values, testing for error handling, and checking 'assertRaises'. Set a fixed random seed in tests involving randomness to ensure consistent and reproducible results when necessary.\\n\\nAvoid mocking calls to APIs or functions (e.g., builtins.open) when actual implementations are simple, accessible, and their use does not compromise the test's isolation or determinism.\\n\\nParticularly, avoid mocking calls to any file I/O APIs, and instead try to create temporary files and directories for testing purposes.\\n\\nYou will return the test for that function and NOT return anything except for the test.\\n\\nPut your fixed test program within code delimiters, for example:\\n\\n```python\\n# YOUR CODE HERE\\n```\\n\\nWrite a test using the 'unittest' library for the function 'function_name'. Assume the reference implementation is 'reference_function_name'. Both the function and the reference are in the module 'fut_module'. Only return the test code and do NOT return anything else. Enclose your code within code delimiters, for example:\\n\\n```python\\n# YOUR CODE HERE\\n```\"}"}
{"id": "kXHgEYFyf3", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.2. Analysis of Equivalence Test Generation\\n\\nBelow is further analysis of the performance of R\u00b2E's equivalence test generation, as described in Section 3.2.\\n\\n- Figure 6. Varying number of lines\\n- Figure 7. Varying number of dependencies\\n- Figure 8. Varying number of branches\\n- Figure 9. Varying number of arguments\"}"}
{"id": "kXHgEYFyf3", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We study the quality of the refined docstrings, specifically on their effectiveness as problem descriptors for code completion. We analyze two high-level aspects of docstrings:\\n\\n\u2022 **Interface Understandability:** Does the refined docstring clarify the function's input-output interface? Here, we look at how the parameter descriptions and the I/O examples aid in understanding the interface.\\n\\n\u2022 **Problem Intent:** Does the refined docstring articulate the problem the function is intended to solve? Here, we look at how the docstring text and the I/O examples illustrate how the function solves the problem and if it can lead to alternate solutions that do not satisfy the original implementation.\\n\\nEach criterion was evaluated using a 3-point scale, ranging from 'Poor' (1) to 'Excellent' (3). The lead authors applied this rubric to study a sample of 62 problems. Results in the following tables.\\n\\n| Param Desc | I/O Examples | Overall |\\n|------------|--------------|---------|\\n|            |              | 2.9     |\\n\\nTable 4. Interface Understandability Scores\\n\\n| Text Desc | I/O Examples | Overall |\\n|-----------|--------------|---------|\\n|           |              | 2.5     |\\n\\nTable 5. Problem Intent Scores\"}"}
{"id": "kXHgEYFyf3", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The list of unique input and output data types is provided below. This highlights that problems in our benchmark are interesting.\\n\\n```python\\n{\"__main__.ComplexDataClass\", \"__main__.ExampleDataClass\", \"__main__.MockTextDocument\", \"__main__.NestedDataClass\", \"__main__.PickleCoder\", \"__main__.SimpleDataClass\", \"ast.Attribute\", \"ast.Call\", \"astroid.nodes.scoped_nodes.scoped_nodes.FunctionDef\", \"builtins.bool\", \"builtins.builtin_function_or_method\", \"builtins.bytes\", \"builtins.dict\", \"builtins.EOFError\", \"builtins.float\", \"builtins.function\", \"builtins.generator\", \"builtins.int\", \"builtins.list\", \"builtins.list_reverseiterator\", \"builtins.method\", \"builtins.module\", \"builtins.NoneType\", \"builtins.property\", \"builtins.set\", \"builtins.slice\", \"builtins.str\", \"builtins.tuple\", \"builtins.type\", \"builtins.ValueError\", \"casadi.casadi.Function\", \"cascades._src.handlers.Record\", \"celpy.celtypes.BoolType\", \"collections.defaultdict\", \"collections.OrderedDict\", \"dacite.config.Config\", \"dis.Instruction\", \"diskcache.core.Cache\", \"docile.dataset.bbox.BBox\", \"dpkt.ethernet.Ethernet\", \"dynamicprompts.parser.config.ParserConfig\", \"fullcontrol.combinations.gcode_and_visualize.classes.Point\", \"fullcontrol.geometry.vector.Vector\", \"_Stats\", \"Compression\", \"Encoding\", \"Graph\", \"GroupedTensor\", \"Indicator\", \"KGFn\", \"LogSeverity\", \"LogTensor\", \"NamedList\", \"RangeSlotList\", \"RequestsCookieJar\", \"return_type_ptiva_linalg_eigh\", \"Sound\", \"SSH\", \"TextSlotList\", \"WildcardSlotList\", \"iamspy.iam.Document\", \"jaxlib.xla_extension.ArrayImpl\", \"klongpy.core.KGSym\", \"kork.ast.FunctionCall\", \"lgssl.evaluation.logistic_regression.LogisticRegression\", \"libcst._nodes.module.Module\", \"mypy.nodes.OpExpr\", \"networkx.classes.digraph.DiGraph\", \"networkx.classes.graph.Graph\", \"networkx.classes.multigraph.MultiGraph\", \"numpy._ArrayFunctionDispatcher\", \"numpy.bool_\", \"numpy.float64\", \"numpy.int64\", \"numpy.ndarray\", \"numpy.random.mtrand.RandomState\", \"numpyro.distributions.continuous.Normal\", \"open_rarity.models.collection.Collection\", \"open_rarity.models.token.Token\", \"ormdantic.models.models.Map\", \"pandas.core.frame.DataFrame\", \"pandas.core.series.Series\", \"pathlib.PosixPath\", \"pydantic.main.BaseModel\", \"pygame.surface.Surface\", \"pyparsing.core.Forward\", \"pywhy_graphs.classes.admg.ADMG\", \"pywhy_graphs.classes.pag.PAG\", \"pywhy_graphs.classes.timeseries.digraph.StationaryTimeSeriesDiGraph\", \"pywhy_graphs.classes.timeseries.pag.StationaryTimeSeriesPAG\", \"rdkit.Chem.rdchem.Mol\", \"scipy.sparse._csr.csr_matrix\", \"scipy.sparse._lil.lil_matrix\", \"sklearn.linear_model._logistic.LogisticRegression\", \"sklearn.neighbors._kde.KernelDensity\", \"sqlalchemy.sql.sqltypes.DateTime\", \"sympy.core.add.Add\", \"sympy.core.mul.Mul\", \"sympy.core.numbers.Integer\", \"sympy.core.numbers.NegativeOne\", \"sympy.core.numbers.One\", \"sympy.core.numbers.Pi\", \"sympy.core.numbers.Zero\", \"sympy.functions.elementary.exponential.log\", \"sympy.functions.elementary.trigonometric.cos\", \"sympy.functions.elementary.trigonometric.sin\", \"torch.device\", \"torch.nn.modules.conv.Conv2d\", \"torch.nn.modules.linear.Linear\", \"torch.nn.parameter.Parameter\", \"torch.Tensor\", \"torchsig.utils.types.SignalCapture\", \"torchsig.utils.types.SignalData\", \"tracr.rasp.rasp.Aggregate\", \"tracr.rasp.rasp.Map\", \"tracr.rasp.rasp.SelectorWidth\", \"typing._AnnotatedAlias\", \"typing._GenericAlias\", \"typing._UnionGenericAlias\", \"unittest.mock.MagicMock\", \"uuid.UUID\", \"xarray.core.dataset.Dataset\", \"z3.z3.BoolRef\", \"z3.z3.SeqRef\"}\\n```\\n\\nListing 1. Unique input and output data types in our benchmark. We have over 100 unique data types arising from over 60 libraries.\"}"}
{"id": "kXHgEYFyf3", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"if has_pose_pred:\\n    AssignStereochemistryFrom3D(mol_pred, replaceExistingTags=True)\\nif has_pose_true:\\n    AssignStereochemistryFrom3D(mol_true, replaceExistingTags=True)\\ninchi_pred = standardize_and_get_inchi(mol_pred, options=inchi_options)\\ninchi_true = standardize_and_get_inchi(mol_true, options=inchi_options)\\nresults = _compare_inchis(inchi_true, inchi_pred)\\nresults['inchi_crystal_valid'] = _is_valid_inchi(inchi_true)\\nresults['inchi_docked_valid'] = _is_valid_inchi(inchi_pred)\\nresults['inchi_crystal'] = inchi_true\\nresults['inchi_docked'] = inchi_pred\\nreturn {'results': results}\\n\\nListing 3.\\n\\nModels struggle to understand the interface functions provided in context and tend to repeat the content from the standardize_and_get_inchi function instead of using the abstraction.\"}"}
{"id": "kXHgEYFyf3", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The first example demonstrates dependency context vs full context for the `get_2d_sincos_pos_embed` function. In the dependency context, only two functions which the oracle ground truth functions calls are provided and the task is to stitch them together. This becomes simpler in the full context case where how to call those functions is also provided (via other functions).\\n\\n```python\\nimport torch\\n\\ndef get_2d_sincos_pos_embed_from_grid(embed_dim: int, grid: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Get 2D sine-cosine positional embedding from grid.\\n    Args:\\n        embed_dim: embedding dimension.\\n        grid: positions\\n    Returns:\\n        (torch.Tensor): [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim]\\n    \"\"\"\\n    assert embed_dim % 2 == 0\\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])\\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])\\n    emb = torch.cat([emb_h, emb_w], dim=1)\\n    return emb\\n\\ndef get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Get 1D sine-cosine positional embedding.\\n    Args:\\n        embed_dim: output dimension for each position\\n        pos: a list of positions to be encoded: size (M,)\\n    Returns:\\n        (torch.Tensor): tensor of shape (M, D)\\n    \"\"\"\\n    assert embed_dim % 2 == 0\\n    omega = torch.arange(embed_dim // 2, dtype=torch.float)\\n    omega /= embed_dim / 2.0\\n    omega = 1.0 / 10000 ** omega\\n    pos = pos.reshape(-1)\\n    out = torch.einsum('m,d->md', pos, omega)\\n    emb_sin = torch.sin(out)\\n    emb_cos = torch.cos(out)\\n    emb = torch.cat([emb_sin, emb_cos], dim=1)\\n    return emb\\n\\ndef get_2d_sincos_pos_embed(embed_dim: int, grid_size: int, cls_token: bool=False) -> torch.Tensor:\\n    \"\"\"Generates a 2D sine-cosine positional embedding tensor.\\n    This function creates a positional embedding for a 2D grid using sine and cosine functions.\\n    The embedding can optionally include a leading zero vector to represent a classification (CLS) token.\\n    Args:\\n        embed_dim (int): The dimensionality of the embedding for each position.\\n        grid_size (int): The height and width of the square grid for which embeddings are generated.\\n        cls_token (bool): If True, the output tensor will include an additional first row with zeros to represent a CLS token. Defaults to False.\\n    Returns:\\n        torch.Tensor: A tensor of shape (grid_size * grid_size, embed_dim) without a CLS token, or (1 + grid_size * grid_size, embed_dim) with a CLS token. The tensor contains the positional embeddings for the grid and is of type 'torch.float32'.\\n    \"\"\"\\n    grid = torch.stack(torch.meshgrid(torch.arange(grid_size), torch.arange(grid_size)), dim=-1)\\n    grid = grid.reshape(-1, 2).float()\\n    emb = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\\n    if cls_token:\\n        cls_emb = torch.zeros(1, embed_dim)\\n        emb = torch.cat([cls_emb, emb], dim=0)\\n    return emb\\n```\\n\\nListing 4. GPT4 code generation with a dependency only prompt. The model needs to understand the provided functions and stitch the solution together since no example usage of the required functions is provided.\"}"}
{"id": "kXHgEYFyf3", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"from typing import Tuple\\nimport torch\\ndef get_3d_sincos_pos_embed(embed_dim: int, tube_shape: Tuple[int, int, int], stride, offset, kernel_size,\\ncls_token: bool=False) -> torch.Tensor:\\n    \"\"\"Get 3D sine-cosine positional embedding.\\n    Args:\\n    tube_shape: (t_size, ....\\n    stride:\\n    offset:\\n    kernel_size:\\n    cls_token: bool, whether to contain CLS token\\n    Returns:\\n    (torch.Tensor): [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim]\\n    \"\"\"\\n\\n    pos_embed_spatial = get_2d_sincos_pos_embed_from_grid(embed_dim_spatial, grid)\\n\\n    def get_2d_sincos_pos_embed(embed_dim: int, grid_size: int, cls_token: bool=False) -> torch.Tensor:\\n        \"\"\"Get 2D sine-cosine positional embedding.\\n        Args:\\n        grid_size: int of the grid height and width\\n        cls_token: bool, whether to contain CLS token\\n        Returns:\\n        (torch.Tensor): [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim]\\n        \"\"\"\\n\\n        grid = torch.arange(grid_size, dtype=torch.float)\\n        grid = torch.meshgrid(grid, grid, indexing='ij')\\n        grid = torch.stack(grid, dim=0)\\n        pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\\n        return pos_embed\\n\\n    def get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: torch.Tensor) -> torch.Tensor:\\n        \"\"\"Get 1D sine-cosine positional embedding.\\n        Args:\\n        embed_dim: output dimension for each position\\n        pos: a list of positions to be encoded: size (M,)\\n        Returns:\\n        (torch.Tensor): tensor of shape (M, D)\\n        \"\"\"\\n\\n        assert embed_dim % 2 == 0\\n        omega = torch.arange(embed_dim // 2, dtype=torch.float)\\n        emb = torch.zeros_like(pos) + 1\\n        emb[:, 0::2] = torch.sin(omega * pos[:, 0::2])\\n        emb[:, 1::2] = torch.cos(omega * pos[:, 1::2])\\n        return emb\\n\\n    def get_2d_sincos_pos_embed(embed_dim: int, grid_size: int, cls_token: bool=False) -> torch.Tensor:\\n        \"\"\"Generates a 2D sine-cosine positional embedding tensor.\\n        This function creates a positional embedding for a 2D grid using sine and cosine functions.\\n        The embedding can optionally include a leading zero vector to represent a classification (CLS) token.\\n        Args:\\n        embed_dim (int): The dimensionality of the embedding for each position.\\n        grid_size (int): The height and width of the square grid for which embeddings are generated.\\n        cls_token (bool): If True, the output tensor will include an additional first row with zeros\\nto represent a CLS token. Defaults to False.\\n        Returns:\\n        torch.Tensor: A tensor of shape (grid_size * grid_size, embed_dim) without a CLS token, or\\n        (1 + grid_size * grid_size, embed_dim) with a CLS token. The tensor contains\\n        the positional embeddings for the grid and is of type 'torch.float32'.\\n        \"\"\"\\n\\n        assert embed_dim % 2 == 0\\n        grid = torch.arange(grid_size, dtype=torch.float)\\n        grid = torch.meshgrid(grid, grid, indexing='ij')\\n        grid = torch.stack(grid, dim=0)\\n        pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\\n        return pos_embed\\n\\nListing 5.\\nThe full context for this example provides example usage of the required functions making it an easier problem.\"}"}
{"id": "kXHgEYFyf3", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"class Image(BaseImage):\\n\\ndef qvec2rotmat(self):\\n    return qvec2rotmat(self.qvec)\\n\\ndef read_next_bytes(fid, num_bytes, format_char_sequence, endian_character='<'):\\n    data = fid.read(num_bytes)\\n    return struct.unpack(endian_character + format_char_sequence, data)\\n\\ndef read_images_binary(path_to_model_file):\\n    \"\"\"Read camera images and their properties from a binary file. This function parses a binary file containing\\n    the properties of images used in 3D reconstruction\\n    and returns a dictionary of Image namedtuples, each containing the properties of an image.\\n    Args:\\n        path_to_model_file (str): The path to the binary model file to be read.\\n    Returns:\\n        dict: A dictionary where keys are image IDs (int) and values are Image namedtuples.\\n        Each Image namedtuple has the following fields:\\n        - id (int): The unique identifier of the image.\\n        - qvec (numpy.ndarray): A 4-element array representing the quaternion vector.\\n        - tvec (numpy.ndarray): A 3-element array representing the translation vector.\\n        - camera_id (int): The identifier of the camera used to capture the image.\\n        - name (str): The name of the image file.\\n        - xys (numpy.ndarray): An Nx2 array of [x, y] coordinates of 2D points in the image.\\n        - point3D_ids (numpy.ndarray): An N-element array of identifiers of corresponding 3D points\\n    \\n    ## GPT-4 Completion\\n    with open(path_to_model_file, 'rb') as fid:\\n        num_reg_images = read_next_bytes(fid, 4, 'I')[0]\\n        images = {}\\n        for _ in range(num_reg_images):\\n            image_id = read_next_bytes(fid, 4, 'I')[0]\\n            qvec = np.array(read_next_bytes(fid, 4 * 4, 'dddd'))\\n            tvec = np.array(read_next_bytes(fid, 3 * 4, 'fff'))\\n            camera_id = read_next_bytes(fid, 4, 'I')[0]\\n            image_name = read_next_bytes(fid, 1, 'c')[0]\\n            num_points2D = read_next_bytes(fid, 4, 'I')[0]\\n            xys = np.array(read_next_bytes(fid, num_points2D * 2 * 4, 'ff' * num_points2D))\\n            point3D_ids = np.array(read_next_bytes(fid, num_points2D * 4, 'I' * num_points2D))\\n            images[image_id] = Image(id=image_id, qvec=qvec, tvec=tvec, camera_id=camera_id, name=image_name, xys=xys, point3D_ids=point3D_ids)\\n    return images\\n\\nListing 6.\\nThe dependency context for the read_images_binary function only provides read_next_bytes function in context requiring\\nunderstanding the interface of returned value.\"}"}
{"id": "kXHgEYFyf3", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Equivalence Test Generation\\n\\n\u2714 Harness, not I/O examples\\n\u2714 Functional equivalence checks\\n\u2714 Coverage for reliability\\n\\n\ud83e\udd16 func()\\n\\nIn-file context\\nExternal Context\\nCodebase\\nutils\\ntorchsig\\nsrc\\nmain.py\\nhome.py...\\nviz\\nRepository\\n\\nCode Instrumentation\\n\\nTest Setup\\nPython 3.9.12 (Apr 5, 01:53:17)\\n\\nInstall \u2699\\nSelf-Repair \u2699\\nToT \u2699\\nCoT \u2699\\nReAct \u2699\\nParsel \u2699\\nReflection \u2699\\n\\nCode Generation\\n\\n\ud83e\udd16\\n\\nFigure 1.\\nAn overview of our R\u00b2E framework that takes any GITHUB repository and converts it into a programming agent test environment. Given a repository, we first scan for interesting functions and collect corresponding in-file and external-file contexts from the repository. Next, we use our test generation approach to develop high-quality equivalence test harnesses for the function. Our key insight is decoupling the test outputs from inputs by relying on the ground truth implementation to get the expected outputs. Our framework yields problem instances comprising docstrings, test harnesses, and repository context (instantiated in the form of R\u00b2E-Eval1 benchmark).\\n\\nNext, we build the repository and set up an interactive environment with an interpreter. Via these environments, generated benchmarks can be used to evaluate code generation systems, either static ones that directly generate code or programming agents that interact with the test harness and interpreter to improve code generation performance for repository-level problems.\\n\\n# torchsig/utils/index.py\\nfrom torchsig.utils.types import SignalCap, SignalDesc\\nimport os\\ndef _parse_sigmf(absolute_file_path: str):\\n...\\n\\ndef indexer(root) -> ...\\n\\n\"An indexer with classes by folders\"\\nnon_empty_dirs = [d for d in os.listdir(root)...\\n\\n...\\n\\nfor idx, dir_name in enumerate(non_empty_dirs):\\n\\nfor f in sigmf_files:\\n\\nfor sig_file in _parse_sigmf(...\\n\\n...\\n\\n# torchsig/utils/types.py\\n\\nclass SignalDesc:\\n...\\n\\nclass SignalCap:\\n...\\n\\nclass TestIndexerFromFoldersSigmf(unittest.TestCase):\\n\\ndef setUp(self):\\ntest_dir = TemporaryDirectory()()\\nclass_dirs = ['class_x', 'class_y']\\nfor class_dir in self.class_dirs:\\nos.makedirs(test_dir.name +\"/\"+class_dir))\\nself.sigmf_files = {'class_x': ['file1.sigmf-data', 'file2.sigmf-data']},\\n...\\n\\nfor class_name, files in self.sigmf_files.items():\\n\\nfor file_name in files:\\n...\\n\\nwith open(data_file_path, 'wb') as data_file:\\ndata_file.write(...\\n\\n\\ndef test_indexer_from_folders_sigmf(self):\\nresult = indexer(self.test_dir.name)\\nexpected = ref_indexer(self.test_dir.name)\\nself.assertEqual(len(result), len(expected))\\nfor res, exp in zip(result, expected):\\nself.assertEqual(res[1].num_bytes, exp[1].num_bytes)\\nself.assertEqual(res[1].byte_offset, exp[1].byte_offset)\\n\\nFigure 2.\\nAn example problem (left) in the R\u00b2E-Eval1 benchmark. The problem contains a function indexer from the Torchsig2GITHUB repository. TorchSig is an open-source signal processing machine learning toolkit based on the PyTorch data handling pipeline. The function indexer has dependencies within its file (_parse_sigmf) and from external files (SignalDesc and SignalCap from the file torchsig/utils/types.py). On the right is the generated equivalence test harness from our R\u00b2E framework. It contains a complex test setup where files expected by the function indexer are created and added to the file system. Then, the test harness performs functional equivalence checks for various granular properties of the returned output. Particularly, our harnesses test the functional behaviour of a given function directly against the ground truth program available on GITHUB, instrumented in the python namespace. Thus, we avoid predicting the expected output behaviour of a given function and only require constructing diverse inputs to test the function on.\\n\\ntest harness from our dataset. R\u00b2E-Eval1 comprises of 246 tasks extracted from 137 repositories containing 127,2 code tokens, 11.5 tests, and 3.7 dependencies per problem.\\n\\nFinally, in Section 5, we evaluate current LLM on real-world scenarios from our benchmark. We find that compared to HUMAN models perform worse on these problems, highlighting the challenges of real-world programming. Popular techniques like Chain-of-Thought (COT) do not help with performance. On the other hand, LLM agents that interactively program using the test harness and execution feedback greatly improve their performance. We also provide insights into model behavior specific to real-world programs, such as challenges in understanding interfaces to existing functions and reasoning about complex objects.\"}"}
{"id": "kXHgEYFyf3", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Overall, we find that real-world programming is complicated, even for SOTA LLMs (GPT-4), motivating the use of better workflows that mimic a typical developer\u2019s programming process. This underscores the need to move from static functional coding to interactive programming, the evaluation of which our framework enables.\\n\\nFinally, R\u00b2E environments enable collecting interaction traces for code generation, optimization, repair. We believe that beyond evaluation R\u00b2E is particularly well-suited for collecting large amounts of execution assisted synthetic data for improving coding abilities of LLMs.\\n\\n2. Background\\n\\nOur R\u00b2E pipeline is powered by a synergy of program analysis and LLMs. Here, we provide background on some concepts used in the following sections.\\n\\nTesting. Testing for functional correctness extends beyond mere input-output pairs, encompassing the broader dependencies that real-world software relies on. A Test Harness encapsulates this by combining Test Cases (defining inputs and expected outputs) and a Setup (establishing the operational conditions and dependencies like configuration files). The complexity of test harnesses, as illustrated in Figure 2, surpasses the simple input-output examples in previous benchmarks, like HUMAN EVAL (Chen et al., 2021). For instance, in Figure 2, the test harness contains the required setup of files in a directory (i.e., file system dependency) that the program expects to run successfully.\\n\\nCode Coverage. The quality of tests is widely measured by its coverage\u2014the fraction of code elements (e.g., statements or branches) it exercises (Ivankovi\u0107 et al., 2019). For example, a test that executes all lines of a function is said to have line coverage of 100%. A high coverage is desirable to ensure a function is tested thoroughly. We use branch coverage to evaluate the quality of our tests as it offers a more fine-grained measure than line coverage.\\n\\nProgram Analysis for Slicing Context. To effectively test repository code, we must grasp the function\u2019s operational context, which encompasses the functions and global variables it interacts with. We employ dependency slicing to construct this context, defining a slice \\\\( D_f \\\\) for function \\\\( f \\\\) as the set of functions \\\\( F' \\\\) called by \\\\( f \\\\) and global variables \\\\( G' \\\\) accessed by \\\\( f \\\\), both directly and indirectly. The top-left of Figure 2 shows an example of a dependency slice extracted for a function \\\\( \\\\text{indexer} \\\\), that serves as a minimal context necessary to comprehend the function\u2019s behavior. The resulting slice \\\\( D_f \\\\) provides the minimal context for understanding \\\\( f \\\\)\u2019s behavior and indicates its connectivity within the repository, quantified by the slice size \\\\( |D_f| \\\\). Details on computing the slice are in the Appendix A.\\n\\n3. The R\u00b2E Framework\\n\\nGitHub is a rich data source for realistic code problems, but repositories in the wild can be quite noisy, hard to run, and poorly maintained. We here propose R\u00b2E, an automated framework that turns any GitHub repository into a test environment to evaluate the performance of code generation systems on real-world code.\\n\\nSection 3.1 details our initial problem curation process. Section 3.2 describes our test harness synthesis approach. We evaluate the quality of our synthesized tests in Section 3.3. Finally, we describe how to refine problem specifications to build a high-quality benchmark in Section 3.4.\\n\\n3.1. Problem Curation\\n\\n3.1.1. Repository Curation\\n\\nWe scraped PYTHON repositories on GitHub created after July\u201922 that are non-forks, have at least 40 stars, and contain either a toml or setup.py file. This date aligns with the reported cutoff data for OpenAI models GPT-3.5-TURBO and GPT-4, thus preventing contamination. Next, we filter out some failing repositories from an earlier iteration of this process. We saved each repository in a common Docker image. We built them using pdm and pip install commands with cross repository package-caching whenever possible to reduce the memory footprint. Further, we used pipreqs to add requirements missing in the toml, setup, or requirements files. Finally, we semi-manually installed uninstallable repositories when possible, resulting in a docker with 429 installed repositories sizing over 400 GB.\\n\\n3.1.2. Function Curation\\n\\nWe first extract all functions (and methods) from the collected repositories to identify problems suitable for natural-language-driven code generation and functional correctness evaluation. Particularly, we filter out functions lacking docstrings to ensure we have a natural language prompt equivalent. We then apply keyword-based filters to exclude functions associated with GPUs, cloud tasks, etc., since they are not conducive to standard functional correctness evaluations. We estimate the complexity of the functions using its connectivity (detailed in Section 2). We filter out functions that do not call other components in the repository. We provide a more comprehensive list of filters applied in the appendix.\\n\\nThrough these stages of filtering, we collected candidate 9825 problems from our 429 repositories.\"}"}
{"id": "kXHgEYFyf3", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Test Harness Generation: A Key to Environments\\n\\nRepositories lack high-quality tests necessary for evaluating code generation, thus requiring automated test harness generation to collect problems scalably. If generated, these tests can act as checks and orchestrators for execution-guided programming agents. As checks, they can evaluate the functional correctness of generated code. Tests also enable applications beyond code generation, such as refactoring, optimization, and transpilation, where tests can check if the transformed program is equivalent to the original. Finally, as orchestrators, they can run the generated code, capture compiler feedback, enable repair, and more.\\n\\nTo tackle this, R\u00b2E synthesizes tests for arbitrary GitHub code using a novel synergy of program analysis with LLM prompting. Below, we summarize some of the key design choices of R\u00b2E's test generation approach.\\n\\nEquivalence Tests, not Output Prediction. R\u00b2E decouples test outputs from inputs. Instead, it uses the original function as a reference implementation to generate expected outputs. This key insight dramatically simplifies test generation since it removes the need to predict outputs. Consequently, we generate equivalence tests\u2014they check if the outputs of the original function and the generated function are equivalent against a given set of inputs.\\n\\nHarnesses, not I/O pairs. R\u00b2E generates equivalence test harnesses (Section 2) for each function, which contain the test cases and the required setup, such as database connections, external files, configurations, etc., that makes it possible to run functions in the wild. This is a departure from I/O examples with primitive types in traditional benchmarks such as HUMAN EVAL (Chen et al., 2021). It is necessary because real-world code often requires more than simple input arguments. They may need several dependencies, such as access to files, environment variables, APIs, and user-defined functions or classes.\\n\\nSliced Context, not Entire Repositories. Test generation using LLM has been effective in prior work like HUMAN-EVAL+ (Liu et al., 2023b) for simple isolated functions. However, in a repository setting, prompting with the function alone is insufficient, and providing the entire repository is expensive. R\u00b2E uses a novel dependency slicing based prompt to extract the minimal repository context required to understand the functionality of the function under test. As described in Section 2, it finds functions and global variables on which the function directly or indirectly depends.\\n\\nExecution and Coverage for Quality Control. Finally, recent studies have shown that execution-based benchmarks can be flawed due to low-quality tests (Liu et al., 2023b). To avoid this, we execute the generated test harnesses in the docker container built for the repository. Equivalence in-file out-file strategy.\\n\\n| Strategy       | Validity | Coverage |\\n|----------------|----------|----------|\\n| Output Pred.   | 44.25%   | 76.59%   |\\n| Equivalence    | 51.73%   | 77.23%   |\\n| Sliced         | 52.37%   | 79.65%   |\\n\\nTable 1. Test generation evaluation results across 2 strategies \u2013 Output Prediction and Equivalence. Further, we vary prompt context creation across \u2013 Na\u00efve, Full, and Sliced. The results are compared on 2 settings: In-File where the function under test only depends on the context within its file, and Out-File where it depends on external files in the repository. The metrics used are Validity (Val) and Coverage (Cov), for which higher is better. Our equivalence testing approach results in considerably more valid test harnesses than a direct output-prediction approach. Next, our dependency slicing-based prompt context offers a good balance of broader but focused context and fares better than na\u00efve or full context settings.\\n\\nIn \"self-equivalence\" mode, tests are run in \"self-equivalence\" mode, so the function under test and the reference implementation are the same. Inoperative harnesses due to issues like missing packages are excluded. An (equivalence) test harness is deemed valid if all the (equivalence) tests pass. We further emphasize the quality of test cases by using branch coverage (Section 2). This check is critical to ensure that the generated tests cover the function's complete behavior and can be used to check functional equivalence.\\n\\nWe encode our design decisions as guidelines to prompt GPT-4-TURBO and use the sliced context to generate high-quality test harnesses. Figure 2 shows the resulting harnesses that handle complex data types and unique setups, depending on the function's requirements. We outline additional guidelines for test generation in the appendix.\"}"}
{"id": "kXHgEYFyf3", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in the repository context: Na\u00efve, Full, and Sliced. The Na\u00efve strategy prompt contains the function and no context. The Full strategy provides the file containing the function and all files it imports (until a 6000 token limit). Finally, the Sliced strategy implements our proposed dependency slicing to provide the minimal context required for the function. We compare these strategies in 2 problem settings: (1) In-File: where the function under test depends only on the context within its file and (2) Out-File: where it depends on external files in the repository. We generate all tests using the state-of-the-art GPT-4-TURBO model. We elaborate further details such as prompts in Appendix B.\\n\\n3.3.2 Validity and Quality Results\\n\\nTable 1 shows the results of our evaluation. Equivalence tests outperform output prediction. By design (Section 3.2), R\u00b2E generates equivalence tests that decouple test outputs from inputs by using the original function as a reference implementation to generate expected outputs. This significantly boosts the number of valid tests generated (by \u224820%) when compared to generating tests with expected outputs predicted by the LLM. Focussed context improves coverage. The Na\u00efve strategy performs relatively poorly on validity (as low as 19%), but the valid test harnesses it generates have high coverage (\u224888%). For example, na\u00efvely generated tests often fail to generate correct input argument types (e.g., schemas or custom classes) due to the lack of necessary context. Broader context improves validity. On the other hand, the Full strategy generates more valid tests (as high as 51.7%) but has lower coverage (77.2%). This indicates that a focused context can be more effective in covering corner cases in the function, but a broader context is necessary to understand the function's dependencies. Our sliced strategy strikes a good balance between the two and achieves the best results in validity and coverage. Overall, we observe that R\u00b2E's dependency slicing-based strategy generates \u224844% valid test harnesses with a high \u224883% code coverage.\\n\\n3.3.3 Failure Modes\\n\\nWe collected and classified invalid equivalence test harnesses and study their failure modes. We discovered that 40% of errors were due to ValueErrors and TypeErrors, reflecting improper key, attribute, or type usage in tests. Additionally, 15% were DataFormatErrors, caused by incorrect data formats or schemas, highlighting the complexity of testing GITHUB code beyond primitive types. Assertions (expected and actual outputs don't match) accounted for a notable 25% of errors, showing a nuanced\\n\\n| Dataset | Exec? | Repo? | LOC | #Tests |\\n|---------|-------|-------|-----|--------|\\n| HUMAN   | \u2713     | \u2717     | 6.26| 6.60   |\\n| ODEX    | \u2713     | \u2717     | 3.05| 1.90   |\\n| CROSSCODE | \u2717  | \u2713     | 1.00| -      |\\n| REPOBENCH | \u2717   | \u2713     | 1.00| -      |\\n| R\u00b2E-EVAL | \u2713    | \u2713     | 10.84| 4.40   |\\n\\nTable 2. Comparing R\u00b2E-Eval1 with other NL-to-code generation benchmarks, in terms of test execution-based support (Exec?), use of repository context (Repo?), and the number of lines in the ground truth function (LOC). R\u00b2E-Eval1 is the only executable benchmark, has repository context, and is automated, enabling scalability. Additionally, our benchmark contains more tests (harnesses) per function with diverse input types and quality assurance.\\n\\n3.4 Refinement of Specifications\\n\\nNatural language docstrings in GITHUB repositories might be under-specified to be used for code generation. Here, we use an automated approach to refine the natural language docstring of a given function by asking the model to refine the docstring in a self-instruct-like fashion (Wang et al., 2022a). Distinctly, however, we provide the model with additional context in the form of the original docstring, synthesized test harness, argument types, and serialized input-output arguments constructed from running the test harness. Anecdotally and from related works, providing execution outputs allows models to better comprehend the function semantics and leads to better quality docstrings. Appendix C provides the prompt used.\\n\\nWe further study the quality of refined docstrings on a subset of functions in Appendix Section C.1. Finally, we perform rigorous manual evaluations and filter problems with poor or ambiguous specifications (detailed further).\\n\\n4. The R\u00b2E-Eval1 Benchmark\\n\\nIn Section 3, we showed that R\u00b2E enables a scalable framework for building execution-based test environments for programming agents. R\u00b2E takes a function from a codebase and converts it into a tuple $I = \\\\{D, R, T\\\\}$, where $D$ is a refined docstring for a function, $R$ is the remainder of the codebase, and $T$ is the generated test harness. Zhang et al. (2023d) did not release associated tests.\"}"}
{"id": "kXHgEYFyf3", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We instantiate this framework to construct $R_2E$-Eval1, the first large-scale dataset of real-world code generation problems with functional correctness tests. Table 2 compares $R_2E$-Eval1 against several popular benchmarks used to evaluate code generation capabilities of LLM. Prior work like HUMAN-EVAL (Chen et al., 2021) and ODEX (Wang et al., 2022b) support execution-based metrics but for isolated simple problems with no real-world repository setting. Recent work on repository-level code generation like CROSCODE-EVAL (Ding et al., 2023), REPOBENCH (Liu et al., 2023c), and REPOEVAL (Zhang et al., 2023d) use repository context, but either forego execution-based evaluation or depend heavily on human-written tests, which are seldom available at scale on GitHub. $R_2E$-Eval1 is the only executable benchmark that has repository context and is automated, enabling scalability. Following, we describe the construction of $R_2E$-Eval1 and analysis.\\n\\n### 4.1. Benchmark Construction\\n\\n#### 4.1.1. Dataset Quality\\n\\nWe emphasize heavily on the quality of problems in this work. Quality, here, means how well the function, docstring, and test cases are written. To ensure this, we only consider functions with high branch coverage. Our final benchmark problems have an average of 11.5 test cases with 92.2% average branch coverage. An additional round of manual inspection helps us select high-quality problems. We look for the following characteristics during manual evaluations:\\n\\n- (a) docstring is clear and well-defined\\n- (b) docstring matches the intent of the original function and the generated tests identifying key functionalities\\n- (c) failures and successes in model generations actually correspond to the problem intent (akin to the process used in Cassano et al. (2023))\\n\\nThis process prunes out complex functions such as ones having arbitrary or peculiar corner cases which cannot be specified well with simple docstrings.\\n\\n#### 4.1.2. Dataset Composition\\n\\nWe also consider the diversity and interestingness of the problems in the benchmark. We identify several properties of code that calibrate interestingness, such as # of dependencies, argument types, lines, libraries used, etc. Table 3 showcases statistics of our benchmark. Our manual analysis also shows that $R_2E$-Eval1 problems are diverse in terms of the domains they cover: pythonic operations (list, str, dict manipulations), data manipulation (JSON, files, pandas, numpy), algorithm and protocol implementations (networkx, statistics), domain-specific problems (programming languages, networks, quantum computing, formal verification, numerical computing), and more. We also ensure that the benchmark is diverse in terms of the number of distinct repositories, preventing bias towards a codebase or domain. Overall this process leads to a curated set of 246 problems from 137 repositories in $R_2E$-Eval1.\\n\\nEach problem instance $I$ can be used to evaluate a code generation system by providing docstring $D$ to the system and evaluating its response (in the context of the repository $R$) against the generated test harness $T$.\\n\\n### 5. $R_2E$: Towards Programming Agents\\n\\nWe conduct experiments to understand three important problems about LLM performance on real-world coding.\\n\\n- **Q1** How well can current LLMs solve the real-world code generation tasks statically? (Sec. 5.1)\\n- **Q2** What are the typical LLM failure modes? (Sec. 5.2)\\n- **Q3** How do programming agent paradigms (like self-repair) perform against static programming? (Sec. 5.3)\\n\\nOur results show that the SOTAILLM model (GPT-4) can only achieve $\\\\sim 50\\\\%$ performance in $R_2E$-Eval1, despite high accuracy on HUMAN-EVAL. Throughout the analysis, we find that LLMs struggle at understanding interfaces to existing functions and reasoning about complex objects. Finally, we compare static coding approaches (e.g., COT) with the proposed interactive programming paradigm, demonstrating significant benefits from the latter.\\n\\n#### 5.1. Static Code Generation\\n\\nFirst, we study direct code generation on the $R_2E$-Eval1 dataset, i.e., using code generation without interaction. Owing to the test harnesses generation approach, we perform functional correctness evaluations for the generated code. This contrasts with prior works (Liu et al., 2023c; Ding et al., 2023) that rely on execution-free exact-match metrics to evaluate code completion in the repository setting, which can be unreliable and restrict the scope of the evaluation. We use $P@1$ to evaluate the functional correctness, computed by generating 5 candidate completions for each problem instance and computing the fraction that passes.\"}"}
{"id": "kXHgEYFyf3", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Functional correctness ($\\\\text{P@1}$) of various models (GPT and CODILLA families) on our R2E-Eval1. First, we note that, overall, models perform worse on our benchmark against HUMAN, highlighting the challenging nature of real-world code generation tasks. GPT-4 performs particularly well, achieving a $\\\\text{P@1}$ close to 50%, much better than other models.\\n\\nNext, we study two retrieval settings\u2014dependency context and full context and find a trade-off between the two (discussed in Section 5.1).\\n\\nGiven a problem instance $I = \\\\{D, R, T\\\\}$ in our benchmark, we need to use the remaining repository context to generate the code. Since the entire repository context can be very large, we retrieve content to provide the model (detailed ahead). We first evaluate how current models hold up on our benchmark and then study how the choice of retrieval impacts performance. Next, we study the effect of using chain-of-thought prompting (COT) (Wei et al., 2022) for improving model performance on harder tasks.\\n\\nModel Performance. Figure 3 compares the performance of various models on our benchmark using the $\\\\text{P@1}$ ($\\\\text{CL}$ used for brevity in the figure instead of CODILLA). We find that the performance of various models is relatively lower than other benchmarks like HUMAN. This is expected since our benchmark represents more challenging real-world problems collected from GITHUB, which require understanding existing context from the repository before generating the code. We find that GPT-4 performs significantly better than all other models with a $\\\\text{P@1}$ close to 50% whereas other models only achieve $\\\\text{P@1}$ in the vicinity of 30%.\\n\\nEffect of retrieval. We study the effect of function-definition retrieval vs. function-usage retrieval using dependency slicing (Section 2) on the ground-truth function. Specifically, dependency-only-context only provides the necessary definitions, while the full context setting adds the remainder of the file and other files until a 6000 token limit. Figure 3 compares the two settings. The two retrieval methods perform similarly, achieving \u00b11% of each other's performance across most models. On a closer look, we find non-overlapping problems with a Pearson correlation coefficient of 0.48. We find that dependency-only-context vs full-context provides an interesting trade-off. On the one hand, dependencies provide a more focused view of relevant function implementations to the model. At the same time, function usage (present in full context) is often reused and enables models to copy it directly. See Appendix F.1 in the Appendix for a more detailed discussion and examples of this trade-off. Finally, we believe that R2E-Eval1 provides a unique opportunity to study this problem in the future with execution enabled.\\n\\nEffect of COT. We study better-prompting strategies and look at both zero-shot and two-shot COT prompts that sketch a plan for the function implementation before generating the code. We study this for the instruct GPT-3.5-TURBO and GPT-4 models but find that COT-like setup does not improve performance over direct prompt (Table 7 in Appendix).\"}"}
{"id": "kXHgEYFyf3", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. We measure whether self-repair using test harnesses and interpreter feedback can help the models correct mistakes and improve performance. We collect problems on which GPT-4 and GPT-3.5-TURBO fail and ask the models to iteratively correct by providing some error messages. We find that models improve performance from black-box feedback (33% and 21% respectively for GPT-4 and GPT-3.5-TURBO after 5 iterations).\\n\\nWe find that both these measures are (inversely) correlated with the \\\\( P_{\\\\text{ASS}} \\\\) of the models. In Figures 4 and 12, we plot the \\\\( P_{\\\\text{ASS}} \\\\) of the models against the number of dependencies and the number of tokens used in the ground-truth implementation showing a downward trend.\\n\\nSingle File vs Multi-File Context. We compare how models perform on problems that require only a single file to be generated against problems that require multiple files to be generated. Model performance is significantly better on single-file problems than multi-file problems (Figure 13). This suggests that a.) models struggle with multi-file contexts compared to single-file contexts and b.) problems in the multi-file category are more complex than single-file problems in our benchmark, also observed in practice.\\n\\nDo not understand the interface to provided functions. We find that when provided with complex functions in the context, LLMs do not understand the right input-output behavior of such functions and pass in wrong inputs or expect wrong outputs. Thus, even strong LLMs like GPT-4 make mistakes when provided with complex functions in the context. See Listings 8 for reference. This motivates that if provided access to execution context, programming agents can understand such interfaces and perform better.\\n\\nRepeat vs Reuse Code. Abstractions are an integral part of writing good code. LLMs, however, tend to duplicate code instead of using existing context. Specifically, when provided with some existing function in the context, models re-implement the same functionality instead of directly using it. Listings 2 and 3 provide examples. This aligns with findings on how copilot affects code quality (Blog.).\\n\\n5.3. Self-Repair Agent\\n\\nSo far, we described model evaluations on our benchmark using the direct code generation approach. However, testing harnesses and access to the interpreter allow us to evaluate programming agents that can interact with the interpreter and get feedback. Specifically, we instantiate a self-repair agent that uses the test harness.\\n\\nWe study that when provided with feedback from (oracle) testing harnesses (present in our benchmark instances), can LLMs correct their own mistakes? We sample 56 and 48 instances from our benchmark for GPT-4 and GPT-3.5-TURBO on which the models do not generate a correct solution (detailed experiment setup in Section E.2 in the Appendix). We consider the incorrect programs generated by the models as the initial programs and then provide the models with error feedback using the harness iteratively for 5 iterations.\\n\\nFigure 5 shows the self-repair rate of the models on our benchmark as a function of the number of iterations. First note that since we subsample only the failing instances where models do not generate correct solutions, the 0-th iteration score is 0% for both models. Next, we find that GPT-4 attains a maximum self-repair rate of 33% while GPT-3.5-TURBO only attains a maximum self-repair rate of 20%. This highlights that using execution, interpreter, and test cases, programming agents can improve code generation. Note that while advanced prompting techniques do not improve performance (Table 7), using an interpreter enables programming agents to achieve strong results.\\n\\n6. Related Work\\n\\nCode Generation Benchmarks. Code generation is primarily evaluated using functional-correctness and has been explored in multiple domains. HUMAN EVAL (Chen et al., 2021) and MBPP (Austin et al., 2021) study code generation on isolated single-function problems. APPS (Hendrycks et al., 2021) and CODE-CONTESTS (Li et al., 2022) benchmarks are primarily used for evaluating algorithmic code generation capabilities. DS-1000 (Lai et al., 2023), ARCADE (Yin et al., 2022), NUMPY EVAL (Zhang et al., 2023b), and PANDAS-EVAL (Jain et al., 2022) study data science API code generation. More recently, Wang et al. (2022b) proposed ODEX that evaluates coding on APIs with human-written input-output examples. These works evaluate code generation capabilities in isolated settings devoid of surrounding context or dependencies from other files. In contrast, \\\\( R^2 \\\\)E coding problems are curated directly from GITHUB thus more similar to real-world setups.\\n\\nINTERCODE and WEBRENA provide general environments for domain-specific interactive programming and web tasks respectively. We provide a\"}"}
{"id": "kXHgEYFyf3", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"framework and environments for interactive general-purpose programming tasks extract from GITHUB.\\n\\nFor the repository setting, prior works have primarily focused on execution-free evaluation metrics like exact-match and BLEU due to absence of test harnesses. CONALA (Yin et al., 2018) curated a large dataset from STACKOVERFLOW with paired natural language and program snippets. Shrivastava et al. (2023b;a) study different context selection methods for prompting and training LLMs for repository-level code generation. REPOEVAL (Zhang et al., 2023a), REPOBENCH (Liu et al., 2023c), and CROSSCODEEVAL (Ding et al., 2023) study repository-level code completion. However, these works only evaluate short context code generation capabilities without execution or functional correctness restriction to short completions. In contrast, we synthesize function-level test harnesses using our novel test generation approach and use them for performing function correctness checks on repository code. Recently, Jimenez et al. (2023) proposed SWEBENCH to evaluate whether LLMs can solve GITHUB issues. However, they assume test cases availability from pull requests preventing scalable collection of problems. Our test harness synthesis in contrast allows collecting problems from diverse set of repositories (137 repositories vs 12 repositories). Finally, Du et al. (2023) proposed, CLASEVAL, manually curated for evaluating LLMs.\\n\\nOther code-related tasks. Beyond codegen, tasks like self-repair (Chen et al., 2023; Olausson et al., 2023; Madaan et al., 2023b; Peng et al., 2023; Zhang et al., 2023c), test generation (Tufano et al., 2022; Watson et al., 2020), execution (Austin et al., 2021; Liu et al., 2023a; Gu et al., 2024), and optimization (Madaan et al., 2023a) have been studied. These enable various agentic setups as CODET (Chen et al., 2022), Key et al. (2022), PARSEL (Zelikman et al., 2023), FUNSEARCH (Romera-Paredes et al., 2023), REFLExION (Shinn et al., 2023), LEVER (Ni et al., 2023), CODEPLAN (Bairi et al., 2023), ALPHA CODIUM (Ridnik et al., 2024), REACT (Yao et al., 2022), and TOOT (Yao et al., 2023).\\n\\n7. Discussion\\n\\nLimitations. Natural language is inherently ambiguous and docstrings might not specify the corner cases properly. We tried to mitigate this effect with our specification refinement approach along with manual filtering. Future work study this ambiguity in more and also look into better interaction mechanisms. Next, we use observational equivalence to check whether the model-generated candidates are correct over a set of inputs. We use branch coverage as a metric for evaluating tests but it is still a softer check. Future work can apply mutation testing and oversampling to provide further confidence on generated tests.\\n\\nConclusion. We propose R2E, a scalable framework to convert GITHUB repositories to programming agent test environments. R2E-Eval1 constructed via this framework can evaluate both static and interactive code generation systems, offering valuable insights into model behaviors and the need for better programming workflows. Prior work has applied rejection sampling and reinforcement learning to improve coding capabilities of LLMs (Singh et al., 2023; Jain et al., 2023; Le et al., 2022). We believe R2E can enable such attempts for real-world programs. Similarly, R2E enables collecting LLM-based interaction traces for tasks like code generation, debugging, optimization which can be further useful for building LLM agents.\\n\\nAcknowledgement\\n\\nThis work was supported in part by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research through the X-STACK: Programming Environments for Scientific Computing program (DE-SC0021982), by the National Science Foundation through grant CCF-1900968, and by SKY Lab industrial sponsors and affiliates Astronomer, Google, IBM, Intel, Lacework, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Nexla, Samsung SDS, Uber, and VMware. Finally, we thank Alex Gu, Wen-Ding Li, Pengcheng Yin, Miltos Allamanis, Michael Pradel, Zijian Wang, and Sida Wang for helpful comments and feedback on the paper.\\n\\nImpact Statement\\n\\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.\\n\\nReferences\\n\\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n\\nBairi, R., Sonwane, A., Kanade, A., D C, V ., Iyer, A., Parthasarathy, S., Rajamani, S., Ashok, B., and Shet, S. Codeplan: Repository-level coding using llms and planning. In Neural Information Processing Systems Workshop on Foundation Models for Decision Making (FMDM-NeurIPS), November 2023.\\n\\nBarei\u00df, P., Souza, B., d'Amorim, M., and Pradel, M. Code generation tools (almost) for free? a study of few-shot, pre-trained language models on code. arXiv preprint arXiv:2206.01335, 2022.\"}"}
{"id": "kXHgEYFyf3", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Can it edit? evaluating the ability of large language models to follow code editing instructions. arXiv preprint arXiv:2312.12450, 2023.\\n\\nChen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.-G., and Chen, W. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n\\nChen, X., Lin, M., Sch\u00e4rli, N., and Zhou, D. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.\\n\\nDing, Y., Wang, Z., Ahmad, W. U., Ding, H., Tan, M., Jain, N., Ramanathan, M. K., Nallapati, R., Bhatia, P., Roth, D., et al. Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. arXiv preprint arXiv:2310.11248, 2023.\\n\\nDu, X., Liu, M., Wang, K., Wang, H., Liu, J., Chen, Y., Feng, J., Sha, C., Peng, X., and Lou, Y. Classeeval: A manually-crafted benchmark for evaluating llms on class-level code generation, 2023.\\n\\nFraser, G. and Arcuri, A. Evosuite: automatic test suite generation for object-oriented software. In Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering, pp. 416\u2013419, 2011.\\n\\nFraser, G. and Arcuri, A. Whole test suite generation. IEEE Transactions on Software Engineering, 39(2):276\u2013291, 2012.\\n\\nGu, A., Rozi\u00e8re, B., Leather, H., Solar-Lezama, A., Synnaeve, G., and Wang, S. I. Cruxeval: A benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024.\\n\\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.\\n\\nIvankovi\u0107, M., Petrovi\u0107, G., Just, R., and Fraser, G. Code coverage at google. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 955\u2013963, 2019.\\n\\nJain, N., Vaidyanath, S., Iyer, A., Natarajan, N., Parthasarathy, S., Rajamani, S., and Sharma, R. Jigsaw: Large language models meet program synthesis. In Proceedings of the 44th International Conference on Software Engineering, pp. 1219\u20131231, 2022.\\n\\nJain, N., Zhang, T., Chiang, W.-L., Gonzalez, J. E., Sen, K., and Stoica, I. Llm-assisted code cleaning for training accurate code generators. arXiv preprint arXiv:2311.14904, 2023.\\n\\nJimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.\\n\\nKey, D., Li, W.-D., and Ellis, K. I speak, you verify: Toward trustworthy neural program synthesis. arXiv preprint arXiv:2210.00848, 2022.\\n\\nLahiri, S. K., Naik, A., Sakkas, G., Choudhury, P., von Veh, C., Musuvathi, M., Inala, J. P., Wang, C., and Gao, J. Interactive code generation via test-driven user-intent formalization. arXiv preprint arXiv:2208.05950, 2022.\\n\\nLai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, W.-t., Fried, D., Wang, S., and Yu, T. Ds-1000: A natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319\u201318345. PMLR, 2023.\\n\\nLe, H., Wang, Y., Gotmare, A. D., Savarese, S., and Hoi, S. C. H. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314\u201321328, 2022.\\n\\nLemieux, C., Inala, J. P., Lahiri, S. K., and Sen, S. Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models. In International conference on software engineering (ICSE), 2023.\\n\\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097, 2022.\\n\\nLiu, C., Lu, S., Chen, W., Jiang, D., Svyatkovskiy, A., Fu, S., Sundaresan, N., and Duan, N. Code execution with pre-trained language models. arXiv preprint arXiv:2305.05383, 2023a.\\n\\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. arXiv preprint arXiv:2305.01210, 2023b.\"}"}
{"id": "kXHgEYFyf3", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Liu, T., Xu, C., and McAuley, J. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091, 2023c.\\n\\nMadaan, A., Shypula, A., Alon, U., Hashemi, M., Ranganathan, P., Yang, Y., Neubig, G., and Yazdanbakhsh, A. Learning performance-improving code edits. arXiv preprint arXiv:2302.07867, 2023a.\\n\\nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023b.\\n\\nNi, A., Iyer, S., Radev, D., Stoyanov, V., Yih, W.-t., Wang, S., and Lin, X. V. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 26106\u201326128. PMLR, 2023.\\n\\nOlausson, T. X., Inala, J. P., Wang, C., Gao, J., and Solar-Lezama, A. Demystifying gpt self-repair for code generation. arXiv preprint arXiv:2306.09896, 2023.\\n\\nPacheco, C. and Ernst, M. D. Randoop: feedback-directed random testing for java. In Companion to the 22nd ACM SIGPLAN conference on Object-oriented programming systems and applications companion, pp. 815\u2013816, 2007.\\n\\nPanichella, A., Kifetew, F. M., and Tonella, P. Automated test case generation as a many-objective optimisation problem with dynamic selection of the targets. IEEE Transactions on Software Engineering, 44(2):122\u2013158, 2017.\\n\\nPeng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L., Yu, Z., Chen, W., and Gao, J. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.\\n\\nRidnik, T., Kredo, D., and Friedman, I. Code generation with alphacodium: From prompt engineering to flow engineering. arXiv preprint arXiv:2401.08500, 2024.\\n\\nRomera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M. P., Dupont, E., Ruiz, F. J., Ellenberg, J. S., Wang, P., Fawzi, O., et al. Mathematical discoveries from program search with large language models. Nature, pp. 1\u20133, 2023.\\n\\nRyder, B. G. Constructing the call graph of a program. IEEE Transactions on Software Engineering, (3):216\u2013226, 1979.\\n\\nSalis, V., Sotiropoulos, T., Louridas, P., Spinellis, D., and Mitropoulos, D. Pycg: Practical call graph generation in python. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp. 1646\u20131657. IEEE, 2021.\\n\\nShinn, N., Labash, B., and Gopinath, A. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\\n\\nShrivastava, D., Kocetkov, D., de Vries, H., Bahdanau, D., and Scholak, T. Repofusion: Training code models to understand your repository. arXiv preprint arXiv:2306.10998, 2023a.\\n\\nShrivastava, D., Larochelle, H., and Tarlow, D. Repository-level prompt generation for large language models of code. In International Conference on Machine Learning, pp. 31693\u201331715. PMLR, 2023b.\\n\\nSingh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil, P., Liu, P. J., Harrison, J., Lee, J., Xu, K., Parisi, A., et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023.\\n\\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and Sundaresan, N. Unit test case generation with transformers and focal context. arXiv preprint arXiv:2009.05617, 2020.\\n\\nTufano, M., Deng, S. K., Sundaresan, N., and Svyatkovskiy, A. Methods2test: A dataset of focal methods mapped to test cases. In Proceedings of the 19th International Conference on Mining Software Repositories, pp. 299\u2013303, 2022.\\n\\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022a.\\n\\nWang, Z., Zhou, S., Fried, D., and Neubig, G. Execution-based evaluation for open-domain code generation. arXiv preprint arXiv:2212.10481, 2022b.\\n\\nWatson, C., Tufano, M., Moran, K., Bavota, G., and Poshyvyvanyk, D. On learning meaningful assert statements for unit test cases. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pp. 1398\u20131409, 2020.\\n\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.\\n\\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting.\"}"}
{"id": "kXHgEYFyf3", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.\\n\\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig, G. Learning to mine aligned code and natural language pairs from stack overflow. In International Conference on Mining Software Repositories, MSR, pp. 476\u2013486. ACM, 2018. doi: https://doi.org/10.1145/3196398.3196408.\\n\\nYin, P., Li, W.-D., Xiao, K., Rao, A., Wen, Y., Shi, K., Howland, J., Bailey, P., Catasta, M., Michalewski, H., et al. Natural language to code generation in interactive data science notebooks. arXiv preprint arXiv:2212.09248, 2022.\\n\\nZelikman, E., Huang, Q., Poesia, G., Goodman, N. D., and Haber, N. Parsel: A (de-)compositional framework for algorithmic reasoning with language models. arXiv preprint arXiv:2212.10561, 2023.\\n\\nZhang, F., Chen, B., Zhang, Y., Liu, J., Zan, D., Mao, Y., Lou, J.-G., and Chen, W. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023a.\\n\\nZhang, K., Li, G., Li, J., Li, Z., and Jin, Z. Toolcoder: Teach code generation models to use APIs with search tools. arXiv preprint arXiv:2305.04032, 2023b.\\n\\nZhang, K., Li, Z., Li, J., Li, G., and Jin, Z. Self-edit: Fault-aware code editor for code generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 769\u2013787, Toronto, Canada, July 2023c. Association for Computational Linguistics.\\n\\nZhang, T., Yu, T., Hashimoto, T., Lewis, M., Yih, W.-t., Fried, D., and Wang, S. Coder reviewer reranking for code generation. In International Conference on Machine Learning, pp. 41832\u201341846. PMLR, 2023d.\"}"}
{"id": "kXHgEYFyf3", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Program Analysis\\n\\nA.1. Callgraphs\\n\\nA callgraph (Ryder, 1979) is a directed graph where nodes represent subroutines (functions, methods, constructors, etc.), and edges denote the calling relationships between them. It is a directed graph $G = (V, E)$ where $V$ is a set of subroutines and $E$ is a set of edges representing the calling relationship between these subroutines. If a subroutine $u$ invokes (i.e., calls) a subroutine $v$, then there is a directed edge from $u$ to $v$ in the callgraph. For instance, in Figure 2, the function indexer calls the function \\\\_parse_sigmf_capture. This repository abstraction enables analyzing several properties of repository code. For instance, we use it to extract the dependencies that a function relies on for its execution\u2014a valuable property for test generation. In this work, we use the PYZCG tool (Salis et al., 2021) to generate callgraphs for PYTHON repositories.\\n\\nA.2. Dependency Slicing\\n\\nWhile callgraphs abstract direct interactions between functions, a PYTHON function can interact with parts of the repository through global variables, too\u2014in the same file and imported from other files. We can summarize these interactions in a dependency slice $D_f$ for a function $f$, as the set of all functions $F'$ that $f$ calls, and all global variables $G'$ that $f$ accesses, both directly and indirectly.\\n\\nFor a function $f$, we define a mapping called depends which identifies all functions $F'$ that $f$ calls, and all global variables $G'$ that $f$ accesses. Then a dependency slice $D_f$ is the transitive closure of all functions and global variables that $f$ depends on, directly or indirectly.\\n\\n$$D_f = \\\\left\\\\{ (F', G') \\\\in \\\\text{depends}^* (f) \\\\mid F' \\\\cup G' \\\\right\\\\}$$\\n\\nComputing this slice is generally an undecidable problem, but we make a few simplifying assumptions to make it tractable. We begin by utilizing the callgraph to identify the functions that $f$ calls. We then use bytecode analysis to identify the set of global variables that $f$ accesses. We add these functions and global variables to the slice and recursively repeat the process for each function in the slice. We utilize this context for test generation in Section 3.2.\"}"}
