{"id": "bYRYb7DMNo", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Abstract**\\n\\nDeep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world data-scarce scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progress has been achieved with the emergence of large language models, exhibiting unprecedented abilities such as few-shot generalization, scalability, and task generality, which are however absent in small deep models. To change the status quo of training scenario-specific small models from scratch, this paper aims at the early development of large time series models (LTSM).\\n\\nDuring pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet diverse application needs, we convert forecasting, imputation, and anomaly detection of time series into a unified generative task. The outcome of this study is a Time Series Transformer (Timer), which is generative pre-trained by next token prediction and adapted to various downstream tasks with promising capabilities as an LTSM. Code and datasets are available at: https://github.com/thuml/Large-Time-Series-Model.\\n\\n1. Introduction\\n\\nTime series analysis encompasses a broad range of critical tasks, including time series forecasting (Box et al., 2015), forecasting, imputation (Friedman, 1962), anomaly detection (Breunig et al., 2000), etc. Despite the ubiquity of real-world time series, training samples can be scarce in specific applications. While remarkable advances have been made in deep time series models (Wu et al., 2022; Zeng et al., 2023; Liu et al., 2023b), the accuracy of state-of-the-art deep models (Nie et al., 2022) can still deteriorate drastically in such scenarios, even within prevalent benchmarks as shown in Figure 1. Concurrently, we are witnessing rapid progress of large language models (Radford et al., 2018), involving training on large-scale text corpora and exhibiting remarkable few-shot and zero-shot capabilities (Radford et al., 2019). It can be indicative for the community to develop large time series models (LTSM) that are transferable on various data-scarce scenarios by pre-training on numerous time series data. Further, large models evolved by generative pre-training (GPT) have demonstrated several advanced capabilities that are absent in small models: the generalization ability that one model fits many domains, the versatility that one model copes with various scenarios and tasks, and the scalability that performance improves with the scale of parameters and pre-training corpora. Fascinating capabilities have fostered the advancement of artificial general intelligence (OpenAI, 2023). Time series holds comparable practical value to natural language. Essentially, they exhibit inherent similarities in generative modeling (Bengio et al., 2000) and autoregression (Box, 2013). Consequently, the unprecedented success of the generative pre-trained large language models (Zhao et al., 2023) serves as a blueprint for the progress of LTSMs.\"}"}
{"id": "bYRYb7DMNo", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Although unsupervised pre-training on time series data has been widely explored, yielding breakthroughs based on the masked modeling (Zerveas et al., 2021) and contrastive learning (Woo et al., 2022), there are still unsolved fundamental issues for developing LTSMs. Firstly, the dataset infrastructure and unified treatment for heterogeneous time series are lagging behind other fields. As a result, prior unsupervised pre-training methods are typically constrained to a small scale and primarily focus on in-dataset transfer (Zhang et al., 2022; Nie et al., 2022). Secondly, the architecture of scalable large models remains underexplored in the field of time series. It is observed that non-autoregressive structures, which are prevalent and effective in small time series models, may not be suitable for LTSMs. Thirdly, existing large-scale pre-trained models (Woo et al., 2023; Das et al., 2023b) primarily concentrated on a single task (e.g., forecasting), and have scarcely addressed task unification. Consequently, the applicability of LTSMs remains elevatable.\\n\\nIn this paper, we dive into the pre-training and adaptation of large time series models. By aggregating publicly available time series datasets and following curated data processing, we construct the **Unified Time Series Dataset (UTSD)** of hierarchical capacities to facilitate the research on the scalability of LTSMs. To pre-train large models on heterogeneous time series data, we propose the **single-series sequence (S3)** format that converts multivariate series with reserved patterns into unified token sequences. For better generalization and versatility, we adopt the GPT-style objective that predicts the next token (Bengio et al., 2000). Eventually, we present **Timer**, a large-scale pre-trained Time Series Transformer.\\n\\nUnlike prevalent encoder-only architecture (Nie et al., 2022; Wu et al., 2022; Das et al., 2023a), Timer exhibits similar characteristics as large language models such as flexible context length and autoregressive generation. It also presents notable few-shot generalization, scalability, and task generality, outperforming state-of-the-art task-specific models on forecasting, imputation, and anomaly detection. Overall, our contributions can be summarized as follows:\\n\\n- We delve into the LTSM development by curating large-scale datasets comprised of 1B time points, proposing a unified sequence format to cope with data heterogeneity, and presenting Timer, a generative pre-trained Transformer for general time series analysis.\\n- We apply Timer on various tasks, which is realized in our unified generative approach. Timer exhibits notable feasibility and generalization in each task, achieving state-of-the-art performance with few samples.\\n- By pre-training on increasing available time series data, Timer exhibits zero-shot forecasting capability. Quantitative evaluations and quality assessments are provided among concurrent large time series models.\\n\\n### 2. Related Work\\n\\n#### 2.1. Unsupervised Pre-training on Sequences\\n\\nUnsupervised pre-training on large-scale data is the essential step for modality understanding for downstream applications, which has achieved substantial success in sequences, covering natural language (Radford et al., 2021), patch-level image (Bao et al., 2021) and video (Yan et al., 2021). Supported by powerful backbones (Vaswani et al., 2017) for sequential modeling, the paradigms of unsupervised pre-training on sequences have been extensively studied in recent years, which can be categorized into the masked modeling (Devlin et al., 2018), contrastive learning (Chen et al., 2020), and generative pre-training (Radford et al., 2018).\\n\\nInspired by significant progress achieved in relevant fields, masked modeling and contrastive learning have been well-developed for time series. TST (Zerveas et al., 2021) and PatchTST (Nie et al., 2022) adopt the BERT-style masked pre-training to reconstruct several time points and patches respectively. LaST (Wang et al., 2022b) proposes to learn the representations of decomposed time series based on variational inference. Contrastive learning is also well incorporated in prior works (Woo et al., 2022; Yue et al., 2022). TF-C (Zhang et al., 2022) constrains the time-frequency consistency by temporal variations and frequency spectrums. SimMTM (Dong et al., 2023) combines masked modeling and contrastive approach within the neighbors of time series. However, generative pre-training has received relatively less attention in the field of time series despite its prevalence witnessed in developing large language models (Touvron et al., 2023; OpenAI, 2023). Most large language models are generative pre-trained (Zhao et al., 2023) with token-level supervision, where each token is generated based on the previous context and independently supervised (Bengio et al., 2000). Consequently, they are not constrained by specific input and output lengths and excel at multi-step generation. Furthermore, prior studies (Wang et al., 2022a; Dai et al., 2022) have demonstrated that scalability and generalization largely stem from generative pre-training, which requires more training data than other pre-training paradigms. Thus, our work aims to investigate and revitalize generative pre-training towards LTSMs, facilitated by extensive time series and deftly designed adaptation on downstream tasks.\\n\\n#### 2.2. Large Time Series Models\\n\\nPre-trained models with scalability can evolve to large foundation models (Bommasani et al., 2021), featured by increasing model capacity and pre-training scale to solve various data and tasks. Large language models even demonstrate advanced capabilities such as in-context learning and emergent abilities (Wei et al., 2022). As of present, research on large time series models remains at a nascent stage. Exceptions such as **Wav2Vec2** (Baevski et al., 2020) and **MobileBERT** (Gu et al., 2021a) have demonstrated promising results on large language tasks, but they are not specifically designed for time series analysis. Therefore, there is a need for dedicated large time series models to address the complexities and challenges of time series data.\"}"}
{"id": "bYRYb7DMNo", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Timer: Generative Pre-trained Transformers Are Large Time Series Models\\n\\nUnified Time Series Dataset\\n\\nDiverse Shapes, Patterns and Domains\\n\\nFigure 2. Illustration of Unified Time Series Dataset (UTSD) that is composed of various time series domains with hierarchical capacities.\\n\\nisting efforts towards LTSMs can be categorized into two groups, with one being large language models for time series. FPT (Zhou et al., 2023) regards GPT-2 as a representation extractor of sequences, which is respectively fine-tuned on different downstream tasks. LLMTime (Chang et al., 2023) encodes time series into numerical tokens for LLMs, exhibiting model scalability in the forecasting task. Time-LLM (Jin et al., 2023) investigates prompting techniques to enhance prediction, demonstrating the generalization ability of LLMs. Unlike these methods, Timer is pre-trained natively on time series and free from extra modality alignment. Another category includes pre-trained models on large-scale time series. ForecastFPN (Dooley et al., 2023) is trained on synthetic series for zero-shot forecasting. CloudOps (Woo et al., 2023) adopts masked modeling on Transformer for domain-specific forecaster. Lag-Llama (Rasul et al., 2023) is a probabilistic univariate forecaster that adopts lags as covariates. PreDcT (Das et al., 2023b) is a decoder-only Transformer pre-trained on Google Trends, exhibiting no-table zero-shot ability. TimeGPT-1 (Garza & Mergenthaler-Canseco, 2023) releases the first commercial API for zero-shot forecasting. Different from prior works, our UTSD contains 1B real-world time points, which is not a simple aggregation but follows curated data processing. Timer is applicable to downstream tasks beyond forecasting and exhibits promising scalability. We are also the first to establish a zero-shot forecasting benchmark on concurrent LTSMs.\\n\\n3. Approach\\n\\nInspired by the sequential structure inherent in language and time series, we leverage the advancement of large language models for developing LTSMs. In this paper, we advocate the development of large models for time series with (1) the utilization of extensive time series corpora, (2) the adoption of a standardized format for diverse time series data, and (3) the generative pre-training on the decoder-only Transformer that autoregressively predict the next time series token.\\n\\n3.1. Data\\n\\nLarge-scale datasets are of paramount importance for pre-training large models. However, the curation of time series datasets can be prohibitively challenging. In spite of their ubiquity, there are numerous data of low quality, including missing values, unpredictability, variance in shape, and irregular frequencies, which significantly impact the efficacy of pre-training. Therefore, we establish the criteria for filtering high-quality data and stacking up the hierarchy of time series corpora. Concretely, we record the statistics of each dataset, including (1) basic properties, such as time steps, variate number, file size, frequency, etc; and (2) time series characteristics: periodicity, stationarity, and predictability. This also allows us to assess the complexity of different datasets and progressively conduct scalable pre-training.\\n\\nWe curate Unified Time Series Dataset (UTSD) as shown in Figure 2. UTSD is constructed with hierarchical capacities to facilitate the scalability research of large models. UTSD encompasses seven domains with up to 1 billion time points (UTSD-12G), covering typical scenarios of time series analysis. Following the principle of keeping pattern diversity, we include as diverse datasets as possible in each hierarchy, ensure the data size of each domain is nearly balanced when scaling up, and the complexity gradually increases in accordance with the calculated statistics. We release four volumes on https://huggingface.co/datasets/thuml/UTSD. Notably, we make our curation applicable to the increasing open-source datasets, which is beneficial for the continuous expansion of time series corpora. Particularly, we conduct the same procedure on the recent LOTSA (Woo et al., 2024), a great endeavor with 27B time points, to explore zero-shot forecasting and establish the benchmark of LTSMs. Detailed construction and statistics are provided in Appendix A.\\n\\n3.2. Training Strategy\\n\\nDifferent from natural language, which has been facilitated by the well-established discrete tokenization and sequential\"}"}
{"id": "bYRYb7DMNo", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative Pre-trained Transformers Are Large Time Series Models\\n\\n... look forward to the proposal of more diverse and comprehensive benchmarks in the future.\\n\\nTable 3.\\n\\n| Time Point | Frequency | ADF Statistic | Forecastability |\\n|------------|-----------|--------------|-----------------|\\n| Domain     | Dataset   | Time Points  | Variates        | Frequency |\\n| Rainfall   | Australia | 11.54M       | 3               | HOURLY    |\\n|            |           |              | -150.10         | 0.458     |\\n| Transport  | PeDESTrian| 0.08M        | 1               | HOURLY    |\\n|            |           |              | -23.462         | 0.297     |\\n| Sensor     | Data      | 3.24M        | 18              | 0.002 sec |\\n|            |           |              | -15.892         | 0.917     |\\n| Health     | BIDMC32HR | 0.04M        | 1000            | -         |\\n|            |           |              | -14.135         | 0.523     |\\n\\nTable 4.\\n\\nForecasting results on well-acknowledged deep forecasters and Timer, where Timer is pre-trained on the held-out datasets and then all models are superwisedly trained on the four datasets in the 672-96 setting.\\n\\n| Models   | MSE | MAE |\\n|----------|-----|-----|\\n| Timer    |     |     |\\n| Match    |     |     |\\n| Transformer |   |     |\\n| DL LINEAR |   |     |\\n| METRIC   |     |     |\\n\\n| Australia Rainfall | 0.800 | 0.720 |\\n|                   | 0.802 | 0.720 |\\n|                   | 0.804 | 0.804 |\\n| PeDESTrian Counts | 0.054 | 0.133 |\\n|                   | 0.058 | 0.153 |\\n|                   | 0.056 | 0.143 |\\n|                   | 0.060 | 0.149 |\\n| Sensor Data       | 0.049 | 0.094 |\\n|                   | 0.056 | 0.094 |\\n|                   | 0.052 | 0.091 |\\n|                   | 0.057 | 0.111 |\\n| Health BIDMC32HR  | 0.030 | 0.062 |\\n|                   | 0.188 | 0.284 |\\n|                   | 0.159 | 0.249 |\\n|                   | 0.320 | 0.409 |\\n\\nDomain transfer\\n\\nTo investigate the domain partitioning of UTSD, we use different domains of UTSD as the source and adapt the trained model to different target datasets to establish in-domain and out-of-domain transfer. The results in Table 5 indicate that in-domain transfer can further enhance the downstream performance. Additionally, as the number of downstream data samples increases, the relative improvement of pre-training will gradually diminish, and even lead to negative transfer in some out-of-domain scenarios. It provides a promising direction to develop domain-specific models.\\n\\nTable 5.\\n\\nIn-domain and out-of-domain forecasting results by pre-training on the source domain and fine-tuning on the target dataset under different data scarcity. ECL and Weather belong to the Energy and Nature domains respectively.\\n\\n| Target Dataset | Weather | ECL |\\n|----------------|---------|-----|\\n| Source Domain  | Energy  | Nature |\\n| From Scratch   | Energy  | Nature |\\n\\n| Metrics | MSE | MAE |\\n|---------|-----|-----|\\n| 5% Target | 0.229 | 0.279 |\\n| 20% Target | 0.185 | 0.238 |\\n| 100% Target | 0.158 | 0.209 |\\n\\nB. Implementation Details\\n\\nB.1. Pre-training\\n\\nBased on the constructed UTSD datasets of different sizes and difficulties in the unified single series sequence (S3) format, Timer is pre-trained with increasing data sizes and model parameters to validate the scalability. Detailed configurations and parameter counts of the pre-trained models involved in this paper are provided in Table 6.\"}"}
{"id": "bYRYb7DMNo", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"TABLE 6.\\nDetailed model configurations of Timer and corresponding parameter counts. The number of heads for models is fixed as 8.\\n\\n| SCENARIO | MODEL DIM. | SCALE-UP LAYER NUMBER | SCALE-UP OTHERS |\\n|----------|------------|-----------------------|-----------------|\\n| SCENARIO 3 | 256 | 512 | 1024 |\\n| SCENARIO 13 | 512 | 1024 | 1536 |\\n| SCENARIO 29 | 768 | 1024 | 2048 |\\n| SCENARIO 51 | 1024 | 1536 | 2048 |\\n| SCENARIO 1M | 256 | 256 | 256 |\\n| SCENARIO 2M | 256 | 256 | 256 |\\n| SCENARIO 3M | 768 | 1024 | 2048 |\\n| SCENARIO 4M | 768 | 1024 | 2048 |\\n| SCENARIO 38M | 37.97M | 67.40M |\\n| SCENARIO 67M | 37.97M | 67.40M |\\n\\nAll experiments are implemented in PyTorch (Paszke et al., 2019) and trained using NVIDIA A100 Tensor Core GPU. We use AdamW (Kingma & Ba, 2015) as the optimizer and cosine annealing algorithm for learning rate decay. The base learning rate is $5 \\\\times 10^{-5}$, and the final learning rate is $2 \\\\times 10^{-6}$. The decay steps are proportional to the number of training steps of 10 epochs. During pre-training, we use $N = 15$ as the number of tokens, and the batch size is set to 8192.\\n\\nDuring the pre-training on the UTSD-1G to UTSD-4G, we adopt a global shuffle strategy by loading the whole time series into the memory. Due to the much greater data scale of UTSD-12G compared to any commonly used time series dataset in the past, it is difficult to load all 12GB of the pre-training dataset into memory for global shuffling. Therefore, we use a local shuffle strategy, which randomly selects and divides the 12GB pre-training dataset into three 4G subsets in the storage space through file selection and segmentation, and then takes turns loading them into memory for pre-training with global steps. In this strategy, we also ensure the continuity of learning rate decay.\\n\\nB.2. Downstream Tasks\\nWe introduce the details of downstream experiments and present the generative scheme for each task, including time series forecast, imputation, and anomaly detection. Configurations for downstream adaptation are listed in Table 8. Corresponding detailed results are provided in Section C. And showcases of downstream tasks are shown in Figure 19, 20, and 21.\\n\\nForecasting\\nThe downstream forecasting task is tested on the real-world datasets, including (1) ETT (Zhou et al., 2021) contains 7 variates of power transformers, with the period from July 2016 to July 2018, including four subsets and sampling intervals of one hour and fifteen minutes. (2) ECL (Wu et al., 2021) mainly consists of hourly electricity consumption data from 321 customers. (3) Traffic (Wu et al., 2021) collected hourly road occupancy rates measured by 862 sensors on the San Francisco Bay Area highway from January 2015 to December 2016. (4) Weather (Wu et al., 2021) consists of 21 meteorological variates collected every 10 minutes from the Max Planck Institute of Biogeochemistry meteorological station in 2020. (5) PEMS contains California public transportation network data collected through a 5-minute window with the same four common subsets (PEMS03, PEMS04, PEMS07, PEMS08) used in SCINet (Liu et al., 2022).\\n\\nWe adopt the autoregressive generation training objective (Bengio et al., 2000) for downstream forecasting datasets in the fine-tuning stage. Specifically, we divide the lookback length into $N = 7$ tokens with the segment length $S = 96$. The model naturally outputs $N$ next tokens, which we calculate the mean squared error (MSE) of the $N$ tokens with corresponding ground truth and backpropagate the loss. During inference, we conduct iterative multi-step forecasting by concatenating the forecasted result with the lookback series and repeatedly adopting the model to generate the next token until the total length of predicted tokens reaches the expected length. If exceeding the predicted length, we will crop the excess value of the end.\\n\\nFor constructing data-scarce scenarios, we perform retrieval with the uniform interval in the training split according to the sampling ratio and conduct random shuffling at the end of each epoch to train the model. The construction pipeline with the fixed random seed ensures the reproducibility of our experimental results. In order to maintain comparability with previous benchmarks, we keep the same validation and testing sets of original downstream datasets and train the baseline model and Timer with the same set of training samples.\\n\\nImputation\\nConsidering the real-world scenario that missing values at time points often appear in succession, we adjust the previous point-level imputation proposed by TimesNet (Wu et al., 2022) and increase the difficulty of the task, that is, changing the masked unit from point to time series segment. The protocol poses challenges to recovering successive points.\"}"}
{"id": "bYRYb7DMNo", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7. Downstream forecasting dataset descriptions. \\n\\n| Dataset | Variable Split | Frequency | Information |\\n|---------|----------------|-----------|-------------|\\n| ETT1, ETT2 | 7 (8545, 2881, 2881) | HOURLY | ELECTRICITY |\\n| ETTM1, ETTM2 | 7 (34465, 11521, 11521) | 15 MIN | ELECTRICITY |\\n| ECL321 | 7 (18317, 2633, 5261) | HOURLY | ELECTRICITY |\\n| TRAFFIC862 | 862 (12185, 1757, 3509) | HOURLY | TRANSPORTATION |\\n| WEATHER21 | 21 (36792, 5271, 10540) | 10 MIN | WEATHER |\\n| PEMSO3 | 358 (15617, 5135, 5135) | 5 MIN | TRANSPORTATION |\\n| PEMSO4 | 307 (10172, 3375, 3375) | 5 MIN | TRANSPORTATION |\\n| PEMSO7 | 883 (16911, 5622, 5622) | 5 MIN | TRANSPORTATION |\\n| PEMSO8 | 170 (10690, 3548, 3548) | 5 MIN | TRANSPORTATION |\\n\\nwhich underscore higher demands for the model capacity to restore a span of series variations. Concretely, for a time series consisting of \\\\( N = 8 \\\\) segments with a length of \\\\( 24 \\\\), we randomly mask several segments as zeros except for the first segment, ensuring that the first segment is observed by the model to learn about initial series variations for imputation. For the training objective of downstream adaption, we adopt the denoising autoencoding (Raffel et al., 2020), which takes the masked parts as special tokens and unmasked segments as tokens as the model input. Due to the generative capability of Timer acquired by pre-training, we regard outputted tokens as the next predicted tokens and backpropagate the reconstruction error between the generated next token of the masked segment with the ground truth. During inference, we take the MSE of the masked segments as the indicator to evaluate the imputation performance. Based on the above protocol, we conduct the imputation task on the same datasets of the forecasting task in Table 7.\\n\\nTable 8. Detailed explanation of model hyperparameters and corresponding parameter quantities. We adopt the learning rate schedule strategy with exponential decay at a base of \\\\( 0.5 \\\\) under all three downstream tasks.\\n\\n| Task | Hyperparameter | Training Process | Forecasting | IMputation | Anomaly Detection |\\n|------|----------------|------------------|-------------|------------|-------------------|\\n|       | \\\\( \\\\text{min} \\\\) | \\\\( \\\\text{max} \\\\) | \\\\( d_{\\\\text{min}} \\\\) | \\\\( d_{\\\\text{max}} \\\\) | \\\\( \\\\text{LR} \\\\) | \\\\( \\\\text{LOSS} \\\\) | \\\\( \\\\text{BATCH SIZE} \\\\) | \\\\( \\\\text{EPOCHS} \\\\) | \\\\( \\\\text{MSE} \\\\) | \\\\( \\\\text{POS} \\\\) |\\n| Forex | 28 | 256 | 2048 | 3 | \\\\( \\\\text{E}^{-5} \\\\) | MSE | 2048 | 10 |\\n| Impute | 4 | 4 | 256 | 256 | 3 | \\\\( \\\\text{E}^{-5} \\\\) | MSE | 32 | 10 |\\n| Anomaly | 4 | 4 | 256 | 256 | 3 | \\\\( \\\\text{E}^{-5} \\\\) | MSE | 128 | 10 |\\n\\n\\\\( \\\\text{LR}^* \\\\) MEANS THE INITIAL LEARNING RATE.\\n\\nAnomaly detection For anomaly detection, prevalent protocols represented by Anomaly Transformer (Xu et al., 2021) and TimesNet (Wu et al., 2022) adopt the reconstructive approach that learns a feature extractor to reconstruct raw series, and the output is regarded as standard values. With all the mean squared errors between the standard and input series from the datasets, a specific threshold with the given quantile is determined to label the anomalies. Considering the prevalent scenarios of anomaly detection by monitoring real-time measurements, the quick judgment of on-the-fly time series anomaly can be more practical in the real world. Therefore, we propose a predictive protocol of anomaly detection based on generative models. Concretely, we use the observed segments to predict the future segment, and the predicted segment will be established as the standard to be compared with the actual value received. We adopt the UCR Anomaly Archive proposed by Wu & Keogh (2021). The task is to find the position of an anomaly in the test series based on a single normal series for training, which is an extremely data-scarce scenario with only one available sample.\"}"}
{"id": "bYRYb7DMNo", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"downstream adaptation, we adopt the same next token prediction as the pre-training, that is, training Timer with the lookback series containing \\\\( N = 7 \\\\) segments of the length \\\\( S = 96 \\\\) to generate the next token with length 96, which is regarded as the standard value. After training, we record the MSE of all segments in the test set and sort them in descending order. We find the first segment hit the anomaly interval labeled in the dataset within the first \\\\( \\\\alpha \\\\) quantile, and we record the quantile. Based on the above protocol, the real-time judgment ability of the model for sudden anomalies can be predictively examined.\\n\\nDetailed quantiles of Timer in 250 tasks are provided in Table 15. With more complex time series anomalies introduced in UCR Anomaly Archive, we hope to establish a reasonable and challenging benchmark in the field of anomaly detection.\\n\\nZero-shot forecasting\\n\\nWe conduct zero-shot forecasting experiments on seven datasets from iTransformer (Liu et al., 2023b). Notably, PEMS datasets are not included, as they have already appeared in the LOSTA dataset for pre-training. We apply the same data-split strategy as Autoformer (Wu et al., 2021) and calculate the averaged MSE of all windows in the test split. We evaluate five open-source large time series models, including Timer, Moiria (Woo et al., 2024), TimesFM (Das et al., 2023b), Chronos (Ansari et al., 2024), and MOMENT (Goswami et al., 2024). We further assess the qualities in Table 9, which includes more LTSMs and summarizes several attributes and abilities of large models.\\n\\n- **MOMENT**: MOMENT trained by masking modeling is applied to zero-shot forecasting by concatenating the lookback series with a mask with the length to be predicted. The mask through the model is regarded as the prediction.\\n- **Chronos**: Chronos is a probabilistic forecaster. Chronos-S1 means sampling one prediction trajectory and Chronos-S20 means sampling 20 trajectories and using the average trajectory.\\n- **TimesFM**: We use the official checkpoint from HuggingFace, which supports various input and output lengths.\\n- **Moiria**: The Moiria family has three different sizes, labeled as Moiria-S, Moiria-M, and Moiria-L.\\n- **Timer**: We provide three versions with increased scopes of pre-training. Timer-1B is pre-trained on UTSD; Timer-16B is pre-trained on UTSD and Buildings900K (Emami et al., 2023); and Timer-28B is pre-trained on UTSD and LOTSA.\\n\\nTable 9. Quality evaluation of large time series models.\"}"}
{"id": "bYRYb7DMNo", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Domain | Dataset | Time Points of Files | Size of Req. ADF. | Forecast Source | Time | % Error | % RMSE |\\n|--------|---------|----------------------|------------------|-----------------|-------|---------|--------|\\n| Health Motor Imagery | SCP1* | 0.001 | 279M | -3.132 | 0.449 | DAU ET AL. (2019) |\\n| | SCP2* | 0.004 | 12M | -3.191 | 0.504 | DAU ET AL. (2019) |\\n| | Fibrillation | 0.008 | 1M | -7.061 | 0.167 | DAU ET AL. (2019) |\\n| | Systolic Blood Pressure | 0.008 | 3M | -7.649 | 0.739 | DAU ET AL. (2019) |\\n| | Diastolic Blood Pressure | 0.008 | 3M | -4.855 | 0.577 | DAU ET AL. (2019) |\\n| | PPG | -0.02 | 61M | -7.725 | 0.380 | TAN ET AL. (2021) |\\n| | BIDMC32HR | -0.14 | 244M | -14.135 | 0.523 | TAN ET AL. (2021) |\\n| | TDBRAIN | -0.02 | 283M | -3.167 | 0.967 | WANG ET AL. (2023) |\\n| | CDC FluView | W | -0.28 | 2M | -4.381 | 0.307 | CDC (2017) |\\n| | WHO NREVSS FluView | W | -0.14 | 1M | -7.928 | 0.233 | CDC (2017) |\\n| | Project Tycho | W | -1.35 | 5M | -8.167 | 0.111 | VAN PANAIS ET AL. (2018) |\\n| | IoT Sensor Data | -0.02 | 631M | -15.892 | 0.917 | REAL-WORLD MACHINE LOGS |\\n| | Eigenforms | -0.02 | 107M | -12.201 | 0.393 | DAU ET AL. (2019) |\\n| | ERA5 | -0.02 | 368610M | -7.970 | 0.581 | NGUYEN ET AL. (2024) |\\n| | CMIP6 | -0.02 | 399069M | -7.960 | 0.573 | NGUYEN ET AL. (2024) |\\n| | Temperature Rain | D | -0.02 | 93M | -10.952 | 0.133 | GODAHEWA ET AL. (2021) |\\n| | Solar Light Curves | D | -0.02 | 37M | -1.891 | 0.555 | DAU ET AL. (2019) |\\n| | Santa River Flow | D | -0.02 | 1M | -19.305 | 0.300 | GODAHEWA ET AL. (2021) |\\n| | KDD Cup 2018 | H | -2.94 | 12M | -10.107 | 0.362 | GODAHEWA ET AL. (2021) |\\n| | US Births | D | 0.00 | 1M | -3.352 | 0.675 | GODAHEWA ET AL. (2021) |\\n| | SUNSPOT | D | 0.07 | 1M | -7.866 | 0.287 | GODAHEWA ET AL. (2021) |\\n| | Worms | -0.02 | 1M | -3.851 | 0.395 | DAU ET AL. (2019) |\\n| | Subseasonal Precipitation | D | 56.79 | 217M | -12.391 | 0.414 | MOUATADID ET AL. (2024) |\\n| | Subseasonal Temperature | D | 23.25 | 93M | -10.952 | 0.133 | GODAHEWA ET AL. (2021) |\\n| | Montgomery Precipitation | D | 9.76 | 38M | -13.567 | 0.276 | MOUATADID ET AL. (2024) |\\n| | Transport Pedestrian Counts | H | 3.13 | 12M | -23.462 | 0.297 | GODAHEWA ET AL. (2021) |\\n| | PEMS 03 | 5 | 9.38 | 36M | -19.051 | 0.411 | JIANG ET AL. (2023) |\\n| | PEMS 04 | 5 | 15.65 | 60M | -15.192 | 0.494 | JIANG ET AL. (2023) |\\n| | PEMS 07 | 5 | 24.92 | 96M | -20.603 | 0.466 | JIANG ET AL. (2023) |\\n| | PEMS 08 | 5 | 9.11 | 35M | -14.918 | 0.551 | JIANG ET AL. (2023) |\\n| | PEMS Bay | 5 | 16.94 | 65M | -12.770 | 0.704 | JIANG ET AL. (2023) |\\n| | LOS-LOP | 5 | 7.09 | 28M | -16.014 | 0.657 | JIANG ET AL. (2023) |\"}"}
{"id": "bYRYb7DMNo", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Domain | Dataset | Files Size | Req. Forecast | Source | Min. Forecast | Misc. Forecast |\\n|--------|---------|------------|---------------|--------|---------------|----------------|\\n| Transport | T RANSPORT | 33.95M | 130M | 5 | -32.209 | 0.535 |\\n| | L OOP | 0.46M | 2M | 15 | -5.900 | 0.217 |\\n| | S EATTLE | 0.87M | 22M | 30 | -8.571 | 0.219 |\\n| | S ZHETROY | 5.07M | 20M | 15 | -17.014 | 0.222 |\\n| | H ZMETRO | 0.38M | 2M | 15 | -11.254 | 0.232 |\\n| | Q-T RAFFIC | 264.39M | 1011M | 15 | -15.761 | 0.490 |\\n| | T AXI | 55.00M | 212M | 30 | -8.302 | 0.146 |\\n| Utilities | U BER T L C D AILY | 0.05M | 1M | Daily | -1.778 | 0.285 |\\n| | U BER T L C H OURLY | 1.13M | 5M | Hourly | -9.022 | 0.124 |\\n| Cloud | C LOUD O PS A LIBABA C LUSTER T RACE 2018 | 4452.20M | 16988M | 5 | -38.020 | 0.436 |\\n| | A ZURE VM T RACES 2017 | 885.52M | 10140M | 5 | -11.482 | 0.290 |\\n| | B ORG C LUSTER D ATA 2011 | 1073.89M | 14362M | 5 | -8.975 | 0.505 |\\n| Sales | M 5 | 58.33M | 224M | Daily | -6.985 | 0.247 |\\n| | FAVORITA S ALES | 139.18M | 535M | Daily | -6.441 | 0.097 |\\n| | K AGGLE FAVORITA T R ANSACTIONS | 0.08M | 1M | Daily | -5.481 | 0.362 |\\n| | K AGGLE R ESTAURANT | 0.29M | 2M | Daily | -4.650 | 0.126 |\\n| | K AGGLE H IERARCHICAL S ALES | 0.21M | 1M | Daily | -8.704 | 0.078 |\\n| Finance | G O D ADDY | 0.26M | 2M | Monthly | -1.539 | 0.784 |\\n| | B ITCOIN * | 0.07M | 1M | Daily | -2.493 | 0.398 |\\n| Misc. | M1 Y EARLY | 0.00M | 1M | Yearly | -0.791 | 0.473 |\\n| | M1 Q UARTERLY | 0.01M | 1M | Quarterly | -0.175 | 0.572 |\\n| | M1 M ONTHLY | 0.04M | 1M | Monthly | -1.299 | 0.588 |\\n| | M3 Y EARLY | 0.02M | 1M | Yearly | -0.850 | 0.524 |\\n| | M3 Q UARTERLY | 0.04M | 1M | Quarterly | -0.897 | 0.624 |\\n| | M3 M ONTHLY | 0.1M | 1M | Monthly | -1.954 | 0.635 |\\n| | M3 O THER | 0.01M | 1M | - | -0.568 | 0.801 |\\n| | M4 Y EARLY | 0.84M | 4M | Yearly | -0.036 | 0.533 |\\n| | M4 Q UARTERLY | 2.214M | 10M | Quarterly | -0.745 | 0.696 |\\n| | M4 M ONTHLY | 10.38M | 41M | Monthly | -1.358 | 0.665 |\"}"}
{"id": "bYRYb7DMNo", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Domain Dataset Time Points of Files Size of Req.\\n\\n| M4 Weekly | 0.37M | 2M | Weekly | -0.53 | 0.64 |\\n|-----------|-------|----|--------|--------|------|\\n| M4 Daily  | 9.96M | 39M| Daily  | -1.33 | 0.84 |\\n| M4 Hourly | 0.35M | 2M | Hourly | -2.07 | 0.53 |\\n\\n* The asterisk marks the dataset that originally belongs to UTSD.\\n\\n### A.3. UTSD Composition Analysis\\n\\nUTSD is constructed with hierarchical capacities, namely UTSD-1G, UTSD-2G, UTSD-4G, and UTSD-12G, where each smaller dataset is a subset of the larger ones. We adhere to the principle of progressively increasing the complexity and pattern diversity. Hierarchical structuring allows for a nuanced analysis that accounts for different levels of data granularity and complexity, ensuring that the pattern diversity is maintained across each hierarchy of the dataset. This approach not only facilitates a comprehensive evaluation across different scales but also ensures that each subset within the larger dataset offers a unique and incrementally challenging perspective, thus contributing to a more scalable pre-training.\\n\\n### Dataset Complexity\\n\\nWe conduct a comprehensive analysis of individual datasets to obtain the stationarity and forecastability measures and construct the UTSD hierarchically regarding these indicators. Code for calculating the statistics is provided in the repository of UTSD. Consequently, based on the ADF-Statistic of each dataset, we categorized the predictive difficulty of the datasets into three levels: Easy, Medium, and Hard. The criteria are listed as follows:\\n\\n- **Easy**: $\\\\text{ADF-Statistic} < -15.00$.\\n- **Medium**: $-15.00 \\\\leq \\\\text{ADF-Statistic} < -5.00$.\\n- **Hard**: $-5.00 \\\\leq \\\\text{ADF-Statistic}$.\\n\\nFor excessively long datasets in the temporal dimension, we additionally adopt the forecastability to assess the complexity of time series across different periods. As the capacity of UTSD increases, the periods with low forecastability will be further incorporated correspondingly. In a nutshell, larger datasets contain a greater proportion of challenging tasks as shown in Figure 16, thereby escalating the complexity of the pre-training process. The hierarchy reflects an increase in the difficulty of patterns as the dataset size grows. This approach enables a structured examination of the learning challenges presented by different dataset sizes, underlining the intricate balance between data volume and pre-training difficulty.\\n\\n### Pattern Diversity\\n\\nEach dataset within the UTSD collection demonstrates unique patterns, highlighting the importance of maintaining pattern diversity. We build the UTSD dataset in a top-down manner, ensuring that each hierarchy within UTSD comprehensively represents all individual datasets and contains as many patterns as possible. As shown in Figure 17, we select several representative datasets for visualization analysis:\\n\\n- **AtrialFibrillation**: The dataset showcases a fluctuating trend with minimal seasonality. This pattern is an indicator of irregular heart rhythm characteristics, typical in medical recordings related to cardiac health. Such fluctuations, lacking a clear seasonal pattern, are crucial for understanding the unpredictable nature of atrial fibrillation.\"}"}
{"id": "bYRYb7DMNo", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 17. Visualization of representative patterns in UTSD. Each time series is decomposed into trend, seasonal, and residual components.\\n\\n- **PigArtPressure**: The dataset reveals a fluctuating trend interspersed with notable seasonality. This pattern is representative of the physiological variations in blood pressure that can occur due to environmental or internal factors. The presence of both fluctuating trends and seasonality in this dataset underscores the complex nature of biological data.\\n\\n- **US Births**: The dataset distinctly exhibits a clear trend alongside pronounced seasonality. This pattern is characteristic of demographic data, where trends and seasonal variations can reflect socio-cultural factors and environmental influences. The consistent trend and seasonality in birth rates provide insights into population dynamics and reproductive behaviors.\\n\\nTo avoid selecting trivial temporal variations and provide a comprehensive representation of the varied patterns inherent in the individual datasets, we employ a downsampling technique for individual datasets. For those with a larger number of variates, we selectively choose representative variates that best encapsulate the distinct patterns of respective datasets. Similarly, for datasets with considerable temporal length, we resample them by the representative period. This methodical selection process ensures that the integrity and distinctive characteristics of each dataset are preserved, thereby maintaining the diversity of patterns across the hierarchical structure of the UTSD dataset.\\n\\n### A.4. Experiments\\n\\n#### Forecasting benchmarks\\n\\nIn the field of time series forecasting, several classical datasets such as ETT (Zhou et al., 2021), ECL (Wu et al., 2021), Traffic (Wu et al., 2021) and Weather (Wu et al., 2021) have become widely recognized benchmarks for evaluating model performance. However, the variability in several datasets, such as ECL, is relatively homogeneous, and they do not adequately address aspects such as non-stationarity and predictability when assessing the strengths and weaknesses of models. Consequently, the development of a new benchmark is essential. Therefore, we have carefully considered factors such as domain, number of variables, frequency, non-stationarity, and predictability, and have selected a subset from the UTSD as the new benchmark. The datasets we have selected are presented in Table 3. Furthermore, we have evaluated our model along with other baseline models on these benchmarks. The results are presented in Table 4.\\n\\nAdmittedly, relying solely on these benchmarks is not sufficient to comprehensively assess model performance. We also...\"}"}
{"id": "bYRYb7DMNo", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Therefore, we proceed to compare their forecasting performance under varying degrees of data scarcity.\\n\\nTo make sure the evaluation is comparable on different backbone architectures, we introduce a consistent training and validation setting. While a smaller training loss is achieved by the encoder-only Transformer in Figure 12, the progress of large language models indicates that the bottleneck in accommodating diverse time series data.\\n\\nTable 1. Downstream forecasting results under different data scarcity of the encoder-only and decoder-only Transformer respectively.\\n\\n| Time Points | Training Loss | Validation Loss |\\n|-------------|---------------|-----------------|\\n| Decoder-only Trm (Timer) | Encoder-only Trm | LSTM | TiDE | TCN |\\n| PEMS (A) | 0.166 | 0.149 | 0.285 | 0.361 | 0.390 |\\n| ETT (A) | 0.132 | 0.138 | 0.180 | 0.295 | 0.352 |\\n| CENARIO | 0.140 | 0.149 | 0.224 | 0.334 | 0.372 |\\n| TRAFFIC | 0.151 | 0.166 | 0.246 | 0.340 | 0.372 |\\n| REACH | 0.390 | 0.445 | 0.339 | 0.545 | 0.672 |\\n\\nFor other non-autoregressive backbones, we pre-train them by direct multi-step forecasting in the end-to-end scenarios. Meanwhile, the decoder-only Transformer achieves the best performance in most downstream scenarios, indicating better generalization than the encoder-only pre-trained model. The observations are consistent with several findings in large language models.\\n\\nWe elaborately evaluate two architectures on six benchmarks and elucidate why the encoder-only structure has become a promising choice for developing large time series models.\\n\\nFlexible sequence length\\n\\nTypically, current deep forecasting models are trained on specific lookback and forecast sequence lengths. For instance, one Timer is applicable on specific time series. As illustrated in Figure 12, Transformer exhibits excellent scalable ability as the backbone for LTSMs, whereas MLP-based and CNN-based architectures may encounter difficulties in accommodating diverse time series.\\n\\nMLP-based and CNN-based architectures may encounter difficulties in accommodating diverse time series. As illustrated in Figure 12, Transformer exhibits excellent scalable ability as the backbone for LTSMs, whereas MLP-based and CNN-based architectures may encounter difficulties in accommodating diverse time series.\\n\\nWe pre-train them by direct multi-step forecasting in the end-to-end scenarios. Meanwhile, the decoder-only Transformer achieves the best performance in most downstream scenarios, indicating better generalization than the encoder-only pre-trained model. The observations are consistent with several findings in large language models.\\n\\nBy contrast, the encoder-only pre-trained model is capable of addressing different domains, is end-to-end scenarios. Meanwhile, the decoder-only Transformer exhibits superior generalization on different domains. A possible reason is the encoder-only pre-trained model is capable of addressing different domains, is end-to-end scenarios. Meanwhile, the decoder-only Transformer exhibits superior generalization on different domains. A possible reason is the encoder-only pre-trained model is capable of addressing different domains, is end-to-end scenarios. Meanwhile, the decoder-only Transformer exhibits superior generalization on different domains. A possible reason is the encoder-only pre-trained model is capable of addressing different domains, is end-to-end scenarios. Meanwhile, the decoder-only Transformer exhibits superior generalization on different domains.\\n\\nWhile a smaller training loss is achieved by the encoder-only Transformer in Figure 12, the progress of large language models indicates that the bottleneck in accommodating diverse time series data. As illustrated in Figure 12, Transformer exhibits excellent scalable ability as the backbone for LTSMs, whereas MLP-based and CNN-based architectures may encounter difficulties in accommodating diverse time series.\\n\\nFor other non-autoregressive backbones, we pre-train them by direct multi-step forecasting in the end-to-end scenarios. Meanwhile, the decoder-only Transformer achieves the best performance in most downstream scenarios, indicating better generalization than the encoder-only pre-trained model. The observations are consistent with several findings in large language models.\"}"}
{"id": "bYRYb7DMNo", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.6. Evaluation of Large Time Series Models\\n\\nThere is a growing surge in the development of large models in the field of time series (Garza & Mergenthaler-Canseco, 2023; Das et al., 2023b; Woo et al., 2024; Ansari et al., 2024; Goswami et al., 2024). One particularly fascinating direction of research is focused on zero-shot forecasting (ZSF), which has the potential to renovate the conventional practice of training small models or fine-tuning language models for each specific scenario. Zero-shot generalization represents a sophisticated capability of large models, necessitating substantial model capacity and pre-training on extensive datasets. Consequently, we are actively expanding our dataset by incorporating the latest data infrastructure (Woo et al., 2024) in this field to pre-train Timer on ever larger scales (1B/16B/28B). Given the significant value to researchers and practitioners, we extensively evaluate current large models and establish the first zero-shot forecasting benchmark of LTSMs as detailed in Appendix B.2.\\n\\nQuality assessments\\n\\nOur evaluation assesses the quality of LTSMs in Table 9, including (1) fundamental attributes such as pre-training scale, parameters; (2) abilities such as applicable tasks, context length, etc. Current LTSMs essentially build upon Transformer, with a significantly smaller number of parameters compared to LLMs. There is still potential to support more tasks and longer contexts.\\n\\nQuantitative evaluations\\n\\nWe apply official checkpoints on seven datasets that do not appear during pre-training. The performance is fairly evaluated using MSE by predicting future 96 points of all windows in each dataset. Figure 14 presents the result and rank of each model, where the top-ranked LTSMs are Timer, Moiria (Woo et al., 2024), and TimesFM (Das et al., 2023b). However, the positive correlation between performance and pre-training scale remains relatively weak, highlighting the significance of high-quality data and synchronized scaling of data and model size.\\n\\n5. Conclusion and Future Work\\n\\nReal-world time series analysis is increasingly underscoring the demand for large time series models (LTSM). In this paper, we release a time series dataset with 1 billion time points, propose a unified sequence format to address the heterogeneity of multivariate time series, and develop a generative pre-trained Transformer as a generalizable, scalable, task-general LTSM. Empirically, we evaluate our model in forecasting, imputation, and anomaly detection, yielding state-of-the-art performance and notable pre-training benefits in the data-scarce scenario. Further analysis validates the model scalability, explores the architecture for LTSMs, and highlights the versatility of our autoregressive generation.\\n\\nBy performing zero-shot forecasting on available large models, we conduct the initial quantitative assessments among LTSMs. Quality evaluations unveil crucial pathways for future development, including better zero-shot generalization and facilitating probabilistic and long-context forecasting.\"}"}
{"id": "bYRYb7DMNo", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Timer: Generative Pre-trained Transformers Are Large Time Series Models\\n\\nImpact Statement\\n\\nThis paper aims to advance the development of large models for time series. In this work, we release a high-quality and unified time series dataset for scalable pre-training, which can serve as a foundation for pre-training and establishing new benchmarks. The outcome large model demonstrates notable effectiveness of generalization, versatility across various tasks, and scalability to refine performance, offering valuable insights for future investigations and application values for practitioners. Our paper mainly focuses on scientific research and has no obvious negative social impact.\\n\\nAcknowledgements\\n\\nThis work was supported by the National Natural Science Foundation of China (62022050 and U2342217), the BNRist Innovation Fund (BNR2024RC01010), and the National Engineering Research Center for Big Data Software.\\n\\nReferences\\n\\nAlexandrov, A., Benidis, K., Bohlke-Schneider, M., Flunkert, V., Gasthaus, J., Januschowski, T., Maddix, D. C., Rangapuram, S., Salinas, D., Schulz, J., et al. Gluonts: Probabilistic and neural time series modeling in python. Journal of Machine Learning Research, 21(116):1\u20136, 2020.\\n\\nAnsari, A. F., Stella, L., Turkmen, C., Zhang, X., Mercado, P., Shen, H., Shchur, O., Rangapuram, S. S., Arango, S. P., Kapoor, S., et al. Chronos: Learning the language of time series. arXiv preprint arXiv:2403.07815, 2024.\\n\\nBai, S., Kolter, J. Z., and Koltun, V. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.\\n\\nBao, H., Dong, L., Piao, S., and Wei, F. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.\\n\\nBengio, Y. A neural probabilistic language model. Advances in neural information processing systems, 13, 2000.\\n\\nBergmeir, C., Bui, Q., de Nijs, F., and Stuckey, P. Residential Power and Battery Data, 2023. URL https://doi.org/10.5281/zenodo.8219786.\\n\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-lut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\\n\\nBox, G. Box and jenkins: time series analysis, forecasting and control. In A Very British Affair: Six Britons and the Development of Time Series Analysis During the 20th Century, pp. 161\u2013215. Springer, 2013.\\n\\nBox, G. E., Jenkins, G. M., Reinsel, G. C., and Ljung, G. M. Time series analysis: forecasting and control. John Wiley & Sons, 2015.\\n\\nBreunig, M. M., Kriegel, H.-P., Ng, R. T., and Sander, J. Lof: identifying density-based local outliers. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pp. 93\u2013104, 2000.\\n\\nCDC. Flu portal dashboard, 2017. URL https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html. Accessed: [insert date of access].\\n\\nChang, C., Peng, W.-C., and Chen, T.-F. Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms. arXiv preprint arXiv:2308.08469, 2023.\\n\\nChen, S. Beijing Multi-Site Air Quality. UCI Machine Learning Repository, 2019. DOI: https://doi.org/10.24432/C5RK5G.\\n\\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597\u20131607. PMLR, 2020.\\n\\nDai, D., Sun, Y., Dong, L., Hao, Y., Sui, Z., and Wei, F. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022.\\n\\nDas, A., Kong, W., Leach, A., Sen, R., and Yu, R. Long-term forecasting with tide: Time-series dense encoder. arXiv preprint arXiv:2304.08424, 2023a.\\n\\nDas, A., Kong, W., Sen, R., and Zhou, Y. A decoder-only foundation model for time-series forecasting. arXiv preprint arXiv:2310.10688, 2023b.\\n\\nDau, H. A., Bagnall, A., Kamgar, K., Yeh, C.-C. M., Zhu, Y., Gharghabi, S., Ratanamahatana, C. A., and Keogh, E. The ucr time series archive. IEEE/CAA Journal of Automatica Sinica, 6(6):1293\u20131305, 2019.\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\nDong, J., Wu, H., Zhang, H., Zhang, L., Wang, J., and Long, M. Simmtm: A simple pre-training framework for masked time-series modeling. arXiv preprint arXiv:2302.00861, 2023.\"}"}
{"id": "bYRYb7DMNo", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Timer: Generative Pre-trained Transformers Are Large Time Series Models\\n\\nDooley, S., Khurana, G. S., Mohapatra, C., Naidu, S., and White, C. Forecastpfn: Synthetically-trained zero-shot forecasting. arXiv preprint arXiv:2311.01933, 2023.\\n\\nElliott, G., Rothenberg, T. J., and Stock, J. H. Efficient tests for an autoregressive unit root. Econometrica, 1996.\\n\\nEmami, P., Sahu, A., and Graf, P. Buildingsbench: A large-scale dataset of 900k buildings and benchmark for short-term load forecasting. Advances in Neural Information Processing Systems, 2023.\\n\\nFriedman, M. The interpolation of time series by related series. Journal of the American Statistical Association, 57(300):729\u2013757, 1962.\\n\\nGarza, A. and Mergenthaler-Canseco, M. Timegpt-1. arXiv preprint arXiv:2310.03589, 2023.\\n\\nGodahewa, R., Bergmeir, C., Webb, G. I., Hyndman, R. J., and Montero-Manso, P. Monash time series forecasting archive. arXiv preprint arXiv:2105.06643, 2021.\\n\\nGoerg, G. Forecastable component analysis. In International conference on machine learning, pp. 64\u201372. PMLR, 2013.\\n\\nGoswami, M., Szafer, K., Choudhry, A., Cai, Y., Li, S., and Dubrawski, A. Moment: A family of open time-series foundation models. arXiv preprint arXiv:2402.03885, 2024.\\n\\nHochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.\\n\\nJiang, J., Han, C., Jiang, W., Zhao, W. X., and Wang, J. Libcity: A unified library towards efficient and comprehensive urban spatial-temporal prediction. arXiv preprint arXiv:2304.14343, 2023.\\n\\nJin, M., Wang, S., Ma, L., Chu, Z., Zhang, J. Y., Shi, X., Chen, P.-Y., Liang, Y.-F., Pan, S., et al. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728, 2023.\\n\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\\n\\nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In ICLR, 2015. URL http://arxiv.org/abs/1412.6980.\\n\\nLiu, M., Zeng, A., Chen, M., Xu, Z., Lai, Q., Ma, L., and Xu, Q. Scinet: time series modeling and forecasting with sample convolution and interaction. NeurIPS, 2022.\\n\\nLiu, X., Xia, Y., Liang, Y., Hu, J., Wang, Y., Bai, L., Huang, C., Liu, Z., Hooi, B., and Zimmermann, R. Largest: A benchmark dataset for large-scale traffic forecasting. In Advances in Neural Information Processing Systems, 2023a.\\n\\nLiu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., and Long, M. itransformer: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625, 2023b.\\n\\nLiu, Y., Qin, G., Huang, X., Wang, J., and Long, M. Auto-times: Autoregressive time series forecasters via large language models. arXiv preprint arXiv:2402.02370, 2024.\\n\\nMancuso, P., Piccialli, V., and Sudoso, A. M. A machine learning approach for forecasting hierarchical time series. Expert Systems with Applications, 182:115102, 2021.\\n\\nMouatadid, S., Orenstein, P., Flaspohler, G., Oprescu, M., Cohen, J., Wang, F., Knight, S., Geogdzhayeva, M., Levang, S., Fraenkel, E., et al. Subseasonalclimateusa: A dataset for subseasonal forecasting and benchmarking. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nNguyen, T., Jewik, J., Bansal, H., Sharma, P., and Grover, A. Climatelearn: Benchmarking machine learning for weather and climate modeling. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nNie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.\\n\\nOpenAI, R. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2:13, 2023.\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Rai, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019.\\n\\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training. 2018.\\n\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR, 2021.\"}"}
{"id": "bYRYb7DMNo", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Timer: Generative Pre-trained Transformers Are Large Time Series Models\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. *The Journal of Machine Learning Research*, 21(1):5485\u20135551, 2020.\\n\\nRasul, K., Ashok, A., Williams, A. R., Khorasani, A., Adamopoulos, G., Bhagwatkar, R., Bilo\u02c7s, M., Ghonia, H., Hassen, N. V., Schneider, A., et al. Lag-llama: Towards foundation models for time series forecasting. *arXiv preprint arXiv:2310.08278*, 2023.\\n\\nTan, C. W., Bergmeir, C., Petitjean, F., and Webb, G. I. Time series extrinsic regression: Predicting numeric values from time series data. *Data Mining and Knowledge Discovery*, 35:1032\u20131060, 2021.\\n\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*, 2023.\\n\\nvan Panhuis, W. G., Cross, A., and Burke, D. S. Project tycho 2.0: a repository to improve the integration and reuse of data for global population health. *Journal of the American Medical Informatics Association*, 25(12):1608\u20131617, 2018.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. *Advances in neural information processing systems*, 30, 2017.\\n\\nWang, T., Roberts, A., Hesslow, D., Le Scao, T., Chung, H. W., Beltagy, I., Launay, J., and Raffel, C. What language model architecture and pretraining objective works best for zero-shot generalization? In *International Conference on Machine Learning*, pp. 22964\u201322984. PMLR, 2022a.\\n\\nWang, Y., Han, Y., Wang, H., and Zhang, X. Contrast everything: A hierarchical contrastive framework for medical time-series. *arXiv preprint arXiv:2310.14017*, 2023a.\\n\\nWang, Z., Xu, X., Zhang, W., Trajcevski, G., Zhong, T., and Zhou, F. Learning latent seasonal-trend representations for time series forecasting. *Advances in Neural Information Processing Systems*, 35:38775\u201338787, 2022b.\\n\\nWang, Z., Wen, Q., Zhang, C., Sun, L., Von Krannichfeldt, L., and Wang, Y. Benchmarks and custom package for electrical load forecasting. *arXiv preprint arXiv:2307.07191*, 2023b.\\n\\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*, 2022.\\n\\nWoo, G., Liu, C., Sahoo, D., Kumar, A., and Hoi, S. Cost: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. *arXiv preprint arXiv:2202.01575*, 2022.\\n\\nWoo, G., Liu, C., Kumar, A., and Sahoo, D. Pushing the limits of pre-training for time series forecasting in the cloudops domain. *arXiv preprint arXiv:2310.05063*, 2023.\\n\\nWoo, G., Liu, C., Kumar, A., Xiong, C., Savarese, S., and Sahoo, D. Unified training of universal time series forecasting transformers. *arXiv preprint arXiv:2402.02592*, 2024.\\n\\nWu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. *Advances in Neural Information Processing Systems*, 34:22419\u201322430, 2021.\\n\\nWu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M. Timesnet: Temporal 2d-variation modeling for general time series analysis. *arXiv preprint arXiv:2210.02186*, 2022.\\n\\nWu, R. and Keogh, E. Current time series anomaly detection benchmarks are flawed and are creating the illusion of progress. *IEEE Transactions on Knowledge and Data Engineering*, 2021.\\n\\nXu, J., Wu, H., Wang, J., and Long, M. Anomaly transformer: Time series anomaly detection with association discrepancy. *arXiv preprint arXiv:2110.02642*, 2021.\\n\\nYan, W., Zhang, Y., Abbeel, P., and Srinivas, A. Videogpt: Video generation using vq-vae and transformers. *arXiv preprint arXiv:2104.10157*, 2021.\\n\\nYue, Z., Wang, Y., Duan, J., Yang, T., Huang, C., Tong, Y., and Xu, B. Ts2vec: Towards universal representation of time series. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 36, pp. 8980\u20138987, 2022.\\n\\nZeng, A., Chen, M., Zhang, L., and Xu, Q. Are transformers effective for time series forecasting? In *Proceedings of the AAAI conference on artificial intelligence*, volume 37, pp. 11121\u201311128, 2023.\\n\\nZerveas, G., Jayaraman, S., Patel, D., Bhamidipaty, A., and Eickhoff, C. A transformer-based framework for multivariate time series representation learning. In *Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining*, pp. 2114\u20132124, 2021.\\n\\nZhang, X., Zhao, Z., Tsiligkaridis, T., and Zitnik, M. Self-supervised contrastive pre-training for time series via time-frequency consistency. *Advances in Neural Information Processing Systems*, 35:3988\u20134003, 2022.\"}"}
{"id": "bYRYb7DMNo", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Timer: Generative Pre-trained Transformers Are Large Time Series Models\\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.\\nZheng, Y., Yi, X., Li, M., Li, R., Shan, Z., Chang, E., and Li, T. Forecasting fine-grained air quality based on big data. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pp. 2267\u20132276, 2015.\\nZhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 11106\u201311115, 2021.\\nZhou, J., Lu, X., Xiao, Y., Su, J., Lyu, J., Ma, Y., and Dou, D. Sdwpf: A dataset for spatial dynamic wind power forecasting challenge at kdd cup 2022. arXiv preprint arXiv:2208.04360, 2022.\\nZhou, T., Niu, P., Wang, X., Sun, L., and Jin, R. One fits all: Power general time series analysis by pretrained lm. arXiv preprint arXiv:2302.11939, 2023.\"}"}
{"id": "bYRYb7DMNo", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Unified Time Series Dataset\\n\\nA.1. Datasets Details\\n\\nUnified Time Series Dataset (UTSD) is meticulously assembled from a blend of publicly accessible online data repositories and empirical data derived from real-world machine operations. To enhance data integrity, missing values are systematically addressed using linear interpolation techniques. We follow the unified data storage format (parquet) used in (Woo et al., 2024). For each univariate, multivariate, or irregular-sampled time series, we store them with timestamps and other meta-information in one directory using ARROW format. One dataset may composed of multiple related time series. We continue to expand the UTSD to include data from public datasets such as LOSTA for zero-shot forecasting. UTSD encompasses 29 individual datasets as listed with the asterisk mark in Table 2, intricately representative of a wide range of domains. All datasets can be classified into ten distinct domains by their source: Energy, Environment, Health, Internet of Things (IoT), Nature, Transport, Web, CloudOps, Finance, and Multiple Sources (Misc.), where the first seven domains originally come from our curated UTSD. The datasets exhibit diverse sampling frequencies, ranging from macro intervals such as yearly and quarterly to more fine-grained intervals like hourly and minutely. Notably, several datasets can demonstrate exceptionally high-frequency sampling rates, such as the MotorImagery dataset, which operates at a millisecond frequency. In the pursuit of advanced data analysis, we have also analyzed the stationarity manifested as ADF test statistics (Elliott et al., 1996) and forecastability (Goerg, 2013). The rigorous methodologies and intricate details are elaborated in Section A.2. We utilize these statistical indicators to filter four high-quality subsets of UTSD, namely UTSD-1G, UTSD-2G, UTSD-4G, and UTSD-12G. As we expand the dataset, we continuously analyze statistical indicators and employ various methodologies to ensure the selection of high-quality datasets. LOTSA has not been sorted in this hierarchy due to its immensity.\\n\\nA.2. Statistics\\n\\nWe analyze each dataset within our collection, examining the time series through the lenses of stationarity and forecastability. This approach allows us to characterize the level of complexity inherent to each dataset.\\n\\nStationarity\\n\\nThe stationarity of time series is a fundamental property that can be rigorously quantified using the Augmented Dickey-Fuller (ADF) test. Notably, a larger ADF test statistic typically signifies a higher degree of non-stationarity within the time series (Elliott et al., 1996). In the context of datasets comprising multiple time series, the challenge of aligning these series arises, particularly when they vary in length. To address this, we implement a length-weighted ADF method that evaluates the stationarity of the entire dataset, taking into consideration the varying lengths of individual series. This approach ensures that the contribution of each series to the overall stationarity metric is proportional to its length, thus reflecting its relative significance within the dataset. By doing so, the length-weighted ADF provides a more granular and accurate depiction of the stationarity of the dataset, highlighting the impact of longer series on the overall stability and ensuring that shorter series do not disproportionately affect the assessment. The weighted statistic is formulated as follows:\\n\\n$$T = \\\\sum_{i=1}^{C} T_i, \\\\quad ADF-Statistic(D) = \\\\sum_{i=1}^{C} T_i \\\\times ADF-Statistic(S_i),$$\\n\\nwhere \\\\(S_i \\\\in \\\\mathbb{R}^{T_i}\\\\) denotes the \\\\(i\\\\)-th series in dataset \\\\(D\\\\), \\\\(T_i\\\\) is the length of \\\\(S_i\\\\) and \\\\(C\\\\) is the number of time series of dataset \\\\(D\\\\).\\n\\nForecastability\\n\\nForecastability is calculated by subtracting the entropy of the series Fourier decomposition adopted from Goerg (2013), where a higher forecastability value indicates superior predictability. Just as with the assessment of stationarity, when considering a dataset composed of multiple time series of varying lengths, it is essential to adjust the measure of forecastability to account for these differences. Therefore, we extend the concept of forecastability to a weighted version, analogous to the length-weighted ADF method, to finely tune the predictability assessment to the characteristics of each series. The weighted forecastability for a dataset can be formulated as follows:\\n\\n$$T = \\\\sum_{i=1}^{C} T_i, \\\\quad Forecastability(D) = \\\\sum_{i=1}^{C} T_i \\\\times (1 - \\\\text{Entropy}(F(S_i))),$$\\n\\nwhere \\\\(F(S_i)\\\\) denotes the Fourier decomposition of the \\\\(i\\\\)-th series in dataset \\\\(D\\\\).\"}"}
{"id": "bYRYb7DMNo", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Timer: Generative Pre-trained Transformers Are Large Time Series Models\\n\\nwhere \\\\( S_i \\\\in \\\\mathbb{R} \\\\)\\n\\ndenotes the \\\\( i \\\\)-th time series in dataset \\\\( D \\\\),\\n\\n\\\\( T_i \\\\) is the length of \\\\( S_i \\\\)\\n\\nand \\\\( C \\\\) is the number of time series in dataset \\\\( D \\\\).\\n\\n\\\\( F(S_i) \\\\)\\n\\ndenotes the Fourier decomposition of series \\\\( S_i \\\\).\\n\\nTable 2: Dataset detailed descriptions.\\n\\n| Time Points | File Size | Freq. | ADF. | Forecast. | Source |\\n|-------------|-----------|-------|------|-----------|--------|\\n| **ENERGY**  |           |       |      |           |        |\\n| **LONDON**  | **168.50M** | 636M  | HOURLY | -13.158  | 0.173  |\\n|             | **166.50M** |       |       |           |        |\\n| **SMART**   |           |       |      |           |        |\\n| **IDEAL**   |           |       |      |           |        |\\n| **SCEAUX**  |           |       |      |           |        |\\n| **FORECAST**|           |       |      |           |        |\\n| **BUIDLINGS** |           |       |      |           |        |\\n| **C0VID-19 ENERGY** |           |       |      |           |        |\\n| **GOF12**   |           |       |      |           |        |\\n| **GOF14**   |           |       |      |           |        |\\n| **GOF17**   |           |       |      |           |        |\\n| **PDB**     |           |       |      |           |        |\\n| **SPANISH** |           |       |      |           |        |\\n| **ELF**     |           |       |      |           |        |\\n| **KDD CUP 2022** |           |       |      |           |        |\\n| **RESIDENTIAL LOAD POWER** |           |       |      |           |        |\\n| **RESIDENTIAL PV POWER** |           |       |      |           |        |\\n| **ENVIRONMENT AUSTRALIA** |           |       |      |           |        |\\n| **BEIJING PM25 QUALITY** |           |       |      |           |        |\\n| **BENZENE CONCENTRATION** |           |       |      |           |        |\\n| **CHINA AIR QUALITY** |           |       |      |           |        |\\n| **BEIJING AIR QUALITY** |           |       |      |           |        |\\n| **CHINA AIR QUALITY** |           |       |      |           |        |\\n\\nZhou et al. (2022)\\n\\nBerger et al. (2023)\\n\\nTan et al. (2021)\\n\\nPang (2019)\"}"}
{"id": "bYRYb7DMNo", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 14. Full results of downstream imputation of TimesNet under different data scarcities as the baseline.\\n\\n| Sample Ratio | 5%  | 20% | 100% |\\n|--------------|-----|-----|------|\\n| | 0.676 | 0.671 | 0.678 | 0.682 | 0.684 | 0.687 | 0.675 | 0.679 |\\n| | 0.284 | 0.296 | 0.269 | 0.289 |\\n| | 0.665 | 0.734 | 0.441 | 0.483 |\\n| | 0.138 | 0.135 | 0.143 | 0.153 |\\n| | 0.226 | 0.222 | 0.230 | 0.230 |\\n| | 0.802 | 0.794 | 0.801 | 0.809 |\\n| | 0.155 | 0.141 | 0.162 | 0.168 |\\n\\nTable 15. Full results of anomaly detection on UCR Anomaly Archive, which contains 250 datasets (arranged in 25 rows for a total of 10 rows). We provide the quantile (%) of each dataset, where the bold parts represent the better results that benefited from pre-training.\\n\\n| Index | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 | 25 |\\n|-------|---|---|---|---|---|---|---|---|---|---|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|\\n| Timer (From Scratch) | 1.1 | 16.2 | 6.7 | 1.2 | 19.0 | 23.8 | 16.7 | 19.0 | 14.3 |\\n| | 2.4 | 0.5 | 13.3 | 2.0 | 1.8 | 0.0 | 0.4 | 0.4 | 30.7 |\\n| | 2.0 | 1.5 | 0.9 | 22.8 |\\n| Timer (Pre-Trained) | 3.9 | 5.8 | 1.5 | 1.2 | 23.8 | 19.0 |\\n| | 7.1 | 47.6 |\\n| | 14.3 |\\n| | 0.3 | 3.9 | 0.2 | 5.9 | 6.0 | 17.0 | 5.6 | 13.9 | 5.3 | 0.9 | 1.2 | 38.1 | 0.4 | 1.5 | 18.3 | 1.7 | 0.4 |\\n| | 31.7 | 0.7 | 1.3 | 26.4 | 12.4 | 1.9 | 54.2 | 77.5 | 0.6 | 1.5 | 0.6 | 1.2 | 16.7 | 4.8 | 35.7 | 9.5 | 2.4 | 0.5 | 7.3 | 2.0 | 29.4 | 0.0 | 0.4 | 0.4 |\\n| | 20.6 | 3.0 | 3.0 | 1.5 | 1.3 | 1.5 | 2.4 | 2.4 | 1.7 | 18.3 | 1.7 | 6.2 | 2.4 | 2.1 | 1.7 | 3.3 | 1.7 | 2.1 | 6.7 | 0.4 | 2.7 | 4.3 | 28.5 | 1.3 | 90.6 |\\n| | 0.7 | 0.9 | 21.0 | 9.0 | 3.3 | 1.3 | 1.3 | 3.8 | 5.1 | 1.3 | 85.4 | 2.8 | 3.1 | 28.9 | 25.5 | 20.7 | 3.0 | 4.5 | 13.3 | 15.8 | 2.6 | 1.2 | 47.0 | 3.3 | 47.9 |\\n| | 47.6 | 1.5 | 0.3 | 19.7 | 1.0 | 13.3 | 7.2 | 0.1 | 0.1 | 15.6 | 0.9 | 0.1 | 15.6 | 5.3 | 0.9 | 1.2 | 38.1 | 0.4 | 1.5 | 18.3 | 1.7 | 0.4 |\\n| | 0.3 | 1.2 | 63.5 | 0.3 | 19.2 | 1.7 | 0.3 | 18.6 | 0.1 | 29.8 | 38.9 | 16.2 | 0.0 | 0.0 | 0.0 | 0.0 | 6.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.1 | 16.5 | 11.0 | 5.7 |\\n| | 0.3 | 1.4 | 63.5 | 0.3 | 19.2 | 1.7 | 0.3 | 18.6 | 0.1 | 29.8 | 38.9 | 16.2 | 0.0 | 0.0 | 0.0 | 0.0 | 6.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.1 | 16.5 | 11.0 | 5.7 |\"}"}
{"id": "bYRYb7DMNo", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 16. Detailed results for scaling up the pre-trained scale and the parameter of Timer.\\n\\n| PREDICTED | TRAINED | 4G | 4G | 1G | 2G | 4G | 12G |\\n|-----------|---------|----|----|----|----|----|-----|\\n| MODELDIM. | 256     | 512| 768| 1024| 1024| 768|\\n| LAYERS    | 2       | 4  | 6  | 8  | 6  | 8  |\\n| 5% SAMPLES| 0.188   | 0.174| 0.168| 0.160| 0.146| 0.138| 0.133| 0.130| 0.128| 0.125|\\n| PEMS03    | 0.223   | 0.208| 0.200| 0.190| 0.166| 0.154| 0.145| 0.145| 0.142| 0.143| 0.135|\\n| PEM04     | 0.147   | 0.131| 0.123| 0.120| 0.106| 0.097| 0.092| 0.092| 0.090| 0.090| 0.087|\\n| PEM08     | 0.367   | 0.339| 0.322| 0.319| 0.289| 0.256| 0.239| 0.228| 0.221| 0.216| 0.204|\\n| 20% SAMPLES| 0.154 | 0.141| 0.137| 0.134| 0.127| 0.124| 0.123| 0.121| 0.120| 0.117| 0.114|\\n| PEM03     | 0.182   | 0.162| 0.155| 0.150| 0.140| 0.132| 0.124| 0.124| 0.123| 0.122| 0.115|\\n| PEM04     | 0.115   | 0.104| 0.098| 0.095| 0.086| 0.082| 0.080| 0.079| 0.079| 0.078| 0.076|\\n| PEM08     | 0.326   | 0.277| 0.247| 0.238| 0.206| 0.193| 0.194| 0.193| 0.185| 0.185| 0.187|\\n\\nTable 17. Additional downstream forecasting results of the subsets of PEMS and ETT under different data scarcity of the encoder-only and decoder-only Transformer. The bold part indicates that the result performs best in the current dataset and sample ratio.\\n\\n| SCENARIO | 1% TARGET | 5% TARGET | 20% TARGET |\\n|----------|-----------|-----------|------------|\\n| ARCHITECTURE | ENCODER | DECODER | ENCODER | DECODER | ENCODER | DECODER | ENCODER | DECODER | ENCODER | DECODER |\\n| PREDICTED | 4G | 12G | 4G | 12G | 4G | 12G | 4G | 12G | 4G | 12G |\\n| PEM03    | 0.446 | 0.413 | 0.428 | 0.366 | 0.437 | 0.405 | 0.426 | 0.362 | 0.409 | 0.404 | 0.385 |\\n| PEM04    | 0.338 | 0.304 | 0.315 | 0.284 | 0.329 | 0.293 | 0.314 | 0.280 | 0.308 | 0.299 | 0.294 |\\n| PEM07    | 0.463 | 0.370 | 0.407 | 0.345 | 0.391 | 0.340 | 0.354 | 0.321 | 0.344 | 0.323 | 0.332 |\\n| PEM08    | 0.220 | 0.181 | 0.207 | 0.183 | 0.197 | 0.174 | 0.190 | 0.176 | 0.190 | 0.176 | 0.177 |\\n| PEM03    | 0.225 | 0.196 | 0.249 | 0.151 | 0.165 | 0.160 | 0.158 | 0.125 | 0.144 | 0.145 | 0.135 |\\n| PEM04    | 0.253 | 0.226 | 0.320 | 0.172 | 0.198 | 0.184 | 0.195 | 0.135 | 0.167 | 0.161 | 0.145 |\\n| PEM07    | 0.170 | 0.156 | 0.179 | 0.112 | 0.126 | 0.125 | 0.114 | 0.087 | 0.102 | 0.103 | 0.093 |\\n| PEM08    | 0.496 | 0.405 | 0.563 | 0.286 | 0.389 | 0.319 | 0.391 | 0.204 | 0.280 | 0.246 | 0.241 |\"}"}
{"id": "bYRYb7DMNo", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"### Table 18. Zero-shot forecasting evaluation\\n\\nWe extensively evaluate available large time series models. We provided the average rank on all downstream datasets, where the lower is better. For the probabilistic forecaster Chronos: S1 means sampling one trajectory and S20 means sampling 20 trajectories and using the average. '-' indicates that multi-step error accumulation leads to failure predictions.\\n\\n| Model       | ETT H1 | ETT H2 | ETT M1 | ETT M2 | ECL | TRAFFIC | WEATHER | Rank (Avg.) |\\n|-------------|--------|--------|--------|--------|-----|---------|---------|-------------|\\n| Timer       | 0.438  | 0.364  | 0.393  | 0.441  | 0.383| 0.394   | 0.674   | 1.571       |\\n| Timer       | 0.314  | 0.294  | 0.308  | 0.295  | 0.295| 0.330   | 0.318   | 2.286       |\\n| Timer       | 0.690  | 0.766  | 0.420  | 0.562  | 0.448| 0.452   | 0.670   | 4.429       |\\n| Timer       | 0.213  | 0.234  | 0.247  | 0.218  | 0.225| 0.214   | 0.257   | 2.250       |\\n| Timer       | 0.192  | 0.139  | 0.147  | 0.212  | 0.162| 0.155   | 0.744   | -           |\\n| Timer       | 0.458  | 0.399  | 0.414  | 0.616  | 0.425| 0.399   | 1.293   | -           |\\n| Timer       | 0.181  | 0.203  | 0.243  | 0.195  | 0.197| 0.221   | 0.255   | -           |\\n| Timer       | 1.571  | 2.286  | 4.429  | 2.250  | 5.500| 3.250   | -       |             |\\n\\nFigure 20. Visualization of anomaly detection results of Timer on partial UCR Anomaly Archive (Wu & Keogh, 2021). The masked part represents the abnormal position, and the model locates the abnormal interval by generating results that deviate from the abnormal series.\"}"}
{"id": "bYRYb7DMNo", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Timer: Generative Pre-trained Transformers Are Large Time Series Models\\n\\nFigure 21. Visualization of imputation results of Timer trained with 5% and 20% samples.\"}"}
{"id": "bYRYb7DMNo", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C. Full Results\\n\\nC.1. Time Series Forecasting\\n\\nWe provide all the results of the forecasting task in Figure 6. As shown in Table 10, we include six representative real-world datasets, demonstrating that Timer achieves state-of-the-art forecasting performance and the large-scale pre-training helps to alleviate performance degradation as the available downstream samples decrease.\\n\\nTable 10. Full forecasting results of Timer obtained by training from scratch (None) and fine-tuning from UTSD-12G pre-trained model. The bold values we use indicate that the pre-trained model results have positive benefits compared to from scratch. We attach the current state-of-the-art results as SOTA in this table, including PatchTST (Nie et al., 2022) on ETTh1 and Weather, as well as iTransformer (Liu et al., 2023b) on ECL, Traffic, PEMS03, and PEMS04. We adopt the unified lookback length as 672 and the forecast length as 96.\\n\\n- **Dataset** | **PRE-TRAINED** | **NONE** | **PRE-TRAINED** | **NONE** | **PRE-TRAINED** | **NONE** | **PRE-TRAINED** | **NONE** | **PRE-TRAINED** | **NONE** | **PRE-TRAINED** | **NONE** |\\n- **ETT H1** | | | | | | | | | | | | |\\n- **ECL** | | | | | | | | | | | | |\\n- **Traffic** | | | | | | | | | | | | |\\n- **Weather** | | | | | | | | | | | | |\\n- **PEMS03** | | | | | | | | | | | | |\\n- **PEMS04** | | | | | | | | | | | | |\\n\\n- **100%** | 0.363 | 0.358 | 0.132 | 0.136 | 0.352 | 0.351 | 0.165 | 0.154 | 0.126 | 0.118 | 0.125 | 0.107 |\\n- **75%** | 0.364 | 0.358 | 0.132 | 0.137 | 0.353 | 0.351 | 0.162 | 0.157 | 0.124 | 0.114 | 0.126 | 0.110 |\\n- **50%** | 0.370 | 0.356 | 0.132 | 0.135 | 0.356 | 0.352 | 0.161 | 0.151 | 0.129 | 0.114 | 0.131 | 0.110 |\\n- **25%** | 0.387 | 0.359 | 0.135 | 0.134 | 0.368 | 0.352 | 0.162 | 0.153 | 0.133 | 0.114 | 0.141 | 0.117 |\\n- **20%** | 0.385 | 0.359 | 0.137 | 0.134 | 0.372 | 0.352 | 0.166 | 0.151 | 0.135 | 0.116 | 0.145 | 0.120 |\\n- **15%** | 0.391 | 0.360 | 0.141 | 0.134 | 0.379 | 0.353 | 0.174 | 0.152 | 0.138 | 0.118 | 0.152 | 0.123 |\\n- **10%** | 0.426 | 0.361 | 0.144 | 0.133 | 0.387 | 0.353 | 0.182 | 0.152 | 0.140 | 0.120 | 0.165 | 0.126 |\\n- **5%** | 0.426 | 0.362 | 0.154 | 0.132 | 0.407 | 0.361 | 0.198 | 0.151 | 0.158 | 0.125 | 0.195 | 0.135 |\\n- **4%** | 0.424 | 0.362 | 0.161 | 0.135 | 0.416 | 0.366 | 0.208 | 0.152 | 0.166 | 0.127 | 0.210 | 0.138 |\\n- **3%** | 0.427 | 0.363 | 0.169 | 0.134 | 0.431 | 0.369 | 0.218 | 0.153 | 0.180 | 0.131 | 0.234 | 0.143 |\\n- **2%** | 0.427 | 0.363 | 0.186 | 0.137 | 0.467 | 0.380 | 0.230 | 0.159 | 0.201 | 0.137 | 0.257 | 0.152 |\\n- **1%** | 0.428 | 0.366 | 0.215 | 0.140 | 0.545 | 0.390 | 0.246 | 0.166 | 0.249 | 0.151 | 0.320 | 0.172 |\\n\\nC.2. Imputation\\n\\nIn this section, we provide the detailed results of the imputation task, including Timer trained from scratch and adapting pre-trained models with 5% available samples in Table 11, 20% samples in Table 12, and full samples in Table 13 on the downstream task. We also report the results of TimesNet at the above three ratios in Table 14. Based on the result, we provided an improvement in imputation performance before and after pre-training with {5%, 20%, 100%} samples in Figure 8 and 18, reflecting the benefits of autoregressive pre-training in segment-wise imputation task.\\n\\nC.3. Anomaly Detection\\n\\nIn this section, we provide detailed results of anomaly detection in Table 15, including the results of Timer from scratch and pre-trained. We conducted experiments on all 250 datasets of UCR Anomaly Archive and calculated the corresponding \u03b1 quantiles. The results show that the pre-trained Timer can detect time series anomalies with smaller \u03b1 on most datasets.\\n\\nC.4. Scalability\\n\\nWe provide detailed downstream forecasting results conducted on PEMS subsets with the scaling of model size (Figure 10) and data size (Figure 11). As shown in Table 16, it supports the scalability of our decoder-only Timer trained in GPT-style.\"}"}
{"id": "bYRYb7DMNo", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Timer\\n\\nPre-training benefit of Timer on the downstream imputation task with 20% and 100% available samples. Complete results used to calculate the pre-training benefits relative to training from scratch in the figures are listed in Table 11-14.\\n\\nC.5. Zero-shot forecasting\\n\\nIn this section, we provide detailed results of zero-shot forecasting in Table 18. We conducted experiments on seven datasets that are not included in the pre-training corpora of LTSMs. The results show that the top-ranked LTSMs are Timer, Moiria, and TimesFM. The performance of probabilistic forecaster Chronos can be improved by sampling more trajectories. It can still be an issue that scaling behavior is not evident on some datasets in the zero-shot scenario, and the failure of multi-step prediction can also appear in some models, indicating the development of zero-shot LTSMs is still in the early stage.\\n\\nD. Showcase\\n\\nTo present a clear performance of our proposed Timer, we provide visualizations for downstream forecasting, imputation, and anomaly detection tasks in Figure 19, 20, and 21. The forecasting and imputation contain experimental results at different sample ratios. For anomaly detection, we provide the position of the anomaly and the generated normal series by Timer.\"}"}
{"id": "bYRYb7DMNo", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"**Table 11.** Downstream imputation with 5% samples. Pre-training benefit $\\\\Delta\\\\%$ is calculated as the ratio of decreased imputing error in MSE. In the case of 5% samples, our pre-trained model outperforms TimesNet (Table 14) in all 44 settings on datasets and masked ratios.\\n\\n| Dataset | $M$ | ASR | Ratio |\\n|---------|-----|-----|-------|\\n|         | 12.5% | 25.0% | 37.5% | 50.0% |\\n| One     | 12G   | 12G   | 12G   | 12G   |\\n| PRE-TRAINED |       |       |       |       |\\n|         | 0.301 | 0.292 | +3.08 |       |\\n|         | 0.313 | 0.299 | +4.46 |       |\\n|         | 0.322 | 0.307 | +4.59 |       |\\n|         | 0.325 | 0.325 | 0.00  |       |\\n| ETT     |       |       |       |       |\\n| H       | 0.172 | 0.168 | +2.64 |       |\\n|         | 0.182 | 0.180 | +1.26 |       |\\n|         | 0.197 | 0.190 | +3.22 |       |\\n|         | 0.216 | 0.215 | +0.47 |       |\\n| M       |       |       |       |       |\\n| 1       | 0.397 | 0.347 | +12.52|       |\\n|         | 0.403 | 0.332 | +17.72|       |\\n|         | 0.428 | 0.374 | +12.77|       |\\n|         | 0.473 | 0.425 | +10.13|       |\\n| 2       |       |       |       |       |\\n|         | 0.118 | 0.116 | +1.59 |       |\\n|         | 0.127 | 0.121 | +4.69 |       |\\n|         | 0.134 | 0.131 | +2.22 |       |\\n|         | 0.147 | 0.144 | +1.99 |       |\\n| ECL     |       |       |       |       |\\n| 0.152   |       | +7.67 |       |\\n|         | 0.162 | +7.27 |       |\\n|         | 0.172 | +6.76 |       |\\n|         | 0.185 | +6.17 |       |\\n| TRAFFIC |       |       |       |       |\\n| 0.538   |       | +14.60|       |\\n|         | 0.567 | +14.14|       |\\n|         | 0.598 | +13.16|       |\\n|         | 0.633 | +11.91|       |\\n| WEATHER |       |       |       |       |\\n| 0.113   |       | -3.18 |       |\\n|         | 0.116 | +2.31 |       |\\n|         | 0.128 | +3.28 |       |\\n|         | 0.155 | +12.42|       |\\n| PEM03   |       |       |       |       |\\n| 0.160   |       | +15.78|       |\\n|         | 0.196 | +14.60|       |\\n|         | 0.257 | +13.51|       |\\n|         | 0.354 | +13.49|       |\\n| PEM04   |       |       |       |       |\\n| 0.193   |       | +16.80|       |\\n|         | 0.238 | +15.30|       |\\n|         | 0.305 | +15.28|       |\\n|         | 0.410 | +15.14|       |\\n| PEM07   |       |       |       |       |\\n| 0.166   |       | +16.19|       |\\n|         | 0.210 | +12.89|       |\\n|         | 0.278 | +12.72|       |\\n|         | 0.378 | +13.76|       |\\n| PEM08   |       |       |       |       |\\n| 0.185   |       | +15.33|       |\\n|         | 0.232 | +15.98|       |\\n|         | 0.303 | +12.75|       |\\n|         | 0.417 | +13.26|       |\\n\\n**E. Limitations**\\n\\nUTSD is constructed with hierarchical capacities. Though it is helpful to study the scalability of the model, it is not big enough since we have witnessed recent work claims the pre-training on ten and even a hundred billion time points. Therefore, we advocate for the ongoing expansion of data infrastructure while upholding high quality and hierarchy, which may significantly advance the time series community. In terms of the method, this work aims at an early but important development of large models. Despite the generalization, scalability, and task-generality that Timer has achieved, time series classification has not been unified in our generative formulations and Timer does not yet support probabilistic forecasting and specially adapts for multiple variables. It also leaves for better zero-shot generalization and advanced abilities, such as in-context learning and multi-modality, which are scheduled to be developed by ever-large pre-training.\\n\\n**F. Societal Impacts**\\n\\nReal-world applications\\nThis paper develops large models for the field of time series. We present a general-purpose time series analysis model to handle data-scarce scenarios. Given the state-of-the-art performance of Timer, this model may be applied to many real-world applications, which helps our society prevent risks in advance and make better decisions with limited available samples. Our paper mainly focuses on scientific research and has no obvious negative social impact.\\n\\nAcademic research\\nIn this paper, we release a high-quality dataset for scalable pre-training. Different from prior works, the dataset is not merely aggregation but follows deftly curation. Based on it, the research on scalable time series architectures and pre-training techniques can be facilitated. Towards large time series models, the proposed Timer shows its generalization and versatility in many tasks. The regime of generative pre-training and autoregression can be instructive for future research.\"}"}
{"id": "bYRYb7DMNo", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 12. Downstream imputation with 20% samples. Pre-training benefit $\\\\Delta\\\\%$ is calculated as the ratio of decreased imputing error in MSE. In the case of 20% samples, our pre-trained model outperforms TimesNet in 86.4% of 44 settings on datasets and masked ratios.\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.289       | 0.278   | 3.83 |\\n| 25.0%     | 0.293       | 0.287   | 1.91 |\\n| 37.5%     | 0.305       | 0.297   | 2.56 |\\n| 50.0%     | 0.322       | 0.314   | 2.44 |\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.168       | 0.166   | 1.21 |\\n| 25.0%     | 0.180       | 0.178   | 1.02 |\\n| 37.5%     | 0.192       | 0.190   | 0.73 |\\n| 50.0%     | 0.208       | 0.208   | 0.17 |\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.349       | 0.328   | 5.98 |\\n| 25.0%     | 0.335       | 0.326   | 2.72 |\\n| 37.5%     | 0.378       | 0.360   | 4.84 |\\n| 50.0%     | 0.426       | 0.407   | 4.34 |\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.139       | 0.133   | 4.77 |\\n| 25.0%     | 0.158       | 0.123   | 22.30|\\n| 37.5%     | 0.176       | 0.136   | 22.95|\\n| 50.0%     | 0.146       | 0.143   | 2.22 |\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.136       | 0.130   | 5.01 |\\n| 25.0%     | 0.146       | 0.138   | 5.30 |\\n| 37.5%     | 0.157       | 0.149   | 5.04 |\\n| 50.0%     | 0.170       | 0.162   | 4.54 |\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.451       | 0.420   | 6.89 |\\n| 25.0%     | 0.481       | 0.446   | 7.26 |\\n| 37.5%     | 0.513       | 0.477   | 7.09 |\\n| 50.0%     | 0.550       | 0.511   | 7.10 |\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.125       | 0.129   | -3.40|\\n| 25.0%     | 0.125       | 0.147   | -17.46|\\n| 37.5%     | 0.154       | 0.125   | 18.77|\\n| 50.0%     | 0.141       | 0.153   | -7.93|\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.134       | 0.120   | 10.41|\\n| 25.0%     | 0.169       | 0.150   | 11.35|\\n| 37.5%     | 0.221       | 0.198   | 10.61|\\n| 50.0%     | 0.305       | 0.273   | 10.32|\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.162       | 0.146   | 9.93 |\\n| 25.0%     | 0.203       | 0.184   | 9.61 |\\n| 37.5%     | 0.262       | 0.236   | 9.86 |\\n| 50.0%     | 0.354       | 0.320   | 9.65 |\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.140       | 0.125   | 10.96|\\n| 25.0%     | 0.182       | 0.162   | 10.85|\\n| 37.5%     | 0.240       | 0.214   | 10.77|\\n| 50.0%     | 0.327       | 0.290   | 11.57|\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.155       | 0.139   | 10.39|\\n| 25.0%     | 0.198       | 0.174   | 12.11|\\n| 37.5%     | 0.268       | 0.236   | 11.94|\\n| 50.0%     | 0.366       | 0.324   | 11.63|\\n\\nTable 13. Downstream imputation with 100% samples. Pre-training benefit $\\\\Delta\\\\%$ is calculated as the ratio of decreased imputing error in MSE. In the case of 100% samples, our pre-trained model outperforms TimesNet in 56.8% of 44 settings on datasets and masked ratios.\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.274       | 0.273   | 0.34 |\\n| 25.0%     | 0.283       | 0.283   | -0.04|\\n| 37.5%     | 0.295       | 0.294   | 0.52 |\\n| 50.0%     | 0.313       | 0.312   | 0.17 |\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.207       | 0.177   | 14.44|\\n| 25.0%     | 0.186       | 0.186   | -0.49|\\n| 37.5%     | 0.192       | 0.195   | -1.18|\\n| 50.0%     | 0.210       | 0.209   | 0.57 |\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.342       | 0.352   | -3.04|\\n| 25.0%     | 0.359       | 0.345   | 3.87 |\\n| 37.5%     | 0.400       | 0.371   | 7.09 |\\n| 50.0%     | 0.418       | 0.413   | 1.15 |\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.149       | 0.161   | -8.01|\\n| 25.0%     | 0.153       | 0.171   | -11.46|\\n| 37.5%     | 0.173       | 0.176   | -1.53|\\n| 50.0%     | 0.183       | 0.158   | 13.27|\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.125       | 0.122   | 2.98 |\\n| 25.0%     | 0.134       | 0.130   | 3.06 |\\n| 37.5%     | 0.144       | 0.139   | 3.12 |\\n| 50.0%     | 0.157       | 0.152   | 2.87 |\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.402       | 0.392   | 2.50 |\\n| 25.0%     | 0.424       | 0.414   | 2.48 |\\n| 37.5%     | 0.454       | 0.443   | 2.46 |\\n| 50.0%     | 0.488       | 0.477   | 2.29 |\\n\\n| ASK Ratio | PRE-TRAINED | ONE 12G | \u2206%  \\n|-----------|-------------|---------|------\\n| 12.5%     | 0.144       | 0.157   | -8.67|\\n| 25.0%     | 0.159       | 0.146   | 8.01 |\\n| 37.5%     | 0.162       | 0.147   | 9.41 |\\n| 50.0%     | 0.168       | 0.158   | 6.15 |\"}"}
{"id": "bYRYb7DMNo", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative Pre-trained Transformers Are Large Time Series Models\\n\\nStructure with the regular shape, constructing unified time series sequences is not straightforward due to the heterogeneity of series such as amplitude, frequency, stationarity, and disparities of the datasets in the variate number, series length and purpose. To promote pre-training on extensive time series, we propose to convert heterogeneous time series into a single-series sequence (S3), which reserves the patterns of series variations with the unified context length.\\n\\n**S3**: single-series sequence\\n\\nVariates from diverse datasets and domains\\n\\n**Figure 3.** Pre-training strategy for heterogeneous time series.\\n\\nAs depicted in Figure 3, our initial step involves normalizing and merging at the level of variates. Each series representing a variate will be divided into training and validation splits at a ratio of 9:1 for pre-training. We apply the statistics of the training split to normalize the entire series. The normalized time series are merged into a pool of single-variate series. The time points of single-variate series for training follow the normal distribution, which mitigates the discrepancies in the amplitude and variate numbers across multiple datasets. We uniformly sample sequences from the pool by a window, obtaining single-series sequences with a fixed context length, as the format of S3. The proposed format is essentially an extension of Channel Independence CI (Nie et al., 2022). However, CI necessitates time-aligned multivariate series and flattens the variate dimension to the same batch, thereby requiring the batch of samples to originate from the same dataset. Based on our format, the model observes sequences from different periods and different datasets, thus increasing the pre-training difficulty and directing more attention to the temporal variation. S3 does not require time alignment, which applies to broad univariate and irregular time series.\\n\\nWe then employ generative pre-training, where single-series sequences are regarded as standard sentences of time series.\\n\\n**3.3. Model Design**\\n\\nGiven the limited exploration of the backbone for large time series models, we extensively evaluate candidate backbones on the same pre-training scale in Section 4.5, which validates Transformer as the scalable choice. Further, we review Transformer-based models in time series forecasting, which have experienced notable development in recent years. They can be categorized into encoder-only and decoder-only architectures following a similar pipeline. As illustrated in Figure 4, prevalent small time series forecasters, the encoder-only non-autoregressive models, generate predictions with the globally flattened representation of lookback series. Although direct projection may benefit from end-to-end supervision, flattening can also wipe out sequential dependencies modeled by attention, thereby weakening Transformer layers to reveal the patterns of temporal variations.\"}"}
{"id": "bYRYb7DMNo", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. We demonstrate Timer as a large time series model in time with the flexibility to address unfixed context length during next token prediction, which is detailed in Appendix B.2. The downstream datasets are not included in the pre-training stage to prevent data leakage. We provide comprehensive zero-shot evaluation across concurrent large time series models. All the downstream datasets are adopted including ETT, ECL, Traffic, Weather, and PEMS adopted challenges in real-world applications. To thoroughly evaluate the model/data size and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try to build a comprehensive model scalability, including model/data size, and try"}
{"id": "bYRYb7DMNo", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8. Performance comparison with state-of-the-art small deep models. For imputation, Time is compared with TimesNet (Wu et al., 2022) under different data scarcities, each of which contains available samples. Following TimesNet (Wu et al., 2022), we compare the number of detected anomalies under given confidence quantiles. Detailed results are provided in Table 11-15.\\n\\nWhen attempting to recover a span of time points encompassed results of other data scarcities are provided in Figure 18. Imputation is ubiquitous in real-world applications. Anomalies are labeled as potential positions of anomalies. In order to detect anomalies, we introduce UCR Anomaly Archive (Wu & Keogh, 2021) with the actual value received. Unlike the previous method requiring to collect time series of a period for reconstruction, detection in a predictive approach, which utilizes the observed data. However, while various machine learning algorithms and simple linear interpolation can effectively cope with the corruptions randomly happening at the point of being completely masked. We obtain Timer with the token number $5\\\\% = 24$ and we calculate the average reduced set. By regarding the MSE of all segments as the confidence between the predicted series and ground truth on the test series is provided for training, and the model should locate the position of an anomaly in the test series. We first train a predictive model on the training set and calculate the MSE of all segments as the standard to be compared. Based on our generative model, we cope with anomaly detection on the fly. Thus, the task is converted into a next token prediction task as detailed in Appendix B.2. For downstream samples. Additional experiments on available samples are provided in Figure 18, validating the effectiveness of Timer on the challenging imputation task. Pre-training benefit of Timer on the downstream imputation benchmark, which includes datasets with four mask ratios each. Timer is compared with the previous state-of-the-art imputation model (Wu et al., 2022). As shown in the left of Figure 7, Timer outperforms in respectively $0\\\\%$, $4\\\\%$, $8\\\\%$, $12\\\\%$, $16\\\\%$, $20\\\\%$, $24\\\\%$, $28\\\\%$, $32\\\\%$, $36\\\\%$, $40\\\\%$, $44\\\\%$, $48\\\\%$, $52\\\\%$, $56\\\\%$, $60\\\\%$, $64\\\\%$, $68\\\\%$, $72\\\\%$, $76\\\\%$, $80\\\\%$, $84\\\\%$, $88\\\\%$, $92\\\\%$, $96\\\\%$, $100\\\\%$ of the data scarcities of $5\\\\%$ and $50\\\\%$.\\n\\nFigure 7. Results\\n\\n4.2. Imputation\\n\\nIn T5 (Raffel et al., 2020) as detailed in Appendix B.2 to stream adaptation, we conduct the denoising autoencoding task with $5\\\\%$ length $S$ on UTSD-4G by generative pre-training with the segment-level imputation. Each time series is divided into segments and each segment has the length of $8$. segment-level imputation. Each time series is divided into intricate series variations. In this task, we conduct the recovery. Consequently, imputation can be ever challenging to alleviate by the few-shot generalization of LTSMs.\\n\\nWe establish a comprehensive segment-level imputation scenarios. In UCR Anomaly Detection Archive (Wu & Keogh, 2021), we compare the number of detected anomalies under given confidence quantiles. Detailed results are provided in Table 11-15.\"}"}
{"id": "bYRYb7DMNo", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"indicating the higher parameter efficiency in the time series models, the parameter scale of Timer can still be small, in-saturation on these datasets. Compared with large language size previously, which can be attributed to the performance benefit is relatively small compared to expanding the model\\n\\n25\\n\\nL forecasting errors in two few-shot scenarios by an average of\\n\\nD dimension\\n\\nResults are presented in Figure 10. While keeping model\\n\\nbehavior of Timer, we pre-train Timer with increased model\\n\\n4.4. Scalability\\n\\nTimer: Generative Pre-trained Transformers Are Large Time Series Models\\n\\nLarger Timer demonstrates better performance on down-\\n\\n\\nto\\n\\n0\\n\\n123\\n\\n138\\n\\n231\\n\\n139\\n\\nOverall, by increasing the model size and data scale, Timer\\n\\nmodels highlights the significance of synchronized scaling\\n\\nof data with the model parameters, there is still an urgent\\n\\nbones for modeling the sequential time series modality be-\\n\\nDeep learning approaches have\\n\\n(0\\n\\n2023b) training on full samples of PEMS datasets\\n\\nsurpassing state-of-the-art multivariate forecaster (Liu et al.,\\n\\n2023b). As the scaling law (Kaplan et al., 2020) of large\\n\\nmodality, which is also supported by prior works (Das et al.,\\n\\nFigure 9. with different UTSD sizes, which exhibits steady improve-\\n\\nFigure 11. ing proposed. To validate the appropriate option for large\\n\\n7\\n\\nResults\\n\\nWe evaluate well-acknowledged anomaly detec-\\n\\nances with given quantiles, where Timer outperforms other\\n\\nadvanced anomaly detection models, exhibiting the versatil-\\n\\nlies with proposed.\"}"}
