{"id": "hQpUhySEJi", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2. Evaluations on Basic Architectures. Training and Evaluation in Team Reach Environments. We report Success Rate (%) on the final step.\\n\\n| Methods       | 1st ant     | 1st centipede | 2nd ants | 2nd ant | 2nd claw | 2nd unimals | 3rd ant | 3rd claw | 3rd centipede |\\n|---------------|-------------|---------------|----------|---------|----------|-------------|---------|---------|---------------|\\n| MLP+HN        | 93.39 \u00b1 5.25 | 11.28 \u00b1 3.21  | 5.25 \u00b1 1.39 | 9 4.52 \u00b1 3.93 | 10.86 \u00b1 8.74 | 3.98 \u00b1 1.83 |\\n| SHNN          | 97.26 \u00b1 1.51 | 47.82 \u00b1 20.62 | 77.93 \u00b1 22.22 | 17.40 \u00b1 3.54 | 11.97 \u00b1 2.31 | 8.70 \u00b1 2.42 |\\n| Transformer+HN| 5.47 \u00b1 2.84  | 5.55 \u00b1 2.99   | 2.47 \u00b1 0.73 | 0.51 \u00b1 0.19 | 0.25 \u00b1 0.10 | 2.26 \u00b1 1.92 |\\n| SHTransformer | 63.61 \u00b1 39.57 | 11.52 \u00b1 2.39  | 11.37 \u00b1 15.50 | 1.17 \u00b1 0.70 | 0.26 \u00b1 0.15 | 7.12 \u00b1 2.29 |\\n\\nOur proposed SHNN demonstrates a clear advantage, outperforming all baselines across various scenarios. Team Sumo in Figure 5: with an increase in the number and complexity of agent morphologies, the advantage of our method over baselines progressively amplifies. These observations underscore that our proposed SHNN method not only exhibits superior capabilities in compressing the global state space of multi-entity tasks and significantly enhances the generalization of body-level policies, but also sees these advantages magnified in scenarios with increasingly complex agent morphologies and a greater number of agents.\\n\\n5.4. Ablation Studies\\n\\nAblations on Assignment\\nWe evaluate the impact of task assignment by comparing several variants, as depicted in Figure 6 and Figure 11. We consider the following configurations:\\n\\n- **greedy**: a default model that employs a greedy strategy-based bipartite matching for task assignment;\\n- **stochastic**: a variant based on stochastic assignments;\\n- **w/o assignment**: a variant where no task assignment is performed, with the entity-level graph configured as a fully connected graph and sparse root mean \\\\( \\\\vec{c} = 1_{N+M} \\\\) serving as the origin of LRF in Equation (11).\\n\\nOur empirical findings affirm that assignment consistently enhances performance across various neural networks architectures in the Team Reach environments. Specifically, MLP networks exhibit marked improvement when incorporating task assignments. For SHNN, the greedy graph matching strategy of task assignment significantly outperforms both stochastic and non-assignment strategies. This evidence underscores that our task assignment effectively facilitates task-specific dynamics, managing complex entity interactions by decoupling local transformations from the overall structures while compressing the search space through local physical symmetry. For Team Sumo, see Appendix C.5.\\n\\nAblations on Equivariance\\nWe ablate the following variants in Figure 7 and Figure 12:\\n\\n- **SHNN:SOg3**: our full model, which is SO \\\\( \\\\vec{g}(3) \\\\)-equivariant;\\n- **SHNN:Og3**: an \\\\( O \\\\vec{g}(3) \\\\)-equivariant variant, where \\\\( O_i = \\\\vec{Z}'_i \\\\) in Equation (10);\\n- **SHNN:SO3**: a full model, which is SO \\\\( \\\\vec{g}(3) \\\\)-equivariant;\"}"}
{"id": "hQpUhySEJi", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\nFigure 8. Local Translation Invariance. Training and Evaluation Curves in 3 ant claw centipede Team Reach Environment.\\n\\nSHNN:SO3, a SO (3) -equivariant variant, where \\\\( \\\\vec{g} \\\\) is omitted in the computation in Equation (8);\\nHNN+DN, a non-equivariant variant, where \\\\( \\\\vec{Z} \\\\) are treated as scalars, but which utilizes the goal direction to construct the LRF;\\nHNN+HN, a non-equivariant variant, where \\\\( \\\\vec{Z} \\\\) are treated as scalars, but which utilizes the agent's heading direction to construct the LRF;\\nHNN, a non-equivariant variant, where \\\\( \\\\vec{Z} \\\\) are treated as scalars.\\n\\n1. Which symmetry group works best within our network framework? Comparative experiments between the SO (3) group and the O (3) group show that the positive impact of emphasizing rotational symmetry significantly outweighs any disadvantages from the reduced focus on reflection symmetry, particularly in terms of reducing the massive search space. Furthermore, the comparison between the SO (3) group and the SO (3) group demonstrates the importance of sensing the direction of gravity in policy learning.\\n\\n2. Can equivariant network methods replace or even surpass methods based on hand-crafted LRF? Experiments demonstrate that whether using goal orientation or the agent's heading orientation to construct the LRF, the performance is inferior compared to ours. This indicates that for current mainstream neural networks, entity-level subequivariant message passing emerges as a simpler yet more effective alternative to hand-crafted LRF, providing a plug-in solution for equivariant modifications.\\n\\nImportance of Local Symmetry\\nThe analysis of local symmetry in the Team Reach environments, illustrated by comparing the red and green lines in the left plot of Figure 6, and the red and purple lines in the right plot of Figure 7, underscores the indispensable roles of task assignment and equivariance. Their integration is pivotal for surmounting the benchmark's challenges, validating the SHNN method's design philosophy. To further substantiate the criticality of local symmetry, we embarked on ablation studies exploring the influence of local translation invariance with the following variants:\\n\\n- without subtraction, where no translation invariance implemented;\\n- with sparse root mean subtraction:\\n  \\\\[ \\\\vec{c} = \\\\frac{1}{N+M} \\\\sum_{i=1}^{N+M} \\\\vec{p}_i, \\\\]\\n- with dense body mean subtraction,\\n- with relative positions in assignment:\\n  \\\\[ \\\\vec{c} = \\\\vec{p}_{C(i)}, \\\\]\\n\\nThe results in Figure 8 clearly indicate that without achieving translation invariance or properly decoupling it, the model struggles to mitigate complexity challenges, leading to significantly poorer performance. This observation further emphasizes the pivotal importance of local symmetry.\\n\\n5.5. Analyses of Morphology-shared Policy\\nPrevious works have achieved generalization across agents with different morphologies using morphology-aware Graph Neural Networks (Wang et al., 2018; Huang et al., 2020). In addition to the previously mentioned SHTransformer (our plug-in applied to body-level Transformer control), we have also developed a new variant, denoted as SHGNN (our plug-in applied to body-level message passing control). This variant learns a shared policy network across different agents, detailed in Appendix C.6. As observed in Table 3 and Table 5, despite body-level message passing excelling in complex, multi-morphological environments by enhancing knowledge sharing and performance, its implementation significantly slows down the training process. Above all, our benchmark provides the community with a test bed that takes into account both agent-interaction and morphology-aware considerations.\\n\\n6. Limitations and Future Works\\nTo learn policies in 3D multi-entity physical environments, we propose the SHNN, a framework that uniquely integrates task assignment with local subequivariant message passing in a hierarchical structure. However, in the Team Sumo environments, as shown in Figure 11, the impact of assignment is not as significant as that of equivariance. Therefore, the interdependence of task assignment and equivariance necessitates a novel co-learnable formulation. Our model's reliance on the E (3) symmetry encompasses a broad range of physical interactions but does not extend to non-Euclidean or highly irregular environments not governed by classical physics. Additionally, our model predominantly leverages state-based inputs, which are often costly to acquire in practice due to the need for precision sensors. Addressing the challenge of applying our equivariance-focused approach to vision-based inputs remains an open area for future research.\"}"}
{"id": "hQpUhySEJi", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nThis work is funded by the National Science and Technology Major Project of the Ministry of Science and Technology of China (No.2018AAA0102903), and partly by THU-Bosch JCML Center. We sincerely thank the reviewers for their comments that significantly improved our paper's quality.\\n\\nImpact Statement\\n\\nThe research presented in this paper is a step toward advancing the field machine learning (Yin et al., 2024; Guan et al., 2024). While our contributions focus on the theoretical and technical advancements in machine learning, we acknowledge the broader societal implications as our work indirectly supports the development of technologies in delivery, transportation, and automated systems. The practical applications, such as in autonomous vehicles and drones, offer promising avenues for societal benefits, including improved safety and efficiency. While our work heralds technical progress, we are mindful of its societal impacts, notably privacy concerns from using detailed state data, emphasizing the need for robust policies to ensure these technologies' ethical and secure societal integration.\\n\\nReferences\\n\\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Morodatch, I. Emergent complexity via multi-agent competition. In International Conference on Learning Representations, 2018.\\n\\nBernstein, D. S., Givan, R., Immerman, N., and Zilberstein, S. The complexity of decentralized control of Markov decision processes. Mathematics of Operations Research, 27(4):819\u2013840, 2002.\\n\\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\\n\\nChen, R., Han, J., Sun, F., and Huang, W. Subequivariant graph reinforcement learning in 3d environment. In International Conference on Machine Learning. PMLR, 2023.\\n\\nChen, T., Murali, A., and Gupta, A. Hardware conditioned policies for multi-robot transfer learning. In Advances in Neural Information Processing Systems, volume 31, 2018.\\n\\nClaus, C. and Boutilier, C. The dynamics of reinforcement learning in cooperative multiagent systems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 1998, pp. 2, 1998.\\n\\nCohen, T. S. and Welling, M. Group equivariant convolutional networks. In International Conference on Machine Learning, volume 48, pp. 2990\u20132999. PMLR, 2016.\\n\\nCohen, T. S. and Welling, M. Steerable CNNs. In International Conference on Learning Representations, 2017.\\n\\nde Witt, C. S., Peng, B., Kamienny, P.-A., Torr, P., B\u00f6hm, W., and Whiteson, S. Deep multi-agent reinforcement learning for decentralized continuous cooperative control. arXiv preprint arXiv:2003.06709, 19, 2020.\\n\\nDeng, C., Litany, O., Duan, Y., Poulenard, A., Tagliasacchi, A., and Guibas, L. J. Vector neurons: A general framework for SO(3)-equivariant networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12200\u201312209, 2021.\\n\\nDeng, C., Lei, J., Shen, B., Daniilidis, K., and Guibas, L. Banana: Banach fixed-point network for pointcloud segmentation with inter-part equivariance. In Advances in Neural Information Processing Systems, 2023.\\n\\nD'Eramo, C., Tateo, D., Bonarini, A., Restelli, M., Peters, J., et al. Sharing knowledge in multi-task deep reinforcement learning. In International Conference on Learning Representations, 2020.\\n\\nDevin, C., Gupta, A., Darrell, T., Abbeel, P., and Levine, S. Learning modular neural network policies for multi-task and multi-robot transfer. In IEEE International Conference on Robotics and Automation, pp. 2169\u20132176. IEEE, 2017.\\n\\nDong, H., Wang, T., Liu, J., and Zhang, C. Low-rank modular reinforcement learning via muscle synergy. In Advances in Neural Information Processing Systems, 2022.\\n\\nDresselhaus, M. S., Dresselhaus, G., and Jorio, A. Group theory: application to the physics of condensed matter. Springer Science & Business Media, 2007.\\n\\nEllis, B., Cook, J., Moalla, S., Samvelyan, M., Sun, M., Mahajan, A., Foerster, J. N., and Whiteson, S. Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2212.07489, 2022.\\n\\nFoerster, J., Assael, I. A., De Freitas, N., and Whiteson, S. Learning to communicate with deep multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 29, 2016.\\n\\nFoerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\"}"}
{"id": "hQpUhySEJi", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\nFreeman, C. D., Frey, E., Raichuk, A., Girgin, S., Mordatch, I., and Bachem, O. Brax - a differentiable physics engine for large scale rigid body simulation, 2021. URL http://github.com/google/brax.\\n\\nFuchs, F., Worrall, D., Fischer, V., and Welling, M. Se(3)-transformers: 3d roto-translation equivariant attention networks. In Advances in Neural Information Processing Systems, volume 33, pp. 1970\u20131981. Curran Associates, Inc., 2020.\\n\\nFuruta, H., Iwasawa, Y., Matsuo, Y., and Gu, S. S. A system for morphology-task generalization via unified representation and behavior distillation. In International Conference on Learning Representations, 2023.\\n\\nGodwin*, J., Keck*, T., Battaglia, P., Bapst, V., Kipf, T., Li, Y., Stachenfeld, K., Veli\u010dkovi\u0107, P., and Sanchez-Gonzalez, A. Jraph: A library for graph neural networks in JAX., 2020. URL http://github.com/deepmind/jraph.\\n\\nGu, S. S., Diaz, M., Freeman, D. C., Furuta, H., Ghasemipour, S. K. S., Raichuk, A., David, B., Frey, E., Coumans, E., and Bachem, O. Braxlines: Fast and interactive toolkit for rl-driven behavior engineering beyond reward maximization. arXiv preprint arXiv:2110.04686, 2021.\\n\\nGuan, H., Cai, G., and Xu, H. Automatic requirement dependency extraction based on integrated active learning strategies. Machine Intelligence Research, pp. 1\u201318, 2024.\\n\\nGupta, A., Fan, L., Ganguli, S., and Fei-Fei, L. Metamorph: Learning universal controllers with transformers. In International Conference on Learning Representations, 2022.\\n\\nHan, J., Huang, W., Ma, H., Li, J., Tenenbaum, J. B., and Gan, C. Learning physical dynamics with subequivariant graph neural networks. In Advances in Neural Information Processing Systems, volume 35, pp. 26256\u201326268, 2022.\\n\\nHan, J., Cen, J., Wu, L., Li, Z., Kong, X., Jiao, R., Yu, Z., Xu, T., Wu, F., Wang, Z., et al. A survey of geometric graph neural networks: Data structures, models and applications. arXiv preprint arXiv:2403.00485, 2024.\\n\\nHeek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A., and van Zee, M. Flax: A neural network library and ecosystem for JAX, 2023. URL http://github.com/google/flax.\\n\\nHong, S., Yoon, D., and Kim, K.-E. Structure-aware transformer policy for inhomogeneous multi-task reinforcement learning. In International Conference on Learning Representations, 2021.\\n\\nHuang, W., Mordatch, I., and Pathak, D. One policy to control them all: Shared modular policies for agent-agnostic control. In International Conference on Machine Learning, pp. 4455\u20134464. PMLR, 2020.\\n\\nHuang, W., Han, J., Rong, Y., Xu, T., Sun, F., and Huang, J. Equivariant graph mechanics networks with constraints. In International Conference on Learning Representations, 2022.\\n\\nIqbal, S. and Sha, F. Actor-attention-critic for multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 2961\u20132970. PMLR, 2019.\\n\\nJeon, J., Kim, W., Jung, W., and Sung, Y. Maser: Multi-agent reinforcement learning with subgoals generated from experience replay buffer. In International Conference on Machine Learning, pp. 10041\u201310052. PMLR, 2022.\\n\\nJing, B., Eismann, S., Suriana, P., Townshend, R. J. L., and Dror, R. Learning from protein structure with geometric vector perceptrons. In International Conference on Learning Representations, 2020.\\n\\nJoshi, C. K., Bodnar, C., Mathis, S. V., Cohen, T., and Lio, P. On the expressive power of geometric graph neural networks. In International Conference on Machine Learning. PMLR, 2023.\\n\\nKraemer, L. and Banerjee, B. Multi-agent reinforcement learning as a rehearsal for decentralized planning. Neurocomputing, 190:82\u201394, 2016.\\n\\nKuba, J. G., Chen, R., Wen, M., Wen, Y., Sun, F., Wang, J., and Yang, Y. Trust region policy optimisation in multi-agent reinforcement learning. In International Conference on Learning Representations, 2021.\\n\\nKurin, V., Igl, M., Rockt\u00e4schel, T., Boehmer, W., and Whitehouse, S. My body is a cage: the role of morphology in graph-based incompatible control. In International Conference on Learning Representations, 2020.\\n\\nLechner, M., Yin, L., Seyde, T., Wang, T.-H., Xiao, W., Hasani, R., Rountree, J., and Rus, D. Gigastep\u2014one billion steps per second multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, 2023.\\n\\nLittman, M. L. Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pp. 157\u2013163. Elsevier, 1994.\\n\\nLiu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T. Emergent coordination through competition. In International Conference on Learning Representations, 2018.\"}"}
{"id": "hQpUhySEJi", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\n...Moreover, existing multi-agent reinforcement learning (MARL) environments (Samvelyan et al., 2019; de Witt et al., 2020; Ellis et al., 2022; Rutherford et al., 2023; Lechner et al., 2023) often lack scenarios that encompass both geometric symmetry and morphology-based control. We expand existing environments (Chen et al., 2023; Furuta et al., 2023) into a suite of new multi-entity benchmark (MeBen) in 3D space, not only facilitating a comprehensive exploration of multi-entity dynamics but also ensuring a realistic emulation of real-world scenarios through randomized initial conditions and orientations. Our work aligns with the Dec-POMDP framework and capitalizes on the strengths of MAPPO (Yu et al., 2022) to optimize policies effectively. Furthermore, our environment is implemented using JAX (Bradbury et al., 2018), ensuring efficient simulations on advanced hardware accelerators such as GPUs and TPUs.\\n\\nGeometrically Equivariant Models\\n\\nThe physical world exhibits specific symmetries, extensively explored in studies on group equivariant models (Cohen & Welling, 2016; Cohen & Welling, 2017; Worrall et al., 2017). Building upon this, the field of geometrically equivariant graph neural networks (Han et al., 2024) has emerged, utilizing symmetry as a fundamental bias in learning. These models are designed to ensure that outputs will rotate, translate, or reflect in the same manner as their inputs, thereby retaining inherent symmetries. Techniques such as group convolution via irreducible representation (Thomas et al., 2018; Fuchs et al., 2020) and invariant scalarization methods, like inner product computation (Villar et al., 2021; Satorras et al., 2021; Huang et al., 2022; Han et al., 2022), are employed to achieve this symmetry preservation. Our method, similar to GMN (Huang et al., 2022) and SGNN (Han et al., 2022), especially focuses on scalarization strategies. In Markov decision processes (MDPs) that exhibit symmetries (van der Pol et al., 2020), these symmetries in the state-action space enable the optimization of policies within a simplified abstract MDP. The work of van der Pol et al. (2020) concentrates on learning equivariant policies and invariant value networks in 2D environments. In contrast, Chen et al. (2023) explores body-level equivariant policy networks in more complex 3D physics environments, facilitating policy generalization across different directions. Our work diverges by introducing an entity-level subequivariant message-passing mechanism, which proves to be highly effective in 3D multi-body scenes (Han et al., 2022).\\n\\nC. More Experimental Details\\n\\nC.1. Subequivariant Function\\n\\nWe resort to subequivariant function with \\\\( \\\\vec{Z}, \\\\vec{g}, h \\\\) as input (Han et al., 2022; Chen et al., 2023) to instill desired geometric symmetry into the model:\\n\\n\\\\[\\n\\\\vec{Z}', h' = \\\\vec{M} \\\\vec{g} W \\\\vec{g}, h, \\\\\\\\\\n\\\\text{s.t. } W \\\\vec{g} = \\\\sigma(\\\\vec{M}^\\\\top \\\\vec{g} \\\\vec{M} \\\\vec{g}, h),\\n\\\\]\\n\\nwhere \\\\( \\\\vec{M} \\\\vec{g} = [\\\\vec{Z}, \\\\vec{g}] \\\\) is a mixing of the vectors to capture the interactions between channels, with a learnable weight matrix \\\\( W \\\\in \\\\mathbb{R}^{(m+1)\\\\times m} \\\\) and \\\\( [\\\\vec{Z}, \\\\vec{g}] \\\\in \\\\mathbb{R}^{3 \\\\times (m+1)} \\\\) is a stack of \\\\( \\\\vec{Z} \\\\) and \\\\( \\\\vec{g} \\\\) along the last dimension. The inner product \\\\( \\\\vec{M}^\\\\top \\\\vec{g} \\\\vec{M} \\\\vec{g} \\\\in \\\\mathbb{R}^{m \\\\times m} \\\\) is computed and concatenated with \\\\( h \\\\). The resultant invariant term is then transformed by a Multi-Layer Perceptron (MLP):\\n\\n\\\\[\\n\\\\mathbb{R}^{m \\\\times m} + h 7\\\\rightarrow \\\\mathbb{R}^{m \\\\times m}\\n\\\\]\\n\\nproducing \\\\( W \\\\vec{g} \\\\in \\\\mathbb{R}^{m \\\\times m} \\\\).\\n\\nC.2. Environments\\n\\nIn this subsection, we present the technical details involved in constructing our challenging M\\\\( \\\\text{EB} \\\\)EN. The environments utilized in this work are listed in Table 4.\\n\\nAgents\\n\\nIn our studies, we leverage a variety of morphologies, including ants, claws, and centipedes from MxT-Bench (Furuta et al., 2023), as well as unimals from Gupta et al. (2022). Significantly, the asymmetrical forms of centipedes and unimals have the potential to influence the overall system's symmetry and dynamics. This diversity in agent morphologies facilitates a nuanced exploration of agent dynamics within multi-entity, morphology-based RL environments.\\n\\nTeam Reach\\n\\nWe expand the \u201cReach\u201d task from a single-agent challenge to a collaborative \u201cTeam Reach\u201d task, as shown in Figure 9.\\n\\n1. Initial Conditions.\\n\\nEntities set \\\\( \\\\Omega \\\\) include \\\\( N \\\\) agents and \\\\( M \\\\) fixed balls, \\\\( N \\\\geq M \\\\). Within an area of radius \\\\( R \\\\), we randomly position \\\\( N \\\\) agents and \\\\( M \\\\) fixed balls, also setting the initial orientations of the agents randomly. Specific details for \\\\( N, M, R \\\\), and the agents' morphology are provided in Table 4.\\n\\n2. Termination.\\n\\nThe goal of this...\"}"}
{"id": "hQpUhySEJi", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\n3. Reward. The designed reward structure comprises four components:\\n\\na. Success Bonus: A significant sparse reward of 10,000 is awarded.\\n\\nb. Distance Reward: A dense reward to incentivize the achievement of the task objective. It is computed as $5 \\\\times \\\\prod_{j=1}^{M} \\\\exp(-\\\\text{dist}_j)$, where $\\\\text{dist}_j$ represents the distance from the nearest agent to ball $j$.\\n\\nc. Moving Reward: Designed to motivate agents to move closer to any ball, it is quantified as $0.2 \\\\times \\\\prod_{i=1}^{N} \\\\prod_{j=1}^{M} \\\\max(\\\\vec{v}_i \\\\cdot (\\\\vec{p}_j - \\\\vec{p}_i), 0)$.\\n\\nd. Control Cost: This penalty discourages agents from executing excessively large actions, and is calculated as $-0.2 \\\\times \\\\prod_{i=1}^{N} q \\\\prod_{k=1}^{K} (a_{i,k})^2$. Thus, the shared team reward is equal to the sum of the aforementioned rewards.\\n\\nTeam Sumo\\nWe evolve the \\\"Sumo\\\" task from a purely competitive challenge to a mixed cooperative-competitive \\\"Team Sumo\\\" task, as shown in Figure 10.\\n\\n1. Initial Conditions. Entities set $\\\\Omega$ comprise one fixed ball, $N$ agents forming Team 1, and another $M$ agents constituting Team 2. The sumo arena, a circle with radius $R$, has its center marked by the fixed ball. Around this fixed ball, within a radius of $R-1$, we randomly position $N$ agents from Team 1 and another $M$ from Team 2, ensuring each agent's orientation is also randomized. Specific details for $N$, $M$, $R$, and the agents' morphology are provided in Table 4.\\n\\n2. Termination. The objective is for either Team 1 or Team 2 to win by having an opposing team's agent disqualified, which occurs if it exceeds a distance $R$ from the fixed ball. The team with the disqualified agent loses, triggering the termination of the episode.\\n\\n3. Reward. The reward designed for Team 1 (for Team 2, simply swap $N$ and $M$) is divided into five parts:\\n\\na. Win Bonus: A sparse reward of 1000 for achieving the win condition.\\n\\nb. Lose Bonus: A sparse penalty of -1000 for the losing condition.\\n\\nc. Distance Reward: A dense reward to encourage achieving the task's objective, computed as $5 \\\\times \\\\prod_{i=1}^{N} \\\\prod_{j=1}^{M} \\\\max(\\\\vec{v}_i \\\\cdot (\\\\vec{p}_j - \\\\vec{p}_i), 0)$.\\n\\nd. Moving Reward: This motivates agents to move closer to any member of the opposing team, calculated as $5 \\\\times \\\\prod_{i=1}^{N} \\\\prod_{j=1}^{M} \\\\max(\\\\vec{v}_i \\\\cdot (\\\\vec{p}_j - \\\\vec{p}_i), 0)$.\\n\\ne. Control Cost: A penalty for excessively large actions by agents, quantified as $-0.1 \\\\times \\\\prod_{i=1}^{N} q \\\\prod_{k=1}^{K} (a_{i,k})^2$. In line with the Exploration Curriculum suggested by Bansal et al. (2018), the shared team reward is calculated as $\\\\alpha \\\\times \\\\text{dense reward} + (1-\\\\alpha) \\\\times \\\\text{sparse reward}$, where $\\\\alpha$ is a linear annealing factor. Agents train with the dense reward for 25% of the training epochs.\\n\\nC.3. Baselines\\nWe compare our method SHNN against mainstream neural networks, particularly MLP as utilized in (Furuta et al., 2023), and a variant employing heading normalization technique, denoted as MLP+HN (Chen et al., 2023).\"}"}
{"id": "hQpUhySEJi", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Full list of environments used in this work.\\n\\n| Benchmark Environment Morphology | N | M | R | (meter) |\\n|----------------------------------|---|---|---|---------|\\n| Team Reach                       | 1 | ant | 1 | 5 |\\n| Team Sumo                        | 1 | centipede | 1 | 3 |\\n|                                  | 2 | ants | 2 | 3 |\\n|                                  | 2 | ant, claw | 2 | 3 |\\n|                                  | 3 | ants vs ants | 3 | 4 |\\n\\nMLP\\nOur implementation of MLP consists of three linear layers. The middle two layers are activated by ReLU activation functions. The input layer is designed to accommodate the size of the observation space, while the output layer matches the dimensionality of the action space. The intermediate layers facilitate the non-linear transformation of input features. For specific hyperparameters, please refer to Table 8.\\n\\nMLP+HN\\nHeading Normalization technique has been widely utilized in the 3D RL literature. For instance, in morphology control, the presence of gravity allows for the normalization of state and action spaces in the heading (yaw) direction, as demonstrated in a recent work (Won et al., 2020; 2022; Chen et al., 2023). This heading normalization (HN) technique transforms the global coordinate frame into the LRF, enabling the input geometric information to be mapped to a rotation- and translation-invariant representation.\\n\\nSpecifically, we acquire the quaternion of the root body from the simulation environment and calculate the heading angle to construct the rotation matrix $O_i$. This matrix is then multiplied by $\\\\vec{Z}_{i,k}$ to transform it into an invariant representation.\\n\\nFinally, all invariant representations are input into the MLP. It is noteworthy that Chen et al. (2023) discusses the limitations of this technique in their appendix.\\n\\nAlgorithm 1\\nGreedy Bipartite Matching for Task Assignment\\n\\nInput: $\\\\vec{p}_i, 1, i \\\\in \\\\Omega, N \\\\{agents (Team0) set\\\\}$, $M \\\\{objects (Team1) set\\\\}$\\n\\nOutput: A local entity-level graph $G = (V, E)$\\n\\nInitialize the graph $G$ with vertices $V \\\\leftarrow \\\\Omega$ and edges $E \\\\leftarrow \\\\emptyset$\\n\\nInitialize the assignment labels $C \\\\leftarrow \\\\emptyset$\\n\\nInitialize distance matrix $D$ between $N$ and $M$\\n\\nfor each $i \\\\in N$ and $j \\\\in M$ do\\n\\n$D[i, j] \\\\leftarrow \\\\|\\\\vec{p}_j, 1 - \\\\vec{p}_i, 1\\\\|_2$\\n\\nend for\\n\\nfor each $j \\\\in M$ do\\n\\n$i = \\\\arg \\\\min_k D[k, j]$\\n\\n$D[i,:] \\\\leftarrow \\\\infty$ \\\\{Prevent re-matching\\\\}\\n\\nAdd $(i, j)$ to $E$: $C(i) = i, C(j) = i$\\n\\nend for\\n\\nreturn A local entity-level graph $G = (V, E)$\\n\\nC.4. Task Assignment\\nGreedy Bipartite Matching Algorithm\\nThe Greedy algorithm is a fundamental optimization algorithm, this part is about the process of applying the greedy matching algorithm to allocate a fixed ball (or an opposing agent) to each agent in the environments. Algorithm 1 provides a pseudo-code implementation of the Greedy Bipartite Matching algorithm.\"}"}
{"id": "hQpUhySEJi", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Additional Ablations on Assignment\\n\\nIn the Team Sumo environments, task assignment yields limited improvement. This could be attributed to the environments' complex mixture of cooperative and competitive dynamics, which are challenging to effectively decouple using bipartite matching methods, as demonstrated in Figure 11.\\n\\nAdditional Ablations on Equivariance\\n\\nIn the Team Sumo environments, task assignment leads to the formation of local graphs composed of triplets of agents from both sides and a fixed ball, making it challenging to determine the specific goal orientation learned by equivariant networks. Consequently, our method was primarily compared against HNN+HN. The experimental outcomes align with the main text findings, as demonstrated in Figure 12.\\n\\nImportance of Local Symmetry\\n\\nBy comparing the red and blue lines in the left plot of Figure 11 and in the right plot of Figure 12, it becomes evident that in the Team Sumo environments, the impact of assignment is not as significant as that of equivariance. Therefore, careful design of assignment strategies is crucial to harness the advantages offered by equivariance effectively.\\n\\nAdditional Ablations on Entity Abstraction\\n\\nFigure 13. Additional Ablations on Entity Abstraction. Training and Evaluation Curves in 1 ant, 1 centipede, and 3 ant_claw_centipede Team Reach Environments.\"}"}
{"id": "hQpUhySEJi", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\nAblation Study on Entity Abstraction\\n\\nThe additional ablation study examines the effects of using different abstraction for each entity. Our hierarchical structure dictates a representative abstraction for each entity, and within RL environments, the reward is often contingent on the root body's state. Utilizing the first (root) body as a representative aligns with the common computational practices in RL, focusing on the pivotal elements that influence agent behavior and reward structures. Moreover, representing entity-level information through the mean of all K bodies is a viable alternative, offering a more comprehensive local dynamic. The ablation study examines the effects of using the root body versus the average of all K bodies:\\n\\n- **Root body**\\n  \\n  \\\\[ \\\\vec{Z}_i \\\\text{ is assigned as } \\\\vec{Z}_i, \\\\quad h_i \\\\text{ is set as } [h_i, 1, \\\\vec{p}_{z_i}, 1] \\\\]\\n  \\n  \\\\[ \\\\vec{Z}_{ij} = [(\\\\vec{p}_j, 1 - \\\\vec{p}_i, 1), \\\\vec{Z}_i, \\\\vec{Z}_j] \\\\]\\n  \\n  \\\\[ h_{ij} = [\\\\|\\\\vec{p}_j, 1 - \\\\vec{p}_i, 1\\\\|_2, h_i, h_j] \\\\]\\n\\n- **Average of all K bodies**\\n  \\n  \\\\[ \\\\vec{Z}_i \\\\text{ is assigned as } \\\\frac{1}{K_i} \\\\sum_{k=1}^{K_i} \\\\vec{Z}_{i,k}, \\\\quad h_i \\\\text{ is set as } \\\\left[ \\\\frac{1}{K_i} \\\\sum_{k=1}^{K_i} h_{i,k}, \\\\frac{1}{K_i} \\\\sum_{k=1}^{K_i} \\\\vec{p}_{z_{i,k}} \\\\right] \\\\]\\n  \\n  \\\\[ \\\\vec{Z}_{ij} = \\\\left[ \\\\left( \\\\frac{1}{K_i} \\\\sum_{k=1}^{K_i} \\\\vec{p}_{j,k} - \\\\frac{1}{K_i} \\\\sum_{k=1}^{K_i} \\\\vec{p}_{i,k} \\\\right), \\\\vec{Z}_i, \\\\vec{Z}_j \\\\right] \\\\]\\n  \\n  \\\\[ h_{ij} = \\\\left[ \\\\|\\\\frac{1}{K_i} \\\\sum_{k=1}^{K_i} \\\\vec{p}_{j,k} - \\\\frac{1}{K_i} \\\\sum_{k=1}^{K_i} \\\\vec{p}_{i,k}\\\\|_2, h_i, h_j \\\\right] \\\\]\\n\\nThe empirical results in Figure 13, particularly for the asymmetric centipede scenario, reveal a significant performance gap, clearly demonstrating the superiority of using the root body's state over the average of all K bodies.\\n\\nFigure 14. Evaluations on Transformer Architecture. Training and Evaluation Curves in Team Reach Environments.\\n\\nC.6. Analyses of Morphology-shared Policy\\n\\nWe provide an extended example of our methodology applied to morphology tasks. Previous works have achieved generalization across agents with different morphologies using morphology-aware Graph Neural Networks (Wang et al., 2018; Huang et al., 2020).\\n\\nOur variant, referred to as SHGNN, replaces the body-level MLP with body-level message passing. This adaptation enables the learning of a shared policy network across different agents.\\n\\nFirst, we utilize the morphology topology information of each agent \\\\( i \\\\) to construct a body-level inner-entity graph, denoted as \\\\( G_i = (V_i, E_i) \\\\). For each body, \\\\( k \\\\in V_i \\\\), input node features are initialized using the body's state. Specifically, \\\\( \\\\vec{Z}_k \\\\) is assigned as \\\\( \\\\vec{Z}_{i,k} \\\\), and \\\\( h_k \\\\) is set as \\\\( [h_{i,k}, h_{i}', \\\\vec{p}_{z_{i,k}}] \\\\), where \\\\([ \\\\] \\\\) is the stack along the last dimension and \\\\( \\\\vec{p}_{z_{i,k}} \\\\) represents the projection of the coordinate \\\\( \\\\vec{p}_{i,k} \\\\) onto the \\\\( z \\\\)-axis. In a body-level overview, we denote our body-level message passing as the function \\\\( \\\\phi_b \\\\) that updates each body's node features given the input node features of all body and graph connectivity:\\n\\n\\\\[\\n\\\\left\\\\{ (\\\\vec{Z}_k', h_k') \\\\right\\\\}_{k=1}^{K_i} = \\\\phi_b \\\\left\\\\{ (\\\\vec{Z}_k, h_k) \\\\right\\\\}_{k=1}^{K_i}, E_i. \\\\quad (27)\\n\\\\]\\n\\nNotably, the unfolding of \\\\( \\\\phi_b \\\\) is similar to that of \\\\( \\\\phi_o \\\\), and will not be elaborated further here.\\n\\nFor each agent \\\\( i \\\\), the invariant actor policy \\\\( \\\\pi_{\\\\theta_i} \\\\) is defined as\\n\\n\\\\[\\n\\\\pi_{\\\\theta_i} = \\\\left\\\\{ \\\\pi_{\\\\theta_{i,k}} \\\\right\\\\}_{k=2}^{K_i} = \\\\left\\\\{ \\\\sigma_{\\\\pi} (O^\\\\top \\\\vec{Z}_k', h_k') \\\\right\\\\}_{k=2}^{K_i}, \\\\quad (28)\\n\\\\]\\n\\nwhere \\\\( \\\\sigma_{\\\\pi} \\\\) is a linear layer with bias. Here, \\\\( \\\\pi_{\\\\theta_i} \\\\in \\\\mathbb{R}^{2 \\\\times (K_i - 1)} \\\\) represents the location and scale parameters of a Normal Tanh Distribution for the \\\\((K_i - 1)\\\\) actuators of agent \\\\( i \\\\). Each actuator samples its corresponding torque \\\\( a_{i,k} \\\\in [-1, 1] \\\\) from this distribution.\"}"}
{"id": "hQpUhySEJi", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\nThe invariant critic value-function $V_{\\\\phi}$ remains as described in the main text. Within the same team, different agents share the weight parameters of the body-level message passing.\\n\\nAs observed in Figure 15, while body-level message passing struggles to learn effective control strategies for a single morphology (e.g., 2 ants), it interestingly excels in more complex, multi-morphological environments like 3 ant claw centipede. In such scenarios, knowledge sharing across different morphologies leads to mutual enhancement, significantly outperforming the body-level MLP approach. However, as indicated in Table 5, the implementation of message passing considerably slows down the training process.\\n\\n### C.7. Model Comparison: Parameters and Training Time\\n\\nTable 5 compares the parameters and Training Wall Times of our model with several model variants in the 3 ant claw centipede Team Reach Environments. Here, total timesteps are 50M. Since Transformer and GNN can share parameters across different morphologies, they have fewer parameters.\\n\\n| Model          | Parameters (M) | Training Wall Time (h) |\\n|----------------|----------------|------------------------|\\n| MLP+HN         | 1.759          | 0.299 \u00b1 0.003          |\\n| Transformer+HN | 0.416          | 1.545 \u00b1 0.001          |\\n| SHNN           | 0.772          | 1.302 \u00b1 0.012          |\\n| SHTransformer  | 0.711          | 2.487 \u00b1 0.014          |\\n| SHGNN          | 0.613          | 5.538 \u00b1 1.411          |\\n\\n### C.8. Equivariance Test\\n\\nWe conduct an experiment, as depicted in Figure 16, to evaluate the rotational generalization of both the baselines and our method. Training and evaluation are conducted in the fixed initial conditions of the 1 centipede and 2 ants Team Reach environments. Additionally, we conduct an evaluation with a 180\u00b0 rotation of the entire scene. The results presented in Table 6, with detailed curves provided in Figure 17, illustrate that both our SHNN method and the MLP+HN approach exhibit stable performance pre- and post-rotation. Conversely, MLP demonstrates rotational generalization for symmetric morphologies, such as ants, yet entirely lacks this capability with asymmetric morphologies like centipedes, evidenced by the underline in Table 6 and the green line in the first plot of Figure 17. These results empirically validate that both our SHNN method and the HN approach are rotation equivariance, which can robustly generalize to unseen rotation transformations.\\n\\n### Table 6. Equivariance Test\\n\\nWe report Success Rate (%) on the final step in Team Reach Environments.\\n\\n| Methods          | 1-centipede $0^\\\\circ$ | 1-centipede $180^\\\\circ$ | 2-ants $0^\\\\circ$ | 2-ants $180^\\\\circ$ |\\n|------------------|------------------------|-------------------------|------------------|---------------------|\\n| MLP              | 41.55 \u00b1 19.70         | 0.18 \u00b1 0.22             | 44.00 \u00b1 12.90    | 40.03 \u00b1 12.03       |\\n| MLP+HN           | 38.02 \u00b1 24.98         | 37.92 \u00b1 25.91           | 39.19 \u00b1 12.85    | 38.60 \u00b1 12.53       |\\n| SHNN             | 40.23 \u00b1 22.28         | 41.39 \u00b1 21.89           | 87.78 \u00b1 6.59     | 87.84 \u00b1 6.42        |\\n\\nThese results empirically validate that both our SHNN method and the HN approach are rotation equivariance, which can robustly generalize to unseen rotation transformations.\"}"}
{"id": "hQpUhySEJi", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\nFigure 16. Equivariance Test Scenarios. Training are conducted in the fixed initial conditions, a generalization evaluation is conducted with a 180\u00b0 rotation of the entire scene.\\n\\nFigure 17. Equivariance Test. Training and Evaluation Curves in Team Reach Environments. MLP+HN and SHNN, by explicitly addressing local transformations, exhibit overlapping curves in equivariance tests.\\n\\nC.9. Hyperparameters\\n\\nTable 7 and Table 8 provide the hyperparameters needed to replicate our experiments. Code and Environments are available on our project page: https://alpc91.github.io/SMERL/.\\n\\nTable 7. Hyperparameters of MAPPO.\\n\\n| Hyperparameter | Value |\\n|----------------|-------|\\n| total timesteps Team Reach | 50M / Team Sumo: 20M |\\n| eval frequency | 50 |\\n| num envs | 2048 |\\n| action repeat | 1 |\\n| batch size Team Reach | 1024 / Team Sumo: 128 |\\n| reward scaling | 1.0 |\\n| episode length | 1000 |\\n| entropy cost | 1e-2 |\\n| unroll length | 5 |\\n| discounting | 0.97 |\\n| learning rate | 3e-4 |\\n| num minibatches | 32 |\\n| num update epochs | 4 |\\n| gradient clipping | 0.1 |\\n| normalize observations | True |\\n\\nTable 8. Hyperparameters of Network.\\n\\n| Module | Hyperparameters | Value |\\n|--------|----------------|-------|\\n| MLP    | hidden dim     | 256   |\\n|        | output dim \u03c0   | 2 \u00d7 (K_i \u2212 1) / V |\\n|        | # linear layers | 3 |\\n|        | activation     | relu |\\n| Message Passing | hidden dim | 64 |\\n|        | vector dim     | 32 |\\n|        | # MLP layers   | 2 |\\n|        | activation     | relu |\\n|        | propagation steps | 2 |\\n| Transformer | model dim | 128 |\\n|        | feedforward dim | 256 |\\n|        | # layers       | 3 |\\n|        | # heads        | 2 |\\n|        | activation     | relu |\\n|        | transformer norm | LayerNorm |\\n|        | condition decoder | True |\\n|        | positional encoding | False |\"}"}
{"id": "hQpUhySEJi", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\nLiu, S., Lever, G., Wang, Z., Merel, J., Eslami, S. A., Hennes, D., Czarnecki, W. M., Tassa, Y., Omidshafiei, S., Abdolmaleki, A., et al. From motor control to team play in simulated humanoid football. *Science Robotics*, 7(69): eabo0235, 2022.\\n\\nLowe, R., Tamar, A., Harb, J., Pieter Abbeel, O., and Mordatch, I. Multi-agent actor-critic for mixed cooperative-competitive environments. In *Advances in Neural Information Processing Systems*, volume 30, 2017.\\n\\nLuo, S., Li, J., Guan, J., Su, Y., Cheng, C., Peng, J., and Ma, J. Equivariant point cloud analysis via learning orientations for message passing. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 18932\u201318941, 2022.\\n\\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. *Nature*, 518(7540): 529\u2013533, 2015.\\n\\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In *International Conference on Machine Learning*, pp. 1928\u20131937. PMLR, 2016.\\n\\nOliehoek, F. A., Amato, C., et al. *A concise introduction to decentralized POMDPs*, volume 1. Springer, 2016.\\n\\nPathak, D., Lu, C., Darrell, T., Isola, P., and Efros, A. A. Learning to control self-assembling morphologies: a study of generalization via modularity. In *Advances in Neural Information Processing Systems*, volume 32, 2019.\\n\\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and Whiteson, S. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In *International Conference on Machine Learning*, pp. 4295\u20134304. PMLR, 2018.\\n\\nRutherford, A., Ellis, B., Gallici, M., Cook, J., Lupu, A., Ingvarsson, G., Willi, T., Khan, A., de Witt, C. S., Souly, A., Bandyopadhyay, S., Samvelyan, M., Jiang, M., Lange, R. T., Whiteson, S., Lacerda, B., Hawes, N., Rocktaschel, T., Lu, C., and Foerster, J. N. Jaxmarl: Multi-agent rl environments in jax. *arXiv preprint arXiv:2311.10090*, 2023.\\n\\nSamvelyan, M., Rashid, T., Schroeder de Witt, C., Farquhar, G., Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H., Foerster, J., and Whiteson, S. The starcraft multi-agent challenge. In *International Conference on Autonomous Agents and Multiagent Systems*, pp. 2186\u20132188, 2019.\\n\\nSatorras, V. G., Hoogeboom, E., and Welling, M. E (n) equivariant graph neural networks. In *International Conference on Machine Learning*, pp. 9323\u20139332. PMLR, 2021.\\n\\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*, 2017.\\n\\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. *Nature*, 529(7587): 484\u2013489, 2016.\\n\\nSpelke, E. S. *What babies know: Core knowledge and composition volume 1*. Oxford University Press, 2022.\\n\\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambraldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J. Z., Tuyls, K., et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In *International Conference on Autonomous Agents and MultiAgent Systems*, pp. 2085\u20132087, 2018.\\n\\nThomas, N., Smidt, T., Kearnes, S., Yang, L., Li, L., Kohlfhoff, K., and Riley, P. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. *arXiv preprint arXiv:1802.08219*, 2018.\\n\\nvan der Pol, E., Worrall, D., van Hoof, H., Oliehoek, F., and Welling, M. Mdp homomorphic networks: Group symmetries in reinforcement learning. In *Advances in Neural Information Processing Systems*, volume 33, pp. 4199\u20134210, 2020.\\n\\nVillar, S., Hogg, D. W., Storey-Fisher, K., Yao, W., and Blum-Smith, B. Scalars are universal: Equivariant machine learning, structured like classical physics. In *Advances in Neural Information Processing Systems*, volume 34, pp. 28848\u201328863, 2021.\\n\\nWang, L., Chen, R., Wang, Y., Sun, F., Wang, X., Kai, S., Fu, G., Zhang, J., and Huang, W. Equivariant local reference frames for unsupervised non-rigid point cloud shape correspondence. *arXiv preprint arXiv:2404.00959*, 2024.\\n\\nWang, T., Liao, R., Ba, J., and Fidler, S. Nervenet: Learning structured policy with graph neural networks. In *International Conference on Learning Representations*, 2018.\\n\\nWen, M., Kuba, J., Lin, R., Zhang, W., Wen, Y., Wang, J., and Yang, Y. Multi-agent reinforcement learning is a sequence modeling problem. *Advances in Neural Information Processing Systems*, 35:16509\u201316521, 2022.\"}"}
{"id": "hQpUhySEJi", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\nWon, J., Gopinath, D., and Hodgins, J. A scalable approach to control diverse behaviors for physically simulated characters. ACM Transactions on Graphics (TOG), 39(4):33\u20131, 2020.\\n\\nWon, J., Gopinath, D., and Hodgins, J. Physics-based character controllers using conditional vaes. ACM Transactions on Graphics (TOG), 41(4):1\u201312, 2022.\\n\\nWorrall, D. E., Garbin, S. J., Turmukhambetov, D., and Brostow, G. J. Harmonic networks: Deep translation and rotation equivariance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5028\u20135037, 2017.\\n\\nXiong, Z., Beck, J., and Whiteson, S. Universal morphology control via contextual modulation. In International Conference on Machine Learning. PMLR, 2023.\\n\\nXu, Z., Bai, Y., Zhang, B., Li, D., and Fan, G. Haven: Hierarchical cooperative multi-agent reinforcement learning with dual coordination mechanism. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 11735\u201311743, 2023.\\n\\nYin, Q., Yu, T., Shen, S., Yang, J., Zhao, M., Ni, W., Huang, K., Liang, B., and Wang, L. Distributed deep reinforcement learning: A survey and a multi-player multi-agent learning toolbox. Machine Intelligence Research, pp. 1\u201320, 2024.\\n\\nYu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., and Wu, Y. The surprising effectiveness of ppo in cooperative multi-agent games. In Advances in Neural Information Processing Systems, volume 35, pp. 24611\u201324624, 2022.\"}"}
{"id": "hQpUhySEJi", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\nA. Proofs\\n\\nIn this section, we theoretically prove that our proposed SHNN ensures the final output action and critic value preserve the symmetry as desired.\\n\\nTheorem A.1.\\nThe learned entity-wise rotation matrix, denoted as $O_i = OP(\\\\vec{u}_i)$, are $SO_\\\\vec{g}(3)$-equivariant, satisfying any transformation $g \\\\in SO_\\\\vec{g}(3)$, $g \\\\cdot O_i = OP(g \\\\cdot \\\\vec{u}_i)$.\\n\\nProof.\\nTo prove that the entity-wise rotation matrix $O_i = OP(\\\\vec{u}_i)$ are $SO_\\\\vec{g}(3)$-equivariant, we need to show that under any transformation $g \\\\in SO_\\\\vec{g}(3)$, the transformation of $O_i$ through $g$ is equivariant to the rotation matrix obtained from the transformed vectors $g \\\\cdot \\\\vec{u}_i$.\\n\\nLet $g$ be a transformation in $SO_\\\\vec{g}(3)$, which includes a rotation $O$ along the direction of $\\\\vec{g}$. Specifically, the transformation is applied as follows:\\n\\n$$g \\\\cdot O_i = O \\\\vec{u}_i,$$\\n$$g \\\\cdot \\\\vec{u}_i = O \\\\vec{u}_i,$$\\n\\nLet $O^* = OP(g \\\\cdot \\\\vec{u}_i)$. By the properties of the orthogonalization process,\\n\\n$$\\\\vec{e}^*_{i1} = O \\\\vec{u}_i - \\\\langle O \\\\vec{u}_i, \\\\vec{e}^*_{i3} \\\\rangle \\\\vec{e}^*_{i3} / \\\\| O \\\\vec{u}_i - \\\\langle O \\\\vec{u}_i, \\\\vec{e}^*_{i3} \\\\rangle \\\\vec{e}^*_{i3} \\\\|,$$\\n$$\\\\vec{e}^*_{i2} = \\\\vec{e}^*_{i1} \\\\times \\\\vec{e}^*_{i3},$$\\n$$\\\\vec{e}^*_{i3} = [0, 0, 1]^T.$$\\n\\nSince $O_\\\\vec{g} = \\\\vec{g}$, $O^T O = I$ and $\\\\det(O) = 1$, it preserves inner products and norms. And, the cross product obeys the following identity under matrix transformations:\\n\\n$$(M \\\\vec{a}) \\\\times (M \\\\vec{b}) = (\\\\det M) M^{-1} (\\\\vec{a} \\\\times \\\\vec{b}).$$\\n\\nTherefore, $\\\\vec{e}^*_{i3} = O \\\\vec{e}_i^*_{i3}$, $\\\\| O \\\\vec{u}_i \\\\| = \\\\| \\\\vec{u}_i \\\\|$ and $\\\\langle O \\\\vec{u}_i, O \\\\vec{e}_i^*_{i3} \\\\rangle = \\\\langle \\\\vec{u}_i, \\\\vec{e}_i^*_{i3} \\\\rangle$. This implies:\\n\\n$$\\\\vec{e}^*_{i1} = O \\\\vec{u}_i - \\\\langle O \\\\vec{u}_i, O \\\\vec{e}_i^*_{i3} \\\\rangle O \\\\vec{e}_i^*_{i3} / \\\\| O \\\\vec{u}_i - \\\\langle O \\\\vec{u}_i, O \\\\vec{e}_i^*_{i3} \\\\rangle O \\\\vec{e}_i^*_{i3} \\\\| = O \\\\vec{u}_i - \\\\langle \\\\vec{u}_i, \\\\vec{e}_i^*_{i3} \\\\rangle \\\\vec{e}_i^*_{i3} / \\\\| \\\\vec{u}_i - \\\\langle \\\\vec{u}_i, \\\\vec{e}_i^*_{i3} \\\\rangle \\\\vec{e}_i^*_{i3} \\\\|,$$\\n$$\\\\vec{e}^*_{i2} = O \\\\vec{e}_i^*_{i1} \\\\times O \\\\vec{e}_i^*_{i3} = \\\\det(O) O \\\\vec{e}_i^*_{i1} \\\\times \\\\vec{e}_i^*_{i3} = O \\\\vec{e}_i^*_{i3},$$\\n\\nTherefore, with $O^* = OP(g \\\\cdot \\\\vec{u}_i) = [O \\\\vec{e}_i^*_{i1}, O \\\\vec{e}_i^*_{i2}, O \\\\vec{e}_i^*_{i3}] = O \\\\vec{e}_i$ where the equivariance is disrupted due to the properties of the cross product. Hence, although we utilize an $O_\\\\vec{g}(3)$-equivariant message passing network in Equation (8), the orthogonalization process can transform a $O_\\\\vec{g}(3)$-equivariant vector into a $SO_\\\\vec{g}(3)$-equivariant matrix.\\n\\nAs for the actor and critic, we additionally have the following corollary.\\n\\nCorollary A.2.\\nLet $\\\\pi^\\\\theta_i, V^\\\\phi$ be output of the actor and the critic of SHNN with $\\\\vec{Z}, \\\\vec{g}, h$ as input. Let $\\\\pi^{\\\\ast \\\\theta}_i, V^{\\\\ast \\\\phi}$ be the actor and critic with $O \\\\vec{Z}, O \\\\vec{g}, h$ as input, $O \\\\in SO_\\\\vec{g}(3)$. Then,\\n\\n$$(\\\\pi^\\\\ast \\\\theta_i, V^\\\\ast \\\\phi) = (\\\\pi^\\\\theta_i, V^\\\\phi),$$\\n\\nindicating the output actor and critic preserve $SO_\\\\vec{g}(3)$-invariance.\\n\\nProof.\\nGiven the subequivariance of Equation (8) as per (Han et al., 2022; Chen et al., 2023), we have $h_i^* = h_i^\\\\ast$. Hence,\\n\\n$$V^\\\\ast \\\\phi = \\\\sigma V\\\\left(\\\\left\\\\{h_i^\\\\ast\\\\right\\\\}_{N+M_i=1}\\\\right) = V^\\\\phi.$$\"}"}
{"id": "hQpUhySEJi", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\nThen, we get $o^*_{i}$ with $O_{\\\\vec{Z}}, O_{\\\\vec{g}}, h$ as input:\\n\\n$$o^*_{i} = \\\\left[ O_{\\\\vec{Z}}_{i,k}, h_{i,k} \\\\right]_{K_{i,k}=1}, O_{\\\\vec{Z}}_{j,1}, h_{j,1} \\\\right]. \\\\quad (17)$$\\n\\nBy Theorem A.1, we can obtain $O^*_{i}$ with $O_{\\\\vec{Z}}, O_{\\\\vec{g}}, h$ as input.\\n\\nTherefore, the LRF invariant observation inputs $o'_{i}^*$ with $O_{\\\\vec{Z}}, O_{\\\\vec{g}}, h$ as input:\\n\\n$$o'_{i}^* = O_{i}^* o_{i}^{\\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top \\\\top"}
{"id": "hQpUhySEJi", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nLearning policies for multi-entity systems in 3D environments is far more complicated against single-entity scenarios, due to the exponential expansion of the global state space as the number of entities increases. One potential solution of alleviating the exponential complexity is dividing the global space into independent local views that are invariant to transformations including translations and rotations. To this end, this paper proposes Subequivariant Hierarchical Neural Networks (SHNN) to facilitate multi-entity policy learning. In particular, SHNN first dynamically decouples the global space into local entity-level graphs via task assignment. Second, it leverages subequivariant message passing over the local entity-level graphs to devise local reference frames, remarkably compressing the representation redundancy, particularly in gravity-affected environments. Furthermore, to overcome the limitations of existing benchmarks in capturing the subtleties of multi-entity systems under the Euclidean symmetry, we propose the Multi-entity Benchmark (MEB), a new suite of environments tailored for exploring a wide range of multi-entity reinforcement learning. Extensive experiments demonstrate significant advancements of SHNN on the proposed benchmarks compared to existing methods. Comprehensive ablations are conducted to verify the indispensability of task assignment and subequivariance.\\n\\n1. Introduction\\nLearning to navigate, control, cooperate, and compete in the 3D physical world is a fundamental task in developing intelligent agents. Deep reinforcement learning (RL) has made impressive breakthroughs, particularly in single-entity systems, with agent policies evolving through environmental interactions (Mnih et al., 2015; Silver et al., 2016; Mnih et al., 2016; Schulman et al., 2017; Bansal et al., 2018; Liu et al., 2018; 2022). However, an intricate challenge is generalizing across configurations like transformations, morphologies, and tasks, which are interlinked and complicate the learning process. In particular, multi-entity systems, which include agents, objects, and other entities defined (Spelke, 2022), present considerable challenges compared to single-entity scenarios, partly due to exponential expansion of global transformations as the number of entities increases (Deng et al., 2023). In Figure 1, for example, any horizontal rotation of the entities, although producing different global coordinates and representations, does not change the essential geometry and their local views of different entities. Such symmetry, defined as subequivariance,\\n\\nFigure 1. Illustration of the symmetry in our 3D multi-entity physical environments. In this example, there are two agents (red and blue) navigating towards two objects (grey). To mitigate the exponential-growth complexity, we conduct task assignment to decouple the whole state space into local views (the orange and green transparent coordinate frames), where one agent is assigned for one object. These local views can be represented by local reference frame (LRF), leading to representations that are independent of any translation, or rotation around the gravity direction of the global coordinates of the entities. Such symmetry is called $E(3)$ subequivariance, distinct from conventional $E(3)$ equivariance, accounting for gravitational effects. In form, subequivariance is encapsulated by the group $\\\\mathbf{E}\\\\,\\\\mathbf{g}(3)$\u2014translations/rotations/reflections along gravity $\\\\mathbf{g}$ (a 3D Euclidean subgroup of $E(3)$ in Section 2). Codes are available on our project page: https://alpc91.github.io/SMERL/.\"}"}
{"id": "hQpUhySEJi", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments (formal definition is given in Section 2) in this paper, provides a potential way to reduce the complexity of the state space. There are certain previous studies leveraging this type of symmetry in RL. For instance, the methods based on heading normalization (HN) (Won et al., 2020; 2022) transform global coordinates into a local reference frame (LRF), which is yet non-learnable and non-adjustable with respect to the goal of the task. Morphology-based RL advances (Chen et al., 2023) for single entity have incorporated subequivariance (Han et al., 2022) into policy modeling, reducing reliance on hand-crafted LRFs. However, extending subequivariance from single-entity to inter-entity transformations reveals unexplored challenges due to the coupled local space of each entity.\\n\\nTo tackle the difficulties of interdependence and generalization, we introduce a novel framework, named Subequivariant Hierarchical Neural Networks (SHNN), integrating task assignment and local entity-level subequivariant message passing within a hierarchical network architecture. SHNN offers two key advancements:\\n\\n1. We implement the task assignment using bipartite graph matching to dynamically construct local entity-level graphs. This approach aids in managing interdependence among entities by decoupling local transformations from the overall structure.\\n\\n2. We implement local entity-level subequivariant message passing, effectively compiling information from related entities. To guide body-level control, we utilize entity-level information to define a LRF for each entity, effectively compressing the global state space and facilitating the generalization of body-level policy in a lossless way.\\n\\nMoreover, another reason for this limited exploration is the absence of suitable environments in existing morphology-based (Huang et al., 2020; Chen et al., 2023; Furuta et al., 2023) and multi-agent reinforcement learning (MARL) (Samvelyan et al., 2019; de Witt et al., 2020; Ellis et al., 2022; Rutherford et al., 2023; Lechner et al., 2023) frameworks. These benchmarks inadequately probe complex entity interactions, especially in scenarios involving multi-agent dynamics under a diverse range of inter-entity transformations. To bridge this gap, we present a new suite of Multi-entity Benchmark (MEBEN) in 3D space. Built upon JAX-based RL environments (Bradbury et al., 2018; Godwin* et al., 2020; Heek et al., 2023; Freeman et al., 2021; Gu et al., 2021), MEBEN is designed to investigate multi-entity interactions, encompassing both cooperative and competitive dynamics, within physical geometric symmetry constraints that include a diverse range of inter-entity transformations.\\n\\nOur contributions are summarized as follows:\\n\\n\u2022 To effectively optimize the policy in 3D multi-entity physical environments, we propose SHNN, a novel framework that offers a superior plug-in alternative to hand-crafted LRFs. It decouples local transformations from the overall structure and compresses the state space by leveraging local physical geometric symmetry, particularly in gravity-affected environments.\\n\\n\u2022 We introduce MEBEN, a collection of subequivariant morphology-based MARL environments, designed for in-depth exploration of multi-entity interactions within physical geometric symmetry constraints. These environments, including a diverse range of inter-entity transformations, facilitate both cooperative and competitive dynamics.\\n\\n\u2022 We demonstrate the effectiveness of SHNN in the proposed 3D multi-entity physical environments, including Team Reach and Team Sumo. Our extensive ablations and comparative analyses also reveal the efficacy of the proposed ideas.\\n\\n2. Preliminaries\\n\\nGeometric Symmetry\\n\\nThe symmetrical structure in 3D environments is E(3), which is a 3-dimensional Euclidean group (Dresselhaus et al., 2007) that consists of rotations, reflections, and translations.\\n\\nDefinition 2.1 (Group). A group \\\\( G \\\\) is a set of transformations with a binary operation \\\\( \\\\cdot \\\\) satisfying these properties:\\n\\n- \\\\( \\\\cdot \\\\) is closed under associative composition, there exists an identity element, and each element must have an inverse.\\n\\nSymmetrical structure enforced on the model (Worrall et al., 2017; van der Pol et al., 2020; Thomas et al., 2018; Fuchs et al., 2020; Jing et al., 2020; Deng et al., 2021; Villar et al., 2021; Satorras et al., 2021; Huang et al., 2022; Han et al., 2022; Luo et al., 2022; Chen et al., 2023; Joshi et al., 2023; Wang et al., 2024; Han et al., 2024) is formally described by the concept of equivariance.\\n\\nDefinition 2.2 (Equivariance). Suppose \\\\( \\\\mathbf{Z} \\\\) to be 3D geometric vectors (positions, velocities, etc) that are steerable by a group \\\\( G \\\\), and \\\\( h \\\\) non-steerable features. The function \\\\( f \\\\) is \\\\( G \\\\)-equivariant, if for any transformation \\\\( g \\\\in G \\\\),\\n\\n\\\\[\\n    f(g \\\\cdot \\\\mathbf{Z}, h) = g \\\\cdot f(\\\\mathbf{Z}, h),\\n\\\\]\\n\\n\\\\( \\\\forall \\\\mathbf{Z} \\\\in \\\\mathbb{R}^3 \\\\times m, h \\\\in \\\\mathbb{R}^d \\\\). Similarly, \\\\( f \\\\) is invariant if\\n\\n\\\\[\\n    f(g \\\\cdot \\\\mathbf{Z}, h) = f(\\\\mathbf{Z}, h).\\n\\\\]\\n\\nSpecifically, the E(3) operation \\\\( \\\\cdot \\\\) is instantiated as\\n\\n\\\\[\\n    g \\\\cdot \\\\mathbf{Z} := O \\\\mathbf{Z}\\n\\\\]\\n\\nfor the orthogonal group that consists of rotations and reflections where \\\\( O \\\\in O(3) := \\\\{ O \\\\in \\\\mathbb{R}^{3 \\\\times 3} | O^\\\\top O = I \\\\} \\\\), and is additionally implemented as the translation\\n\\n\\\\[\\n    g \\\\cdot \\\\mathbf{x} := \\\\mathbf{x} + \\\\mathbf{t}\\n\\\\]\\n\\nfor the 3D coordinate vector where \\\\( \\\\mathbf{t} \\\\in T(3) := \\\\{ \\\\mathbf{t} \\\\in \\\\mathbb{R}^3 \\\\} \\\\). To align with the principles of classical physics under the influence of gravity, we introduce a relaxation of the group constraint. Particularly, we consider equivariance\\n\\n1 For detailed task settings, see Section 4.1.\\n\\n2 Note that for the input of \\\\( f \\\\), we have added the right-arrow superscript on \\\\( \\\\mathbf{Z} \\\\) to distinguish it from the scalar \\\\( h \\\\) that is unaffected by the transformations.\"}"}
{"id": "hQpUhySEJi", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yu et al., 2022) is employed to optimize a joint policy within the subgroup of E. Thus, the complete observation for agent \\\\(i\\\\) for each agent \\\\(i\\\\) involves selecting an action \\\\(a_i\\\\) based on its policy \\\\(\\\\pi_i\\\\) at time \\\\(t\\\\). Consequently, the aggregated action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\) is for each agent \\\\(i\\\\), denoted as \\\\(a_t^i\\\\). Each actuator \\\\(i\\\\), ranging from 2 to \\\\(\\\\pi\\\\), contributes by generating a torque \\\\(\\\\vec{g}_t\\\\) of \\\\(\\\\vec{g}_{t}^i\\\\) as \\\\(a_t\\\\) for the global state \\\\(s_t\\\\). By this means, \\\\(\\\\vec{g}_t = \\\\sum_{i=2}^{\\\\pi} \\\\vec{g}_t^i\\\\). We term \\\\(\\\\vec{g}_t\\\\) as \\\"torso\\\", \\\"limb\\\", or \\\"ball\\\". The direction of gravity \\\\(\\\\vec{g}_t\\\\) is confined to states of \\\\(s_t\\\\) of cooperative agents, while \\\\(s_t\\\\) is transformed into a translation-invariant representation by redefining it as \\\\(g_t\\\\) from 2 to \\\\(\\\\pi\\\\) which incorporates detailed state information about objects (or competitive agents). Agent \\\\(i\\\\)'s awareness of other entities is confined to states of \\\\(s_t\\\\) via \\\\(o_t\\\\), denoted as \\\\(o_t^i\\\\). Each agent \\\\(i\\\\) has a unique observation \\\\(o_t^i\\\\) at time \\\\(t\\\\). Involving selecting an action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\), the environment reacts by transitioning \\\\(s_t\\\\) to \\\\(s_{t+1}\\\\) and allocating a shared team reward \\\\(r_t\\\\) to a new global state \\\\(s_{t+1}\\\\). The environment is typically categorized into single-agent, cooperative, competitive, and mixed interactions. To effectively navigate this intricate setting as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002; Oliehoek et al., 2016), represented by \\\\(E = \u03a9 = \\\\{S, A, P, O, r, \u03b3\\\\}\\\\), the global state \\\\(s_t\\\\) is formulated as \\\\(o_t = o_{t|s_t}\\\\) of cooperative agents, while \\\\(s_t\\\\) is categorized into single-agent, cooperative, competitive, and mixed interactions. Thus, the complete observation for agent \\\\(i\\\\) for each agent \\\\(i\\\\) involves selecting an action \\\\(a_t\\\\) based on its policy \\\\(\\\\pi_i\\\\) at time \\\\(t\\\\). Consequently, the aggregated action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\) is for each agent \\\\(i\\\\), denoted as \\\\(a_t^i\\\\). Each actuator \\\\(i\\\\), ranging from 2 to \\\\(\\\\pi\\\\), contributes by generating a torque \\\\(\\\\vec{g}_t\\\\) of \\\\(\\\\vec{g}_{t}^i\\\\) as \\\\(a_t\\\\) for the global state \\\\(s_t\\\\). By this means, \\\\(\\\\vec{g}_t = \\\\sum_{i=2}^{\\\\pi} \\\\vec{g}_t^i\\\\). We term \\\\(\\\\vec{g}_t\\\\) as \\\"torso\\\", \\\"limb\\\", or \\\"ball\\\". The direction of gravity \\\\(\\\\vec{g}_t\\\\) is confined to states of \\\\(s_t\\\\) via \\\\(o_t\\\\), denoted as \\\\(o_t^i\\\\). Each agent \\\\(i\\\\) has a unique observation \\\\(o_t^i\\\\) at time \\\\(t\\\\). Involving selecting an action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\), the environment reacts by transitioning \\\\(s_t\\\\) to \\\\(s_{t+1}\\\\) and allocating a shared team reward \\\\(r_t\\\\) to a new global state \\\\(s_{t+1}\\\\). The environment is typically categorized into single-agent, cooperative, competitive, and mixed interactions. To effectively navigate this intricate setting as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002; Oliehoek et al., 2016), represented by \\\\(E = \u03a9 = \\\\{S, A, P, O, r, \u03b3\\\\}\\\\), the global state \\\\(s_t\\\\) is formulated as \\\\(o_t = o_{t|s_t}\\\\) of cooperative agents, while \\\\(s_t\\\\) is categorized into single-agent, cooperative, competitive, and mixed interactions. Thus, the complete observation for agent \\\\(i\\\\) for each agent \\\\(i\\\\) involves selecting an action \\\\(a_t\\\\) based on its policy \\\\(\\\\pi_i\\\\) at time \\\\(t\\\\). Consequently, the aggregated action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\) is for each agent \\\\(i\\\\), denoted as \\\\(a_t^i\\\\). Each actuator \\\\(i\\\\), ranging from 2 to \\\\(\\\\pi\\\\), contributes by generating a torque \\\\(\\\\vec{g}_t\\\\) of \\\\(\\\\vec{g}_{t}^i\\\\) as \\\\(a_t\\\\) for the global state \\\\(s_t\\\\). By this means, \\\\(\\\\vec{g}_t = \\\\sum_{i=2}^{\\\\pi} \\\\vec{g}_t^i\\\\). We term \\\\(\\\\vec{g}_t\\\\) as \\\"torso\\\", \\\"limb\\\", or \\\"ball\\\". The direction of gravity \\\\(\\\\vec{g}_t\\\\) is confined to states of \\\\(s_t\\\\) via \\\\(o_t\\\\), denoted as \\\\(o_t^i\\\\). Each agent \\\\(i\\\\) has a unique observation \\\\(o_t^i\\\\) at time \\\\(t\\\\). Involving selecting an action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\), the environment reacts by transitioning \\\\(s_t\\\\) to \\\\(s_{t+1}\\\\) and allocating a shared team reward \\\\(r_t\\\\) to a new global state \\\\(s_{t+1}\\\\). The environment is typically categorized into single-agent, cooperative, competitive, and mixed interactions. To effectively navigate this intricate setting as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002; Oliehoek et al., 2016), represented by \\\\(E = \u03a9 = \\\\{S, A, P, O, r, \u03b3\\\\}\\\\), the global state \\\\(s_t\\\\) is formulated as \\\\(o_t = o_{t|s_t}\\\\) of cooperative agents, while \\\\(s_t\\\\) is categorized into single-agent, cooperative, competitive, and mixed interactions. Thus, the complete observation for agent \\\\(i\\\\) for each agent \\\\(i\\\\) involves selecting an action \\\\(a_t\\\\) based on its policy \\\\(\\\\pi_i\\\\) at time \\\\(t\\\\). Consequently, the aggregated action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\) is for each agent \\\\(i\\\\), denoted as \\\\(a_t^i\\\\). Each actuator \\\\(i\\\\), ranging from 2 to \\\\(\\\\pi\\\\), contributes by generating a torque \\\\(\\\\vec{g}_t\\\\) of \\\\(\\\\vec{g}_{t}^i\\\\) as \\\\(a_t\\\\) for the global state \\\\(s_t\\\\). By this means, \\\\(\\\\vec{g}_t = \\\\sum_{i=2}^{\\\\pi} \\\\vec{g}_t^i\\\\). We term \\\\(\\\\vec{g}_t\\\\) as \\\"torso\\\", \\\"limb\\\", or \\\"ball\\\". The direction of gravity \\\\(\\\\vec{g}_t\\\\) is confined to states of \\\\(s_t\\\\) via \\\\(o_t\\\\), denoted as \\\\(o_t^i\\\\). Each agent \\\\(i\\\\) has a unique observation \\\\(o_t^i\\\\) at time \\\\(t\\\\). Involving selecting an action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\), the environment reacts by transitioning \\\\(s_t\\\\) to \\\\(s_{t+1}\\\\) and allocating a shared team reward \\\\(r_t\\\\) to a new global state \\\\(s_{t+1}\\\\). The environment is typically categorized into single-agent, cooperative, competitive, and mixed interactions. To effectively navigate this intricate setting as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002; Oliehoek et al., 2016), represented by \\\\(E = \u03a9 = \\\\{S, A, P, O, r, \u03b3\\\\}\\\\), the global state \\\\(s_t\\\\) is formulated as \\\\(o_t = o_{t|s_t}\\\\) of cooperative agents, while \\\\(s_t\\\\) is categorized into single-agent, cooperative, competitive, and mixed interactions. Thus, the complete observation for agent \\\\(i\\\\) for each agent \\\\(i\\\\) involves selecting an action \\\\(a_t\\\\) based on its policy \\\\(\\\\pi_i\\\\) at time \\\\(t\\\\). Consequently, the aggregated action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\) is for each agent \\\\(i\\\\), denoted as \\\\(a_t^i\\\\). Each actuator \\\\(i\\\\), ranging from 2 to \\\\(\\\\pi\\\\), contributes by generating a torque \\\\(\\\\vec{g}_t\\\\) of \\\\(\\\\vec{g}_{t}^i\\\\) as \\\\(a_t\\\\) for the global state \\\\(s_t\\\\). By this means, \\\\(\\\\vec{g}_t = \\\\sum_{i=2}^{\\\\pi} \\\\vec{g}_t^i\\\\). We term \\\\(\\\\vec{g}_t\\\\) as \\\"torso\\\", \\\"limb\\\", or \\\"ball\\\". The direction of gravity \\\\(\\\\vec{g}_t\\\\) is confined to states of \\\\(s_t\\\\) via \\\\(o_t\\\\), denoted as \\\\(o_t^i\\\\). Each agent \\\\(i\\\\) has a unique observation \\\\(o_t^i\\\\) at time \\\\(t\\\\). Involving selecting an action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\), the environment reacts by transitioning \\\\(s_t\\\\) to \\\\(s_{t+1}\\\\) and allocating a shared team reward \\\\(r_t\\\\) to a new global state \\\\(s_{t+1}\\\\). The environment is typically categorized into single-agent, cooperative, competitive, and mixed interactions. To effectively navigate this intricate setting as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002; Oliehoek et al., 2016), represented by \\\\(E = \u03a9 = \\\\{S, A, P, O, r, \u03b3\\\\}\\\\), the global state \\\\(s_t\\\\) is formulated as \\\\(o_t = o_{t|s_t}\\\\) of cooperative agents, while \\\\(s_t\\\\) is categorized into single-agent, cooperative, competitive, and mixed interactions. Thus, the complete observation for agent \\\\(i\\\\) for each agent \\\\(i\\\\) involves selecting an action \\\\(a_t\\\\) based on its policy \\\\(\\\\pi_i\\\\) at time \\\\(t\\\\). Consequently, the aggregated action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\) is for each agent \\\\(i\\\\), denoted as \\\\(a_t^i\\\\). Each actuator \\\\(i\\\\), ranging from 2 to \\\\(\\\\pi\\\\), contributes by generating a torque \\\\(\\\\vec{g}_t\\\\) of \\\\(\\\\vec{g}_{t}^i\\\\) as \\\\(a_t\\\\) for the global state \\\\(s_t\\\\). By this means, \\\\(\\\\vec{g}_t = \\\\sum_{i=2}^{\\\\pi} \\\\vec{g}_t^i\\\\). We term \\\\(\\\\vec{g}_t\\\\) as \\\"torso\\\", \\\"limb\\\", or \\\"ball\\\". The direction of gravity \\\\(\\\\vec{g}_t\\\\) is confined to states of \\\\(s_t\\\\) via \\\\(o_t\\\\), denoted as \\\\(o_t^i\\\\). Each agent \\\\(i\\\\) has a unique observation \\\\(o_t^i\\\\) at time \\\\(t\\\\). Involving selecting an action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\), the environment reacts by transitioning \\\\(s_t\\\\) to \\\\(s_{t+1}\\\\) and allocating a shared team reward \\\\(r_t\\\\) to a new global state \\\\(s_{t+1}\\\\). The environment is typically categorized into single-agent, cooperative, competitive, and mixed interactions. To effectively navigate this intricate setting as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002; Oliehoek et al., 2016), represented by \\\\(E = \u03a9 = \\\\{S, A, P, O, r, \u03b3\\\\}\\\\), the global state \\\\(s_t\\\\) is formulated as \\\\(o_t = o_{t|s_t}\\\\) of cooperative agents, while \\\\(s_t\\\\) is categorized into single-agent, cooperative, competitive, and mixed interactions. Thus, the complete observation for agent \\\\(i\\\\) for each agent \\\\(i\\\\) involves selecting an action \\\\(a_t\\\\) based on its policy \\\\(\\\\pi_i\\\\) at time \\\\(t\\\\). Consequently, the aggregated action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\) is for each agent \\\\(i\\\\), denoted as \\\\(a_t^i\\\\). Each actuator \\\\(i\\\\), ranging from 2 to \\\\(\\\\pi\\\\), contributes by generating a torque \\\\(\\\\vec{g}_t\\\\) of \\\\(\\\\vec{g}_{t}^i\\\\) as \\\\(a_t\\\\) for the global state \\\\(s_t\\\\). By this means, \\\\(\\\\vec{g}_t = \\\\sum_{i=2}^{\\\\pi} \\\\vec{g}_t^i\\\\). We term \\\\(\\\\vec{g}_t\\\\) as \\\"torso\\\", \\\"limb\\\", or \\\"ball\\\". The direction of gravity \\\\(\\\\vec{g}_t\\\\) is confined to states of \\\\(s_t\\\\) via \\\\(o_t\\\\), denoted as \\\\(o_t^i\\\\). Each agent \\\\(i\\\\) has a unique observation \\\\(o_t^i\\\\) at time \\\\(t\\\\). Involving selecting an action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\), the environment reacts by transitioning \\\\(s_t\\\\) to \\\\(s_{t+1}\\\\) and allocating a shared team reward \\\\(r_t\\\\) to a new global state \\\\(s_{t+1}\\\\). The environment is typically categorized into single-agent, cooperative, competitive, and mixed interactions. To effectively navigate this intricate setting as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002; Oliehoek et al., 2016), represented by \\\\(E = \u03a9 = \\\\{S, A, P, O, r, \u03b3\\\\}\\\\), the global state \\\\(s_t\\\\) is formulated as \\\\(o_t = o_{t|s_t}\\\\) of cooperative agents, while \\\\(s_t\\\\) is categorized into single-agent, cooperative, competitive, and mixed interactions. Thus, the complete observation for agent \\\\(i\\\\) for each agent \\\\(i\\\\) involves selecting an action \\\\(a_t\\\\) based on its policy \\\\(\\\\pi_i\\\\) at time \\\\(t\\\\). Consequently, the aggregated action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\) is for each agent \\\\(i\\\\), denoted as \\\\(a_t^i\\\\). Each actuator \\\\(i\\\\), ranging from 2 to \\\\(\\\\pi\\\\), contributes by generating a torque \\\\(\\\\vec{g}_t\\\\) of \\\\(\\\\vec{g}_{t}^i\\\\) as \\\\(a_t\\\\) for the global state \\\\(s_t\\\\). By this means, \\\\(\\\\vec{g}_t = \\\\sum_{i=2}^{\\\\pi} \\\\vec{g}_t^i\\\\). We term \\\\(\\\\vec{g}_t\\\\) as \\\"torso\\\", \\\"limb\\\", or \\\"ball\\\". The direction of gravity \\\\(\\\\vec{g}_t\\\\) is confined to states of \\\\(s_t\\\\) via \\\\(o_t\\\\), denoted as \\\\(o_t^i\\\\). Each agent \\\\(i\\\\) has a unique observation \\\\(o_t^i\\\\) at time \\\\(t\\\\). Involving selecting an action \\\\(a_t\\\\) from the global state \\\\(s_t\\\\), the environment reacts by transitioning \\\\(s_t\\\\) to \\\\(s_{t+1}\\\\) and allocating a shared team reward \\\\(r_t\\\\) to a new global state \\\\(s_{t+1}\\\\). The environment is typically categorized into single-agent, cooperative, competitive, and mixed interactions. To effectively navigate this intricate setting as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002; Oliehoek et al., 2016), represented by \\\\(E = \u03a9 = \\\\{S, A, P, O, r, \u03b3\\\\}\\\\), the global state \\\\(s_t\\\\) is formulated as \\\\(o_t = o_{t|s_t}\\\\) of cooperative agents, while \\\\(s_t\\\\) is categorized into single-agent, cooperative, competitive, and mixed interactions. Thus, the complete observation for agent \\\\(i\\\\) for each agent \\\\(i\\\\) involves selecting an"}
{"id": "hQpUhySEJi", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The flowchart of SHNN. On the left, the states of each entity are shown as the ground as the reference point. For each entity $i$, a body-level control policy is obtained. Here, function vectors $\\\\vec{g}$ serve as the local entity-level subequivariant MP. It updates each entity's node features by considering the collective input features and the established graph connectivity.\\n\\nSpecifically, $\\\\phi$ selects the ground as the reference point. The right side illustrates our key innovation: the dynamic task assignment leveraging bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching (refer to Algorithm 1).\\n\\nWe propose enhancing mainstream Neural Networks (such as MLPs) in RL with an additional local entity-level subequivariant message passing (MP). This task assignment entity-level graph adeptly integrates local entity-level information from related entities and, through local subequivariant MP, efficiently instills the desired local physical geometric symmetry in environments with gravity into mainstream Neural Networks to address local transformations.\\n\\nWe establish a subequivariant edge representation in environments where gravity is a major factor. This is simplified as follows:\\n\\n$$\\\\phi(\\\\mathbf{Z}, \\\\mathbf{h}) = (\\\\mathbf{Z}, \\\\mathbf{h})$$\\n\\nHere, $\\\\mathbf{Z}$ is the local entity-level graph topology. The right side illustrates our key innovation: the dynamic task assignment leveraging bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging. Consequently, we employ a greedy strategy-based bipartite graph matching, guided by inter-entity distance costs. Due to the static programming characteristics of JAX, implementing the Hungarian matching algorithm proves challenging."}
{"id": "hQpUhySEJi", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\nFollowing the local entity-level subequivariant MP, we integrate the resultant entity-level information $h_i'$ and $\\\\vec{Z}_i'$ with body-level information to guide body-level control. Here, $\\\\vec{u}_i$ represents a local reference frame (LRF) transform vector, derived through the linear transformation $\\\\vec{u}_i = \\\\vec{Z}_i W$, with $W \\\\in \\\\mathbb{R}^{m \\\\times 1}$.\\n\\nWe normalize and orthogonalize the transform vector $\\\\vec{u}_i$ for each entity:\\n\\n$$\\\\vec{e}_i1 = \\\\vec{u}_i - \\\\langle \\\\vec{u}_i, \\\\vec{e}_i3 \\\\rangle \\\\vec{e}_i3 / \\\\| \\\\vec{u}_i - \\\\langle \\\\vec{u}_i, \\\\vec{e}_i3 \\\\rangle \\\\vec{e}_i3 \\\\|,$$\\n\\n$$\\\\vec{e}_i2 = \\\\vec{e}_i1 \\\\times \\\\vec{e}_i3,$$\\n\\n$$\\\\vec{e}_i3 = [0, 0, 1] \\\\top.$$\\n\\nHere, $\\\\langle \\\\cdot, \\\\cdot \\\\rangle$ denotes the inner product, and $\\\\times$ the cross product. We refer to the aforementioned procedure as OP, following which we proceed to construct an entity-wise rotation matrix on transform vectors.\\n\\n$$O_i = [\\\\vec{e}_i1, \\\\vec{e}_i2, \\\\vec{e}_i3] = \\\\text{OP}(\\\\vec{u}_i), i \\\\in \\\\Omega.$$\\n\\nTheorem 3.1. The learned entity-wise rotation matrix, denoted as $O_i = \\\\text{OP}(\\\\vec{u}_i)$, are $\\\\text{SO}_3$-equivariant, satisfying any transformation $g \\\\in \\\\text{SO}_3$, $g \\\\cdot O_i = \\\\text{OP}(g \\\\cdot \\\\vec{u}_i)$.\\n\\nProof. See Appendix A.\\n\\nAt this stage, we establish the LRF for each agent $i$, with $\\\\vec{c}$ as the origin and $\\\\vec{e}_i1$ as the orientation of the $x$-axis. Notably, with the task assignment, the origin of each agent's LRF shifts from the collective average root position $\\\\vec{c} = \\\\frac{1}{N+M} \\\\sum_{i=1}^{N+M} \\\\vec{p}_i$ to the specific entity's position, $\\\\vec{c} = \\\\vec{p}_{C(i)}$. This LRF construction enables us to achieve invariant observation inputs:\\n\\n$$o_i' = O_i^\\\\top o_i = \\\\{O_i^\\\\top \\\\vec{Z}_{i,k}, h_{i,k}\\\\}_{k=1}^{K_i},$$\\n\\nwhere $j \\\\in N(i)$, and $[\\\\cdots]$ is the stack along the last dimension, with adjustments for relative positioning in assignment as $\\\\vec{p}_{i,k} = \\\\vec{p}_{i,k} - \\\\vec{p}_{C(i)}$, $\\\\vec{p}_{j,1} = \\\\vec{p}_{j,1} - \\\\vec{p}_{C(j)}$, thus achieving decoupled translation and rotation invariance.\\n\\nOur methodology integrates subequivariant information across entity and body levels via LRF Transform. While disrupting reflection symmetry, the necessity to construct an orthogonal rotation matrix significantly enhances the capabilities of subequivariant networks. This emphasis on rotation symmetry substantially outweighs the reduced focus on reflection symmetry, particularly in diminishing the massive search space for optimal policies. Empirical validations of this enhancement are detailed in Section 5.4.\\n\\nBody-level Control\\n\\nWe are now equipped to output the invariant actor policy $\\\\pi_\\\\theta_i$ and invariant critic value-function $V_\\\\phi$ for the training objective in Equation (1). For each agent\\n\\nTeam Sumo: cooperation & competition\\nTeam Reach: cooperation\\nTask assignment entity-level graph\\nFully connected entity-level graph\\nFixed ball\\nTeam0 agent\\nTeam1 agent\\nAssignment edge\\nConnected edge\\ntask assignment\\n\\n![Figure 3](image)\\n\\nFigure 3. Illustration of $\\\\mathcal{M}_\\\\mathcal{E}_\\\\mathcal{B}_\\\\mathcal{E}_\\\\mathcal{N}$: Team Reach (left) where agents cooperate to collectively reach all fixed balls, and Team Sumo (right) where agents engage in both cooperation and competition to push opponents away from the fixed ball. These tasks necessitate the decoupling of local transformations via dynamic task assignment graph construction from the overall structure (depicted by a fully connected graph), while employing $\\\\mathcal{E}_\\\\vec{g}(3)$-equivariance to effectively compress global state space.\\n\\nThe invariant actor policy $\\\\pi_\\\\theta_i$ leverages the invariant $o_i'$ and $h_i'$, defined by\\n\\n$$\\\\pi_\\\\theta_i = \\\\sigma_{\\\\pi_i}(o_i', h_i'),$$\\n\\nwhere $\\\\sigma_{\\\\pi_i}$ is a MLP. Here, $\\\\pi_\\\\theta_i \\\\in \\\\mathbb{R}^{2 \\\\times (K_i - 1)}$ represents the loc and scale parameters of a Tanh-Normal Distribution for the $(K_i - 1)$ actuators of agent $i$. Consequently, each actuator samples its corresponding torque $a_{i,k} \\\\in [-1, 1]$ from this distribution.\\n\\nBoth the invariant critic value-function $V_\\\\phi$ utilizes the entity-level invariant features $\\\\{h_i'\\\\}_{i=1}^{N+M}$, formulated as\\n\\n$$V_\\\\phi = \\\\sigma_V(\\\\{h_i'\\\\}_{i=1}^{N+M}),$$\\n\\nwhere $\\\\sigma_V$ is a MLP, and $V_\\\\phi \\\\in \\\\mathbb{R}$. Formal proof of the $\\\\text{SO}_3$-invariance of the output action and value are presented in Appendix A.\\n\\n4. Multi-entity Benchmark ($\\\\mathcal{M}_\\\\mathcal{E}_\\\\mathcal{B}_\\\\mathcal{E}_\\\\mathcal{N}$)\\n\\n4.1. Environments descriptions\\n\\nIn this subsection, we present the details involved in constructing our environments: Team Reach and Team Sumo, as illustrated in Figure 3.\\n\\nAgents\\n\\nIn our studies, we leverage a variety of morphologies, including ants, claws, and centipedes from MxT-Bench (Furuta et al., 2023), as well as unimals from Gupta et al. (2022). Notably, centipedes and unimals exhibit asymmetrical forms, potentially influencing the overall system's symmetry and dynamics. This diverse range of agent morphologies enables a nuanced exploration of agent dynamics in multi-entity, morphology-based RL environments.\"}"}
{"id": "hQpUhySEJi", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\nFigure 4. Training and Evaluation Curves in Team Reach Environments. The shaded area represents the standard error.\\n\\nTeam Reach\\nWe expand the \u201cReach\u201d task from a single-agent challenge to a collaborative \u201cTeam Reach\u201d task, as shown in Figure 9.\\n\\n1. Initial Conditions.\\n   Entities set \\\\( \\\\Omega \\\\) include \\\\( N \\\\) agents and \\\\( M \\\\) fixed balls, \\\\( N \\\\geq M \\\\). Within an area of radius \\\\( R \\\\), we randomly position \\\\( N \\\\) agents and \\\\( M \\\\) fixed balls, also setting the initial orientations of the agents randomly. Specific details for \\\\( N \\\\), \\\\( M \\\\), \\\\( R \\\\), and the agents' morphology are provided in Table 4.\\n\\n2. Termination.\\nThe goal of this task is for \\\\( M \\\\) fixed balls to be simultaneously occupied by agents. An episode is considered successfully completed and terminates once this condition is fulfilled.\\n\\n3. Reward.\\nThe designed reward is detailed in Appendix C.2.\\n\\nTeam Sumo\\nWe evolve the \u201cSumo\u201d task from a purely competitive challenge to a mixed cooperative-competitive \u201cTeam Sumo\u201d task, as shown in Figure 10.\\n\\n1. Initial Conditions.\\n   Entities set \\\\( \\\\Omega \\\\) comprise one fixed ball, \\\\( N \\\\) agents forming Team 1, and another \\\\( M \\\\) agents constituting Team 2. The sumo arena, a circle with radius \\\\( R \\\\), has its center marked by the fixed ball. Around this fixed ball, within a radius of \\\\( R - 1 \\\\), we randomly position \\\\( N \\\\) agents from Team 1 and another \\\\( M \\\\) from Team 2, ensuring each agent's orientation is also randomized. Specific details for \\\\( N \\\\), \\\\( M \\\\), \\\\( R \\\\), and the agents' morphology are provided in Table 4.\\n\\n2. Termination.\\nThe objective is for either Team 1 or Team 2 to win by having an opposing team's agent disqualified, which occurs if it exceeds a distance \\\\( R \\\\) from the fixed ball. The team with the disqualified agent loses, triggering the termination of the episode.\\n\\n3. Reward.\\nThe reward designed for each team is detailed in Appendix C.2.\\n\\nTable 1. Comparison of Morphology-based Environment Setup.\\n\\n| Aspect                     | SGRL | MxT-Bench | ME-EN |\\n|----------------------------|------|-----------|-------|\\n| Multi-Morphology           | \u2713    | \u2713         | \u2713     |\\n| Multi-Agent                | \u00d7    | \u00d7         | \u2713     |\\n| Diverse-Task               | \u00d7    | \u2713         | \u2713     |\\n| Supported-Symmetry         | \u2713    | \u00d7         | \u2713     |\\n| Accelerated-Hardware       | \u00d7    | \u2713         | \u2713     |\\n\\n4.2. Design considerations\\nWe present the key features and limitations of existing benchmarks in Table 1, comparing them with our introduced ME-EN. More comprehensive details are provided below.\\n\\nMulti-Entity Dynamics\\nDiverging from the single-agent focus in SGRL (Chen et al., 2023) and MxT-bench (Furuta et al., 2023), ME-EN expands to include environments with multiple entities (Spelke, 2022), enabling a detailed exploration of dynamics and interactions among varied entities, such as agents with complex morphologies, and other significant objects impacting system symmetry and behavior.\\n\\nME-EN's unique design makes it an ideal platform for a wide range of morphology-based reinforcement learning studies, encompassing single-agent and multi-agent systems (cooperative, competitive, and mixed scenarios).\\n\\nGeometric Symmetry\\nIn contrast to the fixed agent initialization in MxT-bench (Furuta et al., 2023), ME-EN provides a more realistic and geometric symmetric setup through its...\"}"}
{"id": "hQpUhySEJi", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments\\n\\nFigure 5. Training and Evaluation Curves in Team Sumo Environments. The shaded area represents the standard error.\\n\\nAccelerated-Hardware\\nIn contrast to traditional CPU-based environments such as SGRL (Chen et al., 2023), ME Bun harnesses the advanced capabilities of the Brax physics simulator (Freeman et al., 2021) and Composer (Gu et al., 2021), and further builds upon the MxT-bench framework (Furuta et al., 2023). This advancement propels ME Bun into facilitating efficient and scalable hardware-accelerated iterations on GPUs or TPUs, making it exceptionally suitable for morphology-based reinforcement learning experiments.\\n\\n5. Experiments\\n5.1. Experimental Setup\\nBaselines\\nWe compare our method SHNN, against mainstream neural networks, particularly MLP (Furuta et al., 2023), and its variant utilizing heading normalization tricks, denoted as MLP+HN (Chen et al., 2023). Please refer to Appendix C.3 for details about baselines.\\n\\nMetrics\\n1. Team Reach: Success Rate = \\\\#success \\\\( \\\\div \\\\#evaluation \\\\)\\n2. Team Sumo: For each team, Win Rate = \\\\#win \\\\( \\\\div \\\\#evaluation \\\\).\\n\\nEach experiment is conducted with 10 seeds to report the Success (or Win) Rate over \\\\#evaluation \\\\( = 1024 \\\\). Implementations\\nThe environments used in this work are detailed in Table 4. To ensure fairness, all baselines, ablations, and our SHNN model use the same input information and employ MAPPO (Yu et al., 2022) as the training algorithm for MARL. SHNN is developed based on the MxT-bench (Furuta et al., 2023) codebase, leveraging JAX (Bradbury et al., 2018) and Brax (Freeman et al., 2021; Gu et al., 2021) for efficient, hardware-accelerated simulations. The value of the maximum timesteps per episode is 1000. Hyperparameter details are in Appendix C.9. Within the same team, agents share only the weights of entity-level message passing, not the body-level MLP. For the Team Reach environments, we construct a directed graph by dynamically assigning a fixed ball to each agent. For the Team Sumo environments, we assign each agent an opponent from the opposing team and assign each with the arena\u2019s center ball. Additionally, in Team Sumo environments, we adopt Bansal et al. (2018)\u2019s approach where teams employing baseline methods and SHNN compete within an arena.\\n\\n5.2. Evaluations in Diverse Environments\\nWe begin by evaluating our method on diverse multi-entity tasks in 3D environments.\\n\\nTeam Reach in Figure 4: 1. The MLP generally fails to achieve meaningful returns in most Team Reach cases, which is due to its vulnerability to local extremes within the expansive exploration space of our environments. 2. While MLP enhanced with Heading Normalization (MLP+HN) shows close performance to SHNN...\"}"}
