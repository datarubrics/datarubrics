{"id": "tl2qmO5kpD", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWe show that offline actor-critic reinforcement learning can scale to large models \u2013 such as transformers \u2013 and follows similar scaling laws as supervised learning. We find that offline actor-critic algorithms can outperform strong, supervised, behavioral cloning baselines for multi-task training on a large dataset; containing both sub-optimal and expert behavior on 132 continuous control tasks. We introduce a Perceiver-based actor-critic model and elucidate the key features needed to make offline RL work with self- and cross-attention modules. Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data.\\n\\n1. Introduction\\n\\nIn recent years, scaling both model and dataset sizes has led to multiple breakthroughs in machine learning. In particular, generative pre-training of large (vision-)language models on web-scale data is now the standard way to solve many language and vision tasks (OpenAI, 2023; Alayrac et al., 2022) and generative models of images and music have, in the last years, reached unprecedented quality (Rombach et al., 2021; Kang et al., 2023).\\n\\nRecent work on scaling up policy learning for control has shown that, when similar model architectures are used (e.g. transformers), supervised behaviour cloning (BC) from large datasets can lead to surprisingly capable multi-task policies (Reed et al., 2022; Bousmalis et al., 2023; Branch et al., 2022; Octo Model Team et al., 2023). Although impressive, these examples come with the drawback that high-quality ('expert' demonstration) data is needed for training. While such high quality data is readily available for language and vision domains via the internet, in robotics (and other real world domains) expert data is scarce and expensive to obtain \u2013 and in many cases not available in the first place. It is thus desirable to use different training methods, such as reinforcement learning (RL), that can utilize sub-optimal data or data generated without a human in the loop, i.e. generated by an agent, \u2013 which can be more readily available \u2013 while retaining scaling benefits.\\n\\nHowever, training large behaviour models via offline RL methods is a largely unexplored area of research. While first explorations of applying pure Q-learning on larger multi-task datasets exist (Kumar et al., 2022a; Chebotar et al., 2023) they either consider non-transformer models of moderate size (Kumar et al., 2022a) or adapt relatively small models and incur significant computational overhead during training (Chebotar et al., 2023). What is missing is a clear recipe detailing how to scale offline RL to large transformers accompanied by an efficient model.\\n\\nIn this work we provide such a recipe and introduce the Perceiver-Actor-Critic (PAC) approach outlined in Figure 1. We show that a specific class of offline RL algorithms (offline actor-critic methods) can indeed scale to large models and datasets without incurring a large additional computational cost. In addition we establish, for the first time, that offline RL follows similar scaling laws to those observed in the supervised learning regime (Henighan et al., 2020; Kaplan et al., 2020). We further establish that this class of methods is ideally suited for slowly moving away from supervised BC towards RL during training, allowing us to run large and expensive experiments without fear of instability and to adapt our method depending on the quality of the data.\\n\\nWe introduce a simple offline actor-critic algorithm that optimises a KL-regularized RL objective and can be seen as a simplified variant of MPO/DIME (Abdolmaleki et al., 2018; ...\"}"}
{"id": "tl2qmO5kpD", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nFigure 1. PAC is a scalable neural architecture for continuous control able to smoothly interpolate between BC and offline RL. The system design enables training on heterogenous, multi-modal data of varying quality. We demonstrate that our system achieves higher performance than BC across a series of model scales. The method also enables a seamless transition into offline and online RL finetuning for fast adaptation and mastery of control tasks.\\n\\nWe find that regularizing the policy towards the data distribution (via BC) is sufficient to stabilize offline RL for large models and also allows convenient interpolation between BC and RL. We additionally introduce architectural advances which enable training with RL at scale. E.g. incorporating the action into the Q-function via cross-attention (allowing fast estimation of Q-values for multiple actions) and incorporating a large number of inputs via Perceiver-style cross-attention to learned latent variables; enabling training with many inputs of different modalities (text, proprioception, vision) while enabling inference of a large 1B parameter model at 20 Hz on a local machine.\\n\\nPAC outperforms BC on a number of benchmarks in continuous control, including outperforming Gato (Reed et al., 2022) on Control Suite (Tunyasuvunakool et al., 2020) tasks and recovering expert performance from heterogeneous data in a real robot benchmark. This establishes that RL should be considered a viable alternative to BC for large policies. Videos of our agent can be found at https://sites.google.com/view/perceiver-actor-critic.\\n\\n2. Background and Related Work\\n\\nSupervised Generalist Agents\\nSeveral recent works have trained large transformer-based (Vaswani et al., 2017) generalist agents via BC by building on previous works in which control tasks were transformed into sequence prediction problems (Chen et al., 2021; Janner et al., 2021). Gato (Reed et al., 2022) for example, was trained on tasks ranging from Atari games to robotics manipulation. Subsequently, large generalist robotics agents (Brohan et al., 2022; Bousmalis et al., 2023; Zitkovich et al., 2023) have been trained on large datasets with multiple tasks, object sets and embodiments, and have been shown to generalize to new tasks and domains after fine-tuning (Bousmalis et al., 2023; Open X-Embodiment Collaboration, 2023). Perceiver-based networks with cross-attention (Jaegle et al., 2021) have also been applied to robotics to minimize computational demands when handling voxel observations (Shridhar et al., 2023; Ze et al., 2023). Finally, Octo Model Team et al. (2023) used multi-headed attention to predict outputs in a similar way to the cross-attention in our system.\\n\\nOffline RL\\nOffline RL methods (Levine et al., 2020; Lange et al., 2012) learn from fixed datasets without online exploration. Unlike supervised algorithms, they can learn from suboptimal trajectories and thus more data. However, they are at risk of issues like overoptimism for unseen state-action pairs. This is often addressed by regularizing the policy to stay close to the data (Peng et al., 2019; Wang et al., 2020; Fujimoto et al., 2019; Wu et al., 2019). Like prior work (Abdolmaleki et al., 2022; Fujimoto & Gu, 2021), we combine a BC regularization term with an off-policy RL method. Other offline RL methods penalize the value function (Kumar et al., 2020) or prevent value propagation (Kostrikov et al., 2021) for unseen state-action pairs. While most offline RL works use relatively small benchmarks, recent ones have tackled challenging multi-task problems (Kumar et al., 2022a) and pre-trained robotics generalists that can be fine-tuned to new tasks (Kumar et al., 2022b). However, to our knowledge, only the recent Q-Transformer (Chebotar et al., 2023) provides an example of a transformer trained with offline RL on larger datasets, albeit with a relatively small model. Our actor-critic-based approach is more naturally suited for extending BC-based methods and less computationally demanding. This allows us to explore much larger models and perform a scaling law analysis.\\n\\nScaling Law Analysis\\nOur scaling law analysis mirrors analyses of large language models for which several studies have shown smooth power-law relations between model size and performance (Kaplan et al., 2020; Hoffmann et al., 2022; Henighan et al., 2020). Some recent works have also investigated scaling behavior of neural networks for online RL (Neumann & Gros, 2022; Hilton et al., 2023) albeit...\"}"}
{"id": "tl2qmO5kpD", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We target a KL-regularized RL objective, where the goal is to find a policy $\\\\pi$ that maximizes the per-task discounted cumulative return. We make use of behaviour cloning (BC) terms via $\\\\pi_{BC}$ to find a policy $\\\\pi$ that generates the dataset and is used as the reference policy in Equation (2), i.e. $\\\\pi_{BC} = \\\\pi_{imp}$. We consider learning in a multi-task Markov decision process setting and present our model in Section 3.3. We achieve this, we adapt methods from the offline RL literature to work for actor-critic methods and provide a thorough scaling analysis. Concurrent work also shows promising scaling of offline RL baselines for models up to 200M parameters. In contrast, we find scaling to be unfavourable for Decision Transformer (Chen et al., 2021) and Perceiver-IO (Jaegle et al., 2022) architectures. We assume the availability of an offline RL baseline for models up to 200M parameters and multimodal observations interchangeably, although the true environment state is often not fully observable. We use an actor-critic setting and present our proposed algorithm in Section 3.2. We adapt different choices for estimating the $Q$-function, and turn the problem of learning a policy $\\\\pi$ into a weighted supervised learning problem (cf. Abdolmaleki et al. (2018)). Assuming access to an estimate of $Q_{\\\\pi}$ or reward annotations, we can fit a parametric distribution $\\\\rho_{\\\\pi}$ for the state-action value function. To balance the $Q$-value loss with negative log likelihood of a discretized $Q$-function distribution, we employ tools from the distributional reinforcement learning literature (Bellemare et al., 2017) which allows us to transform the problem of learning the $Q$-function into one of supervised learning. This observation allows us to transform the RL problem of finding an optimal policy into a weighted supervised learning problem. We assume the availability of a reference policy $\\\\pi_{imp}$ which we use as the reference policy in Equation (2), i.e. $\\\\pi_{imp} = \\\\pi_{BC}$. We train the policy $\\\\pi$ periodically updated to a time-lagged version of $\\\\pi$, $\\\\pi'$. We use the term state $s_t$ for its current state $t \\\\in \\\\mathbb{N}$. We assume that $s_t \\\\sim p(s_t)$ and access to either the trajectory distribution $D_{s_t, \\\\pi}$ or reward annotations. We can compute a sample-based negative log likelihood of a discretized $Q$-function distribution $\\\\pi_{Q}$, using the distributional TD operator $\\eta$.\\n\\nWe use the term state $s_t$ for its current state $t \\\\in \\\\mathbb{N}$. We assume that $s_t \\\\sim p(s_t)$ and access to either the trajectory distribution $D_{s_t, \\\\pi}$ or reward annotations. We can compute a sample-based negative log likelihood of a discretized $Q$-function distribution $\\\\pi_{Q}$, using the distributional TD operator $\\\\eta$.\\n\\nThe expectation over the data is estimated by sampling $s_t, a_t, \\\\tau_t \\\\sim \\\\pi_{imp}$, $s'_t, a_t, \\\\tau_t \\\\sim \\\\pi'$. We sample $s_t, a_t, \\\\tau_t \\\\sim \\\\pi_{imp}$, $s'_t, a_t, \\\\tau_t \\\\sim \\\\pi'$ with $s_t = s_{t-1} + \\\\Delta s_t$, $a_t = \\\\arg \\\\max_{a'} Q_{\\\\pi}(s_t, a') + \\\\epsilon$, $\\\\tau_t = \\\\tau_{t-1} + \\\\Delta \\\\tau_t$, and $s'_t = s'_t - \\\\epsilon$, $a_t = \\\\arg \\\\max_{a'} Q_{\\\\pi}(s'_t, a') + \\\\epsilon$, $\\\\tau_t = \\\\tau_t + \\\\Delta \\\\tau_t$, where $\\\\Delta s_t, \\\\Delta \\\\tau_t, \\\\epsilon$ are random variables.\\n\\nWe scale up offline actor-critic methods to large models. To achieve this, we adapt methods from the offline RL literature finding no favourable scaling. In contrast, we find scaling to turn to be unfavourable. We use an actor-critic setting and present our proposed algorithm in Section 3.2. We adapt different choices for estimating the $Q$-function, and turn the problem of learning a policy $\\\\pi$ into a weighted supervised learning problem. We assume the availability of a reference policy $\\\\pi_{imp}$ which we use as the reference policy in Equation (2), i.e. $\\\\pi_{imp} = \\\\pi_{BC}$. We train the policy $\\\\pi$ periodically updated to a time-lagged version of $\\\\pi$, $\\\\pi'$. We use the term state $s_t$ for its current state $t \\\\in \\\\mathbb{N}$. We assume that $s_t \\\\sim p(s_t)$ and access to either the trajectory distribution $D_{s_t, \\\\pi}$ or reward annotations. We can compute a sample-based negative log likelihood of a discretized $Q$-function distribution $\\\\pi_{Q}$, using the distributional TD operator $\\\\eta$.\\n\\nThe expectation over the data is estimated by sampling $s_t, a_t, \\\\tau_t \\\\sim \\\\pi_{imp}$, $s'_t, a_t, \\\\tau_t \\\\sim \\\\pi'$. We sample $s_t, a_t, \\\\tau_t \\\\sim \\\\pi_{imp}$, $s'_t, a_t, \\\\tau_t \\\\sim \\\\pi'$ with $s_t = s_{t-1} + \\\\Delta s_t$, $a_t = \\\\arg \\\\max_{a'} Q_{\\\\pi}(s_t, a') + \\\\epsilon$, $\\\\tau_t = \\\\tau_{t-1} + \\\\Delta \\\\tau_t$, and $s'_t = s'_t - \\\\epsilon$, $a_t = \\\\arg \\\\max_{a'} Q_{\\\\pi}(s'_t, a') + \\\\epsilon$, $\\\\tau_t = \\\\tau_t + \\\\Delta \\\\tau_t$, where $\\\\Delta s_t, \\\\Delta \\\\tau_t, \\\\epsilon$ are random variables.\"}"}
{"id": "tl2qmO5kpD", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nFigure 7. The different parts of the PAC architecture for which FLOPs are counted. The FLOPs for the different encoders are counted as $\\\\text{ENC}\\\\{P,V,L,A\\\\}$, respectively. All FLOPs used to aggregate the encoded input tokens $e_i$ into the Perceiver's latent $z_i$ during the input cross-attention are counted as $\\\\text{XATTN}_\\\\text{IN}$. The FLOPs used for processing the latents via $\\\\text{M}$ self-attention blocks are counted as $\\\\text{SATTN}_\\\\text{PROC}$. The FLOPs used to decode the policy $\\\\pi$ and the action-value function $Q$ from $z_M$ via cross-attention and subsequent projections are counted as $\\\\text{XATTN}_\\\\text{PI}$ and $\\\\text{XATTN}_Q$ respectively.\\n\\n$\\\\times$ FLOPS ATTN instead of $\\\\times$ seq_len in $\\\\times$ seq_len in $\\\\times$ FLOPS ATTN like in a normal self-attention block.\\n\\nAfter tokenization of proprioception, vision, language and action (cf. Appendix C.3) we obtain $T_P$, $T_V$, $T_A$ and $T_L$ tokens respectively. Their encoders are simple projections and we count the FLOPs used for the embeddings as:\\n\\n- $\\\\text{ENC}_P = \\\\text{MAF} \\\\times T_P \\\\times \\\\text{DI}$\\n- $\\\\text{ENC}_V = \\\\text{MAF} \\\\times T_V \\\\times \\\\text{DI} + \\\\text{FLOPS}_\\\\text{RESNET}$\\n- $\\\\text{ENC}_L = \\\\text{MAF} \\\\times T_L \\\\times NT \\\\times \\\\text{DI}$\\n- $\\\\text{ENC}_A = \\\\text{MAF} \\\\times T_A \\\\times \\\\text{DI}$\\n\\nSimilar to Hoffmann et al. (2022) we use a multiply-accumulate-factor (MAF) of 2 to describe the multiply accumulate cost. The FLOPs needed to transform the raw input images into tokens using a ResNet are captured by $\\\\text{FLOPS}_\\\\text{RESNET}$. We count each 2D convolution operation in the ResNet as $\\\\text{num kernels} \\\\times (w_1 \\\\ast w_2) \\\\times (o_1, o_2) \\\\times \\\\text{MAF}$ where $w_\\\\{1,2\\\\}$ and $o_\\\\{1,2\\\\}$ are the kernel and output dimensions respectively.\\n\\nThe total number of FLOPs used for one forward pass of PAC are:\\n\\n$$\\\\text{FLOPS}_\\\\text{FWD} = \\\\text{ENC}_P + \\\\text{ENC}_V + \\\\text{ENC}_L + \\\\text{ENC}_A + \\\\text{XATTN}_\\\\text{IN} + \\\\text{M} \\\\times \\\\text{SATTN}_\\\\text{PROC} + \\\\text{XATTN}_\\\\text{PI} + \\\\text{XATTN}_Q$$\\n\\nWhen estimating the FLOPs for the backward pass, we slightly deviate from Kaplan et al. (2020) and also factor in the target network updates (cf Section 3.2) which occur every $f_{\\\\theta}'$ updates (in all of our experiments we keep $f_{\\\\theta}' = 100$). Therefore, we count the FLOPs for PAC's backward pass as:\\n\\n$$\\\\text{FLOPS}_\\\\text{BWD} = (2 + 1) \\\\times \\\\text{FLOPS}_\\\\text{BWD}$$\\n\\nLastly, the FLOPs for an update with batch size $B$ are counted as:\\n\\n$$\\\\text{FLOPS}_\\\\text{UPDATE} = B \\\\times (\\\\text{FLOPS}_\\\\text{BWD} + \\\\text{FLOPS}_\\\\text{BWD})$$\\n\\nWe list the FLOPs costs for the core operations of our PAC model series in Table 17.\\n\\nE.3. Model Loss as Performance Indicator\\n\\nWe plot the training loss against the FLOPs for both model sets in Figure 8 when training on the scaling data mix (cf. Table 9). The training losses suggest that the $\\\\text{BC+Q}$ model should outperform PAC because its final loss is about 0.1 points lower. In order to assert whether the training loss can be a reliable indicator of model performance in our experiments, we compare the final model checkpoints of $\\\\text{BC+Q}$ and PAC (both of size L) on 73 tasks from our training dataset in simulation and report the results in Figure 8. However, this performance-based comparison does not support the assumption that a lower training loss is indicative of higher task performance. On the contrary, PAC significantly outperforms BC on the simulated stacking task despite its...\"}"}
{"id": "tl2qmO5kpD", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nFigure 8. Top: Training loss comparison of the PAC model families for \\\\( \\\\alpha = 1.0 \\\\) (BC+Q) and \\\\( \\\\alpha = 0.75 \\\\) (PAC). Bottom: Success rates in different simulation tasks of the final checkpoints after 3M updates for the respective L-size models. Each model was evaluated in 100 trials on each of the \\\\( \\\\#(T) \\\\) in each domain. For the Control domain, the percentage of achieved expert average reward and the standard-error-based 95% confidence intervals are reported. For all other domains, the average success rates and their corresponding Wilson score intervals for \\\\( \\\\alpha_w = 0.05 \\\\) are reported.\\n\\nHigher training loss. Hence, we cannot use the model directly for selecting interpolants to fit the scaling laws with and need use the proxy of return profiles (cf. Appendix E.1). Our finding is consistent with previous works (Hilton et al., 2023) as we find that loss is not necessarily a reliable indicator of model performance in RL settings. This can be due to a variety of reasons, e.g. there can be degenerate solutions when the policy overfits to the data and is unable to generalize to new states or when the Q-function collapses. Additionally, when a TD-style loss is used, losses are generally not comparable and we refer to (Fujimoto et al., 2022) for a more elaborate discussion on this issue.\\n\\nE.4. PAC+V Scaling Analysis\\n\\nWe also conduct a scaling analysis for the V-function variant of our architecture: PAC+V (cf. Appendix A.3) following the protocol defined in Section 4.1. We plot the scaling laws for PAC+V in Figure 12 comparing it to the scaling laws for the BC+Q baseline as well. We also compare the Iso-Return contours between the two models in Figure 9. The conclusions of the PAC+V scaling laws are consistent with the ones drawn for the Q-function variant of PAC in Section 4.1. The suggested model size for 2.45T tokens is with 852M parameters slightly smaller than PAC\u2019s with 954M parameters, but the parameters should be scaled slightly more aggressively as compute increases indicated by \\\\( a(\\\\text{PAC+V}) > a(\\\\text{PAC}) : 0.970 > 0.920 \\\\). However, the data scaling is in line with both PAC as well as the BC+Q baseline: \\\\( b(\\\\text{PAC+V}) \\\\approx 0.266 \\\\).\\n\\nE.5. Scaling-based Performance Analysis of PAC on Control Suite\\n\\nIn this section we provide additional details about the progression of PAC\u2019s task performance with model scale based on our experiments in Section 4.1. When expanding the task performances on Control Suite during the scaling experiments, the results fall in one of three categories: A) tasks with significant model scaling benefits; B) tasks which are\"}"}
{"id": "tl2qmO5kpD", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nFigure 10. PAC\u2019s performance across different Control Suite tasks where significant model scaling benefits are observed. We report average task performance over 100 trials for each task and model scale.\\n\\nFigure 11. Average performance delta of PAC over the BC+Q baseline across all model scales reported for all Control Suite tasks which benefit significantly from model scaling.\\n\\nTasks in category B) are: ball in cup catch, cartpole {balance, swingup}, finger {spin, turn {easy, hard}}, fish {swim, upright}, pendulum swingup, point mass easy, quadruped {run, walk}, reacher easy and walker {run, stand, walk}.\\n\\nAll of these tasks already achieve scores \u2265 95 with PAC-32M. Tasks in category C) are swimmer {6, 15} which plateau at scores around 60 with PAC-32M and do not improve further with model scale. However, 14 out of 32 tasks fall in category A) and show significant performance improvements with model scale. We report the performance progression of PAC on these tasks in Figure 10. Among these tasks, cheetah run, hopper stand, dog {stand, walk, trot} and humanoid {stand, walk} stand out in particular as model scaling pushes them from scores as low as 33 to over 95 into the \u2018task mastery range\u2019 without any additional data.\\n\\nWe also compare PAC\u2019s performance in the 14 \u2018scaling tasks\u2019 to the BC+Q baseline across all model scales in Figure 11. We find that the improvements attributable to PAC\u2019s offline RL objective are most pronounced in the hopper and humanoid tasks. There are tasks where the BC+Q baseline has an initial advantage, e.g. cheetah run, dog walk or cartpole three poles. However, as PAC is scaled up, it always overcomes this initial disadvantage and outperforms BC+Q consistently. This corroborates the key finding of the scaling analysis: the offline RL objective initially needs higher parameter counts compared to BC+Q (cf. Figure 3 right panel and Figure 12 top right panel), but ultimately scales better with more compute than BC+Q leading to higher performance plateaus (cf. Figure 4 bottom panel).\"}"}
{"id": "tl2qmO5kpD", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nFigure 12. Scaling laws based on the return profile envelopes for BC+Q (top) and PAC+V (bottom). We select 100 logarithmically spaced points between 5E+18 and 5E+20 FLOPs on the envelope of the return profiles (left) and use them to fit the scaling laws (middle, right). For both the token and parameter scaling plots, we indicate the scaling trend with a dashed red line. The dashed green line represents the optimal number of parameters and compute budget needed to fit the data in one epoch of training. The dashed teal line represents the optimal data and parameter trade-off for a FLOP budget of 1E+21.\\n\\nF. Additional Experiments\\n\\nF.1. Large-scale Training of PAC+V\\n\\nAs mentioned in Section 4.2, we also conduct a large-scale training on the full pre-training corpus (cf. Table 11) following the same protocol as for the other models. We report the additional results obtained with PAC+V in Table 18. The results for the V-function variant of PAC are in line with the ones obtained using the Q-function variant with two notable exceptions. First, in the case of RC:Insertion which consists exclusively of human teleoperated data, PAC+V provides another significant improvement over PAC to \u224889% success rate. This suggests that PAC+V could have an edge in cases where many successful task demonstrations exist which are however beset by minor inefficiencies, e.g. motion jitter or pauses caused by human teleoperation. The results suggest that the one-step improvement over the data-generating policy afforded by PAC+V could be enough to prevent imitating many inefficiencies present in the training data. Second, in the case of CHEF:sim, PAC+V lags behind its Q-function-based counterpart PAC. This suggests that in cases where the data is mostly sub-optimal, a value function alone might not be sufficient to filter transitions effectively enough to improve the policy.\\n\\nF.2. RL Fine-tuning and Self-improvement\\n\\nIn Section 4.3 we evaluated PAC on a robotic stacking benchmark, and performed iterative fine-tuning to improve performance in this domain. Here, we provide additional details for those results. The RGB Stacking benchmark (Lee et al., 2021) defines five distinct sets of test objects, each highlighting a different challenge of object manipulation. For brevity, we only reported mean success rates above. In Table 19, we provide success rates for each object separately. The continuous self-improvement of PAC is particularly visible on \u201cSet 2\u201d, which requires precise force-based control to flip objects onto their side. However, the same holds for the other object sets, which improve across fine-tuning rounds until converging at >90% success rate.\\n\\nData collection was carried out only for the CHEF:real domain, so it is worth examining whether such focused self-improvement causes the PAC model's performance on other tasks to degrade. As Table 20 shows, performance on the simulated tasks is unaffected, even after three rounds of fine-tuning.\\n\\nWhile we started the iterative improvement in Section 4.3 by pre-training \\\\( \\\\alpha \\\\)-PAC, the BC/RL trade-off parameter \\\\( \\\\alpha \\\\) also allows flexibility as to what data to start from.\"}"}
{"id": "tl2qmO5kpD", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\n[Figure 2.]\\n\\nHigh-level PAC model architecture. Modality-specific encoders transform proprioceptive (P), visual (V), and language (L) inputs into embedding vectors $e_I$, which are cross-attended by learnable latent queries $z_0$. This is followed by a series of self-attention blocks to yield the latent encoding $z_M$, which is then queried via additional cross-attention modules to decode the desired outputs. The policy decoder employs a learnable query $q_\\\\pi$ to cross-attend $z_M$ and outputs the logits of action distributions. The Q-value decoder employs a query $q_Q$ based on the encoded actions to cross-attend $z_M$ and outputs the action-specific logits of the distributional Q-function.\\n\\n$\\\\pi_{\\\\theta'}$ is estimated based on $N = 10$ samples and the expectation $E_{q_\\\\sim \\\\Gamma_{\\\\theta'}}$ can be evaluated analytically. Finally $\\\\alpha$ and $\\\\beta$ are multipliers trading off different loss components (which are relatively easy to set due to all losses corresponding to weighted categorical log likelihoods). We refer to Appendix A.2 for a step-by-step derivation.\\n\\nNotably, aside from the KL towards the improved policy $\\\\pi_{\\\\text{imp}}$, Equation (3) also includes a KL towards the behaviour policy $b$. This additional regularization is necessary to prevent $\\\\pi_{\\\\theta'}$ from converging to action samples that have high Q-values but are far away from those observed in the data (and are thus at the risk of being overestimated); a common issue in offline RL with Q-functions (Levine et al., 2020). The additional BC term prevents this, following prior examples for using a BC loss as a simple regularisation technique in offline RL (Abdolmaleki et al., 2022; Fujimoto & Gu, 2021).\\n\\nWe find that this is the only term needed to stabilize learning. In addition, it gives us a natural way for moving away from learning via pure behavioral cloning ($\\\\alpha = 1$) towards pure policy optimisation against the learned Q-function ($\\\\alpha = 0$). This also allows us to perform expensive training runs of large models with confidence since we can set $\\\\alpha$ to a larger value such that the policy stays close to BC, guaranteeing stable training, and can reduce it later during fine-tuning.\\n\\n3.3. Scalable Architecture for Actor-Critic Learning\\n\\nWith the proposed offline actor-critic algorithm, we now describe how $\\\\pi_{\\\\theta'}$ and $Q_{\\\\theta'}$ are instantiated with scalable network architectures. In particular, we aim for an architecture that is flexible enough to incorporate different modalities of state observations and task descriptions as well as various action specifications, while also being computationally efficient for consuming high-dimensional inputs during learning and at inference time (to enable $20 \\\\text{ Hz}$ control of real robots).\\n\\nIn this section, we describe how we adopt a Perceiver-IO architecture (Jaegle et al., 2021) to achieve the above. The model is depicted in Figure 2.\\n\\nObservation Encoding\\n\\nGiven multimodal inputs, in particular proprioceptive and visual observations $s_t = (s_{Pt}, s_{Vt})$ along with visual and language task descriptions $\\\\tau = \\\\tau_V, \\\\tau_L$), our model first deploys one encoder ($\\\\phi$) per modality to encode the inputs into embedding vectors:\\n\\n$e_I = \\\\phi_P(s_{Pt}) \\\\oplus \\\\phi_V(s_{Vt}) \\\\oplus \\\\phi_V(\\\\tau_V) \\\\oplus \\\\phi_L(\\\\tau_L) \\\\in \\\\mathbb{R}^{N \\\\times D_I}$,\\n\\nwith $N$ and $D_I$ denoting the number and dimensionality of the embedding vectors. Details of each modality encoder are provided in Appendix B.2. For the proprioception encoder $\\\\phi_P$ we propose a novel multi-scale normalizer to account for arbitrary input scales and provide further details and ablations on this encoder choice in Appendices B.1 and D.2.1. We highlight that our model uses task descriptions of different modalities (text and vision) and we analyse this multimodal task conditioning in Appendix D.2.4.\\n\\nTransformer on Latent Space\\n\\nAt this point, the commonly adopted approach would be to feed the embedding sequence $e_I \\\\in \\\\mathbb{R}^{N \\\\times D_I}$ directly into a transformer consisting of multiple stacked self-attention blocks. However, for the domains we consider, the input sequence length amounts to thousands of tokens for a single time step. As the computational complexity and memory usage of self-attention\\n\\n4\"}"}
{"id": "tl2qmO5kpD", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nScales quadratically with the sequence length, this common treatment potentially hinders the learned controller from being applicable to real robotic systems that impose real-time constraints. To address this, we adopt the methodology from the Perceiver model (Jaegle et al., 2021). Specifically, a cross-attention block is placed at the front-end of the network in which the input sequence of embeddings $e_I$ are queried by $N_Z$ trainable latent vectors each of size $D_Z$:\\n\\n$$z \\\\in \\\\mathbb{R}^{N_Z \\\\times D_Z},$$\\n\\nwhich outputs latent embeddings $z_0$. This is followed by $M$ self-attention operations on the latents which finally yield $z_M \\\\in \\\\mathbb{R}^{N_Z \\\\times D_Z}$. Since the number of latent vectors is typically much smaller than the input sequence length ($N_Z \\\\ll N$), and the self-attention operation is shifted from the input embeddings to the latent vectors, this effectively reduces the computation and memory usage to $O(N_Z^2)$. We provide more details on the Perceiver backbone in Appendix B.3.\\n\\nPolicy and Value Decoding\\n\\nTo implement an actor-critic algorithm, the model needs to output both a Q-value estimate and an action prediction. While the action prediction $\\\\hat{a}_t$ can be directly modeled as a function of the inputs $(s_t, \\\\tau)$ which are encoded into $e_I$ and thus $z_M$, the value estimate $Q_\\\\theta(s_t, a_t, \\\\tau)$ also depends on the action $a_t$ which is not encoded in $z_M$. To obtain the two types of outputs we cross-attend the latent embeddings $z_M$ with dedicated queries. While the queries for the policy are learned vectors, the Q-value queries are computed by encoding the action $a_t \\\\in \\\\mathbb{R}^{N_A}$ via our multi-scale normalizer. This has the advantage that the model is less prone to ignoring the action compared to when the action would be presented as an input (a common problem when learning Q-values). It also allows efficient evaluation of the Q-function for multiple action samples via caching of the action-independent latent $z_M$.\\n\\nWe provide more details in Appendix B.4 and ablate the importance of the cross-attention for Q-value prediction in Appendix D.2.2.\\n\\n4. Experiments\\n\\nWe present three sets of experiments investigating different aspects of PAC. Section 4.1 analyzes whether PAC follows scaling laws similar to established supervised learning settings. Section 4.2 compares PAC's performance after large-scale training with the RL objective to different BC baselines across over 100 continuous control tasks. Finally, Section 4.3 studies how PAC can be finetuned by leveraging its Q-function to hone in on a real robot task and further improve its performance using self-generated data.\\n\\nWe use a large dataset throughout all experiments which combines tasks from three different sources: Gato data (Reed et al., 2022) consist of records of an RL agent solving 32 simulation tasks in Control Suite (Tunyasuvu-nakool et al., 2020). RoboCat data (Bousmalis et al., 2023) operates on the RGB Stacking benchmark (Lee et al., 2021) using RL in simulation to build pyramid and tower structures using a 7-DoF Panda robot. It also contains an Insertion task featuring teleoperated simulation data of the same robot inserting differently sized gears onto pegs. Lastly, CHEF (Lampe et al., 2023) data contains simulated and real-world records of a 5-DoF Sawyer robot stacking two objects in the RGB Stacking benchmark using an RL algorithm. For all episodes in our dataset, a short language instruction describing the task is added to each frame, e.g. `humanoid.run` or `panda.sim.pyramid`, which serves as a unique goal instruction to differentiate between the different tasks. For all RoboCat tasks an additional goal image is provided as the goal instruction. We again emphasize that our model can handle both language and visual goal descriptions (where present) and refer to Appendix D.2.4 for details about the goal conditioning. In total, our data mix consists of 3.64M episodes across 102 simulated and 30 real continuous control tasks which equates to approximately 2.45T tokens for model training (cf. Appendices C.3 and C.4).\\n\\n4.1. Scaling Analysis for Offline RL Objectives\\n\\nA central part of our investigation is to understand the interaction between offline actor-critic algorithms and scalable neural network architectures that use (self-)attention. When trained with supervised objectives, such as next-token prediction, architectures of this type usually follow scaling laws (Kaplan et al., 2020), i.e. for all performance-optimal models the number of tokens consumed and the number of model parameters used follow power-laws in the number of FLOPs spent. However, it has so far been unclear whether these scaling laws also extend to RL. To investigate this relationship, we adopt the methodology from Hoffmann et al. (2022) (also known as \u2018Chinchilla scaling laws\u2019) and apply it to PAC. We define five different model scales (XXS, XS, S, M and L) ranging from 32M to 988M parameters to study the scaling behavior of PAC and report the full model architecture hyper-parameters in Appendix C.1.\\n\\nTo conduct our analysis, we train PAC across the different scales with two different values of $\\\\alpha$ for the BC/RL trade-off. Setting $\\\\alpha = 0.0$ results in a BC objective for the policy and constitutes our baseline $BC+Q^2$ while PAC performs offline RL with $\\\\alpha = 0.75$. With a batch size of 512 trajectories of length five, one epoch of our data mix takes approximately 2.7M steps. Therefore we train each model for 3M updates to stay in a single-epoch regime.\\n\\nFollowing Kaplan et al. (2020); Hoffmann et al. (2022), the power laws between compute operations $C$, number of parameters $P$, and number of FLOPs $M$ are defined as:\\n\\n$$C \\\\propto P^\\\\alpha \\\\text{ and } P \\\\propto M^\\\\beta.$$  \\n\\nUsing a Q-value loss term with $\\\\beta > 0$ never decreased the performance in our BC experiments; we keep it for comparability.\"}"}
{"id": "tl2qmO5kpD", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nFigure 3. Scaling laws based on the return profile envelope for PAC. We select 100 logarithmically spaced points between 5E+18 and 5E+20 FLOPs on the envelope of the return profiles (left) and use them to fit the scaling laws (middle, right). For both the token and parameter scaling plots, we indicate the scaling trend with a dashed red line. The dashed green line represents the optimal number of parameters and compute budget needed to fit the data in one epoch of training. The dashed teal line represents the optimal data and parameter trade-off for a FLOP budget of 1E+21.\\n\\nThe scaling laws are different for the BC and offline RL settings. When we constrain the data budget to a single epoch, i.e. 2.45T tokens, the fits suggest to train a 1.33B parameter model in the BC+Q case whereas in the case of PAC a smaller model of only 954M parameters is suggested. This is consistent with our observation that the L-size of PAC with 988M parameters performs best which is close to the predicted optimality point while the BC+Q model likely would benefit from being scaled up further.\\n\\nData-wise, BC+Q and PAC scale nearly the same (b(PAC) \u2248 b(BC+Q) \u2248 0.266). However, the RL objective seems to benefit more from additional parameters as the compute budget increases compared to BC (a(PAC) = 0.920 > a(BC+Q) = 0.828) suggesting that the capacity needed for the Q-function is larger (though as we will see the Q-function can learn from lower quality data).\\n\\nAnother way to compare the scaling behaviors between the BC and offline RL objectives is through the lens of the Iso-Return contours (analogous to the Iso-Loss landscape of Hoffmann et al. (2022)) as presented in Figure 4. The Iso-Return landscape projects the return vs. FLOPs from the return profiles into the parameters vs. FLOPs space and indicates the average return via the color gradient. We draw Isolines between the data points observed during model evaluation to survey the return landscape of the two model families. The Isolines of PAC are drawn solid, the ones of BC+Q are dashed. We also plot the parameter scaling laws for both model families in red over the landscape to indicate the direction of 'optimal scaling' for both families, also called the 'efficient frontier' (Hoffmann et al., 2022). In this direction, the return landscape for both is likely to incline. Comparing the Isolines between both families, we observe that the RL objective shifts all return plateaus to the top left compared to the BC baseline. This suggests that the RL objective can use additional parameters and training FLOPs increasingly more effectively compared to a pure BC objective and scales better with increased compute.\\n\\n4.2. Large-scale Offline Actor-Critic Learning\\n\\nThe scaling analysis above suggests that PAC's offline RL outperforms a BC objective when scaled up. We now investigate whether this still holds when comparing against two strong BC baselines: Gato (Reed et al., 2022) and RoboCat (Bousmalis et al., 2023). The pre-training phase of such large models typically only uses a BC objective to ensure 'safe' optimization and reduce the risk of divergence for these costly training runs. However, if an offline RL objective could be used safely, this would allow using sub-optimal data from the start and further enhance subsequent self-improvement (since a Q-function is available).\"}"}
{"id": "tl2qmO5kpD", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nTable 1. Policy success rates across \\\\( T \\\\) tasks in each domain for 100 evaluations per task. The average success rate in the training data is reported as \\\\( p_D \\\\). For Gato:Control, the percentage of achieved expert average reward and the standard-error-based 95% CIs are reported. For all other domains, the average success rates and their corresponding Wilson score intervals for \\\\( \\\\alpha = 0.05 \\\\) are reported. Best results (within CI of the best mean) in each row are bold.\\n\\n| Domain          | \\\\#(\\\\( T \\\\)) | \\\\( p_D \\\\) | \\\\( BC \\\\) (Gato \u2020) | \\\\( RC \\\\) \u2605 | FilteredBC | \\\\( BC+Q \\\\) | \\\\( \\\\alpha-PAC \\\\) |\\n|-----------------|-------------|-----------|-------------------|-----------|------------|-----------|-----------------|\\n| Gato:Control    | 32          | N/A       | 63.6\u2020             | 75.8      | 84.6       | 87.7      | 92.1            |\\n| RC:Tower        | 7           | 75         | 61.0              | 64.0      | 71.3       | 69.3      | 69.6            |\\n| RC:Pyramid      | 30          | 75         | 64.5              | 64.0      | 62.4       | 63.5      | 64.9            |\\n| RC:Insertion    | 3           | 97         | 71.3              | 81.0      | 79.7       | 80.3      | 89.3            |\\n| CHEF:sim        | 1           | 28         | 17.0              | 11.0      | 55.0       | 52.0      |                 |\\n\\nFigure 4. Iso-Return comparison of \\\\( BC+Q \\\\) vs PAC. The return profile (top) contrasts the expected average return between the BC baseline and the RL objective across all model scales. The Iso-Return contours (bottom) depict how the reward landscape over the parameter-FLOPs landscape shifts between using the BC objective (dashed contours) and the RL objectives (solid contours). The parameter scaling laws indicating the 'efficient frontier' for both model families are also plotted in red as a reference for the likely scaling progression of the return landscape.\\n\\nFor our comparison, we consider the following PAC-based models: \\\\( \\\\alpha-PAC \\\\) (our main actor-critic model); \\\\( BC+Q \\\\) (\\\\( \\\\alpha = 1, \\\\beta > 0 \\\\)) as a baseline which also learns a Q-function, but never leverages it for policy optimization (we found this to always be at least as good as pure BC in pre-liminary experiments); and \\\\( \\\\text{FilteredBC} \\\\) (\\\\( \\\\alpha = 1, \\\\beta = 0 \\\\)) which does not learn a Q-function and is only trained on successful episodes of our data mix to mimic a 'pure' BC setting. We also add \\\\( \\\\alpha-PAC \\\\) as our best actor-critic model which uses a different value for the BC/RL trade-off \\\\( \\\\alpha \\\\) for each dataset to obtain the best performance and demonstrate that our method can be optimally tuned to deal with data of widely varying quality in the same training mixture. More detailed ablations on the choice of \\\\( \\\\alpha \\\\) and \\\\( \\\\beta \\\\) are presented in Appendix D.1. For a fair comparison to the 1.2B parameter versions of Gato and RoboCat, we use PAC in its L-size with about 1B parameters and train for 3M updates. All details of the pre-training data mix and optimizer hyper-parameters are reported in Appendix C.5. Each PAC model is evaluated across all task families in simulation and the results are reported in Table 1. Where available, we cite the baseline results for Gato and RoboCat (RC) directly from their respective papers. In general, the Q-function-based PAC outperforms BC across tasks, confirming our hypothesis that offline RL is a viable alternative for training large models and we note that a V-function based variant also achieves similar results (see Appendix F.1).\\n\\nIn more detail: On the Control Suite tasks \\\\( \\\\alpha-PAC \\\\) outperforms all baseline tasks reaching 87.7% of expert performance and \\\\( \\\\alpha-PAC \\\\) even boosts it further to 92.1%. It is also worth noting that our BC baselines already outperform the Gato results, potentially due to PAC's improved architecture. On the RoboCat tasks, PAC performs commensurately with all BC baselines and outperforms prior work especially on the more difficult Tower task achieving \\\\( \\\\approx 70\\\\% \\\\) success rate, but the difference is less pronounced since the respective datasets come from near expert policies (>75% success).\\n\\nThe biggest difference is observed on the insertion task where \\\\( \\\\text{FilteredBC} \\\\) and \\\\( \\\\text{BC+Q} \\\\) already improve \\\\( \\\\approx 10\\\\% \\\\) over the RoboCat baseline and \\\\( \\\\alpha-PAC \\\\) yields another significant improvement to 89.3%. Finally, for the stacking task from CHEF which has the poorest data quality \u2013 collected form a sub-optimal policy that only achieved 28% success \u2013 we use the expert performance definition of Reed et al. (2022).\"}"}
{"id": "tl2qmO5kpD", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nAttention blocks is decoupled from the input sequence length to be $O(NZ^2)$. This effectively shifts conducting the self-attention operation from the inputs to the latents, in our case reducing the computation and memory usage by a factor of $(2^{634/32})^2 \\\\approx 6,775$.\\n\\n### B.4. Policy and Value Decoding\\n\\nTo obtain the two types of outputs, we create dedicated query vectors for each output which are then used to query the latent encodings $z_M$; querying information from $z_M$ is again implemented via cross-attention following the decoding architecture of Perceiver-IO (Jaegle et al., 2022) and deriving keys and values from the latents $z_M$.\\n\\nSpecifically, to acquire an action prediction of shape $N_A \\\\times N_B$, with $N_A$ denoting the cardinality of the action space $|A|$ and $N_B$ the number of bins each element gets discretized into, we create $N_A$ learnable policy queries $q_\\\\pi \\\\in \\\\mathbb{R}^{N_A \\\\times N_O}$ to cross-attend to $z_M$, $X$-ATTN$(q_\\\\pi, z_M)$, and get a $N_A \\\\times N_O$ shaped output, which are then linearly projected to $N_A \\\\times N_B$ shaped policy logits.\\n\\nWhereas for the Q-value estimate, since the information about the action are not contained in the encoded latents $z_M$ but required for getting the estimate, the Q-value queries $q_Q$ should not be simply created as randomly initialized trainable vectors as for $q_\\\\pi$. Instead, they are computed by encoding the action $a_t \\\\in \\\\mathbb{R}^{N_A}$ via an action encoder $\\\\phi_A$ composed of an multi-scale normalizer ($\\\\mathbb{R}^{N_A} \\\\rightarrow [-1, 1]^{N_A \\\\times N_G}$, cf. Equation (15) ) followed by two linear projections ($L_A^1 : [-1, 1]^{N_A \\\\times N_G} \\\\rightarrow \\\\mathbb{R}^{(N_A \\\\times N_G) \\\\times D_O}$, $L_A^2 : \\\\mathbb{R}^{(N_A \\\\times N_G)} \\\\rightarrow \\\\mathbb{R}^{1 \\\\times D_O}$): $\\\\phi_A = L_A^2 \\\\cdot L_A^1 \\\\cdot \\\\phi_{multi-scale}$\\n\\n$\\\\phi_A$ is then used to query the latents via cross-attention, $X$-ATTN$(q_Q, z_M)$, the output of which ($1 \\\\times N_O$) is then mapped to $1 \\\\times N_Q$ to generate the $N_Q$ logits. Incorporating the action information by encoding it into a query at the decoding stage instead of concatenating it along with the other observations at the input has the advantage that this way the action is less likely to be ignored by the model (a common problem encountered when learning Q-values). It also allows efficient evaluation of the Q-function for multiple action samples: the latent representation $z_M$ is not dependent on the action and therefore needs to be computed only once to be queried by multiple action samples.\\n\\n### C. Experimental Details\\n\\n#### C.1. Architecture Hyperparameters for Scaling\\n\\nWe vary three parameters of the architecture as detailed in Table 4: The size of the latent vectors $D_Z$, the number of self-attention blocks $M$ and the widening factor $W$ of the attention blocks which define the ratio between the residual MLPs' hidden size to input size. With all the other fixed hyperparameters reported in Appendix C.2, the resulting model sizes range from $32M$ parameters (comparable to RT-1 (Brohan et al., 2022) and Q-Transformer (Chebotar et al., 2023)) to $988M$ parameters (close to the largest versions of Gato (Reed et al., 2022) and RoboCat (Bousmalis et al., 2023).\\n\\n| Scale | $(#(params))$ |\\n|-------|---------------|\\n| XXS   | $32M$         |\\n| XS    | $73M$         |\\n| S     | $164M$        |\\n| M     | $391M$        |\\n| L     | $988M$        |\\n\\n#### C.2. Fixed Architecture Hyperparameters\\n\\nTable 5 provides an overview of all employed model parameters which are kept fixed across model scales. $N_P$, $N_V$, $N_V^\\\\tau$, $N_L^\\\\tau$, $N_A$ all refer to input dimensions and are chosen to accommodate all tasks the data is originating from. $N_E$ and $N_T$ also relate to input dimensions, but depend on the pre-processing that is applied to the data (e.g. the SentencePiece tokenizer for text input, or a ResNet and for image input). $N_G$ is the number of scales (gains) for our proposed multi-scale normalizer. Finally, $N_B$ and $N_Q$ refer to the number of discrete value bins used for the discretization of the action and Q-value respectively, $N_Z$ refers to the number of latent vectors.\\n\\n| Hyperparameter | Value |\\n|----------------|-------|\\n| $N_P$ | $223$ |\\n| $N_V$ | $5$ |\\n| $N_V^\\\\tau$ | $3$ |\\n| $N_L^\\\\tau$ | $50$ |\\n| $N_A$ | $38$ |\\n| $N_G$ | $8$ |\\n| $N_E$ | $100$ |\\n| $N_T$ | $32,000$ |\\n| $N$ | $2634$ |\\n| $N_Z$ | $32$ |\\n| $N_B$ | $101$ |\\n| $N_Q$ | $101$ |\\n| $D_I$ | $256$ |\"}"}
{"id": "tl2qmO5kpD", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nTable 6. The data mixture used for the scaling experiments. For each domain we report the modalities (P = proprioception, V = vision, L = language, A = actions), the number of tasks, the number of episodes recorded, the effective number of trajectories (each consisting of five timesteps), the number of tokens contributed, the percentage of successful episodes and the weight $\\\\lambda$ of the domain during the data sampling process.\\n\\n| Domain       | Mod | # (T) | #(Ep) | #(Traj) | #(Tok) | success | $\\\\lambda$ |\\n|--------------|-----|-------|-------|---------|-------|---------|-----------|\\n| Gato: Control | P, L, A | 32    | 468k  | 187M    | 430B  | 47%     | 8         |\\n| CHEF: sim    | P, V, L, A | 30    | 755k  | 60M     | 430B  | 28%*    | 2         |\\n| CHEF: real   | P, V, L, A | 30    | 2M    | 169M    | 1.12T | 12%*    | 2         |\\n| RC: Tower    | P, V, L, A | 7     | 121k  | 19M     | 122B  | 72%     | 1         |\\n| RC: Pyramid  | P, V, L, A | 30    | 194k  | 31M     | 195B  | 52%     | 1         |\\n| RC: Insertion| P, V, L, A | 3     | 100k  | 40M     | 154B  | 97%     | 2         |\\n| $\\\\sum$       |     | 132   | 3.638M| 506M    | 2.45T | avg = 42%** |\\n\\nC.3. Data Modalities and Tokenization\\n\\nImages\\nAt each timestep, PAC processes up $N_V + N_G$ input images. For domains which provide fewer observations or goal images, these inputs are zero-padded and masked out for the subsequent processing. Across all our experiments, we use an image resolution of $80 \\\\times 80$ pixels. Each image is encoded using a simple ResNet (He et al., 2016) with three blocks using $(32, 64, 64)$ channels, $3 \\\\times 3$ pooling kernels and $2 \\\\times 2$ striding. After the convolutions, each image has been downsampled to $10 \\\\times 10$ 'image tokens' which are projected into an embedding space of $D_I$ dimensions. Under this embedding scheme, each image is counted as 100 tokens in our experiments. Our default implementation uses eight image observations at each timestep resulting in the processing of 800 image tokens.\\n\\nProprioception and Actions\\nOur multi-scale normalizer (cf. Appendix B.1) represents each floating point number of the proprioceptive reading as $N_G$ tokens. Each of these tokens is then projected into an embedding space of $D_I$ dimensions. Our default implementation uses 223 proprioception observations at each timestep resulting in the processing of 1,784 proprioception tokens. In domains with fewer proprioception observations, the remaining inputs are zero-padded and masked out for the subsequent processing.\\n\\nLanguage\\nIn order to differentiate between different tasks in the same domain, e.g. to tell the model whether to execute a run, stand or walk policy in Control Suite's humanoid domain, we feed a simplified task instruction as a language task description ($\\\\tau_L$) at each timestep. For each dataset, the task instruction is constructed from the dataset name (cf. Table 9) and looks like humanoid.walk or sim.insert.large.gear. We use the SentencePiece tokenizer (Kudo & Richardson, 2018) to tokenize the task instruction where each token represents an integer index into a vocabulary of $N_T$ tokens. Each language token is subsequently projected into an embedding space of $D_I$ dimensions. In our default implementation, we use at most 50 language tokens for the task instruction and zero-pad and mask language tokens in cases of shorter instructions.\\n\\nIn summary, our default implementation processes a total amount of 2,634 input tokens per timestep broken down into: 800 image tokens, 1,784 proprioception tokens and 50 text tokens.\\n\\nC.4. Data Mixtures\\nDuring our scaling experiments (cf. Section 4.1), we use a data mixture consisting of 51 datasets and depict a selection of them in Figure 6. We partition the datasets into six groups as outlined in Table 6. When sampling trajectories during training, we first sample uniformly across all groups with a weight of $\\\\lambda$ and then sample uniformly within the group to draw a trajectory sample from a concrete dataset. The effective sampling ratio for each dataset is shown as $P_{eff}$ in Table 9. To maintain the sampling ratios over the entire training process, we simply loop all underlying datasets deterministically such that none of them is ever exhausted.\\n\\nDuring our large-scale pre-training experiments (cf. Section 4.2), we augment the data mixture already used in the scaling experiments (see the previous section), but add the full amount of RoboCat data for the Tower and Pyramid domains from Bousmalis et al. (2023) to enable a fair comparison with prior work. The adjusted overall data mixture is shown in Table 10 and the changes to the individual datasets are reported in Table 11.\\n\\nC.5. Optimization Hyperparameters\\nFor all large-scale experiments (cf. Sections 4.1 and 4.2) we use optimizer hyperparameters as reported in Table 12. Importantly, we use the AdamW optimizer (Loshchilov & Hutter, 2017) with a learning rate schedule which starts at $lr_{init}$, ramps up linearly for $lr_{warmup}$ steps to $lr_{peak}$ and is then cosine-annealed to $lr_{end}$ over $lr_{decay}$ steps which amounts to approximately one epoch in our data mix. In line with the protocol of Hoff-17...\"}"}
{"id": "tl2qmO5kpD", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nFigure 6. A selection of the domains and tasks in our data mix.\\nTop left: Control Suite features 32 different continuous control tasks across 15 different embodiments with a great variance in proprioception and action spaces.\\nTop right: Stacking RGB objects into different configurations (pyramids and towers) with a simulated Panda arm.\\nBottom left: Inserting gears onto pegs in simulation.\\nBottom right: Performing the RGB stacking task on a real Sawyer robot.\\n\\nmann et al. (2022), we decay the learning rate by one order of magnitude over one epoch. We set \\\\( lr^{\\\\text{decay}} \\\\) steps according to the respective epoch lengths for each of our experiments, i.e. to \\\\( 2.7 \\\\times 10^6 \\\\) in the scaling experiments and to \\\\( 4.7 \\\\times 10^6 \\\\) in the pre-training experiments.\\n\\nFor all PAC models, we keep the TD loss scale \\\\( \\\\beta \\\\) constant at \\\\( 38 \\\\) while varying the BC vs RL trade-off \\\\( \\\\alpha \\\\) between \\\\( 1.0 \\\\) for our BC+Q and FilteredBC baselines and \\\\( 0.75 \\\\) for the PAC model series. \\\\( \\\\alpha^{-\\\\text{PAC}} \\\\) sets \\\\( \\\\beta = 19 \\\\) and \\\\( \\\\alpha \\\\) of \\\\( 0.75 \\\\) for the Control Suite and RoboCat data (i.e. leaning more towards BC given the high average success rates in the data) and \\\\( \\\\beta = 199, \\\\alpha = 0.0 \\\\) for the CHEF datasets (i.e. relying fully on the RL term in the presence of highly sub-optimal data). All PAC+V models use \\\\( \\\\alpha = 0.0, \\\\beta = 38 \\\\) and a temperature \\\\( \\\\tau \\\\) of \\\\( 1 \\\\times 10^{-4} \\\\).\\n\\nD. Sensitivity and Ablation Experiments\\n\\nD.1. Sensitivity to Hyperparameters\\n\\nHere we report additional experiments that demonstrate the influence of some of the hyperparameters and that informed the settings that we used in other experiments.\\n\\nD.1.1. BC LOSS SCALE \\\\( \\\\alpha \\\\)\\n\\nIn order to perform offline reinforcement learning (RL), we simply interpolate between the behavioral cloning (BC) loss and the RL loss via a parameter \\\\( \\\\alpha \\\\). We found that, depending on the nature of the data, different parameters work best for different datasets. Empirically, we observed that the more 'expert data' there is (i.e. a higher base success rate of the tasks in the dataset), the higher the contribution of the BC loss should be. Conversely, less expert data can benefit more from RL. As seen in Table 7, for the control suite, averaged over 32 tasks, setting \\\\( \\\\alpha \\\\) at around 0.8 works best. This is because most of the data has been collected by expert agents and therefore small divergence from BC distribution can gain improvement over the BC. This value is different for, for example, CHEF data, where \\\\( \\\\alpha = 0.0 \\\\) (i.e. pure RL) works best mainly because the data expert level is low and this dataset contains more low quality data.\\n\\nTable 7. Control Suite performance of an XS-sized model using different BC loss scales \\\\( \\\\alpha \\\\). The percentage of achieved expert average reward across 100 trials per task and the standard-error-based 95% CIs are reported.\\n\\n| Domain     | # (T) | \\\\( \\\\alpha = 0.0 \\\\) | \\\\( \\\\alpha = 0.4 \\\\) | \\\\( \\\\alpha = 0.8 \\\\) | \\\\( \\\\alpha = 1.0 \\\\) |\\n|------------|-------|---------------------|---------------------|---------------------|---------------------|\\n| Gato: Control 32 |       | 36.8 [33.9, 39.6]   | 74.3 [70.2, 78.3]   | 85.1 [80.4, 89.8]   | 82.4 [76.5, 88.2]   |\\n\\nD.1.2. POLICY LOSS SCALE \\\\( \\\\beta \\\\)\\n\\nFor efficient learning we share the network parameters between the policy and critic networks. This, however, requires some adjustments to balance these two losses for optimal performance. Therefore, we introduce a hyperparameter, \\\\( \\\\beta \\\\), that trades off the policy loss and the critic loss. To illustrate the sensitivity of this parameter, we use the CHEF tasks and perform pure reinforcement learning by setting BC loss scale \\\\( \\\\alpha = 0.0 \\\\) while sweeping over different values of \\\\( \\\\beta \\\\). As can be seen in Table 8, for this task, a lower contribution of the policy loss leads to better results. In general, we have found that setting the \\\\( \\\\beta \\\\) value to 0.005 achieves good performance across various tasks.\\n\\nTable 8. Simulated RGB stacking performance of an XS-sized model using different policy loss scales \\\\( \\\\beta \\\\). The average success rates across 100 trials per task and their corresponding Wilson score intervals for \\\\( \\\\alpha_{W} = 0.05 \\\\) are reported.\\n\\n| Domain     | # (T) | \\\\( \\\\beta = 1.0 \\\\) | \\\\( \\\\beta = 0.1 \\\\) | \\\\( \\\\beta = 0.01 \\\\) | \\\\( \\\\beta = 0.005 \\\\) |\\n|------------|-------|-------------------|-------------------|-------------------|-------------------|\\n| CHEF: sim 1 |       | 22.0 [15.0, 31.7] | 29.0 [21.0, 38.5] | 76.0 [66.8, 83.3] | 83.0 [74.5, 89.1] |\\n\\nD.2. Architecture Ablations\\n\\nTo understand what architectural decisions contribute to the performance we conduct a set of ablation studies.\\n\\n18\"}"}
{"id": "tl2qmO5kpD", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nIn this section, we look into the multi-scale normalizer for encoding continuous inputs, the output action-cross-attention to generate Q-values, and the task conditioning. For compute efficiency reasons, we perform these experiments on a subset of the data and with smaller model sizes.\\n\\nD.2.1. Input Encoding\\n\\nOur multi-scale normalizer is used for embedding both the proprioception observations and actions. We compare the multi-scale normalizer against a vanilla linear encoder and a token-based encoder similar to the one used by Reed et al. (2022). The linear encoder directly embeds the inputs to a single embedding using a linear layer followed by a normalization layer. The token-based encoder uses a lookup-table-based encoder analogous to the language encoder. Across all tokenization variants, the embedded proprioception tokens are separately cross-attended while the embedded action tokens are projected to a single embedding vector.\\n\\nTable 13 reports separate results on the control suite tasks (using our XS-sized model) and RoboCat pyramid and tower tasks (using our S-sized model), where for better comparability with RoboCat (Bousmalis et al., 2023) we set the number of action bins to $1024$. For Control Suite, the linear embedder shows the lowest performance, likely caused by not being able to deal with the range of proprioception measurements. Based on the $\\\\mu$-law discretization, the encoder is able to address the high range of measurements. However, for the RoboCat tasks, the $\\\\mu$-law encoder performs the worst, possibly due to discarding neighborship relations in the data. It also comes with a larger memory footprint due to the lookup table embedding. The multi-scale normalizer addresses all these issues and performs best.\\n\\nD.2.2. Action Cross-Attention\\n\\nIn addition to using a perceiver backbone for our model, a key design choice was also to use the action as output query rather than having it as an input. This has the advantage that we can use a common encoder for the policy and the Q-value estimator and enables quick evaluation of actions because the latent can be cached. Furthermore, this shortens the path from actions to Q-values and the action is thus less likely to be ignored. In the last column of Table 13, we ablate this architectural choice against using the action as input modality for control suite tasks, where we made use of attention masking in order to hide the action input from the policy output. We observe a decrease in performance, likely due to the aforementioned reasons.\\n\\nD.2.3. Comparison to Q-Transformer and Return Conditioned Offline RL\\n\\nWe present additional results for a Q-transformer baseline when training a XS (73M parameters) model on the 32 domains from the control suite in Table 14. As can be observed from the table, in aggregate the Q-transformer baseline is not better than BC. However, when separating results out by domain, we can observe that Q-transformer matches the performance of PAC for lower dimensional domains (e.g., cartpole, finger turn) but fails in some of the higher dimensional domains such as dog walk and humanoid walk. In addition, we ran a baseline that uses return conditioning to train on all data but then uses the same threshold as used for FilteredBC at test time (again training an XS model). This serves as a sensible reference roughly corresponding to a Decision Transformer implementation. We see that this recovers performance roughly equivalent to BC again but does not result in further improvement. Finally, we also ablated the choice of the distributional Q-function (no dist. Q) in the table, verifying that the performance benefit of PAC over other methods is not just due to the distributional Q-function (though it clearly benefits from it).\\n\\nDetails on Q-transformer implementation\\n\\nA naive application of Q-transformer (Chebotar et al., 2023) to our setting is not feasible due to the costly iterative (arg-)max computation involved, which would be prohibitively expensive in our setting where we consider up to 38 action dimensions. We thus implement a version with independent Q-functions for each action dimension. In particular, we parameterize Q values per action dimension as $Q_{\\\\theta}(a_i, s, \\\\tau)$ and aggregate Q-values across dimensions for bootstrapping in the TD-update according to:\\n\\n$$V_{\\\\theta}(s_t) = \\\\frac{1}{\\\\left| A \\\\right|} \\\\sum_{i=0}^{\\\\left| A \\\\right|} \\\\max_{a_i} Q_{\\\\theta}(a_i, s, \\\\tau),$$\\n\\nUsing this definition, we can implement the loss $L_{QT}(\\\\theta) = \\\\mathbb{E}_{(s', a, r, s') \\\\in D}\\\\left[\\\\frac{1}{\\\\left| A \\\\right|} \\\\sum_{i=0}^{\\\\left| A \\\\right|} (r + \\\\gamma V_{\\\\theta}(s') - Q_{\\\\theta}(a_i, s, \\\\tau))^2 + \\\\alpha \\\\mathbb{E}_{(s', a, r, s') \\\\in D}\\\\left(\\\\sum_{a' \\\\neq a_i} (Q_{\\\\theta}(a', s, \\\\tau) - 0)^2\\\\right)\\\\right].$$(17)\\n\\nThis is essentially a cooperative multi-agent RL formulation as in Seyde et al. (2023). We also experimented with learning an additional $V$-function instead of aggregating across dimensions (and then regressing Q-values by using this V-function for bootstrapping), which however gave statistically equivalent results. We swept over the hyperparameter $\\\\alpha$ (in $[0.1, 1, 10]$) as well as learning rates in order to provide a fair comparison.\"}"}
{"id": "tl2qmO5kpD", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nTable 2. Success rates with Wilson score intervals for $\\\\alpha = 0.05$ for CHEF:real tasks (400 trials per task) for different objectives, as well as for an RL finetuning run with self-generated data (RLFT).\\n\\n| Domain # (T) | BC+Q | $\\\\alpha$-PAC | $\\\\alpha$-PAC (RLFT) |\\n|--------------|------|--------------|---------------------|\\n| CHEF: real   | 5    | 69.8         | 93.2                |\\n|              |      | [6.1, 8.2]   | [92.0, 94.2]        |\\n\\nWe can observe that PAC learns policies with good success rates while all BC baseline are barely able to match the average performance of the data collecting policy. This highlights that our method fulfills one of the main promises of offline RL: it can learn successful policies even from severely sub-optimal data. We provide an additional comparison to a Q-transformer (Chebotar et al., 2023) baseline (which is non-trivial to scale and does not outperform BC) in the Appendix D.2.3. Additional ablations regarding the architectural choices are also presented in the appendix.\\n\\n4.3. RL Fine-tuning and Self-improvement\\n\\nWe now demonstrate how PAC's built-in critic can be leveraged to transition into different finetuning scenarios and use this to 'master' a target task (i.e. success rate $> 90\\\\%$). For this we replicate the 5-DoF object stacking scenario of (Lee et al., 2021) on a Rethink Sawyer arm in the real world. Initially, we deploy different PAC models which have been pre-trained for 3M steps on the full data mix from Section 4.2. The best of these models ($\\\\alpha$-PAC) achieves a success rate of 69.8% which far exceeds what is learnable from this data with BC (see Table 2). Additionally we verify that we can change the value of $\\\\alpha$ during training, by first training with $\\\\alpha = 1$ for 3M steps (cf. BC+Q in Table 2) followed by 3M steps with $\\\\alpha = 0$, which achieves 61.9% [60.0, 63.8], in line with the $\\\\alpha$-PAC result. That demonstrates that we can safely transition from BC to RL at any point during the training process.\\n\\nNext, we follow the iterative improvement protocol of Lampe et al. (2023) and collect the evaluation trials in an additional dataset. Afterwards, we add this data to the data mix (retaining all initial data used in previous sections) and train the model for another $\\\\approx 250k$ steps. We repeat this process multiple times, each time adding more data. This cycle of feeding back self-generated data to the offline RL optimization provides a significant performance boost, increasing the success rate in each round, until eventually reaching a near-mastery level of 93.2%. Average scores for each round, and the number of episodes collected for self-improvement, are summarized in Table 3. More detailed scores across the sub-tasks can be found in Appendix F.2. Finally, we repeat this self-improvement experiment for all Control Suite tasks, adding 10,000 episodes per task and performing RL finetuning for 500k steps starting from the checkpoint after three rounds of RLFT in Table 3. This results in an increase to 94.3% [91.3, 97.3], up from 92.1% achieved by $\\\\alpha$-PAC.\\n\\nThe fine-tuning experiments highlight that PAC both outperforms BC on this challenging, low-quality data and can hill-climb its performance towards mastery using self-generated data \u2013 a feat that is only possible with an RL style self-improvement loop. Interestingly, even after mastering the CHEF:real domain, $\\\\alpha$-PAC's performance on the other domains does not decline as a side-effect (cf. Table 20 in the Appendix). It is also worth noting that the L-sized version of PAC runs at 20 Hz on a local Nvidia RTX 3090 GPU during this real-robot experiment.\\n\\n5. Discussion\\n\\nIn this work, we demonstrated that offline actor-critic methods can scale to large models of up to 1B parameters and learn a wide variety of 132 control and robotics tasks. On these tasks, our RL-trained models outperform strong BC baselines, especially in the presence of sub-optimal training data. Our finetuning experiments also showed that RL can be effectively applied after pre-training without any model changes, which enabled the mastery of a real robot task improving from a 70% to a 90% success rate using RL and autonomously collected data. The scaling analysis provides insights into the optimal model sizes and training durations for our datasets and indicates that the performance of offline RL scales better with compute than pure BC. Finally, our system allows for a gradual and stable transition between BC and RL learning, and can process data of various modalities simultaneously, while remaining efficient enough to allow our biggest model to control a real robot at 20 Hz.\\n\\nHowever, our work also has some limitations: First, offline RL requires reward annotations, which can be costly. Progress in the development of universal reward functions (Du et al., 2023) or unsupervised reward labeling (Chebotar et al., 2021) could therefore greatly broaden the applicability of our method. Second, given the wide variety of domains considered, we saw no strong indications of transfer across domains.\"}"}
{"id": "tl2qmO5kpD", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models. However, we would expect generalization to improve with the use of datasets which have more overlap between tasks and domains as in Zitkovich et al. (2023).\\n\\nOverall, we believe our work could pave the way for training large models via offline actor-critic methods on ever larger datasets of robot data. Additionally, an exciting opportunity lies in further scaling offline actor-critic learning to models of multiple billion parameters, and combining our systems with pre-trained VLMs, or even exploring offline actor-critic RL as an alternative method for generative pre-training in language models.\\n\\nAcknowledgements\\nWe would like to thank Konstantinos Bousmalis, Francesco Nori and the members of the DeepMind Control team for feedback on early versions of this manuscript. We would like to thank the greater robotics community at Google DeepMind team for their feedback and contributions with respect to software and robot hardware support. Finally, we thank the anonymous reviewers for their valuable feedback.\\n\\nImpact Statement\\nThis work presents new methods for training generalist agents for control applications including robotic manipulation. The general impact on society from generalist robotics agents at this point is not well understood, and we encourage further work into their risks and benefits. We emphasize that both our model and the actor-critic methods introduced for training at scale are for research use only and are not currently deployed in any production scenario to any users, and we thus expect no direct impact resulting from this work.\\n\\nIn a broader sense, Perceiver-Actor-Critic shares the majority of safety concerns discussed in Gato (Reed et al., 2022) and RoboCat (Bousmalis et al., 2023). In particular, our self-improvement loop has the same safety concerns attached to the BC-style self improvement in Bousmalis et al. (2023). It is worth emphasising that our improvement step is carried out offline from human defined reward functions, and no learning happens while interacting with any real world system. Additionally, in some sense the fact that we use rewards to \u2018shape\u2019 the behaviour of the learned policies makes work on safety via value alignment to human preferences (Russell, 2019; Christiano et al., 2017) more directly applicable although much work still remains to be done on this front.\\n\\nReferences\\nAbdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M. Maximum a posteriori policy optimisation. In International Conference on Learning Representations, 2018.\\n\\nAbdolmaleki, A., Huang, S., Vezzani, G., Shahriari, B., Springenberg, J. T., Mishra, S., Tirumala, D., Bours-Dekker, A., Bousmalis, K., Gy\u00f6rgy, A., et al. On multi-objective policy optimization as a tool for reinforcement learning: Case studies in offline RL and finetuning, 2022. URL https://openreview.net/forum?id=bilHNPhT6-\\n\\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\\n\\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O., and Zaremba, W. Hindsight experience replay. Advances in neural information processing systems, 30, 2017.\\n\\nBellemare, M. G., Dabney, W., and Munos, R. A distributional perspective on reinforcement learning. In International conference on machine learning, pp. 449\u2013458. PMLR, 2017.\\n\\nBousmalis, K., Vezzani, G., Rao, D., Devin, C., Lee, A. X., Bauza, M., Davchev, T., Zhou, Y., Gupta, A., Raju, A., et al. RoboCat: A self-improving foundation agent for robotic manipulation. arXiv preprint arXiv:2306.11706, 2023.\\n\\nBrohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jackson, T., Jesmonth, S., Joshi, N., Julian, R., Kalashnikov, D., Kuang, Y., Leal, I., Lee, K.-H., Levine, S., Lu, Y., Malla, U., Manjunath, D., Mordatch, I., Nachum, O., Parada, C., Peralta, J., Perez, E., Pertsch, K., Quiambao, J., Rao, K., Ryoo, M., Salazar, G., Sanketi, P., Sayed, K., Singh, J., Sontakke, S., Stone, A., Tan, C., Tran, H., Vanhoucke, V., Vega, S., Vuong, Q., Xia, F., Xiao, T., Xu, P., Xu, S., Yu, T., and Zitkovich, B. Rt-1: Robotics transformer for real-world control at scale. In arXiv preprint arXiv:2212.06817, 2022.\\n\\nChebotar, Y., Hausman, K., Lu, Y., Xiao, T., Kalashnikov, D., Varley, J., Irpan, A., Eysenbach, B., Julian, R., Finn, C., et al. Actionable models: Unsupervised offline reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021.\\n\\nChebotar, Y., Vuong, Q., Hausman, K., Xia, F., Lu, Y., Irpan, A., Kumar, A., Yu, T., Herzog, A., Pertsch, K., et al. Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions. In 7th Annual Conference on Robot Learning, 2023.\"}"}
{"id": "tl2qmO5kpD", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021.\\n\\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf.\\n\\nDu, Y., Konyushkova, K., Denil, M., Raju, A., Landon, J., Hill, F., de Freitas, N., and Cabi, S. Vision-language models as success detectors. arXiv preprint arXiv:2303.07280, 2023.\\n\\nFujimoto, S. and Gu, S. S. A minimalist approach to offline reinforcement learning. Advances in neural information processing systems, 34:20132\u201320145, 2021.\\n\\nFujimoto, S., Meger, D., and Precup, D. Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pp. 2052\u20132062. PMLR, 2019.\\n\\nFujimoto, S., Meger, D., Precup, D., Nachum, O., and Gu, S. S. Why should i trust you, bellman? the bellman error is a poor replacement for value error. arXiv preprint arXiv:2201.12417, 2022.\\n\\nHansen, N., Su, H., and Wang, X. Td-mpc2: Scalable, robust world models for continuous control. arXiv preprint arXiv:2310.16828, 2023.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\\n\\nHenighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T. B., Dhariwal, P., Gray, S., et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.\\n\\nHilton, J., Tang, J., and Schulman, J. Scaling laws for single-agent reinforcement learning. arXiv preprint arXiv:2301.13442, 2023.\\n\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\\n\\nJaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with iterative attention. In International conference on machine learning, pp. 4651\u20134664. PMLR, 2021.\\n\\nJaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ding, D., Koppula, S., Zoran, D., Brock, A., Shelhamer, E., Henaff, O. J., Botvinick, M., Zisserman, A., Vinyals, O., and Carreira, J. Perceiver IO: A general architecture for structured inputs & outputs. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=fILj7WpI-g.\\n\\nJanner, M., Li, Q., and Levine, S. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:1273\u20131286, 2021.\\n\\nKang, M., Zhu, J.-Y., Zhang, R., Park, J., Shechtman, E., Paris, S., and Park, T. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\\n\\nKostrikov, I., Nair, A., and Levine, S. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.\\n\\nKudo, T. and Richardson, J. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\\n\\nKumar, A., Zhou, A., Tucker, G., and Levine, S. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191, 2020.\\n\\nKumar, A., Agarwal, R., Geng, X., Tucker, G., and Levine, S. Offline q-learning on diverse multi-task data both scales and generalizes. arXiv preprint arXiv:2211.15144, 2022a.\\n\\nKumar, A., Singh, A., Ebert, F., Nakamoto, M., Yang, Y., Finn, C., and Levine, S. Pre-training for robots: Offline rl enables learning new tasks from a handful of trials. arXiv preprint arXiv:2210.05178, 2022b.\\n\\nLampe, T., Abdolmaleki, A., Bechtle, S., Huang, S. H., Springenberg, J. T., Bloesch, M., Groth, O., Hafner, R., Hertweck, T., Neunert, M., Wulfmeier, M., Zhang, J., Nori, F., Heess, N., and Riedmiller, M. Mastering stacking of diverse shapes with large-scale iterative reinforcement learning on real robots, 2023.\"}"}
{"id": "tl2qmO5kpD", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nLange, S., Gabel, T., and Riedmiller, M. Batch reinforcement learning. In Reinforcement learning: State-of-the-art, pp. 45\u201373. Springer, 2012.\\n\\nLee, A. X., Devin, C., Zhou, Y., Lampe, T., Bousmalis, K., Springenberg, J. T., Byravan, A., Abdolmaleki, A., Gileadi, N., Khosid, D., Fantacci, C., Chen, J. E., Raju, A., Jeong, R., Neunert, M., Laurens, A., Saliceti, S., Casarini, F., Riedmiller, M., Hadsell, R., and Nori, F. Beyond pick-and-place: Tackling robotic stacking of diverse shapes. arXiv preprint arXiv:2110.06192, 2021.\\n\\nLee, K.-H., Nachum, O., Yang, M. S., Lee, L., Freehman, D., Guadarrama, S., Fischer, I., Xu, W., Jang, E., Michalewski, H., et al. Multi-game decision transformers. Advances in Neural Information Processing Systems, 35:27921\u201327936, 2022.\\n\\nLevine, S., Kumar, A., Tucker, G., and Fu, J. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\\n\\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\\n\\nNeumann, O. and Gros, C. Scaling laws for a multi-agent reinforcement learning model. arXiv preprint arXiv:2210.00849, 2022.\\n\\nOcto Model Team, Ghosh, D., Walke, H., Pertsch, K., Black, K., Mees, O., Dasari, S., Hejna, J., Xu, C., Luo, J., Kreiman, T., Tan, Y., Sadigh, D., Finn, C., and Levine, S. Octo: An open-source generalist robot policy. https://octo-models.github.io, 2023.\\n\\nOpen X-Embodiment Collaboration. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023.\\n\\nOpenAI. Gpt-4 technical report, 2023.\\n\\nPeng, X. B., Kumar, A., Zhang, G., and Levine, S. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\\n\\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-maron, G., Gim\u00e9nez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al. A generalist agent. Transactions on Machine Learning Research, 2022.\\n\\nRiedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., Wiele, T., Mnih, V., Heess, N., and Springenberg, J. T. Learning by playing solving sparse reward tasks from scratch. In International conference on machine learning, pp. 4344\u20134353. PMLR, 2018.\\n\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models, 2021.\\n\\nRussell, S. Human compatible: Artificial intelligence and the problem of control. Penguin, 2019.\\n\\nSeyde, T., Werner, P., Schwarting, W., Gilitschenski, I., Riedmiller, M., Rus, D., and Wulfmeier, M. Solving continuous control via q-learning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=U5XOGxAgccS.\\n\\nShridhar, M., Manuelli, L., and Fox, D. Perceiver-actor: A multi-task transformer for robotic manipulation. In Conference on Robot Learning, pp. 785\u2013799. PMLR, 2023.\\n\\nTunyasuvunakool, S., Muldal, A., Doron, Y., Liu, S., Bohez, S., Merel, J., Erez, T., Lillicrap, T., Heess, N., and Tassa, Y. dm control: Software and tasks for continuous control. Software Impacts, 6:100022, 2020. ISSN 2665-9638. doi: https://doi.org/10.1016/j.simpa.2020.100022. URL https://www.sciencedirect.com/science/article/pii/S2665963820300099.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nWang, Z., Novikov, A., Zolna, K., Merel, J. S., Springenberg, J. T., Reed, S. E., Shahriari, B., Siegel, N., Gulcehre, C., Heess, N., et al. Critic regularized regression. Advances in Neural Information Processing Systems, 33:7768\u20137778, 2020.\\n\\nWu, Y., Tucker, G., and Nachum, O. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361, 2019.\\n\\nZe, Y., Yan, G., Wu, Y.-H., Macaluso, A., Ge, Y., Ye, J., Hansen, N., Li, L. E., and Wang, X. Gnfactor: Multi-task real robot learning with generalizable neural feature fields. arXiv preprint arXiv:2308.16891, 2023.\\n\\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In 7th Annual Conference on Robot Learning, 2023.\"}"}
{"id": "tl2qmO5kpD", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 9. Data mixture of our scaling experiments.\\n\\nFor each dataset, we report the number of trajectories and tokens contributed (#(Traj) and #(Tok)), the number of proprioception, vision and action dimensions in the raw data (\\\\(N_P\\\\), \\\\(N_V\\\\), \\\\(N_A\\\\)) and the probability \\\\(P_{\\\\text{eff}}\\\\) with which we sample from each dataset during training.\\n\\n| Dataset Group  | #(Traj)   | #(Tok)    | \\\\(N_P\\\\) | \\\\(N_V\\\\) | \\\\(N_A\\\\) | \\\\(P_{\\\\text{eff}}\\\\) |\\n|----------------|-----------|-----------|---------|---------|---------|---------------------|\\n| Control Suite  |           |           |         |         |         | \\\\(\\\\frac{1}{64}\\\\)     |\\n| acrobot.swingup| 10,843,200| 5.75E+09  | 6       | 0       | 1       |                     |\\n| ball.in.cup.catch| 5,292,800| 3.44E+09  | 8       | 0       | 2       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| cartpole.balance| 10,473,600| 5.13E+09  | 5       | 0       | 1       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| cartpole.swingup| 10,704,800| 5.25E+09  | 5       | 0       | 1       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| cartpole.three.poles| 10,012,800| 7.31E+09  | 11      | 0       | 1       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| cartpole.two.poles| 4,875,200 | 2.97E+09  | 8       | 0       | 1       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| cheetah.run     | 7,672,800 | 8.12E+09  | 17      | 0       | 6       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| dog.run         | 4,238,800 | 4.43E+10  | 223     | 0       | 38      | \\\\(\\\\frac{1}{64}\\\\)     |\\n| dog.stand       | 4,162,400 | 4.45E+10  | 223     | 0       | 38      | \\\\(\\\\frac{1}{64}\\\\)     |\\n| dog.trot        | 4,163,200 | 4.45E+10  | 223     | 0       | 38      | \\\\(\\\\frac{1}{64}\\\\)     |\\n| dog.walk        | 5,861,600 | 6.27E+10  | 223     | 0       | 38      | \\\\(\\\\frac{1}{64}\\\\)     |\\n| finger.spin     | 4,257,200 | 2.94E+09  | 9       | 0       | 2       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| finger.turn     | easy      | 9,893,200 | 12      | 0       | 2       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| finger.turn     | hard      | 6,794,000 | 12      | 0       | 2       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| fish.swim       | 6,172,800 | 7.96E+09  | 21      | 0       | 5       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| fish.upright    | 6,682,400 | 8.62E+09  | 21      | 0       | 5       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| hopper.hop      | 7,054,000 | 7.12E+09  | 15      | 0       | 4       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| hopper.stand    | 3,620,400 | 3.66E+09  | 15      | 0       | 4       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| humanoid.run    | 4,632,000 | 1.75E+10  | 67      | 0       | 21      | \\\\(\\\\frac{1}{64}\\\\)     |\\n| humanoid.stand  | 5,181,200 | 1.95E+10  | 67      | 0       | 21      | \\\\(\\\\frac{1}{64}\\\\)     |\\n| humanoid.walk   | 7,566,800 | 2.85E+10  | 67      | 0       | 21      | \\\\(\\\\frac{1}{64}\\\\)     |\\n| pendulum.swingup| 7,992,400 | 3.28E+09  | 3       | 0       | 1       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| point.mass.easy | 3,980,400 | 1.95E+09  | 4       | 0       | 2       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| quadruped.escape| 3,102,000 | 1.48E+10  | 101     | 0       | 12      | \\\\(\\\\frac{1}{64}\\\\)     |\\n| quadruped.run   | 1,730,400 | 6.66E+09  | 78      | 0       | 12      | \\\\(\\\\frac{1}{64}\\\\)     |\\n| quadruped.walk  | 4,457,600 | 1.72E+10  | 78      | 0       | 12      | \\\\(\\\\frac{1}{64}\\\\)     |\\n| reacher.easy    | 5,501,200 | 3.14E+09  | 6       | 0       | 2       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| swimmer.swimmer15| 5,483,600| 1.78E+10  | 61      | 0       | 14      | \\\\(\\\\frac{1}{64}\\\\)     |\\n| swimmer.swimmer6| 98,000    | 1.42E+08  | 25      | 0       | 5       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| walker.run      | 1,988,000 | 2.88E+09  | 24      | 0       | 6       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| walker.stand    | 7,401,200 | 1.07E+10  | 24      | 0       | 6       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| walker.walk     | 5,334,400 | 7.73E+09  | 24      | 0       | 6       | \\\\(\\\\frac{1}{64}\\\\)     |\\n| CHEF            | sim.lift  | 10,072,080| 129     | 19,200  | 5       | \\\\(\\\\frac{1}{48}\\\\)     |\\n|                  | sim.open  | 10,072,080| 129     | 19,200  | 5       | \\\\(\\\\frac{1}{48}\\\\)     |\\n|                  | sim.place | 10,072,080| 129     | 19,200  | 5       | \\\\(\\\\frac{1}{48}\\\\)     |\\n|                  | sim.reach | 10,072,080| 129     | 19,200  | 5       | \\\\(\\\\frac{1}{48}\\\\)     |\\n|                  |             |           |         |         |         |                     |\\n|                  | real.lift | 28,224,012| 129     | 12,800  | 5       | \\\\(\\\\frac{1}{48}\\\\)     |\\n|                  | real.open | 28,224,012| 129     | 12,800  | 5       | \\\\(\\\\frac{1}{48}\\\\)     |\\n|                  | real.place| 28,224,012| 129     | 12,800  | 5       | \\\\(\\\\frac{1}{48}\\\\)     |\\n|                  | real.reach| 28,224,012| 129     | 12,800  | 5       | \\\\(\\\\frac{1}{48}\\\\)     |\\n|                  |             |           |         |         |         |                     |\\n|                  | panda.sim.triple.stack.success | 13,992,960| 44     | 51,200  | 7       | \\\\(\\\\frac{1}{32}\\\\)     |\\n|                  | panda.sim.triple.stack.failure | 5,439,066| 44     | 51,200  | 7       | \\\\(\\\\frac{1}{32}\\\\)     |\\n|                  | panda.sim.pyramid.success | 15,998,400| 44     | 51,200  | 7       | \\\\(\\\\frac{1}{32}\\\\)     |\\n|                  | panda.sim.pyramid.failure | 15,024,082| 44     | 51,200  | 7       | \\\\(\\\\frac{1}{32}\\\\)     |\\n|                  | sim.insert.large.gear | 13,372,152| 21     | 32,000  | 7       | \\\\(\\\\frac{1}{24}\\\\)     |\\n|                  | sim.insert.medium.gear | 14,182,310| 21     | 32,000  | 7       | \\\\(\\\\frac{1}{24}\\\\)     |\\n|                  | sim.insert.small.gear | 12,260,934| 21     | 32,000  | 7       | \\\\(\\\\frac{1}{24}\\\\)     |\"}"}
{"id": "tl2qmO5kpD", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nTable 10. The data mixture used for the pre-training experiments. For each domain we report the modalities (P = proprioception, V = vision, L = language, A = actions), the number of tasks, the number of episodes recorded, the effective number of trajectories (each consisting of five timesteps), the number of tokens contributed, the percentage of successful episodes and the weight $\\\\lambda$ of the domain during the data sampling process. [* = success rate only available for five stacking tasks; ** = success rate average only computed across 82 tasks with available success rates]\\n\\n| Domain       | Mod | # (T) | #(Ep) | #(Traj) | #(Tok) | success | $\\\\lambda$ |\\n|--------------|-----|-------|-------|---------|--------|---------|-----------|\\n| Gato: Control| P, L, A | 32    | 468k  | 187M    | 430B   | 47%     | 4         |\\n| CHEF: sim    | V, P, L, A | 30    | 755k  | 60M     | 430B   | 28%*    | 1         |\\n| CHEF: real   | V, P, L, A | 30    | 2M    | 169M    | 1.12T  | 12%*    | 1         |\\n| RC: Tower    | V, P, L, A | 7     | 100k  | 16M     | 100B   | 75%     | 2         |\\n| RC: Pyramid  | V, P, L, A | 30    | 601k  | 96M     | 604B   | 75%     | 5         |\\n| RC: Insertion| V, P, L, A | 3     | 100k  | 40M     | 154B   | 97%     | 1         |\\n\\n$\\\\sum = 132$ 4.024M 567M 2.84T avg = 58%**\\n\\nTable 11. Data mixture of our scaling experiments. For each dataset, we report the number of trajectories and tokens contributed (#(Traj) and #(Tok)), the number of proprioception, vision and action dimensions in the raw data ($N_P$, $N_V$, $N_A$) and the probability $P_{eff}$ with which we sample from each dataset during training. Group entries abbreviated by . . . are identical with the ones reported in Table 9.\\n\\n| Dataset Group     | #(Traj) | #(Tok) | $N_P$ | $N_V$ | $N_A$ | $P_{eff}$ |\\n|-------------------|---------|--------|-------|-------|-------|-----------|\\n| Control Suite     |         |        |       |       |       |           |\\n| CHEF              |         |        |       |       |       |           |\\n| RoboCat           |         |        |       |       |       |           |\\n| panda.sim.triple  |         |        |       |       |       |           |\\n| stack.success.eval set | 2     | 1,244,000 | 44   | 51,200 | 7     | 1/42      |\\n| stack.failure.eval set | 2     | 681,818  | 44   | 51,200 | 7     | 1/42      |\\n| stack.success.eval set | 4     | 7,363,360 | 44   | 51,200 | 7     | 1/42      |\\n| stack.failure.eval set | 4     | 1,843,906 | 44   | 51,200 | 7     | 1/42      |\\n| stack.success.eval set | 5     | 3,439,040 | 44   | 51,200 | 7     | 1/42      |\\n| stack.failure.eval set | 5     | 1,366,208 | 44   | 51,200 | 7     | 1/42      |\\n| panda.sim.pyramid.success.eval set | 1    | 15,024,000 | 44   | 51,200 | 7     | 5/140     |\\n| panda.sim.pyramid.failure.eval set | 1    | 4,367,962  | 44   | 51,200 | 7     | 5/140     |\\n| panda.sim.pyramid.success.eval set | 2    | 13,409,760 | 44   | 51,200 | 7     | 5/140     |\\n| panda.sim.pyramid.failure.eval set | 2    | 5,763,134  | 44   | 51,200 | 7     | 5/140     |\\n| panda.sim.pyramid.success.eval set | 3    | 13,249,600 | 44   | 51,200 | 7     | 5/140     |\\n| panda.sim.pyramid.failure.eval set | 3    | 5,749,294  | 44   | 51,200 | 7     | 5/140     |\\n| panda.sim.pyramid.success.eval set | 4    | 15,385,920 | 44   | 51,200 | 7     | 5/140     |\\n| panda.sim.pyramid.failure.eval set | 4    | 3,767,426  | 44   | 51,200 | 7     | 5/140     |\\n| panda.sim.pyramid.success.eval set | 5    | 15,318,240 | 44   | 51,200 | 7     | 5/140     |\\n| panda.sim.pyramid.failure.eval set | 5    | 4,053,880  | 44   | 51,200 | 7     | 5/140     |\\n| sim.insert large gear | 13,372,152 | 5.18E+10 | 21   | 32,000 | 7     | 1/42      |\\n| sim.insert medium gear | 14,182,310 | 5.49E+10 | 21   | 32,000 | 7     | 1/42      |\\n| sim.insert small gear | 12,260,934 | 4.74E+10 | 21   | 32,000 | 7     | 1/42      |\\n\\nTable 12. Optimizer hyperparameters for all model scales across all experiments.\\n\\n| Hyperparameter | XXS (32M) | XS (73M) | S (164M) | M (391M) | L (988M) |\\n|----------------|-----------|----------|----------|----------|----------|\\n| lr init        | 1e-6      | 1e-6     | 1e-7     | 1e-7     | 1e-7     |\\n| lr peak        | 1e-4      | 1e-4     | 5e-5     | 3e-5     | 3e-5     |\\n| lr end         | 1e-5      | 1e-5     | 5e-6     | 3e-6     | 3e-6     |\\n| lr warmup steps| 1.5e4     | 1.5e4    | 1.5e4    | 1.5e4    | 1.5e4    |\\n| adamw beta1    | 0.9       | 0.9      | 0.9      | 0.9      | 0.9      |\\n| adamw beta2    | 0.95      | 0.95     | 0.95     | 0.95     | 0.95     |\\n| adamw weight decay | 1e-3   | 1e-3     | 1e-3     | 1e-3     | 1e-3     |\"}"}
{"id": "tl2qmO5kpD", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nTable 13. Different architectural ablations trained separately on Control Suite (XS-sized model) and RoboCat pyramid and tower (S-sized model). For Gato:Control we report the percentage of achieved expert average reward and the standard-error-based 95% CIs. For RoboCat (RC) we report the average success rates and their corresponding Wilson score intervals for $\\\\alpha_{W}=0.05$. We use 100 trials per task. Best results (within CI of the best mean) in each row are bold.\\n\\n| Domain  | # (T) | Linear Encoder | \u00b5-Law Encoder Multi-scale Normalizer w/o output action |\\n|---------|-------|----------------|------------------------------------------------------|\\n| Gato:Control | 32    | 79.6 [74.5, 84.6] | 82.7 [77.2, 88.1] | 86.7 [82.4, 90.9] | 76.8 [72.6, 81.1] |\\n| RC:Tower | 7     | 68.1 [64.6, 71.5] | 50.7 [47.0, 54.4] | 69.1 [65.6, 72.5] | N/A |\\n| RC:Pyramid | 30 | 63.5 [61.7, 65.2] | 36.1 [34.4, 37.8] | 65.8 [64.1, 67.5] | N/A |\\n\\nTable 14. Comparison between XS sized models for different objectives on Control domains. We show both aggregate performance over tasks as well as per domain results for selected lower dimensional and higher dimensional domains. We implement a Q-transformer style update as well as a simple return conditioned baseline (RC). Neither outperforms BC. Ind. denotes that the Q-transformer is implemented with the simplification of independent Q-values per action dimension. All results are percentage of expert performance, established by an independent RL training run per domain, following (Reed et al., 2022).\\n\\n| Domain  | # (T) | PAC PAC (no dist. Q) | BC Q-transformer (ind.) | RC |\\n|---------|-------|----------------------|------------------------|----|\\n| Gato:Control | 32 | 86.7 [82.4, 90.9] | 83.5 [81.2, 84.2] | 82.4 [81.6, 83.8] | 64.8 [60.1, 65.8] | 80.8 [78.3, 82.4] |\\n| Cartpole | | 97.0 | 96.8 | 95.3 | 99.5 | 95.6 |\\n| Finger turn-hard | | 98.3 | 98.4 | 95.7 | 96.3 | 97.5 |\\n| Dog trot | | 86.5 | 75.7 | 69.8 | 12.7 | 70.4 |\\n| Humanoid walk | | 92.4 | 71.2 | 60.6 | 24.5 | 62.9 |\\n\\nD.2.4. TASK CONDITIONING\\n\\nIn this ablation study we investigate how PAC uses its two different task modalities: the goal image $\\\\tau_V$ and the language instruction $\\\\tau_L$. Specifically, we probe whether a trained model can react to task specifications where one of the modalities is missing and whether it can be adapted to perform better on task conditions which differ from the training setup. To conduct this investigation, we take a PAC model of size M that has been pre-trained on all data (Control Suite, CHEF and RoboCat). Note that out of all these domains, Robocat Tower and Robocat Pyramid are the only ones that have visual task descriptions available besides language task descriptions while all other domains have only language ones and therefore their visual task descriptions are merely padded zeros. We then evaluate this model with different modalities for task specification being available:\\n\\n- Vision+Lang: both modalities $\\\\tau_V$ and $\\\\tau_L$ are present therefore the same as the setup during pretraining;\\n- Vision: only visual task descriptions $\\\\tau_V$ are present and the language ones are masked out;\\n- Lang: only $\\\\tau_L$ are available;\\n- No Goal: both task description modalities are masked out.\\n\\nThe evaluation results are presented in the Pretrained rows of Table 15. A notable observation is that the pretrained model is still very performant when only the language task description $\\\\tau_L$ is available, albeit this differs from the task conditioning during pretraining where the model has only seen visual and language task descriptions both being present for the Tower and Pyramid domains; while the success rate drops substantially when only visual task descriptions $\\\\tau_V$ are present. One hypothesis of such an imbalanced performance of the pretrained model when conditioning on $\\\\tau_V$ or $\\\\tau_L$ could be the following: Since all data domains the model has seen during the pretraining phase have $\\\\tau_L$ available while Tower and Pyramid are the only ones with $\\\\tau_V$, the $N_{\\\\tau_L} = 50$ language task description tokens are attended to for every single datapoint while the $N_{\\\\tau_V} \\\\times N_{E} = 300$ visual task description tokens are masked out except for when learning on the Tower and Pyramid domains of robocat. Therefore we suspect the model would learn to attend more to the language task tokens than the vision task tokens in order to be able to achieve good performance averaging over all task domains. We conduct additional experiments for this hypothesis and present the results in Table 16 which will be discussed directly following.\\n\\nIn order to see if above said model could be quickly adapted such that its performance is more amenable to varying modalities for task description, we conduct a finetuning experiment of the model where either task description modality will be present for a certain percentage $p_{\\\\tau_V}$ of the time and masked out the rest of the time. In particular, for this experiment we set $p_{\\\\tau_V}=0.99$ and $p_{\\\\tau_L}=0.9$ where $\\\\tau_V$ are masked out less often to compensate for the fact that visual task descriptions are only present for two domains while the fine-tuning is carried over all tasks, but with adjusted sampling weights compared to Table 6: from $\\\\lambda=(8, 2, 2, 1, 1, 2)$ to $\\\\lambda=(6, 2, 2, 6, 6, 2)$. We also use a fixed learning rate of $3e-6$. The task conditioning evaluation results of this finetuned model (after 200K steps) are shown in the FT rows of Table 15. We observe that the success rate of the model increases by a large margin when only visual task description $\\\\tau_V$ is available.\"}"}
{"id": "tl2qmO5kpD", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nTable 15. Adapting a pre-trained PAC model (size M) to a new task-conditioning setup during finetuning phase setting $p^V = 0.99$, $p^L = 0.9$. The average success rates across 100 trials per task and their corresponding Wilson score intervals for $\\\\alpha^W = 0.05$ are reported.\\n\\n| Model | Domain | # (T) | Vision+Lang | Vision | Lang | No Goal |\\n|-------|--------|-------|-------------|--------|------|---------|\\n| PT@3M | RC: Tower 7 | 59.7 | [49.6, 69.3] | 0.1 | [0.0, 3.9] |\\n|       | RC: Pyramid 30 | 53.3 | [43.2, 63.1] | 11.4 | [10.3, 12.6] |\\n\\nTable 16. Pre-training a PAC model (size S) with varying goal conditions. Evaluation after 500k training steps. The average success rates across 100 trials per task and their corresponding Wilson score intervals for $\\\\alpha^W = 0.05$ are reported.\\n\\n| Model | Domain | # (T) | Vision+Lang | Vision | Lang | No Goal |\\n|-------|--------|-------|-------------|--------|------|---------|\\n|       | RC: Tower 7 | 55.6 | [43.4, 66.3] | 5.57 | [4.0, 7.5] |\\n|       | RC: Pyramid 30 | 53.1 | [43.1, 62.9] | 11.4 | [9.9, 12.2] |\\n\\nSetting the modality present rate to $p^V = 0.9$, $p^L = 0.9$, we train another model from scratch with otherwise the same setup of above and present the evaluation results in the corresponding rows in Table 16. The results show that with varying goal conditionings being present in the pre-training phase, the resulting model can perform well on all conditioning variations as expected.\\n\\nE. Scaling Details\\n\\nE.1. Return Profile Fitting\\n\\nWe fit the average return of a models as a logistic function of the number of training steps:\\n\\n$$R(n) = a_1 + \\\\exp(-k(n - n_0)) + b.$$  (18)\\n\\nWe evaluate six checkpoints of each training run evenly spaced across the 3M updates. Each evaluation runs 100 trials per task and computes the average episode return across 102 simulation tasks. The parameters $a$, $k$, $n_0$, $b$ of Equation (18) are then fitted to the evaluation data points to obtain a return profile for each model.\\n\\nE.2. FLOP Counting\\n\\n| Model Scale | FWD | BWD | UPDATE |\\n|-------------|-----|-----|--------|\\n| XXS (32M)   | 7.826E+09 | 1.573E+10 | 1.206E+13 |\\n| XS (73M)    | 1.360E+10 | 2.733E+10 | 2.096E+13 |\\n| S (164M)    | 2.341E+10 | 4.705E+10 | 3.607E+13 |\\n| M (391M)    | 4.380E+10 | 8.804E+10 | 6.750E+13 |\\n| L (988M)    | 1.040E+11 | 2.090E+11 | 1.602E+14 |\\n\\nTable 17. FLOPs costs for forward pass, backward pass and batch update for all scales of our PAC model family. All FLOPs are computed for a batch size $B = 512$ and a target update frequency $f^\\\\theta = 100$.\\n\\nWhen counting the FLOPs of the PAC architecture for the scaling analysis, we follow the FLOP counting scheme established by Hoffmann et al. (2022) since the bulk of our model's computation sits in cross-attention and self-attention blocks. FLOPs of cross-attention blocks are counted similarly as in self-attention blocks with the only difference of the length of the input and output sequence being different which yields $\\\\text{seq_len}^\\\\text{in} \\\\times \\\\text{seq_len}^\\\\text{out}$.\"}"}
{"id": "tl2qmO5kpD", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We can now compute the partial derivative w.r.t. $\\frac{\\\\partial J}{\\\\partial \\\\theta}$.\\n\\nWe give a short derivation following previous work by Ab-\\n\\nAfter exponentiation this can be re-arranged into:\\n\\nAnd Equation (2) can be retrieved by re-introducing the\\n\\nAppendix C, following which are algorithm-wise sensitivity\\n\\nThen further details on the experimental setups are given in\\n\\nFinally, additional experiments are presented in Appendix F.\\n\\nThis appendix presents further details and additional experi-\\n\\nA.1. Necessary Optimality Conditions for Policy\\n\\nA. Method Details - Algorithm\\n\\nB discussing architecture details accompanying Section 3.3.\\n\\nalgorithmic details to complement Section 3.2, and Section\\n\\ncussed in Sections A and B, with Section A focusing on\\n\\nand is arranged as follows: Methodological details are dis-\\n\\nments of the proposed Perceiver-Actor-Critic (PAC) model\\n\\nQ\\n\\nbin size $\\\\epsilon$ on $q$\\n\\nq\\n\\nrepresentation: a categorical distribution $p$\\n\\nfrom the distributional reinforcement learning literature: In-\\n\\ning constant, $H$ is a normalizing\\nto balance losses easily we transform the problem of learn-\\n\\nOur default implementation of PAC (called PAC+Q here for\\n\\nOur alternative implementation of PAC uses a state-value\\n\\nA.3. PAC+V Details\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\nAnalogous to the standard temporal difference operator using\\n\\n"}
{"id": "tl2qmO5kpD", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we keep the notation where we dropped the normalization constant for the exponentiated advantage as is common (see e.g. Peng et al. 2019). Even though we do not use target networks here, we can thus rely on an empirical estimate of the advantage, which allows us to combine the state and action representation:\\n\\n\\\\[ V(s_t) = \\\\mathbb{E}_{A_t \\\\sim \\\\pi(b)} [A_t]\\\\]  \\n\\nAnalogously to the Q-function we use a categorical distribution over binned values using samples \\\\( s, \\\\tau \\\\) to BC for this: we could either assume the behaviour policy executed observations (\\\\( s \\\\)) of \\\\( b \\\\) \\\\( \\\\epsilon, \\\\ldots, v \\\\) \\\\( \\\\tau \\\\). \\\\( s \\\\) \\\\( a \\\\) \\\\( b \\\\) \\\\( v \\\\) \\\\( \\\\epsilon \\\\)\\n\\n...importance weights. Two simple strategies are possible for overestimation issues \u2013 and this improvement step reverts to BC for \\\\( \\\\pi \\\\). However, given that we only have access to \\\\( a \\\\) \\\\( s \\\\) \\\\( \u0393 \\\\) \\\\( \u03b8 \\\\) \\\\( v \\\\) \\\\( \u03c4 \\\\) ...2017; Riedmiller et al., 2018). So, we make the dependence on \\\\( \u03c4 \\\\) explicit in notations. In our experiments we allow for multi-scale input modality. Those input modalities are indicated by superscripts in notations. Note that we assume, without loss of generality, that the task description \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\( \u03c4 \\\\) \\\\( s \\\\) \\\\("}
{"id": "tl2qmO5kpD", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Offline Actor-Critic Reinforcement Learning Scales to Large Models\\n\\nFigure 5.\\nHigh-level PAC model architecture, with each sub-procedure outlined in Algorithm 1 visually grouped; note that $z$ is used in both policy decoding and Q-value decoding. Details are discussed in Section B.\\n\\nDescriptions ($\\\\tau_V, \\\\tau_L$):\\n- Proprioception observations $s_P \\\\in \\\\mathbb{R}^{N_P}$ with $N_P$ denoting the dimensions of proprioceptive measurements.\\n- Visual observations $s_V \\\\in \\\\mathbb{R}^{N_V \\\\times H \\\\times W \\\\times C}$ with number of image observations $N_V$, height $H$, width $W$, and number of channels $C$ for each image.\\n- Visual task descriptions $\\\\tau_V \\\\in \\\\mathbb{R}^{N_V \\\\tau \\\\times H \\\\times W \\\\times C}$ with $N_V$ number of images depicting the desired task (e.g., the last frame from a successful episode).\\n- Language task descriptions $\\\\tau_L \\\\in \\\\mathbb{R}^{N_L \\\\tau}$ with number of text tokens $N_L$.\\n\\nAll input modalities across all domains are padded in order to match this format and we keep track of the valid entries with a mask. The complete input at time step $t$ is therefore the concatenation of $(s_t = (s_P t, s_V t), \\\\tau = (\\\\tau_V, \\\\tau_L))$. This could also be easily extended to incorporate more than a single timestep in case of partially observable tasks.\\n\\nThe concatenated input is then mapped via one encoder per modality into multiple embedding vectors of dimension $D_I$. The details of each modality encoder are given below.\\n\\nThe proprioception encoder $\\\\phi_P$ is an instantiation of the multi-scale normalizer (see Appendix B.1) followed by a linear layer $L_P$ with $D_I$ output dimensions to map the multi-scale representation to the desired shape:\\n\\n$$\\\\phi_P = L_P \\\\cdot \\\\phi_{\\\\text{multi-scale}} : \\\\mathbb{R} \\\\rightarrow \\\\mathbb{R}^{N_G \\\\times D_I}.$$  \\n\\nThe image inputs are encoded via a ResNet (He et al., 2016). We omit the last projection layer to obtain $N_E$ spatial dimensions for our image embedding $\\\\phi_V : \\\\mathbb{R}^{H \\\\times W \\\\times C} \\\\rightarrow \\\\mathbb{R}^{N_E \\\\times D_I}$, where $N_E$ depends on the input image size and the down-sampling specifications of the ResNet. We note that just like the proprioception embedding, and in contrast to other approaches, we do not discretize or tokenize image inputs but use continuous mappings instead.\\n\\nWe assume language inputs to be tokenized by the Sentence-Piece tokenizer (Kudo & Richardson, 2018) during data loading. These are then directly embedded using a learnable look-up table $\\\\phi_L : [1..N_T] \\\\rightarrow \\\\mathbb{R}^{D_I}$, with $N_T$ the total number of different language tokens.\\n\\nApplying each encoder to the corresponding input modality thus generates an encoded input, $e_I = \\\\phi_P(s_P t) \\\\oplus \\\\phi_V(s_V t) \\\\oplus \\\\phi_V(\\\\tau_V) \\\\oplus \\\\phi_L(\\\\tau_L) \\\\in \\\\mathbb{R}^{N \\\\times D_I}$, with $N = N_P N_G + N_V N_E + N_V \\\\tau N_E + N_L \\\\tau$.\\n\\nB.3. Transformer on Latent Space\\n\\nMost state-of-the-art large-scale models for control take the encoded input $e_I \\\\in \\\\mathbb{R}^{N \\\\times D_I}$ and directly feed it into a transformer consisting of multiple stacked self-attention blocks. For the domains in our experiments and the data modalities that we consider, this input sequence would be of length $N = 2,634$ for a single time step (cf. Appendix C.3). As the computational complexity and memory usage of the self-attention mechanism scales quadratically with the input sequence length, this commonly adopted treatment potentially hinders the learned generalist controller from being applicable to real robotic systems that impose real-time constraints, and more generally restricts efficient learning and inference, let alone providing feasible solutions to extend...\"}"}
{"id": "tl2qmO5kpD", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\n\\nPerceiver Actor Critic Model\\n\\nvectors, which in turn reduces the computational complexity for ease of discussion, we first give an overview of the at-\\nto a linear one. For ease of discussion, we first give an overview of the at-\\nattention from the input sequence onto a few trainable latent\\nlenge, we adopt the methodology from the perceiver model\\ninclude more high-dimensional inputs. To address this chal-\\n\u03c4\\n\\\\phi\\nTrainable parameters (randomly initialized):\\nL\\n\\\\phi\\nq\\n\u2208\\nz\\n\u03d5\\ns\\n\\nInput:\\n// Observation Encoding.\\ny\\n\\\\gamma\\n\\nOutput:\\n// Policy decoding.\\ny\\n\u03c0\\n\\\\gamma\\ny\\n\\\\gamma\\ny\\n\\nend for\\n// Transformer on latent space.\\n\\\\gamma\\n\u03c0\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\n\\\\bar{Q} = \\\\gamma\\n\\\\gamma\\n\\\\gamma\\nm\\n\\\\gamma\\n= \\\\gamma\\n\\\\gamma\\n1\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma\\n\\\\gamma"}
{"id": "tl2qmO5kpD", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 18.\\n\\nPolicy success rates across #\\\\(T\\\\) tasks in each domain for 100 evaluations per task. The average success rate in the training data is reported as \\\\(p_D\\\\). For Gato: Control, the percentage of achieved expert average reward and the standard-error-based 95% CIs are reported (where available). For all other task families, the average success rates and their corresponding Wilson score intervals for \\\\(\\\\alpha_W = 0.05\\\\) are reported. Best results (within CI of the best mean) in each row are bold.\\n\\n| Domain | #\\\\(T\\\\) | \\\\(p_D\\\\) \\\\(\\\\pm\\\\) SE | \\\\(\\\\alpha_W = 0.05\\\\) \\\\(\\\\pm\\\\) SE |\\n|--------|--------|---------------------|-------------------------------|\\n| Gato: Control | 32 | N/A | 63.6 \\\\(\\\\pm\\\\) 6.2 |\\n| RC: Tower | 7 | 61.0 \\\\(\\\\pm\\\\) 5.5 | 69.3 \\\\(\\\\pm\\\\) 5.5 |\\n| RC: Pyramid | 30 | 64.5 \\\\(\\\\pm\\\\) 5.2 | 63.5 \\\\(\\\\pm\\\\) 5.9 |\\n| RC: Insertion | 3 | 71.3 \\\\(\\\\pm\\\\) 6.5 | 80.3 \\\\(\\\\pm\\\\) 6.3 |\\n| CHEF: sim | 1 | N/A | 17.0 \\\\(\\\\pm\\\\) 22.3 |\\n| CHEF: real | 5 | 69.8 \\\\(\\\\pm\\\\) 5.3 | 93.2 \\\\(\\\\pm\\\\) 5.3 |\\n\\n### Table 19.\\n\\nPer-group success rates and confidence intervals for real-robot stacking tasks across self-improvement iterations (#\\\\(T\\\\) = 5). The policies are evaluated in 400 trials per set. The average success rates and their corresponding Wilson score intervals for \\\\(\\\\alpha_W = 0.05\\\\) are reported.\\n\\n| Iteration | Set 1 \\\\(\\\\pm\\\\) SE | Set 2 \\\\(\\\\pm\\\\) SE | Set 3 \\\\(\\\\pm\\\\) SE | Set 4 \\\\(\\\\pm\\\\) SE | Set 5 \\\\(\\\\pm\\\\) SE | All Sets \\\\(\\\\pm\\\\) SE |\\n|-----------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|\\n| Pretraining | 51.5 \\\\(\\\\pm\\\\) 9.9 | 53.5 \\\\(\\\\pm\\\\) 9.3 | 65.5 \\\\(\\\\pm\\\\) 9.8 | 84.7 \\\\(\\\\pm\\\\) 9.9 | 94.0 \\\\(\\\\pm\\\\) 9.9 | 69.8 \\\\(\\\\pm\\\\) 9.9 |\\n| RLFT #1 | 83.0 \\\\(\\\\pm\\\\) 9.3 | 66.8 \\\\(\\\\pm\\\\) 9.4 | 87.5 \\\\(\\\\pm\\\\) 9.3 | 90.8 \\\\(\\\\pm\\\\) 9.3 | 95.5 \\\\(\\\\pm\\\\) 9.3 | 84.7 \\\\(\\\\pm\\\\) 9.3 |\\n| RLFT #2 | 88.0 \\\\(\\\\pm\\\\) 9.2 | 76.2 \\\\(\\\\pm\\\\) 9.4 | 92.5 \\\\(\\\\pm\\\\) 9.3 | 95.8 \\\\(\\\\pm\\\\) 9.3 | 96.8 \\\\(\\\\pm\\\\) 9.3 | 89.8 \\\\(\\\\pm\\\\) 9.3 |\\n| RLFT #3 | 91.8 \\\\(\\\\pm\\\\) 9.2 | 91.5 \\\\(\\\\pm\\\\) 9.3 | 90.8 \\\\(\\\\pm\\\\) 9.3 | 95.0 \\\\(\\\\pm\\\\) 9.3 | 97.0 \\\\(\\\\pm\\\\) 9.3 | 93.2 \\\\(\\\\pm\\\\) 9.3 |\\n\\n### Table 20.\\n\\nComparison of performance across all domains after pre-training (\\\\(\\\\alpha\\\\)-PAC), after three rounds of self-improvement on the CHEF:real domain (RLFT #3).\\n\\n| Domain | #\\\\(T\\\\) | \\\\(\\\\alpha\\\\)-PAC \\\\(\\\\pm\\\\) SE | RLFT #3 \\\\(\\\\pm\\\\) SE |\\n|--------|--------|---------------------|-------------------|\\n| Gato: Control | 32 | 92.1 \\\\(\\\\pm\\\\) 8.7 | 91.3 \\\\(\\\\pm\\\\) 8.6 |\\n| RC: Tower | 7 | 69.6 \\\\(\\\\pm\\\\) 6.9 | 70.0 \\\\(\\\\pm\\\\) 6.9 |\\n| RC: Pyramid | 30 | 64.9 \\\\(\\\\pm\\\\) 6.2 | 65.1 \\\\(\\\\pm\\\\) 6.3 |\\n| RC: Insertion | 3 | 89.3 \\\\(\\\\pm\\\\) 5.6 | 80.3 \\\\(\\\\pm\\\\) 5.3 |\\n| CHEF: sim | 1 | 52.0 \\\\(\\\\pm\\\\) 12.3 | 59.0 \\\\(\\\\pm\\\\) 12.0 |\\n| CHEF: real | 5 | 69.8 \\\\(\\\\pm\\\\) 7.9 | 93.2 \\\\(\\\\pm\\\\) 7.3 |\\n\\nComparison, we also deploy BC+Q on the robot \u2013 a model which is pre-trained using only the BC part of the objective to optimize its policy (\\\\(\\\\alpha = 1\\\\)), but which has already learned a Q-function on the pre-training data (\\\\(\\\\beta > 0\\\\)). The initial performance of this model on the real robot is unsurprisingly low with only 3.6% success rate. When we continue the training on the same pre-training data (including all sim and real tasks) for another 200k updates, but lower \\\\(\\\\alpha\\\\) to 0 to fully leverage the Q-function for policy improvement, we observe a significant jump to 38.2% success rate when redeploying on the robot. While this performance is still significantly lower than the 69.8% success of the \\\\(\\\\alpha\\\\)-PAC model after initial pre-training, it is high enough to feasibly be used as an alternate starting point for the self-improvement loop. This can be useful in scenarios where no non-expert data is available initially to perform \\\\(\\\\alpha\\\\)-PAC from the start.\"}"}
