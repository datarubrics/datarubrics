{"id": "phan23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability\\n\\nFig. 5. Evaluation of AERIAL, AERIAL (no attention), and the best MessySMAC baselines for different observation negation probabilities $\\\\phi$ affecting observation stochasticity w.r.t. $\\\\Omega$ (20 runs per configuration). (a) The average normalized test win rate across all 6 MessySMAC maps from Section 6.3. (b) The number of maps best out of 6. The legend at the top applies across all plots.\\n\\nFig. 6. Evaluation of AERIAL, AERIAL (no attention), and the best MessySMAC baselines for different initial random steps $K$ affecting initialization stochasticity w.r.t. $b_0$ (20 runs per configuration). (a) The average normalized test win rate across all 6 MessySMAC maps from Section 6.3. (b) The number of maps best out of 6. The legend at the top applies across all plots.\\n\\nDiscussion\\nOur results systematically demonstrate the robustness of AERIAL and AERIAL (no attention) against various stochasticity configurations according to $\\\\Omega$ and $b_0$. State-based CTDE is notably less effective in settings, where observation and initialization stochasticity is high. As AERIAL consistently performs best in all maps when $\\\\phi \\\\geq 15\\\\%$ or $K \\\\geq 10$, we conclude that providing an adequate representation of $P_{\\\\tau_t | b_0}$ according to Eq. 5 that is learned, e.g., through $h_t$ and self-attention, is more beneficial for CTDE than merely relying on true states when facing domains with high stochastic partial observability.\\n\\n7. Conclusion and Future Work\\nTo tackle general multi-agent problems, which are messy and only observable through noisy sensors, we need adequate algorithms and benchmarks that sufficiently consider stochastic partial observability.\\n\\nIn this paper, we proposed AERIAL to approximate value functions under stochastic partial observability with a learned representation of multi-agent recurrence, considering more accurate closed-loop information about decentralized agent decisions than state-based CTDE.\\n\\nWe then introduced MessySMAC, a modified version of SMAC with stochastic observations and higher variance in initial states, to provide a more general and configurable Dec-POMDP benchmark regarding stochastic partial observability. We showed visually in Fig. 2 and experimentally in Section 6 that MessySMAC scenarios pose a greater challenge than their original SMAC counterparts due to observation and initialization stochasticity.\\n\\nCompared to state-based CTDE, AERIAL offers a simple but effective approach to general Dec-POMDPs, being competitive in original SMAC and superior in Dec-Tiger and MessySMAC, which both exhibit observation and initialization stochasticity unlike original SMAC. Simply replacing the true state with memory representations can already improve performance in most scenarios, confirming the need for more accurate closed-loop information about decentralized agent decisions. Self-attention can correct for the naive independence assumption of agent-wise recurrence to further improve performance, especially when observation or initialization stochasticity is high.\\n\\nWe plan to further evaluate AERIAL in SMACv2 and mixed competitive-cooperative settings with multiple CTDE instances (Lowe et al., 2017; Phan et al., 2020).\\n\\nAcknowledgements\\nThis work was partially funded by the Bavarian Ministry for Economic Affairs, Regional Development and Energy as part of a project to support the thematic development of the Institute for Cognitive Systems.\\n\\nReferences\\nAmato, C., Bernstein, D. S., and Zilberstein, S. Optimizing Memory-Bounded Controllers for Decentralized POMDPs. In Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence, pp. 1\u20138, 2007.\\nBernstein, D. S., Hansen, E. A., and Zilberstein, S. Bounded Policy Iteration for Decentralized POMDPs. In IJCAI, pp. 52\u201357, 2005.\"}"}
{"id": "phan23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability\\n\\nBoutilier, C. Planning, Learning and Coordination in Multi-agent Decision Processes. In Proceedings of the 6th conference on Theoretical aspects of rationality and knowledge, pp. 195\u2013210. Morgan Kaufmann Publishers Inc., 1996.\\n\\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision Transformer: Reinforcement Learning via Sequence Modeling. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 15084\u201315097. Curran Associates, Inc., 2021.\\n\\nCho, K., van Merri\u00ebnoort, B., Bahdanau, D., and Bengio, Y. On the Properties of Neural Machine Translation: Encoder-Decoder Approaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103\u2013111, 2014.\\n\\nEllis, B., Moalla, S., Samvelyan, M., Sun, M., Mahajan, A., Foerster, J. N., and Whiteson, S. SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning. 2022.\\n\\nEmery-Montemerlo, R., Gordon, G., Schneider, J., and Thrun, S. Approximate Solutions for Partially Observ-able Stochastic Games with Common Payoffs. In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1, AAMAS '04, pp. 136\u2013143, USA, 2004. IEEE Computer Society. ISBN 1581138644.\\n\\nFoerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. Counterfactual Multi-Agent Policy Gradi-ents. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), Apr. 2018.\\n\\nGupta, J. K., Egorov, M., and Kochenderfer, M. Cooperative Multi-Agent Control using Deep Reinforcement Learning. In Autonomous Agents and Multiagent Systems, pp. 66\u201383. Springer, 2017.\\n\\nHochreiter, S. and Schmidhuber, J. Long Short-Term Memory. Neural Computation, 9(8):1735\u20131780, 1997.\\n\\nHu, H. and Foerster, J. N. Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning. In International Conference on Learning Representations, 2019.\\n\\nIqbal, S. and Sha, F. Actor-Attention-Critic for Multi-Agent Reinforcement Learning. In Chaudhuri, K. and Salakhut-dinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2961\u20132970, Long Beach, California, USA, 09\u201315 Jun 2019. PMLR.\\n\\nIqbal, S., De Witt, C. A. S., Peng, B., Boehmer, W., Whiteson, S., and Sha, F. Randomized Entity-wise Factoriza-tion for Multi-Agent Reinforcement Learning. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4596\u20134606. PMLR, 18\u201324 Jul 2021.\\n\\nKaelbling, L. P., Littman, M. L., and Cassandra, A. R. Plan-ning and Acting in Partially Observable Stochastic Domains. Artificial intelligence, 101(1-2):99\u2013134, 1998.\\n\\nKhan, M. J., Ahmed, S. H., and Sukthankar, G. Transformer-Based Value Function Decomposition for Cooperative Multi-Agent Reinforcement Learning in StarCraft. Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, 18(1):113\u2013119, Oct. 2022.\\n\\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., and Mordatch, I. Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\\n\\nLyu, X., Xiao, Y., Daley, B., and Amato, C. Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning. In Proceedings of the 20th International Conference on Autonomous Agents and Multiagent Systems, pp. 844\u2013852, 2021.\\n\\nLyu, X., Baisero, A., Xiao, Y., and Amato, C. A Deeper Understanding of State-Based Critics in Multi-Agent Reinforcement Learning. Proceedings of the AAAI Conference on Artificial Intelligence, 36(9):9396\u20139404, Jun. 2022. doi: 10.1609/aaai.v36i9.21171.\\n\\nNair, R., Tambe, M., Yokoo, M., Pynadath, D., and Marsella, S. Taming Decentralized POMDPs: Towards Efficient Policy Computation for Multiagent Settings. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, IJCAI'03, pp. 705\u2013711, San Francisco, CA, USA, 2003. Morgan Kaufmann Publishers Inc.\\n\\nOliehoek, F. A. and Amato, C. A Concise Introduction to Decentralized POMDPs, volume 1. Springer, 2016.\\n\\nOliehoek, F. A., Spaan, M. T., and Vlassis, N. Optimal and Approximate Q-Value Functions for Decentralized POMDPs. Journal of Artificial Intelligence Research, 32:289\u2013353, 2008.\\n\\nPhan, T., Gabor, T., Sedlmeier, A., Ritz, F., Kempter, B., Klein, C., Sauer, H., Schmid, R., Wieghardt, J., Zeller, M., et al. Learning and Testing Resilience in Cooperative\"}"}
{"id": "phan23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "phan23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Limitations and Societal Impacts\\n\\nA.1. Limitations\\n\\nIn easier domains without stochastic partial observability, AERIAL does not significantly outperform the state-of-the-art baselines as indicated by the original SMAC results in Table 1. This implies that simplified Dec-POMDP settings might benefit from more specialized algorithms. The dependence on joint memory representations $h_t = \\\\langle h_{t,i} \\\\rangle_{i \\\\in D}$ might induce some bias w.r.t. agent behavior policies which could limit performance in hard exploration domains therefore requiring additional mechanisms beyond the scope of this work. The full version of AERIAL requires additional compute due to the transformer component in Fig. 1, which can be compensated by using a more (parameter) efficient value function factorization operator $\\\\Psi$, e.g., QMIX instead of QPLEX.\\n\\nA.2. Potential Negative Societal Impacts\\n\\nThe goal of our work is to realize autonomous systems to solve complex tasks under stochastic partial observability as motivated in Section 1. We refer to (Whittlestone et al., 2021) for a general overview regarding societal implications of deep RL and completely focus on cooperative MARL settings in the following.\\n\\nAERIAL is based on a centralized training regime to learn decentralized policies with a common objective. That objective might include bias of a central authority and could potentially harm opposing parties, e.g., via discrimination or misleading information. Since training is conducted in a laboratory or a simulator, the resulting system might exhibit unsafe or questionable behavior when being deployed in the real world due to poor generalization, e.g., leading to accidents or unfair decisions. The transformer component in Fig. 1 might require a significant amount of additional compute for tuning and training therefore increasing overall cost. The self-attention weights of Eq. 9 could be used to discriminate participating individuals in an unethical way, e.g., discarding less relevant groups of individuals according to the softmax output.\\n\\nSimilar to original SMAC, MessySMAC is based on team battles, indicating that any MARL algorithm mastering that challenge could be misused for real combat, e.g., in autonomous weapon systems to realize distributed and coordinated strategies. Since MessySMAC covers the aspect of stochastic partial observability, successfully evaluated algorithms could be potentially more effective and dangerous in real-world scenarios.\\n\\nB. Dec-Tiger Example\\n\\nGiven the Dec-Tiger example from Section 4.1 with a horizon of $T = 2$, the tiger being behind the right door ($s_R$), and both agents having listened in the first step, where agent 1 heard $z_L$ and agent 2 heard $z_R$: The final state-based values are defined by $Q^*_{MDP}(s_t, a_t) = R(s_t, a_t)$. Due to both agents perceiving different observations, i.e., $z_L$ and $z_R$ respectively, the probability of being in state $s_R$ is 50% according to the belief state, i.e., $b(s_R | \\\\tau_t) = b(s_L | \\\\tau_t) = \\\\frac{1}{2}$. Thus, the true optimal Dec-POMDP values for the final time step are defined by:\\n\\n$$Q^*(\\\\tau_t, a_t) = \\\\sum_{s_t \\\\in S} b(s_t | \\\\tau_t) R(s_t, a_t) = \\\\frac{1}{2}(Q^*_{MDP}(s_L, a_t) + Q^*_{MDP}(s_R, a_t)) \\\\tag{10}$$\\n\\nThe values of $Q^*_{MDP}$ and $Q^*$ for the final time step $t = 2$ in the example are given in Table 2. Both agents can reduce the expected penalty when always performing the same action. Therefore, it is likely for MARL to converge to a joint policy that recommends the same actions for both agents, especially when synchronization techniques like parameter sharing are used (Tan, 1993; Gupta et al., 2017; Yu et al., 2022).\\n\\nC. Full Algorithm of AERIAL\\n\\nThe complete formulation of AERIAL is given in Algorithm 1. Note that AERIAL does not depend on true states $s_t$ at all, since the experience samples $e_t$ used for training (Line 23) do not record any states. The additional amount regarding wall clock time was negligible in our experiments though.\"}"}
{"id": "phan23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability\\n\\nFigure 1. Illustration of the AERIAL setup. Left: Recurrent agent network structure with memory representations $h_{t-1,i}$ and $h_{t,i}$. Right: Value function factorization via factorization operator $\\\\Psi$ using the joint memory representation $h_t = \\\\langle h_{t,i} \\\\rangle_{i \\\\in D}$ of all agents' RNNs instead of true states $s_t$. All memory representations $h_{t,i}$ are detached from the computation graph to avoid additional differentiation (indicated by the dashed gray arrows) and passed through a simplified transformer before being used by $\\\\Psi$ for value function factorization.\\n\\nAttention-Based Recurrence\\n\\nWhile AERIAL (no attention) offers a simple way to address agent-wise stochastic partial observability, the independence assumption of all individual recurrences $P_{\\\\pi_i}(\\\\tau_t | b_0)$ does not hold in practice due to correlations in observations and actions (Bernstein et al., 2005; Amato et al., 2007).\\n\\nGiven the Dec-Tiger example above, the individual recurrences according to Eq. 8 are $P_{\\\\pi_1}(\\\\tau_t, 1 | b_0) = P_{\\\\pi_2}(\\\\tau_t, 2 | b_0) = 0$. However, the actual multi-agent recurrence according to Eq. 5 is $P_{\\\\pi}(\\\\tau_t | b_0) = 0 \\\\cdot 0 = P_{\\\\pi_1}(\\\\tau_t, 1 | b_0) \\\\cdot P_{\\\\pi_2}(\\\\tau_t, 2 | b_0)$, indicating that individual recurrences are not statistically independent in general (Oliehoek & Amato, 2016).\\n\\nTherefore, we process $h_t$ by a simplified transformer along the agent axis to automatically consider the latent dependencies of all memory representations $h_{t,i}$ through self-attention. The resulting approach, called AERIAL, is depicted in Fig. 1 and Algorithm 1 in Appendix C.\\n\\nOur transformer does not use positional encoding or masking, since we assume no particular ordering among agents. The joint memory representation $h_t$ is passed through a single multi-head attention layer with the output of each attention head $c$ being defined by (Vaswani et al., 2017):\\n\\n$$\\\\text{att}_c(h_t) = \\\\text{softmax}(\\\\frac{W_c q(h_t) W_c k(h_t) \\\\top}{\\\\sqrt{d_{\\\\text{att}}}}) W_c v(h_t)$$\\n\\nwhere $W_c q$, $W_c k$, and $W_c v$ are multi-layer perceptrons (MLP) with an output dimensionality of $d_{\\\\text{att}}$. All outputs $\\\\text{att}_c(h_t)$ are summed and passed through a series of MLP layers before being fed into the factorization operator $\\\\Psi$, effectively replacing the true state $s_t$ by a learned representation of multi-agent recurrence $P_{\\\\pi}(\\\\tau_t | b_0)$ according to Eq. 5.\\n\\nTo avoid additional differentiation of $h_t$ through $\\\\Psi$ or Eq. 9, we detach $h_t$ from the computation graph. Thus, we make sure that $h_t$ is only learned through agent RNNs.\\n\\n4.3. Discussion of AERIAL\\n\\nThe strong focus on state-based CTDE in the last few years has led to the development of increasingly complex algorithms that largely neglect stochastic partial observability in general Dec-POMDPs (Lyu et al., 2021; 2022). In contrast, AERIAL offers a simple way to adjust factorization approaches by replacing the true state $s_t$ with a learned representation of multi-agent recurrence $P_{\\\\pi}(\\\\tau_t | b_0)$ to consider more accurate closed-loop information about decentralized agent decisions. The rest of the training scheme remains unchanged, which eases adjustment of existing approaches.\\n\\nSince the naive independence assumption of individual memory representations $h_{t,i}$ does not hold in practice \u2013 despite decentralization \u2013 we use a simplified transformer to consider the latent dependencies of all $h_{t,i} \\\\in h_t$ along the agent axis to learn an adequate representation of multi-agent recurrence $P_{\\\\pi}(\\\\tau_t | b_0)$ according to Eq. 5. AERIAL does not depend on true states therefore requiring less overall information than state-based CTDE, since we assume $h_t$ to be available in all CTDE setups anyway (Foster et al., 2018; Rashid et al., 2020). Note that AERIAL does not necessarily require RNNs to obtain $h_t$ as hidden layers of MLPs or decision transformers can be used to approximate $h_t$ as well (Son et al., 2019; Chen et al., 2021).\"}"}
{"id": "phan23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability\\n\\nFigure 2.\\nLeft: Screenshot of two SMAC maps.\\nMiddle: PCA visualization of the joint observations in original SMAC within the first 5 steps of 1,000 episodes using a random policy with $K = 0$ initial random steps.\\nRight: Analogous PCA visualization for MessySMAC with $K = 10$ initial random steps. For visual comparability, the observations are deterministic here.\\n\\n5. MessySMAC\\n\\n5.1. Limitation of SMAC as a Benchmark\\n\\nStarCraft Multi-Agent Challenge (SMAC) provides a rich set of micromanagement tasks, where a team of learning agents has to fight against an enemy team, which acts according to handcrafted heuristics of the built-in StarCraft AI (Samvelyan et al., 2019). SMAC currently represents the de facto standard for MARL evaluation (Rashid et al., 2018; 2020; Wang et al., 2021). However, SMAC scenarios exhibit very limited stochastic partial observability due to deterministic observations and low variance in initial states therefore only representing simplified special cases rather than general Dec-POMDP challenges (Lyu et al., 2022; Ellis et al., 2022). To assess practicability of MARL, we need benchmarks with sufficient stochasticity as the real-world is generally messy and only observable through noisy sensors.\\n\\n5.2. SMAC with Stochastic Partial Observability\\n\\nMessySMAC is a modified version of SMAC with observation stochasticity w.r.t. $\\\\Omega$, where all measured values of observation $z_{t,i}$ are negated with a probability of $\\\\phi \\\\in [0, 1)$, and initialization stochasticity w.r.t. $b_0$, where $K$ random steps are initially performed before officially starting an episode. During the initial phase, the agents can already be ambushed by the built-in AI, which further increases difficulty compared to the original SMAC maps if $K > 0$.\\n\\nMessySMAC represents a more general Dec-POMDP challenge which enables systematic evaluation under various stochasticity configurations according to $\\\\phi$ and $K$.\\n\\nFig. 2 shows the PCA visualization of joint observations in two maps of original SMAC ($K = 0$) and MessySMAC ($K = 10$) within the first 5 steps of 1,000 episodes using a random policy. In original SMAC, the initial observations of $s_0$ (dark purple) are very similar and can be easily distinguished from subsequent observations by merely regarding time steps. Therefore, open-loop control might already be sufficient to solve these scenarios satisfactorily as hypothesized in (Ellis et al., 2022). However, the distinction of observations by time steps is more tricky in MessySMAC due to significantly higher entropy in $b_0$, indicating higher initialization stochasticity and a stronger requirement for closed-loop control, where agents need to explicitly consider their actual observations to make proper decisions.\\n\\n5.3. Comparison with SMACv2\\n\\nSMACv2 is an update to the original SMAC benchmark featuring initialization stochasticity w.r.t. position and unit types, as well as observation restrictions (Ellis et al., 2022). SMACv2 addresses similar issues as MessySMAC but MessySMAC additionally features observation stochasticity w.r.t. $\\\\Omega$ according to the general Dec-POMDP formulation in Section 2.1. Unlike MessySMAC, SMACv2 does not support the original SMAC maps thus not enabling direct comparability w.r.t. stochasticity configurations. Therefore, SMACv2 can be viewed as entirely new StarCraft II benchmark, while MessySMAC represents a SMAC extension, enabling systematic evaluation under various stochasticity configurations for the original SMAC maps.\"}"}
{"id": "phan23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Average learning progress w.r.t. the return of AERIAL variants and state-of-the-art baselines in Dec-Tiger over 50 runs. Shaded areas show the 95% confidence interval.\\n\\n6. Experiments\\n\\nWe use the state-based CTDE implementations of QPLEX, CW-QMIX, OW-QMIX, and QMIX from (Rashid et al., 2020) as state-of-the-art baselines with their default hyperparameters. We also integrate MAPPO from (Yu et al., 2022). For all experiments, we report the average performance and the 95% confidence interval over at least 20 runs.\\n\\nAERIAL is implemented using QMIX as factorization operator $\\\\Psi$ according to Fig. 1. We also experimented with QPLEX as alternative with no significant difference in performance. Thus, we stick with QMIX for efficiency due to fewer trainable parameters. The transformer of AERIAL has 4 heads with $W_c^q$, $W_c^k$, and $W_c^v$ each having one hidden layer of $d_{\\\\text{att}} = 64$ units with ReLU activation. The subsequent MLP layers have 64 units with ReLU activation.\\n\\nFor ablation study, we implement AERIAL (no attention), which trains $\\\\Psi$ directly on $h_t$ without self-attention as described in Section 4.2, and AERIAL (raw history), which trains $\\\\Psi$ on the raw joint history $\\\\tau_t$ concatenated with the true state $s_t$ as originally proposed for actor-critic methods (Lyu et al., 2022).\\n\\n6.1. Dec-Tiger Setting\\n\\nWe use the Dec-Tiger problem described in Section 4.1 and (Nair et al., 2003) as simple proof-of-concept domain with $T = 4$ and $\\\\gamma = 1$. We also provide the optimal value of 4.8 computed with MAA* (Szer et al., 2005).\\n\\nResults\\n\\nThe results are shown in Fig. 3. AERIAL comes closest to the optimum, achieving an average return of about zero. AERIAL (no attention) performs second best with an average return of about -8, while all other approaches achieve an average return of about -15.\\n\\nDiscussion\\n\\nThe results confirm the example from Section 4.1 and the findings of (Oliehoek et al., 2008; Lyu et al., 2022). All state-based CTDE approaches and AERIAL (raw history) converge to a one-step policy, where both agents optimistically open the same door regardless of any agent observation. AERIAL (no attention) converges to a local optimum most of the time, where both agents only listen for all $T = 4$ time steps. AERIAL performs best due to considering the latent dependencies of all memory representations $h_{t,i} \\\\in h_t$ via self-attention to learn an adequate representation of multi-agent recurrence $P_\\\\pi(\\\\tau_t | b_0)$ according to Eq. 5.\\n\\n6.2. Original SMAC Setting\\n\\nWe evaluate AERIAL in original SMAC using the maps 3s5z vs 10m vs 11m, which are classified as easy, as well as the hard maps 2c vs 64zg, 3s vs 5z, and 5m vs 6m, and the super hard map 3s5z vs 3s6z (Samvelyan et al., 2019).\\n\\nResults\\n\\nThe final average test win rates after 2 million steps of training are shown in Table 1. AERIAL is competitive to QPLEX and QMIX in the easy maps, while performing best in 3s vs 5z and 5m vs 6m. MAPPO performs best in 2c vs 64zg and 3s5z vs 3s6z, with AERIAL being second best in the super hard map 3s5z vs 3s6z.\\n\\nDiscussion\\n\\nAERIAL is competitive to state-of-the-art baselines in original SMAC, indicating that replacing the true state $s_t$ with the joint memory representation $h_t$ does not notably harm performance. Despite outperforming most baselines in some maps, we do not claim significant outperformance here, since we regard most SMAC maps as widely solved by the community anyway (Ellis et al., 2022).\\n\\n6.3. MessySMAC Setting\\n\\nWe evaluate AERIAL in MessySMAC using the same maps as in Section 6.2. We set $\\\\phi = 15\\\\%$ and $K = 10$.\\n\\nResults\\n\\nThe results are shown in Fig. 4. AERIAL performs best in all maps with AERIAL (no attention) being second best except in 2c vs 64zg. In 3s5z vs 3s6z, only AERIAL and AERIAL (no attention) progress notably. AERIAL (raw history) performs worst in all maps. MAPPO only progresses notably in 2c vs 64zg.\\n\\nDiscussion\\n\\nSimilar to the Dec-Tiger experiment, the results confirm the benefit of exploiting more accurate closed-loop information in domains with stochastic partial observability. AERIAL consistently outperforms AERIAL (no attention), indicating that self-attention can correct for the naive independence assumption of all $h_{t,i} \\\\in h_t$. MAPPO performs especially poorly in MessySMAC due to\"}"}
{"id": "phan23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Average win rate of AERIAL and state-of-the-art baselines after 2 million time steps of training across 400 final test episodes for the original SMAC maps with the 95% confidence interval. The best results per map are highlighted in boldface and blue.\\n\\n| Configuration   | AERIAL | QPLEX | CW-QMIX | OW-QMIX | QMIX   | MAPPO  |\\n|-----------------|--------|-------|---------|---------|--------|--------|\\n| 3s5z            | 0.95 \u00b1 0.01 | 0.94 \u00b1 0.01 | 0.87 \u00b1 0.02 | 0.91 \u00b1 0.02 | 0.95 \u00b1 0.01 | 0.95 \u00b1 0.01 |\\n| 10m vs 11m      | 0.97 \u00b1 0.01 | 0.90 \u00b1 0.02 | 0.91 \u00b1 0.02 | 0.95 \u00b1 0.02 | 0.90 \u00b1 0.02 | 0.90 \u00b1 0.02 |\\n| 2c vs 64zg      | 0.52 \u00b1 0.11 | 0.29 \u00b1 0.10 | 0.38 \u00b1 0.12 | 0.55 \u00b1 0.13 | 0.55 \u00b1 0.13 | 0.55 \u00b1 0.13 |\\n| 3s vs 5z        | 0.96 \u00b1 0.02 | 0.74 \u00b1 0.11 | 0.18 \u00b1 0.06 | 0.08 \u00b1 0.04 | 0.81 \u00b1 0.05 | 0.81 \u00b1 0.05 |\\n| 5m vs 6m        | 0.77 \u00b1 0.03 | 0.66 \u00b1 0.04 | 0.41 \u00b1 0.04 | 0.55 \u00b1 0.06 | 0.67 \u00b1 0.05 | 0.67 \u00b1 0.05 |\\n| 3s5z vs 3s6z    | 0.18 \u00b1 0.09 | 0.1 \u00b1 0.03 | 0.0 \u00b1 0.0 | 0.0 \u00b1 0.0 | 0.02 \u00b1 0.01 | 0.02 \u00b1 0.01 |\\n\\nFigure 4. Average learning progress w.r.t. the win rate of AERIAL variants and state-of-the-art baselines in MessySMAC for 2 million steps over 20 runs. Shaded areas show the 95% confidence interval. The legend at the top applies across all plots.\\n\\n6.4. Robustness against Stochastic Partial Observability\\n\\nSetting\\n\\nTo evaluate the robustness of AERIAL and AERIAL (no attention) against various stochasticity configurations in MessySMAC, we manipulate $\\\\Omega$ through the observation negation probability $\\\\phi$ and $b_0$ through the number of initial random steps $K$ as defined in Section 5.2. We compare the results with QMIX and QPLEX as the best performing state-of-the-art baselines in MessySMAC according to the results in Section 6.3. We present summarized plots, where the results are aggregated across all maps from Section 6.3. To avoid that easy maps dominate the average win rate, since all approaches achieve high values there, we normalize the values by the maximum win rate achieved in the respective map for all tested configurations of $\\\\phi$ and $K$. Thus, we ensure an equal weighting regardless of the particular difficulty level. If not mentioned otherwise, we set $\\\\phi = 15\\\\%$ and $K = 10$ as default parameters based on Section 6.3.\\n\\nResults\\n\\nThe results regarding observation stochasticity w.r.t. $\\\\Omega$ and $\\\\phi$ are shown in Fig. 5. Fig. 5(a) shows that the average win rates of all approaches decrease with increasing $\\\\phi$ with AERIAL consistently achieving the highest average win rate in all configurations. Fig. 5(b) shows that AERIAL performs best in most MessySMAC maps, especially when $\\\\phi \\\\geq 15\\\\%$. AERIAL (no attention) performs second best.\\n\\nThe results regarding initialization stochasticity w.r.t. $b_0$ and $K$ are shown in Fig. 6. Analogously to Fig. 5, Fig. 6(a) shows that the average (normalized) win rates of all approaches decrease with increasing $b_0$ with AERIAL consistently achieving the highest average win rate in all configurations.\"}"}
{"id": "phan23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. The values of $Q^*_{MDP}$ and $Q^*$ for the final time step $t = 2$ in the Dec-Tiger example from Section 4.1.\\n\\n$Q^*_{MDP}(s_L, a_t)$ $Q^*_{MDP}(s_R, a_t)$ $Q^*(\u03c4_t, a_t)$\\n\\n$\u27e8l_i, l_i\u27e9 - 2 - 2 - 2$ $\u27e8l_i, o_L\u27e9 - 101 + 9 - 46$ $\u27e8o_L, l_i\u27e9 - 101 + 9 - 46$\\n\\n$\u27e8o_L, o_L\u27e9 - 50 + 20 - 15$ $\u27e8o_L, o_R\u27e9 - 100 - 100 - 100$ $\u27e8o_R, o_L\u27e9 - 100 - 100 - 100$\\n\\n$\u27e8o_R, l_i\u27e9 + 9 - 101 - 46$ $\u27e8o_R, o_R\u27e9 + 20 - 50 - 15$ $\u27e8l_i, o_R\u27e9$\"}"}
{"id": "phan23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D. Experiment Details\\n\\nD.1. Computing infrastructure\\nAll training and test runs were performed in parallel on a computing cluster of fifteen x86 64 GNU/Linux (Ubuntu 18.04.5 LTS) machines with i7-8700 @ 3.2GHz CPU (8 cores) and 64 GB RAM. We did not use any GPU in our experiments.\\n\\nD.2. Hyperparameters and Neural Network Architectures\\nOur experiments are based on PyMARL and the code from (Rashid et al., 2020) under the Apache License 2.0. We use the default setting from the paper without further hyperparameter tuning as well as the same neural network architectures for the agent RNNs, i.e., gated recurrent units (GRU) of (Cho et al., 2014) with 64 units, and the respective factorization operators \\\\( \\\\Psi \\\\) as specified by default for each state-of-the-art baseline in Section 6. We set the loss weight \\\\( \\\\alpha = 0.75 \\\\) for \\\\( \\\\text{CW-QMIX} \\\\) and \\\\( \\\\text{OW-QMIX} \\\\). For \\\\( \\\\text{MAPPO} \\\\), we use the hyperparameters suggested in (Yu et al., 2022) for SMAC, where we set the clipping parameter to 0.1 and use an epoch count of 5. The parameter \\\\( \\\\lambda \\\\) for generalized advantage estimation is set to 1. The centralized critic has two hidden layers of 128 units with ReLU activation, a single linear output, and conditions on agent-specific global states which concatenate the true state \\\\( s_t \\\\) and the individual observation \\\\( z_{t,i} \\\\) per agent \\\\( i \\\\). The policy network of MAPPO has a similar recurrent architecture like the local utility functions \\\\( Q_i \\\\) and additionally applies softmax to the output layer.\\n\\nAERIAL is implemented using \\\\( \\\\text{QMIX} \\\\) as factorization operator \\\\( \\\\Psi \\\\) according to Fig. 1. We also experimented with \\\\( \\\\text{QPLEX} \\\\) as alternative with no significant difference in performance. Thus, we stick with \\\\( \\\\text{QMIX} \\\\) for computational efficiency due to fewer trainable parameters. The transformer has \\\\( C = 4 \\\\) heads \\\\( c \\\\in \\\\{1, \\\\ldots, C\\\\} \\\\) with respective MLPs \\\\( W_{qc}, W_{kc}, \\\\) and \\\\( W_{vc} \\\\), each having one hidden layer of 64 units with ReLU activation. The three subsequent MLP layers of Line 22 in Algorithm 1 have 64 units with ReLU activation. All neural networks are trained using RMSProp with a learning rate of 0.0005.\"}"}
{"id": "phan23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability\\n\\nThomy Phan\u2020\\nFabian Ritz2\\nPhilipp Altmann2\\nMaximilian Zorn2\\nJonas N\u00fc\u00dflein2\\nMichael K\u00f6lle2\\nThomas Gabor2\\nClaudia Linnhoff-Popien2\\n\\nAbstract\\nStochastic partial observability poses a major challenge for decentralized coordination in multi-agent reinforcement learning but is largely neglected in state-of-the-art research due to a strong focus on state-based centralized training for decentralized execution (CTDE) and benchmarks that lack sufficient stochasticity like StarCraft Multi-Agent Challenge (SMAC). In this paper, we propose Attention-based Embeddings of Recurrence In multi-Agent Learning (AERIAL) to approximate value functions under stochastic partial observability. AERIAL replaces the true state with a learned representation of multi-agent recurrence, considering more accurate information about decentralized agent decisions than state-based CTDE. We then introduce MessySMAC, a modified version of SMAC with stochastic observations and higher variance in initial states, to provide a more general and configurable benchmark regarding stochastic partial observability. We evaluate AERIAL in Dec-Tiger as well as in a variety of SMAC and MessySMAC maps, and compare the results with state-based CTDE. Furthermore, we evaluate the robustness of AERIAL and state-based CTDE against various stochasticity configurations in MessySMAC.\\n\\n1. Introduction\\nA wide range of real-world applications like fleet management, industry 4.0, or communication networks can be formulated as decentralized partially observable Markov decision process (Dec-POMDP) representing a cooperative multi-agent system (MAS), where multiple agents have to coordinate to achieve a common goal (Oliehoek & Amato, 2016). Stochastic partial observability poses a major challenge for decentralized coordination in Dec-POMDPs due to noisy sensors and potentially high variance in initial states which are common in the real world (Kaelbling et al., 1998; Oliehoek & Amato, 2016).\\n\\nMulti-agent reinforcement learning (MARL) is a general approach to tackle Dec-POMDPs with remarkable progress in recent years (Wang et al., 2021; Wen et al., 2022). State-of-the-art MARL is based on centralized training for decentralized execution (CTDE), where training takes place in a laboratory or a simulator with access to global information (Lowe et al., 2017; Foerster et al., 2018). For example, state-based CTDE exploits true state information to learn a centralized value function in order to derive coordinated policies for decentralized decision making (Rashid et al., 2018; Yu et al., 2022). Due to its effectiveness in the StarCraft Multi-Agent Challenge (SMAC) as the current de facto standard for MARL evaluation, state-based CTDE has become very popular and is widely considered an adequate approach to general Dec-POMDPs for more than half a decade, leading to the development of many increasingly complex algorithms (Lyu et al., 2021; 2022).\\n\\nHowever, merely relying on state-based CTDE and SMAC in MARL research can be a pitfall in practice as stochastic partial observability is largely neglected \u2013 despite being an important aspect in Dec-POMDPs (Lyu et al., 2022):\\n\\nFrom an algorithm perspective, purely state-based value functions are insufficient to evaluate and adapt multi-agent behavior, since all agents make decisions on a completely different basis, i.e., individual histories of noisy observations and actions. True Dec-POMDP value functions consider more accurate closed-loop information about decentralized agent decisions though (Oliehoek et al., 2008). Furthermore, the optimal state-based value function represents an upper-bound of the true optimal Dec-POMDP value function thus state-based CTDE can result in overly optimistic behavior in general Dec-POMDPs (Lyu et al., 2022).\\n\\nFrom a benchmark perspective, SMAC has very limited...\"}"}
{"id": "phan23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We formulate cooperative MAS problems as Dec-POMDPs. Therefore, SMAC scenarios only represent simplified value functions under agent-wise stochastic partial observability and low variance in initial states (Ellis et al., 2022).\\n\\nJoint quantities are written in bold face. Over true states, \\\\( \\\\tau \\\\) joint history, \\\\( s \\\\) local history, \\\\( z \\\\) the probability of joint observation, \\\\( r \\\\) the probability of joint action, \\\\( \\\\langle A \\\\rangle \\\\) joint action, \\\\( s_t \\\\) is the set of joint actions, \\\\( M \\\\) is the set of agents, \\\\( R \\\\) is the shared reward, \\\\( Z \\\\times A \\\\) is the state transition probability, \\\\( \\\\Omega(\\\\cdot) \\\\) and \\\\( \\\\Omega(\\\\cdot,b) \\\\) are sets of joint observations, \\\\( \\\\Omega(z) \\\\) is the probability of joint observation, \\\\( s_0 \\\\) is the probability distribution over initial states, and \\\\( \\\\pi \\\\) is the set of joint actions. Each agent \\\\( t,i \\\\) is the set of joint actions, \\\\( X \\\\) is the set of true states, \\\\( s_t \\\\) is the shared reward, \\\\( Z^t \\\\) is the state transition probability, \\\\( \\\\Omega(\\\\cdot) \\\\) and \\\\( \\\\Omega(\\\\cdot,b) \\\\) are sets of local observations, \\\\( \\\\Omega(z) \\\\) is the probability of joint observation, \\\\( s_0 \\\\) is the probability distribution over initial states, \\\\( \\\\pi \\\\) is the set of joint actions. Each agent \\\\( t,i \\\\) maintains a belief state \\\\( b_t^{t,i} \\\\), updatable by Bayes' theorem (Kaelbling et al., 1998). Joint quantities are written in bold face.\\n\\nAttention-based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability\\n\\nIn this paper, we propose AERIAL, which aims to provide a more adequate evaluation. Our contributions are as follows: we formulate and discuss the concepts of AERIAL, we evaluate AERIAL in Dec-Tiger, a small and traditional benchmark that is often used in Dec-POMDPs, and we introduce MessySMAC to enable systematic evaluation of AERIAL in multi-Agent Learning (AERIAL).\\n\\nWe show that AERIAL achieves competitive performance of original SMAC and MessySMAC maps, and competitive performance against state-based CTDE. We then introduce MessySMAC to enable systematic evaluation of AERIAL and state-based CTDE against stochastic partial observability in Dec-POMDPs. Furthermore, we evaluate the robustness of AERIAL and state-based CTDE against stochastic partial observability in Dec-POMDPs. In general Dec-POMDPs, partially observable MAS (POMAS) is greatly simplified. In MDP-like settings with a centralized controller, the optimal value function \\\\( Q^* \\\\) is defined by (Watkins & Dayan, 1992; Boutilier, 1996):\\n\\n\\\\[\\nQ^*(s) = \\\\max_{a} \\\\{ r(s,a) + \\\\gamma \\\\sum_{s'} P(s,a,s') Q^*(s') \\\\}\\n\\\\]\\n\\nwhere \\\\( r(s,a) \\\\) is the reward, \\\\( P(s,a,s') \\\\) is the state transition probability, \\\\( \\\\gamma \\\\) is the discount factor, \\\\( s \\\\) is the probability of joint observation, and \\\\( Q^*(s) \\\\) is the optimal value function. In MDP-like settings with a centralized controller, the optimal value function \\\\( Q^* \\\\) is defined by (Oliehoek et al., 2008):\\n\\n\\\\[\\nQ^*(s) = \\\\max_{a} \\\\{ r(s,a) + \\\\gamma \\\\sum_{s'} P(s,a,s') \\\\max_{a'} Q^*(s') \\\\}\\n\\\\]\\n\\nwhere \\\\( r(s,a) \\\\) is the reward, \\\\( P(s,a,s') \\\\) is the state transition probability, \\\\( \\\\gamma \\\\) is the discount factor, \\\\( Q^*(s) \\\\) is the optimal value function, \\\\( s \\\\) is the probability of joint observation, and \\\\( Q^*(s) \\\\) is the optimal value function.\"}"}
{"id": "phan23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability\\n\\n\\\\[ P_{\\\\pi}(\\\\tau_t | b_0) = P(z_0 | b_0) \\\\prod_{t=1}^{c} P(z_{c,i} | \\\\tau_{c-1,i} , \\\\pi_{i}) \\\\]\\n\\nwhere \\\\( T(s_{c} | s_{c-1} , \\\\pi_{i}(\\\\tau_{c-1})) \\\\) and \\\\( \\\\Omega(z_{c,i} | \\\\pi_{i}(\\\\tau_{c-1}), s_{c}) \\\\).\\n\\nSince all agents act according to their local history \\\\( \\\\tau_t,i \\\\) without access to the complete joint history \\\\( \\\\tau_t \\\\), recurrence depends on more accurate closed-loop information than just true states \\\\( s_t \\\\), i.e., all previous observations, actions, and probabilities according to \\\\( b_0, T, \\\\) and \\\\( \\\\Omega \\\\).\\n\\nQ* MDP is proven to represent an upper bound of \\\\( Q^* \\\\) (Oliehoek et al., 2008). Thus, naively deriving local policies \\\\( \\\\pi_i \\\\) from \\\\( Q^* \\\\) instead of \\\\( Q^* \\\\) can result in overly optimistic behavior as we will show in Section 4.1 and 6.\\n\\n2.3. Multi-Agent Reinforcement Learning\\n\\nFinding an optimal joint policy \\\\( \\\\pi^* \\\\) via exhaustive computation of \\\\( Q^* \\\\) according to Eq. 3-5 is intractable in practice (Nair et al., 2003; Szer et al., 2005). MARL offers a scalable way to learn \\\\( Q^* \\\\) and \\\\( \\\\pi^* \\\\) via function approximation, e.g., using CTDE, where training takes place in a laboratory or a simulator with access to global information (Lowe et al., 2017; Foerster et al., 2018). We focus on value-based MARL to learn a centralized value function \\\\( Q_{tot} \\\\approx Q^* \\\\), which can be factorized into local utility functions \\\\( \\\\langle Q_i \\\\rangle_{i} \\\\in D \\\\) for decentralized decision making via \\\\( \\\\pi_i(\\\\tau_{t,i}) = \\\\arg\\\\max_{a_{t,i}} Q_i(\\\\tau_{t,i},a_{t,i}) \\\\).\\n\\nFor that, a factorization operator \\\\( \\\\Psi \\\\) is used (Phan et al., 2021):\\n\\n\\\\[ Q_{tot}(\\\\tau_t, a_t) = \\\\Psi(Q_1(\\\\tau_{t,1},a_{t,1}),...,Q_N(\\\\tau_{t,N},a_{t,N})) \\\\]\\n\\nIn practice, \\\\( \\\\Psi \\\\) is realized with deep neural networks, such that \\\\( \\\\langle Q_i \\\\rangle_{i} \\\\in D \\\\) can be learned end-to-end via backpropagation by minimizing the mean squared temporal difference (TD) error (Rashid et al., 2018; Sunehag et al., 2018). A factorization operator \\\\( \\\\Psi \\\\) is decentralizable when satisfying the IGM (Individual-Global-Max) such that (Son et al., 2019):\\n\\n\\\\[ \\\\arg\\\\max_{a_{t}} Q_{tot}(\\\\tau_t, a_t) = \\\\left\\\\{ \\\\begin{array}{ll}\\n\\\\arg\\\\max_{a_{t,1}} Q_1(\\\\tau_{t,1},a_{t,1}) ... \\\\\\\\\\n\\\\arg\\\\max_{a_{t,N}} Q_N(\\\\tau_{t,N},a_{t,N})\\n\\\\end{array} \\\\right. \\\\]\\n\\nThere exists a variety of factorization operators \\\\( \\\\Psi \\\\) which satisfy Eq. 7 using monotonicity like QMIX (Rashid et al., 2018), nonlinear transformation like QPLEX (Wang et al., 2021), or loss weighting like CW- and OW-QMIX (Rashid et al., 2020). Most approaches use state-based CTDE to learn \\\\( Q^* \\\\) MDP according to Eq. 1 instead of \\\\( Q^* \\\\) (Eq. 3-5).\\n\\n2.4. Recurrent Reinforcement Learning\\n\\nIn partially observable settings, the policy \\\\( \\\\pi_i \\\\) of agent \\\\( i \\\\) conditions on the history \\\\( \\\\tau_{t,i} \\\\) of past observations and actions (Kaelbling et al., 1998; Oliehoek & Amato, 2016). In practice, recurrent neural networks (RNNs) like LSTMs or GRUs are used to learn a compact representation \\\\( h_t,i \\\\) of \\\\( \\\\tau_{t,i} \\\\) and \\\\( \\\\pi_i \\\\) known as hidden state or memory representation, which implicitly encodes the individual recurrence of agent \\\\( i \\\\), i.e., the distribution \\\\( P_{\\\\pi_i}(\\\\tau_{t,i} | b_0) \\\\) (Hochreiter & Schmidhuber, 1997; Cho et al., 2014; Hu & Foerster, 2019):\\n\\n\\\\[ P_{\\\\pi_i}(\\\\tau_{t,i} | b_0) = P(z_{0,i} | b_0) \\\\prod_{t=1}^{c} P(z_{c,i} | \\\\tau_{c-1,i}, \\\\pi_{i}) \\\\]\\n\\nRNNs are commonly used for partially observable problems and have been empirically shown to be more effective than using raw observations \\\\( z_{t,i} \\\\) or histories \\\\( \\\\tau_{t,i} \\\\) (Hausknecht & Stone, 2015; Samvelyan et al., 2019; Vinyals et al., 2019).\\n\\n3. Related Work\\n\\nMulti-Agent Reinforcement Learning\\n\\nIn recent years, MARL has achieved remarkable progress in challenging domains (Gupta et al., 2017; Vinyals et al., 2019). State-of-the-art MARL is based on CTDE to learn a centralized value function \\\\( Q_{tot} \\\\) for actor-critic learning (Lowe et al., 2017; Foerster et al., 2018; Yu et al., 2022) or factorization (Rashid et al., 2018; 2020; Wang et al., 2021). However, the majority of works assumes a simplified Dec-POMDP setting, where \\\\( \\\\Omega \\\\) is deterministic, and uses true states to approximate \\\\( Q^* \\\\) MDP according to Eq. 1 instead of \\\\( Q^* \\\\) (Eq. 3-5). Thus, state-based CTDE is possibly less effective in more general Dec-POMDP settings. Our approach addresses stochastic partial observability with a learned representation of multi-agent recurrence according to Eq. 5 instead of \\\\( s_t \\\\).\\n\\nWeaknesses of State-Based CTDE\\n\\nRecent works investigated potential weaknesses of state-based CTDE for multi-agent actor-critic methods regarding bias and variance (Lyu et al., 2021; 2022). The experimental results show that state-based CTDE can surprisingly fail in very simple Dec-POMDP benchmarks that exhibit more stochasticity than SMAC. While these studies can be considered an important step towards general Dec-POMDPs, there is neither an approach which adequately addresses stochastic partial observability nor a benchmark to systematically evaluate such an approach yet. In this work, we focus on value-based MARL, where learning an accurate value function is important for meaningful factorization, and propose an attention-based recurrence approach to approximate value functions under stochastic partial observability. We also introduce\"}"}
{"id": "phan23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability\\n\\nA modified SMAC benchmark, which enables systematic evaluation under various stochasticity configurations. Attention has been used in CTDE to process information of potentially variable length $N$, where joint observations $z_t$, joint actions $a_t$, or local utilities $\\\\langle Q_i \\\\rangle_i \\\\in D$ are weighted and aggregated to provide a meaningful representation for value function approximation (Iqbal & Sha, 2019; Wang et al., 2021; Iqbal et al., 2021; Wen et al., 2022; Khan et al., 2022). Most works focus on Markov games without observation stochasticity, which are special cases of the Dec-POMDP setting. In this work, we focus on stochastic partial observability and apply self-attention to the memory representations $h_{t,i}$ of all agents' RNNs instead of the raw observations $z_{t,i}$ to approximate $Q^*$ for general Dec-POMDPs according to Eq. 3-5.\\n\\n4. AERIAL\\n\\n4.1. Limitation of State-Based CTDE\\n\\nMost state-of-the-art works assume a simplified Dec-POMDP setting, where $\\\\Omega$ is deterministic, and approximate $Q^*_{MDP}$ according to Eq. 1 instead of $Q^*$ (Eq. 3-5).\\n\\nIf there are only deterministic observations and initial states $s_0$ such that $b_0(s_0) = 1$ and $b_0(s'_0) = 0$ if $s'_0 \\\\neq s_0$, then multi-agent recurrence $P_\\\\pi(\\\\tau_t | b_0)$ as defined in Eq. 5 would only depend on state transition probabilities $T(s_{t+1} | s_t, a_t)$ which are purely state-based, ignoring decentralization of agents and observations (Oliehoek et al., 2008). In such scenarios, stochastic partial observability is very limited, especially if all $\\\\pi_i$ are deterministic. We hypothesize that this is one reason for the empirical success of state-based CTDE in original SMAC, whose scenarios seemingly have these simplifying properties (Ellis et al., 2022).\\n\\nIn the following, we regard a small example, where state-based CTDE can fail at finding an optimal joint policy $\\\\pi^*$. Example Dec-Tiger is a traditional and simple Dec-POMDP benchmark with $N = 2$ agents facing two doors (Nair et al., 2003). A tiger is randomly placed behind the left ($s_L$) or right door ($s_R$) representing the true state. Both agents are able to listen ($l_i$) and open the left ($o_L$) or right door ($o_R$). The listening action $l_i$ produces a noisy observation of either hearing the tiger to be left ($z_L$) or right ($z_R$), which correctly indicates the tiger's position with $85\\\\%$ chance and a cost of $-1$ per listening agent. If both agents open the same door, the episode terminates with a reward of $-50$ if opening the tiger door and $+20$ otherwise. If both agents open different doors, the episode ends with $-100$ reward and, if only one agent opens a door while the other agent is listening, the episode terminates with $-101$ if opening the tiger door and $+9$ otherwise.\\n\\nGiven a horizon of $T = 2$, the tiger being behind the right door ($s_R$), and both agents having listened in the first step, where agent $1$ heard $z_L$ and agent $2$ heard $z_R$: Assuming that both agents learned to perform the same actions, e.g., due to CTDE and parameter sharing (Tan, 1993; Gupta et al., 2017), $Q^*_{MDP}$ and $Q^*$ would estimate the following values $2$:\\n\\n$$Q^*_{MDP}(s_R, \\\\langle l_i, l_i \\\\rangle) = -2$$\\n$$Q^*(\\\\tau_t, \\\\langle l_i, l_i \\\\rangle) = -2$$\\n$$Q^*_{MDP}(s_R, \\\\langle o_L, o_L \\\\rangle) = 20$$\\n$$Q^*(\\\\tau_t, \\\\langle o_L, o_L \\\\rangle) = -15$$\\n$$Q^*_{MDP}(s_R, \\\\langle o_R, o_R \\\\rangle) = -50$$\\n$$Q^*(\\\\tau_t, \\\\langle o_R, o_R \\\\rangle) = -15$$\\n\\nAny policy $\\\\pi^*_{MDP}$ or decentralizable joint policy $\\\\pi$ w.r.t. IGM (Eq. 7) that maximizes $Q^*_{MDP}$ according to Eq. 2 would optimistically recommend $\\\\langle o_L, o_L \\\\rangle$ based on the true state $s_R$, regardless of what the agents observed. However, any joint policy $\\\\pi^*$ that maximizes the expectation of $Q^*$ according to Eq. 4 would consider agent-wise stochastic partial observability and recommend $\\\\langle l_i, l_i \\\\rangle$, which corresponds to the true optimal decision for $T = 2$ (Szer et al., 2005).\\n\\n4.2. Attention-Based Embeddings of Recurrence\\n\\nPreliminaries\\n\\nWe now introduce Attention-based Embeddings of Recurrence In multi-Agent Learning (AERIAL) to approximate optimal Dec-POMDP value functions according to Eq. 3-5. Our setup uses a factorization operator $\\\\Psi$ like QMIX or QPLEX according to Eq. 6-7. All agents process their local histories $\\\\tau_{t,i}$ via RNNs as motivated in Section 2.4 and schematically shown in Fig. 1 (left).\\n\\nUnlike $Q^*_{MDP}$, the true optimal Dec-POMDP function $Q^*$ considers more accurate closed-loop information about decentralized agent decisions through multi-agent recurrence $P_\\\\pi(\\\\tau_t | b_0)$ according to Eq. 5. Simply replacing $s_t$ with $\\\\tau_t$ as suggested in (Lyu et al., 2022) is not sufficient because the resulting value function would assume a centralized controller with access to the complete joint history $\\\\tau_t$, in contrast to decentralized agents $i$ which can only access their respective local history $\\\\tau_{t,i}$ (Oliehoek et al., 2008).\\n\\nExploiting Multi-Agent Recurrence\\n\\nAt first we propose to naively exploit all individual recurrences by simply replacing the true state $s_t$ in CTDE with the joint memory representation $h_t = \\\\langle h_{t,i} \\\\rangle_i \\\\in D$ of all agents' RNNs. Each memory representation $h_{t,i}$ implicitly encodes the individual recurrence $P_\\\\pi_i(\\\\tau_{t,i} | b_0)$ according to Eq. 8. Therefore, $h_t$ provides more accurate closed-loop information about decentralized agent decisions than $s_t$.\\n\\nThis approach, called AERIAL (no attention), can already be considered a sufficient solution if all individual recurrences $P_\\\\pi_i(\\\\tau_{t,i} | b_0)$ are statistically independent such that $P_\\\\pi(\\\\tau_t | b_0) = \\\\prod_{N=1}^{N} P_\\\\pi_i(\\\\tau_{t,i} | b_0)$. The exact calculation is provided in the Appendix B.\"}"}
