{"id": "bugliarello22a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2. Benchmark languages and tasks. English is only used for training. Tasks legend: \u2713 train and test sets available; \u2713 test-only data available; * Japanese captions in xFlickr&CO are translations from English. Languages legend: A: Asiatic; C: Congo; E: European; T: Tibetan; Austron: Austronesian. Language codes are based on the ISO 639-3 international standard.\\n\\n| Language | Code | Family | Script | XVNLI | xGQA | MaRVL | xFlickr&CO | WIT |\\n|----------|------|--------|--------|-------|------|-------|------------|-----|\\n| English  | ENG  | Indo-E | Latin  | \u2713     | \u2713    | \u2713     | \u2713          | \u2713   |\\n| Arabic   | ARB  | Afro-A | Arabic  | \u2713     | \u2713    | \u2713     | \u2713          | \u2713   |\\n| Bengali  | BEN  | Indo-E | Bengali | \u2713     | \u2713    |       | \u2713          | \u2713   |\\n| Bulgarian| BUL  | Indo-E | Cyrillic| \u2713     |      |       | \u2713          | \u2713   |\\n| Danish   | DAN  | Indo-E | Latin  | \u2713     |      |       | \u2713          | \u2713   |\\n| Estonian | EST  | Uralic | Latin  | \u2713     |      |       | \u2713          | \u2713   |\\n| German   | DEU  | Indo-E | Latin  | \u2713     | \u2713    | \u2713     | \u2713          | \u2713   |\\n| Greek    | ELL  | Indo-E | Greek  | \u2713     |      |       | \u2713          | \u2713   |\\n| French   | FRA  | Indo-E | Latin  | \u2713     | \u2713    |       | \u2713          | \u2713   |\\n| Indonesian| IND | Austron | Latin | \u2713     | \u2713    | \u2713     | \u2713          | \u2713   |\\n| Japanese | JPN  | Japonic| Kanji  | \u2713     | \u2713    | \u2713     | \u2713          | \u2713   |\\n| Korean   | KOR  | Koreanic| Hangul | \u2713     | \u2713    | \u2713     | \u2713          | \u2713   |\\n| Mandarin | CMN  | Sino-T | Hanzi  | \u2713     | \u2713    | \u2713     | \u2713          | \u2713   |\\n| Portuguese| POR | Indo-E | Latin  | \u2713     | \u2713    |       | \u2713          | \u2713   |\\n| Russian  | RUS  | Indo-E | Cyrillic| \u2713     | \u2713    | \u2713     | \u2713          | \u2713   |\\n| Spanish  | SPA  | Indo-E | Latin  | \u2713     | \u2713    | \u2713     | \u2713          | \u2713   |\\n| Swahili  | SWA  | Niger-C | Latin  | \u2713     |      |       | \u2713          | \u2713   |\\n| Tamil    | TAM  | Dravidian| Tamil | \u2713     |      |       | \u2713          | \u2713   |\\n| Turkish  | TUR  | Turkic | Latin  | \u2713     | \u2713    | \u2713     | \u2713          | \u2713   |\\n| Vietnamese| VIE | Austro-A | Latin | \u2713     |      |       | \u2713          | \u2713   |\\n\\nIt is worth noting that 10 out of 20 languages have data for two or more tasks. Thus, IGLUE might also facilitate future research in cross-task knowledge transfer.\\n\\n3.3. From Zero-Shot to Few-Shot Setups\\n\\nThe established practice in multilingual text-based benchmarks (Hu et al., 2020; Liang et al., 2020; Ruder et al., 2021) is to frame cross-lingual transfer as a zero-shot learning problem. However, multilingual pretrained models can be additionally fine-tuned in a few-shot learning setup; that is, on a handful of data points in a target language annotated for a specific task. While computationally inexpensive, this strategy has been proven as very beneficial for performance in text-only tasks, especially on low-resource and distant languages (Lauscher et al., 2020; Ponti et al., 2021a).\\n\\nWe hence extend IGLUE to support few-shot learning setups by collecting samples for each V&L task. In doing so, we use the notion of \u2018annotation context\u2019 to define what a \u2018shot\u2019 means in each dataset: For instance, this corresponds to an image and its caption in cross-modal retrieval, and to an image and all its questions in visual question answering. For more details and statistics, we refer to Appendix A.\\n\\nNotably, the set of examples used for few-shot cross-lingual transfer may vary across experiments. The lack of a standard set for few-shot learning harms the replicability and comparability of such experiments, given that model performance exhibits a strong sensitivity to the selection of few-shot examples (Zhao et al., 2021). Therefore, following Schick & Sch\u00fcte (2021) and Pfeiffer et al. (2022), in addition to the train / validation / test splits, we also release standard few-shot splits for every task and language (Appendix A).\\n\\n4. Evaluation Framework\\n\\nThe new IGLUE benchmark allows for running a series of unprecedented comparative experiments and analyses. Here, we provide an overview of our experimental setup. We evaluate all the existing multilingual V&L pretrained models released so far. In particular, in order to further facilitate research and development in multilingual V&L modelling, we re-implement them in a single framework based on VOLT (Bugliarello et al., 2021) in PyTorch (Paszke et al., 2019), which also provides access to five English pretrained models and twelve downstream tasks. For further details about the framework, we refer to Appendix B.\\n\\nPretrained Models. The state-of-the-art multilingual V&L models that we evaluate follow a BERT-like architecture (Devlin et al., 2019). Their input is the concatenation of image region features extracted with a Faster R-CNN (Ren et al., 2015) object detector and textual tokens (Sennrich et al., 2016; Wu et al., 2016). The inputs are then processed by a single stack of Transformer layers (Vaswani et al., 2017) to obtain multimodal, contextualised representations. Each model is trained to minimise multiple objectives\u2014with minimal variations across models\u2014to learn how to understand text (masked language modelling), vision (masked region modelling) and their co-occurrence (image\u2013text matching).\\n\\nWe provide only condensed summaries here, and refer the reader to the original work for further details.\\n\\n\u2022 mUNITER and xUNITER. Liu et al. (2021) extend the UNITER architecture (Chen et al., 2020) multilingually by following the base approach of Ni et al. (2021): a batch of multimodal English data from Conceptual Captions (CC; Sharma et al. 2018) is alternated with a batch of text-only multilingual Wikipedia data (sampled from the 104 languages used for mBERT). The two models mainly differ in their initialisation: mUNITER from mBERT (Devlin et al., 2019) and xUNITER from XLM-R (Conneau et al., 2020).\\n\\n\u2022 M\u2083P. Ni et al. (2021) further introduce multimodal code-switched training tasks where words in English captions are randomly replaced with a translation with a certain probability. This setup allows the model to explicitly align images with non-English languages. In particular, the authors obtain word translations from bilingual dictionaries in 50 languages. In each multimodal batch, data from CC is fed to the model either fully in English or with code-switched words according to a certain probability.\"}"}
{"id": "bugliarello22a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3. Performance of multimodal models trained and evaluated on English test splits of the IGLUE tasks.\\n\\n| Model       | NLI   | QA    | Reasoning | Retrieval |\\n|-------------|-------|-------|-----------|-----------|\\n| XVNLI       | IR    | TR    | IR        | TR        |\\n| mUNITER     | 76.38 | 54.68 | 71.91     | 44.50     |\\n| xUNITER     | 75.77 | 54.83 | 71.55     | 38.45     |\\n| UC          | 76.89 | 53.75 | 68.22     | 31.35     |\\n| M3P         | 76.38 | 55.19 | 70.56     | 37.40     |\\n| LXMERT      | 76.72 | 53.04 | 68.95     | 36.85     |\\n| UNITER      | 77.15 | 55.59 | 71.11     | 43.05     |\\n| ViLBERT     | 77.15 | 55.82 | 71.02     | 38.70     |\\n| VisualBERT  | 77.15 | 53.67 | 71.18     | 43.95     |\\n| VL-BERT     | 77.49 | 55.74 | 71.72     | 39.90     |\\n\\nThe text-only multilingual batch is sampled from Wikipedia in 100 languages. The model is initialised from XLM-R.\\n\\nEquations and symbols used:\\n- UC\\n- ZO\\n- X\\n\\nTable 4. Average zero-shot and 'translate test' performance of multimodal models trained in English and evaluated on target languages in IGLUE. The best model for each method is in bold.\\n\\n| Model       | NLI   | QA    | Reasoning | Retrieval |\\n|-------------|-------|-------|-----------|-----------|\\n| Zero-Shot   |       |       |           |           |\\n| mUNITER     | 53.69 | 9.97  | 53.72     | 8.06      |\\n| xUNITER     | 58.48 | 21.72 | 54.59     | 14.04     |\\n| UC          | 62.05 | 29.35 | 57.28     | 20.31     |\\n| M3P         | 58.25 | 28.17 | 56.00     | 12.91     |\\n| Translate   |       |       |           |           |\\n| mUNITER     | 73.09 | 49.05 | 63.82     | 40.95     |\\n| xUNITER     | 72.83 | 49.15 | 64.04     | 36.26     |\\n| UC          | 73.67 | 50.19 | 63.09     | 36.03     |\\n| M3P         | 73.37 | 48.83 | 62.52     | 27.74     |\\n| LXMERT      | 72.57 | 48.08 | 62.51     | 34.02     |\\n| UNITER      | 73.65 | 50.62 | 61.92     | 41.04     |\\n| ViLBERT     | 73.45 | 50.33 | 62.39     | 36.97     |\\n| VisualBERT  | 74.12 | 48.72 | 62.35     | 41.64     |\\n| VL-BERT     | 73.86 | 49.78 | 64.16     | 38.18     |\\n\\nExperimental Setup.\\n\\nWe fine-tune all models using the AdamW optimiser (Loshchilov & Hutter, 2019) relying on the same hyper-parameters as in the controlled setup of Bugliarello et al. (2021). For few-shot experiments, we instead search three learning rates: {1e-5, 5e-5, 1e-4} and train for 20 epochs for each dataset-language-shots triplet. Before training, we extract, for each dataset, 36 image regions using a ResNet-101 backbone (He et al., 2016) for mUNITER, xUNITER and UC; and up to 100 image regions using a ResNeXt-101 backbone (Xie et al., 2017) for M3P\u2014both trained on Visual Genome (Krishna et al., 2017; Anderson et al., 2018). We train all models on a single NVIDIA V100 (16GB) GPU card. To increase accessibility to the benchmark by practitioners with limited computing resources:\\n\\n1. We set the number of epochs so that training a baseline can be completed in less than 12 hours per dataset (see Appendix B for details);\\n\\n2. In addition to reporting performance at every number of shots for few-shot learning, we also define a max-shot evaluation setup to reduce the numbers of runs to one per dataset\u2013language pair.\\n\\nFor both fine-tuning and few-shot experiments, we evaluate the parameter sets that yield the best validation performance.\\n\\nFor few-shot learning, models are initialised from the English fine-tuned parameter set and further trained on the data in the target language for the given task.\\n\\n5. Main Results and Discussion\\n\\n5.1. Zero-shot Learning\\n\\nThe main zero-shot results, averaged across languages, are reported in Table 4. In order to measure the gap between cross-lingual transfer and supervised learning, we also provide the results for the same models trained and evaluated on English test data in Table 3. The performance metric is accuracy for all tasks except cross-modal retrieval, which uses Recall@1.\\n\\nFinally, for completeness, the full results subdivided by language are detailed across Appendix C.2. Next, we compare these scores across different dimensions.\\n\\nTransfer Method.\\n\\nThe results in Table 4 clearly demonstrate...\"}"}
{"id": "bugliarello22a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Zero-shot performance on individual target languages as a function of some of their properties: Wikipedia size (left) and typological similarity to English (right). Pearson's correlation coefficients are also reported in the legends.\\n\\nstrate the superiority of 'translate test' transfer over zero-shot model transfer via multilingual encoders. The difference between the best models for each method reaches 12.1 points for XVNLI, 21.6 for xGQA, 6.9 for MaRVL, 21.3 for xFlickr&CO IR, 19.6 for xFlickr&CO TR, 7.1 for WIT IR, and 6.5 for WIT TR. Thus, gains are especially remarkable in cross-modal retrieval tasks and grounded question answering, where the performance is nearly doubled by virtue of 'translate test' transfer.\\n\\nMultilingual Models. Even within each transfer method, we observe considerable variance among individual multilingual multimodal models. For instance, for zero-shot model transfer via multilingual encoders, UC$_2$ is consistently better across the board (except for WIT) and surpasses the other models by a large margin, especially in German, French, Japanese and Mandarin\u2014languages in which the authors had translated CC and pretrained UC$_2$ on (Appendix C.2). Remarkably, this tendency is less accentuated in MaRVL, where pretraining Mandarin multimodal data is still insufficient to tackle the out-of-distribution nature of MaRVL's culture-specific concepts (Table 15 in Appendix C.2). Nonetheless, these results prove how the simple 'translate pretrain' approach of UC$_2$ can be an effective baseline for multilingual transfer in multimodal pre-trained models. As for 'translate test' transfer, no clear winner emerges across the monolingual models, whereas mUNITER generally performs better among multilingual models limited to cross-modal retrieval tasks. We also note that massively multilingual models (top rows) are often on a par with their monolingual counterparts (bottom rows).\\n\\nTransfer Gap. Comparing the results on the English test set (Table 3) with those averaged across the other languages (Table 4) reveals an extremely large gap in performance due to cross-lingual transfer. Considering the best multilingual encoders for each task, the gap is 14.8 points for XVNLI, 26.5 for xGQA, 14.6 for MaRVL, 24.2/23.0 for xFlickr&CO IR/TR, and 10.7/11.9 for WIT IR/TR.\\n\\nTask Complexity. There emerge stark contrasts among the model performances in each individual task. Nonetheless, we remind the reader that the number of classes to predict\u2014and hence random baselines\u2014vary across tasks: XVNLI 3, xGQA 1,842, and MaRVL 2. Moreover, some tasks include a collection of languages that are more homogeneous (e.g. XVNLI contains mostly Indo-European languages) whereas others are more diverse, such as WIT. Performance is expected to suffer the most in the latter group. Moreover, focusing on cross-modal retrieval tasks, scores are higher in xFlickr&CO than in WIT. This is arguably due to the fact that, while all models were pretrained on an image\u2013text matching task, WIT captions are distributionally distant from standard captions as they mostly describe entities.\\n\\nExplanatory Variables. Finally, we assess the impact of a series of explanatory factors on the performance in each individual target language. More specifically, we estimate its correlation 1 with the amount of each language's unlabelled textual data available for pretraining in terms of Wikipedia articles (as of January 2022) in Figure 2(a); and 2 with the typological similarity between the source (English) and the target language in cross-lingual transfer in Figure 2(b). The latter is calculated as the cosine similarity between vectors of morpho-syntactic features extracted from WALS (Dryer & Haspelmath, 2013) by Littell et al. (2017). According to the received wisdom from text-only cross-lingual transfer (Ponti et al., 2020; Ruder et al., 2021), both these factors usually play a pivotal role in determining the downstream performance of each target language. While we find this to partially hold true for Wikipedia size (with the possible exception of WIT), we find that correlations of typological similarity are weaker (and even negative for MaRVL), implying that models may struggle comparatively on many target languages despite different similarities to English.\\n\\n5.2. Few-shot Learning\\n\\nAs we experimented with a range of data sizes for k-shot fine-tuning in the target language, we plot these values against the corresponding performance in Figure 3 and also report the exact scores for the maximum k in Table 5.\"}"}
{"id": "bugliarello22a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Few-shot performance on IGLUE tasks (cross-lingual average) against different data sizes for target language fine-tuning. Values in MaRVL are only averaged across the 3 languages with available few-shot data. Horizontal lines report English performance.\\n\\nTable 5. Max-shot performance (with the largest $k$) averaged across languages with few-shot training data.\\n\\n| Model     | NLI | QA  | Reasoning | Retrieval |\\n|-----------|-----|------|-----------|-----------|\\n| XVNLI     | 53.95 | 37.21 | 53.41     | 8.54      |\\n| xGQA      | 60.55 | 40.68 | 57.46     | 14.30     |\\n| UC2       | 63.68 | 42.95 | 58.32     | 19.79     |\\n| M3P       | 59.36 | 41.04 | 49.79     | 13.21     |\\n\\nGains from Few-shot Learning. Contrary to text-only multilingual tasks, where even few examples in the target language are sufficient to significantly improve the model performance (Lauscher et al., 2020), we find that this is largely not the case in multimodal settings. Figure 3 reveals that learning curves are flat for XVNLI, MaRVL, and xFlickr&CO. Moreover, inspecting the gap between zero-shot and max-shot performance in Table 19 (Appendix C.3), gains from few-shot learning are consistent but modest. A notable exception is xGQA, where as few as 27 examples increase performance dramatically and accuracy continues to grow with the sample size. This difference is possibly due to the highly-structured nature of xGQA, where texts are generated from templates and are extremely short, and its larger label space. To investigate the latter, we follow Pfeiffer et al. (2022) and evaluate the accuracy for each structurally different question type. Figure 4 shows that increasing the number of examples has the least impact on Verify-type questions, which consist of (binary) answers/labels ($\\\\{\\\\text{Yes, No}\\\\}$). These results are in line with our few-shot experiments on the other tasks, where few-shot tuning displays negligible impact on cross-lingual transfer when the label space is small. The largest impact in xGQA is observed on Choose-type and Query-type questions, where the diversity of the label space is large. This again shows that the multilingual misalignment of the prediction space negatively impacts zero-shot results, which can be mitigated by fine-tuning even on a few examples in the target language. On the other hand, current models may require many more examples than the maximum of $k$-shot we established for the other tasks. We test this hypothesis in a few-shot learning experiment with more examples in JPN and DEU on xFlickr&CO. The results in Figure 5 demonstrate that examples in the order of the thousands may be needed to achieve significant gains. This leaves open an important challenge to make future models more sample efficient, in more resource-poor settings like those considered here.\"}"}
{"id": "bugliarello22a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.4. xFlickr&CO\\n\\nWe follow the Flickr30K guidelines to collect our multilingual annotations, as shown in Figure 7(a), which we supplement with examples from Flickr30K (Figure 7(b)). In particular, we limit captions length based on previous work: 40 characters for CMN, 140 for TUR and 100 for the rest. Figure 7(c) shows the additional guidelines provided to validators. The resulting corpora were finally verified by native speakers known by the authors. Notably, the same guidelines were used to annotate both Flickr30K and COCO images. Figure 7 shows the distribution of caption lengths.\\n\\n(a) Flickr30K-like instructions to annotate xFlickr&CO data.\\n\\n(b) Additional Flickr30K examples used to further guide annotators.\\n\\n(c) Instructions given to native speakers to validate the captions.\\n\\nFigure 7. Annotation guidelines for xFlickr&CO.\\n\\nFigure 8. Test sentence length distribution in xFlickr&CO.\\n\\nFew-shot Data.\\n\\nWe use the same guidelines illustrated in Figure 7 to annotate few-shot image descriptions from 100 images samples from the Flickr30K training set. German captions are extracted from Multi30K (Elliott et al., 2016), Mandarin ones from Flickr30K-cn (Mishra et al., 2021), and Turkish ones from TasvirEt (Unal et al., 2016) \u2013 all independently collected from the English captions.\\n\\nA.5. WIT\\n\\nWe extract 500,000 image captions from the caption reference description field of the English portion of the WIT dataset. This field corresponds to the captions shown on the Wikipedia pages directly below the images, and it tends to be the most topical and relevant description (Srinivasan et al., 2021). In particular, we sample among the images that were released by the Wikimedia Foundation for the corresponding Kaggle challenge, which do not include any identifiable personal information. This challenge also provides test data that was extracted from WIT prior to its public release. We use these data to define our multilingual test splits in order to let future practitioners evaluate models that were also trained on the WIT data. Per-language statistics are listed in Table 7. We note that a given image is mapped to a single caption more than 90% of the times in every language except for English (where this happens around 70% of the times and which is not used as an evaluation language in IGLUE). Figure 9 shows the overlap between test images across languages, and Figure 10 illustrates the distribution of caption lengths across languages in our WIT test splits.\\n\\nFew-shot Data.\\n\\nDue to the demanding computational resources to evaluate image\u2013text retrieval systems, we only provide few-shot splits for this task in xFlickr&CO.\"}"}
{"id": "bugliarello22a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9. Number of images overlapping between any two languages in the WIT test splits.\\n\\nFigure 10. Test sentence length distribution in WIT.\\n\\nTable 7. WIT test data statistics. % unique measures the percentage of images that only have a single caption associated with them.\\n\\nTable 8. Pretrained multilingual V&L encoders.\\n\\nB. Experimental Details\\n\\nPretrained Models. Table 8 summarises the key properties of the available pretrained multilingual V&L encoders. They are all single-stream architectures (Bugliarello et al., 2021), which mostly differ in the corpora they used for pretraining.\\n\\nExperimental Settings. We train\u2014both for zero-shot fine-tuning and for few-shot experiments\u2014each encoder end-to-end by learning a task-specific \u2018head\u2019 network for each dataset on a single NVIDIA V100 (16GB) GPU card. For each task, we use the same hyperparameters as in the controlled study of Bugliarello et al. (2021) except for maximum sequence length, which we increase to accommodate the multilingual inputs (see details per task below). We use the AdamW (Loshchilov & Hutter, 2019) optimiser with parameters $\\\\beta_1=0.9$, $\\\\beta_2=0.999$ and $\\\\epsilon=1e-6$. The base learning rate depends on each task (see below) but it is always first linearly warmed up for 10% of the task-specific number of steps, and then linearly decayed until up to 20 epochs of training have been reached. We also apply a weight decay of $10^{-4}$ and gradient clipping of $1.0$. Before training, we extract, for each dataset, 36 image regions using a ResNet-101 backbone (He et al., 2016) for mUNITER, xUNITER and UC$^2$; and 10\u2013100 image regions using a ResNeXt-101 backbone (Xie et al., 2017) for M$^3$P\u2014both trained on Visual Genome (Krishna et al., 2017; Anderson et al., 2018).\\n\\nFine-Tuning Setup. To increase accessibility to the benchmark by practitioners with limited computing resources, we fix the number of epochs for which we train our baselines so that they can be trained in less than 12 hours per dataset. In particular, we set up each task based on prior work (Lu et al., 2020; Bugliarello et al., 2021) as follows.\\n\\n\u2022 Visual Entailment. We treat XVNLI as a three-way classification problem, and apply a linear layer from the pooled representation of each Transformer encoder. We fine-tune each model using a binary cross-entropy loss with a batch size of 128 for 10 epochs. We use a learning rate of $2e-5$ and max token length of 80.\\n\\n\u2022 Visual QA. As in Hudson & Manning (2019), we treat\"}"}
{"id": "bugliarello22a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages\\n\\nGQA as a multi-label classification task by assigning a score to each answer according to its relevancy to the ground truth answer. We apply a two-layer MLP with a GeLU activation (Hendrycks & Gimpel, 2016) function in between on top of the pooled representation. We fine-tune on the balanced split of GQA using a binary cross-entropy loss against 1,842 labels with a batch size of 256 for 5 epochs. We use a learning rate of 4e-5 and max token length of 40.\\n\\n- **Visual Reasoning.**\\n  We cast this task as a binary classification problem: Given the two pooled representations from encoding the textual description with each of the two images independently, we concatenate and feed them into an MLP (same as for GQA) to predict a True or False label. We fine-tune using a binary cross-entropy loss with a batch size of 64 for 20 epochs. We use a learning rate of 1e-5 and max token length of 80.\\n\\n- **Image\u2013Text Retrieval.**\\n  We train a four-way linear classifier against the pooled representations from the true image-caption pair and three hard negatives determined offline. We optimise our models using a cross-entropy loss, except for UC\\\\(^2\\\\) for which we found the authors' triplet loss to work better (see Appendix C). We train with a batch size of 64 for 10 epochs for xFlickr&CO and for 2 epochs on the larger WIT. We use a learning rate of 2e-5 and max token length of 80.\\n\\n**Few-shot Setup.**\\nWe follow the same setup as in the above described fine-tuning procedure, with the exception of searching across three values for the base learning rate \\\\(\\\\{1e^{-5}, 5e^{-5}, 1e^{-4}\\\\}\\\\) and always training for 20 epochs. The batch size is adjusted according to the number of data points available for a given split (Table 9), and models are initialised from the English fine-tuned parameter set. For each run, we select the checkpoint that achieves the largest validation performance for evaluation on the test sets.\\n\\nWhile the validation sets are in English for every dataset except for xGQA, we found no significant difference in using machine translations of each validation set for checkpoint selection. Therefore, we decided to report results using the original validation sets as this more closely matches the few-shot scenario. Nevertheless, we release the corresponding translations for each dataset for future extensions.\\n\\n**C. Reproducibility and Additional Results**\\nWe provide more details regarding our experimental setup in order to ensure reproducibility (\u00a7C.1). Further, we report additional per-language (\u00a7C.2) and few-shot (\u00a7C.3) results, which supplement the ones provided in the main paper.\\n\\n**C.1. A Unified Implementation Framework**\\nIn order to provide a more lightweight setup for all the experiments in IGLUE, which concerns the experiments we conducted for this work as well as any future experiments by other researchers, we reimplemented M\\\\(^3\\\\)P and UC\\\\(^2\\\\) in a unified framework based on VOLTA (Bugliarello et al., 2021), which already implements mUNITER and xUNITER, along with five English pretrained encoders. In order to verify the correctness of the implementation, we evaluated each model on Multi30K (Elliott et al., 2016) in a zero-shot scenario. The results shown in Table 10 prove the correctness of our reimplementation: Performance of both models is within a couple of points from the one reported by the original authors. In particular, we obtain similar results as Geigle et al. (2022) for M\\\\(^3\\\\)P. Moreover, we found that using a cross-entropy loss for UC\\\\(^2\\\\) led to a drop in performance of approximately 2.5 mean Recall points in each language. Similarly, using a triplet loss for M\\\\(^3\\\\)P led to a drop in performance of nearly 3 points.\\n\\nIn Table 11, we report the performance of our models on...\"}"}
{"id": "bugliarello22a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 12. Zero-shot per-language results on WIT (IR and TR).\\n\\n| Language | Model | WIT | IR | TR |\\n|----------|-------|-----|----|----|\\n| ARB      | mUNITER | 7.74 | 8.26 | 10.66 |\\n|          | xUNITER | 7.63 | 8.49 | 10.32 |\\n| BUL      | mUNITER | 8.84 | 10.88 | 11.23 |\\n|          | xUNITER | 8.21 | 10.21 | 7.30  |\\n| DAN      | mUNITER | 9.43 | 7.67 | 6.41 |\\n|          | xUNITER | 9.72 | 6.34 | 9.57 |\\n| ELL      | mUNITER | 8.77 | 7.00 | 5.91 |\\n|          | xUNITER | 9.00 | 6.97 | 9.72 |\\n| EST      | mUNITER | 10.88 | 9.88 | 8.33 |\\n|          | xUNITER | 12.88 | 10.66 | 7.82 |\\n| IND      | mUNITER | 8.95 | 8.79 | 7.82 |\\n|          | xUNITER | 10.17 | 10.10 | 7.30 |\\n| JPN      | mUNITER | 9.57 | 9.80 | 8.32 |\\n|          | xUNITER | 13.00 | 11.74 | 9.69 |\\n| KOR      | mUNITER | 10.88 | 12.88 | 8.32 |\\n|          | xUNITER | 15.07 | 11.74 | 9.69 |\\n| TUR      | mUNITER | 5.91 | 6.75 | 6.34 |\\n|          | xUNITER | 9.57 | 6.97 | 9.72 |\\n| VIE      | mUNITER | 9.57 | 9.72 | 9.72 |\\n|          | xUNITER | 13.00 | 11.74 | 9.69 |\\n\\n### Table 13. Full per-language results on XVNLI.\\n\\n| Language | Model | # shots | 0 | 1 | 5 | 10 | 20 | 25 | 48 |\\n|----------|-------|---------|---|---|---|----|----|----|----|\\n| ARB      | mUNITER | 46.73 | 46.99 | 46.39 | 49.40 | 47.16 | 48.97 | 46.91 |\\n|          | xUNITER | 51.98 | 52.32 | 52.32 | 54.81 | 54.55 | 53.95 | 54.04 |\\n|          | UC      | 56.19 | 56.36 | 57.65 | 57.82 | 59.11 | 58.51 | 56.87 |\\n|          | M3P     | 55.24 | 54.98 | 55.07 | 56.19 | 55.24 | 56.79 | 56.01 |\\n| SPA      | mUNITER | 56.96 | 56.79 | 57.47 | 57.30 | 57.73 | 57.65 | 57.73 |\\n|          | xUNITER | 58.94 | 59.02 | 58.94 | 60.74 | 59.88 | 59.11 | 60.22 |\\n|          | UC      | 57.47 | 57.65 | 61.34 | 59.79 | 62.63 | 59.79 | 62.80 |\\n|          | M3P     | 58.85 | 56.36 | 57.99 | 58.33 | 57.82 | 57.65 | 58.59 |\\n| FRA      | mUNITER | 59.36 | 59.45 | 59.19 | 59.54 | 59.36 | 59.62 | 59.36 |\\n|          | xUNITER | 63.32 | 63.49 | 64.26 | 63.83 | 64.26 | 64.09 | 64.52 |\\n|          | UC      | 69.67 | 69.67 | 69.67 | 69.76 | 69.84 | 69.76 | 69.76 |\\n|          | M3P     | 56.36 | 56.62 | 57.99 | 58.33 | 57.82 | 57.65 | 58.59 |\\n| RUS      | mUNITER | 51.72 | 51.72 | 51.46 | 50.52 | 52.32 | 51.46 | 51.80 |\\n|          | xUNITER | 59.71 | 59.54 | 59.54 | 59.54 | 61.94 | 61.34 | 63.40 |\\n|          | UC      | 64.86 | 64.43 | 64.52 | 64.95 | 65.38 | 65.38 | 65.29 |\\n|          | M3P     | 62.54 | 62.46 | 62.72 | 63.23 | 62.46 | 6.29 | 6.24 |\\n\\n### Table 14. Full per-language results on xGQA.\\n\\n| Language | Model | # shots | 0 | 1 | 5 | 10 | 20 | 25 | 48 |\\n|----------|-------|---------|---|---|---|----|----|----|----|\\n| BEN      | mUNITER | 3.06 | 19.36 | 23.94 | 27.53 | 30.04 | 31.04 | 34.89 |\\n|          | xUNITER | 10.80 | 23.92 | 29.43 | 31.67 | 34.84 | 36.18 | 37.55 |\\n|          | UC      | 19.99 | 22.52 | 30.96 | 32.84 | 35.69 | 35.12 | 38.90 |\\n|          | M3P     | 18.64 | 23.42 | 31.07 | 33.37 | 35.74 | 35.94 | 37.76 |\\n| DEU      | mUNITER | 23.95 | 29.43 | 33.88 | 35.40 | 37.82 | 37.43 | 40.29 |\\n|          | xUNITER | 34.83 | 38.44 | 39.71 | 40.97 | 41.93 | 42.19 | 43.60 |\\n|          | UC      | 42.85 | 43.76 | 44.68 | 45.72 | 46.70 | 47.24 | 48.18 |\\n|          | M3P     | 33.42 | 34.37 | 39.66 | 40.73 | 41.78 | 41.93 | 43.19 |\\n| IND      | mUNITER | 9.36 | 27.31 | 31.39 | 33.28 | 36.49 | 35.85 | 38.06 |\\n|          | xUNITER | 33.73 | 35.28 | 37.38 | 37.96 | 39.48 | 39.40 | 40.90 |\\n|          | UC      | 28.67 | 34.76 | 38.95 | 40.11 | 41.26 | 41.51 | 43.11 |\\n|          | M3P     | 32.48 | 33.11 | 38.14 | 39.84 | 40.51 | 41.53 | 41.38 |\\n| KOR      | mUNITER | 4.21 | 19.40 | 26.47 | 29.03 | 31.95 | 32.76 | 35.23 |\\n|          | xUNITER | 12.12 | 23.45 | 31.49 | 34.70 | 36.66 | 37.26 | 39.32 |\\n|          | UC      | 21.36 | 29.33 | 33.02 | 34.50 | 36.09 | 38.40 | 39.63 |\\n|          | M3P     | 25.11 | 29.74 | 34.53 | 35.77 | 37.11 | 37.81 | 38.58 |\\n| POR      | mUNITER | 13.67 | 22.88 | 31.09 | 34.38 | 37.05 | 37.32 | 39.41 |\\n|          | xUNITER | 22.13 | 30.10 | 36.44 | 38.72 | 39.73 | 41.06 | 42.56 |\\n|          | UC      | 30.42 | 32.10 | 39.42 | 39.57 | 41.73 | 41.27 | 43.23 |\\n|          | M3P     | 31.40 | 33.37 | 37.62 | 39.47 | 41.09 | 41.96 | 43.01 |\\n| CMN      | mUNITER | 7.03 | 18.33 | 30.90 | 32.25 | 36.18 | 35.52 | 37.81 |\\n|          | xUNITER | 19.55 | 31.36 | 37.35 | 37.34 | 39.19 | 39.48 | 41.42 |\\n|          | UC      | 31.16 | 37.54 | 41.21 | 41.49 | 43.46 | 43.58 | 44.82 |\\n|          | M3P     | 28.65 | 30.67 | 36.77 | 37.77 | 39.25 | 39.99 | 41.19 |\\n\\n### Table 15. Full per-language results on MaRVL.\\n\\n| Language | Model | # shots | 0 | 1 | 2 | 4 | 10 | 10 |\\n|----------|-------|---------|---|---|---|---|----|----|\\n| IND      | mUNITER | 54.79 | 51.42 | 52.13 | 54.34 | 56.83 | 51.42 |\\n|          | xUNITER | 55.14 | 56.12 | 57.18 | 58.87 | 58.87 | 58.60 |\\n|          | UC      | 56.74 | 56.12 | 56.29 | 57.53 | 57.62 | 58.51 |\\n|          | M3P     | 56.47 | 49.02 | 48.05 | 50.18 | 50.09 | 50.09 |\\n| SWA      | mUNITER | 51.17 | - | - | - | - | - | - |\\n|          | xUNITER | 55.51 | - | - | - | - | - | - |\\n|          | UC      | 52.62 | - | - | - | - | - | - |\\n|          | M3P     | 55.69 | - | - | - | - | - | - |\\n| TAM      | mUNITER | 52.66 | - | - | - | - | - | - |\\n|          | xUNITER | 53.06 | - | - | - | - | - | - |\\n|          | UC      | 60.47 | - | - | - | - | - | - |\\n|          | M3P     | 56.04 | - | - | - | - | - | - |\\n| TUR      | mUNITER | 54.66 | 52.54 | 55.93 | 55.59 | 54.66 | 54.75 |\\n|          | xUNITER | 56.19 | 55.93 | 57.46 | 57.54 | 57.80 | 58.05 |\\n|          | UC      | 56.70 | 55.76 | 54.49 | 52.63 | 55.93 | 56.27 |\\n|          | M3P     | 56.78 | 51.19 | 49.32 | 49.58 | 49.41 | 49.58 |\\n| CMN      | mUNITER | 55.34 | 56.52 | 54.84 | 54.55 | 55.34 | 54.05 |\\n|          | xUNITER | 53.06 | 54.45 | 53.66 | 54.94 | 53.46 | 55.73 |\\n|          | UC      | 59.88 | 58.99 | 57.02 | 58.99 | 57.21 | 60.18 |\\n|          | M3P     | 55.04 | 50.69 | 50.40 | 50.20 | 49.80 | 49.70 |\"}"}
{"id": "bugliarello22a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reliable evaluation benchmarks designed for replicability and comprehensiveness have driven progress in machine learning. Due to the lack of a multilingual benchmark, however, vision-and-language research has mostly focused on English language tasks. To fill this gap, we introduce the Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings together\u2014by both aggregating pre-existing datasets and creating new ones\u2014visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. Our benchmark enables the evaluation of multilingual multimodal models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups. Based on the evaluation of the available state-of-the-art models, we find that translate-test transfer is superior to zero-shot transfer and that few-shot learning is hard to harness for many tasks. Moreover, downstream performance is partially explained by the amount of available unlabelled textual data for pretraining, and only weakly by the typological distance of target\u2013source languages. We hope to encourage future research efforts in this area by releasing the benchmark to the community.\\n\\n1. Introduction\\nUntil recently, advances in multimodal vision-and-language (V&L) modelling have predominantly focused on English or a few high-resource Indo-European languages (Elliott et al., 2016), disregarding the enormous diversity of the world\u2019s languages and cultures (Bender, 2011; Ponti et al., 2019; Liu et al., 2021), which exacerbated the Anglo-centric bias (Jauhar et al., 2018; Ponti et al., 2020) of V&L models. Nonetheless, this trend is reversing by virtue of a series of independent efforts in the V&L research community: on top of expanding multilingual image\u2013sentence retrieval to lower-resource languages (Srinivasan et al., 2021) and additional modalities (Armitage et al., 2020), we have recently witnessed pioneering work on multimodal machine translation (Yao & Wan, 2020; Huang et al., 2020), multilingual visual question answering (Pfeiffer et al., 2022), multilingual text-to-video search (Huang et al., 2021; Lei et al., 2021), multilingual visual reasoning (Liu et al., 2021), and cross-modal retrieval (Jain et al., 2021). Yet, multilingual multimodal models (Ni et al., 2021; Jain et al., 2021) are typically tested only on the task of cross-modal retrieval. In order to incentivise and guide future research in multilingual V&L research, we introduce the Image-Grounded Language Understanding Evaluation (IGLUE) benchmark.\\n\\nWe create IGLUE by collating current research threads in this area and extending them with two datasets for cross-lingual visual entailment (XVNLI) and image\u2013text retrieval (xFlickr&CO), for an even more comprehensive coverage of tasks. IGLUE is the first evaluation suite for multilingual multitask V&L modelling, comprising five datasets across four structurally different tasks that require different levels of syntactic-semantic V&L understanding in cross-lingual setups: cross-modal retrieval, visual question answering, natural language inference, and visual reasoning (Figure 1). Aiming at a representative selection of languages, IGLUE covers 20 typologically diverse languages, spanning 11 language families, 9 scripts, and 3 WALS-defined (Dryer & Haspelmath, 2013) geographical macro-areas. Similar to previous text-based, cross-lingual transfer benchmarks, such as XGLUE (Liang et al., 2020) and XTREME (Hu et al., 2020), our V&L benchmark is designed to evaluate zero-shot transfer scenarios, where annotated training data is provided in English, but none in the target language. In addition, contrary to the above-mentioned datasets, IGLUE also provides standardised data splits to guide cross-lingual few-shot learning experiments, which\"}"}
{"id": "bugliarello22a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Overview of the tasks in IGLUE, which include grounded natural language inference, visual question answering, grounded reasoning, and cross-modal retrieval. Each task is associated with an example of input and output (English translations at the bottom).\\n\\nCan help reduce the gap between zero-shot and supervised performance (Lauscher et al., 2020; Zhao et al., 2021). In this setup, a small number of task-annotated examples for fine-tuning are available for a given target language. We also release machine-translated versions of the test sets to enable the evaluation of 'translate test' cross-lingual transfer.\\n\\nBy virtue of the newly created IGLUE benchmark, we also run the first systematic comparative evaluation of cutting-edge multilingually pretrained V&L architectures (Ni et al., 2021; Zhou et al., 2021; Liu et al., 2021), as well as a series of representative monolingual V&L models combined with machine translation (Lu et al., 2019; Chen et al., 2020, inter alia) across a range of diverse V&L tasks and languages.\\n\\nThis evaluation offers new insights on the models' core strengths and current limitations. Foreshadowing, we showcase large gaps between performance in English and transfer performance, where (to a limited extent) the gaps are more prominent in lower-resource languages and languages more distant from English. In addition, leveraging target-language in-task few shots is remarkably arduous: whereas in previous text-only experiments (Lauscher et al., 2020), ceteris paribus there are huge gains of few-shot scenarios over their zero-shot counterparts, we demonstrate that current V&L models often require thousands of examples before showing signs of improvement. Finally, performance also seems correlated with task difficulty: for instance, NLI shows the smallest gaps between English and other languages and QA benefits the most from few-shot adaptation. On the other hand, visually grounded reasoning and cross-modal retrieval appear to be harder in both respects.\\n\\nContributions. 1) In order to guide and inspire more work in the area of multilingual V&L research, we present a first evaluation benchmark for cross-lingual transfer learning for V&L tasks, spanning 20 languages, 5 datasets, and 4 different tasks.\\n\\n2) In the process of benchmark creation, we complement existing datasets with new training and evaluation data in several languages to increase diversity and enable few-shot learning, and introduce a first multilingual dataset for visually grounded cross-lingual NLI.\\n\\n3) We conduct systematic evaluations of representative V&L architectures in zero-shot and few-shot cross-lingual transfer scenarios, offering standard data splits and empirical baselines for future research.\\n\\n4) Our results and additional analyses take stock of the current gaps and challenges in cross-lingual V&L research.\\n\\n5) To further facilitate replicable research in this area, we re-implement the existing multilingual V&L pretrained encoders in a unified framework (VOLTA; Bugliarello et al. 2021), which also provides access to five English V&L BERTs and 12 V&L tasks. We provide data and code for the evaluation of multilingual V&L models at https://iglue-benchmark.github.io/.\\n\\n2. Related Work and Motivation\\n\\nMultilingual Multimodal Learning. Multilingual V&L research focuses on collecting resources, developing models, and evaluating systems that need to jointly reason over multilingual text and multimodal inputs, this way combining two areas of crucial importance: multilingual (Snyder & Barzilay, 2010; Ponti et al., 2019) and multimodal learning (Bernardi et al., 2016; Baltrusaitis et al., 2019). A natural overlap of the two areas is language grounding in perception (typically vision; Deng et al., 2009; Kiela et al., 2018), where the perceptual input can be considered as an inherent cross-lingual signal (Kiela et al., 2015; Gella et al., 2017; Caglayan et al., 2021). Many other research goals lie at the intersection of these two areas, such as transfer learning and modularity (Ponti et al., 2021b; Ansell et al., 2022).\\n\\nHowever, until recently the pace of progress in multilingual multimodal learning has not gained as much momentum as V&L in monolingual settings, mostly due to the scarcity of resources for training and evaluation. Texts in most multimodal datasets are usually only available in English, or in high-resource languages (e.g., Chinese and a few major Indo-European languages; Elliott et al., 2016; 2017; Barrault et al., 2018; Wang et al., 2019b). Consequently, models trained on such datasets do not take into account linguistic diversity (Ponti et al., 2020) or cross-cultural nuances (Armitage et al., 2020; Liu et al., 2021; Yin et al., 2021).\\n\\nThe need to expand V&L research towards more languages 1\\n\\nSome notable exceptions, which did reach beyond English, were still limited to the image\u2013text retrieval task and mostly Indo-European languages (Rotman et al., 2018; Wehrmann et al., 2019; Huang et al., 2019; Kim et al., 2020; Su et al., 2021).\"}"}
{"id": "bugliarello22a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages has been recognised by the recent creation of multilingual training and evaluation data across diverse V&L tasks and languages (Armitage et al., 2020; Srinivasan et al., 2021; Su et al., 2021; Pfeiffer et al., 2022; Liu et al., 2021; Wang et al., 2021, inter alia), as well as the emergence of the first large multilingual-multimodal pretrained models (Ni et al., 2021; Zhou et al., 2021; Liu et al., 2021) and monolingual V&L models adapted to multiple languages (Chen et al., 2020; Pfeiffer et al., 2022). In this work, we merge and expand on these two research threads, aiming to highlight current achievements and challenges in this area and to facilitate comparative evaluations, thus bringing together the above-mentioned collective research efforts.\\n\\nMulti-Task Evaluation Benchmarks in NLP. IGLUE has been inspired by recent text-only multi-task benchmarks for natural language understanding: these benchmarks have been proven invaluable as key drivers of recent steep performance progress of NLP. The creation of such benchmarks in monolingual settings, sparked by the pioneering and now omnipresent English-only GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019a), has been extended to other languages such as Indonesian (Wilie et al., 2020), Korean (Park et al., 2021), Russian (Shavrina et al., 2020), and Romanian (Dumitrescu et al., 2021).\\n\\nIGLUE is even more related to multi-task benchmarks developed for cross-lingual transfer settings: XTREME (Hu et al., 2020), XGLUE (Liang et al., 2020), and XTREME-R (Ruder et al., 2021). They have brought in the spotlight the necessity to evaluate not only on a diverse set of tasks, but also on a diverse set of languages, in order to incentivise research on models that forgo monolingual limitations and generalise well in multilingual settings.\\n\\nWith IGLUE, we make the first leap into multimodal cross-lingual evaluation. Driven by analyses of text-only benchmarks and lessons learned from them (Ethayarajh & Jurafsky, 2020), IGLUE aims to extend sheer accuracy-driven evaluation towards other crucial aspects such as fine-tuning efficiency, sample efficiency and adaptation to low-data scenarios, and enhance multilingual inclusivity and diversity.\\n\\n3. IGLUE Benchmark Design Principles. Aiming for a comprehensive resource, we take inspiration from the best practices and design principles of existing multilingual benchmarks (Hu et al., 2020; Liang et al., 2020; Ruder et al., 2021), and adapt them to account for the unique challenges of V&L tasks. In particular, we abide by the following principles:\\n\\n(1) Task Diversity. We selected a wide spectrum of multi-modal tasks that reflect multiple facets of V&L learning.\\n\\n(2) Language Diversity. The datasets included in IGLUE should cover many languages that are diverse in terms of family, geographic area, typological features, and script.\\n\\n(3) Accessibility. The data should be released under a licence that permits use and redistribution for research purposes. Compared to previous text-only benchmarks, V&L tasks are more time- and compute-intensive by nature. To widen usability, we created IGLUE by carefully choosing datasets and training setups that would enable quick development even by practitioners constrained by limited resources.\\n\\n3.1. Tasks and Datasets. Challenges in V&L learning are multi-faceted and entangled with distinct abilities: e.g., drawing inferences, reasoning over and comparing images, answering questions, and retrieving images or their captions. In IGLUE we thus represent all these abilities together with their corresponding tasks, see Table 1 and Figure 1 for an overview. We opt for the following V&L tasks based on (i) (partial) availability of multilingual data, and (ii) computational requirements.\\n\\n**XVNLI.** We propose the new task of Cross-lingual Visual Natural Language Inference (XVNLI). It requires the model to predict if a text-hypothesis \u2018entails\u2019, \u2018contradicts\u2019, or is \u2018neutral\u2019 to an image-premise. We combine the text-only dataset SNLI (Bowman et al., 2015), with its multimodal (Xie et al., 2019) and cross-lingual (Agi\u00b4c & Schluter, 2018) counterparts. We provide new train, development, and test splits such that the test split consists of images covered by the underlying cross-lingual text-only dataset. We discard all text examples of images in the test set for which no cross-lingual version exists. The remaining images for which only English hypothesis-examples exist are randomly split into the train and development set. To mitigate data leakage between the splits, we sample based on the images; all text examples are discarded.\"}"}
{"id": "bugliarello22a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages\\n\\nTo solve the Cross-lingual Grounded Question Answering task (xGQA; Pfeiffer et al. 2022), a model must answer several types of structured questions about an image. The evaluation data in 7 target languages are manually translated from the validation set of GQA (Hudson & Manning, 2019), whereas the training data are sourced from the English training set of GQA. All images were originally sampled from Visual Genome (Krishna et al., 2017). In particular, we use the English balanced training set to train our models, and evaluate on the few-shot evaluation sets defined by Pfeiffer et al. (2022) to allow for direct comparison between zero-shot and few-shot experiments.\\n\\nMaRVL. The Multicultural Reasoning over Vision and Language dataset (Liu et al., 2021) requires to determine whether a textual description is true or false about a pair of images. This involves comparing their visual representations and reasoning about the facts in the description. The creation of MaRVL is entirely driven by native speakers. As a consequence, the descriptions are written from scratch and images are selected to be culturally relevant. The NLVR2 data (Suhr et al., 2019) in English are used for training.\\n\\nxFlickr&CO. We create a new, multilingual evaluation set for retrieval by combining 1,000 images from the Flickr30K (Young et al., 2014) and 1,000 from the COCO (Lin et al., 2014) test split defined by Karpathy & Fei-Fei (2015) (henceforth, Karpathy split). Each image is then associated with a single caption. The English evaluation set is obtained by sampling one caption per image from the existing datasets. With the exception of Japanese (Yoshikawa et al., 2017; Nakayama et al., 2020), no other language covers the evaluation splits of both Flickr and COCO. We hence crowdsource image descriptions in 6 other languages.\\n\\nWIT. The Wikipedia-based Image Text dataset (Srinivasan et al., 2021) collected examples from Wikipedia in 108 languages. Similar to xFlickr&CO, the tasks consist in retrieving the correct image given a textual description (image retrieval) and vice versa (text retrieval). WIT represents a very diverse set of concepts and real world entities. Text fields in WIT often tend to be descriptive, verbose and use specific terminology, different from single-line captions of common words and objects in Flickr30K and COCO. This allows us to evaluate the high-level image\u2013sentence understanding of multilingual V&L models across a wider range of languages and real-world entities than in xFlickr&CO.\\n\\nFor training, we randomly sample a subset of 500K captions from the English training set of WIT. For evaluation, we use the WIT test data released as part of its corresponding Kaggle competition.\\n\\nIn particular, as most of the test languages are Indo-European, we choose the 4 national languages with the lowest Wikipedia coverage (measured in number of articles as of Jan 2022), and 6 more languages to both cover different properties (language families, scripts) and overlap with other tasks in IGLUE.\\n\\nWe ensured that IGLUE includes languages with different levels of unlabelled data available (Joshi et al., 2020; Blasi et al., 2022). Thus, it allows for evaluating models with different data paucity regimes during pretraining. While not all languages are covered in each task\u2014due to the (partly) independent selection of languages in the original datasets\u2014\\n\\n5. www.kaggle.com/c/wikipedia-image-caption\\n\\n6. While Jain et al. (2021) define a set of 8 low-resource languages for evaluation on WIT, they were sampled from the training set. Moreover, we find that several of their test images contain pictures with identifiable people, which would complicate its public release due to regulations such as the GDPR in Europe. By only relying on the true test split, we allow researchers to easily adopt the full WIT training dataset in their experimental pipelines.\"}"}
{"id": "bugliarello22a", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 16. Full per-language results on xFlickr&CO IR.\\n\\n| # shots | DEU | SPA | IND | JPN | RUS | TUR | CMN |\\n|---------|-----|-----|-----|-----|-----|-----|-----|\\n| 0       | mUNITER 12.05 | mUNITER 13.15 | mUNITER 5.95 | mUNITER 6.30 | mUNITER 5.85 | mUNITER 1.75 | mUNITER 11.35 |\\n|         | xUNITER 14.55 | xUNITER 16.10 | xUNITER 16.50 | xUNITER 10.25 | xUNITER 15.90 | xUNITER 9.05 | xUNITER 15.95 |\\n| 1       | mUNITER 10.50 | mUNITER 13.70 | mUNITER 6.05 | mUNITER 6.20 | mUNITER 3.90 | mUNITER 1.90 | mUNITER 11.50 |\\n|         | xUNITER 14.40 | mUNITER 14.25 | mUNITER 16.90 | mUNITER 11.50 | mUNITER 7.60 | mUNITER 10.35 | mUNITER 15.70 |\\n| 5       | mUNITER 11.90 | mUNITER 14.25 | mUNITER 4.55 | mUNITER 6.20 | mUNITER 7.50 | mUNITER 2.45 | mUNITER 12.40 |\\n|         | xUNITER 14.45 | mUNITER 14.25 | mUNITER 16.55 | mUNITER 11.50 | mUNITER 8.50 | mUNITER 10.35 | mUNITER 15.95 |\\n| 10      | mUNITER 11.95 | mUNITER 14.25 | mUNITER 5.65 | mUNITER 6.20 | mUNITER 7.25 | mUNITER 1.95 | mUNITER 12.40 |\\n|         | xUNITER 14.45 | mUNITER 14.25 | mUNITER 16.55 | mUNITER 11.50 | mUNITER 8.50 | mUNITER 10.35 | mUNITER 15.95 |\\n| 25      | mUNITER 11.95 | mUNITER 14.25 | mUNITER 5.65 | mUNITER 6.20 | mUNITER 7.25 | mUNITER 1.95 | mUNITER 12.40 |\\n|         | xUNITER 14.45 | mUNITER 14.25 | mUNITER 16.55 | mUNITER 11.50 | mUNITER 8.50 | mUNITER 10.35 | mUNITER 15.95 |\\n| 50      | mUNITER 11.95 | mUNITER 14.25 | mUNITER 5.65 | mUNITER 6.20 | mUNITER 7.25 | mUNITER 1.95 | mUNITER 12.40 |\\n|         | xUNITER 14.45 | mUNITER 14.25 | mUNITER 16.55 | mUNITER 11.50 | mUNITER 8.50 | mUNITER 10.35 | mUNITER 15.95 |\\n| 100     | mUNITER 12.05 | mUNITER 14.25 | mUNITER 5.65 | mUNITER 6.20 | mUNITER 7.25 | mUNITER 1.95 | mUNITER 12.40 |\\n|         | xUNITER 14.55 | mUNITER 14.25 | mUNITER 16.55 | mUNITER 11.50 | mUNITER 8.50 | mUNITER 10.35 | mUNITER 15.95 |\\n\\nTable 17. Full per-language results on xFlickr&CO TR.\\n\\n| # shots | DEU | SPA | IND | JPN | RUS | TUR | CMN |\\n|---------|-----|-----|-----|-----|-----|-----|-----|\\n| 0       | mUNITER 11.85 | mUNITER 13.05 | mUNITER 7.55 | mUNITER 7.70 | mUNITER 6.80 | mUNITER 3.25 | mUNITER 11.85 |\\n|         | xUNITER 13.25 | xUNITER 15.10 | xUNITER 16.75 | xUNITER 9.85 | xUNITER 14.80 | xUNITER 10.05 | xUNITER 15.95 |\\n| 1       | mUNITER 10.65 | mUNITER 14.70 | mUNITER 8.30 | mUNITER 8.10 | mUNITER 7.60 | mUNITER 3.80 | mUNITER 12.40 |\\n|         | xUNITER 12.95 | xUNITER 15.60 | xUNITER 17.05 | xUNITER 11.80 | xUNITER 14.65 | xUNITER 10.30 | xUNITER 15.70 |\\n| 5       | mUNITER 11.70 | mUNITER 14.05 | mUNITER 4.85 | mUNITER 7.55 | mUNITER 7.50 | mUNITER 2.45 | mUNITER 12.40 |\\n|         | xUNITER 13.15 | xUNITER 15.30 | xUNITER 16.85 | xUNITER 11.45 | xUNITER 15.10 | xUNITER 9.95 | xUNITER 15.95 |\\n| 10      | mUNITER 11.70 | mUNITER 14.05 | mUNITER 4.85 | mUNITER 7.55 | mUNITER 7.50 | mUNITER 2.45 | mUNITER 12.40 |\\n|         | xUNITER 13.15 | xUNITER 15.30 | xUNITER 16.85 | xUNITER 11.45 | xUNITER 15.10 | xUNITER 9.95 | xUNITER 15.95 |\\n| 25      | mUNITER 11.65 | mUNITER 14.05 | mUNITER 4.85 | mUNITER 7.55 | mUNITER 7.50 | mUNITER 2.45 | mUNITER 12.40 |\\n|         | xUNITER 13.15 | xUNITER 15.30 | xUNITER 16.85 | xUNITER 11.45 | xUNITER 15.10 | xUNITER 9.95 | xUNITER 15.95 |\\n| 50      | mUNITER 11.65 | mUNITER 14.05 | mUNITER 4.85 | mUNITER 7.55 | mUNITER 7.50 | mUNITER 2.45 | mUNITER 12.40 |\\n|         | xUNITER 13.15 | xUNITER 15.30 | xUNITER 16.85 | xUNITER 11.45 | xUNITER 15.10 | xUNITER 9.95 | xUNITER 15.95 |\\n| 100     | mUNITER 11.65 | mUNITER 14.05 | mUNITER 4.85 | mUNITER 7.55 | mUNITER 7.50 | mUNITER 2.45 | mUNITER 12.40 |\\n|         | xUNITER 13.15 | xUNITER 15.30 | xUNITER 16.85 | xUNITER 11.45 | xUNITER 15.10 | xUNITER 9.95 | xUNITER 15.95 |\"}"}
{"id": "bugliarello22a", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages\\n\\nTable 18.\\n\\n| Model   | NLI     | QA      | Reasoning | Retrieval |\\n|---------|---------|---------|-----------|-----------|\\n| XVNLI   | 54.10   | 33.67   | 54.68     | 8.36      |\\n| xGQA    | 59.78   | 38.10   | 56.82     | 14.05     |\\n| MaRVL   | 63.43   | 40.43   | 57.11     | 19.24     |\\n| xFlickr&CO | 59.07 | 38.89   | 49.96     | 12.90     |\\n\\nTable 19.\\n\\nGap between Zero-shot and Few-shot Learning.\\n\\nIn contrast to previous results on text-only tasks, we find few-shot learning for multilingual multimodal tasks to be especially challenging, possibly due to the complex nature of the respective tasks combined with the sparsity of the few-shot learning data. As plotted in Figure 3, we observe no clear improvement as the number of shots increase in all tasks, the only exception being xGQA. This suggests that cost-efficiently hand-labelling \\\\( \\\\sim 100 \\\\) shots in order to improve the models performance on the target language, unlike in text-only setups, is insufficient to make meaningful progress on V&L tasks such as XVNLI, MaRVL and xFlickr&Co. This also stresses the importance of developing methods for improved zero-shot transfer in future work, as V&L models still require sufficient amounts of expensive task-annotated data to benefit from in 'non-zero-shot' setups (Table 19).\\n\\nIn contrast to XVNLI, MaRVL and xFlickr&Co, we find that few-shot experiments on xGQA yield large performance gains in the target language. We attribute this to the structured nature of the data and the large label space of the task (1,842 classes) resulting in a misaligned multilingual embedding space in zero-shot setups, as suggested by the original authors. We analyse these results further in \u00a7D.\\n\\nIncreasing the Number of Shots.\\n\\nIn the few-shot experiments, we noticed no observable improvement or clear trend of performance increase on 3 out of the 4 tasks. In order to identify if, and at what point, increasing the number of shots would start being beneficial, we extend our few-shot experiments on the retrieval task using the xFlickr&CO dataset. In Figure 5, we plotted DEU and JPN performance with up to 1,500 shots (instead of the 100 shots used in our standard few-shot setup for this dataset) for the two best performing models in this task. Indeed, we observe an overall trend of increasing performance with more training examples, although with noticeable instability issues (e.g. UC\\\\( _2 \\\\) accuracy in JPN drops when more than 500 shots are used). These results demonstrate that more training examples may indeed further improve the overall performance in the target language, but the large number of examples required questions whether this setup can even be dubbed 'few-shot'.\\n\\nMore Images versus More Captions.\\n\\nAn important question, which concerns data collection efficiency for the target languages in image\u2013text retrieval scenarios, is whether it is more useful, for a given budget, to collect (1) more captions per image or (2) more images with a single caption. To investigate this question, we trained xUNITER and UC\\\\( _2 \\\\) on JPN and DEU data with different number of shots and different ratios of captions per image (Table 20). As indicated by the results, there is no clear trend suggesting that one setting is better than the other. We leave a larger-scale investigation related to this question for future work.\\n\\nD. Further Analyses\\n\\nWe now provide a series of further and finer-grained analyses, which include performance of the 'translate train' transfer approach and domain-specific results in xFlickr&CO, as well as an analysis of performance over structurally different question types of xGQA in few-shot setups.\"}"}
{"id": "bugliarello22a", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12. Image retrieval performance averaged across languages per xFlickr&CO subset.\\n\\nFigure 13. UC2 image retrieval performance across languages per xFlickr&CO subset.\\n\\nTable 21. English image retrieval performance by evaluation subset in xFlickr&CO after fine-tuning.\\n\\n| xFlickr&CO subset | mUNITER | xUNITER | UC2 |\\n|-------------------|---------|---------|-----|\\n| Flickr            | 51.3    | 46.6    | 44.0|\\n| COCO              | 37.7    | 30.3    | 30.8|\\n\\nTranslate Train Performance. We further compare the few-shot results with 'translate train' performance: We directly fine-tune the pretrained UC2 and xUNITER checkpoints on the full, human-translated Flickr30K training set. As shown in Figure 5, UC2 achieves remarkable zero-shot and few-shot performance: 100 or fewer data points are enough for it to match 'translate train' performance, which it surpasses with 1,000 or more data points. Similar results can also be observed in Japanese, although it needs 500 data points to match 'translate train' accuracy. xUNITER\u2014which was not pretrained on German nor Japanese multimodal data\u2014reaps benefits from 500 or more data points, although still being far from its 'translate train' counterpart (despite being initialised from English fine-tuned weights). Finally, we note that 'translate train' performance is still far from the corresponding English performance. This result is rather surprising for UC2, which was solely pretrained on Conceptual Captions in five languages, each of equal size.\\n\\nRetrieval Performance by Evaluation Subset. Our xFlickr&CO dataset is a composition of Flickr and COCO data (each of 1,000 image\u2013sentence pairs). As models are fine-tuned on Flickr, the COCO evaluation subset might require out-of-distribution generalisation due to its different visual domain. Indeed, this is what we observe in English (Table 21). However, Figure 13 shows that this is not the case for the multilingual encoders, which perform similarly in both subsets. A closer look at UC2 further shows that this behaviour is language-independent. Given the relatively poor performance of current models, we cannot assess whether the domain shift we observe in English does not happen in xFlickr&CO due to its unified captioning guidelines, or whether domain generalisation will be a challenge in xFlickr&CO for future, tightly-multilingual models.\"}"}
{"id": "bugliarello22a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Retrieval accuracy on DEU and JPN xFlickr&CO splits with target language fine-tuning (horizontal lines) and more shots.\\n\\nMultilingual Encoders. Again, based on Table 5 the model that yields the highest scores is UC\\\\textsuperscript{2} in all tasks. However, we also note that some models are particularly brittle, and few-shot adaptation can lead them to revert to chance-level performance, as is the case of M\\\\textsubscript{3}P when fine-tuning on MaRVL's target languages IND, CMN, and TUR.\\n\\n6. Conclusion and Outlook\\nWe introduced the Image-Grounded Language Understanding Evaluation (IGLUE) benchmark, aiming to facilitate modelling and evaluation in multilingual vision-and-language research. IGLUE brings together 5 datasets across 4 structurally different tasks in 20 diverse languages, and provides standardised data splits and evaluation protocols to train and evaluate in zero-shot and few-shot transfer setups.\\n\\nWe set strong baselines and carry out analyses that shed new light into multilingual V&L models, now enabled by the creation of the IGLUE benchmark. In particular, we find that 1) 'translate test' transfer vastly outperforms zero-shot transfer via multilingual encoders; 2) few-shot learning requires thousand of examples before yielding gains; 3) unlabelled textual data size and typological distance from English are weaker predictors of models performance than usually observed in text-only tasks; 4) results vary significantly across tasks, with multicultural visual reasoning and cross-modal retrieval having the largest transfer gaps. Given the diversity and manifoldness of each task, we explicitly set IGLUE as a multi-faceted benchmark, without a single average score.\\n\\nAlong with putting forward an evaluation suite for current and future multilingual V&L models, IGLUE also opens up new opportunities for future research that goes beyond the scope of this paper: such as analysing single-source versus multi-source transfer for V&L tasks, conducting multi-task learning, or investigating staged multi-task transfer. A longer-term outlook concerns including other tasks and languages into the benchmark, as well as reaching beyond the image-only visual modality, towards multimodal tasks that rely on processing videos (Li et al., 2021) and speech (Yang et al., 2021) in different languages.\\n\\nAcknowledgements\\n\u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6 \u22c6"}
{"id": "bugliarello22a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages\\n\\nBarrault, L., Bougares, F., Specia, L., Lala, C., Elliott, D., and Frank, S. Findings of the third shared task on multi-modal machine translation. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pp. 304\u2013323, Belgium, Brussels, October 2018.\\n\\nBender, E. M. On achieving and evaluating language-independence in nlp. Linguistic Issues in Language Technology, 6, Oct. 2011.\\n\\nBernardi, R., Cakici, R., Elliott, D., Erdem, A., Erdem, E., Ikizler-Cinbis, N., Keller, F., Muscat, A., and Plank, B. Automatic description generation from images: A survey of models, datasets, and evaluation measures. J. Artif. Int. Res., 55(1):409\u2013442, jan 2016.\\n\\nBlasi, D., Anastasopoulos, A., and Neubig, G. Systematic inequalities in language technology performance across the world's languages. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5486\u20135505, Dublin, Ireland, May 2022.\\n\\nBowman, S. R., Angeli, G., Potts, C., and Manning, C. D. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 632\u2013642, Lisbon, Portugal, September 2015.\\n\\nBugliarello, E., Cotterell, R., Okazaki, N., and Elliott, D. Multimodal pretraining unmasked: A meta-analysis and a unified framework of vision-and-language BERTs. Transactions of the Association for Computational Linguistics, 9:978\u2013994, 2021.\\n\\nCaglayan, O., Kuyu, M., Amac, M. S., Madhyastha, P., Erdem, E., Erdem, A., and Specia, L. Cross-lingual visual pre-training for multimodal machine translation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 1317\u20131324, Online, April 2021.\\n\\nChen, Y.-C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., and Liu, J. UNITER: Universal image-text representation learning. In Vedaldi, A., Bischof, H., Brox, T., and Frahm, J.-M. (eds.), Computer Vision \u2013 ECCV 2020, pp. 104\u2013120, Cham, 2020.\\n\\nConneau, A. and Lample, G. Cross-lingual language model pretraining. In Wallach, H., Larochelle, H., Beygelzimer, A., d\u2019Alch\u00e9-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32, 2019.\\n\\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm\u00e1n, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8440\u20138451, Online, July 2020.\\n\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255, 2009.\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June 2019.\\n\\nDryer, M. S. and Haspelmath, M. (eds.). WALS Online. Max Planck Institute for Evolutionary Anthropology, Leipzig, 2013.\\n\\nDumitrescu, S. D., Rebeja, P., Lorincz, B., Gaman, M., Avram, A., Ilie, M., Pruteanu, A., Stan, A., Rosia, L., Iacobescu, C., Morogan, L., Dima, G., Marchidan, G., Rebedea, T., Chitez, M., Yogatama, D., Ruder, S., Ionescu, R. T., Pascanu, R., and Patraucean, V. Liro: Benchmark and leaderboard for romanian language tasks. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.\\n\\nElliott, D., Frank, S., Sima'an, K., and Specia, L. Multi30K: Multilingual English-German image descriptions. In Proceedings of the 5th Workshop on Vision and Language, pp. 70\u201374, Berlin, Germany, August 2016.\\n\\nElliott, D., Frank, S., Barrault, L., Bougares, F., and Specia, L. Findings of the second shared task on multimodal machine translation and multilingual image description. In Proceedings of the Second Conference on Machine Translation, pp. 215\u2013233, Copenhagen, Denmark, September 2017.\\n\\nEthayarajh, K. and Jurafsky, D. Utility is in the eye of the user: A critique of NLP leaderboards. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4846\u20134853, Online, November 2020.\\n\\nFrank, S., Elliott, D., and Specia, L. Assessing multilingual multimodal image description: Studies of native speaker preferences and translator choices. Natural Language Engineering, 24(3):393\u2013413, 2018.\\n\\nGarc\u00eda, C. MS-COCO-ES, 2020. https://github.com/carlosGarciaHe/MS-COCO-ES.\"}"}
{"id": "bugliarello22a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages\\n\\nGeigle, G., Pfeiffer, J., Reimers, N., Vuli\u0107, I., and Gurevych, I. Retrieve fast, rerank smart: Cooperative and joint approaches for improved cross-modal retrieval. Transactions of the Association for Computational Linguistics, 10, 2022.\\n\\nGella, S., Sennrich, R., Keller, F., and Lapata, M. Image pivoting for learning multilingual multimodal representations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2839\u20132845, Copenhagen, Denmark, September 2017.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, 2016.\\n\\nHendrycks, D. and Gimpel, K. Gaussian Error Linear Units (GELUs). arXiv preprint arXiv:1606.08415, 2016.\\n\\nHu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., and Johnson, M. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 4411\u20134421, 13\u201318 Jul 2020.\\n\\nHuang, P.-Y., Chang, X., and Hauptmann, A. Multi-head attention with diversity for learning grounded multilingual multimodal representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1461\u20131467, Hong Kong, China, November 2019.\\n\\nHuang, P.-Y., Patrick, M., Hu, J., Neubig, G., Metze, F., and Hauptmann, A. Multilingual multimodal pre-training for zero-shot cross-lingual transfer of vision-language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2443\u20132459, Online, June 2021.\\n\\nHudson, D. A. and Manning, C. D. GQA: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\nJain, A., Guo, M., Srinivasan, K., Chen, T., Kudugunta, S., Jia, C., Yang, Y., and Baldridge, J. MURAL: Multimodal, multitask representations across languages. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 3449\u20133463, Punta Cana, Dominican Republic, November 2021.\\n\\nJauhar, S. K., Gamon, M., and Pantel, P. Neural task representations as weak supervision for model agnostic cross-lingual transfer. arXiv:1811.01115, 2018.\\n\\nJia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4904\u20134916, 18\u201324 Jul 2021.\\n\\nJoshi, P., Santy, S., Budhiraja, A., Bali, K., and Choudhury, M. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6282\u20136293, Online, July 2020.\\n\\nKarpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3128\u20133137, 2015.\\n\\nKiela, D., Vuli\u0107, I., and Clark, S. Visual bilingual lexicon induction with transferred ConvNet features. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 148\u2013158, Lisbon, Portugal, September 2015.\\n\\nKiela, D., Conneau, A., Jabri, A., and Nickel, M. Learning visually grounded sentence representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 408\u2013418, New Orleans, Louisiana, June 2018.\\n\\nKim, D., Saito, K., Saenko, K., Sclaroff, S., and Plummer, B. Mule: Multimodal universal language embedding. Proceedings of the AAAI Conference on Artificial Intelligence, 34(07):11254\u201311261, Apr. 2020.\\n\\nKrishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D. A., Bernstein, M. S., and Fei-Fei, L. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vision, 123(1):32\u201373, may 2017.\"}"}
{"id": "bugliarello22a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages\\n\\nLam, Q. H., Le, Q. D., Nguyen, V. K., and Nguyen, N. -T.\\n\\nUit-viic: A dataset for the first evaluation on Vietnamese image captioning. In Nguyen, N. T., Hoang, B. H., Huynh, C. P., Hwang, D., Trawi\u0144ski, B., and Vossen, G. (eds.), Computational Collective Intelligence, pp. 730\u2013742, Cham, 2020.\\n\\nLan, W., Li, X., and Dong, J. Fluency-guided cross-lingual image captioning. In Proceedings of the 25th ACM International Conference on Multimedia, MM '17, pp. 1549\u20131557, New York, NY, USA, 2017.\\n\\nLauscher, A., Ravishankar, V., Vuli\u0107, I., and Glava\u0161, G. From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4483\u20134499, Online, November 2020.\\n\\nLei, J., Berg, T., and Bansal, M. mTVR: Multilingual moment retrieval in videos. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 726\u2013734, Online, August 2021.\\n\\nLi, L., Lei, J., Gan, Z., Yu, L., Chen, Y.-C., Pillai, R., Cheng, Y., Zhou, L., Wang, X. E., Wang, W. Y., Berg, T. L., Bansal, M., Liu, J., Wang, L., and Liu, Z. V ALUE: A multi-task benchmark for video-and-language understanding evaluation. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.\\n\\nLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang, K.-W. VisualBERT: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019a.\\n\\nLi, X., Xu, C., Wang, X., Lan, W., Jia, Z., Yang, G., and Xu, J. COCO-CN for cross-lingual image tagging, captioning, and retrieval. IEEE Transactions on Multimedia, 21(9):2347\u20132360, 2019b.\\n\\nLiang, Y., Duan, N., Gong, Y., Wu, N., Guo, F., Qi, W., Gong, M., Shou, L., Jiang, D., Cao, G., Fan, X., Zhang, R., Agrawal, R., Cui, E., Wei, S., Bharti, T., Qiao, Y., Chen, J.-H., Wu, W., Liu, S., Yang, F., Campos, D., Majumder, R., and Zhou, M. XGLUE: A new benchmark dataset for cross-lingual pre-training, understanding and generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6008\u20136018, Online, November 2020.\\n\\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., and Zitnick, C. L. Microsoft COCO: Common objects in context. In Fleet, D., Pajdla, T., Schiele, B., and Tuytelaars, T. (eds.), Computer Vision \u2013 ECCV 2014, pp. 740\u2013755, Cham, 2014.\\n\\nLittell, P., Mortensen, D. R., Lin, K., Kairis, K., Turner, C., and Levin, L. URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 8\u201314, Valencia, Spain, April 2017.\\n\\nLiu, F., Bugliarello, E., Ponti, E. M., Reddy, S., Collier, N., and Elliott, D. Visually grounded reasoning across languages and cultures. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10467\u201310485, Online and Punta Cana, Dominican Republic, November 2021.\\n\\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.\\n\\nLu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pre-training task-agnostic visiolinguistic representations for vision-and-language tasks. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32, 2019.\\n\\nLu, J., Goswami, V., Rohrbach, M., Parikh, D., and Lee, S. 12-in-1: Multi-task vision and language representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10434\u201310443, June 2020.\\n\\nMishra, S. K., Dhir, R., Saha, S., and Bhattacharyya, P. A Hindi image caption generation framework using deep learning. ACM Trans. Asian Low-Resour. Lang. Inf. Proces., 20(2), Mar 2021.\\n\\nNakayama, H., Tamura, A., and Ninomiya, T. A visually-grounded parallel corpus with phrase-to-region linking. In Proceedings of the 12th Language Resources and Evaluation Conference, pp. 4204\u20134210, Marseille, France, May 2020.\\n\\nNi, M., Huang, H., Su, L., Cui, E., Bharti, T., Wang, L., Zhang, D., and Duan, N. M3p: Learning universal representations via multitask multilingual multimodal pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3977\u20133986, June 2021.\\n\\nPark, S., Moon, J., Kim, S., Cho, W. I., Han, J. Y., Park, J., Song, C., Kim, J., Song, Y., Oh, T., Lee, J., Oh, J., Lyu, S., Jeong, Y., Lee, I., Seo, S., Lee, D., Kim, H., Lee, M., Jang, S., Do, S., Kim, S., Lim, K., Lee, J., Park, K., Shin, J.,\"}"}
{"id": "bugliarello22a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages\\n\\nKim, S., Park, L., Oh, A., Ha, J.-W., and Cho, K. KLUE: Korean language understanding evaluation. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An imperative style, high-performance deep learning library. In Wallach, H., Larochelle, H., Beygelzimer, A., d\u2019Alch\u00b4e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, pp. 8024\u20138035, 2019.\\n\\nPfeiffer, J., Geigle, G., Kamath, A., Steitz, J.-M., Roth, S., Vuli\u00b4c, I., and Gurevych, I. xGQA: Cross-lingual visual question answering. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 2497\u20132511, Dublin, Ireland, May 2022.\\n\\nPonti, E. M., O'Horan, H., Berzak, Y ., Vuli\u00b4c, I., Reichart, R., Poibeau, T., Shutova, E., and Korhonen, A. Modelling language variation and universals: A survey on typological linguistics for natural language processing. Computational Linguistics, 45(3):559\u2013601, September 2019.\\n\\nPonti, E. M., Glava \u02c7s, G., Majewska, O., Liu, Q., Vuli \u00b4c, I., and Korhonen, A. XCOPA: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2362\u20132376, Online, November 2020.\\n\\nPonti, E. M., Aralikatte, R., Shrivastava, D., Reddy, S., and S\u00f8gaard, A. Minimax and neyman\u2013Pearson meta-learning for outlier languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 1245\u20131260, Online, August 2021a.\\n\\nPonti, E. M., Vuli\u00b4c, I., Cotterell, R., Parovic, M., Reichart, R., and Korhonen, A. Parameter space factorization for zero-shot learning across tasks and languages. Transactions of the Association for Computational Linguistics, 9:410\u2013428, 2021b.\\n\\nRen, S., He, K., Girshick, R., and Sun, J. Faster R-CNN: Towards real-time object detection with region proposal networks. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, pp. 91\u201399. Curran Associates, Inc., 2015.\\n\\nRotman, G., Vuli\u00b4c, I., and Reichart, R. Bridging languages through images with deep partial canonical correlation analysis. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 910\u2013921, Melbourne, Australia, July 2018.\\n\\nRuder, S., Constant, N., Botha, J., Siddhant, A., Firat, O., Fu, J., Liu, P., Hu, J., Garrette, D., Neubig, G., and Johnsson, M. XTREME-R: Towards more challenging and nuanced multilingual evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10215\u201310245, Online and Punta Cana, Dominican Republic, November 2021.\\n\\nScaiella, A., Croce, D., and Basili, R. Large scale datasets for image and video captioning in Italian. Italian Journal of Computational Linguistics, 2(5):49\u201360, 2019.\\n\\nSchick, T. and Sch \u00a8utze, H. It's not just size that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2339\u20132352, Online, June 2021.\\n\\nSennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715\u20131725, Berlin, Germany, August 2016.\\n\\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2556\u20132565, Melbourne, Australia, July 2018.\\n\\nShavrina, T., Fenogenova, A., Anton, E., Shevelev, D., Artemova, E., Malykh, V ., Mikhailov, V ., Tikhonova, M., Chertok, A., and Evlampiev, A. RussianSuperGLUE: A Russian language understanding evaluation benchmark. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4717\u20134726, Online, November 2020.\\n\\nSnyder, B. and Barzilay, R. Climbing the tower of belief: Unsupervised multilingual learning. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML'10, pp. 29\u201336, Madison, WI, USA, 2010.\\n\\nSrinivasan, K., Raman, K., Chen, J., Bendersky, M., and Najork, M. WIT: Wikipedia-Based Image Text Dataset for Multimodal Multilingual Machine Learning, pp. 2443\u20132449. Association for Computing Machinery, New York, NY, USA, 2021.\"}"}
{"id": "bugliarello22a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages\\n\\nSu, L., Duan, N., Cui, E., Ji, L., Wu, C., Luo, H., Liu, Y., Zhong, M., Bharti, T., and Sacheti, A.\\n\\nGEM: A general evaluation benchmark for multimodal tasks. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 2594\u20132603, Online, August 2021.\\n\\nSu, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., and Dai, J.\\n\\nVL-BERT: Pre-training of generic visual-linguistic representations. In International Conference on Learning Representations, 2020.\\n\\nSuhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and Artzi, Y.\\n\\nA corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6418\u20136428, Florence, Italy, July 2019.\\n\\nTan, H. and Bansal, M.\\n\\nLXMERT: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5100\u20135111, Hong Kong, China, November 2019.\\n\\nUnal, M. E., Citamak, B., Yagcioglu, S., Erdem, A., Erdem, E., Cinbis, N. I., and Cakici, R.\\n\\nTasvirEt: G\u00e9r\u00fcnt\u00fclerden otomatik t\u00fcrkc\u00e9 a\u00e7\u0131klama olusturma I c\u00fc plver k\u00fcmesi (TasvirEt: A benchmark dataset for automatic Turkish description generation from images). IEEE Sinyal Isleme ve Iletisim Uygulamalar\u0131 Kurultay\u0131 (SIU 2016), 2016.\\n\\nvan Miltenburg, E., Elliott, D., and Vossen, P.\\n\\nCross-linguistic differences and similarities in image descriptions. In Proceedings of the 10th International Conference on Natural Language Generation, pp. 21\u201330, Santiago de Compostela, Spain, September 2017.\\n\\nvan Miltenburg, E., Kadar, A., Koolen, R., and Krahmer, E.\\n\\nDIDEC: The Dutch image description and eye-tracking corpus. In Proceedings of the 27th International Conference on Computational Linguistics, pp. 3658\u20133669, Santa Fe, New Mexico, USA, August 2018.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I.\\n\\nAttention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, pp. 5998\u20136008. Curran Associates, Inc., 2017.\\n\\nVu, H. T., Greco, C., Erofeeva, A., Jafaritazehjan, S., Linders, G., Tanti, M., Testoni, A., Bernardi, R., and Gatt, A.\\n\\nGrounded textual entailment. In Proceedings of the 27th International Conference on Computational Linguistics, pp. 2354\u20132368, Santa Fe, New Mexico, USA, August 2018.\\n\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.\\n\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353\u2013355, Brussels, Belgium, November 2018.\\n\\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.\\n\\nSuper-glue: A stickier benchmark for general-purpose language understanding systems. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32, 2019a.\\n\\nWang, J., Madhyastha, P., Figueiredo, J., Lala, C., and Specia, L.\\n\\nMultiSubs: A large-scale multimodal and multilingual dataset. arXiv:2103.01910, 2021.\\n\\nWang, X., Wu, J., Chen, J., Li, L., Wang, Y.-F., and Wang, W. Y.\\n\\nVatex: A large-scale, high-quality multilingual dataset for video-and-language research. In The IEEE International Conference on Computer Vision (ICCV), October 2019b.\\n\\nWehrmann, J., Lopes, M. A., Souza, D., and Barros, R.\\n\\nLanguage-agnostic visual-semantic embeddings. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 5803\u20135812, 2019.\\n\\nWilie, B., Vincentio, K., Winata, G. I., Cahyawijaya, S., Li, X., Lim, Z. Y., Soleman, S., Mahendra, R., Fung, P., Bahar, S., and Purwarianti, A.\\n\\nIndoNLU: Benchmark and resources for evaluating Indonesian natural language understanding. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pp. 843\u2013857, Suzhou, China, December 2020.\\n\\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al.\\n\\nGoogle's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n\\nXie, N., Lai, F., Doran, D., and Kadav, A.\\n\\nVisual entailment: A novel task for fine-grained image understanding. arXiv:1901.06706, 2019.\\n\\nXie, S., Girshick, R., Dollar, P., Tu, Z., and He, K.\\n\\nAggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\\n\\nYang, S.-w., Chi, P.-H., Chuang, Y.-S., Lai, C.-I., Lakhotia, K., Lin, Y.Y., Liu, A.T., Shi, J., Chang, X., Lin, G.-T.,\"}"}
{"id": "bugliarello22a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages\\n\\nhsien Huang, T., Tseng, W.-C., tik Lee, K., Liu, D.-R., Huang, Z., Dong, S., Li, S.-W., Watanabe, S., rahman Mohamed, A., and yi Lee, H. Superb: Speech processing universal performance benchmark. In Interspeech, 2021.\\n\\nYao, S. and Wan, X. Multimodal transformer for multimodal machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4346\u20134350, Online, July 2020.\\n\\nYin, D., Li, L. H., Hu, Z., Peng, N., and Chang, K.-W. Broaden the vision: Geo-diverse visual commonsense reasoning. In Proceedings of EMNLP 2021, pp. 2115\u20132129, 2021.\\n\\nYoshikawa, Y., Shigeto, Y., and Takeuchi, A. STAIR captions: Constructing a large-scale Japanese image caption dataset. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 417\u2013421, Vancouver, Canada, July 2017.\\n\\nYoung, P., Lai, A., Hodosh, M., and Hockenmaier, J. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.\\n\\nZhao, M., Zhu, Y., Shareghi, E., Vuli\u00b4c, I., Reichart, R., Korhonen, A., and Sch \u00a8utze, H. A closer look at few-shot crosslingual transfer: The choice of shots matters. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5751\u20135767, Online, August 2021.\\n\\nZhou, M., Zhou, L., Wang, S., Cheng, Y., Li, L., Yu, Z., and Liu, J. Uc2: Universal cross-lingual cross-modal vision-and-language pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4155\u20134165, June 2021.\"}"}
{"id": "bugliarello22a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages\\n\\nA. Datasets\\n\\nIn this section, we provide more details about the datasets and splits that we introduce in IGLUE, focusing on reproducibility of our experiments and any follow-up work that leverages the IGLUE benchmark. We also report how few-shot samples were collected for each dataset. In particular, we use the notion of \u2018annotation context\u2019 to define what a \u2018shot\u2019 means in each dataset (see details below). Table 6 lists the size of few-shot splits across all datasets.\\n\\nA.1. XVNLI\\n\\nFigure 6 illustrates statistical properties of XVNLI. In Figure 6(a), we see that the distribution of labels is uniform across splits. Figure 6(b) instead shows that the distribution of sentence lengths\u2014in number of characters\u2014in the test split is similar across languages, and often shorter for ARB.\\n\\n![Table 6](image)\\n\\n- **Dataset Metric**\\n  - **XVNLI**\\n    - # shots: 1, 5, 10, 20, 25, 48\\n    - # images: 1, 5, 10, 20, 25, 48\\n    - # samples: 3, 18, 39, 75, 90, 168\\n  - **xGQA**\\n    - # shots: 1, 5, 10, 20, 25, 48\\n    - # images: 1, 5, 10, 20, 25, 48\\n    - # samples: 27, 155, 317, 594, 704, 1,490\\n  - **MaRVL**\\n    - # shots: 1, 2, 4, 10\\n    - # images: 8, 16, 32, 80\\n    - # samples: 4, 8, 16, 40, 80\\n  - **xFlickr&CO**\\n    - # shots: 1, 5, 10, 25, 50, 100\\n    - # images: 1, 5, 10, 25, 50, 100\\n    - # samples: 1, 5, 10, 25, 50, 100\\n\\nFew-shot Data.\\n\\nWe extract few-shot samples from the original SNLI test split, which was translated by Agi\u0107 & Schluter (2018). This was done prior to defining our test split, and by sampling all the examples corresponding to a given image, so as to avoid data leakage between training and evaluation.\\n\\nTable 6.\\n\\n- **Dataset Metric**\\n  - **XVNLI**\\n    - # shots: 1, 5, 10, 20, 25, 48\\n    - # images: 1, 5, 10, 20, 25, 48\\n    - # samples: 3, 18, 39, 75, 90, 168\\n  - **xGQA**\\n    - # shots: 1, 5, 10, 20, 25, 48\\n    - # images: 1, 5, 10, 20, 25, 48\\n    - # samples: 27, 155, 317, 594, 704, 1,490\\n  - **MaRVL**\\n    - # shots: 1, 2, 4, 10\\n    - # images: 8, 16, 32, 80\\n    - # samples: 4, 8, 16, 40, 80\\n  - **xFlickr&CO**\\n    - # shots: 1, 5, 10, 25, 50, 100\\n    - # images: 1, 5, 10, 25, 50, 100\\n    - # samples: 1, 5, 10, 25, 50, 100\\n\\nFew-shot Data.\\n\\nWe extract few-shot samples from the original SNLI test split, which was translated by Agi\u0107 & Schluter (2018). This was done prior to defining our test split, and by sampling all the examples corresponding to a given image, so as to avoid data leakage between training and evaluation.\\n\\nFigure 6.\\n\\n- Statistical properties of XVNLI.\\n\\nA.2. xGQA\\n\\nFew-shot Data.\\n\\nWe reuse the few-shot splits defined by Pfeiffer et al. (2022), which also include development data in the target languages. The authors sampled 48 images, giving up to 1,490 questions translated for each language.\\n\\nA.3. MaRVL\\n\\nFew-shot Data.\\n\\nWe follow the annotation protocol of Liu et al. (2021) to collect few-shot samples. For each language, we first ask native speakers to select 10 concepts among the ones identified by the authors in order to collect in-domain training data. The annotators are then required to retrieve at least 8 images per concept. When doing so, we further provide the MaRVL test images and require them to select different ones to avoid any visual leakage between few-shot and test samples. We finally sample 8 images per concept, randomly pair them and ask annotators to write a caption that is true for two pairs and false for the other two pairs. These 8 images then constitute an \u2018annotation context\u2019 and as such we consider the resulting data as one shot (i.e. one shot, eight images and one sentence giving four data points).\\n\\nAs image collection is the most time-consuming step in the pipeline, we expand our few-shot data by re-shuffling and re-annotating each image pair (\u201810 \u00d7 2\u2019 setup), resulting in 80 data points per language. Notably, due to the challenges in collecting few-shot data in low-resource languages (i.e. Swahili and Tamil), we only provide few-shot samples for three languages (i.e. Indonesian, Mandarin and Turkish).\"}"}
