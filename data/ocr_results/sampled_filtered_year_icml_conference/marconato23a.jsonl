{"id": "marconato23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\nwhere we chose \\\\[ \\\\zeta = \\\\min_c \\\\min_s \\\\mathbb{Z}(c; K(s)) \\\\]. Therefore, \\n\\n\\\\[ 1 - \\\\beta \\\\| u_{K(s)}(y|x) \\\\|_\\\\infty \\\\cdot \\\\| p_\\\\phi(\\\\cdot | x) - p_\\\\psi(\\\\cdot | x) \\\\|_1 \\\\leq 1 \\\\beta \\\\zeta \\\\| p_\\\\phi(\\\\cdot | x) - p_\\\\psi(\\\\cdot | x) \\\\|_1 \\\\]  \\n\\n(16)\\n\\nTaking \\\\( \\\\gamma = \\\\max_s \\\\gamma \\\\| D(s) \\\\| t \\\\) and replacing the above in Equation (12) yields the claim.\\n\\nIn order to prove Theorem 4.1, let \\\\( \\\\theta(t-1) \\\\) be the parameters learned after observing \\\\( t-1 \\\\) tasks and \\\\( \\\\theta \\\\) to be learned at the current iteration. Applying Lemma A.1 with \\\\( \\\\phi(\\\\cdot | x) = \\\\theta(t-1) \\\\) and \\\\( \\\\psi(\\\\cdot | x) = \\\\theta \\\\) to Equation (3) yields the desired result.\\n\\nRemark: In the worst case, \\\\( \\\\zeta \\\\) can be as small as \\\\( 1 \\\\), which occurs if in all tasks \\\\( s \\\\leq t \\\\) the knowledge \\\\( K(s) \\\\) accepts a single concept configuration \\\\( c \\\\) for every example \\\\((x, y)\\\\); more commonly, \\\\( \\\\zeta \\\\) is exponential in the number of concepts \\\\( k \\\\). Also, \\\\( \\\\beta \\\\), which is the minimum likelihood attained by either \\\\( p_\\\\phi \\\\) or \\\\( p_\\\\psi \\\\) on the data sets \\\\( D(1), \\\\ldots, D(t) \\\\), is actively maximized during learning.\\n\\nA.3. What Are Correct Semantics?\\n\\nStrictly speaking, the only concept distribution with the actual correct semantics is the ground-truth distribution \\\\( p(C|x) \\\\).\\n\\nOur assumption is that the ground-truth concept distribution we are given is always consistent with the knowledge, in the sense that \\\\( p(C|x) | X/x, Y/y = K(t) \\\\) \\\\([X/x, Y/y] \\\\) for every possible task \\\\( T(t) = (D(t), K(t)) \\\\) and example \\\\((x, y) \\\\in D(t)\\\\). In other words, we assume that the knowledge correctly reflects how the world works. Under this assumption, having the correct semantics is useful in practice because, when paired with the knowledge, the ground-truth distribution by construction always yields correct labels in every possible task the learner can in principle receive. This is a form of systematic generalization.\\n\\nNaturally, if the learned distribution \\\\( p_\\\\theta(C|x) \\\\) matches the ground-truth distribution exactly, then it will also achieve systematic generalization. This condition, however, is very restrictive. Pragmatically, we can relax this requirement, and say that a distribution encodes the correct semantics if it is indistinguishable from the ground-truth distribution in terms of what concepts it predicts. Formally, we say that a distribution \\\\( p_\\\\theta(C|x) \\\\) is semantically equivalent to the ground-truth distribution on data \\\\( D \\\\) if it allows us to infer the same concept configuration for all data points, or formally:\\n\\n\\\\[ \\\\forall x \\\\in D. \\\\arg\\\\max_c p(c|x) \\\\equiv \\\\arg\\\\max_c p_\\\\theta(c|x) \\\\]  \\n\\n(17)\\n\\nwhere we used \\\\( \\\\equiv \\\\) to indicate set equivalence. This is quite intuitive: any distribution satisfying Equation (17) will yield the same concepts \\\\( c \\\\) as \\\\( p(C|x) \\\\) for every \\\\( x \\\\), and therefore also the same MAP states \\\\( y \\\\) under any choice of knowledge \\\\( K \\\\).\\n\\nThe opposite is not generally true: the knowledge \\\\( K \\\\) might have multiple possible solutions, in the sense that different choices of concepts \\\\( c \\\\) might yield the same label \\\\( y \\\\). In this case, the label does not carry enough information to recover the ground-truth concepts \\\\( \\\\arg\\\\max_c p(c|x) \\\\), and therefore also a concept distribution \\\\( p_\\\\theta(C|x) \\\\) semantically equivalent to \\\\( p(C|x) \\\\). This is exactly what we mean by reasoning shortcuts: concept distributions that achieve high performance on the observed task(s) but have no guarantee of systematically generalizing to future tasks, or more formally:\\n\\n\\\\[ \\\\arg\\\\max_y p(y|x; K) \\\\equiv \\\\arg\\\\max_y p_\\\\theta(y|x; K) \\\\land \\\\arg\\\\max_c p(c|x) \\\\not\\\\equiv \\\\arg\\\\max_c p_\\\\theta(c|x) \\\\]  \\n\\n(18)\\n\\nA.4. Reasoning Shortcuts in other NeSy Architectures\\n\\nWhile Theorem 3.2 shows that reasoning shortcuts do affect DeepProbLog, which maximizes for label likelihood, we remark that they are a general phenomenon. It is easy to see that reasoning shortcuts occur whenever the prior knowledge admits deducing the correct label \\\\( y \\\\) from concepts \\\\( c \\\\) that do not have the correct semantics, as this makes it is impossible for a model to distinguish between concepts with \\\"correct\\\" vs. \\\"incorrect\\\" semantics by maximizing accuracy alone. This impacts offline NeSy prediction tasks and NeSy-CL problems alike; indeed, Theorem 3.2 makes no assumption on how the training set \\\\( D \\\\) has been generated.\\n\\nReasoning shortcuts are not specific to DeepProbLog. On the contrary, this situation can be triggered by a variety of other NeSy architectures, including but not limited to:\\n\\n(i) NeSy predictors that rely on a top reasoning layer to ensure predictions are consistent with prior knowledge, which are typically trained to maximize some surrogate of the label accuracy, including (Ahmed et al., 2022a; Giunchiglia & Lukasiewicz, 2020; Hoernle et al., 2022; Huang et al., 2021; Winters et al., 2022; van Krieken et al., 2022b).\"}"}
{"id": "marconato23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neural-Symbolic Continual Learning\\n\\n[ii] NeSy predictors that rely on relaxed reasoning layers obtained by softening the logical prior knowledge (Diligenti et al., 2012; Donadello et al., 2017), because this transformation usually preserves existing optima of the label accuracy. As such, it also preserves unintended optima \u2013 that is, reasoning shortcuts.\\n\\n[iii] Neural networks trained to maximize accuracy and consistency with prior knowledge using the Semantic Loss and similar techniques (Xu et al., 2018; Fischer et al., 2019; Ahmed et al., 2022b). In fact, Theorem 3.2 shows that DeepProbLog is affected by shortcuts precisely because, from the neural network\u2019s perspective, its reasoning layer acts exactly like the Semantic Loss; see our remark in Appendix A.\\n\\nMore generally, reasoning shortcuts impact NeSy tasks and architectures beyond these, at least as long as models are trained by optimizing loss functions that do not measure or correlate with concept quality. We leave a detailed analysis of specific cases to future work.\\n\\nB. Implementation Details\\n\\nIn this Section, we report useful details for the models and the metrics adopted in the evaluation.\\n\\nB.1. Hardware and Software Implementation\\n\\nThe code for the project was developed on top mammoth (Boschini et al., 2022), a well-known CL framework. We included the implementation of DeepProbLog for MNIST-Addition from VAEL (Misino et al., 2022). The generation of CLE4EVR was adapted from (Stammer et al., 2021). All experiments were implemented using Python 3 and Pytorch (Paszke et al., 2019) and run on a server with 128 CPUs, 1TiB RAM, and 8 A100 GPUs.\\n\\nB.2. Metrics\\n\\nWe adopted standard CL measures, namely task-incremental (Task-IL) and class-incremental (Class-IL) accuracy, applied here to both labels and concepts predictions, as well as forward transfer (FWT) and backward transfer (BWT) on the labels, see also (Buzzega et al., 2020). Below we write $T$ to indicate the last task.\\n\\n\u2022 Class-IL measures the average accuracy on the test sets of all tasks $T$. In Table 1, we report Class-IL at the very last task $T$, defined as:\\n\\n$$CLASS_{IL}(\\\\theta_T) = \\\\frac{1}{T} \\\\sum_{s=1}^{T} \\\\text{A}_Y(\\\\theta_T, s)$$\\n\\nwhere $\\\\text{A}_Y(\\\\theta_t, s)$ denotes the accuracy on the labels evaluated on the test set of task $s$. Class-IL for concepts is analogous, but builds on the average accuracy over all concepts.\\n\\n\u2022 Task-IL is the average accuracy over the test sets of all tasks up to $t$, evaluated only on examples annotated with the classes or concepts appearing in task $t$. The definition is identical to Equation (19) except that we mask the prediction of model so as to place mass only on the labels appearing in $D_s$, with $s \\\\leq t$.\\n\\n\u2022 FWT evaluates the adaptability of the model at each time-step to the successive task. Formally, at each $t$, FWT measures the average gain in accuracy between $\\\\theta_t$ and a random baseline $\\\\theta_{rand}$ when predicting the labels of the task $t+1$. This can be written as:\\n\\n$$FWT = \\\\frac{1}{T-1} \\\\sum_{t=1}^{T-1} \\\\text{A}_Y(\\\\theta_t, t+1) - \\\\text{A}_Y(\\\\theta_{rand}, t+1)$$\\n\\nwhere $\\\\theta_{rand}$ denotes the initialized model with random weights.\\n\\n\u2022 BWT measures how much forgetting the model undergoes by looking at how much accuracy for each task $t$ is retained after the last task. Formally:\\n\\n$$BWT = \\\\frac{1}{T} \\\\sum_{t=1}^{T} \\\\text{A}_Y(\\\\theta_t, t) - \\\\text{A}_Y(\\\\theta_T, t)$$\"}"}
{"id": "marconato23a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\nFor the sake of brevity, in the main paper we reported only Class-IL on labels and concepts, and FWT. Task-IL was omitted because it does not account for accuracy on past concepts that no longer occur in the last task, and BWT because it does not as informative as Class-IL. All results for these extra metrics on MNAdd-Seq are reported in Appendix E.\\n\\nB.3. Architectures & Models Details\\n\\nMNIST-Addition: For both CBM s and DeepProbLog we adopted the same architecture for extracting concepts \u2013 henceforth, encoder \u2013 and we implemented it as a standard convolutional neural network, with dropout set at 50% after each convolution module. We also inject a noise term $\\\\epsilon \\\\sim N(0, 0.1)$ after the encoder to stabilize the overall training process. The complete structure is reported in Table 2.\\n\\nTable 2. CNN Encoder for MNIST-Addition\\n\\n| INPUT SHAPE | LAYER TYPE | PARAMETERS | ACTIVATION |\\n|-------------|------------|------------|------------|\\n| (28, 28, 1) | Convolution depth=64, kernel=4, stride=2, padding=1 | ReLU | |\\n| (14, 14, 64) | Dropout $p = 0.5$ | | |\\n| (14, 14, 64) | Convolution depth=128, kernel=4, stride=2, padding=1 | ReLU | |\\n| (7, 7, 128) | Dropout $p = 0.5$ | | |\\n| (7, 7, 128) | Convolution depth=256, kernel=4, stride=2, padding=1 | ReLU | |\\n| (3, 3, 256) | Flatten | | |\\n| (2304) | Linear dim=10, bias = True | | |\\n\\nIn both CBMs and DeepProbLog, each input digit $x(i)$ is predicted independently and mapped to a 10-dimensional bottleneck $z(i)$. Then, the two encodings are stacked together, obtaining the overall representation $z = (z(1), z(2))$.\\n\\nThe classifier (top layer) of the CBM is designed to predict the sum the $z(1)$ and $z(2)$. A simple linear layer, which is the standard choice in CBM (Koh et al., 2020), is insufficient to successfully address the task. Therefore, we implemented the classifier via a bi-linear operation on the encodings, i.e.,\\n\\n$$p_\\\\theta(y|z(1), z(2)) = \\\\text{softmax} z(1) \\\\cdot W_y z(2)$$\\n\\nwhere $W_y$ is a learnable class-specific $10 \\\\times 10$ tensor of real entries, and the softmax is over all classes $y \\\\in \\\\{0, \\\\ldots, 17\\\\}$.\\n\\nConversely, the reasoning (top) layer of DeepProbLog is implemented as in Equation (5). Specifically, each $z(i)$ encodes the logits of the probability $p_\\\\theta(C_i|x(i))$ for the $i$-th digit, which we convert into a categorical probability distribution through a softmax activation.\\n\\nCLE4EVR: In a first step, we used Faster-RCNN (Ren et al., 2015) to extract the bounding boxes associated to objects in all images. The bounding box predictor is a pretrained MobileNet (Howard et al., 2017) fine-tuned on training images from the first task only, using the ground-truth bounding boxes of CLE4EVR images. The bounding box predictor is kept frozen in all successive tasks. We discarded all examples $x_i$ with less than 2 predicted bounding boxes. Concept supervision was transferred from the ground-truth bounding boxes to the predicted ones based on overlap. We scale each predicted bounding box to an image of size $28 \\\\times 28 \\\\times 3$ which is then passed to the encoder, implemented once again using a CNN with dropout with $p = 50\\\\%$ after each convolution layer and normal noise $\\\\epsilon \\\\sim N(0, 0.1)$ added to the final output. The architecture is the same as in Table 2, except that the input has depth 3 instead of 1 and that the bottleneck is 20-dimensional, with 10 dimensions allocated for the shape and 10 for the color of the input object, each with its own softmax to produce shape and color probability distributions. The DeepProbLog reasoning layer is as in Equation (5).\\n\\nDuring inference, the prediction returns invalid ($\\\\perp$) whenever the number of predicted bounding boxes is less than 2. We counted invalid predictions as wrong predictions in all metrics evaluated in the test set.\\n\\nB.4. Hyper-parameter Selection\\n\\nAll continual strategies have been trained with the same number of epochs and buffer dimension. The actual values depend on the specific benchmark: 25 epochs per task and a buffer size of 1000 examples for MNAdd-Seq, 100 epochs and 1000 examples for MNAdd-Shortcut, and 50 epochs each task and 250 examples for CLE4EVR. In all experiments, we employed the Adam optimizer (Kingma & Ba, 2015) combined with exponential decay ($\\\\gamma = 0.95$). The initial learning rate is restored at the beginning of a new task.\"}"}
{"id": "marconato23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For each data set, we optimized the weight of the concept supervision $w_c$ based on the Class-IL ($Y$) performance of ER using grid-search on the validation set (union of all tasks). Then, for each strategy, we selected the best learning rate and strategy-specific hyperparameters through a grid-search on the validation set, so as to optimize Class-IL ($Y$) on a single random seed. The learning rate was chosen from the range of $[10^{-5}, 10^{-2}]$. The exact values can be found in the source code.\\n\\nC. NeSy-CL Benchmarks\\n\\nIn this section, we provide a more detailed description of the benchmarks introduced in Section 5.\\n\\nC.1. MNAdd-Seq\\n\\nWe derived MNAdd-Seq from the MNIST-Addition data set of Manhaeve et al. (2018). Here, the knowledge encodes the following constraint:\\n\\n\\\\[ K = \\\\forall c_1, c_2 \\\\in \\\\{0, \\\\ldots, 9\\\\} (C_1 = c_1 \\\\land C_2 = c_2) \\\\Rightarrow Y = (c_1 + c_2) \\\\]  \\n\\n(22)\\n\\nfor a total of 19 possible sums. MNAdd-Seq is both label-incremental and concept-incremental; in each task only two possible sums appear, obtaining in total 9 tasks. Specifically:\\n\\n- Task 1: $Y \\\\in \\\\{0, 1\\\\}$ and $C \\\\in \\\\{0\\\\}$;\\n- Task 2: $Y \\\\in \\\\{2, 3\\\\}$ and $C \\\\in \\\\{0\\\\}$;\\n- Task 3: $Y \\\\in \\\\{4, 5\\\\}$ and $C \\\\in \\\\{0\\\\}$;\\n- Task 4: $Y \\\\in \\\\{6, 7\\\\}$ and $C \\\\in \\\\{0\\\\}$;\\n- Task 5: $Y \\\\in \\\\{8, 9\\\\}$ and $C \\\\in \\\\{0\\\\}$;\\n- Task 6: $Y \\\\in \\\\{10, 11\\\\}$ and $C \\\\in \\\\{0\\\\}$;\\n- Task 7: $Y \\\\in \\\\{12, 13\\\\}$ and $C \\\\in \\\\{0\\\\}$;\\n- Task 8: $Y \\\\in \\\\{14, 15\\\\}$ and $C \\\\in \\\\{0\\\\}$;\\n- Task 9: $Y \\\\in \\\\{16, 17\\\\}$ and $C \\\\in \\\\{0\\\\}$;\\n\\nIn total, the data set counts $42k$ training and $6k$ test examples.\\n\\nC.2. MNAdd-Shortcut\\n\\nThis benchmark is a case-study composed of two tasks, built considering the following constraints:\\n\\n- In the first task, we present only even digits and four possible sums: (i) $0 + 0 = 6$, (ii) $0 + 0 = 10$, (iii) $0 + 0 = 10$, and (iv) $0 + 0 = 12$.\\n- In the second task, only odd numbers are considered, i.e., $C \\\\in \\\\{0\\\\}$, and all their possible sums.\\n\\nThe rationale behind this construction is that it makes it possible to satisfy the knowledge in both tasks by leveraging reasoning shortcuts, and specifically those described in Appendix D.\\n\\nThe overall data set contains $13.8k$ training examples and $2k$ test data. We also collected an OOD test set containing examples not appearing in the training, validation and test sets, which comprise sums of odd and even digits, e.g., $0 + 0 = 1$, and unseen combinations of even numbers, e.g., $0 + 0 = 16$. \"}"}
{"id": "marconato23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\nFigure 3. Examples images for each tasks in the CLE4EVR benchmark.\\n\\nC.3. CLE4EVR\\n\\nFollowing Stammer et al. (2021), the CLE4EVR data set was generated using Blender (Blender Online Community, 2018), using additional objects from (Li & S\u00f8gaard, 2022) as well as three custom shapes. We generated different objects with 10 possible shapes and colors, and with 2 free variations on material \\\\{rubber, metal\\\\} and size \\\\{small, big\\\\}, that play no role in determining the label. Each image is of size $256 \\\\times 430 \\\\times 3$, and contains two non-overlapping objects over a light-gray background.\\n\\nTable 3 reports what combinations of objects appear in each task. All tasks are composed of $1.1$ training examples, $100$ validation examples, and $500$ test examples. Each task includes only two possible shapes (out of ten) and two colors (out of ten), without any overlap between tasks. An illustration is given in Figure 3.\\n\\nThe knowledge $K = K' \\\\land K'' \\\\land K'''$ encodes the following constraints:\\n\\n$K' = (C_{\\\\text{shape}}(1) = C_{\\\\text{shape}}(2)) \\\\iff \\\\text{same shape}$ (23)\\n\\n$K'' = (C_{\\\\text{color}}(1) = C_{\\\\text{color}}(2)) \\\\iff \\\\text{same color}$ (24)\\n\\n$K''' = \\\\text{same shape} \\\\land \\\\text{same color} \\\\iff \\\\text{same}$ (25)\\n\\nwith three output variables $Y_1 = \\\\text{same shape}$, $Y_2 = \\\\text{same color}$ and $Y_3 = \\\\text{same}$, giving rise to four mutually exclusive classes: $0 = \\\\text{different shape and color}$, $1 = \\\\text{same shape and different color}$, $2 = \\\\text{different shape and same color}$, $3 = \\\\text{same shape and same color}$.\\n\\nWe also generated an additional OOD test set, comprising $300$ images depicting unseen combination of training objects, e.g., redsquare and pinkdiamonds (which occur in none of the tasks). All OOD examples all have label $Y = (0, 0, 0)$.\\n\\nAll generated images come with ground-truth bounding boxes annotated with the properties (i.e., concepts) of the objects they contain, as well as annotations for $Y_1$, $Y_2$, and $Y_3$. The concept annotations are transferred to the bounding boxes predicted by Faster R-CNN during pre-processing, cf. Appendix B.\\n\\nTable 3. Task Organization in CLE4EVR\\n\\n| TASK | COLORS | SHAPES |\\n|------|--------|--------|\\n| 1    | red    | gray sphere, cube |\\n| 2    | green  | blue cylinder, tetrahedron |\\n| 3    | brown  | purple cone, triangular prism |\\n| 4    | yellow | cyan pyramid, toroid |\\n| 5    | orange | pink diamond, star prism |\"}"}
{"id": "marconato23a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\nOFFLINE\\n\\nREPLAY\\n\\nMETHODS\\n\\nFigure 4. Confusion matrices of the learned concepts at the end of the very last task, for a single run. Left: The confusion matrix on concepts for OFFLINE without concept supervision. It shows that it is very likely to opt for a reasoning shortcut. Right: Confusion matrices on a single run for all replay-based methods. Here, COOL is pictured in red, while DER, DER++, and ER in shades of blue. Only COOL retains the correct semantics of the concepts, whereas the other replay methods are very likely to pick the shortcut of OFFLINE.\\n\\nD. Shortcut Solutions in MNAdd-Shortcut and CLE4EVR\\n\\nIn this section, we provide a more detailed account on the shortcut solutions for the continual scenarios introduced.\\n\\nD.1. Reasoning Shortcuts Due to Low-Level Correlations\\n\\nBefore proceeding, we observe that simply predicting multiple concepts jointly by a single neural network is sufficient to enable reasoning shortcuts. Intuitively, this happens because, since the network has access to all properties of all objects, it can automatically exploit correlations between them to satisfy the knowledge without the need for extracting any \u201cproper\u201d concepts. For instance, in MNIST-Addition the knowledge can simply group pairs of digits (both of which it has access to) into two single combinations of concepts that always yield the right labels. To see this, consider the following example:\\n\\nExample D.1. Consider a single MNIST-Addition task consisting of digits summing to either 2 or 3. By Theorem 3.2, $\\\\Phi^*(K, D)$ contains $\\\\phi$ with $p_{\\\\phi}(C|x) \\\\not\\\\equiv p(C|x)$, such that:\\n\\n\\\\[\\np_{\\\\phi}(C_1|x) = \\\\begin{cases} 1 & \\\\text{if } C_1 = 2 \\\\\\\\ \\\\emptyset & \\\\text{otherwise} \\\\end{cases}\\n\\\\]\\n\\n\\\\[\\np_{\\\\phi}(C_2|x) = \\\\begin{cases} 1 & \\\\text{if } C_2 = 0 \\\\\\\\ 1 & \\\\text{if } C_2 = 1 \\\\end{cases}\\n\\\\]\\n\\nif $x = (0, 0)$ or $(0, 0)$ otherwise.\\n\\nThis distribution fits the data perfectly and is consistent with the knowledge, and thus cannot be distinguished from the ground-truth distribution based on likelihood alone.\\n\\nNotice that the two learned concepts ignore the value of the individual digits, and rather depend on the correlation between them. In MNIST-Addition, it is straightforward to avoid this situation by construction by simply processing the two digits independently. The same can be done when processing objects in CLE4EVR. This partially motivates our choice of neural architecture, described in Appendix B. More precisely, in MNIST-Addition the adopted architecture factorizes the joint probability distribution on the concepts in:\\n\\n\\\\[\\np(c_1, c_2|x^{(1)}, x^{(2)}) = p(c_1|x^{(1)}) \\\\cdot p(c_2|x^{(2)})\\n\\\\]\\n\\nWhile this is sufficient to guarantee independence among distinct objects, the prior knowledge can still admit reasoning shortcuts. Next, we describe the concrete reasoning shortcuts appearing in MNAdd-Shortcut and CLE4EVR.\"}"}
{"id": "marconato23a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\nIn particular, the problem of assigning the intended semantics of each learned concept can be expressed as a system of linear equations with five variables, which in task 1 of MNAdd-Seq can be written as:\\n\\n\\\\[\\n\\\\begin{align*}\\n&c_0 + c_6 = 6 \\\\\\\\\\n&c_4 + c_6 = 10 \\\\\\\\\\n&c_2 + c_8 = 10 \\\\\\\\\\n&c_4 + c_8 = 12 \\\\\\\\\\n\\\\end{align*}\\n\\\\]\\n\\n(27)\\n\\nThe first equation states that whatever values are assigned to the concepts that fire when a 0 and a 0 are present in the input \\\\(x\\\\), have to satisfy the condition that their sum is 6 (in all examples in which they appear). It should be clear that the linear system is undetermined and infinitely many real solutions can be found for \\\\(c_i\\\\), all of which except one do not capture the intended semantics of digits. One of these unintended solutions is very often picked by label-replay strategies. Specifically, the input can be easily confused for a 9, due to input similarity, which brings the model towards the assignment \\\\(c_4 = 9\\\\). This immediately yields the following unintentional mappings for all other digits: \\\\(c_0 = 5\\\\), \\\\(c_2 = 7\\\\), \\\\(c_6 = 1\\\\), and \\\\(c_8 = 3\\\\). This reasoning shortcut is often found when training offline on this task and also when training on both tasks of MNAdd-Shortcut, as done in our experiments, by ER, DER, and DER++, as shown in Figure 4.\\n\\nD.3. Shortcuts in CLE4EVR\\n\\nSeveral shortcut exist in CLE4EVR. Recall that:\\n\\n- CLE4EVR is composed of 5 tasks, with four possible outcomes (different objects, same shapes, same colors, and same objects).\\n- In each task only two possible shapes and colors are observed, and are never seen again in other tasks.\\n\\nIn order to correctly classify the same color and same shape labels, the model must acquire at least two distinct concepts for shape and two distinct concepts for colors in each task, but the knowledge provides little guidance as to which shapes or colors need to be associated to the input objects. This leaves ample room for reasoning shortcuts.\\n\\nLet us focus on shapes only. Letting \\\\(S\\\\) be the set of the 10 possible shapes \\\\(s_i\\\\), the model needs at least \\\\(10 \\\\cdot(10-1)/2\\\\) different negative examples of the knowledge to uniquely identify all possible shapes (up to permutation), one for each pair of mismatching shapes. Consider the map \\\\(\\\\pi: S \\\\rightarrow S\\\\) mapping from ground-truth shape of the input object to the learned concept for shape. Ideally, we would like \\\\(\\\\pi\\\\) to be injective, so that no two distinct ground-truth shapes are mapped to the same learned shape.\\n\\nConsider a task with 4 possible shapes \\\\(s_1 = \\\\text{cube}, s_2 = \\\\text{cone}, s_3 = \\\\text{cylinder}, \\\\text{and } s_4 = \\\\text{toroid}\\\\). In order to guarantee injectivity, the data has to include enough combinations of shapes so that the map \\\\(\\\\pi\\\\) satisfies the following condition:\\n\\n\\\\[\\n\\\\pi(s_1) \\\\neq \\\\pi(s_2), \\\\pi(s_1) \\\\neq \\\\pi(s_3), \\\\pi(s_1) \\\\neq \\\\pi(s_4), \\\\pi(s_2) \\\\neq \\\\pi(s_3), \\\\pi(s_2) \\\\neq \\\\pi(s_4), \\\\pi(s_3) \\\\neq \\\\pi(s_4).\\n\\\\]\\n\\n(28)\\n\\nNotice that this map is injective, in the sense that \\\\(s_i \\\\neq s_j \\\\Rightarrow \\\\pi(s_i) \\\\neq \\\\pi(s_j), \\\\forall i \\\\neq j\\\\). All tasks in CLE4EVR, however, are designed to distinguishing between only two possible shapes (and colors), hence the condition in Equation (28) is never satisfied. This is what allows for reasoning shortcuts.\\n\\nAs a matter of fact, without concept supervision, all values for shapes and colors are equally likely to be picked up to solve the task. We observed this phenomenon in the case of 0% supervision reported in Figure 5.\"}"}
{"id": "marconato23a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\nSHAPES COLORS LABELS\\n\\nFigure 5. Confusion matrices for COOL (in red) and the other replay strategies (in shades of blue) in CLE4EVR with no concept supervision, at the end of all tasks. All methods fail to attribute the correct semantics to the concepts and learn, instead, a shortcut which optimizes the Class-IL (Y).\\n\\nMNAdd-Seq CLE4EVR (BUFFER SIZE = 1000) (BUFFER SIZE = 250)\\n\\n| Method     | Time per Epoch | Memory Occupation |\\n|------------|----------------|-------------------|\\n| NA         | 0.238 s - 0.335 s | -6.280 Mb - 1.570 Mb |\\n| ER         | 0.385 s | 6.280 Mb |\\n| DER        | 0.475 s | 6.348 Mb |\\n| DER++      | 0.667 s | 6.356 Mb |\\n| COOL       | 0.476 s | 6.360 Mb |\\n\\nTable 4. Time per epoch and memory occupation for MNIST-Addition and CLE4EVR.\\n\\nE. Additional Results and Metrics\\n\\nE.1. Time Overhead and Memory Occupation of COOL\\n\\nWe evaluate the time overhead and the memory requirements of COOL compared to other replay-based strategies in Table 4. All these strategies impose an additional time overhead due to the selection and replay of past examples. We measured the time required for completing a single epoch in the first task of MNIST-Addition and CLE4EVR.\\n\\nNA provides the lower bound for the computation time, as it just fine-tunes the model on the new examples. ER in MNIST-Addition strategy requires almost $\\\\sim 0.1$ s more than NA for storing and replaying. In CLE4EVR, however, the gap is less evident between the two strategies. For MNAdd-Seq, the time per epoch of COOL amounts to $\\\\sim 0.476$ s, which is comparable to that of DER $\\\\sim 0.475$ s and lower than that of DER++ $\\\\sim 0.667$ s. For CLE4EVR, COOL requires $0.439$ s per epoch, slightly increasing compared to DER $\\\\sim 0.412$ s, but still being lower w.r.t. DER++ $\\\\sim 0.482$ s.\\n\\nIn terms of memory occupation, COOL requires slightly more memory than other replay strategies, due to the need of storing the logits of the learned concepts. However, most of the overhead is due to storing the instances themselves, which is the very same for all strategies and amounts to $6.272$ Mb for MNIST-Addition and $1.568$ Mb for CLE4EVR.\\n\\nE.2. MNAdd-Seq\\n\\nWe report in Table 5 results for all competing strategies and performance measures used (including those omitted in the main text), namely Class-IL and Task-IL on labels and concepts (denoted as Y and C, respectively), backward transfer (BWT), and forward transfer (FWT).\\n\\nThese results confirm the ones reported in Section 5. Specifically, COOL attains higher performance w.r.t. all metrics in both CBM and DeepProbLog. They also show that COOL outperforms all other approaches in terms of BWT when paired with DeepProbLog, and is the runner-up with CBMs.\\n\\nThe method with the best BWT on CBM is LWF, which however displays a pathological learning behavior, as made clear by the fact that it is the only method to have negative forward transfer. The reason is that LWF struggles to learn the first few tasks properly, but performs reasonably on the latter ones.\"}"}
{"id": "marconato23a", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We report here in Table 6, all results obtained for all strategies in (at around CLE4EVR)\\n\\nThe complete results for DER, and its lack of impact on completeness, we report the evolution across tasks of concept confusion matrices for all methods in Figure 6. The impact of accuracy only.\\n\\nOOD accuracy by a large margin, while the other approaches pick up the reasoning shortcut, thus achieving high label accuracy.\\n\\nVariance is also quite large for EWC adopting only DeepProbLog with increasing amount of concept supervision. This implies that LWF\\n\\nSurprisingly, this oddball behavior is sufficient for E.3.\\n\\nMNAdd-Shortcut E.3.1.\\n\\n59.1 68\\n\\nLWF 41.4 55.9\\n\\ndER 44 71\\n\\nEWC 71 \u00b1 49.7 \u00b1 0.1\\n\\nCOOL 23.8 \u00b1 12.9\\n\\nRESTART 21.7 \u00b1 12.6\\n\\nCBM 19.6 \u00b1 12.5\\n\\nLASS 13.1 \u00b1 7.3\\n\\nFor\"}"}
{"id": "marconato23a", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 7. CLE4EVR Full results\\n\\n| Method | Task 1 | Task 2 | Task 3 | Task 4 | Task 5 |\\n|--------|--------|--------|--------|--------|--------|\\n| SUPERVISION 0% | 44.9 \u00b1 0.7 | 9.2 \u00b1 1.2 | 25.0 \u00b1 0.9 | 43.5 \u00b1 2.6 | 19.9 \u00b1 0.1 |\\n| SUPERVISION 1% | 39.1 \u00b1 0.9 | 10.5 \u00b1 1.4 | 11.1 \u00b1 3.7 | 39.9 \u00b1 1.0 | 17.8 \u00b1 0.1 |\\n| SUPERVISION 10% | 41.5 \u00b1 7.1 | 8.3 \u00b1 3.2 | 15.7 \u00b1 8.4 | 38.5 \u00b1 6.4 | 17.2 \u00b1 1.4 |\\n| CLASS-IL(Y) | 47.1 \u00b1 0.8 | 6.6 \u00b1 2.2 | 27.2 \u00b1 2.8 | 41.9 \u00b1 1.8 | 19.1 \u00b1 2.9 |\\n| CLASS-IL(C) | 85.3 \u00b1 0.3 | 10.2 \u00b1 5.5 | 26.3 \u00b1 3.5 | 72.8 \u00b1 4.9 | 20.5 \u00b1 3.9 |\\n\\n**DER**\\n\\n| Method | Task 1 | Task 2 | Task 3 | Task 4 | Task 5 |\\n|--------|--------|--------|--------|--------|--------|\\n| SUPERVISION 0% | 77.7 \u00b1 1.4 | 7.7 \u00b1 2.8 | 27.5 \u00b1 2.7 | 73.3 \u00b1 3.0 | 19.1 \u00b1 1.9 |\\n| SUPERVISION 1% | 85.5 \u00b1 0.2 | 11.1 \u00b1 4.4 | 23.2 \u00b1 1.8 | 83.0 \u00b1 1.1 | 20.9 \u00b1 3.9 |\\n| SUPERVISION 10% | 80.7 \u00b1 5.7 | 8.5 \u00b1 4.5 | 25.6 \u00b1 2.3 | 83.2 \u00b1 0.5 | 85.9 \u00b1 2.9 |\\n\\n**COOL**\\n\\n| Method | Task 1 | Task 2 | Task 3 | Task 4 | Task 5 |\\n|--------|--------|--------|--------|--------|--------|\\n| SUPERVISION 0% | 70.7 \u00b1 5.7 | 8.5 \u00b1 4.5 | 25.6 \u00b1 2.3 | 83.2 \u00b1 0.5 | 85.9 \u00b1 2.9 |\\n| SUPERVISION 1% | 77.7 \u00b1 1.4 | 7.7 \u00b1 2.8 | 27.5 \u00b1 2.7 | 73.3 \u00b1 3.0 | 19.1 \u00b1 1.9 |\\n| SUPERVISION 10% | 85.5 \u00b1 0.2 | 11.1 \u00b1 4.4 | 23.2 \u00b1 1.8 | 83.0 \u00b1 1.1 | 20.9 \u00b1 3.9 |\\n\\n**Figure 6.** Dynamics of confusion matrices for shapes and colors encodings on CLE4EVR with 10% concept supervision. **COOL** in red preserves the correct concepts, while **DER** (in blue) suffers for shortcuts. Shape and color range in {0, ..., 9}.\"}"}
{"id": "marconato23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\nCOOL is designed to satisfy two desiderata: (D1) $p_{\u03b8} (C|X)$ should quickly approximate $p (C|X)$, and (D2) $p_{\u03b8} (C|X)$ should remain stable across tasks. D2 is straightforward, however we stress that it is only meaningful if D1 also holds: unless the learned concepts are high-quality, there is little benefit in remembering them.\\n\\nIn order to comply with D1, COOL makes use of a small number of densely annotated examples to quickly identify high-quality concepts, which \u2013 as we have shown \u2013 cannot always be guaranteed using knowledge alone. In practice, an average cross-entropy is added to the loss of these examples. To cope with D2, COOL implements a novel concept rehearsal strategy that stabilizes $p_{\u03b8} (C|X)$ across tasks.\\n\\nThis is motivated by the fact that concept stability helps to upper bound the average risk. Specifically, Theorem 4.1.\\n\\nConsider tasks $T_{1:t}$. If the current model $\u03b8$ and the past one $\u03b8_{t-1}$ assign non-zero likelihood to all examples in $D_{1:t}$, there exists a finite constant $\u03b3$, depending only on the model architecture, knowledge and data, such that the average risk in Equation (3) is upper bounded by:\\n\\n$$\\\\frac{1}{t} L(\u03b8, D_{(t)}) + \\\\frac{t-1}{t} L(\u03b8_{t-1}, D_{1:t}) + \u03b3 \\\\sum_{x, y \\\\in D_s} \\\\| p_{\u03b8} (C|x) - p_{\u03b8_{t-1}} (C|x) \\\\|_1$$\\n\\nIn words, this means that if the past model $\u03b8_{t-1}$ performs well on all past tasks (i.e., the middle term in Equation (6) is small), a new model that performs well on the current task (the first term is small) and whose concept distribution is close to that of the old model (the last term is small), also performs well on past tasks (the average risk in Equation (3) is small). Critically, this result holds regardless of how the prior knowledge $K_{1:t}$ of the various task is chosen.\\n\\nCOOL implement this requirement by combining the original training loss with an extra penalty $L_{COOL}$, defined as:\\n\\n$$L_{COOL} := \\\\frac{1}{|M|} \\\\sum_{x, \\\\tilde{q}_c, y \\\\in M} \u03b1 \\\\cdot KL (p_{\u03b8} (C|x), \\\\tilde{q}_c) - \u03b2 \\\\cdot \\\\log p_{\u03b8} (Y=y|x; K(t))$$\\n\\nHere, $M$ denotes the mini-batch of examples extracted from the replay buffer, $\u03b1$ denotes the scalar weight associated to the concept-reharsal strategy, and $\u03b2$ the weight of the replay strategy on $y$. The $KL$ term is evaluated between the predicted concept distribution and the stored one $\\\\tilde{q}_c = p_{\u03b8_{t-1}} (C|x)$. Notice that, by Pinsker's inequality, the $KL$ upper bounds the (square of the) $L_1$ distance, meaning that COOL indirectly optimizes the bound in Equation (6).\\n\\n4.1. Benefits and Limitations\\n\\nCOOL is explicitly designed to acquire high-quality concepts and retain them across tasks by combining knowledge, concept rehearsal, and a modicum of concept supervision. This substantially improves performance on past, future, and OOD tasks sharing these concepts, as demonstrated in Section 5.\\n\\nCOOL works even if the knowledge $K(t)$ changes across tasks and new concepts appear over time: these can be encoded as additional neural predicates in DeepProbLog, and COOL will take care of remembering the known concepts while leaving room to learn the new ones.\\n\\nOne limitation of COOL is that, in the general case, it requires a handful of densely annotated examples. The same requirement can be found in other settings where concept quality is critical. For instance, concept supervision is key in concept-based models \u2013 which strive to generate concept-level explanations for their predictions \u2013 to ensure the acquired concepts are interpretable (Koh et al., 2020; Chen et al., 2020; Marconato et al., 2022b). It is also a prerequisite for guaranteeing that learned representations acquired by general (deep) latent variable models are disentangled, as shown theoretically (Locatello et al., 2019) and empirically (Locatello et al., 2020). We stress that concept supervision is not required if knowledge and data disallow shortcut solutions, as is the case in MNAdd-Seq (see Section 5), although it does help avoiding sub-optimal parameters even in this case. If reasoning shortcuts are possible, however, concept supervision becomes essential, because \u2013 by construction \u2013 knowledge and labels alone are insufficient to pin down the correct semantics, hindering concept quality. Moreover, in many situations, annotating just some concepts is sufficient to rule out reasoning shortcuts.\\n\\n5. Empirical Evaluation\\n\\nWe address empirically the following research questions:\\n\\nQ1: Does knowledge help to stabilize the continual learning process and reduce the need for supervision?\\n\\nQ2: Does COOL help avoid reasoning shortcuts when knowledge alone fails, thus facilitating past and future continual performance?\\n\\nQ3: How much concept supervision does COOL need?\\n\\nTo answer these questions, we compared COOL against several representative continual strategies on three novel and challenging NeSy-CL benchmarks. Additional results and details on data sets, metrics, and hyperparameters can be found in the Appendices.\"}"}
{"id": "marconato23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is the problem introduced in Example 3.1 and fits a model from scratch for each task, without fine-tunes the old learning over the union of all is a simple two-task version of \\\\( \\\\text{MNAdd-Shortcut} \\\\) with several possible shapes, colors, materials, and sizes. As shown in Example 3.3 and further discussed in Appendix D, the four sums in the first task are not sufficient to include 4 types of examples: (i) \\\\( \\\\text{MNAdd-Shortcut} \\\\) and the second one only odd ones. In the first task we include only even digits \\\\( y \\\\), \\\\( x \\\\), \\\\( x \\\\)\u00b7 \\\\( x \\\\) \u00b7 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b7 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b7 \\\\( x \\\\) \u00b7 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b7 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b7 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b7 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b7 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b7 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b7 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b7 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b7 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x \\\\) \u00b1 \\\\( x"}
{"id": "marconato23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\n**C LASS-IL (Y)** C LASS-IL (C) OOD A CCURACY (Y)\\n\\n**Figure 2.** \\n\\nCOOL avoids shortcuts with few concept-annotated examples. \\n\\n- **Left:** Class-IL (on labels and concepts) and OOD accuracy for DER and COOL on MNAdd-Shortcut (top) and CLE4EVR (bottom). The x-axis is the % of concept annotated examples per task. \\n- **Right:** Confusion matrices of shape (top) and color (bottom), computed on the last task of CLE4EVR, obtained by DeepProbLog paired with COOL (in blue) vs. DER (in red) with 10% concept supervision. \\n\\nCOOL is the only strategy that acquires and maintains the intended semantics. The complete numerical results are reported in Appendix E. \\n\\nThe concepts using a learnable network independent of the knowledge, and serve as a purely neural baseline. Additional architectural choices are reported in Appendix B. The two models were trained for 25 epochs per task, using a fixed buffer size of 1000, but received different amounts of concept supervision: 10% for CBM s, which is enough to learn the intended concepts in the offline setting, and none for DeepProbLog. \\n\\nThe offline performance of CBM and DeepProbLog are excellent, achieving around 96% label accuracy and 99% concept accuracy, showing that both are capable of solving the learning task. In the continual setting, however, the gap between the neural and NeSy models widens noticeably. The results are reported in Table 1. Despite DeepProbLog being harder to learn than CBM s (as shown by RESTART and NA\u00a8IVE), all replay strategies \u2013 i.e., ER, DER, DER++, and COOL \u2013 perform much better when paired with DeepProbLog than CBMs: remarkably, label Class-IL sees gains close to 50% for DER, and similarly FWT for COOL. This highlights the benefits of knowledge, which apply despite DeepProbLog having access to no concept supervision. The regularization strategies LWF and EWC are not informative, as they struggle to improve on the NA\u00a8IVE and RESTART baselines both with and without knowledge. \\n\\nOverall, Table 1 indicates that when reasoning shortcuts are absent, knowledge facilitates identifying better concepts, and thus better predictions. By retaining these concepts, COOL manages to outperform all other competitors on both CBM s and DeepProbLog. The runner-up, DER, keeps up only when knowledge is available and with a substantial margin in terms of FWT (45% vs. 83%). In contrast, even though CBM s can acquire good concepts, this does not always yield good predictions, chiefly because the top layer undergoes forgetting, cf. Section 3.1. Thanks to knowledge, DeepProbLog avoids this issue altogether. \\n\\nThe only exception is LWF on CBM, which displayed pathological behavior, see Appendix E.\\n\\nOverall, Table 1 indicates that when reasoning shortcuts are absent, knowledge facilitates identifying better concepts, and thus better predictions. By retaining these concepts, COOL manages to outperform all other competitors on both CBM s and DeepProbLog. The runner-up, DER, keeps up only when knowledge is available and with a substantial margin in terms of FWT (45% vs. 83%). In contrast, even though CBM s can acquire good concepts, this does not always yield good predictions, chiefly because the top layer undergoes forgetting, cf. Section 3.1. Thanks to knowledge, DeepProbLog avoids this issue altogether. \\n\\nQ2: COOL avoids reasoning shortcuts. Next, we evaluate the impact of concept quality and rehearsal in MNAdd-Shortcut and CLE4EVR, which are affected by reasoning shortcuts. Given the sub-par performance of CBM s, we focus on DeepProbLog from now on. Also, we restrict our attention to COOL and DER, the runner up in the previous experiment. Results for all other competitors are available in Appendix E. For MNAdd-Shortcut we set a buffer size of 1000 and 100 epochs per task, and to 250 examples and 50 epochs for CLE4EVR. \\n\\nThe results in Figure 2 shows that, when no concept supervision is in place, the presence of shortcuts complicates retaining the correct concepts, as displayed by low values of Class-IL (C). This does not impact directly Class-IL (Y) in the case of CLE4EVR, but yields extremely low OOD generalization, around 10% for MNAdd-Shortcut and 25% for CLE4EVR. The effect on increasing concept supervision on DER is only seemingly positive, as label accuracy does improve in both data sets. However, Class-IL on concepts (about 20\u201350%) and OOD accuracy (10%\u201330%) are very poor, despite the supervision. What happens is precisely the issue depicted in Figure 1: the model acquires good concepts for one task, but \u2013 due to reasoning shortcuts and lack of concept rehearsal \u2013 these get corrupted when fitting on the next task. Since COOL retains the high quality\"}"}
{"id": "marconato23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\nconcepts identified via supervision, the latter leads to clear improvements in label accuracy, concept accuracy and **OOD** for **COOL**. As a result, **COOL** improves on **DER** by about +30% and +60% in terms of Class-IL (C) and +40% and +60% in OOD, in the two data sets respectively.\\n\\nWe stress that label-based strategies inevitably fall for reasoning shortcuts even if concept supervision is provided. This is clearly shown by the concept confusion matrices reported in Figure 2 (right). Notice that, out of all strategies, only **COOL** manages to prevent shortcuts. Further details are available in Appendix E.\\n\\nQ3: **COOL** requires minimal concept supervision. Figure 2 shows that **COOL** identifies high-quality concepts when given dense annotations for only 1% of the training set. This translates to about 30 examples per task in MNAdd-Shortcut, and to only 12 in CLE4EVR. Increasing concept supervision to 10% improves Class-IL (C) by 3% and shrinks its variance, but 1% is enough to substantially outperform **DER** in our tests.\\n\\n6. Related Work\\n\\nNeuro-symbolic integration. NeSy encompasses a diverse family of methods integrating learning and reasoning (De Raedt et al., 2021). Here, we focus on approaches for encouraging neural networks to output structured predictions consistent with prior knowledge. The two main strategies introduce an additional loss penalizing inconsistent predictions (Xu et al., 2018; Fischer et al., 2019; Ahmed et al., 2022b) or a top reasoning layer (Manhaeve et al., 2018; Giunchiglia & Lukasiewicz, 2020; Hoernle et al., 2022; Ahmed et al., 2022a). Since the former cannot guarantee that the model outputs consistent predictions, we focus on the latter. In either case, end-to-end training requires to differentiate through the knowledge. One option is to soften the knowledge using fuzzy logic (Diligenti et al., 2012; Donadello et al., 2017), but doing so can introduce semantic and learning artifacts (Giannini et al., 2018; van Krieken et al., 2022a). An alternative is to cast reasoning in terms of probabilistic logics (De Raedt & Kimmig, 2015), which preserves semantics and allows for sound inference and learning. DeepProbLog is just an example of NeSy strategies (Manhaeve et al., 2021; Huang et al., 2021; Winters et al., 2022; Ahmed et al., 2022a; van Krieken et al., 2022b). All NeSy approaches are offline and suffer from catastrophic forgetting, and existing continual strategies do not protect them from reasoning shortcuts, as shown in Section 5. Since these depend only on the latent nature of concepts, they affect probabilistic-logic and fuzzy logic architectures alike. **COOL** applies to all these, cf. Appendix A.4.\\n\\nContinual Learning. CL algorithms attempt to preserve model plasticity while mitigating catastrophic forgetting (Robins, 1995) using a variety of techniques (van de Ven et al., 2022; Qu et al., 2021). A first group of strategies, like Experience Replay (Riemer et al., 2019) and ER-ACE (Caccia et al., 2022), store and rehearse a limited amount of examples from previous tasks. Doing so ignores additional \u201cdark knowledge\u201d learned by the past model, so techniques like **DER** (Buzzega et al., 2020), **DER**++, and others (Rebuffi et al., 2017; Li & Hoiem, 2017; Castro et al., 2018; Hou et al., 2019), drop rehearsal in favor of distillation. **COOL** follows the same strategy. Popular alternatives include architectural approaches (Rusu et al., 2016), which freeze or add model parameters as needed, and regularization strategies (De Lange & Tuytelaars, 2021; Kirkpatrick et al., 2017; Aljundi et al., 2018; Zenke et al., 2017). These introduce extra penalties in the loss function to discourage changing parameters essential for discriminating classes, but can struggle with complex data (Aljundi et al., 2019). To the best of our knowledge, CL has only been tackled in flat prediction settings (e.g., classification), and existing strategies focus on preserving label accuracy only. The only work on forgetting in CBMs is (Marconato et al., 2022a), which however ignores knowledge altogether.\\n\\nReasoning shortcuts. In machine learning, \u201cshortcuts\u201d refer to models that exploit spurious correlations between inputs and annotations to achieve high training accuracy (Ross et al., 2017; Lapuschkin et al., 2019). Proposed solutions include dense annotations (Ross et al., 2017), out-of-domain data (Parascandolo et al., 2020), and interaction with annotators (Teso et al., 2022). Stammer et al. (2021) have investigated shortcuts in NeSy and proposed to fix them using knowledge, under the assumption that concepts are high-quality. We make no such assumption. Our work is the first to investigate reasoning shortcuts that knowledge cannot always prevent and their prominence in NeSy-CL.\\n\\n7. Conclusion\\n\\nWe initiated the study of Neuro-Symbolic Continual Learning and showed that knowledge, although useful, can be insufficient to prevent acquiring reasoning shortcuts that compromise concept semantics and cross-task transfer. Our approach, **COOL**, acquires and preserves high-quality concepts, attaining better concepts and performance than existing CL strategies in three new NeSy-CL benchmarks.\\n\\nAcknowledgements\\n\\nWe acknowledge the support of the MUR PNRR project FAIR - Future AI Research (PE00000013) funded by the NextGenerationEU. The research of AP and ST was partially supported by TAILOR, a project funded by EU Horizon 2020 research and innovation program under GA No 952215. We acknowledge the CINECA award under the...\"}"}
{"id": "marconato23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\nISCRA initiative, for the availability of high performance computing resources and support. The research of SC was partially supported by Italian Ministerial grant PRIN 2020 \u201cLEGO.AI: LEarning the Geometry of knOwledge in AI systems\u201d, n. 2020TA3K9N. The research of EF was partially supported by the European Union\u2019s Horizon 2020 research and innovation program DECIDER under Grant Agreement 965193. We acknowledge Angelo Porrello for his useful discussion with us.\\n\\nReferences\\n\\nAhmed, K., Teso, S., Chang, K.-W., Van den Broeck, G., and Vergari, A. Semantic Probabilistic Layers for Neuro-Symbolic Learning. In NeurIPS, 2022a.\\n\\nAhmed, K., Wang, E., Chang, K.-W., and Van den Broeck, G. Neuro-symbolic entropy regularization. In UAI, 2022b.\\n\\nAljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., and Tuytelaars, T. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.\\n\\nAljundi, R., Lin, M., Goujaud, B., and Bengio, Y. Gradient based sample selection for online continual learning. Advances in neural information processing systems, 32, 2019.\\n\\nBlender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.\\n\\nBoschini, M., Bonicelli, L., Buzzega, P., Porrello, A., and Calderara, S. Class-incremental continual learning into the extended der-verse. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\\n\\nBuzzega, P., Boschini, M., Porrello, A., Abati, D., and Calderara, S. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920\u201315930, 2020.\\n\\nCaccia, L., Aljundi, R., Asadi, N., Tuytelaars, T., Pineau, J., and Belilovsky, E. New insights on reducing abrupt representation change in online continual learning. ICLR, 2022.\\n\\nCastro, F. M., Mar\u00edn-Jim\u00e9nez, M. J., Guil, N., Schmid, C., and Alahari, K. End-to-end incremental learning. In Proceedings of the European conference on computer vision (ECCV), pp. 233\u2013248, 2018.\\n\\nChen, Z., Bei, Y., and Rudin, C. Concept whitening for interpretable image recognition. Nature Machine Intelligence, 2020.\\n\\nChoi, Y., Vergari, A., and Van den Broeck, G. Probabilistic circuits: A unifying framework for tractable probabilistic models. UCLA, 2020.\\n\\nDarwiche, A. and Marquis, P. A knowledge compilation map. Journal of Artificial Intelligence Research, 17:229\u2013264, 2002.\\n\\nDe Lange, M. and Tuytelaars, T. Continual prototype evolution: Learning online from non-stationary data streams. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8250\u20138259, 2021.\\n\\nDe Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., and Tuytelaars, T. A continual learning survey: Defying forgetting in classification tasks. PAMI, 2021.\\n\\nDe Raedt, L. and Kimmig, A. Probabilistic (logic) programming concepts. Machine Learning, 2015.\\n\\nDe Raedt, L., Duman\u010di\u0107, S., Manhaeve, R., and Marra, G. From statistical relational to neural-symbolic artificial intelligence. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 4943\u20134950, 2021.\\n\\nDiligenti, M., Gori, M., Maggini, M., and Rigutini, L. Bridging logic and kernel machines. Machine learning, 86(1):57\u201388, 2012.\\n\\nDonadello, I., Serafini, L., and Garcez, A. D. Logic tensor networks for semantic image interpretation. In IJCAI, 2017.\\n\\nFischer, M., Balunovic, M., Drachsler-Cohen, D., Gehr, T., Zhang, C., and Vechev, M. Dl2: Training and querying neural networks with logic. In International Conference on Machine Learning, pp. 1931\u20131941. PMLR, 2019.\\n\\nGiannini, F., Diligenti, M., Gori, M., and Maggini, M. On a convex logic fragment for learning and reasoning. IEEE Transactions on Fuzzy Systems, 2018.\\n\\nGiunchiglia, E. and Lukasiewicz, T. Coherent hierarchical multi-label classification networks. NeurIPS, 2020.\\n\\nGruber, T. R. Toward principles for the design of ontologies used for knowledge sharing? International journal of human-computer studies, 43(5-6):907\u2013928, 1995.\\n\\nHoernle, N., Karampatsis, R. M., Belle, V., and Gal, K. Multiplexnet: Towards fully satisfied logical constraints in neural networks. In AAAI, 2022.\\n\\nHou, S., Pan, X., Loy, C. C., Wang, Z., and Lin, D. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 831\u2013839, 2019.\"}"}
{"id": "marconato23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\nHoward, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. Mobilenets: Efficient convolutional neural networks for mobile vision applications. ArXiv, abs/1704.04861, 2017.\\n\\nHuang, J., Li, Z., Chen, B., Samel, K., Naik, M., Song, L., and Si, X. Scallop: From probabilistic deductive databases to scalable differentiable reasoning. NeurIPS, 2021.\\n\\nHusz\u00e1r, F. Note on the quadratic penalties in elastic weight consolidation. Proceedings of the National Academy of Sciences, 115(11):E2496\u2013E2497, 2018.\\n\\nJohnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., and Girshick, R. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017.\\n\\nKambhampati, S., Sreedharan, S., Verma, M., Zha, Y., and Guan, L. Symbols as a Lingua Franca for Bridging Human-AI Chasm for Explainable and Advisable AI Systems. In Proceedings of Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI), 2022.\\n\\nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In Bengio, Y. and LeCun, Y. (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\\n\\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.\\n\\nKoh, P. W., Nguyen, T., Tang, Y. S., Mussmann, S., Pierson, E., Kim, B., and Liang, P. Concept bottleneck models. In ICML, 2020.\\n\\nLapuschkin, S., W\u00e4ldchen, S., Binder, A., Montavon, G., Samek, W., and M\u00fcller, K.-R. Unmasking clever hans predictors and assessing what machines really learn. Nature communications, 10(1):1\u20138, 2019.\\n\\nLeCun, Y. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.\\n\\nLi, Z. and Hoiem, D. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 2017.\\n\\nLi, Z. and S\u00f8gaard, A. Qlevr: A diagnostic dataset for quantificational language and elementary visual reasoning. In Findings of NAACL, 2022.\\n\\nLocatello, F., Bauer, S., Lucic, M., Raetsch, G., Gelly, S., Sch\u00f6lkopf, B., and Bachem, O. Challenging common assumptions in the unsupervised learning of disentangled representations. In ICML, 2019.\\n\\nLocatello, F., Tschannen, M., Bauer, S., R\u00e4tsch, G., Sch\u00f6lkopf, B., and Bachem, O. Disentangling factors of variations using few labels. In International Conference on Learning Representations, 2020.\\n\\nManhaeve, R., Dumancic, S., Kimmig, A., Demeester, T., and De Raedt, L. DeepProbLog: Neural Probabilistic Logic Programming. NeurIPS, 2018.\\n\\nManhaeve, R., Marra, G., and De Raedt, L. Approximate inference for neural probabilistic logic programming. In KR, 2021.\\n\\nMarconato, E., Bontempo, G., Teso, S., Ficarra, E., Calderara, S., and Passerini, A. Catastrophic forgetting in continual concept bottleneck models. In Image Analysis and Processing. ICIAP 2022 Workshops: ICIAP International Workshops, Lecce, Italy, May 23\u201327, 2022, Revised Selected Papers, Part II, pp. 539\u2013547. Springer, 2022a.\\n\\nMarconato, E., Passerini, A., and Teso, S. Glancenets: Interpretable, leak-proof concept-based models. NeurIPS, 2022b.\\n\\nMisino, E., Marra, G., and Sansone, E. V AEL: Bridging Variational Autoencoders and Probabilistic Logic Programming. NeurIPS, 2022.\\n\\nParascandolo, G., Neitz, A., ORVIETO, A., Gresele, L., and Sch\u00f6lkopf, B. Learning explanations that are hard to vary. In International Conference on Learning Representations, 2020.\\n\\nParisi, G. I., Kemker, R., Part, J. L., Kanan, C., and Wermter, S. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54\u201371, 2019.\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\\n\\nQu, H., Rahmani, H., Xu, L., Williams, B., and Liu, J. Recent advances of continual learning in computer vision: An overview. arXiv preprint arXiv:2109.11369, 2021.\\n\\nRebuffi, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C. H. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 2001\u20132010, 2017.\"}"}
{"id": "marconato23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\nRen, S., He, K., Girshick, R., and Sun, J. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\\n\\nRiemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y., and Tesauro, G. Learning to learn without forgetting by maximizing transfer and minimizing interference. 2019.\\n\\nRobins, A. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 1995.\\n\\nRoss, A. S., Hughes, M. C., and Doshi-Velez, F. Right for the right reasons: training differentiable models by constraining their explanations. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pp. 2662\u20132670, 2017.\\n\\nRost, B., Liu, J., Nair, R., Wrzeszczynski, K. O., and Ofran, Y. Automatic prediction of protein function. Cellular and Molecular Life Sciences CMLS, 60(12):2637\u20132650, 2003.\\n\\nRusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Hadsell, R. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.\\n\\nStammer, W., Schramowski, P., and Kersting, K. Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3619\u20133629, 2021.\\n\\nTeso, S., Alkan, \u00d6., Stammer, W., and Daly, E. Leveraging explanations in interactive machine learning: An overview. arXiv preprint arXiv:2207.14526, 2022.\\n\\nvan de Ven, G. M., Tuytelaars, T., and Tolias, A. S. Three types of incremental learning. Nature Machine Intelligence, pp. 1\u201313, 2022.\\n\\nvan Krieken, E., Acar, E., and van Harmelen, F. Analyzing differentiable fuzzy logic operators. Artificial Intelligence, 2022a.\\n\\nvan Krieken, E., Thanapalasingam, T., Tomczak, J. M., van Harmelen, F., and Teije, A. t. A-nesi: A scalable approximate method for probabilistic neurosymbolic inference. arXiv preprint arXiv:2212.12393, 2022b.\\n\\nVergari, A., Choi, Y., Liu, A., Teso, S., and Van den Broeck, G. A compositional atlas of tractable circuit operations for probabilistic inference. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nVitter, J. S. Random sampling with a reservoir. ACM Transactions on Mathematical Software (TOMS), 11(1):37\u201357, 1985.\\n\\nWinters, T., Marra, G., Manhaeve, R., and De Raedt, L. DeepStochLog: Neural Stochastic Logic Programming. In AAAI, 2022.\\n\\nWu, Y., Chen, Y., Wang, L., Ye, Y., Liu, Z., Guo, Y., and Fu, Y. Large scale incremental learning. In CVPR, 2019.\\n\\nXu, J., Zhang, Z., Friedman, T., Liang, Y., and Broeck, G. A semantic loss function for deep learning with symbolic knowledge. In ICML, 2018.\\n\\nXu, Y., Yang, X., Gong, L., Lin, H.-C., Wu, T.-Y., Li, Y., and Vasconcellos, N. Explainable object-induced action decision for autonomous vehicles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9523\u20139532, 2020.\\n\\nZenke, F., Poole, B., and Ganguli, S. Continual learning through synaptic intelligence, 2017.\"}"}
{"id": "marconato23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1. Proof of Theorem 3.2\\n\\nTaking Equation (5) as reference, the log-likelihood of \\\\( D \\\\) can be rewritten as:\\n\\n\\\\[\\n\\\\log p(\\\\theta)(y|x; K) = \\\\sum_{(x, y) \\\\in D} \\\\log \\\\langle u_{K}(y|\\\\cdot), p_{\\\\theta}(\\\\cdot|x) \\\\rangle\\n\\\\]  (8)\\n\\nHere, the inner product runs over all possible values of \\\\( c \\\\). To see what the optima of this quantity look like, fix a single example \\\\((x, y) \\\\in D\\\\) and let \\\\( C_y \\\\) be the set of values \\\\( c \\\\) that satisfy the knowledge \\\\( K[Y/y] \\\\) and \\\\( C_{\\\\bar{y}} \\\\) be those that violate the knowledge. The likelihood of \\\\((x, y)\\\\) amounts to:\\n\\n\\\\[\\n\\\\langle u_{K}(y|\\\\cdot), p_{\\\\theta}(\\\\cdot|x) \\\\rangle = \\\\sum_{c \\\\in C_y} u_{K}(y|c)p_{\\\\theta}(c|x) + \\\\sum_{c \\\\in C_{\\\\bar{y}}} u_{K}(y|c) |=0|\\n\\\\]  (9)\\n\\nThe inner product is maximized whenever \\\\( p_{\\\\theta}(C|x) \\\\) allocates all probability mass to values \\\\( c \\\\) that satisfy \\\\( K[Y/y] \\\\), because the remaining ones do not contribute anything to the likelihood. Hence, in order to maximize Equation (8) it is sufficient that, for every \\\\((x, y) \\\\in D\\\\), \\\\( p_{\\\\theta}(C|x) \\\\) assigns zero probability to all concept configurations \\\\( c \\\\) that are ruled out by the knowledge \\\\( K[Y/y] \\\\).\\n\\n\\\\[\\\\square\\\\]\\n\\nRemarks: This result essentially states that DeepProbLog's reasoning layer has the same effect as the Semantic Loss (Xu et al., 2018) on the underlying neural network, and it is of independent interest. Notice that Theorem 3.2 also holds for non-uniform label distributions without any change to the proof.\\n\\nA.2. Proof of Theorem 4.1\\n\\nWe start by proving a general lemma:\\n\\nLemma A.1. Consider tasks \\\\( T_1, \\\\ldots, T_{(t)} \\\\) and two parameter configurations \\\\( \\\\phi, \\\\psi \\\\in \\\\Theta \\\\). If both models assign non-zero likelihood to all examples in \\\\( D_{1:t} \\\\), there exists a finite constant \\\\( \\\\gamma \\\\), depending only on the model architecture, knowledge and data, such that:\\n\\n\\\\[\\n|L(\\\\phi, T_1:t) - L(\\\\psi, T_1:t)| \\\\leq \\\\gamma \\\\sum_{s \\\\leq t} \\\\sum_{(x, y) \\\\in D_{(s)}} \\\\|p_{\\\\phi}(C|\\\\cdot) - p_{\\\\psi}(C|\\\\cdot)\\\\|_1\\n\\\\]  (10)\\n\\nProof. The left-hand side can be expanded to:\\n\\n\\\\[\\n\\\\sum_{s \\\\leq t} \\\\sum_{(x, y) \\\\in D_{(s)}} \\\\log p(\\\\phi)(y|x; K_{(s)}) - \\\\log p(\\\\psi)(y|x; K_{(s)})\\n\\\\]  (11)\\n\\nRecall that, for any \\\\( a, b \\\\in [\\\\beta, \\\\infty] \\\\), it holds that \\\\( |\\\\log a - \\\\log b| \\\\leq \\\\frac{1}{\\\\beta} |a - b| \\\\). For any \\\\((x, y)\\\\) and task \\\\( s \\\\leq t \\\\), it holds that:\\n\\n\\\\[\\n|\\\\log p(\\\\phi)(y|x; K_{(s)}) - \\\\log p(\\\\psi)(y|x; K_{(s)})| \\\\leq \\\\frac{1}{\\\\beta} p(\\\\phi)(y|x; K_{(s)}) - p(\\\\psi)(y|x; K_{(s)})\\n\\\\]  (12)\\n\\n\\\\[\\n= \\\\frac{1}{\\\\beta} \\\\langle u_{K_{(s)}}(y|\\\\cdot), p_{\\\\phi}(\\\\cdot|x) - p_{\\\\psi}(\\\\cdot|x) \\\\rangle \\\\leq \\\\frac{1}{\\\\beta} \\\\|u_{K_{(s)}}(y|\\\\cdot)\\\\|_{\\\\infty} \\\\cdot \\\\|p_{\\\\phi}(\\\\cdot|x) - p_{\\\\psi}(\\\\cdot|x)\\\\|_1\\n\\\\]  (13)\\n\\nThe first step follows by choosing \\\\( \\\\beta := \\\\min \\\\left\\\\{ \\\\min_{(x, y) \\\\in D_{1:t}} \\\\theta \\\\in \\\\{\\\\phi, \\\\psi\\\\} p(\\\\theta)(y|x; K_{(s)}) > 0 \\\\right\\\\} \\\\), the second one from Equation (5), and the last one from H\u00f6lder's inequality. By Equation (4), the max norm of \\\\( u_{K_{(s)}}(y|\\\\cdot) \\\\) amounts to:\\n\\n\\\\[\\n\\\\|u_{K_{(s)}}(y|\\\\cdot)\\\\|_{\\\\infty} = \\\\max_{c, 1 \\\\leq (c, y) \\\\leq K_{(s)}} Z(c; K_{(s)}) = 1 \\\\min_{c} Z(c; K_{(s)}) \\\\leq \\\\frac{1}{\\\\zeta} \\\\leq 1\\n\\\\]  (15)\\n\\n\\\\[12\\\\]\"}"}
{"id": "marconato23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce Neuro-Symbolic Continual Learning, where a model has to solve a sequence of neuro-symbolic tasks, that is, it has to map sub-symbolic inputs to high-level concepts and compute predictions by reasoning consistently with prior knowledge. Our key observation is that neuro-symbolic tasks, although different, often share concepts whose semantics remains stable over time. Traditional approaches fall short: existing continual strategies ignore knowledge altogether, while stock neuro-symbolic architectures suffer from catastrophic forgetting. We show that leveraging prior knowledge by combining neuro-symbolic architectures with continual strategies does help avoid catastrophic forgetting, but also that doing so can yield models affected by reasoning shortcuts. These undermine the semantics of the acquired concepts, even when detailed prior knowledge is provided upfront and inference is exact, and in turn continual performance. To overcome these issues, we introduce COOL, a Concept-level Continual Learning strategy tailored for neuro-symbolic continual problems that acquires high-quality concepts and remembers them over time. Our experiments on three novel benchmarks highlights how COOL attains sustained high performance on neuro-symbolic continual learning tasks in which other strategies fail.\\n\\n1. Introduction\\n\\nWe initiate the study of Neuro-Symbolic Continual Learning (NeSy-CL), in which the goal is to solve a sequence of neuro-symbolic tasks. As is common in neuro-symbolic (NeSy) prediction (Manhaeve et al., 2018; Xu et al., 2018; Giunchiglia & Lukasiewicz, 2020; Hoernle et al., 2022; Ahmed et al., 2022a), the machine is provided prior knowledge relating one or more target labels to symbolic, high-level concepts extracted from sub-symbolic data, and has to compute a prediction by reasoning over said concepts. The central challenge of NeSy-CL is that the data distribution and the knowledge may vary across tasks. E.g., in medical diagnosis knowledge may encode known relationships between possible symptoms and conditions, while different tasks are characterized by different distributions of X-ray scans, symptoms and conditions. The goal, as in continual learning (CL) (Parisi et al., 2019), is to obtain a model that attains high accuracy on new tasks without forgetting what it has already learned under a limited storage budget.\\n\\nExisting approaches are insufficient for NeSy-CL: neuro-symbolic models are designed for offline learning and as such suffer from catastrophic forgetting (Parisi et al., 2019), while continual learning strategies are designed for neural networks that neglect prior knowledge, preventing applications to tasks where compliance with regulations is key, e.g., safety critical tasks (Ahmed et al., 2022a; Hoernle et al., 2022). It is tempting to tackle NeSy-CL by pairing a SotA neuro-symbolic architecture, such as DeepProbLog (Manhaeve et al., 2018), with a proven rehearsal or distillation strategy, for instance dark experience replay (Buzzega et al., 2020). This yields immediate benefits, in the sense that prior knowledge makes the model more robust to catastrophic forgetting, as we will show. However, we show also that it is flawed, because it cannot prevent the model from acquiring reasoning shortcuts (defined in Section 3), through which it attains high task accuracy by acquiring unintended concepts with task-specific semantics, as illustrated in Figure 1. In turn, reasoning shortcuts entail poor cross-task transfer. Our key observation is that, even though neuro-symbolic tasks may differ in terms of knowledge and distribution, the semantics of the concepts they rely on must remain stable.\"}"}
{"id": "marconato23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\nK same \\\\(\\\\neq\\\\) 0\\n\\n\\\\((\\\\quad, \\\\quad)\\\\)\\n\\n\\\\(n\\\\) \\\\(Y = 0\\\\)\\n\\n\\\\(Y = 1\\\\)\\n\\n\\\\(Y = 0\\\\)\\n\\n\\\\(Y = 1\\\\)\\n\\n\\\\(\\\\text{True}\\\\)\\n\\n\\\\(\\\\text{Pred}\\\\)\\n\\n??\\n\\nTask 1 Examples\\n\\n\\\\(\\\\text{True}\\\\)\\n\\n\\\\(\\\\text{Pred}\\\\)\\n\\nTask 2 Examples Future Task\\n\\n\u2639\\n\\nFigure 1.\\n\\nLeft: DeepProbLog extracts concepts \\\\(c\\\\) from a sub-symbolic input \\\\(x\\\\) and reasons over prior knowledge \\\\(K\\\\) provided upfront to obtain a prediction \\\\(y\\\\).\\n\\nRight: Simplified illustration of our CLE4EVR benchmark, restricted to color concepts only. The full setting is reported in Section 5. The goal is to predict whether two objects have the same color. The first task includes examples of gray and red objects only, and the model classifies them by learning the intended mapping between input colors and concepts. The second one includes green and blue objects only, and the model can learn a reasoning shortcut mapping the four colors to two concepts, achieving high accuracy on both tasks but compromising performance on future tasks. This is exactly the issue addressed by our approach, COOL.\\n\\nPrompted by this insight, we propose COOL, a simple but effective COncept-level cOntinual Learning strategy, that aims at acquiring high-quality concepts and preserving them across tasks. COOL makes use of a small amount of concept supervision to acquire high-quality concepts and explicitly preserves them with a concept rehearsal strategy, avoiding reasoning shortcuts in all tasks. COOL is applicable to a variety of NeSy architectures, and \u2013 as shown by our experiments with DeepProbLog (Manhaeve et al., 2018) and Concept-based Models (Koh et al., 2020) \u2013 easily outperforms state-of-the-art continual strategies on three novel, challenging NeSy-CL problems, achieving better concept quality and predictive accuracy on past and OOD tasks.\\n\\nContributions.\\n\\nSummarizing, we:\\n\\n1. Introduce neuro-symbolic continual learning as a novel and challenging machine learning problem.\\n\\n2. We show that knowledge readily improves forgetting in some scenarios, but also that it is insufficient to prevent reasoning shortcuts \u2013 which worsen forgetting and compromise transfer to new tasks \u2013 in others.\\n\\n3. Propose new NeSy-CL benchmarks for evaluating continual performance with and without reasoning shortcuts.\\n\\n4. Introduce COOL, a novel continual strategy that supports identifying concepts with the intended semantics and preserves them across tasks.\\n\\n5. Show empirically that COOL outperforms state-of-the-art continual strategies on these challenging benchmarks.\\n\\n2. Neuro-Symbolic Continual Learning\\n\\nNotation.\\n\\nThroughout, we indicate scalar constants \\\\(x\\\\) in lower-case, random variables \\\\(X\\\\) in upper case, and ordered sets of constants \\\\(x\\\\) and random variables \\\\(X\\\\) in bold typeface. The symbol \\\\([n]\\\\) stands for the set \\\\(\\\\{1, \\\\ldots, n\\\\}\\\\) and \\\\(x|\\\\neq K\\\\) indicates that \\\\(x\\\\) satisfies a logical formula \\\\(K\\\\). We say that a distribution \\\\(p(\\\\cdot|\\\\cdot;K)\\\\) is consistent with \\\\(K\\\\), written \\\\(p|\\\\neq K\\\\), if it holds that \\\\(p(a|b;K) > 0\\\\) implies \\\\((a,b)|\\\\neq K\\\\), i.e., if \\\\(p\\\\) associates zero mass to all states that violate \\\\(K\\\\).\\n\\n2.1. Problem Statement\\n\\nWe are concerned with solving a sequence of neuro-symbolic prediction tasks, each requiring to learn a classifier mapping a (partially) sub-symbolic input \\\\(x\\\\in\\\\mathbb{R}^d\\\\) to \\\\(n\\\\geq 1\\\\) labels \\\\(y\\\\in\\\\mathbb{N}^n\\\\). What makes them neuro-symbolic is that:\\n\\n(i) The labels \\\\(y\\\\) depend entirely on the state of \\\\(k\\\\) symbolic concepts \\\\(c = (c_1, \\\\ldots, c_k)\\\\top\\\\) capturing high-level aspects of the input \\\\(x\\\\).\\n\\n(ii) The concepts \\\\(c\\\\) depend on the sub-symbolic input \\\\(x\\\\) in an intricate manner and are best extracted using deep learning techniques.\\n\\n(iii) The way in which the labels \\\\(y\\\\) depend on the concepts \\\\(c\\\\) is specified by prior knowledge \\\\(K\\\\), necessitating reasoning during inference.\\n\\nWe make the natural assumption that the semantics of the concepts appearing in the various tasks remain constant over time. This assumption lies at the heart of knowledge representation and ontology design, where concepts serve as a lingua franca for the exchange and reuse of knowledge across application boundaries (Gruber, 1995) and with human stakeholders (Kambhampati et al., 2022).\\n\\nFormally, each task \\\\(t\\\\in\\\\mathbb{N}\\\\) is defined by a data generating distribution \\\\(p(t)(X, C, Y; K(t))\\\\) that factorizes as:\\n\\n\\\\[\\np(t)(Y|C;K(t)) \\\\cdot p(C|X) \\\\cdot p(t)(X)\\n\\\\]\\n\\nHere, \\\\(K(t)\\\\) denotes the knowledge relevant to the \\\\(t\\\\)-th task.\\n\\nAs customary in NeSy, we assume the knowledge to correlate with the input distribution. The knowledge might depend also on discrete variables in \\\\(X\\\\); we suppress this dependency in the notation for readability.\"}"}
{"id": "marconato23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As in regular continual learning, we assume storage is\\nwhere $T$ (1). It decomposes prediction into two steps, cf. Fig-\\nmodel ideally suited to solve tasks of the form in Equa-\\nDeepProbLog (Manhaeve et al., 2018) is a state-of-the-art\\n2.2. DeepProbLog\\nshown in Section 3, insufficient in NeSy-CL.\\naccuracy on the labels only\\noptimizing the\\nloss function), or architectural modifications (Rusu et al.,\\nrighting down parameter shift through additional terms in the\\nregularization (Husz\u00e1r, 2018; Li & Hoiem, 2017) (i.e., slow-\\net al., 2017) (i.e., replaying few examples of previous tasks),\\nstrategies like rehearsal (Buzzega et al., 2020; Parisi et al.,\\nprior tasks. CL algorithms mitigate forgetting using\\nels tend to forget the information necessary to solve the\\nforgetting\\ncatastrophic\\npercent.\\nInadequate in NeSy-CL\\noccurs, play different roles in\\nK\\nlabels, and the knowledge\\nCritically, the distribution of observed inputs, concepts and\\nsems are stable. For instance, if $C$ does not depend on\\nfor all $t$\\np\\nrespective prior knowledge, i.e.,\\ntherefore, the label distributions are\\nconsistent\\nrectly describe the ground-truth generative process and that,\\nfor all $t$\\n\\n$c_1$ and\\n$c_2$,\\n\\n$c_1 = 5$\\n$c_2 = 9$\\n$c_{i+1} = c_i + 4$\\n$c_{i+2} = c_i + 8$\\n$c_{i+3} = c_i + 12$\\n$c_{i+4} = c_i + 16$\\n$c_{i+5} = c_i + 20$\\n$c_{i+6} = c_i + 24$\\n$c_{i+7} = c_i + 28$\\n$c_{i+8} = c_i + 32$\\n$c_{i+9} = c_i + 36$\\n$c_{i+10} = c_i + 40$\\n$c_{i+11} = c_i + 44$\\n$c_{i+12} = c_i + 48$\\n$c_{i+13} = c_i + 52$\\n$c_{i+14} = c_i + 56$\\n$c_{i+15} = c_i + 60$\\n$c_{i+16} = c_i + 64$\\n$c_{i+17} = c_i + 68$\\n$c_{i+18} = c_i + 72$\\n$c_{i+19} = c_i + 76$\\n$c_{i+20} = c_i + 80$\\n$c_{i+21} = c_i + 84$\\n$c_{i+22} = c_i + 88$\\n$c_{i+23} = c_i + 92$\\n$c_{i+24} = c_i + 96$\\n$c_{i+25} = c_i + 100$\\n$c_{i+26} = c_i + 104$\\n$c_{i+27} = c_i + 108$\\n$c_{i+28} = c_i + 112$\\n$c_{i+29} = c_i + 116$\\n$c_{i+30} = c_i + 120$\\n$c_{i+31} = c_i + 124$\\n$c_{i+32} = c_i + 128$\\n$c_{i+33} = c_i + 132$\\n$c_{i+34} = c_i + 136$\\n$c_{i+35} = c_i + 140$\\n$c_{i+36} = c_i + 144$\\n$c_{i+37} = c_i + 148$\\n$c_{i+38} = c_i + 152$\\n$c_{i+39} = c_i + 156$\\n$c_{i+40} = c_i + 160$\\n$c_{i+41} = c_i + 164$\\n$c_{i+42} = c_i + 168$\\n$c_{i+43} = c_i + 172$\\n$c_{i+44} = c_i + 176$\\n$c_{i+45} = c_i + 180$\\n$c_{i+46} = c_i + 184$\\n$c_{i+47} = c_i + 188$\\n$c_{i+48} = c_i + 192$\\n$c_{i+49} = c_i + 196$\\n$c_{i+50} = c_i + 200$\\n$c_{i+51} = c_i + 204$\\n$c_{i+52} = c_i + 208$\\n$c_{i+53} = c_i + 212$\\n$c_{i+54} = c_i + 216$\\n$c_{i+55} = c_i + 220$\\n$c_{i+56} = c_i + 224$\\n$c_{i+57} = c_i + 228$\\n$c_{i+58} = c_i + 232$\\n$c_{i+59} = c_i + 236$\\n$c_{i+60} = c_i + 240$\\n$c_{i+61} = c_i + 244$\\n$c_{i+62} = c_i + 248$\\n$c_{i+63} = c_i + 252$\\n$c_{i+64} = c_i + 256$\\n$c_{i+65} = c_i + 260$\\n$c_{i+66} = c_i + 264$\\n$c_{i+67} = c_i + 268$\\n$c_{i+68} = c_i + 272$\\n$c_{i+69} = c_i + 276$\\n$c_{i+70} = c_i + 280$\\n$c_{i+71} = c_i + 284$\\n$c_{i+72} = c_i + 288$\\n$c_{i+73} = c_i + 292$\\n$c_{i+74} = c_i + 296$\\n$c_{i+75} = c_i + 300$\\n$c_{i+76} = c_i + 304$\\n$c_{i+77} = c_i + 308$\\n$c_{i+78} = c_i + 312$\\n$c_{i+79} = c_i + 316$\\n$c_{i+80} = c_i + 320$\\n$c_{i+81} = c_i + 324$\\n$c_{i+82} = c_i + 328$\\n$c_{i+83} = c_i + 332$\\n$c_{i+84} = c_i + 336$\\n$c_{i+85} = c_i + 340$\\n$c_{i+86} = c_i + 344$\\n$c_{i+87} = c_i + 348$\\n$c_{i+88} = c_i + 352$\\n$c_{i+89} = c_i + 356$\\n$c_{i+90} = c_i + 360$\\n$c_{i+91} = c_i + 364$\\n$c_{i+92} = c_i + 368$\\n$c_{i+93} = c_i + 372$\\n$c_{i+94} = c_i + 376$\\n$c_{i+95} = c_i + 380$\\n$c_{i+96} = c_i + 384$\\n$c_{i+97} = c_i + 388$\\n$c_{i+98} = c_i + 392$\\n$c_{i+99} = c_i + 396$\\n$c_{i+100} = c_i + 400$\\n$c_{i+101} = c_i + 404$\\n$c_{i+102} = c_i + 408$\\n$c_{i+103} = c_i + 412$\\n$c_{i+104} = c_i + 416$\\n$c_{i+105} = c_i + 420$\\n$c_{i+106} = c_i + 424$\\n$c_{i+107} = c_i + 428$\\n$c_{i+108} = c_i + 432$\\n$c_{i+109} = c_i + 436$\\n$c_{i+110} = c_i + 440$\\n$c_{i+111} = c_i + 444$\\n$c_{i+112} = c_i + 448$\\n$c_{i+113} = c_i + 452$\\n$c_{i+114} = c_i + 456$\\n$c_{i+115} = c_i + 460$\\n$c_{i+116} = c_i + 464$\\n$c_{i+117} = c_i + 468$\\n$c_{i+118} = c_i + 472$\\n$c_{i+119} = c_i + 476$\\n$c_{i+120} = c_i + 480$\\n$c_{i+121} = c_i + 484$\\n$c_{i+122} = c_i + 488$\\n$c_{i+123} = c_i + 492$\\n$c_{i+124} = c_i + 496$\\n$c_{i+125} = c_i + 500$\\n$c_{i+126} = c_i + 504$\\n$c_{i+127} = c_i + 508$\\n$c_{i+128} = c_i + 512$\\n$c_{i+129} = c_i + 516$\\n$c_{i+130} = c_i + 520$\\n$c_{i+131} = c_i + 524$\\n$c_{i+132} = c_i + 528$\\n$c_{i+133} = c_i + 532$\\n$c_{i+134} = c_i + 536$\\n$c_{i+135} = c_i + 540$\\n$c_{i+136} = c_i + 544$\\n$c_{i+137} = c_i + 548$\\n$c_{i+138} = c_i + 552$\\n$c_{i+139} = c_i + 556$\\n$c_{i+140} = c_i + 560$\\n$c_{i+141} = c_i + 564$\\n$c_{i+142} = c_i + 568$\\n$c_{i+143} = c_i + 572$\\n$c_{i+144} = c_i + 576$\\n$c_{i+145} = c_i + 580$\\n$c_{i+146} = c_i + 584$\\n$c_{i+147} = c_i + 588$\\n$c_{i+148} = c_i + 592$\\n$c_{i+149} = c_i + 596$\\n$c_{i+150} = c_i + 600$\\n$c_{i+151} = c_i + 604$\\n$c_{i+152} = c_i + 608$\\n$c_{i+153} = c_i + 612$\\n$c_{i+154} = c_i + 616$\\n$c_{i+155} = c_i + 620$\\n$c_{i+156} = c_i + 624$\\n$c_{i+157} = c_i + 628$\\n$c_{i+158} = c_i + 632$\\n$c_{i+159} = c_i + 636$\\n$c_{i+160} = c_i + 640$\\n$c_{i+161} = c_i + 644$\\n$c_{i+162} = c_i + 648$\\n$c_{i+163} = c_i + 652$\\n$c_{i+164} = c_i + 656$\\n$c_{i+165} = c_i + 660$\\n$c_{i+166} = c_i + 664$\\n$c_{i+167} = c_i + 668$\\n$c_{i+168} = c_i + 672$\\n$c_{i+169} = c_i + 676$\\n$c_{i+170} = c_i + 680$\\n$c_{i+171} = c_i + 684$\\n$c_{i+172} = c_i + 688$\\n$c_{i+173} = c_i + 692$\\n$c_{i+174} = c_i + 696$\\n$c_{i+175} = c_i + 700$\\n$c_{i+176} = c_i + 704$\\n$c_{i+177} = c_i + 708$\\n$c_{i+178} = c_i + 712$\\n$c_{i+179} = c_i + 716$\\n$c_{i+180} = c_i + 720$\\n$c_{i+181} = c_i + 724$\\n$c_{i+182} = c_i + 728$\\n$c_{i+183} = c_i + 732$\\n$c_{i+184} = c_i + 736$\\n$c_{i+185} = c_i + 740$\\n$c_{i+186} = c_i + 744$\\n$c_{i+187} = c_i + 748$\\n$c_{i+188} = c_i + 752$\\n$c_{i+189} = c_i + 756$\\n$c_{i+190} = c_i + 760$\\n$c_{i+191} = c_i + 764$\\n$c_{i+192} = c_i + 768$\\n$c_{i+193} = c_i + 772$\\n$c_{i+194} = c_i + 776$\\n$c_{i+195} = c_i + 780$\\n$c_{i+196} = c_i + 784$\\n$c_{i+197} = c_i + 788$\\n$c_{i+198} = c_i + 792$\\n$c_{i+199} = c_i + 796$\\n$c_{i+200} = c_i + 800$\\n$c_{i+201} = c_i + 804$\\n$c_{i+202} = c_i + 808$\\n$c_{i+203} = c_i + 812$\\n$c_{i+204} = c_i + 816$\\n$c_{i+205} = c_i + 820$\\n$c_{i+206} = c_i + 824$\\n$c_{i+207} = c_i + 828$\\n$c_{i+208} = c_i + 832$\\n$c_{i+209} = c_i + 836$\\n$c_{i+210} = c_i + 840$\\n$c_{i+211} = c_i + 844$\\n$c_{i+212} = c_i + 848$\\n$c_{i+213} = c_i + 852$\\n$c"}
{"id": "marconato23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-Symbolic Continual Learning\\n\\n3.1. We introduce MNAdd-Seq, a continual extension of MNIST-Addition in which tasks differ in what digits are observed. Specifically, each task \\\\( t = 0, \\\\ldots, 8 \\\\) consists of all pairs of digits \\\\((x_1, x_2)\\\\) whose sum is either \\\\( 2t \\\\) or \\\\( 2t + 1 \\\\). By construction, sums above 9 cannot be obtained by adding smaller digits, so these no longer occur in later tasks. Since DeepProbLog maximizes the likelihood of the current task, it quickly forgets small digits at \\\\( t \\\\geq 5 \\\\) and can no longer classify sums involving them correctly.\\n\\n3.1. Knowledge Helps Remembering\\n\\nA natural first step toward solving NeSy-CL is to bundle a NeSy predictor \u2013 say, DeepProbLog \u2013 with any state-of-the-art CL strategy. Focusing on experience replay, doing so amounts to storing a handful of well chosen labeled (or predicted) examples \\\\((x, y)\\\\) from past tasks \\\\( 1, \\\\ldots, t-1 \\\\), and replaying them when fitting DeepProbLog on the current task \\\\( t \\\\) together with the corresponding prior knowledge \\\\( K_t \\\\).\\n\\nDoing so immediately brings a number of benefits. First and foremost, the knowledge encodes the valid, stable relationship between the concepts and the labels to be prediction. This implies that predicted concepts can be always correctly mapped to a corresponding label, and that this inference step is immune from forgetting. This is especially significant considering that the top layers of neural networks are those most affected by catastrophic forgetting (Wu et al., 2019). This effect is clearly visible in our experiments, cf. Section 5.\\n\\nConversely, prior knowledge effectively reduces the space of candidate concepts, providing further guidance to the model. If it reduces the space to only those having the intended semantics, then this simple setup can be very effective at tackling NeSy-CL problems.\\n\\n3.2. But Does Not Prevent Reasoning Shortcuts\\n\\nIn general, however, this setup is insufficient. The core issue is that knowledge might not be enough to identify the right concept distribution \\\\( p(C|X) \\\\) using label annotations alone, in the sense that \u2013 depending on how the knowledge and training data are structured \u2013 it may be possible (in both offline and continual settings) to correctly classify all training examples even using concepts with unintended semantics. We refer to these situations as reasoning shortcuts.\\n\\nThis intuition is formalized in Theorem 3.2. Here, we write \\\\( \\\\Theta \\\\) to indicate the set of all possible parameters of (the neural networks implementing) \\\\( p(\\\\theta(C|X)) \\\\), and \\\\( \\\\Theta^* (K,D) \\\\subseteq \\\\Theta \\\\) for the parameters that maximize the log-likelihood \\\\( L(\\\\theta,D) \\\\).\\n\\nThe only non-trivial aspect is that, in addition to the replay buffer, we also has to store the past knowledge, so as to ensure be able to match the updated concepts with the past labels. For instance, in MNIST-Addition \\\\( K[Y/2] \\\\) amounts to \\\\( C_1 + C_2 = 2 \\\\).\\n\\nTheorem 3.2. A model with parameters \\\\( \\\\theta \\\\) attains maximal likelihood, i.e., \\\\( \\\\theta \\\\in \\\\Theta^* (K,D) \\\\), if and only if, for all \\\\((x, y)\\\\) \\\\( \\\\in D \\\\), it holds that \\\\( p(\\\\theta(C|x) = K[Y/y] \\\\).\\n\\nAll proofs can be found in Appendix A. Theorem 3.2 states that, as long as the concept distribution output by the learned neural network satisfies the knowledge for each training example, the log-likelihood is maximal.\\n\\n5. The ground-truth concept distribution \\\\( p(C|X) \\\\) is a possible solution, but it is not necessarily the only one. In this case, fitting DeepProbLog \u2013 and indeed any NeSy approach that optimizes for label accuracy only \u2013 does not guarantee that the learned concepts have the correct semantics. To see this, consider the following example.\\n\\nExample 3.3. Consider MNIST-Addition and take a subset \\\\( D \\\\) including only pairs of examples of four possible sums: \\\\( X + Y = 6 \\\\), \\\\( X + Y = 10 \\\\), \\\\( X + Y = 10 \\\\), and \\\\( X + Y = 12 \\\\). Then, there exist many concept distributions that satisfy the knowledge on all examples, including:\\n\\n- \\\\( 7 \\\\rightarrow 0 \\\\)\\n- \\\\( 7 \\\\rightarrow 2 \\\\)\\n- \\\\( 7 \\\\rightarrow 4 \\\\)\\n- \\\\( 7 \\\\rightarrow 6 \\\\)\\n- \\\\( 7 \\\\rightarrow 8 \\\\)\\n- \\\\( 7 \\\\rightarrow 5 \\\\)\\n- \\\\( 7 \\\\rightarrow 7 \\\\)\\n- \\\\( 7 \\\\rightarrow 9 \\\\)\\n- \\\\( 7 \\\\rightarrow 1 \\\\)\\n- \\\\( 7 \\\\rightarrow 3 \\\\)\\n\\nwhere the remaining concepts are allocated arbitrarily and \\\\( x \\\\rightarrow c \\\\) is a shorthand for \\\\( p(C = c|x) = 1 \\\\). Only the first distribution has the intended semantics, whereas the second one is a reasoning shortcut. We remark that DeepProbLog does acquire this shortcut in practice, as illustrated by our experiments. Appendix D explains how shortcuts emerge in the data sets used in our experiments.\\n\\nNotice that the Theorem applies to both offline learning (i.e., \\\\( D \\\\) is fixed) and NeSy-CL (i.e., \\\\( D \\\\) indicates the training set of any given task). Yet, reasoning shortcuts are especially impactful in the latter. This is exemplified in Figure 1 (right). Here, DeepProbLog has learned high-quality concepts to solve the first task, but quickly forgets them when solving the second task, precisely because it falls prey of a reasoning shortcut that achieves high training and rehearsal accuracy on both tasks by satisfying the knowledge using concepts with unintended semantics. We provide additional concrete examples in Appendix D. In turn, reasoning shortcuts can dramatically affect forgetting and performance on future and OOD NeSy tasks, as shown by our experiments.\\n\\n4. Addressing NeSy-CL with COOL\\n\\nTo this end, we introduce COOL, a COncept-level cOntinual Learning that acquires concepts with the intended semantics and preserves them over time, attaining sustained high\"}"}
